% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\newcommand{\wq}[1]{\textcolor{orange}{#1}}
\newcommand{\wqc}[1]{\textcolor{orange}{[WQ: #1]}}
% other packages
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\MYCOMMENT}[1]{/* #1 */}
\usepackage{bm}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{booktabs}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song \\
Department of Computer Science and Engineering, HKUST, Hong Kong SAR, China\\
\texttt{\{cliei, wwangbw, tzhengad, yqsong\}@cse.ust.hk}
}

% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Inductive reasoning, a cornerstone of human cognition, enables generalization from limited data but hasn't yet been fully achieved by large language models (LLMs). 
While modern LLMs excel at reasoning tasks, their ability to maintain stable and consistent rule abstraction under imperfect observations remains underexplored. 
To fill this gap, in this work, we introduce \textbf{Robust Rule Induction}, a task that evaluates LLMs' capability in inferring rules from data that are fused with noisy examples. To address this task,  we further propose Sample-steered Rule Refinement (SRR), a method enhancing reasoning stability via observation diversification and execution-guided feedback. 
Experiments across arithmetic, cryptography, and list functions reveal: (1) SRR outperforms other methods with minimal performance degradation under noise; 
(2) Despite slight accuracy variation, LLMs exhibit instability under noise (e.g., $0\%$ accuracy change with only $70\%$ consistent score);
(3) Counterfactual task gaps highlight LLMs' reliance on memorized patterns over genuine abstraction. 
Our findings challenge LLMs' reasoning robustness, revealing susceptibility to hypothesis drift and pattern overfitting, while providing empirical evidence critical for developing human-like inductive systems. Code and data are available at \href{https://github.com/lcy2723/Robust-Rule-Induction}{https://github.com/lcy2723/Robust-Rule-Induction}.
\end{abstract}

\section{Introduction}
Inductive reasoning\textemdash the cognitive capacity to generalize from specific instances to universal principles, is fundamental to human intelligence~\cite{Lake_Ullman_Tenenbaum_Gershman_2017}. 
This ability enables humans to abstract latent patterns from sparse data while maintaining robustness against conflicting evidence~\cite{Feldman1997TheSO}, as exemplified by children mastering linguistic rules despite exposure to occasional grammatical errors.
The robustness of inductive reasoning manifests in resistance to pattern interference (preserving learned rules when encountering contradictory examples) and tolerance to observational imperfections (maintaining stable performance under noisy learning conditions). Understanding and quantifying this robustness becomes increasingly crucial as artificial systems approach complex real-world applications where clean data remains elusive. 

Recent advances in large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks~\cite{gpt4o,deepseekai2024deepseekv3technicalreport}, reigniting interest in comparing machine and human reasoning paradigms~\cite{collins_building_2024}. While contemporary studies showcase LLMs' proficiency in inductive reasoning, their capacity for robustness remains questionable. Unlike humans who can rapidly converge on correct rules through Bayesian hypothesis updating~\cite{doi:10.1126/science.1192788,doi:10.1126/science.aab3050}, current models often exhibit unstable reasoning trajectories when noise disrupts their thinking process~\cite{zhou2024can}. A growing body of evidence suggests that LLMs primarily engage in pattern matching rather than genuine reasoning~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,wu-etal-2024-reasoning}.

Despite increasing attention to LLM inductive reasoning capabilities, current works exhibit limitations that obscure true inductive reasoning robustness. 
First, in prevailing evaluations~\cite{pmlr-v139-alet21a,ijcai2024p693}, models just predict outputs for novel inputs given exemplars, bypassing explicit rule verification. It fails to diagnose where and why rule induction fails. Existing studies that do generate intermediate rules either disregard noisy learning conditions or assess performance solely through aggregate metrics like task accuracy gap. This neglects instance-level reasoning consistency\textemdash whether models maintain stable rule interpretations when exposed to conflicting patterns, which is a key indicator of human-like robust induction. Besides, prior methods often repurpose existing benchmarks (e.g., SCAN;~\citealt{pmlr-v80-lake18a}, ARC;~\citealt{chollet2019measureintelligence}) without modification, where potential data contamination may undermine validity as models could use memorized solutions.

To address these gaps, we introduce a novel task of robust rule induction, which challenges models to identify underlying functions from input-output exemplars containing controlled noise injections. Our approach features three key innovations: (1) An evaluation pipeline from data synthesis to automatic evaluation, requiring models to explicitly output rules and enabling direct validation through programmatic execution. (2) A multi-dimensional robustness assessment with conventional accuracy and instance-level metrics. (3) A sample-steered refinement (SRR) methodology that improves both the capability and stability of LLMs' inductive reasoning through diverse sampling and diagnostic refinement. 
 
We curate three datasets spanning arithmetic, cryptography, and list operations\textemdash domains requiring progressively abstract rule formalization. The proposed SRR substantially improves performance across most tasks and maintains stability compared to other methods. However, comprehensive experiments reveal a critical dichotomy: while the accuracy variance achieves less than $5.0\%$, the consistency metric exposes fundamental instability. Furthermore, LLMs exhibit significant performance inconsistency with the same task category, performing considerably better on more familiar tasks than others, despite comparable complexity. Correct predictions may stem from recited patterns rather than stable rule abstraction. These findings challenge the ``real'' reasoning capabilities of LLMs.


\section{Preliminary}
\subsection{Problem Definitions}
We formulate robust rule induction as the problem of identifying latent mapping functions from input-output pairs while maintaining consistent reasoning performance under noisy conditions. 
While prior works~\cite{qiu2024phenomenal,wang2024hypothesis} focus on basic rule discovery capabilities, we specifically investigate the robustness of inductive reasoning when the observations contain conflicting patterns.
Some examples are shown in Figure~\ref{fig:dataset_example}. The observations may contain both normal and noisy examples, and the model is required to infer the correct rule regardless of the noise.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{figures/data_examples.pdf}
    \caption{Example instances with noise and rules from Arithmetic$_{\text{base-7}}$, Cryptography$_{\text{Caesar}}$ and List Functions. }
    \label{fig:dataset_example}
\end{figure}

Formally, given a dataset $\mathcal{D}=\{(x_i,y_i)\}_{i=1}^N$ where $x\in\mathcal{X}$ denotes inputs and $y\in\mathcal{Y}$ represents outputs generated by an underlying function $f:\mathcal{X}\rightarrow\mathcal{Y}$, the objective is to induce an approximation $\hat{f}$ from observed examples $\mathcal{D}_{\text{seen}}\subseteq\mathcal{D}$. 
Robust rule induction should satisfy: $\hat{f}(x)=f(x)$ for $(x,y)\in \mathcal{D}_{\text{unseen}}$ despite $\mathcal{D}_{\text{seen}}$ containing noisy samples $\mathcal{D}_{\text{noise}}$ where $y\neq f(x)$.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/overview.pdf}
    \caption{Evaluation pipeline exemplified by base-9 addition, consisting of three stages: (1) Data Synthesis, generating normal, noisy and test examples; (2) Model Inference, prompting models with seen examples to induce rules in Python function form; (3) Performance Evaluation, executing induced rules on test examples to assess correctness and robustness under noise.}
    \label{fig:evaluation_pipeline}
\end{figure*}
\subsection{Evaluation Pipeline}
As illustrated in figure~\ref{fig:evaluation_pipeline}, to evaluate the robustness of inductive reasoning in large language models, we propose a novel evaluation pipeline consisting of three main stages: data synthesis, model inference, and performance evaluation. In the data synthesis stage, due to the characteristics of our data, normal examples, noisy examples, and test examples for each instance in the dataset can be automatically generated by programs given the rule and noise definitions. The first two are mixed to form seen examples, while the latter is used to evaluate the model's reasoning ability. In the model inference stage, we prompt the language model with seen examples in standard input-output (IO) format, and the induced rules are restricted to Python function form for automatic evaluation. Finally, we evaluate the rule by executing it on the test examples. The instance is considered solved if all the test examples are correctly solved. More details about the evaluation pipeline, like the data synthesis process, can be found in appendix~\ref{sec:app_eval}.

We use task accuracy, which is the proportion of solved instances over the total number of instances, and its change under different conditions as the evaluation metrics.  While task accuracy is a comprehensive metric to evaluate the model's reasoning ability, it may not fully capture the robustness of the model at the instance level. To address this, we introduce the consistency score, which is defined as follows:
\begin{equation*}
    \text{Consistency Score} := \frac{1}{N}\sum_{i=1}^N\mathbb{I}[Sol_i^{\text{c}}=Sol_i^{\text{n}}]
\end{equation*}
where $N$ is the total number of instances in the dataset, $\mathbb{I}[\cdot]$ is the indicator function, $Sol_i^{\text{c}}$ is whether the $i$-th instance is solved without noise, $1$ if solved and $0$ otherwise, and $Sol_i^{\text{n}}$ is whether the $i$-th instance is solved with noise. This metric quantifies the model's ability to maintain stable reasoning conclusions under noise and offers a more granular view of the model's robustness.

\section{Sample-steered Rule Refinement}
To address noisy rule induction, we propose Sample-steered Rule Refinement (SRR), a novel framework that combines hypothesis generation, contrastive evidence sampling, and iterative self-correction. This method conducts inductive reasoning through three-phase optimization: (1) \textit{contrastive hypothesis generation} to bootstrap diverse rule candidates, (2) \textit{diagnostic feedback construction} through evidence-aware sampling, and (3) \textit{iterative rule refinement} using failure-driven corrections. The full algorithm is detailed in Algorithm~\ref{alg:srr}.

\begin{algorithm}[ht]
\small
\caption{SRR Framework}
\label{alg:srr}
\begin{algorithmic}
\REQUIRE Seen examples $\mathcal{D}_{\text{seen}}$, LLM $M$, max iterations $T$, max subsets $K$, threshold $\tau$
\ENSURE Best hypothesis $\hat{f}^*$ (in Python function form)
\STATE $\mathcal{H}_0 \leftarrow \emptyset$
\FOR{$k = 1$ \TO $K$}
    \STATE $\mathcal{D}_k \leftarrow \text{SampleSubset}(\mathcal{D}_{\text{seen}})$
    \STATE $\mathcal{H}_0 \leftarrow \mathcal{H}_0 \cup \{M(\mathcal{D}_k, \text{"Generate rule function"})\}$
\ENDFOR
\STATE $\mathcal{H}_0 \leftarrow \mathcal{H}_0 \cup \{M(\mathcal{D}_{\text{seen}}, \text{"Generate rule function"})\}$

\STATE $\hat{f}_0 \leftarrow \arg\max_{h\in\mathcal{H}_0} \text{Acc}(h, \mathcal{D}_{\text{seen}})$

\FOR{$t = 1$ \TO $T$}
    \IF{$\text{Acc}(\hat{f}_{t-1}, \mathcal{D}_{\text{seen}}) \geq \tau$}
        \RETURN $\hat{f}_{t-1}$
    \ENDIF
    \STATE $\mathcal{C}_t \leftarrow \text{CorrectExamples}(\hat{f}_{t-1}, \mathcal{D}_{\text{seen}})$
    \STATE $\mathcal{E}_t \leftarrow \text{WrongExamples}(\hat{f}_{t-1}, \mathcal{D}_{\text{seen}})$
    \STATE $\text{Feedback} \leftarrow \{\mathcal{C}_t[:n], \mathcal{E}_t[:m]\}$ \MYCOMMENT{Sample n,m cases}
    
    \STATE $h_{\text{new}} \leftarrow M(\hat{f}_{t-1}, \text{Feedback}, \text{"Revise rule"})$
    \IF{$\text{Acc}(h_{\text{new}}, \mathcal{D_{\text{seen}}}) > \text{Acc}(\hat{f}_{t-1}, \mathcal{D_{\text{seen}}})$}
        \STATE $\hat{f}_t \leftarrow h_{\text{new}}$
    \ELSE
        \STATE $\hat{f}_t \leftarrow \hat{f}_{t-1}$
    \ENDIF
\ENDFOR

\RETURN $\hat{f}^* = \arg\max_{t}\text{Acc}(\hat{f}_t, \mathcal{D}_{\text{seen}})$
\end{algorithmic}
\end{algorithm}

\paragraph{Diversity-aware Hypothesis Generation} We first generate $K$ hypotheses from random subsets of the seen examples plus one hypothesis using the full observations. This step ensures coverage of both local patterns and global consistency. The number of examples in the subset is less than the full seen set, thus the noise in the subset has a greater impact. The sampled subset may also contain completely noise-free examples, which can better distinguish the rules.

\paragraph{Diagnostic Feedback Construction} After generating the initial hypotheses, the induced rule is executed on the seen examples via an external Python interpreter. We then collect the correct and incorrect cases and sample these cases as feedback. We focus more on the incorrect cases, as they provide more information about the rule refinement, while the correct cases are used as positive feedback to reinforce the rule. The hypothesis with the highest accuracy on the seen examples is selected as the initial selected rule.

\paragraph{Iterative Rule Refinement} At each iteration, the model receives the current selected rule with formatted feedback to generate a new rule. The new hypothesis is compared to the previous one, and the more accurate one is selected. This iterative process continues until the accuracy of the rule on the seen examples exceeds a predefined threshold or the maximum number of iterations is reached.


\section{Experiments}
In this section, we evaluate the robustness of inductive reasoning in language models on different tasks. We also compare the performance of the models under different noise levels and analyze the effectiveness of the different methods in enhancing the robustness of inductive reasoning.

\subsection{Experimental Setup}
Consistent with previous studies~\cite{wang2024hypothesis,qiu2024phenomenal}, we adopt few-shot prompting to assess the models' inductive capabilities. Each instance contains $10$ normal examples, $5$ noisy examples, and $10$ test examples. The normal and noisy examples are mixed to form $10$ seen examples, which serve as prompts. This approach preserves task semantics while introducing controlled perturbations, thereby simulating real-world scenarios where observational data often contains inherent imperfections. LLMs are explicitly informed that examples may contain some noise. We formulate the output of LLMs as Python functions and execute them on the test examples to automatically evaluate the inferred rules. More details about the experiments can be found in the appendix~\ref{sec:app_exp}.

\subsection{Datasets}
We use three datasets with their subsets: Arithmetic, Cryptography, and List Functions. These datasets offer different rule induction challenges, including mathematical calculations, symbol representation, and list operations. Comprehensive statistics are shown in Table~\ref{tab:data_statistic}.


\begin{table}[ht]
    \small
    \centering
    \begin{adjustbox}{max width=1\linewidth}
    {
    \begin{tabular}{c|c|c|ccc}
    \toprule
        \textbf{Dataset} & \textbf{Subset} & \# \textbf{Tasks} & \# \textbf{Normal} & \# \textbf{Noise} & \# \textbf{Test}\\
    \midrule
        \multirow{3}{*}{Arith.} & 7-base & $100$ & $1000$ & $500$ & $1000$ \\
         & 8-base & $100$ & $1000$ & $500$ & $1000$  \\
         & 9-base & $100$ & $1000$ & $500$ & $1000$ \\
    \midrule
        \multirow{3}{*}{Crypto.} & Caesar & $100$ & $1000$ & $500$ & $1000$ \\
         & Atbash & $100$ & $1000$ & $500$ & $1000$ \\
         & Keyboard & $100$ & $1000$ & $500$ & $1000$ \\
    \midrule
       List Func.  & N/A & $250$ & $2500$ & $1250$ & $2500$ \\
    \bottomrule
    \end{tabular}
    }\end{adjustbox}
    \caption{The number of tasks and examples per dataset. Arith., Crypto., List Func. denote Arithmetic, Cryptography and List Functions respectively.}
    \label{tab:data_statistic}
\end{table}

\paragraph{Arithmetic} 
The arithmetic task, proposed by ~\cite{wu-etal-2024-reasoning}, is a counterfactual two-digit addition task. Instead of the common base-10 system, the task uses base-8, 9, and 11 as the counterfactual setup to control the difficulty and avoid the impact of memorization over reasoning. In this task, we focus on the base-7, 8, 9 systems, and the noisy example is the common 10-based equations. The input of each example is two two-digit numbers in the corresponding base, connected by a plus sign, and the output is the sum of the two numbers. To ensure the model can reason correctly, we guarantee that there must be a carry-over in the addition process so that the model must reason about the carry-over instead of simply adding the numbers. 

\paragraph{Cryptography} 
We use three types of substitution ciphers: Caesar, Atbash, and Keyboard. The Caesar cipher is a cipher that shifts the alphabet by a fixed number of positions. The Atbash cipher is a cipher that replaces each letter with the letter symmetrically opposite in the alphabet (e.g. A$\rightarrow$Z). The Keyboard cipher replaces letters according to their positions in the alphabet with the corresponding positions on the keyboard (e.g. A$\rightarrow$Q, $B\rightarrow$W). The input of each example is a word, and the output is the encrypted text. We randomly replace the characters in the output with other characters to generate noisy examples.

\paragraph{List Functions} 
The list functions dataset~\cite{rule2020child} evaluates the concept learning ability in the domain of cognitive science. The task is to induce a function that maps a list of numbers to another list of numbers. The dataset has $250$ tasks. Each item in the dataset corresponds to a list manipulation operation, such as sorting, reversing, or filtering. We randomly generate the examples of each task under controlled conditions to ensure the rule can be induced. The noisy examples are generated by randomly replacing the numbers in the output list with other numbers.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/consistency_score_model.pdf}
    \caption{Consistency score$(\%)$ with clean data of different models on the Cryptography and List Functions datasets under different noise levels.}
    \label{fig:consistency_score}
\end{figure*}

\begin{table}[ht]
\small
\centering
\begin{adjustbox}{max width=1\linewidth}
{
\begin{tabular}{l|ccc|c}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Cryptography}} & \multirow{2}{*}{\textbf{List Functions}} \\
\cmidrule(l){2-4}
&  Caesar & Atbash & Keyboard & \\
\midrule
GPT-4o-mini & $28.7_{_{\pm1.2}}$ & $6.3_{_{\pm1.2}}$ & $0.0_{\pm0.0}$ & $31.3_{\pm0.2}$ \\
 \hspace{10pt}\texttt{10\% noise} & $32.0_{\pm2.4}$&$12.7_{\pm2.5}$& $3.7_{\pm0.5}$ & $28.4_{\pm1.4}$ \\
 \hspace{10pt}\texttt{20\% noise} & $29.0_{\pm0.8}$&$13.3_{\pm2.6}$&$1.3_{\pm1.2}$& $23.5_{\pm1.0}$ \\
 \hspace{10pt}\texttt{30\% noise} & $21.3_{\pm0.9}$&$16.0_{\pm0.0}$&$3.3_{\pm0.5}$& $19.7_{\pm0.2}$ \\
GPT-4o & $68.3_{\pm0.5}$ & $48.0_{\pm2.2}$ & $4.7_{\pm0.5}$ & $39.9_{\pm0.5}$ \\
 \hspace{10pt}\texttt{10\% noise} &$75.3_{\pm0.5}$&$34.7_{\pm0.9}$&$3.7_{\pm0.5}$& $36.9_{\pm1.0}$ \\
 \hspace{10pt}\texttt{20\% noise} &$75.3_{\pm2.1}$&$27.3_{\pm0.5}$&$1.7_{\pm0.9}$& $31.9_{\pm0.8}$ \\
 \hspace{10pt}\texttt{30\% noise} &$64.7_{\pm4.7}$&$22.7_{\pm2.4}$&$2.3_{\pm1.2}$& $26.1_{\pm0.5}$ \\
% open source
DeepSeek-V3 &  $50.0_{\pm1.6}$ & $21.7_{\pm2.5}$ & $25.0_{\pm0.8}$ & $43.1_{\pm0.4}$ \\
 \hspace{10pt}\texttt{10\% noise} & $24.3_{\pm1.7}$ & $15.3_{\pm2.4}$ & $15.7_{\pm1.2}$ & $40.8_{\pm0.9}$ \\
 \hspace{10pt}\texttt{20\% noise} & $15.3_{\pm3.7}$ & $21.3 _{\pm2.5}$ & $11.3_{\pm1.2}$ & $35.3_{\pm 1.9}$ \\
 \hspace{10pt}\texttt{30\% noise} & $11.0_{\pm0.8}$ & $16.7_{\pm0.5}$ & $11.3_{\pm3.1}$ & $30.5_{\pm0.7}$ \\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{Task accuracy ($\%$) on Cryptography (including Caesar, Atbash, and Keyboard subtasks) and List Functions under different noise levels (proportion of noise in seen examples). Results are shown in mean $\pm$ standard deviation over 3 independent runs.}
\label{tab:robust_noise_level}
\end{table}

\subsection{Robustness Under Different Noise Levels}
\label{subsec:noise_level}
We first evaluate the robustness of inductive reasoning in language models under different noise injection ratios. Noise levels are defined as the proportion of noisy examples in the seen examples. We test three representative models: GPT-4o-mini~\cite{gpt4omini}, GPT-4o~\cite{gpt4o}, and DeepSeek-V3~\cite{deepseekai2024deepseekv3technicalreport} and ask the models to directly infer the rules from the seen examples without any additional output. Since the models cannot solve the Arithmetic task under the Direct Output (DO) setting, we present the results for the Cryptography and List Functions. The results are shown in Table~\ref{tab:robust_noise_level}. To better evaluate the robustness of the models, we further investigate the consistency score with clean data under different noise levels, as shown in Figure~\ref{fig:consistency_score}. According to the results, we have the following observations.

First, contrary to conventional assumptions, \textbf{noise introduction does not universally degrade performance, instead, the models exhibit performance fluctuations}, demonstrating their inherent sensitivity to conflicting patterns. For example, GPT-4o achieves improved accuracy on Caesar cipher tasks at $10\%$ noise ($7.0\%$ absolute improvement over clean data), while List Functions exhibit monotonic performance decay with increasing noise levels. In some cases, moderate noise improves performance, consistent with the findings of~\citet{zhou2024can}.

With the exception of tasks where models fundamentally struggle, experimental results demonstrate \textbf{a decline in consistency scores as noise levels escalate, and it declines more sharply than its task accuracy variation}. This discrepancy indicates noise introduces bidirectional reasoning instability: models not only fail on previously solvable instances (noise interference) but also succeed on originally challenging cases (incorrect generalization), leading to different performance changes.

\begin{table*}[ht]
\small
\centering
\begin{adjustbox}{max width=1\linewidth}
{
\begin{tabular}{l|l|ccc|ccc|c}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Arithmetic}} & \multicolumn{3}{c|}{\textbf{Cryptography}} & \multirow{2}{*}{\textbf{List Functions}} \\
\cmidrule(l){3-8}
 & & 7-base & 8-base & 9-base & Caesar & Atbash & Keyboard & \\
\midrule
\multirow{5}{*}{GPT-4o} & DO & $0.0$ & $0.0$ & $0.0$ & $75.3(\uparrow 7.0)$ & 
$34.7(\downarrow 13.3)$ & $3.7(\downarrow 1.0)$ & $36.9(\downarrow 3.0)$ \\
& CoT & $3.0 (\leftrightarrow \underline{0.0})$ & $8.0 (\downarrow 14.0)$ & $8.0(\uparrow 5.0)$ & $\textbf{85.0} (\uparrow 1.0)$ & $20.0(\downarrow 7.0)$ & $4.0 (\leftrightarrow \underline{0.0})$ & $40.4(\downarrow 5.2)$ \\
& SC & $0.0 (\downarrow 1.0)$ & $6.0 (\downarrow 7.0)$ & $0.0 (\downarrow 3.0)$ & $\textbf{85.0} (\leftrightarrow \underline{0.0})$ & $29.0 (\downarrow 11.0)$ & $5.0 (\downarrow 2.0)$ & $42.4 (\downarrow 3.2)$ \\
& SR & $1.0 (\downarrow 4.0)$ & $18.0 (\downarrow \underline{4.0})$ &$4.0 (\downarrow \underline{2.0})$ & $81.0 (\downarrow 2.0)$ & $23.0 (\downarrow 8.0)$ & $5.0 (\leftrightarrow 0.0)$ & $39.6 (\downarrow 4.4)$\\
& Ours & $\textbf{5.0} (\downarrow 1.0)$ & $\textbf{51.0}(\uparrow 6.0)$ & $\textbf{19.0}(\downarrow \underline{2.0})$ & $\textbf{85.0} (\uparrow 3.0)$ & $\textbf{52.0}(\downarrow \underline{3.0})$ & $\textbf{9.0} (\uparrow 1.0)$ & $\textbf{57.2} (\downarrow \underline{1.6})$\\
\midrule
\multirow{5}{*}{Deepseek-V3} & DO & $0.0$ & $0.0$ & $0.0$ & $24.3(\downarrow 25.7)$ & $15.3(\downarrow 6.4)$ & $\textbf{15.7}(\downarrow 9.3)$ & $40.8(\downarrow \underline{2.3})$ \\
& CoT & $77.5(\downarrow 6.0)$ & $83.0(\downarrow 13.0)$ & $67.5(\downarrow 14.0)$ & $80.5(\downarrow 4.0)$ & $26.0(\downarrow 5.5)$ & $5.0(\downarrow 8.5)$ & $52.0(\downarrow 6.4)$ \\
& SC & $83.0(\downarrow 3.0)$ & $93.0 (\downarrow 6.0)$ & $81.0(\downarrow 3.0)$ & $\textbf{86.0}(\downarrow \underline{1.0})$ & $40.0 (\downarrow 3.0)$ & $7.0(\downarrow 4.0)$ & $56.0(\downarrow 3.2)$ \\
& SR &$70.0 (\downarrow 10.0)$ &$74.0 (\downarrow 9.0)$ &$68.0 (\uparrow 6.0)$ & $72.0(\downarrow 10.0)$ & $19.0 (\downarrow \underline{1.0})$ & $8.0 (\downarrow 3.0)$ & $47.2 (\downarrow 10.0)$\\
& Ours & $\textbf{96.0}(\downarrow \underline{1.0})$ & $\textbf{95.0}(\downarrow \underline{4.0})$ & $\textbf{94.0}(\leftrightarrow \underline{0.0})$ & $\textbf{86.0}(\downarrow \underline{1.0})$ & $\textbf{52.0}(\downarrow \underline{1.0})$ & $11.0(\downarrow \underline{2.0})$& $\textbf{64.8}(\downarrow 2.8)$\\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{Task accuracy ($\%$) on different datasets under $10\%$ noise. The numbers in parentheses are the change compared to the clean data, and the arrows indicate the direction of the change. \textbf{Bold} indicates the best performance, and \underline{underline} indicates the smallest change.}
\label{tab:methods_res}
\end{table*}

\begin{table}[ht]
\small
\centering
\begin{adjustbox}{max width=1\linewidth}
{
\begin{tabular}{l|rrrr}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{GPT-4o}} & \multicolumn{2}{c}{\textbf{DeepSeek-V3}} \\
\cmidrule(l){2-5}
&  SC & SRR-0 & SC & SRR-0 \\
\midrule
Arithmetic$_\text{7}$ & $0.0$ & $4.0$ & $83.0$ & $95.0$ \\
Arithmetic$_\text{8}$ &$6.0$ & $40.0$ &$93.0$ & $95.0$ \\
Arithmetic$_\text{9}$ &$0.0$ & $15.0$ &$94.0$ & $91.0$\\
Crypto$_{\text{Caesar}}$ &$85.0$ & $85.0$ &$86.0$ & $86.0$\\
Crypto$_{\text{Atbash}}$ &$29.0$ & $51.0$ &$40.0$ & $48.0$ \\
Crypto$_{\text{Keyboard}}$ &$5.0$ & $8.0$ &$7.0$ & $9.0$ \\
List Functions &$42.4$ & $54.0$ & $56.0$ & $62.8$ \\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\caption{Task accuracy ($\%$) under $10\%$ noise of SC and the initial rule in SRR (SRR-0) on different datasets.}
\label{tab:sc_srr}
\end{table}

\subsection{Method-wise Effectiveness Comparison}
\label{subsec:mec}
Direct Output may limit the model's reasoning ability, as the model must output the rule directly without any intermediate steps. To systematically assess the robustness of different reasoning paradigms, we compare our Sample-steered Rule Refinement (SRR) method with three reasoning methods: (1) Chain of Thought (CoT;~\citealt{wei2022chain}), which decomposes reasoning into step-by-step rationales; (2) Self-Consistency (SC;~\citealt{wang2023selfconsistency}), which aggregates multi CoT trajectories through majority voting; and (3) Self-Refine (SR;~\citealt{madaan2023selfrefine}), which iteratively improves hypotheses using self-generated feedback. Table~\ref{tab:methods_res} presents the task accuracy under $10\%$ noise and the deviation from clean data for GPT-4o and DeepSeek-V3.


\paragraph{Superior Performance of SRR} As shown in Table~\ref{tab:methods_res}, our SRR framework achieves state-of-the-art performance across 13/14 task-model combinations while exhibiting minimal performance degradation ($2.1\%$ average drop vs. $3.6\%$-$8.5\%$ for baselines). This dual advantage stems from two mechanisms: (1) \textit{Diversity-aware hypothesis generation} explores broader solution spaces through subset sampling, outperforming SC's majority voting that amplifies the similar patterns (Table~\ref{tab:sc_srr}); (2) \textit{Execution-guided feedback} leverages Python interpreters for objective error detection, circumventing LLMs' inherent deductive limitations~\cite{chen2023teachinglargelanguagemodels,cheng2024inductivedeductiverethinkingfundamental}.


\paragraph{Consistency Scores Reveal Hidden Instability} While task accuracy provides comprehensive insights, consistency scores uncover fundamental reasoning fragility. As depicted in Figure~\ref{fig:consistency_score_method}, the Atbash cipher task exhibits particularly low consistency despite modest accuracy changes. The slight variation in accuracy may merely be an illusion created by the combined effects of noise interference (solved$\rightarrow$unsolved cases) and incorrect generalization (unsolved$\rightarrow$solved cases). Prior and concurrent work's singular focus on accuracy fluctuations~\cite{qiu2024phenomenal,zhou2024can, mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, huang2025mathperturbbenchmarkingllmsmath} overlooks this critical duality in reasoning robustness.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/consistency_score_method.pdf}
    \caption{Consistency score $(\%)$ between clean data and data with $10\%$ noise of DeepSeek-V3.}
    \label{fig:consistency_score_method}
\end{figure}

\paragraph{Counterfactual Challenges Expose Knowledge Reliance} We observe dramatic performance gaps on counterfactual tasks like base-7 and base-9 arithmetic ($90\%+$ vs. $10\%-$ for DeepSeek-V3 vs. GPT-4o) and Keyboard ciphers. This observation is consistent with the findings reported in~\citet{wu-etal-2024-reasoning}.
Diagnostic analysis of responses from LLMs shows models default to familiar templates rather than true induction. GPT-4o persistently misinterprets base-7 and base-9 addition as ``base-8'' or ``decimal sum with constant'', while Atbash and Keyboard ciphers get erroneously classified as Caesar shifts in thinking process or self-generated feedback. Although Atbash and Caesar cipher exhibit similar levels of complexity in transformation, their performance differences are significant.  
These failures reveal that there is a pattern overfitting in the reasoning process. Current models' inductive reasoning essentially operates through pattern matching rather than abstract induction. When the scarcity of counterfactual tasks in pretraining data forces models to rely on genuine rule induction rather than recitation, model performance plummets dramatically.

\subsection{Extended Explorations}
To further investigate the robustness of LLMs' inductive reasoning, we conduct a comparative evaluation against DeepSeek-R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, one of the state-of-the-art reasoning models, and human reasoning patterns.

\paragraph{Comparion with DeepSeek-R1} We focus on the Atbash cipher and List Functions. Atbash provides unified task semantics yet challenges models with unfamiliar transformation logic, while List Functions captures diverse rule abstraction scenarios. Table~\ref{tab:r1_res} compares task accuracy and consistency scores under clean and $10\%$ noise conditions with DeepSeek-V3 (SRR). Notably, DeepSeek-R1 achieves higher task accuracy on both tasks. However, its consistency scores are not competitive enough, indicating unresolved instability. Detailed breakdowns of consistency scores are shown in~\ref{tab:r1_stat}. For Atbash, the comparable number of right-to-wrong and wrong-to-right suggests instability and randomness in its reasoning process. Manual inspection shows DeepSeek-R1 still interprets Atbash cipher as character shifts in failed cases, mirroring previous pattern-overfitting behavior. For List Functions, the elevated right-to-wrong rate indicates its sensitivity to input perturbations. 
\begin{table}[ht]
    \centering
    \begin{adjustbox}{max width=1\linewidth}
    {
    \begin{tabular}{c|c|ccc}
    \toprule
        \textbf{Dataset} & \textbf{Model} & \textbf{Clean Acc} & \textbf{Noise Acc} &  \textbf{Consistency} \\
    \midrule
        \multirow{2}{*}{Atbash} & SRR & $53.0$ & $52.0 (-1.0)$ & $69.0$  \\
         & DeepSeek-R1 & $65.0$ & $65.0 (\pm0.0)$ & $70.0$ \\
    \midrule
        \multirow{2}{*}{List Func.} & SRR & $67.6$ & $64.8 (-2.8)$ & $90.8$ \\
         & DeepSeek-R1 & $76.0$ & $68.4(-7.6)$ & $87.6$  \\
    \bottomrule
    \end{tabular}
    }\end{adjustbox}
    \caption{Task accuracy (\%) and consistency score (\%) between DeepSeek-V3 with SRR and DeepSeek-R1 under clean and noisy conditions.}
    \label{tab:r1_res}
\end{table}
\begin{table}[ht]
    \centering
    \begin{adjustbox}{max width=1\linewidth}
    {
    \begin{tabular}{c|c|cccc}
    \toprule
        \textbf{Dataset} & \textbf{Model} & \# BothR & \# BothW &  \# RtoW & \# WtoR \\
    \midrule
        \multirow{2}{*}{Atbash} & SRR & $37$ & $32$ & $16$ & $15$ \\
         & DeepSeek-R1 & $50$ & $20$ & $15$ & $15$\\
    \midrule
        \multirow{2}{*}{List Func.} & SRR & $154$ & $73$ & $15$ & $8$ \\
         & DeepSeek-R1 & $165$ & $54$ & $25$ & $6$\\
    \bottomrule
    \end{tabular}
    }\end{adjustbox}
    \caption{Detailed breakdowns of consistency score. BothR, BothW, RtoW, WtoR represent both right, both wrong, right to wrong from clean condition to noisy condition and wrong to right respectively.}
    \label{tab:r1_stat}
\end{table}

\paragraph{Human Reasoning Comparison} ~\citet{rule2020child} reports human performance on List Functions. We divide the tasks into three difficulty levels in a $5:3:2$ ratio based on the sorted mean human performance. To contrast with human-like thinking patterns, we analyze consistency across 12 trials (4 noise levels $\times$ 3 runs) in Section~\ref{subsec:noise_level}. Figure~\ref{fig:consistency_score_lf} visualizes the distribution of consistency and performance per task in List Functions, stratified by difficulty. LLMs show macro-level alignment yet micro-level divergence compared to human reasoning. While LLMs broadly mirror human-like stability, showing higher consistency on both simple and hard tasks, with moderate instability on medium-difficulty tasks, their internal patterns reveal critical deviations. For hard tasks, models display unpredictability (e.g., sporadic success) rather than systematic incapacity. Simple tasks, despite their low complexity, exhibit in consistent performance unrelated to intrinsic difficulty. Notably, GPT-4o and DeepSeek-V3 demonstrate similar behavioral trends, suggesting a shared inductive bias with humans.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/heatmap.pdf}
    \caption{Task-solving consistency of DeepSeek-V3 and GPT-4o on List Functions. Each cell represents a task, arranged by ascending difficulty (top-to-bottom, left-to-right). Colors denote correctness patterns: 12R (all correct) to 12W (all wrong), with intermediate states (e.g., 1W11R: 1 wrong, 11 correct).}
    \label{fig:consistency_score_lf}
\end{figure}


\section{Discussion}
\label{subsec:ana}
Our experimental results reveal limitations in LLMs' inductive reasoning. We try to analyze the results in this section. 

\paragraph{Process Analysis} Inductive reasoning can be conceptualized through a Bayesian paradigm~\cite{doi:10.1126/science.1192788}, where models update posterior distributions over a hypothesis space based on observations. The introduction of noise disrupts the reasoning through dual mechanisms: \textbf{Noise-Induced Hypothesis Drift}, where the initial hypothesis space becomes misaligned with true rules when noise introduces conflicting patterns. This drift particularly impacts Direct Output (DO) method, which lacks intermediate reasoning steps and relies heavily on the initial hypothesis space. This is more evident for tasks with similar internal patterns, the model's performance fluctuates significantly (Cryptography) or even fails (Arithmetic) compared to other methods. 
\textbf{Evidence Ambiguity Amplification}, where noise reduces the effective signal-to-noise ratio during posterior optimization. Methods with iterative or step-by-step reasoning suffer error accumulation cascades\textemdash each reasoning step propagates uncertainty. As shown in Table~\ref{tab:methods_res}, CoT and SR exhibit larger average accuracy drops and worse performance. SRR avoids it by objectively identifying errors through execution. 

\paragraph{Implications for Robust Inductive Reasoning}
LLMs heavily rely on prior knowledge rather than pure induction. They blend memorized pattern matching with shallow reasoning. When observations align with priors (e.g., Caesar cipher), they demonstrate pseudo-robustness. However, conflicting patterns (noise) and counterfactual scenarios (unseen rules) expose this fragility\textemdash models either default to familiar templates or enter unstable hypothesis oscillations. Even the state-of-the-art reasoning model exhibits unstable performance in unified tasks. This contrasts sharply with human few-shot learning, where true rule abstraction enables stable generalization~\cite{doi:10.1126/science.aab3050}. Achieving human-level robust reasoning and few-shot induction requires disentangling knowledge recitation from inductive rule formation. 

\section{Related Work}

\paragraph{Inductive Reasoning} The study of inductive reasoning has been a long-standing focus across multiple disciplines. Early work~\cite{Heit2000} established foundational properties of inductive reasoning. In cognitive science, induction is considered a process of probabilistic belief updating within the Bayesian paradigm~\cite{doi:10.1126/science.1192788}. Human cognition and learning are explained through the integration of prior knowledge with observed data to compute posterior distributions. Comparative studies~\cite{doi:10.1126/science.aab3050,Lake_Ullman_Tenenbaum_Gershman_2017} have highlighted the contrast between human learners and machine intelligence.
With the advent of pre-trained language models~\cite{brown2020language}, research on inductive reasoning has shifted from domain-specific and neural formulation~\cite{tian2020learning,odena2021bustlebottomupprogramsynthesis,SABLEMEYER2022101527} to natural language. Initial approaches~\cite{pmlr-v139-alet21a,ijcai2024p693,mirchandani2023large,yang-etal-2024-language} predominantly relied on input-output (IO) prompting, which evaluates model performance on unseen examples without explicit rule articulation. However, this paradigm overlooks the internal rule-inference process, as it conflates rule induction with rule execution capabilities. Recent efforts~\cite{wang2024hypothesis} align more closely with our work by explicitly generating intermediate rules.~\citet{qiu2024phenomenal} proposed a thorough evaluation containing noisy conditions, whose findings partially overlap with ours, but their analysis is confined to a single dataset and prioritized accuracy metrics over consistency. We further explore the challenges posed by counterfactual tasks and uncover the limitations of LLMs in inductive reasoning.

\paragraph{Robustness of Reasoning in LLMs}
In this work, robustness refers to the ability to maintain consistent performance under imperfections or counterfactual scenarios~\cite{elazar-etal-2021-measuring}, which is intrinsically linked to the diversity and unpredictability of generation process~\cite{zhang2023sirenssongaiocean,huang2025hallu}. Prior works into LLM behavioral variability exhibit inconsistent performance across time~\cite{tu2024chatlogcarefullyevaluatingevolution,chen2023chatgptsbehaviorchangingtime}. Recent works focus on understanding their internal reasoning mechanisms by altering conditions, such as changing numerical values in mathematical tasks~\cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical,huang2025mathperturbbenchmarkingllmsmath} or examining performance on counterfactual tasks~\cite{wu-etal-2024-reasoning}.~\citet{zhou2024can} investigates the impact of noisy rationales on model performance. These studies primarily measure robustness through global accuracy changes, overlooking inconsistencies at the instance level. Our work addresses this gap by introducing a consistency score that quantifies intra-task stability, providing a more granular view of model robustness.

\section{Conclusion}
In this paper, we explore the robustness of inductive reasoning in large language models under imperfect observations. Through introducing the Robust Rule Induction task with metrics of both holistic and individual levels, we systematically evaluate the ability of LLMs to maintain stable and consistent rule abstraction. The Sample-steered Rule Refinement outperforms other reasoning paradigms by effectively leveraging diversity-aware hypothesis generation and execution-guided feedback. Our findings reveal that while LLMs can demonstrate impressive reasoning capabilities, they are inherently sensitive to noise and prone to hypothesis drifting and pattern overfitting. 

\clearpage
\section*{Limitations}
We discuss the limitations of our work here: Our evaluation focuses on highly formalized and symbolic tasks. Real-world inductive reasoning often involves ambiguous rules like social norms from text or visual patterns, which are not captured by our formalism. The tasks in Arithmetic and Cryptography may be relatively simple and lack diversity. Expanding task diversity could reveal deeper limitations in LLMs' capabilities. A large-scale assessment of human performance across all tasks under varying levels of noise is beyond the scope of this study, which poses certain limitations in comparing human reasoning patterns. The purpose of our study is to explore the robustness of LLMs' inductive reasoning capabilities, so we do not specifically tune the hyperparameters and prompt templates.

\section*{Ethics Statement}
The datasets we used are all publicly available, and our research does not involve any personal information. All data is generated by programs. Therefore, we anticipate that this paper does not raise any ethical concerns. We use ChatGPT to paraphrase some sentences.

\bibliography{custom}

\newpage
\appendix

\section*{Appendices}
\section{Details on Evaluation Pipeline}
In this section, we provide detailed information on the evaluation pipeline, including the data construction and the performance assessment. 
For Arithmetic, we generate the base-7, base-8, and base-9 tasks by randomly sampling two two-digit numbers in the corresponding base, and then we check whether there is a carry-over in the addition process. If there is no carry-over, we regenerate the numbers. The noisy examples are generated in base-10.
For Cryptography, we randomly select words of appropriate lengths from the NLTK Word Lists corpus\footnote{\url{https://www.nltk.org/nltk\_data/}}, and then we encrypt the words using the Caesar, Atbash, and Keyboard ciphers. The noisy examples are generated by randomly replacing the letters in the output with other letters.
For List Functions, we first write the corresponding rule functions for each task, and then we automatically generate the input data with appropriate lengths and ranges. The inputs are generated by randomly sampling numbers from a specific range with some constraints. 
. During the data synthesis process, we attach manual supervision to ensure that the generated data can correctly induce the rules.
During the evaluation process, we use the inputs in the test set as the input for rule execution. We evaluate the model's performance using exact match. If the model's output is correct on all the test set examples, we consider the model to have successfully induced the rule. If the model fails to output a valid programmatic rule or the program contains an infinite loop or errors, we consider it a failure.

\label{sec:app_eval}
\section{Experimental Details}
\label{sec:app_exp}
\subsection{Experimental Settings}
For robustness under different noise levels, we run each experiment three times and report the mean and standard deviation of the results to avoid randomness. Except for the self-consistency (SC) and sample-steered rule refinement (SRR) that require diverse generations, we set the temperature to $0.0$ for all models to ensure reproducibility. For SC and SRR, the temperature is set to $0.7$, consistent with the original work of SC~\cite{wang2023selfconsistency}. The positions of noise in seen examples are random to avoid positional bias~\cite{lu-etal-2022-fantastically}. In the implementation, we choose $2$ subsets for SRR by splitting the seen examples into two parts. The number of iterations is set to $3$ for both SR and SRR.

All experiments are conducted through the official OpenAI\footnote{\url{https://openai.com/api/}} and DeepSeek\footnote{\url{https://platform.deepseek.com/usage}} API platform. For the GPT-4o-mini and GPT-4o models, we spend about $200$ USD in total, and for the DeepSeek-V3 and DeepSeek-R1 models, we spend about $50$ USD in total. 
\subsection{Prompts and Failure Cases}
For the Direct Output setting, we restrict the model to output the rule directly without any additional output, as shown in Table~\ref{tab:do_prompt}. For the chain-of-thought and self-consistency setting, we use the instruction in~\citet{NEURIPS2022_8bb0d291}. For the self-refine and sample-steered rule refinement setting, we use the chain-of-thought prompt as the initial prompt; the iterative prompts are shown in Table~\ref{tab:sr_prompt} and Table~\ref{tab:srr_prompt}, respectively.

We provide some representative failure cases in the evaluation. For the Arithmetic task, the model fails to solve the base-7 and base-9 tasks and misinterprets the rule as the base-8 addition or the decimal sum with a constant. For the Cryptography task, the model fails to solve the Atbash and tries to explain it as shifts. The responses of the models are shown in Table~\ref{tab:failure_cases}.

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{p{0.9\linewidth}}
    \toprule
    \textbf{Prompt for Direct Output}\\
    \midrule
    Please generate a rule that maps the following inputs to their corresponding outputs using a Python function. The input is a list of integers. The output is also a list of integers. Note that some examples may be wrong, and you should take this into account when proposing the rule. \\
    \{examples\}\\
    Please format your Python function as follows:\\
    ```python\\
    def fn(x):\\
    \quad\# Your code here\\
    ```\\
    Your response should only include the function definition, not the function call or any other information.\\ 
    \bottomrule
    \end{tabular}
    \caption{The prompt for the Direct Output setting, exemplified by the List Functions task. The \{examples\} in the prompt is replaced by input-output pairs when conducting the experiments.}
    \label{tab:do_prompt}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{p{0.9\linewidth}}
    \toprule
    \textbf{Prompt for Chain-of-Thought}\\
    \midrule
    Please generate a rule that maps the following inputs to their corresponding outputs using a Python function. The input is a list of integers. The output is also a list of integers. Note that some examples may be wrong, and you should take this into account when proposing the rule. \\
    \{examples\}\\
    Please format your Python function as follows:\\
    ```python\\
    def fn(x):\\
    \quad\# Your code here\\
    ```\\
    Think step-by-step and explain your reasoning. Your response should include your thought process and the function definition without the function call.\\
    \bottomrule
    \end{tabular}
    \caption{The prompt for the Chain-of-Thought setting, exemplified by the List Functions task.}
    \label{tab:cot_prompt}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{p{0.9\linewidth}}
    \toprule
    \textbf{Prompt for Feedback Generation in Self-Refine} \\
    \midrule
    You have generated a rule that maps the following inputs to their corresponding outputs using a Python function. The input is a list of integers. The output is also a list of integers.\\
    \{examples\}\\
    In the last step, your rule is:\\
    ```python\\
    \{rule\}\\
    ```\\
    Give some feedback on the rule you have generated, like how can it be improved, what is wrong with it, etc.\\
    Your response should only include the feedback. If you think the rule is good enough, your response should be``NO FEEDBACK'' without other information. Note that some examples may be wrong, and you should take this into account when proposing the feedback.\\
    \midrule
    \textbf{Iteration} \\
    \midrule
    You have generated a rule that maps the following inputs to their corresponding outputs using a Python function. The input is a list of integers. The output is also a list of integers. Note that some examples may be wrong, and you should take this into account when proposing the rule.\\
    \{examples\} \\
    In the last step, your rule is:\\
    ```python\\
    \{rule\}\\
    ```\\
    The feedback you have given is:\\
    \{feedback\}\\
    Generate a new rule that maps the given inputs to their corresponding outputs using a Python function. Please format your rule as follows:\\
    ```python\\
    def fn(x):\\
    \quad\# Your code here\\
    ```\\
    Think step-by-step and explain your reasoning. Your response should include your thought process and the function definition without the function call.\\
    \bottomrule
    \end{tabular}
    \caption{The prompt for the Self-refine setting, exemplified by the List Functions task. The \{examples\}, \{rule\} and \{feedback\} in the prompt are replaced by input-output pairs, rule, and feedback when conducting the experiments.}
    \label{tab:sr_prompt}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{p{0.9\linewidth}}
    \toprule
    \textbf{Prompt for Iteration in SRR}\\
    \midrule
    You have generated a rule that maps the following inputs to their corresponding outputs using a Python function. The input is a list of integers. The output is also a list of integers. Note that some examples may be noisy, and you should take this into account when proposing the rule. In the last step, your rule is\\
    ```python\\
    \{rule\}\\
    ```\\
    But this rule is not correct. It works for the following examples:\\
    \{right\_examples\}\\
    However, it does not work for the following examples:\\ 
    \{wrong\_examples\}
    Generate a new rule that maps the given inputs to their corresponding outputs using a Python function. Please format your rule as follows:\\
    ```python\\
    def fn(x):\\
    \quad\# Your code here\\
    ```\\
    Think step-by-step and explain your reasoning. Your response should include your thought process and the function definition without the function call. You can either modify the existing rule or propose a new one.\\
    \bottomrule
    \end{tabular}
    \caption{The prompt for the Sample-steered Rule Refinement setting, exemplified by the List Functions task. The \{rule\}, \{right\_examples\}, and \{wrong\_examples\} in the prompt are replaced by the rule, right examples, and wrong examples when conducting the experiments.}
    \label{tab:srr_prompt}
\end{table*}

\begin{table*}[ht]
    \centering
    \small
    \begin{tabular}{p{0.9\linewidth}}
    \toprule
    \textbf{Failure Cases for Arithmetic}\\
    \midrule
    Octonary Confusion Error (in base-7):\\
    \\
    To solve this problem, we need to identify a pattern or rule that maps the given input expressions to their corresponding outputs. Let's analyze the examples provided: First, let's verify if the outputs are simply the sum of the inputs in base 10:- For `34 + 62`, the sum is `96`, but the output is `126`.- For `56 + 30`, the sum is `86`, but the output is `116`.- For `22 + 25`, the sum is `47`, but the output is `50`... Clearly, the outputs are not the direct sums of the inputs in base 10. Let's consider the possibility that the numbers are in base 8 (octal):- Convert the inputs to base 10, perform the addition, and then convert the result back to base 8. Let's verify this hypothesis: 1. 34 + 62 in base 8:   - `34` in base 8 is `3*8 + 4 = 28` in base 10.   - `62` in base 8 is `6*8 + 2 = 50` in base 10.   - Sum in base 10: `28 + 50 = 78`.   - Convert `78` to base 8: `78 // 8 = 9` remainder `6`, so `78` in base 8 is `116`.2. 56 + 30 in base 8:   - `56` in base 8 is `5*8 + 6 = 46` in base 10.   - `30` in base 8 is `3*8 + 0 = 24` in base 10.   - Sum in base 10: `46 + 24 = 70`.   - Convert `70` to base 8: `70 // 8 = 8` remainder `6`, so `70` in base 8 is `106`. 3. 22 + 25 in base 8:   - `22` in base 8 is `2*8 + 2 = 18` in base 10.   - `25` in base 8 is `2*8 + 5 = 21` in base 10.   - Sum in base 10: `18 + 21 = 39`.   - Convert `39` to base 8: `39 // 8 = 4` remainder `7`, so `39` in base 8 is `47`. \textcolor{orange}{Following this pattern, we can see that the outputs are indeed the sums of the inputs when interpreted as base 8 numbers and then converted back to base 8.} Let's implement this in a Python function...\\
    \midrule
    Decimal Confusion Error (in base-9):\\
    \\
    To solve this problem, we need to identify a consistent rule or pattern that maps the given input expressions to their corresponding outputs. Let's analyze the provided examples:  1. $68 + 68 = 147$: The correct sum is 136, but the output is 147. 2. $80 + 42 = 132$: The correct sum is 122, but the output is 132. 3. $76 + 17 = 104$: The correct sum is 93, but the output is 104. 4. $38 + 14 = 53$: The correct sum is 52, but the output is 53. 5. $43 + 84 = 137$: The correct sum is 127, but the output is 137. 6. $48 + 55 = 114$: The correct sum is 103, but the output is 114. 7. $72 + 56 = 138$: The correct sum is 128, but the output is 138. 8. $34 + 46 = 81$: The correct sum is 80, but the output is 81. 9. $88 + 15 = 114$: The correct sum is 103, but the output is 114. 10. $67 + 40 = 117$: The correct sum is 107, but the output is 117.  \textcolor{orange}{From these examples, it appears that the output is consistently 10 more than the correct sum of the two numbers. This suggests that the rule is to calculate the sum of the two numbers and then add 10 to the result.}  Let's define a Python function to implement this rule...\\
    \midrule
    \textbf{Failure Case for Cryptography}\\
    \midrule
    Shifts Error:\\
    \\
    To solve this problem, we need to identify a consistent rule or transformation that maps each input string to its corresponding output string. Let's analyze the given examples to find a pattern:  1.  cuissard -> xfrhhziw  2.  tripsis -> girkhrh  3.  Maccabaeus -> Nzxxzyzvfh  4.  jocundness -> qlxfmwmvhh  5.  chiro -> xsril  6.  coapt -> xlzkg  7.  Phecda -> Ksvxwz  8.  unifilar -> fmrurozi  9.  Cordaites -> Xliwzrgvh  10.  Wendell -> Dvmwvoo   \#\#\# Observations:  1.  Character Shifts : The transformation seems to involve shifting characters. For example, 'c' in "cuissard" becomes 'x' in "xfrhhziw". \textcolor{orange}{This suggests a possible Caesar cipher or similar character shift.}  2.  Case Sensitivity : The case of the letters is preserved. For instance, 'M' in "Maccabaeus" becomes 'N' in "Nzxxzyzvfh", and 'P' in "Phecda" becomes 'K' in "Ksvxwz".  3.  Consistent Shifts : By examining the shifts:    - 'c' to 'x' is a shift of 23 positions backward in the alphabet.    - 't' to 'g' is a shift of 13 positions backward.    - 'M' to 'N' is a shift of 1 position forward.    - 'j' to 'q' is a shift of 7 positions forward.    - 'P' to 'K' is a shift of 21 positions backward.    - 'u' to 'f' is a shift of 11 positions backward.    - 'C' to 'X' is a shift of 21 positions backward.    - 'W' to 'D' is a shift of 21 positions backward.  4.  Pattern Recognition : The shifts are not consistent across all examples, suggesting that some examples might be incorrect or that multiple rules are being applied. However, a common shift that appears frequently is 21 positions backward (e.g., 'c' to 'x', 'P' to 'K', 'C' to 'X', 'W' to 'D').  \#\#\# Hypothesis:  Given the frequent appearance of a 21-position backward shift, let's hypothesize that the rule involves shifting each character 21 positions backward in the alphabet. This would mean:  - For lowercase letters, \textcolor{orange}{'a' becomes 'v', 'b' becomes 'w', ..., 'z' becomes 'u'. - For uppercase letters, 'A' becomes 'V', 'B' becomes 'W', ..., 'Z' becomes 'U'.}  \#\#\# Implementation:  Let's implement this hypothesis in a Python function... \\
    \bottomrule
    \end{tabular}
    \caption{The failure cases of the models on the Arithmetic and Cryptography tasks.}
    \label{tab:failure_cases}
\end{table*}

\end{document}

