\section{Related Work}
\paragraph{RLHF.} Reinforcement Learning from Human Feedback (RLHF) has achieved tremendous successes in the modern large language model post training \citep{OpenAI2023GPT4TR, bai2022training, ouyang2022training, team2023gemini}. The dominant approach in the area is based on the reward training and policy optimization with the PPO algorithm \citep{schulman2017proximal}. However, applying PPO effectively in the context of LLMs presents significant challenges \citep{choshen2019weaknesses}. However, getting the PPO work is challenging in the context of LLMs \citep{choshen2019weaknesses}. In view of this, researchers have spent great efforts in proposing alternative approaches to the PPO algorithm. 

One line of research revisits REINFORCE-based variants such as ReMAX and GRPO \citep{li2023remax, shao2024deepseekmath}, with the KL-regularized objective. Another approach focuses on direct preference learning \citep{zhao2023slic, rafailov2023direct, azar2023general}, which bypasses the reward modeling stage and directly optimizes the policy using the preference dataset in a supervised manner. A notable example is the Direct Preference Optimization (DPO) algorithm \citep{rafailov2023direct}, which has gained great attention in both the open-source community \citep{tunstall2023zephyr, lambert2024t} and industrial applications such as Llama \citep{dubey2024llama}. All approaches mentioned above are derived under the KL-regularized framework studied in this paper. An exception to this trend is best-of-n (BoN) sampling and rejection sampling fine-tuning \citep{bai2022training, dong2023raft, touvron2023llama}, where a reward model is used to filter samples for final output or select training samples. However, recent works show that the success of BoN sampling may essentially result from the fact that it is optimal in terms of the KL-reward trade-off \citep{gui2024bonbon, yang2024asymptotics}.


\paragraph{Theory of RLHF.} The theoretical foundation of RLHF traces back to dueling bandits \citep[e.g.,][]{yue2012k,saha2021optimal,bengs2021preference}, which studied preference feedback in non-regularized settings. This was later extended to online reinforcement learning with finite state spaces (tabular settings) and function approximation \citep{xu2020preference, novoseller2020dueling, pacchiano2021dueling, chen2022human}. More recently, \citet{zhan2023query, wu2023making} developed reward-free learning algorithms and sampling-based methods for online RLHF. For the offline learning,  \citet{zhu2023principled, zhan2023provable, li2023reinforcement, zhong2024dpo, huang2024correcting} propose sample-efficient algorithms under suitable coverage conditions. These works mainly develop techniques to estimate the underlying reward model associated with the Bradley-Terry model from querying the preference oracle (human) and achieve similar order regret with the standard reward learning. However, since they only consider reward maximization, the results deviate from the practical applications of RLHF. For example, reward maximization frameworks often assume a deterministic optimal policy, which is unsuitable for generative models.

After these works, the recent project \citet{xiong2024iterative} provides the first provably efficient algorithm of RLHF under the KL-regularized contextual bandit formulation. The result is further refined in \citep{xie2024exploratory} and \citet{xie2024exploratory, liu2024provably, cen2024value} propose provably efficient algorithms with optimistically biased optimization targets, which originate from the feel-good sampling \citep{zhang2022feel}. In parallel, \citet{wang2023rlhf, ye2024theoretical} extend the techniques of preference learning to the general preference setting under the Markov game formulation. However, their techniques simply discard the KL divergence in the target, and use the standard techniques to get results that are similar to the non-regularized problems, which are essentially sub-optimal for the KL-regularized framework. In contrast, in this work, we aim to leverage the structure of the KL-regularized problem and develop new techniques and algorithms that achieve superior theoretical guarantees compared to prior studies.

The most closely related work to our project is \citet{zhao2024sharp}, which considers the KL-regularized RL in the bandit setting. They propose a two-stage mixed-policy sampling algorithm and provide a regret bound which enjoys an $\cO(1 / \epsilon)$ sample complexity. However, their results rely on a relatively strong coverage assumption, which is not compatible with the practical applications of RLHF. In contrast, our work provides a novel algorithm and analysis that achieves a logarithmic regret bound without the coverage assumption. We summarize the comparison of our work with the existing literature in Table~\ref{tab:bandits-mdps}.

\paragraph{Notation.} For a finite function class $\cF$, we use $N_\cF$ to represent its cardinality. We use $\widetilde{\cO}$ to omit the logarithmic orders. We use the convention $[n]=\{1,\ldots,n\}$. For any vector $x$ and a matrix $\Sigma$, let $\|x\|_\Sigma=\sqrt{x^{\top}\Sigma x}$. For a function $R:\cX\times\cA\rightarrow\RR$, parameter $\eta>0$ and the reference policy $\pi_{\reff}$, let the normalization constant $Z_R(x)=\E_{a \sim \pi_{\reff}(\cdot|x)} \exp (\eta R(x,a))$.