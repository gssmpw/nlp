@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774},
}

@article{azar2023general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2310.12036},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{bengs2021preference,
  title={Preference-based online learning with dueling bandits: A survey},
  author={Bengs, Viktor and Busa-Fekete, R{\'o}bert and El Mesaoudi-Paul, Adil and H{\"u}llermeier, Eyke},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={278--385},
  year={2021},
  publisher={JMLRORG}
}

@article{cen2024value,
  title={Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF},
  author={Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo},
  journal={arXiv preprint arXiv:2405.19320},
  year={2024}
}

@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@article{choshen2019weaknesses,
  title={On the weaknesses of reinforcement learning for neural machine translation},
  author={Choshen, Leshem and Fox, Lior and Aizenbud, Zohar and Abend, Omri},
  journal={arXiv preprint arXiv:1907.01752},
  year={2019}
}

@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{gui2024bonbon,
  title={BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling},
  author={Gui, Lin and G{\^a}rbacea, Cristina and Veitch, Victor},
  journal={arXiv preprint arXiv:2406.00832},
  year={2024}
}

@article{huang2024correcting,
  title={Correcting the mythos of kl-regularization: Direct alignment without overparameterization via chi-squared preference optimization},
  author={Huang, Audrey and Zhan, Wenhao and Xie, Tengyang and Lee, Jason D and Sun, Wen and Krishnamurthy, Akshay and Foster, Dylan J},
  journal={arXiv e-prints},
  pages={arXiv--2407},
  year={2024}
}

@article{lambert2024t,
  title={T$\backslash$" ULU 3: Pushing Frontiers in Open Language Model Post-Training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124},
  year={2024}
}

@article{li2023reinforcement,
  title={Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism},
  author={Li, Zihao and Yang, Zhuoran and Wang, Mengdi},
  journal={arXiv preprint arXiv:2305.18438},
  year={2023}
}

@article{li2023remax,
  title={ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models},
  author={Li, Ziniu and Xu, Tian and Zhang, Yushun and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={arXiv e-prints},
  pages={arXiv--2310},
  year={2023}
}

@article{liu2024provably,
  title={Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer},
  author={Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2405.16436},
  year={2024}
}

@inproceedings{novoseller2020dueling,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={1029--1038},
  year={2020},
  organization={PMLR}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{pacchiano2021dueling,
  title={Dueling rl: reinforcement learning with trajectory preferences},
  author={Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan},
  journal={arXiv preprint arXiv:2111.04850},
  year={2021}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{saha2021optimal,
  title={Optimal algorithms for stochastic contextual preference bandits},
  author={Saha, Aadirupa},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30050--30062},
  year={2021}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Y and Guo, Daya},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@article{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL?},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  journal={arXiv preprint arXiv:2306.14111},
  year={2023}
}

@article{wu2023making,
  title={Making RL with Preference-based Feedback Efficient via Randomization},
  author={Wu, Runzhe and Sun, Wen},
  journal={arXiv preprint arXiv:2310.14554},
  year={2023}
}

@article{xie2024exploratory,
  title={Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF},
  author={Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2405.21046},
  year={2024}
}

@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning}
    ,
year={2024}
}

@article{xu2020preference,
  title={Preference-based reinforcement learning with finite-time guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18784--18794},
  year={2020}
}

@article{yang2024asymptotics,
  title={Asymptotics of language model alignment},
  author={Yang, Joy Qiping and Salamatian, Salman and Sun, Ziteng and Suresh, Ananda Theertha and Beirami, Ahmad},
  journal={arXiv preprint arXiv:2404.01730},
  year={2024}
}

@article{ye2024theoretical,
  title={A theoretical analysis of nash learning from human feedback under general kl-regularized preference},
  author={Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2402.07314},
  year={2024}
}

@article{yue2012k,
  title={The k-armed dueling bandits problem},
  author={Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten},
  journal={Journal of Computer and System Sciences},
  volume={78},
  number={5},
  pages={1538--1556},
  year={2012},
  publisher={Elsevier}
}

@article{zhan2023provable,
  title={Provable Offline Reinforcement Learning with Human Feedback},
  author={Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen},
  journal={arXiv preprint arXiv:2305.14816},
  year={2023}
}

@article{zhan2023query,
  title={How to Query Human Feedback Efficiently in RL?},
  author={Zhan, Wenhao and Uehara, Masatoshi and Sun, Wen and Lee, Jason D},
  journal={arXiv preprint arXiv:2305.18505},
  year={2023}
}

@article{zhang2022feel,
  title={Feel-good thompson sampling for contextual bandits and reinforcement learning},
  author={Zhang, Tong},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={4},
  number={2},
  pages={834--857},
  year={2022},
  publisher={SIAM}
}

@article{zhao2023slic,
  title={Slic-hf: Sequence likelihood calibration with human feedback},
  author={Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:2305.10425},
  year={2023}
}

@article{zhao2024sharp,
  title={Sharp Analysis for KL-Regularized Contextual Bandits and RLHF},
  author={Zhao, Heyang and Ye, Chenlu and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2411.04625},
  year={2024}
}

@article{zhong2024dpo,
  title={Dpo meets ppo: Reinforced token optimization for rlhf},
  author={Zhong, Han and Feng, Guhao and Xiong, Wei and Zhao, Li and He, Di and Bian, Jiang and Wang, Liwei},
  journal={arXiv preprint arXiv:2404.18922},
  year={2024}
}

@article{zhu2023principled,
  title={Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons},
  author={Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I},
  journal={arXiv preprint arXiv:2301.11270},
  year={2023}
}

