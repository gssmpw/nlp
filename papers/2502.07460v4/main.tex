\PassOptionsToPackage{table}{xcolor}
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{smile}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphics}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=blue
            ]{hyperref}
\usepackage{enumitem}
\usepackage[usenames]{color}
\newcommand{\rE}{\mathbb E}
\newcommand{\E}{\mathbb E}
\newcommand{\ip}[1]{\langle #1 \rangle}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\regret}{{\mathrm{Regret}}}
\newcommand{\la}{\left\langle}
\newcommand{\ra}{\right\rangle}
\newcommand{\GL}{\mathrm{GL}}
\usepackage[table]{xcolor}


\definecolor{LightCyan}{rgb}{0.5, 0.65, 1}


\title{\huge Logarithmic Regret for Online KL-Regularized Reinforcement Learning}
\author{Heyang Zhao \thanks{Equal contribution} \thanks{University of California, Los Angeles, CA 90095, USA; e-mail: {\tt hyzhao@cs.ucla.edu}}
~~~
Chenlu Ye \footnotemark[1] \thanks{University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; e-mail: {\tt chenluy3@illinois.edu}}
~~~
Wei Xiong \thanks{University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; e-mail: {\tt wx13@illinois.edu}}
~~~
Quanquan Gu \thanks{University of California, Los Angeles, CA 90095, USA; e-mail: {\tt qgu@cs.ucla.edu}}
~~~
Tong Zhang \thanks{University of Illinois Urbana-Champaign, Urbana, IL 61801, USA; e-mail: {\tt tongzhang@tongzhang-ml.org}}
}
\date{}

\allowdisplaybreaks

\begin{document}

\maketitle

\begin{abstract}
    Recent advances in Reinforcement Learning from Human Feedback (RLHF) have shown that KL-regularization plays a pivotal role in improving the efficiency of RL fine-tuning for large language models (LLMs). Despite its empirical advantage, the theoretical difference between KL-regularized RL and standard RL remains largely under-explored. While there is a recent line of work on the theoretical analysis of KL-regularized objective in decision making \citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses either reduce to the traditional RL setting or rely on strong coverage assumptions. In this paper, we propose an optimism-based KL-regularized online contextual bandit algorithm, and provide a novel analysis of its regret. By carefully leveraging the benign optimization landscape induced by the KL-regularization and the optimistic reward estimation, our algorithm achieves an $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$ logarithmic regret bound, where $\eta, N_{\mathcal R},T,d_{\mathcal R}$ denote the KL-regularization parameter, the cardinality of the reward function class, number of rounds, and the complexity of the reward function class. Furthermore, we extend our algorithm and analysis to reinforcement learning by developing a novel decomposition over transition steps and also obtain a similar logarithmic regret bound.
\end{abstract}

\section{Introduction}
We study the KL-regularized contextual bandit \citep{langford2007epoch, xiong2024iterative} and Markov decision processes (MDPs) \citep{sutton2018reinforcement} in this paper. These two frameworks have received significant attention due to their tremendous successes in the post-training stage of modern large language models (LLMs), which is commonly referred to as the \textit{Reinforcement Learning from Human Feedback} (RLHF) \citep{bai2022training, ouyang2022training}. The RLHF learning paradigm has been used to make a powerful chatbot by aligning the LLMs with human preference, making the model generation helpful, honest, and harmless \citep{bai2022training}. Notable examples include the OpenAI's Chat-GPT \citep{OpenAI2023GPT4TR}, Anthropic's Claude \citep{bai2022training}, and Google's Gemini \citep{team2023gemini}. More recently, the KL-regularized RL framework has also been applied to enhance multi-turn reasoning capabilities, resulting in powerful reasoning models such as GPT4-o1 and DeepSeek-R1.

However, the RLHF process also faces challenges. It often causes a decline in certain abilities acquired during pretraining and supervised fine-tuning (SFT), a phenomenon commonly known as “alignment tax” \citep{askell2021general, lin2023speciality}. For instance, contrastive learning without regularization, as observed in \citet{meng2024simpo}, can degrade performance on standard reasoning benchmarks like MATH \citep{hendrycks2021measuring} and GSM8K \citep{cobbe2021training}. Additionally, unregularized RL training presents significant challenges on computational efficiency and training stability \citep{casper2023open}. To address these issues, practitioners often optimize a KL-regularized objective (defined formally in Section~\ref{sec:kl_bandit}) to balance reward optimization and mitigate alignment tax.

Moreover, these KL-regularized frameworks often demonstrate superior sample efficiency compared to standard deep RL tasks. For example, models with 52B parameters (Claude \citep{bai2022training}) and 671B parameters (DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}) achieve substantial policy improvement with only tens of thousands of samples or thousands of training steps. Similar observations apply to direct preference alignment algorithms \citep{rafailov2023direct, tunstall2023zephyr, chen2024self, dong2024rlhf}, which develop state-of-the-art open-source chatbots using fewer than 100K samples. These results highlight the superior sample efficiency of KL-regularized RL, surpassing traditional deep RL applications \citep{silver2016mastering} and their analyses in the RL theory literature.

However, despite the empirical success, a fundamental connection between KL regularization and learning efficiency has not been established so far theoretically. While some recent studies have attempted to analyze these frameworks, they often rely on standard analysis techniques, yielding regret guarantees similar to those of standard RL \citep{xiong2024iterative, xie2024exploratory, xiong2024building, cen2024value}, or they depend on strong coverage assumptions and restricted to the bandit settings \citep{zhao2024sharp}. The following pivotal question remains open:
\begin{center}
    \textit{Is KL-regularized RL more efficient than standard RL in the online setting without additional coverage assumption?}
\end{center}
In this work, we address this question by designing provably efficient algorithms based on the optimism in the face of uncertainty (OFU) principle and develop \textbf{refined policy suboptimimality decomposition} for both contextual bandits and MDPs.

We establish the theoretical guarantees for the algorithms and demonstrate their statistical advantages over standard RL scenarios. We summarize our contributions as follows.

\begin{itemize}
    \item For KL-regularized contextual bandits, we establish the first $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$ regret bound that scales logarithmically with time steps $T$ in the standard online RL setting, where $\eta$ is the KL-regularization parameter, $N_{\mathcal R}, d_\cR$ denote the cardinality of the reward function class $\cR$, and its eluder dimension. This result significantly improves upon the previous $\cO(\sqrt{T})$ bound \citep{xiong2024iterative} and eliminates the strong coverage condition in prior work \citep{zhao2024sharp}.
    \item Distinct from the previous analyses that solely rely on the learned policy maximizing the KL-regularized objective, we take a novel approach by expressing the suboptimality gap in terms of a functional gap with respect to the policy $\pi_R$ induced by a proxy reward function $R$. With a fine-grained analysis for the derivative of the gap, we then establish the monotonicity in the sub-optimality gap via the optimistic reward estimation. This allows us to obtain the uncertainty induced by the policy at the current time step so that the sum of squared uncertainty can be bounded by the eluder dimension.  
    \item We extend these techniques to KL-regularized MDPs and establish the first $\cO(\log T)$ regret bound in the literature. The key to this improved regret bound is a novel policy decomposition technique through multi-steps. These techniques may be of independent interest and have the potential to inspire future research on KL-regularized decision-making problems.
\end{itemize}

\begin{table*}[ht]
    \centering
    \caption{Comparison of online KL-regularized algorithms in bandits and MDPs, where $\epsilon>0$ is the sub-optimality gap, $T$ is the number of rounds, $d$ is the vector dimension for linear models, constant $\eta>0$ is the KL-regularized parameter,  $C_{\GL}$ is the global coverage condition, $d_\cR$ and $d_\cF$ are the complexity measure for general function class $\cR$ and $\cF$, $\widetilde{\cO}$ omits the logarithmic order for $T$ and $1/\epsilon$. The notations $N,N_\cR,N_{\cF\oplus\cB}, N_{\Pi}$ represent the cardinality or covering number for the reward, value and bonus function classes. The fourth column represents the number of samples needed to achieve an $\epsilon$-suboplimality, and the last column shows whether they require the coverage condition. We convert the regret bounds of our algorithms to sample complexity by Lemma~\ref{lem:online2batch}. We remark that the results of \citet{xiong2024iterative, xie2024exploratory, xiong2024building} are based on preference feedback. When comparing with them, we mainly focus on the \textit{order} of dependency on $T$. See Remark~\ref{rmk:preference} for further discussion on the extension to preference feedback.
    }
    
    \label{tab:bandits-mdps}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c c c c c}
    \toprule
    \textbf{Setting} & \textbf{Algorithm} 
                      & \textbf{Regret} 
                      & \textbf{Sample Complexity}
                      & \textbf{Coverage} \\
    \midrule
    
    % ------------------- Bandits block (4 rows)
    \multirow{4}{*}{\textbf{Bandits}}
     & \makecell{Online Iterative GSHF\footnotemark[1] \\ \citep{xiong2024iterative}} & $-$ & $\widetilde{\cO}\big(d^2/{\color{red} \epsilon^2}\big)$ & $\times$ \\
     & \makecell{Two-Stage Mixed-Policy Sampling\\ \citep{zhao2024sharp}} & $-$ & $\widetilde{\cO}\big((\eta^2C_{\mathrm{GL}}^2 + \eta/{\color{red} \epsilon})\log N_\cR \big)$ & \checkmark \\
    \rowcolor{blue!15} 
     & KL-UCB (Theorem \ref{th:bandit}) & $\cO\big(\eta d_\cR {\color{red} \log T}\cdot\log(N_\cR)\big)$ & $\widetilde{\cO}\big(\eta d_\cR \cdot\log(N_\cR)/{\color{red} \epsilon}\big)$ & $\times$ \\
    \midrule
    
    % ------------------- MDPs block (3 rows)
    \multirow{3}{*}{\textbf{MDPs}}
     & \makecell{Online Iterative M-GSHF\footnotemark[2] \\ \citep{xiong2024building}} & \makecell{$\widetilde{\cO}\big(\sqrt{d_\cR {\color{red} T}\log N_\cR}$ \\$+Hd_\cP\log N_\cP\big)$} & \makecell{$\widetilde{\cO}\big(d_\cR \log N_\cR/{\color{red}\epsilon^2}$\\ $+ Hd_\cP\log N_\cP/\epsilon \big)$} & $\times$ \\
     & XPO \citep{xie2024exploratory}\footnotemark[3] & $-$ & $\widetilde{\cO}\big(d_\cF\log N_{\cF}/{\color{red} \epsilon^2}\big)$ & $\times$ \\
     \rowcolor{blue!15} 
     & KL-LSVI-UCB (Theorem \ref{th:mdp}) & $\cO\big(\eta H^2 d_\cF {\color{red} \log T} \cdot\log N_{\cF\oplus\cB})\big)$ & $\widetilde{\cO}\big(\eta H^2 d_\cF  \cdot\log N_{\cF\oplus\cB}) / {\color{red} \epsilon}\big)$ & $\times$ \\
    \bottomrule
    \end{tabular}}
    \end{table*}
    \footnotetext[1]{\citet{xiong2024iterative} studies relative-preference feedback, so their sample complexity additionally depends on $e^\eta$. Since our work focuses on absolute rewards, we omit this dependence.}
    \footnotetext[2]{\citet{xiong2024building} considers the trajectory-level reward and learns the reward in $\cR$ and transition probability in $\cP$ separately, so the dependence on $d_\cR,N_\cR$ and $d_cP,N_\cP$ are separete.}
    \footnotetext[3]{\citet{xie2024exploratory} considers the deterministic-transition setting and optimizes the policy directly.
}

\section{Related Work}

\paragraph{RLHF.} Reinforcement Learning from Human Feedback (RLHF) has achieved tremendous successes in the modern large language model post training \citep{OpenAI2023GPT4TR, bai2022training, ouyang2022training, team2023gemini}. The dominant approach in the area is based on the reward training and policy optimization with the PPO algorithm \citep{schulman2017proximal}. However, applying PPO effectively in the context of LLMs presents significant challenges \citep{choshen2019weaknesses}. However, getting the PPO work is challenging in the context of LLMs \citep{choshen2019weaknesses}. In view of this, researchers have spent great efforts in proposing alternative approaches to the PPO algorithm. 

One line of research revisits REINFORCE-based variants such as ReMAX and GRPO \citep{li2023remax, shao2024deepseekmath}, with the KL-regularized objective. Another approach focuses on direct preference learning \citep{zhao2023slic, rafailov2023direct, azar2023general}, which bypasses the reward modeling stage and directly optimizes the policy using the preference dataset in a supervised manner. A notable example is the Direct Preference Optimization (DPO) algorithm \citep{rafailov2023direct}, which has gained great attention in both the open-source community \citep{tunstall2023zephyr, lambert2024t} and industrial applications such as Llama \citep{dubey2024llama}. All approaches mentioned above are derived under the KL-regularized framework studied in this paper. An exception to this trend is best-of-n (BoN) sampling and rejection sampling fine-tuning \citep{bai2022training, dong2023raft, touvron2023llama}, where a reward model is used to filter samples for final output or select training samples. However, recent works show that the success of BoN sampling may essentially result from the fact that it is optimal in terms of the KL-reward trade-off \citep{gui2024bonbon, yang2024asymptotics}.


\paragraph{Theory of RLHF.} The theoretical foundation of RLHF traces back to dueling bandits \citep[e.g.,][]{yue2012k,saha2021optimal,bengs2021preference}, which studied preference feedback in non-regularized settings. This was later extended to online reinforcement learning with finite state spaces (tabular settings) and function approximation \citep{xu2020preference, novoseller2020dueling, pacchiano2021dueling, chen2022human}. More recently, \citet{zhan2023query, wu2023making} developed reward-free learning algorithms and sampling-based methods for online RLHF. For the offline learning,  \citet{zhu2023principled, zhan2023provable, li2023reinforcement, zhong2024dpo, huang2024correcting} propose sample-efficient algorithms under suitable coverage conditions. These works mainly develop techniques to estimate the underlying reward model associated with the Bradley-Terry model from querying the preference oracle (human) and achieve similar order regret with the standard reward learning. However, since they only consider reward maximization, the results deviate from the practical applications of RLHF. For example, reward maximization frameworks often assume a deterministic optimal policy, which is unsuitable for generative models.

After these works, the recent project \citet{xiong2024iterative} provides the first provably efficient algorithm of RLHF under the KL-regularized contextual bandit formulation. The result is further refined in \citep{xie2024exploratory} and \citet{xie2024exploratory, liu2024provably, cen2024value} propose provably efficient algorithms with optimistically biased optimization targets, which originate from the feel-good sampling \citep{zhang2022feel}. In parallel, \citet{wang2023rlhf, ye2024theoretical} extend the techniques of preference learning to the general preference setting under the Markov game formulation. However, their techniques simply discard the KL divergence in the target, and use the standard techniques to get results that are similar to the non-regularized problems, which are essentially sub-optimal for the KL-regularized framework. In contrast, in this work, we aim to leverage the structure of the KL-regularized problem and develop new techniques and algorithms that achieve superior theoretical guarantees compared to prior studies.

The most closely related work to our project is \citet{zhao2024sharp}, which considers the KL-regularized RL in the bandit setting. They propose a two-stage mixed-policy sampling algorithm and provide a regret bound which enjoys an $\cO(1 / \epsilon)$ sample complexity. However, their results rely on a relatively strong coverage assumption, which is not compatible with the practical applications of RLHF. In contrast, our work provides a novel algorithm and analysis that achieves a logarithmic regret bound without the coverage assumption. We summarize the comparison of our work with the existing literature in Table~\ref{tab:bandits-mdps}.

\paragraph{Notation.} For a finite function class $\cF$, we use $N_\cF$ to represent its cardinality. We use $\widetilde{\cO}$ to omit the logarithmic orders. We use the convention $[n]=\{1,\ldots,n\}$. For any vector $x$ and a matrix $\Sigma$, let $\|x\|_\Sigma=\sqrt{x^{\top}\Sigma x}$. For a function $R:\cX\times\cA\rightarrow\RR$, parameter $\eta>0$ and the reference policy $\pi_{\reff}$, let the normalization constant $Z_R(x)=\E_{a \sim \pi_{\reff}(\cdot|x)} \exp (\eta R(x,a))$.

\section{Background}

\subsection{KL-Regularized Contextual Bandits}
The contextual bandit problem with KL-regularized objective is defined as follows. 

At each round $t \ge 1$, the learner observes a context $x_t \in \cX$, and chooses an action $a_t \in \cA$, where $\cX$ is the context space and $\cA$ is the action space. The learner then receives a reward $r_t \in \RR$ which characterizes the gain of the learner from choosing action $a_t$ under context $x_t$. We assume that the reward $r_t$ is generated by \begin{align*} 
    r_t = R^*(x_t, a_t) + \epsilon_t,
\end{align*} where $R^*(x_t, a_t)$ is the unknown reward function and $\epsilon_t$ is an independent 1-sub-Gaussian zero-mean random noise. In practice, the context can be a prompt, and the action is the response generated by the LLMs. The reward signal is a sentence reward that is commonly used \citep{ouyang2022training, bai2022training, touvron2023llama}.

\begin{remark}\label{rmk:preference}
We do not consider the preference feedback here since the more recent applications in building reasoning models \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} and improving LLM safety \citep{guan2024deliberative} assume the existence of the absolute reward value (e.g. the binary reward indicator of the correctness). The results presented in this work can be readily extended to the preference feedback using the techniques developed in dueling bandit or RL \citep{yue2012k, pacchiano2021dueling, xiong2024iterative, li2024feel}.
\end{remark}

\begin{assumption}[Reward function realizability]\label{as:Reward function realizability}
    Assume that there exists a reward function class $\cR: \cX \times \cA\rightarrow [0, 1]$ such that $R^*(x, a) \in \cR$.
\end{assumption}
Without loss of generality, we assume that the function class has finite cardinality $|\cR|$, and we can generalize the analysis to an infinite function class by considering a covering number \citep{zhang2023mathematical}.

\begin{definition}[Uncertainty and eluder dimension] \label{def:eluder}
    For any sequence $\cD_{t-1} = \{(x_i, a_i)\}_{i=1}^{t-1}$, we define the uncertainty of $(x, a)$ with respect to $\cR$ as:
    $$
        U_\cR(\lambda, x, a; \cD_{t-1}) = \sup_{R_1, R_2\in\cR} \frac{|R_1(x, a)-R_2(x, a)|}{\sqrt{\lambda + \sum_{i=1}^{t-1}\big(R_1(x_i, a_i) - R_2(x_i, a_i)\big)^2}}.
    $$
    Then, the eluder dimension is given by
    $d(\cR, \lambda, T) := \sup_{x_{1:T}, a_{1:T}} \sum_{t=1}^T \min \big(1, [U_\cR(\lambda, x_t, a_t; \cD_{t - 1})]^2\big)$.
\end{definition}
The uncertainty $U_\cR(\lambda, x, a; \cD_t)$ measures how much difference the new-coming data $x,a$ has with the history data $\cD_t$, and is widely adopted in RL literature with general function approximation \citep{zhang2023mathematical,ye2023corruption, agarwal2023vo,zhao2023nearly}.  To illustrate it better, we use the linear function as a special case, where the function class $\cR$ can be embedded into a linear mapping $\cR=\{\theta^{\top}\phi(\cdot,\cdot):~\theta\in\RR^d,~\|\theta\|_2\le B\}$. Let the covariance matrix $\Sigma_t=\sum_{i=1}^{t}\phi(x_i,a_i)\phi(x_i,a_i)^{\top}+\lambda/B\cdot I$. Then, the eluder coefficient can be simplified as
\begin{equation*}
{
\begin{aligned}
    &U_\cR(\lambda, x, a; \cD_t)\\ 
    &= \sup_{\theta_1,\theta_2\in\RR^d}\frac{|((\theta_1-\theta_2)^{\top}\phi(x,a))|}{\sqrt{\lambda+\sum_{i\in[t]}((\theta_1-\theta_2)^{\top}\phi(x_i,a_i))^2}}\\
    &\le \sup_{\theta_1,\theta_2\in\RR^d}\frac{|((\theta_1-\theta_2)^{\top}\phi(x,a))|}{\sqrt{(\theta_1-\theta_2)^{\top}\Sigma_{t}(\theta_1-\theta_2)}} \le \big\|\phi(x,a)\big\|_{\Sigma_{t}^{-1}},
\end{aligned}}
\end{equation*}
where the inequality uses the Cauchy–Schwarz inequality. Hence, the uncertainty reflects how much a direction in the feature space has been explored. Further, the eluder dimension represents how many times the model can be ``surprised" by the out-of-sample data over $T$ rounds. One can show that this definition is more general than the original one in \cite{russo2013eluder} as we further take the magnitude of violation into consideration while the original eluder dimension only counts the frequency \citep{zhang2023mathematical, xie2022role}.

We consider a KL-regularized \textbf{objective} as follows: 
\begin{equation*}
{
\begin{aligned} 
    J(\pi) :=& \rE_{x \sim d_0} \rE_{a \sim \pi(\cdot | x)} \Big[R^*(x, a) - \frac{1}{\eta} \log \frac{\pi(a|x)}{\pi_{\reff}(a|x)}\Big]\\
    =& \rE_{x \sim d_0} \rE_{a \sim \pi(\cdot | x)} [R^*(x, a)] - \frac{1}{\eta}\KL\big(\pi(\cdot|x)\| \pi_{\reff}(\cdot|x)\big),
\end{aligned}}
\end{equation*}
where $\pi_{\reff}$ is the reference policy known to the learner, and $\eta > 0$ is the regularization parameter. This formulation is adopted in nearly all RLHF practice for LLM alignment \citep{bai2022training, ouyang2022training, touvron2023llama}, and there is a closed-form solution for this optimization, also known as Gibbs distribution.
\begin{lemma}[Solution of KL-regularized Optimization (Proposition 7.16 and Theorem 15.3 of \citealt{zhang2023mathematical})] \label{lem:kl_solu} 
For any fixed $x \in \cX$ and reward function $R$, we have
\begin{equation*}
{
\begin{aligned}
    \max_{\pi} \E_{a \sim \pi(\cdot|x)} \Big[R(x,a) - \eta^{-1} \KL\big(\pi(\cdot|x)\|\pi_{\reff}(\cdot|x \big)\Big] = \frac{1}{\eta} \cdot \log \E_{a \sim \pi_{\reff}(\cdot|x)} \exp \big(\eta R(x,a)\big), 
\end{aligned}}
\end{equation*}
where $Z_R(x)$ is the normalization constant and the minimizer of the loss functional is 
\begin{align*}
    \pi_R^\eta(a|x) = \frac{1}{Z_R(x)} \pi_{\reff}(a|x)\exp\Big( \eta R(x,a)\Big).
\end{align*} 
\end{lemma}

\subsection{KL-Regularized Reinforcement Learning}
In this section, we introduce the KL-regularized MDP problem. A Markov Decision Process (MDP) is defined by a tuple $\cM = (\cS, \cA, H, \PP, d_0, r)$, where $\cS$ is the state space, $\cA$ is the action space, $H$ is the time horizon, transition probability $\PP=\{\PP_h\}_{h=1}^H$ denotes the probability $\PP(s_{h+1}|s_h,a_h)$ of transition from the current $(s_h,a_h)$ to the next state $s_{h+1}$ at each step $h$, $d_0$ is the initial state distribution, and $r=\{r_h:\cS\times\cA\rightarrow\RR\}_{h=1}^H$ is the reward function. 

A policy $\pi$ is a sequence of function $\pi = \{\pi_1, \ldots, \pi_h\}$ with $\pi_h: \cS \times \cA \to [0, 1]$ for all $h \in [H]$. The \textbf{objective} is to find a policy $\pi$ that maximizes the following KL-regularized value: 
\begin{equation*}
{
\begin{aligned} 
    J(\pi) =& \E^{\pi} \bigg[\sum_{h = 1}^H r_h (s_h, a_h) - \eta^{-1} \sum_{h = 1}^H \log \frac{\pi_h(a_h|s_h)}{\pi_{\reff,h}(a_h|s_h)}\bigg],
\end{aligned}}
\end{equation*}
where $\E^{\pi}$ denotes the expectation $\{(s_h,a_h,r_h)\}_{h=1}^H$ of the reward and the trajectory induced by policy $\pi$ from the initial state $s_1\sim d_0$.

We can define the value functions as the expected future return with KL-regularization:
\begin{equation*}
{
    \begin{aligned}
        V^{\pi}_h(s_h) =& \sum_{h'=h}^H \EE^{\pi}\Big[ r_{h'}(s_{h'}, a_{h'}) - \frac{1}{\eta} \cdot \KL\bigl(\pi_{h'}(\cdot | s_{h'})\| \pi_{\reff,h'}(\cdot | s_{h'})\bigr) \,\Big|\, s_h \Big],\\
        Q^{\pi}_h(s_h, a_h) =& r_h(s_h,a_h)+ \EE^{\pi}\sum_{h'=h+1}^H \Big[r_{h'}(s_{h'}, a_{h'}) - \frac{1}{\eta} \cdot \KL\bigl(\pi_{h'}(\cdot | s_{h'})\|\pi_{\reff,h'}(\cdot | s_{h'})\bigr) \,\Big|\,  s_h, a_h \Big].
    \end{aligned}}
\end{equation*}
We can also iteratively define the regularized value function as follows: $V_{H + 1}^\pi(s_{H + 1}) = 0$, and
\begin{equation*}
{
    \begin{aligned}
        &V_h^\pi(s_h) = \EE_{a_h \sim \pi_h(\cdot | s_h)} \Big[Q_h^\pi(s_h, a_h) - \frac{1}{\eta} \cdot \KL\bigl(\pi_h(\cdot | s_h)\|\pi_{\reff,h}(\cdot | s_h)\bigr)\Big], \\
        &Q_h^\pi(s_h, a_h) = r_h(s_h,a_h) + \EE_{s_{h + 1} \sim \PP_h(\cdot | s_h, a_h)} \Big[V_{h + 1}^\pi(s_{h + 1})\Big].
    \end{aligned}}
\end{equation*}
We further define the optimal value function $\{V_h^*\}_{h \in [H]}$ and the optimal action-value function $\{Q_h^*\}_{h \in [H]}$ as 
$$
\begin{aligned}
    &V_h^*(s_h) = \max_{\pi} V_h^\pi(s_h), \quad Q_h^*(s_h, a_h) = \max_{\pi} Q_h^\pi(s_h, a_h).
\end{aligned}
$$

Assume optimal policy is achieved at $\pi^*$, using Lemma~\ref{lem:kl_solu} and a backward iteration starting from the $V^*_{H+1}(s_{H+1})=0$, we have the following proposition.
\begin{proposition}
The optimal policy is a layer-wise Gibbs distribution of the $Q^*$:
$$
\begin{aligned}
    \pi_h^*(a_h|s_h) = \frac{1}{Z^*_h(s_h)}\pi_{\reff,h}(a_h|s_h)
\cdot \exp\big(\eta Q_h^*(s_h,a_h)\big),
\end{aligned}
$$
where $Z_h^*(s_h) := \EE_{a_h \sim \pi_{\reff,h}(\cdot |s_h)}
\exp\big(\eta Q_h^*(s_h,a_h)\big)$ is the normalization constant. Also, the V value is the maximum value of the single-step KL-regularized optimization problem:
\begin{equation}\label{eq:V_h}
    V_h^*(s_h) = \frac1\eta \log 
\EE_{a_h \sim \pi_{\reff,h}(\cdot |s_h)}
\exp\big(\eta Q_h^*(s_h,a_h)\big).
\end{equation}
We also have the following connection between the $Q^*$ and $V^*$:
\begin{equation}
    Q_h^*(s_h,a_h) = r_h(s_h,a_h) + \EE_{s_{h+1} \sim \PP_h(\cdot|s_h,a_h)}
V_{h+1}^*(s_{h+1}) .
\end{equation}
\end{proposition}
Without loss of generality, we assume that $Q_h^*(s_h,a_h)\in[0,1]$ for any $(s_h,a_h)\in\cS\times\cA$\footnote{This is for the simplicity of analysis. By multiplying the results by $H$, our analysis can be applied to the cases where $r_h\in[0,1]$ for $h\in[H]$.}. From \eqref{eq:V_h}, we know that $V_h^*(s_h)\in[0,1]$ for any $s_h\in\cA$. 

\paragraph{Function approximation}
We approximate the value function $\{Q_h^*\}_{h\in[H]}$ by a function class $\cF=\cF_1\times\ldots\times\cF_H$ where $\cF_h:\cS\times\cA\rightarrow [0,1]$. Similarly, we can define related optimal policies and value functions with the estimated $Q$-function $\hQ_h\in\cF_h$ and $V$-function $\hV_h$.

Then, we define the KL-regularized Bellman operator $\cT_\eta^h$ on space $\cF$: for any $f_{h+1}\in\cF_{h+1}$,
\begin{equation*}
\begin{aligned}
    &\cT_\eta^h f_{h+1}(s_h,a_h) := r_h(s_h,a_h) + \frac{1}{\eta}\E_{s_{h+1}|s_h,a_h} V^f_{h+1}(s_{h+1}),
\end{aligned}
\end{equation*}
where $V^f_{h+1}(s_{h+1})= \eta^{-1} \log \EE_{a_{h+1} \sim \pi_{\reff,h+1}(\cdot |s_{h+1})}
\exp\big(\eta f_{h+1}(s_{h+1},a_{h+1})\big)$.

To ensure efficient learning, we suppose that the true $Q$-value function belongs to the considered function class and the one-step backward of the value function also remains in the considered function class, which are known as realizability and Bellman completeness assumptions, respectively.
\begin{assumption}[Realizability]\label{as:Realizability}
    For each $h\in[H]$, we have $Q_h^*\in\cF_h$.
\end{assumption}
\begin{assumption}[Bellman completeness]\label{as:Bellman completeness}
For each $h\in[H]$ and any $f_{h+1}\in\cF_{h+1}$, we have $\cT_\eta^h f_{h+1} \in \cF_h$.
\end{assumption}
These two assumptions are standard and normal in RL literature with general function approximation \citep{wang2020reinforcement, jin2021bellman, zhang2023mathematical}. 

\section{KL-Regularized Contextual Bandits} \label{sec:kl_bandit}

\subsection{The Proposed Algorithm and Result}

\begin{algorithm}[ht]
    \caption{KL-Regularized UCB}
    \label{alg:kl-bandits}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} $\eta$, $\epsilon$, $\pi_{\reff}$, $\cR$.
    \FOR {$t$ = $1, \ldots, T$}
        \STATE Sample context $x_t \sim d_0$ and action $a_t \sim \pi_t(\cdot|x_t)$.
        \STATE Observe reward $r_t = R^*(x_t, a_t) + \epsilon_t$, where $\epsilon_t$ is the random noise. 
        \STATE Compute the least square estimate of the reward function based on $\cD_t=\{(x_i, a_i, r_i)\}_{i=1}^t$:
        $$\hat{R}_{t} \gets \argmin_{R \in \cR}\sum_{i=1}^t (R(x_i, a_i) - r_i)^2.
        $$ 
        \STATE Apply the planning oracle to compute $\pi_{t + 1}(\cdot | \cdot) \propto \pi_{\reff}(\cdot | \cdot) \exp\bigl[\eta \big(\hat R_t(\cdot, \cdot) + b_t(\cdot, \cdot)\big)\bigr]$, \label{alg1:line:policy}
        where $b_t$ is the exploration bonus term defined in \eqref{eq:bonus}.
    \ENDFOR
    \end{algorithmic}
\end{algorithm}

We develop the KL-regularized Upper Confidence Bound (KL-UCB) algorithm in Algorithm \ref{alg:kl-bandits}. In each round $t\in[T]$, after observing context $x_t\sim d_0$, taking action $a_t\sim\pi_t(\cdot|x_t)$, and receiving the reward $r_t = R^*(x_t, a_t) + \epsilon_t$, we compute the estimator $\hat{R}_t$ by solving the least square regression. Then, we perform optimism by adding the following bonus term to the reward estimator:
\begin{equation}
\label{eq:bonus}
{
\begin{aligned}
    b_t(x,a)= \min\Big\{1,\beta_T \cdot U_{\cR_{t}}(\lambda,x,a;\cD_{t})\Big\},
\end{aligned}}
\end{equation}
where $U_{\cR_{t-1}}$ is the uncertainty in Definition \ref{def:eluder}, $\cD_{t-1}=\{(x_i,a_i)\}_{i\in[t-1]}$, and we define the confidence set iteratively:
$$
\begin{aligned}
    \cR_t=\{R\in\cR: \sum_{i=1}^{t}(R(x_i,a_i)-\hat{R}_t(x_i,a_i))^2+\lambda \le \beta_T^2\}.
\end{aligned}
$$

\begin{theorem}\label{th:bandit}
    Under Assumption \ref{as:Reward function realizability}, for any $\delta>0$, by taking $\beta_T=16\log(N_{\cR}T/\delta)$, with probability at least $1-\delta$, the output of Algorithm \ref{alg:kl-bandits} satisfies
    $$
    \mathrm{Regret}(T) = \cO\Big(\eta\log(N_{\cR}T/\delta)\cdot d(\cR,\lambda,T)\Big).
    $$
\end{theorem}

\begin{remark}
    Theorem \ref{th:bandit} establishes that the regret of Algorithm~\ref{alg:kl-bandits} scales logarithmically with $T$, rather than at the typical $\cO(\sqrt{T})$ rate, thereby improving upon the previous analysis in \citet{xiong2024iterative} under the standard online RL setting. To the best of our knowledge, this is the first analysis of a KL-regularized contextual bandit that achieves this logarithmic regret bound.
\end{remark}

\subsection{Proof Outline of Theorem~\ref{th:bandit}}
We first explain where the previous methods loosen and then highlight our methods to get the sharp bound.
\paragraph{Review of Analysis in Previous Work.} 
Previous analysis \citep{xiong2024iterative,ye2024theoretical,song2024importance} neglects the pivotal role of KL-regularization in the suboptimality decomposition and reduce to the traditional bandit analysis. Specifically, using the short-hand notation $R(x,\pi)=\E_{a\sim\pi(\cdot|x)}R(x,a)$ and $\KL(\pi\| \pi')=\KL(\pi(\cdot|x)\|\pi'(\cdot|x))$, they derive that
\begin{equation*}
\begin{aligned}
&R^*(x,\pi^*) - \KL(\pi^*\|\pi_\reff)- \big(R^*(x,\pi_t) - \KL(\pi_t\|\pi_\reff)\big)
    \notag\\
    &= \underbrace{R(x,\pi^*) - \hat{R}_t(x,\pi^*) - b_t(x,\pi^*)}_{\displaystyle{\le 0 ~\text{(By Optimism)}}} + \underbrace{\hat{R}_t(x,\pi_t) - R^*(x,\pi_t) + b_t(x,\pi_t)}_{\displaystyle{\le 2b_t(x,\pi_t)}}\notag\\
    &\qquad + \big(\hat R_t(x,\pi^*) + b_t(x,\pi^*) - \KL(\pi^*(\cdot|x)\|\pi_\reff(\cdot|x))\big) \notag\\
    &\qquad - \big(\hat R_t(x,\pi_t) + b_t(x,\pi^*) - \KL(\pi_t(\cdot|x)\|\pi_\reff(\cdot|x))\big),
    \label{eq:suboptimality-gap:loose}
\end{aligned}
\end{equation*}
where on the right-hand side of the last equality, the first two terms follow from the optimistic reward estimator and the definition of the bonus, the subtraction of the last two terms is upper bounded by $0$ since $\pi_t$ is the maximizer of the KL-regularized reward according to Algorithm \ref{alg:kl-bandits}. Essentially, deserting the last two terms is where they loosen and reduce to the traditional analysis. As a result, their regret is upper bounded by the first-order summation of bonuses $2\sum_{t=1}^T \E_{x\sim d_0} b_t(x,\pi_t)$ and finally get an $\tilde{\cO}(\sqrt{T})$ regret.

\paragraph{Our Analysis.}
We provide the proof sketch of Theorem~\ref{th:bandit} here. The details are provided in Appendix \ref{sec:proof_bandit}.
Our analysis is divided into the following four parts.
\paragraph{Part I: Single-Step Regret Decomposition.}
For all $t \ge 1$, with $R^*(x,a) = \frac{1}{\eta}\log \exp \big(\eta R^*(x,a)\big)$ and the closed-form solution of $\pi^*$ and $\pi$ from Lemma \ref{lem:kl_solu}, we have the following equality for the suboptimality gap: 
\begin{equation}
\begin{aligned}
&\EE_{\pi^*} \bigg[R^*(x,a) - \frac{1}{\eta}\log\frac{\pi^*(a|x)}{\pi_{\reff}(a|x)}\bigg]  - \EE_{\pi_{t}} \bigg[R^*(x, a) - \frac{1}{\eta}\log \frac{\pi_{t}(a|x)}{\pi_{\reff}(a|x)}\bigg]
    \\
    &= \frac{1}{\eta} \log Z_{R^*}(x) - \frac{1}{\eta} \log Z_{\hat R_{t - 1} + b_{t - 1} }(x) + \rE_{\pi_{t}} \big[\hat R_{t - 1}(x, a) - R^*(x, a) + b_{t-1}(x, a)\big],
    \label{eq:suboptimality-gap:copy}
\end{aligned}
\end{equation}
where $Z_R(x)$ is the normalization term for the policy with respect to reward function $R$. We defer the detailed derivation of \eqref{eq:suboptimality-gap:copy} to Appendix \ref{sec:proof_bandit}.

\begin{remark}
    Compared to previous work \citep{xiong2024iterative, xie2024exploratory,ye2024theoretical} that neglect the KL term and reduce to the traditional RL analysis, the upper bound in \eqref{eq:suboptimality-gap:copy} is more refined and involves the gap of the normalization term $Z_R(x)$, which is crucial for the analysis of the KL-regularized objective.
\end{remark}

\paragraph{Part II: Shape of Regularized Suboptimality Gap.}
To further analyze \eqref{eq:suboptimality-gap:copy}, we define a function 
\begin{equation}
    \begin{aligned}
        \Delta(x,R) := - \frac{1}{\eta} \log Z_R(x) + \E_{\pi_R^\eta} \big[R(x,a) - R^*(x,a)\big].
        \label{eq:delta-R:copy}
    \end{aligned}  
\end{equation} 
and write \eqref{eq:suboptimality-gap:copy} as the function gap:
\begin{equation}\label{eq:gap}
    \begin{aligned}
        \rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)].
    \end{aligned}  
\end{equation} 
To analyze the behavior of \eqref{eq:gap}, we have the following lemma, which reveals the gradient of $\Delta(x,R)$ with respect to $R$.

\begin{lemma} \label{lem:computation1:copy}
Let $Z_R(x) = \sum_{a \in \cA} \pi_{\reff}(a|x) \cdot \exp(\eta R(x, a))$ and $\pi_R^{\eta}(a|x)=\pi_{\reff}(a|x)\exp(\eta R(x,a))/Z_R(x)$. We have the following partial gradient result:
\begin{equation}
    \label{eqn:computation:sketch}
    \begin{aligned}
        &\frac{\partial\Delta(x,R)}{\partial R(x,a)} = \eta \pi_R^\eta(a|x) \cdot \big(R(x,a) - R^*(x,a)\big)  \\&- \eta \sum_{a' \in \cA} \pi_R^\eta(a|x) \cdot \pi_R^\eta(a'|x) \cdot \big(R(x,a') - R^*(x,a')\big). 
 \end{aligned}
\end{equation}
\end{lemma}
The proof of this lemma is deferred to Appendix \ref{sec:proof_bandit}. With the gradient information above, we can apply the mean value theorem to show that
there exists an $f_\lambda = \lambda \cdot (\hat R_{t - 1} + b_{t - 1}) + (1 - \lambda) \cdot R^*$ for some $\lambda \in (0, 1)$ such that 
\begin{equation}
\begin{aligned} 
    &\rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)]  
    = \rE_{x \sim d_0}\Big[\sum_{a\in\cA}\Big(\eta \pi_{f_\lambda}^\eta(a|x) \cdot \big(f_\lambda(x,a) - R^*(x,a)\big) \notag\\&\qquad  - \eta \sum_{a' \in \cA} \pi_{f_\lambda}^\eta(a|x) \cdot \pi_{f_\lambda}^\eta(a'|x) \cdot \big(f_\lambda(x,a') - R^*(x,a')\big)\Big) \cdot\big(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a)-R^*(x,a)\big)\Big], \label{eq:mean-value:sketch_0}
\end{aligned}
\end{equation}

\paragraph{Part III: Benefits of Optimism.}
The bonus used in Algorithm \ref{alg:kl-bandits} ensures that with high probability, the true reward function is upper bounded by the optimistic reward function, which is formally stated in the following lemma.

\begin{lemma}\label{lm:Confidence Bound for Reward Function}
    Under Algorithm \ref{alg:kl-bandits} and Assumption \ref{as:Reward function realizability} and the condition that the noises $\epsilon_t$ are conditional $1$-sub-Gaussian, we have with probability at least $1-\delta$ for all $t\in[T]$, the uniform optimism event that $\cE_{\opt}^t=\{\hat{R}_t(x,a)+b_t(x,a)-R^*(x,a) \ge 0,~\forall(x,a)\in\cX\times\cA\}$ holds true.
\end{lemma}
Hence, conditioning on $\cE_{\opt}^t$ we have $f_\lambda(x,a') - R^*(x,a') \ge 0,\quad \forall(x,a)\in\cX\times\cA,$ which is substituted back into \eqref{eq:mean-value:sketch_0} to get 
\begin{equation}
\begin{aligned} 
    &\rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)]  
     \\&\le \eta \rE_{x \sim d_0} \big[\sum_{a \in \cA} \bigl(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a) - R^*(x, a)\bigr) \cdot \underbrace{\bigl(f_\lambda(x, a) - R^*(x, a)\bigr) \cdot \pi_{f_\lambda}^\eta(a|x)}_{U(x,a)}\big]
     \\&\le \eta \rE_{x \sim d_0} \rE_{a\sim \pi_{\hat R_{t - 1} + b_{t - 1}}^\eta(\cdot|x)} \big[ \bigl(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a) - R^*(x, a)\bigr)^2 \big], \label{eq:mean-value:sketch}
\end{aligned}
\end{equation}
where the last inequality holds since by taking the derivative of $U(x,a)$ with respect to $\lambda$, we can show that $U(x,a)$ reaches the maximum when $\lambda = 1$.

\paragraph{Part IV: Regret Bound.} Conditioning on high-probability event $\cup_{t\in[T]}\cE_\opt^t$, we derive the regret bound from \eqref{eq:mean-value:sketch}:
\begin{equation*}
\begin{aligned}
    &\mathrm{Regret}(T) \le \eta\sum_{t=1}^T\rE_{x_t \sim d_0} \rE_{a_t\sim \pi_t} \big[ \bigl(\hat R_{t - 1}(x_t, a_t) + b_{t - 1}(x_t, a_t) - R^*(x_t, a_t)\bigr)^2 \big]\\
    &\le 4\eta\sum_{t=1}^T\rE_{x_t \sim d_0} \rE_{a_t\sim \pi_t} \bigl(b_{t - 1}(x_t, a_t)\bigr)^2,
\end{aligned}
\end{equation*}
Thus, according to the definition of bonus $b_t$ in \eqref{eq:bonus}, we can follow standard analyses for general function approximation under the finite eluder dimension to obtain the desired result.

\section{KL-Regularized RL}
\subsection{The Proposed Algorithm and Result}

\begin{algorithm}[ht]
    \caption{KL-regularized LSVI with UCB}
    \label{alg:kl-mdp}
    \begin{algorithmic}[1]
    \STATE \textbf{Input:} $\eta$, $\epsilon$, $\pi_{\reff}$, $\cF$.
    \FOR {episode $t$ = $1, \ldots, T$}
        \STATE Receive the intial state $s_1^t$.
        \FOR {stage $h = H, \ldots, 1$}
            \STATE Set $\hat f_h^t \gets \argmin_{f_h \in \cF_h} \sum_{i = 1}^{t - 1} (f_h(s_h^i, a_h^i) - r_h^i - \hV^t_{h + 1}(s_{h + 1}^i))^2$.
            \STATE Construct bonus $b_h^t(\cdot,\cdot)$ defined in \eqref{eq:bonus_mdp}.
            \STATE Set $\hat Q^t_h(s, a) \gets \hat f^t_h(s, a) + b^t_h(s, a)$ for any $(s, a) \in \cS \times \cA$.
            \STATE Set $\hV^t_h(s) \gets \max_{\pi} \EE_{a \sim \pi(\cdot | s)} \bigl[\hQ^t_h(s, a) - \eta^{-1} \cdot \KL(\pi(\cdot | s), \pi_{\reff}(\cdot | s))\bigr]$.
        \ENDFOR          
        \FOR {stage $h = 1 \ldots, H$}
            \STATE Observe $s_h^t$.
            \STATE Follow $\pi^t_h(\cdot | \cdot) \propto \pi_{\reff}(\cdot | \cdot) \exp\bigl[\eta \hat Q^t_h(\cdot, \cdot)\bigr]$.
            \STATE Choose action $a_h^t \sim \pi^t_h(\cdot | s_h^t)$.
        \ENDFOR
    \ENDFOR
    \end{algorithmic}
\end{algorithm}

We adapt the KL-regularization to the Least Square Value Iteration with UCB framework \citep{zhang2022feel,wang2020reinforcement} to propose the KL-LSVI-UCB algorithm in Algorithm \ref{alg:kl-mdp}, which establishes the optimistic $Q$-value function estimations $\hQ_h^t$ backward from step $H$ to step $1$, with $\hQ_{H+1}^t\equiv0$. In each step $h\in[H]$, we first learn the estimator $\hf_h^t$ to minimize the Bellman backward error:
\begin{align*}
    \hat f_h^t = \argmin_{f_h \in \cF_h} \sum_{i = 1}^{t - 1} (f_h(s_h^i, a_h^i) - r_h^i - \hV^t_{h + 1}(s_{h + 1}^i))^2.
\end{align*}
Then, under the principle of optimism, we construct the confidence set iteratively
\begin{equation*}
\begin{aligned}
    &\cF_h^{t-1} = \Big\{f_h\in\cF_h: \sum_{i=1}^{t-1}(f_h(s_h^i,a_h^i) - \hf_h^t(s_h^i,a_h^i))^2 +\lambda\le (\beta_h^T)^2\Big\},
\end{aligned}
\end{equation*}
and the bonus:
\begin{equation}
\label{eq:bonus_mdp}
\begin{aligned}
    b_h^t(s,a) = \min\Big\{1, \beta_h^T\cdot U^h_{\cF_h^{t-1}}(\lambda,s, a; \cD^h_{t-1})\Big\},
\end{aligned}
\end{equation}
where $U^h_{\cF_h^{t-1}}$ is the uncertainty formulated in Definition \ref{def:eluder}, and $\cD^h_{t-1}=\{s_h^i,a_h^i,r_h^i,s_{h+1}^i\}_{i\in[t-1]}$. Hence, we add the bonus to the value function estimation to get the optimistic estimation $\hat Q^t_h(s, a) =\hat f^t_h(s, a) + b^t_h(s, a)$ and 
\begin{equation*}
\begin{aligned}
    \hV^t_h(s) = \frac1\eta \log \EE_{a \sim \pi_{\reff,h}(\cdot |s)}\exp\big(\eta \hQ^t_{h}(s,a)\big).
\end{aligned}
\end{equation*}

For the analysis, we follow \citet{ye2023corruption,wang2020reinforcement} and assume that there exists a bonus function class $\cB_h$ with cardinality $N_{\cB_{h}}$ that accommodates bonus functions $b_h^t$. \citet{ye2023corruption,wang2020reinforcement} have used examples and algorithms to show that this type of bonus can be approximately calculated under suitable conditions.

\begin{theorem}\label{th:mdp}
Under Algorithm \ref{alg:kl-mdp} and Assumptions \ref{as:Realizability} and \ref{as:Bellman completeness} with $\beta_h^T = \Theta(\sqrt{\log(N_h T H / \delta)})$, we have with probability at least $1-\delta$,
$$
\mathrm{Regret}(T)
= \cO\Big(\eta H^2 d(\cF,\lambda,T) \cdot \log(N_{\cF\oplus\cB} TH /\delta)\Big),
$$
where $d(\cF,\lambda,T)=\sum_{h=1}^Hd(\cF_h,\lambda,T)$ and $N_{\cF\oplus\cB}=\max_h N_{\cF_h}\cdot N_{\cF_{h+1}}\cdot N_{\cB_{h+1}}$.
\end{theorem}

\subsection{Proof Outline of Theorem~\ref{th:mdp}}
We defer the detailed proof in Appendix \ref{sec:proof_mdp} and highlight the crucial part of the proof in the sequel.

The crucial part of the analysis for the MDP setting lies in decomposing the regret into the errors of Bellman backup in each step. For conciseness, we omit the superscript $t$ for estimators when there is no confusion. We denote the Bellman error as
$
e_h(s_h,a_h) = \hQ_h(s_h,a_h)-\cT_\eta^h \hV_{h+1}(s_h,a_h).
$

\paragraph{Review of Standard Policy Loss Decomposition.}
We first recall the standard decomposition without KL-regularization \citep{zhang2023mathematical}:
\begin{equation}
\begin{aligned}
    &V_1^{\pi^*}(s_1) - V_1^{\hat\pi}(s_1) \le \hV_1(s_1) - V_1^{\hat\pi}(s_1)\\
    &\le \E^{\hat\pi}\big[\hQ_1(s_1,a_1) - \cT^1 \hV_2(s_1,a_1) + \cT^1 \hV_2(s_1,a_1) - \cT^1 V_2^{\hat\pi}(s_1,a_1)\big]\\
    &\le \ldots \le \E^{\hat\pi}\Big[\sum_{h=1}^H \big(\hQ_h(s_h,a_h)-\cT_\eta^h \hV_{h+1}(s_h,a_h)\big) \Big],\label{eq:old_decomp}
\end{aligned}
\end{equation}
where $\E^{\hat\pi}$ denotes the expectation of trajectories generated by $d_0\times\hpi$, and $\cT$ denotes the Bellman operator without KL-regularization. Thus, following the standard decomposition results in the polynomial dependence on $T$ for the previous works \citep{xiong2024iterative,xie2024exploratory}.

\paragraph{Our Analysis.}
In this work, to avoid the direct summation of Bellman errors as in \eqref{eq:old_decomp}, we develop a novel decomposition to obtain the square of the Bellman errors. To realize this goal, instead of decomposing the value functions, we decompose the policies.

For each $h$, let $\hat \pi^{(h)} := \hat \pi_{1:h} \oplus \pi^*_{h+1:H}$ be the concatenated policy of $\hat \pi$ and $\pi^*$ at time step $h$. Then, $\pi^*$ can be denoted as $\hat\pi^{(0)}$ and $V_1^*=V_1^{\hat\pi^{(0)}}$. %In other words, we roll in by playing $\hat{\pi}$ to get $s_1 \sim d_0, a_1 \sim \hat{\pi}^1(\cdot|s_1), \cdots, a_{h-1} \sim \hat{\pi}^{h-1}(\cdot|s_{h-1}), s_h \sim \PP_{h-1}(\cdot|s_{h-1}, a_{h-1})$ but play $a_h \sim \pi^*_h(\cdot|s_h)$ at step $h$. 
Then the suboptimality gap for $\hat \pi$ can be decomposed as follows: 
$$
\begin{aligned} 
    J(\pi^*) - J(\hat \pi) &= \sum_{h = 0}^{H - 1} \EE_{s_1 \sim d_0} \underbrace{\bigl[V_1^{\hat \pi^{(h)}}(s_1) - V_1^{\hat \pi^{(h + 1)}}(s_1)\bigr]}_{I_{h + 1}}.
\end{aligned}
$$
For each term $I_{h + 1}$, since $\hat \pi^{(h)}$ and $\hat \pi^{(h+1)}$ only differs at step $h+1$, and they are both $\pi_{h'}^*$ for $h'=h+2,\ldots,H$, we use the notation $\KL(\pi_h,\pi'_h):=\KL(\pi_h(\cdot|s_h)\|\pi'_h(\cdot|s_h))$ for short and can reduce the step-wise gap back into the bandit gap:
\begin{equation*}
\begin{aligned}
&\EE_{s_{h + 1} \sim d_{h + 1}^{\hat\pi}}\Big\{Q_{h + 1}^{\pi^*}(s_{h + 1}, \pi^*_{h+1}) - \eta^{-1}\KL(\pi_{h+1}^*\| \pi_{\reff,h+1})\\
    &\qquad - \bigl[Q_{h + 1}^{\pi^*}(s_{h + 1}, \hpi_{h+1}) - \eta^{-1}\KL(\hpi_{h+1}, \pi_{\reff,h+1})\bigr]\Big\}\\
    &\le \eta\EE^{\hpi}\big[\big(\hQ_{h + 1}(s_{h + 1},a_{h+1}) - Q_{h + 1}^*(s_{h + 1},a_{h+1})\big)^2\big],
\end{aligned}
\end{equation*}
where the inequality holds by applying \eqref{eq:mean-value:sketch} with $R^*=Q_{h + 1}^{\pi^*}$ and $\hat{R}_t+b_t=\hQ_{h + 1}$. Therefore, we can obtain the following bound for each $(I_{h + 1})$.
\begin{lemma}\label{lm:value decompose_mdp}
We use the notation $\E^{\hpi}$ for the expectation of trajectories generated by $d_0\times\hpi$.
Conditioning on the uniform optimism event that $\cE_{\opt}=\{\hQ(s_h,a_h)-Q^*_h(s_h,a_h) \ge 0,~\forall(s_h,a_h)\in\cS\times\cA,~h\in[H]\}$ holds,
$$
\begin{aligned}
    I_{h + 1} \le \eta\EE^{\hpi}_{\cdot|s_{h+1},a_{h+1}} \Big[\Big(\sum_{j=h+1}^H e_j(s_j,a_j) \Big)^2\Big].
\end{aligned}
$$
\end{lemma}
The proof is shown in Appendix \ref{ssec:regret_decomp}. Therefore, we can decompose the regret as follows:
$$
\begin{aligned}
    J(\pi^*) - J(\hat \pi) &\le \eta \sum_{h = 0}^{H - 1}\EE^{\hpi} \Big[\Big(\sum_{j=h+1}^H e_j(s_j,a_j) \Big)^2\Big]\le \eta H^2 \EE^{\hpi} \Big[\sum_{h=1}^H (e_h(s_h,a_h))^2\Big],
\end{aligned}
$$
where the second inequality is the Cauchy-Schwarz inequality. Since the summation over $H$ steps is inside the square, we can only pay an additional order of $H$.

Then, following the standard analysis of MDP literature \citep{zhang2023mathematical}, we can demonstrate that the optimism $\cE_\opt$ holds with a high probability and obtain the desired bound.

\section{Conclusion}
In this paper, we study KL-regularized contextual bandits and MDPs in the standard online RL setting. While these frameworks have been widely applied in the empirical post-training of modern foundation generative models, their theoretical properties are substantially less explored. We propose two provably efficient algorithms: KL-regularized UCB and KL-regularized LSVI with UCB, based on the standard optimism principle and show that they achieve regret bounds that scale logarithmically with $T$, significantly improving over the typical $\cO(\sqrt{T})$ rate. To the best of our knowledge, this is the first theoretical analysis of KL-regularized RL to establish such logarithmic regret bounds in the standard online setting, aligning well with the empirically observed superior sample efficiency of KL-regularized RL \citep{bai2022training, deepseekai2025deepseekr1incentivizingreasoningcapability}. An expense of this logarithmic bound for MDPs is that the bound has additional dependence on the horizon $H$, which can be left as future work.

The key to this improvement lies in a refined value decomposition for the bandit setting and a novel policy decomposition technique for MDPs, both of which may be of independent interest. We hope our study inspires further theoretical investigations into the learning dynamics of KL-regularized RL.

\section*{Acknowledgements}

We are grateful to Qiwei Di and Jiawei Huang for their insightful comments, which helped us identify and correct an error in our proof.

\bibliographystyle{ims}
\bibliography{ref}

\newpage
\appendix

\section{Proofs for KL-Regularized Contextual Bandits} \label{sec:proof_bandit}
\begin{lemma}[Objective Decomposition]\label{lm:regret_decompose_bandit}
    For any $t\in[T]$, conditioning on the uniform optimism event that $\cE_{\opt}^t=\{\hat{R}_t(x,a)+b_t(x,a)-R^*(x,a) \ge 0,~\forall(x,a)\in\cX\times\cA\}$ holds, we have
    \begin{align*}
        J(\pi^*) - J(\pi_t) \le \eta \rE_{x \sim d_0} \rE_{a\sim \pi_t} \bigl[\bigl(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a) - R^*(x, a)\bigr)^2 \bigr].
    \end{align*}
\end{lemma}
\begin{proof}
For all $t \ge 1$, with $R^*(x,a) = \frac{1}{\eta}\log \exp \big(\eta R^*(x,a)\big)$ and the closed-form solution of $\pi^*$ and $\pi$ from Lemma \ref{lem:kl_solu}, we have the following equality for the suboptimality gap: 
\begin{align} 
    &\EE_{\pi^*} \bigg[R^*(x,a) - \frac{1}{\eta}\log\frac{\pi^*(a|x)}{\pi_{\reff}(a|x)}\bigg] - \EE_{\pi_{t}} \bigg[R^*(x, a) - \frac{1}{\eta}\log \frac{\pi_{t}(a|x)}{\pi_{\reff}(a|x)}\bigg] \notag
    \\&= \frac{1}{\eta} \rE_{\pi^*} \biggl[\log \frac{\pi_{\reff}(a|x) \cdot \exp(\eta R^*(x, a))}{\pi^*(a|x)}\biggr] - \frac{1}{\eta} \rE_{\pi_{t}} \biggl[\log \frac{\pi_{\reff}(a|x) \cdot \exp(\eta R^*(x, a))}{\pi_{t}(a|x)}\biggr]\notag
    \\&= \frac{1}{\eta} \rE_{\pi^*} \biggl[\log \frac{\pi_{\reff}(a|x) \cdot \exp(\eta R^*(x, a))}{\pi^*(a|x)}\biggr] - \frac{1}{\eta} \rE_{\pi_{t}} \biggl[\log \frac{\pi_{\reff}(a|x) \cdot \exp(\eta (\hat R_{t - 1} (x, a) + b_{t - 1}(x, a)))}{\pi_{t}(a|x)}\biggr] \notag
    \\&\quad + \rE_{\pi_{t}} \big[\hat R_{t - 1}(x, a) - R^*(x, a) + b_{t-1}(x, a)\big]\notag\\
    &= \frac{1}{\eta} \log Z_{R^*}(x) - \frac{1}{\eta} \log Z_{\hat R_{t - 1} + b_{t - 1} }(x) + \rE_{\pi_{t}} \big[\hat R_{t - 1}(x, a) - R^*(x, a) + b_{t-1}(x, a)\big],
    \label{eq:suboptimality-gap}
\end{align}
where $Z_R(x) := \sum_{a \in \cA} \pi_{\reff}(a|x) \cdot \exp(\eta R(x, a))$ is the normalization term for the policy with respect to reward function $R$.


We define $\Delta(x,R)$ as the following quantity for a reward function $R: \cX \times \cA \to \RR$: \begin{equation}
\begin{aligned}
    \Delta(x,R) :=& -\frac{1}{\eta} \log Z_R(x) + \sum_{a \in \cA}  \frac{ \pi_{\reff}(a|x) \cdot \exp(\eta R(x, a))}{Z_R(x)}\cdot \bigl(R(x, a) - R^*(x, a)\bigr)\\
    &= - \frac{1}{\eta} \log Z_R(x) + \E_{\pi_R^\eta} \big[R(x,a) - R^*(x,a)\big].
    \label{eq:delta-R}
\end{aligned}  
    \end{equation} 
With the definition of $\Delta(x,a)$, the gap in \eqref{eq:suboptimality-gap} can be written as $\rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)]$. To analyze the behavior of $\rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)]$, we now study the gradient of $\Delta(x,R)$. We are interested in the gradient of $\Delta(x,R)$ with respect to $R(x,a)$ for each $a\in\cA$.

Then, by invoking Lemma \ref{lem:computation1} to simplify the computation, we can write the gradient as follows:
\begin{align*}
        &\frac{\partial\Delta(x,R)}{\partial R(x,a)} \\&= \frac{\partial\big[- \frac{1}{\eta} \log Z_R(x) + \E_{\pi_R^\eta} \big[R(x,a) - R^*(x,a)\big]\big]}{\partial R(x,a)}\\
        &= - \frac{1}{\eta} \frac{1}{Z_R(x)} \frac{\partial Z_R(x)}{\partial R(x,a)} + \frac{\partial}{\partial R(x,a)}\big[ \pi_R^\eta(a|x) \cdot  \big(R(x,a) - R^*(x,a)\big)\big] \\
        &\qquad +  \frac{\partial}{\partial R(x,a)}\big[\sum_{a' \neq a} \pi_R^\eta(a'|x) \cdot  \big(R(x,a') - R^*(x,a')\big)\big] \\
        &= - \pi_R^\eta(a|x) + \Big[ \pi_R^\eta(a|x) + \frac{\partial \pi^\eta_R(a|x)}{\partial R(x,a)} \cdot \big(R(x,a') - R^*(x,a')\big)  \Big]\\
        &\qquad + \sum_{a' \neq a} \frac{\partial \pi^\eta_R(a'|x)}{\partial R(x,a)} \cdot \big(R(x,a) - R^*(x,a)\big)\\
        &= \big[\eta \pi_R^\eta(a|x) - \eta \pi_R^\eta(a|x)^2\big] \cdot \big(R(x,a) - R^*(x,a)\big) - \sum_{a' \neq a}\eta \pi_R^\eta(a|x) \cdot \pi_R^\eta(a'|x) \cdot \big(R(x,a') - R^*(x,a')\big) \\
        &= \eta \pi_R^\eta(a|x) \cdot \big(R(x,a) - R^*(x,a)\big)  - \eta \sum_{a' \in \cA} \pi_R^\eta(a|x) \cdot \pi_R^\eta(a'|x) \cdot \big(R(x,a') - R^*(x,a')\big), 
\end{align*}
where the third equality uses \eqref{eqn:computation1} and the fourth equality uses \eqref{eqn:computation2} and \eqref{eqn:computation3}, the second equality and the last equality simply use the linearity of $\partial$ and summation. 

Since $\frac{\partial \Delta(x,R)}{\partial R(x,a)}$ is uniformly bounded when $R$ is uniformly bounded, by the Mean Value Theorem, there exists $f_\lambda = \lambda \cdot (\hat R_{t - 1} + b_{t - 1}) + (1 - \lambda) \cdot R^*$ for some $\lambda \in [0, 1]$ such that 
\begin{align} 
    &\rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)]  \notag
    \\&= \rE_{x' \sim d_0}\Big[\sum_{a\in\cA}\frac{\partial \Delta(x',f_\lambda)}{\partial R(x,a)}(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a)-R^*(x,a))\Big] \notag
    \\&= \rE_{x' \sim d_0}\Big[\sum_{a\in\cA}\Big(\eta \pi_{f_\lambda}^\eta(a|x) \cdot \big(f_\lambda(x,a) - R^*(x,a)\big)  - \eta \sum_{a' \in \cA} \pi_{f_\lambda}^\eta(a|x) \cdot \pi_{f_\lambda}^\eta(a'|x) \cdot \big(f_\lambda(x,a') - R^*(x,a')\big)\Big)\notag
    \\&\qquad\qquad \cdot(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a)-R^*(x,a))\Big] \notag
    \\&\le \eta \rE_{x \sim d_0} \bigl[\underbrace{\sum_{a \in \cA} \bigl(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a) - R^*(x, a)\bigr) \cdot \bigl(f_\lambda(x, a) - R^*(x, a)\bigr) \cdot \pi_{f_\lambda}^\eta(a|x)}_{U(\lambda)}\bigr], \label{eq:mean-value}
\end{align}
where the last inequality holds since conditioning on $\cE_{\opt}^t$ we have 
\begin{align*}
    f_\lambda(x,a') - R^*(x,a') = \lambda(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a)-R^*(x,a))) \ge 0,\quad \forall(x,a)\in\cX\times\cA.
\end{align*}

% To bound the function $U(f,x,a)$ on the right-hand side of \eqref{eq:mean-value}, we take the derivative as follows: 
% \begin{align*} 
%     &\frac{\partial \bigl(f(x, a) - R^*(x, a)\bigr) \cdot \pi_{f}^\eta(a|x)}{\partial f(x, a)} \\&= \pi_{f}^\eta(a|x) + \eta \bigl(f(x, a) - R^*(x, a)\bigr) \cdot \Big(\pi_f^\eta(a|x) - \pi_f^\eta(a|x)^2\Big) \ge 0,
%     %\pi_{f}^\eta(a|x) + \eta \bigl(f(x, a) - R^*(x, a)\bigr) \cdot \Bigl(\frac{\pi_{\reff}(a|x) \cdot \exp\bigl(\eta f(x, a)\bigr)}{Z_f^\eta(x)} - \frac{\bigl[\pi_{\reff}(a|x) \cdot \exp\bigl(\eta \cdot f(x, a)\bigr)\bigr]^2}{[Z_f^\eta(x)]^2}\Bigr) \ge 0.
% \end{align*}
% where we use \eqref{eqn:computation2} in the first equality and use $\pi_f^\eta(a|x) \leq 1$ in the inequality. 

We use the notation $\delta_t=\hat R_t + b_t - R^*$.
Let 
\[
U(\lambda) = \sum_{a\in\cA}\lambda\delta_{t-1}(x,a)^2 \cdot \frac{\pi_0(a|x)\cdot\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)}{\EE_{\pi_0}\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)},\quad \text{for}~\lambda\in[0,1].
\]
We take the derivative as follows:
\begin{align*}
    \frac{\partial U(\lambda)}{\partial \lambda} =& \sum_{a\in\cA} \Big[\delta_{t-1}(x,a)^2 \pi_{f_\lambda}^\eta + \lambda\delta_{t-1}(x,a)^2\cdot\Big(\frac{\pi_0(a|x)\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)\cdot\eta\delta_{t-1}(x,a)}{\EE_{\pi_0}\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)}\Big)\\
    &\qquad - \frac{\pi_0(a|x)\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)\cdot\EE_{\pi_0}\eta\delta_{t-1}(x,a) \cdot\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)}{\big(\EE_{\pi_0}\exp\big(\eta(\lambda\delta_{t-1}(x,a) + R^*(x,a))\big)\big)^2}\Big]\\
    =& \sum_{a\in\cA}\Big[\delta_{t-1}(x,a)^2 \pi_{f_\lambda}^\eta + \lambda\delta_{t-1}(x,a)^2 \pi_{f_\lambda}^\eta\cdot\eta\big(\delta_{t-1}(x,a) - \EE_{\pi_{f_\lambda}^\eta}\delta_{t-1}(x,a)\big)\Big]\\
    =& \EE_{\pi_{f_\lambda}^\eta}\big[\delta_{t-1}(x,a)^2\big] + \lambda\Big\{\EE_{\pi_{f_\lambda}^\eta}\big[\delta_{t-1}(x,a)^3\big] - \EE_{\pi_{f_\lambda}^\eta}\big[\delta_{t-1}(x,a)^2\big]\cdot \EE_{\pi_{f_\lambda}^\eta}\big[\delta_{t-1}(x,a)\big] \Big\}\\
    \ge& 0,
\end{align*}
where the inequality holds since for random variable $X \ge 0$ we have
\begin{align*}
    \EE[X^3] - \EE[X^2] \cdot \EE[X] &= \EE[(X^2 - \EE[X^2])(X - \EE[X])]
    \\&= \EE[\big(X^2 - (\EE[X])^2\big)(X - \EE[X])] + \EE[\big((\EE[X])^2 -\EE[X^2]\big)(X - \EE[X])]
    \\&= \EE[\big(X + \EE[X]\big)(X - \EE[X])^2] + 0
    \\&\ge 0.
\end{align*}
Therefore, since $f_\lambda(x,a)\le \hat R_{t - 1}(x, a) + b_{t - 1}(x, a)$ for any $(x,a)\in\cX\times\cA$, we can obtain that
\begin{align*}
    U(\lambda) \le U(1) = \sum_{a\in\cA}\delta_{t-1}(x,a)^2\cdot \pi_{\hat R_{t - 1} + b_{t - 1}}^\eta(a|x).
\end{align*}

We further have the following bound for RHS of \eqref{eq:mean-value}:

    \begin{align*} 
    J(\pi^*) - J(\pi_t) =& 
    \rE_{x \sim d_0}[\Delta(x, \hat R_{t - 1} + b_{t - 1}) - \Delta(x, R^*)]\\
    \le& \eta \rE_{x \sim d_0} \rE_{a\sim \pi_{\hat R_{t - 1} + b_{t - 1}}^\eta(\cdot|x)} \bigl[\bigl(\hat R_{t - 1}(x, a) + b_{t - 1}(x, a) - R^*(x, a)\bigr)^2 \bigr]. 
    \end{align*}
\end{proof}

\begin{lemma} \label{lem:computation1}
Let $Z_R(x) := \sum_{a \in \cA} \pi_{\reff}(a|x) \cdot \exp(\eta R(x, a))$ and $\pi_R^{\eta}(a|x)=\frac{\pi_{\reff}(a|x)\exp(\eta R(x,a))}{Z_R(x)}$. We have the following partial gradient results:
    \begin{equation} \label{eqn:computation1}
        \frac{\partial Z_R(x)}{\partial R(x,a)} = \eta \pi_{\reff}(a|x) \exp\big(\eta R(x,a)\big),
    \end{equation}
    \begin{equation}\label{eqn:computation2}
        \frac{\partial \pi^\eta_R(a|x)}{\partial R(x,a)} = \eta \pi_R^\eta(a|x) - \eta \pi_R^\eta(a|x)^2.
    \end{equation}
    For $a' \neq a$, we have
    \begin{equation}\label{eqn:computation3}
        \frac{\partial \pi^\eta_R(a'|x)}{\partial R(x,a)} = -\eta \pi_R^\eta(a|x) \cdot \pi_R^\eta(a'|x).
    \end{equation}
\end{lemma}
\begin{proof}
    The proof is deferred to Appendix \ref{s:Technical lemma proof}.
\end{proof}

\begin{lemma}\label{lm:Confidence Bound for Reward Function}
    Under Algorithm \ref{alg:kl-bandits} and Assumption \ref{as:Reward function realizability} and the condition that the noises $\epsilon_t$ are conditional $1$-sub-Gaussian, we have with probability at least $1-\delta$ for all $t\in[T]$, the uniform optimism event that $\cE_{\opt}^t=\{\hat{R}_t(x,a)+b_t(x,a)-R^*(x,a) \ge 0,~\forall(x,a)\in\cX\times\cA\}$ holds true.
\end{lemma}
\begin{proof}
By invoking Lemma \ref{lm:Generalization error of reward function} with $\cF=\cR$, $\cZ=\cX\times\cA$ and $\eta=1$,we have for all $t\in[T]$ with probability at least $1-\delta$,
    \begin{align}\label{eq:aaa}
        \sum_{i=1}^t(\hat{R}_t(x_i,a_i) - R^*(x_i,a_i))^2 \le 8\log(N_{\cR}T/\delta) = \frac{1}{2}\beta_T^2.
    \end{align}
Hence, we deduce that for any $(x,a)\in\cX\times\cA$,
\begin{align*}
    &R^*(x,a) - \hat{R}_t(x,a) \\&\le \sup_{R_1,R_2\in\cR}\frac{|R_1(x,a)-R_2(x,a)|}{\sqrt{\lambda+\sum_{i=1}^{t}(R_1(x_i,a_i)-R_2(x_i,a_i))^2}}\cdot\sqrt{\lambda+\sum_{i=1}^{t}(R^*(x_i,a_i) - \hat{R}_t(x_i,a_i))^2}\\
    &\le U_{\cR_{t}}(\lambda,x,a;\cD_{t})\cdot\sqrt{\lambda+\frac{1}{2}\beta_T^2}\\
    &\le U_{\cR_{t}}(\lambda,x,a;\cD_{t})\cdot\beta_T,
\end{align*}
where the second inequality follows from the definition of $U_{\cR}$ and \eqref{eq:aaa}, and the last inequality uses $\lambda\le\frac{1}{2}\beta_T^2$ and the choice of bonus $b_t$. Besides, since $R^*(x,a) - \hat{R}_t(x,a)\le 1$, we have
$$
R^*(x,a) - \hat{R}_t(x,a) \le b_t(x,a).
$$

Therefore, we get
$$
\hat{R}_t(x,a)+b_t(x,a)-R^*(x,a) \ge 0.
$$
\end{proof}

\begin{proof}[Proof of Theorem \ref{th:bandit}]
Let
$$
U_t := \sup_{R_1,R_2\in\cR_{t-1}}\frac{|R_1(x
_t,a_t)-R_2(x_t,a_t)|}{\sqrt{\lambda+\sum_{i=1}^{t-1}(R_1(x_i,a_i)-R_2(x_i,a_i))^2}}.
$$
We know from Lemma \ref{lm:Confidence Bound for Reward Function} that $\cup_{t\in[T]}\cE_{\opt}^t$ holds with probability at least $1-\delta$. Thus, conditioning on $\cup_{t\in[T]}\cE_{\opt}^t$, we invoke Lemma \ref{lm:regret_decompose_bandit} to get
\begin{align*}
    \mathrm{Regret}(T) &= \sum_{t=1}^T(J(\pi^*)-J(\pi_t))\\
    &\le \eta\sum_{t=1}^T\rE_{x_t \sim d_0} \rE_{a_t\sim \pi_t} \bigl[\bigl(\hat R_{t - 1}(x_t, a_t) + b_{t - 1}(x_t, a_t) - R^*(x_t, a_t)\bigr)^2 \bigr]\\
    &\le 4\eta\sum_{t=1}^T\rE_{x_t \sim d_0} \rE_{a_t\sim \pi_t} \bigl(b_{t - 1}(x_t, a_t)\bigr)^2\\
    &= 4\eta\beta_T^2 \sum_{t=1}^T\rE_{x_t \sim d_0} \rE_{a_t\sim \pi_t} \min\Big\{1, \big(U_{\cR_{t-1}}(\lambda,x_t,a_t;\cD_{t-1})\big)^2\Big\}\\
    &\le \cO\Big(\eta\log(N_{\cR}T/\delta)\cdot d(\cR,\lambda,T)\Big),
\end{align*}
where the second inequality uses the condition $\cE_{t-1}$, the second equality uses the definition of bonus $b_t$ in \eqref{eq:bonus}, and the last inequality follows from the value of $\beta_T$ and the eluder dimension in Definition \ref{def:eluder}.
\end{proof}



\section{Proofs for KL-Regularized RL}\label{sec:proof_mdp}
\subsection{Regret Decomposition}\label{ssec:regret_decomp}

\begin{lemma}\label{lm:value decompose_mdp_restate}
We use the short-hand notation $\E^{\hpi}$ for the expectation of trajectories generated by $d_0\times\hpi$, and 
$$
e_h(s_h,a_h) = \hQ_h(s_h,a_h)-\cT_\eta^h \hV_{h+1}(s_h,a_h) = \hQ_h(s_h,a_h) - \E_{r_h,s_{h+1}|s_h,a_h}[r_h + \hV_{h+1}(s_{h+1})].
$$
Conditioning on the uniform optimism event that $\cE_{\opt}=\{\hQ(s_h,a_h)-Q^*_h(s_h,a_h) \ge 0,~\forall(s_h,a_h)\in\cS\times\cA,~h\in[H]\}$ holds, we have
    $$
    J(\pi^*) - J(\hat \pi) \le \eta \EE^{\hpi} \sum_{h = 1}^H \Big[\Big(\sum_{j=h}^H e_j(s_j,a_j) \Big)\Big)^2\Big].
    $$
\end{lemma}
\begin{proof}
For each $h$, let $\hat \pi^{(h)} := \hat \pi_{1:h} \oplus \pi^*_{h+1:H}$ be the concatenated policy of $\hat \pi$ and $\pi^*$ at time step $h$. Then, $\pi^*$ can be denoted as $\hat\pi^{(0)}$ and $V_1^*=V_1^{\hat\pi^{(0)}}$. %In other words, we roll in by playing $\hat{\pi}$ to get $s_1 \sim d_0, a_1 \sim \hat{\pi}^1(\cdot|s_1), \cdots, a_{h-1} \sim \hat{\pi}^{h-1}(\cdot|s_{h-1}), s_h \sim \PP_{h-1}(\cdot|s_{h-1}, a_{h-1})$ but play $a_h \sim \pi^*_h(\cdot|s_h)$ at step $h$. 
Then the suboptimality gap for $\hat \pi$ can be decomposed as follows: 
\begin{align*} 
    J(\pi^*) - J(\hat \pi) = V_1^{\pi^*}(s_1) -  V_1^{\hat \pi}(s_1)= \sum_{h = 0}^{H - 1} \EE_{s_1 \sim d_0} \underbrace{\bigl[V_1^{\hat \pi^{(h)}}(s_1) - V_1^{\hat \pi^{(h + 1)}}(s_1)\bigr]}_{I_{h + 1}}.
\end{align*}

For each term $I_{h+1}$, we deduce that
\begin{align*}
    &\EE_{s_1 \sim d_0, s_{h + 1} \sim d_{h + 1}^{\hat\pi}} \bigl[V_{h + 1}^{\hat \pi^{(h)}}(s_{h + 1}) - V_{h + 1}^{\hat \pi^{(h + 1)}}(s_{h + 1})\bigr]\\
    &= \EE_{s_1 \sim d_0, s_{h + 1} \sim d_{h + 1}^{\hat\pi}}\Big\{ \EE_{\pi^*_{h+1}}\bigl[Q_{h + 1}^{\hat \pi^{(h)}}(s_{h + 1},a_{h+1}) - \eta^{-1}\KL(\pi_{h+1}^*(\cdot|s_{h+1})\| \pi_{\reff,h+1}(\cdot|s_{h+1}))\bigr]\\
    &\qquad - \EE_{\hpi_{h+1}}\bigl[Q_{h + 1}^{\hat \pi^{(h+1)}}(s_{h + 1},a_{h+1}) - \eta^{-1}\KL(\hpi_{h+1}(\cdot|s_{h+1})\| \pi_{\reff,h+1}(\cdot|s_{h+1}))\bigr]\Big\}\\
    &= \EE_{s_1 \sim d_0, s_{h + 1} \sim d_{h + 1}^{\hat\pi}}\Big\{ \EE_{\pi^*_{h+1}}\bigl[Q_{h + 1}^{\pi^*}(s_{h + 1},a_{h+1}) - \eta^{-1}\KL(\pi_{h+1}^*(\cdot|s_{h+1})\| \pi_{\reff,h+1}(\cdot|s_{h+1}))\bigr]\\
    &\qquad - \EE_{\hpi_{h+1}}\bigl[Q_{h + 1}^{\pi^*}(s_{h + 1},a_{h+1}) - \eta^{-1}\KL(\hpi_{h+1}(\cdot|s_{h+1})\| \pi_{\reff,h+1}(\cdot|s_{h+1}))\bigr]\Big\},
\end{align*}
where the second equality follows from the fact that $\hpi^{(h)}_l=\hpi^{(h+1)}_l=\pi_h^*$ for $l=h+2,\ldots,H$.

Then, we can follow the analysis of Lemma \ref{lm:regret_decompose_bandit} for each step $h+1$ with $d_0=d_{h + 1}^{\hat\pi}$, $R^*=Q_{h + 1}^{\pi^*}$, $\pi^*=\pi^*_{h+1}$ and $\hat\pi=\hpi_{h+1}$ to derive that
\begin{align*}
I_{h+1}
    \le \eta\EE^{\hpi}\big[\big(\hQ_{h + 1}(s_{h + 1},a_{h+1}) - Q_{h + 1}^*(s_{h + 1},a_{h+1})\big)^2\big].
\end{align*}
Further, conditioning on $\cE_{\opt}$, we deduce that
\begin{align*}
0 &\geq Q^*_{h+1}(s_{h+1},a_{h+1})-\hat{Q}_{h+1}(s_{h+1},a_{h+1})\\
&=
\EE_{s_{h+2}|s_{h+1},a_{h+1}} (V_{h+2}^*(s_{h+2})-\hat{V}_{h+2}(s_{h+2})) - e_{h+1}(s_{h+1},a_{h+1}) \\
&= 
\EE^{\hpi}_{\cdot|s_{h+1},a_{h+1}}
  \eta^{-1}\log \E_{a_{h+2}\sim\pi_{\reff,h+2}} e^{\eta(Q_{h+2}^*(s_{h+2},a_{h+2})-\hat{Q}_{h+2}(s_{h+2},a_{h+2}))}
- e_{h+1}(s_{h+1},a_{h+1})\\
&\geq  
\EE^{\hpi}_{\cdot|s_{h+1},a_{h+1}} \left[
  \big(Q_{h+2}^*(s_{h+2},a_{h+2})-\hat{Q}_{h+2}(s_{h+2},a_{h+2})\big)
-e(s_{h+1},a_{h+1}) \right] \\
&\geq  \cdots \\
&\geq  - \E^{\hpi}_{\cdot|s_{h+1},a_{h+1}} \sum_{j\geq h+1}
e_j(s_j,a_j),
\end{align*}
where second inequality holds due to the Jensen's inequality. Taking this back to $I_{h+1}$ leads to 
\begin{align*}
I_{h+1}
    &\le \eta\EE^{\hpi}\big[\big(\E^{\hpi}_{\cdot|s_{h+1},a_{h+1}} \sum_{j\geq h+1}
e_j(s_j,a_j)\big)^2\big]\\
    &\le \eta\EE^{\hpi}_{\cdot|s_{h+1},a_{h+1}} \Big[\Big(\sum_{j=h+1}^H e_j(s_j,a_j) \Big)^2\Big],
\end{align*}
where the second inequality uses the Jensen's inequality.

Therefore, we get
\begin{align*}
    J(\pi^*) - J(\hat \pi) &\le \eta \sum_{h = 0}^{H - 1}\EE^{\hpi} \Big[\Big(\sum_{j=h+1}^H e_j(s_j,a_j) \Big)^2\Big].
\end{align*}
\end{proof}


\begin{lemma}[Confidence Sets for Value Function]\label{lm:Confidence Sets_mdp}
    Let $\hat f^t_h$ be the estimator as defined in Algorithm~\ref{alg:kl-mdp} and
    $$
    \beta_h^T = 4\sqrt{\log(4N_h T H / \delta)},
    $$ 
    where we define $N_h=N_{\cF_h}\cdot N_{\cF_{h+1}}\cdot N_{\cB_{h+1}(\lambda)}$.
    Then, with probability at least $1 - \delta$, we have for all $t\in[T]$ and $h\in[H]$,
    \begin{align*} 
        &| \hat f^t_h(s, a) - \cT_\eta^h \hV^t_{h+1}(s, a) | \le \beta^T_h\cdot U^h_{\cF_h}(s, a; \cD^h_{t-1}) \quad \forall (s, a) \in \cS \times \cA,\\
        &\hQ^t_h(s_h,a_h)-Q^*_h(s_h,a_h) \ge 0,
    \end{align*}
    where $\cD^h_{t-1}$ denotes the history data $\{x_h^i,a_h^i\}_{i\in[t-1]}$.
\end{lemma}

\begin{proof}
For $k \in [K]$, $h \in [H]$, let $\cS^t_h$ be the $\sigma$-algebra generated by the random variables representing the state-action pairs up to and including those that appear stage $h$ of episode $k$. That is, $\cS^t_h$ is generated by
\begin{align*}
s_1^1,a_1^1, \dots, s_h^1,a_h^1, &\dots, s_H^1,a_H^1\,, \\
s_1^2,a_1^2, \dots, s_h^2,a_h^2, &\dots, s_H^2,a_H^2\,, \\
\vdots \\
s_1^t,a_1^t,\dots, s_h^t,a_h^t & \,.
\end{align*}

    Let $\cQ_{h + 1}, V_{h + 1}$ be the set of possible value functions, i.e. \begin{align*}
        &\cQ_{h + 1} = \big\{g: \cS \times \cA \to \RR | \exists f_{h + 1} \in \cF_{h + 1}, b_{h + 1} \in \cB_{h + 1}\ \mathrm{s.t.}\  g = f_{h + 1} + b_{h + 1}\big\}, \\
        &\cV_{h + 1} = \big\{g: \cS \to \RR | \exists Q_{h + 1} \in \cQ_{h + 1} \ \mathrm{s.t.}\ g(\cdot) = \max_{a \in \cA} Q_{h + 1}(\cdot, a)\big\}.
    \end{align*}
    The proof is based on the standard concentration inequality for the least square estimator \citep{zhang2023mathematical}.

    For simplicity, we denote by $e_h^t$ the noise term in the least square estimator with respect to an arbitrary value function $V_{h + 1} \in \cV_{h + 1}$, i.e., \begin{align*} 
        e_h^t := r_h^t + V_{h+1}(s_{h + 1}^t) - \cT_\eta^h V_{h+1}(s_h^t, a_h^t).
    \end{align*}

    By definition, \begin{align*}
        \EE[e_h^t | \cS^t_h] = 0, \quad| e_h^t | \le 1.
    \end{align*}

    Hence, by invoking Lemma \ref{lm:Generalization error of reward function} and using a union bound over $\cV_{h + 1}$, we have for all $t\in[T]$ and $h\in[H]$, with probability at least $1 - \delta$,
    \begin{align} \label{eq:aac}
        \sum_{i = 1}^{t - 1} \big[\hat f^t_h (s_h^i, a_h^i) - \cT_\eta^h \hV^t_{h+1}(s_h^i, a_h^i)\big]^2 \le 8 \log(4N_h t H / \delta) \le \frac{1}{2}(\beta^T_h)^2,
    \end{align} where the first inequality follows from the definition of $N_h$ and $|\cV_{h + 1}| \le N_{\cF_{h+1}} \cdot N_{\cB_{h + 1}}$.
    Then we can complete the proof using union bound over all $t \ge 1$.

    Therefore, for any $(s,a)\in\cS\times\cA$, we have
    \begin{align}\label{eq:aaz}
        | \hat f^t_h(s, a) - \cT_\eta^h \hV^t_{h+1}(s, a) | &\le U^h_{\cF_h}(s, a; \cD^h_{t-1}) \cdot \sqrt{\lambda + \sum_{i = 1}^{t - 1} \big[\hat f^t_h (s_h^i, a_h^i) - \cT_\eta^h \hV^t_{h+1}(s_h^i, a_h^i)\big]^2}\notag\\
        &\le (\beta_h^T)^2\cdot U^h_{\cF_h}(s, a; \cD^h_{t-1}),
    \end{align}
    where the first inequality holds by using the completeness of the Bellman operator $\cT_\eta^h \hV^t_{h+1}\in\cF_h$ and the definition of uncertainty $U^h_{\cF_h}$, and the last inequality applies \eqref{eq:aac} and $\lambda\le \frac{1}{2}(\beta^T_h)^2$.

Moreover, we will prove $\hQ^t_h(s_h,a_h)-Q^*_h(s_h,a_h) \ge 0,\forall~h\in[H]$ by induction. At step $H+1$, we know that $Q^*_h=\hQ\equiv0$. Assume that at step $h+1$, we have
$$
\hQ^t_{h+1}(s_{h+1},a_{h+1})-Q^*_{h+1}(s_{h+1},a_{h+1}) \ge 0.
$$
Then, at step $h$, we obtain that
\begin{align*}
    &\hQ^t_h(s_h,a_h)-Q^*_h(s_h,a_h) \\&= \hf^t_h(s_h,a_h) + b_h^t(s_h,a_h) - \cT_\eta^h V_{h+1}^*(s_h,a_h)\\
    &\ge \cT_\eta^h\hV_{h+1}^t(s_h,a_h) - (\beta_h^T)^2\cdot U^h_{\cF_h}(s, a; \cD^h_{t-1}) + (\beta_h^T)^2\cdot U^h_{\cF_h}(s, a; \cD^h_{t-1}) - \cT_\eta^h V_{h+1}^*(s_h,a_h)\\
    &\ge \frac{1}{\eta}\E_{s_{h+1}|s_h,a_h} \Big[\log\E_{a_{h+1}\sim \pi_{\reff,h+1}(\cdot|s_{h+1})} \exp\big(\eta(\hQ_{h+1}^t(s_{h+1},a_{h+1}) - Q_{h+1}^*(s_{h+1},a_{h+1}))\big)\Big]\\
    &\ge 0,
\end{align*}
where the first inequality uses \eqref{eq:aaz} and the definition of bonus $b_h^t$ in \eqref{eq:bonus}, the second inequality follows from the derivation of the value function in \eqref{eq:V_h}, and the last inequality is due to the induction hypothesis at step $h+1$.
\end{proof}

\begin{proof}[Proof of Theorem \ref{th:mdp}]
From Lemma \ref{lm:Confidence Sets_mdp} we know that $\cE_{\opt}$ holds at all time step $t\in[T]$ with probability a least $1-\delta$. Hence, by applying Lemma \ref{lm:value decompose_mdp} for each $t\in[T]$, we have
\begin{align*}
     J(\pi^*) - J(\hat \pi^t) &\le \eta \EE^{\pi^t} \sum_{h = 1}^H \Big[\Big(\sum_{j=h}^H (\hQ^t_j(s_j,a_j)-\cT_\eta^j \hV^t_{j+1}(s_j,a_j)) \Big)\Big)^2\Big]\\
    &\le \eta H \EE^{\pi^t} \sum_{h = 1}^H \sum_{j=h}^H \Big[\Big(\hQ^t_j(s_j,a_j)-\cT_\eta^j \hV^t_{j+1}(s_j,a_j)\Big)^2\Big]\\
    &\le \eta H^2 \EE^{\pi^t} \sum_{h = 1}^H \Big[\Big(\hQ^t_h(s_h,a_h)-\cT_\eta^h \hV^t_{h+1}(s_h,a_h)\Big)^2\Big],
\end{align*}
where the second inequality applies the Jensen's inequality.

Then, by Lemma \ref{lm:Confidence Sets_mdp}, with probability at least $1 - \delta$, we have for all $t\in[T]$ and $h\in[H]$,
\begin{align*}
    \hQ^t_h(s_h,a_h)-\cT_\eta^h \hV^t_{h+1}(s_h,a_h) &= b^t_h(s_h,a_h) + \hat f^t_h(s_h, a_h) - \cT_\eta^h \hV^t_{h+1}(s_h, a_h)\\
    &\le b^t_h(s_h,a_h) + \beta^T_h\cdot U^h_{\cF_h}(s_h, a_h; \cD^h_{t-1})\\
    &= 2\beta^T_h\cdot U^h_{\cF_h}(s_h, a_h; \cD^h_{t-1}),
\end{align*}
where the last equality uses the definition of bonus $b^t_h$.

Therefore, we have
    \begin{align*}
        \mathrm{Regret}(T) &= \sum_{t = 1}^T \bigl[J(\pi^*) - J(\hat \pi^t)\bigr]
        \\&\le 4 \eta H^2 \sum_{t = 1}^T \sum_{h = 1}^H (\beta^T_h)^2 \EE_{s_1 \sim d_0, \hat \pi}[(U^h_{\cF_h}(s_h, a_h; \cD^h_{t-1}))^2]
        \\&= \cO\Big(\eta H^2 d(\cF,\lambda,T) \cdot O(\log(N_{\cF} TH /\delta))\Big),
    \end{align*}
    where $d(\cF,\lambda,T)=\sum_{h=1}^Hd(\cF_h,\lambda,T)$.
\end{proof}



\section{Proof of Technical Lemmas}\label{s:Technical lemma proof}
\begin{proof}[Proof of Lemma~\ref{lem:computation1}]
    For the first equation, we notice that 
    $$
\begin{aligned}
        \frac{\partial Z_R(x)}{\partial R(x,a)} &= \frac{\partial \sum_{a' \in \cA} \pi_{\reff}(a'|x) \exp\big(\eta R(x,a')\big)}{\partial R(x,a)}, \\
        &= \frac{\partial \pi_{\reff}(a|x) \exp\big(\eta R(x,a)\big)}{\partial R(x,a)},\\
        &= \eta \pi_{\reff}(a|x) \exp\big(\eta R(x,a)\big).
\end{aligned}
    $$
For the second equation, with the derivation rule of $(\frac{f(z)}{g(z)})' = \frac{f'(z)g(z) - f(z)g'(z)}{(g(z))^2}$, we have
$$
\begin{aligned}
     \frac{\partial \pi^\eta_R(a|x)}{\partial R(x,a)} &= \frac{\partial \big[\pi_{\reff}(a|x) \exp\big(\eta R(x,a)\big) / Z_R(x)\big]}{\partial R(x,a)}\\
     &= \frac{\eta \pi_{\reff}(a|x) \exp\big(\eta R(x,a)\big) Z_R(x) - \partial Z_R(x) \cdot \pi_{\reff}(a|x) \exp\big(\eta R(x,a)\big)}{Z_R(x)^2}\\
     &= \eta \pi_R^\eta(a|x) - \eta \pi_R^\eta(a|x)^2,
\end{aligned}
$$
where we use the first equation in the last step. Finally, for the last equation, we have
$$
\begin{aligned}
     \frac{\partial \pi^\eta_R(a'|x)}{\partial R(x,a)} &= \frac{\partial \big[\pi_{\reff}(a'|x) \exp\big(\eta R(x,a')\big) / Z_R(x)\big]}{\partial R(x,a)} \\
     &= -\frac{\pi_{\reff}(a'|x) \exp\big(\eta R(x,a')\big) \cdot \partial Z_R(x)}{Z_R(x)^2}\\
     &= -\eta \pi_R^\eta(a|x) \cdot \pi_R^\eta(a'|x).
\end{aligned}
$$
\end{proof}



We follow Theorem 13.15 from \citet{zhang2023mathematical} and \citet{ye2023corruption} to derive the in-sample error for the function.
\begin{lemma}\label{lm:Generalization error of reward function}
    Consider a function space $\cF:\cZ\rightarrow\RR$ and a filtered sequence $\{z_t,\epsilon_t\}\in\cZ\times\RR$ so that $\epsilon_t$ is conditional zero-mean $\eta$-sub-Gaussian noise. Suppose that $\cF$ is a finite space with cardinality $N_{\cF}$ For $f_*(\cdot):\cZ\rightarrow\RR$, suppose that $y_t=f_*(x_t,a_t)+\epsilon_t$. If $\hat{f}_t$ is an ERM solution:
$$
\hat{f}_t = \argmin_{f\in\cF}\sum_{i=1}^t(f(z_i)-y_i)^2,
$$
with probability at least $1-\delta$, we have for all $t\in[T]$,
\begin{align*}
    \sum_{i=1}^t(\hat{f}_t(z_i)-f_*(z_i)^2 \le& 8\eta^2\log(N_{\cF}T/\delta) .
\end{align*}
\end{lemma}
\begin{proof}
For $f\in\cF$, define
\begin{align*}
    \phi(f,z_t)&= -a\left[ (f(z_t)-y_t)^2-(f_*(z_t)-y_t)^2\right]\\
    &= -a\left[ (f(z_t)-f_*(z_t)-\epsilon_t)^2-\epsilon_t^2\right]\\
    &= 2a(f(z_t)-f_*(z_t))\cdot\epsilon_t -a(f(z_t)-f_*(z_t))^2,
\end{align*}
where $a=\eta^{-2}/4$. Since $\epsilon_t$ is $\eta$-sub-Gasussian conditional on $z_t,\cS_{t-1}:=\{z_i,\epsilon_i\}_{i\in[t-1]}$, we get
\begin{align*}
    &\log\E_{y_t}\big[\exp\big(\phi(f,z_t)+a(f(z_t)-f_*(z_t))^2\big)\big]\\
    &\le \frac{4a^2(f(z_t)-f_*(z_t))^2\eta^2}{2} = \frac{(f(z_t)-f_*(z_t))^2}{8\eta^2},
\end{align*}
which means that
\begin{align*}
    \log\E_{y_t}\big[\exp\big(\phi(f,z_t)\big)\big] \le \frac{(f(z_t)-f_*(z_t))^2}{8\eta^2} - \frac{(f(z_t)-f_*(z_i))^2}{4\eta^2} = -\frac{(f(z_t)-f_*(z_t))^2}{8\eta^2}.
\end{align*}
Then, by Lemma \ref{lm:th13_9} with $\lambda=1$, we have for all $f\in\cF$ and $t\in[T]$, with probability at least $1-\delta$,
\begin{align*}
    \sum_{i=1}^t\phi(f,z_i) \le& \log\E_{y_t}\big[\exp\big(\phi(f,z_t)\big)\big]  + \log(N_{\cF}T/\delta)\\
    \le& -\frac{(f(z_t)-f_*(z_t))^2}{8\eta^2} + \log(N_{\cF}T/\delta).
\end{align*}
Additionally, since $\hf_t$ is the ERM solution, we have
\begin{align*}
    \sum_{i=1}^t\phi(\hf_t,z_i) = a\Big[\sum_{i=1}^t(f_*(z_i)-y_i)^2 - \sum_{i=1}^t(\hf_t(z_i)-y_i)^2\Big] \ge 0.
\end{align*}
Combining the results above leads to
\begin{align*}
    \sum_{i=1}^t(\hat{f}_t(z_i)-f_*(z_i)^2 \le& 8\eta^2\log(N_{\cF}T/\delta).
\end{align*}
\end{proof}

\section{Auxliary Lemmas}

\begin{lemma}[\citealt{russo2013eluder}]\label{lm:th13_9}
Consider a sequence of random variables $\{Z_t\}_{t\in\mathbb N}$ adapted to the filtration $\{\cS_t\}_{t\in\mathbb N}$ and a function $f\in\cF$. For any $\lambda>0$, with probability at least $1-\delta$, for all $t\ge1$, we have
$$
-\sum_{s=1}^tf(Z_s)-\frac{1}{\lambda}\sum_{s=1}^t\log\E[e^{-\lambda f(Z_s)}|\cS_{s-1}]\le \frac{\log(1/\delta)}{\lambda} 
$$
\end{lemma}
\begin{proof}
For a detailed proof, see Theorem 13.2 of \citet{zhang2023mathematical} or Lemma 34 of \citet{foster2023foundations}.
\end{proof}

\begin{lemma}[Online-to-batch conversion] \label{lem:online2batch}
If an algorithm has a sublinear regret of $c^\dagger \cdot \log T $, then the algorithm finds an $\epsilon$-optimal policy with at most $\widetilde\Theta\big(c^\dagger / \epsilon\big)$ samples, where $\widetilde\Theta$ omit logarithmic terms of $c^\dagger/\epsilon$. Here $c^\dagger$ is a problem-dependent constant. 
\end{lemma}
\begin{proof}
        We denote the policy sequence as $\{\pi^1,\cdots,\pi^T\}$. Then, by definition of regret, we know 
    $$
    \begin{aligned}
        \mathrm{Regret}(T) &= T \cdot V_1^*(x_1) - \sum_{t=1}^T V_1^{\pi^t}(x_1)\\
        &\leq c^\dagger \cdot \log T.
    \end{aligned}
    $$
    We consider the uniform policy $\tilde{\pi} := \mathrm{Uniform}(\pi^1, \cdots, \pi^T)$. It follows that 
$$
V^*_1(x_1) - V^{\tilde{\pi}}_1(x_1) = V^*_1(x_1) - \frac{1}{T} \sum_{t=1}^T V_1^{\pi^t}(x_1) \leq c^\dagger \cdot \frac{\log T}{T}.
$$
It suffices to prove that
$$
c^\dagger \cdot \frac{\log T}{T} \le \epsilon,
$$
which is equivalent to solving
$$
T \le \exp(T\cdot \epsilon/c^\dagger).
$$
By using the Lambert W function\footnote{\url{https://en.wikipedia.org/wiki/Lambert_W_function}}, we can prove that
$$
T \ge \frac{W(1)c^\dagger}{\epsilon},
$$
where $W(1)\ge \log (1/\epsilon) - \log\log (1/\epsilon)$.

\end{proof}

\end{document}