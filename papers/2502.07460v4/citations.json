[
  {
    "index": 0,
    "papers": [
      {
        "key": "OpenAI2023GPT4TR",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "choshen2019weaknesses",
        "author": "Choshen, Leshem and Fox, Lior and Aizenbud, Zohar and Abend, Omri",
        "title": "On the weaknesses of reinforcement learning for neural machine translation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "choshen2019weaknesses",
        "author": "Choshen, Leshem and Fox, Lior and Aizenbud, Zohar and Abend, Omri",
        "title": "On the weaknesses of reinforcement learning for neural machine translation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2023remax",
        "author": "Li, Ziniu and Xu, Tian and Zhang, Yushun and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      },
      {
        "key": "shao2024deepseekmath",
        "author": "Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Zhang, Mingchuan and Li, YK and Wu, Y and Guo, Daya",
        "title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      },
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "azar2023general",
        "author": "Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\\'e}mi",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tunstall2023zephyr",
        "author": "Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\\'e}mentine and Habib, Nathan and others",
        "title": "Zephyr: Direct distillation of lm alignment"
      },
      {
        "key": "lambert2024t",
        "author": "Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others",
        "title": "T$\\backslash$\" ULU 3: Pushing Frontiers in Open Language Model Post-Training"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "dong2023raft",
        "author": "Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong",
        "title": "Raft: Reward ranked finetuning for generative foundation model alignment"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gui2024bonbon",
        "author": "Gui, Lin and G{\\^a}rbacea, Cristina and Veitch, Victor",
        "title": "BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling"
      },
      {
        "key": "yang2024asymptotics",
        "author": "Yang, Joy Qiping and Salamatian, Salman and Sun, Ziteng and Suresh, Ananda Theertha and Beirami, Ahmad",
        "title": "Asymptotics of language model alignment"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yue2012k",
        "author": "Yue, Yisong and Broder, Josef and Kleinberg, Robert and Joachims, Thorsten",
        "title": "The k-armed dueling bandits problem"
      },
      {
        "key": "saha2021optimal",
        "author": "Saha, Aadirupa",
        "title": "Optimal algorithms for stochastic contextual preference bandits"
      },
      {
        "key": "bengs2021preference",
        "author": "Bengs, Viktor and Busa-Fekete, R{\\'o}bert and El Mesaoudi-Paul, Adil and H{\\\"u}llermeier, Eyke",
        "title": "Preference-based online learning with dueling bandits: A survey"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xu2020preference",
        "author": "Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur",
        "title": "Preference-based reinforcement learning with finite-time guarantees"
      },
      {
        "key": "novoseller2020dueling",
        "author": "Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel",
        "title": "Dueling posterior sampling for preference-based reinforcement learning"
      },
      {
        "key": "pacchiano2021dueling",
        "author": "Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan",
        "title": "Dueling rl: reinforcement learning with trajectory preferences"
      },
      {
        "key": "chen2022human",
        "author": "Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei",
        "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhan2023query",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Sun, Wen and Lee, Jason D",
        "title": "How to Query Human Feedback Efficiently in RL?"
      },
      {
        "key": "wu2023making",
        "author": "Wu, Runzhe and Sun, Wen",
        "title": "Making RL with Preference-based Feedback Efficient via Randomization"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhu2023principled",
        "author": "Zhu, Banghua and Jiao, Jiantao and Jordan, Michael I",
        "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons"
      },
      {
        "key": "zhan2023provable",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen",
        "title": "Provable Offline Reinforcement Learning with Human Feedback"
      },
      {
        "key": "li2023reinforcement",
        "author": "Li, Zihao and Yang, Zhuoran and Wang, Mengdi",
        "title": "Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism"
      },
      {
        "key": "zhong2024dpo",
        "author": "Zhong, Han and Feng, Guhao and Xiong, Wei and Zhao, Li and He, Di and Bian, Jiang and Wang, Liwei",
        "title": "Dpo meets ppo: Reinforced token optimization for rlhf"
      },
      {
        "key": "huang2024correcting",
        "author": "Huang, Audrey and Zhan, Wenhao and Xie, Tengyang and Lee, Jason D and Sun, Wen and Krishnamurthy, Akshay and Foster, Dylan J",
        "title": "Correcting the mythos of kl-regularization: Direct alignment without overparameterization via chi-squared preference optimization"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "xie2024exploratory",
        "author": "Xie, Tengyang and Foster, Dylan J and Krishnamurthy, Akshay and Rosset, Corby and Awadallah, Ahmed and Rakhlin, Alexander",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "key": "liu2024provably",
        "author": "Liu, Zhihan and Lu, Miao and Zhang, Shenao and Liu, Boyi and Guo, Hongyi and Yang, Yingxiang and Blanchet, Jose and Wang, Zhaoran",
        "title": "Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer"
      },
      {
        "key": "cen2024value",
        "author": "Cen, Shicong and Mei, Jincheng and Goshvadi, Katayoon and Dai, Hanjun and Yang, Tong and Yang, Sherry and Schuurmans, Dale and Chi, Yuejie and Dai, Bo",
        "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhang2022feel",
        "author": "Zhang, Tong",
        "title": "Feel-good thompson sampling for contextual bandits and reinforcement learning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "wang2023rlhf",
        "author": "Wang, Yuanhao and Liu, Qinghua and Jin, Chi",
        "title": "Is RLHF More Difficult than Standard RL?"
      },
      {
        "key": "ye2024theoretical",
        "author": "Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong",
        "title": "A theoretical analysis of nash learning from human feedback under general kl-regularized preference"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zhao2024sharp",
        "author": "Zhao, Heyang and Ye, Chenlu and Gu, Quanquan and Zhang, Tong",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      }
    ]
  }
]