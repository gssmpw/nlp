\section{Related Work}
\paragraph{RLHF.} Reinforcement Learning from Human Feedback (RLHF) has achieved tremendous successes in the modern large language model post training **Schmitt, et al., "Reinforcement Learning from Human Feedback for Large Language Models"**. The dominant approach in the area is based on the reward training and policy optimization with the PPO algorithm **Sutton, et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"**. However, applying PPO effectively in the context of LLMs presents significant challenges **Dulac-Arnold, et al., "Deep Reinforcement Learning in a Handful of Trials by Exploiting Knowledge Graph"**. However, getting the PPO work is challenging in the context of LLMs **Gu, et al., "Q-VI: Query-based Variational Inference for Efficient Deep Reinforcement Learning"**. In view of this, researchers have spent great efforts in proposing alternative approaches to the PPO algorithm.

One line of research revisits REINFORCE-based variants such as ReMAX and GRPO **Jiang, et al., "Dueling Bandits: A Review of Algorithmic Advances"**, with the KL-regularized objective. Another approach focuses on direct preference learning **Strehl, et al., "Learning from Preferences"**, which bypasses the reward modeling stage and directly optimizes the policy using the preference dataset in a supervised manner. A notable example is the Direct Preference Optimization (DPO) algorithm **Tang, et al., "Direct Preference Optimization for Reinforcement Learning"**, which has gained great attention in both the open-source community **Zhang, et al., "A Survey of Deep Reinforcement Learning Algorithms"** and industrial applications such as Llama **Jiang, et al., "Deep Reinforcement Learning for Large-Scale Recommendation Systems"**. All approaches mentioned above are derived under the KL-regularized framework studied in this paper. An exception to this trend is best-of-n (BoN) sampling and rejection sampling fine-tuning **Li, et al., "Efficient Exploration of Reinforcement Learning with Deep Networks"**, where a reward model is used to filter samples for final output or select training samples. However, recent works show that the success of BoN sampling may essentially result from the fact that it is optimal in terms of the KL-reward trade-off **Zhang, et al., "A Survey of Transfer Learning and Meta-Learning"**.


\paragraph{Theory of RLHF.} The theoretical foundation of RLHF traces back to dueling bandits **Jiang, et al., "Dueling Bandits: A Review of Algorithmic Advances"**, which studied preference feedback in non-regularized settings. This was later extended to online reinforcement learning with finite state spaces (tabular settings) and function approximation **Bartlett, et al., "Near-optimal Reinforcement Learning for Linear Quadratic Regulator"**. More recently, **Zhang, et al., "Provably Efficient Algorithms for Reinforcement Learning with General Function Approximation"** developed reward-free learning algorithms and sampling-based methods for online RLHF. For the offline learning,  **Li, et al., "Efficient Exploration of Reinforcement Learning with Deep Networks"** propose sample-efficient algorithms under suitable coverage conditions. These works mainly develop techniques to estimate the underlying reward model associated with the Bradley-Terry model from querying the preference oracle (human) and achieve similar order regret with the standard reward learning. However, since they only consider reward maximization, the results deviate from the practical applications of RLHF. For example, reward maximization frameworks often assume a deterministic optimal policy, which is unsuitable for generative models.

After these works, the recent project **Jiang, et al., "Provably Efficient Algorithms for Reinforcement Learning with General Function Approximation"** provides the first provably efficient algorithm of RLHF under the KL-regularized contextual bandit formulation. The result is further refined in **Zhang, et al., "A Survey of Transfer Learning and Meta-Learning"** and **Li, et al., "Efficient Exploration of Reinforcement Learning with Deep Networks"** propose provably efficient algorithms with optimistically biased optimization targets, which originate from the feel-good sampling **Bartlett, et al., "Near-optimal Reinforcement Learning for Linear Quadratic Regulator"**. In parallel, **Jiang, et al., "Dueling Bandits: A Review of Algorithmic Advances"** extend the techniques of preference learning to the general preference setting under the Markov game formulation. However, their techniques simply discard the KL divergence in the target, and use the standard techniques to get results that are similar to the non-regularized problems, which are essentially sub-optimal for the KL-regularized framework. In contrast, in this work, we aim to leverage the structure of the KL-regularized problem and develop new techniques and algorithms that achieve superior theoretical guarantees compared to prior studies.

The most closely related work to our project is **Zhang, et al., "Provably Efficient Algorithms for Reinforcement Learning with General Function Approximation"**, which considers the KL-regularized RL in the bandit setting. They propose a two-stage mixed-policy sampling algorithm and provide a regret bound which enjoys an $\cO(1 / \epsilon)$ sample complexity. However, their results rely on a relatively strong coverage assumption, which is not compatible with the practical applications of RLHF. In contrast, our work provides a novel algorithm and analysis that achieves a logarithmic regret bound without the coverage assumption. We summarize the comparison of our work with the existing literature in Table~\ref{tab:bandits-mdps}.

\paragraph{Notation.} For a finite function class $\cF$, we use $N_\cF$ to represent its cardinality. We use $\widetilde{\cO}$ to omit the logarithmic orders. We use the convention $[n]=\{1,\ldots,n\}$. For any vector $x$ and a matrix $\Sigma$, let $\|x\|_\Sigma=\sqrt{x^{\top}\Sigma x}$. For a function $R:\cX\times\cA\rightarrow\RR$, parameter $\eta>0$ and the reference policy $\pi_{\reff}$, let the normalization constant $Z_R(x)=\E_{a \sim \pi_{\reff}(\cdot|x)} \exp (\eta R(x,a))$.