\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/RLVR-MedQA.pdf}
    \caption{An Overview of \ours (See \Cref{sec:method} for the details).}
    \label{fig:RLVR-MedQA}
\end{figure}

\newpage

\begin{figure}[!ht]
    \centering
    \includegraphics[width=.95\linewidth]{figures/reasoning-pattern-shift.pdf}
    \caption{The training dynamics of \ours (See \Cref{sec:pattern-shifts} for the details).}
    \label{fig:pattern-shift}
\end{figure}

\section{Introduction}
Recent work on reinforcement learning from verifiable rewards~\citep{lambert2024t} has demonstrated promising results, particularly highlighted by DeepSeek-R1~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} where they show that reasoning can emerge from performing RLVF alone from a base model.
% It reinvigorated interest in RLVR while simultaneously calling to mind earlier work in semantic parsing \citep{zettlemoyer-collins-2007-online}, which illustrated how structured reasoning approaches have evolved over time. 
Subsequent efforts -- such as studies in~\citet{yeo2025demystifying, zeng2025simplerl,HuggingFaceOpenR1, open_r1_multimodal} along with others focusing on synthetic datasets (e.g., countdown~\citep{tinyzero} and counting objects~\citep{chen2025r1v}) -- have attempted to replicate and extend the initial findings on the promising direction of RLVR, underscoring both the interest in and challenges of this approach.

Despite these advances, the application of RLVR has predominantly been focused on domains such as mathematics~\citep{lightman2023lets} and coding~\citep{jain2024livecodebench}, leaving open the question of how to extend its benefits to areas without such data. One promising data source for extending RLVR beyond Math/coding is multiple-choice question answer pairs (MCQA) which provides abundant verifiable labels across many domains including medicine. However, there are essential differences between MCQA and math/coding tasks: the answer space of the latter tends to be large and open-ended whereas MCQA features a much smaller answer space. It is unclear whether the benefits observed in math and coding will translate to MCQA. This gap is particularly pronounced in the medical domain, where MCQA tasks—such as those found in the MedQA~\citep{jin2021disease} dataset—require sophisticated reasoning with clinical knowledge and have consistently presented significant challenges~\citep{nori2023can}.

To bridge this gap, we propose \ours (\Cref{fig:RLVR-MedQA}), an initial exploration of RLVR leveraging MCQA data to elicit medical reasoning from a small base model without explicit reasoning supervision. Our findings are below:

\begin{enumerate}[leftmargin=25pt]
\item RLVR is effective not just for math and coding, but also for multiple-choice medical questions.

\item \ours achieves comparable performance to traditional supervised finetuning (SFT) in in-distribution settings while demonstrating superior generalization to out-of-distribution scenarios with approximately {\bf 8 percentage points improvement in accuracy}. 

\item We analyzed the training dynamics of \ours and observed the emergence of reasoning from the small 3B base model without any supervised reasoning data.
\end{enumerate}

% (3). We also 

% (1) Is RLVR beneficial for MCQA in the context of medicine?

% (2) Can RLVR generalize across different MCQA datasets in medicine?

% (3) How does its performance compare with that of traditional supervised fine-tuning (SFT) methods?
