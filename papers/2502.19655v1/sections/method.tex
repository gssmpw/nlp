
\section{\ours}
\label{sec:method}
% \todo{TODO: Add more context for reinforcement learning}
Reinforcement Learning (RL) is a machine learning algorithm that focuses on learning a policy for an agent to take actions in an environment to maximize cumulative rewards. At each step, the agent observes a state (S), selects an action (A) based on a policy ($\pi$), and receives a reward (R) while transitioning to a new state. RL has been affectively applied to language model post-training for aligning to human preference~\citep{ouyang2022training} and more recently to elicit reasoning without any supervisions~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability} for math and coding tasks. 

We adopt Proximal policy optimization (PPO)~\citep{schulman2017proximal} as our RL algorithm. PPO works by optimizing a policy using multiple epochs of stochastic gradient ascent on minibatch updates, while ensuring updates do not deviate too much from the previous policy. It does this by using a clipped surrogate objective function, which prevents excessively large policy updates, improving training stability:

% $$
% \footnotesize\mathcal{J}_{PPO}(\theta) = \mathbb{E}{[q \sim P(Q), o \sim \pi_{\theta_{old}}(O|q)]} \frac{1}{|O|} \sum_{t=1}^{|O|} \min \left[ \frac{\pi_\theta(o_{t} | q, o_{<t})}{\pi_{\theta_{old}}(o_{t} | q, o_{<t})} A_{t}, \text{clip} \left( \frac{\pi_\theta(o_{t} | q, o_{<t})}{\pi_{\theta_{old}}(o_{t} | q, o_{<t})}, 1 - \epsilon, 1 + \epsilon \right)  A_{t} \right]
% $$

\begin{align}
\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P(Q),\; o \sim \pi_{\theta_{old}}(O|q)} \frac{1}{|O|} \sum_{t=1}^{|O|} & \min \Biggl[ \frac{\pi_\theta(o_{t} | q, o_{<t})}{\pi_{\theta_{old}}(o_{t} | q, o_{<t})} A_{t}, \\
& \quad \text{clip}\Bigl( \frac{\pi_\theta(o_{t} | q, o_{<t})}{\pi_{\theta_{old}}(o_{t} | q, o_{<t})}, 1 - \epsilon, 1 + \epsilon \Bigr)  A_{t} \Biggr]
\end{align}


where  $\pi_{\theta}$ and $\pi_{\theta_{old}}$ are the current and old policy models, and $q, o$ are questions and outputs sampled from the question dataset and the old policy $\pi_{\theta_{old}}$, respectively. $\epsilon$ is a clipping-related hyper-parameter introduced in PPO for stabilizing training. $A_t$ is the advantage, which is computed by applying GAE~\citep{schulman2015high}, based on the rewards $\{r_{\ge t}\}$ and a learned value function $V_{\psi}$. Thus, in PPO, a value function needs to be trained alongside the policy model and to mitigate over-optimization of the reward model, we follow~\citet{ouyang2022training} to add a per-token KL penalty from a reference model in the reward at each token, i.e., 

$$
 r_{t} = r_\phi(q, o) - \beta \log\frac{\pi_{\theta}(o_{t}|q, o_{<t})}{\pi_{ref}(o_{t}|q, o_{<t})},
$$

where $r_\phi$ is the reward model, $\pi_{ref}$ is the reference model, which we initialize with a base model, and $\beta$ is the coefficient of the KL penalty.
    

Our training recipe follows the RLVR framework in Deepseek-R1-Zero~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. RLVR refers to the RL training method where the reward model is a verification function instead of a learned reward model~\citep{lambert2024t}. In our study, we use a simple rule-based reward function which computes reward based on the outcome. To ensure the model output is both correct and in its correct format, we first check the format of the output and assign a -1.0 penalty if the output does not have a valid format. The valid format should follow \verb|<think>...</think> <answer>..</answer>|. Once the format check is passed, we check the correctness of the output and assign zero reward if the output answer is incorrect and gives a 1.0 reward if the output answer is correct. In other words, only an output that is both correct and is in the correct format will receive a positive reward. We determine if the answer is correct if the option letter in the answer output matches the gold option letter.
Below is the pseudo code of the reward function:
\begin{AIbox}
\begin{verbatim}
def reward_fuction(response, answer):
    if not validate_format(response):
        return -1.0
    if extract_answer_choice(response) == answer:
        return 1.0
    else:
        return 0.0
\end{verbatim}
\end{AIbox}

We also compare it with alternative reward functions, including those that penalize incorrect answers. 
Through experiments, we find that penalizing incorrect answers leads to a slight decline in model performance. Additionally, we observe an increase in reward hacking behaviors, where the model directly gives a way the answer within the thinking step.

Note that we train \ours using only the question inputs and the answer labels, without additional supervision. To test if medical reasoning can emerge purely from RLVR in a similar way mathmatical reasoning emerges from DeepSeek-R1-Zero, we perform RLVR directly on a base model without any instruction tuning and without any explicit reasoning supervision.  