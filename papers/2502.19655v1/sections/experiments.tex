\section{Experiments}

\subsection{Dataset}

For training, we use the MedQA-USMLE dataset~\citep{jin2021disease}, which consists of multi-choice questions sourced from professional medical board exams and covers a wide range of medical topics, requiring domain-specific knowledge and reasoning skills. 
Note that we do not use the popular “4\_options” version of MedQA. Instead, we use the original version, which is more challenging as the questions have more than four options (including “None of the above”).
Below is an example question from the dataset:

\begin{table}[ht] 
\centering
\begin{tabular}
{p{0.9\linewidth}}
\toprule
\textbf{Question:} \newline A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98\% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient? Answer with just one of the letters from the options below. \\
\midrule
\textbf{Options:} \newline
\quad \textbf{A.} Ampicillin
\quad \textbf{B.} Ceftriaxone 
\quad \textbf{C.} Ciprofloxacin 
\quad \textbf{D.} Doxycycline \newline
\quad \textbf{E.} Nitrofurantoin 
\quad \textbf{F.} None of the above \\
\midrule
\textbf{Answer:} \textbf{E} \\
\bottomrule
\end{tabular}
\caption{An example question from MedQA-USMLE dataset which is used to train \ours. We only used the question and answer labels without any reasoning supervisions. }
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}
{lcc}
\toprule
& MedQA-USMLE (\# of questions) & MMLU-Pro-Health (\# of questions)\\
\midrule
Train & 10,178 & n/a \\
Development & 1,272 & not used \\
Test & 1,273 & 818 \\
\bottomrule
\end{tabular}
\caption{Data statistics \label{table:data}}
\end{table}
% |  | MedQA-USMLE (# of questions) | MMLU-Pro-Health  (# of questions) |
% | --- | --- | --- |
% | Train | 10,178 | n/a |
% | Development | 1,272 | not used |
% | Test | 1,273 | 818 |

For evaluation, we test our model on the \emph{in-distribution} test set of MedQA-USMLE. In addition, to test the generalizability of the models, we also perform evaluation on an \emph{out-of-distribution} test set: the health subset of MMLU-Pro~\citep{wang2024mmlu} which consists of 818 challenging multiple-choice questions related to health care and medicine. Data statics are summarized in~\Cref{table:data}.



\subsection{Prompt}

We adapt the prompt template from~ \citet{deepseekai2025deepseekr1incentivizingreasoningcapability}, where \verb|{input}| will be replaced with a multi-choice question. We do not force the assistant message to start with \verb|<think>|, because we find that during training the model is able to learn to follow the format requirement quickly. Below is the prompt we used:

\begin{AIbox}
A conversation between User and Assistant. The user asks a question, and the assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within \verb|<think>| \verb|</think>| and \verb|<answer> </answer>| tags, respectively, i.e., \verb|<think>| reasoning process here \verb|</think><answer>| answer here \verb|</answer>|.

User: \verb|{input}|

Assistant:
\end{AIbox}

% \begin{enumerate}
% \item
% A conversation between User and Assistant. The user asks a question, and the assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\n\nUser: {input}\n\nAssistant:
% \end{\enumerate}
% ```python
% "A conversation between User and Assistant. The user asks a question, and the assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\n\nUser: {input}\n\nAssistant:"
% ```

\subsection{Training Setup}

We initialize \ours with Qwen2.5-3B~\citep{qwen2.5} and use OpenRLHF~\citep{hu2024openrlhf} framework to train RLVR with MedQA MCQA labels on the base model for 10 epochs on 4x40G8 A100 GPUs.

{\bf Baselines:} We compare \ours against a SFT baseline using the question-answer pairs from the same MedQA training data. The SFT baseline was trained for 10 epochs and we performed grid search for learning rate [1e-5, 5e-6, 1e-6] to ensure competitive performance from this baseline. Notice that the SFT baseline can only directly output answers without reasoning as it was trained to do so. In addition, we also report the direct and chain-of-thought (CoT) prompting results of the Qwen2.5-3B base model.