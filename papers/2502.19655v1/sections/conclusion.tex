\section{Conclusion}
We introduced MED-RLVR, an initial study of Reinforcement Learning with Verifiable Rewards (RLVR) in the medical domain. Our results show that \ours matches supervised fine-tuning (SFT) on in-distribution tasks while significantly improving out-of-distribution generalization (+8 accuracy points). Notably, medical reasoning emerged without explicit supervision, though challenges like reward hacking were observed. While our study focuses on multiple-choice medical questions, future work should explore more complex medical reasoning tasks and multimodal integration to advance the potential of RL in the medical domain.