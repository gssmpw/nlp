@article{qian2023webbrain,
  title={Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus},
  author={Qian, Hongjing and Zhu, Yutao and Dou, Zhicheng and Gu, Haoqi and Zhang, Xinyu and Liu, Zheng and Lai, Ruofei and Cao, Zhao and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2304.04358},
  year={2023}
}

@article{zhao2023panacea,
  title={PANACEA: An automated misinformation detection system on COVID-19},
  author={Zhao, Runcong and Arana-Catania, Miguel and Zhu, Lixing and Kochkina, Elena and Gui, Lin and Zubiaga, Arkaitz and Procter, Rob and Liakata, Maria and He, Yulan},
  journal={arXiv preprint arXiv:2303.01241},
  year={2023}
}

@article{mendes2022human,
  title={Human-in-the-loop evaluation for early misinformation detection: A case study of COVID-19 treatments},
  author={Mendes, Ethan and Chen, Yang and Xu, Wei and Ritter, Alan},
  journal={arXiv preprint arXiv:2212.09683},
  year={2022}
}

@article{izacard2020leveraging,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2020}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{phan2021scifive,
  title={Scifive: a text-to-text transformer model for biomedical literature},
  author={Phan, Long N and Anibal, James T and Tran, Hieu and Chanana, Shaurya and Bahadroglu, Erol and Peltekian, Alec and Altan-Bonnet, Gr{\'e}goire},
  journal={arXiv preprint arXiv:2106.03598},
  year={2021}
}

@inproceedings{upadhyay2023passage,
  title={A passage retrieval transformer-based re-ranking model for truthful consumer health search},
  author={Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={355--371},
  year={2023},
  organization={Springer}
}

@inproceedings{brand2021bart,
  title={E-bart: Jointly predicting and explaining truthfulness},
  author={Brand, Erik and Roitero, Kevin and Soprano, Michael and Demartini, Gianluca and others},
  booktitle={Proceedings of the Conference for Truth and Trust Online},
  year={2021}
}

@article{upadhyay2023explainable,
  title={Explainable online health information truthfulness in Consumer Health Search},
  author={Upadhyay, Rishabh and Knoth, Petr and Pasi, Gabriella and Viviani, Marco},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  pages={1184851},
  year={2023},
  publisher={Frontiers}
}

@inproceedings{10.1145/3599696.3612902,
author = {Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
title = {Leveraging Socio-contextual Information in BERT for Fake Health News Detection in Social Media},
year = {2023},
isbn = {9798400702259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Fake news is a major challenge in social media, particularly in the health domain where it can lead to severe consequences for both individuals and society as a whole. To contribute to combating this problem, we present a novel solution for improving the accuracy of detecting fake health news, utilizing a fine-tuned BERT model that integrates both user- and content-related socio-contextual information. Specifically, this information is combined with the textual content itself to form a socio-contextual input sequence for the BERT model. By fine-tuning such a model with respect to the health misinformation detection task, the resulting classifier can accurately predict the category to which each piece of content belongs, i.e., either “real health news” or “fake health news”. We validate our solution through a series of experiments conducted on distinct publicly available datasets constituted by health-related tweets. These results illustrate the superiority of the proposed solution compared to the standard BERT baseline model and other advanced models. Indeed, they show that the integration of socio-contextual information in the detection process positively contributes to increasing the overall accuracy of the fake health news detection task. The study also suggests, in a preliminary way, how such information could be used for the explainability of the model itself.},
booktitle = {Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks},
pages = {38–46},
numpages = {9},
keywords = {Transformers, Social Media, Social Context, Health Misinformation, Fake News, Classification, BERT},
location = {Rome, Italy},
series = {OASIS '23}
}

@inproceedings{shang2022knowledge,
  title={A Knowledge-driven Domain Adaptive Approach to Early Misinformation Detection in an Emergent Health Domain on Social Media},
  author={Shang, Lanyu and Zhang, Yang and Yue, Zhenrui and Choi, YeonJung and Zeng, Huimin and Wang, Dong},
  booktitle={2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
  pages={34--41},
  year={2022},
  organization={IEEE}
}

@inproceedings{wu2022bias,
  title={Bias mitigation for evidence-aware fake news detection by causal intervention},
  author={Wu, Junfei and Liu, Qiang and Xu, Weizhi and Wu, Shu},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2308--2313},
  year={2022}
}

@article{kou2022hc,
  title={Hc-covid: A hierarchical crowdsource knowledge graph approach to explainable covid-19 misinformation detection},
  author={Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang, Dong},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={GROUP},
  pages={1--25},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{10.1145/3462203.3475898,
author = {Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
title = {Health Misinformation Detection in Web Content: A Structural-, Content-based, and Context-aware Approach based on Web2Vec},
year = {2021},
isbn = {9781450384780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In recent years, we have witnessed the proliferation of large amounts of online content generated directly by users with virtually no form of external control, leading to the possible spread of misinformation. The search for effective solutions to this problem is still ongoing, and covers different areas of application, from opinion spam to fake news detection. A more recently investigated scenario, despite the serious risks that incurring disinformation could entail, is that of the online dissemination of health information.Early approaches in this area focused primarily on user-based studies applied to Web page content. More recently, automated approaches have been developed for both Web pages and social media content, particularly with the advent of the COVID-19 pandemic. These approaches are primarily based on handcrafted features extracted from online content in association with Machine Learning. In this scenario, we focus on Web page content, where there is still room for research to study structural-, content- and context-based features to assess the credibility of Web pages.Therefore, this work aims to study the effectiveness of such features in association with a deep learning model, starting from an embedded representation of Web pages that has been recently proposed in the context of phishing Web page detection, i.e., Web2Vec.},
booktitle = {Proceedings of the Conference on Information Technology for Social Good},
pages = {19–24},
numpages = {6},
keywords = {Social Web, Machine Learning, Health Misinformation, Deep Learning, Credibility},
location = {Roma, Italy},
series = {GoodIT '21}
}

@article{upadhyay2023vec4cred,
  title={Vec4cred: a model for health misinformation detection in web pages},
  author={Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={4},
  pages={5271--5290},
  year={2023},
  publisher={Springer}
}

@article{chen2023can,
  title={Can llm-generated misinformation be detected?},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv preprint arXiv:2309.13788},
  year={2023}
}

@article{jiang2022fake,
  title={Fake news detection via knowledgeable prompt learning},
  author={Jiang, Gongyao and Liu, Shuang and Zhao, Yu and Sun, Yueheng and Zhang, Meishan},
  journal={Information Processing \& Management},
  volume={59},
  number={5},
  pages={103029},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{yue2022contrastive,
  title={Contrastive domain adaptation for early misinformation detection: A case study on covid-19},
  author={Yue, Zhenrui and Zeng, Huimin and Kou, Ziyi and Shang, Lanyu and Wang, Dong},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2423--2433},
  year={2022}
}

@inproceedings{raunak-etal-2021-curious,
    title = "The Curious Case of Hallucinations in Neural Machine Translation",
    author = "Raunak, Vikas  and
      Menezes, Arul  and
      Junczys-Dowmunt, Marcin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "1172--1183",
    abstract = "In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results.",
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@article{kang2024domain,
  title={Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant},
  author={Kang, Cheng and Novak, Daniel and Urbanova, Katerina and Cheng, Yuqing and Hu, Yong},
  journal={arXiv preprint arXiv:2404.16160},
  year={2024}
}

@article{setty2024improving,
  title={Improving Retrieval for RAG based Question Answering Models on Financial Documents},
  author={Setty, Spurthi and Jijo, Katherine and Chung, Eden and Vidra, Natan},
  journal={arXiv preprint arXiv:2404.07221},
  year={2024}
}

@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@inproceedings{izacard-grave-2021-leveraging,
    title = "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    author = "Izacard, Gautier  and
      Grave, Edouard",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "874--880",
    abstract = "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@article{izacard2023atlas,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={251},
  pages={1--43},
  year={2023}
}

@article{he2022rethinking,
  title={Rethinking with retrieval: Faithful large language model inference},
  author={He, Hangfeng and Zhang, Hongming and Roth, Dan},
  journal={arXiv preprint arXiv:2301.00303},
  year={2022}
}

@inproceedings{trivedi-etal-2023-interleaving,
    title = "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    author = "Trivedi, Harsh  and
      Balasubramanian, Niranjan  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "10014--10037",
    abstract = "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \textit{what to retrieve} depends on \textit{what has already been derived}, which in turn may depend on \textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
}

@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@inproceedings{li-etal-2023-large,
    title = "Large Language Models with Controllable Working Memory",
    author = "Li, Daliang  and
      Rawat, Ankit Singh  and
      Zaheer, Manzil  and
      Wang, Xin  and
      Lukasik, Michal  and
      Veit, Andreas  and
      Yu, Felix  and
      Kumar, Sanjiv",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "1774--1793",
    abstract = "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining.While many downstream applications provide the model with an informational context to aid its underlying task, how the model{'}s world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model{'}s memorized knowledge. This enables model predictions to be grounded in the context, which then facilitates updating specific model predictions without frequently retraining the model. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size. As a solution, we propose a simple yet effective method {--} knowledge aware finetuning (KAFT) {--} to strengthen both controllability and robustness by injecting counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.",
}

@inproceedings{
zhou2023docprompting,
title={DocPrompting: Generating Code by Retrieving the Docs},
author={Shuyan Zhou and Uri Alon and Frank F. Xu and Zhengbao Jiang and Graham Neubig},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@inproceedings{cai-etal-2019-skeleton,
    title = "Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory",
    author = "Cai, Deng  and
      Wang, Yan  and
      Bi, Wei  and
      Tu, Zhaopeng  and
      Liu, Xiaojiang  and
      Lam, Wai  and
      Shi, Shuming",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1219--1228",
    abstract = "Traditional generative dialogue models generate responses solely from input queries. Such information is insufficient for generating a specific response since a certain query could be answered in multiple ways. Recently, researchers have attempted to fill the information gap by exploiting information retrieval techniques. For a given query, similar dialogues are retrieved from the entire training data and considered as an additional knowledge source. While the use of retrieval may harvest extensive information, the generative models could be overwhelmed, leading to unsatisfactory performance. In this paper, we propose a new framework which exploits retrieval results via a skeleton-to-response paradigm. At first, a skeleton is extracted from the retrieved dialogues. Then, both the generated skeleton and the original query are used for response generation via a novel response generator. Experimental results show that our approach significantly improves the informativeness of the generated responses",
}

@inproceedings{cai-etal-2019-retrieval,
    title = "Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework",
    author = "Cai, Deng  and
      Wang, Yan  and
      Bi, Wei  and
      Tu, Zhaopeng  and
      Liu, Xiaojiang  and
      Shi, Shuming",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "1866--1875",
    abstract = "End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the \textit{safe response problem}. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.",
}

@article{peng2023check,
  title={Check your facts and try again: Improving large language models with external knowledge and automated feedback},
  author={Peng, Baolin and Galley, Michel and He, Pengcheng and Cheng, Hao and Xie, Yujia and Hu, Yu and Huang, Qiuyuan and Liden, Lars and Yu, Zhou and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2302.12813},
  year={2023}
}

@article{cui2023chatlaw,
  title={Chatlaw: Open-source legal large language model with integrated external knowledge bases},
  author={Cui, Jiaxi and Li, Zongjian and Yan, Yang and Chen, Bohua and Yuan, Li},
  journal={arXiv preprint arXiv:2306.16092},
  year={2023}
}

@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {factuality in NLG, intrinsic hallucination, faithfulness in NLG, Hallucination, consistency in NLG, extrinsic hallucination}
}

@inproceedings{cao-etal-2020-factual,
    title = "Factual Error Correction for Abstractive Summarization Models",
    author = "Cao, Meng  and
      Dong, Yue  and
      Wu, Jiapeng  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "6251--6258",
    abstract = "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.",
}

@article{du2021glm,
  title={Glm: General language model pretraining with autoregressive blank infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10360},
  year={2021}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{abdullah2022chatgpt,
  title={ChatGPT: Fundamentals, applications and social impacts},
  author={Abdullah, Malak and Madain, Alia and Jararweh, Yaser},
  booktitle={2022 Ninth International Conference on Social Networks Analysis, Management and Security (SNAMS)},
  pages={1--8},
  year={2022},
  organization={Ieee}
}

@inbook{10.5555/3454287.3454581,
author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {294},
numpages = {15}
}

@inproceedings{
wang2018glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
hendrycks2021measuring,
title={Measuring Massive Multitask Language Understanding},
author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2021},
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@misc{zhong2023agieval,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{huang2023ceval,
title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and  Fu, Yao and Sun, Maosong and He, Junxian},
journal={arXiv preprint arXiv:2305.08322},
year={2023}
}

@misc{open-llm-leaderboard,
  author = {Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, Thomas Wolf},
  title = {Open LLM Leaderboard},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = "\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}"
}
@misc{xu2023cvalues,
      title={CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility}, 
      author={Guohai Xu and Jiayi Liu and Ming Yan and Haotian Xu and Jinghui Si and Zhuoran Zhou and Peng Yi and Xing Gao and Jitao Sang and Rong Zhang and Ji Zhang and Chao Peng and Fei Huang and Jingren Zhou},
      year={2023},
      eprint={2307.09705},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{zhang2023m3exam,
      title={M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models},
      author={Wenxuan Zhang and Sharifah Mahani Aljunied and Chang Gao and Yew Ken Chia and Lidong Bing},
      year={2023},
      eprint={2306.05179},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{qin2023toolllm,
      title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs}, 
      author={Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2307.16789},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{xu2023searchinthechain,
      title={Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks}, 
      author={Shicheng Xu and Liang Pang and Huawei Shen and Xueqi Cheng and Tat-Seng Chua},
      year={2023},
      eprint={2304.14732},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{
zhou2023leasttomost,
title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
author={Denny Zhou and Nathanael Sch{\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@inproceedings{
drozdov2023compositional,
title={Compositional Semantic Parsing with Large Language Models},
author={Andrew Drozdov and Nathanael Sch{\"a}rli and Ekin Aky{\"u}rek and Nathan Scales and Xinying Song and Xinyun Chen and Olivier Bousquet and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@misc{bian2023drop,
      title={A Drop of Ink Makes a Million Think: The Spread of False Information in Large Language Models}, 
      author={Ning Bian and Peilin Liu and Xianpei Han and Hongyu Lin and Yaojie Lu and Ben He and Le Sun},
      year={2023},
      eprint={2305.04812},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@article{guo2023close,
  title={How close is chatgpt to human experts? comparison corpus, evaluation, and detection},
  author={Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
  journal={arXiv preprint arXiv:2301.07597},
  year={2023}
}

@article{li2023chatgpt,
      title={Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks}, 
      author={Xianzhi Li and Xiaodan Zhu and Zhiqiang Ma and Xiaomo Liu and Sameena Shah},
      year={2023},
      journal={arXiv preprint arXiv:2305.05862},
}

@article{shen2023chatgpt,
  title={In chatgpt we trust? measuring and characterizing the reliability of chatgpt},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2304.08979},
  year={2023}
}

@misc{liu2023evaluating,
      title={Evaluating Verifiability in Generative Search Engines}, 
      author={Nelson F. Liu and Tianyi Zhang and Percy Liang},
      year={2023},
      eprint={2304.09848},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{maynez-etal-2020-faithfulness,
    title = "On Faithfulness and Factuality in Abstractive Summarization",
    author = "Maynez, Joshua  and
      Narayan, Shashi  and
      Bohnet, Bernd  and
      McDonald, Ryan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "1906--1919",
    abstract = "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
}


@article{chern2023factool,
  title={FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
  author={Chern, I and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei and others},
  journal={arXiv preprint arXiv:2307.13528},
  year={2023}
}

@inproceedings{clarke2020overview,
  title={Overview of the TREC 2020 Health Misinformation Track},
  author={Clarke, Charles LA and Maistro, Maria and Smucker, Mark D and Zuccon, Guido},
  booktitle={TREC},
  year={2020}
}

@article{zeng2024justilm,
  title={JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims},
  author={Zeng, Fengzhu and Gao, Wei},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={334--354},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{zhang2023towards,
  title={Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method},
  author={Zhang, Xuan and Gao, Wei},
  journal={arXiv preprint arXiv:2310.00305},
  year={2023}
}

@article{pan2023fact,
  title={Fact-checking complex claims with program-guided reasoning},
  author={Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav},
  journal={arXiv preprint arXiv:2305.12744},
  year={2023}
}

@inproceedings{goeuriot2021consumer,
  title={Consumer health search at CLEF eHealth 2021},
  author={Goeuriot, Lorraine and Suominen, Hanna and Pasi, Gabriella and Bassani, Elias and Brew-Sam, Nicola and Gonz{\'a}lez-S{\'a}ez, Gabriela and Kelly, Liadh and Mulhem, Phillippe and Seneviratne, Sandaru and Upadhyay, Rishabh and others},
  booktitle={CEUR Workshop Proceedings},
  pages={1--19},
  year={2021},
  organization={CEUR}
}


@misc{ren2023investigating,
      title={Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation}, 
      author={Ruiyang Ren and Yuhao Wang and Yingqi Qu and Wayne Xin Zhao and Jing Liu and Hao Tian and Hua Wu and Ji-Rong Wen and Haifeng Wang},
      year={2023},
      eprint={2307.11019},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{adlakha2023evaluating,
  title={Evaluating correctness and faithfulness of instruction-following models for question answering},
  author={Adlakha, Vaibhav and BehnamGhader, Parishad and Lu, Xing Han and Meade, Nicholas and Reddy, Siva},
  journal={arXiv preprint arXiv:2307.16877},
  year={2023}
}

@inproceedings{buchanan2001information,
  title={Information overload: A decision making perspective},
  author={Buchanan, John and Kock, Ned},
  booktitle={Multiple Criteria Decision Making in the New Millennium: Proceedings of the Fifteenth International Conference on Multiple Criteria Decision Making (MCDM) Ankara, Turkey, July 10--14, 2000},
  pages={49--58},
  year={2001},
  organization={Springer}
}

@inproceedings{goodrich2019assessing,
  title={Assessing the factual accuracy of generated text},
  author={Goodrich, Ben and Rao, Vinay and Liu, Peter J and Saleh, Mohammad},
  booktitle={proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={166--175},
  year={2019}
}

@article{di2022health,
  title={Health misinformation detection in the social web: an overview and a data science approach},
  author={Di Sotto, Stefano and Viviani, Marco},
  journal={International Journal of Environmental Research and Public Health},
  volume={19},
  number={4},
  pages={2173},
  year={2022},
  publisher={MDPI}
}

@article{frisoni2024generate,
  title={To generate or to retrieve? on the effectiveness of artificial contexts for medical open-domain question answering},
  author={Frisoni, Giacomo and Cocchieri, Alessio and Presepi, Alex and Moro, Gianluca and Meng, Zaiqiao},
  journal={arXiv preprint arXiv:2403.01924},
  year={2024}
}

@article{kell2024question,
  title={Question answering systems for health professionals at the point of care--a systematic review},
  author={Kell, Gregory and Roberts, Angus and Umansky, Serge and Qian, Linglong and Ferrari, Davide and Soboczenski, Frank and Wallace, Byron and Patel, Nikhil and Marshall, Iain J},
  journal={arXiv preprint arXiv:2402.01700},
  year={2024}
}

@inproceedings{ackerman2023automatic,
  title={Automatic Multilingual Question Generation for Health Data Using LLMs},
  author={Ackerman, Ryan and Balyan, Renu},
  booktitle={International Conference on AI-generated Content},
  pages={1--11},
  year={2023},
  organization={Springer}
}

@article{saxena2023minimizing,
  title={Minimizing Factual Inconsistency and Hallucination in Large Language Models},
  author={Saxena, Shreya and Prasad, Siva and Prakash, MV and Shankar, Advaith and Vaddina, Vishal and Gopalakrishnan, Saisubramaniam and others},
  journal={arXiv preprint arXiv:2311.13878},
  year={2023}
}

@article{khlaut2024efficient,
  title={Efficient Medical Question Answering with Knowledge-Augmented Question Generation},
  author={Khlaut, Julien and Dancette, Corentin and Ferreres, Elodie and Bennani, Alaedine and H{\'e}rent, Paul and Manceron, Pierre},
  journal={arXiv preprint arXiv:2405.14654},
  year={2024}
}

@article{wang2024jmlr,
  title={JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability},
  author={Wang, Junda and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
  journal={arXiv preprint arXiv:2402.17887},
  year={2024}
}

@article{cao2024can,
  title={Can Large Language Models Detect Misinformation in Scientific News Reporting?},
  author={Cao, Yupeng and Nair, Aishwarya Muralidharan and Eyimife, Elyon and Soofi, Nastaran Jamalipour and Subbalakshmi, KP and Wullert II, John R and Basu, Chumki and Shallcross, David},
  journal={arXiv preprint arXiv:2402.14268},
  year={2024}
}

@inproceedings{choi2024automated,
  title={Automated claim matching with large language models: empowering fact-checkers in the fight against misinformation},
  author={Choi, Eun Cheol and Ferrara, Emilio},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={1441--1449},
  year={2024}
}


@article{wan2024dell,
  title={DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection},
  author={Wan, Herun and Feng, Shangbin and Tan, Zhaoxuan and Wang, Heng and Tsvetkov, Yulia and Luo, Minnan},
  journal={arXiv preprint arXiv:2402.10426},
  year={2024}
}


@inproceedings{abualsaud2021uwaterloomds,
  title={UWaterlooMDS at the TREC 2021 health misinformation track},
  author={Abualsaud, Mustafa and Chen, Irene Xiangyi and Ghajar, Kamyar and Minh, Lnl and Smucker, MD and Tahami, Amir Vakili and Zhang, Dake},
  booktitle={Proceedings of the Thirtieth REtrieval Conference Proceedings (TREC 2021). National Institute of Standards and Technology (NIST), Special Publication},
  pages={1--18},
  year={2021}
}

@article{schlicht2021upv,
  title={Upv at TREC Health Misinformation Track 2021 ranking with sBERT and quality estimators},
  author={Schlicht, Ipek Baris and de Paula, Angel Felipe Magnoss{\~a}o and Rosso, Paolo},
  journal={arXiv preprint arXiv:2112.06080},
  year={2021}
}

@article{zhang2022ds4dh,
  title={DS4DH at TREC health misinformation 2021: multi-dimensional ranking models with transfer learning and rank fusion},
  author={Zhang, Boya and Naderi, Nona and Jaume-Santero, Fernando and Teodoro, Douglas},
  journal={arXiv preprint arXiv:2202.06771},
  year={2022}
}

@inproceedings{fernandez2020citius,
  title={CiTIUS at the TREC 2020 Health Misinformation Track.},
  author={Fern{\'a}ndez-Pichel, Marcos and Losada, David E and Pichel, Juan Carlos and Elsweiler, David},
  booktitle={TREC},
  year={2020}
}

@inproceedings{seneviratne2020sandidoc,
  title={SandiDoc at CLEF 2020-Consumer Health Search: AdHoc IR Task.},
  author={Seneviratne, Sandaru and Daskalaki, Eleni and Hossain, Md Zakir and Lenskiy, Artem},
  booktitle={CLEF (Working Notes)},
  year={2020}
}


@inproceedings{mulhem2020lig,
  title={LIG-Health at Adhoc and Spoken IR Consumer Health Search: expanding queries using umls and fasttext},
  author={Mulhem, Philippe and Saez, Gabriela Gonzalez and Mannion, Aidan and Schwab, Didier and Frej, Jibril},
  booktitle={CLEF 2020},
  year={2020}
}

@inproceedings{di2020study,
  title={{A Study on Reciprocal Ranking Fusion in Consumer Health Search. IMS UniPD ad CLEF eHealth 2020 Task 2.}},
  author={Di Nunzio, Giorgio Maria and Marchesin, Stefano and Vezzani, Federica},
  booktitle={CLEF (Working Notes)},
  year={2020}
}



@inproceedings{goeuriot2021clef,
  title={{CLEF eHealth Evaluation Lab 2021}},
  author={Goeuriot, Lorraine and Suominen, Hanna and Kelly, Liadh and Alemany, Laura Alonso and Brew-Sam, Nicola and Cotik, Viviana and Filippo, Dar{\'\i}o and Gonzalez Saez, Gabriela and Luque, Franco and Mulhem, Philippe and others},
  booktitle={Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28--April 1, 2021, Proceedings, Part II 43},
  pages={593--600},
  year={2021},
  organization={Springer}
}

@inproceedings{hassan2017toward,
  title={Toward automated fact-checking: Detecting check-worthy factual claims by claimbuster},
  author={Hassan, Naeemul and Arslan, Fatma and Li, Chengkai and Tremayne, Mark},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1803--1812},
  year={2017}
}

@article{guo2022survey,
  title={A survey on automated fact-checking},
  author={Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={178--206},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{cormack2009reciprocal,
  title={Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  author={Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan},
  booktitle={Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},
  pages={758--759},
  year={2009}
}

@inproceedings{zeng2024combining,
  title={Combining Large Language Models and Crowdsourcing for Hybrid Human-AI Misinformation Detection},
  author={Zeng, Xia and La Barbera, David and Roitero, Kevin and Zubiaga, Arkaitz and Mizzaro, Stefano},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2332--2336},
  year={2024}
}

@inproceedings{perkovic2024hallucinations,
  title={Hallucinations in LLMs: Understanding and Addressing Challenges},
  author={Perkovi{\'c}, Gabrijela and Drobnjak, Antun and Boti{\v{c}}ki, Ivica},
  booktitle={2024 47th MIPRO ICT and Electronics Convention (MIPRO)},
  pages={2084--2088},
  year={2024},
  organization={IEEE}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{pankaj2022augmented,
  title={Augmented bio-sbert: Improving performance for pairwise sentence tasks in bio-medical domain},
  author={Pankaj, Sonam and Gautam, Amit},
  booktitle={Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)},
  pages={43--47},
  year={2022}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{bajaj2016ms,
  title={{MS MARCO: A human generated machine reading comprehension dataset}},
  author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
  journal={arXiv preprint arXiv:1611.09268},
  year={2016}
}

@inproceedings{schwarz2011augmenting,
  title={Augmenting web pages and search results to support credibility assessment},
  author={Schwarz, Julia and Morris, Meredith},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1245--1254},
  year={2011}
}

@inproceedings{upadhyay2022unsupervised,
  title={An unsupervised approach to genuine health information retrieval based on scientific evidence},
  author={Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
  booktitle={International Conference on Web Information Systems Engineering},
  pages={119--135},
  year={2022},
  organization={Springer}
}

@inproceedings{lioma2017evaluation,
  title={Evaluation measures for relevance and credibility in ranked lists},
  author={Lioma, Christina and Simonsen, Jakob Grue and Larsen, Birger},
  booktitle={Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
  pages={91--98},
  year={2017}
}