@inproceedings{10.1145/3462203.3475898,
author = {Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
title = {Health Misinformation Detection in Web Content: A Structural-, Content-based, and Context-aware Approach based on Web2Vec},
year = {2021},
isbn = {9781450384780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In recent years, we have witnessed the proliferation of large amounts of online content generated directly by users with virtually no form of external control, leading to the possible spread of misinformation. The search for effective solutions to this problem is still ongoing, and covers different areas of application, from opinion spam to fake news detection. A more recently investigated scenario, despite the serious risks that incurring disinformation could entail, is that of the online dissemination of health information.Early approaches in this area focused primarily on user-based studies applied to Web page content. More recently, automated approaches have been developed for both Web pages and social media content, particularly with the advent of the COVID-19 pandemic. These approaches are primarily based on handcrafted features extracted from online content in association with Machine Learning. In this scenario, we focus on Web page content, where there is still room for research to study structural-, content- and context-based features to assess the credibility of Web pages.Therefore, this work aims to study the effectiveness of such features in association with a deep learning model, starting from an embedded representation of Web pages that has been recently proposed in the context of phishing Web page detection, i.e., Web2Vec.},
booktitle = {Proceedings of the Conference on Information Technology for Social Good},
pages = {19–24},
numpages = {6},
keywords = {Social Web, Machine Learning, Health Misinformation, Deep Learning, Credibility},
location = {Roma, Italy},
series = {GoodIT '21}
}

@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {248},
numpages = {38},
keywords = {factuality in NLG, intrinsic hallucination, faithfulness in NLG, Hallucination, consistency in NLG, extrinsic hallucination}
}

@inproceedings{10.1145/3599696.3612902,
author = {Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
title = {Leveraging Socio-contextual Information in BERT for Fake Health News Detection in Social Media},
year = {2023},
isbn = {9798400702259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Fake news is a major challenge in social media, particularly in the health domain where it can lead to severe consequences for both individuals and society as a whole. To contribute to combating this problem, we present a novel solution for improving the accuracy of detecting fake health news, utilizing a fine-tuned BERT model that integrates both user- and content-related socio-contextual information. Specifically, this information is combined with the textual content itself to form a socio-contextual input sequence for the BERT model. By fine-tuning such a model with respect to the health misinformation detection task, the resulting classifier can accurately predict the category to which each piece of content belongs, i.e., either “real health news” or “fake health news”. We validate our solution through a series of experiments conducted on distinct publicly available datasets constituted by health-related tweets. These results illustrate the superiority of the proposed solution compared to the standard BERT baseline model and other advanced models. Indeed, they show that the integration of socio-contextual information in the detection process positively contributes to increasing the overall accuracy of the fake health news detection task. The study also suggests, in a preliminary way, how such information could be used for the explainability of the model itself.},
booktitle = {Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks},
pages = {38–46},
numpages = {9},
keywords = {Transformers, Social Media, Social Context, Health Misinformation, Fake News, Classification, BERT},
location = {Rome, Italy},
series = {OASIS '23}
}

@inproceedings{abualsaud2021uwaterloomds,
  title={UWaterlooMDS at the TREC 2021 health misinformation track},
  author={Abualsaud, Mustafa and Chen, Irene Xiangyi and Ghajar, Kamyar and Minh, Lnl and Smucker, MD and Tahami, Amir Vakili and Zhang, Dake},
  booktitle={Proceedings of the Thirtieth REtrieval Conference Proceedings (TREC 2021). National Institute of Standards and Technology (NIST), Special Publication},
  pages={1--18},
  year={2021}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@inproceedings{brand2021bart,
  title={E-bart: Jointly predicting and explaining truthfulness},
  author={Brand, Erik and Roitero, Kevin and Soprano, Michael and Demartini, Gianluca and others},
  booktitle={Proceedings of the Conference for Truth and Trust Online},
  year={2021}
}

@inproceedings{cai-etal-2019-retrieval,
    title = "Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework",
    author = "Cai, Deng  and
      Wang, Yan  and
      Bi, Wei  and
      Tu, Zhaopeng  and
      Liu, Xiaojiang  and
      Shi, Shuming",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    pages = "1866--1875",
    abstract = "End-to-end sequence generation is a popular technique for developing open domain dialogue systems, though they suffer from the \textit{safe response problem}. Researchers have attempted to tackle this problem by incorporating generative models with the returns of retrieval systems. Recently, a skeleton-then-response framework has been shown promising results for this task. Nevertheless, how to precisely extract a skeleton and how to effectively train a retrieval-guided response generator are still challenging. This paper presents a novel framework in which the skeleton extraction is made by an interpretable matching model and the following skeleton-guided response generation is accomplished by a separately trained generator. Extensive experiments demonstrate the effectiveness of our model designs.",
}

@inproceedings{cai-etal-2019-skeleton,
    title = "Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory",
    author = "Cai, Deng  and
      Wang, Yan  and
      Bi, Wei  and
      Tu, Zhaopeng  and
      Liu, Xiaojiang  and
      Lam, Wai  and
      Shi, Shuming",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1219--1228",
    abstract = "Traditional generative dialogue models generate responses solely from input queries. Such information is insufficient for generating a specific response since a certain query could be answered in multiple ways. Recently, researchers have attempted to fill the information gap by exploiting information retrieval techniques. For a given query, similar dialogues are retrieved from the entire training data and considered as an additional knowledge source. While the use of retrieval may harvest extensive information, the generative models could be overwhelmed, leading to unsatisfactory performance. In this paper, we propose a new framework which exploits retrieval results via a skeleton-to-response paradigm. At first, a skeleton is extracted from the retrieved dialogues. Then, both the generated skeleton and the original query are used for response generation via a novel response generator. Experimental results show that our approach significantly improves the informativeness of the generated responses",
}

@inproceedings{cao-etal-2020-factual,
    title = "Factual Error Correction for Abstractive Summarization Models",
    author = "Cao, Meng  and
      Dong, Yue  and
      Wu, Jiapeng  and
      Cheung, Jackie Chi Kit",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "6251--6258",
    abstract = "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.",
}

@article{cao2024can,
  title={Can Large Language Models Detect Misinformation in Scientific News Reporting?},
  author={Cao, Yupeng and Nair, Aishwarya Muralidharan and Eyimife, Elyon and Soofi, Nastaran Jamalipour and Subbalakshmi, KP and Wullert II, John R and Basu, Chumki and Shallcross, David},
  journal={arXiv preprint arXiv:2402.14268},
  year={2024}
}

@article{chen2023can,
  title={Can llm-generated misinformation be detected?},
  author={Chen, Canyu and Shu, Kai},
  journal={arXiv preprint arXiv:2309.13788},
  year={2023}
}

@article{chern2023factool,
  title={FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios},
  author={Chern, I and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei and others},
  journal={arXiv preprint arXiv:2307.13528},
  year={2023}
}

@inproceedings{choi2024automated,
  title={Automated claim matching with large language models: empowering fact-checkers in the fight against misinformation},
  author={Choi, Eun Cheol and Ferrara, Emilio},
  booktitle={Companion Proceedings of the ACM on Web Conference 2024},
  pages={1441--1449},
  year={2024}
}

@inproceedings{clarke2020overview,
  title={Overview of the TREC 2020 Health Misinformation Track},
  author={Clarke, Charles LA and Maistro, Maria and Smucker, Mark D and Zuccon, Guido},
  booktitle={TREC},
  year={2020}
}

@inproceedings{cormack2009reciprocal,
  title={Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  author={Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan},
  booktitle={Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},
  pages={758--759},
  year={2009}
}

@article{cui2023chatlaw,
  title={Chatlaw: Open-source legal large language model with integrated external knowledge bases},
  author={Cui, Jiaxi and Li, Zongjian and Yan, Yang and Chen, Bohua and Yuan, Li},
  journal={arXiv preprint arXiv:2306.16092},
  year={2023}
}

@inproceedings{di2020study,
  title={{A Study on Reciprocal Ranking Fusion in Consumer Health Search. IMS UniPD ad CLEF eHealth 2020 Task 2.}},
  author={Di Nunzio, Giorgio Maria and Marchesin, Stefano and Vezzani, Federica},
  booktitle={CLEF (Working Notes)},
  year={2020}
}

@inproceedings{fernandez2020citius,
  title={CiTIUS at the TREC 2020 Health Misinformation Track.},
  author={Fern{\'a}ndez-Pichel, Marcos and Losada, David E and Pichel, Juan Carlos and Elsweiler, David},
  booktitle={TREC},
  year={2020}
}

@inproceedings{goeuriot2021consumer,
  title={Consumer health search at CLEF eHealth 2021},
  author={Goeuriot, Lorraine and Suominen, Hanna and Pasi, Gabriella and Bassani, Elias and Brew-Sam, Nicola and Gonz{\'a}lez-S{\'a}ez, Gabriela and Kelly, Liadh and Mulhem, Phillippe and Seneviratne, Sandaru and Upadhyay, Rishabh and others},
  booktitle={CEUR Workshop Proceedings},
  pages={1--19},
  year={2021},
  organization={CEUR}
}

@article{guo2022survey,
  title={A survey on automated fact-checking},
  author={Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={178--206},
  year={2022},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@inproceedings{hassan2017toward,
  title={Toward automated fact-checking: Detecting check-worthy factual claims by claimbuster},
  author={Hassan, Naeemul and Arslan, Fatma and Li, Chengkai and Tremayne, Mark},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1803--1812},
  year={2017}
}

@article{he2022rethinking,
  title={Rethinking with retrieval: Faithful large language model inference},
  author={He, Hangfeng and Zhang, Hongming and Roth, Dan},
  journal={arXiv preprint arXiv:2301.00303},
  year={2022}
}

@inproceedings{izacard-grave-2021-leveraging,
    title = "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    author = "Izacard, Gautier  and
      Grave, Edouard",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "874--880",
    abstract = "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
}

@article{izacard2023atlas,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={251},
  pages={1--43},
  year={2023}
}

@article{jiang2022fake,
  title={Fake news detection via knowledgeable prompt learning},
  author={Jiang, Gongyao and Liu, Shuang and Zhao, Yu and Sun, Yueheng and Zhang, Meishan},
  journal={Information Processing \& Management},
  volume={59},
  number={5},
  pages={103029},
  year={2022},
  publisher={Elsevier}
}

@article{khlaut2024efficient,
  title={Efficient Medical Question Answering with Knowledge-Augmented Question Generation},
  author={Khlaut, Julien and Dancette, Corentin and Ferreres, Elodie and Bennani, Alaedine and H{\'e}rent, Paul and Manceron, Pierre},
  journal={arXiv preprint arXiv:2405.14654},
  year={2024}
}

@article{kou2022hc,
  title={Hc-covid: A hierarchical crowdsource knowledge graph approach to explainable covid-19 misinformation detection},
  author={Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang, Dong},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={6},
  number={GROUP},
  pages={1--25},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{li-etal-2023-large,
    title = "Large Language Models with Controllable Working Memory",
    author = "Li, Daliang  and
      Rawat, Ankit Singh  and
      Zaheer, Manzil  and
      Wang, Xin  and
      Lukasik, Michal  and
      Veit, Andreas  and
      Yu, Felix  and
      Kumar, Sanjiv",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "1774--1793",
    abstract = "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining.While many downstream applications provide the model with an informational context to aid its underlying task, how the model{'}s world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model{'}s memorized knowledge. This enables model predictions to be grounded in the context, which then facilitates updating specific model predictions without frequently retraining the model. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size. As a solution, we propose a simple yet effective method {--} knowledge aware finetuning (KAFT) {--} to strengthen both controllability and robustness by injecting counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.",
}

@article{mendes2022human,
  title={Human-in-the-loop evaluation for early misinformation detection: A case study of COVID-19 treatments},
  author={Mendes, Ethan and Chen, Yang and Xu, Wei and Ritter, Alan},
  journal={arXiv preprint arXiv:2212.09683},
  year={2022}
}

@inproceedings{mulhem2020lig,
  title={LIG-Health at Adhoc and Spoken IR Consumer Health Search: expanding queries using umls and fasttext},
  author={Mulhem, Philippe and Saez, Gabriela Gonzalez and Mannion, Aidan and Schwab, Didier and Frej, Jibril},
  booktitle={CLEF 2020},
  year={2020}
}

@article{pan2023fact,
  title={Fact-checking complex claims with program-guided reasoning},
  author={Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav},
  journal={arXiv preprint arXiv:2305.12744},
  year={2023}
}

@inproceedings{pankaj2022augmented,
  title={Augmented bio-sbert: Improving performance for pairwise sentence tasks in bio-medical domain},
  author={Pankaj, Sonam and Gautam, Amit},
  booktitle={Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)},
  pages={43--47},
  year={2022}
}

@article{peng2023check,
  title={Check your facts and try again: Improving large language models with external knowledge and automated feedback},
  author={Peng, Baolin and Galley, Michel and He, Pengcheng and Cheng, Hao and Xie, Yujia and Hu, Yu and Huang, Qiuyuan and Liden, Lars and Yu, Zhou and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2302.12813},
  year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{raunak-etal-2021-curious,
    title = "The Curious Case of Hallucinations in Neural Machine Translation",
    author = "Raunak, Vikas  and
      Menezes, Arul  and
      Junczys-Dowmunt, Marcin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "1172--1183",
    abstract = "In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results.",
}

@misc{ren2023investigating,
      title={Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation}, 
      author={Ruiyang Ren and Yuhao Wang and Yingqi Qu and Wayne Xin Zhao and Jing Liu and Hao Tian and Hua Wu and Ji-Rong Wen and Haifeng Wang},
      year={2023},
      eprint={2307.11019},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{schlicht2021upv,
  title={Upv at TREC Health Misinformation Track 2021 ranking with sBERT and quality estimators},
  author={Schlicht, Ipek Baris and de Paula, Angel Felipe Magnoss{\~a}o and Rosso, Paolo},
  journal={arXiv preprint arXiv:2112.06080},
  year={2021}
}

@inproceedings{seneviratne2020sandidoc,
  title={SandiDoc at CLEF 2020-Consumer Health Search: AdHoc IR Task.},
  author={Seneviratne, Sandaru and Daskalaki, Eleni and Hossain, Md Zakir and Lenskiy, Artem},
  booktitle={CLEF (Working Notes)},
  year={2020}
}

@inproceedings{shang2022knowledge,
  title={A Knowledge-driven Domain Adaptive Approach to Early Misinformation Detection in an Emergent Health Domain on Social Media},
  author={Shang, Lanyu and Zhang, Yang and Yue, Zhenrui and Choi, YeonJung and Zeng, Huimin and Wang, Dong},
  booktitle={2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
  pages={34--41},
  year={2022},
  organization={IEEE}
}

@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@inproceedings{trivedi-etal-2023-interleaving,
    title = "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    author = "Trivedi, Harsh  and
      Balasubramanian, Niranjan  and
      Khot, Tushar  and
      Sabharwal, Ashish",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    pages = "10014--10037",
    abstract = "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \textit{what to retrieve} depends on \textit{what has already been derived}, which in turn may depend on \textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
}

@article{upadhyay2023explainable,
  title={Explainable online health information truthfulness in Consumer Health Search},
  author={Upadhyay, Rishabh and Knoth, Petr and Pasi, Gabriella and Viviani, Marco},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  pages={1184851},
  year={2023},
  publisher={Frontiers}
}

@article{upadhyay2023vec4cred,
  title={Vec4cred: a model for health misinformation detection in web pages},
  author={Upadhyay, Rishabh and Pasi, Gabriella and Viviani, Marco},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={4},
  pages={5271--5290},
  year={2023},
  publisher={Springer}
}

@article{wan2024dell,
  title={DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection},
  author={Wan, Herun and Feng, Shangbin and Tan, Zhaoxuan and Wang, Heng and Tsvetkov, Yulia and Luo, Minnan},
  journal={arXiv preprint arXiv:2402.10426},
  year={2024}
}

@article{wang2024jmlr,
  title={JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability},
  author={Wang, Junda and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
  journal={arXiv preprint arXiv:2402.17887},
  year={2024}
}

@inproceedings{wu2022bias,
  title={Bias mitigation for evidence-aware fake news detection by causal intervention},
  author={Wu, Junfei and Liu, Qiang and Xu, Weizhi and Wu, Shu},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2308--2313},
  year={2022}
}

@inproceedings{yue2022contrastive,
  title={Contrastive domain adaptation for early misinformation detection: A case study on covid-19},
  author={Yue, Zhenrui and Zeng, Huimin and Kou, Ziyi and Shang, Lanyu and Wang, Dong},
  booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
  pages={2423--2433},
  year={2022}
}

@inproceedings{zeng2024combining,
  title={Combining Large Language Models and Crowdsourcing for Hybrid Human-AI Misinformation Detection},
  author={Zeng, Xia and La Barbera, David and Roitero, Kevin and Zubiaga, Arkaitz and Mizzaro, Stefano},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2332--2336},
  year={2024}
}

@article{zeng2024justilm,
  title={JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims},
  author={Zeng, Fengzhu and Gao, Wei},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={334--354},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{zhang2022ds4dh,
  title={DS4DH at TREC health misinformation 2021: multi-dimensional ranking models with transfer learning and rank fusion},
  author={Zhang, Boya and Naderi, Nona and Jaume-Santero, Fernando and Teodoro, Douglas},
  journal={arXiv preprint arXiv:2202.06771},
  year={2022}
}

@article{zhang2023towards,
  title={Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method},
  author={Zhang, Xuan and Gao, Wei},
  journal={arXiv preprint arXiv:2310.00305},
  year={2023}
}

@article{zhao2023panacea,
  title={PANACEA: An automated misinformation detection system on COVID-19},
  author={Zhao, Runcong and Arana-Catania, Miguel and Zhu, Lixing and Kochkina, Elena and Gui, Lin and Zubiaga, Arkaitz and Procter, Rob and Liakata, Maria and He, Yulan},
  journal={arXiv preprint arXiv:2303.01241},
  year={2023}
}

