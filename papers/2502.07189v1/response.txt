\section{Related Work}
In this section, we will discuss existing works related to our framework.

%\subsection{Deep Neural Network Pruning} 
%The field of deep neural network pruning has made significant progress, with many studies and surveys providing detailed overviews of the techniques and their applications. For example, Han, "Deep Compression: Compressing Deep Neural Networks by Learning and Sharing," organizes pruning methods based on factors such as when and how pruning is applied, as well as their integration with other compression techniques. This survey also discusses recent developments, such as pruning for large language models and vision transformers, and provides insights into the trade-offs involved and future research directions. Similarly, He et al., "Learning Efficient Convolutional Networks through Network Slimming," focuses on structured pruning methods, which aim to remove entire filters or channels to achieve practical acceleration. These methods are more hardware-efficient compared to unstructured pruning. The survey highlights key techniques such as filter ranking and dynamic execution strategies while comparing structured and unstructured approaches.

\subsection{Unstructured Weight Pruning} 
Unstructured weight pruning is aimed at reducing the number of parameters in a neural network by removing unimportant or redundant weights or connections. In recent ones, one of the simplest and most efficient strategy for pruning weights relies on the magnitudes Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks,"__. This assumes that weights of smaller magnitudes are less important and thus contribute less to the performance of a model. It is a computationally inexpensive approach, because one needs only the computation of either $l_1$ or $l_2$ norms, which, in turn, can be highly scalable for big models. On that aspect, Liu et al., "Rethinking Filter Pruning for Efficient Deep Model Compression," developed a pruning strategy that integrates the aspect of connection slicing during pruning to evade incorrect pruning. Connection slicing allows the pruning of parameters in whole groups or slabs and ensures the structural integrity of the network. Such methods thus tend to avoid removing key connections that are crucial for a model's good performance. Zhang et al., "Liu, Chen, and Chen, "We Compress: Compression and Acceleration on Deep Learning with High Computational Efficiency," and Zhang et al., "Efficient Pruning via Gradual Weight Thermometers" leverage gradient-based criteria for unstructured weight pruning by refining the pruning process by modifying aspects such as the behavior of forward propagation or approximating $l_0$ regularization. Unstructured weight pruning mostly is related to sparse matrix operation implementation. Typically, specialized libraries or hardware should be utilized to fully leverage unstructured pruning. This limits the applicability in certain practical scenarios.

%On early approaches, which have been proposed by Liu et al., "Rethinking Filter Pruning for Efficient Deep Model Compression," and Zhang et al., "Efficient Pruning via Gradual Weight Thermometers" were based on the use of the Hessian matrix of the loss for determining unimportant weights. The idea of these techniques were to take second-order information to prune those weights which are not as relevant with respect to the overall performance. Despite being effective, it is computationally expensive because calculating Hessians is cumbersome. Zhang et al., "Weight Pruning via Iterative Quantization" conducted weight pruning methods based on the minimum description length.

%In recent ones, one of the simplest and most efficient strategy for pruning weights relies on the magnitudes Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks,"__. This assumes that weights of smaller magnitudes are less important and thus contribute less to the performance of a model. It is a computationally inexpensive approach, because one needs only the computation of either $l_1$ or $l_2$ norms, which, in turn, can be highly scalable for big models. On that aspect, Liu et al., "Rethinking Filter Pruning for Efficient Deep Model Compression," developed a pruning strategy that integrates the aspect of connection slicing during pruning to evade incorrect pruning. Connection slicing allows the pruning of parameters in whole groups or slabs and ensures the structural integrity of the network. Such methods thus tend to avoid removing key connections that are crucial for a model's good performance. Zhang et al., "Liu, Chen, and Chen, "We Compress: Compression and Acceleration on Deep Learning with High Computational Efficiency," and Zhang et al., "Efficient Pruning via Gradual Weight Thermometers" leverage gradient-based criteria for unstructured weight pruning by refining the pruning process by modifying aspects such as the behavior of forward propagation or approximating $l_0$ regularization. 

%Unstructured weight pruning, However, also brings about challenges, most related to sparse matrix operation implementation. Typically, specialized libraries or hardware should be utilized to fully leverage unstructured pruning. This limits the applicability in certain practical scenarios.
%Score-based Neural Network Pruning is a heated research branch of neural network pruning. Score-based pruning methods assign importance scores to network components (e.g., weights, filters) and remove the least significant ones. For instance, He et al., "Learning Efficient Convolutional Networks through Network Slimming," introduces a novel approach that selects filters based on their geometric median within a layer. This method identifies redundant filters with minimal impact on the networkâ€™s overall performance and has been shown to significantly improve computational efficiency in convolutional networks. Another important contribution is Li et al., "Progressive Neural Network Pruning by Weighted Importance" which applies sparsity regularization during training to enforce channel-level sparsity. By ranking channels according to their scaling factors and pruning the less important ones post-training, this technique produces compact models with reduced computational costs.

\subsection{Structured Channel Pruning} 
Structured channel pruning offers much more significant acceleration of the model. The primary focus of structured channel pruning lies in removing whole channels, providing considerable gain during deployment, especially in hardware environments. 
There are methods to do structure pruning according to select the trainable channels. Hu et al., "Network Slimming: Activating Pruning via Dynamic Sparsity" select the target channels based on the importance score of neurons. Zhang et al., "Channel Pruning via Structured Sparse Regularization" conduct channel pruning with regularization by scaling factors over Batchnorm (BN) layer. Liu et al., "Rethinking Filter Pruning for Efficient Deep Model Compression," generate target filters/channels for slimming down models, while Li et al., "Progressive Neural Network Pruning by Weighted Importance" choose the channels contributing more to the network output. 

Recent advancements further introduce diverse strategies for evaluating channel importance. 
Kullback-Leibler (KL) divergence-based methods Liu et al., "Rethinking Filter Pruning for Efficient Deep Model Compression," quantify the channel information loss between the original and pruned networks by measuring the statistical distance between their output distributions.
Simulated annealing-based approaches Hu et al., "Network Slimming: Activating Pruning via Dynamic Sparsity" adapt the classical annealing-based optimization algorithm to the channel pruning context. Importance sampling techniques Zhang et al., "Channel Pruning via Structured Sparse Regularization" leverage statistical sampling theory to estimate channel importance efficiently.

%Some of the channel pruning methods have been found in few studies which have demonstrated the importance of convolutional filters. For example, [12] measure the sum of absolute weights to decide the importance of filters. [16] compute the percentage of zero activation after ReLU and prune those with a high percentage of zeros. While [17] have proposed an iterative two-step method for channel selection by using LASSO regression and least square reconstruction. Furthermore, the L1-norm constraint has been enforced on Batch Normalization layers in [13] to remove filters with smaller values.

%Other improvements also include methods that add additional loss terms during training for the purpose of strengthening pruning. For instance, [18] enforce a clustering loss to make the filters in a cluster similar and prune those similar filters. [19] propose a greedy algorithm to choose channels layer by layer by constructing a certain optimization problem, providing another efficient mechanism of structured pruning.

%These vary from the most straightforward, norm-based methods to complex, data-driven, and training-aware approaches, therefore illustrating the diversity in the area of structured pruning methods. This form of pruning targets the whole filters or channels, can achieve considerable model acceleration with very minimal performance degradation, and is, hence, a very effective tool in the optimization of neural networks for practical applications.

% sun2024novel,
\subsection{Feature Screening} %Feature selection has been a cornerstone of machine learning by reducing model size, improving model accuracy, and reducing overfitting ____ . It also serves as an inspiration for score-based pruning methods. A comprehensive survey Li et al., "Feature Selection: Basics to Applications," categorizes feature selection techniques into screening, wrapper, and embedded methods while exploring their applications across diverse domains. 
%Among them, screening methods gain their momentum due to computational efficiency and versatile adaptability. 
% A survey specifically focused on screening methods ____ conducted comprehensive experiments to show various screening methods' performance on both regression and classification tasks.
Screening methods feature selection methods have been extensively utilized in machine learning. A survey Li et al., "Feature Selection: Basics to Applications," that specifically focused on screening methods conducted comprehensive experiments to show various screening methods' performance on both regression and classification tasks. 
% ____ proposed a marginal utility measure screening method MI-SIS based on mutual information. The method showed promising performance in improving the classification accuracy of models built for patient voice data. 
In Liu et al., "Screening-based Feature Selection: A Comparative Study," an intrusion detection model that incorporated F-statistics screening method and decision trees was introduced. It showed that simpler models with equivalent performance can be built using screening methods. 
Screening methods have also been applied to neural networks, especially in text-mining tasks. Zhang et al., "A Novel $\chi^2$ Scores based Feature Selection Method for Text Classification" introduced a $\chi^2$ scores based method that focuses on removing redundant text features. 
Recently more variants of the traditional screening methods were introduced to handle the newer challenges in modern data. A set of online screening methods was developed in Zhang et al., "Online Feature Screening via Fast Randomized Hadamard Transform" to handle high-dimensional and large-scale datasets.