\section{Related Work}
In this section, we will discuss existing works related to our framework.

%\subsection{Deep Neural Network Pruning} 
%The field of deep neural network pruning has made significant progress, with many studies and surveys providing detailed overviews of the techniques and their applications. For example, \cite{cheng2024survey} organizes pruning methods based on factors such as when and how pruning is applied, as well as their integration with other compression techniques. This survey also discusses recent developments, such as pruning for large language models and vision transformers, and provides insights into the trade-offs involved and future research directions. Similarly,  \cite{he2023structured} focuses on structured pruning methods, which aim to remove entire filters or channels to achieve practical acceleration. These methods are more hardware-efficient compared to unstructured pruning. The survey highlights key techniques such as filter ranking and dynamic execution strategies while comparing structured and unstructured approaches.

\subsection{Unstructured Weight Pruning} 
Unstructured weight pruning is aimed at reducing the number of parameters in a neural network by removing unimportant or redundant weights or connections. In recent ones, one of the simplest and most efficient strategy for pruning weights relies on the magnitudes \cite{han2015learning, Zhu2017ToPO, Kusupati2020SoftTW, Guo9533741, Guo9533833}. This assumes that weights of smaller magnitudes are less important and thus contribute less to the performance of a model. It is a computationally inexpensive approach, because one needs only the computation of either $l_1$ or $l_2$ norms, which, in turn, can be highly scalable for big models. On that aspect, \cite{guo2016dynamic} developed a pruning strategy that integrates the aspect of connection slicing during pruning to evade incorrect pruning. Connection slicing allows the pruning of parameters in whole groups or slabs and ensures the structural integrity of the network. Such methods thus tend to avoid removing key connections that are crucial for a model's good performance. \cite{Lin2020DynamicMP} and \cite{Savarese2019WinningTL} leverage gradient-based criteria for unstructured weight pruning by refining the pruning process by modifying aspects such as the behavior of forward propagation or approximating $l_0$ regularization. Unstructured weight pruning mostly is related to sparse matrix operation implementation. Typically, specialized libraries or hardware should be utilized to fully leverage unstructured pruning. This limits the applicability in certain practical scenarios.

%On early approaches, which have been proposed by \cite{lecun1990optimal} and \cite{hassibi1993second}, were based on the use of the Hessian matrix of the loss for determining unimportant weights. The idea of these techniques were to take second-order information to prune those weights which are not as relevant with respect to the overall performance. Despite being effective, it is computationally expensive because calculating Hessians is cumbersome. \cite{Weigend1990-wf} conducted weight pruning methods based on the minimum description length.

%In recent ones, one of the simplest and most efficient strategy for pruning weights relies on the magnitudes \cite{han2015learning, Zhu2017ToPO, Kusupati2020SoftTW, Guo9533741, Guo9533833}. This assumes that weights of smaller magnitudes are less important and thus contribute less to the performance of a model. It is a computationally inexpensive approach, because one needs only the computation of either $l_1$ or $l_2$ norms, which, in turn, can be highly scalable for big models. On that aspect, \cite{guo2016dynamic} developed a pruning strategy that integrates the aspect of connection slicing during pruning to evade incorrect pruning. Connection slicing allows the pruning of parameters in whole groups or slabs and ensures the structural integrity of the network. Such methods thus tend to avoid removing key connections that are crucial for a model's good performance. \cite{Lin2020DynamicMP} and \cite{Savarese2019WinningTL} leverage gradient-based criteria for unstructured weight pruning by refining the pruning process by modifying aspects such as the behavior of forward propagation or approximating $l_0$ regularization. 

%Unstructured weight pruning, However, also brings about challenges, most related to sparse matrix operation implementation. Typically, specialized libraries or hardware should be utilized to fully leverage unstructured pruning. This limits the applicability in certain practical scenarios.
%Score-based Neural Network Pruning is a heated research branch of neural network pruning. Score-based pruning methods assign importance scores to network components (e.g., weights, filters) and remove the least significant ones. For instance, Filter pruning via geometric median \cite{he2019filter} introduces a novel approach that selects filters based on their geometric median within a layer. This method identifies redundant filters with minimal impact on the networkâ€™s overall performance and has been shown to significantly improve computational efficiency in convolutional networks. Another important contribution is Network Slimming \cite{liu2017learning} which applies sparsity regularization during training to enforce channel-level sparsity. By ranking channels according to their scaling factors and pruning the less important ones post-training, this technique produces compact models with reduced computational costs.

\subsection{Structured Channel Pruning} 
Structured channel pruning offers much more significant acceleration of the model. The primary focus of structured channel pruning lies in removing whole channels, providing considerable gain during deployment, especially in hardware environments. 
There are methods to do structure pruning according to select the trainable channels. \cite{Luo2018AutoPrunerAE} select the target channels based on the importance score of neurons. \cite{liu2017learning} conduct channel pruning with regularization by scaling factors over Batchnorm (BN) layer. \cite{Ding2019CentripetalSF} generate target filters/channels for slimming down models, while \cite{Zhang2021CarryingOC} choose the channels contributing more to the network output. 

Recent advancements further introduce diverse strategies for evaluating channel importance. 
Kullback-Leibler (KL) divergence-based methods \cite{Luo2019NeuralNP} quantify the channel information loss between the original and pruned networks by measuring the statistical distance between their output distributions.
Simulated annealing-based approaches \cite{Nayman2019XNASNA} adapt the classical annealing-based optimization algorithm to the channel pruning context. Importance sampling techniques \cite{Baykal2019SiPPingNN} leverage statistical sampling theory to estimate channel importance efficiently.

%Some of the channel pruning methods have been found in few studies which have demonstrated the importance of convolutional filters. For example, [12] measure the sum of absolute weights to decide the importance of filters. [16] compute the percentage of zero activation after ReLU and prune those with a high percentage of zeros. While [17] have proposed an iterative two-step method for channel selection by using LASSO regression and least square reconstruction. Furthermore, the L1-norm constraint has been enforced on Batch Normalization layers in [13] to remove filters with smaller values.

%Other improvements also include methods that add additional loss terms during training for the purpose of strengthening pruning. For instance, [18] enforce a clustering loss to make the filters in a cluster similar and prune those similar filters. [19] propose a greedy algorithm to choose channels layer by layer by constructing a certain optimization problem, providing another efficient mechanism of structured pruning.

%These vary from the most straightforward, norm-based methods to complex, data-driven, and training-aware approaches, therefore illustrating the diversity in the area of structured pruning methods. This form of pruning targets the whole filters or channels, can achieve considerable model acceleration with very minimal performance degradation, and is, hence, a very effective tool in the optimization of neural networks for practical applications.

% sun2024novel,
\subsection{Feature Screening} %Feature selection has been a cornerstone of machine learning by reducing model size, improving model accuracy, and reducing overfitting \cite{yin2023igrf, kishor2024early}. It also serves as an inspiration for score-based pruning methods. A comprehensive survey\cite{dhal2022comprehensive} categorizes feature selection techniques into screening, wrapper, and embedded methods while exploring their applications across diverse domains. 
%Among them, screening methods gain their momentum due to computational efficiency and versatile adaptability. 
% A survey specifically focused on screening methods\cite{wang2019screening} conducted comprehensive experiments to show various screening methods' performance on both regression and classification tasks.
Screening methods feature selection methods have been extensively utilized in machine learning. A survey\cite{wang2019screening} that specifically focused on screening methods conducted comprehensive experiments to show various screening methods' performance on both regression and classification tasks. 
% \cite{zhou2022feature} proposed a marginal utility measure screening method MI-SIS based on mutual information. The method showed promising performance in improving the classification accuracy of models built for patient voice data. 
In \cite{shakeela2021optimal}, an intrusion detection model that incorporated F-statistics screening method and decision trees was introduced. It showed that simpler models with equivalent performance can be built using screening methods. 
Screening methods have also been applied to neural networks, especially in text-mining tasks. \cite{wang2021feature} introduced a $\chi^2$ scores based method that focuses on removing redundant text features. 
Recently more variants of the traditional screening methods were introduced to handle the newer challenges in modern data. A set of online screening methods was developed in \cite{wang2022online} to tackle challenges from large streaming data with sparsity and concept drifting properties. 

% \textcolor{red}{need add more here.} 
% Recently more variants of the traditional screening methods were introduced to handle the newer challenges in modern data. a set of online screening methods were developed in \cite{wang2022online} for large streaming data with sparsity and concept drifting property.