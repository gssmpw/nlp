\documentclass{article}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,commath}
\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{amsthm}
\usepackage{regexpatch,fancyvrb,xparse}

\usepackage{mathtools}

\newtheorem{prop}{Proposition}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\def\argmin{\mathop{\rm argmin}}
\def\diff{\mathop{\rm diff}}
\def\diag{\mathop{\rm diag}}
\def\RR{\mathbb R}
\def\ZZ{\mathbb Z}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bbeta}{{\boldsymbol{\beta}}}
\begin{document}

\title{Exploring Neural Network Pruning with Screening Methods}

\author{
Mingyuan Wang\\
\textit{Department of Statistics} \\
\textit{Florida State University}\\
Tallahassee, Florida, USA \\
mw15m@my.fsu.edu\\
\and
Yangzi Guo\\
\textit{Department of Mathematics} \\
\textit{Florida State University}\\
Tallahassee, Florida, USA \\
yg12@my.fsu.edu\\
\and
Sida Liu\\
\textit{Department of Statistics} \\
\textit{Florida State University}\\
Tallahassee, Florida, USA \\
sl15r@my.fsu.edu\\
\and
Yanwen Xiao\\
\textit{Department of Mathematics and Statistics} \\
\textit{Northwestern Polytechnical University}\\
Xi'an, Shaanxi, China \\
xiaoyanwen@mail.nwpu.edu.cn
}

\maketitle

\begin{abstract}
Deep neural networks (DNNs) such as convolutional neural networks (CNNs) for visual tasks, recurrent neural networks (RNNs) for sequence data, and transformer models for rich linguistic or multimodal tasks, achieved unprecedented performance on a wide range of tasks.
The impressive performance of modern DNNs is partially attributed to their sheer scale. 
The latest deep learning models have tens to hundreds of millions of parameters which makes the inference processes resource-intensive. The high computational complexity of these networks prevents their deployment on resource-limited devices such as mobile platforms, IoT devices, and edge computing systems because these devices require energy-efficient and real-time processing capabilities.
This paper proposes and evaluates a network pruning framework that eliminates non-essential parameters based on a statistical analysis of network component significance across classification categories. The proposed method uses screening methods coupled with a weighted scheme to assess connection and channel contributions for unstructured and structured pruning which allows for the elimination of unnecessary network elements without significantly degrading model performance. 
Extensive experimental validation on real-world vision datasets for both fully connected neural networks (FNNs) and CNNs has shown that the proposed framework produces competitive lean networks compared to the original networks. Moreover, the proposed framework outperforms state-of-art network pruning methods in two out of three cases. 
\end{abstract}


\section{Introduction}
Deep Neural Networks or Deep Learning have revolutionized numerous fields, achieving remarkable success in computer vision \cite{krizhevsky2012imagenet, redmon2016you, he2017mask}, natural language processing \cite{mikolov2013efficient, sutskever2014sequence, devlin2018bert}, predictive analytics \cite {guo2017deepfm, arora2022probabilistic, lim2021temporal}, and beyond. Much of the last decade has seen intense effort in developing increasingly complicated architectures in the computer vision image classification area, representing models such as VGG \cite{simonyan2014very}, ResNet \cite{he2016deep}, DenseNet \cite{huang2017densely}, ShuffleNet \cite{zhang2018shufflenet}, MobileNet \cite{howard2017mobilenets}, and VIT \cite{dosovitskiy2020image}. Despite these advances, the large computational and memory requirements of such models challenge real-time applications and resource-constrained device deployments, such as mobile phones and edge computing platforms. These environments require lightweight models that can balance fast inference speed and acceptable performance, underlined by an urgent need for effective model compression techniques.

These challenges have further spawned various approaches to optimize deep neural networks, such as network pruning \cite{han2015learning, liu2017learning}, knowledge distillation \cite {hinton2015distilling, tian2019contrastive}, weight quantization \cite{jacob2018quantization, wu2018training}, low-rank factorization \cite{sun2017svdnet, haeffele2015global}, neural sparsity encoding \cite{wu2018deep, dawer2020neural}, and neural architecture search \cite{zoph2016neural, liu2018darts}, to name a few. Of these, network pruning stands out as one of the effective model compression techniques through the systematic removal of redundant elements like weights, filters, or channels within models. Pruning reduces computational complexity and savings in storage while increasing the potential for inference speed; hence, it has been one practical mean of deploying deep learning models on resource-constrained devices.

Loosely speaking, pruning techniques could be divided into unstructured \cite{vadera2022methods} and structured \cite{lemaire2019structured}. In unstructured pruning, which includes magnitude-based pruning, it is individual weights with small absolute values that are removed, with the outcome being sparse weight matrices. However, unstructured pruning usually leads to irregular sparsity that is difficult for hardware acceleration. On the other hand, structured pruning removes complete filters, channels, or layers. The nature of the network remains intact-structured-and is more hardware- and library-friendly in most modern deep learning hardware. Among these structured pruning methods, channel pruning has recently gained much attention, since it can directly reduce both computation and storage costs without further post-processing.

Channel pruning \cite{he2023structured} methods can be categorized into static and dynamic ones. Static channel pruning \cite{luo2017thinet, he2017channel} permanently removes redundant channels based on the importance evaluated in training. Although it makes the model compact and lightweight, the fixed subnetwork often seriously limits the adaptability of the model to diverse input data. Dynamic channel pruning \cite{lin2017runtime, hua2019channel}, in contrast, performs an adaptive channel selection during inference based on inputs or intermediate feature maps and significantly enriches flexibility to notably boost the performance, although dynamic pruning often needs extra storage for several candidate sub-networks at an acceptable cost and is thus of limited practical use under resource-critical conditions.

The network pruning process typically begins with an either trained or untrained DNN, and two main strategies which are iterative pruning and one-time pruning can be employed. Iterative removal procedure is widely applied not only in neural network pruning fields but also in fields like feature selection \cite{Barbu_undated}, graph network optimization \cite{Yang2021-iq}, and beyond. Iterative pruning \cite{pmlr-v119-tan20a, Guo9533741} gradually removes unimportant elements over multiple steps, often guided by a decay or annealing function, allowing the network to adapt and recover through retraining after each step. In contrast, one-time pruning \cite{liu2017learning, Amersfoort2020SingleSS} eliminates all unimportant components in a single step, offering faster execution but requiring careful tuning to minimize performance degradation. A critical aspect of pruning is selecting an appropriate metric to identify elements for removal. Commonly used metrics include magnitude-based pruning, which removes smaller weights deemed less important; gradient-based pruning, which evaluates the sensitivity of weights or channels through their gradients during training; and correlation-based pruning, which targets redundant features or channels exhibiting high similarity. The choice of metric is closely tied to the pruning granularityâ€”weight pruning focuses on fine-grained optimization by removing individual weights, while channel pruning emphasizes structural sparsity by eliminating entire filters or channels.

In this paper, our major contributions can be summarized as follows:
\begin{itemize}
    \item We propose a novel network pruning technique that leverages statistical analysis to identify and eliminate non-essential parameters by evaluating the significance of network components across different classification categories.
    \item Our approach is inspired by feature screening methodologies to assess the contributions of connections and channels to the deep learning neural networks for both unstructured and structured pruning, which brings an effective removal of redundant network components without loss of model performance.
    \item Extensive experiments on real-world vision datasets such as MNIST and CIFAR validate the effectiveness of our method. The proposed screening method demonstrates promising performance on its own, though it may not consistently align with the results of state-of-the-art network pruning techniques. When combined with a magnitude-based approach, the hybrid method achieves superior or highly competitive results, highlighting the efficacy and complementary nature of the proposed screening strategy.
\end{itemize}

%The development of deep learning models and applications over the past decade has been remarkable, with significant efforts dedicated to creating increasingly sophisticated and comprehensive architectures. For instance, the VGG-16 network\cite{simonyan2014very}, introduced in 2014, contains approximately 138 million parameters, while the ResNet-50\cite{he2016identity} architecture, developed in 2015, is more efficient with about 25 million parameters. Despite their impressive performance, these models often demand substantial computational resources, posing challenges for deployment in resource-constrained environments such as mobile devices, where fast inference speed and acceptable performance are critical.

%To address these challenges, various techniques have been proposed to refine large deep learning models. Knowledge distillation transfers knowledge from a larger model to a smaller one. Quantization reduces parameter precision to save memory and computation. Low-rank factorization approximates weight matrices with lower-rank alternatives. Neural architecture search automates the design of efficient network structures. Among these methods, pruning has emerged as a particularly effective approach to reduce model size without significantly compromising performance by removing redundant components such as nodes, channels, or filters during training.

%Pruning methods are diverse, ranging from magnitude-based pruning, which removes weights with small absolute values, to structured pruning, which eliminates entire filters or channels. Score-based selection is a notable subcategory that assigns importance scores to network components and removes those deemed less critical. This approach parallels feature selection in traditional machine learning, where only the most relevant features are retained to build a model.

%Feature selection methods are typically categorized into screening, wrapper, and embedded techniques. Screening methods are computationally efficient and assign univariate importance scores to features independently of the learning process but may overlook complex relationships. Wrapper methods evaluate feature importance using machine learning algorithms but can be computationally expensive and prone to overfitting. Embedded methods integrate feature selection into the model training process for a balance between efficiency and performance. 

%In this paper, we propose a novel framework that applies screening feature selection principles to pruning deep neural networks. Screening methods were chosen for their simplicity, adaptability, and computational efficiency in identifying key components without requiring extensive resources. Our framework assigns uni-variate importance scores derived from screening methods to network elements (e.g., connections or channels) and removes less critical ones while preserving performance. To the best of our knowledge, there are no existing network pruning methods that incorporate the power of screening feature selection methods. \cite{fisher1934statistical} screening methods were integrated into our framework, [{XX,XX,XX,XX}]. Through experiments, we evaluate the effectiveness of this method and compare it against state-of-the-art pruning techniques. By leveraging the strengths of screening-based feature selection in the context of neural network pruning, our approach contributes to ongoing efforts to create more efficient and deployable deep learning models suited for resource-limited environments. [To be updated, waiting for Yangzi's experiment results]. 

\section{Related Work}
In this section, we will discuss existing works related to our framework.

%\subsection{Deep Neural Network Pruning} 
%The field of deep neural network pruning has made significant progress, with many studies and surveys providing detailed overviews of the techniques and their applications. For example, \cite{cheng2024survey} organizes pruning methods based on factors such as when and how pruning is applied, as well as their integration with other compression techniques. This survey also discusses recent developments, such as pruning for large language models and vision transformers, and provides insights into the trade-offs involved and future research directions. Similarly,  \cite{he2023structured} focuses on structured pruning methods, which aim to remove entire filters or channels to achieve practical acceleration. These methods are more hardware-efficient compared to unstructured pruning. The survey highlights key techniques such as filter ranking and dynamic execution strategies while comparing structured and unstructured approaches.

\subsection{Unstructured Weight Pruning} 
Unstructured weight pruning is aimed at reducing the number of parameters in a neural network by removing unimportant or redundant weights or connections. In recent ones, one of the simplest and most efficient strategy for pruning weights relies on the magnitudes \cite{han2015learning, Zhu2017ToPO, Kusupati2020SoftTW, Guo9533741, Guo9533833}. This assumes that weights of smaller magnitudes are less important and thus contribute less to the performance of a model. It is a computationally inexpensive approach, because one needs only the computation of either $l_1$ or $l_2$ norms, which, in turn, can be highly scalable for big models. On that aspect, \cite{guo2016dynamic} developed a pruning strategy that integrates the aspect of connection slicing during pruning to evade incorrect pruning. Connection slicing allows the pruning of parameters in whole groups or slabs and ensures the structural integrity of the network. Such methods thus tend to avoid removing key connections that are crucial for a model's good performance. \cite{Lin2020DynamicMP} and \cite{Savarese2019WinningTL} leverage gradient-based criteria for unstructured weight pruning by refining the pruning process by modifying aspects such as the behavior of forward propagation or approximating $l_0$ regularization. Unstructured weight pruning mostly is related to sparse matrix operation implementation. Typically, specialized libraries or hardware should be utilized to fully leverage unstructured pruning. This limits the applicability in certain practical scenarios.

%On early approaches, which have been proposed by \cite{lecun1990optimal} and \cite{hassibi1993second}, were based on the use of the Hessian matrix of the loss for determining unimportant weights. The idea of these techniques were to take second-order information to prune those weights which are not as relevant with respect to the overall performance. Despite being effective, it is computationally expensive because calculating Hessians is cumbersome. \cite{Weigend1990-wf} conducted weight pruning methods based on the minimum description length.

%In recent ones, one of the simplest and most efficient strategy for pruning weights relies on the magnitudes \cite{han2015learning, Zhu2017ToPO, Kusupati2020SoftTW, Guo9533741, Guo9533833}. This assumes that weights of smaller magnitudes are less important and thus contribute less to the performance of a model. It is a computationally inexpensive approach, because one needs only the computation of either $l_1$ or $l_2$ norms, which, in turn, can be highly scalable for big models. On that aspect, \cite{guo2016dynamic} developed a pruning strategy that integrates the aspect of connection slicing during pruning to evade incorrect pruning. Connection slicing allows the pruning of parameters in whole groups or slabs and ensures the structural integrity of the network. Such methods thus tend to avoid removing key connections that are crucial for a model's good performance. \cite{Lin2020DynamicMP} and \cite{Savarese2019WinningTL} leverage gradient-based criteria for unstructured weight pruning by refining the pruning process by modifying aspects such as the behavior of forward propagation or approximating $l_0$ regularization. 

%Unstructured weight pruning, However, also brings about challenges, most related to sparse matrix operation implementation. Typically, specialized libraries or hardware should be utilized to fully leverage unstructured pruning. This limits the applicability in certain practical scenarios.
%Score-based Neural Network Pruning is a heated research branch of neural network pruning. Score-based pruning methods assign importance scores to network components (e.g., weights, filters) and remove the least significant ones. For instance, Filter pruning via geometric median \cite{he2019filter} introduces a novel approach that selects filters based on their geometric median within a layer. This method identifies redundant filters with minimal impact on the networkâ€™s overall performance and has been shown to significantly improve computational efficiency in convolutional networks. Another important contribution is Network Slimming \cite{liu2017learning} which applies sparsity regularization during training to enforce channel-level sparsity. By ranking channels according to their scaling factors and pruning the less important ones post-training, this technique produces compact models with reduced computational costs.

\subsection{Structured Channel Pruning} 
Structured channel pruning offers much more significant acceleration of the model. The primary focus of structured channel pruning lies in removing whole channels, providing considerable gain during deployment, especially in hardware environments. 
There are methods to do structure pruning according to select the trainable channels. \cite{Luo2018AutoPrunerAE} select the target channels based on the importance score of neurons. \cite{liu2017learning} conduct channel pruning with regularization by scaling factors over Batchnorm (BN) layer. \cite{Ding2019CentripetalSF} generate target filters/channels for slimming down models, while \cite{Zhang2021CarryingOC} choose the channels contributing more to the network output. 

Recent advancements further introduce diverse strategies for evaluating channel importance. 
Kullback-Leibler (KL) divergence-based methods \cite{Luo2019NeuralNP} quantify the channel information loss between the original and pruned networks by measuring the statistical distance between their output distributions.
Simulated annealing-based approaches \cite{Nayman2019XNASNA} adapt the classical annealing-based optimization algorithm to the channel pruning context. Importance sampling techniques \cite{Baykal2019SiPPingNN} leverage statistical sampling theory to estimate channel importance efficiently.

%Some of the channel pruning methods have been found in few studies which have demonstrated the importance of convolutional filters. For example, [12] measure the sum of absolute weights to decide the importance of filters. [16] compute the percentage of zero activation after ReLU and prune those with a high percentage of zeros. While [17] have proposed an iterative two-step method for channel selection by using LASSO regression and least square reconstruction. Furthermore, the L1-norm constraint has been enforced on Batch Normalization layers in [13] to remove filters with smaller values.

%Other improvements also include methods that add additional loss terms during training for the purpose of strengthening pruning. For instance, [18] enforce a clustering loss to make the filters in a cluster similar and prune those similar filters. [19] propose a greedy algorithm to choose channels layer by layer by constructing a certain optimization problem, providing another efficient mechanism of structured pruning.

%These vary from the most straightforward, norm-based methods to complex, data-driven, and training-aware approaches, therefore illustrating the diversity in the area of structured pruning methods. This form of pruning targets the whole filters or channels, can achieve considerable model acceleration with very minimal performance degradation, and is, hence, a very effective tool in the optimization of neural networks for practical applications.

% sun2024novel,
\subsection{Feature Screening} %Feature selection has been a cornerstone of machine learning by reducing model size, improving model accuracy, and reducing overfitting \cite{yin2023igrf, kishor2024early}. It also serves as an inspiration for score-based pruning methods. A comprehensive survey\cite{dhal2022comprehensive} categorizes feature selection techniques into screening, wrapper, and embedded methods while exploring their applications across diverse domains. 
%Among them, screening methods gain their momentum due to computational efficiency and versatile adaptability. 
% A survey specifically focused on screening methods\cite{wang2019screening} conducted comprehensive experiments to show various screening methods' performance on both regression and classification tasks.
Screening methods feature selection methods have been extensively utilized in machine learning. A survey\cite{wang2019screening} that specifically focused on screening methods conducted comprehensive experiments to show various screening methods' performance on both regression and classification tasks. 
% \cite{zhou2022feature} proposed a marginal utility measure screening method MI-SIS based on mutual information. The method showed promising performance in improving the classification accuracy of models built for patient voice data. 
In \cite{shakeela2021optimal}, an intrusion detection model that incorporated F-statistics screening method and decision trees was introduced. It showed that simpler models with equivalent performance can be built using screening methods. 
Screening methods have also been applied to neural networks, especially in text-mining tasks. \cite{wang2021feature} introduced a $\chi^2$ scores based method that focuses on removing redundant text features. 
Recently more variants of the traditional screening methods were introduced to handle the newer challenges in modern data. A set of online screening methods was developed in \cite{wang2022online} to tackle challenges from large streaming data with sparsity and concept drifting properties. 

% \textcolor{red}{need add more here.} 
% Recently more variants of the traditional screening methods were introduced to handle the newer challenges in modern data. a set of online screening methods were developed in \cite{wang2022online} for large streaming data with sparsity and concept drifting property. 

\section{Proposed Method}
\label{sec:pagestyle}
We give a comprehensive and detailed description in this section, ranging from methodological underpinnings to operational mechanisms of the Network Pruning with Screening framework. First of all, we set up the theoretical framework based on the formal statement and mathematical formulation of the classic problem of network pruning. then, we derive and discuss the screening methodologies, showing its capabilities in quantifying the importance of various neural network components, such as weights, connections, and channels regarding categorical classification. After that, we give an elaborate discussion on the algorithm procedure and practical implementation of our pruning framework upon the two pruning paradigms: unstructured weight pruning and structured channel pruning.

\subsection{Network Pruning Formulation} \label{sec:screeningM}
Given a dataset $\mathcal{D} = \{(\mathbf{x_{i}}, y_i), i=1, \dots, N \}$, where $\mathbf{x_i}$ represents input features and $y_i$ is the corresponding target output, we aim to address the neural network pruning problem using constrained optimization. Let the parameters of the network be denoted by $\mathcal{W} = \{(\mathbf{W_j}, \mathbf{b_j}), j=1, \dots, L \}$ where $\mathbf{W_j}$ and $\mathbf{b_j}$ represents weight matrix and bias vector in each network layer. Let the loss function be denoted as $L(\mathcal{W})$. The pruning task can be expressed as
\begin{alignat}{1}
\min_{\mathcal{W}} \quad & L(\mathcal{W}) \ \ \ \ \ \ \ \ \ \ \ \mbox{s.t.}\quad ||\mathcal{W^{*}}||_0 \leq r||\mathcal{W}||_0
\label{cons_weight}
\end{alignat}
where the $L_0$ norm restricts  the number of non-zero parameters $||\mathcal{W^{*}}||_0$  to a fraction $r$ of $||\mathcal{W}||_0$, where $r$ a known positive float in range $(0, 1)$. In this approach, the individual parameters in the parameter space $\mathcal{W}$ are eliminated irrespective of the position, which ultimately leads to sparse weights irregularly across layers. Though the method provides good flexibility, to realize its benefit in terms of computation, it needs specialized hardware or sparse matrix libraries in most cases.

When pruning focuses on structured elements of CNNs such as filters or channels, we can reformulate the problem as
\begin{alignat}{2}
\min_{\mathcal{W}} \quad & L(\mathcal{W}) \ \ \ \ \ \ \ \ \ \ \ \mbox{s.t.}\quad ||\mathcal{C^{*}}||_0 \leq r||\mathcal{C}||_0
\label{cons_channel}
\end{alignat}
where $\mathcal{C} = \left\{ C_j, j=1,., M \right\}$ is the set of filters or channels within the network, and fraction $r$ bounds the number of non-zero filters or channels to $||\mathcal{C^{*}}||_0$. It therefore yields structured sparsity where zero parameters are localized within particular filters or channels. This makes the approach more hardware-friendly and thus much easier to realize in real-world systems.

Both weight-level and channel-level pruning problems benefit from the constrained formulations using the sparsity ratio $r$ that is interpretable and practical. In particular, $r$ directly controls the level of sparsity and allows fine-tuning of the pruned model more easily.

In this work, we consider the study of weight-level pruning for general networks and channel-level pruning for CNNs that involve Batch Normalization layers as a representative family of models where structured sparsity is induced due to these pruning methods. Solving these formulations would allow us to enhance computational efficiency while preserving predictive model performance.

%In this section, the procedure and mechanism of the Network Pruning with F-Statistic Screening framework are described in detail. First, the classical network pruning problem is formally introduced and mathematically formulated. Then the screening methods F-statistic score will be deduced and explained how it is good metric help assess the importance of weights, connections or channels to categorical classes. After that, we show the implementation of pruning procedure in our framework for two different types of neural network pruning, i.e. unstructured weight pruning and structured channel pruning..

\subsection{F-statistic Screening Methodology} \label{sec:screeningM}
In this study, we use F-statistic screening method to achieve pruning goals of selected neural networks. Our experimental experience shows that the F-statistic screen method can perform well to balance the efficiency and accuracy. It is compatible with the classification tasks this study focuses on. 

% \textbf{Chi-square score} The chi-square score is the chi-square statistic derived from a chi-square test. Commonly it is used to reflect the independence between two variables. Here it is used to reflect the dependency. 

% In network pruning, given a dataset $X$ whose rows are instances and columns are features (e.g. channels, nodes, weight productions, etc.), each feature $X_j$ can be discretized into discrete levels. Let $l=1,2,...,L$ indicate different levels in a feature $X_j$ and $c=1,2,...,C$ indicate different classes in label vector $Y$.
% Then $n_{{l_j}c}$ denotes the number of instances with label $c$ and level $l$ for feature $X_j$, $\hat{n}_{{l_j}c}$ denotes the estimated number of instances with label $c$ and having level $l$ for feature $X_j$, $\hat{n}_{{l_j}c}=\frac{n_{l_j}n_{c}}{n}$, where $n$ is the total number of instances, $n_{l_j}$ is the number of instances having level $l$ for feature $X_j$, and $n_{c}$ is the number of instances with label $c$. The chi-square statistic is then computed as:
% \begin{equation}
% \chi^2_j=\sum_{l=1}^L\sum_{c=1}^C\frac{(n_{{l_j}c}-\hat{n}_{{l_j}c})^2}{\hat{n}_{{l_j}c}}\label{eq:chi2}
% \end{equation}
% A higher chi-square score indicates a stronger relationship between the feature and the label distribution.


% \textbf{Mutual information} The mutual information measures the dependency of two variables. In general, given a variable $A$ and a variable $B$, where $S_A=\{A \in \RR\}$ and $S_B=\{B \in \RR\}$, the mutual information between $A$ and $B$ is described as:
% \vspace{5mm}
% \begin{equation}
% I(A,B)=\int_{S_A} \int_{S_B} p(A,B)\log\frac{p(A,B)}{p(A)p(B)}dA dB\label{eq:mutual_popul}
% \end{equation}
% where $p(A,B)$ is the joint probability density of $A$ and $B$, while $p(A)$ and $p(B)$ are the marginal probability density of $A$ and $B$. 

% In practice, the same discretizing procedure as that from the chi-square score introduction will be performed, where $l=1,2,...,L$ indicates different levels in a feature and $c=1,2,...,C$ indicates different classes. The mutual information between class label vector $Y$ and feature vector $X_j$ can then also be described as:
% \vspace{5mm}
% \begin{equation}
% I(X_j, Y)=\sum_{l=1}^L \sum_{c=1}^C p({X_j}_l,Y_c)\log\frac{p({X_j}_l,Y_c)}{p({X_j}_l)p(Y_c)}
% \label{eq:mutual_sample}
% \end{equation}
% where $p({X_j}_l,Y_c)$ is the joint probability of level ${X_j}_l$ and class $Y_c$, while $p({X_j}_b)$ and $p(Y_c)$ are the marginal probabilities. Features that have higher mutual information values have higher relevance with class label distribution shown in the data.

% \textbf{Fisher Score} By its definition, fisher score feature selection is a subset selection process in the feature space. The score is a measurement of between-class distances and within-class distances for all instances. The feature subset that has the highest fisher score is usually selected. The fisher score for a feature set is computed as:
% \vspace{5mm}
% \begin{equation}
% Fisher=Tr(D_b)(D_t+\gamma I)^{-1}\label{eq:fisherall}
% \end{equation}
% where $\gamma$ is a regularization term, $D_b$ is called between-class scatter matrix, $D_t$ is called total scatter matrix. It is obvious that with the number of features increasing the total number of feature subsets will increase drastically. This will be computationally inefficient. A heuristic is to compute the scores for each feature with respect to the Fisher score criterion.
% The Fisher score of a feature vector and the label vector is computed as:
% \vspace{5mm}
% \begin{equation}
% Fisher_j=\frac{\sum_{c=1}^Cn_c(\mu_c-\mu)^2}{\sum_{c=1}^Cn_c\sigma_c^2}\label{eq:fisherindividual}
% \end{equation}
% where $\mu$ and $\sigma$ are the mean and standard deviation of that feature, and $\mu_c$ is the mean of the feature values for observations with label $c$ and $n_c$ is the number of instances with label $c$.
% Features with larger Fisher scores are preferred.

\textbf{F-statistc Score} The F-statistc score method is based on the calculation of the $F$-statistic. It is essentially the ratio of the between-class variance and the within-class variance. In practice, a feature is split into groups by the class labels. Then the $F$-statistic is calculated to examine if there are statistically significant mean differences across groups.
For each feature $X_j$ and a label vector $Y$ with $C$ classes, the $F$-statistic can be calculated as:
\begin{alignat}{2}
F_j=\frac{\sum_{c=1}^{C} n_c  (\bar{{X_j}_c} - \bar{X_j})^2   /(C-1)}{\sum_{c=1}^{C} \sum_{i \in A_c} ({X_j}_{ci} - \bar{{X_j}_c})^2  /(N-C)}
\label{eq:F}
\end{alignat}
Where $n_c$ is the number of instances in class $c$, $N$ is the total number of all instances, $\bar{{X_j}_c}$ is the mean value of $X_j$ in class $c$ and $\bar{X_j}$ is the mean value of $X_j$. $A_c = \{i| \forall a_i \in{X_j}_c\}$. 
Generally speaking, the higher the $F$-statistic, the more separated the labels are by values of that feature and therefore the more relevant that feature is for classification.

\textbf{Online F-statistic Score} In modern data science, especially in the neural network setting, the magnitude of data can be enormous when it comes to either the number of instances or the number of features, or both. This often causes memory issues and computing time issues. One of the solutions is to utilize an online data processing method. Here we introduce an online F score method that only occupies less memory space and has potentially faster computing speed. For the clarity of expression, the feature index $j$ will be omitted. For example, the $\bar{{X_j}_c}$ in equation \ref{eq:F} will just be $\bar{X_c}$. $\bar{X_c}$ can be represented as $\frac{S_c}{n_c}$, where $S_c$ is the sum of feature values in class $c$ and $n_c$ is the number of instances in class $c$. Similarly, $\bar{X}$ is $\frac{S}{N}$, where $S$ is the sum of all feature values and $N$ is the number of all instances. Breaking up the summation part of the denominator of equation \ref{eq:F}: 
\begin{equation}
\begin{split}
\sum_{c=1}^{C} \sum_{i \in A_c} (X_{ci} - \bar{X_c})^2 &= \sum_{c=1}^{C} \sum_{i \in A_c} (X_{ci}^2-2X_{ci}\bar{X_c}+\bar{X_c}^2) \\
&= \sum_{c=1}^{C}(SS_c-2\bar{X_c}S_c+n_c\bar{X_c}^2)
\label{eq:Fonline}
\end{split}
\end{equation}
Where $SS_c$ is the sum of the squared feature values that fall into class $c$. 
Now instead of storing the entire dataset in the memory when calculating the F score, the dataset is iterated in batches. Parameters $SS_c$, $S_c$, $S$, $n_c$, and $N$ can be cumulatively updated by going through the batches. This aligns with the batch update scheme of usual neural network training. A feature matrix of size $(k,p)$ is generated each time the neural network model trains on the data batch, where $k$ is the number of instances in a batch and $p$ is the number of features (i.e. connections, weights, channels, etc.). This matrix along with label vector are used to update the parameters. Towards the end of each epoch, the F-score $S(w)$ will be calculated from these parameters. 
The $N$ and $C$ are scalers. $n_c$ is the number of instances in each class. $SS_c$, $S_c$, $S$ are 1-D arrays with the size of the number of total features. Thus the space is significantly compressed. The computing speed also increased due to faster indexing on batch.


\subsection{Algorithm Description}\label{sec:pruneM}
Following are some key principles of our algorithm design: 1) We integrate F-statistic screening technique together with the magnitude of parameters to establish a unified metric. The unified metric will evaluate the contribution of each parameter and channel to deep neural networks, which can enable both unstructured and structured pruning strategies; 2) A carefully designed pruning schedule is implemented to directly control the desired sparsity in the parameter space, offering flexibility to meet specific computational and storage requirements; 3) We systematically identify and remove the least important parameters or channels to streamline computations with minimal loss of accuracy. These ideas are captured by the proposed algorithms, which are prototyped in Algorithm 1 and 2. Starting from an untrained or pre-trained model.

\begin{algorithm}[htb]
	\caption{{\bf Weight-Level Screening Network Pruning (WLS)}}
	\label{alg:WLS}
	\begin{algorithmic}
		\STATE {\bfseries Input:} Training set $T=\{(\mathbf{x_i},y_i)\}_{i=1}^{n}$, pruning ratio $\{r_j\}_{j=1}^{L}$, pruning schedule $\{f(e, r_j),e=1,..,E\}_{j=1}^{L}$, an DNN model.
		\STATE {\bfseries Output:} Pruned DNN with parameters respect to pruning ratio $\{r_j\}_{j=1}^{L}$ in parameter space $\{\mathcal{W}_{j} | \cup\mathcal{W}_{j}=\mathcal{W} \ \& \cap\mathcal{W}_{j}=\varnothing\}_{j=1}^{L}$.
	\end{algorithmic}
	\begin{algorithmic} [1]
		\STATE If the DNN is not pre-trained, train it to a satisfying level.
		\FOR {$e = 1$ to $E$}
			\STATE Sequentially update $\mathcal{W} \leftarrow \mathcal{W} - 
            \eta\frac{\partial L(\mathcal{W})}{\partial \mathcal{W}}$ via backpropagation.
                \STATE Update parameters of screening score $S(w)$ using batch data.
			\FOR {$j=1$ to $L$}
			    \STATE Based on the pruning schedule, either continue training or prune the weights with $f(e, r_j)$ most relevant left in $\mathcal{W}_{j}$ based on ranking metric $\mathcal{M}$.
			\ENDFOR
		\ENDFOR
		\STATE Fine-tune the pruned DNN if needed.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htb]
	\caption{{\bf Channel-Level Screening Network Pruning (CLS)}}
	\label{alg:CLS}
	\begin{algorithmic}
		\STATE {\bfseries Input:} Training set $T=\{(\mathbf{x_i},y_i)\}_{i=1}^{n}$, pruning ratio $\{r_j\}_{j=1}^{L}$, pruning schedule $\{f(e, r_j),e=1,..,E\}_{j=1}^{L}$, an CNN model.
		\STATE {\bfseries Output:} Pruned DNN with parameters respect to pruning ratio $\{r_j\}_{j=1}^{L}$ in channel space $\{\mathcal{C}_{j} | \cup\mathcal{C}_{j}=\mathcal{C} \ \& \cap\mathcal{C}_{j}=\varnothing\}_{j=1}^{L}$.
	\end{algorithmic}
	\begin{algorithmic} [1]
		\STATE If the DNN is not pre-trained, train it to a satisfying level.
		\FOR {$e = 1$ to $E$}
			\STATE Sequentially update $\mathcal{W} \leftarrow \mathcal{W} - \eta\frac{\partial L(\mathcal{W})}{\partial \mathcal{W}}$ via backpropagation.
                \STATE Update parameters of screening score $S(w)$ using batch data.
			\FOR {$j=1$ to $L$}
			    \STATE Based on the pruning schedule, either continue training or prune the channels with $f(e, r_j)$ most relevant left in $\mathcal{C}_{j}$ based on ranking metric $\mathcal{M}$.
			\ENDFOR
		\ENDFOR
		\STATE Fine-tune the pruned CNN if needed.
\end{algorithmic}
\end{algorithm}

The WLS and CLS algorithms prune neural networks in an efficient way by iteratively or one-time removing the least important parameters or channels, guided by a pruning schedule. Whether to retain or discard a parameter or channel is determined by a predefined ranking metric $\mathcal{M}$, independent of the objective loss function $L$. The independence of the loss function makes the method special among many pruning techniques, which require changes in the loss and are thus not that suitable for pre-trained models. In the method, scaling-up simplicity and scalability make it effective to prune while maintaining network integrity.

Traditional neural network pruning methods generally do not take the influence of the training samples in the pruning process. On the other hand, F-statistic can be employed to measure the importance of every training sample, weights, and filters with their class labels. Thus, we define the ranking metric by fusing the traditional magnitude-based method with an F-statistic-based screening method which calculates the mutual information score. The combination of these two components in a weighted manner forms the final ranking metric. This would lead to much-informed pruning.

In the case of unstructured weight pruning, we leverage the screening score $S(w)$ which evaluates the significance of network weights across different classification categories with weight magnitude itself to define the ranking metric $\mathcal{M}$.
\begin{equation}
\begin{aligned}
\mathcal{M}(w) = \alpha \cdot S(w) + (1 - \alpha) \cdot |w|, w \in \mathcal{W}, \alpha \in(0, 1]
\label{M_unstructured}
\end{aligned}
\end{equation}
In the case of structured channel pruning, various methods have been suggested. One class of metrics relies the Batch Normalization (BN) scale parameters, as BN has become standard in modern deep convolutional neural networks for accelerating training and improving convergence. The transformation in a BN layer is defined as 
$$
BN(z_{in}) = \frac{z_{in}-\mu_\mathbf{L}}{\sqrt{\sigma^{2}_{\mathbf{B}}+\epsilon}}; z_{out}=\gamma\cdot BN(z_{in}) + \beta
\label{BN_scale}
$$
where $\mu_{\mathbf{B}}$ and $\sigma_{\mathbf{B}}$ denote the mean and variance over the mini-batch $\mathbf{B}$, and $\gamma$ and $\beta$ are trainable scale and shift parameters. \cite{liu2017learning} exploit the $\gamma$ parameters in BN layers for channel pruning, proposing that the $L_1$-norm of $\gamma$ be used to guide the pruning process. Here, we further leverage the screening score $S(\mathcal{C})$ which evaluates the significance of network channels across different classification categories with the $L_1$-norm of $\gamma$ to define the ranking metric $\mathcal{M}$.
\begin{equation}
\begin{aligned}
 \mathcal{M}(\mathcal{C}) = \alpha \cdot S(\mathcal{C}) + (1 - \alpha) \cdot |\gamma_\mathcal{C}|, \alpha \in(0, 1]
\end{aligned}
\label{eq:M_structured}
\end{equation}

Finally, we can do fine-tuning after pruning to recover any lost performance if needed. For unstructured weight pruning, we remove neurons that do not have any incoming or outgoing connections after pruning. For structured channel pruning, we eliminate convolution channels composed entirely of zero parameters to get a compact network that can do efficient inference.

%\textcolor{red}{need add more here, define what is screening score, and the range of alpha.} 
%\textcolor{red}{I mentioned $S(w)$ as F-score in section} \ref{sec:screeningM}, %\textcolor{red}{you can reference there}

%A central theme in the WLS algorithms is that they should systematically remove, in each pruning iteration, a certain number of the least important parameters or channels guided by an annealing schedule. This progressive procedure minimizes excessive noise introduced in the process of pruning and leads to smooth, controlled model complexity reduction.

%Unlike traditional layer-wise pruning, which manually controls the number of retained parameters in every layer with complicated operations, our approach directly regularizes the sparsity level of the entire parameter or channel space. This global pruning strategy not only simplifies the implementation but also reduces computational overhead substantially, since it does not need a layer-by-layer adjustment. Our approach thus prunes weights or channels of all layers simultaneously in a much streamlined and efficient pruning process.

%Inspired by the three-step training pipeline in \cite{han2015learning}, we developed a framework to incorporate screening feature selection methods into the pruning step.

%\subsubsection{Component Matrix} \label{sec:compMatrix}

%Screening methods require a data matrix and a label vector. The data matrix has size $n\times p$, where $n$ is the number of instances and $p$ is the number of features. Thus each row is an instance with $p$ features. The label vector has size $n\times1$. In our framework, we connect the concept of feature to the concept of neural network components (connection, channel, etc.) to generate our data matrix a.k.a component matrix. 

%\textbf{Fully Connected Network} If the neural network is a fully connected network, the weighted connections of each layer are used as features for the instance that is just processed. The weighted connection is the product between a connection and its corresponding layer input value. E.g. if the input vector has size 100 and each element in the input vector has 50 connections to the nodes in that layer, there are 5000 weighted connections in total. This will give us a component matrix of size $n\times k$, where $k$ is the number of weighted connections.

%\textbf{Convolutional Neural Network} In the scenario of implementing our framework to a CNN, the average value of each channel of the convolutional layer is calculated as a feature for the instance that is just processed. This will give us a component matrix of size $n\times k$, where $k$ is the number of channels.

%The component matrix is generated after the training of each batch. The detail of this procedure is shown in Algorithm\ref{alg:screenPrune} where $C_j$ is the component matrix for $j$th layer. The {\bf UPDATE} function takes the component matrix and label vector of the current batch as inputs. It then updates the parameters used to calculate the F score. At the end of each epoch, the {\bf SCORING} function was called to produce the F score. The pruning rate $P$ is utilized to decide what proportion of the components (e.g. channels, connections) with the highest importance score should be kept in line\ref{lst:line:P}. Functions $Len$ and $CONCAT$ perform length calculations and concatenation operations.

\section{Experiments}

In this section, we first demonstrate unstructured weight pruning on the classical FNN LeNet-300-100 using the MNIST dataset \cite{lecun-mnisthandwrittendigit-2010}. Next, we conduct experiments on structured channel pruning with DenseNet-40 \cite{huang2017densely} and two classical CNNs, ResNet-164 \cite{he2016deep}, using the CIFAR dataset \cite{krizhevsky2009learning}.

\subsection{Unstructured Weight Pruning on MNIST}

MNIST dataset \cite{lecun-mnisthandwrittendigit-2010} is one of the well-known benchmark datasets that is being widely used for evaluating machine learning and deep learning models. The MNIST dataset includes grayscale images of handwritten digits from 0 to 9. It contains 50K training samples, 10K validation samples, and 10K testing samples. Each image is 28x28 pixels in size. Its small size make it very popular for the testing and comparison of new algorithms in classification tasks. In this section, we will evaluate the WLS pruning method on LeNet-300-100.

LeNet-300-100 \cite{lecun1998gradient} is a classical FNN with two hidden layers consisting of 300 neurons and 100 neurons, respectively. It is a simple but effective architecture in non-convolutional tasks and thus has been widely used for exploring pruning strategies. LeNet-300-100 contains roughly 267K learnable parameters, both provide a wide range of settings to evaluate the flexibility and effectiveness of the proposed pruning approach.

We go about simultaneous training and pruning on LeNet-300-100. Inspiration by \cite{Dawer2017GeneratingCT}, we define an annealing function to drive the iterative pruning. The logistic annealing function exhibits behavior that provides a sensible trajectory, that it decays initially very slowly, with a gradual allowance of the neural network to get sufficiently trained while removing only a small portion of unimportant weights, so-called "junk weights." As the training progresses and the network becomes more stable, the logistic function further accelerates the pruning. Since by this stage, the network has already learned the underlying patterns, it is efficient to aggressively prune a big portion of the remaining junk weights rather fast. Finally, the function slows down when most of the unimportant weights have been pruned and transitions into a more gradual pruning phase. This way, it makes sure that the rest of the weights are carefully fine-tuned without destabilizing the model. Figure \ref{fig:schedule} shows that the function can dynamically balance training and pruning during the whole process. This allows an effective balancing between model accuracy conservation and high sparsity during the pruning process.

\begin{figure}[t]
\centering
\includegraphics[width=8.8cm]{Logistic_Annealing.png}
\caption{Logistic Annealing Schedule with decay rate in $\{2.0, 4.0, 6.0, 8.0\}$}
\label{fig:schedule}
\end{figure}

We use SGD to train and simultaneously prune LeNet-300-100 for $80$ epochs in total, with a minibatch size of $128$ on the MNIST dataset. The initial learning rate is $0.1$ and is divided by half every $10$ epochs. Weight decay is $1 \times 10^{-4}$ and Nesterov momentum \cite{Sutskever2013OnTI} with a value of $0.9$ (no dampening) is used to assist convergence.

\begin{table}[ht]
\centering
\begin{tabular}{llll}
\hline
\hline
Model  & Best Error & Params &Prune Rate   \\
\hline
Lenet-300-100 (Baseline)  &1.64\% &267k &- \\
Lenet-300-100 (Han \cite{han2015learning}) &1.59\% &22K &91.8\% \\
Lenet-300-100 (Barbu \cite{Guo9533741}) &1.57\% &17.4K &93.5\% \\
Lenet-300-100 (Ours) &\bf{1.51\%} &\bf{11.5K} &\bf{95.7\%} \\
\hline
\hline
\end{tabular}
\vspace{+2mm}
\caption{Unstructured weight pruning comparison on LeNet-300-100.}
\label{tab:MNIST_1}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{llllll}
\hline
\hline
Model  & Layer & Params. & Han\% & Barbu\%     &  Ours\%  \\
\hline
                &fc1   &236K  &8\%   &4.6\%     &\textbf{4.1}\%    \\
Lenet-300-100   &fc2   &30K   &9\%   &20.1\%    &\textbf{5.3}\%   \\
                &fc3   &1K    &26\%  &68.5\%    &\textbf{22.4}\%  \\
                &Total &267K  &8.2\% &6.5\%     &\textbf{4.3}\%    \\
\hline
\hline
\end{tabular}
\vspace{+2mm}
\caption{Layer by layer compression comparisons on LeNet-300-100.}
\label{tab:MNIST_2}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
\hline
\hline
LeNet  & $\alpha = 0.2$ & $\alpha = 0.4$ & $\alpha = 0.6$ & $\alpha = 0.8$ & $\alpha = 1.0$  \\
\hline
Best Error  & 1.57\% & \textbf{1.51\%} & 1.69\% & 2.40\% & 6.46\% \\
\hline
\hline
\end{tabular}
\vspace{+2mm}
\caption{Best error with Ranking metric by different $\alpha$ on LeNet-300-100.}
\label{tab:MNIST_3}
\end{table}

These are summarized in the following tables, which highlight the effectiveness of the proposed screening-based pruning. The best performance is at $\alpha = 0.4$, with $95.7\%$ of the weights pruned at a test error of only $1.51\%$. F-statisic screening pruning approach attains higher accuracy and also eliminates more redundant parameters compared to the pruning methods of \cite{han2015learning} and \cite{Guo9533741}.

In contrast, the poorest performance is $\alpha = 1.0$, reflecting that the reliance solely on F-statistic screening score may fail to capture true importance of each weight. While it is quite intuitive, explicit inclusion of weight magnitude into the ranking metric makes a major improvement in pruning effectiveness. As a matter of fact, this yields an optimum tradeoff between model compression and accuracy.

Table \ref{tab:MNIST_2} lists the layer-wise comparison of achieved compression rates against state-of-the-art methods. Although all three pruning methods give roughly the same overall accuracy, they generate distinctly different network structures. In LeNet-300-100, for example, the first layer is pruned the most, that probably implies that after the first full-connected layer reduces most of the useless features in this dataset, the output fully-connected layer keeps a number of parameters big enough to process these informative features further into classes.

\begin{figure}[t]
\centering
\includegraphics[width=8.8cm]{lenet_fc1.png}
\caption{The mask of the first fully connected layer of pruned Lenet-300-100. The blue dots are the remaining connections. The horizontal axis is the inputs. The vertical axis is the first layer nodes.}
\label{fig:lenet_fc1}
\end{figure}

We also plotted the first fully connected layer of Lenet-300-100 after pruning. Figure \ref{fig:lenet_fc1} shows the remaining connections in color. The image has 28 vertical bins which are the 28-pixel rows of a 28 by 28 input image. The bins towards the left and right sides relate to the top and bottom pixels in the image. They are the most pruned. For each bin, pixels towards the edges of the bin are also pruned primarily. The bin edges correspond to the left and right sides of the image. This suggests that the pruned model heavily focuses on the center of the image. It is where the number is. In addition, the bins close to the top are pruned slightly more than the bins close to the bottom. This suggests that the discriminative information leans more towards the bottom region.



\subsection{Structured Channel Pruning on CIFAR}

The CIFAR-10 dataset, as introduced by \cite{krizhevsky2009learning}, are among the popular benchmarks for image classification and object recognition. There are 60K natural color images of size $32 \times 32$. There are 50K training images and 10K test images. CIFAR-10 consists of 10 object categories, with 6K images per class.

We evaluate the effectiveness of the CLS pruning method for two deep neural network architectures: ResNet-164 \cite{he2016deep} and DenseNet-40 \cite{huang2017densely}. ResNet-164 here adapts its architecture on the CIFAR dataset. On the other hand, DenseNet-40 is a reduced-size version of the DenseNet, including 40 layers with a growth rate of 12.

Each is trained from scratch to establish baseline performances which are comparable in \cite{liu2017learning}. Training is done for $160$ epochs with a batch size of $64$. We adopt the SGD optimizer with an initial learning rate of $0.1$, weight decay $1 \times 10^{-4}$ and momentum $0.9$. The learning rate is divided by $10$ at $50\%$ and $75\%$ of the total training epochs. Furthermore, standard data augmentation techniques including normalization, random flipping and cropping are employed.

We then perform one-shot global pruning with a pre-defined pruning ratio $r$ and further conduct different learning schedules to fine-tune the pruned networks. The best results are summarized in following tables.

\begin{table}[ht]
\small
\centering
\begin{tabular}{lllll}
\hline
\hline
CNN &Model  & Error (\%) &Channels &Pruned   \\
\hline
            &Baseline \cite{liu2017learning}  &5.42 &12112   \\
ResNet-164  &Pruned \cite{liu2017learning}   &5.27 &4845  &60\%   \\
            &Pruned (Ours)   &\bf{5.18} &4845  &60\%   \\
\hline
            &Baseline \cite{liu2017learning}  &6.11  &9360  &- \\
DenseNet-40 &Pruned \cite{liu2017learning}          &5.65 &2808  &70\%   \\
            &Pruned \cite{Guo9533741})              &5.57 &2808  &70\%   \\
            &Pruned                                 &5.63 &2808  &70\%   \\
            
\hline
\hline
\vspace{+1mm}
\end{tabular}
\caption{Pruning performance results comparison on CIFAR-10.}
\label{tab:CIFAR10}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
\hline
\hline
ResNet  & $\alpha = 0.2$ & $\alpha = 0.4$ & $\alpha = 0.6$ & $\alpha = 0.8$ & $\alpha = 1.0$  \\
\hline
Best Error  & \textbf{5.18\%} & 5.20\% & 5.77\% & 6.52\% & 8.44\% \\
\hline
\hline
\end{tabular}
\vspace{+2mm}
\caption{Best error with Ranking metric by different $\alpha$ on ResNet-164.}
\label{tab:CIFAR10_2}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{llllll}
\hline
\hline
DenseNet  & $\alpha = 0.2$ & $\alpha = 0.4$ & $\alpha = 0.6$ & $\alpha = 0.8$ & $\alpha = 1.0$  \\
\hline
Best Error  & 5.78\% & \textbf{5.63\%} & 6.21\% & 6.89\% & 8.08\% \\
\hline
\hline
\end{tabular}
\vspace{+2mm}
\caption{Best error with Ranking metric by different $\alpha$ on DenseNet-40.}
\label{tab:CIFAR10_2}
\end{table}

These results of channel pruning applied to the network models for CIFAR-10 include two models, namely, ResNet164 with $60\%$ reduction and DenseNet-40 with a $70\%$ reduction, and have clearly depicted the performance of our proposed CLS algorithm. Our results show that the F-statistic based CLS pruning approach is competitive with previous methods proposed in \cite{liu2017learning} and \cite{Guo9533741}. Our method does not require a change in the training loss function, hence is quite efficient and easy to implement.

A similar trend to that of LeNet-300-100 with respect to the weighting factor $\alpha$ in ranking metric is depicted here. The weakest pruning performance is at $\alpha = 1.0$, showing that completely relying on the F-statistic screening score may not capture each channel's significance well. By contrast, including weight magnitude into the ranking metric improves overall pruning effectiveness. This gives a best test error of $5.18\%$ for ResNet-164 at $\alpha = 0.2$, while DenseNet-40 results are best for the same test error of $5.63\%$ at $\alpha = 0.4$.

\begin{figure}[tb]
\centering
\includegraphics[width=8cm]{channel_distribution.png}
\caption{A plot depicting the number of original and remaining channels after pruning across different BatchNorm2D layers of DenseNet-40. The x-axis represents sequential layer numbers, while the y-axis shows the number of channels.}
\label{fig:channel_distribution_plot}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=8cm]{weight_distribution.png}
\caption{A histogram showing the distribution of non-zero weight values after pruning. The x-axis represents weight magnitude, and the y-axis indicates the frequency of weights. The plot reveals that most remaining weights are concentrated between 0.2 and 0.5.}
\label{fig:weight_distribution_plot}
\end{figure}

We also visualized the impact of pruning on DenseNet-40 ($\alpha = 0.4$) on CIFAR-10 by generating two plots. Figure \ref{fig:channel_distribution_plot} illustrates the number of remaining channels in each layer. It reveals that two transition layers (layer 13 and 26) exhibit a lower pruning ratio compared to dense blocks layers. Pruning these layers aggressively could disrupt the flow of information through the network since transition layers maintain connectivity between dense blocks. Figure \ref{fig:weight_distribution_plot} shows the values of the remaining weights after pruning. It illustrates that the pruning process effectively remove less important, near-zero weights while retaining moderately high-magnitude weights critical for maintaining performance. These plots provide insights into how pruning affects the network's structure and parameter distribution. It also shows an evidence on our model's ability to keep the balance between sparsity and performance.

\section{Conclusion}

Deep neural networks have brought about a revolution in machine learning and, today, form a very strong machinery of learning complex representations from diverse domains. Despite their great success, the modern DNNs also suffer from severe computational challenges. The training and inference of modern DNNs become extremely resource-intensive due to tens to hundreds of millions of parameters. In this paper, a network-pruning technique has been presented that removes unnecessary parameters to alleviate these issues. Pruning has been done based on the statistical impact of every network parameter on classification categories. Contribution analysis for the connections and channels using screening methods in our proposed approach does unstructured and structured pruning, which gives the leverage of removing unnecessary elements without affecting model performance. Our extensive experimental evaluation using real-world vision datasets, including both FNNs and CNNs, shows that the proposed screening approach alone could give promising results but cannot always outperform state-of-the-art pruning methods. In contrast, the hybrid method with the magnitude-based pruning approach yields much better or at least highly competitive performance, thereby presenting a practical solution toward neural network optimization in efficiency.

\bibliographystyle{apalike}
\bibliography{refs}

\end{document}
