\section{Introduction}\label{sec:intro}
\definecolor{model}{rgb}{0.7098,0.2235,0.2549}
\definecolor{ode}{rgb}{0.7216,0.8078,0.5569}

At the core of denoising diffusion models \cite{sohl2015diffusion} lies the idea of corrupting clean data with \textit{\mbox{various}} levels of noise and learning to reverse this process. The remarkable success of these models has been partially underpinned by the concept of ``\textit{noise conditioning}'' \cite{sohl2015diffusion,song2019ncsn,ho2020denoising}: a single neural network is trained to perform denoising across all noise levels, with the noise level provided as a conditioning input. The concept of noise conditioning has been predominantly incorporated in diffusion models and is widely regarded as a critical component.

In this work, we examine the necessity of noise conditioning in denoising-based generative models. Our intuition is that, in natural data such as images, the noise level can be reliably estimated from corrupted data, making \textit{blind} denoising (\ie, without knowing the noise level) a feasible task. Notably, noise-level estimation and blind image denoising have been active research topics for decades \cite{stahl2000quantile,salmeri2001noise,rabie2005robust}, with neural networks offering effective solutions \cite{chen2018image,guo2019toward,zhang2023blind}. This raises an intriguing question: can related research on image denoising be generalized to denoising-based generative models?

\begin{figure}
   \vspace{0.8cm}
    \centering
    \begin{subfigure}[b]{0.42\linewidth}
        \centering
        \includegraphics[width=0.5\linewidth]{teaser_left.pdf}
        \caption{}
        \label{fig:teaser_left}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.42\linewidth}
        \centering
        \includegraphics[width=0.5\linewidth]{teaser_right.pdf}
        \caption{}
        \label{fig:teaser_right}
    \end{subfigure}
    \vspace{-.5em}
    \caption{
    \textbf{(a)} A denoising generative model takes a noisy data $\rvz$ and a noise level indexed by $t$ (such as $\sigma_t$) as the inputs to the neural network $\net_{\vtheta}$. \textbf{(b)} This work investigates the scenario of removing noise conditioning in the network.
 }
    \label{fig:main}
    \vspace{-1em}
\end{figure}

Motivated by this, in this work, we systematically compare a variety of denoising-based generative models --- \mbox{\textit{with and without}} noise conditioning. Contrary to common belief, we find that many denoising generative models perform robustly even in the absence of noise conditioning. In this scenario, most methods exhibit only a modest degradation in generation performance. More surprisingly, we find that some relevant methods---particularly flow-based ones \cite{lipman2023flow,liu2023flow}, which originated from different perspectives---can even produce \textit{improved} generation results \textit{without} noise conditioning. Among all the popular methods we studied, only one variant fails disastrously. Overall, our empirical results reveal that noise conditioning may \textit{not} be necessary for denoising generative models to function properly.

We present a theoretical analysis of the behavior of these models in the absence of noise conditioning. Specifically, we investigate the inherent uncertainty in the noise level distribution, the error caused by denoising without noise conditioning, and the accumulated error in the iterated sampler. Put together, we formulate an error bound that can be computed without involving any training, depending solely on the noise schedules and the dataset. Experiments show that this error bound correlates well with the noise-unconditional behaviors of the models we studied---particularly in cases where the model fails catastrophically, its error bound is orders of magnitudes higher.

Because noise-unconditional models have been rarely considered, it is worthwhile to design models specifically for this underexplored scenario. To this end, we present a simple alternative derived from the EDM model \cite{karras2022edm}. Without noise conditioning, our variant can achieve a strong performance, reaching an FID of 2.23 on the CIFAR-10 dataset. This result significantly narrows the gap between a noise-unconditional system and its noise-conditional counterpart (\eg, 1.97 FID of EDM).

Looking ahead, we hope that removing noise conditioning will pave the way for new advancements in denoising-based generative modeling. For example, only in the absence of noise conditioning can a score-based model learn a unique score function and enable the classical, physics-grounded Langevin dynamics.\footnotemark 
~Overall, we hope that our findings will motivate the community to re-examine the fundamental principles of related methods and explore new directions in the area of denoising generative models.

\footnotetext{Otherwise, it relies on the \textit{annealed} Langevin dynamics \cite{song2019ncsn}) that does not correspond to a unique underlying probability distribution independent of noise levels.
.}









  






    
    

