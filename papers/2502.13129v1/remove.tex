\section{Analysis of Noise-Unconditional Models}\label{sec:remove}

Based on the above formulation, we present a theoretical analysis of the influence of removing noise conditioning. Our analysis involves both the training objectives and the sampling process. We first analyze the effective target of regression at the training stage and its error in a single denoising step (\cref{subsec:target,subsec:ptz,subsec:error_effective}), and then give an upper bound on the accumulated error in the iterative sampler (\cref{subsec:final}). Overall, our analysis provides an error bound that is to be examined by experiments.

\begin{figure}[t]
    \centering
    \begin{overpic}[width=0.75\linewidth]{./illu.pdf}
        \centering
        \put(70,50){\scriptsize$r(\rvx_1,\rvepsilon_1,t_1)$}
        \put(78,31){\scriptsize$R(\rvz)$}
        \put(48,33){\scriptsize$\rvz$}
        \put(70,18){\scriptsize$r(\rvx_2,\rvepsilon_2,t_2)$}
    \end{overpic}
    \vspace{-0.8em}
    \caption{A given $z$ corresponds to multiple triplets $(\rvx, \rvepsilon, t)$.
 Here, we take Flow Matching \cite{lipman2023flow} as an example.
 On the left are the samples of $\rvepsilon$, and on the right are samples of $\rvx$.
 For a noisy sample $\rvz = (1-t)\rvx + t\rvepsilon$, it can be produced by different triplets. Each triplet gives a different regression target $r$. The effective target $R(\rvz)$ is the expectation of all possible $r$. 
 }
    \label{fig:illu}
\end{figure}


\subsection{Effective Targets}\label{subsec:target}

While the loss function is often written in a form like \cref{eq:gs_loss}, the underlying \textit{unique} regression target for $\net_{\vtheta}(\rvz|t)$ is \textbf{not} $r(\rvx,\rvepsilon,t)$. The function $\net_{\vtheta}(\rvz|t)$, which is \wrt $\rvz$ and $t$, is regressed onto \textit{multiple} $r$ values corresponding to different possible triplets $(\rvx,\rvepsilon,t)$ that produce the same $\rvz$ (see \cref{fig:illu}). Intuitively, the unique effective target, denoted as $R(\rvz|t)$ to emphasize its dependence on $\rvz$ and $t$, is the expectation of $r$ over all possible triplets.

Formally, optimizing the loss in \cref{eq:gs_loss} is equivalent to optimizing the following loss, where each term inside the expectation $\mbb{E}[\cdot]$ has a unique effective target:
\begin{align}\label{eq:eff_loss_wt}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvz \sim p(\rvz), t \sim p(t|\rvz) }\Big[\big\|\net_{\vtheta}(\rvz|t)-R(\rvz|t)\big\|^2\Big].
\end{align}
Here, $p(\rvz)$ is the marginalized distribution of $\rvz{:=}a(t)\rvx + b(t)\rvepsilon$ in \cref{eq:z_cal}, under the joint distribution $p(\rvx, \rvepsilon, t):=p(\rvx)p(\rvepsilon)p(t)$.\footnotemark~It is easy to show that:
\begin{align}
\label{eq:gs_loss2}
R(\rvz|t) = \mbb{E}_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)} \big[ r(\rvx,\rvepsilon,t) \big],
\end{align}
that is, the expectation over all $(\rvx, \rvepsilon)$ subject to the conditional distribution. One can show (\cref{app:effective}) that minimizing \cref{eq:eff_loss_wt} is equivalent to minimizing \cref{eq:gs_loss}.

\footnotetext{For simplicity, we consider $w(t){=}1$, which happens to be the case for all methods in \cref{tab:coefficients_train} when we expose $\net_{\vtheta}$ explicitly.}

\paragraph{Effective Targets without Noise Conditioning.}

Similarly, if the network $\net_{\vtheta}(\rvz)$ does not accept $t$ as the condition, its unique effective target $R(\rvz)$ should depend on $z$ only. In this case, the loss is:
\begin{align}\label{eq:eff_loss_wot}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvz \sim p(\rvz) }\Big[\big\|\net_{\vtheta}(\rvz)-R(\rvz)\big\|^2\Big],
\end{align}
where the unique effective target is:
\begin{align}
\label{eq:gs_loss3}
R(\rvz) = \mathbb{E}_{t\sim p(t|\rvz)} \big[R(\rvz| t)\big].
\end{align}
\cref{eq:gs_loss3} suggests that if the conditional distribution $p(t | \rvz)$ is close to a Dirac delta function, the \textit{effective target} would be the same with and without conditioning on $t$. If so, assuming the network is capable enough to fit the target, the noise-unconditional variant would produce the same output as the conditional one.

\subsection{Concentration of Posterior $p(t|\rvz)$}\label{subsec:ptz}

Next, we investigate how similar $p(t|\rvz)$ is to a Dirac delta function.
For \textit{high-dimensional} data such as images, it has been long realized that the noise level can be reliably estimated \cite{stahl2000quantile,salmeri2001noise,shin2005block}, implying a concentrated $p(t|\rvz)$. We note that the concentration of $p(t|\rvz)$ depends on data dimensionality: 


 \begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{ptz-afhq.pdf}
    \vspace{-1em}
    \caption{Posterior distribution $p(t|\rvz)$ given different noisy images $\rvz = (1-t_*)\rvx + t_*\rvepsilon$, for $t_*$ from 0.1 to 0.9. This plot is empirically simulated from 15,000 images in the AFHQ-v2 dataset with a size $64{\times} 64$ (see \cref{app:numerical}).}
    \label{fig:ptz}
\end{figure}


\begin{statement}{{Concentration of $p(t|\rvz)$}}\label{approx:var}
Consider a single datapoint $\rvx\in[-1,1]^d$, $\rvepsilon{\sim}\mathcal{N}(\bm0,\mI)$, $t{\sim} \mathcal{U}[0,1]$, and $\rvz = (1-t)\rvx + t\rvepsilon$ (the Flow Matching case). Given a noisy image $\rvz = (1-t_*)\rvx + t_*\rvepsilon$ produced by a given $t_*$, the variance of $t$ under the conditional distribution $p(t|\rvz)$, is:
    \begin{align}\label{eq:approx1}
        \mathrm{Var}_{t\sim p(t|\rvz)} [t] \approx \frac{t_*^2}{2d},
    \end{align}
when the data dimension $d$ satisfies $\frac{1}{d} \ll t_*$ and $\frac{1}{d} \ll 1-t_*$. (Derivation in \cref{app:delta_t})
\end{statement}
\vspace{.5em}
Intuitively, this statement suggests that \textit{high-dimensional} data will lead to $p(t|\rvz)$ of \textit{low variance}.
Because this analysis is based on simplified assumptions, we empirically run a simulation on a real dataset and plot $p(t|\rvz)$ (see \cref{fig:ptz}). The empirical distribution of $p(t|\rvz)$ is well concentrated. Moreover, a smaller $t^*$ leads to a more concentrated $p(t|\rvz)$, as also indicated by \cref{eq:approx1}.


\subsection{Error of Effective Regression Targets}\label{subsec:error_effective}


With $p(t|\rvz)$, we investigate the error between the effective regression targets $R(\rvz)$ and $R(\rvz|t)$. Formally, we consider:

\vskip -15pt

\begin{align}\label{eq:error}
 E(\rvz):=\mathbb{E}_{t\sim p(t|\rvz)}\Big[\|R(\rvz|t)-R(\rvz)\|^2\Big].
\end{align}
We show that this error $E(\rvz)$ is substantially smaller than the norm of $R(\rvz)$:




\begin{statement}{Error of effective regression targets}\label{approx:error}
Consider the scenario in ~\cref{approx:var} and the Flow Matching case. The error defined in \cref{eq:error} satisfies:
\begin{align}\label{eq:approx2}
 E(\rvz) \approx \frac{1}{2}(1+\sigma_{\ud}^2)
\end{align}
when the data dimension $d$ satisfies $\frac{1}{d} \ll t_*$ and $\frac{1}{d} \ll 1-t_*$. Here, $\sigma_{\ud}$ denotes the per-pixel standard deviation of the dataset.
(Derivation in \cref{app:R_z})
\end{statement}


Intuitively, \cref{approx:error} suggests that for sufficiently high-dimension $d$, the error $E(\rvz)$ is substantially smaller (${\approx}1$) than the L2 norm of the target $R(\rvz)$ (${\approx}d$). In our real-data verification, we find that $E(\rvz)$ is at the order of $1/10^3$ of $R(\rvz)$ (see \cref{app:numerical}). In this case, regressing to $R(\rvz|t)$ can be reliably approximated by regressing to $R(\rvz)$.



\subsection{Accumulated Error in Sampling}\label{subsec:final}

Thus far, we have been concerned with the error of a single regression step. In a denoising generative model, the sampler at inference time is iterative. We investigate the accumulated error in the iterative sampler.


To facilitate our analysis, we assume the network $\net_{\vtheta}$ is sufficiently capable of fitting the effective regression target $R(\rvz|t)$ or $R(\rvz)$. Under this assumption, we replace $\net_{\vtheta}$ in \cref{eq:gs_sampler} with $R$. This leads to the following statement:




\begin{statement}{Bound of accumulated error}\label{thrm:bound}
Consider a sampling process, \cref{eq:gs_sampler}, of $N$ steps, starting from the same initial noise $\rvx_0=\rvx_0'$. With noise conditioning, the sampler computes:
   \begin{align*}
    \rvx_{i+1} = \kappa_i \rvx_i + \eta_i R(\rvx_i| t_i) + \zeta_i \tilde{\rvepsilon}_i,
   \end{align*}
   and without noise conditioning, it computes:
   \begin{align*}
    \rvx_{i+1}' = \kappa_i \rvx_i' + \eta_i R(\rvx_i') + \zeta_i \tilde{\rvepsilon}_i.
   \end{align*}
   
Assuming ${\|R(\rvx_i'|t_i)-R(\rvx_i|t_i)\|}~/~{\|\rvx_i'-\rvx_i\|}\le L_i$ and $\|R(\rvx_i')-R(\rvx_i'| t_i)\|\le \delta_i$, it can be shown that the error between the sampler outputs $\rvx_N$ and $\rvx_N'$ is bounded: 
   \begin{align}\label{eq:bound}
    \|\rvx_N{-}\rvx_N'\| &\le A_0B_0+A_1B_1+\ldots+A_{N{-}1}B_{N{-}1},
   \end{align}
 \vspace{-1em} where:
   \begin{align*}
 \vspace{-1em}
 A_i = \prod_{j=i+1}^{N-1}(\kappa_i+|\eta_i|L_i) \quad\text{and}\quad B_i=|\eta_i|\delta_i.
   \end{align*}
   depend on the schedules and the dataset.
 (Derivation in \cref{app:proof_final_bound})
\end{statement}


Here, the assumption on $\delta_i$ can be approximately satisfied as per \cref{approx:error}.
The assumption on $L_i$ models the function $R(\cdot|t)$ as Lipschitz-continuous. Although it is unrealistic for this assumption to hold exactly in real data, we empirically find that an appropriate choice of $L_i$ can ensure the Lipchitz condition holds with high probability (\cref{app:AB}).


\cref{thrm:bound} suggests that the \textit{schedules} $\kappa_i$ and $\eta_i$ are influential to the estimation of the error bound. With different schedules across methods, their behavior in the absence of noise conditioning can be dramatically different.


\paragraph{Discussions.}
Remarkably, the bound estimation can be computed \textit{without} training the neural networks: it can be evaluated solely based on the schedules and the dataset. 

Furthermore, our analysis of the ``error'' bound implies that the noise-conditional variant is more accurate, with the noise-\textit{unconditional} variant striving to approximate it. In fact, there is no reason to assume that the former should be a more accurate generative model. Nonetheless, in experiments, we find that the noise-\textit{unconditional} case can outperform its noise-conditional counterpart in some cases.


