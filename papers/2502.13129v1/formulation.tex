\section{Formulation}\label{sec:formulation}

In this section, we present a reformulation that can summarize the training and sampling processes of various denoising generative models. The core motivation of our reformulation is to \textit{isolate} the neural network $\net_{\vtheta}$, allowing us to focus on its behavior with respect to noise conditioning.


\subsection{Denoising Generative Models}\label{subsec:gs}

\paragraph{Training objective.}
During training, a data point $\rvx$ is sampled from the data distribution $p(\rvx)$, and a noise $\rvepsilon$ is sampled from a noise distribution $p(\rvepsilon)$, such as a normal distribution $\mcal{N}(\bm 0,\mI)$. A noisy image $\rvz$ is given by:
\begin{align}
    \label{eq:z_cal}
    \rvz = a(t)\rvx + b(t)\rvepsilon.
\end{align} 
Here, $a(t)$ and $b(t)$ are schedule functions that are method-dependent.
The time step $t$, which can be a continuous or discrete scalar, is sampled from $p(t)$. Without loss of generality, we refer to $b(t)$, or simply $t$, as the \textit{noise level}.

In general, a denoising generative model involves minimizing a loss function that can be written as:
\begin{align}
    \label{eq:gs_loss}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[w(t)\big\|\net_{\vtheta}(\rvz|t)-r(\rvx,\rvepsilon,t)\big\|^2\Big].
\end{align}
\vspace{.5em}
Here, $\net_{\vtheta}$ is a neural network (\eg, U-Net) to be learned, $r(\rvx,\rvepsilon,t)$ is a \textit{regression target}, and $w(t)$ is a weight.
The regression target $r$ can be written as:
\begin{align}
    \label{eq:T_cal}
 r(\rvx,\rvepsilon,t) = c(t)\rvx+d(t)\rvepsilon,
\end{align}
where $c(t)$ and $d(t)$ are also method-specific schedule functions. Common choices of $r$ include $\rvepsilon$-prediction \cite{ho2020denoising}, $\rvx$-prediction \cite{salimans2022progressive}, or $\rvv$-prediction \cite{salimans2022progressive,lipman2023flow}.

The specifics of the schedule functions of several existing methods are in \cref{tab:coefficients_train}. It is worth noting that, in our reformulation, we concern the regression target $r$ with respect to the neural network $\net_{\vtheta}$'s \textit{direct} output.\footnotemark

\footnotetext{
For methods like EDM  where the network output is wrapped with a precondition, we rewrite the schedules to expose the term of $\net_{\vtheta}$ (see \cref{app:edm_coeff}). This network $\net_{\vtheta}$ is called the ``\textit{raw network}'' in EDM (see Eq.~(8) in \citep{karras2022edm}).
}




\begin{table}[t]
    \centering
    \caption{
 Schedules used by different models in our reformulation. Notations and details are in \cref{app:coefficients}.
 }
    \vspace{-.5em}
    \label{tab:coefficients_train}
    \tablestyle{16pt}{1.5}
    \scriptsize
    \begin{tabular}{c|ccc}
        \hline
        & \textbf{iDDPM, DDIM} & \textbf{EDM} & \textbf{FM} \\
        \hline
        $a(t)$ & $\sqrt{\bar{\alpha}(t)}$ & $\frac{1}{\sqrt{t^2+\sigma_\text{d}^2}}$ & $1-t$ \\
        $b(t)$ & $\sqrt{1{-}\bar{\alpha}(t)}$ & $\frac{t}{\sqrt{t^2+\sigma_\text{d}^2}}$ & $t$ \\
        $c(t)$ & $0$ & $\frac{t}{\sigma_\text{d}\sqrt{t^2+\sigma_\text{d}^2}}$ & $-1$ \\
        $d(t)$ & $1$ & $-\frac{\sigma_\text{d}}{\sqrt{t^2+\sigma_\text{d}^2}}$ & $1$ \\
        \hline
    \end{tabular}
\end{table}



\paragraph{Sampling.} Given trained $\net_{\vtheta}$, the sampler performs iterative denoising. Specifically, with an initial noise $\rvx_0 \sim \mathcal{N}(0, b(t_{\mx})^2\mI)$, the sampler iteratively computes:
\begin{align}
    \label{eq:gs_sampler}
    \rvx_{i+1} := \kappa_i\rvx_i + \eta_i\net_\rvtheta(\rvx_i|t_i) + \zeta_i\tilde{\rvepsilon}_i.
\end{align}
Here, a discrete set of time steps $\{t_i\}$ is pre-specified and indexed by $0 \leq i < N$. The schedules, $\kappa_i$, $\eta_i$, and $\zeta_i$, can be computed from the training-time noise schedules in \cref{tab:coefficients_train} (see their specific forms in \cref{app:coefficients}).
In \cref{eq:gs_sampler},
$\tilde{\rvepsilon}_i\sim \mathcal{N}(\bm 0,\mI)$ is a sampling-time noise that only takes effect in SDE-based solvers; there is no noise added in ODE-based solvers, \ie, $\zeta_i=0$.

\cref{eq:gs_sampler} is a general formulation that can encapsulate many first-order samplers, such as (annealed) Langevin sampling and Euler-based ODE solver. Higher-order samplers (\eg, Heun) can be formulated similarly with extra schedules. In this paper, our theoretical analysis is based on \cref{eq:gs_sampler}, and higher-order cases are evaluated empirically.

















\subsection{Noise Conditional Networks}\label{subsec:time_conditioning}

In existing methods, the neural network $\net_{\vtheta}(\rvz|t)$ is conditioned on the noise level specified by $t$. See \cref{fig:main} (left).
This is commonly implemented as $t$-embedding, such as Fourier \cite{tancik2020fourier} or positional embedding \cite{vaswani2017attention}. This $t$-embedding provides time-level information as an additional input to the network. Our study concerns the influence of this noise conditioning, that is, we consider $\net_{\vtheta}(\rvz)$ vs. $\net_{\vtheta}(\rvz | t)$. See \cref{fig:main} (right). Note that $\net_{\vtheta}(\rvz)$ or $\net_{\vtheta}(\rvz | t)$ involves all learnable parameters in the model, while the schedules ($a(t)$, $b(t)$, \etc) are pre-designed and not learned.

