\definecolor{lightgreen}{HTML}{B6DEC2}
\definecolor{lightyellow}{HTML}{FFFAC0}
\definecolor{lightred}{HTML}{FCCAC5}

\section{A Noise Unconditional Diffusion Model}\label{p:edmv1}

In addition to investigating existing models, we also design a diffusion model specifically tailored for noise \textit{unconditioning}.
Our motivation is to find schedule functions that are more robust in the absence of noise conditioning, while still maintaining competitive performance. To this end, we build upon the highly effective EDM framework \cite{karras2022edm} and modify its schedules.

A core component of EDM is a ``preconditioned'' denoiser:
\begin{align*}
 c_{\text{skip}}(t)\hat{\rvz}+c_{\text{out}}(t)\net_{\vtheta}\big(c_{\text{in}}(t)\hat{\rvz} \mid t\big)
\vspace{-1em}
\end{align*}
Here, $\hat{\rvz} := \rvx + t \rvepsilon $ is the noisy input before the normalization performed by $c_{\text{in}}(t)$,\footnotemark~which we simply set as $c_{\text{in}}(t) = \frac{1}{\sqrt{1+t^2}}$.
The main modification we adopt for the noise \textit{unconditioning} scenario is to set:
\begin{align*}
c_{\text{out}}(t) = 1.
\end{align*}
As a reference, EDM set $c_{\text{out}}(t) = \frac{\sigma_\text{d}t}{\sqrt{\sigma^2_\text{d}+t^2}}$ where $\sigma_\text{d}$ is the data std.
As $c_{\text{out}}(t)$ is the coefficient applied to $\net_{\vtheta}$, we expect setting it to a constant will free the network from modeling a $t$-dependent scale.
In experiments (\cref{subsec:analysis}), this simple design exhibits a lower error bound (\cref{thrm:bound}) than EDM. 
We name this model as \textbf{uEDM}, which is short for \textit{\mbox{(noise-)}unconditional EDM}.
For completeness, the resulting schedules of uEDM are provided in \cref{app:v1}. 


\footnotetext{To make notations consistent with our reformulation in \cref{eq:gs_loss}, we denote $\rvz=c_{\text{in}}(t)\hat{\rvz}$. See details in \cref{app:edm_coeff}.}

\section{Experiments}\label{method}

\paragraph{Experimental Settings.}

We empirically evaluate the impact of noise conditioning across a variety of models: 
\begin{itemize}[topsep=.5em,itemsep=0pt]
    \item \textbf{Diffusion}: iDDPM \cite{nichol2021iddpm}, DDIM \cite{song2021ddim}, ADM \cite{dhariwal2021diffusion}, EDM \cite{karras2022edm}, and uEDM (Sec.~\ref{p:edmv1})
    \item \textbf{Flow-based Models}: we adopt the implementation of Rectified Flow (1-RF) \cite{liu2023flow}, which is a form of Flow Matching \cite{lipman2023flow} (FM).
    \item \textbf{Consistency Models}: iCT \cite{song2024improved} and ECM \cite{geng2025consistency}.
\end{itemize}
\vspace{-.5em}
Our main experiments are on class-unconditional generation on CIFAR-10 \cite{krizhevsky2009CIFAR}, with extra results on ImageNet $32{\times}32$ \cite{deng2009imagenet}, and FFHQ $64{\times}64$ \cite{karras2019style}.
We evaluate Fr\'echet Inception Distance (FID) \cite{heusel2017FID} and report Number of Function Evaluations (NFE).
For a fair comparison, all methods are based on our re-implementation as faithful as possible (see \cref{app:faithfulness}): with and without noise conditioning are run in the same implementation for each method.

\setlength{\fboxsep}{1pt}
\begin{table}[t]
    \caption{
    \textbf{Changes of FID scores in the absence of noise conditioning}, for different methods on CIFAR-10. Here `w/o $t$' means without noise conditioning.
A color of {\fcolorbox{lightyellow}{lightyellow}{yellow}} denotes a non-disastrous (and often decent) degradation; {\fcolorbox{lightgreen}{lightgreen}{green}} denotes improvement; {\fcolorbox{lightred}{lightred}{red}} denotes failure. 
}
\label{tab:exp}
    \centering
 {{\setlength{\extrarowheight}{1.5pt}}
    \vskip -.5em
    \begin{adjustbox}{width=0.9\linewidth}
    \scriptsize
    \begin{tabular}{lcrc}
        \toprule
        \multirow{2}{*}{model} & \multirow{2}{*}{sampler} & \multirow{2}{*}{NFE} & FID \\
         & & & ~ w/ $t$ \quad $\rightarrow$ \quad w/o $t$\\
        \midrule
 iDDPM  & SDE & 500 & \cellcolor{lightyellow} 3.13 \quad $\rightarrow$ \quad 5.51 \\
 iDDPM ($\rvx$-pred)& SDE & 500 & \cellcolor{lightyellow} 5.64 \quad $\rightarrow$ \quad 6.33\\
        \midrule
        \multirow{3}{*}{DDIM} & ODE & 100 & \cellcolor{lightred}  3.99 \quad $\rightarrow$ \;\ 40.90\\
         & SDE & 100 & \cellcolor{lightyellow} 8.07 \quad $\rightarrow$ \;\ 10.85 \\
         & SDE & 1000 & \cellcolor{lightyellow} 3.18 \quad $\rightarrow$ \quad 5.41 \\
        \midrule
 ADM  & SDE & 250 & \cellcolor{lightyellow} 2.70 \quad $\rightarrow$ \quad 5.27 \\
        \midrule
        \multirow{2}{*}{EDM } & Heun & 35 & \cellcolor{lightyellow} \textbf{1.99} \quad $\rightarrow$ \quad 3.36 \\
         & Euler & 50 & \cellcolor{lightyellow} 2.98 \quad $\rightarrow$ \quad 4.55 \\
        \midrule
        \multirow{3}{*}{FM (1-RF)}  & Euler & 100 & \cellcolor{lightgreen} 3.01 \quad $\rightarrow$ \quad 2.61 \\
         & Heun & 99 & \cellcolor{lightgreen} 2.87 \quad $\rightarrow$ \quad 2.63 \\
         & RK45 & $\sim$127 & \cellcolor{lightyellow} 2.53 \quad $\rightarrow$ \quad 2.63 \\
        \midrule
 iCT & - & 2 & \cellcolor{lightyellow} 2.59 \quad $\rightarrow$ \quad 3.57 \\
 ECM & - & 2 & \cellcolor{lightyellow} 2.57 \quad $\rightarrow$ \quad 3.27 \\
        \midrule
 {uEDM} (Sec.~\ref{p:edmv1}) & Heun & 35 & \cellcolor{lightyellow} 2.04 \quad $\rightarrow$ \quad \textbf{2.23}\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
 }
\end{table}

\subsection{Main Observations}


\cref{tab:exp} summarizes the FID changes in different generative models, with and without noise conditioning, denoted as ``w/ $t$'' and ``w/o $t$''.
\cref{fig:samples} shows some qualitative results.
We draw the following observations:


    \textbf{(i)} Contrary to common belief, noise conditioning is \textit{not} an enabling factor for most denoising-based models to function properly. Most variants can work gracefully, exhibiting small but decent degradation ({\fcolorbox{lightyellow}{lightyellow}{yellow}}). 
    
    \textbf{(ii)} More surprisingly, some flow-based variants can achieve \textit{improved} FID ({\fcolorbox{lightgreen}{lightgreen}{green}}) after removing noise conditioning. 
In general, flow-based methods investigated in this paper are insensitive to whether we use noise conditioning or not.
We hypothesize that this is partially because FM's regression target is independent on $t$ (see \cref{tab:coefficients_train}: $c=-1$, $d=1$)

    \textbf{(iii)} The uEDM variant (Sec.~\ref{p:edmv1}) achieves a competitive FID of 2.23 without noise conditioning, narrowing the gap to the strong baseline of the noise-conditional methods (here, 1.99 of EDM, or 1.97 reported in \citet{karras2022edm}).

    \textbf{(iv)} Consistency Models (here, iCT and ECM), which are related to diffusion models but present a substantially different objective function, can also perform gracefully. While iCT was found highly sensitive to the subtleties of $t$-conditioning (see \citet{song2024improved}), we find that removing it does not lead to disastrous failure.
    
    \textbf{(v)} Among all variants we investigate, only ``DDIM w/ ODE sampler" results in a catastrophic failure ({\fcolorbox{lightred}{lightred}{red}}), with FID significantly worsened to 40.90. \cref{fig:samples} (a) demonstrates its qualitative behavior: the model \textit{is still able} to make sense of shapes and structures; it is ``overshoot" or ``undershoot", producing over-saturated or noisy results.
     
\begin{figure}[t]
    \centering
    \begin{minipage}{0.95\linewidth}
        \centering
        \includegraphics[width=\linewidth]{AiBi.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.8\linewidth}
        \centering
        \renewcommand{\arraystretch}{1.1}
 {\setlength{\extrarowheight}{1.5pt}}
        \scriptsize
        \begin{tabular}{lcc}
            \toprule
        \multirow{1}{*}{Model}  & \multirow{1}{*}{accum. bound} & FID w/ $t$ $\rightarrow$ w/o $t$ \\
        \midrule
 DDIM  & 3e6  & 3.99 \quad $\rightarrow$ \quad 40.90 \\
 EDM   & 1e3  & 2.34 \quad $\rightarrow$ \quad \;\  3.80            \\
 FM (1-RF)    & 1e2  & 3.01 \quad $\rightarrow$ \quad \;\  2.61            \\
 uEDM (Sec.~\ref{p:edmv1}) & 1e2  & 2.62 \quad $\rightarrow$ \quad \;\  2.66 \\
        \bottomrule
        \end{tabular}
    \end{minipage}
    \caption{\textbf{Error bound and the influence of noise conditioning.}
ODE with $N=100$ steps is applied for each variant. The plot shows the per-step error bound $A_i B_i$ in \cref{eq:bound}, and the table shows the accumulated error bound. The y-axis is log-scale.
}\label{fig:AB}
\vspace{-1em}
\end{figure}


\begin{figure*}[!ht]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{DDIM_compare.png}
        \caption{DDIM (FID: $3.99\to 40.90$)\\[1.4ex]}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{EDM_compare.png}
        \caption{EDM (FID: $1.99\to 3.36$)\\[1.4ex]}
    \end{subfigure}
    ~\\
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{FM_compare.png}
        \caption{FM (1-RF) (FID: $3.01\to 2.61$)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{EDM_v1_compare.png}
        \caption{uEDM (FID: $2.04\to 2.23$)}
    \end{subfigure}
    \vspace{-.5em}
    \caption{\textbf{Samples of noise-conditional vs. noise-unconditional models.}
    Samples are generated by (a) DDIM, (b) EDM, (c) FM (1-RF), and (d) uEDM, on the CIFAR-10 class-unconditional case. For each subfigure, the left panel is the noise-conditional case, and the right panel is the noise-unconditional counterpart, with the same random seeds. The change of FID is from ``w/ $t$'' to ``w/o $t$''. See also \cref{tab:exp} for more quantitative results.}
    \label{fig:samples}
    \vspace{-.5em}
\end{figure*}

\paragraph{Summary.}
Our experimental findings highlight that \emph{noise conditioning, though often helpful for improving quality, is not essential for the fundamental functionality of denoising generative models}.



\subsection{Analysis}\label{subsec:analysis}

\paragraph{Error bound.}\label{subsec:apply}
In \cref{fig:AB}, we empirically evaluate the error bound in \cref{thrm:bound} for different methods under a 100-step ODE sampler. The computation of the bound depends only on the schedules for each methods, as well as the dataset (detailed in \cref{app:AB}).

\cref{fig:AB} shows a strong correlation between the theoretical bound and the empirical behavior. Specifically, DDIM's catastrophic failure can be explained by its error bound that is orders of magnitudes higher. On the other hand, EDM, FM, and uEDM all have small error bounds throughout. This is consistent with their graceful behavior in the lack of noise conditioning.

These findings suggest that the error bound derived in our analysis serves as a reliable predictor of a model's robustness to the removal of noise conditioning. Importantly, the bound can be computed solely based on the model's formulation and dataset statistics, \textit{without} training the neural network. Consequently, it can provide a valuable tool for estimating whether a given denoising generative model can function effectively without noise conditioning, prior to model training.

\begin{figure}[t]
    \centering
    \vspace{-1em}
    \includegraphics[width=1.0\linewidth]{interpolate.pdf}
    \vspace{-1em}
    \caption{\textbf{Influence of Stochasticity on DDIM}, in the lack of noise conditioning. The level of stochasticity is specified by $\lambda$, with $\lambda=0$ denoting the ODE case. Here, the number of sampling steps is fixed as 500.
   }
    \label{fig:interpolate}
\end{figure}


\paragraph{Level of Stochasticity.}

In \cref{tab:exp}, DDIM only fails with the deterministic ODE sampler (the default sampler in \citep{song2021ddim}); it still performs decently with the SDE sampler (\ie, the DDPM sampler). With this observation, we further investigate the level of stochasticity in \cref{fig:interpolate}. 

Specifically, with the flexibility of DDIM \cite{song2021ddim}, one can introduce a parameter $\lambda$ that interpolates between the ODE and SDE samplers by adjusting $\eta_i$ and $\zeta_i$ in \cref{eq:gs_sampler} (see \cref{eq:ddim_coeff_lmd} in \cref{app:ddim_coeff}).
As shown in \cref{fig:interpolate}, increasing $\lambda$ (more stochasticity) consistently improves FID scores. When $\lambda=1$, DDIM behaves similarly to iDDPM. 

We hypothesize that this phenomenon can be explained by error propagation dynamics. Our theoretical bound in \cref{thrm:bound} assumes worst-case error accumulation, but in practice, stochastic sampling enables error cancellation. The ODE sampler's consistent noise patterns lead to correlated errors, while the SDE sampler's independent noise injections at each step promote error cancellation. This error cancellation mechanism can improve performance with increasing stochasticity, as further evidenced by iDDPM and ADM's results (\cref{tab:exp}) produced by SDE.

\paragraph{Alternative Noise-conditioning Scenarios.}

Thus far, we have focused on removing noise conditioning from existing models. This is analogous to blind image denoising in the field of image process. Following the research topic on noise level estimation, we can also let the network explicitly or implicitly predict the noise level. Specifically, we consider the following four cases (\cref{fig:four_variants}):

\begin{enumerate}[label=\textbf{(\alph*)},topsep=.5em,itemsep=0pt]
	\item The standard noise-conditioning baseline, which is what we have been comparing with. See \cref{fig:four_variants}(a).
	\item A noise-conditioning variant, in which the noise level is predicted by another network. In this variant, the noise predictor $P$ is a small network pre-trained to regress $t$. This predictor is then frozen when training $\net_{\vtheta}$, and $\net_{\vtheta}$ is conditioned on the predicted $t'$, rather than the true $t$. See \cref{fig:four_variants}(b).
	\item An ``unsupervised'' noise-conditioning variant. This architecture is exactly the same as the variant (b), except that the noise predictor $P$ is trained from scratch without any ground-truth $t$. If we consider $P$ and $\net_{\vtheta}$ jointly as a larger network, this also represents a design for noise-unconditional modeling. See \cref{fig:four_variants}(c).
	\item The standard noise-unconditional baseline, which is what we have been investigating. See \cref{fig:four_variants}(d).
\end{enumerate}

\begin{figure}[t]
    \begin{minipage}{1.0\linewidth}
        \centering
        \hfill
        \begin{subfigure}[b]{0.248\linewidth}
            \centering
            \includegraphics[width=0.695\linewidth]{ref_wt.pdf}
            \caption{ }\label{fig:wt_ref_explain}
        \end{subfigure}
        \hspace*{-0.1cm}
        \begin{subfigure}[b]{0.248\linewidth}
            \centering
            \includegraphics[width=0.88\linewidth]{predictor.pdf}
            \caption{ }\label{fig:pred_explain}
        \end{subfigure}
        \hspace{-0.2cm}
        \begin{subfigure}[b]{0.248\linewidth}
            \centering
            \includegraphics[width=0.695\linewidth]{joint.pdf}
            \caption{ }\label{fig:joint_explain}
        \end{subfigure}
        \hspace*{-0.25cm}
        \begin{subfigure}[b]{0.248\linewidth}
            \centering
            \includegraphics[width=0.695\linewidth]{ref_wot.pdf}
            \caption{ }\label{fig:wot_ref_explain}
        \end{subfigure}
        \hfill
    \end{minipage}
    \vspace{1em} \par 
    \begin{minipage}{1.0\linewidth}
 {{\setlength{\extrarowheight}{1.5pt}}
        \centering
        \begin{adjustbox}{max width=\linewidth}
        \scriptsize
        \hspace*{1.6cm}
        \renewcommand{\arraystretch}{1.3}
        \begin{tabular}{l|c|ccc}
            \Xhline{3\arrayrulewidth}
 Model & (a) & (b) & (c) & (d)  \\
            \hline
 iDDPM         & 2.69 & 4.91  & 4.95 & 4.68 \\
 EDM           & 1.99 & 3.27  & 3.39 & 3.36 \\
 FM (1-RF)     & 3.01 & 2.58  & 2.65 & 2.61 \\
            \Xhline{3\arrayrulewidth}
        \end{tabular}
        \end{adjustbox}
 }
    \end{minipage}
    \caption{
\textbf{Alternative Noise-conditional Scenarios.} \textbf{(a)} Noise-conditional baseline. \textbf{(b)} Noise-conditional, but on $t'$ predicted by a noise level predictor $P$. \textbf{(c)} Similar to (b), but the noise level predictor is not supervised and is trained jointly. \textbf{(d)} Noise un-conditional baseline. For iDDPM, EDM, and FM, all of (b), (c), and (d) perform similarly.
    }\label{fig:four_variants}
\end{figure}

\cref{fig:four_variants} compares all four variants.
Notably, consistent behavior is observed for all models (iDDPM, EDM, and FM) studied here: the results of (b), (c), and (d) are similar. This suggests that (b), (c), and (d) could be \textit{subject to the same type of error}, that is, the uncertainty of $t$ estimation. Note that even in the case of (b) where the noise predictor is pre-trained with the true $t$ given, its prediction cannot be perfect due to the small yet inevitable uncertainty in $p(t|\rvz)$ (see \cref{subsec:ptz}). As a result, the supervised pre-trained noise predictor (b) does not behave much different with the unsupervised counterpart (c).

\subsection{Extra Datasets and Tasks.}

Thus far, our experiments have been on the CIFAR-10 class-unconditional task.
To show the generalizability of our findings, we further evaluate class-unconditional generation on ImageNet $32{\times}32$, FFHQ $64{\times}64$, and class-conditional generation on CIFAR-10. See \cref{tab:otherds}.

The behavior is in general similar to that in our previous experiments. Specifically, removing noise conditioning can also be effective for other datasets or the class-conditional generation task. FM can exhibit \textit{improvement} in the absence of noise conditioning; EDM has a decent degradation, but experience no catastrophic failure. 

\begin{table}[t]
    \caption{Changes of FID scores in the absence of noise conditioning, on class-unconditional ImageNet $32{\times}32$ and FFHQ $64{\times}64$, and class-conditional CIFAR-10.}\label{tab:otherds}
    \centering{\setlength{\extrarowheight}{1.5pt}
    \vspace{-.5em}
    \begin{adjustbox}{max width=\linewidth}
    \scriptsize
    \begin{tabular}{lccc}
        \Xhline{3\arrayrulewidth}
        \multirow{2}{*}{Model} & \multirow{2}{*}{Sampler} & \multirow{2}{*}{NFE} & FID \\
         & & & w/ $t$ $\rightarrow$ w/o $t$\\
        \multicolumn{4}{l}{\textbf{ImageNet 32$\times$32}}\\\Xhline{3\arrayrulewidth}
 FM (1-RF) & Euler & 100 & 5.15 $\rightarrow$ 4.85\\

        \multicolumn{4}{l}{\textbf{FFHQ 64$\times$64}}\\\Xhline{3\arrayrulewidth}
 EDM & Heun & 79 & 2.64 $\rightarrow$ 3.59\\
        \multicolumn{4}{l}{\textbf{CIFAR-10 Class-conditional}}\\\Xhline{3\arrayrulewidth}
 EDM & Heun & 35 & 1.76 $\rightarrow$ 3.11\\
 FM (1-RF) & Euler & 100 & 2.72 $\rightarrow$ 2.55\\
    \end{tabular}
    \end{adjustbox}
}
\vspace{-0.8cm}
\end{table}

