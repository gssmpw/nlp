\setcounter{tocdepth}{3}
\tableofcontents
\allowdisplaybreaks
\begin{appendices}

\section{Details of Numerical Experiments}

In this section, we provide additional details on all our real dataset numerical experiments. By first computing the value of some relevant quantities (\eg the underlying time distribution $p(t|\rvz)$, effective target $R(\rvz|t), R(\rvz)$), we are able to evaluate $E(\rvz)$, which is average error introduced by removing noise conditioning. See \cref{app:compute_quant}.

As we introduce single data point assumption in our theoretical framework, we verify the accuracy of this assumption by comparing the empirical values of $p(t|\rvz)$ and $E(\rvz)$ with the theoretical values derived from our estimations. See \cref{app:numerical}.

Finally, in \cref{app:AB}, we show how we derive the numbers in \cref{fig:AB} by detailing on our estimation of bound values $A_i$ and $B_i$ in \cref{thrm:bound}.

\subsection{Computation of relavent quantities}\label{app:compute_quant}

We consider the data distribution $p_{\text{data}}$ constituted solely from the data points in the dataset: $p_{\text{data}}(\rvx) = \frac{1}{N}\sum_{i=1}^{N}\delta(\rvx-\rvx_i)$, where $\rvx_i\in \mbb{R}^{d}$ are the images in the dataset, and $\delta(\cdot)$ is the delta distribution. We denote $N$ as the number of data points in the dataset, and $d$ as the dimension of the image. 

\paragraph{Calculation of $p(t|\rvz)$ (\cref{subsec:ptz}).} 

First, we calculate $p(\rvz|t)$ by marginalizing over all the data points:
\begin{align}
    p(\rvz|t) = \int p(\rvz|t,\rvx)p(\rvx)\ud \rvx \notag = \frac{1}{N}\sum_{i=1}^N p(\rvz|t,\rvx_i).
\end{align}
The random variable $\rvz$ is given by \cref{eq:z_cal}, which implies
\begin{align}\label{eq:pztx_important}
    p(\rvz|t,\rvx) = \mcal{N}\left(\rvz;a(t)\rvx,b(t)^2\mI_d\right),
\end{align}
where $\mcal{N}(\rvz;\bm \mu,\mSigma)$ denotes the probability density function of the Gaussian distribution with mean $\bm \mu$ and covariance $\mSigma$. This leads to
\begin{align}
    p(\rvz|t) = \frac{1}{N}\sum_{i=1}^N \mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right),
\end{align}
and we can finally obtain:
\begin{align}\label{eq:ptz_important}
    p(t|\rvz) =\frac{p(t)}{p(\rvz)}p(\rvz|t)= \frac{p(t)\sum\limits_{i=1}^N \mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right)}{\displaystyle\int_{0}^1 p(t) \sum\limits_{i=1}^N \mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right)\ud t}.
\end{align}
Note that there is an integral to evaluate in \cref{eq:ptz_important}. In practice, the calculation is performed in a two-step manner for a fixed $\rvz$. In the first step, we use a uniform grid of 100 $t$ values in $[0,1]$ (\ie $t=0.00, 0.01,\ldots, 0.99$). We calculate the value of $p(t)p(\rvz|t)$ for each $t$ value. 

Typically, we observe that within an interval $[l,r]$ (where $0\le l<r\le 1$), the value of $p(t)p(\rvz|t)$ is significantly larger than for other $t\in [0,1]$ \footnotemark. We then approximate the integral as:

\footnotetext{Actually, this exactly matches our observation that $p(t|\rvz)$ is concentrated, since $p(t|\rvz)\propto p(t)p(\rvz|t)$ for a fixed $\rvz$.}
\begin{align}
    \int_{0}^1 p(t) \sum\limits_{i=1}^N \mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right)\ud t \approx \int_{l}^r p(t) \sum\limits_{i=1}^N \mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right)\ud t.
\end{align}
In the second step, we evaluate the integral by using a uniform grid of 100 $t$ values in $[l,r]$. This two-step procedure effectively reduces computational costs while maintaining low numerical error.

\paragraph{Calculation of $R(\rvz|t)$ and $R(\rvz)$ (\cref{subsec:target}).} 
By definition,
\begin{align}
    R(\rvz|t) := \mathbb{E}_{\rvx,\rvepsilon \sim p(\rvx,\rvepsilon|\rvz,t)} [r(\rvx, \rvepsilon, t)] \notag = \mathbb{E}_{\rvx\sim p(\rvx|\rvz,t)} \left[c(t)\rvx + d(t)\frac{\rvz-a(t)\rvx}{b(t)}\right].
\end{align}
Notice that $p(\rvx|\rvz,t)=\dfrac{p(\rvx)}{p(\rvz|t)}p(\rvz|\rvx,t)$, and $p(\rvz|\rvx,t)$ is given in \cref{eq:pztx_important}. Consequently, we have
\begin{align}\label{eq:Rzt_important}
    R(\rvz|t) = \frac{\frac{1}{N}\sum\limits_{i=1}^{n}p(\rvz|\rvx_i,t)\left[c(t)\rvx_i + d(t)\frac{\rvz-a(t)\rvx_i}{b(t)}\right]}{\frac{1}{N}\sum\limits_{i=1}^{n}p(\rvz|\rvx_i,t)}=\frac{d(t)}{b(t)}\rvz + \left(c(t)-\frac{a(t)d(t)}{b(t)}\right) \frac{\sum\limits_{i=1}^{n}\mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right)\rvx_i}{\sum\limits_{i=1}^{n}\mcal{N}\left(\rvz;a(t)\rvx_i,b(t)^2\mI_d\right)},
\end{align}
which can be then explicitly calculated by scanning over all the data points $\rvx_i$.


Once we obtain $R(\rvz|t)$, using \cref{theorem:effective2} and $p(t|\rvz)$, $R(\rvz)$ can be calculated by
\begin{align}\label{eq:Rz_important}
    R(\rvz)= \mathbb{E}_{t\sim p(t|\rvz)}[R(\rvz|t)] = \int_0^1 p(t|\rvz)R(\rvz|t)\ud t.
\end{align}

For the integration, we utilize the selected time steps in $[l,r]$ that were used when computing $p(t|\rvz)$. On another word, we ignore the parts where $p(t|\rvz)$ is negligible.

\paragraph{Calculation of $E(\rvz)$ (\cref{subsec:error_effective}).}

$E(\rvz)$ can be computed simply utilizing $p(t|\rvz), R(\rvz|t)$ and $R(\rvz)$:
\begin{align}\label{eq:Ez_important}
    E(\rvz) := \mbb{E}_{t\sim p(t|\rvz)}\|R(\rvz,t)-R(\rvz)\|^2 = \int_0^1 p(t|\rvz)\|R(\rvz|t)-R(\rvz)\|^2\ud t.
\end{align}

We again use the same time steps for estimating the integral term and reuse the terms of $p(\rvz|\rvx_i,t)$. This ensures computational efficiency while maintaining accuracy.


\subsection{Numerical Experiments}\label{app:numerical}

\paragraph{Verification of the single data point assumption.} 

Recall that \cref{approx:var,approx:error} assume that the dataset contains a single data point. In this section, we conduct numerical experiments on CIFAR-10 dataset to demonstrate that this assumption provides a reasonable approximation of the variance of $p(t|\rvz)$ and the error between the effective targets in practice. 

For both $p(t|\rvz)$ and $E(\rvz)$, we calculate their values by scanning the entire dataset as shown in the previous section, and compare them with our estimated theoretical values. 

For the CIFAR-10 dataset, we have $N=50000$ and $d=3\times 32^2 = 3072$, from which we can derive the estimated values of $p(t|\rvz)$ and $E(\rvz)$ in \cref{tab:error_comparison}.

As for empirical calculation, we compute the desired values via Monte Carlo sampling. Specifically, we select 5 time levels $t_*=0.1, 0.3, 0.5, 0.7, 0.9$. For each $t_*$, we sample $M=25$ noisy images $z_j$ by $\rvz_j = a(t_{*})\rvx_{I_j} + b(t_{*})\rvepsilon_j, j=1,2,\ldots, M$. Here, $\rvepsilon_j$ are independent samples from $\mcal{N}(\bm 0, \mI_d)$, and $I_j$ are independent random integers from $[1,N]$. We then compute $\mathrm{Var}_{t\sim p(t|\cdot)}[t]$, $\|R(\cdot)\|^2$ and $E(\cdot)$ for each $\rvz_j$ as we specified in \cref{app:compute_quant}. Finally, we average the $M$ values to obtain the empirical values along with their statistical uncertainties. Results are shown in \cref{tab:error_comparison}.

\begin{table}[t]
    \caption{The variance and $E$ values on the CIFAR-10 dataset. The empirical values are calculated by scanning the entire dataset, while the (theoretical) estimated values are derived from \cref{approx:var,approx:error}. For reference, the values for $\|R(\rvz)\|^2$ are also included to illustrate that $E(\rvz)$ is significant smaller than $\|R(\rvz)\|^2$. The results show that our approximation is generally accurate, except for the $E$ value in the very noisy case, where the single data point approximation becomes less accurate.}
    \label{tab:error_comparison}
    \centering
    \footnotesize

    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            \rule{0pt}{2ex}
            $t_*$ & \multicolumn{2}{c|}{$\mathrm{Var}_{t\sim p(t|\rvz)}[t]$} & \multicolumn{2}{c|}{$E(\rvz)$} & $\|R(\rvz)\|^2$ \\
            \hline
                & Empirical ($\times 10^{-4}$) & Estimation ($\times 10^{-4}$) & Empirical & Estimation & Empirical  \\
            \hline
            0.1 & $0.0143\pm 0.0002$ & $0.0163$ & $0.558\pm0.005$ & $0.628$ & $3894\pm 87$ \\
            0.3 & $0.1280\pm 0.0002$ & $0.1465$ & $0.561\pm0.006$ & $0.628$ & $3953\pm 102$ \\
            0.5 & $0.3695\pm 0.0004$ & $0.4069$ & $0.556\pm0.006$ & $0.628$ & $3878\pm 108$ \\
            0.7 & $0.7008\pm 0.0010$ & $0.7975$ & $0.564\pm0.005$ & $0.628$ & $3968\pm 88$ \\
            0.9 & $1.3085\pm 0.0007$ & $1.3184$ & $1.822\pm0.245$ & $0.628$ & $3310\pm 71$ \\
            \hline
    \end{tabular}
\end{table}

\cref{tab:error_comparison} shows that our estimations closely align with the observed values, except when $t$ gets very close to 1 (\ie in highly noisy images), where the single data point approximation becomes less precise. However, even in these cases, the estimated values remain within the same order of magnitude, providing acceptable explanations for the concentration of $p(t|\rvz)$ and the small error between the two effective targets.

\paragraph{Visualization of $p(t|\rvz)$.} 

We plot the value of $p(t|\rvz)$ in \cref{fig:ptz} for one $\rvz$ and $t_*$ from 0.1 to 0.9. This is carried out exactly in the same manner as the variance calculation for $t$, but with AFHQ-v2 dataset at $64{\times}64$ resolution for a better visualization quality. \cref{fig:ptz} functions as a reliable visual verification of the concentration of $p(t|\rvz)$.


\subsection{Evaluation of the bound values}\label{app:AB}

In this section, we provide additional experiment details on how we compute the bound values and present the plot of the bound terms $A_iB_i$ in \cref{fig:AB}. For reference, we also include separate plots for $A_i$ and $B_i$ in \cref{fig:A,fig:B} \footnotemark.

\footnotetext{An interesting fact in \cref{fig:B} is that, for EDM, there is a ``phase change'' at around $i=50$, which is caused by a non-smooth $\delta$ value. We hypothesize that this transition occurs at a noise level high enough that the data distribution can no longer be approximated as a point distribution, leading to a noticeable shift in behavior.}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Ai.pdf}
        \caption{The values of $A_i$ for different denoising generative models.}
        \label{fig:A}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Bi.pdf}
        \caption{The values of $B_i$ for different denoising generative models.}
        \label{fig:B}
    \end{subfigure}
    \caption{The bound applied on DDIM, EDM, FM and our uEDM model with a first-order ODE sampling process of $N=100$ steps. The figures visualize the different terms $A_i$ and $B_i$ in the bound.}
\end{figure}

Recall that in \cref{thrm:bound}, we define $A_i$ and $B_i$ as
\begin{align}
    A_i = \prod_{j=i+1}^{N-1}(\kappa_i+|\eta_i|L_i), B_i=|\eta_i|\delta_i.
\end{align}
Since $\kappa_i$ and $\eta_i$ are already given by the configurations for each model (see \cref{tab:coefficients}), we only have to evaluate $L_i$ and $\delta_i$. \cref{thrm:bound} requires the following condition to hold:
\begin{align}\label{eq:def_of_delta_L}
    \begin{cases*}
        \|R(\rvx_i')-R(\rvx_i'| t_i)\|\le \delta_i \\
        \dfrac{\|R(\rvx_i'| t_i)-R(\rvx_i| t_i)\|}{\|\rvx_i'-\rvx_i\|}\le L_i
    \end{cases*}
\end{align}

Now we are going to pick reasonable values of $\delta_i$ and $L_i$. As mentioned in \cref{subsec:final}, it is unrealistic for this assumption to hold exactly in real data due to bad-behaviors of the effective target $R$ when the noisy image is close to pure noise or pure data. However, we aim to make the conditions hold with high probability instead of considering worst case. As a result, our choice of $\delta_i, L_i$ corresponds to \textit{high probability case}.

\paragraph{Estimation of $\delta_i$.} 

We estimate $\delta_i$ using a maximum among different samples:
\begin{align}
    \delta_i = \max_{j} \|R(\rvz_{i,j}|t_i)-R(\rvz_{i,j})\|.
\end{align}
where we sample 10 different $\rvz$ from $p(\rvz|t)$. $R(\rvz|t)$ and $R(\rvz)$ values are computed as specified in \cref{app:compute_quant}. We use a \textit{maximum} value across different samples to ensure that the condition holds with high probability.

\paragraph{Estimation of $L_i$.} 

The condition of $L_i$ is similar to ``Lipchitz constant'' of $R(\cdot|t_i)$. Inspired by this, we evaluate the value of $\dfrac{\|R(\rvx_i'| t_i)-R(\rvx_i| t_i)\|}{\|\rvx_i'-\rvx_i\|}$ for $\rvx_i$ and $\rvx_i'$ that are close to each other. 

To model this, we sample $\rvx_i$ from $p(\rvz|t_i)$, and let $\rvx_i'=\rvx_i+\delta\tilde\rvepsilon$. Here, we pick $\delta=0.01$, and $\tilde\rvepsilon\sim \mcal N(\bm0, \mI)$ represents a random direction, which serves as a \textit{first-order estimation}.

Based on this, we sample 10 different pairs of $\rvx_i$ and $\rvx_i'$ for each $t_i$, and evaluate the max value of the ``Lipchitz constant''. In another word, we are calculating
\begin{align}
    L_i = \max_{j} \frac{\left\|R(\rvz_{i,j} + \delta\tilde{\rvepsilon}_{j}|t_i)-R(\rvz_{i,j}|t_i)\right\|}{\delta\|\tilde{\rvepsilon}_j\|}
\end{align}
for $j=1, 2, \ldots, 10$. Again, here we use the \textit{maximum} value across different samples to ensure that the condition holds with high probability.











\paragraph{Bound Values of the uEDM Model.} It is worth noticing that our uEDM model (see \cref{p:edmv1}) has a non-constant weighting $w(t)\ne 1$, which doesn't match our assumption when deriving the effective target $R(\rvz|t)$ and $R(\rvz)$ (as we did in \cref{subsec:target}). However, we choose not to introduce more mathematical complexities, and instead use the formulas above to calculate the bound value for uEDM. This implies that the bound for uEDM is no longer mathematically strict, but it can still serve as a reasonable intuition for the choice of our uEDM configuration.

\paragraph{Absolute magnitude of the Bound Values.} As shown in \cref{fig:AB}, one might observe that the magnitude of the bound (around $10^2$ to $10^6$) is actually significantly larger compared to the typical magnitude of $\|\rvx_N\|$ (which is around $\sigma_{\ud}\sqrt{d}\sim 10^1$). We hypothesize that this is because of the following two reasons: 

(1) We are assuming the error accumulating on each step, which might not be the case in practice, as studied in our discussion of SDE samplers in \cref{subsec:analysis}. 

(2) When the noisy image approaches clean data, some properties of the effective target $R$ become bad (\eg the Lipchitz constant $L_i$ will be very big). This leads to a large error estimation, but in real cases, as the neural network will smooth the learned function, the error is typically smaller. If we consider ignoring the last 10 steps in our bound value, the bound will be in a reasonable range (approximately 10 for FM and uEDM, 140 for EDM, and $>10^5$ for DDIM).

\section{Additional Experimental Details}\label{app:exp}

\subsection{General Experiment Configurations}\label{app:general_exp}

We implement our main code base using Google TPU and the JAX \cite{jax2018github} platform, and run most of our experiments on TPU v2 and v3 cores. As the official codes are mostly provided as GPU code, we re-implemented most of the previous work in JAX. For the faithfulness of our re-implementation, please refer to \cref{app:faithfulness}.

\paragraph{FID Evaluation.} For evaluation of the generative models, we calculate FID \cite{heusel2017FID} between 50,000 generated images and all available real images without any augmentation. We used the pre-trained Inception-v3 model provided by StyleGAN3 \cite{karras2021stylegan3} and converted it into a model class compatible with JAX. As we have reproduced most of the results in \cref{app:faithfulness}, we believe that our FID calculation is reliable.

\paragraph{Noise Conditioning Removal.} When we refer to ``removing noise conditioning'', technically we set the scalar before passing into the time-embedding to zero. Alternatively, we can also set the embedded time vector to zero. The results turn out to have negligible differences.

\paragraph{iDDPM ($\rvx$-pred).}

We design a $\rvx$-prediction version of iDDPM to show the generalizability of our theoretical framework. During training time, we simply change the target $r(\rvx,\rvepsilon,t)$ or $r(\rvx,\rvepsilon)$ to be $\rvx$. The sampling algorithm has to be modified accordingly, and we directly translate the $\rvx$-prediction to $\rvepsilon$-prediction by \cref{eq:z_cal}.


\paragraph{ADM.} In the original work of ADM, \citet{dhariwal2021diffusion} don't provide result on the CIFAR-10 dataset in class-unconditional settings. In our implementation of ADM, we keep the main method of learning $\rvepsilon$-prediction and the variance $\mSigma$ simultaneously, but employ it on the class-unconditional CIFAR-10 task. Notice that this ADM formulation is also \textit{not} included in our theoretical framework, but it still gives a reasonable result after removing noise conditioning (see \cref{tab:exp}).

\paragraph{Hyperparameters.}

A table of selected important hyperparameters can be found in \cref{tab:hyper}. For ICM and ECM we use the RAdam \cite{Liu2020On} optimizer, while for all other models we use the Adam \cite{kingma2014adam} optimizer. Also, we set the parameter $\beta_2$ to $0.95$ to stabilize the training process.

For all CIFAR-10 experiments, we used the architecture of NCSN++ in \cite{song2021scorebased}, with 56M parameters. For the ImageNet $32{\times}32$ experiment, we used the same architecture but a larger scale, with a total of 210M parameters.


We highlight that for all experiments, we only tune hyperparameters on the noise-conditional model, and then \textit{directly use exactly the same hyperparameters} for the noise-unconditional model and don't perform any further hyperparameter tuning. Thus, we expect that tuning these hyperparameters may further improve the performance of the noise-unconditional model.

\paragraph{Class-Conditional Generation on CIFAR-10.}

For the class-conditional CIFAR-10 experiments, we use exactly the same configurations and hyperparameters of EDM and FM with the unconditional generation case, except that we train the network with labels. For the conditioning on labels, we use the architecture in \citet{karras2022edm}. We do not apply any kind of guidance at inference time.

\paragraph{FFHQ $64{\times}64$ Experiments.}

For FFHQ $64{\times}64$ experiments, we directly use the code provided by \citet{karras2022edm} and run it on 8 H100 GPUs. We keep all hyperparameters the same as the original code in the experiments. For the removal of noise-conditioning, we simply set the $c_{\text{noise}}$ variable in the code to zero.

\begin{table}[ht] 
    \caption{Selected important hyperparameters in our main experiments. }\label{tab:hyper} 
    \centering 
    \begin{adjustbox}{max width=\linewidth}
    \renewcommand{\arraystretch}{1.5}
    \scriptsize
    \begin{tabular}{l|ccccccc} 
        \Xhline{3\arrayrulewidth} 
        Experiment & Duration & Warmup Epochs & Batch Size & Learning Rate & EMA Schedule & EMA Half-life Images & Dropout \\ 
        \hline 
        iDDPM \& ADM & 100M & 200 & 2048 & $8{\times} 10^{-4}$ & EDM & 50M & 0.15 \\ 
        iDDPM($\rvx$-pred) & 200M & 200 & 2048 & $1.2{\times} 10^{-3}$ & EDM & 50M & 0.15 \\ 
        DDIM & 100M & 200 & 512 & $4{\times}10^{-4}$ & EDM & 50M & 0.1 \\ 
        FM & 100M & 200 & 2048 & $8{\times}10^{-4}$ & EDM & 50M & 0.15 \\ 
        FM ImageNet32 & 256M& 64 & 2048 & $2{\times}10^{-4}$ & EDM & 50M & 0 \\ 
        EDM & 200M & 200 & 512 & $1{\times}10^{-3}$ & EDM & 0.5M & 0.13 \\ 
        uEDM (ours) & 200M & 200 & 512 & $4{\times}10^{-4}$ & EDM & 0.5M & 0.2 \\ 
        ICM \& ECM & 400M & 0 & 1024 & $1{\times}10^{-4}$ & Const(0.99993)& - & 0.3 \\ 
        \Xhline{3\arrayrulewidth} 
    \end{tabular} 
    \end{adjustbox} 
\end{table}
    
\subsection{Special Experiments}\label{app:general_config}

This section covers the specific experiment details in \cref{subsec:analysis}.

\paragraph{DDIM-iDDPM interpolate sampler.}

In the analysis of stochasticity, we examine VP diffusion models with cosine and linear $\bar{\alpha}(t)$ schedule with and without noise conditioning, using a customized interpolate sampler featured by $\lambda$, which is given by \cref{eq:ddim_coeff_lmd}. For the cosine schedule, we use 500 sampling steps, while for the linear schedule, we use 100 sampling steps. As discussed in \cref{app:ddim_coeff}, when $\lambda=1$, the model with the cosine schedule has the same setting as ``iDDPM'' in \cref{tab:exp}; when $\lambda=0$, the model with the linear schedule has the same setting as ``DDIM ODE 100'' in \cref{tab:exp}.

Results for the experiment are shown in \cref{tab:lambdaaaa}, and the result for the cosine schedule model is visualized in \cref{fig:interpolate}. From \cref{tab:lambdaaaa} one can also find a consistent trend that as $\lambda$ increase, the degradation of FID becomes smaller for the noise-unconditional model, regardless of the specific schedule of $\bar{\alpha}(t)$.



\begin{table}[t]
    \caption{Comparison of inference performance between noise-conditional and noise-unconditional models. The \emph{left} panel uses a cosine noise schedule, while the \emph{right} panel uses a linear noise schedule. Both panels compare the performance of noise-conditional and noise-unconditional settings across different values of the ablation sampler coefficient $\lambda$, ranging from 0.0 to 1.0.}\label{tab:lambdaaaa}
    {\setlength{\extrarowheight}{1.5pt}}

    \scriptsize
        \centering
        \begin{tabular}{l|cr|cr}
            \hline
            \toprule    
                & \multicolumn{2}{c|}{cosine schedule} & \multicolumn{2}{c}{linear schedule}\\
            $\lambda$ & FID & Change & FID & Change\\
            \midrule
            0.0  & 2.98 $\to$ 34.56 & +31.58 & \textbf{3.99} $\to$ 40.90 & +36.91\\
            0.2  &  2.60 $\to$ 30.34 & +27.74 &  4.09 $\to$ 36.04 & +31.95\\
            0.4  & \textbf{2.52} $\to$ 21.33 & +18.81 & 4.45 $\to$ 28.08 & +23.63\\
            0.6  & 2.59 $\to$ 12.47 & +$\ \ $9.88 & 4.95 $\to$ 18.32 & +13.37\\
            0.8  & 2.77 $\to$ $\ \ $7.53 & +$\ \ $4.76 & 5.90 $\to$ 10.36 & +$\ \ $4.46\\
            1.0 & 3.13 $\to$ $\ \ $\textbf{5.51} & \textbf{+$\ \ $2.38}& 8.07 $\to$ \textbf{10.85} & \textbf{+$\ \ $2.78} \\
            \bottomrule
        \end{tabular}
\end{table}


\paragraph{Alternative Architectures: noise level predictor and noise-like condition.}

In our experiment of Noise Level Predictor, we train a very lightweight network to predict the noise level $t$ given the input $\rvz$. To be specific, our predictor network only contains two convolutional layers with relu activation, followed by a global average pooling layer and a linear layer. The network has no more than 30K parameters, so it hardly affects the expressiveness of the whole model.


It's worth noticing that directly predicting the input $t$ is usually not desirable, as the value of $t$ may have different ranges for different models. Instead, for each specific model we choose a customized target for the prediction.

Training objectives for different models are shown below ($P_{\vphi}$ represents the predictor network):

\begin{itemize}

\item FM:
$
    \mcal{L} = \mbb{E}_{\rvx\sim p_{\text{data}}, \rvepsilon\sim\mcal N(\bm 0, \mI)}\mbb E_{t\sim \mcal U[0, 1]}\left[P_{\vphi}((1-t)\rvx+t\rvepsilon)-t\right]^2.
$ 

\item VP models:
$
    \mcal{L} = \mbb{E}_{\rvx\sim p_{\text{data}}, \rvepsilon\sim\mcal N(\bm 0, \mI)}\mbb E_{t\sim \mcal U[0, 1]}\left[P_{\vphi}(\sqrt{1-t}\rvx+\sqrt t\rvepsilon)-t\right]^2.
$

\item EDM models:
$
    \mcal{L} = \mbb{E}_{\rvx\sim p_{\text{data}}, \rvepsilon\sim\mcal N(\bm 0, \mI)}\mbb E_{t\sim \mcal \exp\mcal N(-1.2, 1.2^2)}\left[\frac{1}{P_{\vphi}(c_{\text{in}}(t)(\rvx+t\rvepsilon))+1}-\frac{1}{1+t}\right]^2.
$

\end{itemize}
Here, for EDM, we apply a transformation $y\to \frac{1}{1+y}$, mapping the original noise level $t\in (0,+\infty)$ in EDM to $[0, 1]$.

Experimentally, the MSE loss for all these settings all have a magnitude on the order of $10^{-4}$, which means that even our very lightweight predictor can predict the noise level with a mean error of about $0.01$.



In our Noise-like Conditioning experiment, the same lightweight network as mentioned above is connected to a noise-conditional U-Net, as visualized in \cref{fig:joint_explain}. This joint training architecture is noise-unconditional. The main difference between this experiment and the ``Noise Level Predictor'' experiment is that there is \textit{no supervision} on the intermediate output, so it may not be the noise level $t$ itself. 

For the experiments in \cref{fig:four_variants} column (b) and (c), we again use the same set of hyperparameters and configurations as the noise-conditional and noise-unconditional experiments (in columns (a) and (d)), to ensure a fair comparison. However, a subtle detail is that in the implementation of iDDPM in \cref{fig:four_variants}, the results for noise-conditional and noise-unconditional experiments are not the same as the results in \cref{tab:exp}. This is due to that we use $T=500$ instead of $T=4000$ for the training process.

\subsection{Our reimplementation faithfulness}\label{app:faithfulness}

As we have mentioned, we reimplement most of the models in the platform of JAX and TPU. 
\cref{tab:faithfulness} shows the comparison of our reimplementation and the original reported results. 


For some models, the reproduction doesn't meet our expectations. For example, since the official code for iCT is currently not open-sourced, we can only follow all configurations mentioned in their work, and get a best FID score of 2.59 (compared with the originally reported score of 2.46). For EDM on FFHQ-64, we directly run the given official code with the VP configuration on 8 H100 GPUs, but still can't reproduce the result. We suspect that the difference may come from random variance or different package versions used in the experiments. 

Note that iDDPM \cite{nichol2021iddpm} only reports results with 1000 and 4000 training steps. Due to computational constraints, we only evaluate the model with 500 sampling steps. Since our 500-step result has already been better than the 1000-step result reported in their work, we believe that we successfully reproduced the model.

After all, our goal is to compare the performance of noise-conditional and noise-unconditional models, and the absolute performance is not the focus of our work. Thus, even though we haven't fully reproduced some of the results, we believe that our comparison is still meaningful.

\begin{table}[t]
    \caption{Comparison of our reimplementation and the original reported results.}\label{tab:faithfulness}
    \centering
    \scriptsize
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{Sampler} & \multirow{2}{*}{NFE} & \multicolumn{2}{c}{FID}  \\
            & & &  Reproduced & Original \\
        \midrule
        \multicolumn{4}{l}{\textbf{CIFAR-10}}\\\Xhline{3\arrayrulewidth}
        \\[-1ex]
        \multirow{3}{*}{iDDPM \cite{nichol2021iddpm}} & \multirow{3}{*}{-} & 500 & {3.13} &  - \\
        & & 1000 & - & 3.29 \\
        & & 4000 & - & 2.90 \\
        \midrule
        \multirow{2}{*}{DDIM \cite{song2021ddim}} & ODE & 100 & 3.99 & 4.16 \\
            & ODE & 1000 & 2.85 & 4.04 \\
        \midrule
        EDM \cite{karras2022edm} & Heun & 35 & 1.99 & 1.97 \\
        \midrule
        {1-RF \cite{liu2023flow}}  & RK45 & $\sim$127 & {2.53} & 2.58 \\
        \midrule
        iCT \cite{song2024improved} & - & 2 & 2.59 & 2.46  \\
        \\[-0.5ex]
        \multicolumn{4}{l}{\textbf{CIFAR-10 Class-conditional}}\\\Xhline{3\arrayrulewidth}
        \\[-1ex]
        EDM \cite{karras2022edm} & Heun & 35 & 1.79 & 1.76 \\
        \\[-0.5ex]
        \multicolumn{4}{l}{\textbf{ImageNet 32$\times$32}}\\\Xhline{3\arrayrulewidth}
        \\[-1ex]
        FM \cite{lipman2023flow} & Euler & 100 & 5.15 & -  \\
        FM \cite{lipman2023flow} & RK45 & $\sim$125 & 4.30 & 5.02  \\
        \\[-0.5ex]
        \multicolumn{4}{l}{\textbf{FFHQ 64$\times$64}}\\\Xhline{3\arrayrulewidth}
        \\[-1ex]
        EDM \cite{karras2022edm} & Heun & 79 & 2.64 & 2.39\\
        \bottomrule
    \end{tabular}
\end{table}



\section{Supplementary Theoretical Details}\label{app:proofs}



\subsection{Proof of the Effective Target}\label{app:effective}

\begin{theorem}\label{theorem:effective1}
The original regression loss function with $t$ condition shown in \cref{eq:gs_loss} with $w(t)=1$
\begin{align*}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[\big\|\net_{\vtheta}(\rvz|t)-r(\rvx,\rvepsilon,t)\big\|^2\Big]
\end{align*}
is equivalent to the loss function with the effective target shown in \cref{eq:eff_loss_wt}
\begin{align*}
    \mcal{L}'(\vtheta) = \mbb{E}_{\rvz \sim p(\rvz), t \sim p(t|\rvz) }\Big[\big\|\net_{\vtheta}(\rvz|t)-R(\rvz|t)\big\|^2\Big]
\end{align*}
only up to a constant term that is independent of $\vtheta$, where 
\begin{align*}
    R(\rvz|t) = \mbb{E}_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)} \big[ r(\rvx,\rvepsilon,t) \big].
\end{align*}
Here, $p(\rvz)$ is the marginalized distribution of $\rvz{:=}a(t)\rvx + b(t)\rvepsilon$ in \cref{eq:z_cal}, under the joint distribution $p(\rvx, \rvepsilon, t):=p(\rvx)p(\rvepsilon)p(t)$.
\end{theorem}

\begin{proof}

The original regression loss function with $t$ condition can be rewritten as
\begin{align}
    \mcal{L}(\vtheta) &= \mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\mbb E_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)}\Big[\big\|\net_{\vtheta}(\rvz|t)-r(\rvx,\rvepsilon,t)\big\|^2\Big] \notag \\
    &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz|t)-\mbb{E}_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)} \big[ r(\rvx,\rvepsilon,t) \big]\big\|^2+\mbb V_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)}\big[ r(\rvx,\rvepsilon,t) \big]\Big] \notag \\
    &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz|t)-R(\rvz|t)\big\|^2+\textit{const}\Big] \notag \\
    &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz|t)-R(\rvz|t)\big\|^2\Big]+\textit{const}=\mcal L'(\vtheta)+\textit{const}.
\end{align}
This finishes the proof.
\end{proof}


\begin{theorem}\label{theorem:effective2}
The original regression loss function without noise conditioning
\begin{align*}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[\big\|\net_{\vtheta}(\rvz)-r(\rvx,\rvepsilon,t)\big\|^2\Big]
\end{align*}
is equivalent to the loss function with the effective target shown in \cref{eq:eff_loss_wot}
\begin{align*}
    \mcal{L}'(\vtheta) = \mbb{E}_{\rvz \sim p(\rvz)}\Big[\big\|\net_{\vtheta}(\rvz)-R(\rvz)\big\|^2\Big]
\end{align*}
only up to a constant term that is independent of $\vtheta$, where 
\begin{align*}
    R(\rvz) = \mathbb{E}_{t\sim p(t|\rvz)} \big[R(\rvz| t)\big].
\end{align*}
Defintions on $p(\rvz)$ and $p(t|\rvz)$ are the same as in \cref{theorem:effective1}.
\end{theorem}

\begin{proof}

The original regression loss function without noise conditioning can be rewritten as
\begin{align}
    \mcal{L}(\vtheta) &= \mbb{E}_{\rvz\sim p(\rvz)}\mbb E_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)}\Big[\big\|\net_{\vtheta}(\rvz)-r(\rvx,\rvepsilon,t)\big\|^2\Big] \notag \\
    &=\mbb{E}_{\rvz\sim p(\rvz)}\Big[\|\net_{\vtheta}(\rvz)-\mbb{E}_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)} \big[ r(\rvx,\rvepsilon,t) \big]\big\|^2+\mbb V_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)}\big[ r(\rvx,\rvepsilon,t) \big]\Big] \notag \\
    &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz)-\mbb{E}_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)} \big[ r(\rvx,\rvepsilon,t) \big]\big\|^2+\textit{const}\Big] \notag \\
    &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz)-\mbb{E}_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)} \big[ r(\rvx,\rvepsilon,t) \big]\big\|^2\Big]+\textit{const}
\end{align}
And notice that
\begin{align}
    \mbb{E}_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)} \big[ r(\rvx,\rvepsilon,t) \big] = \mbb{E}_{t\sim p(t|\rvz)} \mbb{E}_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)} \big[ r(\rvx,\rvepsilon,t) \big]=\mbb E_{t\sim p(t|\rvz)} R(\rvz| t).
\end{align}
So
\begin{align*}
    \mcal{L}(\vtheta) &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz)-\mbb{E}_{(\rvx, \rvepsilon, t) \sim p(\rvx, \rvepsilon, t | \rvz)} \big[ r(\rvx,\rvepsilon,t) \big]\big\|^2\Big]+\textit{const} \\
    &=\mbb{E}_{\rvz\sim p(\rvz), t\sim p(t|\rvz)}\Big[\|\net_{\vtheta}(\rvz)-R(\rvz)\big\|^2\Big]+\textit{const}= \mcal{L}'(\vtheta)+\textit{const}.
\end{align*}
This finishes the proof.
\end{proof}






\subsection{Approximation of the Variance of $p(t|\rvz)$}\label{app:delta_t}



\begin{ourcustomizedstatement}{Concentration of $p(t|\rvz)$}{approx:var}
    Consider a single datapoint $\rvx\in[-1,1]^d$, $\rvepsilon \sim \mathcal{N}(\bm0,\mI)$, $t\sim \mathcal{U}[0,1]$, and $\rvz = (1-t)\rvx + t\rvepsilon$ (as in Flow Matching). Given a noisy image $\rvz = (1-t_*)\rvx + t_*\rvepsilon$ produced by a given $t_*$, the variance of $t$ under the conditional distribution $p(t|\rvz)$, is:
    \begin{align}\label{eq:stmt_1_main}
        \mathrm{Var}_{t\sim p(t|\rvz)} [t] \approx \frac{t_*^2}{2d},
    \end{align}
under the situation that the data dimension $d$ satisfies $\frac{1}{d} \ll t_*$ and $\frac{1}{d} \ll 1-t_*$. 
\end{ourcustomizedstatement}

\begin{customproof}[Derivation\footnotemark] Consider a fixed $\rvz = (1-t)\rvx + t\rvepsilon$, where $\rvx$ is the fixed data point and $\rvepsilon\sim \mcal{N}(\bm 0, \mI)$. Without loss of generality, we assume that $\rvx = (x_1,x_2,\ldots,x_d)$ such that $-1\le x_d \le x_{d-1} \le \ldots \le x_1 \le 1$, and denote $\rvepsilon=(\epsilon_1, \epsilon_2, \ldots, \epsilon_d), \rvz=(z_1,z_2,\ldots,z_d)$.

\footnotetext{Here, ``derivation'' refers to an intuitive explanation rather than a formal proof. We will verify the validity of these statements empirically.}

To facilitate analysis, we introduce the following transformed variables\footnotemark:
\begin{align}
    \begin{cases*}
        \rvx':=(x_1-x_2,x_3-x_4,\ldots,x_{d-1}-x_d)  \\
        \rvepsilon':=(\epsilon_1-\epsilon_2, \epsilon_3-\epsilon_4,\ldots, \epsilon_{d-1}-\epsilon_d)   \\
        \rvz':=(z_1-z_2,z_3-z_4,\dots,z_{d-1}-z_d)
    \end{cases*}
\end{align}

and similarly define $\rvepsilon_*'$. Then, we have
\begin{align}
    \rvz' = (1-t)\rvx' + t\rvepsilon'.
\end{align}

\footnotetext{For simplicity and clarity, we assume that $d$ is an even number, which is generally the case for images. However, the proof can be naturally extended to the case where $d$ is odd.}

Notice that $\rvepsilon' \sim \mathcal{N}(0,2I_{\frac{d}{2}})$ and
\begin{align}
    \|\rvx'\|^2 = \sum_{i=1}^{\frac{d}{2}} (x_{2i-1}-x_{2i})^2 \le \sum_{i=1}^{\frac{d}{2}} 2(x_{2i-1}-x_{2i}) \le \sum_{i=1}^{\frac{d}{2}} 2(x_{2i-2}-x_{2i})=2(x_0-x_{d})\le 4,
\end{align}

where we defined $x_0:=1$ for convenience. As a result, we have
\begin{align}\label{eq:app1}
    \|\rvz'\|^2&= \left\|(1-t)\rvx' + t\rvepsilon' \right\|^2 \notag \\
    &=(1-t)^2\|\rvx'\|^2+t^2\|\rvepsilon'\|^2 + 2t(1-t)\rvx'\cdot \rvepsilon'.
\end{align}

Consider the three terms in \cref{eq:app1}. By the concentration property of a Gaussian random variable in high dimension, we know that
\begin{align}
    \|\rvepsilon'\|^2 \approx 2\times \frac{d}{2}=d
\end{align}

Thus, as $d\gg \frac{1}{t}, \frac{1}{1-t}$, we know that the second term in \cref{eq:app1} is of the order $\Theta(d)$, while the first and third term have order $o(d)$. We can then conclude
\begin{align}\label{eq:app2}
    t\approx \frac{\|\rvz'\|}{\|\rvepsilon'\|}.
\end{align}

Notice that \cref{eq:app2} can also be applied to $t_*$ and $\rvepsilon_*$, leading to the approximation $\|\rvz'\|\approx t_* \|\rvepsilon_*'\|$. As both $\|\rvepsilon'\|$ and $\|\rvepsilon_*'\|$ are highly concentrated around $\sqrt{d}$, it follows that $t$ concentrates at $t_*$. 

Next, we aim to quantify the concentration of $\|\rvepsilon'\|$ to further bound the variance of $t$. Importantly, this property depends solely on the standard Gaussian distribution, which we encapsulate in the following lemma.

\begin{lemma}\label{lemma:std_gaussian}
    Consider a standard Gaussian variable $\rva\sim \mcal{N}(\bm 0, \mI_d)$. Then 
    \begin{align}
        \lim_{d\to \infty} \mathrm{Var} \left[\frac{d}{\|\rva\|}\right] = \frac{1}{2}.
    \end{align}
\end{lemma}

For ease of reading, we will leave the proof of this lemma to the end. Now, we can use the lemma and estimate
\begin{align}\label{eq:app3}
    \mathrm{Var}_{t\sim p(t|\rvz)} [t] &= \mathrm{Var}_{\|\rvepsilon'\|} \left[\frac{\|\rvz'\|}{\|\rvepsilon'\|}\right] = \left(\frac{\|\rvz'\|}{d/2}\right)^2\cdot \left(\frac{1}{\sqrt{2}}\right)^2 \mathrm{Var}_{r:=\frac{\|\rvepsilon'\|}{\sqrt{2}}}  \left[\frac{d/2}{r}\right] \notag \\ 
     &\approx \frac{\|\rvz'\|^2}{d^2}\cdot \frac{1}{2}\cdot \frac{1}{2} = \frac{\|\rvz'\|^2}{4d^2}.
\end{align}

Finally, we notice that with a high probability, we have
\begin{align}\label{equation001}
    \|\rvz'\| \approx t_* \|\rvepsilon_*'\| \approx t_*\sqrt{\frac{d}{2}}.
\end{align}

Plugging \cref{equation001} into \cref{eq:app3}, we arrive at the desired result \cref{eq:stmt_1_main}. \end{customproof}

\begin{customproof}[Proof of \cref{lemma:std_gaussian}]
    The distribution of the norm of a standard Gaussian variable $r:=\|\rva\|$ is given by
    \begin{align}
        p(r)=\frac{1}{2^{\frac{d-2}{2}}\Gamma\left(\frac{d}{2}\right)}r^{d-1}e^{-\frac{r^2}{2}},
    \end{align}
    where $\Gamma$ is the gamma function. Then we can directly compute the variance of $d/r$:
    \begin{align}\label{eq:app4}
        \mathrm{Var}\left[\frac{d}{r}\right] &= \int_0^\infty \left(\frac{d}{r}\right)^2 p(r)\ud r - \left(\int_0^\infty \frac{d}{r}p(r)\ud r\right)^2 = \frac{d^2}{2}\left(\frac{2}{d-2}-\left(\frac{\Gamma\left(\frac{d-1}{2}\right)}{\Gamma\left(\frac{d}{2}\right)}\right)^2\right),
    \end{align}
    Finally, we use the Stirling's expansion for the gamma function to get
    \begin{align}\label{eq:app5}
        \frac{\Gamma\left(\frac{d-1}{2}\right)}{\Gamma\left(\frac{d}{2}\right)} &= \frac{e^{-\frac{d-1}{2}}\left(\frac{d-1}{2}\right)^{\frac{d}{2}-1}\left(1+\frac{1}{12}\cdot \frac{2}{d}+o(\frac{1}{d})\right)}{e^{-\frac{d}{2}}\left(\frac{d}{2}\right)^{\frac{d-1}{2}}\left(1+\frac{1}{12}\cdot \frac{2}{d}+o\left(\frac{1}{d}\right)\right)} = \sqrt{\frac{2}{d}}\left(1+\frac{3}{4d}+o\left(\frac{1}{d}\right)\right).
    \end{align}
    Plugging \cref{eq:app5} into \cref{eq:app4}, we yield
    \begin{align}
        \lim_{d\to \infty}\mathrm{Var}\left[\frac{d}{r}\right] &= \lim_{d\to \infty} \frac{d^2}{2}\left(\frac{2}{d}\left(1+\frac{2}{d}+o\left(\frac{1}{d}\right)\right)-\frac{2}{d}\left(1+\frac{3}{2d}+o\left(\frac{1}{d}\right)\right)\right)\notag \\
        &= \frac{1}{2},
    \end{align}
    finishing the proof.
\end{customproof}


\subsection{Approximation of $E(\rvz)$}\label{app:R_z}

\begin{ourcustomizedstatement}{Error of effective regression targets}{approx:error}
Consider a single datapoint $\rvx\in[-1,1]^d$, $\rvepsilon \sim \mathcal{N}(\bm0,\mI)$, $t\sim \mathcal{U}[0,1]$, and $\rvz = (1-t)\rvx + t\rvepsilon$ (as in Flow Matching). Define $R(\rvz)$ and $R(\rvz|t)$ with the Flow Matching configuration in \cref{tab:coefficients}. Given a noisy image $\rvz = (1-t_*)\rvx + t_*\rvepsilon$ produced by a given $t_*$, the mean squared error $E(\rvz)$ in \cref{eq:error} can be approximated by
\begin{align}\label{eq:approx2_derv}
    E(\rvz) \approx \frac{1}{2}(1+\sigma_{\ud}^2)
\end{align}
under the situation that the data dimension $d$ satisfies $\frac{1}{d} \ll t_*$ and $\frac{1}{d} \ll 1-t_*$. Here, $\sigma_{\ud}^2$ denotes the mean of squared pixel values of the dataset. 
\end{ourcustomizedstatement}
    
\begin{customproof}[Derivation]
We start by the definition of $E(\rvz)$:
\begin{align}
    E(\rvz)&:=\mathbb{E}_{t\sim p(t|\rvz)}\|R(\rvz|t)-R(\rvz)\|^2 \notag \\
    &= \mathbb{E}_{t\sim p(t|\rvz)}\left\|R(\rvz|t)-\mbb{E}_{t'\sim p(t'|\rvz)}[R(\rvz|t')]\right\|^2
\end{align}

Next, we compute $R(\rvz, t)$ using its definition under the Flow Matching configuration:
\begin{align}\label{equation002}
    R(\rvz| t) := \mbb{E}_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)} \big[ r(\rvx,\rvepsilon,t) \big]= \mbb{E}_{(\rvx, \rvepsilon) \sim p(\rvx, \rvepsilon | \rvz, t)} \big[&\rvepsilon - \rvx\big]= \frac{\rvz-\rvx}{t}.
\end{align}

Using \cref{equation002}, we obtain
\begin{align}
    E(\rvz) = \|\rvz-\rvx\|^2 \cdot \mathrm{Var}_{t\sim p(t|\rvz)}\left[\frac{1}{t}\right].
\end{align}

We now compute the two terms separately. For the first term, we can rewrite it as
\begin{align}\label{equation003}
    \|\rvz-\rvx\|^2 &= t_*^2 \|\rvx - \rvepsilon_*\|^2 \notag \\
    &\approx t_*^2 \left(\|\rvx\|^2 + \|\rvepsilon_*\|^2\right) \notag \\
    &\approx t_*^2 \left(d \sigma_{\ud}^2 + d\right) = t_*^2 d(1+\sigma_{\ud}^2).
\end{align}
Here, we employ the fact that $\rvx\cdot \rvepsilon_* \ll \|\rvx\|\|\rvepsilon_*\|$, and that $\|\rvepsilon_*\|\approx \sqrt{d}$ with high probability. Also, $\sigma_{\ud}^2=\|\rvx\|^2/d$, since we assume that the dataset contains only a single data point.

For the second term, note that the variance of $p(t|\rvz)$, given in \cref{approx:var}, is significantly smaller than the concentrated mean $t_*$ of $p(t|\rvz)$. Thus, we approximate the variance using a first-order expansion:
\begin{align}\label{equation004}
    \mathrm{Var}_{t\sim p(t|\rvz)}\left[\frac{1}{t}\right] &\approx \mathrm{Var}_{t\sim p(t|\rvz)}\left[\frac{1}{t_*} - \frac{(t-t_*)}{t_*^2}\right]= \frac{1}{t_*^4}\mathrm{Var}_{t\sim p(t|\rvz)}[t] \approx \frac{1}{t_*^2d}.
\end{align}
Combining \cref{equation003,equation004}, we get the estimation in \cref{eq:approx2_derv}.\end{customproof}


\subsection{Bound of accumulated error}\label{app:proof_final_bound}
\begin{ourcustomizedstatement}{Bound of accumulated error}{thrm:bound}
    \label{thm:bound}
    Starting from the same noise $\rvx_0=\rvx_0'$, consider a sampling process (\cref{eq:gs_sampler}) of $N$ steps, with noise conditioning:
    \begin{align*}
     \rvx_{i+1} = \kappa_i \rvx_i + \eta_i R(\rvx_i| t_i) + \zeta_i \tilde{\rvepsilon}_i
    \end{align*} and without noise conditioning:
    \begin{align*}
     \rvx_{i+1}' = \kappa_i \rvx_i' + \eta_i R(\rvx_i') + \zeta_i \tilde{\rvepsilon}_i.
    \end{align*}
 If ${\|R(\rvx_i'|t_i)-R(\rvx_i|t_i)\|}~/~{\|\rvx_i'-\rvx_i\|}\le L_i$ and $\|R(\rvx_i')-R(\rvx_i'| t_i)\|\le \delta_i$,
 it can be shown that the error between the sampler outputs $\rvx_N$ and $\rvx_N'$ is bounded: 
    \begin{align}
     \|\rvx_N - \rvx_N'\| &\le A_0B_0+A_1B_1+\ldots+A_{N{-}1}B_{N{-}1},
    \end{align}
  where
    \begin{align*}
  A_i = \prod_{j=i+1}^{N-1}(\kappa_i+|\eta_i|L_i), B_i=|\eta_i|\delta_i.
    \end{align*}
\end{ourcustomizedstatement}

\begin{proof}
    Define \(a_i := \kappa_i + |\eta_i|L_i\) and \(b_i := |\eta_i|\delta_i\). Then, we have:
    \begin{align}
        \|\rvx_{i+1}' - \rvx_{i+1}\| = \Big\|\kappa_i (\rvx_i' - \rvx_i) + \eta_i \left(R(\rvx_i') - R(\rvx_i | t_i)\right)\Big\|
    \end{align}
    as we assume that the same noise $\tilde{\rvepsilon}_i$ is added in the sampling process with and without noise conditioning. 
    
    Using the triangle inequality, this can be bounded as:
    \begin{align}\label{eq:bound_key}
    \|\rvx_{i+1}' - \rvx_{i+1}\| \leq \kappa_i \|\rvx_i' - \rvx_i\| + |\eta_i|\|R(\rvx_i') - R(\rvx_i' | t_i)\| + |\eta_i| \|R(\rvx_i' | t_i) - R(\rvx_i | t_i)\| \le a_i \|\rvx_i' - \rvx_i\| + b_i. 
    \end{align}
    We now use induction on $n$ to establish the bound:
    \begin{align}
        \|\rvx_n' - \rvx_n\| \leq \sum_{j=0}^{n-1} \left( \prod_{k=j+1}^{n-1} a_k \right) b_j,
    \end{align}
    where \(\prod_{k=j+1}^{N-1} a_k\) is defined as \(1\) for \(j = N-1\).
    
    For the base case \(n = 1\), we need to show:
    \begin{align}
        \|\rvx_1' - \rvx_1\| \leq b_0,
    \end{align}

    which follows directly from \cref{eq:bound_key} with $i=0$. 
    
    Now, assume the bound holds for some $n$, \ie
    \begin{align}
    \|\rvx_n' - \rvx_n\| \leq \sum_{j=0}^{n-1} \left( \prod_{k=j+1}^{n-1} a_k \right) b_j + \left( \prod_{k=0}^{n-1} a_k \right) \|\rvx_0' - \rvx_0\|.
    \end{align}
    
    We prove it holds for $n+1$. Applying \cref{eq:bound_key}, we obtain:
    \begin{align}
        \|\rvx_{n+1}' - \rvx_{n+1}\| \leq a_n \|\rvx_n' - \rvx_n\| + b_n.
    \end{align}
    
    Substitute the inductive hypothesis for \(\|\rvx_n' - \rvx_n\|\):
    \begin{align}
        \|\rvx_{n+1}' - \rvx_{n+1}\| \leq a_n \sum_{j=0}^{n-1} \left( \prod_{k=j+1}^{n-1} a_k \right) b_j + b_n = \sum_{j=0}^{n} \left( \prod_{k=j+1}^{n} a_k \right) b_j.
    \end{align}
    
    Thus, the bound holds for $n+1$. By induction, the bound holds for all $n$. Taking $n=N$ yields the desired result.
\end{proof}




\section{Derivation of Coefficients for Different Denoising Generative Models}\label{app:coefficients}

In this section, we build upon our formulation in \cref{subsec:gs} to express common diffusion models—iDDPM \cite{nichol2021iddpm}, DDIM \cite{song2021ddim}, EDM \cite{karras2022edm}, Flow Matching (FM) \cite{lipman2023flow, liu2023flow}, and our uEDM Model—using a unified notation. The coefficients corresponding to each model are summarized in \cref{tab:coefficients} and \cref{tab:edmv1}, followed by a concise derivation of their formulations.

\subsection{iDDPM}

The loss function of iDDPM \cite{nichol2021iddpm} in DDPM \cite{ho2020denoising}'s notation is
\begin{align*}
    \mcal{L}_{\text{simple}} = \mbb{E}_{t,\rvx_0,\rvepsilon}\left[\left\|\rvepsilon-\rvepsilon_\rvtheta(\sqrt{\bar{\alpha}_t}\rvx_0+\sqrt{1-\bar{\alpha}_t}\rvepsilon,t)\right\|^2\right].
\end{align*}
This can be directly translated into our notation:
\begin{align*}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[w(t)\|\net_{\vtheta}(\rvz|c_{\text{noise}}(t))-r(\rvx,\rvepsilon,t)\|^2\Big],
\end{align*}
where we have the coefficients
\begin{align}
    a(t) = \sqrt{\bar{\alpha}(t)}, b(t) = \sqrt{1-\bar{\alpha}(t)}, c(t) = 0, d(t) = 1
\end{align}
and with the training weighting and distribution of $t$ being
\begin{align}
    w(t)=1,\quad \text{and} \quad p(t) = \mcal{U}\{1, \ldots, T\}.
\end{align}


\begin{table*}[t]
    \centering
    \caption{The coefficients of different models. For iDDPM, we assume a cosine diffusion schedule $\bar{\alpha}(t)$. For both iDDPM and DDIM we follow the original notation of DDPM \cite{ho2020denoising}. Also note that for EDM, all coefficients are calculated according to first-order ODE solver, and in the final step we need to multiply the output by $\sigma_{\ud}$ to get the final image. See \cref{app:coefficients} for more details and derivations.}
    \label{tab:coefficients}
    \renewcommand{\arraystretch}{1}
    \small
    \begin{tabular}{lllll}
        \hline
        & \textbf{iDDPM} & \textbf{DDIM} & \textbf{EDM} & \textbf{FM} \\
        \hline
        \multicolumn{5}{l}{\textbf{Training}} \\
        $a(t)$ & $\sqrt{\bar{\alpha}(t)}$ & $\sqrt{\bar{\alpha}(t)}$ & $\frac{1}{\sqrt{t^2+\sigma_{\ud}^2}}$ & $1-t$ \\
        $b(t)$ & $\sqrt{1-\bar{\alpha}(t)}$ & $\sqrt{1-\bar{\alpha}(t)}$ & $\frac{t}{\sqrt{t^2+\sigma_{\ud}^2}}$ & $t$ \\
        $c(t)$ & $0$ & $0$ & $\frac{t}{\sigma_{\ud}\sqrt{t^2+\sigma_{\ud}^2}}$ & $-1$ \\
        $d(t)$ & $1$ & $1$ & $-\frac{\sigma_{\ud}}{\sqrt{t^2+\sigma_{\ud}^2}}$ & $1$ \\
        $w(t)$ & $1$ & $1$ & $1$ & $1$ \\
        \\[-1.3ex]
        $p_t$ & $\mcal U\{1, \ldots, T\}$ & $\mcal U\{1, \ldots, T\}$ & $\exp\mcal N(-1.2, 1.2^2)$ \footnotemark & $\mcal U[0, 1]$ \\
        \\[0ex]
        \hline
        \multicolumn{5}{l}{\textbf{Sampling}} \\
        $\kappa_i$ &
        $\sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}}$ & 
        $\sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}}$ & 
        $\sqrt{\frac{\sigma_{\ud}^2+t_{i}^2}{\sigma_{\ud}^2+t_{i+1}^2}}\left(1{-}\frac{t_i(t_i{-}t_{i+1})}{t_i^2+\sigma_{\ud}^2}\right)$ 
        & 0 \\
        $\eta_i$ & 
        $\frac{1}{\sqrt{1-\bar{\alpha}_i}}\left(\sqrt{\frac{\bar{\alpha}_i}{\bar{\alpha}_{i+1}}}{-}\sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}}\right)$ & 
        $\sqrt{1-\bar{\alpha}_{i+1}}{-}\sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}(1-\bar{\alpha}_i)}$ & 
        $\frac{\sigma_{\ud} (t_i-t_{i+1})}{\sqrt{(t_i^2+\sigma_{\ud}^2)(t_{i+1}^2+\sigma_{\ud}^2)}}$ & 
        $t_{i+1}-t_{i}$ \\
        $\zeta_i$ & 
        $\sqrt{\left(1{-}\frac{\bar{\alpha}_i}{\bar{\alpha}_{i+1}}\right)\frac{1-\bar{\alpha}_{i+1}}{1-\bar{\alpha}_i}}$ & 
        0 & 
        0 & 
        0 \\
        Schedule $t_{0\sim N}$ &
        $t_i=\frac{N-i}{N}\cdot T$ & 
        $t_i=\frac{N-i}{N}\cdot T$ & 
        $t_i=\left(t_{\mx}^{\frac{1}{\rho}}{+}\frac{i}{N}\left(t_{\mn}^{\frac{1}{\rho}}{-}t_{\mx}^{\frac{1}{\rho}}\right)\right)^{\rho}$ & 
        $t_i=1-\frac{i}{N}$  \\
        \\[-0.8ex]
        \hline
        \multicolumn{5}{l}{\textbf{Parameters}} \\
        & $\bar{\alpha}(t)=\frac{1}{2}\left(1+\cos\frac{\pi t}{T}\right)$ & 
        $\bar{\alpha}(t)=\prod\limits_{i=0}^{t-1}\left(1{-}k_1{-}k_2\frac{i}{T-1}\right)$ &
        $\sigma_{\ud}=0.5, \rho=7$ & \\
        \\[-0.8ex]
        & $\bar{\alpha}_i:=\bar\alpha(t_i)$&
        $\bar{\alpha}_i:=\bar\alpha(t_i)$ &
        $t_{\mx}=80, t_{\mn}=0.002$ & \\
        \\[-0.8ex]
        & $T=4000$ & $T=1000$ & &\\
        \\[-0.8ex]
        &  & $k_1=10^{-4}, k_2=2\times 10^{-2}$ & &\\
        \\[-0.8ex]
        \hline
    \end{tabular}
\end{table*}


Notice the presence of the diffusion schedule $\bar{\alpha}(t)$ inside the coefficients. We adapt a modified version of the cosine schedule in \citet{nichol2021iddpm}:
\begin{align}
    \bar{\alpha}(t) = \frac{1}{2}\left(1+\cos \frac{\pi t}{T}\right),
\end{align}

where $T=4000$ is the total number of diffusion steps during training.

Next, consider the sampling process, which in their notations is iteratively given by
\begin{align*}
    \rvx_{t-1} = \frac{1}{\sqrt{\alpha_t}}\left(\rvx_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\rvepsilon_\rvtheta(\rvx_t,t)\right)+\sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t}\rvz,
\end{align*}
and $\rvz\sim \mcal{N}(\bm 0, \mI)$ is a standard Gaussian random noise. It is also straightforward to translate this sampling equation into our notation:
\begin{align}\label{eq:iddpm_coeff}
    \kappa_i = \sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}}, \eta_i = \frac{1}{\sqrt{1-\bar{\alpha}_i}}\left(\sqrt{\frac{\bar{\alpha}_i}{\bar{\alpha}_{i+1}}}-\sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}}\right), \zeta_i = \sqrt{\left(1-\frac{\bar{\alpha}_i}{\bar{\alpha}_{i+1}}\right)\frac{1-\bar{\alpha}_{i+1}}{1-\bar{\alpha}_i}},
\end{align}
and 
\begin{align}
    t_i =  \frac{N-i}{N}\cdot T.
\end{align}

This will give the first column in \cref{tab:coefficients}.


\subsection{DDIM}\label{app:ddim_coeff}
DDIM \cite{song2021ddim} shares the training process with DDPM \cite{ho2020denoising}. However, we choose to use the linear schedule for $\bar{\alpha}(t)$, to demonstrate the generality of our scheme. This schedule has the form
\begin{align}
    \bar{\alpha}(t) = \prod\limits_{i=0}^{t-1}\left(1-k_1-k_2\frac{i}{T-1}\right),
\end{align}

where $k_1=10^{-4}$ and $k_2=2\times 10^{-2}$, and $T=1000$ is the total number of diffusion steps during training.


\footnotetext{Here, we use the notation $\exp\mcal N(\mu, \sigma^2)$ to denote the distribution of $\exp(u)$, where $u\sim \mcal N(\mu, \sigma^2)$.}


The sampling process is given by
\begin{align*}
    \rvx_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\rvepsilon_\rvtheta (\rvx_t,t)}{\sqrt{\bar{\alpha}_t}}\right)+\sqrt{1-\bar{\alpha}_{t-1}}\rvepsilon_\rvtheta (\rvx_t,t)
\end{align*}
which is obtained by substituting $\sigma_t=0$ in their notation. This is again straightforward to translate into our notation:
\begin{align}\label{eq:ddim_coeff}
    \kappa_i = \sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}}, \eta_i = \sqrt{1-\bar{\alpha}_{i+1}}-\sqrt{\frac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}(1-\bar{\alpha}_i)}, \zeta_i = 0,
\end{align}
and
\begin{align}
    t_i = \frac{N-i}{N} \cdot T.
\end{align}


These give the second column in \cref{tab:coefficients}.

Moreover, we can consider the generalized sampler proposed by \citet{song2021ddim}, which contains an adjustable parameter $\lambda\in [0,1]$. In their original notation, the sampler can be written as
\begin{align*}
    \rvx_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\rvepsilon_\rvtheta (\rvx_t,t)}{\sqrt{\bar{\alpha}_t}}\right)+\sqrt{1-\bar{\alpha}_{t-1}-\lambda^2\sigma_t^2 }\rvepsilon_\rvtheta (\rvx_t,t)+\lambda \sigma_t \rvepsilon_t,
\end{align*}
where
\begin{align*}
    \sigma_t:=\sqrt{\frac{(\bar{\alpha}_{t-1}-\bar{\alpha}_t)(1-\bar{\alpha}_{t-1})}{\bar{\alpha}_{t-1}(1-\bar{\alpha}_t)}}
\end{align*}
and $\rvepsilon_t$ is an independent Gaussian random noise. In our formulation, it can be equivalently written as
\begin{align}\label{eq:ddim_coeff_lmd}
    \begin{cases}
    \kappa_i = \sqrt{\dfrac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}} \\
    \\[-0.8ex]
    \eta_i = \sqrt{1-\bar{\alpha}_{i+1}-\lambda^2 \dfrac{(\bar{\alpha}_{i+1}-\bar{\alpha}_{i})(1-\bar{\alpha}_{i+1})}{\bar{\alpha}_{i+1}(1-\bar{\alpha}_{i})}}-\sqrt{\dfrac{\bar{\alpha}_{i+1}}{\bar{\alpha}_i}(1-\bar{\alpha}_i)} \\
    \\[-0.8ex]
    \zeta_i = \lambda \sqrt{\dfrac{(\bar{\alpha}_{i+1}-\bar{\alpha}_{i})(1-\bar{\alpha}_{i+1})}{\bar{\alpha}_{i+1}(1-\bar{\alpha}_{i})}}
    \end{cases}.
\end{align}

These expressions are used in our experiment of the ``interpolate sampler'' in \cref{subsec:analysis}. One can verify that when $\lambda=1$, the coefficients $\kappa_i, \eta_i$ and $\zeta_i$ will be the same as iDDPM (\cref{eq:iddpm_coeff}); and when $\lambda=0$, the coefficients will be the same as DDIM (\cref{eq:ddim_coeff}).

\subsection{EDM}\label{app:edm_coeff}

The original EDM \cite{karras2022edm} training objective is given by
\begin{align}
    \label{eq:edm_loss}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[ \lambda(t)\|\mD_{\vtheta}(\rvx + t\rvepsilon |t)- \rvx\|^2\Big],
\end{align}
where $\mD_{\vtheta}$ is formed by the \textit{raw network} $\net_{\rvtheta}$ wrapped with a precondition:
\begin{align*}
    \mD_{\vtheta}(\rvz_{\mD}|t)=c_{\text{skip}}(t)\rvz_{\mD}+c_{\text{out}}(t)\net_{\vtheta}(c_{\text{in}}(t)\rvz_{\mD}|t)
\end{align*}
where $\rvz_{\mD}=\rvx+t\rvepsilon$. Here we directly use $t$ instead of $c_{\text{noise}}(t)$ in the original notation. 

As mentioned in \cref{subsec:gs}, we will consider the regression target with respect to $\net_{\vtheta}$ instead of $\mD_{\vtheta}$ and absorb the coefficients $c_{\text{skip}}(t)$, $c_{\text{in}}(t)$ and $c_{\text{out}}(t)$ into the training process. To achieve that, we define $\rvz:= c_{\text{in}}(t)(\rvx + t \rvepsilon)$. Then, we can get an equivalent training objective for $\net_{\vtheta}$ given by

\begin{align*}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[w(t)\|\net_{\vtheta}(\rvz|t)-r(\rvx,\rvepsilon,t)\|^2\Big],
\end{align*}
where 
\begin{align}\label{eq:edm_coeff}
\begin{cases*}
    z = c_{\text{in}}(t) \rvx + t c_{\text{in}}(t) \rvepsilon \\
    w(t) = \lambda(t) c_{\text{out}}(t)^2 \\
    r(\rvx,\rvepsilon,t) = \frac{1}{c_{\text{out}}(t)}\cdot \left(\rvx-c_{\text{skip}}(t)(\rvx+t\rvepsilon)\right) = \frac{1-c_{\text{skip}}(t)}{c_{\text{out}}(t)}\rvx - \frac{t c_{\text{skip}}(t)}{c_{\text{out}}(t)}\rvepsilon
\end{cases*}
\end{align}
Now, we can plug in the specific expressions
\begin{align*}
    c_{\text{in}}(t)=\frac{1}{\sqrt{\sigma_{\ud}^2+t^2}},c_{\text{out}}(t)=\frac{\sigma_{\ud} t}{\sqrt{\sigma_{\ud}^2+t^2}},c_{\text{skip}}(t)=\frac{\sigma_{\ud}^2}{\sigma_{\ud}^2+t^2}, \lambda(t) = \frac{\sigma_{\ud}^2 + t^2}{\sigma_{\ud}^2 t^2}
\end{align*}
and $\sigma_{\ud}=0.5$ to get the coefficients 
\begin{align}
    a(t) = \frac{1}{\sqrt{\sigma_{\ud}^2+t^2}}, b(t) = \frac{t}{\sqrt{\sigma_{\ud}^2+t^2}}, c(t) = \frac{t}{\sigma_{\ud}\sqrt{\sigma_{\ud}^2+t^2}}, d(t) = -\frac{\sigma_{\ud}}{\sqrt{\sigma_{\ud}^2+t^2}},
\end{align}
and $w(t)=1$. Also note that $p(t)$ is given explicitly by the log-norm schedule $\exp\mcal{N}(-1.2,1.2^2)$. This completes the discussion of the training process. 

The (first-order) sampling process is given by
\begin{align*}
    \rvx_{\mD,i+1}=\rvx_{\mD,i} + (t_{i+1}-t_i)\frac{\rvx_{\mD,i}-\left(c_{\text{skip}}(t_i)\rvx_{\mD,i}+c_{\text{out}}(t_i)\net_{\vtheta}(c_{\text{in}}(t_i)\rvx_{\mD,i}|t_i)\right)}{t_i}
\end{align*} 
Here we use the suffix $\mD$ to denote this is the sampling process corresponding to $\mD_\rvtheta$. Since we also have to remove the external conditioning in the sampling process, we should let $\rvx_i = c_{\text{in}}(t_i)\rvx_{\mD,i}$ and rewrite the sampling equation using $\rvx_i$:
\begin{align*}
    \rvx_{i+1} = \frac{t_{i+1}}{t_i}\cdot \frac{c_{\text{in}}(t_{i+1})}{c_{\text{in}}(t_i)}\left(1-\frac{t_{i+1}-t_i}{t_{i+1}}c_{\text{skip}}(t_i)\right)\rvx_i + \frac{t_i-t_{i+1}}{t_i}{c_{\text{out}}(t_i)}{c_{\text{in}}(t_{i+1})}\net_{\vtheta}(\rvx_i|t_i)
\end{align*}
This then gives the general sampling coefficients 
\begin{align}\label{eq:edm_sampling}
\kappa_i = \frac{t_{i+1}}{t_i}\cdot \frac{c_{\text{in}}(t_{i+1})}{c_{\text{in}}(t_i)}\left(1-\frac{t_{i+1}-t_i}{t_{i+1}}c_{\text{skip}}(t_i)\right), \eta_i = \frac{t_i-t_{i+1}}{t_i}{c_{\text{out}}(t_i)}{c_{\text{in}}(t_{i+1})}, \zeta_i = 0.
\end{align}
Then, we can plug in the explicit expressions of $c_{\text{in}}(t_i)$, $c_{\text{skip}}(t_i)$ and $c_{\text{out}}(t_i)$ to get the final coefficients
\begin{align}
    \kappa_i = \sqrt{\frac{\sigma_{\ud}^2+t_i^2}{\sigma_{\ud}^2+t_{i+1}^2}}\left(1+\frac{t_{i}(t_{i+1}-t_i)}{t_{i}^2+\sigma_{\ud}^2}\right), \eta_i = -\frac{\sigma_{\ud} (t_{i+1}-t_i)}{\sqrt{(t_{i+1}^2+\sigma_{\ud}^2)(t_i^2+\sigma_{\ud}^2)}}, \zeta_i = 0.
\end{align}
Moreover, notice that due to our change-of-variable during the removal of external conditioning, $\rvx_N$ is defined as $c_{\text{in}}(t_N)\rvx_{\mD, N}$. But the sampling algorithm ensures $\rvx_{\mD, N}$ to match the data distribution, instead of $\rvx_N$. Thus, we have to multiply the output by $\sigma_{\ud}$ to get the final image, as mentioned in the caption of \cref{tab:coefficients}.

Finally, the sampling time step is also explicitly given in \citet{karras2022edm}, so we can directly use it here:
\begin{align}
    t_i = \begin{cases*}
        \left(\frac{80^{\frac{1}{7}}\cdot (N-i-1) + 0.002^{\frac{1}{7}}\cdot i}{N-1}\right)^7 &if $i<N$ \\
        $0$ &if $i=N$
    \end{cases*}.
\end{align}

These together give the coefficients for EDM, which are shown in the third column of \cref{tab:coefficients}.

\subsection{Flow Matching}

The training process of FM \cite{lipman2023flow} is given by
\begin{align*}
    \mcal{L}(\vtheta) = \mbb{E}_{\rvx,\rvepsilon,t}\Big[\|\rvv_{\vtheta}(t\rvepsilon+(1-t)\rvx,t)-(\rvepsilon-\rvx)\|^2\Big].
\end{align*}
This can be directly translated into our notation:
\begin{align}
    a(t) = 1-t, b(t) = t, c(t) = -1, d(t) = 1,
\end{align}
and with $w(t)=1$ and $p(t)=\mcal{U}([0,1])$. The sampling process is given by solving the ODE
\begin{align*}
    \frac{\ud \rvx}{\ud t} = \rvv_\rvtheta(\rvx,t)
\end{align*}
from $t=1$ to $t=0$. Since we assume using a first-order method (\ie Euler method), the sampling equation is 
\begin{align*}
    \rvx_{i+1} = \rvx_i + \rvv_\rvtheta(\rvx_i,t_i)\cdot (t_{i+1}-t_i).
\end{align*}
This will give
\begin{align}
    \kappa_i = 0, \eta_i = t_{i+1}-t_i, \zeta_i = 0
\end{align}
as well as the sampling time
\begin{align}
    t_i = \frac{N-i}{N},
\end{align}

as in the fourth column of \cref{tab:coefficients}.


\subsection{Our uEDM Model in the Formulation}\label{app:v1}

Introduced in \cref{p:edmv1}, the uEDM model designed by us is a modified version of EDM \cite{karras2022edm}. The only modification is that we change $c_{\text{in}}(t)$ and $c_{\text{out}}(t)$ by
\begin{align*}
\begin{cases*}
    c_{\text{in}}(t) = \dfrac{1}{\sqrt{t^2+\sigma_{\ud}^2}} \\
    c_{\text{out}}(t) = \dfrac{t\sigma_{\ud}}{\sqrt{t^2+\sigma_{\ud}^2}}
\end{cases*}
\qquad\longrightarrow\qquad
\begin{cases*}
    c_{\text{in}}(t) = \dfrac{1}{\sqrt{t^2+1}} \\
    c_{\text{out}}(t) = 1
\end{cases*}
\end{align*}
and remain all other configurations the same as the original EDM model. 

In \cref{app:edm_coeff}, we have already derived the general form of the coefficients of EDM with functions $c_{\text{in}}(t)$, $c_{\text{out}}(t)$, $c_{\text{skip}}(t)$ and $\lambda (t)$ in \cref{eq:edm_coeff,eq:edm_sampling}. Plugging in the new set of these functions, we can then derive the coefficients of uEDM, as shown in \cref{tab:edmv1}.

\begin{table}[!ht]
    \caption{Comparison of coefficients of EDM and our uEDM.}\label{tab:edmv1}
    \centering
    \scriptsize
    \begin{tabular}{lccccccc}
        \toprule
        Coefficients & $a(t)$ & $b(t)$ & $c(t)$ & $d(t)$ & $w(t)$ & $\kappa_i$ & $\eta_i$ \\
        \midrule
        EDM & $\dfrac{1}{\sqrt{t^2+\sigma_{\ud}^2}}$ & 
        $\dfrac{t}{\sqrt{t^2+\sigma_{\ud}^2}}$ & 
        $\dfrac{t}{\sigma_{\ud}\sqrt{t^2+\sigma_{\ud}^2}}$ & 
        $-\dfrac{\sigma_{\ud}}{\sqrt{t^2+\sigma_{\ud}^2}}$ & 
        $1$ &
        $\sqrt{\dfrac{\sigma_{\ud}^2+t_{i}^2}{\sigma_{\ud}^2+t_{i+1}^2}}\left(1{-}\dfrac{t_i(t_i{-}t_{i+1})}{t_i^2+\sigma_{\ud}^2}\right)$ & 
        $\dfrac{\sigma_{\ud} (t_i-t_{i+1})}{\sqrt{(t_i^2+\sigma_{\ud}^2)(t_{i+1}^2+\sigma_{\ud}^2)}}$ \\
        \\[-1ex]
        uEDM & $\dfrac{1}{\sqrt{t^2+1}}$ & 
        $\dfrac{t}{\sqrt{t^2+1}}$ & 
        $\dfrac{t^2}{t^2+\sigma_{\ud}^2}$ & 
        $-\dfrac{t\sigma_{\ud}^2}{t^2+\sigma_{\ud}^2}$ & 
        $\dfrac{\sigma_{\ud}^2+t^2}{\sigma_{\ud} t}$ &
        $\sqrt{\dfrac{t_{i}^2+1}{t_{i+1}^2+1}}\left(1{-}\dfrac{t_i(t_i{-}t_{i+1})}{t_i^2+\sigma_{\ud}^2}\right)$ 
        & $\dfrac{t_i-t_{i+1}}{t_i\sqrt{t_{i+1}^2+1}}$ \\
        \bottomrule
    \end{tabular}
\end{table}



\section{Additional Samples}

Beyond the comparison shown in \cref{fig:samples} for noise-conditional and noise-unconditional models, we also provide additional samples for other models, on other datasets, or in class-conditional settings. We use the same configuration as in \cref{tab:exp}. \cref{fig:add_icm,fig:add_ecm} show the samples of ICM and ECM on CIFAR-10 with both 1 and 2 inference steps. \cref{fig:add_imgnet} show the samples of FM on ImageNet 32$\times$32 with both Euler and EDM-Heun sampler. \cref{fig:add_cond} shows the samples of FM and EDM on CIFAR-10 in a class-conditional setting.

\newpage

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{ICM-1step_compare.png}
        \caption{ICM 1 step (FID: $3.37\to 12.03$)\\[1.4ex]}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{ICM-2step_compare.png}
        \caption{ICM 2 step (FID: $2.59\to 3.57$)\\[1.4ex]}
    \end{subfigure}
    \caption{Samples generated by ICM on CIFAR-10. From left to right: 1 step w/ $t$, 1 step w/o $t$, 2 step w/ $t$, 2 step w/o $t$. All corresponding samples use the same noise.}\label{fig:add_icm}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{ECM-1step_compare.png}
        \caption{ECM 1 step (FID: $3.49\to 12.60$)\\[1.4ex]}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{ECM-2step_compare.png}
        \caption{ECM 2 step (FID: $2.57\to 3.27$)\\[1.4ex]}
    \end{subfigure}
    \caption{Samples generated by ECM on CIFAR-10. From left to right: 1 step w/ $t$, 1 step w/o $t$, 2 step w/ $t$, 2 step w/o $t$. All corresponding samples use the same noise.}\label{fig:add_ecm}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{FM-imgnet-euler_compare.png}
        \caption{ImageNet FM, Euler Sampler (FID: $5.15\to 4.85$)\\[1.4ex]}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \includegraphics[width=\textwidth]{FM-imgnet-heun_compare.png}
        \caption{ImageNet FM, Heun Sampler (FID: $4.43\to 4.58$)\\[1.4ex]}
    \end{subfigure}
    \caption{Samples generated by FM on ImageNet 32$\times$32 with Euler and EDM-Heun sampler. From left to right: Euler w/ $t$, Euler w/o $t$, Heun w/ $t$, Heun w/o $t$. All corresponding samples use the same noise.}\label{fig:add_imgnet}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.67\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fm-cond-euler_compare.png}
        \caption{Class-conditional FM (FID: $2.72\to 2.55$)}
        \vspace{1em}
    \end{subfigure}
    \centering
    \begin{subfigure}[b]{0.67\textwidth}
        \centering
        \includegraphics[width=\textwidth]{edm-cond_compare.png}
        \caption{Class-conditional EDM (FID: $1.76\to 3.11$)}
    \end{subfigure}
    \caption{Class-conditional samples generated by FM and EDM on CIFAR-10. In rasterized order: FM w/ $t$, FM w/o $t$, EDM w/ $t$, EDM w/o $t$. All corresponding samples use the same noise and the same label.}\label{fig:add_cond}
\end{figure*}



\end{appendices}