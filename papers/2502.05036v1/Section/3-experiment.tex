\section{Experiments and Analysis}

\begin{table*}[!t]
\centering
\vspace{-1em}
\begin{threeparttable}
\setlength{\tabcolsep}{2pt} % Default value: 6pt
\scalebox{0.88}{
\begin{tabular}{l|ccccc|ccccc}
\toprule[1.5pt]
\multirow{2}{*}{\textbf{Method}}& \multicolumn{5}{c|}{\textbf{Single-Table}} & \multicolumn{5}{c}{\textbf{Multi-Table}} \\
% \cmidrule{2-11}
& Invalid($\downarrow$) & Illegal($\downarrow$) & Pass($\uparrow$) & Read.($\uparrow$) & Qual.($\uparrow$) & Invalid($\downarrow$) & Illegal($\downarrow$) & Pass($\uparrow$) & Read.($\uparrow$) & Qual.($\uparrow$) \\
\midrule
\multicolumn{11}{c}{\textbf{\textit{GPT-4o}}} \\
\midrule
CoML4Vis & \textbf{0.67\%} & 24.14\% & 75.17\% & 3.42 & 2.58 & 1.87\% & 26.27\% & 71.84\% & 3.45 & 2.48\\
LIDA &  1.13\% & 21.20\% & 77.66\% & 2.53 & 1.99 & 14.80\% & 83.56\% & 1.62\% & 3.62 & 0.06\\
Chat2Vis &  0.86\% & 21.37\% & 77.75\% & \textbf{3.87} & 3.02 & 38.74\% & 59.84\% & 1.40\% & \textbf{3.76} & 0.05\\
\textbf{\system} & 0.72\% & \textbf{13.63\%} & \textbf{85.63\%} & 3.66 & \textbf{3.13} & \textbf{1.34\%} & \textbf{17.57\%} & \textbf{81.07\%} & 3.61 & \textbf{2.93}\\ 
$\Delta$ & \textcolor{red}{-0.05\%} & \textcolor{green!60!black}{+7.57\%} & \textcolor{green!60!black}{+7.88\%} & \textcolor{red}{-5.42\%} & \textcolor{green!60!black}{+3.64\%} & \textcolor{green!60!black}{+0.53\%} & \textcolor{green!60!black}{+8.70\%} & \textcolor{green!60!black}{+9.23\%} & \textcolor{red}{-3.98\%} & \textcolor{green!60!black}{+18.15\%} \\
\midrule
\multicolumn{11}{c}{\textbf{\textit{GPT-4o-mini}}} \\
\midrule
CoML4Vis & \textbf{0.36\%} & 25.74\% & 73.88\% & 3.33 & 2.47 & 10.01\% & 33.06\% & 56.92\% & 3.24 & 1.86\\
LIDA &  9.09\% & 23.04\% & 67.85\% & 3.10 & 2.12 & 17.61\% & 80.86\% & 1.51\% & 3.10 & 0.04\\
Chat2Vis &  2.14\% & 25.92\% & 71.92\% & \textbf{3.81} & 2.76 & 35.78\% & 61.93\% & 2.27\% & 2.30 & 0.05\\
\textbf{\system} & 1.97\% & \textbf{22.86\%} & \textbf{75.16\%} & 3.67 & \textbf{2.77} & \textbf{8.15\%} & \textbf{25.99\%} & \textbf{65.85\%} & \textbf{3.66} & \textbf{2.42}\\ 
$\Delta$ & \textcolor{red}{-1.61\%} & \textcolor{green!60!black}{+0.18\%} & \textcolor{green!60!black}{+1.28\%} & \textcolor{red}{-3.67\%} & \textcolor{green!60!black}{+0.36\%} & \textcolor{green!60!black}{+1.86\%} & \textcolor{green!60!black}{+7.07\%} & \textcolor{green!60!black}{+8.93\%} & \textcolor{green!60!black}{+12.96\%} & \textcolor{green!60!black}{+30.11\%}\\
\midrule
\multicolumn{11}{c}{\textbf{\textit{GPT-3.5-turbo}}} \\
\midrule
CoML4Vis & 6.17\% & 29.28\% & 64.54\% & 3.33 & 2.18 &  13.92\% & 30.09\% & 55.98\% & 3.37 & 1.93 \\
LIDA & 47.32\% & 15.84\% & 36.83\% & 3.32 & 1.23 & 62.57\% & 36.56\% & 0.86\% & 3.50 & 0.03\\
Chat2Vis & 3.90\% & 28.11\% & 67.98\% & 3.03 & 2.08 & 40.77\% & 57.66\% & 1.55\% & 3.31 & 0.05\\
\textbf{\system} & \textbf{2.98\%} & 20.93\% & \textbf{76.08\%} & \textbf{3.58} & \textbf{2.72} & \textbf{7.18\%} & \textbf{28.51\%} & \textbf{64.29\%} & \textbf{3.61} & \textbf{2.32}\\
$\Delta$ & \textcolor{green!60!black}{+0.92\%} & \textcolor{red}{-5.09\%}\textdagger & \textcolor{green!60!black}{+8.10\%} & \textcolor{green!60!black}{+7.51\%} & \textcolor{green!60!black}{+24.77\%} & \textcolor{green!60!black}{+6.74\%} & \textcolor{green!60!black}{+1.58\%} & \textcolor{green!60!black}{+8.11\%} & \textcolor{green!60!black}{+3.14\%} & \textcolor{green!60!black}{+20.21\%}\\
\bottomrule[1.5pt]
\end{tabular}}
% \vspace{0.5em}
\begin{tablenotes}
    \scriptsize
    \item[*] $\Delta$ represents the percentage improvement or decrease of \system compared to the best-performing baseline for each metric. \\
    For the first three columns, $\Delta$ is calculated using absolute differences, while for the last two columns, it is calculated as the relative change. 
    \item \textdagger:~\system actually performs best, while LIDA has a lower Illegal due to its high Invalid rate.
\end{tablenotes}
\vspace{-2mm}
\caption{Performance of our approach with baselines using different backbone models.}
\label{tab:performance_comparison}
\end{threeparttable}
\vspace{-1em}
\end{table*}





\subsection{Experimental Setup}
\label{setup}
\paragraph{Dataset.}
VisEval \cite{viseval} is a benchmark designed based on nvBench \cite{nvbench} to assess the capabilities of LLMs in the \nlvis task. It consists of 1,150 distinct visualizations (VIS) and 2,524 (NL, VIS) pairs across 146 databases, with accurately labeled ground truths and meta-information detailing feasible visualization options. The dataset is divided into \textit{single-table} scenario and \textit{multi-table} scenario. Moreover, visualizations are classified into four distinct levels of hardness: easy, medium, hard, and extra hard. Cases across different hardness levels can be found in Appendix~\ref{example}.

\paragraph{Baselines.} We conduct our experiments compared with three formerly SOTA baselines\footnote{We try the vanilla baseline similar to the GPT-4o with code interpreter in \url{https://platform.openai.com/docs/assistants/tools/code-interpreter}. Due to the API still in the beta stage and often failing, we do not include it as a baseline.}: Chat2Vis \cite{chat2vis}, which uses prompt engineering to generate visualizations from natural language descriptions; LIDA \cite{lida}, which employs a four-step process for incrementally translating natural language inputs into visualizations; and CoML4Vis \cite{coml}, which applies a few-shot prompt method integrating multiple tables for visualization tasks. 
More details can be found in Appendix~\ref{detailed_baselines}.
% \paragraph{Models.} 
We implement our approach and baselines using three different backbone models: GPT-4o \citep{openai_gpt4o_2024}, GPT-4o-mini \citep{openai2024gpt4omini}, and GPT-3.5-turbo \citep{chatgpt3.5}. 

\paragraph{Evaluation Metrics.}
We evaluate the performance using both rule-based and model-based metrics for quantitative and qualitative assessment. 
    \textbf{Invalid Rate} and \textbf{Illegal Rate} represent the percentages of visualizations that fail to render or meet query requirements, respectively. 
    \textbf{Pass Rate} measures the proportion of valid and legal visualizations in the evaluation set.
    \textbf{Readability Score} is the average score ranging from 0 to 5 assigned by MLLM-as-a-Judge \citep{chen2024mllm, ye2024justice} to assess their visual clarity for legal visualization. We assess MLLM-scoring by calculating the similarity of GPT-4o-mini and GPT-4o with human-annotated scores in a subset with 500 samples. 
    Empirically, we select GPT-4o-mini as the vision model for judgment. More details are referred to the Appendix~\ref{detailed_experiment_setups}.
    % Table~\ref{tab:pearson_corr} demonstrates that GPT-4o-mini is more suitable for serving as a judge. 
    % Overall, these metrics collectively evaluate the functionality, accuracy, and interpretability of generated visualizations, providing a comprehensive assessment of LLMs' performance in visualization tasks.
    \textbf{Quality Score} is 0 for invalid or illegal visualizations, otherwise equal to the readability score.


% For \textit{Readability Score} given by visual language model, 

% \paragraph{Ablation Setting.} To evaluate the effectiveness of each component in \system, we conducted comprehensive ablation experiments. We conduct agent workflow ablation studies with GPT-4o, and comprehensive ablation of each module within \system using GPT-3.5-turbo.

% \subsection{Experimental Results}
\subsection{Overall Performance}
\label{results}
% \paragraph{\system achieves state-of-the-art performance across all metrics.}



% \subsection{XXXXXX (RQ2)}
% \paragraph{\system excels in challenging multi-table scenarios.}
Table~\ref{tab:performance_comparison} shows the performance across different methods and backbone models. Generally, our proposed method, \system, demonstrates significant improvements over existing approaches across all metrics in both \textit{single-} and \textit{multi-table} scenarios, particularly on pass rate and quality score.
% As shown in Table~\ref{tab:performance_comparison}, 
Furthermore, \system achieves an impressive 85.63\% pass rate and a quality score of 3.13 in \textit{single-table} scenarios using GPT-4o, surpassing all baseline methods. In more complex \textit{multi-table} scenarios, \system maintains strong performance, significantly outperforming other approaches. Specifically, using GPT-4o, our method attains an 81.07\% pass rate and a 2.93 quality score for \textit{multi-table} queries, exceeding the previous state-of-the-art by 18.15\%. 
% This demonstrates its robust capability in handling intricate \nlvis tasks. 
The minimal gap between \textit{single-} and \textit{multi-table} scenarios (85.63\% vs. 81.07\%) underscores \system's consistency and adaptability across varying complexities, a crucial advantage in real-world applications where multi-table queries are common.
% \textbf{These results demonstrate that our proposed approach achieves superior overall performance and establishes a new state-of-the-art for \nlvis tasks.}

\begin{table*}[!t]
\centering
\vspace{-1em}
 \setlength{\tabcolsep}{4pt} % Default value: 6pt
\vspace{-1em}
\scalebox{0.9}{
\begin{tabular}{l|ccc|ccc|c}
\toprule[1.5pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Single-Table}} & \multicolumn{3}{c|}{\textbf{Multi-Table}} & \textbf{Average} \\
& Invalid & Illegal & Pass & Invalid & Illegal & Pass & Pass Rate\\
\midrule
\multicolumn{8}{c}{\textbf{\textit{GPT-4o}}}\\
\midrule
\textbf{\system}(4-shot) & 0.72\% & 13.63\% & \textbf{85.63\%} & 1.34\% & 17.57\% & 81.07\% & 83.80\% \\
w/o Processor & 0.62\% & 14.27\% & 85.09\% & 1.26\% & 16.42\% & \textbf{82.31\%} & \textbf{83.97\%} \\
w/o Composer & 1.20\% & 74.56\% & 24.22\% & 2.34\% & 74.00\% & 23.64\% & 23.99\% \\
w/o Validator & 5.80\% & 12.22\% & 81.96\% & 7.01\% & 15.95\% & 77.02\% & 79.98\% \\
\midrule
\multicolumn{8}{c}{\textbf{\textit{GPT-3.5-turbo}}}\\
\midrule
\textbf{\system}(4-shot) & 2.98\% & 20.93\% & 76.08\% & 7.18\% & 28.51\% & \textbf{64.29\%} & \textbf{71.35\%} \\
w/o Processor & 3.01\% & 20.15\% & \textbf{76.82\%} & 9.38\% & 31.01\% & 59.60\% & 69.92\% \\ w/o Composer & 18.78\% & 30.97\% & 50.24\% & 25.02\% & 27.92\% & 47.05\% & 48.96\% \\ w/o Validator & 18.04\% & 17.50\% & 64.45\% & 22.64\% & 21.40\% & 55.94\% & 61.04\% \\
\bottomrule[1.5pt]
\end{tabular}%
}

\caption{Ablation results of each agent within \system.}
 \label{tab:ablation_agent}
% \vspace{-1em}
\end{table*}

\begin{table*}[!t]
\centering

% \setlength{\tabcolsep}{4pt} % Default value: 6pt
% \vspace{-1em}
\scalebox{0.9}{
\begin{tabular}{l|ccc|ccc|c}
\toprule[1.5pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Single-Table}} & \multicolumn{3}{c|}{\textbf{Multi-Table}} & \textbf{Average} \\
& Invalid & Illegal & Pass & Invalid & Illegal & Pass & Pass Rate\\
\midrule
\textbf{nvAgent}(4-shot) & 2.98\% & 20.93\% & 76.08\% & 7.18\% & 28.51\% & \textbf{64.29\%} & \textbf{71.35\%} \\
\midrule
w/o schema filtering & 3.36\% & 20.09\% & \textbf{76.53\%} & 12.08\% & 30.14\% & 57.77\% & 69.01\% \\
w/o aug. explanation & 3.23\% & 20.69\% & 76.06\% & 7.10\% & 30.87\% & 62.01\% & 70.44\% \\
w/o complex. classifi. & 4.77\% & 21.42\% & 73.79\% & 7.50\% & 29.80\% & 62.69\% & 69.34\% \\
w/o CoT & 15.81\% & 16.91\% & 67.27\% & 17.73\% & 24.40\% & 57.86\% & 63.50\% \\
w/o ICL & 26.80\% & 24.92\% & 48.27\% & 31.91\% & 28.41\% & 39.66\% & 44.82\% \\
\bottomrule[1.5pt]
\end{tabular}%
}
\vspace{-2mm}
\caption{ Ablation results of each module within \system's agentic workflow. }
\label{tab:ablation_tech}
\vspace{-1em}
\end{table*}


\begin{figure}[!t]
    \centering
    % \setlength{\belowcaptionskip}{-1em}   %调整图片标题与下文距离
    \includegraphics[width=0.96\linewidth]{figure/pass_rate_by_method.pdf}
    % \vspace{-1em}
    \caption{Integrating better LLMs as backbones (\emph{i.e.}, GPT-4o) can bring higher pass rates.
    % \yao{larger fontsize?}
    }
    \vspace{-1em}
    \label{fig:pass_rate}
\end{figure}




% \subsection{Ablation Study}
\subsection{Effectiveness of Each Agent}
% \paragraph{Composer is crucial for performance, while Validator enhances reliability.} 
To evaluate the effectiveness of each component in \system, we conducted comprehensive ablation experiments. We perform agent workflow ablation studies with GPT-4o to assess the contributions of each agent, as shown in Table~\ref{tab:ablation_agent}.
% and more comprehensive ablation of each module within \system using GPT-3.5-turbo to understand their individual contributions.
% From this table, we can see that the \textit{composer} as the most critical component, with its removal causing substantial 22.39\%, 59.81\% drops in overall pass rate when using GPT-3.5-turbo and GPT-4o, respectively. 
From this table, we observe that the \textit{composer} is the most critical component, as its removal leads to significant drops in the overall pass rate—22.39\% with GPT-3.5-turbo and 59.81\% with GPT-4o.
The \textit{validator} also proves vital, as its absence leads to a 3.82\% decrease for GPT-4o and a sharper decrease of 10.31\% using GPT-3.5-turbo, primarily due to increased invalid rate, confirming the effectiveness of the post-processing stage. 

\begin{figure}[!t]
	\centering
    % \setlength{\belowcaptionskip}{-1em}   %调整图片标题与下文距离
	\includegraphics[width=0.9\linewidth,scale=1.0]
    {./figure/few_shot.pdf}
    % \vspace{-1em}
    \vspace{-2mm}
	\caption{More examples for in-context learning bring higher pass rate, using GPT-3.5-turbo.}
\label{fig: few-shot-ablation}
\vspace{-1em}
\end{figure}

Interestingly, while the \textit{processor}'s removal shows only a slight overall performance decline (1.43\%), its impact varies across scenarios: a marginal improvement in \textit{single-table} cases but a notable decrease (4.69\%) in \textit{multi-table} scenarios. This pattern is particularly pronounced when using GPT-3.5-turbo, highlighting the \textit{processor}'s critical role in handling complex database information. However, more capable models like GPT-4o may occasionally find this additional processing step redundant, as similarly observed in \textit{``The Death of Schema Linking''}~\cite{death}.


\subsection{Impact of LLM Backbones}
% \paragraph{Better LLMs significantly boost system performance.}
Figure~\ref{fig:pass_rate} illustrates the performance of different methods across three backbone LLMs in \textit{single-table} scenarios.
It can be observed that the pass rate positively correlates with the capacity of the backbone LLMs.
However, an intriguing phenomenon was noted: using GPT-4o-mini resulted in a slight decrease in performance compared to GPT-3.5-turbo.
This unexpected outcome suggests potential limitations in GPT-4o-mini's reasoning abilities for this specific task, despite its overall advancements. 
% In addition, it is worth noting that our current model employs a simple function to translate VQL into Python code, which can be further optimized for better readability.
% Interestingly, the \textit{Processor}'s removal does not cause obvious changes but is critical for GPT-3.5 to process large database information especially when we focus on \textit{multiple} scenarios. However, there is a slight improvement(0.17\%) when using GPT-4o, suggesting that for high-performing models, the additional processing step may introduce noise in some cases, which has been similarly proved in~\cite{death}.
\subsection{Impact of Prompting Techniques}
% \paragraph{Specialized techniques significantly contribute to the framework's effectiveness.} 
Further ablation results of individual prompting techniques within each agent using GPT-3.5-turbo are demonstrated in Table~\ref{tab:ablation_tech}. From this table, we observe that all three techniques in \textit{processor} show similar results. However, the schema filtering proves more beneficial for \textit{multi-table} scenarios (6.52\%), while complexity classification benefits \textit{single-table} scenarios (2.29\%). In the \textit{composer} agent, the sharp decrease (26.53\%) upon removal of in-context learning demonstrates the critical role of example-based prompts in task comprehension, and the significant increase in Invalid Rate also highlights the step-by-step VQL generation.
Moreover, as shown in Table~\ref{code_refine}, we conduct an exploration study for \textit{validator} to refine Python code directly and find that the pass rate decreased by 1.01\%, indicating the effectiveness of using VQL for correction.
We also include several exploration experiments in Appendix~\ref{additional_experiment_result}.

\begin{table}[t]
\centering
% \small
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\vspace{-1em}
\scalebox{0.9}{
\begin{tabular}{l|ccc|c}
\toprule[1.5pt]
\textbf{Setting} & \textbf{Invalid} & \textbf{Illegal} & \textbf{Pass} & \textbf{Tokens}\\
\midrule
VQL Refine & 4.66\% & \textbf{23.97\%} & \textbf{71.36\%} & \textbf{1179}\\
Code Refine & \textbf{4.11\%} & 25.51\% & 70.35\% & 1365\\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Exploration study of Python code refinement. Tokens represent the usage in the refinement stage.}
\label{code_refine}
% \vspace{-1em}
\end{table}

\begin{table}[!t]
\centering
\small
\renewcommand\arraystretch{1.05}
\setlength{\tabcolsep}{3.5pt}

% \vspace{-1em}

% \scalebox{0.98}{
\begin{tabular}{lcc|cc}
\toprule[1.5pt]
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Single-Table}} & \multicolumn{2}{c}{\textbf{Multi-Table}} \\
% \cline{2-5}
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & Elo & 95\% CI & Elo & 95\% CI \\
\midrule
\textbf{\system} & \textbf{1538.27} & +2.95/-2.95 & \textbf{1529.86} & +2.83/-2.84 \\
CoML4Vis & 1506.71 & +3.00/-3.00 & 1514.96 & +3.00/-3.00 \\
Chat2Vis & 1496.71 & +3.05/-3.05 & 1499.44 & +3.01/-3.01 \\
LIDA & 1458.31 & +2.85/-2.85 & 1455.74 & +2.94/-2.93 \\
\bottomrule[1.5pt]
\end{tabular}
% }
\caption{Elo rankings on \textit{single-} and \textit{multi-table} test sets. \system scores the highest in both scenarios.}
\label{tab:elo_rankings}
\vspace{-1em}
\end{table}

% \subsection{Impact of Diverse Examples}
% \paragraph{Diverse examples in few-shot prompts substantially improve system performance.}
% \cdp{do not use `Impact of examples number` as header} 
We carefully design diverse examples including various visualization types (\emph{e.g.}, grouping scatter) and binning operations (\emph{e.g.}, Year, Weekday) for prompting LLM, and Figure~\ref{fig: few-shot-ablation} illustrates the impact of increasing the number of examples in the prompt. The observed improvement in pass rate suggests that the language model effectively leverages knowledge from few-shot prompts.



\subsection{Qualitative Analysis}

\paragraph{ELO Score.}
We adopt the ELO rating system~\cite{elo1978rating}, a widely-used method for calculating relative skill levels, to evaluate model performance.
We conduct this experiment in 1,000 example pairs from \textit{single-} and \textit{multi-table} datasets with equal weights for different models, using human judgments to assess the accuracy of natural language queries. The results in Table~\ref{tab:elo_rankings} show that our \system outperforms other baselines, highlighting its capability to manage complex queries and produce relevant visualizations. Implementation details of the ELO rating framework are in Appendix~\ref{human}.

\paragraph{Case Study.}
Figure~\ref{fig:case_study} presents three cases illustrating NL queries and their visualizations generated by \system and baseline models. The examples showcase \system's superior performance. In the first case, \system correctly orders data by the X-axis, while Chat2Vis and CoML4Vis use the Y-axis. The second case highlights \system's accurate grouping in a stacked bar chart, unlike the baselines. In the third case, involving a \textit{multi-table} query, \system effectively joins tables and groups data for a line chart, whereas Chat2Vis struggles with the structure, and CoML4Vis overlooks the where condition.
% These examples collectively illustrate \system's robust ability to interpret complex queries, manage multi-table datasets, and implement specific visualization requirements, consistently outperforming baseline models.

\begin{figure}[!t]
    \centering
    % 第一张图
    \begin{minipage}[b]{\linewidth}
        \centering
        % \includegraphics[width=0.76\linewidth]{./figure/ErrorAnalysis1_suitable.pdf}
        \includegraphics[width=0.76\linewidth]{./figure/ErrorAnalysis1.pdf}
        % \vspace{1em}
        \subcaption{Error distribution of \system.}
    \end{minipage}

    % \vspace{0.5em}
    \begin{minipage}[b]{\linewidth}
        \centering
        % \includegraphics[width=0.98\linewidth]{./figure/ErrorAnalysis2_suitable.pdf}
        \includegraphics[width=\linewidth]{./figure/ErrorAnalysis2.pdf}
        \subcaption{Errors of different methods in \textit{single-table} dataset. }
    \end{minipage}

    \caption{Error distributions across hardness levels and chart types. SB, GL, and GS refer to Stacked Bar, Grouping Line, and Grouping Scatter, respectively.}
    \label{fig:Combined_ErrorAnalysis}
    % \vspace{-1em}
\end{figure}

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{figure/case_study.pdf}
    % \vspace{-0.5em}
    \caption{Case study of visualization performed by \system and other baselines. The first two cases are from \textit{single-table} dataset and the third from \textit{multi-table} dataset. \system performed well in most complex cases (\emph{e.g.}, stacked bar charts), while other baselines failed.}
    \label{fig:case_study}
    \vspace{-1em}
    % \vspace{-2mm}
\end{figure*}

\paragraph{Error Analysis.} 
As shown in Figure~\ref{fig:Combined_ErrorAnalysis}, \system's performance varies significantly across chart type and difficulty level, particularly with rare queries in temporal data, such as line charts. Our error analysis reveals that failures stem from insufficient handling of temporal information and an imperfect translate function for time-series binning operations. These challenges related to chart complexity and task difficulty underscore the need for better tabular data understanding in LLMs. Our future work can be focused on improving the reasoning abilities of LLMs in temporal information in tabular data. 
