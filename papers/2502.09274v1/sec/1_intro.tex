\section{Introduction}
\label{sec:intro}
LiDAR is one of the most common sensors for perception in autonomous driving. Semantic segmentation on LiDAR point clouds is essential for getting useful and reliable information of the surrounding 3D environment. To solve this 3D scene understanding task, many prior works propose to integrate deep learning techniques because of its remarkable advancements in the past few years. The publication of various annotated datasets~\cite{geiger2012we, caesar2020nuscenes, sun2020scalability} in the domain of autonomous driving further promotes research in the field.\\
In general, those methods can be categorized based on LiDAR data representation into point-based~\cite{zhao2021point, qi2017pointnet++, wu2024ptv3}, voxel-based~\cite{zhou2020cylinder3d, hong2022dynamic} and projection-based methods~\cite{zhang2020polarnet, 2019rangenet++, cheng2022cenet, zhao2021fidnet}. Both point- and voxel-based approaches typically require substantial computational resources due to the need to process data through networks with numerous 3D convolutional layers, intensive feature pre-processing, and deep architectures involving multiple downsampling and upsampling operations. These requirements can result in slow inference speeds, limiting their suitability for real-time applications. In contrast, rasterizing point cloud into range-view images~\cite{cortinhal2020salsanext} is more advantageous in fast and scalable LiDAR perception, because it allows the use of 2D operators for efficient computation and facilitates the transfer of knowledge from camera images~\cite{ando2023rangevit, kong2023rethinking}.
\begin{figure}[t]
  \centering
    \includegraphics[width=0.95\linewidth]{pics/Introduction.png}
    \caption{Performance analysis of LiDAR semantic segmentation on SemanticKITTI~\cite{behley2019semantickitti} \textit{test} set: the size of each circle in the chart represents the number of model parameters. \coolname{}-boosted approaches (marked in \textcolor{orange}{\text{\faBolt}}) show superior trade-offs between computational efficiency and segmentation accuracy.}
    \label{fig:intro}
    \vspace{-7mm}
\end{figure}\\
Nevertheless, learning from range-view representations can suffer from the performance drop caused by the "many-to-one" conflict of adjacent points. To resolve the problem, most approaches maximize the azimuth resolution to make the range image more informative~\cite{2019rangenet++, cortinhal2020salsanext, cheng2022cenet, zhao2021fidnet, gu2022maskrange, kong2023rethinking}. We argue that this is suboptimal for the following reasons: 1) \textbf{High overhead: }increasing resolution significantly raises computational demand, limiting real-time performance. 2) \textbf{Inefficient computation: }the sparse nature of LiDAR data leads to many empty grid cells. Those unoccupied pixels introduce noise in the data and consume resources needlessly. 3) \textbf{Limited informative gains: } due to distortions caused by ego-motion and sensor-internal errors, points mapped to the same azimuthal location do not perfectly align with the laser beams. This misalignment implies that prioritizing both azimuth and elevation resolutions, rather than increasing image width alone, would yield a higher projection rate, as seen in our analysis in Fig.~\ref{fig:occupancy}.
\begin{figure}[b]
\vspace{-7mm}
  \centering
    \includegraphics[width=0.9\linewidth]{pics/Occupancy_validity.png}
    \caption{Statistics on SemanticKITTI~\cite{behley2019semantickitti}: 3D validity (proportion of projected points) with different azimuth (\textbf{W}) and elevation (\textbf{H}) resolutions. Comparable increases are observable when doubling azimuth and elevation resolution ($\Delta V_{azi}, \Delta V_{ele}$).}
    \label{fig:occupancy}
    \vspace{-3mm}
\end{figure}\\
Building on the aforementioned insights, we introduce \coolname{}, a novel training schema for range-view semantic segmentation of LiDAR point clouds. Rather than focusing on designing a new network architecture, \coolname{} targets to address the common limitations outlined above, hence, our proposal is \textbf{generalizable} to any range-view-based approaches. In essence, \coolname{} divides the point cloud into multiple sub-clouds, with only one projected onto a low-resolution image during training. During inference, all sub-clouds are projected and stacked along the batch dimension for processing. This approach increases the projection rate in both azimuth and elevation, thereby \textbf{enriching informativeness} of the range-view representation at \textbf{lower cost}. \\
Downscaling resolution, however, can exacerbate class imbalance in the dataset~\cite{cortinhal2020salsanext}, potentially leading to overfitting during training. Another problem is decreasing 2D occupancy due to splitting the point cloud. To tackle these two incidental issues, we extend the pipeline with two additional data augmentation steps. Furthermore, we explore improvements in post-processing methods, a topic that has received minimal attention in previous works. \\
In summary, our contributions are as follows: \textbf{1)} We introduce \coolname{}, a newly designed training schema for faster and more accurate LiDAR semantic segmentation. \textbf{2)} We integrate two data augmentation techniques tailored to the new schema to enhance the network performance. \textbf{3)} We propose a novel interpolation-based post-processing approach to resolve the "many-to-one" problem more effectively. \textbf{4)} We generalize this schema and all components across various range-view semantic segmentation networks, demonstrating superior performance over baselines on two different benchmarks.
 






