\section{Experimental Analysis}
\subsection{Settings}
\PAR{Datasets }We conduct experiments on two public LiDAR semantic segmentation datasets. \textbf{SemanticKITTI}~\cite{behley2019semantickitti} dataset~\cite{behley2019semantickitti} consists of 22 sequences captured with a 64-beam LiDAR sensor, encompassing 19 semantic classes. The dataset is split as follows: sequences 00 to 10 (excluding 08) are used for training, sequence 08 is reserved for validation, and sequences 11 to 21 are designated for testing. \textbf{nuScenes} dataset~\cite{caesar2020nuscenes} comprises 1,000 driving scenes recorded in Boston and Singapore using a 32-beam LiDAR sensor, leading to a relatively sparse point cloud. After merging similar and infrequent classes, the dataset includes 16 distinct semantic classes.
\PAR{Implementation Details }Prior works experimented mostly with the resolution of $64\times2048$ for \textbf{SemanticKITTI}~\cite{cheng2022cenet, 2019rangenet++, cortinhal2020salsanext}, and $32\times960$~\cite{kong2023rethinking} or $32 \times 2048$~\cite{ando2023rangevit} for \textbf{nuScenes}. In contrast, we reduce the azimuth resolution while increasing the projection rate in \coolname{} mode: resolutions of $64 \times 512$ for \textbf{SemanticKITTI} and $32 \times 480$ for \textbf{nuScenes} are fixed for the input and the full point cloud is split into up to 3 and 2 ($N_{max}$) sub-clouds during training and inference. We directly use custom configurations of prior works~\cite{zhao2021fidnet, cheng2022cenet, cortinhal2020salsanext, ando2023rangevit} to train the networks in \coolname{} mode. For training the selected models (excluding RangeViT) on the \textbf{nuScenes} dataset, we standardize the hyperparameter set since no default configurations are provided. Specifically, we use the AdamW optimizer~\cite{loshchilov2017adamw} along with a OneCycle scheduler~\cite{smith2017onecycle}, setting the maximum learning rate to $1e^{-3}$ and training for 150 epochs. All models are trained on four NVIDIA GeForce GTX 1080Ti in distributed mode.
\PAR{Evaluation Metrics }Following prior works, we assess the performance using Intersection-over-Union (IoU) $\text{IoU}_i = \frac{TP_i}{TP_i + FP_i + FN_i}$ and Accuracy (Acc) $\text{Acc}_i = \frac{TP_i}{TP_i + FP_i}$ for each class \( i \), and calculate the mean Intersection-over-Union (mIoU) and mean Accuracy (mAcc) across all classes. \( TP_i \), \( FP_i \), and \( FN_i \) represent the true positives, false positives, and false negatives for class \( i \), respectively.

\subsection{Comparative Study}
\input{tables/qualitative_results.tex}
We compare \coolname{} with baseline models across two datasets. As shown in Tab.~\ref{table:semantickitti_nuscenes}, all four networks see significant improvements: SalsaNext has mIoU gains of 5.3\% on SemanticKITTI and 2.3\% on nuScenes, FIDNet improves by 7.9\% and 3.9\%, and CENet by 3.3\% and 3.1\%. RangeViT, as a ViT-based network, also exhibits huge enhancement in performance, confirming \coolname{}'s generalizability across different architectures. This improvement is especially prominent for smaller, dynamic, and under-represented classes such as \textit{truck}, \textit{motorcycle}, \textit{bicycle}, \textit{pedestrian} and \textit{bicyclist}. In Fig.~\ref{fig:qualitative_results}, we present a visual comparison. Notably, with the support of \coolname{}, the network demonstrates improved accuracy in segmenting foreground objects. An exception arises with the \textit{motorcyclist} class in SemanticKITTI, where IoU scores decrease compared to the baseline. Diving into the problem, this can be traced back to the extremely low occurrence of annotations for that class in the dataset. In standard training on low-resolution range images, this class already suffers from poor representation. In \coolname{} mode, the occurrence is further reduced by splitting of the point cloud. This accumulation of downsampling prevents the network from optimizing on that rare class effectively and lead to inferior performance. In contrast, the improvement on nuScenes is more consistent as class frequencies are better balanced. We regard this as a corner case when testing on an class-imbalanced dataset. As a future work to resolve the issue, we aim to explore 3D reconstruction techniques to generate real-world-like pseudo LiDAR point clouds for augmentation~\cite{chang2024just, manivasagam2020lidarsim}.\\
\noindent In Tab.~\ref{tab:SOTA}, we further compare \coolname{}-boosted networks with other state-of-the-arts from various modalities. Given that methods we select preserve relatively fewer number of parameters, \coolname{} helps to improve the segmentation accuracy, being comparable to other point- or voxel-based approaches that deploy much larger and deeper networks. In addition, they outperform others significantly in latency, achieving superior trade-offs in accuracy and efficiency.
\input{tables/semantickitti_test_v2.tex}
\input{tables/val_test.tex}
\subsection{Ablation Study}
To perform the ablation study, we test with CENet~\cite{cheng2022cenet} on \textit{val} set of SemanticKITTI~\cite{behley2019semantickitti}. %
\PAR{Training Schema} As shown in Tab.\ref{tab:ablation_ts}, we conduct a comprehensive evaluation to examine the trade-off between accuracy and efficiency under different training configurations. In the standard mode, a high-resolution range image is used as the input to reproduce baseline results. While training in the STR paradigm\cite{kong2023rethinking}, there is a slight performance drop, the memory consumption is significantly reduced by partitioning the full point cloud through azimuth-wise grouping. The limited increase in latency is due to the fact that STR has to additionally unite the batched prediction along azimuth resolution and the post-processing is still performed on the high-resolution image. In contrast, \coolname{} achieves remarkable improvements, showing increases of 2.7\% in mIoU, 2.1\% in mAcc, and 45\% acceleration in inference. Similar to STR, \coolname{} reduces memory consumption compared to the standard mode, further optimizing the efficiency of range-view semantic segmentation. %
\begin{table}[H]
\vspace{-3mm}
\renewcommand \arraystretch{1}
\resizebox{\columnwidth}{!}{%
    \begin{tabular}{ l|c|c|c|c } 
    \hline
     Method  & Input resolution & mIoU  & mAcc   & Lat.   \\ 
    \hline
    Standard & $1\times64\times2048$ & 64.8 & 77.2 & 44 ms \\
    STR~\cite{kong2023rethinking} & $1(5)\times64\times480$  & 64.3 & 76.8 & 41 ms\\ 
    \coolname &  $1(3)\times64\times512$ & \textbf{67.5} & \textbf{79.3} & \textbf{24 ms} \\
    \hline
    \end{tabular}
    }
\caption{Performance with varying training schemes is evaluated on \textit{val} set of SemanticKITTI~\cite{behley2019semantickitti}. Input resolution is formatted in $B\times H \times W$ (batch size, image width and height). For STR~\cite{kong2023rethinking}, we use the configuration yielding the best performance on CENet~\cite{cheng2022cenet}: a full resolution of $64\times1920$ split into four $64\times480$ sub-images. For the fair comparison, all models are integrated with proposed components and trained from scratch.}
    \vspace{-4.5mm}
    \label{tab:ablation_ts}
\end{table}

\PAR{Data Augmentation} As presented in Fig.~\ref{fig:wpd+}, various data augmentation methods can enhance the segmentation performance by large margins and our WPD+ performs the best among them, which increases baseline mIoU by 5.7\% and mAcc by 2.9\%. Mix3D~\cite{nekrasov2021mix3d} increases the contextual information per frame by fusing one scene to another, however, the model can still suffer from class imbalance. RangeAug~\cite{kong2023rethinking} consists of 4 different range-wise operations to enrich the semantic and structural cues. From their experiments, it demonstrated that this augmentation technique is especially effective for attention-based networks~\cite{vaswani2017attention, dosovitskiy2020image}, which possess significantly higher model capacities and are more reliant on data diversity for optimal performance. Similar to their experimental results, applying this method to lightweight CNN-based networks has shown limited success in improvement.
\begin{figure*}[t]
\centering
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{pics/DA_Weighted_Paste_Drop.png}
    \caption{WPD+}
    \label{fig:wpd+}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{pics/WPD_Number_of_Frames.png}
    \caption{\#Frames sampled in WPD+}
    \label{fig:wpd+_frames}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{pics/DA_WPD_with_Carla_Dataset.png}
    \caption{Inclusion of synthetic dataset}
    \label{fig:wpd+_carla}
\end{subfigure}
\begin{subfigure}{.24\textwidth}
    \centering
    \includegraphics[width=\linewidth]{pics/DA_Multi_Clouds_Fusion.png} 
    \caption{MCF}
    \label{fig:mcf}
\end{subfigure}
\caption{a) Initialization of training with standard geometric data augmentation (GDA) and benchmark several state-of-the-art 3D augmentation techniques, including Mix3D~\cite{nekrasov2021mix3d}, RangeAug~\cite{kong2023rethinking}, and original WPD~\cite{gu2022maskrange}. b) Different number of sampled frames for best performance c) A comparative plot showing the IoU scores of top-rare classes in scenarios both with and without the inclusion of the synthetic dataset. As reference, the class frequencies in the \textit{val} set are provided. d) The models are trained using two different input configurations: either a single range image derived from the full cloud (FC) or a range image generated from a sub-cloud (SC), and inferred in \coolname{} mode. Note that for all trained models in a) - d), we leverage KNN Ensembling during post-processing phase.
}
\vspace{-5mm}
\label{fig:ablation_da}
\end{figure*}\\
WPD+ includes two tunable parameters: the number of sampled frames from the original dataset and the use of the synthetic dataset. To find out the best parameter set, we conduct two additional experiments. Fig.~\ref{fig:wpd+_frames} shows that sampling 6 frames results in the optimal performance, while increasing the number of frames beyond this point leads to performance degradation. This is likely due to the model's limited scalability, similar to the effects observed in case of RangeAug\cite{kong2023rethinking}, in addition, pasting too many pixels belonging to specific classes will again corrupt the semantic balance. Furthermore, Fig.~\ref{fig:wpd+_carla} illustrates that the synthetic dataset plays a key-role in refining semantic prediction of top-rare classes. Noteworthily, using the synthetic dataset is efficient and practical because this allows us to customize sensor configurations to align with the target dataset and to define specific objects within the scene for downstream applications without any labor cost.\\
In the next stage, we study how unoccupied pixels in the range image can affect the performance. Firstly, we use the full point cloud for training and sub-clouds for inference, as the decreasing azimuth resolution can already resolve the problem of low 2D occupancy (the first column in Fig.~\ref{fig:mcf}). However, empirical results reveal a drop in 2D accuracy when inferring from sub-clouds. This is possibly because of the domain shift caused by the occupancy difference in range images. In the next trial, we use the sub-cloud as training input instead, but this directly limits the performance due to the occupancy reduction. To tackle this compounded challenge brought by \coolname{}, we introduce Multi-Cloud Fusion, an additional data augmentation step which fuses multiple sub-clouds through occupancy padding during projection phase. As shown in Fig.~\ref{fig:mcf}, using MCF exhibits the highest performance in the series, achieving 1\% increase in IoU over the model trained on full cloud. As an alternative, we apply RangeIP~\cite{xu2023frnet}, an interpolation-based augmentation technique, to enhance 2D occupancy in the range image, but it results in slight worse accuracy compared to the baseline.
\begin{figure*}[!ht]
\vspace{-2mm}
\centering
\begin{subfigure}{.32\linewidth}
    \includegraphics[width=\linewidth]{pics/Postprocessing.png} 
    \caption{Ablation on Post-Processing}
    \label{fig:post_processing_eval}
\end{subfigure}
\begin{subfigure}{.32\linewidth}
    \includegraphics[width=\linewidth]{pics/PostProcessing_Inference_Time.png} 
    \caption{Inference Time [ms]}
    \label{fig:inference_time}
\end{subfigure}
\begin{subfigure}{.32\linewidth}
    \includegraphics[width=\linewidth]{pics/Input_Resolution.png} 
    \caption{Ablation on Input Resolution}
    \label{fig:input_resolution}
\end{subfigure}
\caption{a) - b): Various post-processing techniques are applied to the same trained model. KNN~\cite{2019rangenet++} and NLA~\cite{zhao2021fidnet} are used on the single range image generated from the full point cloud, whereas KNN Ensembling operates on multiple range images derived from sub-clouds. For NNRI, we assess it using both full cloud and sub-clouds. c): Evaluation on different input resolutions and corresponding number of sub-clouds ($N_{max}$).}
\vspace{-5mm}
\label{fig:ablation_postprocessing}
\end{figure*}
\PAR{Post-Processing }As introduced in Sec.~\ref{sec:post-processing}, a simple LiDAR model is built to approximate the distance-density function of 3D points and compute cut-off values for valid neighbors extraction. To verify the necessity of the step, some qualitative results are provided in Fig.~\ref{fig:cutoff-values}. As can be seen, the adaptive cut-off values refine semantic predictions by better accommodating objects with varying density scales. 
\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.15\textwidth}
  \centering
    \includegraphics[width=\textwidth]{pics/cutoff/cutoff_scene.png}
    \caption{Scene}
  \end{subfigure}
  \begin{subfigure}[b]{0.1\textwidth}
  \centering
    \includegraphics[width=\textwidth]{pics/cutoff/cutoff_gt.png}
    \caption{GT}
  \end{subfigure}
  \begin{subfigure}[b]{0.1\textwidth}
  \centering
    \includegraphics[width=\textwidth]{pics/cutoff/cutoff_constant.png}
    \caption{Constant}
  \end{subfigure}
  \begin{subfigure}[b]{0.1\textwidth}
  \centering
    \includegraphics[width=\textwidth]{pics/cutoff/cutoff_adaptive.png}
    \caption{Adaptive}
  \end{subfigure}
  \caption{Segmentation results with different cut-off values in NNRI: in the case of constant value (set at 1), overlapping points of \textcolor{SemanticKITTI-Road}{Road} are partially misclassified as \textcolor{SemanticKITTI-Car}{Car} in the top image. Similarly, in the bottom image, half of the points that belong to \textcolor{SemanticKITTI-Building}{Building} are incorrectly predicted as \textcolor{SemanticKITTI-Veg}{Vegetation}.}
  \label{fig:cutoff-values}
  \vspace{-7.5mm}
\end{figure}\\
Next, we explore the impact of various post-processing techniques on segmentation performance in Fig.~\ref{fig:post_processing_eval} and \ref{fig:inference_time}. Regarding conventional KNN~\cite{2019rangenet++} as the baseline, NLA~\cite{zhao2021fidnet} demonstrates similar performance in both accuracy and latency. In contrast, we deploy our approach (NNRI) in the standard mode as well and observe a significant improvement: inference time is cut nearly 16\% compared to KNN, while mAcc and mIoU increase by 2.8\% and 2\%, respectively. Unlike KNN, NNRI avoids the computational cost of Gaussian kernel calculations for distance weighting and directly performs nearest neighbor searches on the range image instead of in 3D space, further reducing computational overhead. NNRI interpolates class-wise scores based on relative depths rather than directly voting on hard labels, relying more on weighted information from nearest neighbors, which is the major reason why it outperform other post-processing approaches.\\ %
Switching to \coolname{} mode, we first extend the standard KNN approach to KNN Ensembling, which iteratively gathers votes from all points in each sub-cloud and aggregates them for the final prediction. While this extension improves the accuracy, it comes at approximately doubled latency cost. Conversely, when NNRI is adapted, it consistently provides notable improvements in both efficacy and efficiency. As a reference, we included evaluation scores on 2D predictions (\texttt{--} dashed lines in Fig.~\ref{fig:post_processing_eval}), showing that \coolname{} with NNRI significantly narrows the accuracy gap between 2D and 3D predictions. This suggests that our approach effectively mitigates the "many-to-one" problem, offering substantial gains in segmentation performance.
\PAR{Input Resolution} To further explore the optimal results in \coolname{} mode, we test various input resolutions, as shown in Fig.~\ref{fig:input_resolution}. From the experimental results, we found that resolutions of $512$ and $640$ deliver the best mIoU scores. Increasing the azimuth resolution beyond this point causes a slight performance drop. Nevertheless, all tested configurations outperform the baseline (the first row in Tab.\ref{tab:ablation_ts}), demonstrating the superb effectiveness of our approach on range-view LiDAR semantic segmentation.







