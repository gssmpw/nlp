\section{Related Works}
\label{sec:related_works} 
\PAR{Point- and Voxel-based methods} Some recent works~\cite{qi2017pointnet++, zhao2021point, wu2022ptv2} use raw point cloud data as direct network input, eliminating the need for post-processing after prediction. However, these methods often face high computational complexity and memory usage. To address these issues, \citet{hu2020randla} introduced sub-sampling and feature aggregation techniques for large-scale point clouds to reduce computational costs. Despite these efforts, performance degradation remains significant. Other works~\cite{zhou2020cylinder3d, lai2023spherical} use 3D voxel grids as input, achieving point-based accuracy with reduced computational costs by utilizing sparse 3D convolutions~\cite{choy2019minkowski}. Nonetheless, voxelization and de-voxelization steps continue to be time- and memory-intensive.
\PAR{Range-view-based methods} To address inefficiencies, some prior works~\cite{wu2018squeezeseg, 2019rangenet++, wu2019squeezesegv2} convert the large-scale point cloud to panoramic range image through spherical projection and leverage image segmentation techniques for LiDAR data. SalsaNext~\cite{cortinhal2020salsanext} uses a Unet-like network with dilated convolutions to broaden receptive fields for more accurate segmentation, while Lite-HDSeg~\cite{razani2021lite} introduces an efficient framework using a lite version of harmonic convolutions. Additionally, FIDNet~\cite{zhao2021fidnet} and CENet ~\cite{cheng2022cenet} interpolate and concatenate multi-scale features with a minimal decoder for semantic prediction. These methods share the benefit of lightweight network design, significantly improving efficiency and enabling real-time applications. Nevertheless, they generally underperform 3D methods due to the “many-to-one” issue, where multiple points project to the same pixel. To offset the performance drop caused by the problem, some other recent works propose to use Vision Transformer (ViT)~\cite{dosovitskiy2020image, xie2021segformer, wang2021pyramid}. RangeViT~\cite{ando2023rangevit} deploys standard ViT backbone as encoder, followed by a light-weight decoder for refining the coarse patch-wise ViT representations, while RangeFormer~\cite{kong2023rethinking} utilizes a pyramid-wise ViT-encoder to extract multi-scale features from range images. ViTs offer higher model capacities and excel at capturing long-range dependencies by modeling global interactions between different regions, enhancing segmentation performance over traditional CNNs~\cite{deininger2022comparative}. However, the quadratic computational complexity of self-attention mechanisms in ViTs introduces challenges in achieving an optimal balance between efficiency and accuracy.
\PAR{Training schema} Highlighting inefficiencies stemming from the use of high-resolution range images, some methods have adopted compact networks, significantly reducing network capacity~\cite{zhao2021fidnet, cortinhal2020salsanext, cheng2022cenet}. Unfortunately, high-resolution range images still demand substantial memory, which restricts scalability in terms of batch size and data throughput. Lowering the resolution exacerbates information loss, leading to inferior results. To address this, \citet{kong2023rethinking} proposed Scalable Training from Range view (STR), a strategy that divides range images into multiple sub-images from different perspectives to reduce memory consumption. Although STR lessens memory usage, it results in a slight drop in segmentation accuracy and only minor improvement in inference speed compared to the baseline.
\PAR{Augmentation} Data augmentation plays a crucial role in helping models learn more generalized representations, thereby enhancing scalability. For example, Mix3D~\cite{nekrasov2021mix3d} introduced an out-of-context mixing strategy by fusing two scenes. Similarly, MaskRange~\cite{gu2022maskrange} proposed a weighted paste-drop augmentation to manually balance the class frequencies, while RangeFormer~\cite{kong2023rethinking} employed four consecutive range-wise operations to provide richer semantic and structural cues in the scene. In this work, we introduce two novel augmentation steps, including point-wise and range-wise fusion, specifically designed for the range images utilized in our new schema.
\PAR{Post-Processing} Addressing the prevalent "many-to-one" problem in range-view representations often necessitates a post-processing step to upsample 2D predictions, a critical yet underexplored area in prior research. For example, RangeViT~\cite{ando2023rangevit} introduced a trainable 3D refiner using KPConv~\cite{thomas2019kpconv}. However, while it directly optimizes 3D semantics, the performance improvement is limited, and the approach adds significant computational overhead. Some methods rely on conventional unsupervised techniques to infer semantics for 3D points; for instance, \citet{2019rangenet++} proposed a KNN-based voting approach, and \citet{zhao2021fidnet} introduced Nearest Label Assignment (NLA), which assigns labels based on the closest labeled point in 3D space. Nevertheless, these unsupervised techniques often struggle with accurately predicting boundaries and distant points, with performance further declining as range-image resolution decreases. To overcome these limitations, we design a new post-processing method that better interpolates the predictions of unprojected points using neighborhood information.



