@inproceedings{booth2023perils,
  title={The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications},
  author={Booth, Serena and Knox, W Bradley and Shah, Julie and Niekum, Scott and Stone, Peter and Allievi, Alessandro},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={5920--5929},
  year={2023}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{du2019liir,
  title={Liir: Learning individual intrinsic reward in multi-agent reinforcement learning},
  author={Du, Yali and Han, Lei and Fang, Meng and Liu, Ji and Dai, Tianhong and Tao, Dacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{jaques2019social,
  title={Social influence as intrinsic motivation for multi-agent deep reinforcement learning},
  author={Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, DJ and Leibo, Joel Z and De Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={3040--3049},
  year={2019},
  organization={PMLR}
}

@misc{kapoor2024assigningcreditpartialreward,
      title={Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal Policy Optimization}, 
      author={Aditya Kapoor and Benjamin Freed and Howie Choset and Jeff Schneider},
      year={2024},
      eprint={2408.04295},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2408.04295}, 
}

@article{knox2023reward,
  title={Reward (mis) design for autonomous driving},
  author={Knox, W Bradley and Allievi, Alessandro and Banzhaf, Holger and Schmitt, Felix and Stone, Peter},
  journal={Artificial Intelligence},
  volume={316},
  pages={103829},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{leerlaif,
  title={RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie Ren and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others},
  booktitle={Forty-first International Conference on Machine Learning}
}

@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@inproceedings{lin2024navigating,
  title={Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models},
  author={Lin, Muhan and Shi, Shuyang and Guo, Yue and Chalaki, Behdad and Tadiparthi, Vaishnav and Pari, Ehsan Moradi and Stepputtis, Simon and Campbell, Joseph and Sycara, Katia},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  year={2024}
}

@inproceedings{liu2023lazy,
  title={Lazy agents: a new perspective on solving sparse reward problem in multi-agent reinforcement learning},
  author={Liu, Boyin and Pu, Zhiqiang and Pan, Yi and Yi, Jianqiang and Liang, Yanyan and Zhang, Du},
  booktitle={International Conference on Machine Learning},
  pages={21937--21950},
  year={2023},
  organization={PMLR}
}

@misc{rashid2020monotonicvaluefunctionfactorisation,
      title={Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning}, 
      author={Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
      year={2020},
      eprint={2003.08839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.08839}, 
}

@misc{schulman2017proximal,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{su2020valuedecompositionmultiagentactorcritics,
      title={Value-Decomposition Multi-Agent Actor-Critics}, 
      author={Jianyu Su and Stephen Adams and Peter A. Beling},
      year={2020},
      eprint={2007.12306},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2007.12306}, 
}

@article{sunehag2017value,
  title={Value-decomposition networks for cooperative multi-agent learning},
  author={Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z and Tuyls, Karl and others},
  journal={arXiv preprint arXiv:1706.05296},
  year={2017}
}

@misc{wen2022multiagentreinforcementlearningsequence,
      title={Multi-Agent Reinforcement Learning is a Sequence Modeling Problem}, 
      author={Muning Wen and Jakub Grudzien Kuba and Runji Lin and Weinan Zhang and Ying Wen and Jun Wang and Yaodong Yang},
      year={2022},
      eprint={2205.14953},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2205.14953}, 
}

@misc{yu2022surprisingeffectivenessppocooperative,
      title={The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games}, 
      author={Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},
      year={2022},
      eprint={2103.01955},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.01955}, 
}

@article{zhangsimple,
  title={A Simple Framework for Intrinsic Reward-Shaping for RL using LLM Feedback},
  author={Zhang, Alex and Parashar, Ananya and Saha, Dwaipayan}
}

@misc{ziegler2020finetuninglanguagemodelshuman,
      title={Fine-Tuning Language Models from Human Preferences}, 
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2020},
      eprint={1909.08593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08593}, 
}

