\section{Related Works}
Credit assignment in multi-agent reinforcement learning remains a fundamental challenge, especially in environments with sparse team rewards. There are two main classes of traditional approaches for the credit assignment, value decomposition \cite{sunehag2017value, rashid2020monotonicvaluefunctionfactorisation, du2019liir, foerster2018counterfactual} and slight modifications to known algorithms such as the gradient-descent decomposition \cite{su2020valuedecompositionmultiagentactorcritics}. 

There are also some works that combine the basic ideas of both method classes. \cite{kapoor2024assigningcreditpartialreward} adapts the partial reward decoupling into MAPPO \cite{yu2022surprisingeffectivenessppocooperative} to eliminate contributions from other irrelevant agents based on attention. The other work \cite{wen2022multiagentreinforcementlearningsequence} utilizes transformer with PPO loss \cite{schulman2017proximal} adapted to the value decomposition idea.

The methods above have made progress in assigning credits, but their effectiveness diminishes with delayed or sparse rewards. For example, the work \cite{liu2023lazy} shows the poor performance of QMIX \cite{rashid2020monotonicvaluefunctionfactorisation} with sparse rewards. However, designing dense rewards to combat this challenge is difficult given the complexity of tasks ~\cite{leike2018scalable, knox2023reward, booth2023perils}. Although social-influence-based rewarding calculates dense individual rewards \cite{jaques2019social}, it requires teammates' behavior models, which often need additional training to estimate and update.

One general method of generating dense rewards, particularly in single-agent settings, is Reinforcement Learning with Human Feedback (RLHF) \cite{christiano2017deep} and its extension, Reinforcement Learning with AI Feedback (RLAIF) \cite{leerlaif}. These methods have been successfully applied in domains like text summarization and dialogue generation \cite{ziegler2020finetuninglanguagemodelshuman}, where human or AI-generated feedback is used in training in the absence of clear environmental rewards. However, these approaches are limited to single-agent environments and do not address the unique requirements and challenges that exist within the multi-agent counterparts, according to RLAIF. \cite{zhangsimple} shows one direction of generating dense rewards for credit assignment with LLM in multi-agent scenarios. Utilizing the coding capabilities of LLM, this method iteratively queries LLM to generate multiple reward functions with high density and refine the reward functions gradually in the training process. However, this method can suffer from LLM hallucination problems, which can cause misleading rewards due to inconsistent rankings or other ranking errors. Considering these problems, our method adapts the potential-based RLAIF \cite{lin2024navigating}, which can handle LLM hallucination with the multi-query approach, from the single-agent scenarios to multi-agent ones, and successfully handles the credit assignment problem.