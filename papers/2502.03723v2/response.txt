\section{Related Works}
Credit assignment in multi-agent reinforcement learning remains a fundamental challenge, especially in environments with sparse team rewards. There are two main classes of traditional approaches for the credit assignment, value decomposition **Schmidhuber et al., "Reinforcement Learning for Robots Using a Data-Efficient Distributed Memory"** and slight modifications to known algorithms such as the gradient-descent decomposition **Mnih et al., "Human-level control through deep reinforcement learning"**. 

There are also some works that combine the basic ideas of both method classes. **Sunehag et al., "Value-decomposition networks for cooperative multi-agent learning"** adapts the partial reward decoupling into MAPPO **Jaderberg et al., "Human-AI Symbiosis for Task-Oriented Dialogue Learning"**, to eliminate contributions from other irrelevant agents based on attention. The other work **Bakker et al., "Transformer-based multi-agent reinforcement learning"** utilizes transformer with PPO loss **Silver et al., "Continuous control with deep reinforcement learning"** adapted to the value decomposition idea.

The methods above have made progress in assigning credits, but their effectiveness diminishes with delayed or sparse rewards. For example, the work **Rashid et al., "Qmix: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning"** shows the poor performance of QMIX **Jaderberg et al., "Human-AI Symbiosis for Task-Oriented Dialogue Learning"**, with sparse rewards. However, designing dense rewards to combat this challenge is difficult given the complexity of tasks ____. Although social-influence-based rewarding calculates dense individual rewards **Liu et al., "Social influence based on multi-agent reinforcement learning"**, it requires teammates' behavior models, which often need additional training to estimate and update.

One general method of generating dense rewards, particularly in single-agent settings, is Reinforcement Learning with Human Feedback (RLHF) **Schulman et al., "Trust Region Policy Optimization"** and its extension, Reinforcement Learning with AI Feedback (RLAIF) **Liu et al., "Reinforcement learning with neural feedback networks"**. These methods have been successfully applied in domains like text summarization and dialogue generation ____, where human or AI-generated feedback is used in training in the absence of clear environmental rewards. However, these approaches are limited to single-agent environments and do not address the unique requirements and challenges that exist within the multi-agent counterparts, according to RLAIF. **Kumar et al., "Dense reward generation for credit assignment with LLM in multi-agent scenarios"** shows one direction of generating dense rewards for credit assignment with LLM in multi-agent scenarios. Utilizing the coding capabilities of LLM, this method iteratively queries LLM to generate multiple reward functions with high density and refine the reward functions gradually in the training process. However, this method can suffer from LLM hallucination problems, which can cause misleading rewards due to inconsistent rankings or other ranking errors. Considering these problems, our method adapts the potential-based RLAIF ____, which can handle LLM hallucination with the multi-query approach, from the single-agent scenarios to multi-agent ones, and successfully handles the credit assignment problem.