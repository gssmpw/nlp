\section{Related Works}
Credit assignment in multi-agent reinforcement learning remains a fundamental challenge, especially in environments with sparse team rewards. There are two main classes of traditional approaches for the credit assignment, value decomposition ____ and slight modifications to known algorithms such as the gradient-descent decomposition ____. 

There are also some works that combine the basic ideas of both method classes. ____ adapts the partial reward decoupling into MAPPO ____ to eliminate contributions from other irrelevant agents based on attention. The other work ____ utilizes transformer with PPO loss ____ adapted to the value decomposition idea.

The methods above have made progress in assigning credits, but their effectiveness diminishes with delayed or sparse rewards. For example, the work ____ shows the poor performance of QMIX ____ with sparse rewards. However, designing dense rewards to combat this challenge is difficult given the complexity of tasks ____. Although social-influence-based rewarding calculates dense individual rewards ____, it requires teammates' behavior models, which often need additional training to estimate and update.

One general method of generating dense rewards, particularly in single-agent settings, is Reinforcement Learning with Human Feedback (RLHF) ____ and its extension, Reinforcement Learning with AI Feedback (RLAIF) ____. These methods have been successfully applied in domains like text summarization and dialogue generation ____, where human or AI-generated feedback is used in training in the absence of clear environmental rewards. However, these approaches are limited to single-agent environments and do not address the unique requirements and challenges that exist within the multi-agent counterparts, according to RLAIF. ____ shows one direction of generating dense rewards for credit assignment with LLM in multi-agent scenarios. Utilizing the coding capabilities of LLM, this method iteratively queries LLM to generate multiple reward functions with high density and refine the reward functions gradually in the training process. However, this method can suffer from LLM hallucination problems, which can cause misleading rewards due to inconsistent rankings or other ranking errors. Considering these problems, our method adapts the potential-based RLAIF ____, which can handle LLM hallucination with the multi-query approach, from the single-agent scenarios to multi-agent ones, and successfully handles the credit assignment problem.