\section{Related Work}
\label{subsec:related}

Compared to the vast literature on $k$-means clustering as a whole,
we are only aware of a handful of works that have attempted to apply
any form of approximate nearest-neighbor search to any form of clustering.

\paragraph{Works using ANNS to Accelerate Lloyd's algorithm}
A few existing works have applied
methods for approximate nearest-neighbor search
in a black-box fashion
to accelerate Lloyd's algorithm (or variants)
in various contexts of $k$-means clustering.
Note that we will omit discussion of methods
that are essentially just dimension reduction techniques,
for which there are many works.

Several of these use a similar approach to
the general ``black-box'' methodology
we thoroughly test in
\cref{subsec:black-box-exp}.
\citet{PhilbinCISZ07} presented a method
greedily traversing randomized k-d trees as an ANNS heuristic for this purpose,
but they do not test their solution
w.r.t. $k$-means clustering score
(although some later work uses their solution as a baseline).
\citet{GongPYBBF15} applied techniques for
locality-sensitive hashing
(LSH)
--- which is a subclass of
space-partitioning methods (see the survey by \citet{lshsurvey}) --- 
to binary data under Hamming distance
with ``mini-batch'' $k$-means local search
(\citet{Sculley10} gives a discussion of mini-batches,
which can be seen as a modified form of Lloyd's algorithm
that maintains parallelizability).
\citet{HuWBZC17} present a similar method,
instead using Hamming LSH with a ``reranking'' step
to cluster Euclidean data via binary code quantization.
Note that ANNS over Hamming distance is generally easier than
Euclidean distance, since it a special case.
Moreover, locality-sensitive hashing
is now significantly outperformed
by modern ANNS techniques in practice~\cite{aumuller2020ann},
so this is likely not the most effective use of this black-box approach
(as we will see in \cref{subsec:black-box-exp}).
As part of an implementation
for a variant of ``Product Quantization'',
\citet{baranchuk2018revisiting} applied
HNSW to $k$-means clustering over
relatively small chunks of data.
Their method for doing so
is similar to our initial ``black-box'' methodology
presented in
\cref{subsec:black-box-exp},
although they focus on a much smaller-scale case,
and they did not provide any empirical justification
for their choice of HNSW over other ANNS methods,
nor did they empirically test their methods w.r.t. the $k$-means clustering objective
(nor was obtaining a good clustering score their goal).

A few works have also explored a different method of applying ANNS techniques
to the assignment stage of Lloyd iterations:
Instead of assigning dataset points in $P$ to their nearest center in $C$,
the centers can instead ``flood fill'' the dataset
using certain types of ANNS data structures
constructed over $P$ instead of $C$.
We will call this the \defn{inverse assignment} method.
\citet{kmnpsw-ekmca-02} applied the inverse assignment method
to compute \emph{exact} assignments.
\citet{AvrithisKAE15}
applied the inverse assignment method
by essentially projecting into a quantized two-dimensional
space (thereby doing a quantized dimension reduction).
\citet{WangWKZL15}
also employ a variation of the inverse assignment method
by constructing an approximate neighborhood graph,
which they then leverage to prune distance computations.
In particular, they construct their graph
using
random-projection trees~\cite{DasguptaF08},
an ANNS space-partitioning method.
Unfortunately,
the inverse assignment method is limited to
(small) data sets that can fit wholly in-memory,
since it requires a more careful traversal of
a specialized ANNS data structure built for $P$, rather than $C$.
This is in contrast to methods that build structures
over the
``forward'' assignment method
(including ours),
which only need to build and store a structure
for $k$ points in the metric space ---
even for excessively large values of $k$ (e.g. $k=|P|/100$),
this is still a very significant difference
in memory usage for massive datasets.
There is one method for which this limitation can
be overcome for the inverse method:
\citet{MatsuiOYA17} suggest
using product quantization
on the input vectors.
Since this is a quantization method,
rather than another form of ANNS method,
applying the inverse method
does not actually prune any distance computations,
but rather just speeds them up individually.
The experimental results of \citet{MatsuiOYA17}
suggest that, although faster,
their method cannot achieve the same score
as Lloyd's algorithm,
very quickly arriving at poor local optimums even for quite small values of $k$.
In particular,
since their method amounts to a brute force
with quantization methods,
this suggests that any similar approaches
applying an ANNS technique to Lloyd's algorithm
involving any sort of quantization is likely to result
in poor local optimums.
As we will see, this appears
to be true in our results as well.

Compared to all of these approaches,
we present a more complete analysis
of ANNS methods
for Lloyd's algorithm,
and we furthermore determine that they
are not effective without further work.
Moreover, we complete this further work,
eventually devising seeded search-graphs.


\paragraph{Other Forms of Clustering}
To the best of our knowledge, only one academic work has studied the application of approximate nearest-neighbor search to large-scale clustering that is not $k$-means clustering:
PECANN~\cite{yu2023pecann,yu2024parallel} studies the application of black-box approximate nearest-neighbor search methods to hierarchical density-based clustering.

Although not an academic work,
the software library USearch~\cite{usearch} can produce a hierarchical clustering using HNSW,
although the developer has not
publicized any experimental results or detailed documentation on their methodology,
and the feature is still marked as in-development.
Their method appears to involve treating each point on a non-zero level as a cluster ``center'',
and performing a simultaneous flood fill from all points in a non-zero level to the points of the dataset in the level below.
This would be similar to performing a random sample initialization of $k$-means, with no local search iterations,
and performing the ``assignment'' stage in a way so as to perform graph-based clustering
(i.e., approximating geodesic distance over a manifold approximated by the graph).

\newpage