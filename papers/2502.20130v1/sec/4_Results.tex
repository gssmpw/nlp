\section{Experiments}
\glsreset{resNet}
\glsreset{denseNet}
\glsreset{incv}
\glsreset{cubheader}
\glsreset{travelingheader}
\glsreset{stanfordheader}
\glsreset{fgvcheader}
\glsreset{imgnetheader}
\glsreset{PIP-Net}
Following prototype-based methods we applied our method to \cubheader{} and \stanfordheader{}. 
To showcase \gls{NewlayerName}'s broad applicability, we also include results on the large-scale dataset \imgnetheader, to which most interpretable methods are not applicable.
Notably, \cubheader{} contains annotations of human concepts which we use to measure \cubsim.
An overview of the used datasets is shown in \suppl{}  \cref{table:DatasetOverview}.
As our method is independent of the used backbone, 
we evaluated it across various architectures, but focus on \resnet{} in this paper.
Similar results on Resnet34,  \gls{incv} and Swin Transformer~\citep{liu2021swin}, as well as detailed results with standard deviations, are included in \suppl{} \cref{suppsec:Results}.
We do not apply our method to other interpretable models like \gls{PIP-Net}, as QPM is an alternative way of inducing compactness and the features of \gls{PIP-Net} are not general, thus ill-suited for a broad assignment.


\subsection{Implementation Details}
We generally followed \gls{PIP-Net} for the data preparation.
Specifically, the images are first cropped to the ground truth bounding box for \cubheader{} and \travelingheader{}.
For all datasets, the images are resized to $224\times224$. Following \gls{PIP-Net}, \textit{TrivialAugment}~\citep{Muller_2021_ICCV} is used and %
the strides of ResNets
are also set
to 1 to obtain more fine-grained feature maps.
The remaining parameters, including dense training for $150$ epochs on fine-grained datasets and directly using the pretrained model on \imgnetheader{} with subsequent $40$ epochs of fine-tuning, mirror the \gls{layerName}  and are described in \cref{suppsec:impl}.
Note that \gls{NewlayerName} is trained more efficiently than \gls{qsenn}, as it does not use multiple training iterations during fine-tuning.
We set $\gls{nperClass}=5$ and $\gls{nReducedFeatures}=50$ for \gls{NewlayerName}, unless stated otherwise.
We demonstrate the impact of changing the parameters in the ablation studies but choose these, as it is in line with prior literature~\citep{norrenbrock2024q, norrenbrocktake}, $\gls{nReducedFeatures}<\gls{nClasses}$, and it enables sufficiently compact explanations~\citep{miller1956magical}.
The shown results, \eg{} \cref{tab:aCCproto-table,tab:Interpproto-table}, are the mean across $5$ seeds, with the exception of $3$ for \imgnetheader{}, \gls{PIP-Net} and \gls{ProtoPool}.
For comparison, 
all models are exclusively pretrained on \imgnetheader{}.
This change did affect \gls{ProtoPool}, but even with iNaturalist~\citep{van2018inaturalist} pretraining, we could not reproduce the reported results by~\citet{rymarczyk2022interpretable}. 

\subsubsection{Quadratic Problem}
\label{sec:gurobitricks}
This section presents details on how the described quadratic problem \checktext{with \cref{eq:fullObj} as objective} is solved using \gurobi.
We incorporated deduplication 
and the assignment of an equal number of features to all classes of \cref{eq:5perInit,eq:Unique} using
an iterative approach with relaxed
constraints.
Specifically, 
the model 
is optimized
without these constraints, but instead 
$\boldsymbol{1}^T\wgurobi\fvecgurobi=\gls{nperClass}\gls{nClasses}$.
Then, after each iteration,
all violated constraints 
are added
to the model, but only limited to a running set of 
features $\Gamma\in\{0,1\}^{\gls{nFeatures}}$, which gets extended during the iteration.
Next to the features, we also maintain a set of classes $C_\mathrm{duplicates}$ that were equal at one iteration and classes $C_\mathrm{sparse}$ that ever had too few features assigned.
Instead of \cref{eq:5perInit,eq:Unique} the relaxed constraints
\begin{align}
  \boldsymbol{w}_{\cindex,\Gamma}^T
  \fvecgurobi_{\Gamma} & \geq \gls{nperClass} \quad \forall \, \cindex \in C_\mathrm{sparse} \label{eq:NotGLob}\\
     (\boldsymbol{w}_\cindex \circ \boldsymbol{w}_{\cindex'})_{\Gamma}^T\fvecgurobi_{\Gamma} &< \gls{nperClass} \quad \forall \, \cindex,\cindex' \in C_\mathrm{duplicates} \label{eq:4DedupQP}
\end{align}
are added, where $\wgurobi_{\cindex,\Gamma}$ describes indexing $\wgurobi_\cindex$ where $\Gamma=1$. 
Additionally, we set the start solution for the next optimization to a good, usually optimal, feasible solution for the currently selected set of features. 
As we need multiple iterations to enforce all constraints, we limit the time spent on one iteration to $3$ hours and set the gap to optimality to $10^{-4}$. 
In our experiments, the global optimum for the relaxed problem is usually found in less than $4$ hours 
for fine-grained datasets, and roughly $11$ hours for \imgnetheader{}
using a CPU like \textit{EPYC 72F3}.
While \cref{eq:NotGLob} changes the desired optimization problem, the resulting objective is very close (achievable gap of less than $1\%$) to the global optimum, which is infeasible to compute and does not lead to an improved model. 
The experiments to verify this claim are included in \suppl{} \cref{suppsec:optimal}.
Finally, alongside our experiments, previous work~\citep{hornakova2021making} shows that the exact global optimum is not always preferred for relevant metrics.
To make the relative weighting of the multiple objectives $\objective_A$, $\objective_R$ and $\objective_B$ easier, %
\simmat{} 
is scaled with \gls{nClasses} and \gls{nperClass}
to have a maximum of $1$ for $\gls{nClasses}=200$ and $\gls{nperClass}=5$. %
Since \gls{nReducedFeatures} features need to be chosen, 
all entries below $\epsilon$ in \FSimeaturemat{} are set to $0$, where $\epsilon$ is the highest value,
for which there still exists a selection with $\objective{}_R=0$. 
This is equivalent to finding the maximal $\epsilon$ for which the graph described by $\boldsymbol{G}$ with 
\begin{equation}
  g_{\findex,\findex'} =\begin{cases}
0 \quad\mathrm{if} \quad r_{\findex,\findex'} \geq \epsilon \\
1 \quad\mathrm{else},
\end{cases}
\end{equation}
has a maximum clique of size \gls{nReducedFeatures}.
We used approximations~\citep{pattabiraman2015fast,boppana1992approximating} and a sufficiently sized approximated maximum clique as the start value for \fvecgurobi.
Additionally, the remaining nonzero values in \FSimeaturemat{} are scaled to have a maximum of $1$. 
For scaling the bias \BSimeaturemat{}, we clipped outliers, centered the remaining values around $0$ and scaled the maximum absolute value to be $\lambda = \frac{1}{\sqrt{10}}$, which is empirically found. 

\subsection{Metrics}
\begin{figure*}[thp]
  \centering
\includegraphics[width=\linewidth]{plots/Bronzed_CowbirdClassSepShiny_Cowbird_Bronzed_CowbirdClassSepShiny_Cowbird_6FeaturesTransTextNoP.pdf}
   \vspace{-.5cm}\caption{Contrastive faithful class explanations for \gls{NewlayerName} trained on \cubheader{}: 
   Without any additional supervision, 
   \gls{NewlayerName} learns to differentiate 
   Shiny and Bronzed Cowbird ($\mathrm{\ClassSim}^{gt}=0.97$) using the red eye just like humans do, as the annotations in \cubheader{} or the screenshot in \cref{fig:ScreenshotDiffShiny} show.}
\label{fig:CubSim}
\vspace{-.5cm}
\end{figure*}
\newlength{\globaltextwidth}
\setlength{\globaltextwidth}{\textwidth}%
\begin{figure}
\centering
   \begin{subfigure}[t]{.495\linewidth}
     \centering
\includegraphics[width=.3\globaltextwidth]{plots/Overlap_0.5821352385020999_feature_Densemax_lw_5}
         \captionsetup{justification=centering}
     \caption{
     Baseline 
     \resnet{} 
     with $\mathrm{\contrastiveness{}} = 41.8\%$}
   \end{subfigure}
   \hspace{5mm}
   \begin{subfigure}[t]{.45\linewidth}
     \centering
   \includegraphics[width=.3\globaltextwidth]{plots/Overlap_0.0009853010960311215_feature_Finetunemin_lw_5}%
    \captionsetup{justification=centering}
     \caption{\gls{NewlayerName} with $\mathrm{\contrastiveness{}}= 99.9\%$}
   \end{subfigure}
   \hfill
    \caption{Extreme Examples for feature distributions and their \contrastiveness{} on \cubheader{}. }
    \label{fig:Contr}
    \vspace{-0.35cm}
\end{figure}



\input{sec/reorganizedMetrics}


\subsection{Results}
\begin{table*}[t!]

  \caption{Comparison on compactness and accuracy with \resnet{}: \gls{NewlayerName} shows increased accuracy and compactness.
 The compactness-accuracy trade-off is 
 shown 
 in \cref{fig:interptradeoff}.
 \boldnessstatement
 }
 \label{tab:aCCproto-table}
 \vspace{-.3cm}
 \resizebox{\linewidth}{!}{
 \centering
 \begin{tabular}{l|ccc|ccc|ccc}
  \hline
  Method & \multicolumn{3}{c|}{Accuracy \arrowUp} &
  \multicolumn{3}{c|}{Total Features \arrowDown} & \multicolumn{3}{c}{Features / Class \arrowDown} \\
  & CUB& CARS & INET & CUB& CARS & INET & CUB& CARS & INET 
  \\
  \hline
  Baseline \resnet{}& 86.6 & 92.1 &76.1 & 2048 & 2048 &2048& 2048& 2048& 2048\\
  \hline
  \glmtable{}& 78.0 & 86.8  & 58.0 & 809 & 807& 1627 & \textbf{5} &\textbf{5}&\textbf{5} \\
  \pipnettable{} & 82.0 & 86.5  &-& 731 &669&-& 12 &11 &  - \\ %
  \protopooltable{} & 79.4 &87.5& -& 202 & 195 & - &202 & 195 &  - \\ %
  \slddtable{} & 84.5 & 91.1  & 72.7 & \textbf{50} & \textbf{50} & \textbf{50}& \textbf{5}& \textbf{5} &\textbf{5} \\
  \qsenntable{} & 84.7 & 91.5  & \textbf{74.3} & \textbf{50} & \textbf{50} & \textbf{50}& \textbf{5}& \textbf{5} &\textbf{5} \\
  \hline
  \gls{NewlayerName} (Ours) & \textbf{85.1} & \textbf{91.8}& \underline{74.2} & \textbf{50}  &\textbf{50}& \textbf{50}& \textbf{5}&\textbf{5}&\textbf{5} \\ %
  \hline
 \end{tabular}
}
 \vspace{-.3cm}
\end{table*}
\begin{table*}[t!]
 
 \caption{Comparison on Interpretability metrics with \resnet{}. Due to required annotations, \cubsim{} (abbreviated SG) can only be computed for \cubheader{}. }
\label{tab:Interpproto-table}
\vspace{-.3cm}
\resizebox{\linewidth}{!}{
 \centering 
 \begin{tabular}{l|ccc|ccc|ccc|c}
  \hline
  Method & \multicolumn{3}{c|}{\loc{5} \arrowUp} &
  \multicolumn{3}{c|}{\generality{}\arrowUp} & \multicolumn{3}{c|}{\contrastiveness{} \arrowUp} & SG \arrowUp\\
  & CUB& CARS  & INET & CUB& CARS  & INET & CUB& CARS & INET & CUB
  \\
  \hline
  Baseline \resnet{}& 57.7 & 54.4 &37.1 & 98.0 & 97.8& 99.4 & 74.4 & 75.1&71.6  & 34.0\\
  \hline
  \glmtable{} & 55.4 &51.8 &35.8 &\textbf{97.8} &\textbf{97.6}  &\textbf{99.4} & 74.0 & 74.5 &71.7 & 2.5  \\
  \pipnettable{} & \textbf{99.1} & \textbf{99.0} &- & 75.6 &62.9&- & \textbf{99.5} &\textbf{99.5} &- & 6.7  \\ %
  \protopooltable{} & 24.5 &30.7& -& {96.9}& {96.0} &- &76.7& 78.9 & -  & 13.9 \\
  \slddtable{}
   &88.2 & 88.6 & \underline{64.7} & {96.2} &{95.6}&{98.6} & 87.2 &89.7 &\textbf{93.4} & 29.2 \\
    \qsenntable{}
   &\underline{93.3} &\underline{94.4} &\textbf{82.0} & {95.5} &{94.8}&{98.7 } &  93.0 &94.2 &\underline{92.6} & 23.4 \\
   \hline
   \gls{NewlayerName} (Ours) & {90.1} & {89.6} &{64.1} & \underline{97.0}& \underline{96.5}&\underline{99.1}
   & \underline{96.0} & \underline{97.7} & {89.3} & \textbf{47.9}\\ 
  \hline
 \end{tabular}
 }
\vspace{-.4cm}
\end{table*}





  
 
 

  
This section discusses the experimental results. 
The usual metrics for compactness-based globally interpretable models are shown in \cref{tab:aCCproto-table}. 
For the fine-grained datasets, \gls{NewlayerName} is among the most compact models while showing the highest accuracy, thus setting the state of the art for interpretable models.
On \imgnetheader{}, where prototype-based methods are not even applicable, \gls{NewlayerName} is only marginally beaten by
\gls{qsenn}, which uses compute-intensive iterations and negative reasoning for some classes, which significantly hinders interpretability. 
A runtime analysis is shown in \cref{ssec:Runtime}.\\
The results for the interpretability metrics are shown in \cref{tab:Interpproto-table}.
Note that \glmtable{} and PIP-Net are hardly comparable, as \glmtable{} uses the uninterpretable features of a black-box model and PIP-Net learns very localized class-detectors, with some features activating to 99\% on just a single class.
In contrast, \gls{NewlayerName} achieves excellent values across all metrics and datasets in this multicriterial  task of self-explaining neural networks, summarized in \cref{fig:RadarMax}.
Its
interpretable class representations, composed of diverse, general and contrastive features, mirror reality, as measured by \cubsim{}. 
Note that \gls{NewlayerName} learns grounded representations as shown in \cref{fig:metrics_full,fig:CubSim} without any additional supervision and is able to communicate the only differentiating factor it uses. 
\gls{NewlayerName}'s local behavior then follows its faithful global explanations, which leads to trustworthy classifications and predictable errors when the differentiating factor is not present, as in \cref{fig:LocalFailureP}.
The appendix contains 
more visualizations, including a discussion of failure cases in \cref{ssec:Failure}, 
a discussion on polysemantic features (\cref{ssec:poly}), an extension of \cubsim{} to \imgnetheader{} (\cref{ssec:imgsim}) and a discussion of limitations and future work (\cref{ssec:limits}).\\

     
   
\subsection{Ablation Studies}
\input{sec/MassiveQuadrupleFigure}

This section validates the impact of the individual objectives in the quadratic problem in 
\cref{tab:ablations} and presents the
 compactness trade-off in \cref{fig:interptradeoff}.
 We focus on \cubheader{} but observed similar results for other datasets.
  The compactness-accuracy tradeoff for \gls{NewlayerName} compared with \gls{qsenn} and the \gls{layerName} is visualized in \cref{fig:interptradeoff}. 
 The global optimization clearly leads to a more effective use of the defined capacity, with the highest uplift in the very high compactness regime, \eg{} 1.5 percent points at $\gls{nReducedFeatures}=20$, where a good selection and assignment naturally has more impact.\\
 \checktext{The impact of the feature-feature similarity matrix \FSimeaturemat{} and feature selection bias \BSimeaturemat{} is shown in \cref{tab:ablations}.} 
 Incorporating a bias \BSimeaturemat{} for local feature maps further increases the \loc{5}. 
 On the other hand,
 reducing feature similarity through \FSimeaturemat{} effectively reduces the %
 correlation
 between the resulting features, which improves accuracy, as the model uses its capacity more effectively. 
  \checktext{In summary, the inclusion of the secondary objectives $\objective_R$ and $\objective_B$ is beneficial for the resulting model, improving the desired aspects not just after solving the QP but also in the resulting model after fine-tuning.}\\
The appendix contains further ablation studies to support our claims, demonstrating the ability to steer (\cref{ssec:steer}), validating the choice of correlation as metric for \simmat{} (\cref{ssec:auroc}) and showing the benefits of enforcing exactly \gls{nperClass} features per class (\cref{ssec:balancedAbl}).
  

 


