
\section{Method}
\vspacehack{}
\input{sec/methodoverview}
Our proposed \gls{NewlayerName} is designed for the interpretable classification of an image %
as a class $c \in \{c_1,c_2, \dots, c_{\gls{nClasses}}\}$.
The \gls{NewlayerName} uses a deep feature extractor $\Phi$ to compute feature maps \Gls{featureMapsSmall} of width $\gls{featuresMapwidth}$ and height $\gls{featuresMapheigth}$ and averages them into a feature vector \gls{RedfeatureVector}.
The classification result \glsentrylong{outputVector} of the \gls{NewlayerName} is the matrix multiplication between the sparse binary matrix $\wgurobi^*~\in\{0,1\}^{\gls{nClasses}\times \gls{nReducedFeatures} }$ %
and the features
\gls{RedfeatureVector} formalized as $ \gls{outputVector} = \wgurobi^*\gls{RedfeatureVector}$. \\
The pipeline of our proposed method is shown in \cref{fig:OverviewAppraoch} and is motivated by \citep{norrenbrocktake, norrenbrock2024q}, following their presentation and notation.
It starts with training a conventional black-box model with initially \gls{nFeatures} features using the feature diversity loss~\gls{customLoss}~\citep{norrenbrocktake}, as a high diversity of features is desired for interpretable models.
A detailed explanation of~\gls{customLoss} is included in \cref{suppsec:ldiv}.
Using the black-box model as starting point, we aim to find a selection of \gls{nReducedFeatures} out of the initial \gls{nFeatures} features and their sparse binary assignment $\boldsymbol{W}^*$ to the classes to enable downstream interpretability.
The feature extractor $\Phi$ is then fine-tuned with this solution fixed, so that the features adapt to the sparse solution and become a shared concept of the assigned classes.
This is encouraged through selecting fewer features than there are classes, $\gls{nReducedFeatures} < \gls{nClasses}$, and representing every class with the same number \gls{nperClass}, typically $5$, of features. 
Using the same number of features for every class is beneficial for the interpretability in multiple ways. 
The class representations do not need a bias and can be contrasted as $S_{i}\in\{1, \dots, \gls{nReducedFeatures}\}^{\gls{nperClass}}$, while the composing features can focus on detecting general concepts.
Since we aim to optimize binary variables under constraints with a clear objective, we can formulate it as a discrete optimization problem to get the optimal solution. 
As indicated in~\cref{fig:QP}, we define the constants \simmat{}, \FSimeaturemat{} and \BSimeaturemat{} of the resulting QP so that
in the global optimum different (\FSimeaturemat{}) , localized (\BSimeaturemat{}) features are selected and assigned to classes for which they have high predictive power (\simmat{}).
These fixed simple binary class representations then lead to the emergence of interpretable features during fine-tuning.
How the 
quadratic problem with \simmat{}, \FSimeaturemat{} and \BSimeaturemat{} is formulated to ensure this goal is discussed in the following sections.
\subsection{Quadratic Problem}
\label{sec:QPMethod}
We consider the problem of selecting the~\gls{nReducedFeatures} out of~\gls{nFeatures} features and assigning them to the classes as a binary quadratic problem, that can be solved globally optimal.
Specifically, the feature selection $\fvecgurobi~\in\{0,1\}^{\gls{nFeatures}}$ and 
assignment between features and classes $\wgurobi~\in\{0,1\}^{\gls{nClasses}\times \gls{nFeatures} }$ are jointly
optimized, with $\wgurobi^*$ being \wgurobi{} for the selected features. 
Given a similarity matrix~$\simmat{}~\in \mathbb{R}^{\gls{nClasses}\times \gls{nFeatures} }$ the main objective is to maximize the similarity $\objective{}_A$ between the selected features and their assigned classes 
\begin{equation}
  \objective_A = \simieq 
  \label{eq:mainObj}
\end{equation}
with $\circ$ indicating the Hadamard product. 
Here,~\fvecgurobi{} indicates whether a feature is selected and~\wgurobi{} describes if a feature is assigned to the class.
\checktext{Note that we use \cindex{} to index classes and \findex{} for features.}
The sparsity and low-dimensionality are formulated as constraints for the optimization:
\begin{align}
  \sum_{\findex=1}^{\gls{nFeatures}}\infvec{\findex} &= \gls{nReducedFeatures}\label{eq:FeatureSel}\\
   \sum_{\findex=1}^{\gls{nFeatures}} w_{\cindex,\findex} \infvec{\findex} &= \gls{nperClass} \quad \forall \cindex \in \{1, \dots,\gls{nClasses}\}
   \label{eq:5perInit}
\end{align}
To allow the \gls{NewlayerName} the differentiation between all classes and enable effective fine-tuning, 
we
additionally add constraints that no two classes are assigned to the same set of features:
\begin{equation}
   (\boldsymbol{w}_\cindex \circ \boldsymbol{w}_{\cindex'})^T \fvecgurobi{} < \gls{nperClass} \quad \forall \cindex,\cindex' \in \{1, \dots,\gls{nClasses}\}
   \label{eq:Unique}
\end{equation}
Note that the constraints in \cref{eq:5perInit,eq:Unique} technically define a quadratically constrained quadratic program (QCQP).
To make the QCQP computationally tractable, the constraints are relaxed and added iteratively for classes that violate the constraints.
The 
efficient 
implementation is discussed in detail in \cref{sec:gurobitricks}. %
The general formulation of the problem allows us to add further nuance to the optimization and include more desiderata. Since a high representational capacity is desired for the selected features, the cross-feature similarity matrix$~\FSimeaturemat~\in\mathbb{R}^{\gls{nFeatures}\times \gls{nFeatures} }$ is incorporated to reduce the similarity between the selected features:
\begin{equation}
    \objective_R = -\correq\label{eq:corrObj}
\end{equation}
Additionally, the selection of specific features can be guided via a selection bias$~\BSimeaturemat~\in\mathbb{R}^{\gls{nFeatures} }$ 
\begin{equation}
    \objective_B = \biaseq\label{eq:locObj},
\end{equation}
where a higher value $\BSimeaturemat_i$ leads to a preferred selection of the feature $i$.
The combination of all these objectives leads to:
\begin{equation}
  \max_{\wgurobi,\fvecgurobi{}} \, \objective = \max_{\wgurobi{},\fvecgurobi{}} \,\objective_A + \objective_R + \objective_B\label{eq:fullObj}
\end{equation}




The formulation in standard form for quadratic problems $\frac{1}{2} \mathbf{x}^T \mathbf{Q} \mathbf{x} + \mathbf{c}^T \mathbf{x}$
with $\mathbf{Q}$ capturing the quadratic terms $\objective{}_A$ and $\objective{}_R$, and $\mathbf{c}$ incorporating the linear term $\objective{}_B$ is
included in ~\cref{suppsec:QP}.\\
\subsection{Class-Feature Similarity} %
The class-feature similarity matrix \simmat{} with entries
\insimmat{\cindex}{\findex} should reflect how beneficial the assignment of feature $\findex$ to class $\cindex$ \checktext{is}
for the classifier. 
As 
every feature gets assigned to multiple classes, which themselves become assigned to multiple features, 
the metric should
focus on a robust positive relation between the activation and likelihood of a sample being of the respective class.
This is captured by the Pearson correlation coefficient 
$\insimmat{\cindex}{\findex}$ 
between the feature distribution $\boldsymbol{f}_{:,\findex}$ and the label 
vector $\boldsymbol{l}^{\cindex}\in\{0,1\}^{\gls{nTrainImages}}$, 
in which for all \gls{nTrainImages} training images a $1$ indicates the label being $\cindex$. %

\subsection{Feature-Feature Similarity} %
Just maximizing \cref{eq:mainObj} can lead to very similar features being selected
which is neither beneficial for interpretability nor for accuracy as representational capacity is lost and multiple features develop towards the same concept during fine-tuning.
To prevent this, selecting similar features in \simmat{} should be penalized in the objective.
We choose
the cosine similarity between the class similarities of two features $\findex\neq \findex'$ in \simmat{}
for \FSimeaturemat{} with $ \symbolFeaFea_{\findex,\findex'} = \mathrm{ReLU} \left( \frac{\boldsymbol{a}_{:,\findex}^T\boldsymbol{a}_{:,\findex'}}{|\boldsymbol{a}_{:,\findex}||\boldsymbol{a}_{:,\findex'}|}\right)$,
using ReLU to focus on preventing redundant features and $\symbolFeaFea_{\findex,\findex'}=0$ for $\findex=\findex'$.
As we are only interested in preventing the selection of highly similar features, we can clip all entries in \FSimeaturemat{} below an $\epsilon$ to $0$ to enable a fast solving of the QP.
The details are discussed in \cref{sec:gurobitricks}. 

\subsection{Feature-Bias} %
\label{sec:fbias}
The Feature-Bias \BSimeaturemat{} describes the benefit of selecting each feature.
This can be used to steer the model towards specific desiderata. 
As diversity is generally preferred~\citep{norrenbrocktake, alvarez2018towards} for interpretable models, 
a bias towards more local features is used,
\begin{equation}
  b_\findex = \frac{1}{\gls{nTrainImages} \sum_j f_{j, \findex} } \sum_{j=1}^{\gls{nTrainImages}} max(\boldsymbol{S}^\findex_j) 
  f_{j,\findex}\quad.
\end{equation}
Here $\boldsymbol{S}^\findex_j$ is the softmax over the spatial dimensions of the $\findex$-th feature map for the image $j$.
Scaling the feature bias by their activation leads to the selection of features that are more localized when their activation is high. 
Alternatively, the bias can be used to steer the selection towards other criteria the practitioner might identify as relevant, which we demonstrate in the appendix.
We center \BSimeaturemat{} and scale the maximum absolute value to be $\lambda$, whose strength defines the priority put on the bias.





