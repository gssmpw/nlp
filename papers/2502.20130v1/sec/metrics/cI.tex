Additionally, the features should capture a general concept, instead of a class-specific one. This can be measured via the \textit{\generality} $\tau$:
\begin{align}
\tau = 1-\frac{1}{\gls{nReducedFeatures}} \sum_{\findex=1}^{\gls{nReducedFeatures}} \max_\cindex
&\frac{\sum_{j=1}^{\gls{nTrainImages}} l^\cindex_j (f_{j,\findex} - \min \boldsymbol{f}_{:,\findex})}{\sum_{j=1}^{\gls{nTrainImages}} (f_{j,\findex}- \min \boldsymbol{f}_{:,\findex})}
\end{align}
It
measures which fraction of the zero-based feature activation across the entire dataset is
not
focussed on the most related class.
A model with high \generality{} has features that recognize a shared concept for multiple classes, like the $4$ central features in \cref{fig:metrics_full,fig:CubSim}. 
Notably, as opposed to Dependence~\citep{norrenbrock2024q}, \generality{} can capture the assignment of multiple class detectors to the same class.
