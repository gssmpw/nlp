\section{Related Work}
\label{sec:RelatedWork}




Research towards Interpretable machine learning includes the direct design of models providing interpretability by themselves \citep{alvarez2018towards, sawada2022concept, norrenbrocktake, nauta2023pipnet, nauta2021neural, rymarczyk2022interpretable,zarlenga2022concept, marconato2022glancenets, koh2020concept, rymarczyk2021protopshare,chen2019looks}  or to find post-hoc methods which aim to explain the decision process or single features of the model ~\citep{kim2018interpretability,bau2017network,AlphaZero,Fel_2023_CVPR,yuksekgonul2022posthoc,pmlr-v202-kalibhat23a,oikarinen2023clipdissect}.
As our method is designed to find a compact set of human-understandable features, our work can be assigned to the former type, which we focus on within this section.
However, the alignment of the learned features of our proposed \gls{NewlayerName} with human attributes can be guided by the post-hoc methods. 
When considering the interpretability of a model, a distinction is made between local interpretability, which refers to the explanation of a single decision, and global interpretability, which describes the holistic behavior of the model over the entirety of a dataset~\citep{molnar2020interpretable}.
For local interpretability, \textit{B-Cos Networks}~\citep{bohle2023holistically} already offer faithful explanations in the form of saliency maps.
Therefore, this work focuses on the more challenging global interpretability, which also improves local interpretability.
In the social sciences~\citep{miller2019explanation}, human-friendly explanations are contrastive~\citep{lipton1990contrastive}, concise and general~\citep{read1993explanatory}. 
Further, SENN~\citep{alvarez2018towards} describes diversity and grounding as desirable attributes for features of an interpretable model.
Grounding refers to the alignability with any human concept and is very difficult to quantify, as one would need a full dataset of potentially learned concepts. 
Problematically, deep neural networks typically exhibit superposition and polysemantic neurons~\citep{scherlis2022polysemanticity, elhage2022toy, templeton2024scaling}, which is why we focus on more clearly quantifiable aspects in this work.\\
Models such as \gls{ProtoTree}~\citep{nauta2021neural}, \gls{ProtoPNet}, \gls{ProtoPShare}, \gls{ProtoPool}, and \gls{PIP-Net} aim to learn prototypes from data by employing deep feature extractors.
These prototypes' similarities are subsequently integrated into interpretable models.
However, the extent of their interpretability remains debatable, as \citet{kim2021hive} and \citet{hoffmann2021looks} reveal a gap between human and computed similarities.
Similar to this work, \gls{PIP-Net} also aims for compactness via sparse weights in the final decision layer.
However, they apply a local optimization that aims for sparsity solely, resulting in a big set of used features with many of them being class-specific.
\citet{norrenbrocktake, norrenbrock2024q} additionally select a compact feature set for their \textit{SLDD-Model} and \gls{qsenn}, where a class is to be related to only a few features.
Their diversity is ensured through
the Feature Diversity Loss~\gls{customLoss}, which incurs a higher cost when highly activated and weighted features localize on the same region. 
For both feature selection and the computation of the sparse layer, glm-saga~\citep{wong2021leveraging} is used. 
It locally and iteratively optimizes the problem, leading to a suboptimal feature selection and continuous weights.
In contrast, our global optimization with user-defined steerable criteria jointly finds an optimal selection
of the required number of features and computes their binary assignments.
This leads to a more effective use of the allocated capacity and built-in
easily interpretable class representations for global interpretability.
Another line of research is based on the \textit{Concept Bottleneck Model} (\gls{cbm}) which initially predicts the labeled concepts within a given dataset and subsequently leverages a basic model to predict the target category based on these identified concepts.
This approach remains an area of active exploration and development~\citep{sawada2022concept,zarlenga2022concept, marconato2022glancenets, oikarinen2023label}, but is limited by the annotations, or in case of the \gls{labelfreecbm} by the vision-language model, resulting in subpar accuracy and compactness.
Finally, 
\citet{rosenhahn2023optimization} applies discrete optimization to obtain sparse neural networks~\citep{GlaKai2023a}.



