

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{plots/rottweiler_vs_doberman_orangeNoNumbers.pdf}
   \vspace{-.5cm}\caption{Faithful global interpretability of our \gls{NewlayerName}: %
   Without any additional supervision, 
   \gls{NewlayerName} learns to represent 
   Rottweiler and Doberman using  $5$ diverse and general features.
   \gls{NewlayerName} faithfully explains that it differentiates them 
   exclusively 
   via their visibly distinct
   head.}
   \vspace{-.1cm}
  \label{fig:metrics_full}
\end{figure*}
\input{sec/qpFigurePngs}
\section{Introduction}
\label{sec:intro}
Deep Learning has made remarkable advances in various fields, such as image classification, segmentation or generation~\citep{krizhevsky2012imagenet,kirillov2023segment,rombach2021highresolution,ramesh2022hierarchical}.
For high-stakes decisions, \eg{}applying image classification in the medical domain, legislation moves towards requiring a certain level of interpretability~\citep{veale2021demystifying}, whose measurement is a fairly open task on its own.
However, some desirable and measurable qualities of explanations have been identified~\citep{miller2019explanation}. 
Human-friendly explanations  should be contrastive~\citep{lipton1990contrastive}, diverse~\citep{alvarez2018towards}, general and compact~\citep{read1993explanatory}.
As humans can consider $7\pm2$ cognitive aspects at once~\citep{miller1956magical}, an explanation size of up to $5$ is desirable.
Additionally, an explanation should faithfully explain the model, which is where many post-hoc methods fail~\citep{kindermans2019reliability, adebayo2018sanity,daras2022discovering}.
Therefore, we focus on models that are interpretable by design with built-in faithful explanations.\\
Previous works, such as SENN~\citep{alvarez2018towards}, \gls{qsenn}, \gls{cbm}, \gls{labelfreecbm}, \gls{PIP-Net}, \gls{ProtoPool}, \gls{ProtoTree},  \gls{protopnet} or the \gls{layerName} rely on combining understandable features in an interpretable manner.
However, while most models can offer convincing \textit{local} explanations for a single decision, they struggle with the \textit{global} explanation of their behavior in general.
Some models with global interpretability do not show competitive accuracy~\citep{oikarinen2023label, koh2020concept} and it is debated~\citep{molnar2020interpretable}, if ensembles of very deep decision tress~\citep{nauta2021neural} or dense high-dimensional linear layers~\citep{koh2020concept, rymarczyk2022interpretable, alvarez2018towards} are truly intrinsically interpretable as they lack desired qualities like compactness.
For that reason \gls{PIP-Net} focuses on learning sparse class representations.
These representations lie in a high dimensional feature space, which causes \gls{PIP-Net}'s features to be connected to very few or only one class each.
This leads to the emergence of features that are already detecting the class and no general 
concept.
The sparse representations of \gls{PIP-Net} thus have no interpretable meaning, as classes are represented with themselves.
To alleviate that issue, the \gls{layerName} and \gls{qsenn} reduce both dimensions of compactness: 
They not only reduce the number of features per class \gls{nperClass}, which in isolation leads to class-specific features  but also the number of features in total \gls{nReducedFeatures} to be significantly below the number of classes \gls{nClasses}.
That causes each of the fewer features to be assigned to multiple classes, which prevents the emergence of class detectors.
However, these models still have shortcomings when it comes to global interpretability. 
Their class representations are real-valued, or ternary for \gls{qsenn}, include a bias, and are composed of a varying number of features.
Therefore, the global class explanations are hardly comparable or contrastive.\\
In this work, we introduce the Quadratic Programming Enhanced Model (\gls{NewlayerName}) that offers interpretable class representations and sets a new state of the art for the accuracy of
compactness-based 
interpretable models.
It represents every class with the binary assignment of a low user defined number of features \gls{nperClass}, which themselves are contrastive, general and diverse.
We typically choose $5$, in line with previous work~\citep{norrenbrock2024q, norrenbrocktake}, to accommodate for human limitations~\citep{miller1956magical}.
As shown in~\cref{fig:metrics_full}, \gls{NewlayerName} offers built-in faithful global explanations for classes and enables the intuitive comparison of different learned class representations.
These easy comparisons between compact binary class representations even enable reasoning about the differentiating feature between the classes, like the head in~\cref{fig:metrics_full}.
The improvements in faithful global interpretability of class representations are summarized in \cref{tab:ClassRepresent}.\\
The crucial step in training a \gls{NewlayerName} is
solving a binary QP, applied to a dense black-box model, which jointly finds an optimal solution to both the selection of a reduced subset of the model's features and the sparse assignment between the features and classes, as shown in \cref{fig:QP}.
It maximizes the similarity between features and their assigned classes, while minimizing the similarity of jointly selected features.
Further, the linear term can steer the selection towards desired biases, while the desired interpretability is incorporated via constraints.
This optimal solution is then fixed for the following fine-tuning during which the features adapt to their assigned classes.
As every class is assigned to the same number of features, each of the features detects shared general concepts between its assigned classes instead of also detecting the entire class. This leads to state-of-the-art accuracy.
Finally, the assignments are not maximizing inter-class distance, resulting in more similar representations for similar classes and a form of structural grounding.
Code: \url{https://github.com/ThomasNorr/QPM}


\input{sec/classRepv}
Our main \textbf{contributions} are as follows:
\begin{itemize}
\item We propose the Quadratic Programming Enhanced Model (\gls{NewlayerName}), which incorporates an optimal feature selection and their binary assignment of a few, \eg{} 5 features per class.
It is found by formulating the quadratic problem and solving it optimally.

\item We demonstrate improvements in accuracy, compactness and structural grounding of \gls{NewlayerName} on multiple benchmark datasets and architectures for image classification, including \gls{imgnetheader}. 
Due to optimally using the given capacity, \gls{NewlayerName} sets the new state of the art for 
compactness-based globally 
interpretable models.

\item We show that the learned features exhibit several desired quantifiable properties, such as contrastiveness, generality and diversity, and can be steered towards user-defined criteria.
\item Representing classes as a contrastable compact set of these general features makes \gls{NewlayerName} faithfully globally interpretable, while further closing the accuracy gap to black-box models.

\end{itemize}


