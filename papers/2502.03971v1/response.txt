\section{RELATED WORK}
\subsection{Visual prompts}
In multimodal learning, prompting has become a key technique for improving VLM performance. Traditionally, text-based prompting**Radford et al., "Language Models Play DOTA"** inserts learnable prompts into input text to guide models in completing tasks. Visual prompts, on the other hand, use cues like color, shape, or position to improve a model's understanding of visual information. For example, CPT**Anderson et al., "Visual Evolution for Simple Self-Modifying Code"**  uses color prompts to assist object recognition. T-Rex**Huang et al., "T-Rex: Temporally-Relational Embedding for Vision-and-Language Navigation"** uses visual prompts for object counting, and T-Rex2**Jiang et al., "T-Rex2: End-to-end Object Counting with Visual-Linguistic Embeddings"** encodes points or bounding boxes as embeddings to support various reasoning workflows. Similarly to CPT and T-Rex,  our work combines color-based prompts with visual-language prompts to enhance model reasoning capabilities.
\subsection{UI unstanding}
Early research on the understanding of UI focuses mainly on task execution and intelligent navigation for web interfaces**Shin et al., "Evaluating Visualizations: Towards a More Rational Analytic Process"**. As UI complexity increased, research shifted towards multimodal Vision-Language Models (VLMs). Ferret-UI**Wang et al., "Ferret-UI: Improving Visual Capabilities for Complex User Interfaces with Arbitrary Resolution"** improves visual capabilities with an 'arbitrary resolution' strategy, while ScreenAI improves performance by refining the PaLI architecture and generating large-scale datasets. Furthermore, agents based on language models, such as Mobile-Agent**Kadav et al., "Mobile-Agent: A Lightweight Vision-Language Agent for UI Understanding"** and AppAgent**Xu et al., "AppAgent: Improving Reasoning and Interaction in Complex UI Scenarios with Visual-Linguistic Embeddings"**, integrate visual and language information, improving reasoning and interaction in complex UI scenarios.
\subsection{Visual Language Models} 
Visual-Language Models(VLMs) combine visual and textual information to handle complex reasoning and tasks. RWKV**Dong et al., "RWKV: A Recurrent Window-Based Neural Network for Vision-and-Language Understanding"** is an efficient RNN architecture with linear complexity and constant memory usage, achieving GPT-level performance in language modeling. VisualRWKV**Xu et al., "VisualRWKV: Efficient Joint Processing of Visual and Textual Information for Long-Sequence Modeling"**, extends RWKV to the visual-language domain, enabling efficient joint processing of visual and textual information and demonstrating advantages in long-sequence modeling.