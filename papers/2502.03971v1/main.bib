@inproceedings{visualinstructiontuning,
author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
title = {Visual instruction tuning},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1516},
numpages = {25},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@article{deepseekvl,
  publtype={informal},
  author={Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu},
  title={DeepSeek-VL: Towards Real-World Vision-Language Understanding},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2403.05525},
  url={https://doi.org/10.48550/arXiv.2403.05525}
}

@INPROCEEDINGS{mPLUG-Owl2,
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Hu, Anwen and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={mPLUG-OwI2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration}, 
  year={2024},
  volume={},
  number={},
  pages={13040-13051},
  keywords={Computer vision;Large language models;Computational modeling;Collaboration;Cognition;Pattern recognition;Decoding;Multimodal Large Language Model;Modality Collaboration;Vision Language},
  doi={10.1109/CVPR52733.2024.01239}}

@article{Data-centric,
  publtype={informal},
  author={Muyang He and Yexin Liu and Boya Wu and Jianhao Yuan and Yueze Wang and Tiejun Huang and Bo Zhao},
  title={Efficient Multimodal Learning from Data-centric Perspective},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2402.11530},
  url={https://doi.org/10.48550/arXiv.2402.11530}
}

@inproceedings{Kosmos-2,
  title={Grounding multimodal large language models to the world},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Ye, Qixiang and Wei, Furu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@InProceedings{Ferret-UI,
author="You, Keen
and Zhang, Haotian
and Schoop, Eldon
",
title="Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="240--255",
isbn="978-3-031-73039-9"
}

@inproceedings{
Mobile-Agent,
title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},
author={Junyang Wang and Haiyang Xu and Jiabo Ye },
booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
year={2024},
url={https://openreview.net/forum?id=jE6pDYCnVF}
}

@INPROCEEDINGS{appTesting,
  author={Linares-Vásquez, Mario and Moran, Kevin and Poshyvanyk, Denys},
  booktitle={2017 IEEE International Conference on Software Maintenance and Evolution (ICSME)}, 
  title={Continuous, Evolutionary and Large-Scale: A New Perspective for Automated Mobile App Testing}, 
  year={2017},
  volume={},
  number={},
  pages={399-410},
  keywords={Testing;Mobile communication;Graphical user interfaces;Tools;Systematics;Automation;Manuals},
  doi={10.1109/ICSME.2017.27}}


@misc{ScreenAI,
      title={ScreenAI: A Vision-Language Model for UI and Infographics Understanding}, 
      author={Gilles Baechler and Srinivas Sunkara and Maria Wang and Fedir Zubach},
      year={2024},
      eprint={2402.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.04615}, 
}

@InProceedings{T-Rex2,
author="Jiang, Qing
and Li, Feng
and Zeng, Zhaoyang
and Ren, Tianhe
and Liu, Shilong
and Zhang, Lei",
title="T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy",
booktitle="Computer Vision -- ECCV 2024",
year="2025",
publisher="Springer Nature Switzerland",
address="Cham",
pages="38--57"
}


@misc{T-Rex,
      title={T-Rex: Counting by Visual Prompting}, 
      author={Qing Jiang and Feng Li and Tianhe Ren and Shilong Liu and Zhaoyang Zeng and Kent Yu and Lei Zhang},
      year={2023},
      eprint={2311.13596},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.13596}, 
}

@article{CPT,
title = {CPT: Colorful Prompt Tuning for pre-trained vision-language models},
journal = {AI Open},
volume = {5},
pages = {30-38},
year = {2024},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2024.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666651024000056},
author = {Yuan Yao and Ao Zhang and Zhengyan Zhang},
keywords = {Vision-language pre-training models, Prompt tuning},
}

@inproceedings{Prompting,
author = {Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang},
title = {Prompting
         Visual-Language Models for Efficient Video Understanding},
year = {2022},
isbn = {978-3-031-19832-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-19833-5_7},
doi = {10.1007/978-3-031-19833-5_7},
booktitle = {ECCV 2022},
pages = {105–124},
numpages = {20},
location = {Tel Aviv, Israel}
}

@article{Texts,
  title={Texts as Images in Prompt Tuning for Multi-Label Image Recognition},
  author={Zixian Guo and Bowen Dong and Zhilong Ji and Jinfeng Bai and Yiwen Guo and Wangmeng Zuo},
  journal={CVPR},
  year={2022},
  pages={2808-2817},
  url={https://api.semanticscholar.org/CorpusID:253801830}
}

@INPROCEEDINGS{Conditional-Prompt,
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={CVPR}, 
  title={Conditional Prompt Learning for Vision-Language Models}, 
  year={2022},
  volume={},
  number={},
  pages={16795-16804},
  keywords={Training;Representation learning;Adaptation models;Computer vision;Neural networks;Manuals;Market research;Representation learning},
  doi={10.1109/CVPR52688.2022.01631}}


@misc{
Unified-Vision,
title={Unified Vision and Language Prompt Learning},
author={Yuhang Zang and Wei Li and Kaiyang Zhou and Chen Huang and Chen Change Loy},
year={2023},
url={https://openreview.net/forum?id=1QQnYd02etI}
}

@article{Vision-Language-Prompt,
  title={Multitask Vision-Language Prompt Tuning},
  author={Sheng Shen and Shijia Yang and Tianjun Zhang and Bohan Zhai and Joseph Gonzalez and Kurt Keutzer and Trevor Darrell},
  journal={2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2022},
  pages={5644-5655},
  url={https://api.semanticscholar.org/CorpusID:253734247}
}

@inproceedings{
Navigate-the-Web,
title={Learning to Navigate the Web},
author={Izzeddin Gur and Ulrich Rueckert and Aleksandra Faust and Dilek Hakkani-Tur},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BJemQ209FQ},
}

@article{Reinforcement-Learning,
  title={Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration},
  author={Evan Zheran Liu and Kelvin Guu and Panupong Pasupat and Tianlin Shi and Percy Liang},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.08802},
  url={https://api.semanticscholar.org/CorpusID:3530344}
}

@inproceedings{WorldofBits,
  title={World of Bits: An Open-Domain Platform for Web-Based Agents},
  author={Tianlin Shi and Andrej Karpathy and Linxi (Jim) Fan and Josefa Z. Hern{\'a}ndez and Percy Liang},
  booktitle={International Conference on Machine Learning},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:34953552}
}

@article{AppAgent,
  title={AppAgent: Multimodal Agents as Smartphone Users},
  author={China. Xiaoyan Zhang and Zhao Yang and Jiaxuan Liu and Yucheng Han and Xin Chen and Zebiao Huang and Bin Fu and Gang Yu},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.13771},
  url={https://api.semanticscholar.org/CorpusID:266435868}
}

@inproceedings{VisualRWKV,
    title = "{V}isual{RWKV}: Exploring Recurrent Neural Networks for Visual Language Models",
    author = "Hou, Haowen  and
      Zeng, Peigen  and
      Ma, Fei  and
      Yu, Fei Richard",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.694/",
    pages = "10423--10434"
}


@inproceedings{RWKV,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alonz",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936/",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077"
}

@misc{DocLayout-YOLO,
      title={DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception}, 
      author={Zhiyuan Zhao and Hengrui Kang and Bin Wang and Conghui He},
      year={2024},
      eprint={2410.12628},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.12628}, 
}

@misc{LLaVA-NeXT,
    title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},
    url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},
    author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}

@article{Websight,
  author       = {Hugo Lauren{\c{c}}on and
                  L{\'{e}}o Tronchon and
                  Victor Sanh},
  title        = {Unlocking the conversion of Web Screenshots into {HTML} Code with
                  the WebSight Dataset},
  journal      = {CoRR},
  volume       = {abs/2403.09029},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.09029},
  doi          = {10.48550/ARXIV.2403.09029},
  eprinttype    = {arXiv},
  eprint       = {2403.09029},
  timestamp    = {Fri, 05 Apr 2024 14:02:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-09029.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Web2Code,
 author = {Yun, Sukmin and lin, haokun and Thushara, Rusiru and Bhat, Mohammad },
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {112134--112157},
 publisher = {Curran Associates, Inc.},
 title = {Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/cb66be286795d71f89367d596bf78ea7-Paper-Datasets_and_Benchmarks_Track.pdf},
 volume = {37},
 year = {2024}
}

@inproceedings{Webui-7k,
author = {Wu, Jason and Wang, Siyan and Shen, Siman and Peng},
title = {WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics},
year = {2023},
isbn = {9781450394215},
url = {https://doi.org/10.1145/3544548.3581158},
doi = {10.1145/3544548.3581158},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {286},
numpages = {14},
keywords = {Computational Interaction, Computer Vision, Dataset, Transfer Learning, UI Modeling, Web Semantics},
series = {CHI '23}
}


@misc{Screenqa,
      title={ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots}, 
      author={Yu-Chung Hsiao and Fedir Zubach and Gilles Baechler and Victor Carbune and Jason Lin and Maria Wang and Srinivas Sunkara and Yun Zhu and Jindong Chen},
      year={2024},
      eprint={2209.08199},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.08199}, 
}

@misc{Bunny,
      title={Efficient Multimodal Learning from Data-centric Perspective}, 
      author={Muyang He and Yexin Liu and Boya Wu and Jianhao Yuan and Yueze Wang and Tiejun Huang and Bo Zhao},
      year={2024},
      eprint={2402.11530},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.11530}, 
}

@misc{visualwebbench,
      title={VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?}, 
      author={Junpeng Liu and Yifan Song and Bill Yuchen Lin and Wai Lam and Graham Neubig and Yuanzhi Li and Xiang Yue},
      year={2024},
      eprint={2404.05955},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05955}, 
}

@article{mineru,
  title={MinerU: An Open-Source Solution for Precise Document Content Extraction},
  author={Wang, Bin and Xu, Chao and Zhao, Xiaomeng and Ouyang, Linke and Wu, Fan and Zhao, Zhiyuan and Xu, Rui and Liu, Kaiwen and Qu, Yuan and Shang, Fukai and others},
  journal={arXiv preprint arXiv:2409.18839},
  year={2024}
}

@misc{
zhang2023llavar,
title={Enhanced Visual Instruction Tuning for Text-Rich Image Understanding},
author={Yanzhe Zhang and Ruiyi Zhang and Jiuxiang Gu and Yufan Zhou and Nedim Lipka and Diyi Yang and Tong Sun},
year={2024},
url={https://openreview.net/forum?id=tj4a1JY03u}
}
@InProceedings{IDL,
author="Biten, Ali Furkan
and Tito, Rub{\`e}n
and Gomez, Lluis",
title="OCR-IDL: OCR Annotations for Industry Document Library Dataset",
booktitle="Computer Vision -- ECCV 2022 Workshops",
year="2023",
pages="241--252",
isbn="978-3-031-25069-9"
}

@InProceedings{cordv2,
author="Kim, Geewook
and Hong, Teakgyu
and Yim, Moonbin",
title="OCR-Free Document Understanding Transformer",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="498--517",
isbn="978-3-031-19815-1"
}

@misc{llavanext,
    title={LLaVA-NeXT: What Else Influences Visual Instruction Tuning Beyond Data?},
    url={https://llava-vl.github.io/blog/2024-05-25-llava-next-ablations/},
    author={Li, Bo and Zhang, Hao and Zhang, Kaichen and Guo, Dong and Zhang, Yuanhan and Zhang, Renrui and Li, Feng and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}
