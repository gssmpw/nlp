\section{RELATED WORK}
\subsection{Visual prompts}
In multimodal learning, prompting has become a key technique for improving VLM performance. Traditionally, text-based prompting____________ inserts learnable prompts into input text to guide models in completing tasks. Visual prompts, on the other hand, use cues like color, shape, or position to improve a model's understanding of visual information. For example, CPT____  uses color prompts to assist object recognition. T-Rex____ uses visual prompts for object counting, and T-Rex2____ encodes points or bounding boxes as embeddings to support various reasoning workflows. Similarly to CPT and T-Rex,  our work combines color-based prompts with visual-language prompts to enhance model reasoning capabilities.
\subsection{UI unstanding}
Early research on the understanding of UI focuses mainly on task execution and intelligent navigation for web interfaces________. As UI complexity increased, research shifted towards multimodal Vision-Language Models (VLMs). Ferret-UI____ improves visual capabilities with an 'arbitrary resolution' strategy, while ScreenAI improves performance by refining the PaLI architecture and generating large-scale datasets. Furthermore, agents based on language models, such as Mobile-Agent____ and AppAgent____, integrate visual and language information, improving reasoning and interaction in complex UI scenarios.
\subsection{Visual Language Models} 
Visual-Language Models(VLMs) combine visual and textual information to handle complex reasoning and tasks. RWKV____ is an efficient RNN architecture with linear complexity and constant memory usage, achieving GPT-level performance in language modeling. VisualRWKV____ extends RWKV to the visual-language domain, enabling efficient joint processing of visual and textual information and demonstrating advantages in long-sequence modeling.