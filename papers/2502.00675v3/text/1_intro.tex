\section{Introduction}

Text-to-SQL converts natural language queries into SQL queries, serving as a key technology for lowering the barrier to accessing relational databases~\citep{zelle1996learning,zettlemoyer2012learning,zhong2017seq2sql,yu2018spider,wang2019rat,gao2023text,lei2024spider}. This technique enables natural language interfaces for databases, supporting critical applications such as business intelligence and automated processes. Text-to-SQL reduces repetitive human labor and alleviates the burden on data analysts and programmers alike.

Previous Text-to-SQL research primarily focused on model training and fine-tuning~\citep{zhong2017seq2sql,wang2019rat,scholak2021picard} on simpler datasets like Spider 1.0~\citep{yu2018spider}. The rise of large language models (LLMs) has shifted this paradigm from ``pre-train and fine-tune" to prompting, thanks to the strong code generation capabilities of LLMs~\citep{anthropic2023claude3,roziere2023code,achiam2023gpt}. Consequently, numerous Text-to-SQL methods now rely on prompting~\citep{zhang2023act,gao2023text,pourreza2024din,talaei2024chess} with powerful LLM API services.

These methods achieve impressive performance on classic benchmarks, e.g., exceeding 90\% on Spider 1.0~\citep{yu2018spider} and 70\% on BIRD~\citep{li2024can}. However, these datasets are often built on non-industrial databases with few tables and columns, simplistic SQL queries, and straightforward questions. These limitations fail to reflect the complexity of real-world tasks. As a result, existing methods struggle with the newly proposed Spider 2.0 dataset~\citep{lei2024spider}, which mirrors real-world challenges by requiring multiple SQL dialects, varying syntax and functions, nested columns, external knowledge, and the ability to handle ambiguous requests and column names.

This complexity of realistic Text-to-SQL problems calls for agentic methods that enable LLMs to dynamically interact with their environment, i.e., the databases. These methods utilize tools, execute commands, observe feedback, and plan actions, surpassing simple prompting to tackle more complex tasks such as planning~\citep{wang2023plan,shinn2024reflexion}, reasoning~\citep{wei2022chain,besta2024graph,shao2024deepseekmath}, and advanced code generation~\citep{chen2023teaching,chen2024coder,yang2024intercode}. As a result, \citet{lei2024spider} introduced Spider-Agent, a framework based on the ReAct paradigm~\citep{yao2023react}, which combines reasoning and acting components to navigate and overcome the challenges posed by the Spider 2.0 dataset.

However, code agents often face challenges in maintaining control, particularly in long-context scenarios where they may fail to follow instructions or overlook critical task details. Providing an LLM with complete database information without specifying value types or SQL dialects often leads to repeated iterations for fixing syntax errors, correcting data types, or selecting the correct functions, leaving limited room for meaningful reasoning. Furthermore, existing Text-to-SQL methods struggle to handle challenges such as multiple dialects, nested columns, and complex data types in the Spider 2.0 dataset.

To address these issues, we propose ReFoRCE (Self-\textbf{Re}finement Agent with \textbf{Fo}rmat \textbf{R}estriction and \textbf{C}olumn \textbf{E}xploration), which simplifies the process by breaking it into manageable subtasks for better control. As shown in Figure~\ref{fig:prompting}, we use table information compression to mitigate long-context issues in large databases, a common limitation of current Text-to-SQL methods. We introduce answer format restriction to enhance instruction adherence and ensure accurate responses. Additionally, we conduct column exploration to iteratively execute SQL queries, progressing from simple to complex, to understand SQL dialects, data types, and nested columns. Finally, we implement a self-refinement workflow to correct answers and apply self-consistency to increase confidence in the outputs. To further boost reliability, we apply parallelization by running the entire workflow across multiple threads simultaneously and employ a voting mechanism to determine the most likely correct outcome. Due to the challenge of the dataset and also our strict consistency mechanism, sometimes executing the generated SQL query does not return any rows. For these examples, we apply a Common Table Expression (CTE) based self-refinement approach. CTE is a temporary result set that can be used in a SQL query. We parse agent-generated SQLs and extract CTE statements for execution. In this way, we allow the agent to examine the intermediate CTE results and approach a solution step-by-step. 

We evaluate our methods on two tasks of Spider 2.0~\citep{lei2024spider}: Spider 2.0-Snow and Spider 2.0-Lite. Spider 2.0-Snow is unanimously in Snowflake SQL, while Spider 2.0-Lite includes examples in BigQuery and SQLite, additionally. ReFoRCE supports multiple dialects with minimal changes in prompts, making it versatile across various database systems. Our ReFoRCE agent achieves state-of-the-art results, scoring 31.26 on Spider 2.0-Snow and 30.35 on Spider 2.0-Lite, outperforming Spider-Agent's score around 20. This demonstrates the effectiveness of our method in handling multiple dialects, nested columns and complex data types in Spider 2.0.