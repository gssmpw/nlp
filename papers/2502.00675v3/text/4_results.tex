\section{Experiments}

\subsection{Experimental Setup}

\paragraph{Dataset} 
We evaluate our approach using the Spider 2.0 dataset \citep{lei2024spider}, which includes two subsets: Spider 2.0-Snow and Spider 2.0-Lite. Both subsets consist of 547 examples, encompassing over 150 databases with an average of 800 columns per database. Each SQL query contains approximately 150 tokens, making the task particularly challenging. The key difference between the two subsets lies in their SQL dialects: Spider 2.0-Snow focuses exclusively on the Snowflake dialect, whereas Spider 2.0-Lite supports BigQuery, Snowflake, and SQLite dialects. 

\paragraph{Evaluation Metrics}  We evaluate performance using the widely adopted metric, Execution Accuracy (EX) \citep{yu2018spider, li2024can}. For certain examples, ambiguous questions may not explicitly specify which columns to return. The evaluation scripts are designed to focus on the essential components of the answers, disregarding irrelevant columns and concentrating on the core elements outlined in the instructions. As a result, the inclusion of extra columns is considered acceptable.

\paragraph{Large Language Models} We conduct our experiments using the GPT family models, specifically GPT-4o \citep{achiam2023gpt} and o1-preview \citep{openai2024api}. GPT-4o is used for testing some baselines, while o1-preview is employed for our method and Spider-Agent. We opted for o1-preview instead of the formal o1 or o3-mini because o1-preview achieves better results for this particular task.

\paragraph{Baselines} We employ the state-of-the-art code agent framework Spider-Agent \citep{lei2024spider} for the Spider 2.0-Snow and Spider 2.0-Lite dataset. Additionally, we utilize widely recognized prompting methods such as DAIL-SQL \citep{gao2023text}, DIN-SQL \citep{pourreza2024din} and SFT CodeS \citep{CodeS}.

\subsection{Evaluation Results}

\begin{table}[t]
\centering
\caption{Comparison of Methods with Execution Accuracy (EX) on the Spider 2.0-Snow and Spider 2.0-Lite Datasets. Higher values indicate better performance.}
\label{tab:method_comparison_combined}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Model} & \textbf{EX ($\uparrow$) on Spider 2.0-Snow} & \textbf{EX ($\uparrow$) on Spider 2.0-Lite} \\
\midrule
\textbf{ReFoRCE (ours)} & o1-preview & \textbf{31.26} & \textbf{30.35} \\
Spider-Agent & o1-preview & 23.58 & 23.03 \\
Spider-Agent & GPT-4o & 12.98 & 13.16 \\
DAIL-SQL & GPT-4o & 2.20 & 5.68 \\
CHESS & GPT-4o & 1.28 & 3.84 \\
DIN-SQL & GPT-4o & 0.00 & 1.46 \\
SFT CodeS-15B & -- & 0.00 & 0.73 \\
\bottomrule
\end{tabular}
\end{table}


% \paragraph{Results Analysis.} 
The results in Table~\ref{tab:method_comparison_combined} highlight the superior performance of our method on the Spider 2.0-Snow and Spider 2.0-Lite datasets. Using the \texttt{o1-preview} model, our method achieves execution accuracy (EX) scores of 31.26 on Spider 2.0-Snow and 30.35 on Spider 2.0-Lite, significantly outperforming all other methods.\footnote{Leaderboard data as of February 13, 2025.}

These results demonstrate the robustness of our approach in addressing the challenges posed by the datasets. On Spider 2.0-Snow, our method excels in effectively handling nested columns and complex data types. On Spider 2.0-Lite, which spans multiple dialects, including Snowflake, SQLite, and BigQuery, our method showcases remarkable adaptability and consistency. The slightly lower results on Spider 2.0-Lite are attributed to the fact that our prompts are primarily designed for the Snowflake dialect, leading to occasional errors when handling certain cases in the BigQuery dialect.

Compared to Spider-Agent, which scores around 23 on both datasets using the same \texttt{o1-preview} model, our method achieves higher scores, highlighting its superior capability in solving complex cases in Text-to-SQL tasks. Additionally, methods using \texttt{GPT-4o}, including Spider-Agent, DAIL-SQL, and CHESS, perform poorly, with scores ranging from 0.00 to 5.68. This underscores the importance of our specialized approach, which is specifically designed to address the intricacies of Text-to-SQL tasks across diverse SQL dialects.

In summary, the results firmly establish our method as the state-of-the-art in handling the challenges of Text-to-SQL tasks, particularly in complex scenarios involving multiple dialects, nested columns, and advanced data types. This achievement demonstrates the efficacy of our approach in pushing the boundaries of execution accuracy in Text-to-SQL parsing.
