\section{Methodology}

\subsection{Preliminaries}
Spider 2.0 \citep{lei2024spider} is a comprehensive code agent task where, given a question $\mathcal{Q}$, a database interface $\mathcal{I}$, and a codebase $\mathcal{C}$ (including context, configuration, and documentation as shown in Fig. 1), the goal is to iteratively modify the code (SQL/Python) $\mathcal{C}$ based on observations $\mathcal{O}_k = \text{execute}(\mathcal{C}, \mathcal{I}, \mathcal{Q})$ until the final result $\mathcal{A}$ (text/table/database) is obtained. The final observation $\mathcal{O}_k$ serves as the agent’s answer to the question, i.e., $\mathcal{A} = \mathcal{O}_k$. In contrast, Spider 2.0-snow and Spider 2.0-lite are self-contained Text-to-SQL tasks. Given a database schema $\mathcal{D}$, a natural language question $\mathcal{Q}$, and auxiliary documentation $\mathcal{E}$, the Text-to-SQL parser $f(\cdot)$ generates the SQL query $\mathcal{S} = f(\mathcal{Q}, \mathcal{D}, \mathcal{E} \,|\, \theta)$, where $\theta$ denotes the parser’s parameters.

\cite{lei2024spider} introduced Spider-Agent, a framework built on the ReAct \cite{yao2023react} paradigm with function-calling capabilities such as \texttt{EXEC\_SQL} for executing SQL queries and \texttt{TERMINAL} for performing command-line operations to navigate DBT (Data Build Tool) projects and read schema-related files. The agent operates by receiving observations, which represent the current state of the environment or the outcome of a function call initiated by the agent. Based on these observations, the agent generates a ``Thought" and selects an appropriate ``Action" from a predefined list of function calls. The task is considered complete when the agent invokes the \texttt{TERMINATE} function.

\subsection{ReFoRCE: Self-Refinement Agent with Format Restriction and Column Exploration}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/workflow.pdf}
    \caption{An overview of our Self-\textbf{Re}finement Agent with \textbf{Fo}rmat \textbf{R}estriction and \textbf{C}olumn \textbf{E}xploration (\textbf{ReFoRCE}) workflow. (a) Table compression to address long-context limitations; (b) Format restriction to ensure accurate answer formatting; (c) Iterative column exploration for improved schema understanding; (d) Self-refinement pipeline comprising parallelized workflows with voting mechanisms.}
    \label{fig:prompting}
\end{figure}

Since ReAct agents have a high degree of freedom, their workflow lacks necessary reliability and predictability. To address this problem, we propose \textbf{ReFoRCE} (Self-\textbf{Re}finement Agent with \textbf{Fo}rmat \textbf{R}estriction and \textbf{C}olumn \textbf{E}xploration), which simplifies the process by dividing it into manageable subtasks for greater control. ReFoRCE employs a self-refinement workflow that incorporates format restriction and column exploration to identify challenging examples. We implement a self-refinement workflow to correct answers and apply self-consistency to increase confidence in the outputs, as shown in Figure~\ref{fig:prompting}. To further boost reliability, we apply parallelization by running the entire workflow across multiple threads simultaneously and employ a voting mechanism to determine the most likely correct outcome. Due to the challenge of the dataset and also our strict consistency mechanism, sometimes executing the generated SQL query does not return any rows. For these examples, we apply a Common Table Expression (CTE) based self-refinement approach. CTE is a temporary result set that can be used in a SQL query. We parse agent-generated SQLs and extract CTE statements for execution. In this way, we allow the agent to examine the intermediate CTE results and approach a solution step-by-step. Notably, these techniques work alike on various database systems, thus adding support for a new type of database takes as little effort as adding several prompts. 

\subsubsection{Table Information Compression}
    Following the approach of Spider 2.0~\citep{lei2024spider}, we create a dictionary for each example, incorporating external knowledge and table structures using Database Definition Language (DDL) files. In some specific examples, DDL files exceed setting size, surpassing the context limitations of models like ChatGPT. To address this, we apply a pattern-based matching that merge tables with similar prefixes or suffixes. For these tables, we retain only one representative DDL file as input, while for others, we provide only the table names to the model. Additionally, we use a straightforward API-based schema linking approach for examples with relatively long prompts to ensure the context length remains within limits while minimizing information loss during the linking process.

    Given the database information $\mathcal{D}$ and auxiliary documentation $\mathcal{E}$, we apply the \texttt{compress} function to compress it and concatenate the result with the question $\mathcal{Q}$ as the initial input prompt $\mathcal{P}_{\text{init}}$: 
    \begin{equation}
        \mathcal{P}_{\text{init}} = \texttt{compress}(\mathcal{D}) + \mathcal{E} + \mathcal{Q}.
    \end{equation}
    For example, in the GA360 database, there is one year of data with table names ranging from \texttt{GA\_SESSIONS\_20160801} to \texttt{GA\_SESSIONS\_20170801}. Each table’s DDL file occupies more than 150 KB, resulting in a total length of over 50 MB—an impractically large size for LLMs to process. 
    Our pattern-based compression significantly reduces DDL file sizes of such databases.
    
    \subsubsection{Expected Answer Format Restriction}
    Realistic Text-to-SQL problems often face challenges related to long context issues. When the context exceeds 100k tokens, the model may lose critical information, such as detailed task descriptions. In cases where the task description is clear and unambiguous, determining the expected answer format (e.g., column names, data types, and the number of rows) is typically straightforward. However, when overwhelmed by excessive information, LLMs often struggle to follow instructions accurately. Even with prompts designed to guide or correct the output, LLMs may keep generating incorrect answers and fail to correct their responses.
    
    To address this, we propose Expected Answer Format Restriction, which involves generating the expected format at the outset and consistently reinforcing this format during self-refinement. The response must strictly adhere to the specified format in CSV style, ensuring alignment with executed CSV files. Each column should be explicitly defined, including all necessary attributes, and each record should occupy a separate row. The format should account for specific cases, such as superlatives, percentages, or coordinates, ensuring the output is concise, clear, and unambiguous. For ambiguous terms, potential values or additional columns can be added to maintain clarity and precision. Figure~\ref{fig:prompting} illustrates a case of format restriction. For example, when a question asks for the highest number, the answer should be presented in a single row. Annotations such as ``answer in one row" can be appended to the format table to ensure clarity. Additionally, in the Spider 2.0 evaluation setting~\citep{lei2024spider}, it is acceptable to include extra columns. For instance, even if the task only requires barcodes, our format also includes a ``copy number" column, which is highly relevant to the task. This approach ensures consistency and accuracy, even when managing long contexts and complex task descriptions.  
    
    For an LLM chat session $\mathcal{L}_{\text{session}}$, we input initial prompts alongside format prompts $\mathcal{P}_{\text{format}}$ to generate the expected answer format $\mathcal{F}$:  
    \begin{equation}
        \mathcal{F} = \mathcal{L}_{\text{session}}(\mathcal{P}_{\text{init}}, \mathcal{P}_{\text{format}}).
    \end{equation}

        
    \subsubsection{Exploration of Potentially Useful Columns} When directly providing the entire database information to an LLM, the lack of details on value types and SQL dialects often leads to repeated iterations for refining syntax errors, correcting data types, or invoking the correct functions. This process is not only time-consuming but also leaves little room for real reasoning. For baselines such as DAIL-SQL~\citep{gao2023text} and DIN-SQL~\citep{pourreza2024din} in Spider 2.0-Lite, sample rows provided by the Lite dataset are often used to help the model understand the structure of the tables. However, for nested columns, even a few rows can be too lengthy to be fed into the LLM. Additionally, specific values in sample rows often lack diversity and are biased, which can mislead the model into generating incorrect answers based on these limited samples.
        
    To address these challenges and ensure a comprehensive understanding of the database structure, we design a systematic approach to explore potentially useful columns. The process begins with identifying relevant tables and columns, guided by prompts designed to extract meaningful information. Dynamically generated SQL queries progress from simple, non-nested formats to more complex ones, enabling a gradual understanding of the database and arriving at the correct answer. These queries follow the structure \texttt{SELECT DISTINCT "COLUMN\_NAME" FROM DATABASE.SCHEMA.TABLE WHERE ...} (specific to the Snowflake dialect) and adhere to constraints such as avoiding Common Table Expressions (CTEs) and schema-level checks, while limiting the output to 100 rows or 500 B per query. Columns in JSON or nested formats are explicitly handled using techniques like \texttt{LATERAL FLATTEN} to extract nested values. Additionally, string-matching queries utilize fuzzy patterns (e.g., \texttt{\%target\_str\%}) to enhance flexibility.  
    
    Here, we employ an additional LLM chat session, $\mathcal{L}'_{\text{session}}$, where the input consists of $\mathcal{P}_{\text{init}}$ alongside column exploration prompts $\mathcal{P}_{\text{exploration}}$. This generates exploration content, including relevant tables and columns $\mathcal{P}_{\text{column}}$ and SQL queries $\mathcal{S}_{\text{exploration}}$ for interacting with the database. Database APIs are then invoked to execute the SQL queries and retrieve results $\mathcal{R}_{\text{exploration}}$:
    \begin{align}
        \mathcal{P}_{\text{column}}, \mathcal{S}_{\text{exploration}} &= \mathcal{L}'_{\text{session}}(\mathcal{P}_{\text{init}}, \mathcal{P}_{\text{exploration}}),\\
        \mathcal{R}_{\text{exploration}} &= \texttt{API}(\mathcal{S}_{\text{exploration}}).
    \end{align}
    During this stage, the LLM generates more than 10 SQL queries simultaneously, making it essential to execute each query accurately for effective exploration. However, if one SQL query contains an error, subsequent queries are likely to exhibit similar issues. To address this, we propose Algorithm~\ref{alg:execute_sql}, which offers a structured approach to executing SQL queries while dynamically addressing errors through self-correction.
        

    \subsubsection{Self-Refinement Workflow for Problem-Solving}
\paragraph{Self-Refinement with Execution Feedback}
    After obtaining the table information $\mathcal{P}_{\text{init}}$, exploring value data $\mathcal{P}_{\text{column}} + \mathcal{R}_{\text{exploration}}$, and defining the expected answer format $\mathcal{F}$, we input these elements into the model and employ a self-refinement process. This process enables the model to correct errors, improve its answers, and achieve results $\mathcal{R}_{\text{final}}$ with high confidence through self-consistency.
    \begin{equation}
        \mathcal{R}_{\text{final}} = \texttt{self-refinement}(\mathcal{P}_{\text{init}}, \mathcal{P}_{\text{column}} + \mathcal{R}_{\text{exploration}}, \mathcal{F}).
    \end{equation}

 The details in self-refinement workflow are presented in Algorithm~\ref{alg:self_refine}. Beginning with purified information, it generates SQL queries and executes them via an API, evaluating their correctness and reasonableness based on the returned results. Identified errors are iteratively refined to improve accuracy. The refinement process is governed by termination conditions, including achieving self-consistency, where the same reasonable answer is obtained twice. Empty results or columns containing only empty strings or zeros are treated as incorrect and excluded from consistency checks. The workflow also halts if a predefined maximum number of iterations is reached, as this may indicate that the model is producing low-confidence answers, which are treated as empty. Furthermore, consecutive empty results signal the termination of further refinements. This conservative strategy prioritizes high confidence in non-empty answers, even at the cost of retaining a significant number of empty results, ensuring robust and reliable outputs.

\paragraph{CTE-based Refinement} Additionally, if the self-refinement workflow fails to generate a SQL, we further attempt to construct a step-by-step data flow using Common Table Expressions (CTE). CTEs are intermediate tables that helps breaking down a complex query into multiple easier queries. We explicitly prompt the agent to generate the SQL as a set of meaningful CTEs conjoined. 

After the agent generates a SQL, we parse it to obtain the individual CTEs and obtain execution results at the end of each CTE. The agent is asked to verify each execution result individually.
If a result is not as expected, the agent has the choice to rewrite the current CTE. This pipeline allows the agent to localize errors. This also provides a way for the agent to check the logic of each CTE individually and help explore alternative columns.

\subsubsection{Parallelization}
    Despite the self-consistency mechanism, variations in results may arise across runs of the same example due to differing perspectives in column exploration. To enhance confidence in the outcomes, we employ parallelization by launching multiple threads to execute the entire workflow simultaneously. The results are first compared programmatically, using a voting mechanism to identify the most likely correct outcome. If voting alone cannot resolve discrepancies, the model further evaluates the results to determine the most accurate answer. This approach considers diverse perspectives and multiple iterations, facilitating convergence on higher-quality outcomes.

    Using the same format $\mathcal{F}$ and database information $\mathcal{P}_{\text{init}}$, we  launch multiple threads (we set it to 3 in our experiments) to execute the entire process independently in parallel:
    \begin{equation}
        \mathcal{R}_{\text{parallel}} = \bigcup \text{Parallel}\left\{
        \begin{aligned}
            &\mathcal{P}_{\text{column}}, \mathcal{S}_{\text{exploration}} = \mathcal{L}'_{\text{session}}(\mathcal{P}_{\text{init}}, \mathcal{P}_{\text{exploration}}), \\
            &\mathcal{R}_{\text{exploration}} = \texttt{API}(\mathcal{S}_{\text{exploration}}), \\
            &\mathcal{R}_{\text{final}} = \texttt{self-refinement}(\mathcal{P}_{\text{init}}, \mathcal{P}_{\text{column}} + \mathcal{R}_{\text{exploration}}, \mathcal{F}),
        \end{aligned}
        \right\}.
    \end{equation}
    Finally, the results from the parallel execution are aggregated through a voting mechanism:
    \begin{equation}
        \mathcal{R}_{\text{vote}} = \texttt{model\_vote}(\mathcal{R}_{\text{parallel}}).
    \end{equation}
     
    Furthermore, since each example is independent and both the model and database APIs support parallel execution, we enable parallelization across different examples as well. This strategy significantly accelerates the overall process while ensuring reliable and consistent performance.