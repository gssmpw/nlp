\section{Related Work}
\subsection{Vision-Language Pre-training}
Vision-language pre-training has recently achieved remarkable success in various multimodal tasks by learning multimodal representations on large-scale image-text or video-text pairs \cite{Radford2021LearningTV, Fang2022EVAET, Li2021AlignBF, Bain2021FrozenIT, Luo2021CLIP4ClipAE, Mu2021SLIPSM, Ma2022XCLIPEM, Huang2022CloverTA, Xu2021VideoCLIPCP, Chen2023InternVS, Xu2023mPLUG2AM, Liu2023VisualIT, li2024llava}. 
Methods like CLIP \cite{Radford2021LearningTV}, ALIGN \cite{Jia2021ScalingUV}, and DeCLIP \cite{Li2021SupervisionEE} leverage contrastive learning for pre-training on large massive noisy web data, achieving promising performance on cross-modal retrieval tasks. 
ALBEF \cite{Li2021AlignBF} and FLAVA \cite{Singh2021FLAVAAF} introduce multimodal encoders to perform complex cross-modal interactions, leading to superior performance in multimodal reasoning tasks. SimVLM \cite{Wang2021SimVLMSV}, CoCa \cite{Yu2022CoCaCC}, and BLIP \cite{Li2022BLIPBL} add multimodal decoders, enabling new capabilities in image-conditioned text generation. Recent multimodal large models like BLIP-2 \cite{Li2023BLIP2BL}, LLAVA \cite{li2024llava}, and Qwen-VL  \cite{Bai2023QwenVLAV} harness large language models as a cognitive powerhouse, further advancing performance in various multimodal tasks \cite{Zhang2024MMLLMsRA, Yin2023ASO}.

\subsection{Multimodal Models for Relevance}
Early relevance methods focus on the text modality \cite{Yao2021LearningAP, Chang2021ExtremeML, Zou2021PretrainedLM, Liu2021Que2SearchFA}. In recent years, some multimodal relevance methods have emerged. For instance, Query-LIFE \cite{Zhu2023QueryLIFEQL} utilizes a query-based multimodal fusion to effectively incorporate the image and title of products, improving relevance and conversion efficiency in e-commerce search. QUALITY \cite{Ye2023QueryawareMB} introduces a query-aware multimodal model to enhance the ranking relevance of video search. However, these methods necessitate the addition of extra query towers, which complicates both the model structure and the pre-training process. In contrast, our HCMRM can directly reuse existing mainstream multimodal models. Additionally, their pre-training processes rely on constructing query-item pairs from the online click logs. However, click data is often noisy and reliable query-item pairs are typically limited. This limits the effectiveness of the pre-training.

\subsection{Relevance Fine-tuning Loss}
In industrial scenarios, the relevance of query-item pairs is manually divided into multiple levels \cite{Yao2021LearningAP, Zou2021PretrainedLM, Wen2023EnhancingDI, Ye2023QueryawareMB}. For instance, Baidu search ads categorize query-image relevance as \{0, 1, 2\}, with 0 as negative and 1 or 2 as positive, and binary cross-entropy loss is used for relevance fine-tuning training \cite{Wen2023EnhancingDI}. Tencent Video categorizes query-video relevance into four levels \cite{Ye2023QueryawareMB}. They utilize ordinal regression loss for training, which is a good approach to account for the ordinal nature of relevance labels.


\begin{figure*}
  \includegraphics[width=0.95\textwidth]{fig/pretrained_model.pdf}
  \caption{Overview of HCMRM. It is built based on ALBEF with minimal modifications and pre-trained using four objectives: Image-Text Contrastive Learning (ITC), Image-Text Matching (ITM), Masked Language Modeling (MLM), and Pseudo-Query-Video Matching (PQVM). Note that the downstream relevance task between query and short video ad is consistent with PQVM.}
  \Description{Overview of the proposed multimodal relevance model, HCMRM.}
  \label{fig:pretrained_model}
\end{figure*}