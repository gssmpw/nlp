\section{Related Work}
\subsection{Vision-Language Pre-training}
Vision-language pre-training has recently achieved remarkable success in various multimodal tasks by learning multimodal representations on large-scale image-text or video-text pairs ____. 
Methods like CLIP ____, ALIGN ____, and DeCLIP ____ leverage contrastive learning for pre-training on large massive noisy web data, achieving promising performance on cross-modal retrieval tasks. 
ALBEF ____ and FLAVA ____ introduce multimodal encoders to perform complex cross-modal interactions, leading to superior performance in multimodal reasoning tasks. SimVLM ____, CoCa ____, and BLIP ____ add multimodal decoders, enabling new capabilities in image-conditioned text generation. Recent multimodal large models like BLIP-2 ____, LLAVA ____, and Qwen-VL  ____ harness large language models as a cognitive powerhouse, further advancing performance in various multimodal tasks ____.

\subsection{Multimodal Models for Relevance}
Early relevance methods focus on the text modality ____. In recent years, some multimodal relevance methods have emerged. For instance, Query-LIFE ____ utilizes a query-based multimodal fusion to effectively incorporate the image and title of products, improving relevance and conversion efficiency in e-commerce search. QUALITY ____ introduces a query-aware multimodal model to enhance the ranking relevance of video search. However, these methods necessitate the addition of extra query towers, which complicates both the model structure and the pre-training process. In contrast, our HCMRM can directly reuse existing mainstream multimodal models. Additionally, their pre-training processes rely on constructing query-item pairs from the online click logs. However, click data is often noisy and reliable query-item pairs are typically limited. This limits the effectiveness of the pre-training.

\subsection{Relevance Fine-tuning Loss}
In industrial scenarios, the relevance of query-item pairs is manually divided into multiple levels ____. For instance, Baidu search ads categorize query-image relevance as \{0, 1, 2\}, with 0 as negative and 1 or 2 as positive, and binary cross-entropy loss is used for relevance fine-tuning training ____. Tencent Video categorizes query-video relevance into four levels ____. They utilize ordinal regression loss for training, which is a good approach to account for the ordinal nature of relevance labels.


\begin{figure*}
  \includegraphics[width=0.95\textwidth]{fig/pretrained_model.pdf}
  \caption{Overview of HCMRM. It is built based on ALBEF with minimal modifications and pre-trained using four objectives: Image-Text Contrastive Learning (ITC), Image-Text Matching (ITM), Masked Language Modeling (MLM), and Pseudo-Query-Video Matching (PQVM). Note that the downstream relevance task between query and short video ad is consistent with PQVM.}
  \Description{Overview of the proposed multimodal relevance model, HCMRM.}
  \label{fig:pretrained_model}
\end{figure*}