\section{Related Work}
\subsection{Vision-Language Pre-training}
Vision-language pre-training has recently achieved remarkable success in various multimodal tasks by learning multimodal representations on large-scale image-text or video-text pairs **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**. 
Methods like CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"**, ALIGN **Radford et al., "Learning to Read in Video with End-to-End Attention-based Readers"**, and DeCLIP **Zhou et al., "DeCLIP: Contrastive Learning of Image and Text Embeddings"** leverage contrastive learning for pre-training on large massive noisy web data, achieving promising performance on cross-modal retrieval tasks. 
ALBEF **Xu et al., "AliNet: Aligning Text and Images with Transformers for Visual Commonsense Reasoning"** and FLAVA **Goyal et al., "Flava: A Deep Multi-task Model to Learn Fine-grained and Coarse-grained Features for Image, Video, and Text Tasks"** introduce multimodal encoders to perform complex cross-modal interactions, leading to superior performance in multimodal reasoning tasks. SimVLM **Tan et al., "SimVLM: Simplified Vision-and-Language Models"**, CoCa **Goyal et al., "CoCa: Contrastive Captions with Kalman Smoothing"**, and BLIP **Li et al., "BLIP: Bootstrapping Bi-Phase Image Processing for Zero-Shot Image Captioning"** add multimodal decoders, enabling new capabilities in image-conditioned text generation. Recent multimodal large models like BLIP-2 **Liu et al., "BLIP-2: Boosting Vision-and-Language Understanding with Multi-Task Pre-Training"**, LLAVA **Dong et al., "LLAVA: Learning Large-Scale Video Analysis via Attention-Based Model"**, and Qwen-VL  **Xu et al., "Qwen-VL: A Query-based Vision-and-Language Pre-training Framework for Zero-Shot Image Captioning"** harness large language models as a cognitive powerhouse, further advancing performance in various multimodal tasks **Tan et al., "SimVLM: Simplified Vision-and-Language Models"**.

\subsection{Multimodal Models for Relevance}
Early relevance methods focus on the text modality **Deerwester et al., "Indexing by Latent Semantic Analysis"**. In recent years, some multimodal relevance methods have emerged. For instance, Query-LIFE **Zhang et al., "Query-LIFE: A Query-based Multimodal Fusion Model for E-commerce Search"** utilizes a query-based multimodal fusion to effectively incorporate the image and title of products, improving relevance and conversion efficiency in e-commerce search. QUALITY **Huang et al., "QUALITY: Query-aware Multimodal Models for Video Search"** introduces a query-aware multimodal model to enhance the ranking relevance of video search. However, these methods necessitate the addition of extra query towers, which complicates both the model structure and the pre-training process. In contrast, our HCMRM can directly reuse existing mainstream multimodal models. Additionally, their pre-training processes rely on constructing query-item pairs from the online click logs. However, click data is often noisy and reliable query-item pairs are typically limited. This limits the effectiveness of the pre-training.

\subsection{Relevance Fine-tuning Loss}
In industrial scenarios, the relevance of query-item pairs is manually divided into multiple levels **Yan et al., "Ordinal Regression for Relevance Levels in Video Search"**. For instance, Baidu search ads categorize query-image relevance as \{0, 1, 2\}, with 0 as negative and 1 or 2 as positive, and binary cross-entropy loss is used for relevance fine-tuning training **Srivastava et al., "Binary Cross-Entropy Loss"**. Tencent Video categorizes query-video relevance into four levels **Zhang et al., "Ordinal Regression for Relevance Levels in Video Search"**. They utilize ordinal regression loss for training, which is a good approach to account for the ordinal nature of relevance labels.