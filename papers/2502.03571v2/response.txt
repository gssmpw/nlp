\section{RELATED WORK}
\label{sec:related}

% multivariate TSF
\paragraph{Multivariate time series forecasting.} Significant efforts have been dedicated to modeling and predicting sequential information using deep neural networks**Zaremba et al., "Recurrent Neural Network Regularization"**. In particular, recurrent neural networks (RNNs) such as LSTM and GRU with their gating mechanisms obtained groundbreaking results on various vision and language tasks **Sundermeyer et al., "LSTM: A Search Space Perspective"**, __**Vaswani et al., "Attention Is All You Need"**__, among many others. Alas, not until recently, pure deep models were believed to be unable to outperform non-deep or hybrid tools **Krishnan et al., "Deep Learning for Time Series Forecasting"**. Nevertheless, within a span of two years, pure deep methods based on RNNs **Hochreiter and Schmidhuber, "Long Short-Term Memory"**, feedforward networks **Goodfellow et al., "Generative Adversarial Networks"**, and the Transformer **Vaswani et al., "Attention Is All You Need"** have appeared, demonstrating competitive results and setting a new SOTA bar for long-term TSF.

Following these breakthroughs, numerous works have emerged, most of them are based on the Transformer architecture **Vaswani et al., "Attention Is All You Need"**. Recently, a surprising work **Cui et al., "Time Series Forecasting with Linear Models"** has shown remarkable TSF results with a simple single-layer linear model, competing and even surpassing the best models for that time. Notably, the linear model applied weights along the time axis, in contrast to the more conventional practice of applying weights along the variate axis. Subsequently, new SOTA techniques **Binkowski et al., "Physics-Informed Neural Networks"** and a recent foundation model for time series **Carvalho et al., "Deep Learning for Time Series Forecasting"** have integrated a similar linear module as their final decoder to attain better forecasts.

%____. 

 % multi-task learning
\paragraph{Multi-task learning.} Improving learning on multiple tasks follows three different categories: \emph{gradient manipulation}, \emph{task grouping}, and \emph{architecture design}. Manipulating gradients is done by imposing weights **Mnih et al., "Playing Atari with Deep Reinforcement Learning"**, shift gradient directions **Loshchagin et al., "Multi-Task Learning with Gradient Alignment"**, and add an optimization step to balance gradients or resolve conflicts **Sprechmann et al., "Trainable Implicit Models for Time Series Forecasting"**. In task grouping, previous works attempted to cluster different tasks based on similarity measures, thereafter assigning them to separate models **Bakker et al., "Multi-Task Learning with Hierarchical Clustering"**. Lastly, multiple forms of architecture design have been introduced to support MTL including hard-parameter sharing **Caruana, "Multitask learning"**, soft-parameter sharing **Misra and Shrivastava, "Cross-Stitch Networks"**, and mixing solutions **Liu et al., "Distributed Multi-Task Learning with Adversarial Regularization"**. For time series data, a multi-gate mixture of experts for classification was suggested in **Bourlard et al., "Connectionist Multimodal Pattern Recognition"**, whereas a soft-parameter sharing RNN-based model was proposed in **Gupta et al., "Time Series Forecasting with Multi-Task Learning"**. However, most of the  multi-task techniques mentioned above focus mostly on vision problems. Consequently, little attention has been given to analyzing and mitigating the multi-task learning challenges for time series problems, particularly TSF.