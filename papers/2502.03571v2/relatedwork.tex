\section{RELATED WORK}
\label{sec:related}

% multivariate TSF
\paragraph{Multivariate time series forecasting.} Significant efforts have been dedicated to modeling and predicting sequential information using deep neural networks~\cite{goodfellow2016deep}. In particular, recurrent neural networks (RNNs) such as LSTM and GRU with their gating mechanisms obtained groundbreaking results on various vision and language tasks \cite{hochreiter1997long, cho2014learning} across generative~\cite{naiman2024generative, naiman2024utilizing, ren2024learning} and augmentation~\cite{kaufman2024first, nochumsohn2024beyond} tasks, among many others. Alas, not until recently, pure deep models were believed to be unable to outperform non-deep or hybrid tools~\cite{oreshkin2020nbeats}. Nevertheless, within a span of two years, pure deep methods based on RNNs~\cite{salinas2020deepar}, feedforward networks~\cite{oreshkin2020nbeats}, and the Transformer~\cite{zhou2021informer} have appeared, demonstrating competitive results and setting a new SOTA bar for long-term TSF.

Following these breakthroughs, numerous works have emerged, most of them are based on the Transformer architecture \cite{wu2021autoformer, zhou2022fedformer, zhang2022crossformer}. Recently, a surprising work~\cite{zeng2023Transformers} has shown remarkable TSF results with a simple single-layer linear model, competing and even surpassing the best models for that time. Notably, the linear model applied weights along the time axis, in contrast to the more conventional practice of applying weights along the variate axis. Subsequently, new SOTA techniques \cite{nie2023time, xue2023make} and a recent foundation model for time series \cite{zhou2023one} have integrated a similar linear module as their final decoder to attain better forecasts.

%\cite{olivares2023neural, challu2023nhits}. 

 % multi-task learning
\paragraph{Multi-task learning.} Improving learning on multiple tasks follows three different categories: \emph{gradient manipulation}, \emph{task grouping}, and \emph{architecture design}. Manipulating gradients is done by imposing weights~\cite{groenendijk2020multi}, shift gradient directions~\cite{yu2020gradient}, and add an optimization step to balance gradients or resolve conflicts~\cite{chen2018gradnorm, sener2018multi, liu2021conflict}. In task grouping, previous works attempted to cluster different tasks based on similarity measures, thereafter assigning them to separate models~\cite{zamir2018taskonomy, standley2020tasks, fifty2021efficiently, song2022efficient}. Lastly, multiple forms of architecture design have been introduced to support MTL including hard-parameter sharing~\cite{vandenhende2021multi}, soft-parameter sharing~\cite{misra2016cross, ishihara2021multi}, and mixing solutions~\cite{guangyuan2022recon}. For time series data, a multi-gate mixture of experts for classification was suggested in~\cite{ma2018modeling}, whereas a soft-parameter sharing RNN-based model was proposed in~\cite{chen2020multi}. However, most of the  multi-task techniques mentioned above focus mostly on vision problems. Consequently, little attention has been given to analyzing and mitigating the multi-task learning challenges for time series problems, particularly TSF.