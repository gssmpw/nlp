\documentclass[smallextended]{svjour3}
\listfiles
\makeatletter % https://tex.stackexchange.com/a/499541
\let\cl@chapter\undefined
\makeatletter
% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{natbib}
\usepackage{xparse}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
%\usepackage{authblk}
\usepackage{pgfplots}
\usepackage{import}
\usepackage{color}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{varioref}
\usepackage[capitalise,noabbrev]{cleveref}
\usepackage[title]{appendix}
\usepackage{float}
\usepackage{booktabs}

\pgfplotsset{compat=1.16}

\title{An analysis of optimization problems involving ReLU neural networks}
\author{Christoph Plate \and 
            Mirko Hahn  \and
            Alexander Klimek \and
            Caroline Ganzer \and
            Kai Sundmacher \and
            Sebastian Sager}
\institute{Christoph Plate, Mirko Hahn, Kai Sundmacher, Sebastian Sager \at
         Otto von Guericke University Magdeburg, Magdeburg, Germany \\
         \email{\{christoph.plate, mirhahn, kai.sundmacher, sager\}@ovgu.de}
         \and
         Christoph Plate, Alexander Klimek, Caroline Ganzer, Kai Sundmacher, Sebastian Sager \at
         Max Planck Institute for Dynamics of Complex Technical Systems, Magdeburg, Germany \\
         \email{\{plate, klimek, cganzer, sundmacher, sager\}@mpi-magdeburg.mpg.de}
         \and
         Corresponding author: Christoph Plate
}

\date{Received: date / Accepted: date}
\newcommand{\myReLU}[1]{\textrm{ReLU}\left(#1\right)}
\newcommand{\myClippedReLU}[2][M]{\textrm{ReLU}_{#1}\left(#2\right)}

% Prohibit line breaking in inline math
\binoppenalty=10000
\relpenalty=10000

% Mark overfull boxes with a black bar
\overfullrule=1ex

% ORCID
% Alex: 
% Caroline: 0000-0002-7081-4523
% Sebastian: 0000-0002-0283-9075
% Sundmacher: 0000-0003-3251-0593
% Christoph: 0000-0003-0354-8904
% Mirko: 0000-0002-2442-3978

\begin{document}
\maketitle

\begin{abstract}
Solving mixed-integer optimization problems with embedded neural networks with ReLU activation functions is challenging.
Big-M coefficients that arise in relaxing binary decisions related to these functions grow exponentially with the number of layers. 
We survey and propose different approaches to analyze and improve the run time behavior of mixed-integer programming solvers in this context.
Among them are clipped variants and regularization techniques applied during training as well as optimization-based bound tightening and a novel scaling for given ReLU networks. %which yields a functionally equivalent neural network with lower big-M coefficients. 
We numerically compare these approaches for three benchmark problems from the literature.
We use the number of linear regions, the percentage of stable neurons, and overall computational effort as indicators. 
As a major takeaway we observe and quantify a trade-off between the often desired redundancy of neural network models versus the computational costs for solving related optimization problems.
\keywords{optimization \and machine learning \and neural network \and integer programming}
% \PACS{PACS code1 \and PACS code2 \and more}
%\subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}


\input{sections/01-intro}

\input{sections/02-methods}

\input{sections/03-results}

\input{sections/04-outlook}

\input{sections/05-acknowledgements}

\bibliographystyle{spbasic}
\bibliography{main}

\input{sections/06-appendix}

\end{document}
