\section{Methods}\label{sec:methods}

In this section we discuss several methods that have an impact on the overall performance of a MINLP solver such as \texttt{Gurobi}, when applied to optimization problems of type \eqref{prob:embedded}.
%
We start by introducing two measures of complexity in this context in Section~\ref{subsec:measures}, namely the number of regions partitioning the input domain in which the function $h(x)$ has identical linear output behavior, and the number of stable ReLU neurons.

Then we examine methods that are applicable to trained ANNs. In Section~\ref{subsec:boundtightening} bound tightening approaches for the optimization problem are presented. In Section~\ref{sec:scaling} we propose a novel scaling method that improves the $\ell^1$ regularization term of a pre-trained network without changing its encoded function. This method can be used after completed training of the ANN (a posteriori) and before the optimization is started (a priori).

In Sections~\ref{sec:regularization}, \ref{sec:clipped}, and \ref{sec:dropout} we investigate modifications to the training of the ANN, in particular regularization of training weights, clipped ReLU formulations, and the use of dropout during training.


%The goal of this paper is to examine and expand upon some of the acceleration methods enumerated in the previous section. 
%As the big-M formulation \eqref{eq:bigM} is used in the majority of publications investigating optimization problems with embedded ReLU neural networks, we restrict ourselves to this formulation. 
%Regarding the methods proposed for accelerating optimization algorithms, we particularly focus on the effects of bound tightening and training with regularization. We also propose a simple scaling method that improves the $\ell^1$ regularization term of a pre-trained network without changing its encoded function. Apart from these acceleration methods, we also consider dropout as it is a popular method applied during training. Before analyzing these methods in more detail, we introduce two important characteristics of ReLU networks which are an indicator of the complexity of optimization problems with embedded neural networks. 

\subsection{Measures of complexity of ReLU ANNs}\label{subsec:measures}

While the solution of mixed-integer optimization problems is difficult ($\mathcal{NP}$-complete) in general, it is well known that the number of optimization variables and the tightness of relaxations of the integer variables have a major impact on computational runtimes. In the context of embedded ANNs, we shall consider two particular indicators of complexity.
%From the point of view of optimization, several aspects complicate optimizing over neural networks. These include large number of variables and weak relaxations, among others. In this section, we introduce properties of neural networks which are indicators of the complexity, which we use later in our numerical study.

\subsubsection{Number of linear regions of ReLU networks}

ReLU ANN describe piecewise affine-linear functions \citep{Grigsby2022}. Therefore, the network partitions the input domain $\mathcal{X} \subseteq \mathbb{R}^{n_x}$ into regions in which $h(x)$ is affine linear. These regions are typically called \textit{linear regions}. The bounds on the number of linear regions of a neural network with given depth and width was investigated in \citet{Montufar2014} and later improved on in \citet{Raghu2017}. In general, the number of linear regions of a neural network corresponds to the number of feasible activation patterns in \eqref{eq:bigM}, i.e., the binary decisions whether a neuron is on or off for all neurons in the neural network. Thus, it is an important statistic when considering the complexity of optimizing over neural networks, e.g., in branch-and-bound frameworks, where the variables to branch on represent active or inactive neurons. 

\subsubsection{Number of stable ReLU neurons}

The number of variables a branch-and-bound method has to branch on is an important statistic for estimating the complexity of the optimization problem. In ReLU networks, the variables to branch on are the binary variables $z$ in \eqref{eq:bigM} of every neuron in the network. However, if a neuron can be identified as stable, no binary variable has to be added to model the neuron. To identify stable neurons, their pre-activation bounds are used. The neuron $i$ in layer $j$ is called stably active if $L^{(j)}_i > 0$ and stably inactive if $U^{(j)}_i < 0$, for $j \in [J]$ and $i \in [n_j]$ for all inputs in the input domain $\mathcal{X} \subseteq \mathbb{R}^{n_x}$. 

A regularization to induce ReLU stability was proposed in \citet{Xiao2019} to speed up verification of ReLU networks, whereas in \citet{Serra2020} stable neurons are used to compress neural networks. 
To enumerate the linear regions of a ReLU ANN, we exploit the fact that, within a given linear region, the input and output of each neuron is an affine linear functional in the ANN's overall input space. We use forward sensitivity propagation to calculate the gradient of each neuron's regional input functional and simultaneously perform a forward evaluation of the linearized ANN at the input space's coordinate origin to determine each affine input functional's output shift. With both gradient and shift, we can determine a hyperplane in input space along which the neuron's ReLU activation would switch. We then construct a linear equation system that describes the intersection of halfspaces within which all neurons would retain their current activation pattern. We add the bounds of the input domain to this equation system to ensure boundedness of the linear region. We then use a variant of the \texttt{QuickHull} algorithm~\citep{Barber1996} via the \texttt{SciPy} library~\citep{SciPy2020} to reduce this equation system into an irredundant one and to determine the vertices of the linear region. This also reveals information on which neurons define the facets of the linear region, which means that we can jump across these facets to adjacent regions by switching the activity of those neuron's activation functions. Assuming that there is no facet along which two neurons switch simultaneously, this allows us to enumerate all linear regions that intersect the input domain. We can detect the edge case of two neurons switching simultaneously because it would cause us to enter a region with an empty interior. We do not observe this behavior. 

\subsection{bound tightening} \label{subsec:boundtightening}

Calibrating the big-M coefficients in MILP formulations is crucial for performance of optimization algorithms. bound tightening plays an important role in this context. With ReLU ANNs, there are different ways to compute the big-M coefficients.
%
\subsubsection{Interval arithmetic}\label{subsec:ia_bounds}
%
In the presence of input bounds $L^{(0)} \leq x^{(0)} \leq U^{(0)}$ with $L^{(0)}, U^{(0)} \in \mathbb{R}^{n_x}$, big-M coefficients of formulation \eqref{eq:bigM} can be computed via interval arithmetic (IA).
%
\begin{align}
    L^{(k)}_i &= \sum_{j=1}^{n_{k-1}} \min \{ W_{i,j}^{(k)} L_j^{(k-1)}, W_{i,j}^{(k)} U_j^{(k-1)} \} + b^{(k)}_i, \quad k \in [J],  i \in [n_k] \\
    U^{(k)}_i &= \sum_{j=1}^{n_{k-1}} \max \{ W_{i,j}^{(k)} L_j^{(k-1)}, W_{i,j}^{(k)} U_j^{(k-1)} \} + b^{(k)}_i, \quad k \in [J],  i \in [n_k] 
\end{align}
%
This forward propagation yields valid bounds. However, it ignores the fact that the activation of neurons, i.e., whether they are on the left or right arm of the ReLU function, is not independent between neurons. This results in overly relaxed approximations of the actual bounds. As a result, there is typically an exponential increase of big-M coefficients with increasing depth. This behaviour is exemplified in \vref{fig:IA_bounds}.


\subsubsection{LP-based bound tightening}\label{subsec:lr_bounds}

The bounds from \cref{subsec:ia_bounds} can be tightened by taking advantage of dependencies between the neurons as well as potentially existing bounds on the output of the neural network $L^{(J)} \leq x^{(J)} \leq U^{(J)}$. This is achieved by solving two auxiliary optimization problems per neuron, minimizing and maximizing, respectively, the pre-activation value of each neuron. The optimization problem for computing tighter bounds for neuron $k$ in layer $j$, with $j \in [J],\,k \in [n_k]$ in its general form as an MILP reads

\begin{equation}\label{prob:obbt}
    \begin{aligned}
    \min_{x,z}\ & W^{(j)}_k x^{(j-1)} + b^{(j)}_k \\ 
    \textrm{s.t.}\ & \begin{alignedat}[t]{3}
            x^{(j)}_i & \geq 0, \quad && j \in [J],\, i \in [n_j] \\
            x^{(j)}_i & \geq W^{(j)}_i x^{(j-1)} + b^{(j)}_i, \quad && j \in [J],\, i \in [n_j] \\
            x^{(j)}_i & \leq W^{(j)}_i x^{(j-1)} + b^{(j)}_i - L^{(j)}_i (1-z^{(j)}_i), \quad && j \in [J],\, i \in [n_j] \\
            x^{(j)}_i & \leq U^{(j)}_i z^{(j)}_i, \quad && j \in [J],\, i \in [n_j] \\
            x^{(0)}_i  & \leq U^{(0)}_i,        \quad && i \in [n_x]\\
            x^{(0)}_i  & \geq L^{(0)}_i,        \quad && i \in [n_x]\\
            z^{(j)}_i & \in \{0,1\}. \quad && j \in [J],\, i \in [n_j]
        \end{alignedat}
    \end{aligned}
\end{equation}
Solving \eqref{prob:obbt} yields a valid lower bound $L^{(j)}_k$, while the corresponding upper bound $U^{(j)}_k$ is computed by maximizing instead of minimizing in \eqref{prob:obbt}. In order to reduce the computational effort, typically the LP relaxation of formulation \eqref{prob:obbt} is considered. Hence, the auxiliary problems are linear programs (LPs) and can be solved efficiently. Solving the MILP directly is considered in \citet{Badilla2023,Grimstad2019}. However, the reduction in computational effort in subsequent optimization is quickly outweighed by the effort spent on solving the bound tightening MILPs. Therefore, we only consider the LP-based bound tightening procedure in this paper. One degree of freedom when performing bound tightening is the ordering of variables for which bounds are tightened. As the direction of bound propagation is from the input to the output layer, this is also the natural order to perform the tightening. However, within each layer the order may be chosen arbitrarily. Different methods to choose this order are discussed in \citet{Rossig2021}. However, they do not find any advantage of more advanced methods over a simple, fixed ordering of variables. Therefore, in this contribution, we apply bound tightening in a fixed ordering of variables.% The effects of LP-based bound tightening on a ReLU network with ten hidden layers is illustrated in \vref{fig:big_Ms}.

\subsection{A posteriori scaling of ReLU ANNs} \label{sec:scaling}
Weights of neural networks are not uniquely determined by the training process and the training data, i.e., there are different realizations of weights and biases that define the same functional relationship of input and output. This observation can be exploited to design algorithms that transform a trained neural network into a functionally identical network with some desired property. This could be, e.g., a lower norm of the weight matrices. With the input bounds remaining unchanged, this would lead to a reduction of big-M coefficients, which could be beneficial in subsequent optimization problems.

In case of the ReLU activation function, one can exploit its positive homogeneity. For a single neuron $i$ in layer $k$, with $k \in [J], \ i \in [n_k]$ and a scalar $c^{(k)}_i > 0$ it holds, that
\begin{align}
    \myReLU{c^{(k)}_i \left( W^{(k)}_i x^{(k-1)} + b_i \right)} = c^{(k)}_i \cdot \myReLU{ W^{(k)}_i x^{(k-1)} + b_i},
\end{align}
with $W^{(k)}_i \in \mathbb{R}^{1 \times n_{k-1}}$ being the $i$-th row of the weight matrix in layer $k$. 
%
To ensure the functional equivalence of the neural network, the $i$-th column of the weight matrix of layer $k+1$, corresponding to the scaled neuron $i$ in layer $k$, needs to be multiplied with the reciprocal of $c^{(k)}_i$. As all neurons of the neural network may be scaled, all weight matrices except the first and the last are scaled with the ratio of the two scaling factors of their surrounding layers. As the bias is not multiplied with the output from the previous layer, no multiplication with the reciprocal is needed. In the final layer $J$, no more new scaling factors may be introduced as they can no longer be compensated in subsequent layers. Therefore, only the scaling of layer $J-1$ is compensated by multiplying $W^{(J)}$ with the reciprocals of the scaling factors of the penultimate layer. For any set of scaling factors $c^{(k)}_i > 0,\,k \in [J], i \in [n_k]$, scaled weights and biases  $\tilde{W}$ and $\tilde b$, computed as 
\begin{equation}
    \begin{alignedat}{3}
        \tilde{W}_{i,j}^{(1)} &= W_{i,j}^{(1)} \cdot c_i^{(1)}, \quad && i \in [n_1],\, j \in [n_x] \\       
        \tilde{W}_{i,j}^{(k)} &= W_{i,j}^{(k)} \cdot \frac{c_i^{(k)}}{c_j^{(k-1)}}, \quad && k \in \{2, \ldots, J-1\},\, i \in [n_k],\, j \in [n_{k-1}],\\
        \tilde{W}_{i,j}^{(J)} &= W_{i,j}^{(J)} \cdot \frac{1}{c_j^{(J-1)}}, \quad && i \in [n_J],\, j \in [n_{J-1}] \\
        \tilde{b}_i^{(k)} &= b_i^{(k)} \cdot c_i^{(k)}, \quad && k \in [J-1], \, i \in [n_k]
    \end{alignedat}
\end{equation}
define functionally equivalent neural networks.
This basic idea of an equivalent transformation of ReLU networks via scaling one layer and compensating the effect of scaling in the next layer is illustrated in \vref{fig:rescale}. 

\begin{figure}
    \centering
    \includegraphics[width=.6\linewidth]{figures/hahnRescale.png}
    \caption{Equivalent scaling of ReLU ANNs. Scalar factor $c$ is multiplied row-wise to weight matrix and corresponding bias of current layer, resulting in a scaling of the output of the neuron by a factor of $c$. To compensate this, the weight matrix in the subsequent layer needs to be multiplied column-wise with the reciprocal of $c$.}
    \label{fig:rescale}
\end{figure}
%
The scaling factors $c_i^{(k)}$ can be chosen arbitrarily. However, we can specifically choose them such that the resulting network has favorable properties. We propose formulating an optimization problem to obtain scaling factors that minimize the absolute value of the scaled weights $\tilde{W}$ and biases $\tilde{b}$. This lower norm of the weights then yields lower big-M coefficients. As noted in \cref{subsec:ia_bounds}, these are determined solely\footnote{While bounds on the output of the network can be propagated backwards through the network and thus influence the big-M coefficients \citep{Grimstad2019}, we refer only to the big-M coefficients derived via interval arithmetic and forward propagation of input bounds as explained in \cref{subsec:ia_bounds}.} by the input bounds and the magnitude of weights and biases. Hence, this approach can be applied to networks that were not initially trained with regularization in order to generate an equivalent neural network with lower big-M coefficients. Of course, other effects of regularization, e.g., weight sparsity, cannot be obtained by this method. The proposed optimization problem is
%
\begin{equation}
    \begin{aligned}
        \min_{c}\ & \sum_{i=1}^{n_1} \sum_{j=1}^{n_x} \lvert W_{i,j}^{(1)} \rvert \cdot c_i^{(1)} +  \sum_{k=2}^{J-1} \sum_{i=1}^{n_k} \sum_{j=1}^{n_{k-1}}  \lvert W_{i,j}^{(k)} \rvert \cdot \frac{c_i^{(k)}}{c_j^{(k-1)}} \\*
        & + \sum_{k=1}^{J-1} \sum_{i=1}^{n_k} \lvert b_i^{(k)} \rvert \cdot c_i^{(k)} + \sum_{i=1}^{n_J} \sum_{j=1}^{n_{J-1}} \lvert W_{i,j}^{(J)} \rvert \cdot \frac{1}{c_i^{(J)}} \\
        \textrm{s.t.}\ & \begin{alignedat}[t]{3}
            c_i^{(k)} & > 0, \quad && k \in [J],\, i \in [n_k] \\
            c & \in \bigtimes_{k=1}^J \mathbb{R}^{n_k} \quad
        \end{alignedat}
    \end{aligned}
\end{equation}
%
This optimization problem is not trivial to solve directly because it involves fractions and strict inequality constraints. However, because all $c_i^{(k)}$ have to be strictly positive, we can convert it into a convex optimization problem on a closed set by replacing each $c_i^{(k)}$ with its logarithm. Each summand in the objective function then becomes an evaluation of the exponential function, multiplication becomes addition, and division becomes subtraction. With the logarithm of $c^{(k)}_i$  referred to as $\tilde{c}^{(k)}_i$, the transformed optimization problem reads
%
\begin{equation}\label{prob:scaling}
    \begin{aligned}
        \min_{\tilde{c}}\ & \sum_{i=1}^{n_1} \sum_{j=1}^{n_x} \exp \left( \log \left( \lvert W_{i,j}^{(1)} \rvert\right) + \tilde{c}_i^{(1)} \right)   + \sum_{k=2}^{J} \sum_{i=1}^{n_k} \sum_{j=1}^{n_{k-1}} \exp \left( \log \left( \lvert W_{i,j}^{(k)} \rvert \right)+ \tilde{c}_i^{(k)} - \tilde{c}_j^{(k-1)} \right)\\*
        & + \displaystyle \sum_{k=1}^{J} \sum_{i=1}^{n_k} \exp \left( \log \left( \lvert b_i^{(k)} \rvert \right) +  \tilde{c}_i^{(k)} \right) + \sum_{i=1}^{n_J} \sum_{j=1}^{n_{J-1}}  \exp \left( \log\left(\lvert W_{i,j}^{(J)} \rvert \right) - \tilde{c}_i^{(J)} \right) \\
        \textrm{s.t.}\ & \tilde{c} \in \bigtimes_{k=1}^J \mathbb{R}^{n_k}
    \end{aligned}
\end{equation}
%


\subsection{Regularization} \label{sec:regularization}

The objective function for training neural networks typically consists of two terms. The first 
accounts for the mismatch between prediction and data, while the second term aims at preventing overfitting and thus allowing for a better generalization of the model to unseen data. With $W \in \mathbb{R}^d$ denoting the vector of all weights and biases and $N \in  \mathbb{N}$ representing the number of training samples of inputs and outputs $(x_i, y_i), \, i \in [N]$, the objective reads
%
\begin{equation}\label{prob:training}
    \begin{aligned}
        \min_{W}\ & \frac{1}{N} \sum_{i=1}^{N} \left( h(x_i) -y_i\right)^2 + \lambda \Omega(W)
    \end{aligned}
\end{equation}
%
Popular choices for the regularization term $\Omega : \mathbb{R}^d \mapsto \mathbb{R}$ are the penalization of large magnitudes of weights and biases by using some vector norm, e.g., $\Omega(W) = \| W \|_p$, with typically $p=1$ and $p=2$. Typical ways to measure the generalization performance of a model is to compute the mean absolute percentage error (MAPE) defined as 
\begin{align}
    \text{MAPE}\left(\hat{y}, y\right) = \frac{1}{n} \sum_{i=1}^{n} \frac{|\hat{y}_i - y_i|}{\max\{\varepsilon, |y_i|\}}
\end{align}
for predictions $\hat{y}$ on the test dataset.

While it is known that $\ell^1$ regularization leads to sparser regression models \citep{Tibshirani1996}, \citet{Xiao2019,Serra2020} found that applying $\ell^1$ regularization also increased ReLU stability, i.e., the percentage of stable neurons. The authors of \citet{Xiao2019} also propose a dedicated ReLU stability regularization \eqref{eq:stability_regularization}, which penalizes the sign differences in the pre-activation bounds of each neuron, thus encouraging stability.
\begin{align}\label{eq:stability_regularization}
    \Omega_{\text{RS}}(W) = - \sum_{i=1}^{J} \sum_{j=1}^{n_i} \text{sign}(U_j^{(i)}) \cdot \text{sign}(L_j^{(i)})
\end{align}
For practical purposes, a smooth reformulation of \eqref{eq:stability_regularization} is used, and \citet{Xiao2019} show that verification problems of neural networks trained using this regularization can be solved faster than with $\ell^1$ regularization due to a higher number of stable neurons. In this paper, we will however focus on investigating the effect of varying levels of $\ell^1$ regularization on the performance of optimization algorithms, as it is one of the most commonly used types of regularization.

\subsection{Clipped ReLU} \label{sec:clipped}
%
One of the reasons why big-M coefficients in ReLU networks increase quickly with increasing network depth is that the ReLU activation function is unbounded. A variation of the ReLU function is the clipped ReLU function proposed in \citet{Hannun2014}. In the clipped ReLU function, the output of the function is bounded by an upper value $M \in \mathbb{R}$, i.e.,
\begin{align}\label{eq:relu_m}
    \myClippedReLU[M]{x} = \max\bigl\{0,\, \min\{M,\, x\}\bigr\}
\end{align}
%
Using standard disjunctive programming notation, the feasible set of $x_i^{(j)} = \myClippedReLU[M]{ W^{(j)}_i x^{(j-1)} + b_i}$ can be written as
%
\[
\begin{bmatrix}
    x_i^{(j)} = 0 \\
    W^{(j)}_i x^{(j-1)} + b_i \leq 0
\end{bmatrix}
\vee 
\begin{bmatrix}
    x_i^{(j)} = W^{(j)}_i x^{(j-1)} + b_i\\
    0 < W^{(j)}_i x^{(j-1)} + b_i < M 
\end{bmatrix}
\vee
\begin{bmatrix}
    x_i^{(j)} = M \\
    W^{(j)}_i x^{(j-1)} + b_i \geq M 
\end{bmatrix}
\]
%
We formulate a big-M relaxation of this feasible set as
\begin{align}
    \begin{split}
        x_i^{(j)} & \geq 0, \\
        x_i^{(j)} & \leq M z_{1_i}^{(j)}, \\
        x_i^{(j)} & \leq U_i^{(j)} z_{1_i}^{(j)}, \\
        x_i^{(j)} & \leq W^{(j)}_i x^{(j-1)} + b_i - L_i^{(j)} \cdot (1-z_{1_i}^{(j)}),\\
        x_i^{(j)} & \geq M z_{2_i}^{(j)}, \\
        x_i^{(j)} & \geq W^{(j)}_i x^{(j-1)} + b_i - (U_i^{(j)}-M) z_{2_i}^{(j)}, \\
        z_{1_i}^{(j)}, z_{2_i}^{(j)} & \in \{0,1\},\\
        z_{1_i}^{(j)} & \geq z_{2_i}^{(j)},
    \end{split}
    \label{eq:bigM_clipped}
\end{align}
similar to the formulation suggested in a preprint version of \citet{Anderson2020}.
This formulation comes at the cost of an additional binary variable compared to the standard big-M formulation \eqref{eq:bigM}. If both binary variables are zero, the neuron is inactive and $x_i^{(j)}=0$. In the case $z_{1_i}^{(j)}=1, z_{2_i}^{(j)}=0$, the neuron is active and $0 \leq x_i^{(j)} =  W^{(j)}_i x^{(j-1)} + b_i \leq M$. If both binary variables are non-zero, the neuron's output is limited by the threshold $M$. 

\subsection{Dropout} \label{sec:dropout}

Dropout is a technique applied during training proposed in \citet{Srivastava2014} to prevent overfitting the data by randomly turning off a percentage of the neurons in some or all layers. Therefore, redundancies have to be established in the neural network to achieve an adequate accuracy. There is empirical evidence that neural networks trained with dropout have more linear regions \citep{Zhang2020a} than those trained without. Hence, in contrast to the aforementioned methods, it is expected that applying dropout during training leads to more complex neural networks which makes optimizing over them more difficult.
We will thus apply dropout as an antithesis to validate our conjecture that the runtime of MINLP solvers increases for more redundant and decreases for less redundant ANN models.
