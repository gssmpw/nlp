\section{Numerical results}\label{sec:results}

In the \cref{sec:methods}, we have enumerated some methods to formulate, train, and scale feed-forward neural networks with ReLU activation (or variations thereof), as well as to tighten their relaxation prior to optimization through bound tightening. In this section, we evaluate how these methods affect global optimization performance. In order to do so, we train neural networks as surrogates for several non-convex benchmark functions and compare solver performance with various post-processing steps.

We first present numerical results on relevant characteristics of ReLU ANNs in the context of optimization. These include their expressive power as measured by the number of linear regions they define and the percentage of stable neurons that can be determined from the pre-activation bounds, introduced in the beginning of \cref{sec:methods}.  We count only those linear regions that intersect the relevant input domain of each function.

We show how the methods presented in \cref{sec:methods} impact these quantities and improve the performance of optimization algorithms. For this, we restrict ourselves to minimizing the output of feed-forward ReLU ANNs, i.e., the optimization problem we solve reads
\begin{equation}\label{prob:minANN}
    \min_{x} \ h(x)
\end{equation}
where $h \colon \mathbb{R}^{{n_x}} \mapsto \mathbb{R}$ is the trained neural network. 
The benchmark functions we consider for approximation and subsequent minimization are:

\begin{figure}[t]
    \centering
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/surf_peak.png}
        \subcaption{Peaks function \eqref{fun:peaks}}\label{fig:peaks-function}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/surf_ack.png}
        \subcaption{Ackley's function \eqref{fun:ack}}\label{fig:ack-function}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/surf_him.png}
        \subcaption{Himmelblau's function \eqref{fun:him}}\label{fig:him-function}
    \end{subfigure}
    \caption{Surface plots of the benchmark functions for surrogate model training and optimization.}
    \label{fig:different_functions}
\end{figure}


\begin{enumerate}
    \item The Peaks function $f_{\text{peaks}} \colon \mathbb{R}^2 \mapsto \mathbb{R}$ is given by
    \begin{align}
        \begin{split}
        f_{\text{peaks}}(x,y) &= - 3 (1-x)^2 \exp\bigl(-x^2 - (y+1)^2\bigr)   - 10 \Bigl( \frac{x}{5} - x^3 - y^5\Bigr) \exp(-x^2 - y^2) \\
            & \quad - \frac{1}{3}  \exp\bigl(-(x+1)^2 - y^2\bigr).
        \end{split}
        \label{fun:peaks}
    \end{align}
    It is commonly used as a benchmark function, e.g., in \citet{Schweidtmann2019a} and has multiple local minima and maxima on the domain $x,y \in [-2,2]$. The global minimum is $(0.228, -1.626)$ with objective value $-6.551$. The function is depicted in \vref{fig:peaks-function}.

    \item Ackley's function $f_{\text{ackley}} \colon \mathbb{R}^2 \mapsto \mathbb{R}$ is defined by
    \begin{align}
        \begin{split}
            f_{\text{ackley}}(x,y) & = -20 \cdot \exp \left(-\frac{1}{5}\sqrt{\frac{1}{2}(x^2+y^2)}\right) \\
                            & \quad  - \exp \left(\frac{1}{2} \bigl(\cos(2\pi x) + \cos(2\pi y)\bigr) \right) + \exp(1) + 20    
        \end{split}
        \label{fun:ack}
    \end{align}
    and is often used as a benchmark function for optimization algorithms. For instance, it is used in \citet{Tsay2021}. It is considered on the domain $x,y \in [-3.5,3.5]$. It is non-convex, has several local minima and one global minimum at $x=y=0$ with objective value $0$. A surface plot is depicted in \vref{fig:ack-function}.

    \item Himmelblau's function $f_{\text{himmelblau}} \colon \mathbb{R}^2 \mapsto \mathbb{R}$ with
    \begin{align}\label{fun:him}
        f_{\text{himmelblau}}(x,y) =  (x^2 + y - 11)^2 + (x + y^2 - 7)^2
    \end{align}
    is considered on the domain $x,y \in [-5,5]$, where it has four equivalent local (and global) minima: $(3.0,2.0)$, $(-2.805, 3.131)$, $(-3.779, -3.283)$ and $(3.584,-1.848)$. All have objective function value $0$. A surface plot is depicted in \vref{fig:him-function}.
\end{enumerate}

In our numerical study, we consider a total of 1080 different neural networks. This number of instances stems from considering the three benchmark functions used for the approximation of \cref{fun:peaks,fun:ack,fun:him} and the specific choices for the hyperparameters of the trained neural networks. These differ both in their width and depth, as well as the activation function and the level of $\ell^1$ regularization applied during training. The specific options for these hyperparameters are given in \Cref{table:hyperparameter}. Using Latin Hypercube sampling, we generated training data of 100,000 samples for the Peaks function \eqref{fun:peaks} and Himmelblau's function \eqref{fun:him}, and 150,000 samples for Ackley's function \eqref{fun:ack}, to account for its higher nonconvexity. For training, we first normalize both input and output data, and reserve 30\% of the data as a test set to evaluate the generalization of the networks. All networks are then trained for 300 epochs using the Adam algorithm \citep{Kingma2017}. To study the effect of scaling and bound tightening on each of the trained networks, we solve problems \eqref{prob:scaling} and \eqref{prob:obbt}, where applicable. As the scaling method is not designed for the clipped ReLU, we can only solve \eqref{prob:scaling} for the 360 instances with standard ReLU activation. We use \texttt{OMLT} \citep{Ceccon2022} to set up the constraints for the ReLU ANNs via \texttt{Pyomo}~\citep{Bynum2021,Hart2011}, and \texttt{Gurobi}~\citep{gurobi}~{v11.0.1} with default options and a time limit of 300 seconds to solve the resulting optimization problems. 
%
\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/big_M_unscaled}
        \caption{Big-M coefficients $U^{(k)}$ determined via IA for standard ReLU ANN.}
        \label{fig:IA_bounds}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/big_M_unscaled_OBBT}
        \caption{Big-M coefficients $U^{(k)}$ determined via OBBT for standard ReLU ANN.}
        \label{fig:LR_bounds}
    \end{subfigure}
    
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/big_M_scaled}
        \caption{Big-M coefficients $U^{(k)}$ determined via IA for ReLU ANN after ReLU scaling.}
        \label{fig:scaled_IA_bounds}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/big_M_scaled_OBBT.pdf}
        \caption{Big-M coefficients $U^{(k)}$ determined via OBBT for ReLU ANN after ReLU scaling.}
        \label{fig:scaled_LR_bounds}
    \end{subfigure}
            
    \caption{Comparison of pre-activation bounds  $U^{(k)}$ for functionally equivalent neural networks with ten hidden layers. The original bounds derived via interval arithmetic shown in \ref{fig:IA_bounds} are characterized by the typical exponential increase due to forward propagation of the input bounds. Solving auxiliary LPs yields tighter bounds, although the exponential increase is still present, as shown in \ref{fig:LR_bounds}. Comparable bounds can be computed via solving the scaling problem \eqref{prob:scaling}, with the distinction that the bounds on the output of the network are equivalent to those derived from interval arithmetic. For the scaled neural network, solving the bound tightening problem \eqref{prob:obbt} in addition yields even tighter bounds on the big-M coefficients in the hidden layers with ReLU activation, as can be seen in \ref{fig:scaled_LR_bounds}, while the output bounds are equivalent to those in \ref{fig:LR_bounds}.}
    \label{fig:big_Ms}
\end{figure}


\begin{table}[ht]
    \centering
    \caption{Hyperparameter options for training of neural networks. Besides varying the depth and width of the networks, we investigate two variants of the clipped ReLU activation \eqref{eq:relu_m} and five levels of $\ell^1$ regularization. All hidden layers have the same dimension.}
    \label{table:hyperparameter}
    \begin{tabular}{l c } 
        \toprule 
        Hyperparameter & Options \\ \midrule
        Hidden Layers & $1,\ldots,10$  \\
        Layer Width &  $25,50$ \\
        Activation & ReLU, ReLU$_2$, ReLU$_5$ \\ 
        $\lambda$ &  $0.0, 10^{-7}, 10^{-6},10^{-5}, 10^{-4}, 10^{-3}$\\ \bottomrule 
    \end{tabular}
\end{table}

\subsection{Effect of OBBT}

For the 1080 trained neural networks, we solve the LP-relaxation of \eqref{prob:obbt} to compute tighter big-M coefficients for formulation \eqref{eq:bigM}, and use them in the optimization problem \eqref{prob:minANN}. The effect on a network with ten hidden layers is illustrated in \vref{fig:big_Ms}. Compared to the IA bounds, there is a reduction in big-M coefficients of the last layer by roughly two orders of magnitude. As \vref{tab:results} shows, OBBT is effective for all trained networks. We assess the reduction in big-M coefficients across all networks by comparing the averaged distances between upper and lower bound $U^{(j)}_k -L^{(j)}_k$ for bounds based on OBBT and IA. Then, over all networks, we calculate the geometric mean over the ratio of these averages. The resulting geometric mean of 0.54 suggests, that, as a rough estimate, OBBT is reducing the big-M coefficients by half. 
As a side effect of these tighter bounds, the percentage of stable neurons increases by 5.5 percent on average. We assess the resulting improvement in computational times by calculating the ratios of the measured  computational times with tightened bounds and those with the original bounds, restricted to instances that were solved to global optimality in both cases. Over these ratios, we again form the geometric mean. With a geometric mean of 0.57, bound tightening brings a significant computational speedup, though it does not substantially increase the number of instances that are solved within the time limit. \vref{fig:improvement_obbt} illustrates the parities of percentage of stable neurons and computational time.


\subsection{Effect of ReLU scaling}

\Vref{fig:big_Ms} illustrates the effects of solving the scaling problem \eqref{prob:scaling} on the big-M coefficients of a neural network with ten hidden layers. The first observation is that the output bounds remain unchanged compared to the original neural network, which is expected as the functional relationship is equivalent. However, the lower $\ell^1$ norm of the weights leads to a reduction in the big-M coefficients for the hidden layers. They are roughly on the same order of magnitude as those obtained via LP-based bound tightening. When both scaling and bound tightening are applied sequentially, the bounds for the hidden layers are tighter than those achieved by OBBT on its own. Also, with the sequential application of scaling and tightening we do not observe any clear sign of an exponential increase in bounds with increasing depth.

Using the big-M formulation with standard bounds obtained via IA as a baseline, we compare the following options:
\begin{enumerate}
    \item ReLU scaling only: We solve Problem~\eqref{prob:scaling} to obtain equivalent weights and biases with lower $\ell^1$ norm;
    \item ReLU scaling and subsequent LP-based bound tightening: a combination of the two methods.
\end{enumerate}
As shown in \vref{tab:results}, ReLU scaling on its own, as well as combined with OBBT, is able to reduce the big-M coefficients more than applying OBBT on an unscaled network. This is clearly illustrated by the geometric means over the ratios of averaged distances of upper and lower bounds $L$ and $U$ of 0.388 and 0.16 for ReLU scaling and ReLU scaling combined with OBBT compared to unscaled networks, respectively. 
Again, we compute the improvement in computational times as a geometric mean over the ratios of computational times with improved bounds and those with interval arithmetic bounds. We observe that scaling the neural network weights by solving \eqref{prob:scaling} yields only a marginal improvement with a geometric mean of {0.936}. However, combining this scaling with subsequent bound tightening yields a more substantial computational speedup as indicated by a geometric mean of {0.467}. This seems to stem from the tighter big-M coefficients, but also from an increased percentage of stable neurons. Compared to the default bounds, there is an average increase by {7.2} percent. In \vref{fig:improvement_scaling_obbt}, the parities of  computational times for the two comparisons are shown. We note that the parity plot in \vref{subfig:IA_vs_scaler_and_OBBT} suggests that the average speedup may be driven by a few outlier instances in which in the combined method performs exceptionally well.

Overall, with the scaled ReLU networks and their default bounds from interval arithmetic, 307 instances can be solved within the time limit. With tightened bounds, there is a slight reduction to 299 instances.


\begin{figure}
    \centering
    \begin{subfigure}{.47\linewidth}
        \centering
        Percentage of stable neurons
        \includegraphics[width=\linewidth]{figures/parities/obbt/parity_percentage_fixed}
        \subcaption{Parity plot for percentage of stable neurons compared for bounds from IA and LP-based OBBT.}
        \label{subfig:percentage_fixed_IA_vs_OBBT}
    \end{subfigure}
    \begin{subfigure}{.47\linewidth}
        \centering
        Computational time
        \includegraphics[width=\linewidth]{figures/parities/obbt/parity_time}
        \subcaption{Parity plot for computational time compared for bounds from IA and LP-based OBBT.}
        \label{subfig:time_IA_vs_OBBT}
    \end{subfigure}

    \caption{Parity plots comparing percentage of stable neurons and computational times of optimally solved instances of \eqref{prob:minANN} for bounds derived from IA and LP-based OBBT. Solving \eqref{prob:obbt} leads to an increase of 5.5 percentage points in stable neurons on average. This carries over to a reduction in computational time shown in \subref{subfig:time_IA_vs_OBBT}. The ratios of times with OBBT and IA bounds have a geometric mean of 0.57.}
    \label{fig:improvement_obbt}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}{.47\linewidth}
        \centering
        Computational time
        \includegraphics[width=\linewidth]{figures/parities/scaled/parity_time}
        \subcaption{Runtime comparison between IA bounds for the baseline network (\enquote{Default}) and IA for the scaled ANN (\enquote{ReLU scaling}).}
        \label{subfig:IA_vs_scaler}
    \end{subfigure}
    \begin{subfigure}{.47\linewidth}
        \centering
        Computational time
        \includegraphics[width=\linewidth]{figures/parities/scaled_obbt/parity_time}
        \subcaption{Runtime comparison between IA bounds for the baseline network (\enquote{Default}) and OBBT for the scaled ANN (\enquote{ReLU scaling + OBBT}).}
        \label{subfig:IA_vs_scaler_and_OBBT}
    \end{subfigure}

    \caption{Parity plots comparing computational times for optimally solved instances of \eqref{prob:minANN} in different versions: \subref{subfig:IA_vs_scaler}: IA bounds for baseline vs. scaled ReLU network with a geometric mean ratio of {0.936}; \subref{subfig:IA_vs_scaler_and_OBBT}: IA bounds for baseline network vs. OBBT bounds for scaled network with a geometric mean ratio of {0.467}.}
    \label{fig:improvement_scaling_obbt}
\end{figure}

\subsection{Effect of regularization}

\begin{table}
    \addtolength{\tabcolsep}{-0.2em}
    \centering
    \caption{Influence of training options, bound tightening and ReLU scaling on all trained neural networks and their optimization problems \eqref{prob:minANN}. In each row, the effect of the listed method is evaluated by comparing it to similar networks that differ only in this particular method, e.g., for $\ell^1$ regularization we compare neural networks that were trained with the specified level of regularization to those that were trained without regularization. The first and second column show the number of solved instances without and with the applied technique and the number of instances in total in this comparison. The third column lists the reduction of big-M coefficients as measured by the geometric mean of the ratio of averaged distances of pre-activation bounds $U- L$ of the adapted network and that of the baseline network. The fourth column shows the arithmetic mean of the increase in percentage points of stable neurons due to the applied method. The fifth column shows the geometric mean of the ratio between the number of linear regions of the adapted network and that of the baseline network. The last column shows the geometric mean of the ratio between the computational time with the adapted network and that observed with the baseline network, but is limited to instances in which the optimization problems for both networks are solved within the time limit. We observe a computational speedup with regularization, bound tightening and ReLU-scaling, while dropout leads to a deterioration in performance.}
    \label{tab:results}
    \begin{tabular}{lc|cccccc}
    \multicolumn{1}{c}{}    &  & \begin{tabular}[c]{@{}c@{}}Solved instances\\ (adapted vs. baseline)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Instances\\  total\end{tabular} & 
    \begin{tabular}[c]{@{}c@{}}Geom. mean \\ $\overline{U-L}$ \end{tabular} &
    \begin{tabular}[c]{@{}c@{}}Improvement \\ stable neurons\end{tabular} &    \begin{tabular}[c]{@{}c@{}}Geom. mean \\ lin. regions\end{tabular} & \begin{tabular}[c]{@{}c@{}}Geom. mean \\ time\end{tabular} \\ \hline
    \multirow{5}{*}{$\lambda$} & 1e-3   & 349 vs. 151 & 360 & 0.009  & 0.379    & 0.283  & 0.028 \\
                             & 1e-4   & 358 vs. 151 & 360  & 0.024  & 0.216  & 0.463 & 0.059  \\
                             & 1e-5   & 352 vs. 151 & 360  & 0.052  & 0.122  & 0.817 & 0.109  \\
                             & 1e-6   & 326 vs. 151  & 360 & 0.133  & 0.146  & 1.089 & 0.208  \\
                             & 1e-7   & 287 vs. 151  & 360 & 0.261  & 0.213  & 0.996 & 0.280  \\ \hline
    \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Clipped\\ ReLU\end{tabular}} 
                            & M=2    &609 vs. 608 & 720   & 0.415   & 0.029 & 1.094 & 0.931  \\
                            & M=5    & 606 vs. 608 & 720  & 0.560   & 0.011 & 1.055 & 0.974  \\ \hline
    \multirow{2}{*}{Dropout}    & 10\%   & 146 vs. 217 & 240 & 12.761  & -0.204 & 4.098 & 5.825  \\
                                & 20\%   & 152 vs. 217& 240  & 15.403  & -0.210 & 3.513 & 4.845  \\ \hline \hline
    \multicolumn{2}{l|}{OBBT}   & 912 vs. 911  & 1080  & 0.541 & 0.055  & 1.0   & 0.570  \\ \hline
    \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}ReLU\\ scaling\end{tabular}} &        & 307 vs. 303  & 360   & 0.388 & 0.0    & 1.0   & 0.936  \\
                & OBBT & 299 vs. 303  & 360  & 0.160  & 0.072  & 1.0   & 0.467                                                     
    \end{tabular}
\end{table}

In \vref{fig:model_statistics}, we depict how the mean absolute error on the test set, the number of linear regions, the percentage of neurons of fixed activation, and the solver runtime correlate with the depth of the neural networks for networks with 50 neurons per layer with different regularization parameters. 
In the first row, we see the performance of the neural networks on the test data as measured by the MAPE. We see, that large regularization parameters lead to a degradation of accuracy on the test dataset, especially for Ackley's function. For small regularization parameters there is a high level of agreement between the predictions and the ground truth on the test data. Further, in some instances, training with moderate levels of $\ell^1$ regularization does in fact lead to be better generalization of the neural network. 
%
The second row of \cref{fig:model_statistics} shows the number of linear regions as an indicator of the complexity, or expressive power of the neural network. With increasing levels of regularization, we obtain neural networks with a lower number of linear regions. This is also illustrated in \vref{fig:linear_regions}. Comparing the number of linear regions among the three different functions, the neural networks which approximate Ackley's function have the most linear regions. This is plausible comparing the surface plots in \vref{fig:different_functions}, because Ackley's function exhibits a large number of local oscillations.
%
In the third row of \cref{fig:model_statistics}, we plot the percentage of stable neurons. These are neurons whose input bounds are either non-negative or non-positive, which means that they are in a fixed state of activation regardless of input. No binary variables have to be added to model the activation function of such neurons. Confirming the findings of \citet{Xiao2019,Serra2020}, higher values of $\lambda$ lead to a higher percentage of stable neurons. 
%
The last row shows the computational times in the optimization problem \eqref{prob:minANN}. Comparing the runtimes among the three functions, Ackley's function appears to be the hardest to minimize. Here, we cannot solve unregularized networks with as little as three hidden layers to global optimality within the specified time limit. Based on the observation that ANNs approximating this function have an increased number of linear regions and that several local minima exist in the input domain, this is expected behavior. Increasing the regularization generally lowers the time to compute global minima for all three functions. While the global minima of unregularized networks cannot be determined for any network with more than four layers, applying moderate levels of regularization makes almost all instances tractable. The only exception here is Ackley's function, which remains unsolved for the lowest regularization parameter $\lambda = 10^{-7}$ as well. While \cref{fig:model_statistics} shows the results for all networks with 50 neurons per hidden layer, we obtain similar results for those with 25 neurons (data shown in the appendix).
In combination with the results in \vref{tab:results}, this illustrates that regularization proved the most effective method by improving big-M coefficients, increasing the number of stable neurons and decreasing the number of linear regions, thus enabling the computational speedup.
%
\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        Peaks
        \includegraphics[width=\linewidth]{figures/statistics/regularization/peaks_mape_50_neurons}
        \subcaption{MAPE on test set of ReLU ANNs approximating \eqref{fun:peaks}.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        Ackley
        \includegraphics[width=\linewidth]{figures/statistics/regularization/ackley_mape_50_neurons}
        \subcaption{MAPE on test set of ReLU ANNs approximating \eqref{fun:ack}.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        Himmelblau
        \includegraphics[width=\linewidth]{figures/statistics/regularization/himmelblau_mape_50_neurons}
        \subcaption{MAPE on test set of ReLU ANNs approximating \eqref{fun:him}.}
    \end{subfigure}
    
    %\begin{subfigure}{.32\linewidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/statistics/regularization/peaks_r2_50_neurons}
    %    \subcaption{$R^2$ on test set of ReLU ANNs approximating \eqref{fun:peaks}.}
    %\end{subfigure}
    %\begin{subfigure}{.32\linewidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/statistics/regularization/ackley_r2_50_neurons}
    %    \subcaption{$R^2$ on test set of ReLU ANNs approximating \eqref{fun:ack}.}
    %\end{subfigure}
    %\begin{subfigure}{.32\linewidth}
    %    \centering
    %    \includegraphics[width=\linewidth]{figures/statistics/regularization/himmelblau_r2_50_neurons}
    %    \subcaption{$R^2$ on test set of ReLU ANNs approximating \eqref{fun:him}.}
    %\end{subfigure}

    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/statistics/regularization/peaks_num_regions_50_neurons}
        \subcaption{Number of linear regions of ReLU ANNs approximating \eqref{fun:peaks}.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/statistics/regularization/ackley_num_regions_50_neurons}
        \subcaption{Number of linear regions of ReLU ANNs approximating \eqref{fun:ack}.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/statistics/regularization/himmelblau_num_regions_50_neurons}
        \subcaption{Number of linear regions of ReLU ANNs approximating \eqref{fun:him}.}
    \end{subfigure}

    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/statistics/regularization/peaks_percentage_fixed_50_neurons}
        \subcaption{Percentage of stable neurons for big-M coefficients based on interval arithmetic bounds.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/statistics/regularization/ackley_percentage_fixed_50_neurons}
        \subcaption{Percentage of stable neurons for big-M coefficients based on interval arithmetic bounds.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/statistics/regularization/himmelblau_percentage_fixed_50_neurons}
        \subcaption{Percentage of stable neurons for big-M coefficients based on interval arithmetic bounds.}
    \end{subfigure}


    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/statistics/regularization/peaks_time_50_neurons}
        \subcaption{Solution time of problem \eqref{prob:minANN} for Peaks function.}
        \typeout{PLOT LINE WIDTH: \the\linewidth}%
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/statistics/regularization/ackley_time_50_neurons}
        \subcaption{Solution time of problem \eqref{prob:minANN} for Ackley's function.}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \includegraphics[width=\linewidth]{figures/statistics/regularization/himmelblau_time_50_neurons}
        \subcaption{Solution time of problem \eqref{prob:minANN} for Himmelblau function.}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
            \centering
            \input{figures/statistics/regularization/legend}
    \end{subfigure}
    \caption{Mean absolute percentage error on test set, number of linear regions, percentage of stable neurons and computational times in problem \eqref{prob:minANN} of trained ANNs with varying number of hidden layers with 50 neurons, trained with different levels of $\ell^1$ regularization.}
    \label{fig:model_statistics}%
    \typeout{LINE WIDTH: \the\linewidth}%
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth,trim={0 0 0 8mm},clip]{figures/mesh/relu_peaks_5_layer_25_neurons_0e+00_regu_0_dropout_mesh}
        \subcaption{Standard, 11,898 regions.}
        \label{subfig:mesh_no_regu}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth,trim={0 0 0 8mm},clip]{figures/mesh/relu_peaks_5_layer_25_neurons_1e-05_regu_0_dropout_mesh}
        \subcaption{Regularization, 6,893 regions.}
        \label{subfig:mesh_regu}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth,trim={0 0 0 8mm},clip]{figures/mesh/relu_peaks_5_layer_25_neurons_0e+00_regu_20_dropout_mesh.png}
        \subcaption{Dropout, 14,674 regions.}
        \label{subfig:mesh_dropout}
    \end{subfigure}
    \caption{Linear regions for ReLU networks approximating the Peaks function \eqref{fun:peaks} with five hidden layers of 25 neurons each. Color-coded in the backgrounds are the outputs of the neural networks. Compared are networks with different training options: \cref{subfig:mesh_no_regu} with no regularization or dropout, \cref{subfig:mesh_regu} with $\ell^1$ regularization and $\lambda=10^{-5}$, \cref{subfig:mesh_dropout} with $20$\% dropout. Regularizing the weights of the ANN during training decreases the number of linear regions, applying dropout increases it and also changes their sizes.}
    \label{fig:linear_regions}
\end{figure}


\subsection{Effect of clipped ReLU}

The effect of the clipping is obvious in the big-M coefficients of formulation \eqref{eq:bigM_clipped}, which are illustrated in \vref{fig:big_Ms_clipped} for a threshold of $M=5.0$. Compared to the big-M coefficients derived for the regular ReLU activation function and depicted in \vref{fig:IA_bounds}, the clipped ReLU formulation yields lower bounds, though this may depend on the particular choice of $M$. This is also obvious from the results in \vref{tab:results}, with clipped ReLU leading to greater reductions in big-M coefficients compared to OBBT. We also observe that LP-based bound tightening for neural networks with clipped ReLU activation does not improve the bounds to the same degree as it did for the regular ReLU activation function as depicted in \vref{fig:LR_bounds}.

As the results in \Cref{tab:results} suggest, using the clipped ReLU  \eqref{eq:bigM_clipped} yields only marginal computational speedup compared to the standard ReLU activation. There seems to be a tradeoff between a higher number of binary variables needed for modeling \eqref{eq:bigM_clipped} and the slightly higher number of linear regions on the one hand, and the reduction of big-M coefficients on the other hand. 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/clipped_5_big_M_unscaled}
        \caption{Pre-activation bounds $U^{(k)}$ determined via interval arithmetic for clipped ReLU ANN with the big-M formulation \eqref{eq:bigM_clipped} and $M=5.0$.}
        \label{fig:IA_bounds_clipped}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/clipped_5_big_M_unscaled_OBBT}
        \caption{Pre-activation bounds $U^{(k)}$ determined via LP-based bound tightening for clipped ReLU ANN with big-M formulation \eqref{eq:bigM_clipped} and $M=5.0$.}
        \label{fig:LR_bounds_clipped}
    \end{subfigure}
   
    \caption{Comparison of pre-activation bounds  $U^{(k)}$ for neural networks with ten hidden layers and clipped ReLU formulation \eqref{eq:bigM_clipped} with $M=5.0$ as activation function. Compared to the bounds derived via interval arithmetic for the regular ReLU activation shown in \vref{fig:IA_bounds}, the bounds for the clipped ReLU are generally lower. Moreover, due to the threshold $M$, the bounds stay approximately constant over the layers. Solving auxiliary LPs only noticeably tightens bounds in the first few layers.}
    \label{fig:big_Ms_clipped}
\end{figure}
 
\subsection{Effect of Dropout}

For the Peaks function only, we trained additional networks with different levels of dropout applied to the hidden layers, namely 10 and 20 percent. In combination with the other hyperparameters (regularization, depth and width of the network) this yields a total of 240 trained neural networks with dropout whose properties we can compare. As illustrated in \Cref{tab:results}, we find that dropout leads to neural networks with three to four times more linear regions on average, confirming the findings of \citet{Zhang2020a}. This is also evident in \Cref{fig:linear_regions}, where the linear regions of three exemplary ANNs are compared for networks with five hidden layers. Another effect of dropout is the percentage of stable neurons, which is reduced by approximately 20~\% on average compared to networks trained without dropout and a drastic increase in the magnitude of the big-M coefficients. In combination, this leads to a reduction in instances that could be solved to global optimality and a simultaneous four to six-fold increase in computational time for those instances that could be solved. 
