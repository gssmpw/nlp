\section{Conclusions and outlook}\label{sec:discussion}

In this paper, we compared different variations of training and scaling methods for ReLU networks with respect to their effect on the performance of global optimization solvers on problems with these networks  embedded. We divided these methods into those that are applied during training and those that can be used on trained networks. For the latter category, we proposed a scaling method specific to the ReLU activation function, which equivalently transforms a ReLU ANN such that the $\ell^1$ norm of the networks weights and biases is minimized. This has the desired effect of reducing the constant coefficients in big-M formulations of the network's activation functions. In numerical experiments, we demonstrated that this method can be used to reduce the computational effort of solving subsequent optimization problems, when it is used in combination with bound tightening. Although in our study we only investigated the direct minimization of feed-forward neural networks with their big-M formulation of ReLU networks, we believe that the findings are also applicable in other contexts. These might include optimization problems with ReLU networks using different MILP encodings, e.g., the partition-based formulation from \citet{Tsay2021}, or other optimization settings, e.g., more difficult optimization problems from real-world applications. In fact, by employing regularization during training we were able to solve a complex superstructure optimization problem in chemical engineering that had been computationally intractable before \citep{Klimek2024}.

Moreover, to the best of our knowledge, this is the first computational study that links various training methods to both the number of linear regions and the percentage of fixed neurons as well as the computational effort in subsequent optimization problems. Doing so, we were able to provide empirical evidence for several observations from the literature, e.g., an increased number of linear regions for networks trained with dropout, and computational speedup due to higher rates of fixed neurons for networks trained with $\ell^1$ regularization. 

Further research may include a more thorough analysis into how the used training methods and hyperparameter options used when training a neural network impact its number of linear regions and the number of fixed neurons. Also, different objectives in \eqref{prob:scaling} may be conceivable to promote other properties in the transformed networks. It may also be promising to investigate transformations that allow minor perturbations of the functional relationship.