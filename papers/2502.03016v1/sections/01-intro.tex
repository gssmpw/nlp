\section{Introduction}\label{sec:introduction}

Artificial neural networks (ANNs) are a popular tool for approximating functions from data and have been used in various applications, ranging from modeling and control of batch reactors \citep{Mujtaba2006}, the optimization of cancer treatments \citep{Bertsimas2016} and chemical reactions \citep{Fernandes2006} to the approximation of solutions to complex optimization problems \citep{Bertsimas2022}. 
%
Formally, a feed-forward ANN consists of $J \in \mathbb N$ consecutive layers. Each layer uses an affine-linear transformation of the input with a subsequent, element-wise application of a nonlinear activation function $s: \mathbb R \mapsto \mathbb R$, i.e.,
\begin{align}\label{eq:ANN}
	x^{(j)} = s^{(j)}\left( W^{(j)} x^{(j-1)} + b^{(j)}  \right), \quad j \in [J],
\end{align}
with $x^{(0)} = x \in \mathbb{R}^{n_x}$ as the input to the neural network and $W^{(j)} \in \mathbb{R}^{n_j\times n_{j-1}}, b^{(j)} \in \mathbb{R}^{n_j}$ denoting the weights and biases of layer $j$, respectively.
%
In this paper, we assume $s$ to be the ReLU activation
\begin{align}\label{eq:relu}
    \myReLU{x} = \max\{0,x\},
\end{align}
%
on all hidden layers and the identity on the last layer. 
A recent survey shows that more than 400 different activation functions have been suggested in the literature \citep{kunc2024three}. While in some contexts the usage of other, possibly continuously differentiable, activation functions may be recommendable, ReLU activation variants continue to play a major role. Especially in the context of mixed-integer optimization problems where some of the variables require a non-smooth, non-differentiable treatment, a study of ReLU activation is thus of major interest.
%
Equipped with the ReLU activation function, an ANN describes a piecewise affine-linear function $h \colon \mathbb{R}^{n_x} \mapsto \mathbb{R}^{n_h}$ with $h(x)=x^{(J)}$~\citep{Grigsby2022}. Under mild assumptions on their size and the chosen activation function, neural networks are universal approximators, i.e., they can approximate continuous functions on compact sets to arbitrary precision \citep{Cybenko1989,Hornik1991}. Therefore, they are often used for approximating functional relationships in cases where the underlying function is unknown, hard to model otherwise, or in general expensive to simulate, but where data is available to train the network on. See, e.g., \citet{Misener2023} for a survey on surrogate modeling in process applications. Further, for classification tasks, such as image classification, ReLU ANNs and convolutional neural networks in particular play an important role \citep{Krizhevsky2012}.
%
Due to their widespread use, embedding trained universal approximators in optimization problems and the efficient solution of these problems is of special interest and currently an active field of research \citep{Schweidtmann2019a,Tong2024}. The applications in which these optimization problems appear are manifold. 
In general, the combination of first-principle models with data-driven surrogates has many advantages \citep{camps2023discovering}. We thus expect the optimization of mathematical models involving neural networks to play an important role in future engineering research, compare also \citet{Schweidtmann2021} for the case of chemical engineering.
We are interested in the setting where an ANN is embedded in a mixed-integer nonlinear optimization problem (MINLP), i.e.,
\begin{equation}\label{prob:embedded}
    \begin{alignedat}{3}
        \underset{x \in \mathbb{R}^{n_x}, w \in \mathcal{W}}\min&\ && f(y,w) \\
        \underset{\phantom{x \in \mathbb{R}^{n_x}, w \in \mathcal{W}}}{\textrm{s.t.}}&\ && \begin{aligned}[t]
            y & = h(x),\\
            0 & \leq g(y,w),
        \end{aligned}
    \end{alignedat}
\end{equation}
%^{n_z}
where $x \in \mathbb{R}^{n_x}$ and $y \in \mathbb{R}^{n_h}$ are input and output variables to the neural network, respectively, and $\mathcal{W}$ is a feasible set of $n_w$ additional mixed-integer variables $w$. Note that $w$ does not influence the neural network, but the optimization problem. The objective function $f \colon \mathbb{R}^{n_h} \times \mathcal{W} \mapsto \mathbb{R}$ and the constraint function $g \colon \mathbb{R}^{n_h} \times \mathcal{W} \mapsto \mathbb{R}^{n_c}$ typically depend on the output of the neural network and on the variables $w$. This general formulation is easily extendable towards the embedding of multiple neural networks, but also includes the simple case of minimizing the output of a single ANN in the absence of variables $w$ or additional constraints $g(\cdot)$. 
The task of verifying the reliability of neural networks can also be reduced to solving optimization problems of this type. Such verification problems arise because ANNs can be prone to adversarial attacks, i.e., situations in which minor perturbations in the input cause the network to produce incorrect outputs. Examples of such behavior can be found by solving the optimization problem 
\begin{equation}\label{prob:adversarial}
    \begin{aligned}
        \max_{\varepsilon \in \mathbb{R}^{n_x}}\ & h(x+\varepsilon)_k - h(x+\varepsilon)_i \\
        \textrm{s.t.}\ & \lvert \varepsilon \rvert \leq \delta,
    \end{aligned}
\end{equation}
where $\varepsilon$ is the perturbation which, when added to a current input $x$, changes the prediction from the correct label $i$ to the incorrect label $k$. The bound on the perturbation $\delta \in \mathbb{R}$ and its norm are hyperparameters of this problem and must be chosen on a problem-by-problem basis. See, e.g., \citet{Hein2017} for examples using the $\ell_2$ norm, or \citet{Tjeng2019} using the  $\ell_\infty$ norm. When ANNs are used in safety-critical applications, solving \eqref{prob:adversarial} is important to certify the robustness of the ANN. For instance, it can be used to verify that no adversarial example exists around specific inputs, e.g., those in the training set. Robustness against adversarial attacks can be proven if the optimal objective value of \eqref{prob:adversarial} is negative. See also \citet{Rossig2021} and the references therein for more information on verification problems. 

Embedding the ANNs into optimization problems entails introducing the necessary variables and modeling the equations \eqref{eq:ANN} for each neuron in the neural network. 
%
The first detailed study of the impact on the optimization problem and a comparison of different modeling approaches and algorithms was given in \citet{Joseph-Duran2014}. The application setting was different, because the functions $\max(0, x)$ modeled the overflow of sewage water. Yet, as an identical function to \eqref{eq:relu} was used, the mathematical formulation is identical. A major insight was that tailored constraint branching algorithms outperform standard mixed-integer modeling and continuously differentiable reformulations of \eqref{eq:relu}.
%
Nevertheless, for ReLU activations a big-M formulation is most widely used in the literature \citep{Fischetti2018,Xiao2019,Tjeng2019}. Big-M formulations are a standard technique to model constraints that can be activated or deactivated in mixed-integer linear programming (MILP). For a single neuron $i$ in layer $j$ $x^{(j)}_i = \text{ReLU}(W^{(j)}_i x^{j-1} + b_i)$, and assuming a bounded input $L^{(j)}_i \leq W^{(j)}_i x^{(j-1)} + b_i\leq U^{(j)}_i$, the big-M formulation reads
\begin{align}
    \begin{split}
        x^{(j)}_i & \geq  0, \\
        x^{(j)}_i & \geq W^{(j)}_i x^{(j-1)} + b_i,\\
        x^{(j)}_i & \leq  W^{(j)}_i x^{(j-1)} + b_i - L^{(j)}_i (1-z_i^{(j)}), \\
        x^{(j)}_i & \leq U^{(j)}_i z_i^{(j)}, \\
        z^{(j)}_i & \in \{0,1\},
    \end{split}
    \label{eq:bigM}
\end{align}
where $W^{(j)}_i$ denotes the $i$-th row of the weight matrix in layer $j$. Although this formulation is easily derived and implemented, the choice of the big-M coefficients $L^{(j)}_i$ and $U^{(j)}_i$ is crucial for practical performance. Larger values lead to weaker relaxations and can slow the convergence of MINLP solvers.
There is ongoing research to derive formulations with tighter relaxations or problem-specific cuts. An extended formulation proposed in \citet{Anderson2020}, which can be proven to yield the tightest possible relaxation for each neuron. This comes at the price of introducing additional continuous variables. However, the authors' own numerical studies find that the extended formulation does not offer significant performance improvements in optimization despite its theoretical advantages over the big-M formulation. A class of intermediate formulations between the big-M formulation and the extended formulation were proposed in \citet{Tsay2021,Kronqvist2024}. These formulations allow for a trade-off between dimension and relaxation tightness. In the extreme cases, they correspond exactly to the two formulations. The authors demonstrate with numerical experiments that their proposed partition-based formulation performs better in some application settings. For more information on ReLU ANNs and their MILP encodings we refer to the extensive survey \citet{Huchette2023a} and the references therein.

Several methods have been proposed to reduce the computational burden of solving optimization problems with embedded neural networks. These fall broadly into two categories. 

First, the ANN training can be adapted to yield ANNs with properties that ease the subsequent optimization. In \citet{Xiao2019}, the authors discuss regularization methods that can be applied during the training of the neural network which significantly speed up the solution of subsequent verification problems. Besides standard $\ell^1$ regularization, which is known to encourage sparsity in the coefficients of regression models \citep{Tibshirani1996}, they propose a ReLU stability regularization, which aims at increasing the number of neurons that can be determined active or inactive a priori. Thus, the number of binary variables necessary to model the ANN is reduced, which leads to smaller optimization problems, and in turn to a significant speedup in the verification problems.
%
The main disadvantage of methods from this first category is that in some applications, it may not always be feasible to train a dedicated neural network surrogate specifically for optimization. In this case, optimization algorithms have to work with networks that are trained, for instance, with simulation in mind, and it is not possible to specify desirable network dimensions and training methods.

The second category therefore includes methods that a) modify existing ANNs after the training phase to improve their properties and b) obtain tighter bounds in existing formulations for ReLU ANNs. 
Among others, this category include different compression methods (e.g., weight pruning) and optimization-based bound tightening (OBBT). Pruning is usually done via the removal of connections of neurons that have small weights \citep{Cacciola2024} and results in smaller networks with approximately the same functional relationship. Several papers found that models may be compressed without significant loss in accuracy \citep{Han2015,Suzuki2020a}. Exact compression methods, i.e., methods that keep the functional relationship described by the ANN intact, are described in \citet{Kumar2019,Serra2020} for neural networks with ReLU activation. By investigating the bounds of each neuron, smaller networks can be obtained if it can be determined a priori that the input to a neuron is non-negative or non-positive for inputs in the relevant input domain. In this case, no variables have to be added to model the maximum operator in the activation function. If such determinations can be made for all neurons in a layer, then the whole layer can be removed and merged with the subsequent layer via matrix multiplication and addition of the biases. This results in optimization problems with fewer optimization variables and hence in a computational speedup. More recently, theoretical parallels with tropical geometry have been used to simplify neural networks with ReLU activation \citep{Smyrnis2020}. OBBT procedures form a second pillar of this category. In \citet{Grimstad2019}, various bound tightening methods for ReLU ANNs and their effect on optimization times are investigated. \citet{Badilla2023} considers LP-based and MILP-based bound tightening for ReLU ANNs. They analyse the trade-offs of computing tighter bounds via a more expensive bound tightening method and the benefits of the tighter bounds in the subsequent optimization problem.

In addition to the aforementioned methods to speed up the solution process of general MILP solvers, the development of solution heuristics is an active field of research. \citet{Tong2024} propose a heuristic which performs a local search by traversing neighboring linear regions and solving LP subproblems in each linear region. The authors show that for the case of minimizing the output of an ANN and finding adversary examples, this heuristic outperforms general MILP solvers, e.g., Gurobi, especially for deeper networks.

Several software packages have been published that facilitate the embedding of neural networks and other machine learning models into larger optimization problems. \texttt{OMLT}~\citep{Ceccon2022} is a Python package which supports neural networks and gradient boosted trees, and sets up the variables and constraints in the optimization environment \texttt{Pyomo} \citep{Bynum2021,Hart2011}. \texttt{Pyomo} offers interfaces to different solvers, e.g., Gurobi \citep{gurobi}, with which the problems can be solved. Alternatively, \texttt{gurobi-machinelearning} or \texttt{PySCIPOpt-ML} \citep{Turner2024} can be used to translate trained regression models, including neural networks to MIP formulations and solve them with \texttt{Gurobi} and \texttt{SCIP}, respectively. All of these options support models trained by different machine learning backends, including \texttt{Keras} \citep{Chollet2015} or \texttt{PyTorch} \citep{PyTorch}. A further alternative is the software package \texttt{reluMIP} \citep{reluMIP2021}, which has interfaces to both \texttt{Pyomo} and \texttt{Gurobi}, but only supports models trained using \texttt{Keras} or \texttt{TensorFlow}.

\medskip
\noindent \textbf{Contributions and Outline.}
We provide a survey of popular and novel approaches to improving the computational efficiency of optimization with embedded feed-forward ANNs with ReLU activation functions.
In addition and for the first time to our knowledge, we quantify the impact of these methods systematically.
We evaluate and compare all of them on the same benchmark problems, using \texttt{Gurobi}, an analysis of big-M coefficients, and a novel method to calculate the number of piecewise-linear regions.
Although the effects of regularization and dropout on the training of ANN and redundancy of the obtained mathematical model have received much attention in the literature, we study this effect systematically in the context of embedded optimization.

The rest of the paper is structured as follows. In \cref{sec:methods}, we state several methods that have been proposed in the literature to facilitate optimization with embedded neural networks. In addition, we introduce an equivalent transformation of ReLU neural networks that reduces the magnitude of big-M coefficients in their MILP formulation. In \cref{sec:results} we evaluate the influence of these methods on the performance of optimization algorithms in numerical studies. We conclude with a discussion of the findings and possible future lines of research in \cref{sec:discussion}.