
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{mathabx}
\usepackage{algorithm, algorithmic}
\newcommand{\ourmethod}{MR Sampler}

\title{MRS: A Fast Sampler for Mean Reverting Diffusion}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
In the application scenario of diffusion models, controlled generation is more difficult than unconditional generation, but more valuable. The current controllable generation methods primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions more simple and natural. However, current training-free fast samplers cannot be directly applied to MR Diffusion, which requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named \ourmethod~aiming to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive a semi-analytical solution. This solution consists of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in a few steps. Our approach does not require training and supports the three kinds of training objectives. Extensive experiments demonstrate that \ourmethod~ maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm reduces the time consumption of MR Diffusion to generate samples, making it more practical in controllable generation scenarios.
% Diffusion models have emerged as powerful generative models, particularly in the domains of image synthesis and video generation. And controllable generation has become a crucial focus in this field. Mean Reverting (MR) Diffusion offers a novel perspective on guided sampling; however, it cannot use existing fast samplers. This paper presents a new algorithm, termed \textit{\ourmethod}, aimed at enhancing the sampling efficiency of MR Diffusion. By solving the reverse-time stochastic differential equation (SDE) and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, we derive a semi-analytical solution. This solution consists of a closed-form data function and a neural network-parameterized integral. Notably, our approach does not require training and accommodates three common neural network parameterizations. Extensive experiments demonstrate that \ourmethod \space achieves a speedup of 10 to 20 times and maintains high sampling quality within 5 to 10 number of function evaluations across various image restoration tasks. This advancement positions \ourmethod \space as a practical and efficient solution for real-world applications of Mean Reverting Diffusion, effectively balancing high-quality outputs with rapid generation times.
\end{abstract}
% MRD-solver， MARS， 
%Mean Reverting (MR) Diffusion is such a subtype of diffusion methods which targets on images controlled generation, (also refered as guded sampling)
% we proposed MeAn Reverting diffusion Solver (MARS), aimed at --> leading to the acceleration of ...
% however, the fast sampling method for MR diffusion hasn't been thoroughly investigated.
% we achieve this by adsolving ... and ... associated with MRD and derived ...
% integrates --> consists of
% three common --> different 
% maintains --> can maintain
% various--> 10?
% This advancement positions --> This proves that MR solver can function as a practical ...
% high sampling quality and high sampling speed (low sampling time) quality and speed of sampling


\section{Introduction}

Diffusion models have emerged as a powerful class of generative models, demonstrating remarkable capabilities across a variety of applications, including image synthesis \citep{dhariwal2021diffusion,ruiz2023dreambooth,rombach2022high} and video generation \citep{ho2022video,ho2022imagen}. %These models operate by simulating a diffusion process that gradually transforms data into noise, followed by learning to reverse this process for sample generation \citep{ho2020ddpm,song2020improved,song2021maximum}. 
Popular diffusion models, such as Denoising Diffusion Probabilistic Models (DDPMs) \citep{ho2020ddpm} and Noise Conditional Score Networks (NCSNs) \citep{song2019ncsn}, have achievd better performance compared to generative adversarial networks (GANs) \citep{goodfellow2014gan} and variational autoencoders (VAEs) \citep{kingma2013vae} in practical scenarios. Nonetheless, controllable generation remains a challenging task, and numerous methods have been proposed to incorporate text or image conditions into the score function \citep{ho2022cfg,ye2023ipadapter,zhang2023controlnet}. Meanwhile, the Mean Reverting Stochastic Differential Equation (MRSDE) framework offers a new avenue of control in the generation process \citep{luo2023mrsde}. Instead of converging to pure Gaussian noise, MRSDE is designed to produce final states that follow a Gaussian distribution with a non-zero mean, which provides a simple and natural way to introduce image conditions. This characteristic makes MRSDE particularly suitable for solving inverse problems, such as image restoration tasks \citep{luo2023refusion,luo2024daclip}, where image conditions can be directly modeled as the mean value of the final states without relying on prior knowledge of degradation. However, the sampling process of MR Diffusion requires hundreds of iterative steps, which is time-consuming.

% In MRSDE, the final states of the diffusion process follows a Gaussian...
% where the image condition can be directly modeled as the mean value of the final state in a more natural and eligant way. 


In order to improve the sampling efficiency of diffusion models, various acceleration strategies have been proposed, which can be divided into two categories. The first category explores methods that establish direct mappings between starting and ending points on the sampling trajectory, enabling acceleration through knowledge distillation \citep{salimans2022progressive,song2023consistency,liu2022flow}. Nevertheless, such algorithms often come with trade-offs, such as the need for extensive training and limitations in their adaptability across different tasks and datasets. The second category involves the design of fast numerical solvers that increase step sizes while controlling truncation errors, thus allowing for faster convergence to solutions \citep{lu2022dpmsolver,zhang2022deis,song2020ddim}. 

% the requirement for  delete
% protracted generation times --> redundant generation time
% divided into two categories
% establish direct mappings between data and predefined noise distributions -->?
% step size -->?
% of data variables delete
% SDEs -->SDE
% neural network parameterizations explain --> trainnig objectives
% maintains sampling performance, with speedup factors ranging from 10 to 2 --> achieves a speedup factor ranging from 10 to 20 while maintaining the sampling performance.'
% add 'and rapid speed at the samme time'



Notably, fast sampling solvers mentioned above are designed for common SDEs like VPSDE and VESDE \citep{song2020sde}. Due to the difference between these SDEs and MRSDE, existing training-free fast samplers cannot be directly applied to Mean Reverting (MR) Diffusion. In this paper, we propose a novel algorithm that improves the sampling efficiency of MR Diffusion named \ourmethod. Specifically, we solve the reverse-time stochastic differential equation (SDE) and probability flow ordinary differential equation (PF-ODE) \citep{song2020sde} derived from MRSDE, and obtain a semi-analytical solution, which consists of an analytical function and an integral parameterized by neural networks. We find that the modification of MRSDE only leads to change in analytical part of solution, which can be calculated precisely. And the integral part can be estimated by discretization methods in several previous works \citep{lu2022dpmsolver,zhang2022deis,zhao2024unipc}. We derive sampling formulas for two types of neural network parameterizations: noise prediction \citep{ho2020ddpm,song2020sde} and data prediction \citep{salimans2022progressive}. Through theoretical analysis and experimental validation, we demonstrate that data prediction exhibits superior numerical stability compared to noise prediction. Additionally, we propose transformation methods for velocity prediction networks \citep{salimans2022progressive} so that our algorithm supports all common training objectives. Extensive experiments show that our fast sampler converges in 5 or 10 NFEs with high sampling quality. As illustrated in Figure 1, our algorithm achieves stable performance with speedup factors ranging from 10 to 20.

% Due to the difference ... --> MRSDE is different from the SDE corresponding to DPMs. Thus the existing fast samplers cannot be directly applied to MRSDE. 
% Consequently, challenges still remain in increasing the sampling speed and meanwhile maintaining the sampling quality  for MRSDE currently.
% enhance --> improve
%  in modeling degradation processes for various restoration tasks. ?
% specificially
% derived from
% partial -->part of


In summary, our main contributions are as follows:
\begin{itemize}
    \item We propose \textit{\ourmethod}, a fast sampling algorithm for MR Diffusion, based on solving the PF-ODE and SDE derived from MRSDE. Our algorithm is plug-and-play and adapts to all common training objectives.
    \item We demonstrate that the previous sampling method proposed by \citep{luo2024posterior} for MR Diffusion is equivalent to Euler-Maruyama discretization, whereas \ourmethod~ computes a semi-analytical solution, thereby eliminating part of approximation errors.
    \item Through extensive experiments on ten image restoration tasks, we illustrate that \ourmethod~ can reduce the required sampling time by a factor of 10 to 20 with minimal degradation on sampling quality, and data prediction exhibits superior numerical stability compared to noise prediction.
\end{itemize}

\section{Background}

In this section, we briefly review the basic definitions and characteristics of diffusion probabilistic models and mean-reverting diffusion models.

\subsection{Diffusion Probabilistic Models}

According to \cite{song2020sde}, Diffusion Probabilistic Models (DPMs) can be defined as the solution of the following Itô stochastic differential equation (SDE), which is a stochastic process $\{\boldsymbol{x}_t\}_{t\in [0,T]}$ with $T>0$, called \textit{forward process}, where $\boldsymbol{x}_t\in \mathbb{R}^D$ is a D-dimensional random variable.
\begin{equation}
    \mathrm{d}\boldsymbol{x}=f(\boldsymbol{x},t)\mathrm{d}t + g(t) \mathrm{d}\boldsymbol{w}. \label{1}
\end{equation}
The forward process performs adding noise to the data $\boldsymbol{x}_0$, while there exists a corresponding reverse process that gradually removes the noise and recovers $\boldsymbol{x}_0$. \cite{anderson1982reverse} shows that the reverse of the forward process is also a solution of an Itô SDE:
\begin{equation}
    \mathrm{d}\boldsymbol{x}=[f(\boldsymbol{x},t)-g(t)^2\nabla_{\boldsymbol{x}}\log{p_t(\boldsymbol{x})}]\mathrm{d}t + g(t)\mathrm{d}\bar{\boldsymbol{w}}, \label{2}
\end{equation}
where $\bar{\boldsymbol{w}}$ is a standard Wiener process running backwards in time, and time $t$ flows from $T$ to $0$, which means $\mathrm{d}t<0$. The score function $\nabla_{\boldsymbol{x}}\log{p_t(\boldsymbol{x})}$ is generally intractable and thus a neural network $\boldsymbol{s}_\theta(\boldsymbol{x},t)$ is used to estimate it by optimizing the following objective \citep{song2020sde,hyvarinen2005scorematch}:
\begin{equation}
    \boldsymbol{\theta}^{*}=\arg\min_{\boldsymbol{\theta}}\mathbb{E}_{t}\Big\{\lambda(t)\mathbb{E}_{\boldsymbol{x}_0}\mathbb{E}_{\boldsymbol{x}_t|\boldsymbol{x}_0}\Big[\left\|\boldsymbol{s}_{\boldsymbol{\theta}}(\boldsymbol{x}_t,t)-\nabla_{\boldsymbol{x}_t}\log p(\boldsymbol{x}_t|\boldsymbol{x}_0)\right\|_{2}^{2}\Big]\Big\}.
    \label{3}
\end{equation}
where $\lambda(t):[0,T]\rightarrow\mathbb{R}^+$ is a positive weighting function, $t$ is uniformly sampled over $[0,T]$, $\boldsymbol{x}_0\sim p_0(\boldsymbol{x})$ and $\boldsymbol{x}_t\sim p(\boldsymbol{x}_t|\boldsymbol{x}_0)$. To facilitate the computation of $p(\boldsymbol{x}_t|\boldsymbol{x}_0)$, the drift coefficient $f(\boldsymbol{x},t)$ is typically defined as a linear function of $\boldsymbol{x}$, as presented in Eq.(\ref{4}). Based on the inference by \cite{sarkka2019applied} in Section 5.5, the transition probability $p(\boldsymbol{x}_t|\boldsymbol{x}_0)$ corresponding to Eq.(\ref{4}) follows Gaussian distribution, as shown in Eq.(\ref{5}).
\begin{equation}
    \mathrm{d}\boldsymbol{x}=f(t)\boldsymbol{x}\mathrm{d}t + g(t) \mathrm{d}\boldsymbol{w}, \label{4}
\end{equation}
\begin{equation}
    p(\boldsymbol{x}_t|\boldsymbol{x}_0)\sim\mathcal{N}\left(\boldsymbol{x}_t;\boldsymbol{x}_0e^{\int_0^tf(\tau)\mathrm{d}\tau},\int_0^te^{2\int_\tau^t f(\xi)\mathrm{d}\xi}g^2(\tau)\mathrm{d}\tau\cdot\boldsymbol{I} \right). \label{5}
\end{equation}
\cite{song2020sde} proved that Denoising Diffusion Probabilistic Models \citep{ho2020ddpm} and Noise Conditional
Score Networks \citep{song2019ncsn} can be regarded as discretizations of Variance Preserving SDE (VPSDE) and Variance Exploding SDE (VESDE), respectively. As shown in Table~\ref{table1}, the SDEs corresponding to the two most commonly used diffusion models both follow the form of Eq.(\ref{4}).

\renewcommand{\arraystretch}{1.5}
\begin{table}[t]
\caption{Two popular SDEs, Variance Preserving SDE (VPSDE) and Variance Exploding SDE (VESDE). $m(t)$ and $v(t)$ refer to mean and variance of the transition probability $p(\boldsymbol{x}_t|\boldsymbol{x}_0)$.}
\label{table1}
\begin{center}
\begin{tabular}{c|c|c|c|c}
\hline
SDE & $f(t)$ & $g(t)$ & $m(t)$ & $v(t)$
\\ \hline
VPSDE\citep{ho2020ddpm}     &$-\frac12\beta(t)$ &$\sqrt{\beta(t)}$   &$\boldsymbol{x}_0e^{-\frac12\int_0^t\beta(\tau)\mathrm{d}\tau}$   &$\boldsymbol{I}-\boldsymbol{I}e^{-\int_0^t\beta(\tau)\mathrm{d}\tau}$
\\ \hline
VESDE\citep{song2019ncsn}   &$0$                &$\sqrt{\frac{\mathrm{d}[\sigma^{2}(t)]}{\mathrm{d}t}}$     &$\boldsymbol{x}_0$     &$\left[\sigma^2(t)-\sigma^2(0)\right]\boldsymbol{I}$
\\ \hline
\end{tabular}
\end{center}
\end{table}

\subsection{Mean Reverting Diffusion Models}
\label{section2.2}

\cite{luo2023mrsde} proposed a special case of Itô SDE named Mean Reverting SDE (MRSDE), as follows:
\begin{equation}
    \mathrm{d}\boldsymbol{x}=f(t)\left(\boldsymbol{\mu}-\boldsymbol{x}\right)\mathrm{d}t+g(t)\mathrm{d}\boldsymbol{w},
    \label{6}
\end{equation}
where $\boldsymbol{\mu}$ is a parameter vector that has the same shape of variable $\boldsymbol{x}$, and $f(t), g(t)$ are time-dependent non-negative parameters that control the speed of the mean reversion and stochastic volatility, respectively. To prevent potential confusion, we have substituted the notation used in the original paper \citep{luo2023mrsde}. For further details, please refer to Appendix \ref{appb1}. Under the assumption that $g^2(t)/f(t)=2\sigma_\infty^2$ for any $t\in [0,T]$ with $T>0$, Eq.(\ref{6}) has a closed-form solution, given by
\begin{equation}
    \boldsymbol{x}_t=\boldsymbol{x}_0e^{-\int_0^t f(\tau)\mathrm{d}\tau}+\boldsymbol{\mu}(1-e^{-\int_0^t f(\tau)\mathrm{d}\tau})+\sigma_\infty\sqrt{1-e^{-2\int_0^t f(\tau)\mathrm{d}\tau}}\boldsymbol{z},
    \label{7}
\end{equation}
where $\sigma_\infty$ is a positive hyper-parameter that determines the standard deviation of $\boldsymbol{x}_t$ when $t\rightarrow\infty$ and $\boldsymbol{z}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I})$. Note that $\boldsymbol{x}_t$ starts from $\boldsymbol{x}_0$, and converges to $\boldsymbol{\mu}+\sigma_\infty\boldsymbol{z}$ as $t\rightarrow\infty$. According to \cite{anderson1982reverse}'s result, we can derive the following reverse-time SDE:
\begin{equation}
    \mathrm{d}\boldsymbol{x}=\left[f(t)\left(\boldsymbol{\mu}-\boldsymbol{x}\right)-g^2(t)\nabla_{\boldsymbol{x}}\log p_t(\boldsymbol{x})\right]\mathrm{d}t+g(t)\mathrm{d}\bar{\boldsymbol{w}}.
    \label{8}
\end{equation}
Similar to DPMs, the score function in Eq.(\ref{8}) can also be estimated by score matching methods \cite{song2019ncsn,song2021maximum}. Once the score function is known, we can generate $\boldsymbol{x}_0$ from a noisy state $\boldsymbol{x}_T$. In summary, MRSDE illustrates the conversion between two distinct types of data and has demonstrated promising results in image restoration tasks \citep{luo2023refusion}.

Numerous algorithms have been developed to accelerate sampling of VPSDE, including methods like DDIM \citep{song2020ddim}, PNDM \citep{liu2022pndm}, DPM-Solver \citep{lu2022dpmsolver} and UniPC \citep{zhao2024unipc}. Additionally, \cite{karras2022elucidating} and \cite{zhou2024amed} have introduced techniques for accelerating sampling of VESDE. However, the drift coefficient of VPSDE and VESDE is a linear function of $\boldsymbol{x}$, while the drift coefficient in MRSDE is an affine function w.r.t. $\boldsymbol{x}$, adding an intercept $\boldsymbol{\mu}$ (see Eq.(\ref{4}) and Eq.(\ref{6})). Therefore, current sampling acceleration algorithms cannot be applied to MR diffusion models. To the best of our knowledge, sampling acceleration algorithms for MRSDE have not been proposed so far.

\section{Fast Samplers for Mean Reverting Diffusion with Noise Prediction}

In this section, we solve the noise-prediction-based reverse-time ODE and SDE, and we numerically estimate the non-closed-form component of the solution, which serves to accelerate the sampling process of MR diffusion models. Next, we analyze the sampling method currently used by MR diffusion models and demonstrate that this method corresponds to a kind of discretization for the reverse-time MRSDE.

\subsection{Solutions to Mean Reverting SDEs with Noise Prediction}
\label{section3.1}

\cite{ho2020ddpm} reported that score matching can be simplified to predicting noise, and \cite{song2020sde} revealed the connection between score function and noise prediction models, which is
\begin{equation}
    \nabla_{\boldsymbol{x}_t}\log p(\boldsymbol{x}_t|\boldsymbol{x}_0)=-\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,\boldsymbol{\mu},t)}{\sigma_t},
    \label{9}
\end{equation}
where $\sigma_t=\sigma_\infty\sqrt{1-e^{-2\int_0^t f(\tau)\mathrm{d}\tau}}$ is the standard deviation of the transition distribution $p(\boldsymbol{x}_t|\boldsymbol{x}_0)$. Because $\boldsymbol\mu$ is independent of $t$ and $\boldsymbol{x}$, we substitute $\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,\boldsymbol{\mu},t)$ with $\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)$ for notation simplicity. According to Eq.(\ref{9}), we can rewrite Eq.(\ref{8}) as
\begin{equation}
    \mathrm{d}\boldsymbol{x}=\left[f(t)\left(\boldsymbol{\mu}-\boldsymbol{x}\right)+\frac{g^2(t)}{\sigma_t}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)\right]\mathrm{d}t+g(t)\mathrm{d}\bar{\boldsymbol{w}}.
    \label{10}
\end{equation}
Using Itô's formula (in the differential form), we can obtain the following semi-analytical solution:

\textbf{Proposition 1.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{10}) is 
\begin{equation}
    \boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s
    +\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}+\alpha_t\int_s^tg^2(\tau)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)}{\alpha_{\tau}\sigma_{\tau}}\mathrm{d}\tau
    +\sqrt{-\int_s^t\frac{\alpha_t^2}{\alpha_\tau^2}g^2(\tau)\mathrm{d}\tau}\boldsymbol{z},
    \label{11}
\end{equation}
where we denote $\alpha_t:=e^{-\int_0^tf(\tau)\mathrm{d}\tau}$ and $\boldsymbol{z}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})$. The proof is in Appendix \ref{appa1}.

However, the integral with respect to neural network output is still complicated. There have been several methods \citep{lu2022dpmsolver,zhang2022deis,zhao2024unipc} to estimate the integral numerically. We follow \cite{lu2022dpmsolver}'s method and introduce the half log-SNR $\lambda_t:=\log({\alpha_t/\sigma_t})$. Since both $f(t)$ and $g(t)$ are deliberately designed to ensure that $\alpha_t$ is monotonically decreasing over $t$ and $\sigma_t$ is monotonically increasing over $t$. Thus, $\lambda_t$ is a strictly decreasing function of $t$ and there exists an inverse function $t(\lambda)$. Then we can rewrite $g(\tau)$ in Eq.(\ref{11}) as
\begin{equation}
\begin{aligned}
g^2(\tau)&=2\sigma_\infty^2f(\tau)
=2f(\tau)(\sigma^2_{\tau}+\sigma_\infty^2\alpha_\tau^2)
=2\sigma^2_{\tau}(f(\tau)+\frac{f(\tau)\sigma_\infty^2\alpha_\tau^2}{\sigma^2_{\tau}})\\
&=2\sigma^2_{\tau}(f(\tau)+\frac{1}{2\sigma^2_{\tau}}\frac{\mathrm{d}\sigma^2_{\tau}}{\mathrm{d}\tau})
=-2\sigma^2_{\tau}\frac{\mathrm{d}\lambda_\tau}{\mathrm{d}\tau}.
\label{12}
\end{aligned}
\end{equation}
By substituting Eq.(\ref{12}) into Eq.(\ref{11}), we obtain
\begin{equation}
    \boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s
    +\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}-2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-\lambda}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda +\sigma_t\sqrt{(e^{2(\lambda_t-\lambda_s)}-1)}\boldsymbol{z},
\label{13}
\end{equation}
where $\boldsymbol{x}_{\lambda}:=\boldsymbol{x}_{t(\lambda_\tau)},\;\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\lambda,\lambda):=\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t(\lambda_\tau)},t(\lambda_\tau))$. According to the methods of exponential integrators \citep{hochbruck2010exponential,hochbruck2005explicit}, the $(k-1)$-th order Taylor expansion of $\boldsymbol{\epsilon}_\theta(x_\lambda,\lambda)$ and integration-by-parts of the integral part in Eq.(\ref{13}) yields 
\begin{equation}
-2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-\lambda}\boldsymbol{\epsilon}_\theta(x_\lambda,\lambda)\mathrm{d}\lambda=
-2\sigma_{t}\sum_{n=0}^{k-1}\left[\boldsymbol{\epsilon}_\theta^{(n)}(\boldsymbol{x}_{\lambda_{s}},\lambda_{s})\left(e^{h}-\sum_{m=0}^n\frac{(h)^m}{m!}\right)\right]
+\mathcal{O}(h^{k+1}).
\label{14}
\end{equation}
We drop the discretization error term $\mathcal{O}(h^{k+1})$ and estimate the derivatives with "backward difference method". We name this algorithm as \textit{\ourmethod-SDE-n-k}, where \textit{n} means noise prediction and \textit{k} is the order. Then, we take $k=2$ for demonstration.

\textbf{\ourmethod-SDE-n-2.} Given a final state $\boldsymbol{x}_T$, a Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_i}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see details in Algorithm \ref{alg:sde-n-2}):
\begin{equation}
\begin{aligned}
\boldsymbol{x}_{t_{i}}&=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
+\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}+\sigma_{t_i}\sqrt{e^{2h_i}-1}\boldsymbol{z}_i\\
&-2\sigma_{t_i}\left[(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})+(e^{h_i}-1-h_i)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}} \right].
\end{aligned}
\end{equation}

\subsection{Solutions to Mean Reverting ODEs with Noise Prediction}

\cite{song2020sde} have illustrated that for any Itô SDE, there exists a \textit{probability flow} ODE, sharing the same marginal distribution $p_t(\boldsymbol{x})$ as a reverse-time SDE. Therefore, the solutions of PF-ODEs are also helpful in acceleration of sampling. Specifically, the PF-ODE corresponding to Eq.(\ref{10}) is
\begin{equation}
\frac{\mathrm{d}\boldsymbol{x}}{\mathrm{d}t}=f(t)\left(\boldsymbol{\mu}-\boldsymbol{x}\right)+\frac{g^2(t)}{2\sigma_t}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t).
\label{16}
\end{equation}
The aforementioned equation exhibits a semi-linear structure with respect to $\boldsymbol{x}$, thus permitting resolution through the method of "variation of constants". We can draw the following conclusions:

\textbf{Proposition 2.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{16}) is 
\begin{equation}
\boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s+\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}+\alpha_t\int_s^t
\frac{g^2(\tau)}{2\alpha_\tau\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\mathrm{d}\tau \label{17},
\end{equation}
where $\alpha_t:=e^{-\int_0^tf(\tau)\mathrm{d}\tau}$. The proof is in Appendix \ref{appa1}.

Then we follow the variable substitution and Eq.(\ref{12}-\ref{14}) in Section \ref{section3.1}, and we obtain
\begin{equation}
\boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s+\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}-\sigma_{t}\sum_{n=0}^{k-1}\left[\boldsymbol{\epsilon}_\theta^{(n)}(\boldsymbol{x}_{\lambda_{s}},\lambda_{s})\left(e^{h}-\sum_{m=0}^n\frac{(h)^m}{m!}\right)\right]
+\mathcal{O}(h^{k+1}),
\label{18}
\end{equation}
where $\boldsymbol{\epsilon}_\theta^{(n)}(\boldsymbol{x}_\lambda,\lambda):=\frac{\mathrm{d}^{n}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{\lambda},\lambda)}{\mathrm{d}\lambda^n}$ is the $n$-th order total derivatives of $\boldsymbol{\epsilon}_\theta$ with respect to $\lambda$. By dropping the discretization error term $\mathcal{O}(h^{k+1})$ and estimating the derivatives of $\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{\lambda_{s}},\lambda_{s})$ with "backward difference method", we design the sampling algorithm from the perspective of ODE. And we present the situation for $k=2$.

\textbf{\ourmethod-ODE-n-2.} Given a final state $\boldsymbol{x}_T$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_{i}}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see complete algorithm in Algorithm \ref{alg:ode-n-2}):
\begin{equation}
\begin{aligned}
&\boldsymbol{x}_{t_{i}}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
+\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}\\
&-\sigma_{t_i}\left[(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})+(e^{h_i}-1-h_i)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}} \right].
\end{aligned}
\end{equation}

\subsection{Posterior Sampling for Mean Reverting Diffusion Models}
\label{section3.3}

In order to improve the sampling process of Mean Reverting Diffusion, \cite{luo2024posterior} proposed the \textit{posterior sampling} algorithm. They define a monotonically increasing time series $\{t_i\}_{i=0}^T$ and the reverse process as a Markov chain:
\begin{equation}
    p(\boldsymbol{x}_{1:T}\mid \boldsymbol{x}_0)=p(\boldsymbol{x}_T\mid \boldsymbol{x}_0)\prod_{i=2}^Tp(\boldsymbol{x}_{i-1}\mid \boldsymbol{x}_i,\boldsymbol{x}_0)\;
    \text{and }\boldsymbol{x}_T\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}),
\end{equation}
where we denote $\boldsymbol{x}_i:=\boldsymbol{x}_{t_i}$ for simplicity. They obtain an optimal posterior distribution by minimizing the negative log-likelihood, which is a Gaussian distribution given by
\begin{equation}
\begin{aligned}
    p(\boldsymbol{x}_{i-1}\mid \boldsymbol{x}_{i},\boldsymbol{x}_0)&=\mathcal{N}(\boldsymbol{x}_{i-1}\mid\tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_0), \tilde{\beta}_{i}\boldsymbol{I}),\\
    \tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_{0})&=\frac{(1-\alpha^2_{i-1})\alpha_{i}}{(1-\alpha^2_{i})\alpha_{i-1}}(\boldsymbol{x}_{i}-\boldsymbol\mu)+\frac{1-\frac{\alpha^2_{i}}{\alpha^2_{i-1}}}{1-\alpha^2_{i}}\alpha_{i-1}(\boldsymbol{x}_{0}-\boldsymbol\mu)+\boldsymbol\mu,\\
    \tilde{\beta}_{i}&=\frac{(1-\alpha^2_{i-1})(1-\frac{\alpha^2_{i}}{\alpha^2_{i-1}})}{1-\alpha^2_{i}},
    \label{21}
\end{aligned}
\end{equation}
where $\alpha_{i}=e^{-\int_{0}^{i}f(\tau)\mathrm{d}\tau}$ and $\boldsymbol{x}_0=\left(\boldsymbol{x}_{i}-\boldsymbol\mu-\sigma_i{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_i,\boldsymbol\mu,t_i)\right)/\alpha_{i}+\boldsymbol\mu$. Actually, the reparameterization of posterior distribution in Eq.(\ref{21}) is equivalent to a variant of the Euler-Maruyama discretization of the reverse-time SDE (see details in Appendix \ref{appa2}). Specifically, the Euler-Maruyama method computes the solution in the following form:
\begin{equation}
\boldsymbol{x}_t=\boldsymbol{x}_s+\int_s^t \left[f(\tau)\left(\boldsymbol{\mu}-\boldsymbol{x}_\tau\right)+\frac{g^2(\tau)}{\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\right]\mathrm{d}\tau
+\int_s^tg(\tau)\mathrm{d}\bar{\boldsymbol{w}}_\tau,
\end{equation}
which introduces approximation errors from both the analytical term and the non-linear component associated with neural network predictions. In contrast, our approach delivers an exact solution for the analytical part, leading to reduced approximation errors and a higher order of convergence.

\section{Fast Samplers for Mean Reverting Diffusion with Data Prediction}

Unfortunately, the sampler based on noise prediction can exhibit significant instability, particularly with small NFEs, and may perform even worse than \textit{posterior sampling}. It is well acknowledged that the Taylor expansion has a limited convergence domain, primarily influenced by the derivatives of the neural networks. In fact, higher-order derivatives often result in smaller convergence radii. During the training phase, the noise prediction neural network is designed to fit normally distributed Gaussian noise. Although the standard deviation of this Gaussian noise is set to 1, the values of samples can frequently fall outside the range of $[-1,1]$. This discrepancy results in numerical instability in the output of the neural network, causing its derivatives to exhibit more pronounced fluctuations (refer to the experimental results in Section \ref{section5} for further details). Consequently, the numerical instability leads to very narrow convergence domains, or in extreme cases, no convergence at all, which ultimately yields awful sampling results.

\cite{lu2022dpmsolverplus} have identified that the choice of parameterization for either ODEs or SDEs is critical for the boundedness of the convergent solution. In contrast to noise prediction, the data prediction model \citep{salimans2022progressive} focuses on fitting $\boldsymbol{x}_0$, ensuring that its output remains strictly confined within the bounds of $[-1,1]$, thereby achieving high numerical stability.

\subsection{Solutions to Mean Reverting SDEs with Data Prediction}

According to Eq.(\ref{7}), we can parameterize $\boldsymbol{x}_0$ as follows:
\begin{equation}
\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)=\frac{\boldsymbol{x}_t-\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t)-(1-\alpha_t)\boldsymbol\mu}{\sigma_t}.
\label{23}
\end{equation}
By substituting Eq.(\ref{23}) into Eq.(\ref{10}), we derive the following SDE that incorporates data prediction:
\begin{equation}
\mathrm{d}\boldsymbol{x}=\left(\frac{g^2(t)}{\sigma_t^2}-f(t)\right)\boldsymbol{x}
+\left[f(t)-\frac{g^2(t)}{\sigma_t^2}(1-\alpha_t)\right]\boldsymbol\mu
-\frac{g^2(t)}{\sigma_t^2}\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t)
+g(t)\mathrm{d}\bar{\boldsymbol{w}}.
\label{24}
\end{equation}
This equation remains semi-linear with respect to $\boldsymbol{x}$ and thus we can employ Itô's formula (in the differential form) to obtain the solution to Eq.(\ref{24}).

\textbf{Proposition 3.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{24}) is 
\begin{equation}
\begin{aligned}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}e^{-(\lambda_t-\lambda_s)}\boldsymbol{x}_s
+\boldsymbol\mu\left(1-\frac{\alpha_t}{\alpha_s}e^{-2(\lambda_t-\lambda_s)}-\alpha_t+\alpha_t e^{-2(\lambda_t-\lambda_s)}\right)\\
+2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-2(\lambda_t-\lambda)}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda
+\sigma_t\sqrt{1-e^{-2(\lambda_t-\lambda_s)}}\boldsymbol{z},
\label{25}
\end{aligned}
\end{equation}
where $\boldsymbol{z}\sim\mathcal{N}(\mathbf{0}, \boldsymbol{I})$. The proof is in Appendix \ref{appa1}.

Then we apply Taylor expansion and integration-by-parts to estimate the integral part in Eq.(\ref{25}) and obtain the following stochastic sampling algorithm for data prediction. 

\textbf{\ourmethod-SDE-d-2.} Given a final state $\boldsymbol{x}_T$, a Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_i}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see details in Algorithm \ref{alg:sde-d-2}):
\begin{equation}
\begin{aligned}
&\boldsymbol{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}e^{-h_i}\boldsymbol{x}_{t_{i-1}}
+\boldsymbol\mu\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}e^{-2h_i}-\alpha_{t_i}+\alpha_{t_i}e^{-2h_i}\right)+\sigma_{t_i}\sqrt{1-e^{-2h_i}}\boldsymbol{z}_i\\
&+\alpha_{t_i}\left(1-e^{-2h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})
+\alpha_{t_i}\left(h_i-\frac{1-e^{-2h_i}}{2}\right)\frac{\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}.
\label{26}
\end{aligned}
\end{equation}

\subsection{Solutions to Mean Reverting ODEs with Data Prediction}

By substituting Eq.(\ref{23}) into Eq.(\ref{16}), we can obtain the following ODE parameterized by data prediction. 
\begin{equation}
\frac{\mathrm{d}\boldsymbol{x}}{\mathrm{d}t}=\left(\frac{g^2(t)}{2\sigma^2_t}-f(t)\right)\boldsymbol{x}+\left[f(t)-\frac{g^2(t)}{2\sigma_t^2}(1-\alpha_t)\right]\boldsymbol\mu-\frac{g^2(t)}{2\sigma_t^2}\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t).
\label{27}
\end{equation}
The incorporation of the parameter $\boldsymbol{\mu}$ does not disrupt the semi-linear structure of the equation with respect to $\boldsymbol{x}$, and $\boldsymbol{\mu}$ is not coupled to the neural network. This implies that analytical part of solutions can still be derived concerning both $\boldsymbol{x}$ and $\boldsymbol{\mu}$. We present the solution to the equation in Eq.(\ref{27}) below (see Appendix \ref{appa1} for a detailed derivation).

\textbf{Proposition 4.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{27}) is 
\begin{equation}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}\boldsymbol{x}_s
+\boldsymbol\mu\left(1-\frac{\sigma_t}{\sigma_s}+\frac{\sigma_t}{\sigma_s}\alpha_s-\alpha_t \right)
+\sigma_t\int_{\lambda_s}^{\lambda_t}e^{\lambda}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda.
\label{28}
\end{equation}
Similarly, only the neural network component requires approximation through the exponential integrator method \citep{hochbruck2005explicit,hochbruck2010exponential}. And we can obtain the following deterministic sampling algorithm for data prediction.

\textbf{\ourmethod-ODE-d-2.} Given a final state $\boldsymbol{x}_T$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_{i}}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see complete algorithm in Algorithm \ref{alg:ode-d-2}):
\begin{equation}
\begin{aligned}
\boldsymbol{x}_{t_i}&=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
+\boldsymbol\mu\left(1-\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}+\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\alpha_{t_{i-1}}-\alpha_{t_i} \right)
+\alpha_{t_i}\left(1-e^{-h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})\\
&+\alpha_{t_i}\left(h_i-1+e^{-h_i}\right)\frac{\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}.
\label{29}
\end{aligned}
\end{equation}

\subsection{Transformation between three kinds of parameterizations}

There are three mainstream parameterization methods. \cite{ho2020ddpm} introduced a training objective based on noise prediction, while \cite{salimans2022progressive} proposed parameterization strategies for data and velocity prediction to keep network outputs stable under the variation of time or log-SNR. All three methods can be regarded as score matching approaches \citep{song2020sde,hyvarinen2005scorematch} with weighted coefficients. To ensure our proposed algorithm is compatible with these parameterization strategies, it is necessary to provide transformation formulas for each pairs among the three strategies.

The transformation formula between noise prediction and data prediction can be easily derived from Eq.(\ref{7}):
\begin{equation}
\begin{cases}
\boldsymbol{x}_\theta(t)=\frac{\boldsymbol{x}_t-(1-\alpha_t)\boldsymbol{\mu}-\sigma_t\boldsymbol{\epsilon}_\theta(t)}{\alpha_t},\\
\boldsymbol{\epsilon}_\theta(t)=\frac{\boldsymbol{x}_t-\alpha_t\boldsymbol{x}_\theta(t)-(1-\alpha_t)\boldsymbol{\mu}}{\sigma_t}.
\label{30}
\end{cases}
\end{equation}
For velocity prediction, we define $\phi_t:=\arctan(\frac{\sigma_t}{\sigma_\infty\alpha_t})$, which is slightly different from the definition of \cite{salimans2022progressive}. Then we have $\alpha_t=\cos{\phi_t}$, $\sigma_t=\sigma_\infty\sin{\phi_t}$ and hence $\boldsymbol{x}_t=\boldsymbol{x}_0\cos{\phi_t}+\boldsymbol{\mu}(1-\cos{\phi_t})+\sigma_\infty\sin{(\phi_t)}\boldsymbol{\epsilon}$. And the definition of $\boldsymbol{v}(t)$ is
\begin{equation}
\boldsymbol{v}_t=\frac{\mathrm{d}\boldsymbol{x}_t}{\mathrm{d}\phi_t}
=\boldsymbol{\mu}\sin\phi_t-\boldsymbol{x}_0\sin\phi_t+\sigma_\infty\cos(\phi_t)\boldsymbol\epsilon.
\label{31}
\end{equation}
If we have a score function model $\boldsymbol{v}_\theta(t)$ trained with velocity prediction, we can obtain $\boldsymbol{x}_\theta(t)$ and $\boldsymbol{\epsilon}_\theta(t)$ by (see Appendix \ref{appa3} for detailed derivations)
\begin{align}
\boldsymbol{x}_\theta(t)&=\boldsymbol{x}_t\cos\phi_t+\boldsymbol{\mu}(1-\cos\phi_t)-\boldsymbol{v}_\theta(t)\sin\phi_t, \label{32}\\
\boldsymbol\epsilon_\theta(t)&=(\boldsymbol{v}_\theta(t)\cos\phi_t+\boldsymbol{x}_t\sin\phi_t-\boldsymbol\mu\sin\phi_t)/\sigma_\infty. \label{33}
\end{align}


\section{Experiments}
\label{section5}

In this section, we conduct extensive experiments to show that \ourmethod can significantly speed up the sampling of existing MR Diffusion. To rigorously validate the effectiveness of our method, we follow the codes from \cite{luo2024daclip} and modify only the sampling part to include our proposed algorithm. We also use the pre-trained MR Diffusion checkpoint provided by \cite{luo2024daclip}. Our experiment is divided into three parts. The first part compares the sampling results for different NFE cases. In the second part, we study the effect of various parameter settings on our algorithm, including noise schedule, choice between SDE and ODE, and network parameterization. In the third part, we perform exploratory experiments. First, we visualize the sampling trajectories to show the speedup achieved by our algorithm. Second, we analyze why noise prediction gets obviously worse when NFE is less than 20.

\subsection{Sampling results with different NFEs}

We compare \ourmethod~ with other sampling methods, including posterior sampling \citep{luo2024posterior}, and Heun discretization \citep{ascher1998heun}. According to Section \ref{section3.3}, posterior sampling is equivalent to Euler-Maruyama discretization with a global truncation error $\mathcal{O}(\sqrt{\Delta t})$. 

\subsection{Effects of parameter choice}



\subsection{Analysis}

\textbf{Sampling trajectory.}~ Inspired by the design idea of NCSN \citep{song2019ncsn}, we provide a new perspective of diffusion sampling process. \cite{song2019ncsn} consider each data point (e.g., an image) as a point in high-dimensional space. During the diffusion process, noise is added to each point $\boldsymbol{x}_0$, causing it to spread throughout the space, while the score function (a neural network) "remembers" the direction towards $\boldsymbol{x}_0$. In the sampling process, we start from a random point by sampling Gaussian noise and follow the guidance of the reverse-time SDE (or PF-ODE) and the score function to locate $\boldsymbol{x}_0$. By connecting each intermediate state $\boldsymbol{x}_t$, we obtain a sampling trajectory. However, this trajectory exists in high-dimensional space, making it difficult to visualize. Therefore, we use Principal Component Analysis (PCA) to reduce $\boldsymbol{x}_t$ to two dimensions, obtaining the projection of the sampling trajectory in 2D space. As shown in Figure 1, we present several examples. Previous sampling methods \citep{luo2024posterior} often require a long path to find $\boldsymbol{x}_0$, and reducing NFE can lead to cumulative errors, making it impossible to locate $\boldsymbol{x}_0$. In contrast, our algorithm produces more direct trajectories, allowing us to find $\boldsymbol{x}_0$ with fewer NFEs.

\textbf{Numerical stability of parameterizations.}~ From Table 1, we observe poor sampling results for noise prediction in the case of small NFEs. The reason may be that the neural network parameterized by noise prediction is numerically unstable. Recall that we used Taylor expansion in Eq.(\ref{14}), and the condition for the equality to hold is $|\lambda-\lambda_s|<R$. And the radius of convergence $R$ can be calculated by
\begin{equation}
\frac{1}{R}=\lim_{n\rightarrow\infty}\left|\frac{c_{n+1}}{c_n}\right|,
\end{equation}
where $c_n$ is the coefficient of the $n$-th term in Taylor expansion. We are unable to compute this limit and can only compute the $n=1$ case as an approximation. The output of the neural network can be viewed as a vector, with each component corresponding to a radius of convergence. At each time step, we count the ratio of components that satisfy $R_i>|\lambda-\lambda_s|$ as a criterion for judging the convergence. As shown in Figure 1, the neural network parameterized by data prediction meets the convergence criteria at almost every step. However, the neural network parameterized by noise prediction always has components that cannot converge, which will lead to large errors and failed sampling. Therefore, data prediction has better numerical stability and is a more recommended choice.

\section{Conclusion}

We study the problem of fast sampling of MR Diffusion. Compared with DPMs, MR Diffusion is different in SDE and thus cannot adapt to existing training-free fast samplers. We propose \ourmethod~ for acceleration of sampling of MR Diffusion. We solve the reverse-time SDE
and PF-ODE derived from MRSDE and find a semi-analytical solution. We adopt the methods of \textit{exponential integrators} to estimate the non-linear integral part. Abundant experiments demonstrate that our algorithm achives small errors and fast convergence. Additionally, we visualize sampling trajectories and analyze the reason why the parameterization of noise prediction does not perform well in the case of small NFEs.

\begin{figure}[ht]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}




%%%%%%%%%%%%%%%%%% end of text
\bibliography{reference}
\bibliographystyle{iclr2025_conference}

\section*{Appendix}
\appendix

You may include other additional sections here.

\section{Derivation Details}

\subsection{Proofs of Propositions}\label{appa1}

\textbf{Proposition 1.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{10}) is
\begin{equation}
    \boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s+(1-\frac{\alpha_t}{\alpha_s})\boldsymbol{\mu}+\alpha_t\int_s^tg^2(\tau)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)}{\alpha_{\tau}\sigma_{\tau}}\mathrm{d}\tau
    +\sqrt{-\int_s^t\frac{\alpha_t^2}{\alpha_\tau^2}g^2(\tau)\mathrm{d}\tau}\boldsymbol{z},
    \label{prop1}
\end{equation}
where $\alpha_t:=e^{-\int_0^tf(\tau)\mathrm{d}\tau}$ and $\boldsymbol{z}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})$.

\textit{Proof}. For SDEs in the form of Eq.(\ref{1}), Itô's formula gives the following conclusion:
\begin{equation}
    \mathrm{d}\psi(\boldsymbol{x},t)=\frac{\partial\psi(\boldsymbol{x},t)}{\partial t}\mathrm{d}t + \frac{\partial\psi(\boldsymbol{x},t)}{\partial \boldsymbol{x}}[f(\boldsymbol{x},t) \mathrm{d}t + g(t) \mathrm{d}w] + \frac12\frac{\partial^2\psi(\boldsymbol{x},t)}{\partial \boldsymbol{x}^2}g^2(t)\mathrm{d}t,
    \label{a1-1}
\end{equation}
where $\psi(\boldsymbol{x},t)$ is a differentiable function. And we define 
\begin{equation*}
    \psi(\boldsymbol{x},t)=\boldsymbol{x}e^{\int_0^tf(\tau)\mathrm{d}\tau}
\end{equation*}
By substituting $f(\boldsymbol{x},t)$ and $g(t)$ with the corresponding drift and diffusion coefficients in Eq.(\ref{10}), we obtain 
\begin{equation*}
\mathrm{d}\psi(\boldsymbol{x},t)=\boldsymbol{\mu}f(t)e^{\int_0^tf(\tau)\mathrm{d}\tau}\mathrm{d}t+e^{\int_0^tf(\tau)\mathrm{d}\tau}\left[ \frac{g^2(t)}{\sigma_t}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)\mathrm{d}t+g(t)\mathrm{d}\bar{\boldsymbol{w}}\right].
\end{equation*}
And we integrate both sides of the above equation from $s$ to $t$:
\begin{equation*}
\psi(\boldsymbol{x},t)-\psi(\boldsymbol{x},s)=\boldsymbol{\mu}(e^{\int_0^tf(\tau)\mathrm{d}\tau}-e^{\int_0^sf(\tau)\mathrm{d}\tau})
+\int_s^te^{\int_0^{\tau}f(\xi)\mathrm{d}\xi}g^2(\tau)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)}{\sigma_{\tau}}\mathrm{d}\tau
+\int_s^te^{\int_0^{\tau}f(\xi)\mathrm{d}\xi}g(\tau)\mathrm{d}\bar{\boldsymbol{w}}.
\end{equation*}
Note that $\bar{\boldsymbol{w}}$ is a standard Wiener process running backwards in time and we have the quadratic variation $(\mathrm{d}\bar{\boldsymbol{w}})^2=-\mathrm{d}\tau$. According to the definition of $\psi(\boldsymbol{x},t)$ and $\alpha_t$, we have
\begin{equation*}
\frac{\boldsymbol{x}_t}{\alpha_t}-\frac{\boldsymbol{x}_s}{\alpha_s}=\boldsymbol{\mu}\left(\frac1\alpha_t-\frac1\alpha_s\right)+\int_s^tg^2(\tau)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)}{\alpha_{\tau}\sigma_{\tau}}\mathrm{d}\tau
+\sqrt{-\int_s^t\frac{g^2(\tau)}{\alpha_{\tau}^2}\mathrm{d}\tau}\boldsymbol{z},
\end{equation*}
which is equivalent to Eq.(\ref{prop1}).

\textbf{Proposition 2.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{16}) is 
\begin{equation}
\boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s+(1-\frac{\alpha_t}{\alpha_s})\boldsymbol{\mu}+\alpha_t\int_s^t
\frac{g^2(\tau)}{2\alpha_\tau\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\mathrm{d}\tau, \label{prop2}
\end{equation}
where $\alpha_t:=e^{-\int_0^tf(\tau)\mathrm{d}\tau}$.

\textit{Proof}. For ODEs which have a semi-linear structure as follows:
\begin{equation}
\frac{\mathrm{d}\boldsymbol{x}}{\mathrm{d}t}=P(t)\boldsymbol{x}+Q(\boldsymbol{x},t), \label{a1-2}
\end{equation}
the method of "variation of constants" gives the following solution:
\begin{equation*}
\boldsymbol{x}(t)=e^{\int_0^tP(\tau)\mathrm{d}\tau}\cdot \left[\int_0^tQ(\boldsymbol{x},\tau)e^{-\int_0^\tau P(r)\mathrm{d}r}\mathrm{d}\tau+C \right]. 
\end{equation*}
By simultaneously considering the following two equations
\begin{equation*}
\begin{cases}
    \boldsymbol{x}(t)=e^{\int_0^tP(\tau)\mathrm{d}\tau}\cdot \left[\int_0^tQ(\boldsymbol{x},\tau)e^{-\int_0^\tau P(r)\mathrm{d}r}\mathrm{d}\tau+C \right],\\
    \boldsymbol{x}(s)=e^{\int_0^sP(\tau)\mathrm{d}\tau}\cdot \left[\int_0^sQ(\boldsymbol{x},\tau)e^{-\int_0^\tau P(r)\mathrm{d}r}\mathrm{d}\tau+C \right],
\end{cases}
\end{equation*}
and eliminating $C$, we obtain
\begin{equation}
\boldsymbol{x}(t)=\boldsymbol{x}(s)e^{\int_s^tP(\tau)\mathrm{d}\tau}+ \int_s^tQ(\boldsymbol{x},\tau)e^{\int_\tau^t P(\xi)\mathrm{d}\xi}\mathrm{d}\tau.
\label{a1-3}
\end{equation}
Now we compare Eq.(\ref{16}) with Eq.(\ref{a1-2}) and let 
\begin{align*}
    P(t)&=-f(t)\\
   \text{and\;} Q(\boldsymbol{x},t)&=f(t)\boldsymbol{\mu}+\frac{g^2(t)}{2\sigma_t}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t).
\end{align*}
Therefore, we can rewrite Eq.(\ref{a1-3}) as
\begin{align*}
\boldsymbol{x}_t&=\boldsymbol{x}_se^{-\int_s^tf(\tau)\mathrm{d}\tau}+ \int_s^te^{-\int_\tau^tf(\xi)\mathrm{d}\xi}
\left[f(\tau)\boldsymbol{\mu}+\frac{g^2(\tau)}{2\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\right]\mathrm{d}\tau\\
&=\boldsymbol{x}_se^{-\int_s^tf(\tau)\mathrm{d}\tau}+\boldsymbol{\mu}(1-e^{-\int_s^tf(\tau)\mathrm{d}\tau})+\int_s^te^{-\int_\tau^tf(\xi)\mathrm{d}\xi}
\frac{g^2(\tau)}{2\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\mathrm{d}\tau,
\end{align*}
which is equivalent to Eq.(\ref{prop2}).

\textbf{Proposition 3.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{24}) is 
\begin{equation}
\begin{aligned}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}e^{-(\lambda_t-\lambda_s)}\boldsymbol{x}_s
+\boldsymbol\mu\left(1-\frac{\alpha_t}{\alpha_s}e^{-2(\lambda_t-\lambda_s)}-\alpha_t+\alpha_t e^{-2(\lambda_t-\lambda_s)}\right)\\
+2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-2(\lambda_t-\lambda)}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda
+\sigma_t\sqrt{1-e^{-2(\lambda_t-\lambda_s)}}\boldsymbol{z},
\label{prop3}
\end{aligned}
\end{equation}
where $\boldsymbol{z}\sim\mathcal{N}(\mathbf{0},\boldsymbol{I})$.

\textit{Proof}. According to Eq.(\ref{a1-1}), we define
\begin{align*}
u(t)=\frac{g^2(t)}{\sigma_t^2}-f(t)\\
\text{and}\quad\psi(\boldsymbol{x},t)=\boldsymbol{x}e^{\int_0^tu(\tau)\mathrm{d}\tau}.
\end{align*}
We substitute $f(\boldsymbol{x},t)$ and $g(t)$ in Eq.(\ref{a1-1}) with the corresponding drift and diffusion coefficients in Eq.(\ref{24}), and integrate both sides of the equation from $s$ to $t$:
\begin{equation}
\begin{aligned}
\boldsymbol{x}_t=\boldsymbol{x}_se^{\int_s^tu(\tau)\mathrm{d}\tau}
+\boldsymbol\mu\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}\left[f(\tau)-\frac{g^2(\tau)}{\sigma^2_\tau}(1-\alpha_\tau)\right]\mathrm{d}\tau\\
-\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}\left[\frac{g^2(\tau)}{\sigma^2_\tau}\alpha_\tau\boldsymbol{x}_\theta(\boldsymbol{x}_\tau,\tau)\right]\mathrm{d}\tau
+\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}g(\tau)\mathrm{d}\bar{\boldsymbol{w}}.
\label{a1-4}
\end{aligned}
\end{equation}
We can rewrite $g(\tau)$ as Eq.(\ref{12}) and obtain
\begin{equation}
e^{\int_s^tu(\tau)\mathrm{d}\tau}=\exp{\int_s^t\left(-2\frac{\mathrm{d}\lambda_\tau}{\mathrm{d}\tau}-f(\tau)\right)\mathrm{d}\tau}
=\frac{\alpha_t}{\alpha_s}e^{-2(\lambda_t-\lambda_s)}
=\frac{\sigma_t}{\sigma_s}e^{-(\lambda_t-\lambda_s)}.
\label{a1-5}
\end{equation}
Next, we consider each term in Eq.(\ref{a1-4}) by employing Eq.(\ref{12}) and Eq.(\ref{a1-5}). Firstly, we simplify the second term:
\begin{align}
&\boldsymbol\mu\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}\left[f(\tau)-\frac{g^2(\tau)}{\sigma^2_\tau}(1-\alpha_\tau)\right]\mathrm{d}\tau \nonumber\\
&=\boldsymbol\mu\int_s^t\frac{\sigma_t}{\sigma_\tau}e^{-(\lambda_t-\lambda_\tau)}\left[f(\tau)+2(1-\alpha_\tau)\frac{\mathrm{d}\lambda_\tau}{\mathrm{d}\tau}\right]\mathrm{d}\tau \nonumber\\
&=\boldsymbol\mu\sigma_te^{-\lambda_t}\int_s^t\frac{e^{\lambda_\tau}}{\sigma_\tau}\left[f(\tau)\mathrm{d}\tau+2(1-\alpha_\tau)\mathrm{d}\lambda_\tau\right] \nonumber\\
&=\boldsymbol\mu\sigma_te^{-\lambda_t}\int_s^t\frac{\alpha_\tau}{\sigma^2_\tau}\left[f(\tau)\mathrm{d}\tau+2\mathrm{d}\lambda_\tau-2\alpha_\tau\mathrm{d}\lambda_\tau\right] \nonumber\\
&=\boldsymbol\mu\sigma_te^{-\lambda_t}\int_s^t\frac{-\mathrm{d}\alpha_\tau}{\sigma^2_\tau}+\frac{2\alpha_\tau}{\sigma^2_\tau}\mathrm{d}\lambda_\tau-2\frac{\alpha^2_\tau}{\sigma^2_\tau}\mathrm{d}\lambda_\tau.
\label{a1-6}
\end{align}
Note that
\begin{equation}
\mathrm{d}\lambda_t=\mathrm{d}\left(\log{\frac{\alpha_t}{\sigma_\infty\sqrt{1-\alpha_t^2}}}\right)
=\frac{\mathrm{d}\alpha_t}{\alpha_t}+\frac{\alpha_t\mathrm{d}\alpha_t}{1-\alpha_t^2}=\frac{\mathrm{d}\alpha_t}{\alpha_t(1-\alpha_t^2)}.
\label{a1-7}
\end{equation}
Substitute Eq.(\ref{a1-7}) into Eq.(\ref{a1-6}) and we obtain
\begin{align}
&\boldsymbol\mu\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}\left[f(\tau)-\frac{g^2(\tau)}{\sigma^2_\tau}(1-\alpha_\tau)\right]\mathrm{d}\tau \nonumber\\
&=\boldsymbol\mu\sigma_te^{-\lambda_t}\int_s^t\frac{-\mathrm{d}\alpha_\tau}{\sigma^2_\tau}
+\frac{2\mathrm{d}\alpha_\tau}{\sigma^2_\tau(1-\alpha^2_\tau)}
-2\frac{\alpha^2_\tau}{\sigma^2_\tau}\mathrm{d}\lambda_\tau \nonumber\\
&=\boldsymbol\mu\sigma_te^{-\lambda_t}\int_s^t\frac{1+\alpha^2_\tau}{\sigma^2_\infty(1-\alpha^2_\tau)^2}\mathrm{d}\alpha_\tau
-2e^{2\lambda_\tau}\mathrm{d}\lambda_\tau \nonumber\\
&=\boldsymbol\mu\sigma_te^{-\lambda_t}\int_s^t\frac{1}{\sigma^2_\infty}\mathrm{d}\left(\frac{\alpha_\tau}{1-\alpha^2_\tau}\right)
-2e^{2\lambda_\tau}\mathrm{d}\lambda \nonumber\\
&=\boldsymbol\mu\left(1-\frac{\alpha_t}{\alpha_s}e^{-2(\lambda_t-\lambda_s)}-\alpha_t+\alpha_te^{-2(\lambda_t-\lambda_s)}\right).
\label{a1-8}
\end{align}
Secondly, we rewrite the third term in Eq.(\ref{a1-4}) by employing Eq.(\ref{12}) and Eq.(\ref{a1-5}).
\begin{align}
-\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}\left[\frac{g^2(\tau)}{\sigma^2_\tau}\alpha_\tau\boldsymbol{x}_\theta(\boldsymbol{x}_\tau,\tau)\right]\mathrm{d}\tau
&=-\int_s^t\frac{\sigma_t}{\sigma_\tau}e^{-(\lambda_t-\lambda_\tau)}\left[-2\frac{\mathrm{d}\lambda_\tau}{\mathrm{d}\tau}\alpha_\tau\boldsymbol{x}_\theta(\boldsymbol{x}_\tau,\tau)\right]\mathrm{d}\tau \nonumber\\
&=2\int_s^t \sigma_te^{2\lambda_\tau-\lambda_t}\boldsymbol{x}_\theta(\boldsymbol{x}_\tau,\lambda_\tau)\mathrm{d}\lambda_\tau \nonumber\\
&=2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-2(\lambda_t-\lambda)}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda.
\label{a1-9}
\end{align}
Thirdly, we consider the fourth term in Eq.(\ref{a1-4}) (note that $(\mathrm{d}\bar{\boldsymbol{w}})^2=-\mathrm{d}\tau$):
\begin{align}
\int_s^te^{\int_\tau^tu(\xi)\mathrm{d}\xi}g(\tau)\mathrm{d}\bar{\boldsymbol{w}}
&=\sqrt{-\int_s^te^{2\int_\tau^tu(\xi)\mathrm{d}\xi}g^2(\tau)\mathrm{d}\tau}\boldsymbol{z} \nonumber\\
&=\sqrt{-\int_s^t\frac{\sigma^2_t}{\sigma^2_\tau}e^{-2(\lambda_t-\lambda_\tau)}\left(-2\sigma^2_{\tau}\frac{\mathrm{d}\lambda_\tau}{\mathrm{d}\tau}\right)\mathrm{d}\tau}\boldsymbol{z} \nonumber\\
&=\sqrt{\sigma^2_t\int_s^t 2e^{2(\lambda_\tau-\lambda_t)}\mathrm{d}\lambda_\tau}\boldsymbol{z} \nonumber\\
&=\sigma_t\sqrt{1-e^{-2(\lambda_t-\lambda_s)}}\boldsymbol{z}.
\label{a1-10}
\end{align}
Lastly, we substitute Eq.(\ref{a1-5}) and Eq.(\ref{a1-8}-\ref{a1-10}) into Eq.(\ref{a1-4}) and obtain the solution as presented in Eq.(\ref{prop3}).

\textbf{Proposition 4.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{27}) is 
\begin{equation}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}\boldsymbol{x}_s
+\boldsymbol\mu\left(1-\frac{\sigma_t}{\sigma_s}+\frac{\sigma_t}{\sigma_s}\alpha_s-\alpha_t \right)
+\sigma_t\int_{\lambda_s}^{\lambda_t}e^{\lambda}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda.
\label{prop4}
\end{equation}
\textit{Proof}. Note that Eq.(\ref{27}) shares the same structure as Eq.(\ref{a1-2}). Let
\begin{align*}
P(t)&=\frac{g^2(t)}{2\sigma^2_t}-f(t),\\
\text{and}\quad
Q(\boldsymbol{x},t)&=\left[f(t)-\frac{g^2(t)}{2\sigma_t^2}(1-\alpha_t)\right]\boldsymbol\mu-\frac{g^2(t)}{2\sigma_t^2}\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t).
\end{align*}
According to Eq.(\ref{12}), we first consider
\begin{align}
e^{\int_s^tP(\tau)\mathrm{d}\tau}&=\exp{\int_s^t\left[\frac{g^2(\tau)}{2\sigma^2_\tau}-f(\tau)\right]\mathrm{d}\tau}=\exp{\int_s^t-\mathrm{d}\lambda_\tau+\mathrm{d}\log{\alpha_\tau}} \nonumber\\
&=\exp{\int_s^t\mathrm{d}\log{\alpha_\tau}-\mathrm{d}\log{\frac{\alpha_\tau}{\sigma_\tau}}}=\exp{\int_s^t\mathrm{d}\log{\sigma_\tau}}=\frac{\sigma_t}{\sigma_s}.
\label{a1-11}
\end{align}
Then, we can rewrite Eq.(\ref{a1-3}) as
\begin{equation}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}\boldsymbol{x}_s+
\boldsymbol{\mu}\int_s^t\frac{\sigma_t}{\sigma_\tau}\left[f(\tau)-\frac{g^2(\tau)}{2\sigma_\tau^2}(1-\alpha_\tau)\right]\mathrm{d}\tau
-\int_s^t\frac{\sigma_t}{\sigma_\tau}\frac{g^2(\tau)}{2\sigma_\tau^2}\alpha_\tau\boldsymbol{x}_\theta(\boldsymbol{x}_\tau,\tau)\mathrm{d}\tau.
\label{a1-12}
\end{equation}
Firstly, we consider the second term in Eq.(\ref{a1-12})
\begin{align}
&\boldsymbol{\mu}\int_s^t\frac{\sigma_t}{\sigma_\tau}\left[f(\tau)-\frac{g^2(\tau)}{2\sigma^2_\tau}(1-\alpha_\tau)\right]\mathrm{d}\tau \nonumber\\
&=\boldsymbol\mu\sigma_t\int_s^t\frac{1}{\sigma_\tau}\left[f(\tau)-\frac{g^2(\tau)}{2\sigma^2_\tau}+\frac{g^2(\tau)}{2\sigma^2_\tau}\alpha_\tau\right]\mathrm{d}\tau \nonumber\\
&=\boldsymbol\mu\sigma_t\left[\int_s^t\frac{1}{\sigma_\tau}\left(f(\tau)-\frac{g^2(\tau)}{2\sigma^2_\tau}\right)\mathrm{d}\tau +\int_s^t\frac{g^2(\tau)}{2\sigma^3_\tau}\alpha_\tau\mathrm{d}\tau \right] \nonumber\\
&=\boldsymbol\mu\sigma_t\left[-\int_s^t\frac{\mathrm{d}\log{\sigma_\tau}}{\sigma_\tau}
-\int_s^t\frac{\alpha_\tau}{\sigma_\tau}\mathrm{d}\lambda_\tau \right] \quad\text{(refer to Eq.(\ref{12}) and Eq.(\ref{a1-11})} \nonumber\\
&=\boldsymbol\mu\sigma_t\left[\int_s^t\mathrm{d}\left(\frac{1}{\sigma_\tau}\right)-\int_s^t\mathrm{d}e^{\lambda_\tau} \right] \nonumber\\
&=\boldsymbol{\mu}\left(1-\frac{\sigma_t}{\sigma_s}+\frac{\sigma_t}{\sigma_s}\alpha_s-\alpha_t\right).
\label{a1-13}
\end{align}
Secondly, we rewrite the third term in Eq.(\ref{a1-12})
\begin{equation}
-\int_s^t\frac{\sigma_t}{\sigma_\tau}\frac{g^2(\tau)}{2\sigma_\tau^2}\alpha_\tau\boldsymbol{x}_\theta(\boldsymbol{x}_\tau,\tau)\mathrm{d}\tau
=\sigma_t\int_s^te^{\lambda_\tau}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda_\tau.
\label{a1-14}
\end{equation}
By substituting Eq.(\ref{a1-13}) and Eq.(\ref{a1-14}) into Eq.(\ref{a1-12}), we can obtain the solution shown in Eq.(\ref{prop4}).

\subsection{Equivalence between Posterior Sampling and Euler-Maruyama Discretization}
\label{appa2}

The \textit{posterior sampling} \citep{luo2024posterior} algorithm utilizes the reparameterization of Gaussian distribution in Eq.(\ref{21}) and computes $\boldsymbol{x}_{i-1}$ from $\boldsymbol{x}_{i}$ iteratively as follows:
\begin{equation}
\begin{aligned}
    \boldsymbol{x}_{i-1}&=\tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_0)+\sqrt{\tilde{\beta}_{i}}\boldsymbol{z}_i,\\
    \tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_{0})&=\frac{(1-\alpha^2_{i-1})\alpha_{i}}{(1-\alpha^2_{i})\alpha_{i-1}}(\boldsymbol{x}_{i}-\boldsymbol\mu)+\frac{1-\frac{\alpha^2_{i}}{\alpha^2_{i-1}}}{1-\alpha^2_{i}}\alpha_{i-1}(\boldsymbol{x}_{0}-\boldsymbol\mu)+\boldsymbol\mu,\\
    \tilde{\beta}_{i}&=\frac{(1-\alpha^2_{i-1})(1-\frac{\alpha^2_{i}}{\alpha^2_{i-1}})}{1-\alpha^2_{i}},
    \label{a2-1}
\end{aligned}
\end{equation}
where $\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}),$ $\alpha_{i}=e^{-\int_{0}^{i}f(\tau)\mathrm{d}\tau}$ and $\boldsymbol{x}_0=\left(\boldsymbol{x}_{i}-\boldsymbol\mu-\sigma_{i}{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_{i},\boldsymbol\mu,t_i)\right)/\alpha_{i}+\boldsymbol\mu$. By substituting $\boldsymbol{x}_0$ into $\tilde{\boldsymbol\mu}_{i}$, we arrange the equation and obtain
\begin{equation}
\begin{aligned}
\tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_0)&=\frac{\alpha_{i-1}}{\alpha_{i}}\boldsymbol{x}_{i}+(1-\frac{\alpha_{i-1}}{\alpha_{i}})\boldsymbol{\mu}-\frac{\frac{\alpha_{i-1}}{\alpha_{i}}-\frac{\alpha_{i}}{\alpha_{i-1}}}{1-\alpha^2_{i}}\sigma_{i}\tilde{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_{i},\boldsymbol\mu,t_i)\\
&=\frac{\alpha_{i-1}}{\alpha_{i}}\boldsymbol{x}_{i}+(1-\frac{\alpha_{i-1}}{\alpha_{i}})\boldsymbol{\mu}-\frac{\frac{\alpha_{i-1}}{\alpha_{i}}-\frac{\alpha_{i}}{\alpha_{i-1}}}{\sqrt{1-\alpha^2_{i}}}\sigma_\infty\tilde{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_{i},\boldsymbol\mu,t_i).
\label{a2-2}
\end{aligned}
\end{equation}
We note that
\begin{equation}
\frac{\alpha_{i-1}}{\alpha_{i}}=e^{\int_{i-1}^{i}f(\tau)\mathrm{d}\tau}=1+\int_{i-1}^{i}f(\tau)\mathrm{d}\tau+o\left(\int_{i-1}^{i}f(\tau)\mathrm{d}\tau\right)
\approx1+f(t_{i})\Delta t_i,
\label{a2-3}
\end{equation}
where the high-order error term is omitted and $\Delta t_i:=t_i-t_{i-1}$. By substituting Eq.(\ref{a2-3}) into Eq.(\ref{a2-2}) and Eq.(\ref{a2-1}), we obtain
\begin{align}
\tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_0)&=(1+f(t_i)\Delta t_i)\boldsymbol{x}_i-f(t_i)\Delta t_i\boldsymbol\mu-\frac{2f(t_i)\Delta t_i\sigma_\infty}{\sqrt{1-\alpha^2_i}}\tilde{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_i,\boldsymbol\mu,t_i),\label{a2-4}\\
\tilde{\beta}_i&=\frac{(1-\alpha^2_{i-1})(1-\frac{\alpha^2_i}{\alpha^2_{i-1}})}{1-\alpha^2_i}
\approx\frac{2f(t_i)\Delta t_i(1-\alpha^2_{i-1})}{1-\alpha^2_i}.
\label{a2-5}
\end{align}
On the other hand, the reverse-time SDE has been presented in Eq.(\ref{10}). Combining the assumption $g^2(t)/f(t)=2\sigma_\infty^2$ in Section \ref{section2.2} and the definition of $\sigma_t$ in Section \ref{section3.1}, the Euler–Maruyama descretization of this SDE is
\begin{align}
\boldsymbol{x}_{i-1}-\boldsymbol{x}_i&=-f(t_i)(\boldsymbol\mu-\boldsymbol{x}_i)\Delta t_i-\frac{g^2(t_i)}{\sigma_i}\tilde{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_i,\boldsymbol\mu,t_i)\Delta t_i+g(t_i)\sqrt{\Delta t_i}\boldsymbol{z}_i, \nonumber\\
\therefore \boldsymbol{x}_{i-1}&=(1+f(t_i)\Delta t_i)\boldsymbol{x}_i-f(t_i)\Delta t_i\boldsymbol\mu-\frac{2\sigma_\infty^2f(t_i)}{\sigma_\infty\sqrt{1-\alpha^2_i}}\tilde{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_i,\boldsymbol\mu,t_i)\Delta t_i+g(t_i)\sqrt{\Delta t_i}\boldsymbol{z}_i \nonumber\\
&=(1+f(t_i)\Delta t_i)\boldsymbol{x}_i-f(t_i)\Delta t_i\boldsymbol\mu-\frac{2f(t_i)\Delta t_i\sigma_\infty}{\sqrt{1-\alpha^2_i}}\tilde{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_i,\boldsymbol\mu,t_i)+\sigma_\infty\sqrt{2f(t_i)\Delta t_i}\boldsymbol{z}_i \nonumber\\
&=\tilde{\boldsymbol\mu}_i(\boldsymbol{x}_i,\boldsymbol{x}_0)+\sigma_\infty\sqrt{\frac{1-\alpha^2_i}{1-\alpha^2_{i-1}}\tilde{\beta}_i}\boldsymbol{z}_i.
\end{align}
Thus, the \textit{posterior sampling} algorithm is a special Euler–Maruyama descretization of reverse-time SDE with a different coefficient of Gaussian noise.

\subsection{Derivations about velocity prediction}
\label{appa3}

Following Eq.(\ref{31}), We can define the \textit{velocity prediction} as
\begin{equation}
\boldsymbol{v}_\theta(t)=\boldsymbol{\mu}\sin\phi_t-\boldsymbol{x}_\theta(t)\sin\phi_t+\sigma_\infty\cos(\phi_t)\boldsymbol\epsilon_\theta(t). \label{a3-1}
\end{equation}
And we have the relationship between $\boldsymbol{x}_\theta(t)$ and $\boldsymbol{\epsilon}_\theta(t)$ as follows:
\begin{equation}
\boldsymbol{x}_t=\boldsymbol{x}_\theta(t)\cos{\phi_t}+\boldsymbol{\mu}(1-\cos{\phi_t})+\sigma_\infty\sin{(\phi_t)}\boldsymbol{\epsilon}_\theta(t).
\label{a3-2}
\end{equation}
In order to get $\boldsymbol{x}_\theta$ from $\boldsymbol{v}_\theta$, we rewrite Eq.(\ref{a3-1}) as
\begin{equation}
\boldsymbol{x}_\theta(t)\sin^2\phi_t=\boldsymbol{\mu}\sin^2\phi_t-\boldsymbol{v}_\theta(t)\sin\phi_t+\sigma_\infty\boldsymbol{\epsilon}_\theta(t)\sin\phi_t\cos\phi_t.
\end{equation}
Then we replace $\boldsymbol{\epsilon}_\theta(t)$ according to Eq.(\ref{a3-2})
\begin{align}
\boldsymbol{x}_\theta(t)\sin^2\phi_t&=\boldsymbol{\mu}\sin^2\phi_t-\boldsymbol{v}_\theta(t)\sin\phi_t+\left[\boldsymbol{x}_t-\boldsymbol{x}_\theta(t)\cos{\phi_t}-\boldsymbol{\mu}(1-\cos{\phi_t}) \right]\cos\phi_t \nonumber\\
&=(1-\cos\phi_t)\boldsymbol{\mu}-\boldsymbol{v}_\theta(t)\sin\phi_t+\boldsymbol{x}_t\cos\phi_t-\boldsymbol{x}_\theta(t)\cos^2\phi_t.
\end{align}
Arranging the above equation, we can obtain the transformation from $\boldsymbol{v}_\theta$ to $\boldsymbol{x}_\theta$, as shown in Eq.(\ref{32}).
Similarly, we can also rewrite Eq.(\ref{a3-1}) and replace $\boldsymbol{x}_\theta(t)$ as follows:
\begin{align}
\sigma_\infty\cos^2(\phi_t)\boldsymbol{\epsilon}_\theta(t)&=
\boldsymbol{v}_\theta(t)\cos\phi_t-\boldsymbol{\mu}\sin\phi_t\cos\phi_t+\boldsymbol{x}_\theta(t)\sin\phi_t\cos\phi_t \nonumber\\
&=\boldsymbol{v}_\theta(t)\cos\phi_t-\boldsymbol{\mu}\sin\phi_t\cos\phi_t+\sin\phi_t\left[\boldsymbol{x}_t-\boldsymbol{\mu}(1-\cos{\phi_t})-\sigma_\infty\sin{(\phi_t)}\boldsymbol{\epsilon}_\theta(t)\right] \nonumber\\
&=\boldsymbol{v}_\theta(t)\cos\phi_t-\boldsymbol{\mu}\sin\phi_t+\boldsymbol{x}_\theta(t)\sin\phi_t-\sigma_\infty\sin^2\phi_t\boldsymbol{\epsilon}_\theta(t).
\end{align}
Thus we obtain the transformation from $\boldsymbol{v}_\theta$ to $\boldsymbol{\epsilon}_\theta$, as presented in Eq.(\ref{33}).

\section{Notation Comparison Table}
\label{appb1}

\renewcommand{\arraystretch}{1.5}
\begin{table}[ht]
\caption{The correspondence between the notations used in this paper (left column) and notations used by MRSDE (right column)}
\label{table2}
\begin{center}
\begin{tabular}{|c|c|}
\hline
This paper & MRSDE \citep{luo2023mrsde}
\\ \hline
$f(t)$ & $\theta_t$
\\ \hline
$g(t)$ & $\sigma_t$
\\ \hline
$\alpha_t$ & $e^{-\int_0^t\theta_\tau\mathrm{d}\tau}$
\\ \hline
$\sigma_t$ & $\sigma_\infty\sqrt{1-e^{-2\int_0^t\theta_\tau\mathrm{d}\tau}}$ 
\\ \hline
\end{tabular}
\end{center}
\end{table}

\section{Detailed sampling algorithm of \ourmethod}
\label{appc}

We list the detailed sampling algorithm here.

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-SDE-n-1.}\label{alg:sde-n-1}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \FOR{$i\gets 1$ to $M$}
        \STATE $\boldsymbol{x}_{t_{i}}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
        +\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}-2\sigma_{t_i}(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})
        +\sigma_{t_i}\sqrt{e^{2h_i}-1}\boldsymbol{z}_i$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-SDE-n-2.}\label{alg:sde-n-2}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \STATE $\boldsymbol{x}_{t_1}=\frac{\alpha_{t_1}}{\alpha_{t_{0}}}\boldsymbol{x}_{t_{0}}
        +\left(1-\frac{\alpha_{t_1}}{\alpha_{t_{0}}}\right)\boldsymbol{\mu}-2\sigma_{t_1}(e^{h_1}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{0}},t_{0})
        +\sigma_{t_1}\sqrt{e^{2h_1}-1}\boldsymbol{z}_1$
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_1},t_1)$
        \FOR{$i\gets 2$ to $M$}
        \STATE $\boldsymbol{D}_i=\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}$
        \STATE $\boldsymbol{x}_{t_{i}}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
        +\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}-2\sigma_{t_i}\left[(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})+(e^{h_i}-1-h_i)\boldsymbol{D}_i \right]
        +\sigma_{t_i}\sqrt{e^{2h_i}-1}\boldsymbol{z}_i$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-ODE-n-1.}\label{alg:ode-n-1}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \FOR{$i\gets 1$ to $M$}
        \STATE $\boldsymbol{x}_{t_{i}}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
        +\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}-\sigma_{t_i}(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-ODE-n-2.}\label{alg:ode-n-2}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \STATE $\boldsymbol{x}_{t_1}=\frac{\alpha_{t_1}}{\alpha_{t_{0}}}\boldsymbol{x}_{t_0}
        +\left(1-\frac{\alpha_{t_1}}{\alpha_{t_{0}}}\right)\boldsymbol{\mu}-\sigma_{t_1}(e^{h_1}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_1},t_1)$
        \FOR{$i\gets 2$ to $M$}
        \STATE $\boldsymbol{D}_i=\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}$
        \STATE $\boldsymbol{x}_{t_{i}}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
        +\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}-\sigma_{t_i}\left[(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})+(e^{h_i}-1-h_i)\boldsymbol{D}_i \right]$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-SDE-d-1.}\label{alg:sde-d-1}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \FOR{$i\gets 1$ to $M$}
        \STATE $\boldsymbol{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}e^{-h_i}\boldsymbol{x}_{t_{i-1}}
        +\boldsymbol\mu\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}e^{-2h_i}-\alpha_{t_i}+\alpha_{t_i}e^{-2h_i}\right)+\sigma_{t_i}\sqrt{1-e^{-2h_i}}\boldsymbol{z}_i
        +\alpha_{t_i}\left(1-e^{-2h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-SDE-d-2.}\label{alg:sde-d-2}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \STATE $\boldsymbol{x}_{t_1}=\frac{\sigma_{t_1}}{\sigma_{t_{0}}}e^{-h_1}\boldsymbol{x}_{t_{0}}
        +\boldsymbol\mu\left(1-\frac{\alpha_{t_1}}{\alpha_{t_{0}}}e^{-2h_1}-\alpha_{t_1}+\alpha_{t_1}e^{-2h_1}\right)
        +\alpha_{t_1}\left(1-e^{-2h_1}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{0}},t_{0})
        +\sigma_{t_1}\sqrt{1-e^{-2h_1}}\boldsymbol{z}_1$
        \STATE $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \FOR{$i\gets 2$ to $M$}
        \STATE $\boldsymbol{D}_i=\frac{\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}$
        \STATE $\boldsymbol{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}e^{-h_i}\boldsymbol{x}_{t_{i-1}}
        +\boldsymbol\mu\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}e^{-2h_i}-\alpha_{t_i}+\alpha_{t_i}e^{-2h_i}\right)+\sigma_{t_i}\sqrt{1-e^{-2h_i}}\boldsymbol{z}_i
        +\alpha_{t_i}\left(1-e^{-2h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})
        +\alpha_{t_i}\left(h_i-\frac{1-e^{-2h_i}}{2}\right)\boldsymbol{D}_i$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-ODE-d-1.}\label{alg:ode-d-1}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \FOR{$i\gets 1$ to $M$}
        \STATE $\boldsymbol{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
        +\boldsymbol\mu\left(1-\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}+\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\alpha_{t_{i-1}}-\alpha_{t_i} \right)
        +\alpha_{t_i}\left(1-e^{-h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \centering
    \caption{\ourmethod-ODE-d-2.}\label{alg:ode-d-2}
    \begin{algorithmic}[1]
    \REQUIRE initial value $\boldsymbol{x}_T=\boldsymbol\mu+\sigma_\infty\boldsymbol\epsilon$, time steps $\{t_i\}_{i=0}^M$, data prediction model $\boldsymbol{x}_\theta$. Denote $h_i:=\lambda_{t_i}-\lambda_{t_{i-1}}$ for $i=1,\ldots,M$.
        \STATE $\boldsymbol{x}_{t_0}\leftarrow\boldsymbol{x}_T$. Initialize an empty buffer $Q$.
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_0},t_0)$
        \STATE $\boldsymbol{x}_{t_1}=\frac{\sigma_{t_1}}{\sigma_{t_{0}}}\boldsymbol{x}_{t_{0}}
        +\boldsymbol\mu\left(1-\frac{\sigma_{t_1}}{\sigma_{t_{0}}}+\frac{\sigma_{t_1}}{\sigma_{t_{0}}}\alpha_{t_{0}}-\alpha_{t_1} \right)
        +\alpha_{t_1}\left(1-e^{-h_1}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{0}},t_{0})$
        \STATE $Q\xleftarrow{\text{buffer}}\boldsymbol{x}_\theta(\boldsymbol{x}_{t_1},t_1)$
        \FOR{$i\gets 2$ to $M$}
        \STATE $\boldsymbol{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
        +\boldsymbol\mu\left(1-\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}+\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\alpha_{t_{i-1}}-\alpha_{t_i} \right)
        +\alpha_{t_i}\left(1-e^{-h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})
        +\alpha_{t_i}\left(h_i-1+e^{-h_i}\right)\frac{\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}$
        \STATE If $i < M$, then $Q \xleftarrow{\text{buffer}} \boldsymbol{x}_\theta(\boldsymbol{x}_{t_i}, t_i)$
        \ENDFOR
        \RETURN $\boldsymbol{x}_{t_M}$
    \end{algorithmic}
\end{algorithm}

\end{document}
