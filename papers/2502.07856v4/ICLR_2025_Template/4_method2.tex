\section{Fast Samplers for Mean Reverting Diffusion with Data Prediction}
\label{section4}

Unfortunately, the sampler based on noise prediction can exhibit substantial instability, particularly with small NFEs, and may perform even worse than \textit{posterior sampling}. It is well recognized that the Taylor expansion has a limited convergence domain, primarily influenced by the derivatives of the neural networks. In fact, higher-order derivatives often result in smaller convergence radii. During the training phase, the noise prediction neural network is designed to fit normally distributed Gaussian noise. When the standard deviation of this Gaussian noise is set to 1, the values of samples can fall outside the range of $[-1,1]$ with a probability of 34.74\%. This discrepancy results in numerical instability in the output of the neural network, causing its derivatives to exhibit more pronounced fluctuations (refer to the experimental results in Section \ref{section5} for further details). Consequently, the numerical instability leads to very narrow convergence domains, or in extreme cases, no convergence at all, which ultimately yields awful sampling results.

\cite{lu2022dpmsolverplus} have identified that the choice of parameterization for either ODEs or SDEs is critical for the boundedness of the convergent solution. In contrast to noise prediction, the data prediction model \citep{salimans2022progressive} focuses on fitting $\boldsymbol{x}_0$, ensuring that its output remains strictly confined within the bounds of $[-1,1]$, thereby achieving high numerical stability.

\subsection{Solutions to Mean Reverting SDEs with Data Prediction}

According to Eq.(\ref{7}), we can parameterize $\boldsymbol{x}_0$ as follows:
\begin{equation}
\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)=\frac{\boldsymbol{x}_t-\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t)-(1-\alpha_t)\boldsymbol\mu}{\sigma_t}.
\label{23}
\end{equation}
By substituting Eq.(\ref{23}) into Eq.(\ref{10}), we derive the following SDE that incorporates data prediction:
\begin{equation}
\mathrm{d}\boldsymbol{x}=\left(\frac{g^2(t)}{\sigma_t^2}-f(t)\right)\boldsymbol{x}
+\left[f(t)-\frac{g^2(t)}{\sigma_t^2}(1-\alpha_t)\right]\boldsymbol\mu
-\frac{g^2(t)}{\sigma_t^2}\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t)
+g(t)\mathrm{d}\bar{\boldsymbol{w}}.
\label{24}
\end{equation}
This equation remains semi-linear with respect to $\boldsymbol{x}$ and thus we can employ It√¥'s formula (in the differential form) to obtain the solution to Eq.(\ref{24}).

\textbf{Proposition 3.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{24}) is 
\begin{equation}
\begin{aligned}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}e^{-(\lambda_t-\lambda_s)}\boldsymbol{x}_s
+\boldsymbol\mu\left(1-\frac{\alpha_t}{\alpha_s}e^{-2(\lambda_t-\lambda_s)}-\alpha_t+\alpha_t e^{-2(\lambda_t-\lambda_s)}\right)\\
+2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-2(\lambda_t-\lambda)}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda
+\sigma_t\sqrt{1-e^{-2(\lambda_t-\lambda_s)}}\boldsymbol{z},
\label{25}
\end{aligned}
\end{equation}
where $\boldsymbol{z}\sim\mathcal{N}(\mathbf{0}, \boldsymbol{I})$. The proof is in Appendix \ref{appa1}.

Then we apply Taylor expansion and integration-by-parts to estimate the integral part in Eq.(\ref{25}) and obtain the stochastic sampling algorithm for data prediction (see details in Algorithm \ref{alg:sde-d-1} and \ref{alg:sde-d-2}). 

% \textbf{\ourmethod-SDE-d-2.} Given a final state $\boldsymbol{x}_T$, a Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_i}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see details in Algorithm \ref{alg:sde-d-2}):
% \begin{equation}
% \begin{aligned}
% &\boldsymbol{x}_{t_i}=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}e^{-h_i}\boldsymbol{x}_{t_{i-1}}
% +\boldsymbol\mu\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}e^{-2h_i}-\alpha_{t_i}+\alpha_{t_i}e^{-2h_i}\right)+\sigma_{t_i}\sqrt{1-e^{-2h_i}}\boldsymbol{z}_i\\
% &+\alpha_{t_i}\left(1-e^{-2h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})
% +\alpha_{t_i}\left(h_i-\frac{1-e^{-2h_i}}{2}\right)\frac{\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}.
% \label{26}
% \end{aligned}
% \end{equation}

\subsection{Solutions to Mean Reverting ODEs with Data Prediction}

By substituting Eq.(\ref{23}) into Eq.(\ref{16}), we can obtain the following ODE parameterized by data prediction. 
\begin{equation}
\frac{\mathrm{d}\boldsymbol{x}}{\mathrm{d}t}=\left(\frac{g^2(t)}{2\sigma^2_t}-f(t)\right)\boldsymbol{x}+\left[f(t)-\frac{g^2(t)}{2\sigma_t^2}(1-\alpha_t)\right]\boldsymbol\mu-\frac{g^2(t)}{2\sigma_t^2}\alpha_t\boldsymbol{x}_\theta(\boldsymbol{x}_t,t).
\label{27}
\end{equation}
The incorporation of the parameter $\boldsymbol{\mu}$ does not disrupt the semi-linear structure of the equation with respect to $\boldsymbol{x}$, and $\boldsymbol{\mu}$ is not coupled to the neural network. This implies that analytical part of solutions can still be derived concerning both $\boldsymbol{x}$ and $\boldsymbol{\mu}$. We present the solution below (see Appendix \ref{appa1} for a detailed derivation).

\textbf{Proposition 4.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{27}) is 
\begin{equation}
\boldsymbol{x}_t=\frac{\sigma_t}{\sigma_s}\boldsymbol{x}_s
+\boldsymbol\mu\left(1-\frac{\sigma_t}{\sigma_s}+\frac{\sigma_t}{\sigma_s}\alpha_s-\alpha_t \right)
+\sigma_t\int_{\lambda_s}^{\lambda_t}e^{\lambda}\boldsymbol{x}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda.
\label{28}
\end{equation}
Similarly, only the neural network component requires approximation through the exponential integrator method \citep{hochbruck2005explicit,hochbruck2010exponential}. And we can obtain the deterministic sampling algorithm for data prediction (see Algorithm \ref{alg:ode-d-1} and \ref{alg:ode-d-2} for details).

% \textbf{\ourmethod-ODE-d-2.} Given a final state $\boldsymbol{x}_T$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_{i}}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see complete algorithm in Algorithm \ref{alg:ode-d-2}):
% \begin{equation}
% \begin{aligned}
% \boldsymbol{x}_{t_i}&=\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
% +\boldsymbol\mu\left(1-\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}+\frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\alpha_{t_{i-1}}-\alpha_{t_i} \right)
% +\alpha_{t_i}\left(1-e^{-h_i}\right)\boldsymbol{x}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})\\
% &+\alpha_{t_i}\left(h_i-1+e^{-h_i}\right)\frac{\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol x_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}}.
% \label{29}
% \end{aligned}
% \end{equation}

\subsection{Transformation between three kinds of parameterizations}

There are three mainstream parameterization methods. \cite{ho2020ddpm} introduced a training objective based on noise prediction, while \cite{salimans2022progressive} proposed parameterization strategies for data and velocity prediction to keep network outputs stable under the variation of time or log-SNR. All three methods can be regarded as score matching approaches \citep{song2020sde,hyvarinen2005scorematch} with weighted coefficients. To ensure our proposed algorithm is compatible with these parameterization strategies, it is necessary to provide transformation formulas for each pairs among the three strategies.

The transformation formula between noise prediction and data prediction can be easily derived from Eq.(\ref{7}):
\begin{equation}
\begin{cases}
\boldsymbol{x}_\theta(t)=\frac{\boldsymbol{x}_t-(1-\alpha_t)\boldsymbol{\mu}-\sigma_t\boldsymbol{\epsilon}_\theta(t)}{\alpha_t},\\
\boldsymbol{\epsilon}_\theta(t)=\frac{\boldsymbol{x}_t-\alpha_t\boldsymbol{x}_\theta(t)-(1-\alpha_t)\boldsymbol{\mu}}{\sigma_t}.
\label{30}
\end{cases}
\end{equation}
For velocity prediction, we define $\phi_t:=\arctan(\frac{\sigma_t}{\sigma_\infty\alpha_t})$, which is slightly different from the definition of \cite{salimans2022progressive}. Then we have $\alpha_t=\cos{\phi_t}$, $\sigma_t=\sigma_\infty\sin{\phi_t}$ and hence $\boldsymbol{x}_t=\boldsymbol{x}_0\cos{\phi_t}+\boldsymbol{\mu}(1-\cos{\phi_t})+\sigma_\infty\sin{(\phi_t)}\boldsymbol{\epsilon}$. And the definition of $\boldsymbol{v}(t)$ is
\begin{equation}
\boldsymbol{v}_t=\frac{\mathrm{d}\boldsymbol{x}_t}{\mathrm{d}\phi_t}
=\boldsymbol{\mu}\sin\phi_t-\boldsymbol{x}_0\sin\phi_t+\sigma_\infty\cos(\phi_t)\boldsymbol\epsilon.
\label{31}
\end{equation}
If we have a score function model $\boldsymbol{v}_\theta(t)$ trained with velocity prediction, we can obtain $\boldsymbol{x}_\theta(t)$ and $\boldsymbol{\epsilon}_\theta(t)$ by (see Appendix \ref{appa3} for detailed derivations)
\begin{align}
\boldsymbol{x}_\theta(t)&=\boldsymbol{x}_t\cos\phi_t+\boldsymbol{\mu}(1-\cos\phi_t)-\boldsymbol{v}_\theta(t)\sin\phi_t, \label{32}\\
\boldsymbol\epsilon_\theta(t)&=(\boldsymbol{v}_\theta(t)\cos\phi_t+\boldsymbol{x}_t\sin\phi_t-\boldsymbol\mu\sin\phi_t)/\sigma_\infty. \label{33}
\end{align}