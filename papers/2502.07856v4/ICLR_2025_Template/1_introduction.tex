\section{Introduction}

Diffusion models have emerged as a powerful class of generative models, demonstrating remarkable capabilities across a variety of applications, including image synthesis \citep{dhariwal2021diffusion,ruiz2023dreambooth,rombach2022high} and video generation \citep{ho2022imagen,ho2022video}.  
In these applications, controllable generation is very important in practice, but it also poses considerable challenges. Various methods have been proposed to incorporate text or image conditions into the score function of diffusion models\citep{ho2022cfg,ye2023ipadapter,zhang2023controlnet}, whereas Mean Reverting (MR) Diffusion offers a new avenue of control in the generation process \citep{luo2023mrsde}. Previous diffusion models (such as DDPM \citep{ho2020ddpm}) simulate a diffusion process that gradually transforms data into pure Gaussian noise, followed by learning to reverse this process for sample generation \citep{song2020improved,song2021maximum}. In contrast, MR Diffusion is designed to produce final states that follow a Gaussian distribution with a non-zero mean, which provides a simple and natural way to introduce image conditions. This characteristic makes MR Diffusion particularly suitable for solving inverse problems and potentially extensible to multi-modal conditions. However, the sampling process of MR Diffusion requires hundreds of iterative steps, which is time-consuming.


To improve the sampling efficiency of diffusion models, various acceleration strategies have been proposed, which can be divided into two categories. The first explores methods that establish direct mappings between starting and ending points on the sampling trajectory, enabling acceleration through knowledge distillation \citep{salimans2022progressive,song2023consistency,liu2022flow}. However, such algorithms often come with trade-offs, such as the need for extensive training and limitations in their adaptability across different tasks and datasets. The second category involves the design of fast numerical solvers that increase step sizes while controlling truncation errors, thus allowing for faster convergence to solutions \citep{lu2022dpmsolver,zhang2022deis,song2020ddim}. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth, trim=0 0 0 0]{figs/intro.pdf} %trim左下右上
    \caption{\textbf{Qualitative comparisons between \ourmethod~and Posterior Sampling.} All images are generated by sampling from a pre-trained MR Diffusion \citep{luo2024daclip} on the RESIDE-6k \citep{qin2020hazydata} dataset and the CelebA-HQ \citep{karras2017celebaHQ} dataset.}
    \label{fig:intro}
\end{figure*}



Notably, fast sampling solvers mentioned above are designed for common SDEs such as VPSDE and VESDE \citep{song2020sde}. Due to the difference between these SDEs and MRSDE, existing training-free fast samplers cannot be directly applied to Mean Reverting (MR) Diffusion. In this paper, we propose a novel algorithm named MaRS (\ourmethod) that improves the sampling efficiency of MR Diffusion. Specifically, we solve the reverse-time stochastic differential equation (SDE) and probability flow ordinary differential equation (PF-ODE) \citep{song2020sde} derived from MRSDE, and obtain a semi-analytical solution, which consists of an analytical function and an integral parameterized by neural networks. We prove that the difference of MRSDE only leads to change in analytical part of solution, which can be calculated precisely. And the integral part can be estimated by discretization methods developed in several previous works \citep{lu2022dpmsolver,zhang2022deis,zhao2024unipc}. We derive sampling formulas for two types of neural network parameterizations: noise prediction \citep{ho2020ddpm,song2020sde} and data prediction \citep{salimans2022progressive}. Through theoretical analysis and experimental validation, we demonstrate that data prediction exhibits superior numerical stability compared to noise prediction. Additionally, we propose transformation methods for velocity prediction networks \citep{salimans2022progressive} so that our algorithm supports all common training objectives. Extensive experiments show that our fast sampler converges in 5 or 10 NFEs with high sampling quality. As illustrated in Figure \ref{fig:intro}, our algorithm achieves stable performance with speedup factors ranging from 10 to 20.



In summary, our main contributions are as follows:
\begin{itemize}
    \item We propose \textit{\ourmethod}, a fast sampling algorithm for MR Diffusion, based on solving the PF-ODE and SDE derived from MRSDE. Our algorithm is plug-and-play and can adapt to all common training objectives.
    \item We demonstrate that posterior sampling \citep{luo2024posterior} for MR Diffusion is equivalent to Euler-Maruyama discretization, whereas \ourmethod~computes a semi-analytical solution, thereby eliminating part of approximation errors.
    \item Through extensive experiments on ten image restoration tasks, we demonstrate that \ourmethod~can reduce the required sampling time by a factor of 10 to 20 with comparable sampling quality. Moreover, we reveal that data prediction exhibits superior numerical stability compared to noise prediction.
\end{itemize}