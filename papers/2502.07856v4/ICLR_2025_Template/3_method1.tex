\section{Fast Samplers for Mean Reverting Diffusion with Noise Prediction}
\label{section3}

According to \cite{song2020sde}, the states $\boldsymbol{x}_t$ in the sampling procedure of diffusion models correspond to solutions of reverse-time SDE and PF-ODE. Therefore, we look for ways to accelerate sampling by studying these solutions. In this section, we solve the noise-prediction-based reverse-time SDE and PF-ODE, and we numerically estimate the non-closed-form component of the solution, which serves to accelerate the sampling process of MR diffusion models. Next, we analyze the sampling method currently used by MR Diffusion and demonstrate that this method corresponds to a variant of discretization for the reverse-time MRSDE.

\subsection{Solutions to Mean Reverting SDEs with Noise Prediction}
\label{section3.1}

\cite{ho2020ddpm} reported that score matching can be simplified to predicting noise, and \cite{song2020sde} revealed the connection between score function and noise prediction models, which is
\begin{equation}
    \nabla_{\boldsymbol{x}_t}\log p(\boldsymbol{x}_t|\boldsymbol{x}_0)=-\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,\boldsymbol{\mu},t)}{\sigma_t},
    \label{9}
\end{equation}
where $\sigma_t=\sigma_\infty\sqrt{1-e^{-2\int_0^t f(\tau)\mathrm{d}\tau}}$ is the standard deviation of the transition distribution $p(\boldsymbol{x}_t|\boldsymbol{x}_0)$. Because $\boldsymbol\mu$ is independent of $t$ and $\boldsymbol{x}$, we substitute $\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,\boldsymbol{\mu},t)$ with $\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)$ for notation simplicity. According to Eq.(\ref{9}), we can rewrite Eq.(\ref{8}) as
\begin{equation}
    \mathrm{d}\boldsymbol{x}=\left[f(t)\left(\boldsymbol{\mu}-\boldsymbol{x}\right)+\frac{g^2(t)}{\sigma_t}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t)\right]\mathrm{d}t+g(t)\mathrm{d}\bar{\boldsymbol{w}}.
    \label{10}
\end{equation}
Using Itô's formula (in the differential form), we can obtain the following semi-analytical solution:

\textbf{Proposition 1.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{10}) is 
\begin{equation}
    \boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s
    +\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}+\alpha_t\int_s^tg^2(\tau)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)}{\alpha_{\tau}\sigma_{\tau}}\mathrm{d}\tau
    +\sqrt{-\int_s^t\frac{\alpha_t^2}{\alpha_\tau^2}g^2(\tau)\mathrm{d}\tau}\boldsymbol{z},
    \label{11}
\end{equation}
where we denote $\alpha_t:=e^{-\int_0^tf(\tau)\mathrm{d}\tau}$ and $\boldsymbol{z}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{I})$. The proof is in Appendix \ref{appa1}.

However, the integral with respect to neural network output is still complicated. There have been several methods \citep{lu2022dpmsolver,zhang2022deis,zhao2024unipc} to estimate the integral numerically. We follow \cite{lu2022dpmsolverplus}'s method and introduce the half log-SNR $\lambda_t:=\log({\alpha_t/\sigma_t})$. Since both $f(t)$ and $g(t)$ are deliberately designed to ensure that $\alpha_t$ is monotonically decreasing over $t$ and $\sigma_t$ is monotonically increasing over $t$. Thus, $\lambda_t$ is a strictly decreasing function of $t$ and there exists an inverse function $t(\lambda)$. Then we can rewrite $g(\tau)$ in Eq.(\ref{11}) as
\begin{equation}
\begin{aligned}
g^2(\tau)&=2\sigma_\infty^2f(\tau)
=2f(\tau)(\sigma^2_{\tau}+\sigma_\infty^2\alpha_\tau^2)
=2\sigma^2_{\tau}(f(\tau)+\frac{f(\tau)\sigma_\infty^2\alpha_\tau^2}{\sigma^2_{\tau}})\\
&=2\sigma^2_{\tau}(f(\tau)+\frac{1}{2\sigma^2_{\tau}}\frac{\mathrm{d}\sigma^2_{\tau}}{\mathrm{d}\tau})
=-2\sigma^2_{\tau}\frac{\mathrm{d}\lambda_\tau}{\mathrm{d}\tau}.
\label{12}
\end{aligned}
\end{equation}
By substituting Eq.(\ref{12}) into Eq.(\ref{11}), we obtain
\begin{equation}
    \boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s
    +\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}-2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-\lambda}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\lambda,\lambda)\mathrm{d}\lambda +\sigma_t\sqrt{(e^{2(\lambda_t-\lambda_s)}-1)}\boldsymbol{z},
\label{13}
\end{equation}
where $\boldsymbol{x}_{\lambda}:=\boldsymbol{x}_{t(\lambda_\tau)},\;\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\lambda,\lambda):=\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t(\lambda_\tau)},t(\lambda_\tau))$. According to the methods of exponential integrators \citep{hochbruck2010exponential,hochbruck2005explicit}, the $(k-1)$-th order Taylor expansion of $\boldsymbol{\epsilon}_\theta(x_\lambda,\lambda)$ and integration-by-parts of the integral part in Eq.(\ref{13}) yields 
\begin{equation}
-2\alpha_t\int_{\lambda_s}^{\lambda_t}e^{-\lambda}\boldsymbol{\epsilon}_\theta(x_\lambda,\lambda)\mathrm{d}\lambda=
-2\sigma_{t}\sum_{n=0}^{k-1}\left[\boldsymbol{\epsilon}_\theta^{(n)}(\boldsymbol{x}_{\lambda_{s}},\lambda_{s})\left(e^{h}-\sum_{m=0}^n\frac{(h)^m}{m!}\right)\right]
+\mathcal{O}(h^{k+1}),
\label{14}
\end{equation}
where $h:=\lambda_t-\lambda_s$. We drop the discretization error term $\mathcal{O}(h^{k+1})$ and estimate the derivatives with \textit{backward difference method}. We name this algorithm as \textit{\ourmethod-SDE-n-k}, where \textit{n} means noise prediction and \textit{k} is the order. We present details in Algorithm \ref{alg:sde-n-1} and \ref{alg:sde-n-2}.

% Then, we take $k=2$ for demonstration.

% \textbf{\ourmethod-SDE-n-2.} Given a final state $\boldsymbol{x}_T$, a Gaussian noise sequence $\{\boldsymbol{z}_i|\boldsymbol{z}_i\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})\}_{i=1}^M$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_i}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see details in Algorithm \ref{alg:sde-n-2}):
% \begin{equation}
% \begin{aligned}
% \boldsymbol{x}_{t_{i}}&=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
% +\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}+\sigma_{t_i}\sqrt{e^{2h_i}-1}\boldsymbol{z}_i\\
% &-2\sigma_{t_i}\left[(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})+(e^{h_i}-1-h_i)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}} \right].
% \end{aligned}
% \end{equation}

\subsection{Solutions to Mean Reverting ODEs with Noise Prediction}

\cite{song2020sde} have illustrated that for any Itô SDE, there exists a \textit{probability flow} ODE, sharing the same marginal distribution $p_t(\boldsymbol{x})$ as a reverse-time SDE. Therefore, the solutions of PF-ODEs are also helpful in acceleration of sampling. Specifically, the PF-ODE corresponding to Eq.(\ref{10}) is
\begin{equation}
\frac{\mathrm{d}\boldsymbol{x}}{\mathrm{d}t}=f(t)\left(\boldsymbol{\mu}-\boldsymbol{x}\right)+\frac{g^2(t)}{2\sigma_t}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_t,t).
\label{16}
\end{equation}
The aforementioned equation exhibits a semi-linear structure with respect to $\boldsymbol{x}$, thus permitting resolution through the method of "variation of constants". We can draw the following conclusions:

\textbf{Proposition 2.} Given an initial value $\boldsymbol{x}_s$ at time $s\in[0,T]$, the solution $\boldsymbol{x}_t$ at time $t\in[0,s]$ of Eq.(\ref{16}) is 
\begin{equation}
\boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s+\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}+\alpha_t\int_s^t
\frac{g^2(\tau)}{2\alpha_\tau\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\mathrm{d}\tau \label{17},
\end{equation}
where $\alpha_t:=e^{-\int_0^tf(\tau)\mathrm{d}\tau}$. The proof is in Appendix \ref{appa1}.

Then we follow the variable substitution and Eq.(\ref{12}-\ref{14}) in Section \ref{section3.1}, and we obtain
\begin{equation}
\boldsymbol{x}_t=\frac{\alpha_t}{\alpha_s}\boldsymbol{x}_s+\left(1-\frac{\alpha_t}{\alpha_s}\right)\boldsymbol{\mu}-\sigma_{t}\sum_{n=0}^{k-1}\left[\boldsymbol{\epsilon}_\theta^{(n)}(\boldsymbol{x}_{\lambda_{s}},\lambda_{s})\left(e^{h}-\sum_{m=0}^n\frac{(h)^m}{m!}\right)\right]
+\mathcal{O}(h^{k+1}),
\label{18}
\end{equation}
where $\boldsymbol{\epsilon}_\theta^{(n)}(\boldsymbol{x}_\lambda,\lambda):=\frac{\mathrm{d}^{n}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{\lambda},\lambda)}{\mathrm{d}\lambda^n}$ is the $n$-th order total derivatives of $\boldsymbol{\epsilon}_\theta$ with respect to $\lambda$. By dropping the discretization error term $\mathcal{O}(h^{k+1})$ and estimating the derivatives of $\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{\lambda_{s}},\lambda_{s})$ with \textit{backward difference method}, we design the sampling algorithm from the perspective of ODE (see Algorithm \ref{alg:ode-n-1} and \ref{alg:ode-n-2}). 

% And we present the situation for $k=2$.

% \textbf{\ourmethod-ODE-n-2.} Given a final state $\boldsymbol{x}_T$ and a decreasing time sequence $\{t_i\}_{i=0}^M$. Denote $h_i=\lambda_{t_{i}}-\lambda_{t_{i-1}}$. Starting with $\boldsymbol{x}_{t_0}=\boldsymbol{x}_T$, the sequence $\{\boldsymbol{x}_{t_i}\}_{i=1}^M$ can be computed iteratively as follows (see complete algorithm in Algorithm \ref{alg:ode-n-2}):
% \begin{equation}
% \begin{aligned}
% &\boldsymbol{x}_{t_{i}}=\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\boldsymbol{x}_{t_{i-1}}
% +\left(1-\frac{\alpha_{t_i}}{\alpha_{t_{i-1}}}\right)\boldsymbol{\mu}\\
% &-\sigma_{t_i}\left[(e^{h_i}-1)\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})+(e^{h_i}-1-h_i)\frac{\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-1}},t_{i-1})-\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_{t_{i-2}},t_{i-2})}{h_{i-1}} \right].
% \end{aligned}
% \end{equation}

\subsection{Posterior Sampling for Mean Reverting Diffusion Models}
\label{section3.3}

In order to improve the sampling process of Mean Reverting Diffusion, \cite{luo2024posterior} proposed the \textit{posterior sampling} algorithm. They define a monotonically increasing time series $\{t_i\}_{i=0}^T$ and the reverse process as a Markov chain:
\begin{equation}
    p(\boldsymbol{x}_{1:T}\mid \boldsymbol{x}_0)=p(\boldsymbol{x}_T\mid \boldsymbol{x}_0)\prod_{i=2}^Tp(\boldsymbol{x}_{i-1}\mid \boldsymbol{x}_i,\boldsymbol{x}_0)\;
    \text{and }\boldsymbol{x}_T\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}),
\end{equation}
where we denote $\boldsymbol{x}_i:=\boldsymbol{x}_{t_i}$ for simplicity. They obtain an optimal posterior distribution by minimizing the negative log-likelihood, which is a Gaussian distribution given by
\begin{equation}
\begin{aligned}
    p(\boldsymbol{x}_{i-1}\mid \boldsymbol{x}_{i},\boldsymbol{x}_0)&=\mathcal{N}(\boldsymbol{x}_{i-1}\mid\tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_0), \tilde{\beta}_{i}\boldsymbol{I}),\\
    \tilde{\boldsymbol\mu}_{i}(\boldsymbol{x}_{i},\boldsymbol{x}_{0})&=\frac{(1-\alpha^2_{i-1})\alpha_{i}}{(1-\alpha^2_{i})\alpha_{i-1}}(\boldsymbol{x}_{i}-\boldsymbol\mu)+\frac{1-\frac{\alpha^2_{i}}{\alpha^2_{i-1}}}{1-\alpha^2_{i}}\alpha_{i-1}(\boldsymbol{x}_{0}-\boldsymbol\mu)+\boldsymbol\mu,\\
    \tilde{\beta}_{i}&=\frac{(1-\alpha^2_{i-1})(1-\frac{\alpha^2_{i}}{\alpha^2_{i-1}})}{1-\alpha^2_{i}},
    \label{19}
\end{aligned}
\end{equation}
where $\alpha_{i}=e^{-\int_{0}^{i}f(\tau)\mathrm{d}\tau}$ and $\boldsymbol{x}_0=\left(\boldsymbol{x}_{i}-\boldsymbol\mu-\sigma_i{\boldsymbol\epsilon}_\theta(\boldsymbol{x}_i,\boldsymbol\mu,t_i)\right)/\alpha_{i}+\boldsymbol\mu$. Actually, the reparameterization of posterior distribution in Eq.(\ref{19}) is equivalent to a variant of the Euler-Maruyama discretization of the reverse-time SDE (see details in Appendix \ref{appa2}). Specifically, the Euler-Maruyama method computes the solution in the following form:
\begin{equation}
\boldsymbol{x}_t=\boldsymbol{x}_s+\int_s^t \left[f(\tau)\left(\boldsymbol{\mu}-\boldsymbol{x}_\tau\right)+\frac{g^2(\tau)}{\sigma_\tau}\boldsymbol{\epsilon}_\theta(\boldsymbol{x}_\tau,\tau)\right]\mathrm{d}\tau
+\int_s^tg(\tau)\mathrm{d}\bar{\boldsymbol{w}}_\tau,
\end{equation}
which introduces approximation errors from both the analytical term and the non-linear component associated with neural network predictions. In contrast, our approach delivers an exact solution for the analytical part, leading to reduced approximation errors and a higher order of convergence.
