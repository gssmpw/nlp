\section{Experiments}
\label{section5}

In this section, we conduct extensive experiments to show that \ourmethod~can significantly speed up the sampling of existing MR Diffusion. To rigorously validate the effectiveness of our method, we follow the settings and checkpoints from \cite{luo2024daclip} and only modify the sampling part. Our experiment is divided into three parts. Section \ref{mainresult} compares the sampling results for different NFE cases. Section \ref{effects} studies the effects of different parameter settings on our algorithm, including network parameterizations and solver types. In Section \ref{analysis}, we visualize the sampling trajectories to show the speedup achieved by \ourmethod~and analyze why noise prediction gets obviously worse when NFE is less than 20.


\subsection{Main results}\label{mainresult}

Following \cite{luo2024daclip}, we conduct experiments with ten different types of image degradation: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy, shadowed, snowy, and inpainting (see Appendix \ref{appd1} for details). We adopt LPIPS \citep{zhang2018lpips} and FID \citep{heusel2017fid} as main metrics for perceptual evaluation, and also report PSNR and SSIM \citep{wang2004ssim} for reference. We compare \ourmethod~with other sampling methods, including posterior sampling \citep{luo2024posterior} and Euler-Maruyama discretization \citep{kloeden1992sde}. We take two tasks as examples and the metrics are shown in Figure \ref{fig:main}. Unless explicitly mentioned, we always use \ourmethod~based on SDE solver, with data prediction and uniform $\lambda$. The complete experimental results can be found in Appendix \ref{appd3}. The results demonstrate that \ourmethod~converges in a few (5 or 10) steps and produces samples with stable quality. Our algorithm significantly reduces the time cost without compromising sampling performance, which is of great practical value for MR Diffusion.


\begin{figure}[!ht]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth, trim=0 20 0 0]{figs/main_result/7_lowlight_fid.pdf}
        \subcaption{FID on \textit{low-light} dataset}
        \label{fig:main(a)}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth, trim=0 20 0 0]{figs/main_result/7_lowlight_lpips.pdf}
        \subcaption{LPIPS on \textit{low-light} dataset}
        \label{fig:main(b)}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth, trim=0 20 0 0]{figs/main_result/10_motion_fid.pdf}
        \subcaption{FID on \textit{motion-blurry} dataset}
        \label{fig:main(c)}
    \end{minipage}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth, trim=0 20 0 0]{figs/main_result/10_motion_lpips.pdf}
        \subcaption{LPIPS on \textit{motion-blurry} dataset}
        \label{fig:main(d)}
    \end{minipage}
    \caption{\textbf{Perceptual evaluations on \textit{low-light} and \textit{motion-blurry} datasets.}}
    \label{fig:main}
\end{figure}

\subsection{Effects of parameter choice}\label{effects}

In Table \ref{tab:ablat_param}, we compare the results of two network parameterizations. The data prediction shows stable performance across different NFEs. The noise prediction performs similarly to data prediction with large NFEs, but its performance deteriorates significantly with smaller NFEs. The detailed analysis can be found in Section \ref{section5.3}. In Table \ref{tab:ablat_solver}, we compare \ourmethod-ODE-d-2 and \ourmethod-SDE-d-2 on the \textit{inpainting} task, which are derived from PF-ODE and reverse-time SDE respectively. SDE-based solver works better with a large NFE, whereas ODE-based solver is more effective with a small NFE. In general, neither solver type is inherently better.


% In Table \ref{tab:hazy}, we study the impact of two step size schedules on the results. On the whole, uniform $\lambda$ performs slightly better than uniform $t$. Our algorithm follows the method of \cite{lu2022dpmsolverplus} to estimate the integral part of the solution, while the analytical part does not affect the error.  Consequently, our algorithm has the same global truncation error, that is $\mathcal{O}\left(h_{max}^{k}\right)$. Note that the initial and final values of $\lambda$ depend on noise schedule and are fixed. Therefore, uniform $\lambda$ scheduling leads to the smallest $h_{max}$ and works better.

\begin{table}[ht]
    \centering
    \begin{minipage}{0.5\textwidth}
    \small
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{Ablation study of network parameterizations on the Rain100H dataset.}
    % \vspace{8pt}
    \resizebox{1\textwidth}{!}{
        \begin{tabular}{cccccc}
			\toprule[1.5pt]
            % \multicolumn{6}{c}{Rainy} \\
            % \cmidrule(lr){1-6}
             NFE & Parameterization      & LPIPS\textdownarrow & FID\textdownarrow &  PSNR\textuparrow & SSIM\textuparrow  \\
            \midrule[1pt]
            \multirow{2}{*}{50}
             & Noise Prediction & \textbf{0.0606}     & \textbf{27.28}   & \textbf{28.89}     & \textbf{0.8615}    \\
             & Data Prediction & 0.0620     & 27.65   & 28.85     & 0.8602    \\
            \cmidrule(lr){1-6}
            \multirow{2}{*}{20}
              & Noise Prediction & 0.1429     & 47.31   & 27.68     & 0.7954    \\
              & Data Prediction & \textbf{0.0635}     & \textbf{27.79}   & \textbf{28.60}     & \textbf{0.8559}    \\
            \cmidrule(lr){1-6}
            \multirow{2}{*}{10}
              & Noise Prediction & 1.376     & 402.3   & 6.623     & 0.0114    \\
              & Data Prediction & \textbf{0.0678}     & \textbf{29.54}   & \textbf{28.09}     & \textbf{0.8483}    \\
            \cmidrule(lr){1-6}
            \multirow{2}{*}{5}
              & Noise Prediction & 1.416     & 447.0   & 5.755     & 0.0051    \\
              & Data Prediction & \textbf{0.0637}     & \textbf{26.92}   & \textbf{28.82}     & \textbf{0.8685}    \\       
            \bottomrule[1.5pt]
        \end{tabular}}
        \label{tab:ablat_param}
    \end{minipage}
    \hspace{0.01\textwidth}
    \begin{minipage}{0.46\textwidth}
    \small
    \renewcommand{\arraystretch}{1}
    \centering
    \caption{Ablation study of solver types on the CelebA-HQ dataset.}
    % \vspace{8pt}
        \resizebox{1\textwidth}{!}{
        \begin{tabular}{cccccc}
			\toprule[1.5pt]
            % \multicolumn{6}{c}{Raindrop} \\     
            % \cmidrule(lr){1-6}
             NFE & Solver Type     & LPIPS\textdownarrow & FID\textdownarrow &  PSNR\textuparrow & SSIM\textuparrow  \\
            \midrule[1pt]
            \multirow{2}{*}{50}
             & ODE & 0.0499     & 22.91   & 28.49     & 0.8921    \\
             & SDE & \textbf{0.0402}     & \textbf{19.09}   & \textbf{29.15}     & \textbf{0.9046}    \\
            \cmidrule(lr){1-6}
            \multirow{2}{*}{20}
              & ODE & 0.0475    & 21.35   & 28.51     & 0.8940    \\
              & SDE & \textbf{0.0408}     & \textbf{19.13}   & \textbf{28.98}    & \textbf{0.9032}    \\
            \cmidrule(lr){1-6}
            \multirow{2}{*}{10}
              & ODE & \textbf{0.0417}    & 19.44   & \textbf{28.94}     & \textbf{0.9048}    \\
              & SDE & 0.0437     & \textbf{19.29}   & 28.48     & 0.8996    \\
            \cmidrule(lr){1-6}
            \multirow{2}{*}{5}
              & ODE & \textbf{0.0526}     & 27.44   & \textbf{31.02}     & \textbf{0.9335}    \\
              & SDE & 0.0529    & \textbf{24.02}   & 28.35     & 0.8930    \\
            \bottomrule[1.5pt]
        \end{tabular}}
        \label{tab:ablat_solver}
    \end{minipage}
\end{table}


% \renewcommand{\arraystretch}{1}
%     \centering
%     \caption{Ablation study of step size schedule on the RESIDE-6k dataset.}
%     % \vspace{8pt}
%         \resizebox{1\textwidth}{!}{
%         \begin{tabular}{cccccc}
% 			\toprule[1.5pt]
%             % \multicolumn{6}{c}{Raindrop} \\     
%             % \cmidrule(lr){1-6}
%              NFE & Schedule      & LPIPS\textdownarrow & FID\textdownarrow &  PSNR\textuparrow & SSIM\textuparrow  \\
%             \midrule[1pt]
%             \multirow{2}{*}{50}
%              & uniform $t$ & 0.0271     & 5.539   & 30.00     & 0.9351    \\
%              & uniform $\lambda$ & \textbf{0.0233}     & \textbf{4.993}   & \textbf{30.19}     & \textbf{0.9427}    \\
%             \cmidrule(lr){1-6}
%             \multirow{2}{*}{20}
%               & uniform $t$ & 0.0313     & 6.000   & 29.73     & 0.9270    \\
%               & uniform $\lambda$ & \textbf{0.0240}     & \textbf{5.077}   & \textbf{30.06}    & \textbf{0.9409}    \\
%             \cmidrule(lr){1-6}
%             \multirow{2}{*}{10}
%               & uniform $t$ & 0.0309     & 6.094   & 29.42     & 0.9274    \\
%               & uniform $\lambda$ & \textbf{0.0246}     & \textbf{5.228}   & \textbf{29.65}     & \textbf{0.9372}    \\
%             \cmidrule(lr){1-6}
%             \multirow{2}{*}{5}
%               & uniform $t$ & 0.0256     & 5.477   & \textbf{29.91}     & 0.9342    \\
%               & uniform $\lambda$ & \textbf{0.0228}     & \textbf{5.174}   & 29.65     & \textbf{0.9416}    \\
%             \bottomrule[1.5pt]
%         \end{tabular}}
%         \label{tab:ablat_schedule}



\subsection{Analysis}\label{analysis}
\label{section5.3}

\begin{figure}[ht!]
    \centering
    \begin{minipage}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=0 20 10 0]{figs/trajectory_a.pdf} %trim左下右上
        \subcaption{Sampling results.}
        \label{fig:traj(a)}
    \end{minipage}
    \begin{minipage}[t]{0.35\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=0 0 0 0]{figs/trajectory_b.pdf} %trim左下右上
        \subcaption{Trajectory.}
        \label{fig:traj(b)}
    \end{minipage}
    \caption{\textbf{Sampling trajectories.} In (a), we compare our method (with order 1 and order 2) and previous sampling methods (i.e., posterior sampling and Euler discretization) on a motion blurry image. The numbers in parentheses indicate the NFE. In (b), we illustrate trajectories of each sampling method. Previous methods need to take many unnecessary paths to converge. With few NFEs, they fail to reach the ground truth (i.e., the location of $\boldsymbol{x}_0$). Our methods follow a more direct trajectory.}
    \label{fig:traj}
\end{figure}

\textbf{Sampling trajectory.}~ Inspired by the design idea of NCSN \citep{song2019ncsn}, we provide a new perspective of diffusion sampling process. \cite{song2019ncsn} consider each data point (e.g., an image) as a point in high-dimensional space. During the diffusion process, noise is added to each point $\boldsymbol{x}_0$, causing it to spread throughout the space, while the score function (a neural network) \textit{remembers} the direction towards $\boldsymbol{x}_0$. In the sampling process, we start from a random point by sampling a Gaussian distribution and follow the guidance of the reverse-time SDE (or PF-ODE) and the score function to locate $\boldsymbol{x}_0$. By connecting each intermediate state $\boldsymbol{x}_t$, we obtain a sampling trajectory. However, this trajectory exists in a high-dimensional space, making it difficult to visualize. Therefore, we use Principal Component Analysis (PCA) to reduce $\boldsymbol{x}_t$ to two dimensions, obtaining the projection of the sampling trajectory in 2D space. As shown in Figure \ref{fig:traj}, we present an example. Previous sampling methods \citep{luo2024posterior} often require a long path to find $\boldsymbol{x}_0$, and reducing NFE can lead to cumulative errors, making it impossible to locate $\boldsymbol{x}_0$. In contrast, our algorithm produces more direct trajectories, allowing us to find $\boldsymbol{x}_0$ with fewer NFEs.

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=0 0 0 0]{figs/convergence_a.pdf} %trim左下右上
        \subcaption{Sampling results.}
        \label{fig:convergence(a)}
    \end{minipage}
    \begin{minipage}[t]{0.43\linewidth}
        \centering
        \includegraphics[width=\linewidth, trim=0 20 0 0]{figs/convergence_b.pdf} %trim左下右上
        \subcaption{Ratio of convergence.}
        \label{fig:convergence(b)}
    \end{minipage}
    \caption{\textbf{Convergence of noise prediction and data prediction.} In (a), we choose a low-light image for example. The numbers in parentheses indicate the NFE. In (b), we illustrate the ratio of components of neural network output that satisfy the Taylor expansion convergence requirement.}
    \label{fig:converge}
\end{figure*}

\textbf{Numerical stability of parameterizations.}~ From Table 1, we observe poor sampling results for noise prediction in the case of few NFEs. The reason may be that the neural network parameterized by noise prediction is numerically unstable. Recall that we used Taylor expansion in Eq.(\ref{14}), and the condition for the equality to hold is $|\lambda-\lambda_s|<\boldsymbol{R}(s)$. And the radius of convergence $\boldsymbol{R}(t)$ can be calculated by
\begin{equation}
\frac{1}{\boldsymbol{R}(t)}=\lim_{n\rightarrow\infty}\left|\frac{\boldsymbol{c}_{n+1}(t)}{\boldsymbol{c}_n(t)}\right|,
\end{equation}
where $\boldsymbol{c}_n(t)$ is the coefficient of the $n$-th term in Taylor expansion. We are unable to compute this limit and can only compute the $n=0$ case as an approximation. The output of the neural network can be viewed as a vector, with each component corresponding to a radius of convergence. At each time step, we count the ratio of components that satisfy $\boldsymbol{R}_i(s)>|\lambda-\lambda_s|$ as a criterion for judging the convergence, where $i$ denotes the $i$-th component. As shown in Figure \ref{fig:converge}, the neural network parameterized by data prediction meets the convergence criteria at almost every step. However, the neural network parameterized by noise prediction always has components that cannot converge, which will lead to large errors and failed sampling. Therefore, data prediction has better numerical stability and is a more recommended choice.

