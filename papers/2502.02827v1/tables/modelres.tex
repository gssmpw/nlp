\begin{table}[t]
    \centering
    \caption{The correctness and time efficiency of code solutions generated by LLMs in Table~\ref{tab:models} on \bench. Efficienct@1 and pass@1 are calculated upon all instances in \bench, and speedup is calculated on correct solutions generated by models. Models highlighted in \colorbox{mygray}{gray} are closed-source models. ``$\Delta$'' indicates the difference of efficient@1 and pass@1 in percentage (100\% - efficient@1 / pass@1).}
    \scalebox{0.8}{
    \begin{tabular}{cc|ccc|ccc}
    \toprule
        \multirow{2}*{\textbf{Model}}  & \multirow{2}*{\textbf{Size}} & \multicolumn{3}{c|}{\textbf{Function-level}} & \multicolumn{3}{c}{\textbf{File-level}} \\
        \cmidrule{3-5}
        \cmidrule{6-8}
        & & \textbf{Efficient@1 ($\Delta$)} & \textbf{Speedup} & \textbf{Pass@1} & \textbf{Efficient@1 ($\Delta$)} & \textbf{Speedup} & \textbf{Pass@1} \\
    \midrule
        Phi3 & 3.8B & 26.65 (39\%) & 2.59 & 43.47 & 7.36 (67\%) & 0.08 & 22.63 \\
    \midrule
        \multirow{2}*{MagicCoder} & DS-6.7B & 21.90 (32\%) & 3.04 & 32.41  & 12.02 (48\%) & 0.10 & 22.91 \\
        & CL-7B & 29.82 (36\%) & 3.41 & 46.48 & 5.04 (68\%) & 0.14 & 15.92 \\
    \midrule
        \multirow{3}*{CodeLlama} & 7B & 26.65 (31\%) & 2.49  & 38.69 & 4.26 (51\%) & 0.95 & 8.66 \\
        & 13B & 25.60 (39\%) & 1.03 &41.71  & 1.16 (48\%) & 1.02 & 2.23 \\
        & 34B & 40.37 (38\%) & 3.51 & 64.74 & 22.87 (57\%) & 0.09 & 53.63 \\
    \midrule
        \multirow{2}*{Llama3} & 8B & 27.70 (35\%)  & 3.91 & 42.46 & 0.00 (100\%) & 0.21 & 0.84\\
        & 70B & 40.90 (39\%) & 3.30 & 67.59 & 38.76 (44\%) & 0.14 & 68.99 \\
    \midrule
        StarCoder & 15B & 38.52 (37\%)  & 3.52 & 61.31 & 21.71 (58\%) & 0.10 & 51.11 \\
    \midrule
        WizardCoder & 15B & 28.76 (41\%) & 1.95 & 48.49 & 10.08 (51\%) & 0.07 & 20.67 \\
    \midrule
        Mixtral & 8$\times$7B & 25.59 (43\%) & 5.14 & 44.72 & 8.53 (63\%) & \textbf{1.43} & 22.91\\
    \midrule
        DeepSeek V2 & 236B & 46.70 (40\%) & 2.79 & 78.39 & 41.09 (54\%) & 0.18 & 89.94 \\
    \midrule
        DeepSeek V2 Coder & 236B &  \textbf{46.97} (41\%) & 2.53 & \textbf{79.90}  & 42.25 (46\%) & 0.44 & 78.77 \\
    \midrule
        Llama3.1 & 405B & 39.58 (41\%) & 3.21 & 67.34 & \textbf{46.51} (48\%) & 0.90 & 89.11 \\
    \midrule
        \g Claude 3.5 Sonnet & \g - & \g 43.54 (44\%) & \g 4.90 & \g 77.64 & \g 39.15 (55\%) & \g 0.23 & \g 86.59  \\
    \midrule
        \g Gemini 1.5 Pro & \g - & \g 45.12 (40\%) & \g 1.76 & \g 75.38 & \g 42.64 (43\%) & \g 0.16 & \g 75.44 \\
    \midrule
        \g ChatGPT & \g - & \g 37.73 (45\%) & \g 2.46 & \g 68.19 & \g 39.15 (48\%) & \g 0.12 & \g 75.98\\
    \midrule
        \g GPT-4o & \g - & \g 44.59 (43\%) & \g \textbf{8.28} & \g 77.64 & \g 43.02 (53\%)  & \g 1.11 & \g \textbf{90.78} \\
    \bottomrule
    \end{tabular}}
    
    \label{tab:modelres}
\end{table}