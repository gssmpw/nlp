\begin{table}[t]
    \centering
    \caption{The LLMs we evaluate in this paper. Models highlighted in \colorbox{mygray}{gray} are closed-source models.}
    \scalebox{0.8}{
    \begin{tabular}{ccc|ccc}
    \toprule
        \textbf{Model} & \textbf{Size} & \textbf{Context Size} & \textbf{Model} & \textbf{Size} & \textbf{Context Size}\\
    \midrule
        Phi3~\cite{phi3} & 3.8B & 128k & MagicCoder~\cite{magiccoder} & DS-6.7B/CL-7B & 16,384\\
        CodeLlama~\cite{codellama} & 7B/13B/34B & 16,384 &
        Llama3~\cite{llama3} & 8B/70B & 4,096 \\
        StarCoder~\cite{starcoder} & 15B & 16,384 & WizardCoder~\cite{wizardcoder} & 15B & 2,048 \\
        Mixtral~\cite{mixtral} & 8$\times$7B & 32,768 &
        DeepSeek V2~\cite{deepseekv2} & 236B & 128k \\
        DeepSeek Coder V2~\cite{deepseekcoderv2}& 236B & 128k &
        Llama3.1~\cite{llama31} & 405B & 4,096 \\
        \g Claude 3.5 Sonnet~\cite{claude} & \g - & \g 200k & \g Gemini 1.5 Pro~\cite{gemini} & \g - & \g 200k \\
        \g GPT-3.5~\cite{mixtral} & \g - & \g 16,385 & \g GPT-4o~\cite{gpt4o} & \g - & \g 128k \\
    \bottomrule
    \end{tabular}}
    \label{tab:models}
\end{table}