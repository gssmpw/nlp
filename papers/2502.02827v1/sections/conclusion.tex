\section{Conclusion}\label{sec:conclusion}

In this paper, we propose a new benchmark \bench for the time efficiency evaluation of LLM-generated code. To address the challenges of existing correctness code generation benchmarks, we propose a novel stressful test case generation method \tool that incorporates contracts and two test case formats to improve the accuracy. We also introduce a new time efficiency metric \textit{efficient@k} based on CPU instruction count that stably evaluates both the correctness and time efficiency of code. Based on \bench, we evaluate 14 popular LLMs and identify four important findings. We provide implications based on the findings for LLM researchers and software practitioners.