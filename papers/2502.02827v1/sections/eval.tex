\section{Experiment Results}\label{sec:eval}





\subsection{RQ1: CPU Instruction Count vs. Execution Time}

To demonstrate that CPU instruction count is more suitable for time efficiency evaluation than execution time, we focus on two aspects: stability, which evaluates how solid the measurement is, and correlation, which evaluates how close two measurements are.

\textbf{Stability.} To compare the stability of CPU instruction count and execution time measurements, we run the ground truth solutions of the validated problems on the correctness test cases from the four correctness benchmarks. Note that we do not run them on our stressful test cases to ensure a fair comparison since CPU instruction count is involved in building \bench. We run each solution 12 times and remove the largest and smallest measurements. We then calculate the RSD of the remaining 10 measurements and show the results in the second and third columns of Table~\ref{tab:metric}.

As the experiments are repeated on the same ground truth solution and same test cases, a lower relative standard deviation indicates a more stable measurement. From Table~\ref{tab:metric}, we can observe that execution time has an RSD of about 5\% on function-level benchmarks HumanEval and MBPP and an RSD of about 2\% on file-level benchmarks Code Contests and APPS. On the contrary, CPU instruction count has a more than 1000$\times$ smaller RSD (0.003\%$\sim$0.005\%) than execution time on four benchmarks. This indicates that the ten measurements of CPU instruction count almost remain the same on the same program, and CPU instruction count is quite stable in measuring time efficiency.

\textbf{Correlation.} To validate the linear correlation between CPU instruction count and execution time, as described in Equation~\ref{eq:cic}, we calculate the Pearson correlation coefficient between CPU instruction count and execution time, as shown in the last column of Table~\ref{tab:metric}. We find that the correlations of the two measurements on all benchmarks are very close to 1.0. This indicates that the two measurements are linearly correlated and verifies the correctness of Equation~\ref{eq:cic} as the other two factors \textit{Clock per Instruction} and \textit{Clock Cycle Time} do not change in the same testing environment. Therefore, we can replace execution time with CPU instruction count to measure time efficiency. 

\input{tables/metric}


\answer{1}{CPU instruction count is more suitable to evaluate time efficiency since it is much more stable than execution time by achieving a 1000$\times$ smaller RSD of 0.003\%$\sim$0.005\%, and it is linearly correlated with execution time with Pearson correlation coefficient of 0.96$\sim$1.0.}

\subsection{RQ2: Effectiveness of \tool and Distinguishability of Stressful Test Cases}
To answer RQ2, we study the effectiveness of \tool on stressful test case generation compared with three widely used LLM-based test case generation baselines. For the generated stressful test cases, we evaluate whether they can better distinguish different code solutions generated by LLMs. We show the main results of the comparison between \tool and baselines in Table~\ref{tab:testcase}.

\input{tables/testcase}

\textbf{Effectiveness of \tool.} To study how contracts can improve the accuracy of test cases, we compare \tool with three baselines without contracts and report the accuracy at the third column of Table~\ref{tab:testcase} for function-level and file-level splits of \bench. We do not report the accuracy of the original test cases because they are manually drafted. From the table, we observe that \tool achieves an accuracy of 98.64\% and 98.91\%, outperforming the baselines by up to 17.89\% and 51.77\% in function-level and file-level splits, respectively. This suggests that almost all stressful test cases generated by \tool are correct. Without knowledge enrichment and early validation by contracts, on the contrary, baselines fail to generate about 5\%$\sim$35\% of stressful test cases. 

Apart from the accuracy, a correct test case is representative if it can cover most lines of the target program. To ensure the quality of generated stressful test cases, we evaluate the line coverage and report the results in the fourth column of Table~\ref{tab:testcase}. We find that \tool consistently achieves the highest line coverage of 96.01\% and 95.17\% for function-level and file-level stressful test cases, respectively. This demonstrates that the stressful test cases generated by \tool can thoroughly evaluate the time efficiency of the major code logic in target programs. We also note that the line coverage achieved by \tool is slightly lower than that achieved by the original correctness test cases. This is reasonable because the stressful test cases are much fewer than the correctness test cases in \bench, as can be seen in Table~\ref{tab:benchmark}.



\textbf{Distinguishability of Stressful Test Cases.} To evaluate how well the stressful test cases generated by \tool can distinguish the time efficiency of different code solutions, we sample 20 code solutions from two powerful LLMs, Llama3.1 and GPT-4o, for each problem in \bench. We then run the sampled solutions on different test cases and collect the CPU instruction count usage. We calculate the RSD on the CPU instruction counts of the sampled 20 code solutions, and a higher RSD indicates better distinguishability. We report the RSD on the code solutions of two models at the fifth and sixth columns of Table~\ref{tab:testcase}.  

Firstly, we observe that stressful test cases generated by \tool improve the RSD of original correctness test cases by 43.10\% and 32.08\% on Llama3.1 and GPT-4o, respectively, at the function level, and the improvements are 11.89\% and 13.86\% on Llama3.1 and GPT-4o, respectively, at the file level. \tool also outperforms all three baselines in terms of RSD on both Llama3.1 and GPT-4o. This demonstrates that stressful test cases generated by \tool can better distinguish different code solutions than original correctness test cases and stressful test cases generated by baselines. Secondly, we find that the generator-based prompting method achieves higher RSD than other baselines. This verifies the effectiveness of generator test cases compared with raw test cases in time efficiency evaluation. However, the generator-based prompting method cannot well handle multiple parameters in function-level programs by achieving an accuracy of only 86.91\%. \tool mitigates this problem by generating expression test cases that follow the formats of raw test cases but introduce small expressions to represent each input. As a result, the expression test cases generated by \tool for function-level code solutions outperform the generator-based prompting method by 19.25\% and 32.27\% in terms of RSD on Llama3.1 and GPT-4o, respectively.

 

\answer{2}{With knowledge enrichment and early validation by contracts, \tool is quite effective in generating correct stressful test cases with an accuracy of about 99\% and line coverage of about 96\%. The expression and generator test cases generated by \tool can better distinguish different code solutions' time efficiency with an RSD of up to 28.20\% on GPT-4o.}



\subsection{RQ3: Time Efficiency of Code Generated by LLMs}
Based on \bench, we evaluate the time efficiency of code generated by different LLMs. We select ten popular open-source LLMs and four popular closed-source LLMs, as shown in Table~\ref{tab:models}. We show the Pass@1, efficient@1, and speedup of all LLMs on \bench in Table~\ref{tab:modelres}.

\input{tables/modelres}

\textbf{Overall Time Efficiency.} 
To evaluate the time efficiency of code generated by different models, we study the efficient@1 and speedup. We use efficient@1 to evaluate the probability of an LLM to generate a correct code solution faster than the best ground truth solution and speedup to evaluate how fast the correctly generated code solutions are compared with the best ground truth solutions. From Table~\ref{tab:modelres}, we identify that DeepSeek V2 Coder obtains the highest efficient@1 of 46.97\% at the function level and Llama3.1 obtains the highest efficient@1 of 46.51\% at the file level. As for the speedup, GPT-4o achieves the highest speedup of 8.28 at the function level, and Mixtral obtains the highest speedup of 1.43 at the file level.

\finding{1}{DeepSeek V2 Coder and Llama3.1 have the highest probability of generating efficient code solutions with an efficient@1 of 46.97\% and 46.51\%, respectively. GPT-4o and Mixtral generate the most efficient code solutions with a speedup of 8.28 and 1.43, respectively.}

\textbf{Correctness vs. Time Efficiency}
When comparing the correctness and time efficiency of code solutions generated by current LLMs, we find that the best efficient@1 are 46.97\% and 46.51\%, at function level and file level, respectively, which are much lower than the best pass@1 of 79.90\% and 90.78\%. This indicates that almost half of the correctly generated code solutions are sub-optimal since they are less efficient than ground truth solutions. Furthermore, the speedups achieved by most LLMs in file-level code generation are lower than 1.0, and some LLMs even obtain a speedup of lower than 0.1, indicating their generated code solutions are $10\times$ slower than ground truth solutions. This suggests that efficient code generation is a great challenge for current LLMs despite their remarkable performance on correct code generation.

\finding{2}{The performance of current LLMs drops significantly in efficient code generation with the best efficient@1 of 46.97\% and 46.51\% at the function level and file level, compared with that in correct code generation with the best Pass@1 of 79.9\% and 90.78\%. This indicates that the code solutions generated by current LLMs are correct but not time-efficient.}

\textbf{Function-level Code Generation vs. File-level Code Generation.} 
In function-level code generation, we observe that all LLMs achieve a speedup larger than 1.0, indicating that the code solutions generated by current LLMs are more efficient than ground truths. However, only three LLMs achieve a speedup larger than 1.0 in the file-level code generation. This suggests that current LLMs cannot generate faster code solutions than existing solutions in \bench. Besides, the efficient@1 achieved by current LLMs at the function level is also better than that achieved by current LLMs at the file level. For example, the efficient@1 of Phi3 drops by 72.38\% from function-level to file-level code generation. Furthermore, the performance drop from pass@1 to efficient@1 in function-level code generation is 30\%$\sim$45\%, smaller than 40\%$\sim$100\% in file-level code generation. This indicates that current LLMs perform much worse in file-level efficient code generation than function-level efficient code generation.

\finding{3}{Compared with function-level code generation, code solutions generated by current LLMs are less efficient in file-level code generation, evidenced by the significantly lower speedup, lower efficient@, and larger performance drop from pass@1 to efficient@1.}




\textbf{Impacts of Different Model Sizes.}
To study the impacts of different model sizes on the time efficiency of code solutions generated by current LLMs, we observe the changes of pass@1 and efficient@1 from smaller LLMs to larger LLMs. In both function-level and file-level code generation, we find that larger LLMs can generally generate more correct solutions, evidenced by higher Pass@1 obtained by larger LLMs. However, larger LLMs do not always generate more efficient code solutions. For example, in function-level code generation, CodeLlama-34b achieves an efficient@1 of 40.37\%, which is quite close to the efficient@1 of 40.90\% achieved by Llama3-70b and 39.58\% achieved by Llama3.1-405b, but Llama3.1-405b is more than $10\times$ larger than CodeLlama-34b. In file-level code generation, Llama3-70b achieves an efficient@1 of 38.76\%, which is also quite close to the efficient@1 of 41.09\% achieved by DeepSeek V2, and 42.25\% achieved by Deep Seek V2 Coder, but DeepSeek V2 is more than $3\times$ larger than Llama3-70b. 

\finding{4}{Larger LLMs generally perform better in terms of Pass@1 but do not significantly outperform smaller LLMs in terms of efficient@1, indicating larger parameter sizes of current LLMs do not contribute much to efficient code generation.}


In summary, based on the experiment results of 14 popular LLMs on \bench, we study the time efficiency of function-level and file-level code solutions generated by the LLMs in four aspects. We find efficient code generation much more challenging for current LLMs than correct code generation, especially for file-level efficient code generation. We also identify that larger LLMs do not always perform better on efficient code generation.
