\section{Methodology}\label{sec:meth}

This section describes how we build the benchmark \bench, including selecting the coding problems, proposing \tool to generate stressful test cases for function-level and file-level code generation, and designing a novel time efficiency metric efficient@k.

\subsection{Data Preparation}\label{sec:data}


To construct \bench, we collect problems in the test splits of two existing function-level correctness benchmarks (i.e., HumanEval~\cite{humaneval} and MBPP~\cite{mbpp}), and two existing file-level correctness benchmarks (i.e., APPS~\cite{apps} and CodeContests~\cite{codecontests}). Each benchmark contains multiple coding problems and provides each problem with a description that explains the requirements in natural language, several ground truth solutions that address the problem, and several test cases that evaluate the correctness of generated code solutions. As there are multiple versions for MBPP, we choose the common subset of the sanitized version~\cite{mbppsantized} and the MBPP+ benchmark verified by EvalPlus~\cite{mbppplus} as our base benchmark to ensure the highest quality. 

With the selected benchmarks, we first validate the problems by checking the potential conflicts of provided test cases and ground truth solutions. Secondly, we select problems that most LLMs could correctly answer to reduce the difficulty of problems for the two file-level benchmarks since a problem is not useful in time efficiency evaluation if no LLM can answer it. We show the statistics of four benchmarks in Table~\ref{tab:datasets}.

\input{tables/datasets}

\subsubsection{Problem Validation}
To ensure the quality of test cases and ground truth solutions in the four benchmarks, we run the ground truth solutions in the provided test cases and remove 1) ground truth solutions that cannot pass the provided test cases to ensure consistency, 2) ground truth solutions with file operations to keep safety, and 3) problems without valid ground truth solutions and test cases. We show the number of validated problems in each benchmark in the third column of Table~\ref{tab:datasets}. All problems in HumanEval and MBPP can be successfully validated, so no problem is removed. For the Code Contests benchmark, we identify five problems with file operations, and we remove them to guarantee the safety of testing environments. For the APPS benchmark, we identify 1,894 problems whose ground truth solutions conflict with the provided test cases. The reason for such conflicts is that the APPS benchmark does not require the output of a code solution to exactly match the expected outputs in test cases to be correct, which differs from the other three benchmarks. We remove the 1,894 problems without exact matches in the APPS benchmark to maintain consistent evaluation standards.

\subsubsection{Problem Selection}
Current LLMs are quite effective in function-level code generation by achieving a pass@1 of more than 80\% in the HumanEval benchmark, as discussed in Sec.~\ref{sec:intro}. However, they perform much worse in file-level code generation since the most powerful LLM has a Pass@1 of 28.5\% on the Code Contests benchmark and a Pass@1 of less than 10\% on the APPS benchmark~\cite{codecontestleaderboard,appspsc}. This limits the usage of the full set of the Code Contests and APPS benchmarks because a problem that no LLM can correctly answer does not contribute to the time efficiency evaluation.
Therefore, for the validated problems in the two benchmarks, we sample one code solution with temperature 0 on 14 LLMs used in our experiments described in Table~\ref{tab:models} and remove 48 and 2,223 problems that code solutions from all LLMs failed in the Code Contests and APPS benchmark, respectively. To balance the number of problems in the function-level split and file-level split of \bench, we further select 300 problems in the APPS benchmark for which more LLMs can generate correct code solutions. We show the number of selected problems from the four benchmarks and associated statistics in the 4$\sim$6 columns of Table~\ref{tab:datasets}.



\subsection{Stressful Test Case Generation: \tool}
\begin{figure*}[tbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/overview.pdf}
    \vspace{-20pt}
    \caption{The overview workflow of \tool.} 
    \label{fig: overview}
    \vspace{-10pt}
\end{figure*}

With the selected problems, we propose a novel LLM-based approach \tool to generate stressful test cases automatically. In contrast to current LLM-based test case generation methods~\cite{liu2023is, li24large,ouÃ©draogo24large,bhatia23unit,max24an,sami24a}, \tool aims to generate test cases to evaluate the time efficiency of code solutions under extreme conditions rigorously. This inherently requires constructing exceptionally long and intricate inputs that can hardly be handled by LLMs directly, leading to unsatisfactory accuracy, i.e., the proportion of correctly generated stressful test cases is low.


\subsubsection{Overview}

To improve the accuracy of stressful test case generation, \tool introduces contracts to guide the test case generation and validate the generated test cases. Contracts are collections of assertion statements that record the type, scale, and internal constraints between the inputs. Providing contracts in the test case generation process can help LLMs understand the dependencies between test inputs. Besides, \tool can easily identify incorrect test cases from the assertion errors contracts raise. To avoid overlong stressful test cases that hinder the performance of LLMs, we design two new formats of test cases by reformulating the test case generation task into a code generation task: \textit{expression test cases} and \textit{generator test cases}. Different from raw test cases that directly provide test inputs, expression and generator test cases contain code to generate test inputs, which greatly shortens the length of test cases. 


We present the overview of \tool in Figure \ref{fig: overview}. \tool does not directly generate stressful test cases. Instead, it decomposes the task into three phases: 1) contract generation, 2) stressful test case generation, and 3) test case-contract pair check. In the first phase, \tool generates contracts by analyzing the target program, i.e., the ground truth solution for each problem in the benchmark. The generated contracts are then provided as demonstrations for stressful test case generation in the second phase, in which \tool generates expression and generator test cases instead of raw test cases. Since contracts are also generated and there is no guarantee of their correctness, \tool enters the third phase if the number of \textit{AssertionError} occurrences for a certain contract exceeds a threshold. In the third phase, \tool implements an LLM judge to determine the responsibility for conflicts between generated contracts and test cases. The contracts or test cases that are judged to be incorrect will be sent back for regeneration. This iterative process allows the generation of contracts and stressful test cases to mutually reinforce each other.

\subsubsection{Phase I: Contract Generation}\label{sec:contract}
In the first phase, \tool inserts assertion statements that check the preconditions of inputs as contracts into the target program, such as \textit{assert n > 0}. The contracts ensure that the inputs meet the required specifications in format (e.g., variable type), scale (e.g., input length, order of magnitude), and intrinsic constraints (e.g., right triangle side lengths).

The benefits of inserting contracts before stressful test case generation are twofold: 1) \textbf{Knowledge Enrichment.} Contracts explicitly indicate the functionality of the target program and the dependencies between inputs, which can help LLM better understand natural language descriptions provided in problems~\cite{liu2023is,endres2024can}; 2) \textbf{Early Validation.} Contracts can identify invalid inputs in test cases at the beginning of program execution and stop the execution-based test case validation process early, which largely improves the efficiency of the test case generation process.

In contract generation, \tool generates one assertion statement in an iteration and combines all assertion statements into a contract. When generating assertion statements, \tool prompts LLMs to consider the type, scale, and intrinsic constraints between inputs given the target program, existing correctness test cases, and previously generated assertion statements as demonstrations. \tool implements the same methodology to generate assertion statements for function-level and file-level target programs. However, \tool employs different strategies to insert assertion statements into target programs, given the differences between the code solutions in function-level and file-level code generation illustrated in Sec.~\ref{sec:mot}.

\textbf{Function-level Contract Insertion.} For function-level target programs with a determined number of inputs, \tool generates and inserts assertion statements for function parameters at the beginning of the function body. For example, \tool inserts assertion statements right before the return statements in the function \textit{add()} in Figure ~\ref{fig:probdef}(a). 


\textbf{File-level Contract Insertion.} For file-level target programs with an unknown number of inputs and multiple input locations, \tool reformulates the contract generation problem into a code editing problem. It first identifies all input locations by checking the related system APIs such as \textit{input()} and then inserts assertion statements for each identified input location sequentially. \tool inserts assertion statements right after the input locations in most cases. However, as input locations in loops generally assign values for generic types such as \textit{list} and \textit{dict}, \tool inserts assertion statements after the entire loop where the assignments are complete to check the fully assigned types. For example, \tool identifies two input locations highlighted in blue in Figure~\ref{fig:probdef}(b). \tool first generates assertion statements for the input that assigns values to variable $n$ and inserts them right after the assignment. \tool then generates assertion statements for the second input in the loop, and this time, it inserts assertion statements after the entire \textit{for} loop. 


To improve the correctness of generated assertion statements, \tool tests all generated assertion statements against the correctness test cases each time it inserts a new assertion statement. If the current assertion statement fails on the test cases, \tool only removes the current assertion statement and regenerates a new one while maintaining the assertion statements correctly generated in previous iterations. The iteration ends until no new assertion statements are generated or a maximum iteration number is reached.



\subsubsection{Phase II: Stressful Test Case Generation}
With the generated contracts as demonstrations, in the second phase, \tool generates stressful test cases. Unlike correctness test case generation, it is quite challenging to generate stressful test cases because LLMs must generate test cases of maximal length and complexity within the constraints of its finite context window while simultaneously ensuring adherence to intrinsic input constraints specified by contracts. Correctness test cases in current benchmarks~\cite{humaneval,mbpp,codecontests,apps} are raw test cases that directly provide the values for test inputs. However, due to the limited context window size, it is infeasible to directly generate overlong raw test cases for time efficiency evaluation. For example, it is hard for LLMs to generate a list with more than a million numbers for stressful tests. To address this challenge, we introduce two new formats of stressful test cases:


\textbf{Expression Test Cases.} Expression test cases utilize Python expressions to generate test cases, allowing for more complex input generation while maintaining a compact representation within the LLM's context window. For instance, a list with a million numbers could be easily generated by an expression ``\textit{[random.randint(1, 100000) for \_ in range(1000000)]}'', which is much shorter than listing a million numbers. Expression test cases offer a balance between complexity and conciseness, enabling the creation of structured inputs. They are suitable for function-level test case generation with a determined number of test inputs. To evaluate code solutions on expression test cases, we just need to execute the expressions to get the real test inputs before the code execution. 



\textbf{Generator Test Cases.} Generator test cases are Python functions that output the test inputs. It is quite useful for creating stressful test cases that require intricate logical relationships or patterns that are difficult to express in single expressions. For example, it is suitable for file-level code generation where the number of inputs is undetermined. Expression test cases cannot handle this since we do not know how many expressions should be generated.

To generate expression and generator test cases, \tool prompts the LLMs with contract, verified generated test cases as demonstrations, so LLMs can learn the dependencies between inputs as well as the specific formats of the expected test cases. The generated test cases are then verified against the previously generated contracts and the target program. Test cases that pass the validation of contracts and the execution of the target program are collected to build \bench. Verified stressful test cases are also used as demonstrations to help generate the following stressful test cases.

\subsubsection{Phase III: Test Case-Contract Pair Check}

Although the generated contracts are verified against the existing correctness test cases, correctness test cases do not cover all possible cases and dependencies among inputs, especially in stressful scenarios. Contracts can still make mistakes and induce false positives. During the test case validation, if a generated test case violates the inserted contract, it triggers an \textit{AssertionError}. If the \textit{AssertionError} consistently occurs for multiple test cases, the contract may be incorrect and thereby hinder the entire stressful test case generation procedure. To mitigate this, when the number of conflicts between contracts and test cases (i.e., \textit{AssertionError} occurrences during execution) exceeds a predefined threshold (5 in this paper), the generated test case and the violated contract are paired for further check by an LLM judge checker in the third phase.


The LLM judge takes all accumulated contract-related execution failure pairs as inputs, along with the target program, to analyze and determine the validity of the contracts and the test cases. The judge reviews the violated contract with exact stressful inputs, rethinks the correctness of the generated contract, and determines the root cause of conflicts. Once the root cause is identified, the relevant judgment results and corresponding failure pairs are sent back to the previous phases for regeneration. By providing feedback for incorrect contracts or test cases, \tool enhances the robustness of the test case validation and enables the improvements between contract generation and test case generation. To prevent duplicate judgments, once the LLM judge determines that a contract is valid in the third phase, it will not be checked again, and test cases that fail the validation of this contract will be directly rejected in the future.



\subsection{Time Efficiency Metric: Efficient@k}\label{sec:metric}

Previous work~\cite{shypula2024learning,effibench} intuitively adopts execution time as the performance measurement to evaluate the time efficiency of LLM-generated code. However, execution time measurements could be affected by many factors, such as process scheduling and disk I/O, so it is not stable enough to make a solid comparison between the time efficiency of different code solutions. In this section, we propose to use \textit{CPU instruction count} to replace execution time to measure the time efficiency of code solutions stably. Based on CPU instruction count measurements, we propose a new metric \textit{efficient@k} to evaluate both the correctness and time efficiency of code solutions.


\subsubsection{CPU Instruction Count} To find a more stable measurement to replace execution time, we first look into the factors contributing to the execution time. Patterson and Hennessy~\cite{arch} define the CPU time cost by a program through the following equation.
\begin{equation}\label{eq:cic}
    \text{CPU Time} = [\textbf{\text{Instruction Count}}] \times [\text{Clock per Instruction}] \times [\text{Clock Cycle Time}]
\end{equation}

From the equation, the CPU time of a program is determined by three factors. While \textit{Clock per Instruction} and \textit{Clock Cycle Time} depend on the physical machine where the program runs, the only factor related to the program is \textit{Instruction Count}. Therefore, if a program has a higher CPU instruction count on the same machine, it is less efficient, and vice versa. Unlike the execution time measurements that could be affected by many factors, CPU instruction count measurements are more stable as CPU instruction count for a program does not increase even if the program execution is slowed or stalled by external factors. It is also straightforward to measure CPU instruction count using the system APIs. For example, Linux provides a command tool named \textit{perf}~\cite{perf} to support CPU instruction count measurements.


\subsubsection{Efficient@k} CPU instruction count is a stable measurement for the time efficiency evaluation of different code solutions. However, its absolute value is not meaningful as the same code solution has different CPU instruction counts in different machines. Besides, it is not comprehensive as it does not measure the correctness of generated code solutions. To address these problems, we propose a new metric named \textit{Efficient@k}, inspired by the design of \textit{pass@k}~\cite{humaneval}. We show the original definition of pass@k in Equation~\ref{eq:pass} and the definition of proposed efficient@k in Equation~\ref{eq:eff}.

\begin{equation}\label{eq:pass}
    \text{pass@k}:=\underset{\text { Problems }}{\mathbb{E}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
\end{equation}

\begin{equation}\label{eq:eff}
    \text{efficient@k}:=\underset{\text { Problems }}{\mathbb{E}}\left[1-\frac{\binom{n-c_f}{k}}{\binom{n}{k}}\right]
\end{equation}

Pass@k is an expectation over all problems in the benchmark for the probability that at least one solution in $k$ samples can pass all test cases. In equation~\ref{eq:pass}, total $n$ solutions are sampled from LLMs instead of only $k$ samples to reduce the variance. By running the sampled code solutions on correctness test cases, we can get the solutions $c$ that can pass all the test cases to estimate the probability of correctness. Pass@k is a solid metric with low variance and can be easily reproduced under different platforms. 

We follow the idea of pass@k when designing efficient@k. Pass@k requires the correct code solutions $c$ to contribute, while in efficient@k, we collect the number $c_f$ of the correct solutions faster than the best ground truth solution to replace $c$ in pass@k. Therefore, efficient@k evaluates the probability of LLMs to generate correct and fast enough code solutions. Efficient@k compares the CPU instruction count of code solutions and ground truth solutions to determine which runs faster. By doing so, efficient@k does not consider the absolute values of CPU instruction counts to avoid the impacts of specific systems or machines. With a value range from 0 to pass@k, efficient@k combines correctness and time efficiency evaluation to comprehensively evaluate the quality of code solutions.


\subsection{Code Efficiency Benchmark: \bench}

\input{tables/benchmark}

With the stressful test case generation approach \tool, we add stressful test cases for each problem selected in Sec.~\ref{sec:data}. Specifically, we generate 20 stressful test cases for each problem and measure the CPU instruction count each test case costs. We conduct the measurements 12 times and remove the highest and lowest measurements before calculating the average to ensure the most stable results. In the CPU instruction measurements, we limit the execution time of one single measurement to five seconds so that the measurements for one test case will not exceed one minute. We then rank the average CPU instruction count of each test case and include the five test cases with the highest CPU instruction counts in \bench. We do not include all generated stressful test cases in \bench to avoid large time costs in time efficiency evaluation, since stressful test cases generally take much longer time than correctness test cases to execute. We reserve all existing correctness in \bench to validate the correctness of generated code solutions. We show the statistics of \bench in Table~\ref{tab:benchmark}.