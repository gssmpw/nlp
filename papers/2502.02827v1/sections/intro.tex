\section{Introduction}\label{sec:intro}

Nowadays, large language models (LLMs) such as GPT-4~\cite{gpt4} and Llama3.1~\cite{llama31} have demonstrated great ability to solve different software engineering tasks. With the ability to follow instructions~\cite{chung22scaling,wei22emergent,ouyang22training,Muennighoff24octopack}, LLMs can act like human developers, promptly handle the instructions and generate completed code, reviews, or comments. Code generation, which is tasked with
converting natural language instructions into executable code, 
%takes natural language instructions and outputs the required code, 
has the potential to significantly enhance the efficiency of software development.
%can largely improve the efficiency of software development. 
It is thus a critical software engineering problem being studied by many researchers. Researchers have proposed different approaches to make use of LLMs on code generation via prompting engineering~\cite{shinn23reflexion,zhou23language,chen23teaching,ni23lever,ridnik24code}, multiple-agent cooperation~\cite{huang23agentcoder,hong24metagpt,wang24executable,zhang24autocoderover,yang24sweagent,holt23l2mac}, and retrieval augmentation~\cite{liu21retrieval,parvez21retrieval,zhou23docpromting,zhang23repocoder,su24arks}. 

To facilitate the evaluation of code generation, many benchmarks such as HumanEval~\cite{humaneval}, MBPP~\cite{mbpp}, CodeContests~\cite{codecontests}, and APPS~\cite{apps} are proposed to evaluate the correctness of generated code solutions, and we refer them as correctness benchmarks. These benchmarks include coding tasks drafted by experienced developers~\cite{humaneval,mbpp} or collected from coding competitions~\cite{codecontests,apps}, with several test cases for each problem to examine the correctness of LLM-generated code solutions. With the correctness benchmarks, researchers can thoroughly study and further improve the ability of LLMs to generate correct code. Built upon current advanced techniques, powerful LLMs such as GPT-4 have obtained remarkable performance with the Pass@1 of 86.6\% on the function-level code generation benchmark HumanEval~\cite{humaneval}, reported by the EvalPlus leaderboard~\cite{evalplusboard}.

However, correctness benchmarks alone are insufficient to comprehensively evaluate LLMs' ability of code generation, especially when these models are increasingly used to generate code solutions for software products~\cite{yu2024where}.  In real-world software development, both correctness and time efficiency are crucial for ensuring software quality. Correct but time-inefficient code can lead to a lot of CWE issues~\cite{cwe}. Recent work~\cite{shypula2024learning,peng2024perfcodegen,liu2024evaluating,liu2024learning} on LLM-based code generation steps further to generate correct and efficient code. They directly adopt existing correctness benchmarks and measure the execution time of LLM-generated code solutions to determine the time efficiency. We argue that current correctness benchmarks are not suitable for time efficiency evaluation for the following challenges:
\begin{itemize}[wide=0pt]%[leftmargin=*]
    \item \textbf{Challenge 1: Existing correctness test cases cannot well distinguish the time efficiency of different code solutions.} Test cases in correctness benchmarks usually have small inputs since they aim to cover most corner cases to detect potential logical errors in code solutions. However, such test cases can hardly distinguish the time efficiency of different code solutions since code with different time complexities may cost similar time under small inputs. Therefore, it is necessary to include test cases with larger inputs so that we can better distinguish code solutions with different time efficiency. We refer to such test cases as stressful test cases. Stressful test case generation is not straightforward and cannot be easily handled by current correctness test case generation methods. Stressful test cases usually consume much more execution time, so traditional execution-based test case generation methods with many iterations of complete executions are too time-consuming to be adopted. LLM-based test case generation methods without execution can generate stressful test cases quickly, but they are limited by context windows and can hardly maintain the long inputs in results, threatening the accuracy of stressful test case generation.
    \item \textbf{Challenge 2: Execution time metric is unstable and not comprehensive for time efficiency evaluation.}  Unlike correctness evaluation, which can be easily repeated on any computer machine, execution time measurements highly rely on the machine where the experiments are conducted. Shypula \etal~\cite{shypula2024learning} find that two single time measurements of the code solution on the same environment can differ as much as 1.91$\times$. Unstable execution time measurements threaten the validity of time efficiency evaluation. Besides, previous work~\cite{shypula2024learning,effibench} regards time efficiency evaluation as independent of correctness evaluation for code generation, but time efficiency evaluation is conducted upon correctly generated code solutions. Using separate metrics to evaluate the correctness and time efficiency makes it hard to distinguish the quality of code solutions with high correctness but low time efficiency and those with high time efficiency but low correctness. Currently, there is no single metric evaluating both the correctness and time efficiency of LLM-generated code solutions.

\end{itemize}


To address the two challenges above in evaluating the time efficiency of LLM-generated code solutions, we \textbf{1) propose a new time efficiency benchmark named \bench, along with a novel approach \tool to generate stressful test cases automatically}. 
Specifically, \bench is built upon existing correctness benchmarks HumanEval~\cite{humaneval}, MBPP~\cite{mbpp} for function-level code generation and CodeContests~\cite{codecontests}, and APPS~\cite{apps} for file-level code generation by adding stressful test cases generated by \tool. Hence, it contains two splits for function-level and file-level code generation. \tool implements three phases to improve the accuracy of stressful test case generation. In the first phase, \tool generates contracts that record the dependencies between inputs, and contracts are then used to guide the test case generation in the second phase. An LLM judge checks conflicts between generated contracts and test cases and rejects incorrect test cases in the third phase. By validating test cases on contracts, \tool can identify incorrect test cases early and provide feedback for LLMs to help fix them. \tool also uses expressions and generator functions to replace the raw inputs in the stressful test cases to avoid overlong test cases that hinder the generation of LLMs. 
Furthermore, we \textbf{2) propose a new metric named \textit{efficient@k} that considers both correctness and time efficiency based on \textit{CPU instruction count} measurements.}. Efficient@k follows the same logic as pass@k~\cite{humaneval}, and the difference is that it requires a code solution to be correct and faster than the best ground truth solution to contribute. When comparing code solutions and ground truth solutions, we replace execution time with a more stable measurement \textit{CPU instruction count} to conduct a solid comparison.

Experiments demonstrate that \tool is quite effective in stressful test case generation by correctly generating approximately 99\% of test cases with a 96\% line coverage. Furthermore, 
To evaluate the effectiveness of stressful test cases generated by \tool, the stressful test cases generated by \tool can much better distinguish the time efficiency of code solutions by achieving the relative standard deviation (RSD) of 27.26\% and 17.60\% over different function-level and file-level code solutions generated by Llama3.1~\cite{llama31}, largely improving the RSD of 19.05\% and 15.73\% on the original correctness test cases. This indicates the high quality of \bench. To verify the stability of CPU instruction count, we compare it with execution time and find that CPU instruction count has a RSD of 0.003\%$\sim$0.005\%, which is 1,000$\times$ smaller than that of execution time measurement (2.37\%$\sim$5.65\%). This provides a solid basis for the calculation of efficient@k.

Based on \bench, we evaluate the time efficiency of code solutions generated by ten open-source LLMs and four closed-source LLMs and identify the following important findings:
\begin{itemize}
    \item The performance of current LLMs drops significantly in efficient code generation, indicating that the code solutions generated by current LLMs are correct but not time-efficient.
    \item Compared with function-level code generation, code solutions generated by current LLMs are less efficient in file-level code generation.
    \item Larger LLMs generally perform better in correct code generation but do not significantly outperform smaller LLMs in efficient code generation, indicating larger parameter sizes of current LLMs do not contribute much to efficient code generation.
\end{itemize}





We summarize the contributions of this paper as follows:
\begin{itemize}
    \item We build \bench, a benchmark for evaluating the time efficiency of both function-level and file-level code solutions generated by LLMs.
    \item We propose \tool, the first LLM-based stressful test case generation approach that employs contract validation and test cases with expression and generator functions inputs to improve accuracy.
    \item We introduce a novel metric efficient@k, based on stable CPU instruction count measurement, to evaluate the correctness and time efficiency of the LLM-generated code solutions.
    \item We conduct extensive experiments to evaluate the quality of \bench, the effectiveness of \tool, and the ability of current LLMs to generate efficient code.
\end{itemize}
