\section{Implications}

Based on the findings we conclude in Sec.~\ref{sec:eval}, we provide some implications for researchers who build LLMs and practitioners who use LLMs in software development.

\textbf{LLM Researchers.} We identify that there is a large gap between correct code generation and efficient code generation. This indicates that the current LLM-generated code is correct but sub-optimal, and generating efficient code remains a great challenge, especially for file-level code generation. This challenge cannot be effectively mitigated by just increasing the model size of current LLMs. We recommend that LLM researchers consider the code structure and semantics when improving the time efficiency of LLM-generated code. Besides, LLM researchers should also focus more on file-level code generation since current LLMs perform much worse on it than function-level code generation.



\textbf{Software Practitioners.}  As LLMs are gradually adopted in software development in product environments, software practitioners face the problem of choosing LLMs. In function-level and file-level code generation, generally, code solutions generated by DeepSeek V2 Coder and Llama3.1-405b obtain the best time efficiency, respectively. However, we also find that some LLMs with middle sizes, such as Llama3-70b and CodeLlama-34b, achieve competitive performance. We recommend software practitioners adopt middle-sized LLMs to obtain similar performance on efficient code generation with much lower computational costs.