\section{Threats to Validity}\label{sec:discussion}

Our research may face the following threats to the internal and external validity.


\subsection{Threats to Internal Validity}

\textbf{Performance Measurement.} The time efficiency measurement of code solutions generated by LLMs can introduce errors. We propose to use CPU instruction count instead of execution time to improve the stability of measurements. However, there still exist factors such as specific code optimization techniques that introduce measurement errors. To mitigate the threats posed by the errors in time efficiency measurements, we conduct all measurements in dockers~\cite{docker} to ensure that only one single process is running at the same time. Furthermore, we run the measurements for each code solution 12 times and remove the highest and lowest measurements before calculating the average metric. This could further reduce the errors introduced in a single measurement.

\textbf{Baseline Implementation.} Currently, there are no LLM-based stressful test case generation methods that could be compared with \tool, so we modify three correctness test case generation methods as our baselines. However, such modifications may result in performance changes. To improve the validity of baselines, we run them on the most powerful and robust LLM GPT-4o~\cite{gpt4o}. Besides, we ask the baselines to generate 20 stressful test cases once and only choose the best 5 test cases for most evaluations except for accuracy. Therefore, we believe our implementations can represent the best performance of baselines. 




\subsection{Threats to External Validity}

\textbf{Adaptation to Different Programming Languages.} While code generation is a general task for all programming languages, we mainly focus on the evaluation of Python code generation in this paper. The code generation performance of LLMs on other programming languages such as C++ and Java may be different from the experiment results we show in Sec.~\ref{sec:eval}, as it could be affected by the syntax and coding styles. This threatens the validity of our experiment results in other programming languages. However, Python is the top 2 most popular programming language at GitHub~\cite{octoverse} and is the major programming language used to build the code generation benchmarks~\cite{humaneval,mbpp,codecontests,apps,effibench,hai24repoexec,repobench,repoeval}. Besides, our stressful test case generation method \tool is language-agnostic and fully based on LLMs to generate stressful test cases, we believe it could be easily extended to build benchmarks for other programming languages.