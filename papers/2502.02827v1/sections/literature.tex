\section{Related Work}\label{sec:literature}

\subsection{LLMs for Code Generation}
As a critical task to automate the software development process, code generation has drawn a lot of attention in both the academia and industry. At the beginning, encoder-decoder models such as AlphaCode~\cite{codecontests}, CodeT5~\cite{codet5}, CodeRL~\cite{coderl}, CodeT5+~\cite{codet5plus} are directly trained on large code corpus and obtain good performance on code generation. Recently, decoder-only models such as Codex~\cite{codex}, CodeGen~\cite{codegen, codegen2}, InCoder~\cite{incoder}, CodeGeeX~\cite{humanevalx}, SantaCoder~\cite{santacoder}, StarCoder~\cite{starcoder,starcoder2}, WizardCoder~\cite{wizardcoder}, CodeLlama~\cite{codellama}, MagicCoder~\cite{magiccoder}, DeepSeek-Coder~\cite{deepseekcoder} show superior performance than encoder-decoder models on code generation. Besides, some general LLMs trained on multiple types of data, such as Llama3\cite{llama3}, Llama3.1~\cite{llama31}, GPT-3.5~\cite{chatgpt}, GPT-4~\cite{gpt4} also demonstrate competitive or even better performance compared with code LLMs.



\subsection{Code Generation Benchmarks}

\textbf{Correctness Benchmarks.} There are many benchmarks designed for the correctness evaluation of code generated by LLMs. They provide contexts that indicate the functionality of the generated code and several test cases to evaluate the correctness of the generated code. The benchmarks are initially built from scratch by skilled developers and researchers. HumanEval~\cite{humaneval} is a benchmark that contains 164 Python programming problems with function signatures and docstrings. MBPP~\cite{mbpp} is a benchmark consisting of 974 basic Python programming problems with short functionality descriptions. It also provides a sanitized version with verified ground truth solutions that have 427 problems. In order to comprehensively evaluate the performance of LLMs, some benchmarks are built from code competition problems. APPS~\cite{apps} contains 10,000 Python problems with different difficulty levels and diversified ground truth solutions for each problem. Code Contests~\cite{codecontests} is a multi-lingual benchmark built from various competition sources and includes both correct and incorrect human solutions for each problem. Apart from Code Contests, there are also other multi-lingual benchmarks such as xCodeEval~\cite{xcodeeval} and HumanEval-X~\cite{humanevalx}. The above-mentioned benchmarks focus on the evaluation of function-level or file-level code generation. There are some research efforts, such as RepoEval~\cite{repoeval}, RepoBench~\cite{repobench}, SWE-Bench~\cite{swebench}, and CrossCodeEval~\cite{crosscodeeval}, devoted to the evaluation of the repo-level code generation performance. 

\textbf{Time Efficiency Benchmarks.} Despite the well-explored evaluation for the correctness of code generated by LLMs, the time efficiency of code generated by LLMs is under-explored.  Effibench~\cite{effibench} is the first benchmark designed for evaluating the time and memory efficiency of code generation.  It selects efficiency-critical problems tagged ``LeetCode'' and prompts GPT-3.5 to generate test cases with different input sizes and data distribution. However, the problems in this benchmark are too difficult, so most open-source models cannot even generate correct solutions. Besides, it adopts execution time as the performance metric, which is unreliable to distinguish the efficiency of different code solutions. There is also some work~\cite{zapa09accuracy,laaber20dynamic,traini23towards} on traditional performance engineering, but they are not suitable for evaluating random responses from LLMs.

\subsection{LLM-based Test Case Generation}
Apart from the advances in code generation, LLMs have also been demonstrated to improve software testing~\cite{deng23large}. A lot of work has comprehensively evaluated the ability of LLMs on test case generation~\cite{li24large,ou√©draogo24large,sami24a,karanjai24harnessing,niels24code}. Most recently, Chen \etal~\cite{chen24chatunitest} propose ChatUniTest, a unit test generation framework
based on LLM by utilizing innovative mechanisms such
as adaptive focal context and generation-validation-repair mechanisms. Liu \etal~\cite{liu24llm} propose a novel LLM-powered test oracle generation approach that combines LLMs and differential testing. Hossain \etal~\cite{hossain24togll} propose TOGLL, a fine-tuned LLM on designed instruction prompts to generate test oracle for Java projects. Wang \etal~\cite{wang24testeval} propose TestEval to generate test cases that cover certain lines, branches, and paths of the code under test. Despite the effectiveness of previous approaches on correctness test case generation, there is no work on stressful test case generation that aims to generate large test inputs to evaluate the time efficiency of the code under test. In this paper, we propose a novel approach \tool to generate stressful test cases for Python projects with high accuracy and coverage.