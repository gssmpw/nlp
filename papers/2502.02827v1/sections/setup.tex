\section{Experiment Setup}\label{sec:setup}

\subsection{Research Questions}
We focus on the following research questions:
\begin{itemize}
    \item \textbf{RQ1:} How well does CPU instruction count measure time efficiency compared with execution time?
    \item \textbf{RQ2:} How effective is \tool on stressful test case generation and how well are the generated stressful test cases?
    \item \textbf{RQ3:} How efficient is the code generated by current LLMs?
\end{itemize}

\subsection{Metrics}
To evaluate the stability of CPU instruction count (RQ1), we introduce the following metrics:
\begin{itemize}
    \item \textbf{Relative Standard Deviation (RSD):} The ratio of the standard deviation to the mean. We use it to measure how stable a performance metric is on the same code solution (the lower, the better) and how well a test case can distinguish different code solutions (the higher, the better). We use ``RSD (-)'' when it is used to evaluate stability and ``RSD (+)'' when it is used to evaluate distinguishability.
    \item \textbf{Pearson Correlation Coefficient:} The ratio between the covariance of two variables and the product of their standard deviations. We use it to measure the linear correlation between two metrics.
\end{itemize}


To evaluate the quality of stressful test cases and the effectiveness of \tool (RQ2), we introduce the following metrics:
\begin{itemize}
    \item \textbf{Accuracy:} The proportion of test cases generated by a certain method where the target program does not fail.
    \item \textbf{Line Coverage:} The percentage of executed lines in solutions when executing the test cases.
\end{itemize}

To evaluate the efficiency of code solutions generated by LLMs (RQ3), we use the following metrics:
\begin{itemize}
    \item \textbf{Pass@k:} The probability that at least one of the top k-generated code samples for a problem passes the unit tests, as illustrated in Sec.~\ref{sec:metric}.
    \item \textbf{Speedup:} The ratio $\frac{gt}{o}$ of CPU instruction count of best ground truth solution $gt$ to the CPU instruction count of a code solution $o$.
    \item \textbf{Efficient@k:} The probability that at least one of the top k-generated code samples for a problem is correct and more efficient than the best ground truth solution, as introduced in Sec.~\ref{sec:metric}.
\end{itemize}


\subsection{Baselines}
Since there is no previous work on LLM-based stressful test case generation, we select three widely used LLM-based correctness test case generation methods and adapt them into stressful test case generation:
\begin{itemize}
    \item \textbf{Instruction Prompting~\cite{wang24testeval}.} Wang \etal design several instruction prompt templates to ask LLMs to cover certain lines, branches, or paths of the code in test case generation. We modified their instruction prompt and let LLMs focus on stressful test case generation. This method generates raw test cases.
    \item \textbf{Few-shot Prompting~\cite{ou√©draogo24large}.} Few-shot prompting adds several demonstrations to guide LLMs to generate similar test cases. This method generates raw test cases.
    \item \textbf{Generator-based Prompting~\cite{liu24llm}.} Instead of directly generating test cases, this method prompts LLMs to generate a function that derives the test cases. We adapt this method to our stressful test case generation and let LLMs generate functions that produce stressful test cases. This method generates generator test cases.
\end{itemize}



\subsection{Models}

To investigate the efficiency of code generated by current LLMs, we select 14 popular models for evaluation. We show the model names, sizes, and context lengths in Table~\ref{tab:models}. For GPT-3.5~\cite{chatgpt} and GPT-4o~\cite{gpt4o}, we use the APIs provided by OpenAI~\cite{openai} under engines ``\textit{gpt-3.5-turbo}'' and ``\textit{gpt-4o}'', respectively. For DeepSeek V2~\cite{deepseekv2} and DeepSeek V2 Coder~\cite{deepseekcoderv2}, we use the APIs provided by DeepSeek~\cite{deepseek} under engine ``\textit{DeepSeek-V2-0628}'' and ``\textit{DeepSeek-V2-0724}'', respectively. For Claude 3.5 Sonnect~\cite{claude}, we use the APIs provided by Anthropic~\cite{anthropic} under the engine ``\textit{claude-3-5-sonnet-20240620}''. For Gemini 1.5 Pro, we use the APIs provided by Google~\cite{googleapi} under the engine ``\textit{gemini-1.5-pro}'' to generate code solutions.   Due to limited computing resources, for open-source models larger than 13B, we use the API provided by Deep Infra~\cite{deepinfra} to generate code solutions. 

\input{tables/models}



\subsection{Implementation Details}

We conduct all experiments on a Linux machine with Ubuntu 20.04.4 LTS. It has an Intel(R) Xeon(R) Platinum 8358P CPU of 2.60G HZ with 128 cores and 2 TB memory. We use the Coverage.py~\cite{coverage} library to measure the line coverage of test cases, and the Cirron~\cite{cirron} library to measure the CPU instruction count a program consumes. 