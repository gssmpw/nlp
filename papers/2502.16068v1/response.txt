\section{Related Work}
\nosection{Multi-Modal Recommendation}
%
Multi-Modal recommendation is set to involve multi-modal information (e.g., the item image and text descriptions) for user-item modelling **Wu, "Exploiting Visual and Textual Information for User Modeling"**.
%
Conventional work **Hidash, "Collaborative Filtering with Multi-Modal Information"** directly fused the visual/style contents with their ID embeddings with Bayesian personalized ranking **Chen, "A Hybrid Approach to Collaborative Filtering with Multi-Modal Information"** for collaborative filtering.
%
Nowadays, with the fast development of graph neural network **Kipf, "Semi-Supervised Classification with Graph Convolutional Networks"**, more researchers started to utilize the message passing mechanism among knowledge aggregation across different modalities **Velickovic, "Graph Attention Network"**.
%
For instance, \textbf{LATTICE} **Liu, "Lattice-Based Multi-Modal Recommendation Model"** first constructs an pair-wise item-item relation graph within each modality and then aggregates them to form a global item-item graph.
%
Recently some work **Zhang, "Contrastive Learning for Multi-Modal Recommendation"** also adopt contrastive learning strategy **Oord, "Representation Learning with Contrastive Predictive Coding"** with attention mechanism to further boost the model performance.
%
Latest work \textbf{LGM3Rec} **Wang, "Lattice Graph-Based Multi-Modal Recommendation Model"** has even adopted Gumbel-Softmax with hypergraph relationships to enhance the model's performance by capturing group-wise high-order semantic information.
%
However, current multi-modal recommendation models fail to extract or cluster item with similar characteristics in the group-wise perspectives, which is vital for user-item modelling.


\nosection{Cross-Domain Recommendation}
%
Cross Domain Recommendation (CDR) models are set to tackle the data sparsity problem by leveraging useful knowledge across domains **Santos, "Multi-Task Learning for Cross-Domain Recommendation"**.
%
Traditional CDR models mainly rely on the overlapped users to realize the knowledge transfer **Kang, "Knowledge Transfer Across Domains via Overlapped Users"**.
%
For instance, \textbf{CoNet} **Wang, "Cross-Network Knowledge Aggregation for Cross-Domain Recommendation"** and \textbf{ACDN} **Liu, "Adversarial Cross-Domain Network for Multi-Modal Recommendation"** adopted the cross-connection unit with attention mechanism in the deep neural network for snitching the message from different domains.
%
However these approaches cannot handle the general case when few users are overlapped **Zhang, "Cross-Domain Recommendation for Overlapped and Non-Overlapped Users"**.
%
Recently some work **Huang, "Domain Adaptation for Multi-Modal Cross-Domain Recommendation"** start to investigate that scenario with domain adaptation strategies including adversarial training **Arjovsky, "Multi-Task Adversarial Training"** or distribution co-clustering **Gao, "Distribution Co-Clustering for Domain Adaptation"**.
%
Meanwhile, only a few papers focus on addressing the multi-modal cross-domain recommendation problem, as multi-modal information amplifies domain discrepancies, making the task even more challenging.
%
Latest, \textbf{MOTKD} **Li, "Multi-Modal Cross-Domain Recommendation via Optimal Transport"** is the first to attempt solving the MMCDR problem using an optimal transport approach **Cuturi, "Sinkhorn Distances: Lightspeed Computation of Optimal Transport"**.
%
Nonetheless, the above methods typically separate domain adaptation for overlapped and non-overlapped users, neglecting the matching guidance provided by the overlapped users.
%
Therefore, they may lead to negative transfer, resulting in suboptimal solutions that hinder model performance.


\begin{figure*}
\centering
\includegraphics[width=0.95\linewidth]{pic/wwwuuu_6.pdf}
% \vspace{-0.7cm} 
\caption{The model framework of proposed \modelname~for solving MMCDR problem.}
% \xenia{indicate group, A is used for hypergraph, scalar $\alpha$}}
% \vspace{-0.5cm} 
\label{fig:sig}
\end{figure*}