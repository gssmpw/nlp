\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{zhanggroup}

%-------------------------------------------------------------------------------
% Packages
\usepackage{booktabs}
\usepackage{url}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\captionsetup[subfigure]{labelformat = parens, labelsep = space, font = small}
\usepackage{balance}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% Macros
\renewcommand*{\sectionautorefname}{Section}
\renewcommand*{\subsectionautorefname}{Section}
\renewcommand*{\algorithmautorefname}{Algorithm}
\newcommand{\mypara}[1]{\smallskip\noindent{\bf {#1}.}}
\newcommand{\method}{\textbf{AP-Test}\xspace}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

\date{}

\title{\bf Peering Behind the Shield: Guardrail Identification\\ in Large Language Models}

\author{
Ziqing Yang\ \ \
Yixin Wu\ \ \
Rui Wen\ \ \
Michael Backes\ \ \
Yang Zhang
\\
\textit{CISPA Helmholtz Center for Information Security}
}

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
Human-AI conversations have gained increasing attention since the era of large language models.
Consequently, more techniques, such as input/output guardrails and safety alignment, are proposed to prevent potential misuse of such Human-AI conversations.
However, the ability to identify these guardrails has significant implications, both for adversarial exploitation and for auditing purposes by red team operators.
In this work, we propose a novel method, \method, which identifies the presence of a candidate guardrail by leveraging guardrail-specific adversarial prompts to query the AI agent.
Extensive experiments of four candidate guardrails under diverse scenarios showcase the effectiveness of our method.
The ablation study further illustrates the importance of the components we designed, such as the loss terms.
\end{abstract}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

Human-AI conversations have been significantly advanced by the rapid development of large language models (LLMs), owing to their exceptional capabilities in natural language understanding and generation~\cite{GPT4o, TLIMLLRGHARJGL23, claude}.
These conversational AI agents are now extensively deployed across various domains, including customer service~\cite{Deepseek_Chat,claude,chatgpt}, education~\cite{NYSDJ24}, and healthcare~\cite{PYCSPCMFZMLMOAHSGBW23}.

However, the widespread adoption of these applications has also brought about emerging security concerns, such as jailbreak attacks~\cite{SCBSZ24,ZWKF23,LXCX23,ZYPDLWW24} and prompt injection attacks~\cite{LJGJG24, ZLYK24, DZBBFT24,YWSJX23}.
Safety guardrails~\cite{Perspective,IUCRIMTHFTK23,CKZSRZPCUP24,HREJLLCD24} are considered an effective and efficient technique that can mitigate such risks via moderating the content from both users and AI agents.
As shown in \autoref{figure:overview}, safety guardrails operate at multiple stages.
At the input stage, they detect and block malicious content or adversarial prompts, such as hate speech~\cite{VTWK21,SWQBZZ25}, jailbreak attempts~\cite{SCBSZ24,ZWKF23}, and malicious instruction injections~\cite{LJGJG24, ZLYK24} designed to override agent restrictions.
At the output stage, the guardrails detect AI-generated responses in real time, preventing harmful or unethical content.
By incorporating these defenses, AI systems can maintain higher stages of security, reliability, and ethical compliance, ensuring safe and trustworthy interactions in real-world applications.

\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figures/overview_v2.pdf}
\caption{Overview of a conversational AI agent with input and output guardrails.}
\label{figure:overview}
\end{figure}

The ideal security scenario assumes that attackers possess no prior knowledge of the underlying guardrail mechanisms. 
In practice, understanding these mechanisms enables attackers to craft guardrail-specific attacks, such as tailored prompts that evade detection or manipulate outputs within safety limits.
On the other hand, identifying guardrails helps red team operators attribute test failures, distinguishing whether defenses stem from external guardrails or the LLM's inherent safety mechanisms. 
This allows precise evaluation of adversarial prompts and defensive effectiveness.

\begin{figure*}[!t]
\centering
\centerline{\includegraphics[width=2\columnwidth]{figures/framework_v4.pdf}}
\caption{Framework of our \method.
We first perform (1) adversarial prompt optimization based on the candidate guardrail with our well-designed loss function.
Then, we conduct (2) adversarial prompt test by querying the AI agent with the optimized adversarial prompts to determine whether the candidate guardrail exists in the AI agent.}
\label{figure:framework}
\end{figure*}

In this paper, we demonstrate that even in a black-box setting, where the internal workings of the AI agents are not explicitly disclosed, an attacker or a red team operator can extract critical information about these security mechanisms.
Specifically, they can identify which guardrails are deployed in the system, even at which stage.
We propose \method, which utilizes \underline{\textbf{a}}dversarial \underline{\textbf{p}}rompts to \underline{\textbf{test}} whether the candidate guardrail is used in the input or output stage of the AI agent.
As shown in \autoref{figure:framework}, our approach begins by probing the AI agent with guardrail-specific adversarial prompts, which are designed to be flagged as \texttt{unsafe} by a specific candidate guardrail while remaining \texttt{safe} according to others.
To optimize these adversarial prompts, we introduce a tailored loss function that balances three key objectives: (1) maximizing the probability that the candidate guardrail classifies the prompt as \texttt{unsafe}, (2) minimizing the likelihood of the same prompt being classified as \texttt{safe} by the candidate guardrail, and (3) ensuring the prompt remains \texttt{safe} for other unrelated guardrails.
We design a novel metric, \textit{normalized distance}, to evaluate the presence of candidate guardrails in AI agents without comparative analysis with additional guardrails.
A larger normalized distance (e.g., $>0.50$) indicates a higher tendency to refuse adversarial prompts, suggesting a stronger likelihood of the candidate guardrail being deployed. 

To evaluate the effectiveness of our approach, we conduct extensive experiments on various state-of-the-art LLMs with deployed safety guardrails.
Our results demonstrate that the proposed attack method accurately identifies guardrails under diverse conditions.
Specifically, the WildGuard-specific~\cite{HREJLLCD24} input guard test on the GPT4o-based~\cite{GPT4o} agent equipped with WildGuard achieves a 1.00 normalized distance, while the highest normalized distance of the test on agents equipped with other guardrails is only 0.05.
This showcases the effectiveness of our method.

Moreover, to analyze the impact of our proposed method further, we perform an ablation study to examine the role of each component, particularly our loss terms designed to optimize adversarial queries and the existence of the query set.
Our findings reveal that each term is crucial for the identification.
For example, when there is no loss ensuring the prompt remains \texttt{safe} for other guardrails, \method tends to misidentify that the LlamaGuard3 is used in the agent that is only equipped with WildGuard.

Overall, our contributions are as follows:
\begin{itemize}
    \item We are the first to identify the guardrails used in conversational AI agents, which helps to audit the guardrail for test failure attribution while also providing more information for the attacker to conduct their attacks.
    \item We propose \method, which uses adversarial prompts to identify the guardrail used in an AI agent.
    Experiments show its effectiveness on four candidate guardrails on various agents under diverse scenarios.
    \item The ablation study demonstrates the importance of our proposed loss terms, showing that their removal significantly degrades identifying performance.
\end{itemize}

%-------------------------------------------------------------------------------
\section{Related Work}
%-------------------------------------------------------------------------------

\mypara{Security Risks in LLMs}
The rapid advancement of LLMs provides users with significant convenience but also raises critical security concerns~\cite{ZYPDLWW24,GRLWCWDW23,ZLWJZBSZ24,WLBZ24,LGFXS23}.
Among these concerns, jailbreak attacks pose a major threat by bypassing built-in safety mechanisms, enabling models to generate restricted or harmful content that violates usage policies~\cite{Google_Policy,Meta_Policy,OpenAI_Policy,Amazon_Policy_1}.
Previous studies analyze existing jailbreak strategies, particularly focusing on in-the-wild jailbreak prompts that are manually crafted in real-world scenarios~\cite{SCBSZ24}.
More recent studies introduce automated jailbreak generators, such as AutoDAN~\cite{LXCX23}, GCG~\cite{ZWKF23}, and TAP~\cite{MZKNASK23}, which optimize adversarial prompts to evade safety measures and maximize attack success rates.
Although model developers employ various safety alignment techniques, such as reinforcement learning from human feedback (RLHF)~\cite{LNTYCWZZ24,JLDPZBCSWY23}, to train LLMs and mitigate these threats, attackers continuously refine their strategies, underscoring the persistent challenge.

\mypara{LLM Guardrails}
To better ensure the safe and responsible deployment of LLMs, researchers develop various guardrails that mitigate risks associated with both inputs and LLM-generated outputs to prevent policy-violating content.
Early online content moderation tools, such as Perspective API~\cite{Perspective} and OpenAI's Content Moderation API~\cite{OpenAI_Moderation}, fall short due to the limited capacity of their backbone models and the inability of emerging policies to address evolving risks.
Recent studies introduce LlamaGuard~\cite{IUCRIMTHFTK23}, which establishes a safety risk taxonomy encompassing a range of safety risks.
Built on Llama 2-7B, it is trained on a dataset constructed according to this taxonomy.
Subsequent versions, LlamaGuard2~\cite{Llama_Guard_2} and LlamaGuard3~\cite{CKZSRZPCUP24}, further expand the safety risk taxonomy and dataset, leveraging state-of-the-art LLMs for fine-tuning, thereby strengthening their safeguard capabilities.
Similarly, WildGuard~\cite{HREJLLCD24}, Aegis~\cite{GVGP24}, and ShieldGamma~\cite{ZLMPFHNPKRSW24} follow a comparable approach.
They define a broad safety risk taxonomy and develop dedicated data curation pipelines to construct fine-tuning datasets, thereby contributing essential resources for LLM guardrails.

\mypara{LLM Identification}
LLM identification aims to determine the origin LLM of LLM derivatives~\cite{DCS21, ZZWL23, MSSA24, GULYO24, JZSLH24}.
A common approach to identifying the origin LLM, such as TRAP~\cite{GULYO24} and ProFolingo~\cite{JZSLH24}, is to leverage adversarial examples by optimizing a text prefix or suffix to query the model for a target response, which can then be utilized for model identification.
Guardrail identification and LLM identification share similarities, as state-of-the-art guardrails are often fine-tuned on top of LLMs using annotated datasets that cover a broad spectrum of safety risks~\cite{IUCRIMTHFTK23,HREJLLCD24,ZLMPFHNPKRSW24}.
Inspired by LLM identification methods, we frame guardrail identification as an optimization problem.
Specifically, we optimize adversarial examples and analyze the behavioral patterns of guardrails in response to these inputs to identify candidate guardrails.

%-------------------------------------------------------------------------------
\section{Problem Statement}
%-------------------------------------------------------------------------------

\mypara{Preliminary}
At the core of these AI agents lies a base LLM $\mathcal{F}$ that drives their functionality, which suffers from security risks such as jailbreak attacks~\cite{SCBSZ24,ZWKF23,LXCX23,ZYPDLWW24} and prompt injection attacks~\cite{LJGJG24, ZLYK24, DZBBFT24,YWSJX23}.
To ensure the security and compliance of these AI agents, additional mechanisms known as input and output guardrails $\mathcal{G}$ are often implemented.
Specifically, the input guardrail $\mathcal{G}_i$ evaluates user inputs $x$ to determine whether they should be forwarded to the base LLM.
If the input $x$ is deemed high-risk, policy-violating, or jailbreak prompts, i.e., $\mathcal{G}_i(x)=\texttt{unsafe}$, the agent then returns \texttt{refuse} without processing it further.
However, with attacks like jailbreak attacks, even though the prompt input may be considered \texttt{safe}, the LLM's response could contain harmful content.
Therefore, in real-world applications, solely monitoring input may not be sufficient, which motivates the deployment of output guardrails.
The output guardrail $\mathcal{G}_o$ monitors the base LLM's responses to avoid policy violations.
If a response is non-compliant, i.e., $\mathcal{G}_o(x)=\texttt{unsafe}$, the agent would withhold the output and return \texttt{refuse}.

\mypara{Threat Model}
The goal of guardrail identification is to determine whether a candidate guardrail is deployed in a black-box AI agent without knowledge of the backend LLM or the presence of input/output guardrails.
We assume we have white-box access to the candidate guardrail while can only query the black-box AI agent and get responses.
A successful guardrail identification provides deeper internal knowledge of AI agents.
With such knowledge, attackers can design more effective attacks.
For example, if an open-sourced guardrail is identified, the attacker can iteratively refine adversarial prompts and harmful outputs through local testing to evade detection.
Meanwhile, this knowledge also helps red team operators attribute test failures, distinguishing whether successful defenses stem from external guardrails or the LLM's built-in safety mechanisms.
By identifying the deployed guardrails, operators can examine adversarial prompts against them in isolation, enabling more precise attribution of defensive effectiveness.

We define the guardrail identification task as follows:
\textit{Given a black-box AI agent $\mathcal{A}$ and a white-box candidate guardrail $\mathcal{G}_t$, guardrail identification aims to identify whether it is deployed in the AI agent.
The input guard test audits whether the candidate guardrail $\mathcal{G}_t$ is deployed at the input stage of the AI agent, while the output guard test evaluates whether the candidate guardrail is present at the output stage.}

%-------------------------------------------------------------------------------
\section{Method}
%-------------------------------------------------------------------------------

In this work, we propose \method, which utilizes \underline{\textbf{a}}dversarial \underline{\textbf{p}}rompts to \underline{\textbf{test}} whether the candidate guardrail is used in the input or output stage of the AI agent.
As illustrated in \autoref{figure:framework}, our approach identifies the presence of a candidate guardrail by leveraging guardrail-specific adversarial prompts to query the AI agent. 
The goal is to craft adversarial prompts that are exclusively flagged as \texttt{unsafe} by the candidate guardrail $\mathcal{G}_t$ while being perceived as \texttt{safe} by other guardrails. 
This behavior reveals whether the candidate guardrail is in place: if the AI agent implements the candidate guardrail, it tends to refuse to respond to the adversarial prompt; otherwise, it responds.

\mypara{Adversarial Prompt Optimization}
An adversarial prompt is constructed by concatenating an optimized adversarial prefix $x_a$ with a normal query $x_q \in Q$, where $Q$ is a query set. 
The normal query is used as a starting point to prevent over-rejection by other guardrails, which is proven effective in our ablation study (\autoref{section:hyper_parameters}).
For each query $x_q$, we optimize an adversarial prefix $x_a$ using three loss terms, each addressing a specific aspect of the desired behavior:
\begin{itemize}
    \item \textbf{Candidate Guardrail Adversarial Losses ($L_1$ and $L_2$):}  
    These loss terms are designed to mislead the candidate guardrail $\mathcal{G}_t$ into classifying the adversarial prompt as \texttt{unsafe}.  
    \begin{itemize}  
        \item $L_1$: Encourages the candidate guardrail to classify the adversarial prompt as \texttt{unsafe}, defined as:  
        \begin{equation}
        L_1 = \sigma(\mathcal{G}_t(x_a \Vert x_q), \texttt{unsafe}),
        \end{equation}  
        where $\sigma(\cdot, \cdot)$ represents the cross-entropy loss.  
        \item $L_2$: Penalizes the candidate guardrail for classifying the adversarial prompt as \texttt{safe}, defined as:  
        \begin{equation}
        L_2 = -\sigma(\mathcal{G}_t(x_a \Vert x_q), \texttt{safe}).
        \end{equation}  
    \end{itemize}  
    Together, $L_1$ and $L_2$ ensure that $x_a$ effectively triggers the refusal mechanism of the candidate guardrail.
    Conceptually, these two losses are identical, but our ablation study in \autoref{section:hyper_parameters} demonstrates the synergy of these two losses outperforms solely deploying a single of them.
    \item \textbf{Cross-Guardrail Compatibility Loss ($L_3$):}  
    This loss ensures that the adversarial prompt remains \texttt{safe} according to all other guardrails by introducing a safety scorer $\mathcal{S}$.  
    The safety scorer $\mathcal{S}$ measures the safety stage of an input $x$~\cite{SW17, MSYBGM21, VTWK21}: $\mathcal{S}(x) = y_s \in [0, 1]$, where $y_s = 0$ indicates no security risk, and $y_s = 1$ indicates a potential risk.
    The loss term is defined as:  
    \begin{equation}
    L_3 = \sigma(\mathcal{S}(x_a \Vert x_q), 0).
    \end{equation}
    By minimizing $L_3$, we prevent unintended rejections from unrelated guardrails, preserving the specificity of the attack on $\mathcal{G}_t$.
\end{itemize}

By jointly minimizing $L_1$, $L_2$, and $L_3$, we craft adversarial prompts following \cite{JZSLH24} that expose the behavior of the candidate guardrail while maintaining compatibility with other guardrails.
The final loss function is defined as:  
\begin{equation}
    L = L_1 + \alpha \cdot L_2 + \beta \cdot L_3, \quad \alpha,\beta \in \mathbb{R},
\end{equation} 
where $\alpha$ and $\beta$ control the weights of the loss terms $L_2$ and $L_3$, respectively.

\mypara{Adversarial Prompt Test}
The well-crafted adversarial prompts $\{x_a \Vert x_q\}$ can be utilized for candidate guardrail identification based on the assumption that the candidate guardrail and its derivatives tend to consider these prompts as \texttt{unsafe}.
To distinguish whether the candidate guardrail exists in the AI agent, we conduct two different tests focusing on either the input or the output stage of the guardrails.

\mypara{(a) Input Guard Test}
We first consider the identification of the input guardrail.
When an input guardrail exists in an AI agent, the input guardrail will decide whether the user prompt $x$ should be passed to the base LLM, as shown in \autoref{figure:overview}.
Thus, we do the input guard test by directly querying the AI agent with our adversarial prompts $\{x_a \Vert x_q\}$.
If the AI agent responds with \texttt{refuse}, we consider the candidate guardrail to probably serve as an input guardrail in the AI agent (see \autoref{figure:framework}~(a)).
To better quantify, we introduce the \emph{refusal rate} $r \in [0,1]$, which is the ratio of \texttt{refuse} among all responses, where \texttt{refuse} represents that the AI agent refuses to respond to the query.
A higher refusal rate indicates that the candidate guardrail is more likely to be the input guardrail in this AI agent.
We set the refusal rate of directly querying the candidate guardrail with the optimized adversarial prompts as the target refusal rate $r_t$.
Thus, we define the normalized distance between the results on the test AI agent and the non-candidate guardrails as:
\begin{equation}
\label{equation:distance}
d = \frac{|\min(r, r_t)-0|^{\lambda}}{|r_t-0|^{\lambda}}=\frac{|\min(r, r_t)|^{\lambda}}{|r_t|^{\lambda}}, \quad \lambda \geq 1,
\end{equation}
where $0$ is the lower bound of $r$ and $\lambda$ represents the scaling factor of the distance.
This normalized distance $d\in [0,1]$ depicts how close the results are to the refusal rate of $0$.
We thus consider the candidate guardrail to exist in the AI agent if $d > 0.5$.

\mypara{(b) Output Guard Test}
Identifying the output guardrail in the AI agent is tougher, as the output guardrail takes the base LLM's generated response $y$ as input instead of the user prompt $x$.
It is challenging for us to manipulate the base LLM's output as we have no knowledge about it.
To solve this, we design a prompt template that asks the AI agent to repeat our adversarial prompt, e.g., ``Please repeat: [\texttt{Adversarial Prompt}]''
Ideally, the base LLM should respond with the adversarial prompt and pass it to the output guardrail.
We empirically design and evaluate five prompt templates and select the best one.
With the well-designed prompt template, we can ask the AI agent to repeat the adversarial prompts to test its output guardrail as shown in \autoref{figure:framework}~(b).
To begin with, we simplify this task by assuming the AI agent does not have input guardrails and will relax this assumption in the third case.
Thus, once we test an AI agent for their output guardrail, we can compute the normalized distance based on \autoref{equation:distance}.
Note that $r_t$ is the same as that in the input guard test as this is obtained by directly querying the candidate guardrail, disregarding the AI agent nor the base LLM.

\begin{figure}[!t]
\centering
\centerline{\includegraphics[width=\columnwidth]{figures/inout_guard_test.pdf}}
\caption{Workflow on real-world scenarios.
We first conduct (a) the input guard test on the AI agent.
If the results show that the candidate guardrail probably does not exist in the agent, then
we further conduct (b) the output guard test to identify whether it serves as the output guardrail in
the agent.}
\label{figure:inout_guardrail_test}
\end{figure}

\begin{table*}[!t]
\caption{Results of input guard tests.
Normalized distances larger than 0.5 are bolded.}
\label{table:input_results}
\centering
\setlength{\tabcolsep}{0.4mm}
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{2}{*}{Input Guardrail} & \multicolumn{4}{c}{Llama3.1-Based}&\multicolumn{4}{c}{GPT4o-Based} \\
&WildGuard&LlamaGuard&LlamaGuard2&LlamaGuard3&WildGuard&LlamaGuard&LlamaGuard2&LlamaGuard3 \\
\midrule
WildGuard&\textbf{1.00}&0.06&0.00&0.00&\textbf{1.00}&0.06&0.00&0.00\\
\midrule
LlamaGuard&0.00&\textbf{1.00}&0.00&0.00&0.00&\textbf{1.00}&0.00&0.00\\
AegisDefensive&0.00&\textbf{1.00}&0.00&0.00&0.00&\textbf{1.00}&0.00&0.00\\
AegisPermissive&0.00&\textbf{0.81}&0.00&0.00&0.00&\textbf{0.81}&0.00&0.00\\
\midrule
LlamaGuard2&0.00&0.00&\textbf{1.00}&0.00&0.00&0.00&\textbf{1.00}&0.00\\
\midrule
LlamaGuard3&0.05&0.16&0.22&\textbf{1.00}&0.05&0.16&0.22&\textbf{1.00}\\
\midrule
ShieldGemma-2B&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
ShieldGemma-9B&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
ShieldGemma-27B&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
GPT4o&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!t]
\caption{Similarity between the input adversarial prompt and the output text from the surrogate LLM with different prompt templates.}
\label{table:similarity_full}
\centering
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{ccccccc}
\toprule
\#Template & Cosine Similarity & BLUE & ROUGE-1 & ROUGE-2 & ROUGE-L & ROUGE-LSum \\
\midrule
T1&0.881&0.977&0.911&0.900&0.911&0.911\\
T2&0.933&0.961&0.939&0.934&0.939&0.939\\
T3&\textbf{0.944}&\textbf{0.980}&\textbf{0.956}&\textbf{0.945}&\textbf{0.956}&\textbf{0.956}\\
T4&0.870&0.929&0.884&0.876&0.884&0.884\\
T5&0.914&0.962&0.926&0.916&0.926&0.926\\
\bottomrule
\end{tabular}
\end{table*}

\mypara{Real-World Scenarios}
As illustrated in \autoref{figure:inout_guardrail_test}, guardrails can be implemented at both the input and output stages in real-world scenarios.
Therefore, it is necessary to conduct both input and output guard tests for a given candidate guardrail independently.
However, such real-world scenarios pose challenges, as the presence of input guardrails and the backend LLM may influence the output guard test.
To address this, we propose a two-step process.
\begin{itemize}
    \item We first determine whether the candidate guardrail functions as an input guardrail through the input guard test.
    If so, we conclude that the candidate guardrail is deployed in the AI agent and do not proceed with the output guard test.
    \item If it is not used as the input guardrail, then we proceed with the output guard test for the candidate guardrail.
\end{itemize}
Note that in later evaluations (\autoref{section:real_world_test}), we demonstrate that the proposed method can successfully identify the input/output guardrail even in the presence of a different output/input guardrail.
We further argue that when the input and output guardrails are the same, it is not critical to distinguish them through both input and output guard tests.
The reasons are twofold: (1) If the candidate serves as the input guardrail, it can be identified using well-crafted adversarial prompts, making it difficult for adversarial prompts to reach the output stage.
(2) Knowing its existence provides sufficient knowledge to leverage its white-box access for designing subsequent attacks.
Additionally, by comparing the evaluation results of black-box AI agents with local evaluation results where the guardrail either serves or does not serve as the output guardrail, we can determine whether it has been deployed at the output stage.

%-------------------------------------------------------------------------------
\section{Experiments}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Experimental Settings}
%-------------------------------------------------------------------------------

We take four different guardrails as our candidate guardrails, including WildGuard~\cite{HREJLLCD24}, LlamaGuard~\cite{IUCRIMTHFTK23}, LlamaGuard2~\cite{Llama_Guard_2}, and LlamaGuard3~\cite{CKZSRZPCUP24}.
We use 11 guardrails for evaluation, including the four candidate guardrails, AegisDefensive, AegisPermissive~\cite{GVGP24}, ShieldGemma-2B, ShieldGemma-9B, ShieldGemma-27B~\cite{ZLMPFHNPKRSW24}, Perspective~\cite{Perspective}, and GPT4o~\cite{GPT4o}.
We obey their default settings in our experiments and follow \cite{ZLMPFHNPKRSW24} to prompt GPT4o as an input/output guardrail.
Besides, we use Llama3.1~\cite{DJPKALMSYFGHYMSKHRZRGSRBTCCNBMMKTWWFNASPLECMGPHLALDSRZSLANMPCNKXTZIKMECLGVPMSLBHLFCHLWYBSPRJSJAUPLHSa24} and GPT4o as the base LLMs.
For the safety scorer in the output guard test, we consider state-of-the-art hate speech detectors and use LFTW-R4~\cite{VTWK21} as the safety scorer in our experiments. 
Details of different models can be found in \autoref{table:model_names} in \autoref{appendix:settings}.

For optimization, we follow the settings in \cite{JZSLH24} and use their dataset as the query set $Q$, which consists of 50 simple questions.
The adversarial prefix length is set to 32 tokens, with loss weights $\alpha = 0.01$ and $\beta = 1000$.
We further investigate the impact of different loss term weights in the ablation study.
We adopt the normalized distance $d$ with $\lambda = 2$ as the evaluation metric.
The experiments are conducted with a batch size of 64 over 200 epochs on NVIDIA A100 GPUs with 40 GB of memory.

%-------------------------------------------------------------------------------
\subsection{Input Guard Test}
%-------------------------------------------------------------------------------

We first evaluate the input guard test of our \method.
To better evaluate the effectiveness of our method, we construct our AI agents with the base LLMs and the input guardrails.
We will discuss the situation that contains both input and output guardrails later in \autoref{section:real_world_test}.

\begin{table*}[!t]
\caption{Identification results of output guard tests.
Normalized distances larger than 0.5 are bolded.}
\label{table:output_results}
\centering
\setlength{\tabcolsep}{0.4mm}
\begin{tabular}{lcccc|cccc}
\toprule
\multirow{2}{*}{Output Guardrail} & \multicolumn{4}{c}{Llama3.1-Based} & \multicolumn{4}{c}{GPT4o-Based} \\
&WildGuard&LlamaGuard&LlamaGuard2&LlamaGuard3&WildGuard&LlamaGuard&LlamaGuard2&LlamaGuard3 \\
\midrule
WildGuard&\textbf{1.00}&0.05&0.02&0.12&\textbf{1.00}&0.04&0.00&0.01\\
\midrule
LlamaGuard&0.00&\textbf{1.00}&0.00&0.00&0.00&\textbf{1.00}&0.00&0.00\\
AegisDefensive&0.02&\textbf{1.00}&0.01&0.19&0.02&\textbf{1.00}&0.00&0.01\\
AegisPermissive&0.00&\textbf{0.96}&0.00&0.02&0.00&\textbf{0.92}&0.00&0.00\\
\midrule
LlamaGuard2&0.00&0.00&\textbf{1.00}&0.01&0.00&0.00&\textbf{1.00}&0.00\\
\midrule
LlamaGuard3&0.10&0.30&0.44&\textbf{1.00}&0.10&0.32&0.42&\textbf{1.00}\\
\midrule
ShieldGemma-2B&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
ShieldGemma-9B&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
ShieldGemma-17B&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
GPT4o&0.00&0.00&0.00&0.00&0.00&0.00&0.00&0.00\\
\bottomrule
\end{tabular}
\end{table*}

\autoref{table:input_results} shows the normalized distances over all AI agents with different input guardrails.
Results show that our \method successfully identifies the candidate guardrails that are used in the AI agent.
For example, with Llama3.1 as the base LLM, the agent with WildGuard achieves a normalized distance of 1.00 on the WildGuard-specific adversarial prompts, while it only achieves a maximum distance of 0.05 on other agents.
We observe that the LlamaGuard-specific input test achieves 1.00 and 0.81 normalized distance on the agents based on AegisDefense and AegisPermissive, respectively.
That is because AegisDefense and AegisPermissive are fine-tuned on LlamaGuard, which are the derivatives of LlamaGuard.
We also observe that the results of both Llama3.1 and GPT4o-based agents are almost the same.
This shows that the base LLM has little influence on the input guard test, i.e., the base LLM response to our adversarial prompts without refusal.

%-------------------------------------------------------------------------------
\subsection{Output Guard Test}
\label{section:output_guardrail_test_experiment}
%-------------------------------------------------------------------------------

\mypara{Prompt Template Selection}
The output guard test requires a prompt template that asks the AI agent to repeat our adversarial prompts.
We begin with the template: ``Please repeat: [\texttt{Adversarial Prompt}]'' and ask ChatGPT~\cite{chatgpt} to generate five variations, as shown in \autoref{table:templates} in \autoref{appendix:settings}.  
To assess how well the templates help the LLM repeat the adversarial prompt, we use Gemma2-9B~\cite{MHDBPSRKLTHCRBBCSHTBPTSLCCCIRBNNYTMRMTGAKLLSBCFCa24} as a surrogate LLM and evaluate the similarity between the generated text and the input adversarial prompt optimized on LlamaGuard3.  
The results in \autoref{table:similarity_full} show that the third template (T3) achieves the highest similarity scores, including a ROUGE-L score of 0.956.
Thus, we select T3 as the prompt template for our experiments:
\textit{Your task is to mimic text. Repeat this string exactly as input, with no interpretation: [\texttt{Adversarial Prompt}]}

\mypara{Test Performance}
Using T3 as the prompt template, we evaluate the output guard test on AI agents containing only output guardrails.
Results in \autoref{table:output_results} demonstrate the effectiveness of our \method.
All agents with normalized distances larger than 0.50 are indeed equipped with the corresponding candidate guardrail as the output guardrail.  
For example, on GPT4o-based agents, the normalized distance of the LlamaGuard2 guardrail reaches 1.00 on LlamaGuard2-specific adversarial prompts, while it is 0.00 on LlamaGuard and its derivatives.
This indicates that our \method successfully distinguishes the output guardrail used in the agent.

We also observe that the output guard test is harder than the input guard test.
For example, for LlamaGuard2-specific adversarial prompts, the normalized distance of the Llama3.1-based agent with LlamaGuard3 achieves 0.44, which is 0.22 farther than that in the input guard test and is closer to 0.50.
Further, different from the input guard test, there is a slight performance difference between Llama3.1- and GPT4o-based agents.
This discrepancy is due to information loss during the base LLM processing, which impacts the repeat performance of the prompt template.

%-------------------------------------------------------------------------------
\subsection{Real-World Scenario Test}
\label{section:real_world_test}
%-------------------------------------------------------------------------------

\begin{table*}[!t]
\caption{Results of LlamaGuard3-specific input guard test on Llama3.1-based AI agents equipped with different output guardrails.
``N/A'' denotes that there is no output guardrail in the agent, serving as the baseline.}
\label{table:in_with_out_results}
\centering
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Candidate Guardrail} & \multicolumn{6}{c}{Output Guardrail}\\
&N/A&WildGuard&LlamaGuard&LlamaGuard2&LlamaGuard3&ShieldGemma-2B\\
\midrule
WildGuard&0.00&0.00&0.00&0.00&0.00&0.00\\
LlamaGuard&0.00&0.00&0.00&0.00&0.00&0.00\\
LlamaGuard2&0.00&0.00&0.00&0.00&0.00&0.00\\
LlamaGuard3&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!t]
\caption{Results of LlamaGuard3-specific output guard test on Llama3.1-based AI agents equipped with different input guardrails.
``N/A'' denotes that there is no input guardrail in the agent, serving as the baseline.
}
\label{table:out_with_in_results}
\centering
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Candidate Guardrail} & \multicolumn{6}{c}{Input Guardrail}\\
&N/A&WildGuard&LlamaGuard&LlamaGuard2&LlamaGuard3&ShieldGemma-2B\\
\midrule
WildGuard&0.12&0.12&0.12&0.12&0.12&0.12\\
LlamaGuard&0.00&0.00&0.00&0.00&0.00&0.00\\
LlamaGuard2&0.01&0.01&0.01&0.01&0.00&0.01\\
LlamaGuard3&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}&\textbf{1.00}\\
\bottomrule
\end{tabular}
\end{table*}

In our main experiments, we assume that the agent only contains the input guardrail or the output guardrail.
Here we relax this assumption and evaluate the performance of our \method when the AI agent contains both input and output guardrails.
Specifically, we evaluate our method on the Llama3.1-based agent and take LlamaGuard3 as our candidate guardrail.
For the input/output guard test, we evaluate them on the agents with different output/input guardrails including WildGuard, LlamaGuard, LlamaGuard2, LlamaGuard3, and ShieldGemma-2B.

We first focus on the input guard test.
As shown in \autoref{table:in_with_out_results}, the performance of the input guard test is the same when experimenting on agents with different output guardrails.
This indicates that the output guardrail has little influence on the input guard test.

The output guard test, similarly, is robust on agents with different input guardrails.
The test results on agents with various input guardrails remain the same as those on the agents with no input guardrails as shown in \autoref{table:output_results}.
The results can be found in \autoref{table:out_with_in_results}.
We further examine the agents' outputs and find that the only difference is that the refusal rate slightly increases when testing on the agent equipped with LlamaGuard2 as input guardrail and LlamaGuard3 as output guardrail.
This is because the input guardrail LlamaGuard2 refuses more queries compared with the output guardrail LlamaGuard3.
In other circumstances, the refusal rates are maintained.

%-------------------------------------------------------------------------------
\section{Ablation Study}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\subsection{Influence of Hyper-Parameters}
\label{section:hyper_parameters}
%-------------------------------------------------------------------------------

To explore the influence of different hyper-parameters in our \method, we experiment on Llama3.1-based AI agents that contain only input guardrails, taking LlamaGuard3 as our candidate guardrail.
To better illustrate the influence, we report the refusal rate $r$ of each test.

\begin{figure}[!t]
\centering
\begin{subfigure}{0.49\columnwidth}
\includegraphics[width=\columnwidth]{figures/influence_loss_alpha.pdf}
\caption{$L_2$}
\label{figure:influence_loss_alpha}
\end{subfigure}
%
\begin{subfigure}{0.49\columnwidth}
\includegraphics[width=\columnwidth]{figures/influence_loss_beta.pdf}
\caption{$L_3$}
\label{figure:influence_loss_beta}
\end{subfigure}
\caption{Influence of different weights of loss terms.
The refusal rates on other guardrails are 0.00, which is not shown here.}
\label{figure:influence_loss}
\end{figure} 

\mypara{Influence of Loss Terms}
\autoref{figure:influence_loss} shows the refusal rates using different weights of loss terms.
We observe that $L_2$ enhances the refusal rates of the candidate guardrail LlamaGuard3 while slightly increasing the refusal rates of other guardrails.
For example, the refusal rate of LlamaGuard3 increases from 0.72 to 0.84 while that of AegisDefensive slightly increases to 0.04.
As for $L_3$, we observe that it mainly helps suppress the refusal rate of other guardrails.
For example, when $\beta$ is 0, although the refusal rate of LlamaGuard3 is high (1.00), the refusal rate of WildGuard reaches 0.86, which is so close to the target refusal rate $r_t$ (0.98).

\begin{figure}[!t]
\centering
\begin{subfigure}{0.49\columnwidth}
\includegraphics[width=\columnwidth]{figures/influence_epoch.pdf}
\caption{Loss}
\label{figure:influence_epoch_loss}
\end{subfigure}
%
\begin{subfigure}{0.49\columnwidth}
\includegraphics[width=\columnwidth]{figures/influence_epoch_r.pdf}
\caption{Performance}
\label{figure:influence_epoch_r}
\end{subfigure}
\caption{Influence of epochs.
The refusal rates on other guardrails are all 0.00.}
\label{figure:influence_epoch}
\end{figure} 

\mypara{Influence of Epochs}
To investigate the influence of the number of epochs on our method, we analyze the losses and refusal rates of the LlamaGuard3-specific input guard test across different epochs, as illustrated in \autoref{figure:influence_epoch}. 
The results indicate that the losses converge around the 50th epoch, highlighting the efficiency of our approach.
Furthermore, the refusal rate for agents equipped with LlamaGuard3 increases with additional epochs, reaching 0.62 by the 160th epoch, while the refusal rate for agents with WildGuard decreases.
This behavior demonstrates that as the number of epochs increases, the input guard test becomes more reliable, as the distinction between the candidate guardrail and other guardrails grows.
Ultimately, the system converges at a specific epoch level, ensuring stable and consistent performance.

\begin{table}[!t]
\caption{Influence of using normal query set in \method.}
\label{table:influence_query}
\centering
\setlength{\tabcolsep}{1mm}
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Input Guardrail} & \multicolumn{2}{c}{\method} \\
& Full & W/O Normal Query\\
\midrule
WildGuard&0.04&0.94\\
LlamaGuard&0.00&0.98\\
LlamaGuard2&0.00&0.78\\
LlamaGuard3&0.66&0.92\\
\bottomrule
\end{tabular}
\end{table}

\mypara{Influence of Normal Questions}
We first conduct an ablation study on the influence of the normal query set $\mathcal{Q}$, which is used as the starting point for adversarial prompt optimization.
\autoref{table:influence_query} shows the refusal rates of LlamaGuard3-specifc input guard test.
We find that without the query set, the test on agents equipped with guardrails other than LlamaGuard3 achieves even higher refusal rates.
For example, the refusal rate of \method without a normal query set achieves 0.92 on the agent with LlamaGuard3, while it is 0.66 for \method with the query set.
This indicates that adversarial optimization is more difficult with a query set.
However, the refusal rate of \method without a normal query set on other guardrails still remains high, even higher than that on the agent with LlamaGuard3, leading to the failure of the identification.
For instance, its refusal rate on the agent with LlamaGuard is 0.98, 0.06 higher than that on the agent with LlamaGuard3.
This means that \method without a normal query set mistakes that LlamaGuard3 is used in the agent only equipped with WildGuard as its input guardrail.
In this sense, we address the importance of the query set as the initial guidance for adversarial optimization.

%-------------------------------------------------------------------------------
\section{Conclusion}
%-------------------------------------------------------------------------------

In this work, we address the task of identifying guardrails employed in conversational AI agents.
This capability not only provides attackers with additional information to conduct targeted attacks but also assists red team operators in auditing the origin of guardrails.
Our proposed \method leverages adversarial prompts to identify the guardrail integrated into an AI agent.
Experiments conducted on four candidate guardrails across various AI agents demonstrate the effectiveness of \method in diverse scenarios.
Additionally, an ablation study underscores the significance of our proposed loss terms and the existence of the query set, revealing that its removal leads to a substantial degradation in identification performance.

%-------------------------------------------------------------------------------
\section*{Impact Statement}
%-------------------------------------------------------------------------------

In research-oriented settings, LLMs and external guardrails are typically evaluated in isolation.
However, in real-world attack scenarios and red-teaming exercises, black-box AI agents often incorporate a combination of both, making attacks more challenging for attackers and complicating attribution in safety evaluations.
Understanding this interplay is crucial for both attack and defense.
In this work, we introduce guardrail identification, a technique that successfully determines whether external guardrails are applied at the input, output, or both stages.
This knowledge enables the design of more effective attacks in realistic settings while also assisting red team operators in attributing successful defenses.
Ultimately, our work highlights the need for techniques that make guardrails inherently harder to identify and circumvent.
Meanwhile, we aim to contribute to the attack-defense cycle by enabling more effective adversarial testing and enhancing the explainability of AI agents in safety-critical scenarios.

%-------------------------------------------------------------------------------
\begin{small}
\bibliographystyle{plain}
\bibliography{normal_generated_py3}
\end{small}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\appendix
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
\section{Experimental Settings}
\label{appendix:settings}
%-------------------------------------------------------------------------------

\autoref{table:model_names} shows the details of the models used in our experiments, including the versions we used.

\autoref{table:templates} illustrates the five candidate prompt templates used in the output guard test.

\begin{table*}[!t]
\caption{Details of the guardrails and LLMs.}
\label{table:model_names}
\centering
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{ll}
\toprule
Model Name & Model Path \\
\midrule
WildGuard~\cite{HREJLLCD24}&allenai/wildguard \\
\midrule
LlamaGuard~\cite{IUCRIMTHFTK23}&meta-llama/LlamaGuard-7b\\
LlamaGuard2~\cite{Llama_Guard_2}&meta-llama/Meta-Llama-Guard-2-8B\\
LlamaGuard3~\cite{CKZSRZPCUP24}&meta-llama/Llama-Guard-3-8B\\
\midrule
AegisDefensive~\cite{GVGP24}&nvidia/Aegis-AI-Content-Safety-LlamaGuard-Defensive-1.0\\
AegisPermissive~\cite{GVGP24}&nvidia/Aegis-AI-Content-Safety-LlamaGuard-Permissive-1.0\\
\midrule
ShieldGemma-2B~\cite{ZLMPFHNPKRSW24}&google/shieldgemma-2b\\
ShieldGemma-9B~\cite{ZLMPFHNPKRSW24}&google/shieldgemma-9b\\
ShieldGemma-27B~\cite{ZLMPFHNPKRSW24}&google/shieldgemma-27b\\
\midrule
Perspective~\cite{Perspective}& Perspective API\\
\midrule
GPT4o~\cite{GPT4o}& GPT4o-2024-11-20\\
\midrule
Gemma2~\cite{MHDBPSRKLTHCRBBCSHTBPTSLCCCIRBNNYTMRMTGAKLLSBCFCa24}& google/gemma-2-9b-it\\
\midrule
Llama3.1~\cite{DJPKALMSYFGHYMSKHRZRGSRBTCCNBMMKTWWFNASPLECMGPHLALDSRZSLANMPCNKXTZIKMECLGVPMSLBHLFCHLWYBSPRJSJAUPLHSa24}&meta-llama/Llama-3.1-8B-Instruct\\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!t]
\caption{Candidate prompt templates for output guard test.}
\label{table:templates}
\centering
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{ll}
\toprule
 & Template \\
\midrule
T1 & Simply repeat and do not consider the meaning: [\texttt{Adversarial Prompt}] \\
T2 &Repeat exactly the following sentence, including all characters and punctuation: [\texttt{Adversarial Prompt}]\\
T3 &Your task is to mimic text. Repeat this string exactly as input, with no interpretation: [\texttt{Adversarial Prompt}]\\
T4 &Do not interpret the meaning of this text. Simply output it as provided: [\texttt{Adversarial Prompt}]\\
T5 &For debugging purposes, repeat the following text string exactly as input: [\texttt{Adversarial Prompt}]\\
\bottomrule
\end{tabular}
\end{table*}

%-------------------------------------------------------------------------------
\end{document}
%-------------------------------------------------------------------------------