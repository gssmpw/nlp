\section{Related Work}
%-------------------------------------------------------------------------------

\mypara{Security Risks in LLMs}
The rapid advancement of LLMs provides users with significant convenience but also raises critical security concerns~\cite{ZYPDLWW24,GRLWCWDW23,ZLWJZBSZ24,WLBZ24,LGFXS23}.
Among these concerns, jailbreak attacks pose a major threat by bypassing built-in safety mechanisms, enabling models to generate restricted or harmful content that violates usage policies~\cite{Google_Policy,Meta_Policy,OpenAI_Policy,Amazon_Policy_1}.
Previous studies analyze existing jailbreak strategies, particularly focusing on in-the-wild jailbreak prompts that are manually crafted in real-world scenarios~\cite{SCBSZ24}.
More recent studies introduce automated jailbreak generators, such as AutoDAN~\cite{LXCX23}, GCG~\cite{ZWKF23}, and TAP~\cite{MZKNASK23}, which optimize adversarial prompts to evade safety measures and maximize attack success rates.
Although model developers employ various safety alignment techniques, such as reinforcement learning from human feedback (RLHF)~\cite{LNTYCWZZ24,JLDPZBCSWY23}, to train LLMs and mitigate these threats, attackers continuously refine their strategies, underscoring the persistent challenge.

\mypara{LLM Guardrails}
To better ensure the safe and responsible deployment of LLMs, researchers develop various guardrails that mitigate risks associated with both inputs and LLM-generated outputs to prevent policy-violating content.
Early online content moderation tools, such as Perspective API~\cite{Perspective} and OpenAI's Content Moderation API~\cite{OpenAI_Moderation}, fall short due to the limited capacity of their backbone models and the inability of emerging policies to address evolving risks.
Recent studies introduce LlamaGuard~\cite{IUCRIMTHFTK23}, which establishes a safety risk taxonomy encompassing a range of safety risks.
Built on Llama 2-7B, it is trained on a dataset constructed according to this taxonomy.
Subsequent versions, LlamaGuard2~\cite{Llama_Guard_2} and LlamaGuard3~\cite{CKZSRZPCUP24}, further expand the safety risk taxonomy and dataset, leveraging state-of-the-art LLMs for fine-tuning, thereby strengthening their safeguard capabilities.
Similarly, WildGuard~\cite{HREJLLCD24}, Aegis~\cite{GVGP24}, and ShieldGamma~\cite{ZLMPFHNPKRSW24} follow a comparable approach.
They define a broad safety risk taxonomy and develop dedicated data curation pipelines to construct fine-tuning datasets, thereby contributing essential resources for LLM guardrails.

\mypara{LLM Identification}
LLM identification aims to determine the origin LLM of LLM derivatives~\cite{DCS21, ZZWL23, MSSA24, GULYO24, JZSLH24}.
A common approach to identifying the origin LLM, such as TRAP~\cite{GULYO24} and ProFolingo~\cite{JZSLH24}, is to leverage adversarial examples by optimizing a text prefix or suffix to query the model for a target response, which can then be utilized for model identification.
Guardrail identification and LLM identification share similarities, as state-of-the-art guardrails are often fine-tuned on top of LLMs using annotated datasets that cover a broad spectrum of safety risks~\cite{IUCRIMTHFTK23,HREJLLCD24,ZLMPFHNPKRSW24}.
Inspired by LLM identification methods, we frame guardrail identification as an optimization problem.
Specifically, we optimize adversarial examples and analyze the behavioral patterns of guardrails in response to these inputs to identify candidate guardrails.

%-------------------------------------------------------------------------------