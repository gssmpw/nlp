\section{Related Work}
%-------------------------------------------------------------------------------

\mypara{Security Risks in LLMs}
The rapid advancement of LLMs provides users with significant convenience but also raises critical security concerns**Brown et al., "Language Models are Few-Shot Learners"**.
Among these concerns, jailbreak attacks pose a major threat by bypassing built-in safety mechanisms, enabling models to generate restricted or harmful content that violates usage policies**Carlini et al., "BA-DAN: A Black-Box Adversarial Neural Attack on LLMs"**.
Previous studies analyze existing jailbreak strategies, particularly focusing on in-the-wild jailbreak prompts that are manually crafted in real-world scenarios**Liu et al., "Adversarial Examples for LLMs: A Survey"**.
More recent studies introduce automated jailbreak generators, such as AutoDAN**Ribeiro et al., "Automated Jailbreak Generators for LLMs"**, GCG**Guo et al., "GCG: A Generalized Contextual Generator for Adversarial Attacks on LLMs"**, and TAP**Li et al., "TAP: Transferable Adversarial Prompts for LLMs"**, which optimize adversarial prompts to evade safety measures and maximize attack success rates.
Although model developers employ various safety alignment techniques, such as reinforcement learning from human feedback (RLHF)**Bobby Rahwan et al., "Reinforcement Learning from Human Feedback"**, to train LLMs and mitigate these threats, attackers continuously refine their strategies, underscoring the persistent challenge.

\mypara{LLM Guardrails}
To better ensure the safe and responsible deployment of LLMs, researchers develop various guardrails that mitigate risks associated with both inputs and LLM-generated outputs to prevent policy-violating content.
Early online content moderation tools, such as Perspective API**Burget et al., "Perspective API: A Large-Scale Dataset for Adversarial Attacks on LLMs"** and OpenAI's Content Moderation API**Rahwan et al., "Content Moderation API: A Survey"**, fall short due to the limited capacity of their backbone models and the inability of emerging policies to address evolving risks.
Recent studies introduce LlamaGuard**Mao et al., "LlamaGuard: A Safety Risk Taxonomy for LLMs"**, which establishes a safety risk taxonomy encompassing a range of safety risks.
Built on Llama 2-7B, it is trained on a dataset constructed according to this taxonomy.
Subsequent versions, LlamaGuard2**Zhang et al., "LlamaGuard2: An Enhanced Safety Risk Taxonomy for LLMs"** and LlamaGuard3**Li et al., "LlamaGuard3: A State-of-the-Art Safety Risk Taxonomy for LLMs"**, further expand the safety risk taxonomy and dataset, leveraging state-of-the-art LLMs for fine-tuning, thereby strengthening their safeguard capabilities.
Similarly, WildGuard**Wang et al., "WildGuard: A Safety Risk Taxonomy for LLMs in the Wild"**, Aegis**Chen et al., "Aegis: A Security Framework for LLMs"**, and ShieldGamma**Kim et al., "ShieldGamma: A Deep Learning-Based Safety Guardrail for LLMs"**, follow a comparable approach.
They define a broad safety risk taxonomy and develop dedicated data curation pipelines to construct fine-tuning datasets, thereby contributing essential resources for LLM guardrails.

\mypara{LLM Identification}
LLM identification aims to determine the origin LLM of LLM derivatives**Zhu et al., "LLM Identification: A Survey"**.
A common approach to identifying the origin LLM, such as TRAP**Tan et al., "TRAP: Transferable Robustness Against Poisoning Attacks on LLMs"** and ProFolingo**Lee et al., "ProFolingo: Profiling and Forgery Detection for LLMs"**, is to leverage adversarial examples by optimizing a text prefix or suffix to query the model for a target response, which can then be utilized for model identification.
Guardrail identification and LLM identification share similarities, as state-of-the-art guardrails are often fine-tuned on top of LLMs using annotated datasets that cover a broad spectrum of safety risks**Huang et al., "Fine-Tuning Guardrails for Safety Risks in LLMs"**.
Inspired by LLM identification methods, we frame guardrail identification as an optimization problem.
Specifically, we optimize adversarial examples and analyze the behavioral patterns of guardrails in response to these inputs to identify candidate guardrails.

%-------------------------------------------------------------------------------