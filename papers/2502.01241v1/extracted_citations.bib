@misc{Amazon_Policy_1,
howpublished = {\url{https://aws.amazon.com/cn/machine-learning/responsible-ai/policy/}},
}

@article{CKZSRZPCUP24,
author = {Jianfeng Chi and Ujjwal Karn and Hongyuan Zhan and Eric Smith and Javier Rando and Yiming Zhang and Kate Plawiak and Zacharie Delpierre Coudert and Kartikeya Upasani and Mahesh Pasupuleti},
title = {{Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations}},
journal = {{CoRR abs/2411.10414}},
year = {2024}
}

@inproceedings{DCS21,
author = {Nirav Diwan and Tanmoy Chakraborty and Zubair Shafiq},
title = {{Fingerprinting Fine-tuned Language Models in the Wild}},
booktitle = {{Annual Meeting of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL/IJCNLP)}},
pages = {4652-4664},
publisher = {ACL},
year = {2021}
}

@article{GRLWCWDW23,
author = {Yichen Gong and Delong Ran and Jinyuan Liu and Conglei Wang and Tianshuo Cong and Anyu Wang and Sisi Duan and Xiaoyun Wang},
title = {{FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts}},
journal = {{CoRR abs/2311.05608}},
year = {2023}
}

@inproceedings{GULYO24,
author = {Martin Gubri and Dennis Ulmer and Hwaran Lee and Sangdoo Yun and Seong Joon Oh},
title = {{{TRAP:} Targeted Random Adversarial Prompt Honeypot for Black-Box Identification}},
booktitle = {{Annual Meeting of the Association for Computational Linguistics (ACL)}},
publisher = {ACL},
year = {2024}
}

@article{GVGP24,
author = {Shaona Ghosh and Prasoon Varshney and Erick Galinkin and Christopher Parisien},
title = {{{AEGIS:} Online Adaptive {AI} Content Safety Moderation with Ensemble of {LLM} Experts}},
journal = {{CoRR abs/2404.05993}},
year = {2024}
}

@misc{Google_Policy,
howpublished = {\url{https://policies.google.com/terms/generative-ai/use-policy?hl=en}},
}

@article{HREJLLCD24,
author = {Seungju Han and Kavel Rao and Allyson Ettinger and Liwei Jiang and Bill Yuchen Lin and Nathan Lambert and Yejin Choi and Nouha Dziri},
title = {{WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs}},
journal = {{CoRR abs/2406.18495}},
year = {2024}
}

@article{IUCRIMTHFTK23,
author = {Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa},
title = {{Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations}},
journal = {{CoRR abs/2312.06674}},
year = {2023}
}

@inproceedings{JLDPZBCSWY23,
author = {Jiaming Ji and Mickel Liu and Josef Dai and Xuehai Pan and Chi Zhang and Ce Bian and Boyuan Chen and Ruiyang Sun and Yizhou Wang and Yaodong Yang},
title = {{BeaverTails: Towards Improved Safety Alignment of {LLM} via a Human-Preference Dataset}},
booktitle = {{Annual Conference on Neural Information Processing Systems (NeurIPS)}},
publisher = {NeurIPS},
year = {2023}
}

@article{JZSLH24,
author = {Heng Jin and Chaoyu Zhang and Shanghao Shi and Wenjing Lou and Y. Thomas Hou},
title = {{ProFLingo: {A} Fingerprinting-based Copyright Protection Scheme for Large Language Models}},
journal = {{CoRR abs/2405.02466}},
year = {2024}
}

@article{LGFXS23,
author = {Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Yangqiu Song},
title = {{Multi-step Jailbreaking Privacy Attacks on ChatGPT}},
journal = {{CoRR abs/2304.05197}},
year = {2023}
}

@article{LNTYCWZZ24,
author = {Zhendong Liu and Yuanbi Nie and Yingshui Tan and Xiangyu Yue and Qiushi Cui and Chongjun Wang and Xiaoyong Zhu and Bo Zheng},
title = {{Safety Alignment for Vision Language Models}},
journal = {{CoRR abs/2405.13581}},
year = {2024}
}

@article{LXCX23,
author = {Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
title = {{AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models}},
journal = {{CoRR abs/2310.04451}},
year = {2023}
}

@misc{Llama_Guard_2,
howpublished = {\url{https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-guard-2/}},
}

@article{MSSA24,
author = {Hope McGovern and Rickard Stureborg and Yoshi Suhara and Dimitris Alikaniotis},
title = {{Your Large Language Models Are Leaving Fingerprints}},
journal = {{CoRR abs/2405.14057}},
year = {2024}
}

@article{MZKNASK23,
author = {Anay Mehrotra and Manolis Zampetakis and Paul Kassianik and Blaine Nelson and Hyrum Anderson and Yaron Singer and Amin Karbasi},
title = {{Tree of Attacks: Jailbreaking Black-Box LLMs Automatically}},
journal = {{CoRR abs/2312.02119}},
year = {2023}
}

@misc{Meta_Policy,
howpublished = {\url{https://ai.meta.com/llama/use-policy/}},
}

@misc{OpenAI_Moderation,
howpublished = {\url{https://platform.openai.com/docs/guides/moderation/overview}},
}

@misc{OpenAI_Policy,
howpublished = {\url{https://openai.com/policies/usage-policies}},
}

@misc{Perspective,
howpublished = {\url{https://www.perspectiveapi.com}},
}

@inproceedings{SCBSZ24,
author = {Xinyue Shen and Zeyuan Chen and Michael Backes and Yun Shen and Yang Zhang},
title = {{Do Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models}},
booktitle = {{ACM SIGSAC Conference on Computer and Communications Security (CCS)}},
publisher = {ACM},
year = {2024}
}

@inproceedings{WLBZ24,
author = {Rui Wen and Zheng Li and Michael Backes and Yang Zhang},
title = {{Membership Inference Attacks Against In-Context Learning}},
booktitle = {{ACM SIGSAC Conference on Computer and Communications Security (CCS)}},
publisher = {ACM},
year = {2024}
}

@article{ZLMPFHNPKRSW24,
author = {Wenjun Zeng and Yuchi Liu and Ryan Mullins and Ludovic Peran and Joe Fernandez and Hamza Harkous and Karthik Narasimhan and Drew Proud and Piyush Kumar and Bhaktipriya Radharapu and Olivia Sturman and Oscar Wahltinez},
title = {{ShieldGemma: Generative {AI} Content Moderation Based on Gemma}},
journal = {{CoRR abs/2407.21772}},
year = {2024}
}

@inproceedings{ZLWJZBSZ24,
author = {Rui Zhang and Hongwei Li and Rui Wen and Wenbo Jiang and Yuan Zhang and Michael Backes and Yun Shen and Yang Zhang},
title = {{Instruction Backdoor Attacks Against Customized LLMs}},
booktitle = {{USENIX Security Symposium (USENIX Security)}},
publisher = {USENIX},
year = {2024}
}

@article{ZWKF23,
author = {Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
title = {{Universal and Transferable Adversarial Attacks on Aligned Language Models}},
journal = {{CoRR abs/2307.15043}},
year = {2023}
}

@article{ZYPDLWW24,
author = {Xuandong Zhao and Xianjun Yang and Tianyu Pang and Chao Du and Lei Li and Yu-Xiang Wang and William Yang Wang},
title = {{Weak-to-Strong Jailbreaking on Large Language Models}},
journal = {{CoRR abs/2401.17256}},
year = {2024}
}

@article{ZZWL23,
author = {Boyi Zeng and Chenghu Zhou and Xinbing Wang and Zhouhan Lin},
title = {{HuRef: HUman-REadable Fingerprint for Large Language Models}},
journal = {{CoRR abs/2312.04828}},
year = {2023}
}

