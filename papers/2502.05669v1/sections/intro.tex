\section{Introduction}\label{sec:intro}

When simulating stiff objects, rigid body simulators are extremely popular and are often used in lieu of more accurate deformable simulators due to their performance and simplicity. 
Of course, even very stiff materials are deformable at some scale. These deformations change how objects respond to contacts and how internal stresses propagate through an object. Moreover, spatial variations of material properties in an object are not captured by the object-level parameters of a rigid body simulator: mass, center of mass, and moment of inertia. 

\revision{As a result, it is possible that simulating certain objects with rigid body methods can lead to less accurate results. This can be dangerous in contexts where safety of the system is determined by the robustness of the underlying model.}
For example, in machine learning, there is increasing interest in learning physical models (e.g. \cite{physml1,physml2,physml3,physml4}), including rigid body dynamics (e.g. \cite{rbdml1,rbdml2,rbdml3}), for use in various downstream applications where inaccuracies can cause problems.
\revision{In graphics, if a rigid body simulation might be used for previsualization and replaced with an elasticity simulation later, abrupt changes could lead to surprises.} 
In robotics, Sim2Real training (see \cite{sim2real1,sim2real2,sim2real3}) based on vulnerable physics simulation could be dangerous in safety critical tasks.

In deep learning, such vulnerabilities are studied in the form of \emph{adversarial attacks}, where seemingly normal malicious inputs to the network are generated (typically as imperceptible perturbations to a reference input) in order to cause the model to make mistakes. For instance, images with some underlying change such as small modifications to pixel values or lighting can be used to trick image classifiers into misclassifying them, which can have grave consequences if the classifier was used in a safety critical application \cite{chakraborty2021survey, xu2020adversarial}.
%The study of these adversarial attacks has moved from the realm of theory to practical application. 
These attacks exist in physical real-world applications. A few examples include the use of adversarial background music to disrupt the functionality of the voice assistant tool Amazon Alexa \cite{adv-music}, the use of makeup to cause face recognition software to fail \cite{adv-makeup}, and even placing small markings on the road to trick Tesla vehicles' autopilot lane detection software \cite{keen-lab-tesla}. 
In machine learning, researching these adversarial attacks was the first step towards improving safety, eventually leading to adversarial training which turned attacks into strategies to make models more robust.

In this paper, we propose using optimization techniques to construct adversarial objects using physically reasonable materials, which will behave identically to a reference object in rigid body simulation, but maximally different in more physically accurate deformable simulation. As rigid body simulators use only the collision geometry and the moments of the object, the adversarial objects require identical external geometry and first three moments of mass to the reference object so that they are indistinguishable in the rigid body setting.
To achieve this objective, we first define a cost function that encodes the difference in the simulation result of the reference and adversarial object. We choose the degrees of freedom of the optimization to be the object's material distribution and internal geometry. By using the adjoint method we can efficiently compute gradients of the cost with respect to these degrees of freedom, allowing the use of a descent method to determine the internal geometry and materials that minimizes the cost subject to having certain first three moments of mass. 

We demonstrate the efficacy of our method by constructing several adversarial objects and comparing the results of their simulations with their reference in \textsc{Polyfem}, a commercially available simulator. Thus, we show that robotics planning and control tools make a potentially dangerous model assumption when using rigid body simulators.