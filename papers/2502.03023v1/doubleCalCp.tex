%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{cite}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{float}

\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\var}{\text{Var}}
\newcommand{\textscbf}[1]{\textup{\textsc{#1}}}
\newcommand{\Eqref}[1]{Eq.~\eqref{#1}}
\newcommand{\id}[1]{\mathds{1}\{#1\}}

\newcommand{\of}[1]{\left(#1\right)}
\newcommand{\off}[1]{\left[#1\right]}
\newcommand{\offf}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\left|#1\right|}

%%%newrecommand for draft
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% 在导言区添加包
\usepackage{enumitem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand\HW[1]{\textcolor{blue}{[Hongxin: #1]}}
\newcommand\zhadd[1]{\textcolor{orange}{#1}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Parametric Scaling Law of Tuning Bias in Conformal Prediction}

\tolerance=1000

\begin{document}

\twocolumn[
\icmltitle{Parametric Scaling Law of Tuning Bias in Conformal Prediction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao Zeng}{equal,sustech}
\icmlauthor{Kangdao Liu}{equal,sustech,um}
\icmlauthor{Bingyi Jing}{sustech}
\icmlauthor{Hongxin Wei}{sustech}

\end{icmlauthorlist}

\icmlaffiliation{sustech}{Department of Statistics and Data Science, Southern University of Science and Technology}
\icmlaffiliation{um}{Department of Computer and Information Science, University of Macau }


\icmlcorrespondingauthor{Hongxin Wei}{weihx@sustech.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in

]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution}  % TODO
% otherwise use the standard text.

\begin{abstract}
Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees.
To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional hold-out set for parameter tuning.
Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. 
In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. 
In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size.
Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound.
In the end, we discuss how to reduce the tuning bias, guided by the theories we developed.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Quantifying uncertainty in predictions is crucial for the safe deployment of machine learning, particularly in high-risk domains such as financial decision-making and medical diagnostics. 
Conformal prediction stands out as a promising statistical framework for quantifying uncertainty in the predictions of any predictive algorithm~\citep{vovk2005algorithmic, papadopoulos2008inductive, vovk2012conditional}.
It transforms point predictions into prediction sets guaranteed to contain ground truths with a user-specified coverage rate. 
Under the assumption of data exchangeability, these prediction sets offer non-asymptotic coverage guarantees without distributional assumptions.
However, the assumption may not always hold due to the inherent characteristics of the data or its usage in practice, such as time series analysis, domain shifts, and parameter tuning~\citep{barber2023conformal, oliveira2024split}.
Therefore, it is essential to understand and quantify the coverage gap that arises from the violation of exchangeability.


Many conformal prediction methods utilize a hold-out set for \textit{parameter tuning}, in addition to the calibration set. 
For example, 
(1) RAPS~\citep{angelopoulos2021uncertainty} and SAPS~\citep{huang2024conformal} search their parameters on the hold-out set; 
(2) score aggregation methods~\citep{yang2024selection, luo2024weighted, fan2024utopia} optimize the selection or weights for candidate scores;
(3) confidence calibration methods~\citep{guo2017calibration, xi2024does, dabah2024temperature} tune the scale parameters for better calibration;
(4) training methods~\citep{stutz2022learning,liu2025cadapter} fine-tune models on the hold-out set.
Previous works claim that leveraging the same hold-out set for tuning and calibration will destroy the exchangeability~\citep{angelopoulos2021uncertainty, yang2024selection, xi2024does, dabah2024temperature}, but they did not provide a formal quantification on the coverage gap introduced by the violation of exchangeability systematically.
This prompts us to investigate the influence of parameter tuning on the coverage gap in the absence of a hold-out set.

In this work, we reveal a previously unrecognized phenomenon: the tuning bias - the coverage gap introduced by using the same dataset for tuning and calibration, is negligible for simple parameter tuning in conformal prediction methods. % motivation and expected result
Empirically, we find that most methods maintain their coverage rates, except for vector scaling~\citep{guo2017calibration} and the fine-tuning version of ConfTr~\citep{stutz2022learning}.  % exps and results
Furthermore, we observe the \textit{parametric scaling law} of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. % results
An intuitive explanation for this phenomenon is that models with more complex parameter space require more data to tune for the optimal parameters. % explanation 
% Details of the experiments and results are presented in~\cref{sec:emprical_study}.

% We empirically reveal an unobserved phenomenon for conformal prediction methods with \textit{parameter tuning} in \cref{sec:emprical_study}: We conducted the above-mentioned experiments of parameter tuning methods on various classic benchmarks in conformal prediction. We find that \textit{for most methods, the coverage gap (\cref{def:coverage_gap}) introduced by parameter tuning without a hold-out set is negligible, as shown in \cref{fig:motivation}}. 
% \cref{fig:motivation} shows that many commonly used parameter tuning practices maintain coverage rate well, except for vector scaling~\citep{guo2017calibration} and the fine-tuning version of ConfTr method~\citep{stutz2022learning}.
% To investigate factors influencing the coverage gap, we further conducted a more detailed empirical study.
% % We further conduct another empirical study on the coverage gap for \textit{vector scaling} in \cref{sec:emprical_study} for its larger gap and easy-to-control the complexity of the tuning parameter space. 
% We examine how the coverage gap varies with the complexity of parameter space and the size of the calibration set by detailed experimental design.
% We find that this gap is positively associated with the complexity of parameter space and negatively associated with the size of the calibration set.

Formally, we propose a theoretical framework to quantify the tuning bias, which is formulated as a specifically designed constrained ERM problem~\citep{bai2022efficient}. 
In particular, we measure the tuning bias using the classical empirical process within the extended parameter space~\citep{vandervaart1996weak}.  
Furthermore, we derive the upper bounds of tuning bias in the cases of finite and infinite parameter spaces, respectively.
We then provide rigorous proof for the parametric scaling law of tuning bias through the upper bounds, which aligns with empirical observations. 
Finally, we present the tuning biases of parameter tuning in several conformal prediction methods, such as RAPS, score aggregation, and confidence calibration methods.


We further discuss the potential solutions to mitigate tuning bias. 
With the parametric scaling law, we could increase the size of the calibration set or reduce the complexity of the parameter space to reduce the tuning bias. 
The former may not be practical due to data scarcity. 
On the latter, we discuss two cases: one is to reduce the number of parameters to tune; the other is to apply regularization on the parameter space.  
Our main contributions are summarized as follows:

\begin{itemize}
    \item We identify a new phenomenon: simple parameter tuning in many conformal prediction methods leads to negligible tuning bias, where we leverage the same dataset for tuning and calibration.
    % \item However, this observation does not hold for vector scaling and the fine-tuning version of ConfTr, where the increased complexity of their parameter spaces leads to larger coverage gaps.
    \item We present the \textit{parametric scaling law} of tuning bias: this bias increases with parameter space complexity and decreases with calibration set size.
    \item We establish a theoretical framework to quantify tuning bias and rigorously prove the parametric scaling law of tuning bias via its upper bounds.
    \item We discuss the potential application of our theoretical results and explain order-preserving regularization as a possible solution to reduce tuning bias. 
    % We discuss the potential application of our theoretical results by a delicate analysis of conformal prediction with temperature scaling in binary classification. 
\end{itemize}



\section{Background}\label{sec:background}



\paragraph{Conformal prediction} 
% To provide theoretical guarantees for model predictions, 
Conformal prediction \citep{vovk2005algorithmic} aims to produce prediction sets that contain ground-truth labels with a desired coverage rate. 
Let $\mathcal{X} \subset \mathbb{R}^d$ denote the input space and $\mathcal{Y}$ denote the label space. 
Formally, the goal of conformal prediction is to construct a set-valued mapping $\mathcal{C}:\mathcal{X}\rightarrow 2^\mathcal{Y}$ that satisfies the marginal coverage:
\begin{equation}
\label{eq:validity}
    \mathbb{P}(y\in {\C}(\boldsymbol{x}))\geq 1-\alpha,  
\end{equation}
for a user-specified error rate $\alpha\in(0,1)$, where input \(\boldsymbol{x} \in\mathcal{X}\), and output \(y \in \mathcal{Y}\). 
As a widely used procedure, split conformal prediction~\citep{papadopoulos2008inductive} initiates with a calibration step.
For each sample $(\boldsymbol{x}_i, y_i)$ from the calibration set $\mathcal{D}_{\text{cal}}:=\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n$, we compute the \textit{non-conformity score} $s_i := S(\boldsymbol{x}_i, y_i)$ for a score function $S:\X\times\Y\to\mathbb{R}$. 
The non-conformity score function $S$ measures the strangeness of a given sample.  
Based on the scores computed on the calibration set, we search for a threshold $\hat{t}$ such that the probability of observing scores on test samples below the threshold satisfies a pre-specified error rate $\alpha$. 
Specifically, we determine the threshold by finding the $(1-\alpha)$-th empirical quantile of the calibration scores: 
\begin{equation}\label{eq:threshold}
\hat{t}=\inf\offf{s:|\{i:S(\boldsymbol{x}_{i},y_{i})\leq s\}| \geq \lceil(n+1)(1-\alpha)\rceil}.
\end{equation}
For the test step with a given feature of test sample \(\boldsymbol{x}\), the non-conformity score is computed for each label \(y \in \mathcal{Y}\). 
The prediction set \(\widehat{\mathcal{C}}(\boldsymbol{x})\) is constructed by including all labels whose non-conformity scores are below the threshold \(\hat{t}\):
\begin{equation} \label{eq:cp_set}
\widehat{\C}(\boldsymbol{x}) := \widehat{\C}_{\hat{t}}(\boldsymbol{x}) = \offf{y \in \mathcal{Y}: S(\boldsymbol{x},y)\leq\hat{t}}.
\end{equation}
By this construction, the conformal prediction set \(\widehat{\C}(\boldsymbol{x})\) in \Eqref{eq:cp_set} has a finite-sample coverage guarantee with the assumption of exchangeability:
\begin{theorem}\citep{lei2018distributionfree, barber2023conformal}\label{prop:finite-sample_coverage_guarantee}
If the samples from $\mathcal{D}_{\text{cal}}$ and the test sample $(\boldsymbol{x},y)$ are exchangeable, the conformal prediction set defined in \Eqref{eq:cp_set} satisfies
\begin{align*}
1-\alpha\leq\P\of{y\in\widehat{\C}(\boldsymbol{x})}\leq1-\alpha+\frac{1}{n+1}.
\end{align*}
\end{theorem}
If the assumption of exchangeability does not hold, there could be a large deviation in the coverage rate.
We could use the coverage gap to measure the conformal predictor $\C$ for a given error rate $\alpha$ and test sample $(\boldsymbol{x}, y)$: 
\begin{equation*}
    \mathop{CovGap}(\C) := |(1-\alpha) - \P(y \in \C(\boldsymbol{x}))|.
\end{equation*}




\paragraph{Parameter tuning}
Parameter tuning is a common practice in conformal prediction. 
We characterize the \textit{parameter tuning} as follows. 
Given a score function $S$, we can transform it using a function $\sigma$ to obtain $S^{\sigma} := \sigma \circ S \in \{f: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}\}$. 
When this transformation $\sigma$ is parameterized by $\lambda \in \Lambda$, where $\Lambda$ is the parameter space, we denote the transformed score as $S^\lambda$ for simplicity. 
There are some examples: 
\begin{itemize}
    \item RAPS~\citep{angelopoulos2021uncertainty} uses a scalar $\lambda$ and an integral to tune the best score function on a hold-out set, and so does SAPS \citep{huang2024conformal}.
    \item For selection and aggregation of scores problems~\citep{yang2024selection, luo2024weighted, fan2024utopia,ge2024optimal}, the optimal score function or weight vector is obtained from a finite number of candidates.  The selection or weights could be viewed as the transformation parameter \(\lambda\) to tune. 
    \item Conformal prediction with confidence calibration \citep{guo2017calibration, xi2024does, dabah2024temperature} uses a single positive scalar \(\lambda > 0\) or a vector \(\lambda \in \mathbb{R}^K\) to calibrate deep learning classifiers.  
    \item For training models from scratch in conformal prediction, they could be modified as a post-training method.  
    Researchers fine-tune the fully connected layer or an appended layer, and the efficiency of conformal predictors could be enhanced \citep{stutz2022learning, huang2023uncertainty, liu2025cadapter}.  
    The parameters in the layer could be viewed as the transformation parameter.  
\end{itemize}
These practices tune their parameters using a hold-out set that is separate from the one used for conformal prediction calibration.
Given the transformation parameter $\lambda$, we can calculate the threshold $\hat{t}$ in \Eqref{eq:threshold} with parameter $\lambda$.  
If we use the same set for both parameter tuning and conformal prediction calibration, it will destroy the data exchangeability assumption and probably introduce an additional coverage gap. 
We defined the additional coverage gap as:
\begin{definition}[Tuning Bias]
\label{def:tuning_bias}
    For the conformal prediction with parameter tuning, the tuning bias for parameter tuning on the same set as the calibration set is defined as the additional coverage gap caused by the practice, i.e.,
    \begin{equation*}
        \mathop{TuningBias}(\widehat{\mathcal{C}}) =  
        \mathop{CovGap}(\widehat{\mathcal{C}}) - \mathop{CovGap}(\widehat{\mathcal{C}}_{\text{hold-out}}),
    \end{equation*}
    where $\widehat{\mathcal{C}}$ is the conformal predictor with parameter tuned on the same set as the calibration set, and $\widehat{\mathcal{C}}_{\text{hold-out}}$ is the conformal predictor with parameter tuning using a hold-out set separated from the set used for conformal prediction.
\end{definition}
% We will further quantify the tuning bias in the theoretical analysis in Section~\ref{sec:theoretical_results}.
In the next section, we first conduct an extensive empirical study of various conformal prediction methods with parameter tuning. 
Then, we explore how the complexity of parameter space and the size of the calibration set influence tuning bias, respectively. 
We empirically demonstrate the scaling laws of tuning bias on the complexity of parameter space and the size of the calibration set.


\section{Empirical Study}
\label{sec:emprical_study}



\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{Images/motivation_cal_1000}
        % \caption{Size of calibration set: 1000}
    \end{subfigure} 
    % \begin{subfigure}[b]{0.45\textwidth}
        % \includegraphics[width=\textwidth]{Images/motivation_cal_2500}
        % \caption{Size of calibration set: 2500}
    % \end{subfigure}
    \caption{\textbf{Tuning biases of various methods in conformal prediction},  using ResNet-18 on CIFAR-100 at \(\alpha = 0.1\).
    The APS is used except for RAPS, SAPS and SA\@, with a calibration set size of 1000.
    The `hold-out' and `same' denote that we use separate/same datasets for parameter tuning and calibration, respectively.
    \textit{Tuning bias} is the difference between the coverage gaps of models tuned in the setting of \textit{hold-out} and \textit{same}. 
    % \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as the calibration set
    % various parameter tuning methods, with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set. 
    The tuning biases are not large, except for \textbf{VS} and \textbf{ConfTr (ft.)}.}
    \label{fig:motivation}
\end{figure}

\subsection{Tuning Bias of Current Methods}
We empirically investigate the potential influences of the tuning bias. 
% We empirically study the coverage gap of various parameter tuning methods. 
The methods considered here include RAPS~\citep{angelopoulos2021uncertainty}, SAPS~\citep{huang2024conformal}, score aggregation~\citep[SA]{luo2024weighted}, temperature scaling (TS), vector scaling (VS)~\citep{guo2017calibration, xi2024does, dabah2024temperature}, C-Adapter~\citep{liu2025cadapter}, and the fine-tuning version of ConfTr~\citep{stutz2022learning}. 
We conduct experiments on the CIFAR-100 dataset~\citep{krizhevsky2009learning} and use pre-trained model ResNet-18~\citep{he2016deep}. 
The introduction of methods and detailed experimental setup are provided in Appendix~\ref{sec:basic_experimental_setup}. 
The size of the calibration set considered here is 1000.
We use separate or the same datasets for parameter tuning and calibration, respectively.
% For parameter tuning, we use the same set for both parameter tuning and conformal prediction calibration as \textit{same} case and use an equal-sized hold-out set for parameter tuning as \textit{hold-out} case. 
For each experiment, the test set comprises the remaining data.
By default, we use the APS score for experiments involving confidence calibration and conformal training.
% The results are provided in Figure~\ref{fig:motivation}. 

\paragraph{Tuning bias is negligible for simple parameter tuning.}
From Figure~\ref{fig:motivation}, the tuning biases are not significant for most methods, except for vector scaling and the fine-tuning version of ConfTr.  
The tuning bias is not significant for simple parameter tuning as a result of the small size of the parameter space. 
The large tuning bias observed in vector scaling and the fine-tuning version of ConfTr may be due to the large number of parameters requiring tuning. 
This tuning could potentially result in overfitting, where the transformed score distribution is overly adapted to the hold-out set, significantly reducing its performance on the test sample. 
As the size of the calibration set increases, the tuning bias converges to zero because more data helps reduce the generalization bias. 
Following this, we further explore influences on the tuning bias from the perspectives of parameter space and calibration set.  






\begin{figure*}[t]
    \centering
    % \begin{subfigure}[b]{0.48\textwidth}
    %     \includegraphics[width=\textwidth]{Images/cifar100_emprical_freeze_aps.pdf}
    %     \caption{CIFAR-100}
    % \end{subfigure} 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/imagenet_emprical_freeze_aps.pdf}
        \caption{\textbf{The complexity of parameter space.} }    
        % \label{fig:empirical_analysis_parameter_space}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Images/imagenet_emprical_size.pdf}
        \caption{\textbf{The size of the calibration set.} }  
        % \label{fig:empirical_analysis_calibration_set_size}
    \end{subfigure}
    \caption{\textbf{Parametric scaling law, 
    % Coverage gaps over various amounts of unfrozen parameters in \textbf{vector scaling} with an equal-sized hold-out set (\textit{w/ hold-out set}) or without hold-out set (\textit{w/o hold-out set}).
    }
    on (a) the complexity of parameter space and (b) the size of the calibration set, using ResNet-18 on ImageNet using APS\@.
    % The score function used here is APS\@.
    The calibration set size for Figure (a) is 6000.
    % The figure illustrates the tuning bias of conformal prediction with vector scaling \textbf{scales up with the number of unfrozen parameters}.
    The `hold-out' and `same' denote that we use separate/same datasets for parameter tuning and calibration, respectively.
    % Figure (a) shows the tuning bias scales up with the number of unfrozen parameters in ImageNet with calibration set size 6000.
    % Figure (b) shows the tuning bias scales down with the size of the calibration set in ImageNet.
    % The coverage gaps are obtained on the conformal prediction with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set to parameter tuning.
    \textit{Tuning bias} is the difference between the coverage gaps of models tuned in the setting of \textit{hold-out} and \textit{same}.
    The tuning bias scales up with the number of parameters and scales down with the size of the calibration set.
    % The calibration set size is fixed at 1000 for CIFAR-100 and 6000 for ImageNet.
    }
    \label{fig:empirical_analysis}
\end{figure*}
\subsection{Parametric Scaling Laws of Tuning Bias}
We explore the factors influencing the tuning biases.
By the convention to control the number of unfrozen parameters, we focus on confidence calibration using vector scaling.
% We focus on confidence calibration using vector scaling because it is convenient to freeze parameters in vector scaling to control the complexity of parameter space. 
We could control the number of unfrozen parameters by freezing a portion of the parameters in vector scaling. 
We \textit{freeze} a portion of the parameters in vector scaling. 
In detail, we do not let the proportion of the parameters be optimized randomly. 
The ratio of frozen parameters ranges from \(0\%\) to \(40\%\) by \(10\%\) increments. 
The detail of vector scaling is provided in \cref{subsec:parameter_tuning}. 
% To explore the relationship between the tuning bias and the complexity of parameter space,
% After the parameter tuning, the transformed score function \(S^{\hat{\lambda}}(\boldsymbol{x},y)\) is a function of $p^{\text{VS}}_{\hat{\lambda}}(y\mid \boldsymbol{x})$. 
% We use APS in Figure~\ref{fig:empirical_analysis}.
We use separate or the same datasets for parameter tuning and calibration, respectively, with APS.
% Here the basic score used in Figure~\ref{fig:empirical_analysis_parameter_space} is APS~\citep{romano2020classification}:
% As for the THR score~\citep{sadinle2019least} in \Eqref{eq:thr}, the experiment results are provided in \cref{sec:additional_experimental_results}. 
% We use the calibration set for both parameter tuning and conformal prediction calibration as \textit{same} case and use a hold-out set for parameter tuning as \textit{hold-out} case. 
All methods of conformal prediction are under the same error rate $\alpha=0.1$.
To ensure the reliability of our results, we repeated each experiment 30 times. And we reported both the average result and its corresponding standard deviation, respectively. 

\paragraph{The tuning bias scales up with the complexity of parameter space.}
We first consider the tuning bias under a different number of unfrozen parameters in Figure~\ref{fig:empirical_analysis} (a).
As shown in Figure~\ref{fig:empirical_analysis} (a), the tuning bias increases with the number of unfrozen parameters in vector scaling. 
% For CIFAR-100 and ImageNet, the trends are similar.
With a larger number of parameters, the tuning bias increases by more than \(50\%\) compared to the lower number of parameter spaces. 
% Although the size of the calibration set (6000) used in ImageNet is larger than it (1000) in CIFAR-100, the tuning bias in ImageNet is still larger than it is in CIFAR-100. 
% Since the number of classes of CIFAR-100 and ImageNet is \(100\) and \(1000\), respectively, then the number of parameters for vector scaling is \(200\) for CIFAR-100 and \(2000\) for ImageNet. 
% The number of parameters to tune is from \(120\) to \(200\) for CIFAR-100, and from \(1200\) to \(2000\) for ImageNet. 
In general conclusions in learning theory, the higher number of parameters to tune means more complex parameter space without additional constraints. 
The more complex parameter space leads to more generalization bias. 
Here, our empirical results also support this conclusion: \textit{the tuning bias scales with the complexity of parameter space}.

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[width=\textwidth]{Images/cifar100_emprical_size.pdf}
%         \caption{CIFAR-100}
%     \end{subfigure} 
%     \begin{subfigure}[b]{0.49\textwidth}
%         \includegraphics[width=\textwidth]{Images/imagenet_emprical_size.pdf}
%         \caption{ImageNet}
%     \end{subfigure}
%     \caption{\textbf{Parametric scaling law of tuning bias on the size of the calibration set. 
%     % Coverage gaps of the vector scaling over various calibration set sizes: conformal prediction with parameter tuning using an equal-sized hold-out set (\textit{w/ hold-out set}) or without hold-out set (\textit{w/o hold-out set}).
%     }
%     The figure illustrates the tuning bias of conformal prediction with vector scaling \textbf{scales down with the size of the calibration set}.
%     The coverage gaps are obtained on the conformal prediction with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set to parameter tuning.
%     The experiment is conducted with ResNet-18 on CIFAR-100 and ImageNet.
%     The score function used here is APS\@.
%     }
%     
% \end{figure*}
\paragraph{The effect of calibration set size}
Then, we consider the tuning bias under different sizes of the calibration set on Figure~\ref{fig:empirical_analysis} (b).
The calibration set size varies from 6000 to 10000 with an increment of 1000.
The parameter tuning method used here is vector scaling with full parameter space. 
For the size of the calibration set, the tuning bias decreases with the size of the calibration set increasing, as shown in~\cref{fig:empirical_analysis} (b).
The tuning bias decreases about 50\% from the size of 6000 to 10000.
As the size of the calibration set increases, the tuning bias approaches zero.
% The influence of the size of the calibration set on the tuning bias vanishes as the size of the calibration set increases.
The empirical results support the conclusion that \textit{the tuning bias scales down with the size of the calibration set}.


\subsection{Intuitive Explanation}
\label{subsec:intuitive_explanation}
Combining the above two empirical results, we could see the tuning bias scales up with the complexity of parameter space and down with the size of the calibration set empirically.
Based on the framework of learnability of conformal prediction~\citep{gupta2022nested,bai2022efficient}, we could reinterpret the threshold calculation step~\Eqref{eq:threshold} as solving a constrained empirical risk minimization (ERM) problem with a single learnable parameter: 
\begin{align*}
    \hat{t} & = \arg\min_{t} \sum_{i\in\mathcal{I}_{\text{cal}}}|\hat{\C}_{t}(\boldsymbol{x}_i)|/n  \\ 
    & \text{ such that } \sum_{i \in \mathcal{I}_{\text{cal}}} \id{ y_i \notin \hat{\C}_{t}(\boldsymbol{x}_i)}/n \leq \alpha
\end{align*}
where $\mathcal{I}_{\text{cal}}$ is the index set of the hold-out set $\mathcal{D}_{\text{cal}}$.
Then, the tuning parameter in the conformal prediction could be placed into the optimization problem with the learnable parameter \((\lambda, t)\). 
According to the general results in ERM theory, a more complex parameter space requires more data to learn the optimal tuning parameters. 
These results intuitively explain the phenomenon that appears above. 
Further, we will quantify the parametric scaling law of the tuning bias on the complexity of parameter space and the size of the calibration set, respectively, under the constrained ERM framework.
To our knowledge, we are the first to explore this framework for the learnability of tuning parameters in the context of conformal prediction.
Based on the learnability of tuning parameters in the context of conformal prediction, we further establish the theoretical results of the tuning bias in the following section.



In summary, the empirical results reveal that the tuning bias is negligible for simple parameter tuning and scales up with the complexity of parameter space and down with the size of the calibration set. 
We intuitively explain the empirical results using the learnability of tuning parameters in the context of conformal prediction.
Next, we will provide the theoretical results of the tuning bias in the following section.





\section{Theoretical Results}
\label{sec:theoretical_results}
In this section, we provide the theoretical results for the tuning bias. 
We first introduce the problem formulation in Section~\ref{subsec:tuning_bias}. 
We introduce the tuning bias in general cases. 
Then, we consider the theoretical results of the tuning bias in finite and infinite parameter cases in Section~\ref{subsec:finite_parameter_tuning_bias} and Section~\ref{subsec:infinite_parameter_tuning_bias}, respectively. 
The application of these results to some specific cases is also considered. 
% All the proofs are provided in \cref{sec:proof_tuning_bias} - \cref{sec:proof_tuning_bias_contrast_vs_ts}.
% We apply these results to some specific cases mentioned above.  

\subsection{Problem Formulation}
\label{subsec:tuning_bias}


For a score function $S^{\lambda}$ with $\lambda \in \Lambda$, the prediction set is defined as:
\begin{equation*}
    \widehat{\C}^{\lambda}_t(\boldsymbol{x}) := \{y \in \mathcal{Y} : S^\lambda(\boldsymbol{x},y) \leq t\},
\end{equation*}
where $t$ is a threshold. 
Given a dataset $\mathcal{D}$, we denote the set of scores under parameter $\lambda$ as $\mathcal{S}^{\lambda}_{\mathcal{D}} := \{S^\lambda(\boldsymbol{x}_i, y_i) \mid (\boldsymbol{x}_i, y_i) \in \mathcal{D}\}$ and $\mathcal{S}_{\mathcal{D}} := \{S(\boldsymbol{x}_i, y_i) \mid (\boldsymbol{x}_i, y_i) \in \mathcal{D}\}$. 
The associated threshold for $\mathcal{S}^{\lambda}_{\mathcal{D}}$ is defined as:
\begin{equation} \label{eq:threshold_reusing_data}
    \hat{t}^\lambda_\mathcal{D} = Q_{((1-\alpha)(1+1/n))}{(\mathcal{S}^{\lambda}_{\mathcal{D}})},
\end{equation}
where $Q_{p}(\mathcal{S})$ denotes the $p$-th empirical quantile of non-empty set $\mathcal{S}$: \(Q_{p}(\mathcal{S}) = \inf\{q: |\{s \in \mathcal{S}: s \leq q\}| \geq p |\mathcal{S}|\}\).
We formulate the parameter tuning using the same set as the calibration set for conformal prediction as follows: the tuning parameter $\hat{\lambda}$ is selected by minimizing a pre-specified loss function $\ell$ on the calibration set $\mathcal{D}_{\text{cal}}$:
\begin{equation*}
    \hat{\lambda} := \hat{\lambda}_{\mathcal{D}_{\text{cal}}} 
    = \arg\min_{\lambda \in \Lambda} \ell(\lambda, \mathcal{S}^{\lambda}_{\mathcal{D}_{\text{cal}}}). 
\end{equation*}
The conformal prediction set with parameter tuning is: 
\begin{equation}\label{eq:reusing_data_cp}
    \widehat{\C}(\boldsymbol{x}) := \widehat{\C}^{\hat{\lambda}}_{\hat{t}^{\hat{\lambda}}_{\mathcal{D}_{\text{cal}}}}(\boldsymbol{x}).
\end{equation}

\paragraph{Tuning bias} 
While this procedure appears straightforward, using the same set for both parameter tuning and conformal prediction introduces a tuning bias.
To understand this phenomenon, let us examine why the standard split conformal prediction works. 
The validity guarantee in \Eqref{eq:validity} relies on the exchangeability assumption - specifically, the score function $S$ is independent of the calibration set $\mathcal{D}_{\text{cal}}$. 
This independence ensures that the score of a test sample $S(\boldsymbol{x}_{\text{test}}, y_{\text{test}})$ is exchangeable with the scores of calibration samples $\{S(\boldsymbol{x}_i, y_i) \mid (\boldsymbol{x}_i, y_i) \in \mathcal{D}_{\text{cal}}\}$.
However, if we reuse the same set for both parameter tuning and calibration for conformal prediction, this crucial exchangeability property is violated. 
The transformed score $S^{\hat{\lambda}}$ depends on the calibration set $\mathcal{D}_{\text{cal}}$ through the parameter selection process.
Consequently, the scores of test samples lose their exchangeability with the scores of calibration samples, invalidating the coverage guarantee in \Eqref{eq:validity}.
To understand this phenomenon mathematically, we demonstrate the coverage gap for the conformal prediction with parameter tuning by decomposing the coverage gap into two components:
\begin{theorem}[Coverage gap for conformal prediction with parameter tuning]
\label{thm:tuning_bias}
    If the samples in $\mathcal{D}_{\text{cal}} \cup \mathcal{D}_{\text{test}}$ are independent and identically distributed, then the coverage gap of the conformal prediction set with parameter tuning $\widehat{\C}$ defined in \Eqref{eq:reusing_data_cp} are bounded as follows:
    \begin{equation*}
        \mathop{CovGap}(\widehat{\C}, \alpha) \leq \E \mathfrak{R}_{\Lambda} + \varepsilon_{\alpha, n},
    \end{equation*}
    where \(\varepsilon_{\alpha, n}:= \lceil (1+n) (1-\alpha) \rceil/n - (1-\alpha) \geq 0\) is the coverage gap of the standard split conformal prediction set with the size of the calibration set $n$ and error rate $\alpha$, and the term \(\mathfrak{R}_{\Lambda}\) with $\mathcal{T} \subset \mathbb{R}$,
    \begin{equation*}
        \begin{aligned}
            \mathfrak{R}_{\Lambda} := 
            \sup_{\lambda \in \Lambda, t \in \mathcal{T}} \Big|
                & \frac{1}{n} \sum_{i \in [n]} \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} \\
                & - \P\of{S^{\lambda}(\boldsymbol{x}_{\text{test}}, y_{\text{test}})\leq t \mid \mathcal{D}_{\text{cal}}}
            \Big|.
        \end{aligned}
    \end{equation*} 
    
\end{theorem}
The proof of Theorem~\ref{thm:tuning_bias} is provided in \cref{sec:proof_tuning_bias}.
% \begin{remark}
The upper bound of the coverage gap could be decomposed into two components: $\E\mathfrak{R}_{\Lambda}$ and $\varepsilon_{\alpha, n}$. 
% The first one is an additional bound for the coverage gap compared with the gap in the standard conformal prediction containing only the second term $\varepsilon_{\alpha, n}$. 
The first term, $\E\mathfrak{R}{\Lambda}$, introduces an additional bound for the coverage gap, which is not present in the standard conformal prediction framework, containing only the second term, $\varepsilon_{\alpha, n}$. 
% The term $\mathfrak{R}_{\Lambda}$ is the empirical \textit{tuning bias} caused by the parameter tuning. 
As the definition of tuning bias in~\cref{def:tuning_bias}, we could conclude that
\begin{equation*}
    \mathop{TuningBias}(\widehat{\mathcal{C}}) \leq \E \mathfrak{R}_{\Lambda}.
\end{equation*}
In fact $\E \mathfrak{R}_{\Lambda}$ is closely related to the classical empirical process theory~\citep{vandervaart1996weak, vershynin2018highdimensional}. 
We could define the class associated with $\mathfrak{R}_{\Lambda}$ as:
    \begin{equation*}
        \mathcal{H} = \left\{ \id{S^\lambda(\boldsymbol{x},y)\le t}, \lambda\in \Lambda, t\in \mathbb{R} \right\}.
    \end{equation*}
The bound $\E \mathfrak{R}_{\Lambda}$ is the empirical process on the class $\mathcal{H}$: 
\begin{equation*}
    \E \mathfrak{R}_{\Lambda} = 
    \E \sup_{g \in \mathcal{H}} \Big|
         \frac{1}{n} \sum_{i \in [n]} g(\boldsymbol{x}_i, y_i) - \E[g(\boldsymbol{x}_{\text{test}}, y_{\text{test}}) \mid \mathcal{D}_{\text{cal}}]
    \Big|.
\end{equation*}
It depends on the complexity of parameter space $\Lambda$.
Theorem~\ref{thm:tuning_bias} is a general result that applies to any parameter space $\Lambda$ without any constraint. Next, we provide the bound analysis of the tuning bias in finite and infinite parameter space and consider several applications using these results. 



\subsection{Tuning Bias in Finite Parameter Space}
\label{subsec:finite_parameter_tuning_bias}
When the parameter space $\Lambda$ is finite, we bound the tuning bias using classical concentration inequality, Dvoretzky–Kiefer–Wolfowitz inequality~\citep{dvoretzky1956asymptotic}, on the empirical process with the finite union of probability:

% for finite parameter case
\begin{proposition}
    \label{prop:finite_parameter_tuning_bias}
    For a finite parameter space $\Lambda$, we have
    \begin{equation*}
        \mathop{TuningBias}(\widehat{\mathcal{C}}) \leq
        \sqrt{\frac{\log(2|\Lambda|)}{2n}} + \frac{1}{\sqrt{2n}\sqrt{\log(2|\Lambda|)}}.
    \end{equation*}
\end{proposition}
The proof of Proposition~\ref{prop:finite_parameter_tuning_bias} is provided in \cref{sec:proof_finite_parameter_tuning_bias}.
The result in Proposition~\ref{prop:finite_parameter_tuning_bias} is a general result that applies to any finite parameter space $\Lambda$. 
The examples include parameter tuning in some specific score functions~\citep{angelopoulos2021uncertainty,huang2024conformal}, score selection~\citep{yang2024selection} and aggregation~\citep{luo2024weighted} with finite candidates, and early stopping in conformal prediction~\citep{liang2023conformal} and so on.

\paragraph{Application to parameter tuning in RAPS score} The Regularized Adaptive Prediction Sets (RAPS) method~\citep{angelopoulos2021uncertainty} introduces a score function that requires tuning two hyperparameters:
\begin{equation*} \label{eq:raps}
S_{\text{RAPS}}(\boldsymbol{x}, y, p) = S_{\text{APS}}(\boldsymbol{x}, y, p) + \gamma \cdot (o(y, \boldsymbol{x}, p) - k_{\text{reg}})^+,
\end{equation*}
where \(S_{\text{APS}}\) is defined in \Eqref{eq:aps}, \(o(y, \boldsymbol{x}, p)\) represents the rank of probability \(p(y|\boldsymbol{x})\) among all possible classes, \(p(y'|\boldsymbol{x}) \ (y' \in [K])\), and  \(\gamma\) and \(k_{\text{reg}}\) are hyperparameters. 
The notation \((z)^+\) signifies the positive part of \(z\). 
The RAPS method optimize the set \textit{size} or \textit{adaptiveness} with hyperparameters \(\lambda = (\gamma, k_{\text{reg}})^\top\) from a predetermined finite parameter space with\(|\Lambda_{\text{RAPS}}| = M\cdot \bar{K}\) where \(M\) is the number of grid points for \(\gamma\), and \(\bar{K}\) is the pre-specified max number of classes~\citep{angelopoulos2021uncertainty}. 
The tuning bias is bounded by Proposition~\ref{prop:finite_parameter_tuning_bias} as follows:
\begin{corollary}
    \label{corollary:tuning_bias_raps}
    For a classification problem and the hold-out set size \(n\), the tuning bias of RAPS is bounded by:
    \begin{equation*}
        \mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{RAPS}}) \leq \sqrt{\frac{\log(2M\bar{K})}{2n}} + \frac{1}{\sqrt{2n}\sqrt{\log(2M\bar{K})}}.
    \end{equation*}
\end{corollary}
The proof of Corollary~\ref{corollary:tuning_bias_raps} is provided in \cref{sec:proof_tuning_bias_raps_score_aggregation}.
Although the RAPS method~\citep{angelopoulos2021uncertainty} recommends using an additional hold-out set for hyperparameter selection, our findings allow for direct hyperparameter selection without the need for an extra hold-out set.
As for SAPS~\citep{huang2024conformal}, the parameter space is also finite, and we could also apply the same analysis to SAPS\@.

\paragraph{Application to score selection with finite candidate}
Considering there are $M$ candidate score functions $\{S_m, m \in [M]\}$ for constructing conformal prediction sets, let $\mathcal{C}_m(\boldsymbol{x})$ be the prediction set obtained using score function $S_m$ at a given error rate $\alpha$. 
% The oracle score function is selected by minimizing the expected size of the prediction set:
% \begin{equation*}
%     m^* = \arg\min_{m \in \{1,\ldots,M\}} \mathbb{E}[|\mathcal{C}_m(\boldsymbol{x})|]
% \end{equation*}
% where $|\mathcal{C}_m(\boldsymbol{x})|$ denotes the size of the prediction set. 
The optimal score function is selected by minimizing the empirical size of the prediction set:
\begin{equation*}
    \hat{m} = \arg\min_{m \in \{1,\ldots,M\}} \sum_{i \in \mathcal{I}_{\text{cal}}}|\mathcal{C}_m(\boldsymbol{x}_i)|/|\mathcal{I}_{\text{cal}}|.
\end{equation*}
This selection process introduces a tuning bias that can be bounded using Proposition~\ref{prop:finite_parameter_tuning_bias} with $|\Lambda| = M$. 
This case is also considered by the score selection issues~\citep{yang2024selection}.  
They propose a score selection framework for constructing minimal conformal prediction regions by selecting optimal score functions from finite candidates. 
Their work introduces two methods. 
The first optimizes prediction width using the same hold-out set as the calibration set of conformal prediction, and the second maintains coverage guarantees using a separate hold-out set. 
The first achieves minimal prediction interval width with approximate coverage guarantees. 
It provides a bound of coverage gap for the first method, as a special case of our result:
\begin{corollary}
    \label{corollary:tuning_bias_score_aggregation}
    For a classification problem, and the hold-out set size \(n\), the tuning bias of selection of score functions~\citep{yang2024selection} with the number of candidates \(M\) is bounded by:
    \begin{equation*}
        \mathop{TuningBias}(\widehat{\mathcal{C}}) \leq \sqrt{\frac{\log(2M)}{2n}} + \frac{1}{\sqrt{2n}\sqrt{\log(2M)}}.
    \end{equation*}
\end{corollary}
The proof of Corollary~\ref{corollary:tuning_bias_score_aggregation} is provided in \cref{sec:proof_tuning_bias_raps_score_aggregation}.
For the score aggregation~\citep{luo2024weighted}, we analyze it in \cref{sec:additional_theoretical_results}. 
Next, we extend our analysis to an infinite parameter space and apply it to conformal prediction with confidence calibration, where the parameter space is infinite.

\subsection{Tuning Bias in Infinite Parameter Space}
\label{subsec:infinite_parameter_tuning_bias}
Before introducing the infinite parameter case, we first introduce the concept of Vapnik-Chervonenkis (VC) dimension~\citep{vapnik1971uniform}. 
VC-dimension is a measure of the complexity of classes of Boolean functions. 
\begin{definition}[VC Dimension]
Let $\mathcal{F}$ be a class of Boolean functions defined on a domain $\Omega$,  $f: \Omega \to \{0,1\}$ for all $f \in \mathcal{F}$. A subset $\Omega' \subseteq \Omega$ is said to be \emph{shattered} by $\mathcal{F}$ if any function $g: \Omega' \to \{0,1\}$ can be obtained by restricting some function $f \in \mathcal{F}$ onto $\Omega'$. The VC dimension of $\mathcal{F}$, denoted by $\text{VC}(\mathcal{F})$, is the size of the largest subset $\Omega' \subseteq \Omega$ that can be shattered by $\mathcal{F}$. If the size of the largest shattered subset is infinite, we say the VC dimension is infinite.
\end{definition}
Although the concept may seem hard to understand, it can be viewed as the maximum number of points that the class of functions can shatter intuitively. 
With a more complex class of functions, it is more likely to shatter a larger number of points. 
It is a measure of the complexity of the class of functions, \(\mathcal{F}\).
The intuition is that the larger the complexity of the class of functions, the larger the bound of the empirical process is. 
By the definition of \(\mathcal{H}\) and Lemma~\ref{lemma:infinite_parameter_empirical_process} in Appendix~\ref{sec:useful_lemmas}, 
% we could have \(\E \mathfrak{R}_{\Lambda} \lesssim\sqrt{{\text{VC}(\mathcal{H})}/{n}}\). 
we develop a general bound of the tuning bias:
\begin{proposition}
    \label{prop:infinite_parameter_tuning_bias}
    For an infinite parameter space $\Lambda \subset \mathbb{R}^d$, we have the tuning bias is bounded by:
    \begin{equation*}
        \mathop{TuningBias}(\widehat{\mathcal{C}}) \leq C\sqrt{\frac{d+1}{n}}
    \end{equation*}
    for some universal constant $C > 0$.
\end{proposition}
The proof of Proposition~\ref{prop:infinite_parameter_tuning_bias} is provided in \cref{sec:proof_infinite_parameter_tuning_bias}.
The bound in Proposition~\ref{prop:infinite_parameter_tuning_bias} is a general result that applies to any infinite parameter space $\Lambda \subset \mathbb{R}^d$. 
It shows that the tuning bias increases with the dimension of parameter space $d$ and decreases with the size of the hold-out set $n$.
Next, we will present an application for confidence calibration.

\paragraph{Application to confidence calibration} 
Consider a classification problem with $K$ classes, where we have two confidence calibration methods: Temperature scaling (TS) and vector scaling (VS) \citep{guo2017calibration, xi2024does, dabah2024temperature}. 
The details of temperature scaling and vector scaling with tuning parameter $\lambda$ are given in \cref{subsec:parameter_tuning}.
For the two methods, there is a significant difference in the dimension of parameter space $\Lambda$:
For temperature scaling, the parameter space is $\Lambda_{\text{TS}} = \mathbb{R}^+$, containing only one positive real number $\lambda$. 
For vector scaling, the parameter space is $\Lambda_{\text{VS}} = \mathbb{R}^{K} \times \mathbb{R}^K$, containing $2K$ parameters.
Since vector scaling has a more complex parameter space compared to temperature scaling, the tuning bias of vector scaling is larger than that of temperature scaling:
\begin{corollary}\label{corollary:tuning_bias_contrast_vs_ts}
    For a classification problem with $K$ classes, the calibration set size $n$ and the same pre-trained model, the tuning bias of temperature scaling is smaller than that of vector scaling: $\mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{TS}}) \leq \mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{VS}})$,
    where $\widehat{\mathcal{C}}_{\text{TS}}$ and $\widehat{\mathcal{C}}_{\text{VS}}$ are the prediction sets of temperature scaling and vector scaling, respectively.
\end{corollary} 
The proof of Corollary~\ref{corollary:tuning_bias_contrast_vs_ts} is provided in \cref{sec:proof_tuning_bias_contrast_vs_ts}. The result shows that the tuning bias of temperature scaling is smaller than that of vector scaling. We provide a result for the binary classification case in \cref{sec:discussion}, where the tuning bias of temperature scaling is zero.



In this section, we derive the upper bounds of tuning bias in finite and infinite spaces, proving the parametric scaling law of tuning bias, and provide examples including RAPS, score selection and confidence calibration. 
% Furthermore, we present the tuning biases of various concrete scenarios, including RAPS, score aggregation, and confidence calibration. 
Next, we discuss how to reduce the tuning bias with two specific cases. 

\section{Discussion of Potential Solutions}
\label{sec:discussion}
In the previous analysis, we demonstrate the parametric scaling law of tuning bias in relation to parameter space complexity and calibration set size.
Here, we provide an initial discussion of potential solutions to mitigate the tuning bias that arises from using the same data for both calibration and parameter tuning. 
It is important to note that the common practice is to use separate hold-out datasets for calibration and parameter tuning.
Mitigating tuning bias can be particularly beneficial in real-world applications when splitting the dataset is impractical due to data scarcity.
% This might be useful in practical applications, especially in data-scarce scenarios.


Following the scaling law, we can implement two principles to reduce the tuning bias: increasing the size of calibration sets or reducing the complexity of tuning parameters. The former indicates that we can decrease the bias by collecting more data points for the calibration set, but it is challenging in data-scarce scenarios. We then focus on the latter~-~reducing the complexity of tuning parameters, which is commonly implemented to address overfitting issues. In the following, we discuss this principle with two special cases of parameter tuning in split conformal prediction.


\begin{table}[ht!]
    \centering
    \caption{\textbf{Tuning bias comparison of various methods.} The calibration set size is 2500 for CIFAR-100 and 5000 for ImageNet using APS\@. Results in \textbf{bold} indicate superior performance.}
    \label{tab:performance}
    \renewcommand\arraystretch{1.2}
    \resizebox{0.500\textwidth}{!}{
    \setlength{\tabcolsep}{5mm}{
    \begin{tabular}{lcc}
        \toprule
        Methods & CIFAR-100 & ImageNet \\
        \midrule
        Temperature scaling &\textbf{ 0.14\%} & \textbf{0.04\%} \\
        Vector scaling & 1.13\% & 6.63\% \\
        \midrule
        ConfTr (ft.) w/ \ OP & \textbf{0.04\%} & \textbf{0.02\%} \\
        ConfTr (ft.) w/o OP & 5.5\% & 21.3\% \\
        \bottomrule
    \end{tabular}    
    }
    }
\end{table}
% \paragraph{Temperature scaling vs. vector scaling}
\paragraph{Reducing the parameter number}
A straightforward way to constrain parameter complexity is to reduce the number of parameters in model tuning. 
Here, we provide an example of parameter reduction through weight sharing. In particular, we show the benefits of weight sharing by comparing the tuning biases of temperature scaling (TS) and vector scaling (VS), where the former can be viewed as a special case of vector scaling with a shared parameter $|\Lambda_{\mathrm{TS}}|<|\Lambda_{\mathrm{VS}}|$.
The empirical results in \cref{tab:performance} show that TS with fewer parameters achieves much smaller tuning biases than VS\@. 
Theoretically, we provide a theoretical result for the comparison in the case of binary classification:
\begin{proposition}
    \label{prop:tuning_bias_ts_vs}
    For a binary classification problem, we have $\mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{TS}}) = 0 \leq \mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{VS}})$,
    where $\widehat{\mathcal{C}}_{\text{TS}}$ and $\widehat{\mathcal{C}}_{\text{VS}}$ are the prediction sets of models tuned by temperature scaling and vector scaling, respectively.
\end{proposition}
We provide the proof of the above proposition in~\cref{sec:proof_tuning_bias_ts_vs}. 
From the example, we demonstrate that designing tuning algorithms with few parameters will be beneficial in preventing the tuning bias.
However, reducing the parameter number may severely degrade the performance of parameter tuning, making it inapplicable in some scenarios.


% \paragraph{Fine-tuning methods}
\paragraph{Regularization in the tuning}
Another effective approach is the use of regularization methods, which introduce additional constraints or penalties to model parameters, predictions, or loss functions. 
Here, we present an example of applying an order-preserving constraint to model predictions: 
\(\forall (\boldsymbol{x}, y), o(y, \boldsymbol{x}, p) = o(y, \boldsymbol{x}, p_\lambda)\) where \(o(y, \boldsymbol{x}, p)\) is the order of the true label \(y\) in the sorted predicted probabilities \(p(\cdot|\boldsymbol{x})\), and \(p_\lambda\) is the probability after transformation \(\lambda\).
Taking ConfTr \citep{stutz2022learning} as the tuning algorithm on the final linear layer, we present the following proposition to show the benefit of the order-preserving constraint. 
% In conformal prediction, we present 
% Second, we discuss the fine-tuning methods used in conformal prediction to show \textit{the effect of regularization}.
% By fine-tuning the fully connected layer or an appended layer, we could modify the training method used in the conformal prediction, like ConfTr~\citep{stutz2022learning}. 
% Before fine-tuning, a regularization method could be applied to the practice. 
% For example, we could apply the order-preserving regularization to the pre-trained model and then fine-tune the parameters in the layer, and we could have the following proposition:
\begin{proposition}
    \label{prop:tuning_bias_order_preserving}
    For a classification problem with $K$ classes, and the calibration set size $n$, we have 
    the tuning bias of \(\widehat{\mathcal{C}}_{\text{op}}\) is less than the one of \(\widehat{\mathcal{C}}\):
    \(\mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{op}}) \leq \mathop{TuningBias}(\widehat{\mathcal{C}})\),
    where $\widehat{\mathcal{C}}$ and $\widehat{\mathcal{C}}_{\text{op}}$ represent the prediction sets of models tuned by ConfTr, with $\widehat{\mathcal{C}}_{\text{op}}$ under the order-preserving constraint and $\widehat{\mathcal{C}}$ without it.
\end{proposition}
The proof of Proposition~\ref{prop:tuning_bias_order_preserving} is provided in \cref{sec:proof_tuning_bias_order_preserving}.
We also verify the argument by the empirical results shown in \cref{tab:performance}. 
The tuning bias of ConfTr with order-preserving constraint is much smaller than that without it.
This approach is advantageous as it can generalize to complex paradigms of parameter tuning (e.g., fine-tuning the final layer). 
% It is worth noting that designing different regularization tailored to specific tuning algorithms in conformal prediction, may being an interesting direction for future research.
It is worth noting that designing tailored regularization techniques for specific tuning methods in conformal prediction may be a promising direction for future research.

% In this section, we discuss the potential solutions for mitigating the tuning bias in conformal prediction. 

% Besides, the performance gap between C-Adapter~\citep{liu2025cadapter} and ConfTr\citep{stutz2022learning} presented in~\cref{fig:motivation} also supports our result as C-Adapter .
% Totally, the above discussion shows that applying the regularization may reduce the tuning bias. 

% In this section, we have discussed the potential solutions to reduce the tuning bias from the perspective of the parametric scaling law.
% We focus on two specific cases of reducing the complexity of parameter space, reducing the number of parameters or applying regularization. 
% We hope the discussion could help mitigate tuning bias in data-scarce scenarios.

\section{Related Works}
\label{sec:related_work}

% cp -> parameter tuning in cp -> our gap 
\paragraph{Conformal prediction}
Conformal prediction is a method for uncertainty quantification that ensures that the prediction intervals or sets cover the true label with a user-defined error rate~\citep{papadopoulos2008inductive, vovk2005algorithmic, vovk2012conditional}. 
% It could be used for regression~\citep{lei2018distributionfree} and classification~\citep{sadinle2019least}. 
On the one hand, conditional coverage validity is a key property of conformal prediction, ensuring the algorithm fairness~\citep{angelopoulos2021uncertainty, gibbs2024conformal, romano2020classification,huang2024conformal}.
On the other hand, to enhance the efficiency of conformal prediction, recent studies~\citep{liu2025cadapter, stutz2022learning, kiyani2024length, kiyani2024conformal} have proposed training-based methods, which could be regarded as adapters, performing tuning prior to conformal prediction.
Beyond these, with many non-conformity scores proposed, the score selection or aggregation is another challenge in conformal prediction~\citep{yang2024selection, luo2024weighted, fan2024utopia, gasparin2024conformal, ge2024optimal, qin2024sat}.
These recent paradigms of conformal prediction typically require tuning some parameters on a hold-out dataset, and our work provides the first study to quantify the negative effect induced by using the same dataset for tuning and calibration.
% Though a great deal of work has been researched for efficiency or (conditional) coverage validity, the tuning bias of parameter tuning in conformal prediction is still an open problem. 


% PAC and learnability of cp 1) PAC 2) 引出 CP 使用 3） our work uses PAC 
\paragraph{Learnability}
As classical learnability theory, a learnable model could be regarded as a risk minimization model with a specific hypothesis class~\citep{vapnik1971uniform,vapnik1991principles,vandervaart1996weak, vapnik1999overview, vershynin2018highdimensional}.
And further, the constrained risk minimization model is developed as a special case of learnability theory, where the hypothesis class is the set with a specific constraint or structure, such as structural risk minimization of character recognition~\citep{guyon1991structural}, of data-dependent hypothesis class~\citep{shawe-taylor1998structural}, of Rademacher penalty method~\citep{koltchinskii2001rademacher} and rough set-based classifier~\citep{liu2020structural}.
As for conformal prediction, several works study learn the conformal prediction directly by designing a specific objective function~\citep{stutz2022learning, noorani2024conformal}. 
Further, the conformal prediction is a special case of risk minimization problem with the constraint of coverage guarantee, and the generalization loss for the size of prediction set could be quantified~\citep{gupta2022nested, yang2024selection}, including classification~\citep{bai2022efficient} and regression~\citep{gupta2022nested, fan2024utopia}.
As we proposed in~\cref{sec:theoretical_results}, we interpret the tuning bias as a further constrained risk minimization problem of conformal prediction and provide the first theoretical analysis of the tuning bias.
% In this work, we interpret the tuning bias as a further constrained ERM problem of conformal prediction. 
% In this work, we are the first to interpret the tuning bias as a further constrained ERM problem of conformal prediction.

% The issue of tuning bias could also be interpreted as a special case of learnability, where the hypothesis class is the set of all possible indicator functions that score below the thresholds with transformation parameters.
% Though a great deal of work has been done on learnability in conformal prediction, the research on learnability in tuning bias remains blank.




\section{Conclusion}
\label{sec:conclusion}

% In this paper, we first reveal an unobservable phenomenon in the tuning bias of the conformal prediction with parameter tuning.
% Empirically, we find that the tuning bias is negligible when the complexity of parameter space is low, and the hold-out set is large enough. 
% Our theoretical results build the parametric scaling law of tuning bias.
% Furthermore, we discuss the potential solutions to our theoretical results, focusing on how to mitigate the tuning bias by reducing the complexity of parameter space.
% For future research, incorporating the structure of parameter space may enhance the precision of our theoretical results.
In this paper, we first uncover an unobservable phenomenon related to the tuning bias in conformal prediction with parameter tuning. 
Empirically, we observe that the tuning bias is negligible when the parameter space's complexity is low, and the hold-out set is sufficiently large. 
Our theoretical findings establish a parametric scaling law for the tuning bias. 
Additionally, we explore potential solutions to address the implications of our theoretical results, emphasizing strategies to mitigate tuning bias by reducing the complexity of the parameter space.
For future research, incorporating the structure of the parameter space could improve the precision of our theoretical results.



\paragraph{Limitation}
Our theoretical results provide a unified framework for understanding tuning bias - a general phenomenon in conformal prediction. 
However, the bound of tuning bias is not tight without more assumptions on the modelling. 
The bound could be more precise with more assumptions on the modelling. 
Here, we provide an example of regularization on the learnability for conformal prediction, which roughly tightens tuning bias. 
The tuning bias could be more precise under more assumptions on the modelling, such as the structure of the score and transformation. 
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{doubleCalCp}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn




\section{Basic Experimental Setup}
\label{sec:basic_experimental_setup}
In this section, we provide a detailed experimental setup. In Appendix~\ref{subsec:pre-trained_models}, we describe the pre-trained models. In Appendix~\ref{subsec:parameter_tuning}, we describe the parameter tuning processes for confidence calibration, score function tuning, score aggregation and ConfTr.  
% And for the empirical study in Section~\ref{sec:emprical_study}, we introduce the detailed experimental setup in Appendix~\ref{sec:detailed_emprical_experiments}. For the synthetic data experiment in Section~\ref{sec:synthetic}, we introduce the detailed experimental setup in Appendix~\ref{sec:detailed_synthetic_experiments}.

\subsection{Pre-trained Models}
\label{subsec:pre-trained_models}
We conduct this empirical study on three image classification benchmarks: CIFAR-10, CIFAR-100, and ImageNet. 
For CIFAR-10, we use a simple CNN consisting of three convolutional layers and two fully connected layers. 
For CIFAR-100 and ImageNet, we employ ResNet-18~\citep{he2016deep}. 
The ImageNet experiments utilize pre-trained classifiers available in TorchVision~\citep{paszke2019pytorch}, while the CIFAR-10 and CIFAR-100 classifiers are trained from scratch using their respective full training sets. 
For CIFAR-100, the network is trained for 200 epochs using SGD with a momentum of 0.9, a weight decay of 0.0005, and a batch size of 256. 
The initial learning rate is set to 0.1 and decreases by a factor of 5 at epochs 60, 120, and 160. 
Similarly, for CIFAR-10, the network is trained for 120 epochs using SGD with identical momentum, weight decay, and batch size settings. The initial learning rate is also set to 0.1 and decreases by a factor of 5 at epochs 30, 60, and 90.


\subsection{Setup for Parameter Tuning}
\label{subsec:parameter_tuning}
\paragraph{Confidence calibration} 
We investigate two commonly used confidence calibration methods, including temperature scaling and vector scaling~\citep{guo2017calibration, xi2024does, dabah2024temperature}. 
Temperature scaling is the simplest extension for Platt scaling~\citep{platt1999probabilistic} and is a calibration method that adjusts the model's output logits through a scalar parameter $\lambda$.
The transformed output probability is defined as a special case of vector scaling with tuning parameter $\lambda \in \mathbb{R}^{+}$:
\begin{equation}
    \label{eq:ts}
    p^{\text{TS}}_{\lambda}(y\mid \boldsymbol{x}) = \psi_y(f(\boldsymbol{x})/\lambda),
\end{equation}
where $f(\boldsymbol{x})$ is the original logit output from the pre-trained deep classifier model in Section~\ref{subsec:pre-trained_models}, and $\psi_y$ denotes the $y$-th element of the softmax function:
\begin{equation}
    \label{eq:softmax}
    \psi_j(z) = \frac{\exp(z_j)}{\sum_{k=1}^K \exp(z_k)}, \quad \text{for } j=1,\ldots,K,
\end{equation}
where $z \in \mathbb{R}^K$ is the input logit vector and \(z_j\) is the $j$-th component of the input logit vector and $\psi_j(z)$ represents the $j$-th component of the output probability vector.  
% Vector scaling is a more general calibration method that adjusts the model's output logits through a vector parameter $\lambda$ and the transformed output probability in \Eqref{eq:vs}.
The vector scaling~\citep{guo2017calibration} for a logits output of the pre-trained deep classifier model $f(\boldsymbol{x}) \in \mathbb{R}^K$ with classes \(K\) is defined as:
\begin{equation}
    \label{eq:vs}
    p^{\text{VS}}_{\lambda}(y\mid \boldsymbol{x}) = \psi_y(W \circ f(\boldsymbol{x}) + b),
\end{equation}
where \(\lambda = (W^\top, b^\top)^\top\in \mathbb{R}^{2K}\), the notation ``\(\circ\)'' means element-wise product of elements in two vectors.
We optimize the parameter \(\lambda\) using the objective negative log-likelihood (NLL) to obtain the optimal parameter 
\begin{equation*}
\ell_{\text{NLL}, p}(\lambda) = -\frac{1}{n}\sum_{(\boldsymbol{x},y)\in\mathcal{D}_{\text{cal}}} \log  (p_{\lambda}(y\mid \boldsymbol{x})),
\end{equation*}
where \(p_{\lambda}(y\mid \boldsymbol{x})\) is the transformed output probability from temperature scaling or vector scaling.
The transformed score function \(S^{\lambda}(X,y)\) is a function of \(p(y |X )\) based on some base score function, such as APS~\citep{romano2020classification} defined 
as:
\begin{equation}
    \label{eq:aps}
    \begin{aligned}
        S_{\text{APS}}(\boldsymbol{x}, y, p) = \sum_{y_i \in \mathcal{Y}} 
        p(y_i|\boldsymbol{x}) \cdot 
        \mathbb{I}\{p(y_i|\boldsymbol{x}) > p(y|\boldsymbol{x})\} + u \cdot p(y|\boldsymbol{x})
    \end{aligned}
\end{equation}
given probability output \(p := p(y|\boldsymbol{x})\) for a classification task, and \(u\) is a random variable drawn from the uniform distribution \(U[0,1]\).
and THR~\citep{sadinle2019least} defined as:
\begin{equation}
    \label{eq:thr}
S_{\text{THR}}(\boldsymbol{x}, y, p) = 1 - p(y\mid \boldsymbol{x}).
\end{equation}



\paragraph{Score function tuning}
Score function tuning is a common step in the pipeline of conformal prediction. In this study, we focus on two of the most widely used score functions: RAPS~\citep{angelopoulos2021uncertainty} and SAPS~\citep{huang2024conformal}:
\begin{equation*}
    S_{\text{SAPS}}(\boldsymbol{x}, y, p) := 
    \begin{cases} 
        u \cdot p_{\max}(\boldsymbol{x}), & \text{if } o(y, \boldsymbol{x}, p) = 1, \\
        p_{\max}(\boldsymbol{x}) + (o(y, p(\boldsymbol{x})) - 2 + u) \cdot \gamma, & \text{else},
    \end{cases}
\end{equation*}
where \( p_{\max}(\boldsymbol{x}) \) is the maximum predicted probability among all classes for input \( \boldsymbol{x} \), \( o(y, \boldsymbol{x}, p) \) is the order of the true label \( y \) in the sorted predicted probabilities \(p(y|\boldsymbol{x})\), and \( \gamma \) is a tuning parameter.

A two-stage grid search for parameter searching is performed for both RAPS and SAPS. Initially, the search is conducted over the range $[0.01, 0.3]$ with a step size of 0.01 to determine the optimal parameter $x$. Subsequently, a finer search is carried out within $[x - 0.005, x + 0.005]$ using a granularity of 0.001. Notably, since RAPS is not sensitive to its parameter $k_{\text{reg}}$, we focus on $\gamma$.




\paragraph{Score aggregation}   
Score aggregation could significantly enhance the performance of conformal predictors. Recent studies~\citep{yang2024selection, luo2024weighted, fan2024utopia, gasparin2024conformal, ge2024optimal, qin2024sat} have proposed aggregation techniques that combine multiple non-conformity scores with tunable weights, which can enhance the efficiency of conformal predictors. These methods typically require a hold-out set to search for the optimal weights through grid search. In this study, we investigate the aggregation of three score functions—APS, RAPS, and SAPS—all derived from the same pre-trained classifier as a case example. Specifically, we use three non-negative parameters $w_1$, $w_2$, and $w_3$, which represent the tunable weights for aggregating the three score functions. These parameters satisfy the constraint $w_1 + w_2 + w_3 = 1$, and we evaluate all possible value combinations with a granularity of $0.1$. The parameters $w_1$, $w_2$, and $w_3$ are restricted to the set $\{0, 0.1, 0.2, \dots, 1\}$. We enumerate all triplets $(w_1, w_2, w_3)$ such that:
\[
w_1 + w_2 + w_3 = 1, \quad w_1, w_2, w_3 \geq 0, \quad \text{and} \quad w_1, w_2, w_3 \in \{0, 0.1, 0.2, \dots, 1\}.
\]
This approach ensures that all feasible combinations of $w_1$, $w_2$, and $w_3$ are comprehensively explored under the given constraint and granularity.




\paragraph{C-Adapter} Conformal Adapter (C-Adapter) \cite{liu2025cadapter} is an adapter-based tuning method designed to enhance the efficiency of conformal predictors without sacrificing accuracy. It achieves this by appending an adapter layer to pre-trained classifiers for conformal prediction. With C-Adapter, the model tends to produce extremely high non-conformity scores for incorrect labels, thereby improving the efficiency of prediction sets across various coverage rates.

For tuning the C-Adapter, we use the Adam optimizer \cite{kingma2017adam} with a batch size of 256 and a learning rate of 0.1. The model is tuned for 10 epochs, and the only parameter, \( T \), is set to \( 1 \times 10^{-4} \) by default.  In our empirical study, we explore the application of C-Adapter using hold-out data.



\paragraph{Conformal Training} Conformal Training (ConfTr) \cite{stutz2022learning} is a training framework designed to enhance the efficiency of conformal predictors. Its loss function is defined as:  
\begin{equation*}
\mathcal{L}_{\text{ConfTr}}(f(\boldsymbol{x}; \boldsymbol{\theta}), y, \tau^{\text{soft}}_\alpha) = \mathcal{L}_{\text{cls}}(f(\boldsymbol{x}; \boldsymbol{\theta}), y) + \lambda \mathcal{L}_{\text{size}}(f(\boldsymbol{x}; \boldsymbol{\theta}), \tau^{\text{soft}}_\alpha),
\end{equation*}  
where \(\mathcal{L}_{\text{cls}}\) represents the classification loss, \(\mathcal{L}_{\text{size}}\) approximates the size of the prediction set at a coverage rate of \(1 - \alpha\), \(\tau^{\text{soft}}_\alpha\) denotes the soft threshold, and \(\lambda\) controls the strength of the regularization term.  
Here, \(\tau^{\text{soft}}_\alpha\) denotes the soft threshold and \(\lambda\) controls the strength of the regularization term. To improve efficiency, ConfTr adjusts the training objective by incorporating a regularization term that minimizes the average prediction set size at a specified error rate. 

Beyond training models from scratch, ConfTr can also function as a post-training adjustment method, allowing fine-tuning of the fully connected layer or an appended layer to enhance further the efficiency of conformal predictors \citep{stutz2022learning, huang2023uncertainty}. We denote this version of ConfTr as \textbf{ConfTr (ft.)}. In our empirical study, we explore this application of ConfTr (ft.) using hold-out data. Specifically, we append a fully connected layer to the trained classifier and fine-tune only the parameters of this layer using the Adam optimizer \citep{kingma2017adam} with a learning rate of 0.001. The hyperparameter \(\alpha\) is set to 0.01 by default. For \(\lambda\), we use a value of 0.0001. 
  





% \section{Details of the experiments in Section~\ref{sec:introduction}}
% \label{sec:detailed_demo_experiments}



% \section{Details of the experiments in Section~\ref{sec:emprical_study}}
% \label{sec:detailed_emprical_experiments}
% All the basic settings remain the same as in Appendix \ref{sec:basic_experimental_setup}.

% \section{Details of the experiments in Section~\ref{sec:synthetic}}
% \label{sec:detailed_synthetic_experiments}


\section{Additional Experimental Results}
\label{sec:additional_experimental_results}
% \subsection{Additional Results on the Impact of Reusing Tuning Data}
We investigate various conformal prediction algorithms that require parameter tuning. This section provides additional results for these methods. Typically, we vary the size of the calibration set to examine the CovGap. For vector scaling, we consider both the size of the calibration set and the number of parameters (i.e., the amount of frozen weights).


\subsection{Additional Results for Confidence Calibration}

\paragraph{Temperature scaling} Additional experimental results on the tuning bias with temperature scaling are shown in Figures~\ref{fig:TS_APS} and~\ref{fig:TS_THR}, using APS and THR as the score functions, respectively.

\paragraph{Vector scaling} Figure~\ref{fig:VS_THR_Calnum} and Figure~\ref{fig:VS_THR_Amountpara} present additional experimental results on the tuning bias with vector scaling, using THR as the score function. The experiments vary the size of the calibration set and the number of parameters.
\begin{figure}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_Temperature_Scaling_APS.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_Temperature_Scaling_APS.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_Temperature_Scaling_APS.pdf}
        \end{subfigure}
        \caption{Temperature Scaling, APS}
        \label{fig:TS_APS}
    \end{subfigure}
        \begin{subfigure}[b]{\textwidth}
        \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_Temperature_Scaling_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_Temperature_Scaling_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_Temperature_Scaling_THR.pdf}
        \end{subfigure}
        \caption{Temperature Scaling, THR}
        \label{fig:TS_THR}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_Vector_Scaling_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_Vector_Scaling_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_Vector_Scaling_THR.pdf}
        \end{subfigure}
        \caption{Vector Scaling, THR, Size of Calibration Set}
        \label{fig:VS_THR_Calnum}
    \end{subfigure}  
    \begin{subfigure}[b]{\textwidth}
        \centering
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_emprical_freeze_thr.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_emprical_freeze_thr.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_emprical_freeze_thr.pdf}
        \end{subfigure}
        \caption{Vector Scaling, THR, Amount of Parameters}
        \label{fig:VS_THR_Amountpara}
    \end{subfigure}  
    \label{fig:addition_result}
\caption{\textbf{Additional results for confidence calibration across various datasets and score functions.} 
The coverage gaps are obtained on the conformal prediction with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set to parameter tuning.
Figures~\ref{fig:TS_APS} and~\ref{fig:TS_THR} show the tuning bias with temperature scaling, using APS and THR as the score functions, respectively. 
The coverage gaps are obtained on the conformal prediction with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set to parameter tuning.
Figures~\ref{fig:VS_THR_Calnum} and~\ref{fig:VS_THR_Amountpara} show the tuning bias with vector scaling, using THR as the score function. 
The experiments vary the size of the calibration set and the number of parameters.
The experiments are conducted on pre-trained models in \cref{subsec:pre-trained_models}. 
The coverage rate is set to 0.9. 
The figure shows that the tuning bias of temperature scaling is small for all datasets and score functions under various settings. 
The tuning bias of vector scaling is generally large except for CIFAR-10. 
For comparison, the tuning bias of temperature scaling is generally smaller than that of vector scaling.
}
\label{fig:additional_result_confidence_calibration}
\end{figure}

\subsection{Additional Results for Score Function Tuning}
We focus on two widely used score functions: RAPS and SAPS. The results for them are provided in Figures \ref{fig:RAPS_tunng} and \ref{fig:SAPS_tunng}.


\subsection{Additional Results for Score Aggregation}  In this study, we investigate the aggregation of three score functions—APS, RAPS,
and SAPS—all derived from the same pre-trained classifier as a case example. The results are provided in Figure~\ref{fig:ScoreAggregation}.
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \includegraphics[width=\textwidth]{Images/cifar10_Score_Aggregation.pdf}
%         \caption{CIFAR-10}
%     \end{subfigure} 
%     \begin{subfigure}[b]{0.33\textwidth}
%         \includegraphics[width=\textwidth]{Images/cifar100_Score_Aggregation.pdf}
%         \caption{CIFAR-100}
%     \end{subfigure} 
%     \begin{subfigure}[b]{0.33\textwidth}
%         \includegraphics[width=\textwidth]{Images/imagenet_Score_Aggregation.pdf}
%         \caption{ImageNet}
%     \end{subfigure}
% \caption{Additional results on score aggregation across various datasets.}
%     \label{fig:add_res_score_aggre}
% \end{figure*}

\subsection{Additional Results for C-Adapter}
Additional experimental results on CovGap with C-Adapter are shown in Figures~\ref{fig:Cadapter_APS} and~\ref{fig:Cadapter_THR}, using APS and THR as the score functions, respectively.
\subsection{Additional Results for ConfTr (ft.)}
Additional experimental results on CovGap with ConfTr (ft.) are shown in Figures~\ref{fig:ConfTr_APS} and~\ref{fig:ConfTr_THR}, using APS and THR as the score functions, respectively.

\begin{figure}
    \centering
    % 第一行，三个子图共享一个 subcaption
    \begin{subfigure}[b]{\textwidth}
        \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_RAPS_Tuning.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_RAPS_Tuning.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_RAPS_Tuning.pdf}
        \end{subfigure}
        \caption{RAPS}
        \label{fig:RAPS_tunng}
    \end{subfigure}
        \begin{subfigure}[b]{\textwidth}
        \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_SAPS_Tuning.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_SAPS_Tuning.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_SAPS_Tuning.pdf}
        \end{subfigure}
        \caption{SAPS}
        \label{fig:SAPS_tunng}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_Score_Aggregation.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_Score_Aggregation.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_Score_Aggregation.pdf}
        \end{subfigure}
        \caption{Score Aggregation}
        \label{fig:ScoreAggregation}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_CAdapter_APS.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_CAdapter_APS.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_CAdapter_APS.pdf}
        \end{subfigure}
        \caption{C-Adapter, APS}
        \label{fig:Cadapter_APS}
    \end{subfigure}
    
    \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_CAdapter_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_CAdapter_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_CAdapter_THR.pdf}
        \end{subfigure}
        \caption{C-Adapter, THR}
        \label{fig:Cadapter_THR}
    \end{subfigure}   
 
    \label{fig:addition_result_sec}
\caption{\textbf{Additional results for score function tuning, aggregation, and C-Adapter across various datasets and score functions.} 
The coverage gaps are obtained on the conformal prediction with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set to parameter tuning.
Figures~\ref{fig:RAPS_tunng} and~\ref{fig:SAPS_tunng} show the tuning bias with RAPS and SAPS, respectively. 
Figure~\ref{fig:ScoreAggregation} shows the tuning bias with score aggregation. Figures~\ref{fig:Cadapter_APS} and~\ref{fig:Cadapter_THR} show the tuning bias with C-Adapter, using APS and THR as the score functions, respectively.
The experiments are conducted on pre-trained models in \cref{subsec:pre-trained_models}. 
The coverage rate is set to 0.9.
The figure shows that the tuning bias of RAPS, SAPS, score aggregation, and C-Adapter is not significantly different from the one with a hold-out set under various settings.
}
\end{figure}

\begin{figure}
    \centering
    % 第一行，三个子图共享一个 subcaption
    \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_ConfTr_APS.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_ConfTr_APS.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_ConfTr_APS.pdf}
        \end{subfigure}
        \caption{ConfTr(ft.), APS}
        \label{fig:ConfTr_APS}
    \end{subfigure}
    
    \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar10_ConfTr_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/cifar100_ConfTr_THR.pdf}
        \end{subfigure} 
        \begin{subfigure}[b]{0.33\textwidth}
            \includegraphics[width=\textwidth]{Images/imagenet_ConfTr_THR.pdf}
        \end{subfigure}
        \caption{ConfTr(ft.), THR}
        \label{fig:ConfTr_THR}
    \end{subfigure}   
 
    \label{fig:addition_result_third}
\caption{\textbf{Additional results for ConfTr(ft.) across various datasets and score functions.} 
The coverage gaps are obtained on the conformal prediction with a \textbf{\textit{hold-out}} set or the \textbf{\textit{same}} set as calibration set to parameter tuning.
Figures~\ref{fig:ConfTr_APS} and~\ref{fig:ConfTr_THR} show the tuning bias with ConfTr(ft.), using APS and THR as the score functions, respectively.
The experiments are conducted on pre-trained models in \cref{subsec:pre-trained_models}. 
The coverage rate is set to 0.9.
The figure shows that the tuning bias of ConfTr (ft.) is generally small for all datasets and score functions under various settings.
}
\end{figure}
\section{Some Useful Lemmas and Corollaries}
In this section, we provide some useful lemmas and corollaries that are used in the main text.
\label{sec:useful_lemmas}
% DKW Inequality
\begin{lemma}\citep[Dvoretzky–Kiefer–Wolfowitz Inequality]{dvoretzky1956asymptotic,massart1990tight}
    \label{lemma:dkw_inequality}
    Let $\boldsymbol{x}_1, \boldsymbol{x}_2, \ldots, \boldsymbol{x}_n$ be independent and identically distributed (i.i.d.) random variables with common distribution function $F$. The empirical distribution function $F_n$ is defined by
    \begin{equation*}
        F_n(\boldsymbol{x}) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}\{\boldsymbol{x}_i \leq \boldsymbol{x}\}.
    \end{equation*}
    Then for any $\varepsilon > 0$, the following bound holds:
    \begin{equation*}
        \mathbb{P}\left(\sup_{\boldsymbol{x}\in\mathbb{R}^d}|F_n(\boldsymbol{x}) - F(\boldsymbol{x})| > \varepsilon\right) \leq 2\exp(-2n\varepsilon^2).
    \end{equation*}
\end{lemma}

\begin{lemma}[Gaussian Tail Inequality]
    \label{lemma:tail_inequality}
    For any $a > 0$, we have:
    $$ \int_a^\infty e^{-t^2}\,dt \leq \frac{e^{-a^2}}{2a} $$
\end{lemma}
\begin{proof}
For any $a > 0$, we have:
\begin{align*}
    \int_a^\infty e^{-t^2}\,dt &= \int_a^\infty \frac{t}{t}e^{-t^2}\,dt \\
    &= \frac{1}{2}\int_a^\infty \frac{1}{t}\cdot 2te^{-t^2}\,dt \\
    &\leq \frac{1}{2a}\int_a^\infty 2te^{-t^2}\,dt \\
    &= \frac{e^{-a^2}}{2a}
\end{align*}
The inequality in the third line is because when $t \geq a > 0$, $\frac{1}{t} \leq \frac{1}{a}$.
\end{proof}



% infinite space corollary
\begin{lemma}
    \label{lemma:tuning_bias_infinite_1d}
    For a score function $S^\lambda$ with one-dimensional parameter space $\Lambda = \mathbb{R}$, consider the class:
    \[
    \mathcal{H} = \{\id{S^\lambda(\boldsymbol{x},y)\le t} \mid \lambda\in \mathbb{R}, t\in \mathbb{R}\}.
    \]
    If $S^\lambda(\boldsymbol{x},y)$ is continuous, bounded over $\lambda$ for any fixed $(\boldsymbol{x},y)$, then $\text{VC}(\mathcal{H}) \leq 2$. Specifically, if $S^\lambda(\boldsymbol{x},y)$ is distinct over \(\mathcal{X} \times \mathcal{Y}\) for any fixed $\lambda$, then $\text{VC}(\mathcal{H}) = 2$.
\end{lemma}

\begin{proof}
We prove that $\text{VC}(\mathcal{H}) \leq 2$ in two steps:

\begin{enumerate}
    \item We show that $\text{VC}(\mathcal{H}) \ge 2$. Given any two distinct points $(\boldsymbol{x}_1,y_1)$ and $(\boldsymbol{x}_2,y_2)$, we need to verify that all four possible binary labellings (0,0), (0,1), (1,0), (1,1) can be realized by some choice of $\lambda$ and $t$. Consider $S^\lambda(\boldsymbol{x},y)$:

    \begin{itemize}
        \item For labeling (0,0): Choose $t$ small enough such that both points have values greater than $t$.
        \item For labelling (1,1): Choose $t$ large enough such that both points have values less than or equal to $t$.
        \item Then forlabellingss (1,0) or (0,1): Since the points are distinct and $S^\lambda(\boldsymbol{x},y)$ is continuous in $\lambda$ over $\mathbb{R}$, by the intermediate value theorem, there exists some $\lambda$ where $S^\lambda(\boldsymbol{x}_1,y_1) \neq S^\lambda(\boldsymbol{x}_2,y_2)$. Then, choose $t$ between these two values.
    \end{itemize}
    Therefore, $\text{VC}(\mathcal{H}) \ge 2$.
    \item We prove that $\text{VC}(\mathcal{H}) \le 2$. Suppose for contradiction that three distinct points can be shattered. Consider the labelling pattern (1,0,1). This would require some $\lambda$ and $t$ such that:
    \[
    S^\lambda(\boldsymbol{x}_1,y_1)\le t,\quad S^\lambda(\boldsymbol{x}_2,y_2)> t,\quad S^\lambda(\boldsymbol{x}_3,y_3)\le t
    \]
    implying $S^\lambda(\boldsymbol{x}_2,y_2)>\max\{S^\lambda(\boldsymbol{x}_1,y_1), S^\lambda(\boldsymbol{x}_3,y_3)\}$. However, since $\lambda$ is one-dimensional and $S^\lambda(\boldsymbol{x},y)$ is continuous in $\lambda$, such a complex ordering cannot be achieved at a single $\lambda$ value for all possible three-point configurations. 
    This phenomenon is because the continuity of $S^\lambda(\boldsymbol{x},y)$ over $\mathbb{R}$ ensures that any crossing of function values must occur in pairs, making it impossible to maintain the required ordering consistently. Therefore, three points cannot be shattered, and $\text{VC}(\mathcal{H}) \le 2$.
\end{enumerate}

Combining both results, we conclude that $\text{VC}(\mathcal{H}) = 2$.
\end{proof}


\begin{corollary}
    \label{corollary:tuning_bias_positive}
    For a score function $S^\lambda$ with parameter space $\Lambda = \mathbb{R}^+$, consider the class:
    \[
    \mathcal{H} = \{\id{S^\lambda(\boldsymbol{x},y)\le t} \mid \lambda\in \mathbb{R}^+, t\in \mathbb{R}\}.
    \]
    If $S^\lambda(\boldsymbol{x},y)$ is continuous and bounded over $\lambda$ for any fixed $(\boldsymbol{x},y)$ and $S^\lambda(\boldsymbol{x},y)$ is distinct over \(\mathcal{X} \times \mathcal{Y}\) for any fixed $\lambda$, then $\text{VC}(\mathcal{H}) = 2$.
\end{corollary}

\begin{proof}
The proof mirrors Lemma~\ref{lemma:tuning_bias_infinite_1d}, with $\lambda > 0$. This restriction does not alter the VC dimension:
First, we show that $\text{VC}(\mathcal{H}) \ge 2$. Two points can still achieve all labellings with positive $\lambda$ and $t$.
Next, we show that $\text{VC}(\mathcal{H}) \le 2$. The restriction to $\lambda > 0$ does not allow shattering three points.
Thus, the VC dimension is 2.
\end{proof}

\begin{lemma}
    \label{lemma:tuning_bias_infinite_d}
    For a score function $S^\lambda$ with $d$-dimensional parameter space $\Lambda = \mathbb{R}^d$, consider the class:
    \[
    \mathcal{H} = \{\id{S^\lambda(x,y)\le t} \mid \lambda\in \mathbb{R}^d, t\in \mathbb{R}\}.
    \]
    If $S^\lambda(x,y)$ is continuous and bounded over $\lambda$ for any fixed $(x,y)$, then $\text{VC}(\mathcal{H}) \leq d+1$.
\end{lemma}

\begin{proof}
We prove that $\text{VC}(\mathcal{H}) \le d+1$. Suppose for contradiction that $d+2$ distinct points can be shattered. Consider any labelling pattern that requires one point to have a value strictly larger than $d+1$ other points at the same $\lambda$. This would require some $\lambda$ and $t$ such that:
\[
S^\lambda(x_i,y_i)\le t \text{ for } i\in I, \quad S^\lambda(x_j,y_j)> t \text{ for some } j\notin I
\]
where $|I|=d+1$. This implies $S^\lambda(\boldsymbol{x}_j,y_j)>\max_{i\in I}\{S^\lambda(\boldsymbol{x}_i,y_i)\}$. 
However, since $\lambda$ is $d$-dimensional and $S^\lambda(\boldsymbol{x},y)$ is continuous in $\lambda$, such a complex ordering requiring one point to dominate $d+1$ other points cannot be achieved at a single $\lambda$ value. 
This phenomenon is because the continuity of $S^\lambda(\boldsymbol{x},y)$ over $\mathbb{R}^d$ ensures that the ordering relationships between $d+2$ points cannot be arbitrary with only $d$ degrees of freedom. Therefore, $d+2$ points cannot be shattered, and $\text{VC}(\mathcal{H}) \le d+1$.
\end{proof}

\begin{lemma}[Empirical Process Bound via VC Dimension]
    \label{lemma:infinite_parameter_empirical_process}
    For the class $\mathcal{H}$ with finite VC dimension $\text{VC}(\mathcal{H}) \geq 1$, we have
    \begin{equation*}
        \E \mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} \leq C\sqrt{\frac{\text{VC}(\mathcal{H})}{n}},
    \end{equation*}
    where $C$ is a universal constant.
\end{lemma}
\begin{proof}
    It is a direct result of the VC dimension bound in~\citet[Theorem 8.3.23]{vershynin2018highdimensional}.
\end{proof}

\begin{lemma}
    \label{lemma:order_preserving_score}
    If \(S^\lambda(\boldsymbol{x},y)\) is order-preserving for any transformation \(\lambda \in \Lambda\), i.e., \(S^\lambda(\boldsymbol{x},y) \leq S^\lambda(\boldsymbol{x}',y')\) if and only if \(S(\boldsymbol{x},y) \leq S(\boldsymbol{x}',y')\) for any transformation \(\lambda\), then for $(X,y)$ with the same distribution as $(X_i, y_i) \in \mathcal{D}_{\text{cal}}$
    \begin{equation*}
        \P(S^{\lambda}(\boldsymbol{x},y) \leq \hat{t}^\lambda_{\mathcal{D}_{\text{cal}}} \mid \mathcal{D}_{\text{cal}}) = \P(S(\boldsymbol{x},y) \leq \hat{t} \mid \mathcal{D}_{\text{cal}})
    \end{equation*}
    for \(\lambda \in \Lambda\) where \(\hat{t}\) is defined in \Eqref{eq:threshold}.
\end{lemma}

\begin{proof}
    Since \(S^\lambda(\boldsymbol{x},y)\) is order-preserving for any transformation \(\lambda \in \Lambda\), we have 
    \begin{equation*}
        S^\lambda(\boldsymbol{x},y) \leq S^\lambda(\boldsymbol{x}',y') \quad \text{if and only if} \quad S(\boldsymbol{x},y) \leq S(\boldsymbol{x}',y')
    \end{equation*}
    for any transformation \(\lambda\). By the definition of \(\hat{t}(\lambda, \mathcal{D}_{\text{cal}})\) and \(\hat{t}\), the position of \(S^{\lambda}(\boldsymbol{x},y)\) among \(S^{\lambda}(\boldsymbol{x}_i, y_i)\) is the same as the position of \(S(\boldsymbol{x},y)\) among \(S(\boldsymbol{x}_i, y_i)\). Then, we have
    \begin{equation*}
        \P(S^{\lambda}(\boldsymbol{x},y) \leq \hat{t}(\lambda, \mathcal{D}_{\text{cal}}) \mid \mathcal{D}_{\text{cal}}) 
        = \P(S(\boldsymbol{x},y) \leq \hat{t} \mid \mathcal{D}_{\text{cal}}).
    \end{equation*}
\end{proof}

\begin{lemma}
    \label{lemma:order_preserving_ts_sample}
    For temperature scaling in binary classification, we have \(p^{TS}_\lambda(y|X)\) is order-preserving over \((\boldsymbol{x},y)\) for any transformation \(\lambda \in \Lambda\).
\end{lemma}

\begin{proof}
    Noting that 
    \begin{equation*}
        p^{\text{TS}}_{\lambda}(y\mid \boldsymbol{x}) = \psi_y\left(\frac{f(\boldsymbol{x})}{\lambda}\right)
    \end{equation*}
    where \(\psi_y\) is the softmax function defined in \Eqref{eq:softmax}. Since 
    \begin{equation*}
        \psi_y(X) = \frac{\exp{(f_y(\boldsymbol{x}))}}{\exp{(f_y(\boldsymbol{x}))}+\exp{(f_{1-y}(\boldsymbol{x}))}} = \frac{1}{1+\exp{(f_{1-y}(\boldsymbol{x})-f_y(\boldsymbol{x}))}}
    \end{equation*}
    for binary label \(y \in \{0,1\}\), we have
    \begin{equation*}
        \psi_y(f(\boldsymbol{x})) \leq \psi_{y'}(f(\boldsymbol{x}'))
    \end{equation*}
    \begin{equation*}
         \iff \frac{1}{1+\exp{(f_{1-y}(\boldsymbol{x})-f_y(\boldsymbol{x}))}} \leq \frac{1}{1+\exp{(f_{1-y'}(\boldsymbol{x}')-f_{y'}(\boldsymbol{x}'))}}
    \end{equation*}
    \begin{equation*}
        \iff \exp{(f_{1-y}(\boldsymbol{x})-f_y(\boldsymbol{x}))} \geq \exp{(f_{1-y'}(\boldsymbol{x}')-f_{y'}(\boldsymbol{x}'))}
    \end{equation*}
    \begin{equation*}
        \iff \exp{((f_{1-y}(\boldsymbol{x})-f_y(\boldsymbol{x}))/\lambda)} \geq \exp{((f_{1-y'}(\boldsymbol{x}')-f_{y'}(\boldsymbol{x}'))/\lambda)}, \forall \lambda > 0
    \end{equation*}
    \begin{equation*}
        \iff \psi_y\left(\frac{f(\boldsymbol{x})}{\lambda}\right) \leq \psi_{y'}\left(\frac{f(\boldsymbol{x}')}{\lambda}\right)
    \end{equation*}
    for any transformation \(\lambda>0\).
\end{proof}


\section{Proof of \cref{thm:tuning_bias}}
\label{sec:proof_tuning_bias}
\begin{proof}
    For any sample $(\boldsymbol{x},y)$ with the same distribution as $(\boldsymbol{x}_i, y_i) \in \mathcal{D}_{\text{cal}}$ and independent of $\mathcal{D}_{\text{cal}}$, recalling the definition of \(\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}}\) in Theorem~\ref{thm:tuning_bias}:
    \begin{equation*}
            \mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} := \sup_{\lambda \in \Lambda, t \in \mathcal{T}} \Big| \frac{1}{n} \sum_{i \in [n]} \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} 
            - \P\of{S^{\lambda}(\boldsymbol{x}_{\text{test}}, y_{\text{test}})\leq t \mid \mathcal{D}_{\text{cal}}}
            \Big|
    \end{equation*}
    we have
    \begin{equation*}
        |\frac{1}{n} \sum_{i=1}^n \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} - \P(S^{\lambda}(\boldsymbol{x},y) \leq t \mid \mathcal{D}_{\text{cal}}) |
         \leq \mathfrak{R}_{\Lambda, \mathcal{D}} 
    \end{equation*}
    Then, we have
    \begin{equation*}
        \frac{1}{n} \sum_{i=1}^n \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} - \mathfrak{R}_{\Lambda, \mathcal{D}} 
        \leq \P(S^{\lambda}(\boldsymbol{x},y) \leq t \mid \mathcal{D}_{\text{cal}}) \leq \frac{1}{n} \sum_{i=1}^n \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} + \mathfrak{R}_{\Lambda, \mathcal{D}}.
    \end{equation*}
    Letting \(t = \hat{t}^\lambda_{\mathcal{D}_{\text{cal}}}\) defined in \Eqref{eq:threshold_reusing_data} and noting the truth that 
    \begin{equation*}
        \frac{1}{n} \sum_{i=1}^n \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq \hat{t}(\lambda, \mathcal{D}_{\text{cal}})} = \frac{\left\lceil (1-\alpha)(1+n) \right\rceil}{n},
    \end{equation*}
    we have
    \begin{equation*}
        \frac{\left\lceil (1-\alpha)(1+n) \right\rceil}{n} - \mathfrak{R}_{\Lambda, \mathcal{D}} 
        \leq \P(S^{\lambda}(\boldsymbol{x},y) \leq \hat{t}^\lambda_{\mathcal{D}_{\text{cal}}} \mid \mathcal{D}_{\text{cal}}) \leq \frac{\left\lceil (1-\alpha)(1+n) \right\rceil}{n} + \mathfrak{R}_{\Lambda, \mathcal{D}}.
    \end{equation*}
    Take expectations on both sides; we have
    \begin{equation*}
        \mathop{CovGap}(\widehat{\C}, \alpha) \leq \E \mathfrak{R}_{\Lambda, \mathcal{D}} + \varepsilon_{\alpha, n}.
    \end{equation*}
\end{proof}


\section{Proof of Proposition~\ref{prop:finite_parameter_tuning_bias}}
\label{sec:proof_finite_parameter_tuning_bias}
% \begin{corollary}
%     \label{corollary:finite_parameter_empirical_process}
%     For a finite parameter space $\Lambda$, we have
%     \begin{equation}
%         \E[\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}}] \leq \sqrt{\frac{\log(2|\Lambda|)}{2n}} + \frac{1}{\sqrt{2n}\sqrt{\log(2|\Lambda|)}}.
%     \end{equation}
% \end{corollary}

\begin{proof}
    By Lemma~\ref{lemma:dkw_inequality}, we have
    \begin{equation*}
        \P(\sup_{t\in\mathcal{T}}|\frac{1}{n} \sum_{i=1}^n \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} - \P(S^{\lambda}(\boldsymbol{x},y) \leq t \mid \mathcal{D}_{\text{cal}})| > u) \leq 2\exp(-2n u^2).
    \end{equation*}
    By the property of probability of finite union over \(\lambda \in \Lambda\), we have
    \begin{equation*}
        \P(\sup_{\lambda \in \Lambda, t \in \mathcal{T}}|\frac{1}{n} \sum_{i=1}^n \id{S^{\lambda}(\boldsymbol{x}_i, y_i) \leq t} - \P(S^{\lambda}(\boldsymbol{x},y) \leq t \mid \mathcal{D}_{\text{cal}})| > u) \leq 2|\Lambda| \exp(-2n u^2),
    \end{equation*}
    i.e.,
    \begin{equation*}
        \P(\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} > u ) \leq 2|\Lambda| \exp(-2n u^2).
    \end{equation*}
    Noting that the bound is too large around 0, we take a departure from the bound into two parts from the point \(\sqrt{\frac{\log(2|\Lambda|)}{2n}}\) where the probability is just 1. Before this point, the bound is too large, we do not use it. 
    Then, the expectation of \( \mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} \) is bounded by two parts:
    \begin{align*}
        \E[\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}}] 
        &= \int_0^\infty \P(\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} > u)\,du \\
        &= \int_0^{\sqrt{\frac{\log(2|\Lambda|)}{2n}}} \P(\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} > u)\,du + \int_{\sqrt{\frac{\log(2|\Lambda|)}{2n}}}^\infty \P(\mathfrak{R}_{\Lambda, \mathcal{D}_{\text{cal}}} > u)\,du \\
        &\leq \sqrt{\frac{\log(2|\Lambda|)}{2n}} + \int_{\sqrt{\frac{\log(2|\Lambda|)}{2n}}}^\infty 2|\Lambda|\exp(-2nu^2)\,du \\
        &= \sqrt{\frac{\log(2|\Lambda|)}{2n}} + \frac{2|\Lambda|}{\sqrt{2n}}\int_{\sqrt{\log(2|\Lambda|)}}^\infty \exp(-t^2)\,dt \\
        & \leq \sqrt{\frac{\log(2|\Lambda|)}{2n}} + \frac{2|\Lambda|}{\sqrt{2n}} \cdot \frac{1}{2|\Lambda|\sqrt{\log(2|\Lambda|)}} \\
        & = \sqrt{\frac{\log(2|\Lambda|)}{2n}} + \frac{1}{\sqrt{2n}\sqrt{\log(2|\Lambda|)}}
    \end{align*}
    where the second inequality is due to Lemma~\ref{lemma:tail_inequality}.
\end{proof}
% \begin{proof}
%     It is a direct result of Corollary~\ref{corollary:finite_parameter_empirical_process}.
% \end{proof}


\section{Proof of \cref{corollary:tuning_bias_raps} and~\ref{corollary:tuning_bias_score_aggregation}}
\label{sec:proof_tuning_bias_raps_score_aggregation}
\begin{proof}
    It is a direct application of Proposition~\ref{prop:finite_parameter_tuning_bias}.
\end{proof}



\section{Proof of \cref{prop:infinite_parameter_tuning_bias}}
\label{sec:proof_infinite_parameter_tuning_bias}
\begin{proof}
    It is a direct application of Lemma~\ref{lemma:tuning_bias_infinite_d} and~\ref{lemma:infinite_parameter_empirical_process}.
\end{proof}


\section{Proof of \cref{corollary:tuning_bias_contrast_vs_ts}}
\label{sec:proof_tuning_bias_contrast_vs_ts}
\begin{proof}
    It is a direct application of Proposition~\ref{prop:infinite_parameter_tuning_bias}.
\end{proof}


\section{Proof of \cref{prop:tuning_bias_ts_vs}}
\label{sec:proof_tuning_bias_ts_vs}
\begin{proof}
    We consider a binary classification problem with temperature scaling and THR score applied.
    The probability \(p^{\text{TS}}_{\lambda}(y\mid \boldsymbol{x})\) is order-preserving of the label for a given input $X$ for any $\lambda \in \mathbb{R}^+$, i.e, \(\forall \lambda \in \mathbb{R}^+\),
    \begin{equation}\label{eq:order_preserving_label}
        p^{\text{TS}}_{\lambda}(y\mid \boldsymbol{x}) \leq p^{\text{TS}}_{\lambda}(y'\mid \boldsymbol{x}) \iff p(y\mid \boldsymbol{x}) \leq p(y'\mid \boldsymbol{x}).
    \end{equation}
    Then, for any two samples $(\boldsymbol{x}, y)$ and $(\boldsymbol{x}', y')$, with the property of softmax function \(\psi_y\) on binary classification (see Lemma~\ref{lemma:order_preserving_ts_sample} in Appendix~\ref{sec:useful_lemmas}), we have
    \begin{equation}\label{eq:order_preserving_sample}
    p^{\text{TS}}_{\lambda}(y\mid \boldsymbol{x}) \leq p^{\text{TS}}_{\lambda}(y'\mid \boldsymbol{x}) \iff p(y\mid \boldsymbol{x}) \leq p(y'\mid \boldsymbol{x}).
    \end{equation}
    Then, the direct result is that the order of score function is preserved, i.e.,
    \begin{align*}
        & S_{\text{THR}}(\boldsymbol{x}, y, p^{\text{TS}}_{\lambda}) \leq S_{\text{THR}}(\boldsymbol{x}', y', p^{\text{TS}}_{\lambda}) \\ 
        \iff 
        & S_{\text{THR}}(\boldsymbol{x}, y, p) \leq S_{\text{THR}}(\boldsymbol{x}', y', p).
    \end{align*}
    Therefore, the order of the scores on the dataset \(\mathcal{D}_{\text{cal}}\) is preserved for any transformation \(\lambda \in \mathbb{R}^+\), i.e, the order of \(\mathcal{S}^{\lambda}_{\mathcal{D}_{\text{cal}}}\) is the same as \(\mathcal{S}_{\mathcal{D}_{\text{cal}}}\) for any \(\lambda \in \mathbb{R}^+\). 
    Then by Lemma~\ref{lemma:order_preserving_score} in Appendix~\ref{sec:useful_lemmas}, we have 
    \begin{equation}\label{eq:order_preserving_score}
    \P\of{S^{\lambda}(\boldsymbol{x}, y)\leq \hat{t}^{\lambda}_{D_{\text{cal}}} | \mathcal{D}_{\text{cal}}} = 
    \P\of{S(\boldsymbol{x}, y)\leq \hat{t} | \mathcal{D}_{\text{cal}}}
    \end{equation}
    for any $\lambda \in \mathbb{R}^+$, which means \(\mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{TS}}) = 0 \leq \mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{VS}})\).
    
\end{proof}
\section{Order-preserving Regularization and Proof of \cref{prop:tuning_bias_order_preserving}}
\label{sec:proof_tuning_bias_order_preserving}
\paragraph{Order-preserving regularization} 
As discussed in \cref{sec:discussion}, applying the regularization on the conformal prediction with confidence calibration or fine-tuning methods could reduce the tuning bias significantly. 
Taking order-preserving regularization as an example, we modify the class associated with \(\mathfrak{R}_{\Lambda}\) as regularization form:
\begin{equation}
    \label{eq:op_regularization}
    \mathcal{H}_{\text{op}} = \{ f_{\lambda, t} \in \mathcal{H} \text{ s.t. }  o(y, \boldsymbol{x}, p) = o(y, \boldsymbol{x}, p_\lambda) \},
\end{equation}
where \(o(y, \boldsymbol{x}, p)\) is the order of the true label \(y\) in the sorted predicted probabilities \(p(\cdot|\boldsymbol{x})\), and \(p_\lambda\) is the probability after confidence calibration like temperature scaling or vector scaling. 
The class \(\mathcal{H}_{\text{op}}\) is a constraint version of \(\mathcal{H}\) with a special regularization. 
With the constrained ERM with artificial order-preserving regularization, we could obtain a less tuning bias. 
\begin{proof}[Proof of \cref{prop:tuning_bias_order_preserving}]
    The ConfTr(op) is a regularization case of ConfTr, where the class \(\mathcal{H}_{\text{op}}\) is used as the constraint. 
    Since the class \(\mathcal{H}_{\text{op}}\) defined in \Eqref{eq:op_regularization} is a subset of \(\mathcal{H}\), we have
    \begin{equation*}
        \E \sup_{f \in \mathcal{H}_{\text{op}}} \Big|
        \frac{1}{n} \sum_{i \in [n]} f(\boldsymbol{x}_i, y_i) - \E[f(\boldsymbol{x}_{\text{test}}, y_{\text{test}}) \mid \mathcal{D}_{\text{cal}}]
        \Big|
        \leq \E \sup_{f \in \mathcal{H}} \Big|
        \frac{1}{n} \sum_{i \in [n]} f(\boldsymbol{x}_i, y_i) - \E[f(\boldsymbol{x}_{\text{test}}, y_{\text{test}}) \mid \mathcal{D}_{\text{cal}}]
        \Big|,
    \end{equation*}
    which means that 
    \begin{equation*}
        \mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{ConfTr(op)}}) \leq \mathop{TuningBias}(\widehat{\mathcal{C}}_{\text{ConfTr}}).
    \end{equation*}
\end{proof}



\section{Additional Theoretical Results of Score aggregation}
\label{sec:additional_theoretical_results}
% \paragraph{Score aggregation}
For a more general case about the selection of scores, we could also apply the same analysis to the score aggregation~\citep{luo2024weighted}. 
In the setting of score aggregation~\citep{luo2024weighted}, they choose an aggregation weights vector \(\boldsymbol{w}\) to aggregate multiple score functions:
\begin{equation*}
    S_{\boldsymbol{w}}(\boldsymbol{x}, y) = \sum_{m=1}^M w_m S_m(\boldsymbol{x}, y),
\end{equation*}
where \(\boldsymbol{w} = (w_1, \ldots, w_M)^\top\) is the aggregation weights vector. 
Define the parameter space $\Lambda$ as the set of all possible aggregation weights vectors, i.e., $\Lambda = \mathcal{W} = \{ \boldsymbol{w} \in \mathbb{R}^M: \sum_{m=1}^M w_m = 1, w_m \geq 0 \}$. 
Then, for grid research, we could also apply Proposition~\ref{prop:finite_parameter_tuning_bias} to bound its tuning bias to obtain a more similar result as~\cref{corollary:tuning_bias_score_aggregation}.

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
