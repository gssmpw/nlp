% \section{Limitations}
% A key limitation of \emph{DiGraP} is its reduced effectiveness on far OOD datasets, where large distribution shifts occur. While it performs well on near OOD data, the lack of strong regularization limits its ability to handle far OOD scenarios. However, introducing stronger regularization may degrade ID and near OOD performance (Sec.~\ref{sec:ref_vqa} and Sec.~\ref{sec:ft_vqa}), presenting a trade-off. Future work should aim to balance ID, near OOD, and far OOD performance simultaneously. Additionally, \emph{DiGraP}'s application is limited to fine-tuning \textit{well} pre-trained models, and its effectiveness in training from scratch or with non-robust initialization remains unexplored.


\section{Conclusion}
% In this work, we introduce Directional Gradient Projection, a novel approach to robust fine-tuning by leveraging gradient-based directional information to bridge regularization and multi-objective optimization. This method addresses the limitations of existing approaches, e.g., hyper-parameter sensitivity and underfitting. Through extensive experiments on both image classification and VQA benchmarks, we demonstrate that \emph{DiGraP} outperforms current baselines, improving ID accuracy and enhancing OOD robustness. Our work further bridges the gap between uni-modal and multi-modal evaluation settings, contributing to a broader understanding of robust fine-tuning in diverse domains.

We present Directional Gradient Projection (\emph{DiGraP}), a novel method for robust fine-tuning that leverages gradient-based directional information to unify regularization and multi-objective optimization. \emph{DiGraP} addresses hyperparameter sensitivity and underfitting issues in existing methods. Experiments on image classification and VQA benchmarks show that \emph{DiGraP} surpasses baselines, improving ID accuracy and OOD robustness, while bridging uni-modal and multi-modal evaluation for robust fine-tuning across domains. 
However, \emph{DiGraP} struggles with far OOD datasets due to limited regularization, excelling in near OOD scenarios but facing a trade-off as stronger regularization may harm ID and near OOD performance (Sec.~\ref{sec:ref_vqa}, \ref{sec:ft_vqa}). Future work should balance ID, near OOD, and far OOD performance. Additionally, \emph{DiGraP} is suited for fine-tuning well pre-trained models, with its efficacy in training from scratch or non-robust initialization yet to be explored.