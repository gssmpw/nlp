\section{Introduction}
Robust fine-tuning has become an essential technique in adapting pre-trained models to downstream tasks, particularly in the face of distribution shifts that challenge model generalization. While pre-trained models excel in capturing a wide range of features from diverse datasets, fine-tuning them on specific tasks often leads to overfitting, reducing their robustness to out-of-distribution (OOD) data~\citep{wortsman_robust_2022, nguyen_saft_2024}. The goal of robust fine-tuning is to strike a balance between task-specific performance and maintaining the generalization abilities of the pre-trained model \citep{wortsman_robust_2022}. This is particularly crucial for real-world applications such as visual question answering (VQA), where models are frequently exposed to varying distributions in images, questions, and answers \citep{agrawal_dont_2018, shah_cycle-consistency_2019}. Effective robust fine-tuning strategies aim to mitigate performance degradation by incorporating techniques like regularization~\citep{li_explicit_2018} and bi-level optimization \citep{tian_trainable_2023, tian_fast_2023,tian2024rethinkingweightdecayrobust}, ensuring that models retain their learned knowledge while adapting to new domains.

To tailor the model for downstream tasks while retaining the capabilities of the pre-trained model (e.g. robustness to distribution shifts), L2-SP~\citep{li_explicit_2018} imposes a regularization term on the distance between the fine-tuned and pre-trained weights. More recently, instead of viewing robust fine-tuning as a regularization problem, TPGM~\citep{tian_trainable_2023} and FTP~\citep{tian_fast_2023} consider the regularization term as the constraint to reformulate the problem from a \textit{bi-level optimization} prospective and propose to learn different hard constraints for each layer. However, these methods are computationally heavy and often apply overly strong constraints which results in underfitting (See Sec.~\ref{sec:ref_vqa} and Sec.~\ref{sec:ft_vqa}). Moreover, these methods perform weight projection to enforce the distance between fine-tuned and pre-trained weights within a set of projection radii, which is magnitude-wise but does not encode any directional information. \textit{This motivates us to think of direction-based methods for this fundamental problem.}

We re-examine the regularization problem from a multi-objective optimization perspective and propose \textbf{Di}rectional \textbf{Gra}dient \textbf{P}rojection (\emph{DiGraP}). We consider the regularization term as the second objective besides the first objective, i.e., original loss function, and the goal is to minimize the two simultaneously. This new method involves two aspects: the \textit{projecting condition} and the \textit{directional projection strength}. If the projecting condition meets, i.e., the gradient directions of the two objectives are opposite, the decrease of one objective will lead to the increase of the other objective. In this case, we project the opposite gradient to the orthogonal direction of the other to get rid of the conflicting directional information. We also add a directional projection strength $\omega \in [0,1]$ to learn the priority of the two objectives and make it trainable so that it can be dynamic throughout the training process. 

\begin{itemize}
    \item Regularization (L2-SP): $\min \mathcal{L(\theta)} = \Tilde{\mathcal{L}}(\theta) + \frac{\lambda}{2}\|\theta-\theta_0\|^2_2$
    \item Bi-level optimization (TPGM, FTP): $\min \Tilde{\mathcal{L}}(\theta)~\text{s.t.}~\|\theta-\theta_0\|^2_2 \leq \gamma$
    \item Multi-objective optimization (\emph{DiGraP}): $\min \{\Tilde{\mathcal{L}}(\theta), \frac{1}{2}\|\theta-\theta_0\|^2_2\}$
\end{itemize}

Typical evaluations on robust fine-tuning methods are primarily done in uni-modal settings, e.g., image classification or semantic segmentation.  However, they have not been analyzed in the context of multiple modalities. We first bridge the gap by comparing our method with baselines on an Image Classification reformulated VQA benchmark. We further propose a new setting for evaluating robust fine-tuning of VQA by leveraging ten VQA datasets and categorizing them into in-distribution (ID), near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. \emph{DiGraP} achieves SOTA ID and OOD performance on both image classification and VQA benchmarks. Our contributions are:
\begin{itemize}
    \item We propose a layer-wise trainable directional gradient projection method \emph{DiGraP} for robust fine-tuning of large foundation models with the intuition of bridging regularization and multi-objective optimization. This is the first robust fine-tuning methods that considers the directional information of the gradients.
    \item We propose new settings for evaluating robust fine-tuning of VQA. We first conduct experiments on Image Classification reformulated VQA benchmarks. We then categorize the existing OOD VQA datasets into different types and degrees of distribution shifts and present a consistent comparative analysis of robust fine-tuning algorithms.
    \item We show that \emph{DiGraP} consistently outperforms other baselines across uni-modal and multi-modal tasks in both ID generelization and OOD robustness.
\end{itemize}



