\newpage
\section{Appendix}
% \subsection{ImageNet Experiments}
% \input{tables/imagenet_ft}
% \subsection{DomainNet CLIP-ViT}
% \input{tables/domainnet_clip}

% \subsection{DomainNet-oVQA LlaVA}
% \input{tables/domainnet_ft_llava}

% \subsection{VQA LlaVA}
% \input{tables/vqa_llava}

\subsection{Additional Results}
\label{sec:more_results}

\noindent \textbf{1) More Backbones: CLIP, LLaVA.} We include experiments of fine-tuning on DomainNet with CLIP ViT-Base~\citep{radford_learning_2021} (Tab.~\ref{tab:domainnet_clipvit}), DomainNet-oVQA (Tab.~\ref{tab:domainnet_ft_llava}), and VQA (Tab.~\ref{tab:vqa_llava}) with LLaVA-7B~\citep{liu2023visualinstructiontuning}. \textit{DiGraP} consistently achieves the best performance across all experiments.

\input{tables/vqa_llava}
\input{tables/domainnet_ft_llava}
\input{tables/domainnet_clip}

\noindent \textbf{2) Language-only Experiments.} We also conduct a language-only experiment in Tab.~\ref{tab:sa_comparison}, where we use BOSS~\citep{yuan2023revisitingoutofdistributionrobustnessnlp}, an NLP benchmark suite designed for OOD robustness evaluation. This suite includes both ID and OOD language-only datasets across multiple tasks (e.g., Sentiment Analysis, Toxic Detection). 

\input{tables/boss_t5}

\noindent \textbf{3) LP-FT Controlled Version.} In Tab.~\ref{tab:domainnet_moco_appendix}, we conduct an LP-FT-C experiment (a controlled version of LP-FT), similar to TPGM-C in~\citet{tian_trainable_2023}, by increasing the regularization strength to ensure that the ID performance matches that of DiGraP. Despite this adjustment, \textit{DiGraP} still outperforms LP-FT-C on OOD datasets.

\input{tables/domainnet_moco_appendix}

\noindent \textbf{4) Sensitivity Analysis on Single-Modal Tasks.} In Tab.~\ref{tab:hyper_img}, we include the hyper-parameter tuning experiment for DomainNet on CLIP ViT-Base. The results demonstrate that \textit{DiGraP} remains robust to hyperparameter variations on both ID and OOD datasets in the uni-modal image classification task.

\input{tables/sensitivity_image}

\noindent \textbf{5) Visualization of the Variation in Regularization Strength across Layers over Epochs.} We present a visualization of the variation in regularization strength ($\lambda$) across different layers over epochs in Fig.~\ref{fig:layer_epoch_reg_strength}. The results show that the regularization strength evolves dynamically during training, starting small, increasing over iterations, and eventually converging. In the vision layers (blue), early layers tend to experience weaker regularization compared to later layers throughout the training process. Conversely, the language layers (orange) display a more uniform regularization strength, with comparable levels observed between early and later layers. The weaker regularization in early vision layers likely allows them to preserve foundational low-level features, while stronger regularization in later layers encourages the model to focus on high-level semantic representations.

\input{figures/layer_epoch_reg_strength}

\subsection{Training Details}
\label{sec:training}

\paragraph{Image Classification.} For \emph{DiGraP}, we fine-tune the model using SGD with a learning rate of $1e-2$ and $\mu=0.1$ with a batchsize of 256. The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 4 RTX 2080 GPUs for each experiment. \zkn{Might be better to have this before sec. 4.1}

\paragraph{Fine-Tuning DomainNet-oVQA.} We use the model pretrained with $224*224$ input images and $128$ token input/output text sequences and fine-tune with the precision of bfloat16. We use the LAVIS~\citep{li2022lavislibrarylanguagevisionintelligence} public repository to fine-tune all methods. Standard hyper-parameters are used for all: learning rate ($1e-3$), weight-decay ($1e-4$), optimizer (AdamW), scheduler (Linear Warmup With Cosine Annealing), warm-up learning rate ($1e-4$), minimum learning rate ($1e-4$), accumulation steps ($2$), beam size (5). The model is trained for $10$ epochs with a batch size of $128$ for Tab.~\ref{tab:domainnet_ft}. For LoRA~\citep{hu_lora_2021}, we limit our study to only adapting the attention weights and freeze the MLP modules for parameter-efficiency, specifically apply LoRA to $W_q, W_k, W_v, W_o$ with $r=8$ in Tab.~\ref{tab:domainnet_ft}. We use $\lambda=0.5$ for all \emph{DiGraP} results in Tab.~\ref{tab:domainnet_ft}. The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 8 A40 GPU for each experiment.

\paragraph{Fine-tuning VQA.} We use the model pretrained with $224*224$ input images and $128$ token input/output text sequences and fine-tune with the precision of bfloat16. We use the LAVIS~\citep{li2022lavislibrarylanguagevisionintelligence} public repository to fine-tune all methods. Standard hyper-parameters are used for all: learning rate ($1e-3$), weight-decay ($1e-4$), optimizer (AdamW), scheduler (Linear Warmup With Cosine Annealing), warm-up learning rate ($1e-4$), minimum learning rate ($1e-4$), accumulation steps ($2$), beam size (5). The model is trained for $10$ epochs with a batch size of $128$ for Tab.~\ref{tab:main_result}. For LoRA~\citep{hu_lora_2021}, we limit our study to only adapting the attention weights and freeze the MLP modules for parameter-efficiency, specifically apply LoRA to $W_q, W_k, W_v, W_o$ with $r=8$ in Tab.~\ref{tab:main_result}. We use $\lambda=0.5$ for all \emph{DiGraP} results in Tab.~\ref{tab:main_result}. The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 8 A40 GPU for each experiment.

\subsection{Measuring OOD Distance}
\label{sec:ood_dist}

We follow procedures similar to typical feature-based OOD detection methods \citep{Shi_2024_WACV}\zkn{cite}. Specifically, given our input training split \( X_{\text{in}}^{\text{train}} \), we compute feature representations \( z \) of the training samples to estimate the empirical mean \( \mu \) and covariance matrix \( \Sigma \). For each test split, we compute the test set shift relative to the training domain using the Mahalanobis distance metric defined in Eq.~\ref{eq:maha_distance}. The overall shift score for each test dataset, denoted as \( S_{\text{maha}} \), is calculated as the average 
\( S_{\text{Maha}} \) across all samples. Let \( q \) denote the question, \( v \) the image (vision input), and \( a \) the answer. The input features used in measuring shifts include uni-modal embeddings \( f(v) \), \( f(q) \) and joint embeddings \( f(q,v) \).

\begin{equation}
S_{\text{Maha}}(z_{\text{test}}) = \sqrt{(z_{\text{test}} - \mu)^\top \Sigma^{-1} (z_{\text{test}} - \mu)}
\label{eq:maha_distance}
\end{equation}

We utilize the vanilla fine-tuned PaliGemma model on the VQAv2 training dataset as our feature encoder. For the image embedding \( f(v) \), we obtain it via masking out the question input tokens and mean-pooling the image portion from the final layer of the model before the language model head. Similarly, to get \( f(q) \), we mask out the image tokens and extract the question portion from the final layer. To obtain \(f(v,q)\), we pass in both image and text tokens as input, compute the average embedding for both modalities and then taking the overall mean. 

\subsubsection{Correlation between Uni- \& Multi-Modal Shifts per Dataset}

Fig.~\ref{fig:crosscorrheatmap} shows the heatmap of the correlation between uni-modal and multi-modal shifts per dataset. Question-joint shift correlations are higher than image-joint shift correlations across all VQA datasets and fine-tuning methods. However, pre-train model maintains similar correlation between both modalities. Vanilla FT and SPD exhibits the lowest question-joint shift correlation shown by the darkest row color across all fine-tuning methods in \ref{fig:ques_ft_correlation_heatmap}. Whilst, SPD shows the lowest image-joint shift correlation across the datasets in \ref{fig:img_final_correlation_heatmap}.

\input{figures/crosscorrheatmap}

% \clearpage

\subsubsection{Histograms for Evaluating Different Distribution Shifts}



\begin{figure*}[!h]
    \centering
    
    % Subfigure for VQAv2 val
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_vqa_raw_val.png}
        \caption{VQAv2 Val}
        \label{fig:VQAv2_val}
    \end{subfigure}
    \hfill
    % Subfigure for IV VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_iv-vqa_val.png}
        \caption{IV VQA}
        \label{fig:ivvqa}
    \end{subfigure}
    \hfill
    % Subfigure for CV VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_cv-vqa_val.png}
        \caption{CV VQA}
        \label{fig:vcvvqa}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for VQA Rephrasings
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_vqa_rephrasings_val.png}
        \caption{VQA Rephrasings}
        \label{fig:vvqa_rephrasings}
    \end{subfigure}
    \hfill
    % Subfigure for VQA CP v2
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_vqa_cp_val.png}
        \caption{VQA CP v2}
        \label{fig:vvqa_cp_v2}
    \end{subfigure}
    \hfill
    % Subfigure for VQA CE
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_vqa_ce_val.png}
        \caption{VQA CE}
        \label{fig:vvqa_ce}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for ADVQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_advqa_val.png}
        \caption{ADVQA}
        \label{fig:vadvqa}
    \end{subfigure}
    \hfill
    % Subfigure for Text VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_textvqa_val.png}
        \caption{Text VQA}
        \label{fig:vtextvqa}
    \end{subfigure}
    \hfill
    % Subfigure for VizWiz
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_vizwiz_val.png}
        \caption{VizWiz}
        \label{fig:vvizwiz}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for OK VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/visual/lora_img_final_coco_okvqa_val.png}
        \caption{OK VQA}
        \label{fig:vokvqa}
    \end{subfigure}
    \caption{Histogram for Vanilla FT Visual Shifts: We depict the \( S_{\text{Maha}} \) score on the visual modality for each sample in the VQAv2 train split in blue and the corresponding test samples in orange. There's minimal visual shifts for all VQA datasets from the VQAv2 train, except for Figure \subref{fig:vvizwiz} which shows evidence of greater shifts between the orange distribution and the blue distribution. }
    \label{fig:v_histograms}
\end{figure*}



\begin{figure*}[!h]
    \centering

    % Subfigure for VQAv2 val
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_vqa_raw_val.png}
        \caption{VQAv2 Val}
        \label{fig:vqvqav2_val}
    \end{subfigure}
    \hfill
    % Subfigure for IV VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_iv-vqa_val.png}
        \caption{IV VQA}
        \label{fig:vqivvqa}
    \end{subfigure}
    \hfill
    % Subfigure for CV VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_cv-vqa_val.png}
        \caption{CV VQA}
        \label{fig:vqcvvqa}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for VQA Rephrasings
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_vqa_rephrasings_val.png}
        \caption{VQA Rephrasings}
        \label{fig:vqvqa_rephrasings}
    \end{subfigure}
    \hfill
    % Subfigure for VQA CP v2
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_vqa_cp_val.png}
        \caption{VQA CP v2}
        \label{fig:vqvqa_cp_v2}
    \end{subfigure}
    \hfill
    % Subfigure for VQA CE
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_vqa_ce_val.png}
        \caption{VQA CE}
        \label{fig:vqvqa_ce}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for ADVQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_advqa_val.png}
        \caption{ADVQA}
        \label{fig:vqadvqa}
    \end{subfigure}
    \hfill
    % Subfigure for Text VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_textvqa_val.png}
        \caption{Text VQA}
        \label{fig:vqtextvqa}
    \end{subfigure}
    \hfill
    % Subfigure for VizWiz
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_vizwiz_val.png}
        \caption{VizWiz}
        \label{fig:vqvizwiz}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for OK VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/question/lora_ques_ft_coco_okvqa_val.png}
        \caption{OK VQA}
        \label{fig:vqokvqa}
    \end{subfigure}

    \caption{Histogram for Vanilla FT Question Shifts: We depict the \( S_{\text{Maha}} \) score on the question modality for each sample in the VQAv2 train split in blue and the corresponding test samples in orange. Similar to the visual shift histograms, far OODs (Figures \subref{fig:vqtextvqa}, \subref{fig:vqvizwiz}, \subref{fig:vqokvqa}) also show evidence of greater shifts between the orange distribution and the blue distribution than near OODs.}
    \label{fig:vq_histograms}
\end{figure*}


\begin{figure*}[!h]
    \centering

    % Subfigure for VQAv2 val
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_vqa_raw_val.png}
        \caption{VQAv2 Val}
        \label{fig:vqaVQAv2_val}
    \end{subfigure}
    \hfill
    % Subfigure for IV VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_iv-vqa_val.png}
        \caption{IV VQA}
        \label{fig:vqaivvqa}
    \end{subfigure}
    \hfill
    % Subfigure for CV VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_cv-vqa_val.png}
        \caption{CV VQA}
        \label{fig:vqacvvqa}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for VQA Rephrasings
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_vqa_rephrasings_val.png}
        \caption{VQA Rephrasings}
        \label{fig:vqavqa_rephrasings}
    \end{subfigure}
    \hfill
    % Subfigure for VQA CP v2
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_vqa_cp_val.png}
        \caption{VQA CP v2}
        \label{fig:vqavqa_cp_v2}
    \end{subfigure}
    \hfill
    % Subfigure for VQA CE
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_vqa_ce_val.png}
        \caption{VQA CE}
        \label{fig:vqavqa_ce}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for ADVQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_advqa_val.png}
        \caption{ADVQA}
        \label{fig:vqaadvqa}
    \end{subfigure}
    \hfill
    % Subfigure for Text VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_textvqa_val.png}
        \caption{Text VQA}
        \label{fig:vqatextvqa}
    \end{subfigure}
    \hfill
    % Subfigure for VizWiz
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_vizwiz_val.png}
        \caption{VizWiz}
        \label{fig:vqavizwiz}
    \end{subfigure}

    \vspace{0.5cm} % Space between rows

    % Subfigure for OK VQA
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/histograms/joint/lora_joint_coco_okvqa_val.png}
        \caption{OKVQA}
        \label{fig:vqaokvqa}
    \end{subfigure}

    \caption{Histogram for Vanilla FT V+Q Shifts : We depict the \( S_{\text{Maha}} \) score on the V+Q shift for each sample in the VQAv2 train split in blue and the corresponding test samples in orange. For all test splits, V+Q shifts show a greater degree of shift compared to the corresponding visual and question shift.}
    \label{fig:vqa_histograms}
\end{figure*}


% VQAV2 val 
% ivvqa 
% cvvqa 
% vqa_rephrasings
% vqa_cp v2 
% vqa_ce
% advqa 
% textvqa
% vizwiz
% okvqa













