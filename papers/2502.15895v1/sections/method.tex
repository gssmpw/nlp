\section{Directional Gradient Projection for Robust Fine-tuning}
In this section, we first describe the intuition and the mathematical motivation behind \emph{DiGraP}. Then, we provide our method's concrete algorithmic design.

\input{figures/method_pic}


\subsection{Robust Fine-tuning as a Multi-objective Optimization Problem}
In order to adapt the model to the downstream tasks but preserve the power of the pre-trained model (e.g. robustness to distribution shifts), methods such as L2-SP~\citep{li_explicit_2018} impose a regularization term on the distance between the fine-tuned and pre-trained weights% . These methods have the advantage of being simple to implement and do not require more computionally heavy methods requiring bi-level optimization
\zkn{What about positioning w.r.t. TPGM/FTP? Perhaps it can be added in related work that those methods have the complexity of bi-level optimization, etc. Depending on that narrative, we can perhaps not mention here. }. Formally,
\begin{align}
    \label{eq:l2_sp}
    \mathcal{L(\theta)} = \Tilde{\mathcal{L}}(\theta) + \frac{\lambda}{2}\|\theta-\theta_0\|^2_2
\end{align}
where $\theta$ denotes the fine-tuned weights, $\theta_0$ the pre-trained weights, $\Tilde{\mathcal{L}}(\theta)$ the original loss function\zkn{maybe mention e.g. cross-entropy to emphasize no regularization}, and $\lambda$ the hyper-parameter for regularization strength, i.e., weight decay. In this case, $\|\theta-\theta_0\|^2_2$ serves as a constraint so that the updated model will not deviate from the initialization too much, thus we can maintain some strengths from the pre-trained model. However, L2-SP is not intuitive to tune $\lambda$ which often spans over a wide range\zkn{Should we mention that this is also harder to tune if it's applied differently across layers?}: a small $\lambda$ may achieve better in-distribution performance but leads to poor OOD robustness, while a large $\lambda$ results in underfitting. Besides, L2-SP is also harder to tune if applied differently across layers. 

In this work, we instead propose\zkn{Use explicit words like ``we propose'' to make it clear what your contribution is. I added this.} to view robust fine-tuning from a \textit{multi-objective optimization} perspective, leading to a more explicit method to balance this trade-off. Specifically, there are two objectives that we want to optimize,
\begin{align}
    \label{eq:two_obj}
    \textbf{Objective}_1 = \Tilde{\mathcal{L}}(\theta), \textbf{Objective}_2 = \frac{1}{2}\|\theta-\theta_0\|^2_2
\end{align}
where the first objective represents the original loss function and the second objective represents the distance between the fine-tuned and pre-trained weights. Our goal is to minimize both at the same time to achieve ID generalization and OOD robustness.


\subsection{Projecting Conflicting Gradients}
Viewed from this perspective, we can leverage prior multi-objective methods towards our problem.\zkn{Added transition} PCGrad~\citep{yu2020gradientsurgerymultitasklearning} hypothesizes that the key optimization issue in multi-objective learning arises from conflicting gradients, where gradients for different objectives point away from each other. Thus, optimizing one of them will lead to the suboptimality of the others.
They propose a form of gradient surgery by projecting a taskâ€™s gradient onto the normal plane of the gradient of any other task that has a conflicting gradient, therefore benefiting all objectives. 

Inspired by PCGrad, we propose the following algorithm for robust finetuning: When the gradients between the two objectives are in conflict, i.e. their cosine similarity is negative, we project the gradient for the original loss function to the orthogonal direction of the gradient for the regularization term. Specifically, the gradients for the two objectives and the projection of the first gradient in the direction of the second gradient are respectively:
\begin{align}
    \label{eq:grad}
    \tilde{g}_{1} = \nabla_{\theta} \Tilde{\mathcal{L}}(\theta),
    ~\tilde{g}_{2} = \theta - \theta_0,
    ~\tilde{g}_{1}^{\text{proj}} = \frac{\tilde{g}_{1} \tilde{g}_{2}}{\|\tilde{g}_{2}\|^2} \tilde{g}_{2}
\end{align}
We add a hyper-parameter $\omega \in [0,1]$ to further control the projection strength. $\omega=0$ is equivalent to an unconstrained gradient update, while $\omega=1$ is the same as a full orthogonal projection. The final projected gradient is the following:
\begin{align}
    \label{eq:final_grad}
    g = \tilde{g}_1 - \omega \tilde{g}_{1}^{\text{proj}} = \tilde{g}_1 -\omega \frac{\tilde{g}_{1} \tilde{g}_{2}}{\|\tilde{g}_{2}\|^2} \tilde{g}_{2},~\omega \in [0,1]
\end{align}

Note that for L2-SP, the gradient for the regularized loss function is formulated similarly:
\begin{align}
    \label{l2sp_grad}
    g = \nabla_{\theta} \mathcal{L(\theta)} = \nabla_{\theta} \Tilde{\mathcal{L}}(\theta) + \lambda (\theta-\theta_0) = \tilde{g}_1 + \lambda \tilde{g}_2
\end{align}

Thus, \emph{DiGraP} is equivalent to L2-SP with different $\lambda$ for every layer\zkn{This is not clear; you don't have $t$ subscript above; do you mean that part of your proposed algorithm is to make this layer/iteration dependent? Also, wasn't L2-SP also applied per layer at least? Need to clarify here and be more precise. }. In summary, for each layer $i$:
\begin{itemize}
    \item \textbf{Gradients are non-conflicting} ($\tilde{g}_{1}^i \tilde{g}_{2}^i \geq 0$): $\lambda^i = 0$
    \item \textbf{Gradients are conflicting} ($\tilde{g}_{1}^i \tilde{g}_{2}^i < 0$): $\lambda^i = -\omega^i \frac{\tilde{g}_{1}^i \tilde{g}_{2}^i}{\|\tilde{g}_{2}^i\|^2}$
\end{itemize}

Compared to L2-SP, the hyper-parameter $\omega$ in \emph{DiGraP} is within the range between 0 and 1, which is more intuitive to tune. Furthermore, even with one fixed $\omega$, the regularization strength $\lambda$ varies across both layers and iterations, making the fine-tuning process more flexible to fit the training data.


\subsection{Layer-wise Trainable Directional Gradient Projection}
We emphasize that the regularization problem in Eq.~\ref{eq:l2_sp} is still not fully equivalent to the multi-objective optimization in Eq.~\ref{eq:two_obj}\zkn{This is confusing; 3.2 makes it seem like you're already converting eq. 1 into a multi-objective problem inspired by pccgrad. Now you're going back to eq. 1 and 2. }. Specifically, for a multi-objective optimization problem, we want to optimize all objective functions, i.e., to minimize both $\Tilde{\mathcal{L}}(\theta)$ and $\frac{1}{2}\|\theta-\theta_0\|^2_2$ in our case. However, for a regularization problem, the regularization term does not necessarily decrease. Instead, it acts as a constraint on the original loss function and the regularization term is smaller compared to the one in a model trained without regularization. Projecting the original gradient to the orthogonal direction of the gradient for the regularization term will potentially lead to underfitting. It is especially detrimental at the beginning of the training, where the fine-tuned weights are close to the pre-trained weights, thus it is more benefitial for the model to stick to its original gradient descent direction.

As a result, we aim for the projection strength $\omega$ to be dynamic throughout the training process. Intuitively, $\omega$ should start small during the early iterations, allowing the model to prioritize fitting to the downstream task. As training progresses and the fine-tuned model diverges further from its initial state, $\omega$ should gradually increase to guide the fine-tuned gradient direction towards alignment with the regularization gradient direction. {In Sec.~\ref{sec:sensitivity_analysis} we will visualize the variation of projection strength $\omega$ throughout training to further validate this motivation.} \zkn{Are you adding analysis later confirming this? That's crucial if this is one of the motivations.}

To achieve this, we make the projection strength $\omega$ trainable, allowing it to adapt throughout the training process\zkn{Does this also remove the extra hyper-parameter (which would be a benefit)? If so, mention}.
For the $t$ step of unconstrained gradient descent with the learning rate of $\alpha$, the model weights update as follows\zkn{It's not clear what this is. A definition/background for the derivative in eq 8? Need some text/transition. Same with next eq 7 description as well. },
\begin{align}
    \label{eq:unconstrained_gd}
    \tilde{\theta}_{t} = \theta_{t-1} - \alpha \tilde{g}_{t,1}
\end{align}
where $\tilde{\theta}_{t}$, $\theta_{t-1}$ and $\tilde{g}_{t,1}$ denote the unconstrained model weights at current step $t$, the model updates of previous step $t-1$ and the gradient for the original loss function at current step $t$.

For one step of directional gradient descent, the model weights update as follows,
\begin{align}
    \label{eq:directional_gd}
    \theta_{t} = \theta_{t-1} - \alpha (\tilde{g}_{t,1} - \omega_t \frac{\tilde{g}_{t, 1} \tilde{g}_{t, 2}}{\|\tilde{g}_{t, 2}\|^2} \tilde{g}_{t, 2})
    = \tilde{\theta}_{t} + \alpha \omega_t \frac{\tilde{g}_{t, 1} \tilde{g}_{t, 2}}{\|\tilde{g}_{t, 2}\|^2} \tilde{g}_{t, 2}
\end{align}
where $\theta_{t}$, $\omega_t$ and $\tilde{g}_{t, 2}$ denote the constrained model weights, the projection strength and the gradient for the regularization term at current step $t$. $\tilde{\theta}_{t}$ and $\tilde{g}_{t,1}$ are the same as the ones in Eq.~\ref{eq:unconstrained_gd}.

The derivative of the original loss function $\Tilde{\mathcal{L}}(\theta)$ w.r.t. $\omega$ is as follows using the chain rule:
\begin{align}
    \label{eq:omega_lr}
    \nabla\omega := \frac{\partial \Tilde{\mathcal{L}}(\theta_{t-1})}{\partial \omega}
    = \frac{\partial \Tilde{\mathcal{L}}(\theta_{t-1})}{\theta} \frac{\partial \theta_{t-1}}{\partial \omega}
    = \alpha \tilde{g}_{t,1} \tilde{g}_{t-1, 1}^{\text{proj}}
\end{align}

We initialize $\omega_0=0$ for the first iteration and update with learning rate of $\mu$. We also add normalization on $\nabla\omega$ for numerical stability. For \emph{DiGraP}, instead of tuning the weight decay $\lambda$ in L2-SP, we only tune the learning rate $\mu$ of the projection strength $\omega$\zkn{Is this less sensitive? We should have sensitivity analysis}. {We argue that tuning $\mu$ is less sensitive and will provide sensitivity analysis in Sec.~\ref{sec:sensitivity_analysis}. We also compare fixed and trainable projection strength $\omega$ in Sec.~\ref{sec:compare_fixed_trainable}.} The final algorithm of \emph{DiGraP} is illustrated in Alg.~\ref{algo:digrap}.

\input{sections/algorithms}

{In summary, our unique contribution lies in adapting gradient projection to the specific challenges of robust fine-tuning. While PCGrad was designed for multi-task learning, \emph{DiGraP} extends these principles to the fine-tuning of pre-trained models by introducing a hyper-optimizer and tailoring gradient projection to balance both ID and OOD performance. This refinement allows us to address the unique trade-offs in robust fine-tuning, which are distinct from those in multi-task optimization. }

\subsection{Compatability with Parameter-Efficient Fine-tuning (PEFT) Methods}

\emph{DiGraP} is further compatible with PEFT methods such as LoRA~\citep{hu_lora_2021}, which is a prevalent fine-tuning strategy for large foundation models. PEFT methods generally update new parameters to add to the original weights. In this case, instead of optimizing the distance between fine-tuned and pre-trained weights $\frac{1}{2}\|\theta - \theta_0\|^2$, we minimize the distance between the updated weight and origin $\frac{1}{2}\|\theta\|^2$ in PEFT. Thus, when combined with PEFT, \emph{DiGraP} does not need to save an additional pre-trained copy and requires the same amount of memory as PEFT. In Sec.~\ref{sec:ref_vqa} and Sec.~\ref{sec:ft_vqa}, we demonstrate that \emph{DiGraP} can further improve the results of LoRA on VQA tasks.

% We emphasize that our unique contribution lies in adapting gradient projection to the specific challenges of robust fine-tuning. While PCGrad was designed for multi-task learning, \emph{DiGraP} extends these principles to the fine-tuning of pre-trained models by introducing a hyper-optimizer and tailoring gradient projection to balance both ID and OOD performance. This refinement allows us to address the unique trade-offs in robust fine-tuning, which are distinct from those in multi-task optimization. 

