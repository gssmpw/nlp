\begin{algorithm}[!h]
\caption{Adam with Trainable Directional Gradient Projection}
\label{algo:digrap}
\begin{algorithmic}[1]
\State \textbf{Input:} $\theta_0$: pre-trained model, $\alpha$: learning rate, $\mu$: learning rate for $\omega$, $(\beta_1, \beta_2) \leftarrow (0.9, 0.999)$
\State \textbf{Initialize:} $m_0 \leftarrow 0$, $v_0 \leftarrow 0$
\For{$t = 1$ to $T$}
    \State 
    $\begin{cases}
    \tilde{g}_{t, 1} \leftarrow \nabla_{\theta} \mathcal{L}(\theta_{t-1}) \\
    \tilde{g}_{t, 2} \leftarrow \theta_{t-1} - \theta_0
    \end{cases}$
    \Comment{Gradients of the Objectives (Eq.~\ref{eq:grad})}
    
    \State $\tilde{g}_{t, 1}^{\text{proj}} \leftarrow \frac{\tilde{g}_{t, 1} \tilde{g}_{t, 2}}{\|\tilde{g}_{t, 2}\|^2} \tilde{g}_{t, 2}$
    \Comment{Gradient Projection (Eq.~\ref{eq:grad})}

    \If {$t = 1$}
        \State $\omega_t \leftarrow 0$ \Comment{Initialize $\omega$}
    \Else
        \State 
        $\begin{cases}
        \nabla\omega_{t} \leftarrow \textbf{Normalization}(\alpha_{t-1} \tilde{g}_{t,1} \tilde{g}_{t-1, 1}^{\text{proj}}) \\
        \omega_{t} \leftarrow \max(0, \min(1, \textbf{AdamUpdate}(\omega_{t-1}, \nabla \omega_{t}, \mu, t))
        \end{cases}$
        \Comment{Updating $\omega$ (Eq.~\ref{eq:omega_lr})}
    \EndIf
    
    \If {$\tilde{g}_{t, 1}\tilde{g}_{t, 2} < 0$}
        \State $g_t \leftarrow \tilde{g}_{t,1}$
        \Comment{Unconstrained Gradient Descent (Eq.~\ref{eq:unconstrained_gd})}
    \Else
        \State 
        $
        g_{t} \leftarrow \tilde{g}_{t, 1} - \omega_{t}\tilde{g}_{t, 1}^{\text{proj}}
        $
        \Comment{Directional Gradient Projection (Eq.~\ref{eq:final_grad}, Eq.~\ref{eq:directional_gd})}
    \EndIf
    
    \State $m_t \leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t$
    \State $v_t \leftarrow \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$
    \State \textbf{Bias Correction:} $\widehat{m_t} \leftarrow \frac{m_t}{1 - \beta_1^t}$, $\widehat{v_t} \leftarrow \frac{v_t}{1 - \beta_2^t}$
    \State \textbf{Update:} $\theta_t \leftarrow \theta_{t-1} - \frac{\alpha_t \widehat{m_t}}{\sqrt{\widehat{v_t}} + \epsilon}$
\EndFor
\end{algorithmic}
\end{algorithm}
