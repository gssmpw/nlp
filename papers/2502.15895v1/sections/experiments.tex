\section{Experiments}

\noindent \textbf{Overview.} We test \emph{DiGraP} on a variety of benchmarks, tasks and architectures to validate its effectiveness. The experiments are split into three sections including image classification (Sec.~\ref{sec:img_cls}), reformulating image classification as VQA tasks (Sec.~\ref{sec:ref_vqa}) and fine-tuning on VQA datasets (Sec.~\ref{sec:ft_vqa}). We emphasize that it is important to move robust fine-tuning towards multi-modal models, given their popularity and richness in terms of different types and strenghts of distribution shfit. For Sec.~\ref{sec:img_cls}, we use a discriminative backbone - ImageNet pretrained MOCO-V3 ResNet50~\citep{chen2021empiricalstudytrainingselfsupervised} as the pre-trained model. We follow the setting of the previous work and further details can be found at~\citet{tian_trainable_2023}. For Sec.~\ref{sec:ref_vqa} and Sec.~\ref{sec:ft_vqa}, we use a generative backbone - Google's recently released PaliGemma~\citep{beyer_paligemma_2024} pretrained on a broad mixture of large-scale vision-language tasks, which is lightweight and achieves state-of-the-art performance on VQAv2.
In Appendix~\ref{sec:more_results}, we include additional results with more backbones and a langugae-only experiment.

% we include experiments of fine-tuning on DomainNet with CLIP ViT-Base~\citep{radford_learning_2021} (Tab.~\ref{tab:domainnet_clipvit}), DomainNet-oVQA (Tab.~\ref{tab:domainnet_ft_llava}), and VQA (Tab.~\ref{tab:vqa_llava}) with LLaVA-7B~\citep{liu2023visualinstructiontuning}. We also conduct a langugae-only experiment in Tab.~\ref{tab:sa_comparison}, where we use BOSS~\citep{yuan2023revisitingoutofdistributionrobustnessnlp}, an NLP benchmark suite designed for OOD robustness evaluation. This suite includes both ID and OOD language-only datasets across multiple tasks (e.g., Sentiment Analysis, Toxic Detection).

\noindent \textbf{Datasets.} For Sec.~\ref{sec:img_cls} and Sec.~\ref{sec:ref_vqa}, we use DomainNet~\citep{peng2019momentmatchingmultisourcedomain} as the benchmark, which consists of six domains (real, sketch, painting, infograph, clipart and quickdraw) with 345 classes. We fine-tune our model on real domain and evaluate on all other domains. For Sec.~\ref{sec:ft_vqa}, we fine-tune on VQAv2~\citep{goyal_making_2017} and test on nine OOD datasets using LoRA~\citep{hu_lora_2021}. For the near OODs, we evaluate on VQAv2's six variants, namely IV-VQA~\citep{agarwal_towards_2020}, CV-VQA~\citep{agarwal_towards_2020}, VQA-Rephrasings~\citep{shah_cycle-consistency_2019}, VQA-CP v2~\citep{agrawal_dont_2018}, VQA-CE~\citep{dancette_beyond_2021} and AdVQA~\citep{sheng_human-adversarial_2021}, which cover uni-modal, multi-modal and adversarial distribution shifts from VQAv2. We also include TextVQA~\citep{singh_towards_2019}, VizWiz~\citep{bigham_vizwiz_nodate} and OK-VQAv2~\citep{reichman_outside_2023}, which are constructed from different sources than VQAv2, as the far OOD datasets. Further details can be found in Sec.~\ref{sec:ft_vqa}.\zkn{Need a section on hyper-parameter tuning, perhaps in supplementary if there is no room. Do you tune $\mu$ per-dataset or just on one dataset and use the same on the rest (same question for each type, image classification, image-VQA, and VQA? Emphasize tuning on validation set. }

\noindent \textbf{Training Details.} The hyper-parameter $\mu$ is found through cross-validation per dataset, and the model with the best ID validation accuracy is taken. We leave all training details to Appendix~\ref{sec:training}

\subsection{Image Classification Experiments}
\label{sec:img_cls}
\noindent \textbf{\emph{DiGraP} outperforms robust fine-tuning baselines on image classification task.} We utilize the DomainNet as benchmark and compare \emph{DiGraP} with several existing methods using ImageNet pre-trained MOCO-V3 ResNet50 as initialization. We follow the training scheme and use the same hyper-parameters of prior work~\citep{tian_trainable_2023}. In Tab.~\ref{tab:domainnet_moco}, we observe that \emph{DiGraP} achieves the best OOD performance across all OOD domains and a competitive ID performance on the real domain. Specifically, \emph{DiGraP} outperforms L2-SP~\citep{li_explicit_2018} and magnitude-based projection methods~\citep{tian_trainable_2023,tian_fast_2023} on both ID and OOD results\zkn{This is a hard argument to make; how do we know? Need sensitivity analysis or similar, or tone down this claim}. Note that we use the reported baseline results from \citet{tian_fast_2023} where the Quickdraw results are not presented.

\input{tables/domainnet_moco}

\subsection{Reformulating Image Classification as VQA tasks}
\label{sec:ref_vqa}

\input{figures/domainnet-ovqa}

Previous work primarily focuses on uni-modal benchmarks but is seldom tested on multi-modal settings. As an additional contribution, we first bridge the gap by using the same benchmark but reformulate it as a VQA task and tested several robust fine-tuning methods. Specifically, inspired by~\citet{ging2024openendedvqabenchmarkingvisionlanguage}, we change DomainNet to DomainNet-oVQA by using the same images but asking questions such as "What is in the image?" with class labels as the ground truth answers. To make the two tasks more comparable, we use the ClipMatch (ClipM) metric from~\citet{ging2024openendedvqabenchmarkingvisionlanguage} by embedding the model prediction and class names with EVA-Clip~\citep{sun2023evaclipimprovedtrainingtechniques} and obtain the most similar one by matching them using cosine similarity.
We consider this benchmark as one OOD dataset with distribution shifts only in image modality and will conduct a more comprehensive experiments with distribution shifts in other modalities in Sec.~\ref{sec:ft_vqa}. We use the pre-trained generative vision-language model (VLM) PaliGemma~\citep{beyer_paligemma_2024} as initialization.

\input{tables/domainnet_ft}

\noindent \textbf{Vanilla fine-tuning outperforms zero-shot on both ID and OOD datasets.} Surprisingly, vanilla fine-tuning on DomainNet-Real-oVQA improves the performance on every domain in DomainNet-oVQA, while doing the same but as an image classification task degrades the OOD performance compared to zero-shot~\citep{wortsman_robust_2022}. Note that during fine-tuning in the image classification task, we only take the image encoder from the backbone model and add a linear head after it, i.e., we remove the text encoder and uses cross-entropy loss which is different from the pre-trained loss used during pre-training. However, ~\citep{goyal2023finetune} points out that fine-tuning is more robust when trained with the same objective as pre-training. This may be a potential reason for the robustness of vanilla fine-tuning for VQA tasks, since fine-tuning on the VQAs does not change the pre-training model structure and loss function. We will not focus on this problem in this paper and leave further discussion to future work. Nevertheless, one interesting question remains: \textit{when vanilla fine-tuning performs well on datasets with distributon shifts, can previous robust fine-tuning baselines further increase the robustness?}

\noindent \textbf{WiSE worsens both ID generalization and OOD robustness under VQA tasks.} ~\citet{wortsman_robust_2022} reports that ensembling the weights of the zero-shot and fine-tuned models can benefit from both the robustness of the zero-shot and the downstream adaptation of the fine-tuned models for image classification tasks. However, WiSE is highly dependent on the performance of the pre-trained model, and only works when vianilla fine-tuning decreases the robustness. In Tab.~\ref{tab:domainnet_ft}, there is a huge gap between the zero-shot and fine-tuned results. In this case, naively combining the pre-trained weights reduces the model's robustness under all distribution shifts compared to vanilla fine-tuning. 
% \textcolor{red}{TODO: WiSE weight sweeping}

\noindent \textbf{Directional Gradient Projection benefits the models in general.} In Tab.~\ref{tab:domainnet_ft}, \emph{DiGraP} outperforms two-stage training (LP-FT), weight interpolation (WiSE-FT) and bi-level optimization (FTP) on both ID and average OOD performance. We argue that WiSE-FT only works for the setting where zero-shot performance is strong while \emph{DiGraP} is beneficial for general cases. 


\subsection{Fine-tuning on VQA Datasets}
\label{sec:ft_vqa}

We now conduct a comprehensive experiment on various VQA datasets with different distribution shifts. We consider VQAv2~\citep{goyal_making_2017} as the ID dataset. We further evaluate the model on six near OOD datasets which construct different types of distribution shifts from VQAv2 and three far OOD datasets in which the image and text sources are completely different from VQAv2.

\input{figures/vqa_datasets}

\textbf{ID Dataset.}
VQAv2~\citep{goyal_making_2017} builds upon VQAv1~\citep{agrawal2017cvqacompositionalsplitvisual} by balancing question-answer pairs with complementary images to minimize the bias in language priors, thus is more challenging and is widely used as a benchmark for popular vision-language models.


\textbf{OOD Datasets.}
1) \textit{Distribution Shifts to Images.} IV-VQA~\citep{agarwal_towards_2020} and CV-VQA~\citep{agarwal_towards_2020} remove the objects irrelevant to answering the question and generate complementary images with one instance of the object removed respectively.
2) \textit{Distribution Shifts to Questions.} VQA-Rephrasings~\citep{shah_cycle-consistency_2019} collects three rephrasings of each question. 
3) \textit{Distribution Shifts to Answers.} VQA-CP~\citep{agrawal_dont_2018} reorganizes the correlation between the question type and correct answer.
4) \textit{Distribution Shifts to Multi-modalities.} VQA-CE~\citep{dancette_beyond_2021} selects a subset of VQAv2 that are counterexamples of potential multi-modal shortcuts.
5) \textit{Adversarial Distribution Shifts.} AdVQA~\citep{sheng_human-adversarial_2021} provides human-adversarial examples for questions where the model’s predicted answer is incorrect.
6) \textit{Far OODs.} TextVQA~\citep{singh_towards_2019} requires models to answer questions by understanding text embedded in images. VizWiz~\citep{bigham_vizwiz_nodate} contains user-generated images with diverse challenges like poor quality, ambiguity, and irrelevant content for answering visual questions. OK-VQAv2~\citep{reichman_outside_2023} represents a knowledge-based VQA task where the visual question cannot be answered without external knowledge.

\noindent \textbf{Evaluation and Metrics.} We follow the metric from VQAv2~\citep{goyal_making_2017} and the evaluation is based on the accuracy of predicted answers compared to ground truth human-annotated answers. For each question, the dataset includes 10 human-provided answers. The accuracy is calculated as: $\text{Accuracy} = \min\left(\frac{\text{number of humans who gave the answer}}{3}, 1\right)
$.

\noindent \textbf{Measuring the OOD Distance.} We explore shifts on single image modality and joint image question (V+Q) shifts, as well as image question answer (V+Q+A) shifts by computing
the test set shift relative to the training domain (i.e. VQAv2 train) using the negative Mahalanobis distance metric. The higher the value, the less the distribution shift. More details are in Appendix~\ref{sec:ood_dist}.

\noindent \textbf{Experimental Results.} We fine-tune the PaliGemma-3B model on the VQAv2 dataset with LoRA and evaluate on the other OOD datasets. The results of \emph{DiGraP} and other robust fine-tuning methods are shown in Tab.~\ref{tab:main_result}. We have the following observations.

\input{tables/main_results} 

\noindent \textbf{Smaller distribution shifts correlate with better OOD performance.} The analysis of image and joint shifts reveals a high correlation with VQA performance, evidenced by correlation values of 0.83 and 0.80, respectively. This suggests that larger shifts significantly degrade VQA performance. Such trends validate our methodology in quantifying shifts, as far OOD scenarios align with increased shift levels and diminished performance. 
% Further, joint shifts are higher than those of the image modality alone, suggesting that the model effectively maintains spatial separation between in-distribution (ID) and out-of-distribution (OOD) embeddings across both visual and joint modalities. 
% However, the model's sensitivity to both image and joint shifts indicates a need for improved generalizability, as we would expect it to maintain consistent performance across varying degrees of shifts. 

% , consistent with theoretical generalization bounds

\noindent \textbf{Full fine-tuning improves zero-shot performance across ID, near OOD, and far OOD datasets.} 
As shown in Tab.~\ref{tab:main_result}, we observe no degradation in OOD performance following vanilla fine-tuning, even when PaliGemma is pre-trained on VQA tasks. This may be attributed to reasons similar to those discussed in Sec.~\ref{sec:ref_vqa}, or due to differing characteristics of the backbone models and tasks. Once again, WiSE negatively impacts both ID generalization and OOD robustness in VQA tasks when interpolating between pre-trained and fine-tuned weights.

% According to the PaliGemma~\citep{beyer_paligemma_2024} paper, for PaLI-based models, the pretraining is designed to result in a model that transfers well, not necessarily a model that is usable out of the box (“0 shot”). In this case, WiSE-FT~\citep{wortsman_robust_2022} fails.

\noindent \textbf{\emph{DiGraP} outperforms baselines on both ID and near-OOD datasets.} Beyond improvements on uni-modal tasks, \emph{DiGraP} enhances vanilla fine-tuning and consistently outperforms other baselines in multi-modal settings. As shown in Tab.~\ref{tab:main_result}, \emph{DiGraP} achieves the highest ID and average near-OOD results, demonstrating robustness to distribution shifts across various modalities, including vision, question, answer, their combinations, and adversarial shifts.

\noindent \textbf{\emph{DiGraP} is competitive on far OOD datasets.} From Tab.~\ref{tab:main_result}, we observe that \emph{DiGraP} also improves vanilla fine-tuning on the three far OOD benchmarks and is the second best among all robust fine-tuning methods. Notably, while FTP~\citep{tian_trainable_2023} performs well on OOD, it severely underfits the training domain with a substantial lower ID performance compared to vanilla fine-tuning and \emph{DiGraP}, even with the positive gradient annealing factor $\kappa=0$\zkn{Not sure if they will know what this is without reading FTP paper. }\zkn{In the past, one strategy is to tune the other method (FTP) to achieve the same ID performance as our method. We may want to do this at least for rebuttal}, which indicates weakest regularization. However, FTP demonstrates outstanding performance on far OOD tasks with significant higher results on the three far OOD datasets. One potential reason is that FTP imposes much stronger regularization since even when $\kappa=0$ the projection constraints are \textit{non-decreasing} during training, which means it still provides regularization. In FTP, the authors also mention that $\kappa=0$ is necessary to obtain the best performance if underfitting is observed. However, in \emph{DiGraP}, $\omega=0$ is equivalent to unconstrained fine-tuning with no regularization. Thus, \emph{DiGraP} enforces weaker regularization than FTP. We emphasize that the significant performance decrease from zero-shot to fine-tuning is mostly observed on rather simple tasks (e.g. image classification), while \emph{DiGraP} is more general for all cases. Nevertheless, it remains interesting why FTP significantly increases the far OOD performance given the zero-shot is poor in Tab.~\ref{tab:main_result}, and we will leave the exploration to future work.
