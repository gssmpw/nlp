\section{Hyper-Parameter Tuning and Ablation Study}

\subsection{Variation of Projection Strength Throughout Training}

\input{figures/sensitivity_plot}

We visualize the variation of the average projection strength $\omega$ of all layers over iterations for five different hyper-parameters $\mu \in \{0.01, 0.1, 0.5, 1, 100\}$ in Fig.~\ref{fig:variation}. As we increase $\mu$, the projection strength $\omega$ becomes larger. For all cases, the projection strength $\omega$ starts from zero and converges at the end of the training. This aligns with our intuition that the projection strength should vary over time to learn dynamic priority of the two objectives during different stage of training.

\subsection{Impact of Hyper-Parameter Sensitivity on Robustness}
\label{sec:sensitivity_analysis}

\input{tables/sensitivity}

We further perform the sensitivity analysis of the hyper-parameter $\mu$ on ID and average OOD performance for DomainNet-oVQA and VQA experiments. Results from Tab.~\ref{tab:hyper} show that both ID and OOD results fluctuate slightly even when $\mu$ spans over a wide range from 0.01 to 100. This again proves that \emph{DiGrap} is more controllable and less sensitive to hyper-parameter change.

\subsection{Ablating Fixed and Trainable Projection Strength}
\label{sec:compare_fixed_trainable}
\input{tables/ablate_fixed_omega}

To validate the effectiveness of trainable projection strength $\omega$, we conduct analysis to compare with fixed projection strength with different values $\omega \in \{0.1, 0.5, 0.9\}$. Tab.~\ref{tab:ablate_fixed_omega} shows that trainable \emph{DiGraP} outperforms the others and achieves the best ID and average OOD results.


% \subsection{Ablating Constrained and Unconstrained Projection strength}