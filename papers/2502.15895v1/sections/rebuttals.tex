% \zkn{Please add ``thank you'' parts for each reviewer in the beginning!}

% \section{Overall Response}

% We thank all the reviewers for their positive feedback on this work's adaptability (fvUQ), performance (fvUQ, D9Ep), and insights (fvUQ, 1rWn, D9Ep). We aim to address your questions with concrete responses and clarify any points of confusion.

% The Additional Experiments for Rebuttal Section in Appendix includes new experiments and studies requested by the reviewers. The new experiments are summarized below.
% \begin{itemize}
% \item  (D9Ep) We add experiments of fine-tuning on DomainNet with CLIP ViT-Base, DomainNet-oVQA and VQA with LlaVA-7B. DiGraP consistently achieves the best performance across all experiments.
% \item (fvUQ, 1rWn) We conduct an LP-FT-C experiment (a controlled version of LP-FT) by increasing the regularization strength to ensure that the ID performance matches that of DiGraP. Despite this adjustment, DiGraP still outperforms LP-FT-C on OOD datasets.
% \item (1rWn) We include the hyper-parameter tuning experiment for DomainNet on CLIP ViT-Base. The results demonstrate that DiGraP remains robust to hyperparameter variations on both ID and OOD datasets in the uni-modal image classification task.
% \item (D9Ep) We present a visualization of the variation in regularization strength ($\lambda$) across different layers over epochs.
% \end{itemize}

% We hope our response answers your questions and are open to further discussion during the rebuttal period.

% \section{Individual Response}

% \subsection{Reviewer fvUQ}

% We thank the reviewer for the positive feedback and for suggesting ablation study on the ID and OOD trade-off and insightful new tasks. We have provided detailed answers in our response and are happy to discuss further during the rebuttal period if needed.

% \textbf{While the paper addresses the problem of robust fine-tuning, the proposed approach Directional Gradient Projection appears similar to PCGrad. The problem of conflicting gradients and the idea of projecting the gradient to the normal plane have been addressed in PCGrad. I would encourage the authors to highlight the unique contributions more clearly.}

% The differences between DiGraP and PCGrad are as follows: 1) Objective Focus: PCGrad is designed to address multi-objective optimization problems, while DiGraP is specifically tailored for robust fine-tuning. Unlike previous robust fine-tuning approaches that treat it as a regularization task or a bi-level optimization problem, DiGraP is the first to frame robust fine-tuning as a multi-objective optimization problem. 2) Weighting and Adaptability: PCGrad requires manually defining weights for different objectives and treats projection strength as a fixed hyperparameter throughout training. In contrast, DiGraP dynamically learns the dominance of different objectives through a hyper-optimizer, allowing the projection strength to adapt across layers and evolve during training. This adaptive approach is significantly more robust compared to manually defined hyperparameters, and is hence a crucial enhancement to the specific problem that we tackle (robust fine-tuning).

% \textbf{The paper proposes a general approach for robust fine-tuning of foundation models, but the paper focuses on Image classification and VQA tasks. It would be useful to evaluate the approach by fine-tuning open-source LLMs like LLaMA on a multi-task benchmark like MMLU.}

% Thank you for pointing this out. Previous work on robust fine-tuning has primarily focused on image classification, with limited benchmarks addressing language distribution shifts. In this paper, we propose \textit{a comprehensive multi-modal setting}, which includes language and language-specific shifts such as VQA-Rephrasings (question shift) and VQA-CP (answer shift).
% % \zkn{Point them to the experiments/datasets which have language-only shift, if possible}. 
% % \zkn{Removed: ``but language-only benchmarks remain underexplored''.} 
% While MMLU is a multi-task benchmark, it does not include settings related to distribution shifts, which differs from the focus of our work. 

% % \zkn{I'm not sure about adding this; it is very orthogonal to what they are asking for}
% Nevertheless, we provide additional experiments on different backbones to further demonstrate the effectiveness of DiGraP. The results of the experiments are in the Additional Experiments for Rebuttal Section in Appendix: DomainNet with CLIP ViT-Base (Tab.~\ref{tab:domainnet_clipvit}), DomainNet-oVQA (Tab.~\ref{tab:domainnet_ft_llava}) and VQA (Tab.~\ref{tab:vqa_llava}) with LlaVA. Currently, we are working on in-distribution experiments on MMLU and will share the results once they are complete.

% \textbf{In Table 1, while DiGraP performs well on OOD domains, the performance on the ID domain is poor compared to other approaches like LP-FT and TPGM. By changing the strength of ‘w’, does the approach allow a trade-off to improve performance on ID while compromising OOD performance?}

% We note that the performance of DiGraP is better on ID than vanilla finetuning in Table 1 and better than all other methods in all other tables. Further, LP-FT's performance comes at greater cost on OOD performance. For the rebuttal, we equalize LP-FT's ID performance similar to TPGM-C in~\citet{tian_trainable_2023}
% % \zkn{Missing citation, make sure to get all of them below!}
% , where we conduct an LP-FT-C experiment (a controlled version of LP-FT) in Tab.~\ref{tab:domainnet_moco_appendix} by increasing the regularization strength to ensure that the ID performance matches that of DiGraP. Despite this adjustment, DiGraP still outperforms LP-FT-C on OOD datasets.

% \textbf{In Table 4(b), when the learning rate is increased, which makes the projection strength larger, why does the ID performance improve?}

% When the learning rate is increased, the overall projection strength becomes larger. However, the individual projection strength for each layer adjusts dynamically during training, which aims to both preserve pre-train robustness and adapt to downstream tasks, so it does not necessarily lead to the decrease of ID performance. This is a major benefit of our adaptive approach to robust finetuning (highlighted above).

% \subsection{Reviewer 1rWn}

% Thank you for your positive remarks and for suggesting additional qualitative study and hyper-parameter tuning experiments. We have provided comprehensive answers in our response and would be happy to discuss further during the rebuttal period if required.

% \textbf{Although the work attempts to utilize gradient direction information, there are still deficiencies in the innovation of the method. It looks like a combination of previous work, so the innovation needs to be explained more.}

% The distinctions between DiGraP and PCGrad can be summarized as follows: 
% 1) Purpose: PCGrad is developed for addressing general multi-objective optimization challenges, whereas DiGraP is designed specifically to enhance robust fine-tuning. DiGraP introduces a novel perspective by framing robust fine-tuning as a multi-objective optimization problem, setting it apart from conventional methods that rely on regularization techniques or bi-level optimization frameworks.
% 2) Flexibility and Automation: PCGrad requires users to manually assign weights to different objectives and fixes the projection strength as a static hyperparameter throughout the training process. In contrast, DiGraP leverages a hyper-optimizer to automatically learn the relative importance of objectives, allowing the projection strength to dynamically adjust at different layers and adapt as training progresses. This capability makes DiGraP significantly more flexible and robust than manually tuned approaches, and is hence a crucial enhancement to the specific problem that we tackle (robust fine-tuning).

% \textbf{Sec 4.1 lacks qualitative analysis of the experimental results, especially the reasons why the ID performance on the real domain is worse than LP-FT.}

% We note that the performance of DiGraP is better on ID than vanilla finetuning in Table 1 and better than all other methods in all other tables. Further, LP-FT's performance comes at greater cost on OOD performance. For the rebuttal, we equalize LP-FT's ID performance as in TPGM-C from~\cite{}, where we perform an LP-FT-C experiment (a controlled variant of LP-FT) in Tab.~\ref{tab:domainnet_moco_appendix} by increasing the regularization strength to align the ID performance with that of DiGraP. Even with this adjustment, DiGraP demonstrates superior performance compared to LP-FT-C on OOD datasets.

% \textbf{The article seems to have only conducted a quantitative analysis of hyper-parameters for multi-modal tasks, and corresponding ablation experiments are still needed on singal-modal tasks.}

% We include the hyperparameter tuning experiment for DomainNet in Tab.~\ref{tab:hyper_img} of the Additional Experiments for Rebuttal Section in Appendix. The results demonstrate that DiGraP remains robust to hyperparameter variations on both ID and OOD datasets in the uni-modal image classification task.

% \subsection{Reviewer D9Ep}

% We appreciate the reviewer’s kind comments and the suggestion of adding more backbones and visualization of the projection strength across the layers over training. We have included thorough answers in our response and are happy to engage further during the rebuttal period if clarification is needed.

% \textbf{There are many public unimodal and multimodal foundation models, e.g., MAE, CLIP, BEiT3, LLaVa, etc. It is unclear why ResNet50 and PaliGemma are selected as foundation models. The ResNet50 pretrained in a supervised manner on ImageNet can hardly be deemed as foundation models. The PaliGemma is pretrained on a broad mixture of large-scale vision-language tasks. Whether the conclusion of this paper holds across other, more general foundation models, e.g., CLIP or LLaVa, is questionable.}

% We adopt the MOCO-V3 ResNet50 as the backbone for the image classification task, following and to compare to previous work~\citet{tian_fast_2023,tian_trainable_2023}. MOCO-V3 ResNet50 builds on the ResNet50 architecture but is pre-trained on ImageNet-1k in a self-supervised manner rather than supervised. For the multi-modal backbone, we use PaliGemma-3B, a recently released lightweight model by Google that achieves state-of-the-art performance on VQAv2.

% For this rebuttal, we additionally  include results for DomainNet with CLIP ViT-Base (Tab.~\ref{tab:domainnet_clipvit}), DomainNet-oVQA (Tab.~\ref{tab:domainnet_ft_llava}) and VQA (Tab.~\ref{tab:vqa_llava}) with LlaVA in the Additional Experiments for Rebuttal Section in Appendix. DiGraP consistently achieves the best performance across all experiments. Thank you for this suggestion and we will include these in the paper. 

% \textbf{Although DiGraP demonstrates improved performance in near OOD settings, its performance on far OOD tasks remains limited compared to other methods. The authors acknowledge this trade-off between ID/near OOD and far OOD robustness, but a deeper investigation into addressing this limitation would enhance the model’s versatility.}
% \textbf{Could additional techniques be incorporated to enhance DiGraP’s performance on far OOD datasets without sacrificing ID and near OOD performance?}

% Thanks for pointing this out. Enhancing robustness across both near and far OOD settings is a challenging and underexplored problem. Most prior work, such as~\citep{wortsman_robust_2022,kumar_fine-tuning_2022,tian_trainable_2023,tian_fast_2023}, focuses on general OOD robustness without explicitly differentiating between near and far OOD scenarios. Addressing DiGraP’s limitations on far OOD datasets while maintaining strong ID and near OOD performance requires a more detailed investigation into the dynamics of projection strength and its interaction with diverse distribution shifts. This remains a promising direction for future research, which could significantly enhance the versatility and robustness of DiGraP across a wider range of tasks.

% \textbf{Could the authors provide more insights into how the projection strength parameter adapts dynamically across different layers and tasks, especially in multi-modal settings?}

% % \zkn{I would put this one above the previous one on interpretability, as it is a stronger response with new visualizations}
% We present a visualization of the variation in regularization strength ($\lambda$) across different layers over epochs in Fig.~\ref{fig:layer_epoch_reg_strength} of the Additional Experiments for Rebuttal Section in Appendix. The results show that the regularization strength evolves dynamically during training, starting small, increasing over iterations, and eventually converging. In the vision layers (blue), early layers tend to experience weaker regularization compared to later layers throughout the training process. Conversely, the language layers (orange) display a more uniform regularization strength, with comparable levels observed between early and later layers.

% The weaker regularization in early vision layers likely allows them to preserve foundational low-level features, while stronger regularization in later layers encourages the model to focus on high-level semantic representations. Thank you for the suggestion; we will include these visualizations in the paper in the hopes that they provide additional ideas for future work. 

% \textbf{The model’s gradient projection mechanism, while theoretically sound, lacks interpretability in how projection strength decisions impact specific instances.}

% DiGraP is designed to balance pre-trained and fine-tuned trajectories by dynamically adjusting projection strength to optimize overall training directions. This adjustment can be interpreted layer-wise, showing how different layers contribute to balancing pre-trained knowledge and fine-tuning, and iteration-wise, revealing how projection strength evolves during training. However, it lacks interpretability at the instance level, as it focuses on global optimization across layers and iterations rather than tailoring projection strength to individual data instances.

% \textbf{The figures in Appendix have too small font.}

% Thanks for the suggestion! We will increase the font size and update the figures.

% \clearpage
\input{tables/domainnet_moco_appendix}
\input{tables/sensitivity_image}
\input{tables/vqa_llava}
\input{tables/domainnet_ft_llava}
\input{tables/domainnet_clip}

\input{tables/boss_t5}

\input{figures/layer_epoch_reg_strength}