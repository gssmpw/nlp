\section{Related Works}

\noindent \textbf{Robust Fine-Tuning of Foundation Models.} LP-FT~\citep{kumar_fine-tuning_2022} proposes a two-step strategy of linear probing then full fine-tuning to prevent the feature distortion of the pre-trained layers. WiSE-FT~\citep{wortsman_robust_2022} interpolates the pre-trained and fine-tuned weights to combine the strengths of the two embedding space. L2-SP~\citep{li_explicit_2018} explicitly adds an L2 norm penalty on the deviation between the fine-tuned and pre-trained weights. MARS-SP~\citep{gouk_distance-based_2021} further studies different forms of norms as the penalty. More recently, TPGM~\citep{tian_trainable_2023} approaches the regularization term as a constraint, reformulating the problem as a \textit{bi-level optimization} and proposing to learn distinct hard constraints for each layer. FTP~\citep{tian_fast_2023} further improves the efficiency of TPGM~\citep{tian_trainable_2023} by learning the constraint from training set of previous step instead of the current validation set. We argue that these methods are still computationally heavy and requires lots of tuning, whereas \emph{DiGraP} reformulates the problems as \textit{multi-objective optimization}, injects directional information and is more intuitive to tune.

\noindent \textbf{OOD Robustness in VQA.} Previous works have proposed various settings for evaluating robust VQA models. \citet{agrawal_reassessing_2023} conducts cross-dataset evaluations with four VQA datasets, while \citet{ma_robust_2024} and \citet{li_closer_2021} provides a more comprehensive and detailed robustness analysis by incorporating VQAv2 variants and categorizing different types of distribution shifts. We build on these efforts by introducing further granularity with near and far OOD distinctions and measuring the distance between distributions. More importantly, while previous work has primarily focused on testing different backbone models, they have not yet compared different robust fine-tuning methods.

% \noindent \textbf{Parameter Efficient Fine-Tuning (PEFT).} 
% ~\citep{biderman_lora_2024}~\citep{hu_llm-adapters_2023}
