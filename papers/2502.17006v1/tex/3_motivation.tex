\section{Motivation}
\update{This section describes the motivation behind developing a dual-mode-aware DNN compiler for CIM accelerators.
We identify the diverse on-chip computing and memory requirements inherent in real-world DNNs. Additionally, we discuss the opportunities of meeting these application requirements through the optimization of CIM array mode configuration during compilation.}
% the configuration of CIM array mode during compilation.

\subsection{Insights into Diverse DNN Requirements}

% 资源量需求是不同，反映到Cim上直接的需求是不一样的，横轴不太直观，就写compute/memory资源配比不同，他们的假设全部都是compute，我们发现由于计算密度不一样，没有办法说只用一种C/M比例就可以是最好的性能，需要动态，然后说现在的CIM是支持的，但是编译都没有考虑这件事情，我们就要去做一个利用双模的特性来满足所有的DNN的多样性。让其性能最佳
% 网络多样性：在我们的分析发现单个网络内也有多样性
% 思考是不是把下面这个图和计算/访存比的图对应起来放在两边

% 程序特性差异需要动态调整的memory支持，直观
%[621ZTODO] 这图字太多，有时间了再调下大小
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{image/new_diversity3D.pdf}
     \caption{(a)(b) Normalized performance variation with the changes of compute/memory array; (c) Arithmetic Intensity.}
    \label{fig:background2}
\end{figure}

\noindent
\textbf{Variations among different network architectures.}
Mainstream neural networks exhibit diverse architecture designs, leading to varied hardware requirements \cite{he2016deep,simonyan2014very,dosovitskiy2020image,devlin2018bert,redmon2016you,touvron2023llama,nerf,rombach2022high}. 
% For instance, convolutional neural networks (CNNs) are commonly used as backbone networks for computer vision tasks\cite{he2016deep,simonyan2014very,redmon2016you}, while transformer-based architectures are prevalent in natural language processing (NLP)\cite{devlin2018bert,touvron2023llama}.
% The differing designs of these network architectures lead to varied hardware requirements. 
% number -> ratio?
\fig \ref{fig:background2} (a)(b) illustrates the normalized performance variation heatmap of Llama2 \cite{touvron2023llama} and ResNet-50 \cite{he2016deep} with changes in the number of arrays in compute/memory mode. We assume there is a total of 100 dual-mode CIM arrays on-chip, where the switchable CIM array is as Dynapalsis \cite{kim202316}. 
% when the total on-chip resources are fixed, increasing the number of compute arrays results in a corresponding decrease in on-chip memory capacity. 
% Using switchable dual-mode CIM arrays in as Dynapalsis \cite{kim202316} as the hardware configuration, we assume a total of 100 CIM arrays on-chip. 
% To simplify, the compute array and memory array in this figure refer to the CIM array in compute and memory mode, respectively.
% The horizontal and vertical axis represent
The memory and compute axes represent the number of CIM arrays in memory and compute mode, respectively.
The vertical axis indicates the theoretical performance normalized to the optimal performance with the same amount of total arrays.
% , where each model is normalized against its best achievable performance as the ratio of compute/memory arrays varies.
Green indicates better performance, while dark blue indicates poor performance.
% The color scale indicates the normalized theoretical performance, where each model is normalized against its best achievable performance as the ratio of compute/memory arrays varies.
% Dark blue indicates better performance, while green indicates poor performance.
% The red star indicates the optimal memory/compute unit allocation.
% 这里是不是的解释一下为什么计算有时候计算单元多了，反而也没有很好的收益:解释了
Llama2 and ResNet-50 exhibit distinct preferences for hardware resource allocation, stemming from their differing arithmetic intensities. 
% the memory and compute resources originate from their different arithmetic intensity.
 As illustrated in \fig \ref{fig:background2}(c), ResNet-50 has a significantly higher average arithmetic intensity compared to Llama2. % as shown in the \fig \ref{fig:background2}(c). 
Consequently, Llama2, with its lower arithmetic intensity, does not require extensive computing resources but necessitates increased on-chip memory to complete its operations. 
Thus, Llama2 demands more CIM arrays in memory mode to meet its computational needs. Conversely, ResNet-50, with its higher arithmetic intensity, benefits from greater compute resources to achieve optimal performance.
Furthermore, \fig \ref{fig:background2}(c) indicates the varying arithmetic intensities across different models. 
Therefore, the dynamic adjustment of memory and compute resources in CIM is essential to provide optimal performance for diverse DNNs.
\begin{comment}
% The horizontal axis represents the number of CIM arrays set to compute mode, with the total on-chip resources fixed. Increasing the number of arrays in compute mode decreases on-chip memory capacity accordingly.
% The vertical axis shows normalized theoretical performance, where each model is normalized against its best achievable performance as the number of compute/memory arrays varies.
% The inflection point on the graph indicates the optimal allocation of compute and memory modes for CIM arrays that maximize application performance within the constraints of the total available CIM arrays. 
% To the left of the inflection point, performance improves due to increased compute resources, as data access does not yet limit application execution. To the right, performance declines because the reduction in on-chip memory arrays impacts achievable performance due to insufficient memory capacity.
% It is evident that different applications have very different tendencies regarding their demand for storage/computation arrays.

% Various networks have various optimal allocations of compute and memory modes for CIM arrays. 

This phenomenon is consistent with the arithmetic intensity of the network, defined as the ratio of the amount of computation to the amount of input data.
ResNet-50 has a much higher arithmetic intensity compared to Llama2. 
Consequently, Llama2 requires more CIM arrays in memory mode than ResNet-50 under the same total number of CIM arrays. 

% For CIM architectures, network weights are pre-mapped onto the chip, and 
The volume of data accessible from the main memory and on-chip buffer (which includes the original buffer and CIM arrays in memory mode) each cycle determines the maximum input data. 
The product of arithmetic intensity and memory bandwidth, along with the number of compute arrays, determines the maximum computation per cycle. The smaller of these two values represents the actual achievable computational efficiency.
Thus, even with the same total hardware resources, 
more compute arrays but limited memory resources can result in the underutilization of compute units due to insufficient data transfer support. This scenario can lead to lower application performance compared to configurations with fewer compute units but sufficient data transfer support.
% if memory resources are limited, having more compute units may result in fewer utilized compute resources due to insufficient data transfer support. This can lead to lower application performance compared to scenarios with fewer compute units but adequate data transfer support.
 As shown in the \fig \ref{fig:background2}(c), various models have distinct arithmetic intensities, leading to diverse compute/memory resource requirements.
\end{comment}
% 
 % As shown in the \fig \ref{fig:background2}(c), models with higher arithmetic intensity tend to require more compute resources.
% for an optimal compute/memory balance.
% Generative models like llama2\cite{touvron2023llama} favor having more on-chip memory over computational resources, whereas CNNs like ResNet\cite{he2016deep} and encoder-only language models like BERT\cite{devlin2018bert} prefer more computational resources. This preference is due to the nature of the tasks. 
% For instance, during the generation phase, the computational load for generative models Llama 2 \cite{touvron2023llama} is roughly twice the data loading volume. At the same time, ResNet-50's arithmetic intensity is about 66 \cite{he2016deep}. 
% As the arithmetic intensity of ResNet-50 is much higher than Llama 2, Llama 2 needs more CIM arrays in memory mode than ResNet-50 under the same CIM array amount constraints.
% This phenomenon highlights the varying demands for on-chip memory and computational resources across different applications.
% 表达在硬件总资源限制固定的情况下，面对不同需求的应用，实现计算/存储资源均衡分配才能发挥硬件的全部潜力。
% Therefore, we can maximize the applications' performance under the same hardware resource constraints by adjusting an optimal compute-to-memory resource allocation. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{image/diversity2_3.pdf}
    \caption{(a) Layer-specific arithmetic intensity of ResNet-50 \cite{he2016deep}; (b)Arithmetic intensity of BERT-Large with different sequence lengths \cite{devlin2018bert}.}
    \label{fig:layer}
\end{figure}

% [621] 该图需要根据后面内容确定后进行修改
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{image/workflow_pdf_new.pdf}
    \caption{Overview of dual-mode-aware compilation process.}
    \label{fig: workflow}
\end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{image/diversity2.pdf}
%     \caption{Layer-specific arithmetic intensity of ResNet-50 \cite{he2016deep}.}
%     \label{fig:layer}
% \end{figure}

\noindent
\textbf{Layer-wise variations within the same network.} Within the same neural network, different layers also have varying hardware demands due to factors such as input data size and network parameters, including weight kernel size. 
% Consequently, each layer requires a distinct balance of computational and memory resources.
For example, as illustrated in the \fig \ref{fig:layer}(a), ResNet-50\cite{he2016deep} comprises four distinct blocks, each containing three configurations of convolution layers. The arithmetic intensity of these three layers varies significantly, ranging from below $100~\mathrm{FLOPs/MOP}$ to over $700~\mathrm{FLOPs/MOP}$.
% as an example, it has four different blocks and each one has 3 different configurations of convolution layers. The arithmetic intensity varies from under 100 FLOPs/MOP to over 700 FLOPs/MOP.
% Therefore, even within a single network architecture, it is crucial to dynamically adjust the on-chip compute and memory resources for each layer to optimize performance.

 
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{image/diversity3.pdf}
%     \caption{Arithmetic intensity of BERT-Large with different sequence lengths.}
%     \label{fig:stage}
% \end{figure}

\noindent
\textbf{Variations on different workload scales.}
Transformer-based \cite{vaswani2017attention} NLP models, such as BERT \cite{devlin2018bert}, show dynamic resource requirements based on varying input and output sequence lengths.
% Transformer-based \cite{vaswani2017attention} natural language processing (NLP) models, such as BERT \cite{devlin2018bert} and GPT, have garnered significant attention in recent years \cite{touvron2023llama}. 
As illustrated in the \fig \ref{fig:layer}(b), the arithmetic intensity of the model fluctuates significantly with the input and output sequence length, varying from under 150 FLOPs/MOP to over 1000 FLOPs/MOP.
Additionally, different computation stages within the models, such as fully connected (FC) layers and query-key-value (QKV) computations, display varying arithmetic intensities. For example, FC layers demonstrate much higher arithmetic intensity compared to QKV computations as the sequence length increases.
As sequence length grows, more memory is needed to store intermediate states and longer contextual information, while additional computational resources are required to manage the increased complexity of the attention mechanism. Consequently, the demand for compute and memory resources dynamically adjusts with sequence length.
\begin{comment}

% the model\cite{devlin2018bert} exhibits varying arithmetic intensities depending on the length of the input and output sequences, varying from under 150 FLOPs/MOP to over 1000 FLOPs/MOP. 
Meanwhile, the different computation stages also have various arithmetic intensities, e.g. the arithmetic intensities of FCs are much higher than the QKV when the sequence length increases.
As the sequence length increases, the model requires more memory to store intermediate states and longer contextual information. Additionally, longer sequences necessitate more computational resources to handle the increased complexity of the attention mechanism. Consequently, the compute and memory resources requirement changes dynamically with sequence length. 
% Effectively managing CIM unit mode to change the compute and memory ratio in response to varying sequence lengths enhances the model's scalability and generalization capabilities.
    
\end{comment}


\subsection{Opportunity of CIM Dual-Mode Switch}
\update{Given the diverse resource demands of various DNN architectures, layers, and workload scales, dynamic hardware resource allocation is crucial for optimizing model execution performance. A static compute/memory resource ratio is often insufficient to achieve optimal efficiency across different scenarios. Traditional compilation techniques typically struggle with inefficiencies due to their inability to adapt to fluctuating resource requirements.}
% It is impossible to achieve optimal performance with a single compute/memory resource ratio.
% Traditional compilation methods often face inefficiencies due to fixed resource allocations that do not adapt to varying demands. 
\update{By leveraging the dual-mode switching capability of CIM arrays, we can dynamically alternate between compute and memory modes, enabling CIM accelerators to more effectively accommodate the diverse needs of DNN workloads. Specifically, repurposing compute arrays into memory arrays allows CIM accelerators to expand on-chip memory resources, which is particularly beneficial for storing dynamically generated activations in DNNs.
This flexibility can significantly boost overall system performance and energy efficiency by tailoring hardware configurations to the unique requirements of each DNN model.}
% and reduce idle resource waste. 
% Fortunately, with the introduction of dual-mode CIM arrays, it becomes possible to switch between memory and compute modes based on the specific needs of the applications.
% This adaptability allows for more efficient resource utilization and can significantly reduce execution latency.
% Meanwhile, the possible allocation for dual-mode CIM arrays provides us with a vast of options.
% Therefore, fully leveraging the potential of CIM architecture requires considering its dual-mode characteristics, which support both compute and memory functionalities.
% This allows for adjusting computation-memory resource allocation according to specific application needs with minimal overhead.
% Therefore, leverage this flexibility to minimize latency, ensuring that the dual-mode CIM arrays provide the best possible performance for any given workload.
% According to the different resource demands of various architectures, various layers, and various execution stages, dynamic hardware resource allocation can optimize model performance and reduce idle resource waste. Therefore, to fully leverage the potential of CIM architecture, it is essential to consider its dual-mode characteristics of supporting both compute and memory functionality comprehensively, which provides us a chance to adjust the computation-memory resource allocation according to the specific needs of the applications with minimal overhead.

% 


% ; (c)  Performance improvement opportunities brought by dual-mode-aware compilation
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.5\linewidth]{image/heatmap.pdf}
%     \caption{Hardware resources utilization with the computation-storage unit change}
%     \label{fig:heatmap}
% \end{figure}
% 如果这里要说挑战的话，需要拿具体例子来说
% \subsection{CIM dual-mode switch for performance improvement: Opportunity and Challenge}
% \noindent
% \textbf{Opportunity.} CIM provides the possibility of converting the CIM array between compute and memory modes. By controlling the inputs to CIM arrays or changing the driver, we can adjust the array's functionality with minimal overhead. This presents a new opportunity to accommodate the diverse computation and memory resource demands of various real-world DNNs. We can flexibly allocate compute and memory resources based on the requirements of different inference tasks, enabling dynamic resource management and optimization through the CIM array compute/memory mode switch.
% For example, when processing long sequence inputs, we can allocate more CIM arrays for memory purposes to enhance the network's capability in handling long sequence data, thus meeting the demands of tasks with varying sequence lengths.


\noindent
% \textbf{Challenge.} Existing CIM compilation works have not considered the inherent dual mode of CIM. 
% In the fixed memory-compute hardware resource scenario, the total memory access demands and computation requirements are predictable, existing compilation optimization methods strive to optimize program scheduling based on known hardware resources.
% They only optimize the application mapping with fixed hardware resources. To fully harness the potential of CIM architecture, we need to find efficient ways of jointly optimizing compute/memory mode allocation and application mapping scheduling to adjust memory and compute resources for different applications during compilation optimization. To achieve this, we still need to address the following challenges:

% [TODOhere]Intro的challenge改了后这里同步也要对描述进行修改
% find an efficient way to express the dual-mode switch.(in a simple and convenient way)
% \textbf{1. How to effectively incorporate the dimension of \switch~ into compilation optimization?}
% Traditionally, compilation optimization focuses on mapping applications onto CIM processors with fixed resources. Introducing the compute-memory switch as a new dimension shifts the paradigm. We need to co-optimize CIM mode allocation with scheduling, rather than treating them as separate problems.
% The compiler requires visibility into hardware architecture parameters. It's crucial to develop a method that clearly expresses the CIM architecture’s capability to switch between compute and memory modes. 
% Therefore, a hardware abstraction should be established to quantify the impacts of switching between compute and memory modes.
% % The hardware abstraction provides the compiler with visible hardware architecture parameters\cite{qu2024cim,sun2023pimcomp}.
% Specifically, compilation optimization is based on the hardware abstraction of CIM processors\cite{qu2024cim,sun2023pimcomp}. 
% From the perspective of hardware abstraction, it is crucial to find a clear way to express the CIM architecture's ability to switch between compute and memory modes.
% Existing abstractions cannot represent the ways of \switch~and their corresponding overheads. Therefore, we cannot rely solely on existing hardware abstractions for dual-mode aware compilation. 
% Based on this hardware abstraction, we also need to establish a cost model that incorporates the impact of memory/computation arrays on latency, integrating dual-mode switches into the joint optimization space.
% Meanwhile, current mapping scheduling optimization algorithms are also based on fixed hardware resources. We need to introduce dual-mode switch-related hardware description into the hardware abstraction to obtain a joint optimization space as well as find efficient optimization algorithms to explore the joint optimization space.

% \textbf{2. How to efficiently explore the joint scheduling optimization space?}
% One approach for \switch~optimization is to brute-force traverse all possible mode combinations of on-chip CIM arrays based on existing compilation optimization methods for CIM architecture.
% % to explore suitable compute-memory unit allocation. 
% However, assuming there are $N$ dual-mode arrays, the search complexity would increase to $2^N$, leading to exponential growth. This makes brute-force searching for multidimensional scheduling optimization impractical. Additionally, separate optimization of the dual-mode allocation and application scheduling could result in suboptimal compilation optimization due to the non-orthogonality of the two spaces.
% Therefore, we should find a customized optimal solution exploration method to explore the dual-mode allocation and application scheduling optimization space jointly.
% Therefore, we need to combine the dual-mode allocation and application scheduling into a joint optimization space. Based on the compilation optimization space formalization, we also should find a customized optimal solution exploration method. 
% Meanwhile, for the real-world DNN and CIM processors, 

% Therefore, we need to find a joint optimization space representation and compilation method to efficiently explore scheduling for applications on dual-mode CIM.

To leverage this flexibility, we propose CMSwitch, a dual-mode-aware DNN compiler for CIM processors, ensuring that the dual-mode CIM arrays provide optimal performance for any given workload.
In the following sections, we will introduce the workflow of CMSwitch, elaborating on the dual-mode-aware compilation optimization pass.