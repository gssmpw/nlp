
%620 围绕challenge强调技术特点
% 比如：为了实现我们的优化目标解决设计中的挑战，我们需要在硬件抽象中体现array的dual mode功能，我们是通过xx来实现的，这样在之后的映射和调度的时候，compliler能够通过xx实现mode switch. 然后在编译中，我们进行网络的和映射，具体来说对进行网络分段并进行资源分别，（介绍，分段和资源分配时候的优化目标是什么，dual mode 在这个阶段怎么考虑的，是逐层去做然后进行全局调优，还是迭代优化要讲清楚），这样就实现了xx, 我们得到了xx结果。 
% DAG换成model计算图DONE
% 更新了一版但还需要进一步和challenge对齐，图也在相应的进行调整
\section{Dual-Mode-Oriented Compilation Stack}

\subsection{Overall Workflow}
\fig \ref{fig: workflow} illustrates the workflow of CMSwitch, \update{which takes user-defined hardware parameters and neural network applications as inputs.}
The neural network is initially converted into ONNX format \cite{bai2019}, lowering it to a computation graph expression.
 To integrate compute-memory mode switching into the compilation optimization space, we incorporate the dual-mode functionality of arrays in the hardware abstraction. This is achieved by introducing the methods and overheads associated with compute-memory mode switching into the hardware abstraction parameters.
 
 During compilation optimization, 
 % the compiler can perceive the mode switch characteristics through the parameters from the hardware abstraction.
to minimize application latency within the joint optimization space, \name develops a divide-and-conquer two-step policy.
% \name develops network segmentation that accounts for dual-mode switch overheads. 
\name first decides network segmentation that accounts for dual-mode switch overheads, and then optimizes the dual-mode resource allocation and scheduling for operators within each segment.
% This is integrated with optimizing dual-mode resource allocation and operator scheduling to achieve a global compilation scheme. 
% This approach determines the allocation, mapping, and scheduling of memory and compute resources for each operator. 
Through iterative exploration and optimization using Dynamic Programming (DP) and Mixed-Integer Programming (MIP), \name derives the globally optimal network segmentation schedule, along with the corresponding resource allocation and mapping results for each operator.
% This is coupled with dual-mode resource allocation and operator scheduling co-optimization to search for the global compilation scheme. 

Furthermore, to effectively present our compilation results, we introduce meta-operators specifically designed for dual-mode switching. These meta-operators facilitate the output of compilation results that incorporate the compute-memory switch scheme.
Upon obtaining the memory-compute mode switch plan offline, the actual dual-mode switch needs to be executed online with the support of the dual-mode CIM.
% HERE 是否需要写预先记录每个算子计算密度等 We also record the operators' arithmetic intensity in each node. 
% For the dual-mode-aware compilation optimization, since existing CIM processors\cite{kim202316,yan20221,yue202115,chih202116} often cannot fully accommodate the entire network, we use two steps to co-explore the CIM mode allocation. 
% [6.8]这里想说是在支持存算转换的基础上来进行网络分段的，网络分段的方式是于存算转换息息相关的（防止有人问之前也有网络分段，有啥不一样，或者为啥要网络分段），没想好咋说
% Concurrently, we establish a hardware abstraction that records dual-mode switch information.
% To minimize the overall latency, we first explore dual-mode switch overhead-aware network segmentation using dynamic programming algorithms. During the segmentation, a mixed linear programming model is established to efficiently find the optimal ratio of memory-compute units for each segment. The optimal memory-compute dual-mode CIM array allocation plan and the corresponding segment performance are used to update the action function of dynamic programming, aiming to achieve the globally optimal network segmentation.
% based on mode switchable CIM arrays.
% and the corresponding demands for dual-mode CIM array allocation in each segment.
% Finally, new instructions controlling the memory-compute mode switch are introduced based on the dual-mode CIM array allocation solution. 
% 这里我们可以说提供了meta-operator，然后可以接不同的backend
% For better generality, we use the compiler CIM-MLC \cite{qu2024cim} for backend code generation. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/abstraction.pdf}
    \caption{Dual-mode Enhanced Hardware Abstraction.}
    \label{fig: abstraction}
\end{figure}
\subsection{Dual-Mode Enhanced Hardware Abstraction (DEHA)}
% 硬件抽象有义务为编译器提供可见的硬件参数
Hardware abstraction is crucial to providing essential hardware information to the compiler for the compilation process.
% as it is responsible for including essential hardware information 
As shown in the \fig \ref{fig: abstraction}, we incorporate the dual-mode parameter of the array and the related switch function into the hardware abstraction. Together with architecture parameters, this abstraction enables \name to access the optimization space of the dual-mode CIM array and relevant architecture parameters.
% The dual-mode-aware CIM compilation process requires the introduction of the hardware's memory-compute mode switchable features into the hardware abstraction.
% The dual-mode-aware CIM compilation process requires hardware configuration information and introducing the hardware's memory-compute transformable features into the compilation optimization space. 

% As shown in the \fig \ref{fig: abstraction}, we construct a hardware abstraction that mainly includes CIM architecture and dual-mode switch parameters, which determines the supported compute-memory mode switch space and the architecture parameters exposed to the compiler.

\noindent
\textbf{Dual-mode CIM architecture.} 
In abstracting the CIM architecture, we model the CIM chip hierarchically. Given our focus on optimizing the dual-mode CIM, we simplify the abstraction to include only two essential tiers: chip and array.
At the finest granularity, our abstraction is at the CIM array level, which represents the smallest hardware unit capable of mode switching.  
% At the finest granularity, the CIM array level is represented, which serves as the minimal hardware unit for mode switching. 
Users are required to define parameters such as the number of dual-mode arrays, the array sizes, internal bandwidth, and external global bandwidth, all of which significantly influence the behavior and performance of the CIM chip.
\begin{comment}
For the CIM architecture abstraction, we abstract the CIM processor into a hierarchical architecture. 
Meanwhile, since we focus on optimization for the dual-mode CIM chip, we simplify the abstraction, retaining only two necessary abstraction tiers, chip and array.
The finest granularity of abstraction is at the CIM array level, as this represents the minimum hardware granularity for mode switching. Users need to define parameters such as the number of switchable arrays, their sizes, internal bandwidth, and external global bandwidth information, all of which impact the behavior of the CIM chip.
\end{comment}

\noindent   
\textbf{Dual mode switch.} 
During the compilation optimization process, it is crucial to consider both the functionality and overhead of the dual-mode switch to enable the compiler to balance the benefits of mode switching and explore optimal performance results.
To facilitate the compilation process, it is essential to define the method and overhead of the compute-memory mode switch adopted by the target chip at the hardware abstraction stage. This may involve techniques such as altering the inputs of wordlines or bitlines. Additionally, the overhead of the compute-memory mode switch is assessed at the granularity of the switchable arrays. These parameters significantly impact subsequent compilation optimization solutions.
\begin{comment}
   % During the compilation optimization process, we must account for the dual-mode switch function as well as the overhead introduced by mode switching. Thus, the compiler can balance the mode switch benefit to make the exploration toward the optimal performance result.
% If the mode switch overhead exceeds the benefit, we tend to output compilation results that lean towards not performing a dual-mode switch. 
% To facilitate the compilation process, we should define the method and overhead of \switch~adopted by the target chip in the hardware abstraction stage, \eg~altering the inputs of wordlines or bitlines. Meanwhile, the overhead of \switch~is collected at the granularity of the switchable array. These parameters will affect the subsequent compilation optimization solutions. 
\end{comment}

When CIM arrays operate in different modes, executing a single operation—such as activating an array for computation or data reading—may incur varying overheads. To account for this, we introduce an option in the hardware abstraction parameters to record the overhead of computation and memory operations. 
This approach ensures that the associated overheads of read-write and computation of arrays are considered during the compilation optimization process, thereby evaluating the impact of different modes on overall performance.
% Otherwise, default values based on the memory device type will be used. 

% 备用版本
% \begin{table}[]
% \caption{Notation Used in Dual-mode Resource allocation }
%     \label{tab:not}
% \resizebox{\linewidth}{!}
% {

% \begin{tabular}{l|l}
% \hline
%  \rowcolor{mygray} \multicolumn{2}{c}{Indices} \\ \hline
%          % Notation &  Description        \\ \hline
%          $IN_{O_i}$ & input data of operator $O_i$        \\ \hline
%          $OUT_{O_i}$ & output data of operator $O_i$        \\ \hline
%          $AI_{O_i}$ & arithmetic intensity  of operator $O_i$        \\ \hline
%          $W$ & \parbox[t]{8cm}{Set of operators with data dependence, $w_{i,j}$ $\in$ W means output of $O_i$ is one of the inputs of $O_j$}        \\ \hline
%           \rowcolor{mygray}\multicolumn{2}{c}{Variables} \\ \hline
%          $\lambda_{z}(i,x,y)$ &  \parbox[t]{8cm}{taking value 1 if CIM array (x,y) is assigned to $O_i$ as z, 0 otherwise. z $\in {min,mout,c}$, min means input buffer, mout means output buffer, c means compute array}       \\ \hline
         % %$\lambda_{mout}(i,x,y)$ &  \parbox[t]{8cm}{taking value 1 if CIM array (x,y) is assigned to $O_i$ in memory mode as output buffer, 0 otherwise.}       \\ \hline
%          %$\lambda_{c}(i,x,y)$ & \parbox[t]{8cm}{taking value 1 if CIM array (x,y) is assigned to $O_i$ in compute mode, 0 otherwise.}        \\ \hline
%          $Mem_{O_i}$ & \parbox[t]{8cm}{the number of CIM array in memory mode that $O_i$ has. $Mem_{O_i} = \sum \lambda_{min}(i,x,y)+\sum \lambda_{mout}(i,x,y)$ } \\ \hline
%          $Com_{O_i}$ & \parbox[t]{8cm}{the number of CIM array in compute mode that $O_i$ has. $Com_{O_i} = \sum \lambda_{c}(i,x,y)$}        \\ \hline
%          $Switch_{m\to c}$ / $Switch_{c\to m}$ & \parbox[t]{8cm}{arrays from memory(compute) mode to compute(memory) mode for segment S' to S, $Switch_{m\to c} =\textstyle \sum_{\forall\{x,y\}}(\sum_{O_i \in S'}\lambda_{m}(i,x,y) \cdot \sum_{O_j \in S}\lambda_{c}(j,x,y))$, $Switch_{m\to c} =\textstyle \sum_{\forall\{x,y\}}(\textstyle\sum_{O_i \in S'}\lambda_{c}(i,x,y) \cdot \textstyle\sum_{O_i \in S}\lambda_{m}(j,x,y))$, where $\lambda_{m}(i,x,y)=\lambda_{min}(i,x,y)+\lambda_{mout}(i,x,y)$}        \\ \hline
%          %$Switch_{c\to m}$ & \parbox[t]{8cm}{arrays from compute mode to memory mode for segment S' to S, $Switch_{m\to c} =\textstyle \sum_{\forall\{x,y\}}(\textstyle\sum_{O_i \in S'}\lambda_{c}(i,x,y) \cdot \textstyle\sum_{O_i \in S}\lambda_{m}(j,x,y))$}        \\ \hline
%           \rowcolor{mygray}\multicolumn{2}{c}{Constants} \\ \hline
%          $N_{cim}$ & the number of dual-mode switchable CIM array         \\ \hline
%          $OP_{cim}$ & operation/cycle a CIM array can provide, $OP_{cim} \propto as$         \\ \hline
%          $D_{cim}$ & data/cycle a CIM array can provide in the memory mode         \\ \hline
%          $D_{main}$ & data/cycle main memory and on-chip buffer can provide         \\ \hline
% \end{tabular}}
% \end{table}
%[TODO623] 需要根据intro的修改，修改这部分的描述，说清楚是要干啥
\subsection{Dual-Mode-Aware Compilation Optimization}
Based on the abstraction of the hardware architecture, we proposed the Dual-Mode Aware Compilation Optimization (DACO) to allocate the dual-mode CIM arrays for each CIM-supported operator, to minimize overall execution latency.
% to meet their resource requirements. 
This phase gets the ONNX-format DNN and hardware abstraction as input. 
% \update{Users need to provide the ONNX-format DNN and hardware abstraction for this phase as input.}
We employ a divide-and-conquer two-step policy.
First, dynamic programming (DP) is utilized to partition the network into multiple segments, taking into account the switching overhead between segments.
% where each segment can utilize all on-chip resources. 
Following segmentation, mixed linear programming (MIP) is applied to co-optimize the allocation of on-chip computing/memory resources and scheduling for operators, adhering to the constraint of total available resources. 
During this optimization process, the dual-mode CIM arrays are dynamically adjusted to either memory or compute mode based on the optimization objective. 
Finally, this phase outputs the network segmentation results along with the corresponding allocation of dual-mode CIM array in memory and compute mode for each segment. %dual-CIM array allocation.
The notation we used for optimization space formalization in this section is summarized in \tab \ref{tab:not}. 

\subsubsection{Dual-Mode-Aware Network Segment}
\begin{table}[]
\caption{Notations Used in Dual-mode Compilation.}
    \label{tab:not}
\resizebox{\linewidth}{!}
{

\begin{tabular}{l|l}
\hline
 % \rowcolor{mygray} \multicolumn{2}{c}{Indices} \\ \hline
         % Notation &  Description        \\ \hline
         % $W$ & \parbox[t]{8cm}{Set of operators with data dependence, $w_{i,j}$ $\in$ W means output of $O_i$ is one of the inputs of $O_j$}        \\ \hline
          \rowcolor{mygray}\multicolumn{2}{c}{Variables - decide during optimization} \\ \hline
         $\lambda_{min}(i,x,y)$ &  \parbox[t]{9cm}{taking value 1 if CIM array (x,y) is assigned to $O_i$ in memory mode as input buffer, 0 otherwise.}       \\ \hline
         $\lambda_{mout}(i,x,y)$ &  \parbox[t]{9cm}{taking value 1 if CIM array (x,y) is assigned to $O_i$ in memory mode as output buffer, 0 otherwise.}       \\ \hline
         $\lambda_{c}(i,x,y)$ & \parbox[t]{9cm}{taking value 1 if CIM array (x,y) is assigned to $O_i$ in compute mode, 0 otherwise.}        \\ \hline
         $Mem_{O_i}$ & \parbox[t]{9cm}{the number of CIM array in memory mode that $O_i$ has. $Mem_{O_i} = \textstyle\sum_{x,y} \lambda_{min}(i,x,y)+\sum_{x,y} \lambda_{mout}(i,x,y)$ } \\ \hline
         $Com_{O_i}$ & \parbox[t]{9cm}{the number of CIM array in compute mode that $O_i$ has. $Com_{O_i} =\textstyle\sum_{x,y} \lambda_{c}(i,x,y)$}        \\ \hline
         $Switch_{m\to c}$ & \parbox[t]{9cm}{arrays from memory mode to compute mode for segment S' to S, $Switch_{m\to c} =\textstyle \sum_{x,y}(\sum_{O_i \in S'}\lambda_{m}(i,x,y) \odot \sum_{O_j \in S}\lambda_{c}(j,x,y))$, where $\lambda_{m}(i,x,y)=\lambda_{min}(i,x,y)+\lambda_{mout}(i,x,y)$}        \\ \hline
         $Switch_{c\to m}$ & \parbox[t]{9cm}{arrays from compute mode to memory mode for segment S' to S, $Switch_{m\to c} =\textstyle \sum_{x,y}(\textstyle\sum_{O_i \in S'}\lambda_{c}(i,x,y) \odot \textstyle\sum_{O_i \in S}\lambda_{m}(j,x,y))$}        \\ \hline
          \rowcolor{mygray}\multicolumn{2}{c}{Constants - determine by application and CIM chip initially} \\ \hline
         $IN_{O_i}$/$OUT_{O_i}$ & input data/output data of operator $O_i$        \\ \hline
         % $OUT_{O_i}$ & output data of operator $O_i$        \\ \hline
         $AI_{O_i}$ & arithmetic intensity  of operator $O_i$        \\ \hline
         $N_{cim}$ & the number of dual-mode switchable CIM array         \\ \hline
         $OP_{cim}$ & operation/cycle a CIM array can provide, $OP_{cim} \propto array\_size$         \\ \hline
         $D_{cim}$ &  \parbox[t]{9cm}{data/cycle a CIM array can provide in the memory mode, \update{which is impacted by architecture design and user-defined topology.}}\\ \hline
         $D_{main}$ & \parbox[t]{9cm}{data/cycle main memory and original on-chip buffer can provide, $D_{main} \propto extern\_bw + internal\_bw$}\\ \hline
\end{tabular}}
\end{table}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/seg1.pdf}
    \caption{Illustration of Network Segment.}
    \label{fig:seg1}
\end{figure}
%0627 Bing:这里缺个动机，为什么要在dual-mode下考虑这件事情。
In the network segment step, we organize the network compute operators at the granularity of network segments, being allocated on the dual-mode CIM arrays holistically.
Network segmentation offers two key benefits: 1) it reduces the optimization space from an exponential size, considering the entire number of operators, to a more feasible one, and 2)  it facilitates the accommodation of real-world DNN networks that cannot fit entirely on the chip to be optimized on the dual-mode CIM.
% Mainstream network inference demands vast hardware resources \cite{devlin2018bert,touvron2023llama,brown2020language}, and existing CIM processors often cannot accommodate the mapping of the entire network. 
% To balance the potential overhead and benefits of compute-memory mode switching and consider the real-world DNN requirements, we allocate the dual-mode CIM resources at the granularity of network segments, ensuring that compute-memory mode switches only occur between segments. 
% As for the small network that can be held on the chip processor, it will 
% Therefore, we first segment the network. 
As shown in the \fig \ref{fig:seg1}, taking the attention layer as an example, if we segment the attention process into two segmentations, we will execute them serially.
The first segment maps to the hardware and executes, followed by the second segment after the first completes.
Both of them will have their resource allocation plan.
% The segmentation method decides the upper bound of the inference performance \cite{cai2022deepburning}. 
% Existing network segment methods rarely consider the conversion overheard between segments, which can significantly impact the performance 
% since the different segments may prefer various dual-mode resource allocations, and the dual-mode switches happen in the conversion process.
% since the dual-mode switches happened in the conversion process.s
To further refine the segmentation search space, we employ a dynamic programming approach to optimize network latency in the context of scheduling network segments.
% Since the minimum unit for segmenting the network is a single operator, the segmentation search space grows exponentially as the number of operators increases.
% % , there may be $2^m$ possible segmentation methods. 
% For example, ResNet-50 has approximately $10^{15}$ possible segments.
% % Meanwhile, 
% To efficiently find the segmentation method considering the segment conversation overhead that optimizes the overall latency of the network, we adopt a dynamic programming approach.

For a network $N$ with $m$ CIM-supportable operators (e.g., MVM and MMM), we first topologically sort these operators, denoted as $O_1, O_2, ..., O_m$, where $O_i$ and $O_j$ ($i,j \in \{1,2,...,m\}$) satisfy that if $i < j$, then $O_i$ is completed no later than $O_j$. The dependency relationship between operators is denoted as $W$, where $w_{i,j } \in W$ indicates that the output of $O_i$ is input to $O_j$. 
The segments after segmentation are denoted as $S_{i,j}$, indicating that operators from $O_i$ to $O_j$ belong to the same segment $S_{i,j}$. 
\update{For operators that cannot fit directly onto the CIM accelerator, we will partition them into smaller sub-operators. This partitioning process uses a greedy strategy, with the partition granularity determined by the available on-chip resources. This approach ensures that each resulting sub-operator can be fully mapped onto the chip. Finally, we replace the original operator in the sorted operator list with these sub-operators, enabling efficient execution on the CIM accelerator.}
To minimize inference latency, the segmented network execution overhead includes both intra-segment execution latency and inter-segment mode-switch latency.
% Given the target to minimize the inference latency, segmented network execution overhead includes the intra-segment execution latency and the inter-segment conversation latency.

\noindent
\textbf{Intra-segment latency.} Within each segment, all operators are mapped to the CIM chip simultaneously. Therefore, optimization methods such as pipelining can be employed to minimize the overall latency within the segment. The introduction of dual-mode CIM arrays makes the intra-segment latency highly dependent on on-chip memory and computation resource allocation. To optimize the intra-segment overhead, we will detail the dual-mode-aware resource allocation algorithm in the following subsection. We denote the intra-segment latency with resource allocation plan $A$ as $T_{i,j}^{intra}(A)$ for $S_{i,j}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/seg2.pdf}
    \caption{Three sources of inter-segment overhead.}
    \label{fig:seg2}
\end{figure}

\noindent
\textbf{Inter-segment latency.} Inter-segment latency encompasses the latency caused by switching CIM memory and computation modes and on-chip and off-chip data swapping. 
Specifically, as illustrated in \fig \ref{fig:seg2}, the inter-segment mode switch mainly consists of three steps, storing the valid on-chip data to storage, performing the mode switch between the compute/memory, and loading data. 
We formalize the inter-segment latency based on the two adjacent segments, $S_{k,i-1}$ with the resource allocation plan $A'$ and $S_{i,j}$ with the resource allocation plan $A$ ($k< i\leq j$).

For step one: if segment $S_{k,i-1}$ contains more memory arrays than $S_{i,j}$, and the output data from the segment $S_{k,i-1}$ is needed for subsequent computations, this data must be written back to the main memory before switching this CIM arrays from memory mode to compute mode. We denote the data store latency as $T_{i-1,i}^{wb}(A',A)$, and it can be estimated according to the data transfer volume and the memory bandwidth.
For data that can be processed in place and will not be reused, such as softmax results in attention, the corresponding CIM arrays can be directly switched from memory to compute mode without data write-back.
% 出度大于1意味着数据之后还会被使用
% The data that will be used later refers to the operators with edges connected to other segment subgraphs in the current segment subgraph (edges represent data dependencies). 
% \begin{equation}
%    \scriptsize T_{i,j}^{wb} =\textstyle \sum{OUT_{o_i} \textstyle \bigcap C(Mem_{o_i})}-\textstyle \sum{C(Mem_{o_j})}if w_{i,j}\in W, k(i)>=1,j\in S_{j,x} 
% \end{equation}

For the second step, when switching between memory and compute modes, if the required arrays differ between adjacent network segments, the dual-mode switch overhead must be considered. When the $S_{k,i-1}$ uses arrays in compute mode while the $S_{i,j}$ requires them in memory mode, the overhead of switching those arrays from compute mode to memory mode must be accounted for, and vice versa. The overhead $T_{i-1,i}^{swc}(A',A)$ is detailed in the Eq. \ref{eq:swc}\
\begin{equation}
    \scriptsize 
    T_{i-1,i}^{swc}(A',A) =
        L_{M\to C}\times Switch_{m \to c} +
        L_{C\to M}\times Switch_{c \to m}.
    \label{eq:swc}
\end{equation}

%[0619问题] 这里是否需要加一句，已经OK的数据就不用再加载的了，比如K,V这些可能已经存在片上，直接转换模式就可以
For the third step, as different segments process different operators, the weights stored in the compute arrays must be reloaded and rewritten. 
Each operator has unique weights, necessitating an update of the compute arrays' stored weights accordingly.
% Since each operator has unique weights, the compute arrays' stored weights need to be updated accordingly.
% For step three: We should load and rewrite the weights of compute arrays as different segments process different operators, and the weights of the operators are different, compute arrays' stored weights need to be rewritten. 
The overhead $T_{i-1,i}^{rw}(A',A)$ is shown in the Eq. \ref{eq:1}:
\begin{equation}
  T_{i-1,i}^{rw}(A',A) =\textstyle \max_{O_l \in S_{i,j}}{Com_{O_l}\times Latency\_write}.
  \label{eq:1}
\end{equation}

Based on the intra- and inter-segment overhead, network segmentation can be formalized as the following dynamic programming problem.
As shown in Eq. \ref{eq:dy}, we denote the best network segment solution with the corresponding resource allocation plan $A^*$ as the $L[m][A^*]$. The total cost of the network from operator 1 to $m$ is the sum of the costs of two segments: from 1 to $i-1$ and from $i$ to $m$, plus the mode-switch latency between these segments. To reduce the solution space, any network segment exceeding the total on-chip resources is deemed invalid. %This is because if 
If a segment requires more compute and memory arrays than the available on-chip resources, it must be further segmented during execution.
\begin{equation}
    L[m][A^*] = \min_{1 \leq i < m} \{L[i][A'] + T_{i, m}^{intra}(A) + T_{i-1, i}^{inter}(A',A)\}, \\
    \label{eq:dy}
\end{equation}
where $T_{i-1, i}^{inter}(A',A)$ equals to 
\begin{equation}
    T_{i-1, i}^{inter}(A',A) = T_{i-1, i}^{wb}(A',A)+T_{i-1, i}^{swc}(A',A)+T_{i-1, i}^{rw}(A',A).
    \label{eq:inter}
\end{equation}
%[622] 这里需要提一句，在求解并记录求解过程后回溯就可以得到最好的方案
% By formalizing the network segmentation problem as a DP problem, 
By traversing all potential choices and backtracking the segmentation plan according to Eq \ref{eq:dy}, we can efficiently find the optimal segmentation strategy, thereby minimizing execution time. This dynamic programming approach significantly reduces the search space complexity compared to exhaustive search methods.


% 流水线时空图，表示一下cost model 用计算/访存单元数量来估计
% 修改这个图中
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{image/mip_small.pdf}
%     \caption{(a)-(c) Latency with different allocation of memory and compute resources; (d) example of MMM.}
%     \label{fig:mip}
% \end{figure}
% 建模的过程中，考虑了调度的优化的，调度和allocation joint 的优化，子标题里面要体现joint的特点，用可以让编译的人懂得词。和challenge还有技术点对应
% Q: 把调度同步加进去
\subsubsection{Unified Dual-Mode Allocation with Scheduling}
% Separate optimization of the dual-mode allocation and scheduling could result in suboptimal compilation optimization due to the non-orthogonality of the two spaces.
In this section, we formalize the resource allocation and operator scheduling co-optimization problem as a Mixed-Integer Programming (MIP) problem to minimize the execution latency.
For each network segment, the allocation optimization aims to determine the optimal modes of CIM arrays assigned to each operator. Operators within the segment are scheduled in the pipelined fashion to maximize computing parallelism. To capture the interdependence of compute and memory allocation and the scheduling for operators within a segment, we define specific objectives and constraints, where constraints dictate dual-mode CIM resources available for operators and objectives that optimize latency considering the pipeline structure, respectively.
The details of our formulation are explained in the following contents.
% During the segmentation process, we account for the mode-switching overhead. 
% Within each segment, we focus on dual-mode CIM array resource allocation and operator scheduling to minimize the execution latency.
% Meanwhile, for a network segment, a dual-mode CIM array can operate in either memory or compute mode. The resource allocation problem can be transformed into determining the mode of each array in the CIM processor for a given network segment and the number of input buffer arrays, compute arrays, and output buffer arrays assigned to each operator. 
% By mapping the operators in one segment to the chip simultaneously, the operators can be scheduled in a pipelined manner to enable parallel computation.

% The notation we used is shown in the \tab \ref{tab:not}. 
%Here we use the 2-d coordinate $(x,y)$ to indicate the CIM arrays in the chip and use the $\lambda_{z}(i,x,y), z \in \{min, mout, c\}$ to indicate the resources allocation and mapping for operator $O_i$, where $min$/$mout$ mean array in memory mode as input/output buffer, respectively, and $c$ means array in compute mode. 
Here we use the 2-d coordinate $(x,y)$ to indicate the CIM arrays in the chip and use the $\lambda_{z}(i,x,y), z \in \{min, mout, c\}$ to indicate the resources allocation and mapping for operator $O_i$, 
where $min$/$mout$ means array in memory mode as input/output buffer, respectively, and $c$ means array in compute mode. 
When the CIM array (x,y) assigned to $O_i$ is in memory mode as input and output buffer, $\lambda_{min}(i,x,y)=1$ and $\lambda_{mout}(i,x,y)=1$, otherwise they are 0. Similarly, when the CIM array (x,y) assigned to $O_i$ is in compute mode, $\lambda_{c}(i,x,y)=1$.
Our goal is to find the optimal $\lambda_{z}(i,x,y)$ for each segment under the constraints imposed by the resource allocation and the objective related to the pipeline scheduling strategy.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/mip_example.pdf}
    \caption{Constraints illustration for resource allocation.}
    \label{fig:cons}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/mip_small_new.pdf}
    \caption{Example of MMM.}
    \label{fig:mip}
\end{figure}
\noindent\textbf{Constraints.}
As the dual-mode resource space is defined, some optimization rules should be considered to include only the valid CIM array allocation solutions.
% for dual-mode-aware compilation. 
Here, we use $S_*$ to indicate any possible network segment. 
% The constraint examples are shown in the \fig \ref{fig:cons}.

\textit{1) Array overlap.} A CIM array can be either memory or compute mode for an operator, but not both. Therefore, for each CIM array $(x,y)$, if it is allocated to operator $O_i$, only one of $\lambda_{min}(i,x,y), \lambda_{mout}(i,x,y)$, or $\lambda_{c}(i,x,y)$ can be 1.
\begin{equation}
    \scriptsize\forall O_i \in S_*, \forall x,y: \\
    \lambda_{min}(i,x,y)+\lambda_{mout}(i,x,y)+\lambda_{c}(i,x,y) \leq 1.
\end{equation}
As shown in \fig \ref{fig:cons}(b), CIM array $(0,0)$ is assigned to $O_1$ in compute mode, so it can not be a memory array.

\textit{2) Operator dependency.} If one operator's output $O_i$ serves as the input of the next $O_j$, the output memory arrays of $O_i$ can directly serve as the input memory arrays of $O_j$. Thus, a portion of the output memory buffer for $O_i$ can be reused as an input memory buffer for $O_j$.
% for can overlap by the amount of intersecting data.  
\begin{equation}
\scriptsize
\begin{split}
    &\forall O_i,O_j \in S_*, if\space w_{i,j} \in W: \\
    &\textstyle \sum_{x,y} \lambda_{mout}(i,x,y) \cdot \lambda_{min}(j,x,y) < \frac{OUT_{o_i} \bigcap IN_{o_j}}{array\_size}.
\end{split} 
\end{equation}
As the example shown in \fig\ref{fig:cons}(c), 
$O_1$'s output is the input of $O_2$, where array $(0,1)$ assigned to $O_1$ as output buffer can be input buffer of $O_2$.


Otherwise, there are no reused CIM resources between the adjacent operators. 
\begin{equation}
\begin{split}
    &\forall O_i,O_j \in S_*, w_{i,j} \notin W, z\in\{mout,min,c\}: \\
    &\textstyle \sum_{x,y} \lambda_{z}(i,x,y) \cdot \lambda_{z}(j,x,y) = 0 .
\end{split} 
\end{equation}
As shown in \fig \ref{fig:cons}(c), CIM array $(1,1)$ can not be compute array for operator $O_1$ and $O_2$ at the same time.
% \textit{2) Array Overlap} When the next operator uses two operators' output as the operand, memory/compute units can overlap between operators. For example, when we get the $Q, K$ in attention and if they are both stored on the chip, we can compute $Kt*Q$ in the memory array that stored $Kt$ or $Q$ by turning the array to compute mode.
% \begin{equation}
%     cons1
% \end{equation}

% [TODO] here需要一个简短的表达来表示给不同算子分配的单元的交集 $H_{i,j}(x,y)$
\textit{3) Resource limit.} The total number of CIM arrays assigned to all operators must not exceed the available resources. $H_{i,j}(x,y)$ denotes $\lambda_{mout}(i,x,y) \cdot \lambda_{min}(j,x,y)$. 
% $H_{i,j}(x,y)=1$ means the array (x,y) is assigned to both $O_i$ and $O_j$, which are computed twice in the Mem_{O_i} and Com_{O_i}
\begin{equation}
    \textstyle \sum_{O_i \in S_*}(Mem_{O_i}+Com_{O_i}) - \textstyle \sum_{i,j} \textstyle \sum_{x,y} H_{i,j}(x,y) < N_{cim}.
\end{equation}
As shown in \fig \ref{fig:cons}(d), the allocated arrays are under the resource limit.

\noindent\textbf{Objective function.}
Our objective is to optimize the allocation of arrays in memory or compute mode to minimize the execution latency of the network segment under a pipeline scheduling strategy. 
% Our objective is to optimize the allocation of arrays in memory or compute mode to minimize the execution latency of the network segment under a pipeline scheduling strategy. 
% Since operators within the same segment are simultaneously mapped to the chip, they can be pipelined and executed in parallel. 
Through the pipeline, operators can execute in parallel.
Thus, the latency of the segment can be approximated as the maximum execution time of any single operator within that segment. 
We denote the latency of $O_i$ as $L_{O_i}$ and get the following objective function:
\begin{equation}
    \label{eq:objct}
    \min \max L_{O_i}, \forall O_i \in S_* .
\end{equation}

The system performance cost model for $L_{O_i}$ estimates off-chip data access latency and on-chip computation latency based on the allocated compute and memory arrays.
To quickly estimate the latency $L_{O_i}$, we develop a latency model as shown in Eq.~\ref{eq:latency}. 
It is a function of the number of allocated compute and memory arrays ($Com_{O_i}$ and $Mem_{O_i}$).
When we allocate $Com_{O_i}$ compute arrays for $O_i$, they support $\scriptsize C=Com_{O_i} \cdot OP_{cim}$ computation amount per cycle, where $OP_{cim}$ denotes the computation amount per cycle a CIM array can provide. 
When we allocate $Mem_{O_i}$ memory arrays, they can access $Mem_{O_i} \cdot D_{cim}$ data per cycle, where $D_{cim}$ represents the data per cycle a CIM array can provide. Combined with the data from storage and the original buffer ($D_{main}$), the accessible data per cycle is $Mem_{O_i} \cdot D_{cim} + D_{main}$. Given the arithmetic intensity of $O_i$ ($AI_{O_i}$), this supports $\scriptsize M=(Mem_{O_i} \cdot D_{cim} + D_{main}) \cdot AI_{O_i}$ computation amount.
The smaller value between the $C$ and $M$ determines the effective computation amount per cycle.
$L_{O_i}$ can be estimated by the total computation amount ($OP_{O_i}$) and the $min(C,M)$.
Therefore, when $AI_{O_i}$ is large, it is preferable to allocate more compute arrays. Conversely, when $AI_{O_i}$ is small, it is better to allocate more memory arrays.

% , combined with the HW/SW search space by considering the memory/computation units in the cost model.
% 为了结合软硬件优化空间，我们建立了含有存算单元比例的cost model
% 这里需要一个图片，类似流水线图，说明性能的上限
% [6.14 仍然没想好]Q，这里这个cost model，感觉是不是得详细的解释一下,，公式10需要细化，说明公式的由来，在细化中，还没想好怎么写，写多了又感觉很啰嗦，说不清，主要就是一个CIM array可以支持多少计算这个事情.或者加一小段例子
% To quickly estimate the latency, we analyze the impact of the number of on-chip compute units and memory units on the latency of individual operators. This allows us to express the latency as a function of compute and memory units, facilitating the problem-solving process.
% For a single operator, the number of available compute arrays determines the maximum number of computations that can be performed per cycle. 
% Similarly, memory resources determine the maximum data access per cycle. The arithmetic intensity of each operator ($AI_{O_i}$), combined with the data access speed ($D_{main}$), influences the number of computations that can be performed per cycle from a memory access perspective.
% As shown in \fig \ref{fig:mip} (a)-(c), during actual execution, the limiting factor—whether it's compute capacity or memory access—determines the effective computation amount per cycle.
% Thus, the latency  $L_{O_i}$ of an operator $O_i$ can be approximated by the total amount of computation divided by the effective computation amount per cycle. Formally, we express this as follows:
\begin{equation}
    \scriptsize L_{O_i} \propto \frac{OP_{O_i}}{\min (Com_{O_i} \cdot OP_{cim}, (Mem_{O_i}\cdot D_{cim} + D_{main})\cdot AI_{O_i} )}. 
\label{eq:latency}
\end{equation}
Taking matrix-matrix multiplication(MMM) as an example, as shown in \fig \ref{fig:mip}, for an operator $O_i$, the computation amount a CIM array can provide is: $\scriptsize \frac{N\times K}{\lceil\frac{N}{array\_size_h}\rceil\times\lceil\frac{K}{array\_size_w}\rceil}$.
% In a bit-serial scenario, it requires $bit$ cycles to complete the corresponding bit-width computation. 
% From a data loading perspective
Meanwhile, $N$ data can support $N \times K$ MAC computations, which means $AI_{O_i} = K$. 
The total number of multiply - accumulate operations needed is $OP_{O_i} = M \times N \times K$. 
%  $D_{cim}$ and $D_{main}$ is decided by chip design

% Taking convolution computation as an example, for an operator $O_i$, the effective computation per cycle is given by: $\scriptsize \frac{CIM_{O_i}}{\frac{k\times k \times C_{in}}{array\_size_h}\times\frac{ C_{out}}{array\_size_w}}\times k\times k\times C_{in}\times C_{out}$.
% In a bit-serial scenario, it requires $bit$ cycles to complete the corresponding bit-width computation. 
% From a data loading perspective, $k\times k\times cin$ data can support $k \times k \times C_{in} \times C_{out}$ computations. The total number of multiply-accumulate operations needed is: $OP_{O_i} =h_{out} \times w_{out} \times k \times k \times C_{in} \times C_{out}$. 

Using the optimization solver Gurobi \cite{gurobi}, we solve the most efficient memory/compute mode allocation for each network segment, thereby optimizing the overall performance of the dual-mode CIM processor. 

\begin{algorithm}[t!]
  \caption{Summary of DACO.}
  \label{alg:sum}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} Neural network $N$ in ONNX format and CIM hardware abstraction.
    \STATE {\bfseries Output:} The network segment $S$ and the corresponding dual-mode CIM array allocation $A^*$ for each segment.
    \STATE \emph{\textcolor{gray}{// Preprocess graph.}}
    \STATE $(O_1, \ldots, O_m) \leftarrow \text{Flatten}(G)$
    \STATE \emph{\textcolor{gray}{// Run the network segment dynamic programming}}
    % L[0][$k$]$_{k=1,...,N_{cim}}$ $\leftarrow$ 0
     \STATE L[0][$\cdot$] $\leftarrow$ 0
    \FOR{$1 \le i \le j \le m$}        
        \STATE \emph{\textcolor{gray}{//Impossible cases are skipped to reduce search space}}
        \IF{min \# of CIM array $S_{i,j}$ required $< N_{cim}$}
            \STATE  \emph{\textcolor{gray}{// Run the resources allocation.}}
            \STATE $T_{i,j}^{intra}(A)\leftarrow MIP(S_{i,j})$ according to Eq \ref{eq:objct}. 
        \ELSE
            \STATE $T_{i,j}^{intra}(\cdot)\leftarrow \infty$
        \ENDIF
        \STATE Compute $T_{i-1,i}^{inter}(A',A)$ according to Eq \ref{eq:inter}.
        \STATE L[j][A] $\leftarrow$ min(L[i][A$'$] + $T_{i,j}^{intar}(A)$ + $T_{i-1,i}^{inter}(A',A)$, L[j][A])  
        \STATE S$_{record}$.update(i,j)
    \ENDFOR
    \STATE L$_{min}$ = $\min$ L[m][$A^*$]
    \RETURN backtrack(L, S$_{record}$, L$_{min}$) for network segment plan S and corresponding $A^*$.
\end{algorithmic}
\end{algorithm}
Algorithm~\ref{alg:sum} summarizes the dual-mode-aware compilation optimization (DAMO) workflow, detailing the interaction between network segment and resource allocation. \update{Once the network segment and CIM array allocation plans are established, we perform post-allocation optimization, such as weight duplication, commonly used in CIM compilation optimization \cite{qu2024cim}, to further enhance kernel mapping.}
%  while adhering to on-chip resource constraints
% 算法模板

\subsection{Dual-mode Support in Meta-Operator (DMO)}
% \subsubsection{Computation-Memory switch aware codegen}
% adopt the same strategy as the general CIM compiler, CIM-MLC, by 
Once the frontend optimization outputs the network segmentation and the allocation strategy of memory/compute resources, 
\name uses meta-operator flow to express the compilation result.
% the compiler starts code generation to express the compilation result to users. 
% In the backend code generation phase, we should produce compiled code that includes information on the memory/compute mode switch plan. 
To accommodate the diverse methods for performing these switches, we use a meta-operator flow rather than machine code for better generality. Additionally, it can be integrated into other backends \cite{qu2024cim}. 
% our backend with the general CIM compiler CIM-MLC. 
% Dual-mode switches require hardware-specific support. 
The introduced meta-operators and their corresponding syntax are shown in \fig \ref{fig:syntax}. 
We add the $CM.switch$ operator, which supports two types, $TOM$ and $TOC$, representing mode switching at the granularity of CIM arrays. 
When the meta-operator type is $TOM$/$TOC$, it means switching the $array_{addr}$ to memory/compute mode.
When a CIM array is converted to memory mode, it is marked as a valid memory unit, effectively serving as an on-chip buffer. Alongside the new dual-mode switch meta-operators, we use standard operators to represent normal computation and memory access. 
Additionally, we use $parallel \{\}$ to represent the network segment as operators in a segment will execute in parallel.
% we utilize all meta-operators from CIM-MLC, $<CIM>,<DCOM><DMOV>$, to represent normal computation and memory access. 
% We established the code generation part to include custom meta-operators and incorporated logic related to dual-mode switches to generate these meta-operator flows based on frontend optimization results. 
% Based on meta-operator flow, users can convert it into machine code suitable for their specific CIM chips.
% An example of the generated meta-operator flow is shown in \fig , where instructions for dual-mode switches of corresponding units are inserted as needed.
% \subsubsection{Online dual mode switch}
\update{Users can convert the meta-operator flow into ISA or machine code suitable for their specific CIM chips.}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/syntax.pdf}
    \caption{Syntax of dual-mode enhanced code generation.}
    \label{fig:syntax}
\end{figure}