\section{Evaluation}
\subsection{Setup}
% 实验把PUMA的baseline加上？
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{image/data2.png}
%     \caption{End-to-end performance}
%     \label{fig:data2}
% \end{figure}
% TODO1028: add the result of VGG
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{image/datawithpumalong_short.pdf}
    \caption{\update{Speedup compared to the baselines. The red arrows with numbers highlight the speedup of CMSwith compared to the main baseline, CIM-MLC\cite{qu2024cim}. The red line highlights the Geomean bar of performance improvement over CIM-MLC.} }
    \label{fig:data}
\end{figure*}


\noindent\textbf{Simulator.}
For the functional simulation, we adopt the CIM-MLC functional simulator \cite{qu2024cim}. We use the functional simulator to execute the generated meta-operator flows within the CIM architecture.
% perform the generated meta-operator flows as the execution trace in the CIM. 
By comparing the execution result with the PyTorch framework \cite{paszke2019pytorch}, we verify the effectiveness of our compilation results.
To evaluate execution latency, we built our simulator upon existing open-source simulators \cite{chen2018neurosim,xia2017mnsim}, incorporating necessary modifications to simulate the dual-mode switch. We also modified the hardware configuration to align with the specifications of Dynaplasia \cite{kim202316}, our target hardware.
% We adopt most of the simulation pass. We add the necessary pass to simulate the dual-mode switch and modify the hardware configuration to match the published information of Dynaplasia \cite{kim202316}, the target hardware we use.


\noindent\textbf{CIM architecture configuration.}
Our target CIM architecture configuration is based on Dynaplasia \cite{kim202316}, a real CIM chip that supports the compute-memory mode switch. 
The main parameters of the architecture are listed in the \tab \ref{tab:config}. The CIM mode switch of Dynaplasia is achieved by altering the global wordline input.
% which causes negligible overhead. 
Consequently, the actual execution of the $CM.switch$ operator at the runtime is setting the corresponding signal to the global wordline.  


\noindent\textbf{Network benchmark.}
To verify both the efficiency and generality of CMSwitch, we use various types of neural networks as our network benchmark. For convolution-based architecture, we use the classic MobileNet \cite{sandler2018mobilenetv2}, ResNet \cite{he2016deep}, and VGG \cite{simonyan2014very} series, tested on the ImageNet dataset\cite{deng2009imagenet}. For the Transformer-based architecture, we use the encode-only model BERT \cite{devlin2018bert} and decode-only models OPT \cite{zhang2022opt} and LLaMA 2 \cite{touvron2023llama}. All models are quantized with 8-bit precision for weights and activations.

% baseline 是因为这些工作采用的方法都是不太一样的优化方向
\noindent\textbf{Baseline.}
To evaluate the benefits of dual-mode switching for application execution, we compare the compilation results of 
% our proposed dual-mode-aware compilation method 
\name with existing CIM compilation works. We adopt three compilers with different optimization strategies as baselines to demonstrate the effectiveness: PUMA \cite{ankit2019puma} (focusing on operator duplication and pipeline scheduling), OCC \cite{siemieniuk2021occ} (optimizing operator mapping via tiling and loop unrolling), and CIM-MLC \cite{qu2024cim} (employing multi-grained pipelining and operator duplication for diverse architecture). Our primary comparison metric is the execution latency of the compiled applications.

\begin{table}[]
    \centering
    \caption{CIM Architecture Configuration.}
    \label{tab:config}
    \resizebox{0.85\linewidth}{!}
{
    \begin{tabular}{c|c}
    \hline
    Parameter & Configuration\\\hline
   $ \#\_switch\_array$     &  96\\
   $ array\_size$     & 320 $\times$ 320\\
   $ buffer\_size$ & $ 10KB\times$8\\
    $internal\_bw$ & 32b/cycle\\
    Method$_{c \to m}$ / Method$_{c \to m}$ & change the input of global IA and IA'\\
    L$_{c \to m}$ / L$_{c \to m}$ & 1 cycle \\\hline
     \end{tabular}}    
\end{table}

% 字好像有些太小了
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{image/data.pdf}
%     \caption{End-to-end performance}
%     \label{fig:data}
% \end{figure}
% \todo{1. Add the result of VGG to the fig 14; 2. change all the result, compare to CIM-MLC}

\subsection{End-to-End Performance}
% We first compared the end-to-end performance of \name~ with the baselines. For transformer-based models, we set the sequence length to 64. The results are shown in \fig \ref{fig:data}.
% % For the encode-only BERT model, \name achieved a performance improvement of 1.19$\times$\textasciitilde 1.28$\times$, with an average of approximately 1.24$\times$, compared to the baseline. For the decode-only OPT, \name outperformed the baseline by 1.64$\times$\textasciitilde2.11$\times$. For the llama model, \name gained an improvement of 1.47$\times$\textasciitilde1.59$\times$. In the case of convolutional neural networks, \name delivered a performance boost of 2.05$\times$\textasciitilde2.19$\times$ for Mobilenet and 1.53$\times$\textasciitilde2.09$\times$ for ResNet.
% % If we have the puma as a baseline
% \update{Compareed to the SOTA compilation work CIM-MLC\cite{qu2024cim}, for the encode-only BERT-large model, \name achieved a performance improvement of 1.02$\times$\textasciitilde1.25$\times$, with an average of approximately 1.17$\times$. 
% For the decode-only LLaMA2-7B model, \name gained an improvement of 1.13$\times$\textasciitilde1.30$\times$, with an average improvement of approximately 1.24$\times$.  
% For the OPT-13B, \name outperformed the baseline by 1.20$\times$\textasciitilde2.03$\times$, with an average improvement of approximately 1.73$\times$. 
% In the case of CNNs, \name delivered a performance boost of 1.06$\times$\textasciitilde 1.23$\times$ for MobileNet,  1.07$\times$\textasciitilde 1.23$\times$ for ResNet18, and 1.32$\times$\textasciitilde 1.48$\times$ for VGG-16, respectively. In summary, compared to CIM-MLC, the average speedup is 1.31$\times$, with a maximum of 2.03$\times$.}

% \update{
% The fundamental improvement of CMSwitch comes from its ability to broaden the compilation optimization space for CIM accelerators. By recognizing the dual-mode switchable capabilities of CIM arrays, CMSwitch can modify the balance between computation and memory capability according to the needs of the applications, offering a broader space of compiling optimization options than CIM-MLC. CMSwitch is especially advantageous for DNN applications that demand substantial memory and exhibit varying levels of computational intensity. In contrast, previous compilation works treated all arrays as computational units. For example, MLC's three-tier compilation optimization process yields good results but only considers fixed resources, preventing it from fully unleashing the potential of CIM accelerators.
% Therefore, CMSwitch achieves better performance for popular decoder-only transformer models compared to MLC since they are notoriously memory-bound applications.
% }


\update{We first evaluated the end-to-end performance of CMSwitch, with the results presented in Figure \ref{fig:data}. For transformer-based models, we set the sequence length to 64. Compared to the main baseline, state-of-the-art compilation work CIM-MLC \cite{qu2024cim}, \name demonstrated performance improvements ranging from 1.02$\times$ to 1.25$\times$ (average 1.17$\times$) for the encode-only BERT-large model. For the decode-only LLaMA2-7B model, \name yielded a performance gain of 1.13$\times$ to 1.30$\times$ (average 1.24$\times$). For OPT-13B, \name outperformed the baseline by 1.20$\times$ to 2.03$\times$ (average 1.73$\times$). For CNN models, \name delivered a performance boost of 1.06$\times$ to 1.23$\times$ for MobileNet, 1.07× to 1.23$\times$ for ResNet18, and 1.32$\times$ to 1.48$\times$ for VGG-16. Overall, \name achieved an average speedup of 1.31$\times$, with a maximum improvement of 2.03$\times$ compared to CIM-MLC.}

\update{The fundamental enhancement brought by CMSwitch lies in its ability to expand the compilation optimization space for CIM accelerators. By leveraging the dual-mode switchable capabilities of CIM, CMSwitch dynamically adjusts the balance between computation and memory resources based on the application's requirements, offering more flexible optimization options than all the baselines. This adaptability is particularly advantageous for DNN applications with substantial memory demands and varying arithmetic intensity.}

% Given that 
% we used actual CIM hardware parameters for the evaluation configuration, 
\update{Specifically, for large transformer-based benchmarks, where the hardware may be unable to fully map the weights, \name optimizes network segmentation by accounting for the overhead associated with switching between compute and memory modes. Meanwhile, combined with a tailored allocation of dual-mode CIM resources, \name allows some arrays to operate in memory mode.}
% The hardware could not fully map the weights for large models, like transformer-based benchmarks. 
\update{In contrast, prior compilation frameworks have not adequately considered this issue, leading to under utilization of hardware resources.}
% often employing greedy methods for network partitioning. This can  
% In contrast, \name fully considers the impact of limited hardware resources on network execution.
% in real-world scenarios. 
% By allocating memory/compute resources based on application requirements, 
% \update{Therefore, compared to CIM-MLC, \name achieves an average performance improvement of 1.38 $\times$, with a maximum of 2.03$\times$ for the transformer-based models.} 
\update{Although acceleration benefits for certain CNNs may be less pronounced, the introduction of the resource-switching capabilities enables better handling of the diverse computational and memory demands across different CNN layers.}
% For convolutional neural networks, the primary source of performance enhancement also stems from the flexible adjustment of on-chip resources. Since the compute and memory resource demands of different layers in convolutional neural networks vary significantly, 
CMSwitch’s dual-mode-aware compilation optimization ensures more efficient handling of resource demands.
% , resulting in a more balanced and efficient resource allocation.
% \name’s dual-mode-aware compilation optimization can allocate appropriate resources for each layer, thereby leveraging acceleration opportunities within the constraints of total CIM resources.

The results demonstrate CMSwitch’s versatility across a variety of neural networks with differing hardware resource requirements. It effectively adjusts CIM array modes based on workload characteristics, consistently showing advantages across various batch sizes. This adaptability reflects CMSwitch's capability to optimally allocate compute and memory resources, ensuring performance acceleration at different scales and workloads.

% The performance across various neural networks with differing hardware resource demands demonstrates CMSwitch’s versatility. It indicates that \name enables reasonable adjustments to the CIM array modes based on the workload needs.
% through resource allocation by MIP.
% \name provides dual-mode-aware compilation support for networks with varying hardware requirements, enabling reasonable adjustments to the CIM array modes based on the network architecture needs.` 
% For CIM processors, larger batch sizes require various hardware resource demands for weight replication and synchronous computation across multiple inputs. Existing CIM compilation work predominantly focuses on single-batch optimization, with little attention given to multi-batch scenarios. 
% \name consistently showed advantages across different batch sizes, which reflects its ability to find suitable compute/memory array allocation based on application demands, ensuring acceleration effects at varying scales.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/allocation.pdf}
    \caption{Resources allocation for applications.}
    \label{fig:allocation}
\end{figure}
% will show more results
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{image/scalability_re.pdf}
   \caption{\update{Effectiveness for various workload scales. The horizontal header indicates the model and the vertical header indicates the batch size. (top three rows) Visualization of the memory/compute allocation sensitivity across different sequence lengths for the transformer-based model. (last row) The number in red indicates the speedup compared to the CIM-MLC. }}
    \label{fig:scale}
\end{figure*}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/fix_scale_re.pdf}
    \caption{\update{Effectiveness for generative models. (a) Fixed input sequence length; (b) Fixed output sequence length.}}
    \label{fig:scale_vis}
\end{figure}


% 更多的描述for transformer based 架构
\subsection{Dual-mode Switch Result Demonstration}
This section presents the allocation of compute/memory arrays after compilation.
As shown in \fig \ref{fig:allocation}, the dashed boxes show the network segment, and the pie charts show the proportion of compute/memory arrays allocated.
% for each segment. 
% Using VGG16 as an example, as shown in \fig \ref{fig:allocation}, we demonstrate the network segmentation and the allocation of memory and compute units for each segment. 

For the convolution-based VGG16 (\fig \ref{fig:allocation}(a)), the segmentation results divided the topologically sorted convolution operators 1-4 and 5-6 into two segments, while each of the remaining convolution operators was placed in its own segment. This segmentation aligns with intuitive expectations.
Due to the increasing number of feature map channels in VGG, 
% and when mapping to CIM compute units, 
% the weights are unrolled into a $[kernel\_size \times kernel\_size\times input\_channel,output\_channel] $ matrix, requiring \( k \times k \times \text{in}/array\_size\_height, \text{out}/array\_size\_width \) compute units. 
% Since the kernel size of VGG layers is consistent, 
the earlier layers require fewer compute arrays for weight mapping compared to the later layers, making it feasible to group multiple operators into one segment for pipeline parallelism.
% During segmentation, we fully considered both intra-segment and inter-segment overheads. Compared to the greedy segmentation method used by CIM-MLC, our segmentation strategy provides approximately 1.19 $\times$ times speedup, better balancing the benefits of pipeline parallelism within segments and the overhead of switching between segments.
Meanwhile, our MIP-based compute/memory allocation strategy provides customized hardware resource support for different network segments. 
As shown in the \fig \ref{fig:allocation}(a), the compiled results allocate more compute arrays for the earlier convolutional operators,
% , while for the later layers, more CIM units are set to memory mode.
% This allocation strategy aligns with theoretical expectations. 
% More compute resources are needed for the earlier network layers since multiple operators are computed in parallel. 
facilitating parallel computation of multiple operators.
% For a single multiply and accumulate computation, an operator needs to load \( k \times k \times \text{in} \) data. 
With fewer channels, the amount of data required to be loaded is relatively small, and the original on-chip buffer can support the data transfer needs, thus more arrays are used in compute mode.
For the later network layers, especially the final convolutional layers, with more input channels, 
% the weights of a single operator cannot be fully mapped on-chip under the actual hardware parameters used. Therefore, after completing part of the computation, weight swapping is required. Additionally, 
more data is needed in a single MAC computation. Therefore, our model tends to set some CIM arrays to memory mode, providing greater bandwidth for data retrieval.
% and weight swapping.

For the transformer-based model OPT-6.7B, the allocation of compute/memory arrays for one layer is shown in the \fig \ref{fig:allocation} (b). In the standard matrix multiplication, like QKV generation and feed-forward net (FFN), \name allocates 33\%\textasciitilde 67\% CIM arrays in the memory mode based on the operators' demands. 
For example, the last FFN operator needs more input data for each computation and is therefore allocated slightly more memory arrays. In contrast, for attention calculations, which consume data immediately after computation, more compute arrays are allocated. Moreover, after calculating the $K$ value, some $K$ data is stored on-chip as it owns some CIM array in memory mode. Once the respective CIM arrays switch from memory to compute mode, $QK^T$ computations can proceed directly in place, aligning to minimize data transfer. This strategic allocation demonstrates CMSwitch's ability to optimize resource usage and improve performance effectively.


\subsection{Effectiveness for Various Workload Scale}
In this section, we analyze the impact of workload scale on the compilation results of CMSwitch.
\update{As depicted in \fig \ref{fig:scale}, we evaluate BERT-large, Llama 2-7B, OPT-6.7B, and OPT-13B with batch sizes ranging from 4 to 16 and input/output sequence lengths from 32 to 2048.
Meanwhile, we visualize the memory/compute allocation sensitivity across different sequence lengths in \fig \ref{fig:scale} last row. 
The average proportion of arrays operating in memory mode across all segments serves as an indicator of the overall resource allocation strategy, though variations may occur within individual segments.}

% As shown in \fig \ref{fig:scale}, compared to existing CIM compilers, \name achieves performance improvements of 1.53$\times$, 1.37$\times$, and 1.38$\times$ for Bert-large when the input and output sequence lengths range from 32 to 128. 
% For Llama-7B, \name achieves performance gains of 2.02$\times$, 2.0$\times$, 1.64$\times$.
% Similarly, for OPT-13B, \name achieves performance gains of 2.25$\times$, 2.68$\times$, 2.07$\times$.
% % For llama-13B, \name delivers performance improvements of 1.81$\times$, 1.78$\times$, and 1.42$\times$, respectively.
% % If we use the puma as one of the baseline
% % As shown in \fig \ref{fig:scale}, compared to existing CIM compilers, \name achieves performance improvements of 1.8$\times$, 1.74$\times$, and 1.32$\times$ for OPT-6.7B when the input and output sequence lengths range from 32 to 128. 
% % For OPT-6.7B, \name delivers performance improvements of 2.31$\times$, 2.28$\times$, and 1.78$\times$, respectively. 
% \name outperforms existing CIM compilers consistently across all tested sequence lengths. This superiority is attributed to its efficient handling of the dual-mode CIM arrays, which optimizes the allocation of compute and memory resources dynamically.
% % based on the workload requirements.
% Meanwhile, the consistent performance gains across different sequence lengths demonstrate its robustness and adaptability to various workload demands.
% \todo{figure: 1. add the batched inference with various sequence length evaluations; 2. add the result of allocation}

\update{As shown in \fig \ref{fig:scale}, across various batch sizes, CMSwitch achieves an average performance improvement of 1.19$\times$ to 1.03 $\times$ over CIM-MLC for BERT when sequence lengths range from 32 to 256. For sequence lengths exceeding 512, CMSwitch demonstrates equivalent performance to CIM-MLC, a trend that remains consistent across different batch sizes. Furthermore, the last row of \fig \ref{fig:scale}  reveals that, as the sequence length increases, the average ratio of arrays in memory mode gradually decreases to zero. This trend aligns with the characteristics of BERT, where the arithmetic intensity increases with longer sequence lengths, necessitating a shift toward compute resources for the corresponding computations. Therefore, as the sequence length extends, CMSwitch’s performance converges with that of CIM-MLC, as we adopt its kernel optimizations.}

\update{For generative models, across various batch sizes, CMSwitch yields average performance gains of 1.25$\times$ to 1.08$\times$ for LLaMA2-7B, 1.50$\times$ to 1.23$\times$ for OPT-6.7B, and 1.76$\times$ to 1.32$\times$ for OPT-13B. However, as sequence lengths extend to 512, the speedup diminishes, with fewer arrays operating in memory mode, after which performance stabilizes. For instance, when evaluating OPT-6.7B with a batch size of 8, and varying input and output sequence lengths from 32 to 2048, the speedups compared to CIM-MLC are 1.41$\times$, 1.58$\times$, 1.27$\times$, 1.23$\times$, 1.22$\times$, and 1.22$\times$, while the corresponding average memory mode array ratios decrease from 20.8\% to 12.0\%. As the input and output sequence lengths increase, the arithmetic intensity during the input processing grows, prompting more arrays to switch to compute mode. This shift reduces the dual-mode switching benefits, resulting in lower overall speedups compared to CIM-MLC, which statically configures all arrays in compute mode.}
\update{To further assess the performance of \name for generative models during different inference stages, we use LLaMA2-7B and OPT-13B as benchmarks. We fix the input length at 128 and vary the output length from 32 to 2048, and vice versa, to observe the speedup. As shown in Figure \ref{fig:scale_vis}, when the input length is fixed, \name achieves a performance improvement of 1.10$\times$ to 1.24$\times$ over CIM-MLC for LLaMA-2 7B and 1.43$\times$ to 1.62$\times$ for OPT-13B, with a nearly consistent speedup as the output sequence length increases.  This consistency arises because the decode phase that generates the output sequence processes tokens incrementally, leaving the arithmetic intensity unaffected by changes in output length. Additionally, the varying memory space required by caching KV with longer output sequences benefits from dynamic mode switching. Conversely, when the output length is fixed, CMSwitch's performance diminishes as the input sequence length increases for both models, driven by the higher arithmetic intensity, which demands additional compute resources.}

\update{Evaluations with varying sequence lengths demonstrate that CMSwitch exhibits adaptability to diverse workload demands by allocating memory and compute resources efficiently, yielding customized compilation results. Moreover, CMSwitch better supports applications with dynamically changing on-chip memory requirements, as existing compilation works typically treat all arrays in compute mode, overlooking their potential use as scratchpads memory.}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.3\linewidth]{example-image-a}
%     \caption{Scalability for various workload scale}
%     \label{fig:scalability}
% \end{figure}

% \subsection{Scalability}
%  

\subsection{Cost and Scalability Analysis}
\noindent\textbf{Dual-mode switch overhead.}
In our evaluation, the dual-mode switch process introduces negligible overhead, contributing around 3\% - 5\% to the total execution time when providing considerable performance improvement. 
% To quantify this, we 
The dual-mode switch process takes time to configure the input drivers for the mode switch. 
% against the overall execution time across various models. 
This minimal overhead is attributable to the efficient design of the CIM chip, which ensures that the switch between compute and memory modes is both swift and seamless. 
Furthermore, the performance gains realized through our switching overhead-aware network segment strategy, which optimizes network segment scheduling, outweigh the minor switching costs. This evidence confirms that the dual-mode switch mechanism
% does not compromise application performance on the CIM chip and 
is a valuable consideration in the CIM compilation optimization process. 

\noindent\textbf{Scalability.}
After adjusting the hardware settings to the PRIME \cite{chi2016prime} architecture, we evaluated the performance of transformer-based networks. Compared to Dynaplasia, PRIME offers larger and more CIM arrays that can contain large network segments. 
However, PRIME has higher write overhead as it uses the ReRAM as the memory device. 
According to our assessment, compared to the CIM-MLC, we achieved speedups of 1.48$\times$ for Bert, 1.09$\times$ for Llama-7B, and 1.10$\times$ for OPT-13B. 
These results indicate that \name can provide performance improvements across different target hardware configurations. This adaptability demonstrates the robustness and efficiency of \name in leveraging the capabilities of various CIM accelerator architectures.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{image/compilation_time.pdf}
    \caption{\update{Compilation overhead of different workloads. The number over the bar is the absolute number of compilation time (in s).}}
    \label{fig:overhead}
\end{figure}
\subsection{\update{Compilation overhead}}
\update{We compare the compilation time of CMSwitch with CIM-MLC to demonstrate CMSwitch's overhead. To reduce variability, each benchmark used in the end-to-end evaluation was compiled 20 times. As illustrated in \fig \ref{fig:overhead}, CMSwitch's compilation time is approximately 2.8$\times$ to 6.3$\times$ longer than that of CIM-MLC, indicating a higher compilation overhead. This increase stems from an exponentially expanded compilation space, which introduces more opportunities for optimization. Given that the compilation process is a one-time operation, the extended duration is justified by the potential for substantial performance gains through a more thorough exploration of the optimization space.
Regarding the sensitivity to compilation overhead, we observe that CNNs like ResNet18 and VGG16 require roughly 2.5$\times$ more compilation time compared to transformer-based models. This is attributed to the larger compilation space for CNNs, which feature approximately three times as many convolution types with diverse kernel sizes, whereas transformer-based models allow the compilation results of a single block to be reused across all layers. Consequently, the compilation space for transformers is relatively compact. Therefore, for the evaluated benchmarks, the compilation overhead scales almost linearly with the workload size, even as the optimization space expands. This linear scaling is achieved by leveraging techniques such as impossible-case pruning and non-enumerative solving methods by DP and MIP, which accelerate the exploration of the optimization space and effectively reduce potential overhead.
}

% \textbf{Comparison with ideal performance}