\section{Background}
\subsection{\update{Computing-In-Memory Accelerator Architecture}}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/CIM_archi.pdf}
    \caption{Hierarchical CIM architecture (a) CIM core with CIM arrays and the corresponding peripheral units; (b) CIM array and the corresponding peripheral units; (c) CIM array.}
    \label{fig:CIM_archi}
\end{figure}
\update{As depicted in \fig \ref{fig:CIM_archi} (a)-(c), independent CIM-based DNN accelerators are typically structured as hierarchical architecture comprising multiple CIM cores. Each core integrates a CIM array along with its peripheral buffer and circuitry. This design enables in-situ computation within the memory, thereby mitigating the data transfer bottlenecks commonly seen in conventional architectures that separate computation and memory. Prior researches \cite{le202364,wan2022compute,chen2020survey,ankit2019puma,eckert2018neural,biswas2018conv,seshadri2017ambit,song2017pipelayer,li2017drisa,shafiee2016isaac,chi2016prime,ahn2015pim,li2018reram,joardar2019regent,qu2020raqu} have proposed various CIM accelerators, providing robust support for high-performance computing and naturally aligning with large-scale parallel computing applications such as DNN inference. }
% [620Z，问题]这里加图后后面MIP那块相应的要做修改，先放一个卷积的图在这，也画了别的，可以随时调整。Q：何种形式表示计算过程，现在这个可能比较抽象

 
%which is then executed within the memory.
% which cannot be directly mapped, the process is unrolled into an equivalent matrix-matrix multiplication, which is then executed within the memory.
% As shown in the \fig \ref{fig:background1} (d) right, this structure allows the multiply-accumulate (MAC) operations to be completed entirely within the array in parallel.
% we can complete the MAC operations within the array in parallel. 
% For other operations that can not directly map like convolution, we will unroll the operation to the equivalent matrix-matrix multiplication and then execute these using the memory.
 
% For example, ISSAC, which proposed a cross-array-based architecture for accelerating neural network inference, and XXX, which introduced XXX. 
%Such architectures 
% can perform parallel matrix-vector multiplication within a single cycle, 
% providing robust support for high-performance computing and naturally aligning with large-scale parallel computing applications like deep neural network inference. 
% For instance, ISAAC\cite{shafiee2016isaac} accelerates convolutional neural networks, while ReTransformer\cite{yang2020retransformer} supports transformer-based network inference.
\subsubsection{Dual Modes CIM Array}

% \begin{figure}[th]
%     \centering
%     \includegraphics[width=0.6\linewidth]{example-image-a}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}
% \subsection{Dual Modes Support of CIM}
% In this section, we will introduce the design of t
The dual-mode CIM array can operate as both a memory and compute unit when applying a slight enhancement on the input or output drivers~\cite{kim202316,yan20221,yue202115,chih202116,fujiwara20225,wang2022dimc,spetalnick202240nm,ankit2020panther}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/CIM_switch.pdf}
    \caption{Dual-mode CIM Array.}
    \label{fig:cim_switch}
\end{figure}
% One of the important characteristics of CIM is the dual mode support, i.e., each CIM array has both memory and compute modes. 
% 给出例子说明转换就行。
As illustrated in \fig \ref{fig:cim_switch}, switching between memory and compute modes of CIM arrays can be achieved by altering the array's driver inputs, as demonstrated by DynaPlasia \cite{kim202316}.
% with minimal overhead\cite{kim202316,yan20221,yue202115,chih202116,fujiwara20225,wang2022dimc,spetalnick202240nm,ankit2020panther}, \eg~altering the array's driver inputs.
% or utilizing different array drivers for a different mode.
% there exist two mainstream ways to switch the CIM array between the compute and memory mode. One is altering the array's driver inputs, the other is utilizing different CIM array drivers \cite{kim202316,yan20221,yue202115,chih202116,fujiwara20225,wang2022dimc,spetalnick202240nm,ankit2020panther}. 
% For instance, in the work DynaPlasia \cite{kim202316},
This mode-switching functionality is controlled by modifying the input signals on the global lines.
When the Global Input Activation line (GIA) and its complement (GIAb) are set to a high state (1), the array functions in memory mode, allowing standard memory read-write operations. 
% works in compute mode to perform bit-series multiplication-addition operations, 
% functions as an in-memory computation unit for bit-series multiplication-addition operations, 
Conversely, when the GIA and GIAb are configured as input activation (IA) and inverse of input activation IA (/IA), respectively, the array operates in compute mode, performing bit-series multiplication-addition operations.  
% Work \cite{yan20221} employs two different memory drivers to switch between dual modes. When using the CIM driver, the array functions as a computation unit, while when using the memory read/write circuits, it operates as a memory unit. 
% Whether altering the array's driver inputs or changing the driver, switching between memory and compute modes of CIM arrays can be achieved with minimal overhead. 

% Q:可不可以直接用fig 3的右边来展示计算的过程，然后直接把figure 4删掉，后面有一个相似的图，而且后面的删了可能会有点解释不清
\subsubsection{CIM Compute Paradigm}
% CIM processors always have a hierarchical architecture, containing numbers of CIM cores, each core consists of a CIM array and its peripheral buffer and circuit.
% It enables performing in-situ computation within the memory, thus avoiding the bottleneck of data transfer in computation-memory separate architectures.
When CIM arrays perform computations, they enable the multiply-accumulate (MAC) operations to be executed entirely within the array in parallel, as illustrated in the \fig \ref{fig:cim_switch} (right). This architecture inherently supports matrix-vector multiplication (MVM) and matrix-matrix multiplication (MMM).
In the case of MVM, the matrix is mapped onto the CIM array, while the vector serves as the array input. 
The multiplication is performed within each cell, with accumulation occurring along the bitlines or at the output side, producing MVM results directly from the array.
% The products of MVM are generated from the array.
% After performing the computation in the memory, it directly outputs the MVM results. 
\update{Many classic DNN operators, such as fully connected layers and convolutions, can be readily transformed into MMM or MVM operations.
For instance, while convolutional kernels cannot be directly mapped onto the array, the convolution operations can be unrolled into an equivalent matrix-matrix multiplication (MMM).} This equivalent MMM is subsequently mapped and executed on the CIM array, following the standard MMM procedure.


% We can flexibly allocate compute and memory resources based on the requirements of different inference tasks, enabling dynamic resource management and optimization through the CIM array compute/memory mode switch.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{image/compute_paradigm.pdf}
%     \caption{An example of mapping and executing convolution on CIM.}
%     % \caption{CIM compute paradigm using the example of convolution.}
%     \label{fig:paradigm}
% \end{figure}

\subsection{\update{CIM Compilation Works for DNN}}
With the increasing attention on CIM, there has been a significant surge in efforts to develop a compilation optimization stack aimed at facilitating the deployment of DNN algorithms across various CIM architectures \cite{siemieniuk2021occ,sun2023pimcomp,qu2024cim,ankit2019puma}. 

Existing compilation optimization approaches for CIM predominantly emphasize scheduling optimizations, such as task mapping, resource allocation, and dataflow scheduling, to fully exploit the static on-chip resources of CIM chips, thereby reducing latency.
For example, OCC \cite{siemieniuk2021occ}, built upon MLIR, utilizes a specific ISA to support scheduling optimization for multiple operators. 
% PIMCOMP\cite{sun2023pimcomp} employs genetic algorithms to optimize weight replication and core mapping. 
CIM-MLC \cite{qu2024cim} addresses the challenges posed by multi-level and heterogeneous program interfaces in CIM accelerators, implementing weight duplication and pipelining techniques tailored for various CIM computation modes. These compilation strategies alleviate the programming complexity of utilizing CIM processors, accelerate application deployment, and allow researchers to focus more on architecture design.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/whatwedo2.pdf}
    \caption{(a) Existing typical CIM mapping method that treats all the CIM arrays as compute arrays; (b) Dual-mode-aware mapping method.}
    \label{fig:motivate}
\end{figure}
However, existing compilers overlook a crucial aspect: the dual-mode capability of CIM arrays, resulting in suboptimal performance.
As shown in \fig \ref{fig:motivate}, when taking the dual-mode feature of CIM arrays into account, the compiler can dynamically allocate compute and memory resources (b). Thus, it can enhance the DNN performance when keeping more data on the chip by switching the CIM arrays to memory mode. Instead, the traditional compiler has to move these data to off-chip memory, incurring extra latency.
% static resource allocation can constrain performance (a), whereas dynamic allocation of compute and memory resources (b), tailored to the real needs of applications, can 
% substantially enhance performance. 

% Existing compilation works only optimize the application mapping with fixed hardware resources and treat all the CIM arrays as compute units.
In summary, the dual-mode capability of CIM arrays introduces a powerful mechanism for dynamically adjusting on-chip resources to meet the diverse computation and memory demands of various DNN workloads. By intelligently switching CIM arrays between compute and memory modes, we can flexibly allocate resources based on the specific requirements of different DNN inference tasks, ultimately optimizing performance.
% which is insufficient to meet the diversity of memory/compute requirements of real-world DNN. 
% This work aims to develop efficient methods for jointly optimizing compute/memory mode allocation and application mapping scheduling. By adjusting memory and compute resources dynamically during complication optimization, we seek to enhance the performance of diverse applications.
% 可以在这里讲一下dual model给编译带来的挑战，在这里提一下



% , which brings new possibilities to computing systems.
% CIM provides the possibility of converting the CIM array between compute and memory modes. 
% By controlling the inputs to CIM arrays or changing the driver, we can adjust the array's functionality with minimal overhead. 
