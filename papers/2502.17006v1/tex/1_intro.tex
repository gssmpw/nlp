% [DDL] 6.25 北京时间晚上8点
\section{Introduction}
The Computing-In-Memory (CIM) architecture is highly regarded for enabling in-situ computation \cite{chen2020survey,ankit2019puma,biswas2018conv,eckert2018neural,song2017pipelayer,chi2016prime,shafiee2016isaac}. CIM minimizes frequent data transfers and enhances the parallel execution of matrix multiply-and-accumulate (MAC) operations, resulting in notable performance improvements. Compared to conventional architectures, CIM significantly mitigates persistent memory wall problem \cite{wulf1995hitting} and demonstrates strong competitiveness in data-intensive applications \update{especially deep neural network (DNN) inference} \cite{yang2020retransformer,shafiee2016isaac,sridharan2023x}.

% For instance, ISAAC\cite{shafiee2016isaac} accelerates convolutional neural networks, while ReTransformer\cite{yang2020retransformer} supports transformer-based networks.

To enhance efficiency and fully realize the potential of the CIM accelerators, researchers have explored various compilation optimization techniques, aimed at various CIM architectures such as resistant RAM and SRAM-based solutions \cite{qu2024cim,sun2023pimcomp,farzaneh2023c4cam,han2021polyhedral,siemieniuk2021occ,drebes2020tc}. These compilation tools significantly reduce entry barriers for users adopting the CIM architecture and support the widespread deployment of CIM chips.
% These techniques form the software stack foundation necessary for extensively utilizing CIM architecture. 
% For instance, CIM-MLC\cite{qu2024cim} introduces multi-level compilation optimization strategies to meet the diverse requirements of different applications and CIM chip designs.
% Based on the weight stationary paradigm that preloads the model's weight into the CIM array, 
% current CIM compilation methods can be broadly categorized into spatial and temporal optimizations.
Earlier compilation tools for CIMs assume that neural network weights are pre-loaded into memory. These tools formulate an optimized policy for weight mapping and devise a computation scheduling scheme across various operational granularities to maximize memory resource utilization and enhance computational performance. \cite{qu2024cim,siemieniuk2021occ,ankit2019puma}.
% From a spatial perspective, common compilation methods include operator weight replication \cite{qu2024cim,shafiee2016isaac,ankit2019puma}, computation loop unrolling \cite{siemieniuk2021occ}, and weight remapping \cite{qu2024cim}. These techniques aim to fully utilize on-chip resources for weight mapping, thereby enabling parallel computation. From a temporal perspective, common scheduling methods involve intra-layer or inter-layer pipeline schedule strategies,
% % pipelining between network layers or operators, 
% overlapping the execution times of multiple operators or layers to minimize overall latency \cite{qu2024cim,shafiee2016isaac,sun2023pimcomp,song2017pipelayer}.
Although there have been significant improvements in compilation optimization techniques for CIM architecture, current methods still consider the memory and compute resources on the chip static, which does not accurately represent the modern advancements in CIM designs.
In practice, many modern CIM designs feature dual-mode memory arrays that can dynamically switch between memory and compute modes~\cite{kim202316,yan20221,guo202328nm,yue202115}. As depicted in \fig\ref{fig:intro}(a), the CIM array transitions between these modes by resetting the input driver. This dynamic functionality broadens the compiler's optimization possibilities for CIM mapping, \update{enhancing DNN application performance}. Previous compiler-level optimization efforts did not fully exploit these opportunities, missing out on the benefits of dual-mode CIM arrays.

 % the diversity of computational and memory resource requirements for real-world DNNs. Existing compilation optimization treats all CIM arrays as purely computational units and aims to maximize the utilization of computational resources, leading to suboptimal compilation results in real-world scenarios.

% 620下午开会：解释，在图题里面解释一下，实证的结果
% 有没有例子，在特定的算子workload模型下更好，决策一看就存算转换更好的，纯的优，决策维度，在更大的空间做决策，一目了然的例子相比于原来更有用
% 直观让读者理解到，图标要改一下，性能与计算存储资源比例
% 这里计算资源比例的增大片上存储单元随之减少，片上可存储数据和带宽也就随之下降。对于计算单元比例少却反而性能更好的情况是因为模型对于带宽有着极大的需求，计算单元多的情况下由于片上存储和带宽跟不上，所以根本无法发挥那些计算单元的能力，实际有效利用的计算单元数量还没有计算单元比例少的时候多。
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{image/motivate_intro.pdf}
    \caption{(a) CIM switching between memory and compute mode by setting up control signals to the input driver; (b) Normalized performance variation with the ratio of arrays in compute mode changes. Please note that putting more CIM arrays in compute mode deprives them of the chance of working as scratchpad memory for storing and loading intermediate data, e.g. activations. CIM arrays in compute mode must store static data, i.e. pre-determined weights.}
    \label{fig:intro}
\end{figure}
% random-access 
% 考虑REAL-WORLD的时候他们的问题出现在哪里，real world里面神经网络会呈现多样性，在这里举例子，CNN/Transformer,说明情况都是恰恰相反的（大概的数值），原来的那种只是针对CNN进行优化的，那他的结果就是不是最优的，然后再将，对于CIM来说，他的array就是同时又两种功能的，我们在编译的时候就要利用到CIM的这个特性，去为real world的网络提供最优的性能（可以用后面那段的例子，语气强烈一点，说明他们就是不好，不是最优的，这种real world多样性， 一定要考虑我们这种的必要性，
Moreover, different real-world DNN architectures have distinct memory and computation resource requirements. \fig\ref{fig:intro} (b) depicts the varying demands on the memory and computation resource of different DNN models (\ie CNNs \cite{he2016deep}, LLaMA \cite{touvron2023llama}, GPT~\cite{brown2020language}, \etc) to reach the optimal performance on the CIM chip.
Convolutional neural networks (CNNs) have relatively high arithmetic intensity (FLOPs/ Memory OP) and demand a higher ratio of compute to memory resources on CIM. For instance,  ResNet50 has an average arithmetic intensity of 66, and its performance reaches the highest point when the ratio of compute to memory resource reaches almost 80\%. Thus, some typical CNNs require more CIM arrays working in compute mode, when they already have sufficient CIM arrays configured as the on-chip scratchpad memory for activation caching.
 % Thus, ResNet50 has an average arithmetic intensity of 66.
 In contrast, Transformer-based models typically have much lower arithmetic intensity. For instance, the generative model LLaMA 2 has an average arithmetic intensity of around 2 for single batch inference and \fig\ref{fig:intro} (b) depicts that LLaMA 2 garners the best performance when the ratio of compute to memory in CIM arrays is about 10\%, which means it is better to offer more on-chip random memory for activations and KV cache rather than to increase compute-power, given that it is almost impossible to cache all the massive parameters of large language models on a single CIM chip. Although this conclusion drawn from Figure 1 only makes sense for certain models and hardware configurations, like the on-chip memory space, main memory bandwidth, etc, it reveals a fact that it is not necessarily correct to assume all CIM arrays should be put at the compute mode as in prior compilation works.
 % Therefore, within the same hardware resources budget, CNNs may prefer more compute units, whereas generative models may require more on-chip memory. 
 Moreover, the requirements for memory and compute CIM arrays of the same model may vary across different layers or stages of execution. 
 Therefore, a compiler customized for dual-mode CIM that optimizes the memory and compute mode of the CIM array is significant.
 
 % typically have higher arithmetic intensity (FLOPs/Memory OP). 
 % ResNet50 has an average arithmetic intensity of 66. In contrast, Transformer-based models typically have much lower arithmetic intensity; e.g., the generative model LLaMA has an average arithmetic intensity of around 2. Therefore, within the same hardware resources budget, CNNs may prefer more compute units, whereas generative models may require more on-chip memory. Additional, the requirements of the same model may vary across different layers or stages of execution. 
\begin{comment}
    

% CIM's inherent support for both memory and compute modes provides an opportunity to dynamically customize hardware resources \cite{kim202316,yan20221,guo202328nm}. Unfortunately, despite the advancements in compilation optimization methods for CIM architecture, these methods never consider the diversity of computational and memory resource requirements for real-world DNNs. Existing compilation optimization treats all CIM arrays as purely computational units and aims to maximize the utilization of computational resources, leading to suboptimal compilation results in real-world scenarios.

% Despite the advancements in compilation optimization methods for CIM architecture, they often overlook a crucial characteristic of CIM — the seamless integration of memory and computation. This integration offers the opportunity to dynamically adjust the on-chip computation-memory resources during application execution \cite{kim202316,yan20221,yue202115,fujiwara20225,wang2022dimc,spetalnick202240nm,ankit2020panther}. 
% simple describe about how it supports CS-switch? 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{image/intro.pdf}
%     \caption{intro}
%     \label{fig:intro}
% \end{figure}
% From an application perspective, different real-world deep neural network (DNN) architectures have distinct requirements for memory and computation resources  \cite{he2016deep,simonyan2014very,dosovitskiy2020image,devlin2018bert,touvron2023LLaMA,redmon2016you,nerf,rombach2022high}. For instance, 
% Convolutional Neural Networks (CNNs) and Transformer-based generative models exhibit contrasting resource demands during inference. 
% convolutional neural networks (CNNs) \cite{he2016deep} typically have higher arithmetic intensity (FLOPs/Memory OP), whereas Transformer-based generative models prioritize memory operations, especially during the generation phase \cite{kim2023full}. Within the same hardware resources budget, CNN may prefer more compute units while generative models may prefer more on-chip memory units. Additionally, the resource needs of the same model may also vary across different layers or stages of execution \cite{he2016deep,touvron2023LLaMA}.
% From a hardware perspective, existing compilation techniques treat all CIM arrays as compute units, potentially causing resource waste. 
% For example, for the attention computation, the generated K value has to be written back rather than stored on CIM arrays which introduces unnecessary overhead.
% as they never consider the application preference. 
% CIM's inherent support for both memory and compute modes offers an opportunity to customize hardware resources dynamically \cite{kim202316,yan20221,guo202328nm}. 
% By adjusting the mode of CIM arrays based on application characteristics, we can provide tailored compilation optimization support with dedicated hardware resource allocation for diverse applications. 
% 这个例子和计算密度的关系关系不是很大，讲CNN和transform的大概数值，引用一下算法
% For example, in the attention computation, when CIM arrays are all in compute mode, the generated Q and K values need to be written back. 
% However, in practice, by using some CIM arrays as memory during the generation process, Q and K values can be stored directly on the CIM arrays. 
% Subsequently, by converting the corresponding arrays from memory mode to compute mode, in-suit computation can be performed, thereby reducing data movement.
% By adding a dual-mode-aware compilation optimization pass, we can fully exploit the potential of the CIM processor. Therefore, there is an urgent need to develop compilation techniques that support memory-compute dual-mode switches to better utilize CIM architecture resources. 
% As shown in \fig \ref{fig:intro}, under the budget of 100 CIM arrays, the model with an arithmetic intensity of 2 reaches the best performance when 8 CIM arrays are in compute mode and 92 CIM arrays are in memory mode. 
% , providing acceleration opportunities for applications with varying demands.
% This approach fully exploits the potential of hardware, providing acceleration opportunities for applications with varying demands.
% With the same hardware overhead, our goal is to reduce application runtime latency and improve throughput. Given that, 
\end{comment}
%[TODO620]图要改成和文字可以对上的，先把内容改完就接着来改这个图
% \begin{figure*}[th]
%     \centering
%     \includegraphics[width=\linewidth]{image/CIM.pdf}
%     \caption{The example of CIM architecture and dual mode conversion (a) CIM core with compute-memory mode switchable CIM arrays and the corresponding peripheral units; (b) Switchable CIM array and the corresponding peripheral units; (c) Switchable CIM array (d) illustration of conversion between memory mode and compute mode.}
%     \label{fig:background1}
% \end{figure*}


% Therefore, to support real-world DNNs, we propose a dual-mode CIM compiler that fully exploits the potential of the CIM processor. By adjusting the mode of CIM arrays based on application characteristics, we provide tailored compilation optimizations and dedicated hardware resource allocation for diverse applications. 
% Additionally, we allocate a specific number of arrays to each operator, optimizing performance by balancing their computational and memory needs.
\update{In this work, we propose a novel CIM compiler that takes the CIM mode switch into account and co-adjusts the CIM working mode and the mapping of the DNN applications in the context of dual-mode CIMs. 
Specifically, for a given target CIM architecture and the neural network workload, the proposed compiler can determine the arrays' mode being the compute or memory, and the optimal allocation of those arrays.} Once the mode-switch decision is made, the compiler also schedules operators on the respective arrays
% in both memory and compute modes 
to achieve optimal performance.


However, to achieve this goal, we have to address the following two challenges: 
% I suggest challenge1:Exponential space expansion
% \slee{Exponential space expansion} 
(1) \textbf{Exponential space expansion}: 
In dual-mode CIM, each CIM array can work in memory or compute mode. \update{Consequently, the problems of array mode selection and weight mapping are entangled in the deployment of target DNNs}, which constitutes a larger exploration space for the compiler.
For instance, with $m$ arrays in CIM, there are $2^m$ choices of the mode allocation during the compilation. It is proved that model scheduling for CIM is already a complicated problem with the optimization space about polynomial complexity, dual-mode CIM will face a $2^m$ times larger space with an exponential level. 
%量化之前编译和现在编译的空间，之前应该是相对于有N个算子的网络而言是关于N的多项式的复杂度，现在就直接是指数复杂度了
% We need to explore dual-mode CIM allocation and mapping schemes for operators simultaneously. As the number of CIM arrays increases, the allocation space expands exponentially, complicating the search for the optimal solution. 
Therefore, we have to formalize such a jointly optimized exploration space that combines the allocation and mapping decisions, along with a search strategy when designing our compiler.
%I suggestion challeng2: switching cost optimization
% \slee{Dual-mode switching cost optimization}
%620 因为存算转换使得映射分配调度是耦合的，需要进行全局的重新设计优化
(2) \textbf{Dual-mode switching schedule}: When scheduling each DNN operator in a dual-mode CIM, we should deliberately determine the mode of arrays as the number of arrays in different modes also affects the efficiency of the current operator and the scheduling of subsequent operators. Thus, the compiler for dual-mode CIM must account for the interdependence between array mode scheduling and weight mapping. The separate treatment of mapping and scheduling in previous compilers for CIM with fixed-mode arrays is insufficient for enhancing performance in dual-mode CIM architectures.
\update{Thus, we propose a holistic optimization framework that integrates DNN mapping and array mode scheduling for dual-mode CIMs.} 
%[问题]上面这句改成we require?， 然后在下面再说，我们做了什么
% Additionally, we develop an iterative search process to identify the optimal solution for supporting the given DNN application.
% Switching between modes in CIM for DNN inference offers potential benefits but also introduces switching overhead. Existing pipeline scheduling methods are inadequate for this temporal optimization. We need to consider the switching costs in the scheduling to fully leverage the dual-mode features, aiming to enhance performance through efficient network scheduling within limited resources.

To address these challenges, our approach at first provides the hardware abstraction of dual-mode CIM accelerators \update{based on CIM-MLC}~\cite{qu2024cim} so that the dimension of CIM reconfiguration can be fused into the original mapping/scheduling space of CIM as a formalized optimization space.
% , and then uses a two-step policy 
\update{Second, to make the joint optimization problem tractable for modern large-scale neural networks, we employ a divide-and-conquer two-step policy, co-optimizing the array mode switch, allocation, and mapping of neural networks.} Given that CIM memory space often cannot accommodate the entire model on the chip, the network must be executed in segmented partitions in serial. This is a common trend with billion-scale large language models.
% Second, for co-optimizing dual-mode array switch, allocation and mapping of neural networks,
% Our approach first integrates memory-computation switch information into the optimization space by abstracting the dual-mode switch method and its associated overhead. 
%可以简要介绍 问题和技术 然后合并进入contribution，不知道说的这个segmentation跟前面有什么关系
% 嗯嗯，好的，确实和前面有点对不上。。。
%啰嗦了
% Second, 
% to make the joint optimization problem tractable for modern large-scale networks, we employ a divide-and-conquer two-step policy to reduce the problem through network segmentation, given that most of the time the CIM memory space cannot accommodate the entire model on the chip and must execute the network as segmented partitions in serial, which is a trendy phenomenon in days of billion-scale parameter large language models. 
\update{We first utilize dynamic programming (DP) to network segmentation. The overhead introduced by the array mode switch is taken into account when applying DP for global optimization.}
Afterward, we use mixed integer programming to automatically explore the optimization space for operator mapping with tunable hardware resources within each segment. The compiled results are then output in a meta-operator flow marked with memory-compute switch information. 
%[6.24日讨论] To reduce the exponential optimization space, we employ a divide-and-conquer two-step policy. We first reduce the problem through network segmentation, using dynamic programming to determine how the networks are segmented scheduling, accounting for the segment conversion overhead introduced by CIM mode switches. Afterward, we use mixed integer programming to automatically explore the optimization space for operator mapping with tunable hardware resources within each segment. The compiled results are then output in a meta-operator flow marked with memory-compute switch information.
% To effectively explore the dual-mode CIM array allocation for application mapping, we formalize the joint optimization problem in two steps. 
% 啰嗦了 To minimize application execution latency, 

\begin{comment}
    

%Therefore, to support real-world DNN, we propose a dual-mode-aware CIM compiler that fully exploits the potential of the CIM processor for real-world DNN.
% To address the limitations of existing CIM compilation tools, we propose a dual-mode-aware CIM compiler that fully leverages the potential of CIM processors for real-world DNN applications. 
%Our approach integrates the capability of CIM arrays to switch between compute and memory modes within the compilation process. 
%By adjusting the mode of CIM arrays based on application characteristics, we can provide tailored compilation optimization support with dedicated hardware resource allocation for diverse applications. 
% By optimizing the use of CIM arrays' dual-mode feature, we aim to determine the optimal mode for each array and efficiently allocate resources to various operators.
%For instance, given a target CIM processor and a network with various operators, our goal is to decide how many arrays should be in compute mode and how many should be in memory mode. We also need to allocate a specific number of arrays to each operator to optimize performance by balancing their computational and memory needs.
% 这里是否要加入challenge的描述，还是在后面说就好，这里只放观察到的内容
% Therefore, incorporating the compute-memory dual-mode switch into the scheduling optimization space is essential, as it introduces a variable hardware optimization dimension. Meanwhile, exhaustively exploring all memory-compute variations for each configuration is impractical due to the vast optimization space. Therefore, we also seek an efficient method to navigate and optimize this joint space.
% In the fixed memory-compute hardware resource scenario, the total memory access demands and computation requirements are predictable, existing compilation optimization methods strive to optimize program scheduling based on known hardware resources. 
% However, when CIM supports memory-compute dual-mode switches, the ideal allocation of the CIM array in memory or compute mode can be adjusted according to task requirements, introducing a variable hardware optimization space.
% So we need to first incorporate the dimension of the compute-memory dual-mode switch into the scheduling optimization space.
% [6.11]或许就不扯challenge了，就说现在的编译方法没考虑这些事情，导致了资源的利用不是很好，然后我们采用dual-mode aware的方式来解决了这些问题。具体的挑战之后在motivation部分再说?
% However, supporting memory-compute dual-mode switches in the CIM-targeted compilation process introduces new challenges. When CIM supports these switches, the ideal allocation of CIM arrays in memory or compute mode can be adjusted according to task requirements, creating a variable hardware optimization space. Current compilation schemes overlook this switchable hardware optimization space, presenting two main challenges for building a dual-mode-aware CIM compiler.
% In the fixed memory-compute hardware resource scenario, the total memory access demands and computation requirements are predictable, existing compilation optimization methods strive to optimize program scheduling based on known hardware resources, like duplicating operators on CIM to maximize resource utilization. 
% However, when CIM supports memory-compute dual-mode switches, the ideal allocation of the CIM array in memory or compute mode can be adjusted according to task requirements, introducing a variable hardware optimization space. 
% This optimization space must be integrated into the compilation optimization space for an efficient compilation process. 
% As current compilation schemes overlook the switchable hardware optimization space, building a dual-mode-aware CIM compiler has two main challenges. 
% First, we must incorporate the dimension of the compute-memory dual-mode switch into the scheduling optimization space. Since CIM hardware abstraction lacks information related to dual-mode switches, we cannot rely solely on existing hardware abstractions. 
% Second, we need to efficiently explore the joint optimization space. Exhaustively enumerating memory-compute variations and performing existing compilation optimization based on each configuration is impractical due to the vastness of the switchable space. Therefore, we must formalize the memory-compute dual-mode switch optimization space and the application scheduling optimization space into a unified compilation problem to efficiently obtain the optimal solution for specific objectives.


% To address these challenges, we propose a dual-mode-aware CIM compilation process tailored to meet the requirements of real-world DNN applications. To incorporate memory-computation switch information into the hardware abstraction, we first abstract the dual-mode switch method and overhead. 
% To effectively explore the hardware allocation for application mapping, we use two steps to formalize the joint optimization problem. 
% Given that CIM chips often cannot accommodate the entire model weights in the real world, we partition the network and map it to the chip serially. To minimize application execution latency, we use dynamic programming to determine network segmentation considering the segment conversation overhead introduced by CIM mode switches. Based on the formal description of the memory-computation switchable characteristics, we employ mixed integer programming to automatically explore the optimization space for operator mapping with turnable hardware resources within each segment. The compiled results are then output in a meta-operator flow marked with memory-computation switch information.
\end{comment}

% challenge
%620 挑战：和程序特性相关，有没有tradeoff，算力换带宽有没有什么结论。

Specifically, the main contributions of this work include:
\begin{itemize}
    \item %We first identify the various compute/memory resource requirements of real-world DNNs that are negligent by existing CIM compilation works. 
    % To harness the dynamic compute/memory resources adjustment capacity provided by CIM, 
    \update{To support various DNNs including nowadays large language models,
    we introduce CMSwitch, a novel dual-mode-aware CIM compiler that leverages the mode switch capability of compute/memory of CIM arrays to meet diverse DNN application requirements. We formalized the joint-optimization problem of mode-switch, mapping, and scheduling for standalone CIM accelerators, and released the first compiler aware of this important CIM feature.}
    \item We comprehensively consider the challenges and opportunities brought by mode switching. Without causing too much exploration overhead, we propose a two-step optimization strategy to make the compilation process converge at the optimal design point of the large joint design space. 
    By formalizing the overhead and performance improvement introduced by CIM mode switch, we employ DP and MIP to determine the optimal network segment in temporal and allocate compute-memory resources in spatial.
    \item We evaluate the \name across a set of DNN benchmarks. \update{Compared with state-of-the-art compilation works \cite{qu2024cim}, \name achieves average inference speed improvement by 1.31$\times$.} We also verify \name for various workload scales, demonstrating robust dual-mode-aware compilation support for \update{diverse DNN architecture demands}. It is proved the proposed compiler shows especially great potential for popular large models that cannot be fitted into the on-chip memory.
\end{itemize}



% \begin{itemize}
    % \item We first identify the potential for dynamically adjusting a CIM chip's compute and memory resources. To harness this potential, we introduce \name, a novel dual-mode-aware CIM compiler that can customize the mode of CIM arrays to effectively meet diverse application requirements.
    % % 我们利用到CIM给支持real world更好的性能，在编译的过程中，我们解决了在考虑switch下所带来的挑战
    % \item We integrate the dual-mode switchable characteristic of CIM arrays into the compilation process. By formalizing this problem, we develop an efficient optimization algorithm. This algorithm employs dynamic programming (DP) and mixed-integer programming (MIP) to determine the optimal network segment and allocate compute-memory resources.
    % \item We evaluate the \name across a set of DNN benchmarks. Compared with existing compilation works \cite{qu2024cim,ankit2019puma,siemieniuk2021occ}, \name achieves inference speed by up to 2.6$\times$. We also verify \name for various workload scales, demonstrating robust dual-mode-aware compilation support for diverse application demands.
% \end{itemize}
% TODO contribution
