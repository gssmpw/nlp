% [6.14Z]是否还需要discussion的章节来讨论一下还有进一步优化的空间，比如在段间数据的换入换出进行优化等;对于生成式的模型，在生成的序列增长的过程中，其需要是动态变化的，如果有新的运行时支持的话，可以更好的在运行的过程中对其进行动态的调整
\section{\update{Discussion}}
% \noindent\textbf{Runtime optimization.}
% In generative models, the number of generated tokens increases as the task progresses, leading to changes in the attention computation and KV cache storage requirements. Consequently, the demand for computing and storage resources evolves throughout the sentence generation process. If runtime support is available, dynamically adjusting compute and memory resources online based on feedback from the running state could further optimize performance. This adaptive approach allows for more efficient resource utilization, potentially improving overall system performance.


% 感觉对于通用system的讨论还是必要加的，一是说明我们现在是给CIM加速器加速的，二是说明insight,所提出的内容有更广泛的应用场景
\textbf{\update{Opportunity for general-purpose system.}}
\update{
While this work focuses on a standalone CIM accelerator system, exploring the compute/memory dual-functionality of CIM architecture offers significant potential for improving general-purpose system performance. Whether CIM is used as a co-processor or a standalone accelerator, dedicating all of its resources solely to computation may not achieve the optimal operating point for many tasks, due to their varying resource requirements.
In a real system enhanced with CIM, tasks that go beyond DNNs will be encountered more frequently \cite{fujiki2019duality,lockerman2020livia,orenes2023dalorex}. 
These tasks often involve complex and dynamic memory and computation needs. To address this, it is crucial to adapt the CIM architecture to meet the specific demands of each task. One effective approach lies in dynamically adjusting the allocation of computation capacity, memory size, and bandwidth through mode switching. This flexibility allows the system to better match the resource requirements of diverse workloads.
The trend toward tighter integration of memory and computation is key to supporting the growing diversity of memory-intensive tasks. 
% Thus, leveraging CIM's dual-mode functionality in a flexible and adaptive manner is essential to optimizing system efficiency and performance.
% switching between compute and memory modes, registering the corresponding units as either computational or scratchpad memory in the system to change the ratio of computation capacity/memory size & bandwidth. This approach allows for a closer alignment with the application’s requirements, ultimately improving overall system performance. 
% The further integration of memory and computation is an emerging trend to support increasingly diverse memory-intensive tasks. 
Therefore, leveraging the dual-mode capability of CIM in a more flexible manner is crucial to optimizing system efficiency.
% Enable flexible expansion to a wide range of applications that are not limited to DNNs.
}

% \noindent\textbf{Segment overhead optimization.}
% \update{During segment transitions, some compute unit weights need to be reloaded. Different operators' weights may exhibit a degree of similarity. If we can exploit this similarity to reduce the amount of weight reloading required, the overall runtime latency could be further decreased. By minimizing the weight reloading overhead, we can enhance the efficiency of segment transitions, thereby improving the overall performance of the CIM-based system. This approach highlights the potential for further optimizations in the compilation and execution phases by leveraging inherent data characteristics.}


\section{Conclusion}
In this paper, we presented a novel compilation optimization process tailored for dual-mode CIM chips. By incorporating the compute-memory dual-mode switch into the compilation optimization space, we developed a comprehensive approach that optimizes the memory/compute CIM array allocation for various networks. Our dynamic programming-based network segmentation, coupled with mixed integer programming for operator mapping and scheduling, ensures efficient utilization of CIM resources. \update{Experimental results demonstrate 1.31 $\times$ performance improvements on average across different models compared to the state-of-the-art CIM compilation work}, highlighting the effectiveness of our approach in diverse workloads. By utilizing the dual-mode feature of the CIM processor, our method \name achieves substantial speedup while maintaining low overhead. This work paves the way for more efficient and flexible use of CIM chips in real-world DNN applications, enhancing the performance and scalability of CIM-based systems.