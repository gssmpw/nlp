\section{Analysis}
\label{analysis}

In this section, we present three analytical experiments. The first one shows the performance of other VLLMs if we employ the identified knowledge boundary as a surrogate. The second shows how the RAG ratio and VQA performance are affected by the threshold defined in the $SKB$ variant. The third one presents the accuracy of VLLM boundary identification on held-in data at training time. 


\subsection{Surrogate Boundary for Other VLLMs}
\label{plugin_section}

We assemble around 340 thousand VQA samples from various domains discussed in Sec.~\ref{training_time_section}. Sampling each data thirty times is prohibitively expensive for closed-source VLLMs. Although different VLLMs intuitively possess varying scopes of knowledge, we believe that it is highly probable that a significant portion of these scopes overlap.
For instance, queries regarding recently occurring news events typically fall outside the knowledge boundaries of any model. Thus, we conduct an experiment that validates whether the identified knowledge boundary can function as a surrogate boundary for other VLLMs.


The experimental results with Qwen as a boundary model are presented in Table~\ref{main_results_llm_table} and Table~\ref{main_results_acc_table}. The results with DeepSeek as a boundary model are presented in Appendix~\ref{supplementary_results_of_surrogate}.


From Table~\ref{main_results_llm_table} Mix row, Qwen-VL-Max, Qwen-VL-2 and GPT4-o achieve better performance than all three baseline settings. Deepseek-VL-7B-Chat remains competitive to the ``All RAG'' setting with LLM metric and outperforms all other settings in Table~\ref{main_results_acc_table} Mix row. For other datasets, we show that the previously identified knowledge boundary can help maintain the performance with a reduced RAG ratio. For example, GPT4-o achieves 54.83 with only 61.74\% RAG ratio while the ``All RAG'' setting achieves 55.47 on the Life VQA dataset. Deepseek-VL-7B-Chat maintains its performance on the Dyn-VQA (en) dataset compared to the ``All RAG'' setting and keeps a clear margin compared to the ``No RAG'' setting with a 23.92\% retrieving deduction.



\subsection{Effect of $\epsilon$ for $SKB$}
\label{epsilon_section}


\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.32\linewidth]{figs/life_llm_ana.pdf}
    \includegraphics[width=0.32\linewidth]{figs/dyn_llm_ana.pdf}
    \includegraphics[width=0.32\linewidth]{figs/mix_llm_ana.pdf}
    \includegraphics[width=0.32\linewidth]{figs/life_acc_ana.pdf}
    \includegraphics[width=0.32\linewidth]{figs/dyn_acc_ana.pdf}
    \includegraphics[width=0.32\linewidth]{figs/mix_acc_ana.pdf}
    \caption{Effect of $\epsilon$. The lighter dashed lines accordingly indicate the performance under each base model's ``No RAG'' setting. Knowledge Boundary model is Qwen-VL-7B-Chat.}
    \label{epsilon_fig}
\end{figure*}


In Sec~\ref{main_result}, we show the result of the $SKB$ method with the least RAG ratio, i.e., $\epsilon$ is set to maintain a low tendency to resort to RAG. Here we show how the overall VQA performance is affected by $\epsilon$. The results of three datasets are illustrated in Fig.~\ref{epsilon_fig}. The leftmost point of the horizontal axis corresponds to the ``All RAG'' setting (with $\epsilon=s_w$), while the rightmost point represents the minimal search ratio. Light-coloured dashed lines depict the ``No RAG'' setting. 
For the left two data in Fig~\ref{epsilon_fig}, where RAG can greatly affect the performance, our methods can maintain a clear margin between the ``No RAG'' setting and obtain a relatively stable performance with a decreased search ratio. For the Mix data where all types of data are fused, our methods can still lower the search ratio while maintaining, or improving, the performance.


\subsection{Efficiency}

\begin{table}
\small
\centering
\scalebox{0.95}{
\begin{tabular}{llccc} 
\toprule
 & \textbf{Model} & \textbf{All RAG} & \begin{tabular}[c]{@{}c@{}}\textbf{HKB}\\\textbf{(Mix)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{SKB}\\\textbf{(Mix)}\end{tabular} \\ 
\midrule
\multirow{2}{*}{\textbf{Time (s) }} & \textbf{QW} & \multirow{2}{*}{619.20} & 598.13 & 427.85 \\
 & \textbf{DS} &  & 386.61 & 326.74 \\
\midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}\textbf{Improvement }\\\textbf{(\%) }\end{tabular}} & \textbf{QW} & \multirow{2}{*}{-} & 3.40\% & 30.90\% \\
 & \textbf{DS} &  & 37.56\% & 47.23\% \\
\bottomrule
\end{tabular}}
\caption{Efficiency illustration of Knowledge Boundary model Qwen-VL-7B-Chat (QW) and DeepSeek-VL-7B-Chat (DS). \textbf{Time} row shows the time spent before generating the answer in the VQA task.}
\label{efficiency_table}
\end{table}

Our method incorporates an additional forward pass for each VQA example for knowledge boundary identification. We report the overall efficiency in Table~\ref{efficiency_table} on the Mix dataset, where the All RAG setting always uses RAG (calls to search engine included) and does not perform the forward pass, and $HKB/SKB$ refers to partially performing RAG according to our model's predictions with forward-pass time included. 


\subsection{Performance of Knowledge Boundary Identification on Held-In Data}

\begin{table}[tb]
\centering
\small
\scalebox{1}{
\begin{tabular}{llccc} 
\toprule
\textbf{Model} & \textbf{Fold} & \textbf{Human-labeled} & \textbf{Hard} & \textbf{Soft} \\ 
\midrule
\multirow{2}{*}{\textbf{QW}} & Train & 96.25 & 90.50 & 88.41 \\
 & Val. & - & 91.16 & 88.96 \\ 
\midrule
\multirow{2}{*}{\textbf{DS}} & Train & 96.25 & 93.91 & 92.10 \\
 & Val & - & 93.76 & 92.11 \\
\bottomrule
\end{tabular}
}
\caption{Training and validation results on the held-in dataset. Metrics are shown in the accuracy defined in \texttt{swift} package. We have a limited number of human-labeled samples thus we do not set a validation set for ``Human-labeled'' setting.}
\label{training_held_in_table}
\end{table}

The training results of the Knowledge Boundary model are shown in Table~\ref{training_held_in_table}. We show that by training Qwen-VL-7B-Chat (QW) and DeepSeek-VL-7B-Chat (DS), they succeed in modeling the knowledge boundary on held-in data we constructed according to Sec.~\ref{training_section}.


\begin{mycomment}
    
% \subsection{On Data RAG Struggles to Help}

% \begin{table*}
% \centering
% \scalebox{0.85}{
% \begin{tabular}{llcccccccc} 
% \toprule
%  &  & \textbf{No RAG} & \textbf{All RAG} & \textbf{Human} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} \\ 
% \midrule
% \multirow{4}{*}{\textbf{MMMU}} & Qwen-VL-Chat & 20.12 & 20.28 & \textbf{21.24} & 6.88\% & 20.35 & 97.08\% & 20.18 & 61.26\% \\
%  & Qwen-VL-Max & 51.33 & 41.37 & \textbf{52.67} & 6.88\% & 41.46 & 97.08\% & 44.40 & 61.26\% \\
%  & Qwen-VL-2 & 51.45 & 42.39 & \textbf{51.93} & 6.88\% & 42.54 & 97.08\% & 45.61 & 61.26\% \\
%  & GPT4-o & 56.60 & 56.64 & \textbf{57.36} & 6.88\% & 56.92 & 97.08\% & \uline{56.91} & 61.26\% \\
% \bottomrule
% \end{tabular}
% }
% \caption{Results evaluated by LLM on MMMU validation set.}
% \label{mmmu_results_llm_table}
% \end{table*}



% In this section, we show the experimental results of our methods on a challenging dataset, MMMU\footnote{We converted the dataset's original multiple-choice format into a conventional VQA format to ensure consistency with the aforementioned experimental settings.} \cite{yue2023mmmu} in Table~\ref{mmmu_results_llm_table}. MMMU is a dataset containing VQA samples demanding college-level subject knowledge and deliberate reasoning, and it is hard to verify the knowledge boundary that our methods depict by simply adopting RAG.  

% The results in Table~\ref{mmmu_results_llm_table} show the Knowledge Boundary model trained by human-labeled data helps achieve the best performance. It verifies that the aforementioned Human-labeled training data is effective in modeling the knowledge boundary of both VLLMs and search engines nowadays. In addition, we show that our methods also exhibit substantial potential within this setting, in which both the $HKB$ and $SKB$ models predict a high search ratio over MMMU. We contend that the suboptimal performance of this dataset arises because it lies beyond the knowledge boundaries, that are challenging to validate using RAG, as delineated by the white dashed lines in Fig.~\ref{outline}. We present the performance of each of the 30 subjects in the MMMU validation set in Sec.~\ref{mmmu_appendix_section}. 
\end{mycomment}



% \subsection{Performance of Knowledge Boundary Identification on Held-In Data}

% The training results of the Knowledge Boundary model are shown in Table~\ref{training_held_in_table}. We show that by training Qwen-VL-Chat, it succeeds in modeling the knowledge boundary on held-in data we constructed according to Sec.~\ref{training_section}.

% \begin{table}[tb]
% \centering
% \small
% \scalebox{1}{
% \begin{tabular}{lccc} 
% \toprule
%  & \textbf{Human-labeled} & \textbf{Hard} & \textbf{Soft} \\ 
% \midrule
% Train & 96.25 & 90.50 & 88.41 \\
% Val. & - & 91.16 & 88.96 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Training and validation results on the held-in dataset. Metrics are shown in the accuracy of ``True/False'' or score $s^{'}$ prediction. We have a limited number of human-labeled samples thus we do not set a validation set for ``Human-labeled'' setting.}
% \label{training_held_in_table}
% \end{table}




