\section{Experiment}

\subsection{Setup}

\subsubsection{Training Data}
\label{training_time_section}

With method stated in Sec.~\ref{sampling_section} and \ref{training_section}, we adopt InfoSeek \cite{chen2023can}, OK-VQA \cite{marino2019ok}, VQAv2.0 \cite{goyal2017making}, MMBench \cite{liu2025mmbench}, and MME \cite{fu2023mme} to construct the training set where we randomly sample two subsets from InfoSeek and VQAv2.0 respectively due to their large sizes. Table~\ref{training_set_table} presents the detailed sizes for each dataset we use along with the average scores $s$. In our experiments $s_w=1$ and $s_c=5$. We adopt all these datasets to increase the diversity of queries as much as possible. A detailed description of each dataset is stated in Sec.~\ref{training_data_description}.

% Below is a brief description of each dataset (for training).

% \paragraph{InfoSeek} is designed to assess the capability of models to seek and incorporate external information for question answering. It features a variety of queries that necessitate fact retrieval and reasoning that go beyond the provided context.

% \paragraph{OK-VQA} is a dataset where images are paired with open-ended questions that require answers stemming from general knowledge that extends beyond the image alone.

% \paragraph{VQAv2.0} is a comprehensive VQA dataset that requires interpretation or understanding of the visual content. It features a diverse and balanced range of answers.

% \paragraph{MMBench} is a benchmarking suite for evaluating multi-modal understanding, ensuring that multi-modal machine learning systems can effectively process and synthesize data from different sources.

% \paragraph{MME} is focused on tasks related to multi-modal entity recognition and extraction. The dataset contains annotations of text and images with multi-modal entities that need to be identified or linked.

% \paragraph{Human-Labeled} A group of annotators is asked to annotate whether RAG can help solve a VQA sample. We construct this data to form a reference setting. 



% \begin{table}[t]
% \centering
% \small
% % \refstepcounter{table}
% \scalebox{1.0}{
% \begin{tabular}{lcc} 
% \toprule
% \textbf{Source} & \textbf{\# Samples} & \textbf{Avg. Score $\pm$ \scriptsize{std.}}\\
% \midrule
% InfoSeek & $216000$ & $1.82 \space \pm$ \scriptsize{$1.17$} \\ 
% \midrule
% OK-VQA & $9009$ & $3.70 \space \pm$ \scriptsize{$1.48$} \\ 
% \midrule
% VQAv2.0 & $108000$ & $4.27 \space \pm$ \scriptsize{$1.36$} \\
% \midrule
% MMBench (en) & $4329$ & $3.92 \space \pm$ \scriptsize{$1.72$} \\
% \midrule
% MME & $2374$ & $4.15 \space \pm$ \scriptsize{$1.63$} \\
% \bottomrule
% \end{tabular}}
% \caption{Training set sources and statistics. Answers are sampled from Qwen-VL-7B-Chat (QW) \cite{Qwen-VL} DeepSeek-VL-7B-Chat (DS) \cite{lu2024deepseek} respectively. Scores are evaluated by Qwen-Max \cite{qwen1.5}}
% \label{training_set_table}
% \end{table}
% \footnotetext{{\url{https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api}}}


\begin{table}[t]
\centering
\small
% \refstepcounter{table}
\scalebox{1}{
\begin{tabular}{lccc} 
\toprule
\textbf{Source} & \textbf{\# Samples} & \textbf{Model} & \textbf{Avg. Score $\pm$ std.} \\ 
\midrule
\multirow{2}{*}{InfoSeek} & \multirow{2}{*}{$216000$} & QW & $1.82 \space \pm$ \scriptsize{$1.17$} \\
 &  & DS & $1.86 \space \pm$ \scriptsize{$1.28$} \\ 
\midrule
\multirow{2}{*}{OK-VQA} & \multirow{2}{*}{$9009$} & QW & $3.70 \space \pm$ \scriptsize{$1.48$} \\
 &  & DS & $4.92 \space \pm$ \scriptsize{$0.47$} \\ 
\midrule
\multirow{2}{*}{VQAv2.0} & \multirow{2}{*}{$108000$} & QW & $4.27 \space \pm$ \scriptsize{$1.36$} \\
 &  & DS & $4.50 \space \pm$ \scriptsize{$1.22$} \\ 
\midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}MMBench \\(en)\end{tabular}} & \multirow{2}{*}{$4329$} & QW & $3.92 \space \pm$ \scriptsize{$1.72$} \\
 &  & DS & $4.08 \space \pm$ \scriptsize{$1.65$} \\ 
\midrule
\multirow{2}{*}{MME} & \multirow{2}{*}{$2374$} & QW & $4.15 \space \pm$ \scriptsize{$1.63$} \\
 &  & DS & $4.15 \space \pm$ \scriptsize{$1.64$} \\
\bottomrule
\end{tabular}}
\caption{Training set sources and statistics. Answers are sampled from Qwen-VL-7B-Chat (QW) \cite{Qwen-VL} DeepSeek-VL-7B-Chat (DS) \cite{lu2024deepseekvl} respectively. Scores are evaluated by Qwen-Max \cite{qwen1.5}}
\label{training_set_table}
\end{table}


\subsubsection{Test Data}

As we aim to construct a model that can take various input queries and make good judgments about the knowledge boundary, we adopt held-out data to evaluate the final VQA performance. We summarize the overall RAG Effect on each data in Table~\ref{test_set_table} and a brief introduction as follows. 


\paragraph{Life VQA} We collect a set of VQA data from people's daily lives and extract the ones current VLLMs do not perform well, which is used to verify whether our model decides to resort to RAG for help. We will release this data along with the code and name it Life VQA. 

\paragraph{Private VQA} is an internal dataset spanning broad categories, including animals, plants, architecture, geographic locations, etc. Due to the complexity of the backgrounds and the presence of multiple objects, this collection poses a notable challenge for advanced visual reasoning and understanding. This dataset will not be released for now.
%due to personal information in it.

\paragraph{Dyn-VQA} is released by \citet{li2024benchmarkingmultimodalretrievalaugmented} and contains three types of questions: questions with rapidly changing answers, questions requiring multi-modal knowledge and multi-hop questions. This dataset is a challenging one in our evaluation. \textit{Gold query} is annotated by \citet{li2024benchmarkingmultimodalretrievalaugmented} that combines the text query and image to be used to retrieve useful information.

\paragraph{NoCaps} \cite{agrawal2019nocaps} is an open-domain image captioning dataset derived from Open Images \cite{openimages}, focusing on generating captions for a diverse array of objects and scenes. We sample a subset of size 500. 

\paragraph{Visual7W} \cite{zhu2016visual7w} is a VQA dataset containing images from COCO \cite{lin2014microsoft}, paired with seven types of questions (who, what, when, where, how, why and which). It aims to evaluate models' abilities in object recognition and deeper reasoning within visual contexts.

\paragraph{Mix} is a composite dataset consisting of 100 samples from each of the aforementioned datasets. It is designed to integrate the characteristics of each dataset and simulate real-world scenarios. Thus the effect of RAG on this dataset is mixed and hard to predict intuitively.


% \paragraph{MMMU} \cite{yue2023mmmu} (Massive Multi-discipline Multimodal Understanding) is a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. 


\begin{table}[t]
\centering
\small
% \refstepcounter{table}
\scalebox{1.0}{
\begin{tabular}{lc} 
\toprule
\textbf{Test Data} & \textbf{RAG Effect} \\
\midrule
Life VQA & High   \\ 
\midrule
Private VQA & Medium  \\ 
\midrule
Dyn-VQA & High \\
\midrule
NoCaps & Low \\
\midrule
Visual7W & Low \\
% \midrule
% MMMU & Low \\
\midrule 
Mix & ? \\
\bottomrule
\end{tabular}}
\caption{Test data property illustration that whether RAG is helpful in answering the queries.}
\label{test_set_table}
\end{table}



\subsubsection{Use of RAG} We aim not only to locate the queries that need RAG to answer better but also adopt retrieval techniques to verify the final VQA performance with the search decision $HKB_\phi$ and $SKB_\phi$ defined in equations \ref{KB}. We note that although there are various options for retrieval, such as text search and image search, we do not design detailed methods to determine the best option in this paper. Instead, we directly use text search (Google) for Dyn-VQA and image search (Bing) for the rest for better retrieval information quality towards answering the question. We note that Dyn-VQA is a challenging dataset that includes many samples of multi-hop property, therefore we use the \textit{golden query} \citet{li2024benchmarkingmultimodalretrievalaugmented} have summarized for retrieving useful information.

In the following sections, the ``No RAG'' setting refers to the performance of only VLLMs and no retrieval information is given, and ``All RAG'' refers to always incorporating RAG. ``Prompt-based'' refers to prompting the model that is sampled to adopt RAG or not. 


\begin{table*}
\centering
\scalebox{0.8}{
\begin{tabular}{llcccccccc|cc} 
\toprule
\multicolumn{1}{l}{\textbf{Dataset}} & \textbf{Metric} & \begin{tabular}[c]{@{}c@{}}\textbf{No }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{All }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Prompt-}\\\textbf{based}\end{tabular} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} & \textbf{Human} & \textbf{\%} \\ 
\midrule
\multirow{2}{*}{\textbf{Life VQA}} & \textbf{LLM} & 30.00 & 40.70 & 33.89 & 12.75\% & 40.64 & 96.64\% & 36.78 & 61.74\% & 39.33 & 71.14\% \\
  & \textbf{Acc.} & 17.80 & 36.11 & 21.38 & 12.75\% & 36.11 & 96.64\% & 29.44 & 61.74\% & 33.36 & 71.14\% \\
\midrule
\multirow{2}{*}{\textbf{Private VQA}} & \textbf{LLM} & 22.90 & 24.35 & 24.95 & 14.80\% & 24.50 & 99.20\% & 22.89 & 67.80\% & 24.20 & 72.00\% \\
  & \textbf{Acc.} & 16.26 & 18.40 & 17.26 & 14.80\% & 18.40 & 99.20\% & 17.35 & 67.80\% & 18.55 & 72.00\% \\
\midrule
\multirow{2}{*}{\textbf{Dyn-VQA ch}} & \textbf{LLM} & 19.16 & 38.95 & 19.70 & 6.38\% & 37.94 & 95.66\% & 36.53 & 84.26\% & 28.89 & 46.95\% \\
  & \textbf{Acc.} & 23.41 & 43.06 & 24.37 & 6.38\% & 42.71 & 95.66\% & 40.97 & 84.26\% & 33.13 & 46.95\% \\
\midrule
\multirow{2}{*}{\textbf{Dyn-VQA en}} & \textbf{LLM} & 21.60 & 34.93 & 23.51 & 14.13\% & 33.30 & 89.79\% & 32.06 & 76.08\% & 25.73 & 29.51\% \\
  & \textbf{Acc.} & 25.64 & 41.87 & 27.58 & 14.13\% & 40.66 & 89.79\% & 38.51 & 76.08\% & 30.83 & 29.51\% \\
\midrule
\multirow{2}{*}{\textbf{NoCaps}} & \textbf{LLM} & 50.13 & 30.37 & 50.13 & 0.00\% & 42.50 & 38.40\% & 50.13 & 0.00\% & 50.13 & 0.00\% \\
  & \textbf{Acc.} & 40.50 & 30.72 & 40.50 & 0.00\% & 36.95 & 38.40\% & 40.50 & 0.00\% & 40.50 & 0.00\% \\
\midrule
\multirow{2}{*}{\textbf{Visual7W}} & \textbf{LLM} & 54.48 & 52.04 & 55.32 & 31.36\% & 52.95 & 35.37\% & 54.27 & 2.96\% & 54.53 & 0.52\% \\
  & \textbf{Acc.} & 44.34 & 44.94 & 44.18 & 31.36\% & 44.32 & 35.37\% & 44.68 & 2.96\% & 44.34 & 0.52\% \\
\midrule
\midrule
\multirow{2}{*}{\textbf{Mix}} & \textbf{LLM} & 34.44 & 38.60 & 34.98 & 12.67\% & \uline{39.59} & 76.83\% & \textbf{39.93} & 49.33\% & 38.29 & 38.33\% \\
  & \textbf{Acc.} & 26.13 & 32.39 & 27.23 & 12.67\% & \textbf{32.73} & 76.83\% & 30.98 & 49.33\% & 31.02 & 38.33\% \\
\bottomrule
\end{tabular}
}
\caption{Main results of Qwen-VL-Chat. Scores are shown in columns except for the \% ones. Metrics are evaluated by Qwen-Max (LLM) and Token Accuracy (Acc.). \underline{Underlines} mark the results that outperform three baseline ``No RAG'', ``All RAG'' and ``Prompt-based'' settings. \textbf{Boldface} marks the best results.}
\label{main_results_7b_table}
\end{table*}

\subsubsection{Base Models}
When constructing the training set according to the method stated in Sec.~\ref{sampling_section}, we experiment with Qwen-VL-7B-Chat and DeepSeek-VL-7B-Chat that are used to be sampled $R=30$ times and fine-tuned according to Sec.~\ref{training_section} respectively. Refer to Sec.~\ref{hyper_appendix_section} for detailed training settings. Qwen-Max is prompted to score the $R$ predictions to get scores $s_i$ where we adopt $s_w=1$ and $s_c = 5$ referenced from \citet{Liu_LlamaIndex_2022}. 


For Visual Question Answering, we first evaluate the performance of the original models to be sampled. In addition, we seek to validate whether the identified knowledge boundary can function as a surrogate boundary for other VLLMs since constructing training datasets through sampling (Sec.~\ref{training_section}) on (larger) models can be prohibitively expensive. We further validate the surrogate knowledge boundary on the following VLLMs, Qwen-VL-Max \cite{Qwen-VL}, Qwen-VL-2 \cite{Qwen2-VL} and GPT4-o \cite{hurst2024gpt}, to evaluate its potential for generalizing across different VLLMs.


\subsection{Main Results}
\label{main_result}


We present our main results of Qwen-VL-7B-Chat in Table~\ref{main_results_7b_table} and result of DeepSeek-VL-7B-Chat in Appendix~\ref{main_results_ds_section}. In this section, we focus on the results of Qwen. 


Metrics \textbf{LLM} represents that the score is evaluated by a text LLM, Qwen-Max, given the model prediction and gold answer. Metrics \textbf{Acc.} refers to token accuracy which involves determining the proportion of tokens in the model's predictions that match the tokens in the gold answer. Both Scores range from 0 to 100 and a higher score indicates a higher performance. The \% columns refer to the ratio of data that our knowledge boundary model predicts to lie beyond the VLLM's knowledge boundaries. The ``Human'' column represents the corresponding statistics where the Knowledge Boundary model is trained on the human-labeled data mentioned in Sec.~\ref{training_time_section} and we deem it a reference result. 

First, the results in the Mix row, which considers all kinds of VQA queries in our setting and simulates a real situation, show that our methods outperform all other baseline and reference settings. Our $HKB$ method lowers the retrieval demand by 23.17\%, and the $SKB$ method lowers it by 50.67\%.

Second, as shown by the \% columns and the RAG Effect we summarized in Table~\ref{test_set_table}, our Knowledge Boundary models succeed in predicting a high ratio on test data when RAG can effectively aid in answering the query, and it lowers the ratio for data where the queries tend to fall within the knowledge scope of a VLLM. 


\begin{table*}[t]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{llcccccccc|cc} 
    \toprule
    \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Metric:}\\\textbf{LLM}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textbf{No}\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{All }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Prompt-}\\\textbf{based}\end{tabular} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} & \textbf{Human} & \textbf{\%} \\ 
    \midrule
    \multirow{4}{*}{\textbf{Life VQA}} & Ds.-VL-Chat & 25.54 & 47.38 & 27.68 & 12.75\% & 46.91 & 96.64\% & 41.21 & 61.74\% & 41.61 & 71.14\% \\
     & Qwen-VL-Max & 43.26 & 56.38 & 45.97 & 12.75\% & 56.85 & 96.64\% & 53.86 & 61.74\% & 55.23 & 71.14\% \\
     & Qwen-VL-2 & 42.55 & 54.43 & 46.28 & 12.75\% & 54.03 & 96.64\% & 52.28 & 61.74\% & 53.96 & 71.14\% \\
     & GPT4-o & 47.52 & 55.47 & 48.26 & 12.75\% & 56.14 & 96.64\% & 54.83 & 61.74\% & 54.90 & 71.14\% \\ 
    \midrule
    \multirow{4}{*}{\textbf{Private VQA}} & Ds.-VL-Chat & 23.01 & 27.06 & 23.89 & 14.80\% & 26.94 & 99.20\% & 26.19 & 67.80\% & 25.83 & 72.00\% \\
     & Qwen-VL-Max & 35.20 & 41.90 & 38.30 & 14.80\% & 41.68 & 99.20\% & 40.45 & 67.80\% & 43.18 & 72.00\% \\
     & Qwen-VL-2 & 35.16 & 38.02 & 36.57 & 14.80\% & 37.84 & 99.20\% & 35.85 & 67.80\% & 38.25 & 72.00\% \\
     & GPT4-o & 39.70 & 38.21 & 40.06 & 14.80\% & 37.85 & 99.20\% & 38.83 & 67.80\% & 40.21 & 72.00\% \\ 
    \midrule
    \multirow{4}{*}{\textbf{Dyn-VQA ch}} & Ds.-VL-Chat & 21.62 & 44.10 & 22.98 & 6.38\% & 42.92 & 95.66\% & 40.99 & 84.26\% & 34.24 & 46.95\% \\
     & Qwen-VL-Max & 32.97 & 51.24 & 34.23 & 6.38\% & 50.86 & 95.66\% & 48.24 & 84.26\% & 43.33 & 46.95\% \\
     & Qwen-VL-2 & 32.78 & 50.74 & 34.02 & 6.38\% & 50.48 & 95.66\% & 48.19 & 84.26\% & 43.05 & 46.95\% \\
     & GPT4-o & 41.91 & 56.31 & 42.53 & 6.38\% & 56.31 & 95.66\% & 54.49 & 84.26\% & 48.95 & 46.95\% \\ 
    \midrule
    \multirow{4}{*}{\textbf{Dyn-VQA en}} & Ds.-VL-Chat & 25.58 & 38.10 & 27.19 & 14.13\% & 36.86 & 89.79\% & 36.32 & 76.08\% & 29.44 & 29.51\% \\
     & Qwen-VL-Max & 37.19 & 43.98 & 38.32 & 14.13\% & 43.09 & 89.79\% & 42.78 & 76.08\% & 39.48 & 29.51\% \\
     & Qwen-VL-2 & 37.12 & 44.20 & 37.17 & 14.13\% & 42.47 & 89.79\% & 42.32 & 76.08\% & 40.07 & 29.51\% \\
     & GPT4-o & 45.41 & 50.93 & 45.24 & 14.13\% & 49.88 & 89.79\% & 48.75 & 76.08\% & 47.14 & 29.51\% \\ 
    \midrule
    \multirow{4}{*}{\textbf{NoCaps}} & Ds.-VL-Chat & 63.67 & 59.81 & 63.67 & 0.00\% & 61.23 & 38.40\% & 63.67 & 0.00\% & 63.67 & 0.00\% \\
     & Qwen-VL-Max & 62.10 & 49.66 & 62.10 & 0.00\% & 57.09 & 38.40\% & 62.10 & 0.00\% & 62.10 & 0.00\% \\
     & Qwen-VL-2 & 62.10 & 49.93 & 62.10 & 0.00\% & 56.93 & 38.40\% & 62.10 & 0.00\% & 62.10 & 0.00\% \\
     & GPT4-o & 61.43 & 63.98 & 61.43 & 0.00\% & 62.12 & 38.40\% & 61.43 & 0.00\% & 61.43 & 0.00\% \\ 
    \midrule
    \multirow{4}{*}{\textbf{Visual7W}} & Ds.-VL-Chat & 58.34 & 57.29 & 57.26 & 31.36\% & 57.85 & 35.37\% & 58.13 & 2.96\% & 58.28 & 0.52\% \\
     & Qwen-VL-Max & 58.37 & 55.51 & 62.11 & 31.36\% & 57.10 & 35.37\% & 58.25 & 2.96\% & 58.30 & 0.52\% \\
     & Qwen-VL-2 & 58.16 & 54.41 & 62.19 & 31.36\% & 56.66 & 35.37\% & 57.85 & 2.96\% & 58.02 & 0.52\% \\
     & GPT4-o & 52.96 & 47.06 & 51.82 & 31.36\% & 50.87 & 35.37\% & 52.89 & 2.96\% & 52.87 & 0.52\% \\ 
    \midrule
    \midrule
    \multirow{4}{*}{\textbf{Mix}} & Ds.-VL-Chat & 34.96 & 45.18 & 35.71 & 12.67\% & 45.08 & 76.83\% & 43.35 & 49.33\% & 42.20 & 38.33\% \\
     & Qwen-VL-Max & 46.54 & 49.26 & 47.30 & 12.67\% & \uline{50.64} & 76.83\% & \uline{51.06} & 49.33\% & 52.05 & 38.33\% \\
     & Qwen-VL-2 & 46.36 & 47.89 & 47.46 & 12.67\% & \uline{49.31} & 76.83\% & \uline{49.29} & 49.33\% & 51.41 & 38.33\% \\
     & GPT4-o & 51.44 & 52.90 & 50.57 & 12.67\% & \uline{54.10} & 76.83\% & \uline{52.97} & 49.33\% & 55.27 & 38.33\% \\
    \bottomrule
    \end{tabular}
    }
    \caption{Knowledge Boundary model (Qwen-VL-7B-Chat) as a surrogate boundary identifier for other VLLMs. }
    \label{main_results_llm_table}
\end{table*}

Third, on the first four datasets where RAG can (greatly) enhance the VQA performance, we show that with our $HKB$ and $SKB$, the performance is close to that achieved with the ``All RAG'' setting. For example, with the $SKB$ model, Qwen-VL-Chat archives a 32.06 LLM score on the Dyn-VQA (en) dataset with 76.08\% RAG ratio, whereas the ``All RAG'' setting achieves 34.93. With the $HKB$ model, Qwen-VL-Chat exceeds the ``All RAG'' setting on Private VQA, even though we note that ``All RAG'' is a strong setting on this data. 

At last, on the NoCaps and Visual7W datasets where VLLMs can perform well without RAG and RAG tends to supply noise, our method can identify a much lower search ratio. Specifically, the search ratio from $SKB$ is close to or equal to zero.

