\clearpage
\newpage

\section{Appendix}
\label{appendix}


\subsection{Training Examples}
\label{training_example_appendix_section}

\paragraph{Hard Knowledge Boundary} Query $\bm{q}$, together with prompts $P_h$ (in blue)\footnote{where <ST\_*> means optional special tokens to specify the position of $\bm{q}$ and indicate the output starting position after <ST\_2>. The detailed format of <ST\_*> and <Image> tokens might need to be modified according to different VL model input formats.}, will be constructed into a training sample $\bm{x}(\bm{q}, P_h)$ as follows:
\begin{framed}
\noindent\textcolor[rgb]{0.392, 0.471, 0.871}{You are an assistant capable of deciding whether a search is needed in a multimodal question-answering scenario. Below, I will provide you with a multimodal question that includes a text question and an image link.
Please respond with "true" or "false," indicating whether a search is necessary (true) or not (false) to answer this multimodal question.
<ST\_1> \\
Text question: \textcolor{black}{$\bm{q_t}$} \\
<Image>: \textcolor{black}{$\bm{q_i}$} \\
<ST\_2>}
\end{framed}


\paragraph{Soft Knowledge Boundary} Query $\bm{q}$, together with prompts $P_s$ (in blue), will be constructed into a training sample $\bm{x}(\bm{q}, P_s)$ as follows: 

\begin{framed}
\noindent\textcolor[rgb]{0.392, 0.471, 0.871}{You are an assistant capable of deciding whether a search is needed in a multimodal question-answering scenario. Below, I will provide you with a multimodal question that includes a text question and an image link.
Please respond with a score ranging from 1.0 to 5.0 indicating whether a search is necessary or not to answer this multimodal question. \\ \\
Follow these guidelines for scoring: \\
- Your score has to be between 1.0 and 5.0, where 1.0 stands for an unnecessary search and 5.0 stands for a necessary search.  \\
- The score does not have to be integer. \\
Example Response: \\
4.0 \\ \\
<ST\_1> \\
Text question: \textcolor{black}{$\bm{q_t}$} \\
<Image>: \textcolor{black}{$\bm{q_i}$} \\
<ST\_2> \\
Your score: }
\end{framed}


\subsection{Training Dataset Description}
\label{training_data_description}

Below is a brief description of each dataset (for training).

\paragraph{InfoSeek} is designed to assess the capability of models to seek and incorporate external information for question answering. It features a variety of queries that necessitate fact retrieval and reasoning that go beyond the provided context.

\paragraph{OK-VQA} is a dataset where images are paired with open-ended questions that require answers stemming from general knowledge that extends beyond the image alone.

\paragraph{VQAv2.0} is a comprehensive VQA dataset that requires interpretation or understanding of the visual content. It features a diverse and balanced range of answers.

\paragraph{MMBench} is a benchmarking suite for evaluating multi-modal understanding, ensuring that multi-modal machine learning systems can effectively process and synthesize data from different sources.

\paragraph{MME} is focused on tasks related to multi-modal entity recognition and extraction. The dataset contains annotations of text and images with multi-modal entities that need to be identified or linked.

\paragraph{Human-Labeled} A group of annotators is asked to annotate whether RAG can help solve a VQA sample. We construct this data to form a reference setting. 



\subsection{Training Details and Hyperparameters}
\label{hyper_appendix_section}


\begin{table}[tb]
\centering
\small
\scalebox{1.0}{
\begin{tabular}{ll} 
\toprule
Base Model & Qwen- \& DeepSeek-VL-7B-Chat \\
\midrule
LoRA & $Q,K,V$ \\
\midrule 
LoRA Rank & 8 \\
\midrule
LoRA Alpha & 32 \\
\midrule
Learning Rate & 1e-4 \\ 
\midrule
Optimizer & AdamW \\ 
\midrule
LR Scheduler & Linear \\
\midrule
Precision & bf16 \\
\midrule 
Batch Size & 1 \\
\midrule 
GPU & NVIDIA A100-SXM4-80GB \\
\bottomrule
\end{tabular}
}
\caption{Detailed hyperparameters.}
\label{hyper_table}
\end{table}

Recall that our methods need to train a VLLM, parameterized by $\phi$, as a Knowledge Boundary model discussed in Sec.~\ref{training_section}. In experiments, we adopt LoRA \cite{hu2021lora} to optimize $\phi$ and the related hyperparameters are shown in Table~\ref{hyper_table}. We note that our method does not rely heavily on tuning hyperparameters. We just choose intuitive values and it works fairly well. 




\subsection{Main Results on DeepSeek}
\label{main_results_ds_section}

We present our main results of DeepSeek-VL-7B-Chat in Table~\ref{main_results_ds_7b_table}. For both the $HKB$ and $SKB$ methods, DeepSeek performs more confidently than Qwen, and it tends to predict a lower ratio of resorting to RAG. On the Mix dataset, DeepSeek also well maintains the performance with the $SKB$ method compared to the All RAG setting and outperforms the Prompt-based method. In addition, compared to Qwen, DeepSeek better utilizes human-labeled data to depict the knowledge boundary and obtains the best result among all settings.

\begin{table*}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{llcccccccc|cc} 
    \toprule
    \textbf{Dataset} & \textbf{Metric} & \begin{tabular}[c]{@{}c@{}}\textbf{No }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{All }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Prompt-}\\\textbf{based}\end{tabular} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} & \textbf{Human} & \textbf{\%} \\ 
    \midrule
    \multirow{2}{*}{\textbf{Life VQA}} & \textbf{LLM} & 25.81 & 47.35 & 33.36 & 30.20\% & 35.81 & 46.31\% & 42.18 & 73.83\% & 47.21 & 96.64\% \\
     & \textbf{Acc.} & 10.82 & 36.79 & 20.84 & 30.20\% & 24.81 & 46.31\% & 31.93 & 73.83\% & 36.79 & 96.64\% \\ 
    \midrule
    \multirow{2}{*}{\textbf{Private VQA}} & \textbf{LLM} & 22.80 & 27.28 & 23.93 & 21.20\% & 25.45 & 27.60\% & 26.03 & 56.40\% & 27.08 & 88.20\% \\
     & \textbf{Acc.} & 15.51 & 19.75 & 16.38 & 21.20\% & 17.70 & 27.60\% & 17.75 & 56.40\% & 19.57 & 88.20\% \\ 
    \midrule
    \multirow{2}{*}{\textbf{Dyn-VQA ch}} & \textbf{LLM} & 21.32 & 44.20 & 25.63 & 12.48\% & 28.29 & 27.00\% & 37.81 & 79.10\% & 43.54 & 97.42\% \\
     & \textbf{Acc.} & 20.74 & 46.91 & 24.15 & 12.48\% & 28.07 & 27.00\% & 41.18 & 79.10\% & 46.23 & 97.42\% \\ 
    \midrule
    \multirow{2}{*}{\textbf{Dyn-VQA en}} & \textbf{LLM} & 24.90 & 38.36 & 25.63 & 12.73\% & 29.41 & 33.57\% & 32.31 & 60.56\% & 37.77 & 96.78\% \\
     & \textbf{Acc.} & 24.37 & 43.28 & 26.51 & 12.73\% & 30.22 & 33.57\% & 35.49 & 60.56\% & 43.01 & 96.78\% \\ 
    \midrule
    \multirow{2}{*}{\textbf{NoCaps}} & \textbf{LLM} & 63.10 & 59.39 & 62.95 & 2.00\% & 63.12 & 0.20\% & 61.40 & 32.40\% & 62.50 & 6.20\% \\
     & \textbf{Acc.} & 43.89 & 40.45 & 43.62 & 2.00\% & 43.88 & 0.20\% & 42.50 & 32.40\% & 43.48 & 6.20\% \\ 
    \midrule
    \multirow{2}{*}{\textbf{Visual7W}} & \textbf{LLM} & 58.54 & 57.68 & 58.17 & 2.44\% & 58.24 & 7.67\% & 58.16 & 10.98\% & 56.98 & 54.70\% \\
     & \textbf{Acc.} & 46.55 & 46.62 & 46.40 & 2.44\% & 46.27 & 7.67\% & 46.55 & 10.98\% & 46.18 & 54.70\% \\ 
    \midrule
    \midrule
    \multirow{2}{*}{\textbf{Mix}} & \textbf{LLM} & 35.07 & 45.37 & 37.17 & 13.50\% & 39.38 & 25.00\% & 42.46 & 54.83\% & 45.63 & 74.50\% \\
     & \textbf{Acc.} & 25.81 & 35.23 & 28.11 & 13.50\% & 29.08 & 25.00\% & 33.23 & 54.83\% & 35.83 & 74.50\% \\
    \bottomrule
    \end{tabular}
    }
    \caption{Main results of DeepSeek-VL-7B-Chat.}
    \label{main_results_ds_7b_table}
\end{table*}



\subsection{Supplementary Results of ``Surrogate Boundary'' Experiments}
\label{supplementary_results_of_surrogate}

We provide the supplementary experimental results for Sec~\ref{plugin_section} where the token accuracy metrics are shown in Table~\ref{main_results_acc_table}. It can be concluded that similar conclusions can be drawn as in Sec~\ref{plugin_section}. The experiment where DeepSeek-VL-7B-Chat is trained for surrogate boundary prediction is shown in Table~\ref{main_results_ds_llm_table} and \ref{main_results_ds_acc_table}.


\begin{table*}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{llcccccccc|cc} 
\toprule
 & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Metric:}\\\textbf{Acc.}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textbf{No}\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{All }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Prompt-}\\\textbf{based}\end{tabular} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} & \textbf{Human} & \textbf{\%} \\ 
\midrule
\multirow{4}{*}{\textbf{Life VQA}} & Ds.-VL-Chat & 10.82 & 36.79 & 14.12 & 12.75\% & 36.12 & 96.64\% & 30.97 & 61.74\% & 30.50 & 71.14\% \\
 & Qwen-VL-Max & 24.21 & 42.30 & 27.66 & 12.75\% & 41.96 & 96.64\% & 38.37 & 61.74\% & 38.20 & 71.14\% \\
 & Qwen-VL-2 & 23.06 & 41.05 & 27.64 & 12.75\% & 40.71 & 96.64\% & 37.27 & 61.74\% & 37.05 & 71.14\% \\
 & GPT4-o & 31.72 & 40.85 & 32.81 & 12.75\% & 40.85 & 96.64\% & 38.47 & 61.74\% & 41.88 & 71.14\% \\ 
\midrule
\multirow{4}{*}{\textbf{Private VQA}} & Ds.-VL-Chat & 15.51 & 19.75 & 16.65 & 14.80\% & 19.75 & 99.20\% & 18.20 & 67.80\% & 18.51 & 72.00\% \\
 & Qwen-VL-Max & 27.93 & 28.14 & 28.08 & 14.80\% & 28.29 & 99.20\% & 27.68 & 67.80\% & 28.96 & 72.00\% \\
 & Qwen-VL-2 & 27.69 & 30.72 & 27.75 & 14.80\% & 30.87 & 99.20\% & 28.96 & 67.80\% & 31.13 & 72.00\% \\
 & GPT4-o & 31.12 & 27.02 & 30.88 & 14.80\% & 26.87 & 99.20\% & 27.72 & 67.80\% & 29.10 & 72.00\% \\ 
\midrule
\multirow{4}{*}{\textbf{Dyn-VQA ch}} & Ds.-VL-Chat & 20.74 & 46.91 & 22.37 & 6.38\% & 46.05 & 95.66\% & 44.13 & 84.26\% & 33.60 & 46.95\% \\
 & Qwen-VL-Max & 31.53 & 46.73 & 33.53 & 6.38\% & 46.38 & 95.66\% & 44.82 & 84.26\% & 39.79 & 46.95\% \\
 & Qwen-VL-2 & 31.52 & 46.70 & 33.52 & 6.38\% & 46.28 & 95.66\% & 44.69 & 84.26\% & 39.85 & 46.95\% \\
 & GPT4-o & 36.46 & 51.27 & 37.32 & 6.38\% & 50.85 & 95.66\% & 49.4 & 84.26\% & 42.45 & 46.95\% \\ 
\midrule
\multirow{4}{*}{\textbf{Dyn-VQA en}} & Ds.-VL-Chat & 24.37 & 43.28 & 26.80 & 14.13\% & 42.08 & 89.79\% & 40.61 & 76.08\% & 31.67 & 29.51\% \\
 & Qwen-VL-Max & 37.54 & 45.27 & 38.03 & 14.13\% & 44.30 & 89.79\% & 43.55 & 76.08\% & 39.40 & 29.51\% \\
 & Qwen-VL-2 & 37.37 & 45.16 & 37.25 & 14.13\% & 43.84 & 89.79\% & 43.48 & 76.08\% & 40.66 & 29.51\% \\
 & GPT4-o & 43.33 & 49.71 & 42.40 & 14.13\% & 48.48 & 89.79\% & 47.66 & 76.08\% & 45.07 & 29.51\% \\ 
\midrule
\multirow{4}{*}{\textbf{NoCaps}} & Ds.-VL-Chat & 43.89 & 40.45 & 43.89 & 0.00\% & 42.76 & 38.40\% & 43.89 & 0.00\% & 43.89 & 0.00\% \\
 & Qwen-VL-Max & 37.47 & 34.55 & 37.47 & 0.00\% & 36.75 & 38.40\% & 37.47 & 0.00\% & 37.47 & 0.00\% \\
 & Qwen-VL-2 & 37.26 & 34.61 & 37.26 & 0.00\% & 36.35 & 38.40\% & 37.26 & 0.00\% & 37.26 & 0.00\% \\
 & GPT4-o & 32.12 & 36.25 & 32.12 & 0.00\% & 33.22 & 38.40\% & 32.12 & 0.00\% & 32.12 & 0.00\% \\ 
\midrule
\multirow{4}{*}{\textbf{Visual7W}} & Ds.-VL-Chat & 46.55 & 46.62 & 46.29 & 31.36\% & 46.03 & 35.37\% & 46.58 & 2.96\% & 46.55 & 0.52\% \\
 & Qwen-VL-Max & 46.07 & 44.44 & 48.63 & 31.36\% & 45.16 & 35.37\% & 46.13 & 2.96\% & 46.07 & 0.52\% \\
 & Qwen-VL-2 & 45.94 & 43.86 & 48.47 & 31.36\% & 45.06 & 35.37\% & 45.99 & 2.96\% & 45.94 & 0.52\% \\
 & GPT4-o & 41.59 & 37.16 & 40.09 & 31.36\% & 39.41 & 35.37\% & 41.80 & 2.96\% & 41.48 & 0.52\% \\ 
\midrule
\midrule
\multirow{4}{*}{\textbf{Mix}} & Ds.-VL-Chat & 25.81 & 35.23 & 26.55 & 12.67\% & \textbf{35.38} & 76.83\% & 33.06 & 49.33\% & 32.73 & 38.33\% \\
 & Qwen-VL-Max & 32.35 & 34.78 & 33.00 & 12.67\% & \uline{35.48} & 76.83\% & \uline{34.84} & 49.33\% & 35.51 & 38.33\% \\
 & Qwen-VL-2 & 32.59 & 35.56 & 33.27 & 12.67\% & \uline{36.29} & 76.83\% & \uline{35.62} & 49.33\% & 36.33 & 38.33\% \\
 & GPT4-o & 34.52 & 35.96 & 33.99 & 12.67\% & 35.90 & 76.83\% & 35.86 & 49.33\% & 36.49 & 38.33\% \\
\bottomrule
\end{tabular}
}
\caption{Knowledge Boundary model (Qwen-VL-7B-Chat) as a surrogate boundary identifier for other VLLMs. Results evaluated by token accuracy.}
\label{main_results_acc_table}
\end{table*}


\begin{table*}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{llcccccccc|cc} 
\toprule
 & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Metric:}\\\textbf{LLM}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textbf{No}\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{All }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Prompt-}\\\textbf{based}\end{tabular} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} & \textbf{Human} & \textbf{\%} \\ 
\midrule
\multirow{4}{*}{\textbf{Life VQA}} & Qwen-VL-Chat & 31.14 & 42.85 & 33.62 & 30.20\% & 37.08 & 46.31\% & 41.78 & 73.83\% & 43.05 & 96.64\% \\
 & Qwen-VL-Max & 44.09 & 56.64 & 46.51 & 30.20\% & 48.59 & 46.31\% & 54.16 & 73.83\% & 56.51 & 96.64\% \\
 & Qwen-VL-2 & 42.95 & 54.23 & 45.00 & 30.20\% & 48.66 & 46.31\% & 53.36 & 73.83\% & 54.23 & 96.64\% \\
 & GPT4-o & 47.45 & 56.38 & 53.15 & 30.20\% & 54.16 & 46.31\% & 55.37 & 73.83\% & 56.11 & 96.64\% \\ 
\midrule
\multirow{4}{*}{\textbf{Private VQA}} & Qwen-VL-Chat & 24.45 & 26.16 & 25.21 & 21.20\% & 25.35 & 27.60\% & 26.08 & 56.40\% & 26.01 & 88.20\% \\
 & Qwen-VL-Max & 36.84 & 42.97 & 36.45 & 21.20\% & 39.16 & 27.60\% & 42.26 & 56.40\% & 43.73 & 88.20\% \\
 & Qwen-VL-2 & 36.76 & 38.20 & 36.65 & 21.20\% & 38.52 & 27.60\% & 39.37 & 56.40\% & 39.03 & 88.20\% \\
 & GPT4-o & 40.13 & 38.72 & 39.15 & 21.20\% & 40.59 & 27.60\% & 40.76 & 56.40\% & 39.56 & 88.20\% \\ 
\midrule
\multirow{4}{*}{\textbf{Dyn-VQA ch}} & Qwen-VL-Chat & 37.73 & 44.68 & 38.44 & 12.48\% & 40.31 & 27.00\% & 39.63 & 79.10\% & 43.85 & 97.42\% \\
 & Qwen-VL-Max & 32.67 & 50.85 & 35.11 & 12.48\% & 37.20 & 27.00\% & 46.62 & 79.10\% & 50.32 & 97.42\% \\
 & Qwen-VL-2 & 45.95 & 50.91 & 46.33 & 12.48\% & 48.03 & 27.00\% & 46.42 & 79.10\% & 50.13 & 97.42\% \\
 & GPT4-o & 42.10 & 56.51 & 44.55 & 12.48\% & 44.80 & 27.00\% & 51.99 & 79.10\% & 56.23 & 97.42\% \\ 
\midrule
\multirow{4}{*}{\textbf{Dyn-VQA en}} & Qwen-VL-Chat & 22.07 & 35.23 & 23.58 & 12.73\% & 26.91 & 33.57\% & 30.06 & 60.56\% & 34.27 & 96.78\% \\
 & Qwen-VL-Max & 19.41 & 39.90 & 22.86 & 12.73\% & 24.71 & 33.57\% & 36.01 & 60.56\% & 39.44 & 96.78\% \\
 & Qwen-VL-2 & 37.90 & 44.29 & 38.58 & 12.73\% & 40.45 & 33.57\% & 40.11 & 60.56\% & 43.58 & 96.78\% \\
 & GPT4-o & 32.73 & 51.17 & 35.25 & 12.73\% & 37.36 & 33.57\% & 47.15 & 60.56\% & 50.65 & 96.78\% \\ 
\midrule
\multirow{4}{*}{\textbf{NoCaps}} & Qwen-VL-Chat & 50.46 & 30.41 & 50.00 & 2.00\% & 50.48 & 0.20\% & 44.43 & 32.40\% & 49.48 & 6.20\% \\
 & Qwen-VL-Max & 62.04 & 49.63 & 61.82 & 2.00\% & 61.92 & 0.20\% & 57.63 & 32.40\% & 61.16 & 6.20\% \\
 & Qwen-VL-2 & 61.88 & 49.84 & 61.66 & 2.00\% & 61.78 & 0.20\% & 57.44 & 32.40\% & 60.92 & 6.20\% \\
 & GPT4-o & 61.58 & 64.51 & 61.68 & 2.00\% & 61.56 & 0.20\% & 62.00 & 32.40\% & 61.68 & 6.20\% \\ 
\midrule
\multirow{4}{*}{\textbf{Visual7W}} & Qwen-VL-Chat & 55.53 & 54.52 & 55.45 & 2.44\% & 55.76 & 7.67\% & 55.31 & 10.98\% & 55.01 & 54.70\% \\
 & Qwen-VL-Max & 61.72 & 58.16 & 61.59 & 2.44\% & 61.27 & 7.67\% & 61.08 & 10.98\% & 59.04 & 54.70\% \\
 & Qwen-VL-2 & 61.81 & 58.07 & 61.58 & 2.44\% & 61.25 & 7.67\% & 61.23 & 10.98\% & 58.78 & 54.70\% \\
 & GPT4-o & 53.34 & 47.44 & 53.30 & 2.44\% & 52.71 & 7.67\% & 52.70 & 10.98\% & 49.97 & 54.70\% \\ 
\midrule
\midrule
\multirow{4}{*}{\textbf{Mix}} & Qwen-VL-Chat & 34.58 & 39.03 & 35.54 & 13.50\% & 37.12 & 25.00\% & \uline{39.76} & 54.83\% & 42.61 & 74.50\% \\
 & Qwen-VL-Max & 46.13 & 49.02 & 46.47 & 13.50\% & 47.43 & 25.00\% & 48.98 & 54.83\% & 51.39 & 74.50\% \\
 & Qwen-VL-2 & 46.26 & 47.84 & 46.64 & 13.50\% & \uline{48.17} & 25.00\% & \uline{48.55} & 54.83\% & 50.13 & 74.50\% \\
 & GPT4-o & 51.21 & 52.70 & 51.81 & 13.50\% & 52.28 & 25.00\% & 52.04 & 54.83\% & 53.42 & 74.50\% \\
\bottomrule
\end{tabular}
}
\caption{Knowledge Boundary model (DeepSeek-VL-7B-Chat) as a surrogate boundary identifier for other VLLMs. Results evaluated by LLM.}
\label{main_results_ds_llm_table}
\end{table*}

\begin{table*}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{llcccccccc|cc} 
\toprule
 & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Metric:}\\\textbf{Acc.}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\textbf{No}\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{All }\\\textbf{RAG}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Prompt-}\\\textbf{based}\end{tabular} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} & \textbf{Human} & \textbf{\%} \\ 
\midrule
\multirow{4}{*}{\textbf{Life VQA}} & Qwen-VL-Chat & 17.80 & 36.11 & 23.68 & 30.20\% & 28.05 & 46.31\% & 34.43 & 73.83\% & 36.78 & 96.64\% \\
 & Qwen-VL-Max & 25.42 & 42.30 & 30.09 & 30.20\% & 32.83 & 46.31\% & 38.38 & 73.83\% & 42.07 & 96.64\% \\
 & Qwen-VL-2 & 25.29 & 41.05 & 29.77 & 30.20\% & 33.75 & 46.31\% & 38.48 & 73.83\% & 40.83 & 96.64\% \\
 & GPT4-o & 31.72 & 40.85 & 36.49 & 30.20\% & 38.53 & 46.31\% & 40.01 & 73.83\% & 42.19 & 96.64\% \\ 
\midrule
\multirow{4}{*}{\textbf{Private VQA}} & Qwen-VL-Chat & 16.26 & 18.40 & 17.28 & 21.20\% & 18.11 & 27.60\% & 18.34 & 56.40\% & 18.90 & 88.20\% \\
 & Qwen-VL-Max & 27.12 & 28.14 & 26.77 & 21.20\% & 27.94 & 27.60\% & 28.18 & 56.40\% & 28.31 & 88.20\% \\
 & Qwen-VL-2 & 27.04 & 30.72 & 27.89 & 21.20\% & 28.78 & 27.60\% & 29.94 & 56.40\% & 30.95 & 88.20\% \\
 & GPT4-o & 31.12 & 27.02 & 29.74 & 21.20\% & 30.78 & 27.60\% & 29.73 & 56.40\% & 28.24 & 88.20\% \\ 
\midrule
\multirow{4}{*}{\textbf{Dyn-VQA ch}} & Qwen-VL-Chat & 37.37 & 45.16 & 38.25 & 12.48\% & 39.83 & 27.00\% & 39.37 & 79.10\% & 44.84 & 97.42\% \\
 & Qwen-VL-Max & 31.66 & 46.70 & 34.29 & 12.48\% & 35.11 & 27.00\% & 42.80 & 79.10\% & 46.35 & 97.42\% \\
 & Qwen-VL-2 & 43.33 & 49.71 & 43.68 & 12.48\% & 45.47 & 27.00\% & 45.17 & 79.10\% & 49.28 & 97.42\% \\
 & GPT4-o & 36.46 & 51.27 & 38.75 & 12.48\% & 39.78 & 27.00\% & 46.96 & 79.10\% & 51.13 & 97.42\% \\ 
\midrule
\multirow{4}{*}{\textbf{Dyn-VQA en}} & Qwen-VL-Chat & 25.64 & 41.87 & 27.33 & 12.73\% & 31.66 & 33.57\% & 35.00 & 60.56\% & 41.63 & 96.78\% \\
 & Qwen-VL-Max & 23.41 & 43.06 & 26.81 & 12.73\% & 29.04 & 33.57\% & 39.36 & 60.56\% & 42.57 & 96.78\% \\
 & Qwen-VL-2 & 37.54 & 45.27 & 37.52 & 12.73\% & 40.05 & 33.57\% & 40.50 & 60.56\% & 44.95 & 96.78\% \\
 & GPT4-o & 31.66 & 46.73 & 34.25 & 12.73\% & 34.93 & 33.57\% & 42.96 & 60.56\% & 46.39 & 96.78\% \\ 
\midrule
\multirow{4}{*}{\textbf{NoCaps}} & Qwen-VL-Chat & 40.50 & 30.72 & 40.39 & 2.00\% & 40.49 & 0.20\% & 37.88 & 32.40\% & 39.92 & 6.20\% \\
 & Qwen-VL-Max & 37.47 & 34.55 & 37.44 & 2.00\% & 37.42 & 0.20\% & 36.47 & 32.40\% & 37.22 & 6.20\% \\
 & Qwen-VL-2 & 37.26 & 34.61 & 37.30 & 2.00\% & 37.21 & 0.20\% & 36.21 & 32.40\% & 37.04 & 6.20\% \\
 & GPT4-o & 32.12 & 36.25 & 32.23 & 2.00\% & 32.12 & 0.20\% & 32.96 & 32.40\% & 32.35 & 6.20\% \\ 
\midrule
\multirow{4}{*}{\textbf{Visual7W}} & Qwen-VL-Chat & 44.34 & 44.94 & 44.26 & 2.44\% & 44.64 & 7.67\% & 44.86 & 10.98\% & 45.11 & 54.70\% \\
 & Qwen-VL-Max & 49.41 & 45.13 & 49.39 & 2.44\% & 49.28 & 7.67\% & 48.13 & 10.98\% & 46.04 & 54.70\% \\
 & Qwen-VL-2 & 49.71 & 44.19 & 49.48 & 2.44\% & 49.58 & 7.67\% & 48.43 & 10.98\% & 45.51 & 54.70\% \\
 & GPT4-o & 41.59 & 37.16 & 41.76 & 2.44\% & 40.96 & 7.67\% & 40.91 & 10.98\% & 39.10 & 54.70\% \\ 
\midrule
\midrule
\multirow{4}{*}{\textbf{Mix}} & Qwen-VL-Chat & 26.13 & 32.39 & 28.00 & 13.50\% & 29.55 & 25.00\% & \uline{32.46} & 54.83\% & 34.06 & 74.50\% \\
 & Qwen-VL-Max & 32.35 & 34.78 & 32.91 & 13.50\% & 33.17 & 25.00\% & \uline{35.12} & 54.83\% & 35.96 & 74.50\% \\
 & Qwen-VL-2 & 32.45 & 35.56 & 33.51 & 13.50\% & 33.86 & 25.00\% & \uline{35.88} & 54.83\% & 36.63 & 74.50\% \\
 & GPT4-o & 34.52 & 35.96 & 35.17 & 13.50\% & 35.77 & 25.00\% & 35.86 & 54.83\% & 36.24 & 74.50\% \\
\bottomrule
\end{tabular}
}
\caption{Knowledge Boundary model (DeepSeek-VL-7B-Chat) as a surrogate boundary identifier for other VLLMs. Results evaluated by token accuracy.}
\label{main_results_ds_acc_table}
\end{table*}



\subsection{Supplementary Results on MMMU Dataset}
\label{mmmu_appendix_section}

In this section, we show the experimental results of our methods on a challenging dataset, MMMU\footnote{We converted the dataset's original multiple-choice format into a conventional VQA format to ensure consistency with the aforementioned experimental settings.} \cite{yue2023mmmu} in Table~\ref{mmmu_results_llm_table}. MMMU is a dataset containing VQA samples demanding college-level subject knowledge and deliberate reasoning, and it is hard to verify the knowledge boundary that our methods depict by simply adopting RAG.  

The results in Table~\ref{mmmu_results_llm_table} show the Knowledge Boundary model trained by human-labeled data helps achieve the best performance. It verifies that the aforementioned Human-labeled training data is effective. In addition, we show that our methods also exhibit substantial potential within this setting, in which both the $HKB$ and $SKB$ models predict a high search ratio over MMMU. We contend that the suboptimal performance of this dataset arises because it lies beyond the knowledge boundaries, that are challenging to validate using RAG, as delineated by the white dashed lines in Fig.~\ref{outline}. We present the performance of each of the 30 subjects in the MMMU validation set in Fig~\ref{mmmu_radar_fig}. The first row shows the LLM evaluation results and the second shows the token accuracy metric. We can see that in most subjects ``Human'' setting succeeds in obtaining a higher performance than both ``All RAG'' and ``No RAG'' settings. 


\begin{table*}
\centering
\scalebox{0.85}{
\begin{tabular}{llcccccccc} 
\toprule
 &  & \textbf{No RAG} & \textbf{All RAG} & \textbf{Human} & \textbf{\%} & \textbf{HKB} & \textbf{\%} & \textbf{SKB} & \textbf{\%} \\ 
\midrule
\multirow{4}{*}{\textbf{MMMU}} & Qwen-VL-Chat & 20.12 & 20.28 & \textbf{21.24} & 6.88\% & \uline{20.35} & 97.08\% & 20.18 & 61.26\% \\
 & Qwen-VL-Max & 51.33 & 41.37 & \textbf{52.67} & 6.88\% & 41.46 & 97.08\% & 44.40 & 61.26\% \\
 & Qwen-VL-2 & 51.45 & 42.39 & \textbf{51.93} & 6.88\% & 42.54 & 97.08\% & 45.61 & 61.26\% \\
 & GPT4-o & 56.60 & 56.64 & \textbf{57.36} & 6.88\% & \uline{56.92} & 97.08\% & \uline{56.91} & 61.26\% \\
\bottomrule
\end{tabular}
}
\caption{Results evaluated by LLM on MMMU validation set.}
\label{mmmu_results_llm_table}
\end{table*}


\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.49\linewidth]{figs/Qwen-VL-Max_MMMU_val_LLM_Eval.pdf}
    \includegraphics[width=0.49\linewidth]{figs/Qwen-VL-2_MMMU_val_LLM_Eval.pdf}
    \includegraphics[width=0.49\linewidth]{figs/Qwen-VL-Max_MMMU_val_Acc.pdf}
    \includegraphics[width=0.49\linewidth]{figs/Qwen-VL-2_MMMU_val_Acc.pdf}
    \caption{Qwen-VL-Max and Qwen-VL-2 performance on MMMU validation set with Knowledge Boundary model trained on Human-labeled data.}
    \label{mmmu_radar_fig}
\end{figure*}
