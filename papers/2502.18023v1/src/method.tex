\section{Method}
\label{method}

\begin{figure*}[tb]
\centering
\includegraphics[width=0.8\linewidth]{figs/method.pdf}
\caption{Method illustration of training a Knowledge Boundary model.}
\label{method_fig}
\end{figure*}


We propose a method with two variants that fine-tunes a VLLM, which can depict the \textit{hard} or \textit{soft} knowledge boundary of VLLMs. The proposed method relies only on (V)LLMs and does not require manual annotation. 
In the following sections, we first introduce the background and necessary notations. Then we give details on constructing two types of datasets for fine-tuning a VLLM for knowledge boundary approximation. 


\subsection{Background}
Consider a Visual Question Answering query $\bm{q}$ with gold text answer $\bm{a}$, where $\bm{q}$ contains image(s) $\bm{q_i}$ and a text query  $\bm{q_t}$. Also, contexts $\bm{k}$ related to $\bm{q}$ can be retrieved from a given corpus, where $\bm{k}$ can refer to the collection of both texts and images. Given a VL model, parameterized by $\theta$, we can answer the query with or without RAG by running decoding $(Dec)$ on the model:
\begin{equation}
\label{norag_all_rag}
    \begin{aligned}
    \bm{y_n} &= Dec_\theta(\bm{y}|\bm{q}) \\
    \bm{y_r} &= Dec_\theta(\bm{y}|\bm{q}, \bm{k})
    \end{aligned}
\end{equation}
where $\bm{k}$ might also contain prompts connecting related content and it is omitted here for simplicity.

It is acknowledged that VLLMs have a limited knowledge scope \cite{lin-byrne-2022-retrieval, wu2022multi}, denoted as $S$, and the boundary is a rather vague concept and is hard to depict accurately. % For a query that tends to lie beyond the boundaries of the model's knowledge, for example, a query regarding the latest released movie, $\bm{y_r}$ usually gives better quality than $\bm{y_n}$ and requires more time and resources. 

\subsection{Sampling}
\label{sampling_section}

To approximate whether a query $\bm{q}$ should lie in VLLMs' knowledge scope $S$, we run repeated sampling of a VLLM and collect its outputs. The sampling methods include but are not limited to, top-p sampling and top-k sampling. These sampling-based methods are widely adopted to study the model's knowledge boundary problems \cite{li2025refine, zhang2024exploring, cheng2024can}. By running $R$ times sampling, we obtain $R$ outputs given query $\bm{q}$:
\begin{align}\label{sampling}
\bm{y^{(i)}} &= Dec_\theta(\bm{y}|\bm{q}), i \in \{1, 2, ..., R\}
\end{align}
After obtaining the $R$ predictions, a text LLM is prompted\footnote{The prompt is referenced from \citet{Liu_LlamaIndex_2022}. Please refer to our code for a detailed definition.} to evaluate each prediction $y^{(i)}$ where the gold answer is also given. Subsequently a score $s_i \in [s_w, s_c]$ is provided by this text LLM. We define the score range within $s_w$ and $s_c$, where $s_c$ indicates a perfectly correct answer and $s_w$ indicates a wrong answer. Then an average score is calculated over $R$ scores, indicating the overall performance of this query:
\begin{align}\label{score}
s &= mean(s_i), i \in \{1, 2, ..., R\}
\end{align}
and we note that $s$ is also $\in [s_w, s_c]$. 

\subsection{Training}
\label{training_section}

The score $s$ is used to construct the knowledge boundary training data. We differentiate our method into two variants. 
A VLLM is adopted to train on the knowledge boundary training data. We denote the parameters by $\phi$.
% \footnote{Our method does not require $\phi$ to be identical to the parameters of the previously sampled VLLM.}.
% However, in our subsequent experiments, they are the same model.}.

\paragraph{Hard Knowledge Boundary} By setting a threshold $\epsilon$, we deem the queries with score $s \geq \epsilon$ inside the knowledge boundary $S$ and the rest outside $S$. The query $\bm{q}$, together with proper prompts $P_h$, will be constructed into a training sample $\bm{x}(\bm{q}, P_h)$ as shown in Sec.~\ref{training_example_appendix_section}. For any $\bm{x}(\bm{q}, P_h)$ in the training dataset, we define the training objective $J_h$ w.r.t. $\phi$ as follows: 
\begin{equation}
\label{objective_hard}
    \begin{aligned}
    J_h (\phi) &= - \sum\limits_{\bm{x}(\bm{q}, P_h): \bm{q} \notin S} \log P_\phi(\text{``True''}|\bm{x}(\bm{q}, P_h)) \\
    &- \sum\limits_{\bm{x}(\bm{q}, P_h): \bm{q} \in S} \log P_\phi(\text{``False''}|\bm{x}(\bm{q}, P_h))
    \end{aligned}
\end{equation}
where $P_\phi(a|b)$ stands for the probability model $\phi$ predicts on $a$ given input $b$. $\phi$ is optimized by minimizing $J_h(\phi)$.


\paragraph{Soft Knowledge Boundary}
Setting a threshold to binarily classify the queries might be an overly rigid method and there is no room for adjustment when the knowledge boundary model performs poorly in possibly unseen scenarios unless we adjust $\epsilon$ and retrain the model. Thus, we also propose a method that can depict a softer boundary. Recall that for query $\bm{q}$, the average score $s$ over $R$ model predictions ranges in $[s_w, s_c]$, where $s_w$ indicates a wrong answer and $s_c$ indicates a correct one. We linearly flip the score, for example, the new score $s^{'}=s_w$ represents a strong tendency for external knowledge while $s^{'}=s_c$ represents a refusal to external knowledge. 


The query $\bm{q}$, together with prompts $P_s$, will be constructed into a training sample $\bm{x}(\bm{q}, P_s)$ as shown in Sec.~\ref{training_example_appendix_section}. For any $\bm{x}(\bm{q}, P_s)$ in the training dataset, we define the training objective as follows:
\begin{align}\label{objective_soft}
J_s(\phi) = - \sum\limits_{\bm{x}(\bm{q}, P_s)} \log P_\phi(s^{'}|\bm{x}(\bm{q}, P_s))
\end{align}
where $\phi$ is optimized by minimizing $J_s(\phi)$.


By optimizing objective \ref{objective_hard}, we get a Hard Knowledge Boundary model $HKB_{\phi}$ that can take a VQA sample and predict a \textit{binary} output ``True'' or ``False'' indicating whether the RAG technique can help solve this query. Similarly, a Soft Knowledge Boundary model $SKB_{\phi}$ that can predict a \textit{soft score}, ranging from $s_w$ to $s_c$, is trained by optimizing objective \ref{objective_soft}:
\begin{equation}
    \begin{aligned}\label{KB}
    HKB_{\phi}(\bm{x}(\bm{q},P_h)) &= \text{True / False} \\
    SKB_{\phi}(\bm{x}(\bm{q},P_s)) &\in [s_w, s_c]
    \end{aligned}
\end{equation}



\subsection{Application of RAG in Our Method}

An indicator function is defined to map the prediction of a Hard/Soft Knowledge Boundary model to a real search decision:
\begin{equation}
\label{search_decision}
    \mathbb{I}(\bm{q}, \bm{k})=\left\{
    \begin{aligned}
    & \bm{k}, \text{if } HKB_{\phi}(\bm{x}(\bm{q},P_h)) == \text{true} \\
    & \quad \text{ or } SKB_{\phi}(\bm{x}(\bm{q},P_s)) \geq \epsilon\\
    & None, \text{else}
    \end{aligned}
    \right.
\end{equation}



Then we can combine the decoding with or without RAG stated in equation \ref{norag_all_rag} into:
\begin{equation}
    \begin{aligned}
    \label{KB_decoding}
    \bm{y_{kb}} &= Dec_{\theta,\phi}(\bm{y}|\bm{q}, \mathbb{I}(\bm{q}, \bm{k}))
    \end{aligned}
\end{equation}











\iffalse

\begin{figure}[tb]
\centering
\includegraphics[width=0.78\linewidth]{figs/method_v.pdf}
\caption{Method illustration of model architecture (purple blocks) and data flows (along black/purple arrows). The purple dashed arrows mean that the output of \texttt{MLP} module will be the "query" to the next layer of \texttt{Cross-attn} module. $\times N$ means that the modules with dotted backgrounds are repeated with multiple layers in the task model.}
\label{method_fig}
\end{figure}

\subsection{Background}

Consider an example query $\bm{q}$ with gold answer $\bm{a}$ and independent $C$ pieces of corresponding context information $\bm{k} = \{\bm{k_1}, \bm{k_2}, ..., \bm{k_C}\}$, with each being a sequence of tokens, where $\bm{k}$ is retrieved by some retriever from a given corpus\footnote{Refer to Sec.~\ref{exp_set} for detailed definition of corpus and retriever in our experiments.}
$$\bm{k} = \text{Retriever}(\bm{q}, \text{corpus})$$
Ideally, the $C$ retrieved contexts contain the knowledge needed to answer $\bm{q}$ correctly, but there may also be noise. Given a decoder model $Dec$ parameterized by $\theta$, the output sequence $\bm{y}$ is usually modeled by
$$P_\theta(\bm{y} | \bm{q}, \bm{k_{max}}, \bm{P}) = Dec(\bm{y} | \bm{q}, \bm{k_{max}}, \bm{P})$$
where $\bm{k_{max}} = \{\bm{k_1}, \bm{k_2}, ..., \bm{k_m}\} \in \bm{k}, m < C$. $m$ refers to the number of contexts that reach the model's throughput. $\bm{P}$ stands for the prompts that connect related content\footnote{The forms of $\bm{P}$ vary with different settings, and there will be detailed definitions in Sec.~\ref{exp_set}.}. Given the model, $\bm{k_{max}}$ is usually a subset of $\bm{k}$ because the maximum length of contexts is often constrained by the model's throughput or computing resources, and  

During training, we aim to maximize the term $P_\theta(\bm{a} | \bm{q}, \bm{k_{max}}, \bm{P})$, and formalize the ODQA problem as a language modeling task. Specifically, for a query $\bm{q}$, its gold answer $\bm{a}$ and contexts $\bm{k_{max}}$, they are connected linguistically with proper prompts $\bm{P}$, together denoted as an input sequence $\bm{x}(\bm{q}, \bm{a}, \bm{k_{max}}, \bm{P}) = \{x_1,x_2, ...\}$. Then we aim to minimize the language modeling loss over the set $\mathcal{D}$ of all training examples:

\begin{align}\label{objective}
\begin{split}
L_\theta(\mathcal{D}) = - \sum\limits_{ \bm{x}(\bm{q}, \bm{a}, \bm{k_{max}}, \bm{P}) \in \mathcal{D}} &\sum\limits_{i} \\[4pt] 
log(P_\theta(x_i&|x_{<i}))
\end{split}
\end{align}

\subsection{Encoding and Cross-Attention}
\label{enc_and_cros}
We propose a method that can utilize additional contexts $\bm{k_{add}} = \{\bm{k_{m+1}}, \bm{k_{m+2}}, ...\}$ several times longer than $\bm{k_{max}}$. First, we introduce an encoder parameterized by $\phi$. Then we apply cross-attention with the original task model and introduce a projector, a cross-attention module and a Multi-Layer Perceptron (MLP) in each layer, together denoted the parameters as $\pi$. Denote $\omega=\{\phi, \pi, \theta\}$ as all the parameters in our model. On the whole, our method models the output $\bm{y}$ by an encoder-decoder model $Enc$-$Dec$
\vspace{1pt}
\begin{align*}
&Q_{\omega}(\bm{y} | \bm{q}, \bm{k_{max}}, \bm{P}, \bm{k_{add}}) \quad \\[4pt]
= ~ &Enc\text{-}Dec(\bm{y} | \bm{q}, \bm{k_{max}}, \bm{P}, \bm{k_{add}}) \quad
\end{align*}
During training, inputs $\bm{x} (\bm{q}, \bm{a}, \bm{k_{max}}, \bm{P})$ are embedded by the original task model's embedding layer $Emb$
\vspace{1pt}
\begin{align*}
\bm{h_{q}} = Emb(\bm{x} (\bm{q}, \bm{a}, \bm{k_{max}}, \bm{P}))
\end{align*}
and each of the additional contexts $\bm{k_i}$ in  $\bm{k_{add}}$ is encoded by the encoder $Enc$
\vspace{1pt}
\begin{align*}
\bm{h_{add}^{(i)}} = Enc(\bm{k_i})
\end{align*}
Note that the length of encoding from the encoder is flexible practically and we compress each $\bm{k_i}$ into one vector. Following the output of the encoder, a projector $Proj$ is used to align the high-dimensional hidden spaces between the encoder and task model in each layer
\vspace{1pt}
\begin{align*}
\bm{h_{kv}} = Proj(\bm{h_{add}})
\end{align*}
where $\bm{h_{add}}$ is concatenated of all $\bm{h_{add}^{(i)}}$ calculated from last step. Each layer of the task model is assigned to an independent projector as different layers may learn different representations.

In each layer, to incorporate the information stored in $\bm{k_{add}}$ we add a cross-attention module, where representations of additional contexts $\bm{h_{kv}}$ serve as "key" and "value", followed by an MLP. In the first layer, the embeddings of original input $\bm{h_{q}}$ act as "query", and in the rest of the layers output $\bm{h_{q}^{'}}$ from the previous layer act as "query" ($\bm{h_{q}^{'}}$ will be defined later).
\vspace{1pt}
\begin{align*}
\bm{h_{c}} &= Cross\text{-}attn(\bm{h_{q}}/\bm{h_{q}^{'}}, \bm{h_{kv}}) \\[1pt]
\bm{h_{m}} &= MLP(\bm{h_c}) 
\end{align*}
$Cross\text{-}attn(\bm{h_{q}}, \bm{h_{kv}})$ is calculated as follows
\begin{align*}
Q &= W^Q \bm{h_{q}} \\[4pt]
K, V &= W^K \bm{h_{kv}}, W^V \bm{h_{kv}} \\[4pt]
o &= softmax(\frac{QK^T}{\sqrt{d_k}})V \\[4pt]
\bm{h_{c}} &= W^O o
\end{align*}
where $W^Q, W^K, W^V, W^O$ refer to weight matrices and $d_k$ refers to the dimension of each attention head.
Then the output of cross-attention and MLP is normally processed by a self-attention and another MLP module. The output acts as "query" input to the cross-attention module in the next layer. 
\begin{align*}
\bm{h_{q}^{'}} &= MLP( Self\text{-}attn(\bm{h_{m}}) )
\end{align*}

At last, the output of the last layer is expanded to the vocabulary-size dimension to predict the next token (not shown in Fig.~\ref{method_fig} for simplicity), and we aim to maximize the probability
$$Q_{\omega}(\bm{a} | \bm{q}, \bm{k_{max}}, \bm{P}, \bm{k_{add}})$$

Consistent with the setup mentioned before, to maximize term  $Q_{\omega}(\bm{a} | \bm{q}, \bm{k_{max}}, \bm{P}, \bm{k_{add}})$, we turn it into minimizing the language modeling loss
\begin{equation}\label{our_objective}
\begin{split}
J_{\omega}(\mathcal{D}) = - \sum\limits_{ \bm{x}(\bm{q}, \bm{a}, \bm{k_{max}}, \bm{P}), \bm{k_{add}} \in \mathcal{D}} &\sum\limits_{i} \\[4pt]
log (Q_{\omega}(x_i&|x_{<i}, \bm{k_{add}}))
\end{split}
\end{equation}

\subsection{ICL Setting}
\label{icl_setting_sec}

Our method can also be applied to ICL settings. Based on the aforementioned setup, we denoted ICL samples as $\bm{l_{max}} = \{\bm{l_1}, \bm{l_2}, ..., \bm{l_m}\}$, with each $\bm{l_i}$ composed of another pair of query and answer. We optimize objective \ref{our_objective_icl} below on data where each $\bm{l_i}(\bm{q^{'}}, \bm{a^{'}})$ refers to only query-answer ICL samples (without context) and $\bm{q^{'}}$ $\bm{a^{'}}$ refer to another query-answer pair:
\begin{equation}\label{our_objective_icl}
\begin{split}
J^{'}_{\omega}(\mathcal{D}) = - \sum\limits_{ \bm{s}(\bm{q}, \bm{a}, \bm{l_{max}}, \bm{P}), \bm{k_{add}} \in \mathcal{D}} &\sum\limits_{i} \\[4pt]
log(Q^{'}_{\omega}(s_i&|s_{<i}, \bm{k_{add}}))
\end{split}
\end{equation}
$\bm{s}=\{s_1, s_2, ... \}$ refers to the inputs composed of $(\bm{q}, \bm{a}, \bm{l_{max}}, \bm{P})$ and $Q^{'}$ shares a similar definition to $Q$ in objective \ref{our_objective}. Additional contexts $\bm{k_{add}} $ are utilized in the same way as in Sec.~\ref{enc_and_cros} by performing encoding, cross-attention, etc.


\subsection{Training}
\label{training_sec}

Theoretically, training processes stated in Sec.~\ref{enc_and_cros} all remain differentiable and thus all the parameters can be optimized via normal gradient descent w.r.t. objective \ref{our_objective}. Note that the parameters $\phi$ of the encoder can be initialized from a well-pre-trained model on a large scale corpus and the pre-trained parameters possess good performance in many downstream tasks based on text encoding. However, the parameters in the projector module are randomly initialized. Thus at the start of the training, according to the chain rule, the gradients to the whole encoder will be random as well, which poses a risk of breaking the encoding utility of the encoder. This intuition proves to be true in our experiments. 

Therefore, we design two strategies of training:

\begin{enumerate}
    \item Directly freeze parameters $\phi$ and make parameters $(\pi, \theta)$ trainable during the whole training process.
    \item In the first few training steps (e.g., one epoch), $\phi$ is kept frozen to prevent random gradients from breaking its well-pre-trained parameters. After that, $\phi$ is optimized w.r.t. objective \ref{our_objective} together with the other modules $(\pi, \theta)$.
\end{enumerate}

%We experiment with both strategies and discuss the performance in Sec.~\ref{main_result} and Sec.~\ref{enc_train} respectively.

\fi