\section{Related Work}

\subsection{Knowledge Boundary Study of Text LLM}
As the LLMs are applied to a wider range of fields, users expect them to perform well on any query. However, inevitably, the knowledge embedded within LLMs does not automatically update over time, resulting in certain queries consistently falling outside the modelâ€™s knowledge boundaries. Some works study the Knowledge Boundaries of text LLMs. A commonly used approach prompts LLMs to output content like \textit{``I don't know''} \cite{li2025refine, cheng2024can, ren2023investigating}. Alternatively, another approach is to construct a dataset and perform Supervised Fine-Tuning (SFT) \cite{zhang2024exploring, cheng2024can, li2025refine}. Both aforementioned types of approaches focus on making the models express \textit{``I know''} or \textit{``I don't know''}. Most aforementioned works find that prompt-based methods are poorly performed. 

We contend that this task is actually challenging for two primary reasons. First, regarding whether a model can itself articulate its own knowledge boundaries, considerable debate persists in current research. For example, \citet{ren2023investigating} states that LLMs struggle to perceive their factual knowledge boundary, and tend to be overconfident, however, \citet{cheng2024can} conclude that the AI assistant can, to a significant extent, identify what it does not know. Second, it is difficult to verify the accuracy of the predicted boundaries. 



\subsection{Retrieval-Augmented Generation}
The RAG technique is widely adopted to help models answer certain queries needing external information in both texts \cite{jeong2024adaptive, chen-etal-2024-improving-retrieval, lewis2020retrieval} and image-text scenarios \cite{lin-byrne-2022-retrieval, wu2022multi}. However, current RAG techniques are far from being perfect for enhancing (V)LLMs in all settings. For example, \citet{zhang2024exploring} finds that for math reasoning and code questions, RAG usually brings noise rather than useful information, and thus RAG may even yield adverse effects. Therefore, more effective utilization of RAG can not only result in savings of time and computational resources but also enhance performance in certain scenarios.


















\iffalse
\subsection{Retrieval Augmentation}
Recently, retrieval augmentation has been utilized to improve a large amount of Natural Language Processing downstream tasks such as question-answering \cite{chen-etal-2017-reading, lewis2020retrieval, kwiatkowski-etal-2019-natural, fan-etal-2019-eli5}, dialogue \cite{moghe-etal-2018-towards}, language modeling \cite{khandelwal2020generalization}, NER \cite{wang-etal-2022-damo, wang2021improving} and machine translation \cite{gu2018search, xu-etal-2022-boosting}. In the aforementioned work, the utilization of retrieval information has been fundamentally capable of enhancing model performance across all dimensions.

\subsection{Related Model Architectures}
Referring to the base model, there has been increasing interest in using models of encoder-decoder or decoder-only architectures in solving downstream tasks with retrieval augmentation recently. 

\citet{allaouzi2019encoder} and \citet{zhou2023medical} employ models of encoder-decoder architectures to solve visual question answering task in the medical domain. In their work, the encoder model is responsible for extracting prominent features from a medical image and the decoder part generates the answer. \citet{math11071624} utilizes an encoder-decoder model with constrained decoding to solve extractive question answering task. 

Decoder-only models, e.g., ChatGPT and GPT-4 \cite{achiam2023gpt}, are more famous for their surprisingly great performance on tasks like question answering \cite{ali2022performance} and there is abundant work that tries to improve the performance based on GPTs \cite{pereira2023visconde}. \citet{kim2024rag} introduce a chatbot model that utilizes generative AI and the Retrieval Augmented Generation method to address the issue that achieving regulatory compliance necessitates the intricate navigation of exceptionally complex and voluminous guidelines in the pharmaceutical industry.

In our work, we also incorporate an encoder for context encoding. However, compared to the traditional encoder-decoder models, the encoder part in our method is several times smaller than the decoder part. Although our method does not alter the quadratic complexity of the attention mechanism, it instead processes the long contexts in a much lower dimension, thus being able to quintuple the capacity to cover context information without the need to utilize additional computing resources.

\subsection{Utilizing Long Contexts}

To handle contexts with excessive length, recently proposed techniques such as context compression are increasingly investigated in NLP research. 

\citet{chevalier2023adapting} proposes "AutoCompressors" that uses OPT \cite{zhang2022opt} and Llama-2 \cite{touvron2023llama} to compress texts into summary vectors and show that utilizing long contexts can improve perplexity. In their method, the compression is done by the billion-level language model, and in one of their experiments, they train on sequence with 30720 tokens with 20 compression steps. However, the complete computation graph cannot be fully kept in such settings, and the optimizing process has to rely on stopping gradients, which poses potential risks to the mathematical principle behind gradient descent. Similarly in \citet{zhang2024soaring}'s work, the long context is first partitioned into multiple intervals, and then a sliding window is employed to sequentially process one interval at a time and the compressed token embeddings are kept for the next token prediction. It is implemented by introducing additional trainable parameters to the origin language model to finish the task of "Activation Condensing", and original parameters are frozen throughout the training process.

%As for reranking, techniques based on attention mechanism \cite{cai-etal-2019-retrieval}, dot products of sentence representations \cite{lewis2020retrieval, karpukhin2020dense}, etc have shown good performance in mining relevant contexts in massive amounts of data. Besides, sparse space models like TF-IDF/BM25 have also been applied to various question answering tasks \cite{wolfson2020break, chen-etal-2017-reading}. With reranking on massive contexts, the models implicitly get to cover long contexts when finishing certain tasks. 


\fi