\section{Related Work}
NER methods are generally categorized into span-based methods, generation-based methods, and other methods.

\subsection{Span-based Methods}

Span-based methods____ reframe the NER task as a \tokenspan
classification task. 
They identify spans based on start and end positions, enumerate all possible candidate spans in a sentence, and perform classification. 
Most existing methods focus on obtaining high-quality span representations and modeling interactions between spans.
CNN-NER____ utilizes Convolutional Neural Networks (CNNs) to model spatial relations in the \tokenspan tensor. 
UTC-IE____ further incorporates axis-aware interaction with plus-shaped self-attention for the \tokenspan tensor on top of CNN-NER.
These methods offer parallel extraction, simple decoding, and advantages in handling nested entity recognition, leading to widespread use and excellent performance. 
However, calculating all span representations and aggregating interactions between \tokenspans requires substantial computational resources, which limits their effectiveness for long texts. 

\subsection{Generation-based Methods}

Generation-based methods extract entities from text in an end-to-end manner, 
where the generated sequence can be 
text____, entity pointers____, or code____. 
With the rise of large language models (LLMs), such methods____ achieve good performance with only a few examples due to their generalization abilities. 
Some methods____ enhance general extraction capabilities by using powerful LLMs, high-quality data, diverse extraction tasks, and comprehensive prior knowledge. 
GoLLIE____ ensures adherence to annotation guidelines through strategies such as class order shuffling, class dropout, guideline paraphrasing, representative candidate sampling, and class name masking. 
ADELIE____ performs instruction tuning
on a high-quality alignment corpus and further optimizes it with a Direct Preference Optimization (DPO) objective. 
However, compared to span-based methods,
these methods often require significant computational resources and may perform poorly in generating accurate longish entities from long texts.
The construction of instructions and use of examples can compress input text length, leading to low text utilization. Additionally, autoregressive generation can result in long decoding times. 

\subsection{Other Methods}

In addition to the two main paradigms, there are a few other types of methods. 
Some methods____ model the NER task as a sequence labeling task. 
However, these methods struggle with nested entities. 
Some methods____ use two independent multi-layer perceptrons (MLPs) to predict the start and end positions of entities separately, which can lead to errors due to treating the entity as separate modules. 
Some approaches____ employ hypergraphs to represent spans, but their decoding processes is complex.