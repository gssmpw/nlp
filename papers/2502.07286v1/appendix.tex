\section{Hyper-parameter Search}

We optimize the model's hyper-parameters based on its performance metrics on the validation set, and then evaluate its final performance on the test set. The parameter search range is as follows: the learning rate search range includes $2e^{-4}$, $3e^{-4}$, $4e^{-4}$; the unilateral window size for the arrow attention and \biswa mechanisms includes 32, 64, 128, 256, 512; and the masking strategy options are token masking, whole word masking, and span masking.

\section{Prompt Construction}

On the three datasets, we prompt GPT-4o and Claude-3.5 to extract entities by providing five similar demonstrations. The format of the prompt is as follows:

Recognize entities from the following sentence and classify the entity type into the options. Options: [type 1, type 2, ..., type r]. Please give the answer in json format.
{example 1}, {example 2}, ...,  {example 5}. Text: .... Output: 

\section{Detailed Experimental Setup}

For the baseline methods, to balance GPU memory usage and training time overhead, the maximum length of the input text is set to 512. A sliding window of 512 is used to segment the text, and the results of these segments are integrated during prediction. The remaining hyper-parameters are determined through searching on the validation set, and the optimal ones are selected. Since some methods require annotation guidelines on entity types (GOLLIE and ADELIE) as complementary knowledge, we use GPT-4o to generate five detailed descriptions for each entity type.

\section{Entity Types}

The three datasets contain the following entity types, arranged from smallest to largest based on average length.The numbers in parentheses represent the average length of entities of the corresponding type, counted in words.

The Scholarst-XL dataset contains 12 entity types: Gender (1), Highest Education (1.07), Birthday (2.43), Position (2.49), Birth Place (2.67), Institution (5.21), Award (7.58), Interest (8.53), Honorary Title (8.83), Social Service (11.12), Educational Experience (40.34), and Work Experience (56.33). 

The SciREX dataset contains 4 entity types: Metric (1.95), Material (2.02), Method (2.35), and Task (2.44).

The Profiling-07 dataset contains 13 entity types: Date (1.38), Major (2.18), Position (2.19), Degree (3.2), Univ (3.41), Interest (4.85), Phone (6.28), Fax (6.34), Affiliation (7.07), Email (7.41), Address (9.61), Contact\_info (44.78), and Education\_info (78.59). Here, Date and Univ denote the date and university of graduation, respectively. Contact\_info and Education\_info represent contact information and educational experience, respectively.

% \section{Hyper-parameter Sensitivity}
\section{Window Size Sensitivity}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\columnwidth]{figure/figure4.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{
Impact of window sizes of arrow attention and \biswa mechanism on the \profiling dataset (\%).
% Comparative analysis of modules Arrow attention and \biswa for different window sizes on the \profiling dataset (\%).
}
\label{fig:window-size-exploration}
\end{figure}


We investigate how the unilateral window size of the arrow attention and \biswa mechanism impacts the performance, as shown in Figure \ref{fig:window-size-exploration}. 
For the arrow attention, a small window restricts the information aggregation capability of local attention, leading to the loss of critical information. 
Conversely, an excessively large window increases the difficulty in information focusing, 
resulting in degraded performance. 

For the \biswa mechanism, a small window reduces the number of candidate entities in the \tokenspan tensor, making it difficult to extract long entities effectively. 
On the other hand, a large window retains a large number of redundant candidate entities, introduces more false positives, and consumes significant computational resources. 
Additionally, when the window size is set to $512$, an Out-of-Memory (OOM) error occurs, further demonstrating the effectiveness of the \biswa mechanism.