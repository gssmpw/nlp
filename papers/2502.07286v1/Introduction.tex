\section{Introduction}

Named entity recognition (NER), a fundamental task in information extraction (IE), aims to identify spans indicating specific types of entities.
It serves as the foundation for numerous downstream tasks, including relation extraction~\cite{miwa2016end}, knowledge graph construction~\cite{xu2017cn}, and question answering~\cite{molla2006named}.

Despite extensive studies, existing NER research rarely focuses on extracting named entities from long texts, a common real-world scenario such as extracting author attributes from homepages and identifying ``methods'' and ``problems'' in academic papers.
For example, in Figure \ref{fig:data-example},
``work experience'' is a long entity block 
while ``award'' is a long entity,
posing greater challenges for the NER task.
We also extend 
the input length of a NER method to extract short entities in academic papers, as shown in 
Figure \ref{fig:length-analysis-scirex},
suggesting that long input length brings clear benefits to extract entities more precisely due to the perception of longer contexts.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure/figure0.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{An example of entity/attribute extraction from an author's homepage,
where ``work experience'' is a long entity block
and ``award'' is a long entity.
% Exemplification of input text.
}
\label{fig:data-example}
\end{figure}

\begin{figure*}[t]
    % \centering
	\begin{subfigure}{0.47\linewidth}
		\centering
		\includegraphics[width=0.9\linewidth]{figure/figure6.pdf}
		\caption{Impact of input length for our model \model regarding extraction performance on the \scirex dataset (\%).}
		\label{fig:length-analysis-scirex}
	\end{subfigure}
    % \centering
    \hspace{0.8cm}
    \begin{subfigure}{0.45\linewidth}
		% \centering
		\includegraphics[width=0.9\linewidth]{figure/figure1.pdf}
		\caption{Model parameters of span-based and generation-based methods vs. F1 score on the \profiling dataset.}
		\label{fig:para-vs-f1}
	\end{subfigure}
	\caption{Performance of entity recognition with respect to input length and the number of model parameters of NER methods.}
	% \label{da_chutian}
\end{figure*}

Traditional approaches treat the NER task as a sequence labeling task, assigning a single label to each token, exemplified by the BIOES format.
However, these methods are inadequate for recognizing nested entities. 
To address this issue, later efforts typically employ span-based methods~\cite{su2022global,yan-etal-2023-utc}, which consider all possible \tokenspans and classify each span.
These methods achieve satisfactory accuracy on sentence-based NER tasks
but struggle to identify entity blocks across sentences or extract entities from long texts
due to substantial redundant computations and GPU memory usage resulting from $\mathcal{O}(L^2)$ computation complexity based on the \tokenspan tensor,
where $L$ is the input length.

Recently, large language models (LLMs) demonstrate remarkable performance on a spectrum of natural language understanding and generation tasks~\cite{zhao2023survey}.
However, LLMs still fall short and do not align well with information extraction tasks~\cite{qi2024adelie}.
On the Scholar-XL dataset~\cite{zhang2024oag},
extracting author attributes via prompting GPT-4o~\cite{achiam2023gpt} by providing $5$ similar demonstrations only achieves a $21.87\%$ F1 score, as shown in Figure \ref{fig:para-vs-f1}.
Fine-tuning LLMs to extract entities from long texts is also feasible~\cite{sainz2023gollie,qi2024adelie}, but it incurs significant time costs for training and inference compared to span-based methods, without guaranteeing high accuracy.

Therefore, we aim to recognize named entities from long texts in a GPU-memory-friendly way without compromising accuracy.
Given these limitations, 
we propose \model,
a lightweight span-based method to extract entities from long texts.
The main idea of \model is to reduce the redundant computations during the encoding and extraction processes of long texts.
\model presents two core components that have an edge over existing span-based NER methods.


\begin{itemize}
    \item To encode long texts effectively and efficiently,
    we employ a bidirectional arrow attention mechanism that encodes both local and global contexts simultaneously.
    To overcome the entropy instability issue of input texts of varied length, we apply 
    LogN-Scaling~\cite{su2021logn} on the \texttt{[CLS]} token to keep the entropy of attention scores stable.
    \item To reduce superfluous span-based computation and model interactions between \tokenspans, we propose a novel bidirectional sliding-window plus attention (\biswa) mechanism to efficiently compute horizontal and vertical attention on focused spans.
\end{itemize}

To enhance the robustness and generalization of our model,
we employ the whole word masking strategy~\cite{cui2021pre}  and the LoRA~\cite{hu2021lora} technique during training.
Extensive experimental results on three datasets highlight the superiority of our proposed method.
\model achieves state-of-the-art accuracy while maintaining relatively small model parameters, as depicted in Figure \ref{fig:para-vs-f1}.
Additionally, under the same hardware and configuration, our model is capable of handling texts $6$ times longer than previous advanced span-based NER methods.
