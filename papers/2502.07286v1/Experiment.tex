\section{Experiment}

\subsection{Datasets}

We conduct experiments on three NER datasets: \profiling~\cite{zhang2024oag}, \scirex~\cite{jain-etal-2020-scirex}, and \oldprofiling~\cite{tang2007social,tang2008arnetminer}. The statistics of all datasets are detailed in Table \ref{statistics}. 
As shown in Table \ref{statistics}, the input lengths and entity lengths of the three datasets are longer than those of traditional named entity recognition datasets, presenting greater challenges. 

\begin{table}[t]
        \newcolumntype{C}{>{\centering\arraybackslash}p{3.2em}}
        \small
        \centering
        \renewcommand\arraystretch{1.0}
        \begin{tabular}{c|@{~ }*{1}{ccc}}
            \toprule[1.2pt]
        {}   & {\profiling}  & {\scirex}  & {\oldprofiling} \\
        \midrule
        Input avg. len. & 433.42 & 5678.29 & 785.09 \\
        Input max. len. & 692 & 13731 & 17382 \\
		Input num. & 2099 & 438 & 1446 \\
        Entity num. & 20994 & 156931 & 17416 \\
        Entity type & 12 & 4 & 13 \\
        Entity avg. len. & 12.45 & 2.28 & 8.88  \\
        Entity max. len. & 480 & 18 & 307  \\
        \bottomrule[1.2pt]
    \end{tabular}
    \caption{
            \label{statistics} Statistics of the datasets (in words). 
        }
\end{table}


\subsection{Baselines}

We compare our model with several recent NER methods:

Span-based Methods: 
\textbf{CNN-NER}~\cite{yan-etal-2023-embarrassingly}: is a span-based method that utilizes Convolutional
Neural Networks (CNN) to model local spatial correlations between spans.
\textbf{UTC-IE}~\cite{yan-etal-2023-utc}: models axis-aware interaction with plus-shaped self-attention and local interaction with CNN on top of the \tokenspan tensor.

Others Methods: 
\textbf{DiffusionNER}~\cite{shen2023diffusionner}: formulates the NER task as a boundary-denoising diffusion process and thus generates named entities from noisy spans.

Generation-based Methods: 
\textbf{UIE}~\cite{lu2022unified}: uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions, and captures the common IE abilities via a large-scale pre-trained text-to-structure model.
\textbf{InstructUIE}~\cite{wang2023instructuie}: leverages natural language instructions and instruction tuning to guide large language models for IE tasks.
\textbf{GOLLIE}~\cite{sainz2023gollie}: is based on Code-Llama~\cite{roziere2023code} and fine-tunes the foundation model to adhere to specific annotation guidelines.
\textbf{ADELIE}~\cite{qi2024adelie}: 
builds a high-quality instruction tuning dataset and utilizes supervised fine-tuning (SFT) followed by direct preference optimization (DPO).
\textbf{ToNER}~\cite{jiang2024toner}: firstly employs an entity type matching model to discover the entity types that are most likely to appear in the sentence, 
and then adds multiple binary classification tasks to fine-tune the encoder in the generative model.
\textbf{GPT-4o}~\cite{achiam2023gpt}: employs the gpt-4o-2024-08-06 API, utilizing a 5-shot in-context learning approach to enhance performance.
\textbf{Claude-3.5}~\cite{TheC3}: uses the claude-3-5-sonnet-20241022 API, also adopting 5-shot in-context learning.


\subsection{Experimental Setup}

% We use 
All experiments are conducted on an 8-card 80G Nvidia A100 server. 
The entire text is used for the \profiling dataset, while the other two datasets are truncated to $5120$ using a sliding window approach, as a trade-off due to limited GPU memory.
For prediction, % the results of different segments are combined.
the prediction of the text segment is mapped to the starting/ending position of the original text.
Hyper-parameters are selected based on the F1 score on the validation set.
For each experiment, we run $3$ times with different random seeds and report the average results.
We choose DeBERTa-V3-large~\cite{he2023debertav3improvingdebertausing} as the PLM for span-based methods and DiffusionNER. 
We use AdamW~\cite{loshchilov2017fixing} optimizer with a weight decay of $1e^{-2}$. 
The unilateral window sizes of the arrow attention and BiSPA mechanism are both set to $128$. 
We only use low-rank adaptation on the $Q$ and $V$ matrix of the self-attention mechanism with a rank of $8$. 

\subsection{Evaluation Metrics}

We report the micro-F1 score for all attributes. 
An entity is considered correct only if both the entity type and the entity span are predicted correctly.
Precision (P) is the portion of correctly predicted spans over predicted spans, while Recall (R) is the portion of correctly predicted spans over ground-truth entity spans.

\subsection{Main Results}

\begin{table*}[t]
        \newcolumntype{C}{>{\centering\arraybackslash}p{2.2em}}
        \centering
        \renewcommand\arraystretch{1.0}
        \begin{tabular}{c|c|@{~ }*{1}{CCC|}*{1}{CCC|}*{1}{CCC}}
            \toprule[1.2pt]
        \multirow{2}{*}{Method type}    
            &\multirow{2}{*}{Method} 
            &\multicolumn{3}{c|}{\profiling}
            &\multicolumn{3}{c|}{\scirex}
            &\multicolumn{3}{c}{\oldprofiling}
        \\
        \cmidrule{3-5} \cmidrule{6-8}  \cmidrule{9-11} 
        & & {P} & {R} & {F1}  & {P} & {R} & {F1}  & {P} & {R} & {F1} \\
        \midrule
        \multirow{7}{*}{\shortstack{Generation-based\\Methods}}
        & UIE & 43.32 & 36.80 & 39.80 & 65.88 & 56.44 & 60.80 & 65.92 & 57.51 & 61.43 \\
		& ToNER & 40.08 & 29.48 & 33.97 & 57.43 & 31.56 & 40.73 & 48.80 & 41.21 & 44.68  \\
        & InstructUIE & 34.63 & 36.50 & 35.54 & 56.31 & 54.60 & 55.44 & 59.09 & 63.19 & 61.07  \\
        & GOLLIE & 43.74 & 40.88 & 42.26 & 71.56 & 71.50 & 71.53 & 64.51 & 9.46 & 16.50 \\
        & ADELIE & 45.60 & 39.05 & 42.07 & 70.10 & 71.84 & 70.96 & 65.75 & 49.53 & 56.50  \\
        & Claude-3.5 & 17.15 & 30.16 & 21.87 & 57.78 & 7.97 & 14.01 & 34.40 & 43.07 & 38.25  \\
        & GPT-4o & 18.24 & 27.31 & 21.87 & 40.44 & 7.69 & 12.92 & 36.96 & 43.73 & 40.06  \\
        \midrule
        \multirow{1}{*}{Others Methods}
        & DiffusionNER & \underline{55.33} & 29.87 & 38.80 & \textbf{77.11} & 62.36 & 68.96 & \textbf{70.51} & 44.16 & 54.31   \\
        \midrule
        \multirow{3}{*}{\shortstack{Span-based\\Methods}}
        & CNN-NER & 50.92 & 44.72 & 47.59 & 72.13 & 74.56 & 73.32 & 69.19 & 62.79 & 65.56 \\
        & UTC-IE & 53.17 & \underline{46.01} & \underline{49.10} & 71.90 & \underline{75.09} & \underline{73.42} & \underline{69.79} & \underline{65.28} & \textbf{67.43} \\
        % \midrule
        & \model (Ours) & \textbf{57.41} & \textbf{46.80} & \textbf{51.56} & \underline{72.89} & \textbf{76.17} & \textbf{74.49} & 67.52 & \textbf{67.17} & \underline{67.34} \\
        \bottomrule[1.2pt]
    \end{tabular}
    \caption{
            \label{performance}  Main results on three long NER datasets (\%). 
            The best results are \textbf{boldfaced} and the second best results are \underline{underlined}.
        }
\end{table*}

Table \ref{performance} provides a holistic comparison of different NER methods on three datasets.
Generally speaking, span-based methods (CNN-NER, UTC-IE, and our model \model) outperform other types of NER methods.

Generation-based methods utilize generation loss to fine-tune the foundation model to adapt to the long NER task, achieving unfavorable performance.
UIE outperforms InstructUIE,
possibly because UIE defines a structured extraction language that suits the long NER problem better than naively performing instruction tuning.
GOLLIE and ADELIE achieve similar performance, except for \oldprofiling dataset,
which is due to the fact that this dataset is sliced and diced with a high number of empty data and thus makes GOLLIE overfit these empty examples.
ToNER obtains unsatisfactory performance,
possibly since the two-stage framework leads to error propagation
and the usage of small language models for generation limits its potential.
GPT-4o and Claude-3.5-sonnet are less effective, suggesting that the proprietary model prompting does not perform the long text NER task well.

The span-based NER methods (CNN-NER, UTC-IE, and \model) outperform other types of NER methods, including Diffusion-NER.
DiffusionNER is a diffusion-based method that recovers the boundaries of the entities from a fixed amount of Gaussian noise
and it is hard to recover longish entities from long texts.
CNN-NER models fine-grained span interactions via CNN,
achieving decent extraction performance.
UTC-IE further improves CNN-NER by introducing plus-shaped attention on the \tokenspan tensor,
achieving consistent outperformance over CNN-NER.

Our model \model exhibits noticeable improvements or is on par with the best baseline,
suggesting that with the design of arrow attention coupled with LogN-Scaling on the \texttt{[CLS]} in the PLM encoder, as well as the BiSPA mechanism on the \tokenspan tensor,
our model is capable of saving computation and memory resources without degrading the extraction accuracy.
In addition, longer text with more focused attention can effectively help the model understand the semantic information of the text in more detail and extract the corresponding entities.

\subsection{Ablation Study}

\begin{table}[t]
\small
        \newcolumntype{C}{>{\centering\arraybackslash}p{2.2em}}
        \centering
        \renewcommand\arraystretch{1.0}
        \begin{tabular}{c|@{~ }*{1}{CC|}*{1}{CC|}*{1}{CC}}
            \toprule[1.2pt]

        {Method} & \multicolumn{2}{c|}{\profiling} & \multicolumn{2}{c|}{\scirex} & \multicolumn{2}{c}{\oldprofiling} \\
        \cmidrule{2-3} \cmidrule{4-5}  \cmidrule{6-7} 
        & {F1} & {Mem} & {F1} & {Mem}  & {F1} & {Mem} \\
        \midrule
        \model & \textbf{51.56} & 16.95 & \textbf{74.49} & 69.23 & \textbf{67.34} & 63.36 \\
        \text{w/o Arrow} & 51.34 & 16.94 & - & OOM & - & OOM \\
        \text{w/o LogN} & 50.78 &17.18 & 74.27 & 68.71 & 67.11 & 63.47 \\
        \text{w SWA} & 49.95 & 16.46 & 73.50 & 68.33 & 65.60 & 63.16 \\
        \text{w/o LoRA} & 49.93 & 17.98 & 73.98 & 70.79 & 66.53 & 66.58 \\
        \text{w/o \biswa} & 50.48 & 41.39 & - & OOM & - & OOM  \\
        \text{w/o WWM} & 50.45 & 17.14 & 74.24 & 69.00 & 64.97 & 63.37 \\
        \bottomrule[1.2pt]
    \end{tabular}
    \caption{
            \label{ablation}  Ablation studies on three long NER datasets. Mem means memory usage (GB), SWA denotes sliding window attention and WWM is whole word masking.
            % in the \profiling, \scirex and \oldprofiling datasets 
        }
\end{table}

Table \ref{ablation} presents a justification for the effectiveness of
each component in our model.
Removing either the arrow attention or \biswa mechanism results in a decrease in model performance on \profiling and Out-of-Memory (OOM) errors on \scirex and \oldprofiling. 
It indicates that both modules effectively reduce explicit memory usage, enabling the model to handle longer texts and thereby improving overall performance.
Specifically, \biswa significantly reduces compute and memory footprint by reducing negative samples.
In contrast, the arrow attention has a limited ability to reduce memory usage for short text on the \profiling dataset.
Substituting the arrow attention with sliding window attention (SWA) leads to a significant performance drop, highlighting the necessity of 
imposing attention scores on the \texttt{[CLS]} token  to absorb global contextual information.
Adding LogN-Scaling consistently improves the performance, 
thereby enhancing model stability and robustness. 
Although removing LoRA does not cause OOM errors, the F1 score decreases across all datasets to some extent, demonstrating that LoRA can effectively reduce training parameters and prevent overfitting. 
Whole Word Masking (WWM) increases the diversity of input texts,
thus improving the generalization capacity of the model.

\subsection{Detailed Analysis for Entity Types}

In this subsection, we focus on comparing the performance of our method with span-based methods (CNN-NER and UTC-IE) and LLM-based methods (InstructUIE, GOLLIE, and ADELIE) across entities of varying lengths and types.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure/figure7.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Performance of different entity types on the \profiling dataset (\%).
The average length of entities increases clockwise from Gender to Work Exp. (Highest Edu.: Highest Education, Education Exp.: Education Experience, Work Exp.: Work Experience.)
}
\label{fig:radar-profiling}
\end{figure}

The results on the \profiling dataset are depicted in Figure \ref{fig:radar-profiling},
with the average length of the entity types increasing clockwise from ``Gender'' to ``Work Experience''. 
Generative methods, leveraging the powerful capabilities of LLMs, achieve superior performance in extracting ``Gender'' and ``Birth Place'' types. 
However, for other types of entities, span-based methods demonstrate consistent superiority. 
Our model \model
outperforms CNN-NER and UTC-IE in most types of entities, with particularly notable improvements for longish entities. 
Specifically, 
for ``Social Service'',
our method achieves an improvement of $6.38\%$ over CNN-NER and $4.14\%$ over UTC-IE, respectively. 
The performance of \model for entity types ``Education Experience'' and ``Work Experience'' falls behind the leading ones a little,
indicating that the approximation strategy in our model inevitably loses some information, especially on very long entities. 

\subsection{Analysis for Maximum Input Length}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figure/figure10.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{
Blue bar: Maximum input length comparison of different methods (k).
Orange bar: Inference time comparison of different methods (second).
Both are conducted on the longest \scirex dataset.
}
\label{fig:memory-analysis}
\end{figure}

We examine the maximum input length supported by training each NER method
on a single Nvidia A100 with a batch size of $1$,
as shown in Figure \ref{fig:memory-analysis}. 
Generation-based methods 
often employ various lightweight strategies, such as quantization,  FlashAttention~\cite{dao2022flashattention}, and Zero Redundancy Optimizer (ZeRO)~\cite{rajbhandari2020zero}
enabling models like GOLLIE and ADELIE to handle long texts. 
In contrast, 
span-based methods need to model \tokenspan tensor, 
resulting in supporting shorter input length.
Our method, \model, demonstrates substantial improvements over CNN-NER and UTC-IE, supporting input lengths that are $3$ times and $6$ times longer, respectively.

\subsection{Efficiency Performance for Inference Time}
Figure \ref{fig:memory-analysis} also displays the inference time of span-based methods and LLM-based methods.
It can be observed that LLM-based methods lead to $10$ times longer inference time than span-based methods.
Our method \model achieves a similar inference time compared with CNN-NER, achieving significantly better extraction accuracy simultaneously.
\model can save $20\%$ inference time compared with UTC-IE and encode longer input texts, still maintaining state-of-the-art extraction accuracy.
