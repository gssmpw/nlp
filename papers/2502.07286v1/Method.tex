\section{Method}

As previously discussed, conventional NER methods fall into two main categories: span-based and generation-based.
For NER in long texts, span-based methods need to model interactions between \tokenspans, which incurs substantial GPU memory and computation.
In contrast, generation-based methods, 
commonly based on LLMs, are arduous to generate longish entity spans accurately.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\columnwidth]{figure/figure8.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{An overview of the \model model.}
\label{fig:model}
\end{figure}

In response to these limitations, we propose a lightweight span-based NER model, \model, that efficiently encodes long input texts and models \tokenspans interactions.
First, we employ a pre-trained language model (PLM) with a arrow attention mechanism to encode long inputs efficiently. 
To alleviate entropy instability resulting from varied input lengths, we apply LogN-Scaling~\cite{su2021logn} to the \texttt{[CLS]} token.
Next, we leverage a Biaffine model~\cite{dozat2017deep} to obtain the hidden representation of each \tokenspan. %token pair.
Then, we present the \tokenspan interaction module,
where we propose a novel \biswa mechanism to significantly reduce redundant candidate token pairs
and model interactions between token pairs simultaneously.
Finally, we introduce the training strategy and prediction method.
An overview of our model is shown in Figure \ref{fig:model}.

\subsection{Long Input Encoding}
Given a piece of text, we pass it into a PLM to obtain its contextual vector representation.

\begin{equation}
H=\left [ h_{1}, h_{2}, ..., h_{L} \right ]=\text{PLM}\left ( \left [ x_{1}, x_{2}, ..., x_{L} \right ] \right )\label{eq1}
\end{equation}

\noindent where $H\in \mathbb{R}^{L\times d}$, $L$ is the input length, and $d$ is the output dimension of the PLM. 

Traditional NER methods utilize PLMs with full bidirectional attention, incuring a large amount of GPU memory footprint and computation for long texts.
Moreover, full attention for long texts is often unnecessary since distant tokens are usually semantically unrelated.
In light of this, a straightforward idea is to use sliding window attention (SWA)~\cite{beltagy2020longformer, zaheer2020big},
which adopts a fixed window, say $w$, so that 
each token attends to $w$ tokens to its left and $w$ tokens to its right.
However, SWA ignores the global context, impairing the ability of the Transformer layers to acquire a comprehensive understanding of the entire input text.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure/figure2.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Illustration of arrow attention, full attention, and sliding window attention.}
\label{fig:arrow-attention}
\end{figure}

To this end, we propose an \textbf{Arrow Attention} mechanism,
where the \texttt{[CLS]} token uses global attention while other tokens use local sliding window attention, as illustrated in Figure \ref{fig:arrow-attention}.
Arrow Attention strikes a balance between global and local attention.
Compared to the computational complexity of $\mathcal{O} (L^2)$ for the full attention, 
arrow attention only requires $\mathcal{O} (wL)$.
Furthermore, the global information captured by the \texttt{[CLS]} token supplements the knowledge of SWA, 
enhancing the representation of each token and mitigating the information loss caused by the fixed receptive field.
Thus, the \texttt{[CLS]} token acts as an attention sink~\cite{xiao2023efficient} 
that balances the weights of global and local contexts.

However, varying text lengths can cause entropy instability for the {[CLS]} token, where the scale of attention scores can change significantly.
In this regard, we employ a LogN-Scaling technique on the \texttt{[CLS]} token to stabilize the entropy of attention scores.
Specifically, LogN-Scaling is defined as follows:

\begin{align}
{H}^{t}_{\texttt{[CLS]}}=\text{Attn}_{\text{s}}\left ( H^{t-1}_{\texttt{[CLS]}}W^{Q}, H^{t-1}W^{K}, H^{t-1}W^{V} \right )\\
\text{Attn}_{\text{s}}\left ( Q, K, V \right )=\text{softmax}\left ( \frac{{\log_{512}}{L}}{\sqrt{d}}QK^{\top} \right )V\label{eq3}
\end{align}

\noindent where $\text{Attn}_{s}$ is the scaled attention,
$H^{t}$ is the hidden representation of the $t$-th Transformer layer, and $W^{Q}, W^{K}, W^{V}\in \mathbb{R}^{d\times d}$ are projection matrices.

Note that LogN-Scaling is commonly used for length extrapolation in LLMs and imposed on all input tokens.
Here we utilize LogN-Scaling solely on the \texttt{[CLS]} token to improve the stability and robustness of our model.

\subsection{Biaffine Model}
Subsequently, the hidden representation $H$ is fed into a Biaffine
model to extract features for each candidate span.

\begin{align}
H^{s}, H^{e}=\text{MLP}_{\text{start}}\left ( H \right ), \text{MLP}_{\text{end}}\left ( H \right )\\
S_{i, j}=( H^{s}_i )^{\top}W_{1}H^{e}_j +W_{2} ( H^{s}_i\oplus H^{e}_j )+b\label{eq2}
\end{align}

\noindent where $\text{MLP}_{\text{start}}$ and $\text{MLP}_{\text{end}}$ are multi-layer perceptrons, 
$H^{s}/H^{e}\in \mathbb{R}^{L\times d}$ are hidden start/end embeddings, 
$W_{1}\in \mathbb{R}^{d\times c\times d}$, $W_{2}\in \mathbb{R}^{c\times 2d}$, $b\in \mathbb{R}^{c}$, 
and $c$ is the output dimension of the Biaffine model. The symbol $\oplus$ represents the concatenation operation. $S\in \mathbb{R}^{L\times L\times c}$, called \tokenspan tensor, 
denotes the hidden representation of each candidate span. 
For example, $S_{i, j}$ represents the features of $\left [ x_{i}, ..., x_{j} \right ]$.

\subsection{Token-Pair Span Interaction Module}

Note that the \tokenspan tensor  
$S$ considers each possible candidate span.
However, for long input texts,
it is unnecessary to consider every candidate span,
especially for extremely long spans.
Additionally, the GPU memory occupied by tensor $S$ increases quadratically with the input length $L$.
In light of this, we propose preserving only the hidden features of spans whose lengths do not exceed
$w'$ as shown in Figure \ref{fig:token-pair-matrix}.
Thus, $S$ is compressed to $S_{h} \in \mathbb{R}^{L\times w'\times c}$.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figure/figure3.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{Diagram of the transformation for the \tokenspan tensors in \biswa mechanism.
% Token-Pair matrix transformation.
}
\label{fig:token-pair-matrix}
\end{figure}

Previous studies~\cite{yan-etal-2023-embarrassingly,yan-etal-2023-utc} show that modeling the interactions between token pairs, such as plus-shaped and local interaction, should be helpful.
Plus-shaped attention applies the self-attention mechanism horizontally and vertically.
However, plus-shaped attention cannot be performed directly on the compressed hidden feature tensor $S_{h}$ since either the original horizontal or vertical dimension is disrupted.
Therefore, we propose a novel bidirectional sliding-window plus attention (\textbf{BiSPA}) mechanism to perform plus-shaped attention on the compressed $S_{h}$.

Specifically, we first compute the horizontal self-attention on $S_{h}$,
as shown in the top middle of Figure \ref{fig:token-pair-matrix}.
Next, we propose a transformation method,
that transforms the top left matrix $S$ to the bottom middle matrix $S_{v}$,
and then compute the vertical self-attention based on $S_{v}$.
Finally, we concatenate the horizontal and vertical attention matrices and feed them into an MLP to aggregate plus-shaped perceptual information.
Notably, the computational complexity of the BiSPA mechanism is reduced from $\mathcal{O}(L^3)$ to $\mathcal{O}(L \times (w^{'})^2)$,
optimizing the 
training efficiency significantly.

\begin{align}
Z^{h/v}_{i,:} =\text{Attn}\left ( S^{h/v}_{i,:}W_{h/v}^{Q}, S^{h/v}_{i,:}W_{h/v}^{K}, S^{h/v}_{i,:}W_{h/v}^{V} \right )\\
\text{Attn}\left ( Q, K, V \right )=\text{softmax}\left ( \frac{QK^{T}}{\sqrt{c}} \right )V\label{eq3}
\end{align}

\begin{equation}
{S}'=\text{MLP}\left ( Z^{h}\oplus Z^{v} \right )\label{eq4}
\end{equation}

\noindent where $W_{h}^{Q}, W_{h}^{K}, W_{h}^{V}$, $W_{v}^{Q}, W_{v}^{K}, W_{v}^{V}\in \mathbb{R}^{c\times c}$, $Z^{h}/Z^{v}$ is intermediate representation after horizontal/vertical self-attention, 
and ${S}'\in \mathbb{R}^{L\times w'\times c}$ is the \tokenspan feature after \biswa mechanism.

The BiSPA mechanism endows the model with the capacity to perceive horizontal and vertical directions.
We further use two types of position embeddings to enhance the sense of  distances between token pairs and the area the token pair locates~\cite{yan-etal-2023-utc}. (1) Rotary Position Embedding (RoPE)~\cite{su2024roformer} 
encodes the relative distance between token pairs,
which is used for both horizontal and vertical self-attention.
(2) Matrix Position Embedding indicates whether each entry in $S'$ is the original upper or lower triangles,
which adds to $S_{h}$ and $S_{v}$.

After the BiSPA mechanism, we employ  CNN 
with kernel size $3 \times 3$ on $S^{'}$ to model the local interactions between \tokenspans.

\begin{equation}
{S}''= \text{Recover} ( \text{Conv}\left ( \sigma \left ( \text{Conv}\left ( {S}' \right ) \right ) \right ) ) \label{eq5}
\end{equation}

\noindent where ${S}''\in \mathbb{R}^{L\times L \times c}$ is recovered to the square size,
and $\sigma $ is the activation function. 

We name the module encompassing the BiSPA mechanism and the convolutional module as the BiSPA Transformer block.
The BiSPA Transformer blocks will be repeatedly used to ensure full interaction between token pairs.

\subsection{Training and Prediction}

We utilize MLP layers to transform the output of the final BiSPA Transformer block into output scores. We use binary cross-entropy as the loss function.

\begin{equation}
\widehat{Y}=\text{MLP}\left ( {S}''+ S \right )\label{eq6}
\end{equation}

% \begin{equation}
\begin{align}
\mathcal{L}= &-\sum_{i,j=1}^{L}\sum_{r=1}^{R}\left ( Y_{i,j}^{r}log\left ( \widehat{Y}_{i,j}^{r} \right ) \right.\\
&\left. +\left ( 1-Y_{i,j}^{r} \right )log\left ( 1-\widehat{Y}_{i,j}^{r} \right ) \right )
\label{eq7}
\end{align}
% \end{equation}

\noindent where $\widehat{Y}\in \mathbb{R}^{L\times L\times R}$ represents the scores of candidate entities, and $R$ is the number of entity types.

To improve the robustness and generalization of our model,
we employ the whole word masking strategy~\cite{cui2021pre} during training and utilize LoRA~\cite{hu2021lora} technique to train the PLM parameters.

During prediction, our model uses the average of the upper triangular and lower triangular values as the final prediction score, as follows:

\begin{equation}
P_{i,j}^{r}= \frac{\left ( \widehat{Y}_{i,j}^{r}+\widehat{Y}_{j,i}^{r} \right )}{2}, i\leq j\label{eq8}
\end{equation}

All text spans that satisfy $P_{i,j}^{r}>0$ 
are outputted. 
If the boundaries of multiple candidate spans conflict, the span with the highest prediction score is selected.