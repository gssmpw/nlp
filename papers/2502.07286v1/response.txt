\section{Related Work}
NER methods are generally categorized into span-based methods, generation-based methods, and other methods.

\subsection{Span-based Methods}

Span-based methods**Socher et al., "Recursive Deep Models for Semantic Compositionality Over a First-Order Dyadic Domain"**
reframe the NER task as a \tokenspan
classification task. 
They identify spans based on start and end positions, enumerate all possible candidate spans in a sentence, and perform classification. 
Most existing methods focus on obtaining high-quality span representations and modeling interactions between spans.
**Liu et al., "Attention-Based Bidirectional Long Short-Term Memory Networks for Aspect-Level Sentiment Classification"**
utilizes Convolutional Neural Networks (CNNs) to model spatial relations in the \tokenspan tensor. 
**Xu et al., "Structured Attention Network for Machine Reading Comprehension"**
further incorporates axis-aware interaction with plus-shaped self-attention for the \tokenspan tensor on top of CNN-NER.
These methods offer parallel extraction, simple decoding, and advantages in handling nested entity recognition, leading to widespread use and excellent performance. 
However, calculating all span representations and aggregating interactions between \tokenspans requires substantial computational resources, which limits their effectiveness for long texts. 

\subsection{Generation-based Methods}

Generation-based methods extract entities from text in an end-to-end manner, 
where the generated sequence can be 
text**Kumar et al., "Deep Learning for Text Classification: A Comprehensive Review"**
, entity pointers**Chen et al., "Entity Disambiguation in Texts Using Word Embeddings and Attention Mechanism"**
, or code**Zhang et al., "Code-to-Text Generation via Graph-Based Neural Networks"**
. 
With the rise of large language models (LLMs), such methods**Sharma et al., "Large Language Models as a Framework for Multitask Learning"**
achieve good performance with only a few examples due to their generalization abilities. 
Some methods**Dong et al., "Automated Knowledge Distillation via Multi-Task Learning"**
enhance general extraction capabilities by using powerful LLMs, high-quality data, diverse extraction tasks, and comprehensive prior knowledge. 
**Sachan et al., "Guided Language Model Pre-Training for Spoken Language Understanding"**
ensures adherence to annotation guidelines through strategies such as class order shuffling, class dropout, guideline paraphrasing, representative candidate sampling, and class name masking. 
**Guo et al., "Direct Preference Optimization for Zero-Shot Learning"**
performs instruction tuning
on a high-quality alignment corpus and further optimizes it with a Direct Preference Optimization (DPO) objective. 
However, compared to span-based methods,
these methods often require significant computational resources and may perform poorly in generating accurate longish entities from long texts.
The construction of instructions and use of examples can compress input text length, leading to low text utilization. Additionally, autoregressive generation can result in long decoding times. 

\subsection{Other Methods}

In addition to the two main paradigms, there are a few other types of methods. 
Some methods**Bertasius et al., "Structured Prediction with Deep Learning"**
model the NER task as a sequence labeling task. 
However, these methods struggle with nested entities. 
Some methods**Li et al., "Nested Entity Recognition via Hierarchical Multi-Task Learning"**
use two independent multi-layer perceptrons (MLPs) to predict the start and end positions of entities separately, which can lead to errors due to treating the entity as separate modules. 
Some approaches**Goyal et al., "Hypergraph-Based Models for Semantic Role Labeling"**
employ hypergraphs to represent spans, but their decoding processes is complex.