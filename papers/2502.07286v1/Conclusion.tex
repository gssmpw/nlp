\section{Conclusion} 
In this paper, we tackle the problem of extracting entities from long texts, a less explored area in Named Entity Recognition (NER).  
Current span-based and generation-based NER methods face issues such as computational inefficiency and memory overhead in span enumeration, along with inaccuracy and time costs in text generation. 
To address these challenges, we introduce \model, a lightweight span-based approach that featuring a bidirectional arrow attention mechanism and LogN-Scaling for effective long-text embedding. 
Additionally, we propose a bidirectional sliding-window plus-shaped attention (\biswa) mechanism that significantly reduces redundant candidate \tokenspans and models their interactions. 
Extensive experiments show that SeNER achieves state-of-the-art accuracy in extracting entities from long texts across three NER datasets, while maintaining GPU-memory efficiency.
 Our innovations in arrow attention and the \biswa mechanism have the potential to advance future research in information extraction tasks.