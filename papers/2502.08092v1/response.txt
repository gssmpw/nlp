\section{Related Work}
\stitle{Chain-of-Thought prompting.}
Chain-of-Thought (CoT) prompting has emerged as a groundbreaking technique in natural language processing (NLP), empowering language models to address complex reasoning tasks by producing intermediate reasoning steps **Viall et al., "Reinforcing Reasoning Steps with Chain of Thought Prompting"**. By breaking down problems into a sequence of logical steps, CoT emulates human-like thought processes, leading to significant improvements in model performance on tasks requiring structured reasoning **Stiennon et al., "Chain-of-Thought Prompting Accelerates Knowledge Distillation"**. Despite its remarkable success in the NLP domain, the potential of CoT for graphs remains largely unexplored.  A contemporary work, GraphCoT **Gupta and Larochelle, "Graph Chain-of-Thought: Learning to Reason on Text-Attributed Graphs"**, leverages the inherent relational information in text-attributed graphs **Zhang et al., "Graph Reasoning with Coordinated Reasoning"** to guide the reasoning process. However, GraphCoT primarily focuses on question-answering tasks for natural language and cannot be extended to general graphs lacking textual descriptions.

\stitle{Graph representation learning.}
GNNs **Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"** are dominant technique for graph representations learning. They generally update nodes embeddings iteratively by aggregating information from their local neighborhoods based on a message-passing mechanism **Gilmer et al., "Neural Message Passing for Quantum Chemistry"**. Despite their success, GNNs often demand substantial amounts of task-specific labeled data and necessitate retraining for each new task, limiting their flexibility and scalability.
Recently, researchers have extensively investigated pre-training techniques for graphs **Hu et al., "Strategies for Pre-Training Graph Neural Networks"**. These approaches involve pre-training a graph encoder using self-supervised objectives, and then adapt the pre-trained knowledge to downstream tasks. However, a significant gap exists between the objectives of pre-training and those of downstream tasks, resulting in suboptimal performance. %Moreover, these methods typically employ only a single inference step during the downstream phase, which may further lead to their suboptimal performance.


\stitle{Graph prompt learning.}
First proposed in NLP, prompt learning has emerged as a powerful framework for bridging the gap between pre-training and downstream tasks **Liu et al., "PPT: Pre-trained Prompt Tuning for Text Classification"**. Recently, this paradigm has been extended to the graph domain as a compelling alternative to fine-tuning approaches **Zhang et al., "GraphPrompt: A Framework for Graph-Based Prompt Learning"**. These methods typically utilize a universal template to align pre-training and downstream tasks, followed by task-specific prompts that facilitate seamless adaptation to downstream tasks while keeping the pre-trained model frozen. However, current graph prompt learning methods directly produce a final answer in a single step, resulting in insufficient refinement to the answer.