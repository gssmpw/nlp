\section{Related Work}
\stitle{Chain-of-Thought prompting.}
Chain-of-Thought (CoT) prompting has emerged as a groundbreaking technique in natural language processing (NLP), empowering language models to address complex reasoning tasks by producing intermediate reasoning steps \cite{wei2022chain,wang2023self,gao2023pal}. By breaking down problems into a sequence of logical steps, CoT emulates human-like thought processes, leading to significant improvements in model performance on tasks requiring structured reasoning \cite{yao2024tree,feng2024towards}. Despite its remarkable success in the NLP domain, the potential of CoT for graphs remains largely unexplored.  A contemporary work, GraphCoT \cite{jin2024graph}, leverages the inherent relational information in text-attributed graphs \cite{yan2023comprehensive,wen2023augmenting} to guide the reasoning process. However, GraphCoT primarily focuses on question-answering tasks for natural language and cannot be extended to general graphs lacking textual descriptions.

\stitle{Graph representation learning.}
GNNs \cite{kipf2016semi, velivckovic2017graph} are dominant technique for graph representations learning. They generally update nodes embeddings iteratively by aggregating information from their local neighborhoods based on a message-passing mechanism \cite{xu2018powerful, hamilton2017inductive}. Despite their success, GNNs often demand substantial amounts of task-specific labeled data and necessitate retraining for each new task, limiting their flexibility and scalability.
Recently, researchers have extensively investigated pre-training techniques for graphs \cite{kipf2016variational, hu2020strategies, hu2020gpt, lu2021learning}. These approaches involve pre-training a graph encoder using self-supervised objectives, and then adapt the pre-trained knowledge to downstream tasks. However, a significant gap exists between the objectives of pre-training and those of downstream tasks, resulting in suboptimal performance. %Moreover, these methods typically employ only a single inference step during the downstream phase, which may further lead to their suboptimal performance.


\stitle{Graph prompt learning.}
First proposed in NLP, prompt learning has emerged as a powerful framework for bridging the gap between pre-training and downstream tasks \cite{brown2020language,liu2021gpt,lester2021power}. Recently, this paradigm has been extended to the graph domain as a compelling alternative to fine-tuning approaches \cite{liu2023graphprompt,sun2022gppt,yu2023hgprompt,yu2024generalized}. These methods typically utilize a universal template to align pre-training and downstream tasks, followed by task-specific prompts that facilitate seamless adaptation to downstream tasks while keeping the pre-trained model frozen. However, current graph prompt learning methods directly produce a final answer in a single step, resulting in insufficient refinement to the answer.
