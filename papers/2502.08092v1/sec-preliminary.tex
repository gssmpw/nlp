\section{Preliminaries}\label{sec.preliminaries}
\stitle{Graph.}
A graph is defined as \( G = (V, E) \), where \( V \) is the set of nodes and \( E \) is the set of edges. The nodes are associated with a feature matrix $\mathbf{X} \in \mathbb{R}^{|V| \times d}$, where \( \vec{x}_v \in \mathbb{R}^d \) is a row of $\mathbf{X}$ representing the feature vector for node \( v \in V \). For a collection of multiple graphs, we define it as \( \mathcal{G} = \{ G_1, G_2, \dots, G_N \} \).

\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]{figures/framework.pdf}
\caption{Overall framework of \model.}
\label{fig.framework}
\end{figure*}


\stitle{Graph encoder.}
Towards graph representation learning, one of the most widely used families of graph encoders is graph neural networks (GNNs), which generally rely on message passing to capture structural knowledge \cite{wu2020comprehensive,zhou2020graph}. Each node updates its representation by aggregating information from its neighbors, and stacking multiple GNN layers enables iterative message propagation across the graph.
Formally, let $\vec{H}^l$ denote the embedding matrix at the $l$-th layer, where each row $\vec{h}_i^l$ represents the embedding of node $v_i$. This matrix is iteratively computed using the embeddings from the preceding layer:
\begin{equation}\label{eq.gnn}
\vec{H}^l = \textsc{MP}(\vec{H}^{l-1},G;\theta^l),
\end{equation}
where $\textsc{MP}(\cdot)$ is the message passing function, and $\theta^l$ represents the learnable parameters of the graph encoder at layer $l$. The initial embedding matrix, $\vec{H}^0$, is the input feature matrix, i.e., $\vec{H}^0=\vec{X}$. The output after a total of $L$ layers is then $\vec{H}^L$; for brevity we simply write $\vec{H}$. We abstract the multi-layer encoding process as 
\begin{align}
    \{\vec{H}^1, \vec{H}^2, \cdots, \vec{H}^L\} = \textsc{GraphEncoder}(\vec{X},G;\Theta),
\end{align}
where $\{\vec{H}^1, \vec{H}^2, \cdots, \vec{H}^L\}$ denotes the embedding matrix of the each layer of the graph encoder, respectively. $\Theta=(\theta^1,\ldots,\theta^L)$ is the collection of weights across the layers.

\stitle{Pre-training.}
As stated by prior studies \cite{yu2024generalized,yu2024non}, all contrastive pre-training task on graphs \cite{liu2023graphprompt,velickovic2019deep,you2020graph} can be unified under the task template of similarity calculation.
Formally, the unified pre-training objective is defined as follows:
\begin{equation}\label{eq:generalized_loss}
     \bL(\Theta)= -\sum_{o\in \bT_\text{pre}}\ln\frac{\sum_{a\in Pos_o}\exp(\text{sim}(\vec{h}_{a}, \vec{h}_{o})/\tau)}{\sum_{b\in Neg_o}\exp(\text{sim}(\vec{h}_{b}, \vec{h}_{o})/\tau)},
\end{equation}
where \( Pos_o \) and \( Neg_o \) denote the sets of positive and negative samples for a target instance \( o \), respectively. \( \vec{h}_o \) represents the embedding of the target instance, while \( \vec{h}_a \) and \( \vec{h}_b \) correspond to the embeddings of positive and negative samples. The hyperparameter \( \tau \) controls the temperature scaling in the similarity computation. In our framework, we follow previous work \cite{liu2023graphprompt,yu2024generalized} by employing similarity calculation as the task template and using link prediction as the pre-training task.

\stitle{Problem definition.}
In this work, we explore chain-of-thought (CoT) prompt learning framework for text-free graphs. We focus on two widely used tasks in graph learning: node classification and graph classification, in few-shot scenarios.
For node classification, given a graph \( G = (V, E) \) with a set of node classes \( Y \), each node \( v_i \in V \) is associated with a label \( y_i \in Y \). In contrast, graph classification considers a collection of graphs \( \mathcal{G} \), where each graph \( G_i \in \mathcal{G} \) is assigned a class label \( Y_i \in Y \).
In the few-shot setting, only \( m \) labeled examples per class are available (e.g., \( m \leq 10 \)), a paradigm defined as \( m \)-shot classification \cite{liu2023graphprompt,yu2024generalized}.
