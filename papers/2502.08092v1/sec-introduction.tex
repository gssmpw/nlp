\section{Introduction}
Recently, Chain-of-Thought (CoT) prompting has demonstrated significant advancement in the field of natural language processing (NLP) \cite{wei2022chain,wang2023self,chu2023survey}, which mimics the logical process a person may employ to solve a task. Instead of directly providing an answer, CoT prompting decomposes a problem into several steps, guiding the pre-trained language model to follow these steps that lead to the final answer. For example, 
given the math question in Fig.~\ref{fig.intro-motivation}(a), a language model processes with CoT (\eg, initial quantity, eaten quantity, and subtraction) to reach the final answer of 4.
%given the question ``If there are 9 apples, I eat 5 apples, how many apples are left?''. A language model answers with chain-of thoughts ``There are 9 apples initially. 5 are eaten. Now there are 9 - 5 = 4 apples. The answer is 4'', as shown in Fig.~\ref{fig.intro-motivation}(a). %achieving better performance across various downstream applications compared to finetuning \cite{bao2022beit,devlin2019bert} and conventional prompting \cite{brown2020language,liu2023pre} methods in NLP.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figures/motivation.pdf}
%\caption{Illustration of standard graph prompting and chain-of-thought prompting.}
\caption{Illustration of chain-of-thought in NLP, standard graph prompts and graph chain-of-thought prompting.}
\label{fig.intro-motivation}
\end{figure}

However, the vast potential of CoT prompt learning remains unexplored on pre-trained graph models. Graphs can capture the interactions between entities in a wide range of domains, exhibiting a non-linear topological structure, such as molecular graphs \cite{wang2023automated,lee2023shift}, citation networks \cite{kanakia2019scalable,xiong2017explicit}, and social networks \cite{ji2023community,zhang2023capacity}.
Conventional approaches typically retrain graph neural networks (GNNs) \cite{kipf2016semi,velivckovic2017graph} or graph transformers \cite{ying2021transformers,yun2019graph} for each specific task in an end-to-end supervised manner, relying on abundant labeled data.
More recent pre-training methods \cite{velickovic2019deep,you2020graph} learn task-invariant, general properties from unlabeled graphs through self-supervised pretext tasks and are then fine-tuned via task-specific labels to adapt to various downstream applications \cite{hu2020gpt,qiu2020gcc}. To further narrow the gap between pre-training and downstream tasks, prompt learning \cite{yu2024few,liu2023graphprompt,fang2024universal} has emerged as a low-resource alternative. They unify the objectives of pre-training and downstream tasks using the same template and employ a lightweight prompt to modify either the input features or hidden embeddings of the pre-trained graph encoder, while keeping the pre-trained weights frozen, as shown in Fig.~\ref{fig.intro-motivation}(b).
A contemporary work \cite{jin2024graph} leverages graphs to guide reasoning; however, it is built on the CoT mechanism from NLP and relies on textual data to construct thoughts, precluding its application to general \emph{text-free} graphs.
For text-free graphs, existing graph learning methods---including supervised, pre-training, and prompting approaches---produce a ``final answer'' in a single inference step, which may limit the refinement of the final prediction. In this work, we explore the following question: \textit{Would introducing additional inference steps in a CoT style enhance the ability of pre-trained graph models to refine their predictions?} 

Due to the significant differences between language and graph data, replicating CoT prompting from NLP for graphs is challenging. In NLP, a CoT prompt can be handcrafted before the learning phase and typically consists of a structured text in the form \(\langle \text{input, chain of thought, output} \rangle\). This prompt serves as an example to guide the model in generating intermediate thoughts that lead to the final answer. In contrast, our work explores a different prompting format for text-free graphs, which mimics the CoT approach in NLP but is not a direct application. Instead of designing prompts prior to the learning phase, \textit{we generate them step by step based on intermediate ``thoughts''}, improving the model's inference ability on downstream tasks by incorporating additional inference steps while freezing the pre-trained weights. To realize this vision, we must address two questions.

First, \textit{what should be the inference steps and thoughts for a graph task?} In NLP, a thought is defined as a short instruction that reflects a single reasoning step, with each intermediate textual answer serving as a thought \cite{chu2023survey}. However, in general text-free graphs, we cannot directly leverage text as prompts or thoughts. %A previous work \cite{jin2024graph} leverages graphs to guide reasoning, but it relies on textual data to construct thoughts, limiting its applicability to general, text-free graphs. 
In this work, we aim to improve the inference capability of a pre-trained graph model by incorporating additional steps to refine the answer. We design an \emph{inference step} with three substages: prompt-based inference, thought construction, and prompt learning. For prompt-based inference, we feed the input graph for the downstream task, along with the prompts, into a pre-trained graph encoder. Then, 
we construct a ``thought'' by fusing embeddings from each layer of the pre-trained encoder
to capture the current working state with varying levels of graph topological knowledge \cite{kipf2016semi}. Lastly, the thought is used to learn a set of prompts that guide the next step.

Second, \textit{how can we leverage a ``thought'' to learn prompts and guide the next-step inference?} In NLP, chain-of-thought (CoT) prompting is typically implemented by appending specific phrases such as ``let's think step by step'' or by providing few-shot CoT examples \cite{wei2022chain,feng2024towards}. Then, following a given prompt template, the language model generates new thoughts based on the query and prior thoughts, which in turn facilitate the next reasoning step. %However, for text-free graphs, the absence of textual data prevents us from directly instructing the model to learn step by step or providing it with exemplar thoughts. 
In our work, the absence of textual data prevents us from explicitly guiding the next step. 
Moreover, since each node in a graph exhibits unique characteristics, inference may benefit from node-specific prompts. Thus, we propose a \emph{thought-conditioned prompt learning} method to guide the next inference step. Specifically, inspired by conditional prompt learning \cite{zhou2022conditional}, we generate a unique prompt for each node via a conditional network (condition-net), which is conditioned on the node-specific element of the previously constructed thought. The generated prompts then feed into the graph encoder to initiate and guide the next step, repeating the process. 
%This approach guides the model to learn iteratively with patterns tailored to each node's characteristics. These prompts then modify the query graph's node features, effectively guiding the model to the next inference step.

In summary, the contributions of this work are fourfold.
(1)  We propose \model, a Graph CoT prompting approach to guide pre-trained graph models to perform step-by-step inference. To the best of our knowledge, this is the first exploration of CoT prompting on text-free graphs.
(2) We design an inference step with three substages: prompt-based inference, thought construction, and thought-conditioned prompt learning. In particular, a thought is constructed by fusing embeddings from each layer of the graph encoder to capture fine-grained topological knowledge in each step. 
(3) We employ a condition-net to generate node-specific prompts based on the previous thought, enabling the model to perform inference for the downstream task through a step-by-step, node-specific adaptation.
(4) We conduct extensive experiments on eight benchmark datasets, demonstrating the superior performance of \model\ compared to a suite of state-of-the-art methods.