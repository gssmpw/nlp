\subsection{Further Descriptions of Datasets} \label{app.dataset}
In this section, we provide more comprehensive descriptions of the benchmark datasets used in our experiments, as summarized in Table~\ref{table.datasets},
in the following.
%Specifically, average node degree, average shortest path length \cite{borgwardt2005shortest}, and average clustering coefficient \cite{giatsidis2014corecluster} reflect the structural and topological properties of various datasets or domains, from which we observe that different domains exhibit unique structural characteristics, highlighting the importance of structural alignment in both multi-domain pre-training and cross-domain adaptation.

\begin{itemize}[leftmargin=*]
    \item \emph{Cora}\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/cora.npz}} \cite{mccallum2000automating} is a citation network composed of 2,708 research papers in the field of computing, each classified into one of seven categories. The network consists of 5,429 citation links between papers. Each paper is represented by a binary word vector, where each entry indicates the presence or absence of a word from a predefined vocabulary of 1,433 unique terms.
    \item \emph{Citeseer}\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/citeseer.npz}} \cite{sen2008collective} includes 3,312 computer science publications, categorized into six distinct classes, separate from those in \textit{Cora}. The citation network contains 4,732 edges. Each document is encoded as a binary word vector that captures the presence or absence of words from a dictionary comprising 3,703 unique terms.
    \item \emph{PubMed}\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/pubmed.npz}} \cite{sen2008collective} is a citation network of 19,717 biomedical articles related to diabetes, divided into three categories. The network includes 44,338 citation edges. Unlike \textit{Cora} and \textit{Citeseer}, each document is represented by a TF/IDF-weighted word vector derived from a dictionary of 500 unique terms.
    \item \emph{Photo}\footnote{\url{https://github.com/shchur/gnn-benchmark/raw/master/data/npz/amazon_electronics_photo.npz}} \cite{shchur2018pitfalls} consists of 7,487 photography-related products, each assigned to one of eight categories. The co-purchase network contains 119,043 edges, where connections indicate products frequently bought together. Each product is described by a feature vector extracted from metadata and customer reviews, with category labels corresponding to product types.
    \item \textit{MUTAG}\footnote{\url{https://huggingface.co/datasets/graphs-datasets/MUTAG}} \cite{debnath1991structure} is a dataset of nitroaromatic compounds aimed at predicting their mutagenic effects on \textit{Salmonella typhimurium}. Each compound is modeled as a graph, where nodes represent atoms with categorical labels (encoded as one-hot vectors) based on atom types, and edges depict the chemical bonds connecting them. The dataset comprises 188 molecular graphs with 7 unique node types.
    
    \item \emph{BZR}\footnote{\url{https://chrsmrrs.github.io/datasets/docs/datasets/}} \cite{nr} consists of 405 molecular graphs representing ligands that interact with benzodiazepine receptors. Each molecule is treated as an independent graph and is classified into one of two categories.
    
    \item \emph{COX2}\footnote{\url{https://chrsmrrs.github.io/datasets/docs/datasets/}} \cite{nr} includes 467 molecular structures of cyclooxygenase-2 inhibitors. In this dataset, nodes correspond to atoms, while edges define chemical bonds—which may be single, double, triple, or aromatic. The molecules are divided into two distinct classes.
    
    \item \emph{PROTEINS}\footnote{\label{footnote:data}\url{https://chrsmrrs.github.io/datasets/docs/datasets/}} \cite{borgwardt2005protein} is a dataset of protein structure graphs that encode both biochemical and structural properties. In this dataset, nodes represent secondary structural elements, while edges capture connectivity based on spatial proximity or amino acid sequence adjacency. Each node falls into one of three categories, and graphs are classified into two broader groups.
\end{itemize}


\subsection{Further Descriptions of Baselines} \label{app.baselines}
In this section, we provide additional details about the baselines used in our experiments.

\noindent (1) Supervised GNNs:
\begin{itemize}[leftmargin=*]
    \item \textbf{GCN} \cite{kipf2016semi}: A graph neural network that aggregates node information using mean-pooling, thereby enabling nodes to capture structural information from their neighbors.
    \item \textbf{GAT} \cite{velivckovic2017graph}: Unlike GCN, GAT incorporates attention mechanisms to assign different weights to neighboring nodes, refining the aggregation process based on their relative importance.
\end{itemize}

\noindent (2) Graph Pre-training and Fine-tuning Models:
\begin{itemize}[leftmargin=*]
    \item \textbf{DGI} \cite{velivckovic2017graph}: A self-supervised pre-training method that maximizes mutual information between local node embeddings and the graph’s global representation, thereby enhancing structural awareness.
    \item \textbf{InfoGraph} \cite{sun2019infograph}: An extension of DGI designed for graph-level tasks, aligning node and graph representations by optimizing their similarity.
    \item \textbf{GraphCL} \cite{you2020graph}: A contrastive learning framework that leverages diverse graph augmentations to extract structural patterns, aiming to improve representation consistency across transformations.
\end{itemize}

\noindent (3) Single-step Prompting Models:
\begin{itemize}[leftmargin=*]
    \item \textbf{ProG} \cite{sun2023all}: Reformulates node- and edge-level tasks as graph-level problems by employing prompt graphs with task-specific structures to guide adaptation.
    \item \textbf{GPF} \cite{fang2024universal}: A universal prompt-tuning strategy for pre-trained graph models that transforms input graph features to mimic various prompting effects.
    \item \textbf{GPF+} \cite{fang2024universal}: An enhanced version of GPF that integrates an attention mechanism to dynamically refine prompt representations.
    \item \textbf{GraphPrompt} \cite{liu2023graphprompt}: Bridges pre-training and downstream tasks using subgraph similarity-based prompting, where a learnable prompt is optimized to incorporate task-relevant information for both node and graph classification.
\end{itemize}

\subsection{Implementation Details} \label{app.parameters}

\stitle{Environment.}
\label{app.general-setting}
\textbf{Optimizer:} 
The environment in which we ran all experiments is listed below.
\begin{itemize}
   \item Operating system: Ubuntu 22.04.2,
   \item CPU information: AMD EPYC 7742 64-Core Processor,
   \item GPU information: NVIDIA GeForce RTX 3090 (24 GB).
\end{itemize}

We outline key settings for the baselines and \model. 

\stitle{Baseline settings.}
We utilized the official codes for all open-source baselines. Each model was tuned based on the settings recommended in their respective work to achieve optimal performance.

For the baseline GCN \cite{kipf2016semi}, we employ a 2-layer architecture, and set the hidden dimensions to 64. 
For GAT \cite{velivckovic2017graph}, we employ a 2-layer architecture and set the hidden dimension to 64. Additionally, we apply 8 attention heads in the first GAT layer.

For DGI \cite{velivckovic2017graph}, we utilize a 1-layer GCN as the backbone and set the hidden dimensions to 256. Additionally, we employ prelu as the activation function.
For InfoGraph \cite{sun2019infograph}, a 1-layer GCN is used as the backbone, with its hidden dimensions set to 256.
For GraphCL \cite{you2020graph}, a 1-layer GCN is also employed as its backbone, with the hidden dimensions set to 256. Specifically, we select edge dropping as the augmentations, with a default augmentation ratio of 0.2.
For GraphPrompt \cite{liu2023graphprompt}, a 3-layer GCN is used as the backbone for all datasets, with the hidden dimensions set to 256.
For GPF \cite{fang2024universal}, we employ a 3-layer GCN as the backbone for all datasets, following the recommended settings. The hidden dimensions are set to 256.
For GPF+ \cite{fang2024universal}, we employ a 3-layer GCN as the backbone for all datasets, following the recommended settings. The hidden dimensions are set to 256.
For ProG \cite{sun2023all},

\stitle{\model\ settings.}
For our proposed \model, we utilize a 3-layer GCN as the backbone for all datasets, with the hidden dimensions set to 256. We set the number of inference step as 2, and the number of bias prompts to generate inital prompts as 5.


\subsection{Details about Heterophilic Datasets}\label{app.hetero}
To examine the robustness of \model\ on heterophilic graphs, we conduct experiments on heterophilic datasets in Sect.~\ref{sec.hetero}. The details of these datasets are as follows:
(1) \emph{Cornell} \cite{pei2020geom} is a network of 183 nodes representing webpages, with 295 edges indicating hyperlink connections among them. Node features are constructed using a bag-of-words approach based on webpage content. Each node is assigned to one of five categories: student, project, course, staff, or faculty.
(2) \textit{Squirrel} \cite{rozemberczki2021multi} consists of 5,201 Wikipedia pages discussing predefined topics. Nodes represent individual webpages, while edges capture 217,073 hyperlink connections between them, forming a page-page network. The dataset is categorized into five groups based on average monthly traffic. Node features are constructed using key informative nouns extracted from the text content of the Wikipedia pages.

