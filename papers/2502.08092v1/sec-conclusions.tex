\section{Conclusions}
In this paper, we propose \model, the first CoT prompting framework for graphs. We define an inference step with three substages: prompt-based inference, thought construction, and thought conditioned prompt learning. Specifically, we first feed the prompt modified query into the pre-trained encoder, and then construct a thought by fusing the hidden embeddings from each layer of the pre-trained graph encoder. To guide the subsequent inference step, we generate a series of prompts conditioned on the thought from the previous step. By repeating the above inference steps, \model\ obtain the answer. Finally, we conduct extensive experiments on eight public datasets, demonstrating that \model\ significantly outperforms a range of state-of-the-art baselines.
