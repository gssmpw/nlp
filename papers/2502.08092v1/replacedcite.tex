\section{Related Work}
\stitle{Chain-of-Thought prompting.}
Chain-of-Thought (CoT) prompting has emerged as a groundbreaking technique in natural language processing (NLP), empowering language models to address complex reasoning tasks by producing intermediate reasoning steps ____. By breaking down problems into a sequence of logical steps, CoT emulates human-like thought processes, leading to significant improvements in model performance on tasks requiring structured reasoning ____. Despite its remarkable success in the NLP domain, the potential of CoT for graphs remains largely unexplored.  A contemporary work, GraphCoT ____, leverages the inherent relational information in text-attributed graphs ____ to guide the reasoning process. However, GraphCoT primarily focuses on question-answering tasks for natural language and cannot be extended to general graphs lacking textual descriptions.

\stitle{Graph representation learning.}
GNNs ____ are dominant technique for graph representations learning. They generally update nodes embeddings iteratively by aggregating information from their local neighborhoods based on a message-passing mechanism ____. Despite their success, GNNs often demand substantial amounts of task-specific labeled data and necessitate retraining for each new task, limiting their flexibility and scalability.
Recently, researchers have extensively investigated pre-training techniques for graphs ____. These approaches involve pre-training a graph encoder using self-supervised objectives, and then adapt the pre-trained knowledge to downstream tasks. However, a significant gap exists between the objectives of pre-training and those of downstream tasks, resulting in suboptimal performance. %Moreover, these methods typically employ only a single inference step during the downstream phase, which may further lead to their suboptimal performance.


\stitle{Graph prompt learning.}
First proposed in NLP, prompt learning has emerged as a powerful framework for bridging the gap between pre-training and downstream tasks ____. Recently, this paradigm has been extended to the graph domain as a compelling alternative to fine-tuning approaches ____. These methods typically utilize a universal template to align pre-training and downstream tasks, followed by task-specific prompts that facilitate seamless adaptation to downstream tasks while keeping the pre-trained model frozen. However, current graph prompt learning methods directly produce a final answer in a single step, resulting in insufficient refinement to the answer.