\section{Related Works}
\label{2}

Fault prediction in distribution grid insulators is critical for maintaining the reliability and safety of power systems~\citep{rocha2019inspection}. Recent studies have explored various methodologies to predict insulator failures, focusing on analyzing leakage currents, employing ML techniques, and utilizing advanced signal processing methods. A way to identify faults in this context involves monitoring the time series data of the leakage current of insulators under contaminated conditions.

Several authors have applied computer vision-based methods considering convolutional neural networks (CNNs)  for insulator fault identification~\citep{9314102}. Prominent in this area is the you only look once (YOLO) model, which was applied in its third generation by~\citet{9866802}, in the fourth generation by~\citet{10012377}, in the fifth generation by~\citet{10106252}, considering a hypertuned version by~\citet{STEFENON2024102722}, or using a hybrid version in~\citep{gtd2.12886}. 

\citet{deng2022research} proposed a modified YOLO that can be computed on edge devices. The modification to make the algorithm more efficient was in the backbone of the YOLO. Instead of using a CSPDarknet53 (standard for the YOLO version that they considered), they applied a lightweight network MobilieNetv3. Based on this modification, the proposed algorithm achieved an accuracy of 0.945 with a speed of 58.5 frames per second. \citet{10298815} also applied a modified version of YOLO, considering a model called multi-fault insulator detection they achieved an accuracy of 0.939. 


\citet{9537742} presented an improved faster region CNN for insulator fault detection. They considered preprocessing techniques for image segmentation, reducing the image noise, and having the focus on the insulators, even when complex backgrounds are considered. Based on their model, it was possible to achieve a mean average precision of 0.908 considering glass insulators and 0.917 when composite insulators were evaluated. Based on CNNs, \citet{10258031} considered infrared images for insulator defect diagnosis. Applications based on CNNs are promising, as they can be handled indirectly (without contact with the electrical network)~\citep{singh2023interpretable}, while in the case of leakage current it is necessary to measure the insulators directly, making it a more challenging approach.

\citet{s22166121} subjected insulators to saline environments to simulate contamination and analyzed the progression of leakage currents leading up to disruptive discharges. The researchers evaluated several time series forecasting models, including group method of data handling (GMDH), long short-term memory (LSTM), adaptive neuro-fuzzy inference system (ANFIS), and various ensemble learning models. They found that integrating wavelet transforms with these models improved prediction accuracy, with the wavelet-ANFIS model achieving the best performance. In \citep{s23136118} the Christiano-Fitzgerald filter was combined with the GMDH to predict faults in contaminated insulators.

Studies of the contamination process that leads to the development of flashover, such as the one presented by~\citet{9828523} are rarer. Especially when it comes to applications of ML models, as presented by~\citet{10587253}, or hybrid methods for time series forecasting. In other applications, some authors have researched how to better identify faults based on time series analysis, as presented in Table~\ref{re_w}.

\begin{table}[]
%\footnotesize
\small
\caption{Fault prediction and anomaly detection using ML approaches considering time series.}
\label{re_w}
\centering
\begin{tabular}{p{3.6cm}p{6cm}p{5cm}}\hline
Work  & Method & Application\\ \hline
\citep{10356858} & Parallel time series modeling with LightNet and DarkNet. & Fault detection on intelligent vehicles.\\
\citep{guo2021mechanical} & Error fusion of multiple sparse auto-encoder LSTM. & Mechanical fault prediction.\\
\citep{xu2021two} & Attention-based-LSTM, random forest, and extra-tree. & Machinery fault prediction.
\\
\citep{liu2021machinery} & CNN, gated recurrent unit (GRU), attention, and knowledge graph. & Machinery fault diagnosis.\\
\citep{hsu2021multiple} & Multiple time-series CNN. & Fault %detection and diagnosis 
in semiconductor. \\% manufacturing.\\
\citep{he2022graph} & Masked spatial graph attention network with GRU. & Fault detection for unmanned aerial vehicles.\\
\citep{jin2022time} & Time series transformer. & Machinery fault diagnosis. \\
\citep{chen2023unsupervised} & Unsupervised deep autoencoder with dimension fusion function. & Fault detection in aeroengines. \\
\citep{zhang2023gaussian} & Gaussian-linearized transformer with %tranquilized time series 
decomposition. & Fault diagnosis in methane gas sensors.\\
\citep{xie2021attention} & CNN-LSTM with attention. & Wind turbine fault prediction.\\
\citep{arunthavanathan2021deep} & CNN-LSTM with one-class support vector machine (SVM). & Fault detection in multivariate complex process systems.\\
\citep{nguyen2021forecasting} & 
Standard LSTM and an LSTM autoencoder with a one-class SVM. & Anomaly detection in supply chain management.\\
\citep{9612196} & Spatial and temporal attention-based GRU with seasonal-trend decomposition. & Fault diagnosis  of electro-mechanical actuators.\\
\citep{10662411} & Autoformer enhanced by Dilated loss module. & Potential bushing and transformer faults.\\
\citep{chen2021bearing} & Multi-scale CNN and LSTM. & Bearing fault diagnosis. \\
\citep{s22218323} & Wavelet tranform with LSTM. & Fault %forecasting 
in power grids.\\
\citep{211126} & Linear regression, support vector regression, multilayer Perceptron, deep neural network, and RNNs. & Failure prediction in contaminated insulators.\\
\citep{SEMAN2023109269} & Ensemble random subspace with Hodrick–Prescott filter.& Fault forecasting in pin-type insulators.\\
\citep{klaar2023optimized} & EWT-sequence-to-sequence-LSTM with attention mechanism. &  Insulators fault prediction.\\
\citep{3076410} & Stacking ensemble learning model with wavelet transform. & Insulators contamination forecasting.\\
\citep{branco2024bootstrap} & Bootstrap aggregation with Christiano–Fitzgerald random walk filter. & Fault prediction based on leakage current.\\
\citep{en13020484} & Adaptive neuro-fuzzy inference system with wavelet packets transform. & Insulator fault forecasting.\\
\hline 
\end{tabular}
\end{table}

Several authors (see Table \ref{re_w}) presented hybrid methods for predicting time series, such as CNN-LSTM, which uses CNN for feature extraction and LSTM for time series forecasting, showing that combining techniques with different objectives can improve the architecture, resulting in a hybrid approach that outperforms the lasted architectures for time series forecasting. A more detailed discussion of which state-of-the-art models are used for time series forecasting is presented in the next subsection.

\subsection{State-of-the-Art in Time Series Forecasting}

In recent years, advances in ML and DL models have driven the development of more robust and accurate methods to predict time series. 
In this regard, DL-based approaches including recurrent neural networks (RNN)~\citep{rnnTS2021} and transformers~\citep{transformerTS2023}, have shown promising performance.
In~\citep{tft2021} the authors considered the use of the temporal fusion transformer (TFT) model for time series. This model is an attention-based deep neural network architecture designed for multi-horizon time series forecasting, combining high performance with interpretability. The model incorporates static covariate encoders, gating mechanisms, variable selection networks, and hybrid temporal processing, which uses LSTMs for local patterns and self-attention to capture long-term dependencies.

\citet{NBEATS2020} proposed the neural basis expansion analysis for time series forecasting (N-BEATS), which is a deep neural network architecture designed for univariate time series forecasting based on residual connections and multiple fully connected layers. The deep temporal convolutional network (DeepTCN) was proposed in~\citep{cnnTS2020} and is a CNN architecture developed for probabilistic forecasting of multiple related time series. The model employs dilated causal convolutions, which guarantee dependence only on past inputs and capture long-range patterns with computational efficiency. DeepTCN demonstrates robustness outperforming models such as seasonal autoregressive integrated moving average, light gradient boosting machine~\citep{lightgbmTS2017}, and the probabilistic forecasting with autoregressive recurrent network~\citep{deepARTS2020}.

In~\citep{dotsTS2020} the authors presented the multivariate time series forecasting with a graph neural networks model. This is a graph neural network designed-based model to forecast multivariate time series by learning the underlying graph structure. Its architecture combines three main components: a graph learning layer, which extracts dynamic relationships between series without the need for a predefined graph structure; graph convolution modules, which model spatial dependencies between variables; and temporal convolution modules, which capture long-term patterns through dilated convolutions, making it a robust approach for spatiotemporal forecasting in different datasets types.

\citet{spatioTS2018} proposed the spatiotemporal graph convolutional network (STGCN), which is a DL architecture designed for traffic prediction, combining graph convolutions and temporal convolutions with gated linear units to efficiently model spatial and temporal dependencies. The architecture is composed of spatiotemporal convolutional blocks, where causal temporal convolutions extract sequential patterns and graph convolutions capture spatial relationships without relying on a fixed grid structure. STGCN is computationally efficient, enabling fast and scalable training for large networks. The model outperformed other approaches such as 
full-connected LSTM and graph convolutional GRU in several error metrics such as mean absolute error (MAE), mean absolute percentage error (MAPE), and root-mean-square error (RMSE), consuming up to 14 times less training time compared to state-of-the-art models.

In~\citep{hybridTS2023} the authors claim that using hybrid models can improve time series forecasting. The hybrid model proposed by the authors combines CNNs, attention-based LSTM (A-LSTM), and an auto-regressive model (AR) to forecast energy generation from multiple renewable sources. CNN captures spatial correlations between energy sources, A-LSTM models non-linear temporal patterns, and AR extracts linear trends. Its main advantages include the ability to model capture complex temporal patterns and predictive superiority over traditional models such as artificial neural networks and decision trees. The model also demonstrated a significant reduction in prediction errors compared to previous state-of-the-art approaches, reducing MAE by up to 27.1\% and MAPE by 53.6\% for photovoltaic solar energy. The high R$^{2}$ values ($>$0.945) confirm its good fit with the observed data, making it a robust and effective solution for forecasting renewable energy.

\citet{emWavTS2023} employed the EWT high-order fuzzy cognitive map that is also a hybrid model for forecasting time series that combines the EWT, high-order fuzzy cognitive maps (HFCM), and ridge regression. The model's architecture decomposes the time series data with EWT, which adapts filters to the signal spectrum, followed by modeling with HFCM, which captures long-term temporal dependencies, and hyperparameter optimization with ridge regression, avoiding overfitting. The forecast is reconstructed using inverse EWT, allowing complex patterns in non-stationary series to be captured. Evaluated with RMSE on 15 real data sets, the model outperformed 11 state-of-the-art algorithms, including LSTM, RNN, ANFIS, temporal convolutional network (TCN), and CNN-fuzzy cognitive maps.

~\citet{timellmTS2024} proposed a time series forecasting by reprogramming large language models (time-LLM) framework to predict time series, without modifying the backbone language model. The approach transforms time series into prototypical textual representations and uses the prompt-as-prefix technique to improve the input context. Its main advantages include generalization to multiple domains, data efficiency, advanced reasoning capabilities, and no need for fine-tuning, allowing robust predictions even with few examples or in zero-shot learning scenarios. Evaluated on different datasets, time-LLM outperformed conventional models in metrics such as mean-square error, MAE, symmetric MAPE (SMAPE), and overall weighted average, demonstrating high accuracy in short and long-term forecasts.

In~\citep{hyperparameterTS2024} proposes the use of CNN-LSTM for wind energy forecasting, with advanced hyperparameter optimization to improve accuracy and efficiency. CNN is used to extract spatial patterns from the data, while LSTM models short- and long-term temporal dependencies. Different hyperparameter optimization algorithms are evaluated, %(Scikit-opt, Optuna, and Hyperopt), 
with Bayesian optimization via TPE being the most common approach. The results show that the advanced selection of hyperparameters significantly improves the effectiveness of wind energy forecasting, making the models more %accurate and 
reliable.

These studies underscore the importance of integrating advanced analytical methods, such as ML and signal processing, with traditional monitoring techniques to improve fault prediction in distribution grid insulators. Continued research in this area is essential to develop more accurate and reliable predictive models, ultimately contributing to the improved stability and efficiency of power distribution grids.