% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\documentclass[11pt]{article}
\usepackage{arabtex}
\usepackage{utf8}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{pifont}
\usepackage{graphicx} % Required for including images
\usepackage{lipsum}   % For sample text



% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl_natbib}
\usepackage{times}
\usepackage{latexsym}
% \setlength{\parindent}{0pt}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0} 
\definecolor{verylightgray}{rgb}{0.97, 0.97, 0.97}
\newcommand{\cmark}{\textcolor{darkgreen}{\scalebox{1}[1.0]{\ding{51}}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}  % Cross

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{
%   {\includegraphics[width=1cm]{LOGO.png} CAMEL-Bench: A Comprehensive Arabic LMM  Benchmark
% }}

\title{
    \raisebox{-0.3cm}{\includegraphics[width=1.2cm]{figures/LOGO_1.pdf}}  % Adjust vertical position
    \hspace{0.01cm} \textbf{\Large KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding}
}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{ 
 Ahmed Heakl$^{\spadesuit \clubsuit}\footnotemark[1]$\hspace{0.5mm},
 Abdullah Sohail$^{\spadesuit}\footnotemark[1]$\hspace{0.5mm},
 Mukul Ranjan$^{\spadesuit}\footnotemark[1]$\hspace{0.5mm}, 
 Rania Hossam$^{\spadesuit}\footnotemark[1]$\hspace{0.5mm},
 Ghazi Ahmed$^\spadesuit$\hspace{0.5mm} \\
  \bf{Mohamed El-Geish$^{\clubsuit}$\hspace{0.5mm},
 Omar Maher$^{\clubsuit}$\hspace{0.5mm},  
 Zhiqiang Shen$^{\spadesuit}$\hspace{0.5mm},
 Fahad Khan$^{\spadesuit \heartsuit}$\hspace{0.5mm},
 Salman Khan$^{\spadesuit \diamondsuit}$}\hspace{0.2mm}\hspace{1.5mm} \\
$^\spadesuit$ MBZUAI \ \ \ \quad$^\clubsuit$Monta AI \quad$^\heartsuit$Link√∂ping University \ \ \ \quad$^\diamondsuit$Australian National University\ \ \ 
 \\
 \quad\texttt{\{ahmed.heakl,mabdullah.sohail,mukul.ranjan,salman.khan\}@mbzuai.ac.ae} \\
 \quad\texttt{\{geish,omar\}@monta.ai} \\
 \url{https://mbzuai-oryx.github.io/KITAB-Bench/}
}

% \author{Ahmed Heakl \\
%   MBZUAI, Monta AI \\
%   \texttt{ahmed.heakl@mbzuai.ac.ae} \\\And
%   Mohammed Sohail \\
%   MBZUAI \\
%   \texttt{email@domain} \\\And
%   Mukul Ranjan \\
%   MBZUAI \\
%   \texttt{email@domain} \\
%   \And \\
%   Mukul Ranjan \\
%   MBZUAI \\
%   \texttt{email@domain} \\\And 
%   }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{\ Equal Contributions}

\begin{abstract}
With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. 
While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. 
We present \textbf{\texttt{KITAB-Bench}}, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems.
Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence.
Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of $60\%$ in Character Error Rate (CER).
Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only $65\%$ accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection.
This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies.
\end{abstract}

% \input{sections/intro}
\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\textwidth]{figures/benchmark_taxonomy_final_v11.pdf}
    \caption{Overview of the core domains and sub-domains in KITAB-Bench. Our benchmark spans nine major domains (e.g., OCR, charts to JSON, table recognition) and 36 sub-domains (e.g., scanned text, handwritten text, various chart types), providing a comprehensive evaluation framework for modern Arabic document processing and analysis.}
    \label{fig:task_taxonomy}
\end{figure}

With the upsurge in adoption of Retrieval-Augmented Generation (RAG) based systems for document processing, the quality of document ingestion pipelines has become increasingly critical. Optical Character Recognition (OCR) plays a crucial role in this pipeline, enabling the conversion of physical documents into machine-readable text and databases for enabling effective knowledge retrieval. Although significant progress has been made in the multilingual OCR \cite{easyocr, fu2024ocrbench, wei2024general, smith2007overviewtesseract}, with comprehensive datasets like PubLayNet \cite{zhong2019publaynet}, DocBank \cite{li2020docbank}, M6Doc \cite{cheng2023m6doc}, and DocLayNet \cite{doclaynet2022}, Arabic OCR continues to lag behind. This gap is largely due to the unique challenges of the Arabic script, including its cursive nature, complex typography, and right-to-left text orientation.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/pipeline_tasks_overview_v11.pdf}
    \caption{Overview of different tasks in our benchmark: Eight key components illustrating the task inputs and outputs for table recognition, chart understanding, text recognition, diagram analysis, VQA, line detection, layout analysis, and PDF-to-Markdown conversion, complete with input/output examples for each task.}
    \label{fig:pipeline_task_overview}
\end{figure*}


% \input{tables/arabic_benchmark_comparison}



\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\hline
\textbf{Domain/} & \textbf{EXAMS-V$^*$} & \textbf{Camel-} & \textbf{MIDAD$^\dagger$} & \textbf{KHATT} & \textbf{KITAB-} \\
 \textbf{Characteristics} &  & \textbf{Bench} & \textbf{} & \textbf{} & \textbf{Bench (Ours)} \\
\hline
PDF to Markdown & \xmark & \xmark & \xmark & \xmark & \cmark \\
Layout Detection & \xmark & \xmark & \xmark & \xmark & \cmark \\
Line Detection & \xmark & \xmark & \xmark & \xmark & \cmark \\
Line Recognition & \xmark & \cmark & \xmark & \xmark & \cmark \\
Table Recognition & \xmark & \xmark & \xmark & \xmark & \cmark \\
Image to Text & \cmark & \cmark & \cmark & \cmark & \cmark \\
Charts to JSON & \xmark & \xmark & \xmark & \xmark & \cmark \\
Diagram to Code & \xmark & \xmark & \xmark & \xmark & \cmark \\
VQA & \cmark & \cmark & \xmark & \xmark & \cmark \\

Handwritten Samples & \xmark & \xmark & \cmark & \cmark & \cmark \\


Open Source & \cmark & \cmark & \xmark & \cmark & \cmark \\
\hline
Total Samples (\#) & 823 & 3,004 & 29,435 & 5,000 & 8,809 \\
\hline
\end{tabular}
}

\caption{Comparison of Arabic OCR Benchmarks Across Different Domains. Benchmarks compared: LaraBench \cite{abdelali2023larabench}, CamelBench \cite{ghaboura2024camel}, MIDAD \cite{bhatia2024qalam}, KHATT \cite{mahmoud2014khatt}, and KITAB-Bench (Ours).  ($*$: Only the Arabic  samples are considered.) ($\dagger$: The test set of the dataset is considered.)}
\label{tab:ocr_domain_comparison}
\vspace{-5pt}
\end{table}


\noindent Existing Arabic OCR datasets (Table~\ref{tab:ocr_domain_comparison}), like KHATT~\cite{mahmoud2014khatt} and IFN/ENIT~\cite{pechwitz2002ifn} focus mainly on handwritten text, whereas APTI~\cite{slimane2009new} covers only specific aspects of printed text. These efforts fail to address advanced document processing challenges such as table parsing, font detection, and numeral recognition. Arabic benchmarks like CAMEL-Bench~\cite{ghaboura2024camel} and LAraBench~\cite{abdelali2023larabench} evaluate large multimodal and language models, but they give limited attention to document understanding tasks. Consequently, there remains a need for a more comprehensive framework to systematically evaluate and compare Arabic OCR solutions. Our benchmark addresses these gaps by offering diverse document types and evaluation tasks to facilitate in-depth assessments of modern OCR systems.


% \noindent Current Arabic OCR datasets like KHATT \cite{mahmoud2014khatt} and IFN/ENIT \cite{pechwitz2002ifn} focus primarily on handwritten text recognition, while others like APTI \cite{slimane2009new} address specific aspects of printed text. However, these datasets do not provide comprehensive coverage of modern document processing challenges. Our benchmark fills this gap by incorporating diverse document types and evaluation tasks, enabling a more thorough assessment of OCR systems.
% \noindent There remains a lack of comprehensive evaluation frameworks for Arabic document processing. The absence of standardized benchmarks has made it particularly challenging to systematically evaluate and compare Arabic OCR systems across different tasks. While datasets like CAMEL-Bench \cite{ghaboura2024camel} and LAraBench \cite{abdelali2023larabench} provide valuable insights for Arabic language processing, their primary focus is on evaluating large multimodal models (LMMs) and large language models (LLMs) across various NLP and speech processing tasks, including sequence tagging, content classification, and visual reasoning. However, they do not specifically emphasize document understanding tasks. Existing benchmarks like CAMEL-Bench focus on NLP tasks, leaving document understanding under-addressed. Current systems struggle with font detection, numeral recognition, and table structure parsing.

% The challenges in Arabic OCR extend beyond basic text recognition. Current systems struggle with complex font detection, often misinterpreting decorative Arabic text as images. Numerical recognition presents another significant challenge, with systems frequently confusing Arabic numerals with characters. Furthermore, the handling of elongated words and merged table cells remains problematic, affecting the accuracy of document structure analysis.

We present KITAB-Bench, a comprehensive Arabic OCR benchmark spanning 9 domains and 36 sub-domains. Our framework evaluates layout detection (text blocks, tables, figures), multi-format recognition (printed/handwritten text, charts, diagrams), and structured output generation (HTML tables, DataFrame charts, markdown). This enables rigorous assessment of both basic OCR capabilities and advanced document understanding tasks.

% Our benchmark aims to systematically evaluate OCR performance across diverse Arabic document elements while facilitating the development of robust Arabic-specific solutions for document analysis.


The contributions of this work include (1) A comprehensive Arabic OCR benchmark covering multiple document types and recognition tasks.
(2) Detailed evaluation metrics for assessing performance across different document understanding challenges. We also propose CharTeX and CODM metric to evaluate chart extraction and diagram extraction respectively.
(3) Baseline results for popular OCR systems and Vision Language Models (VLMs), highlighting current limitations and areas for improvement.
(4) A standardized framework for comparing Arabic OCR systems, facilitating future research and development.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.97\textwidth]{figures/comparison.pdf}
    \caption{Comparison of model performance across four document understanding tasks (Table Recognition, Image to Text, Diagram to JSON, and Layout Detection) showing successful and failed cases for different models including Ground Truth, EasyOCR, GPT-4, Qwen, Surya, Tesseract, Yolo, and DETR on Arabic document benchmark data.}
    \label{fig:model_comparison}
\end{figure*}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.95\textwidth]{figures/pipeline_tasks_overview_v11.pdf}
%     \caption{Overview of different tasks in our benchmark: Eight key components illustrating the task inputs and outputs for table recognition, chart understanding, text recognition, diagram analysis, VQA, line detection, layout analysis, and PDF-to-Markdown conversion, complete with input/output examples for each task.}
%     \label{fig:pipeline_task_overview}
% \end{figure*}
\section{Related Work}
The development of robust Optical Character Recognition (OCR) systems has been extensively studied across document layout analysis \cite{zhao2024doclayout, shen2021layoutparser, paruchuri2024surya, easyocr, auer2024docling, li2020docbank}, table detection \cite{li2019tablebank, paliwal2019tablenet, nassar2022tableformer, li2021gfte, schreiber2017deepdesrt}, and document understanding \cite{staar2018corpus, weber2023wordscape, livathinos2021robust}. While English OCR benefits from rich datasets like PubLayNet \cite{zhong2019publaynet}, DocBank \cite{li2020docbank}, M6Doc \cite{cheng2023m6doc}, and DocLayNet \cite{doclaynet2022}, Arabic lacks standardized benchmarks for diverse fonts and layouts. Recent efforts like MIDAD \cite{bhatia2024qalam} curates extensive training data for Arabic OCR and handwriting recognition, while Peacock \cite{alwajih2024peacock} introduces culturally-aware Arabic multimodal models. Existing resources such as CAMEL-Bench \cite{ghaboura2024camel}, LAraBench \cite{abdelali2023larabench}, MADAR \cite{bouamor2018madar}, OSACT \cite{mubarak2022overview}, and Tashkeela \cite{zerrouki2017tashkeela} focus on language modeling or specific tasks rather than full-page OCR evaluation. Handwriting datasets including HistoryAr \cite{pantke2014historical}, IFN/ENIT \cite{pechwitz2002ifn}, KHATT \cite{mahmoud2014khatt}, APTI \cite{slimane2009new}, and Muharaf \cite{saeed2024muharaf} emphasize word/line recognition over document structure analysis.

\input{tables/distribution}

\noindent Arabic table recognition faces challenges from merged cells and RTL formatting \cite{pantke2014historical}. While methods like GTE \cite{zheng2021global}, GFTE \cite{li2021gfte}, CascadeTabNet \cite{prasad2020cascadetabnet}, TableNet \cite{paliwal2019tablenet}, and TableFormer \cite{nassar2022tableformer} advance Latin table detection, their effectiveness on Arabic documents remains unproven. Document conversion pipelines (CCS \cite{staar2018corpus}, Tesseract \cite{smith2007overviewtesseract}, Docling \cite{auer2024docling}, Surya \cite{paruchuri2024surya}, Marker \cite{paruchuri2024marker}, MinerU \cite{wang2024mineru}, PaddleOCR \cite{du2020paddleocr}) lack Arabic-specific optimizations for segmentation and diacritic handling \cite{mahmoud2018online, badam2024benjamin}. This highlights the critical need for comprehensive Arabic OCR benchmarks addressing text recognition, table detection, and layout parsing.

\section{KITAB-Bench}
Our methodology offers a novel approach to benchmarking Arabic OCR systems via a comprehensive data collection strategy and a systematic evaluation framework. We gather curated samples from existing Arabic document datasets, manually collected and annotated PDFs, and employ a five-phase LLM-assisted human-in-the-loop pipeline (Figure~\ref{fig:data_creation_pipeline}) to generate diverse supplementary content. Our evaluation framework spans nine specialized tasks, enabling thorough assessment of OCR performance across various document processing challenges and providing a robust benchmark for Arabic document understanding tasks.
\begin{figure*}[htbp]
    \centering
    \small
    \includegraphics[width=0.95\textwidth]{figures/data_creation_pipeline_diagram.pdf}
    \caption{Synthetic Data Generation Pipeline: A 5-stage process using LLMs to generate topics, create raw data, produce visualization code, render charts, and perform human evaluation for quality control.}
    \label{fig:data_creation_pipeline}
\end{figure*}

\subsection{PDF Data Collection}

We curated 33 diverse PDFs from online sources in academia, medicine, law, and literature. To ensure challenging cases, we selected documents featuring richly formatted tables with extensive color usage, merged cells, Arabic numerals, historical texts, watermarks, and handwritten annotations. Each PDF averaged three pages, and we then manually annotated them. This dataset comprehensively captures real-world complexities, making it a valuable benchmark for PDF-to-Markdown conversion.


% \subsubsection{Existing Dataset Curation}
% \label{subsection:data_curation}
% We carefully curated samples from established Arabic document datasets to ensure comprehensive coverage of real-world scenarios. For layout analysis, we incorporated 1,700 samples from BCE-Arabic-v1 dataset \cite{saad2016bce}, 400 samples from DocLayNet dataset \cite{doclaynet2022} focusing on financial, academic, legal, and patent documents. The line detection and recognition tasks contains 378 samples each from self-developed dataset, supplemented by 456 samples from generated using our pipeline pipeline for table structure analysis. We further enriched the dataset with 500 samples from PATS-A01 \cite{muhtaseb2010pats} benchmark to ensure diverse representation.

% For handwritten text recognition, we assembled a comprehensive collection of 1,000 samples combining datasets from Khatt \cite{mahmoud2014khatt} (both paragraph and line-level annotations), Adab \cite{adab2019bench}, Muharaf \cite{saeed2024muharaf}, and OnlineKhatt \cite{mahmoud2018online}. The benchmark also includes specialized content from ISI-PPT \cite{wu2017iccv} (500 samples), and Hindawi \cite{elfilali2023hindawi} (200 samples) for various document types. Scene text understanding is supported by 800 samples from EvArest \cite{hassan2021arabicEvArEST}, providing real-world context diversity. A detailed table showing all the dataset is provided in the Appendix \ref{sec:appendix1}.

\subsection{LLM-Assisted Data Generation Pipeline}
\label{subsection:data_generation}
To generate data for charts, diagrams and tables, we implemented a five-phase LLM-assisted generation pipeline with human validation at critical stages, as illustrated in Figure~\ref{fig:data_creation_pipeline}. 
\textit{In Phase I (Topic Generation)}, our system employs an LLM to generate diverse topic names across multiple domains. This phase incorporates various personas (academic, legal, medical, technical) to ensure broad coverage of document types. 
\textit{Phase II (Data Generation)} transforms the validated topics into structured raw data. The LLM generates content following Arabic linguistic and formatting conventions across various domains. 
\textit{In Phase III (Code Generation)}, the system converts the validated raw data into plotting code, with special attention to Arabic text rendering requirements and RTL content management. 
\textit{Phase IV (Image Rendering)} utilizes specialized rendering engines (Mermaid, Plotly, Vegalite, HTML) to create visual representations while maintaining Arabic text integrity.

\textit{The final phase (Human Evaluation)} implements rigorous quality control through expert validation. Evaluators filter charts, tables and diagrams based on detected anomalies and ensure adherence to Arabic-specific document conventions. This phase is crucial for maintaining the high quality of our benchmark dataset.
\setlength{\parindent}{0pt}

\subsection{Dataset Statistics}

Our benchmark dataset comprises over 8,809 samples across 9 major domains and 36 sub-domains, representing a comprehensive collection of Arabic document types for OCR evaluation. As detailed in Table \ref{tab:dataset_distribution}, the dataset combines carefully curated samples from established datasets, manually annotation PDFs, and synthetically generated content created through our LLM-assisted pipeline (Figure \ref{fig:data_creation_pipeline}). The Image-to-Text portion (3,760 samples) includes data from historical documents (HistoryAr \cite{pantke2014historical}), handwritten text collections (Khatt \cite{mahmoud2014khatt}, ADAB \cite{adab2019bench}, Muharaf \cite{saeed2024muharaf}), and scene text (EvAREST \cite{hassan2021arabicEvArEST}), while layout detection comprises 2,100 samples from BCE-Arabic-v1 \cite{saad2016bce} and DocLayNet \cite{doclaynet2022}.

For layout analysis, we incorporated 1,700 samples from BCE-Arabic-v1 dataset \cite{saad2016bce}, 400 samples from DocLayNet dataset \cite{doclaynet2022} focusing on financial, academic, legal, and patent documents. The line detection and recognition tasks contains 378 samples each from self-developed dataset. We further enriched the dataset with 500 samples from PATS-A01 \cite{muhtaseb2010pats} benchmark to ensure diverse representation.
\input{tables/line-detection}
For handwritten text recognition, we assembled a comprehensive collection of 1,000 samples combining datasets from Khatt \cite{mahmoud2014khatt} (both paragraph and line-level annotations), Adab \cite{adab2019bench}, Muharaf \cite{saeed2024muharaf}, and OnlineKhatt \cite{mahmoud2018online}. The benchmark also includes specialized content from ISI-PPT \cite{wu2017iccv} (500 samples), and Hindawi \cite{elfilali2023hindawi} (200 samples) for various document types. Scene text understanding is supported by 800 samples from EvArest \cite{hassan2021arabicEvArEST}, providing real-world context diversity. A detailed table showing all the dataset is provided in the Appendix \ref{sec:appendix1}.

A significant portion of our dataset consists of synthetically generated content, including 576 samples for Charts-to-DataFrame (spanning 16 different chart types), 422 samples for Diagram-to-Code (covering sequence diagrams, flowcharts, and tree maps), 456 samples for Tables-to-CSV/HTML, and 902 samples for VQA tasks. These synthetic samples were generated through our five-phase LLM-assisted human-in-the-loop pipeline (Figure \ref{fig:data_creation_pipeline}). Every sample in our dataset - whether from existing sources or newly generated - underwent validation by native Arabic speakers before inclusion in the final benchmark. This rigorous validation, reinforced by expert review and automated checks, ensures high quality and authenticity across all domains. A detailed analysis is in Appendix \ref{sec:appendix3}.



\section{Experiments}
Our experimental evaluation comprehensively assesses the capabilities of current OCR systems and state-of-the-art vision-language models (VLMs) across different Arabic and multilingual document understanding tasks. Figure~\ref{fig:pipeline_task_overview} illustrates the nine distinct tasks in our evaluation framework.

We evaluate three categories of systems: VLMs, traditional OCR systems, and specialized document processing tools. For VLMs, we include both closed-source models like \texttt{gpt-4o-2024-08-06}, \texttt{gpt-4o-mini-2024-07-18} \cite{hurst2024gpt, achiam2023gpt}, and \texttt{gemini-2.0-flash} \cite{team2024gemini, google2025gemini}, as well as open-source alternatives such as \texttt{Qwen2-VL-7B}~\cite{wang2024qwen2}, \texttt{Qwen2.5-VL-7B} \cite{Qwen2.5-VL}, and the \texttt{AIN-7B} \cite{heakl2025ain}. Traditional OCR approaches in our evaluation include \texttt{Surya} \cite{paruchuri2024surya}, \texttt{Tesseract} \cite{smith2007overviewtesseract}, \texttt{EasyOCR} \cite{easyocr}, and \texttt{PaddleOCR} \cite{li2022paddleocr, du2021paddleocr}. For specialized document processing tasks, we employ systems like \texttt{Docling} \cite{auer2024docling}, and \texttt{Marker} \cite{paruchuri2024marker}. Layout detection capabilities are evaluated using methods implemented in \texttt{Surya-layout} \cite{paruchuri2024surya}, \texttt{Yolo-doclaynet} \cite{zhao2024doclayout} from \texttt{MinerU} \cite{ wang2024mineru}, and \texttt{RT-DETR} \cite{zhao2023detrs} based method in Docling \cite{auer2024docling}.

\input{tables/layout-detection}
\subsection{Evaluation Frameworks and Metrics}
Our evaluation framework comprises nine specialized tasks designed to assess different aspects of Arabic OCR systems, as demonstrated in Figure~\ref{fig:pipeline_task_overview}. Each task addresses specific challenges in Arabic document processing. For this reason, we employ task-specific metrics to evaluate different aspects of document understanding.

\textbf{PDF-to-Markdown:} It evaluates the conversion of Arabic PDFs to structured markdown while preserving the text and table structure. Since both table and text structure are important, for evaluating PDF to Markdown conversion quality, we propose MARS (Markdown Recognition Score), which combines chrF \cite{popovic2015chrf} with Tree-Edit-Distance-based Similarity (TEDS) \cite{zhong2020image} :
\begin{equation}
\label{eq:mars}
    \text{MARS} = \alpha \cdot \text{chrF}_3 + (1-\alpha) \cdot \text{TEDS}(T_a, T_b)
\end{equation}
where $\alpha$ ($0 \leq \alpha \leq 1$) is the weight. $T_a$ represent predicted table structure  and $T_b$ the ground truth structure.


\textbf{Table Recognition:} We evaluate table extraction using both HTML and CSV formats, where HTML format (evaluated using TEDS \cite{zhong2020image}) preserves rich structural information including cell spans and hierarchical relationships crucial for complex Arabic tables, while CSV format (evaluated using Jaccard Index \ref{eq:jaccard}) focuses on raw data extraction optimized for machine processing and data analysis pipelines. This dual-format evaluation ensures systems can both maintain complex table structures for human readability and provide clean, structured data for automated processing, specifically important for RAG based systems.
\begin{equation}
    \label{eq:jaccard}
    J(P,G) = \frac{|P \cap G|}{|P \cup G|} = \frac{|P \cap G|}{|P| + |G| - |P \cap G|}
\end{equation}
where $|P \cap G|$ represents the number of exact matching cells between predicted and ground truth tables, and $|P \cup G|$ represents the total number of unique cells across both tables.

\textbf{Chart-to-Dataframe:} This task evaluates extracting structured data from Arabic charts into machine-readable dataframes. Systems must accurately parse numerical values, text labels, and preserve data relationships across chart types (bar, line, pie). We use the Structuring Chart-oriented Representation Metric (SCRM) \cite{xia2024chartx}‚Äîwhich combines type recognition, topic understanding, and structural numerical fidelity (see Appendix \ref{sec:appendix4})‚Äîand also propose our own CharTeX (Chart Extraction Score) metric. CharTeX combines the chrF scores for chart type and topic with the jaccord index for the dataframe, using fuzzy matching (80\% threshold) when columns do not exactly align.
\vspace{-3pt}
\begin{equation} \label{eq:codm} 
\text{Metric} = \alpha J_{type} + \beta J_{topic} + (1-\alpha-\beta)J_{data} \end{equation}

Here, $J_{type}$ and $J_{topic}$ denote the chrF scores between the predicted and ground-truth chart type and topic, while $J_{data}$ measures the structural similarity of the predicted and ground-truth JSON data.

\textbf{Diagram-to-JSON: } This task evaluates the conversion of Arabic flowcharts and technical diagrams into JSON while preserving semantic relationships and technical specifications. We propose CODM (Code-Oriented Diagram Metric), extending SCRM~\cite{xia2024chartx}, with the same fomulation as in Eq~\ref{eq:codm}. More detail about this metric is provided in Appendix \ref{sec:appendix5}.

\input{tables/ocr_pdf}
\textbf{Image-to-Text: } This task assess the basic text recognition capabilities across different Arabic fonts and styles, including the handling of cursive script connections, diacritical marks, and various text orientations. We use we use Character Error Rate (CER) and Word Error Rate (WER). For a predicted text sequence $\hat{y}$ and ground truth sequence $y$, CER is computed as:
$
\label{eq:cer}
\text{CER} = \frac{\text{L}(y, \hat{y})}{|y|}
$,
where $\text{L}(y, \hat{y})$ is the Levenshtein distance between character sequences and $|y|$ is the ground truth length. WER is calculated the same way with words as the unit of error.

\textbf{Visual Question Answering: } Tests the ability of models to understand and reason about Arabic document content, we evaluate using standard accuracy for MCQ questions and exact word match. 

\textbf{Line Detection: } Focuses on the accurate identification and processing of individual text lines in Arabic documents. We evaluate using mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds: mAP@0.5 and mAP@0.5:0.95, which assess the localization accuracy of detected text lines.

% \textbf{Line Recognition: } Evaluates the accuracy of text recognition within detected lines, incorporating the challenges of Arabic script's contextual shaping and diacritical marks. We employ CER and WER metrics as defined in Equations~\ref{eq:cer}, providing character and word-level accuracy assessments respectively.

\textbf{Layout Detection: } Assesses document structure analysis capabilities, including the identification of headers, paragraphs, and complex layout elements in Arabic documents. Performance is measured using mAP@0.5 and mAP@0.5:0.95 for localization accuracy, complemented by Precision, Recall, and F1 scores to evaluate the overall detection quality across different layout components.

% \textbf{PDF-to-Markdown Conversion:} One of the major challenge in PDF-to-Markdown conversion is page structure preservation, which includes the preservation of both text structure and table.
% We evaluate markdown conversion using a combination of CHrF \cite{popovic2015chrf} for text similarity and TEDS \cite{zhong2020image} for table structure preservation, providing a holistic assessment of both textual and structural fidelity.

% \textbf{Text Recognition Metrics:} For basic text recognition, we use Character Error Rate (CER) and Word Error Rate (WER). For a predicted text sequence $\hat{y}$ and ground truth sequence $y$, CER is computed as:
% $$
% \text{CER} = \frac{\text{L}(y, \hat{y})}{|y|}
% $$
% where $\text{L}(y, \hat{y})$ is the Levenshtein distance between character sequences and $|y|$ is the ground truth length. WER is calculated as:
% $$
% \text{WER} = \frac{S + D + I}{N}
% $$
% where $S$, $D$, and $I$ represent substitutions, deletions, and insertions respectively, and $N$ is the total ground truth words.

% \textbf{Layout Understanding Metrics:} For layout detection and classification, we utilize standard object detection metrics including mean Average Precision (mAP) at different IoU thresholds (mAP@0.5 and mAP@0.5:0.95), along with Precision, Recall, and F1 scores. These metrics evaluate both the localization accuracy and classification performance of document layout components.

% \textbf{Line Detection and Recognition:} Line detection performance is measured using mAP@50 and mAP@0.5:0.95, while line recognition accuracy is evaluated using WER and CER at the line level. These metrics are particularly important for Arabic text due to its cursive nature and context-dependent character shapes.

% \textbf{Table Understanding:} For table recognition tasks, we employ the Tree-Edit-Distance-Based Similarity (TEDS) \cite{zhong2019image} for HTML output evaluation and Jaccard Index for CSV format accuracy. These metrics capture both structural and content-based similarities between predicted and ground truth tables.

% \textbf{Charts and Diagrams:} For chart interpretation and diagram-to-code conversion tasks, we utilize the Structural Chart-oriented Representation Metric (SCRM) \cite{xia2024chartx, xia2023structchart}. SCRM evaluates the accuracy of extracted information through a weighted combination of type recognition, topic understanding, and structural representation:
% $$
% SCRM = 0.5J_1 + 0.2J_2 + 0.3J_3
% $$
% where $J_1$, $J_2$, and $J_3$ represent edit distances for chart type, topic, and structural data respectively.

% \textbf{Visual Question Answering:} For VQA tasks, we combine accuracy metrics with exact word match scores for open-ended responses, providing a comprehensive evaluation of both understanding and generation capabilities.



All metrics are computed on our diverse benchmark dataset, which encompasses various document types and complexity levels in both Arabic and multilingual contexts. Table~\ref{tab:doc-understanding-metric-model} provides a detailed mapping of tasks, metrics, and evaluated systems.
\input{tables/ocr_table}


\subsection{Experimental Setup}
We implement our evaluation pipeline with careful consideration of hyperparameters for different metrics. All experiments use NVIDIA A100 GPUs. For VLMs, we use their official implementations or API endpoints. Traditional OCR systems are evaluated using pre-trained models provided by the frameworks. For PDF-to-Markdown evaluation metric MARS \ref{eq:mars}, we choose $\alpha=0.5$ and $\alpha =0.5$ and $\beta = 0.2$ for Diagram-to-JSON evaluation metric CODM. We average the results over multiple runs, with performance comparisons shown in different tables [\ref{tab:line-detection}, \ref{tab:ocr-tasks}, \ref{tab:table-pdf-tasks}, \ref{tab:visual-tasks}, and \ref{tab:layout-detection}].


\input{tables/chart_diagram_vqa}
\section{Results and Discussion}
In this section, we present a comprehensive evaluation of different models across different tasks of our framework. The results provide a clear distinction between the performance of closed-source models, open-source models, and framework-based solutions, revealing both their strengths and limitations. We observe very clear performance gap between closed and open-source solutions. While closed-source models like Gemini-2.0-Flash consistently outperform other models almost all the tasks.

\subsection{Charts, Diagrams, and VQA}
Table [\ref{tab:visual-tasks}] presents model performance across different chart and diagram understanding tasks, evaluated using SCRM and CharTeX (for charts), and VQA-based accuracy metrics. Among closed-source models, Gemini-2.0 achieves the highest performance on chart understanding metrics, scoring 71.4\% on SCRM and 56.28\% on CharTeX. The performance gap between Gemini-2.0 and GPT-4o is particularly pronounced in CharTeX evaluation (10.33\%) compared to SCRM (2.8\%). Open-source models shows a significant limitation in complex chart understanding. While their SCRM scores remain competitive, both Qwen variants score below 23\% on CharTeX evaluation.
The visual question-answering results reveal an important exception to the general closed-source advantage. AIN achieves 87\% on PATDVQA, surpassing Gemini-2.0 by 11.5\%. AIN also shows competitive performance on MTVQA (31.50\%), which is similar to GPT-4o and ~4\% better than GPT-4o-mini. This shows that open-source models can be competitive with closed-source alternatives.

\subsection{Layout and Lines: Document Structure}
Our evaluation of document structure understanding reveals distinct performance patterns across layout detection and line processing tasks. In layout detection (Table~\ref{tab:layout-detection}), RT-DETR \cite{zhao2023detrs} achieves superior overall performance with mAP@0.5 scores of 0.750 and 0.758 on BCE (arabic only) and DocLayNet (english) datset respectively. However, Surya \cite{paruchuri2024surya} demonstrates higher precision (0.782 on DocLayNet, 0.751 on BCE), despite lower recall rates. This trade-off suggests that different architectures optimize for different aspects of layout detection.

The line processing results (Table~\ref{tab:line-detection}) highlight a clear contrast between detection and recognition capabilities. While Surya excels in detection with a mAP@0.50 of 79.67\%, EasyOCR demonstrates superior recognition performance (WER: 0.53, CER: 0.20). This inverse relationship between detection and recognition performance across models indicates a fundamental challenge in optimizing both capabilities simultaneously. Notably, Tesseract shows consistent but lower performance across both metrics, suggesting that newer architectures have made significant improvements over traditional approaches.
We also observe that no single model excels at both detection and recognition, which requires for hybrid solutions.

\subsection{Tables, OCR, and PDF-to-Markdown}

Across table extraction tasks (Table~\ref{tab:table-pdf-tasks}), closed-source models maintain a clear advantage, with GPT-4o achieving 85.76\% TEDS and 66.36\% Jaccard scores. Among open-source models, AIN (75.94\% TEDS) significantly outperforms Qwen variants, while specialized frameworks like Surya achieve competitive results (70.42\% Jaccard) through targeted pipelines. In OCR evaluation (Table~\ref{tab:ocr-tasks}), Gemini-2.0-Flash leads with the lowest error rates (CER: 0.13, WER: 0.32). Notably, AIN matches this performance level (WER: 0.28), while traditional OCR frameworks like EasyOCR and Tesseract show moderate performance (CER: 0.58, 0.54). The significant performance drop in Paddle (CER: 0.79) and Surya (CER: 4.95) highlights the challenges in developing robust OCR systems.

End-to-end document processing (Table \ref{tab:table-pdf-tasks}) reveals the largest gaps between approaches. Closed-source models maintain consistent performance (GPT-4o: 65.12\% MARS, Gemini-2.0: 65.65\% MARS), while open-source models show substantial degradation (Qwen2-VL-7B: 21.42\% MARS). Framework approaches achieve better stability, with Tesseract and EasyOCR scoring above 50\% MARS, suggesting that specialized pipelines can partially bridge the gap with larger models in complete document processing tasks.

Our comprehensive evaluation demonstrates that while closed-source models maintain superior performance over open-source models across most tasks, specialized frameworks like Surya, RT-DETR Layout, and EasyOCR achieve competitive performance in targeted scenarios like table extraction, layout detection, and text recognition respectively. However, this framework advantage significantly diminishes in end-to-end pdf-to-markdown tasks where the integration capabilities of large models prove crucial, as evidenced by the performance gaps between commercial VLMs and traditional systems like EasyOCR, Surya and Tesseract in End-to-End PDF task (Table \ref{tab:table-pdf-tasks}).
% The experimental results, reveal several interesting patterns in Arabic OCR performance. Among the vision-language models, AIN demonstrates superior performance with a CER of 0.19 and WER of 0.24, significantly outperforming other approaches. This is followed by Gemini-2.0-Flash (CER: 0.41, WER: 0.61) and GPT-4o (CER: 0.54, WER: 0.81). Traditional OCR systems like Tesseract show moderate performance (CER: 0.79, WER: 1.16), while specialized Arabic OCR systems like Surya exhibit higher error rates (CER: 1.66, WER: 1.58).

% The performance variation across different document types highlights specific challenges in Arabic OCR. Historical documents and handwritten text consistently prove more challenging across all systems, with higher error rates compared to modern printed text. The synthetic data in SythenAR helps establish a performance ceiling for each system under ideal conditions, while real-world documents from sources like PATS and EvArest provide a more realistic assessment of practical performance.




% Vision-language models demonstrate particular strength in handling complex layouts and mixed content types, likely due to their pre-training on diverse visual data. However, specialized Arabic OCR systems show better performance on specific challenges such as diacritic placement and ligature handling. Traditional OCR systems, while robust, generally lag behind both VLMs and specialized solutions in handling Arabic-specific features.

% The results also reveal persistent challenges in Arabic OCR, particularly in handling elongated words, merged table cells, and complex font styles. These challenges affect all systems to varying degrees, suggesting areas for future improvement in Arabic document processing technologies.


% \subsection{Table Evaluation}
% The experimental results in Table {} reveal various trends in table extraction. Among closed-source VLMs, GPT-4o achieves the best performance (76.06), outperforming Gemini-2.0-Flash (74.32) and GPT-4o-mini (59.41), showcasing their superiority in structured text extraction. Qwen2-VL (49.02) lags behind, indicating challenges for open-source models.



% Within open-source VLMs, AIN (70.39) outperforms Qwen2.5-VL (59.45) and Qwen2-VL (49.02), particularly in HTML extraction, though all three still trail behind GPT-4o. Traditional OCR models like Docling [Tesseract] (21.54) and Img2Table (27.34) struggle, especially with CSVs, while Marker (60.29) performs better, suggesting some table-specific optimizations.

% Closed-source VLMs lead in table extraction, with open-source models improving but still behind. Further, traditional OCR methods, especially Tesseract-based ones, are significantly weaker, while hybrid models like Marker offer a middle ground. As open-source VLMs evolve, they may close the gap with proprietary solutions.

% \subsection{Document Detection and Recognition}
% The results in Table {} provide a comparative analysis of traditional OCR models for document detection and recognition. In the detection task, Surya demonstrates the highest performance with a mAP@0.5 of 79.67, significantly surpassing Tesseract (46.39) and EasyOCR (68.02). However, in the stricter mAP@0.5:0.95 metric, EasyOCR (32.74) performs better than both Surya and Tesseract, suggesting it has greater adaptability across varying document complexities.

% For recognition tasks, EasyOCR achieves the best results with the lowest WER (0.53) and CER (0.20), outperforming both Surya and Tesseract. Tesseract shows moderate performance with a CER of 0.66, while Surya lags slightly behind. These results indicate that while Surya excels in detection, EasyOCR provides superior text recognition capabilities.

% Overall, the results highlight the trade-offs between detection and recognition performance across traditional OCR models. Surya is more effective for document localization, whereas EasyOCR demonstrates enhanced text recognition, making it a more balanced choice for end-to-end OCR tasks.


% \subsection{Layout Detection Evaluation}
% A comparison of layout detection models across various datasets and evaluation measures is shown in Table {}. On both the BCE and DocLayNet datasets, Detr (docling) achieves the highest mAP@0.5 (0.750, 0.758) and mAP@0.5:0.95 (0.566, 0.541), respectively, outperforming the other models in terms of detection accuracy. This demonstrates how well it can locate and identify layout components in a variety of document formats.

% Surya performs well in both precision and recall, especially on DocLayNet, where it achieves the top F1 Score (0.799) with the highest precision (0.782) and recall (0.856). This implies that Surya successfully balances false positives and false negatives, making it a good choice for situations demanding high detection reliability.

% However, Yolo-doclaynet (MinerU) performs poorly in the majority of measures, especially recall and F1 Score, suggesting that it has trouble reliably identifying intricate and non-linear layout features.

% Overall, the findings show that Surya is a great candidate for high-confidence document parsing tasks due to its superior precision and recall trade-offs, whereas Detr (docling) is the most reliable model for layout recognition. Despite its reasonable performance, Yolo-doclaynet (MinerU) needs more optimizations to match the other models' capabilities.

% \subsection{VQA Evaluation}
% The results in Table {} highlight the performance gap between closed-source and open-source vision-language models (VLMs) in Visual Question Answering (VQA) tasks. Closed-source models (GPT-4o, GPT-4o-mini, Gemini-2.0-Flash) consistently outperform open-source models across all datasets. Gemini-2.0-Flash excels in DiagramsVQA (88.24) and MTVQA (35.00), while GPT-4o leads in structured tasks like PATDVQA (82.50) and ChartsVQA (77.00). The smaller GPT-4o-mini struggles, reinforcing the trade-off between model size and performance.

% \noindent Among open-source models, Qwen2.5-VL shows improvements over Qwen2-VL, particularly in ChartsVQA (75.00 vs. 59.00) and MTVQA (23.00 vs. 19.60), yet both lag behind proprietary alternatives. This indicates that while open-source models are progressing, they still face challenges in structured and complex reasoning tasks.

\input{sections/conclusion}
\input{sections/limitations}

\bibliography{acl_natbib}

\appendix

\section{Source of the Existing Dataset Collection}
\label{sec:appendix1}

Our benchmark integrates diverse data sources to ensure comprehensive coverage of Arabic document types. As detailed in Table \ref{tab:dataset-distribution}, the dataset combines manually curated samples, synthetic data generated through our LLM-assisted pipeline (Figure \ref{fig:data_creation_pipeline}), and existing publicly available datasets. Key sources include:

\begin{itemize}
\item {Handwritten Text: KHATT (paragraph and line-level annotations), ADAB, Muharaf, and OnlineKhatt.}

\item {Historical Documents: HistoryAr and HistoricalBooks.}

\item {Scene Text: EvAREST for real-world context diversity.}

\item {Layout Analysis: BCE-Arabic-v1 and DocLayNet.}

\item {Synthetic Content: 576 chart samples (16 types) and 422 diagram samples generated via our five-phase pipeline (Section 3.2).}
\end{itemize}
The dataset emphasizes domain diversity, covering academic, medical, legal, financial, and technical documents. All samples underwent rigorous validation by native Arabic speakers to ensure linguistic and structural accuracy.

\section{Detailed Performance Comparison}
\label{sec:appendix2}

Table \ref{tab:VLM_COMP} provides granular performance metrics for VLMs and OCR frameworks across 12 Arabic text recognition datasets. Gemini-2.0-Flash demonstrates exceptional robustness on synthetic datasets (CER: 0.01 on PATS), while AIN-7B excels in historical manuscript recognition (CER: 0.26 on HistoryAr). Traditional OCR systems like Tesseract show limitations in handwritten text (CER: 1.26 on HistoryAr), highlighting the need for script-specific optimizations.


\section{Data Analysis}
\label{sec:appendix3}

Our data generation pipeline (Figure \ref{fig:data_creation_pipeline}) enabled the creation of 1,502 synthetic samples (576 charts, 422 diagrams, 456 tables). The pipeline's human validation phase rejected 18\% of initial outputs due to RTL formatting errors or semantic inconsistencies. As shown in Figure \ref{fig:prompta} and \ref{fig:promptb}, domain-specific prompts ensured adherence to Arabic linguistic conventions during LLM-assisted generation. The final dataset exhibits balanced representation across:

\begin{itemize}
    \item Font Styles: 21 Arabic calligraphic styles
    \item Document Types: 36 sub-domains including financial reports and technical manuals
    \item Structural Complexity: 43\% of tables contain merged cells; 29\% of charts use dual-axis configurations

    
\end{itemize}

\section{Tasks Models and Metrics}
\label{sec:appendix4}

Table \ref{tab:doc-understanding-metric-model} maps evaluation tasks to corresponding models and metrics. The framework evaluates nine core capabilities:

\begin{itemize}
    \item Structural Understanding: Layout detection (mAP), line detection (IoU)
    \item Content Extraction: Text recognition (CER), table parsing (TEDS)
    \item Semantic Reasoning: VQA accuracy, chart-to-dataframe conversion (SCRM)
    \item Specialized metrics like MARS (
    $\alpha$=0.5) address the dual requirements of text fidelity and structural preservation in PDF-to-Markdown conversion.
\end{itemize}



\section{SCRM and CODM}
\label{sec:appendix5}

The Structuring Chart-oriented Representation Metric (SCRM) evaluates chart understanding through three components:
\begin{equation}
\text{SCRM} = 0.4J_{\text{type}} + 0.3J_{\text{topic}} + 0.3J_{\text{data}}
\end{equation}
where $J_{type}$, and 
$J_{topic}$ are chrF scores, and  $J_{data}$ measures JSON structural similarity.

The Code-Oriented Diagram Metric (CODM) extends SCRM for flowcharts and technical diagrams:
\begin{equation}
\text{CODM} = 0.5J_{\text{topology}} + 0.5J_{\text{semantics}}
\end{equation}
assessing both node-edge relationships and semantic labels. As shown in Figure \ref{fig:prompta} and \ref{fig:promptb}, domain-specific prompts guided model responses for metric calculation. For instance, sequence diagrams required strict adherence to Arabic UML notation standards during evaluation.



\begin{table*}[htbp]
\centering
\small
\begin{tabular}{@{}p{2.5cm}p{2.5cm}p{5.0cm}rrr@{}}
\hline
\textbf{Domain} & \textbf{Sub-Domain} & \textbf{Dataset Source} & \textbf{Original} & \textbf{Selected} & \textbf{Total} \\
\hline
PDF to Markdown & General & Manual & 33 & 33 & 33 \\
\hline
Layout Detection & Docs & BCE-Arabic-v1 \cite{saad2016bce} & 1.9k & 1,700 & \multirow{2}{*}{2,100} \\
 & & DocLayNet \cite{doclaynet2022} & 80k & 400 & \\
\hline
Line Detection & Docs & Manual & 375 & 378 & 378 \\
\hline
Line Recognition & Docs & Manual & 375 & 378 & 378 \\
\hline
Table Recognition & Financial & Pixmo \cite{deitke2024molmo} & 490 & 456 & 456 \\
\hline
\multirow{13}{*}{Image to Text} & \multirow{2}{*}{Synthetic} & PATS \cite{muhtaseb2010pats} & 21.6k & 500 & \multirow{13}{*}{3,760} \\
 & & SythenAR & 39.1k & 500 & \\
 & \multirow{2}{*}{Historical} & HistoryAr  \cite{pantke2014historical} & 1.5k & 200 & \\
 & & HistoricalBooks & 40 & 10 & \\
 & Hand. Paragraph & Khatt \cite{mahmoud2014khatt} & 2.72k & 200 & \\
 & Hand. Word & ADAB \cite{adab2019bench} & 15k & 200 & \\
 & \multirow{3}{*}{Hand. Line} & Muharaf \cite{saeed2024muharaf} & 24.5k & 200 & \\
 & & OnlineKhatt \cite{mahmoud2018online} & 8.5k & 200 & \\
 & & Khatt \cite{mahmoud2014khatt} & 13.4k & 200 & \\
 & PPT & ISI-PPT \cite{wu2017iccv} & 86.5k & 500 & \\
 & \multirow{2}{*}{Blogs} & ArabicOCR & 20.3k & 50 & \\
 & & Hindawi \cite{elfilali2023hindawi} & 79k & 200 & \\
 & Scene & EvAREST \cite{hassan2021arabicEvArEST} & 5.59k & 800 & \\
\hline
\multirow{16}{*}{Charts to DataFrame} & Bar & Synthetic & 100 & 61 & \multirow{16}{*}{576} \\
 & Line & Synthetic & 100 & 43 & \\
 & Pie & Synthetic & 100 & 56 & \\
 & Box & Synthetic & 100 & 31 & \\
 & Violin & Synthetic & 100 & 36 & \\
 & Area & Synthetic & 50 & 29 & \\
 & SunBurst & Synthetic & 30 & 15 & \\
 & Dot & Synthetic & 30 & 15 & \\
 & Dual Axis & Synthetic & 20 & 26 & \\
 & Density Curve & Synthetic & 10 & 5 & \\
 & Bubble & Synthetic & 20 & 13 & \\
 & Grouped Bar & Synthetic & 50 & 60 & \\
 & Stacked Bar & Synthetic & 50 & 82 & \\
 & Histogram & Synthetic & 100 & 70 & \\
 & HeatMap & Synthetic & 10 & 11 & \\
 & Scatter & Synthetic & 100 & 23 & \\
\hline
\multirow{7}{*}{Diagram to Json} & Sequence & Synthetic & 50 & 46 & \multirow{7}{*}{226} \\
 & Funnel & Synthetic & 20 & 52 & \\
 & Class & Synthetic & 20 & 30 & \\
 & Network & Synthetic & 20 & 18 & \\
 & Venn & Synthetic & 20 & 7 & \\
 & FlowChart & Synthetic & 100 & 112 & \\
 & TreeMap & Synthetic & 100 & 157 & \\
\hline
\multirow{4}{*}{VQA} & Diagrams & Manual & 102 & 102 & \multirow{4}{*}{902} \\
 & Charts & Manual & 105 & 100 & \\
 & News Letter & PATD \cite{bouressace2019printed} & 2.42k & 200 & \\
 & Scene & MTVQA & 818 & 500 & \\
\hline
\multicolumn{3}{l}{\textbf{Total Dataset Size}} & -- & \multicolumn{2}{r}{8,809} \\
\hline
\end{tabular}
\caption{Dataset Distribution Across Different Domains, sub-domains and Data Source}
\label{tab:dataset_distribution}
\end{table*}

% \section{Dataset Domain-Subdomain and data count}
% \label{sec:appendix2}
% \begin{table*}[htbp]
% \footnotesize
% \centering
% \caption{Distribution of samples across different domains and sub-domains in our dataset.}
% \label{tab:dataset-distribution}
% \begin{tabular}{@{}p{2.3cm}p{2.5cm}rr@{}}
% \toprule
% \textbf{Domain} & \textbf{Sub-Domain} & \textbf{Selected} & \textbf{Total} \\
% \midrule
% PDF to Markdown & General & 25 & 25 \\
% \midrule
% \multirow{3}{*}{Layout} & Receipts & 200 & \multirow{3}{*}{2,300} \\
%  & Docs & 1,700 & \\
%  & Financial, Academic etc. & 400 & \\
% \midrule
% \multirow{2}{*}{Line Detection} & Historical & 70 & \multirow{2}{*}{448} \\
%  & Docs & 378 & \\
% \midrule
% Line Recognition & Docs & 378 & 378 \\
% \midrule
% Table Recognition & Financial, sciences etc. & 456 & 456 \\
% \midrule
% \multirow{9}{*}{Image to Text} & Synthetic & 1,000 & \multirow{9}{*}{3,760} \\
%  & Historical & 210 & \\
%  & Handwritten-Paragraph & 200 & \\
%  & Handwritten-Word & 200 & \\
%  & Handwritten-Line & 600 & \\
%  & PPT & 500 & \\
%  & Blogs & 250 & \\
%  & Scene & 800 & \\
% \midrule
% \multirow{16}{*}{Charts to DataFrame} & Bar & 61 & \multirow{16}{*}{576} \\
%  & Line & 43 & \\
%  & Pie & 56 & \\
%  & Box & 31 & \\
%  & Violin & 36 & \\
%  & Area & 29 & \\
%  & SunBurst & 15 & \\
%  & Dot & 15 & \\
%  & Dual Axis & 26 & \\
%  & Density Curve & 5 & \\
%  & Bubble & 13 & \\
%  & Grouped Bar & 60 & \\
%  & Stacked Bar & 82 & \\
%  & Histogram & 70 & \\
%  & HeatMap & 11 & \\
%  & Scatter & 23 & \\
% \midrule
% \multirow{7}{*}{Diagram to Json} 
%  & Sequence Diagram & 46 & \multirow{7}{*}{422} \\
%  & Funnel & 52 & \\
%  & Class & 30 & \\
%  & Network & 18 & \\
%  & Venn & 7 & \\
%  & FlowChart & 112 & \\
%  & TreeMap & 157 & \\
% \midrule
% \multirow{4}{*}{VQA} & News Letter & 200 & \multirow{4}{*}{902} \\
%  & Scene & 500 & \\
%  & Diagrams & 102 & \\
%  & Charts & 100 & \\
% \midrule
% \multicolumn{2}{@{}l}{\textbf{Total Samples}} & \textbf{9005} & \textbf{9005} \\
% \bottomrule
% \end{tabular}
% \end{table*}




%% OCR EVAL
\begin{table*}[htbp]
\centering
\label{tab:results-vlm}
\renewcommand{\arraystretch}{1.2} % Improves row height for better readability
\setlength{\tabcolsep}{6pt} % Adjusts column spacing

\footnotesize

\begin{tabular}{l r | cc cc cc | cc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Size}} 
& \multicolumn{2}{c}{\textbf{GPT-4o}} & \multicolumn{2}{c}{\textbf{GPT-4o-mini}} & \multicolumn{2}{c|}{\textbf{Gemini-2.0-Flash}}
& \multicolumn{2}{c}{\textbf{Qwen2-VL}} \\
& & CER & WER & CER & WER & CER & WER & CER & WER \\
\midrule
PATS & 500 & 0.23 & 0.30 & 0.53 & 0.71 & 0.01 & 0.02 & 1.02 & 1.02 \\
SythenAR & 500 & 0.09 & 0.20 & 0.14 & 0.32 & 0.07 & 0.17 & 0.59 & 1.13 \\
HistoryAr & 200 & 0.51 & 0.82 & 0.67 & 0.96 & 0.28 & 0.64 & 3.46 & 2.86 \\
HistoricalBooks & 10 & 0.41 & 0.76 & 0.59 & 0.88 & 0.05 & 0.22 & 1.90 & 2.16 \\
Khatt & 200 & 0.45 & 0.74 & 0.64 & 0.91 & 0.19 & 0.45 & 1.12 & 5.04 \\
Adab & 200 & 0.30 & 0.73 & 0.35 & 0.83 & 0.19 & 0.56 & 0.63 & 1.08 \\
Muharaf & 200 & 0.56 & 0.90 & 0.63 & 0.94 & 0.33 & 0.69 & 3.57 & 2.87 \\
OnlineKhatt & 200 & 0.29 & 0.63 & 0.41 & 0.76 & 0.17 & 0.44 & 1.30 & 2.01 \\
ISI-PPT & 500 & 0.08 & 0.18 & 0.15 & 0.31 & 0.06 & 0.15 & 1.03 & 1.06 \\
ArabicOCR & 50 & 0.06 & 0.26 & 0.16 & 0.46 & 0.00 & 0.02 & 1.25 & 1.50 \\
Hindawi & 200 & 0.34 & 0.56 & 0.48 & 0.71 & 0.01 & 0.04 & 1.82 & 2.05 \\
EvArest & 800 & 0.20 & 0.38 & 0.25 & 0.51 & 0.18 & 0.36 & 0.41 & 0.95 \\
\midrule
  & 3,760 & {0.31} & {0.55} & 0.43 & 0.71 & 0.13 & 0.32 & 1.48 & 1.20 \\
\bottomrule
\end{tabular}

\vspace{4mm} % Adds vertical space between tables

% Second Half of the Table
\begin{tabular}{l r | cc cc | cc cc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Size}} 
& \multicolumn{2}{c}{\textbf{Qwen2.5-VL}} & \multicolumn{2}{c|}{\textbf{AIN}} 
& \multicolumn{2}{c}{\textbf{Tesseract}} & \multicolumn{2}{c}{\textbf{Surya}} \\
& & CER & WER & CER & WER & CER & WER & CER & WER \\
\midrule
PATS & 500 & 0.26 & 0.36 & 0.00 & 0.00 & 0.14 & 0.28 & 4.66 & 4.67 \\
SythenAR & 500 & 0.21 & 0.40 & 0.04 & 0.16 & 0.31 & 0.72 & 4.82 & 7.90 \\
HistoryAr & 200 & 0.47 & 0.83 & 0.26 & 0.54 & 0.72 & 1.26 & 10.32 & 12.78 \\
HistoricalBooks & 10 & 0.33 & 0.72 & 0.84 & 0.88 & 0.74 & 0.99 & 6.81 & 6.30 \\
Khatt & 200 & 0.07 & 0.22 & 0.61 & 1.12 & 0.67 & 1.06 & 4.25 & 3.77 \\
Adab & 200 & 0.00 & 0.01 & 1.00 & 1.00 & 1.00 & 1.14 & 7.28 & 8.71 \\
Muharaf & 200 & 0.61 & 0.96 & 0.38 & 0.54 & 0.77 & 1.22 & 6.19 & 7.48 \\
OnlineKhatt & 200 & 0.36 & 0.70 & 0.03 & 0.12 & 0.59 & 1.20 & 6.71 & 6.95 \\
ISI-PPT & 500 & 0.36 & 0.54 & 0.52 & 0.53 & 0.31 & 0.64 & 4.25 & 3.77 \\
ArabicOCR & 50 & 1.00 & 1.00 & 0.01 & 0.01 & 0.01 & 0.01 & 2.75 & 3.58 \\
Hindawi & 200 & 1.00 & 1.00 & 0.11 & 0.15 & 0.31 & 0.72 & 0.15 & 0.20 \\
EvArest & 800 & 0.19 & 0.36 & 0.30 & 0.32 & 0.85 & 1.02 & 5.91 & 3.86 \\
\midrule
  & 3,760 & 0.28 & 0.54 & {0.20} & {0.58} & 0.89 & 0.79 & 4.95 & 5.61 \\
\bottomrule
\end{tabular}
\caption{Performance comparison of Large Vision-Language Models on KITAB-Bench (lower is better).}
\label{tab:VLM_COMP}
\end{table*}



\begin{table*}[htbp]
\centering
\small
\resizebox{\textwidth}{!}{
\begin{tabular}{p{2.5cm}p{4cm}p{2.5cm}p{2.5cm}p{4cm}}
\toprule
\textbf{Task} & \textbf{Metrics} & \textbf{Open LLMs} & \textbf{Closed LLMs} & \textbf{OCR Systems} \\
\midrule
\multicolumn{5}{l}{\textit{Document Understanding Tasks}} \\
\midrule
PDF to Markdown & chrF + TEDS & -- & -- & \begin{tabular}[t]{@{}l@{}} Docling \\ Marker \\ MinerU \\ PDF-Extract-Kit \end{tabular} \\
\midrule
Layout Detection & \begin{tabular}[t]{@{}l@{}} mAP@0.5 \\ mAP@0.5:0.95 \\ Precision \\ Recall \\ F1 \end{tabular} & -- & -- & \begin{tabular}[t]{@{}l@{}} Surya \\ Yolo-doclaynet (MinerU) \\ Detr (docling) \end{tabular} \\
\midrule
Line Detection & \begin{tabular}[t]{@{}l@{}} mAP@0.5 \\ mAP@0.5:0.95 \end{tabular} & -- & -- & \begin{tabular}[t]{@{}l@{}} Surya \\ Tesseract \\ EasyOCR \end{tabular} \\
\midrule
Line Recognition & WER, CER & -- & -- & \begin{tabular}[t]{@{}l@{}} Surya \\ Tesseract \\ EasyOCR \end{tabular} \\
\midrule
\multicolumn{5}{l}{\textit{Table Understanding Tasks}} \\
\midrule
Tables Recognition (HTML) & TEDS \cite{zhong2019image} & \begin{tabular}[t]{@{}l@{}} Qwen2-VL \\ Qwen2.5-VL \\ AIN \\ PaliGemma \end{tabular} & \begin{tabular}[t]{@{}l@{}} GPT-4o \\ GPT-4o-mini \\ Gemini-2.0-Flash \end{tabular} & \begin{tabular}[t]{@{}l@{}} Docling[EasyOCR] \\ Docling[Tesseract] \\ Marker \\ Img2Table[EasyOCR] \\ Img2Table[Tesseract] \end{tabular} \\
\midrule
Tables Recognition (CSV) & Jaccard Index & \begin{tabular}[t]{@{}l@{}} Qwen2-VL \\ Qwen2.5-VL \\ AIN \\ PaliGemma \end{tabular} & \begin{tabular}[t]{@{}l@{}} GPT-4o \\ GPT-4o-mini \\ Gemini-2.0-Flash \end{tabular} & \begin{tabular}[t]{@{}l@{}} Docling[EasyOCR] \\ Docling[Tesseract] \\ Marker \\ Img2Table[EasyOCR] \\ Img2Table[Tesseract] \end{tabular} \\
\midrule
\multicolumn{5}{l}{\textit{Visual Understanding Tasks}} \\
\midrule
Image to Text & \begin{tabular}[t]{@{}l@{}} CER, WER \\ chrF, BLEU \\ METEOR \end{tabular} & \begin{tabular}[t]{@{}l@{}} Qwen2-VL \\ Qwen2.5-VL \\ AIN-7B \\ PaliGemma \end{tabular} & \begin{tabular}[t]{@{}l@{}} GPT-4o \\ GPT-4o-mini \\ Gemini-2.0-Flash \end{tabular} & \begin{tabular}[t]{@{}l@{}} Docling[EasyOCR] \\ Docling[Tesseract] \\ Marker \\ Img2Table[EasyOCR] \\ Img2Table[Tesseract] \end{tabular} \\
\midrule
Charts to DataFrame & SCRM \cite{xia2024chartx, xia2023structchart} & \begin{tabular}[t]{@{}l@{}} Qwen2-VL \\ Qwen2.5-VL \\ AIN \\ PaliGemma \end{tabular} & \begin{tabular}[t]{@{}l@{}} GPT-4o \\ GPT-4o-mini \\ Gemini-2.0-Flash \end{tabular} & -- \\
\midrule
Diagram to Json & SCRM & \begin{tabular}[t]{@{}l@{}} Qwen2-VL \\ Qwen2.5-VL \\ AIN-7B \\ PaliGemma \end{tabular} & \begin{tabular}[t]{@{}l@{}} GPT-4o \\ GPT-4o-mini \\ Gemini-2.0-Flash \end{tabular} & -- \\
\midrule
VQA & \begin{tabular}[t]{@{}l@{}} Accuracy + \\ Word Match Score \end{tabular} & \begin{tabular}[t]{@{}l@{}} Qwen2-VL \\ Qwen2.5-VL \\ AIN-7b \\ PaliGemma \end{tabular} & \begin{tabular}[t]{@{}l@{}} GPT-4o \\ GPT-4o-mini \\ Gemini-2.0-Flash \end{tabular} & -- \\
\bottomrule
\end{tabular}
}
\caption{Comprehensive evaluation metrics and models for document understanding tasks. The table is organized into three main categories: document understanding, table understanding, and visual understanding tasks. Each task is evaluated using specific metrics and implemented across various models and OCR systems.}
\label{tab:doc-understanding-metric-model}
\end{table*}


\clearpage

\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/Prompt_a.pdf}
    \caption{Prompts for Different Task Categories.}
    \label{fig:prompta}
\end{figure}

\clearpage

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/Prompt_b.pdf}
    \caption{Prompts for Diagrams and Tables.}
    \label{fig:promptb}
\end{figure}


\end{document}
