\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{5pt}
\resizebox{\linewidth}{!}{
\begin{tabular}{l l c c c}
\toprule
\textbf{Group} & \textbf{Models} & \textbf{CHrF $\uparrow$} & \textbf{CER $\downarrow$} & \textbf{WER $\downarrow$} \\
\midrule
\multirow{3}{*}{\makecell[l]{Closed}} 
& GPT-4o & 61.01 & 0.31 & 0.55 \\
& GPT-4o-mini & 47.21 & 0.43 & 0.71 \\
& Gemini-2.0-Flash & 77.95 & \textbf{0.13} & 0.32 \\
\midrule
\multirow{3}{*}{\makecell[l]{Open}}
& Qwen2VL-7B & 33.94 & 1.48 & 1.55 \\
& Qwen2.5VL-7B & 49.23 & 1.20 & 1.41 \\
& AIN-7B & \textbf{78.33} & 0.20 & \textbf{0.28} \\
\midrule
\multirow{4}{*}{\makecell[l]{Framework}}
& Tesseract & 39.62 & 0.54 & 0.84 \\
& EasyOCR & 45.47 & 0.58 & 0.89 \\
& Paddle & 16.73 & 0.79 & 1.02 \\
& Surya & 20.61 & 4.95 & 5.61 \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison of models for OCR (image to text) tasks on our benchmark. A detailed performance comparison among different open-source dataset is available in Appendix \ref{sec:appendix2}}
\label{tab:ocr-tasks}
\end{table}

