\section{Introduction}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.50\textwidth]{latex/figures/benchmark_taxonomy.pdf}
    \caption{Taxonomy of the Kitab-Bench.}
    \label{fig:task_taxonomy}
\end{figure}

With the upsurge in adoption of Retrieval-Augmented Generation (RAG) based systems for document processing, the quality of document ingestion pipelines has become increasingly critical. Optical Character Recognition (OCR) plays a crucial role in this pipeline, enabling the conversion of physical documents into machine-readable text and databases for enabling effective knowledge retrieval. Although significant progress has been made in the multilingual OCR \cite{easyocr, fu2024ocrbench, wei2024general, smith2007overviewtesseract}, with comprehensive datasets like PubLayNet \cite{zhong2019publaynet}, DocBank \cite{li2020docbank}, M6Doc \cite{cheng2023m6doc}, and DocLayNet \cite{doclaynet2022}, Arabic OCR presents unique challenges due to its cursive nature, complex typography, and right-to-left text orientation. 
% \szq{can revise slightly to avoid only one word in a line.} 

Despite significant advances in multilingual OCR capabilities, there remains a lack of comprehensive evaluation frameworks for Arabic document processing. The absence of standardized benchmarks has made it particularly challenging to systematically evaluate and compare Arabic OCR systems across different tasks. While datasets like CAMEL-Bench \cite{ghaboura2024camel} and LAraBench \cite{abdelali2023larabench} provide valuable insights for Arabic language processing, their primary focus is on evaluating large multimodal models (LMMs) and large language models (LLMs) across various NLP and speech processing tasks, including sequence tagging, content classification, and visual reasoning. However, they do not specifically emphasize document understanding tasks.

The challenges in Arabic OCR extend beyond basic text recognition. Current systems struggle with complex font detection, often misinterpreting decorative Arabic text as images. Numerical recognition presents another significant challenge, with systems frequently confusing Arabic numerals with characters. Furthermore, the handling of elongated words and merged table cells remains problematic, affecting the accuracy of document structure analysis.

To address these limitations, we present a comprehensive Arabic OCR benchmark comprising diverse document types across $9$ major domains and $36$ sub-domains. Our evaluation framework systematically assesses key aspects of Arabic document processing: layout analysis for structural understanding (including text blocks, tables, and figures), multi-format recognition capabilities (spanning printed text, handwritten content, charts, and diagrams), and structured output generation (DataFrame for charts, HTML for tables, markdown for documents, and code for diagrams). This thorough approach enables rigorous evaluation of both fundamental OCR capabilities and advanced document understanding tasks.
Our benchmark aims to systematically evaluate OCR performance across diverse Arabic document elements while facilitating the development of robust Arabic-specific solutions for document analysis.


\noindent Current Arabic OCR datasets like KHATT \cite{mahmoud2014khatt} and IFN/ENIT \cite{pechwitz2002ifn} focus primarily on handwritten text recognition, while others like APTI \cite{slimane2009new} address specific aspects of printed text. However, these datasets do not provide comprehensive coverage of modern document processing challenges. Our benchmark fills this gap by incorporating diverse document types and evaluation tasks, enabling a more thorough assessment of OCR systems.

The contributions of this work include:
\begin{itemize}
    \item A comprehensive Arabic OCR benchmark covering multiple document types and recognition tasks.
    \item Detailed evaluation metrics for assessing performance across different document understanding challenges. \szq{the evaluation metrics are existing from previous papers or newly proposed in this work? If they are new, can highlight ``new metrics'', if they are existing, have to emphasize detailed/comprehensive evaluation/benchmarking rather than metrics.}
    \item Baseline results for popular OCR systems and Vision Language Models (VLMs), highlighting current limitations and areas for improvement.
    \item A standardized framework for comparing Arabic OCR systems, facilitating future research and development.
\end{itemize}
