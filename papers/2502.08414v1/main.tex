\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper]{geometry}
\usepackage{amssymb}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[ruled,vlined,algosection]{algorithm2e}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage[authoryear]{natbib}
\usepackage{url}

\bibpunct{(}{)}{;}{a}{,}{,}  % Author-year citation style



\title{Sparse Estimation of Inverse Covariance and Partial Correlation Matrices via Joint Partial Regression}
\author{Samuel Erickson\footnote{Division of Decision and Control Systems, EECS, KTH Royal Institute of Technology. Email: \texttt{samuelea@kth.se}.} \and Tobias Rydén\footnote{Lynx Asset Management AB. Email: \texttt{tobias.ryden@lynxhedge.se}.}}
\date{November 2024}

\input{defs}

\begin{document}

\maketitle


\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
We present a new method for estimating high-dimensional sparse partial
correlation and inverse covariance matrices, which exploits the connection
between the inverse covariance matrix and linear regression. The method is a
two-stage estimation method wherein each individual feature is regressed on all
other features while positive semi-definiteness is enforced simultaneously. We
provide statistical rates of convergence for the proposed method which match,
and improve upon, the state-of-the-art for inverse covariance and partial
correlation matrix estimation, respectively. We also propose an efficient
proximal splitting algorithm for numerically computing the estimate. The
effectiveness of the proposed method is demonstrated on both synthetic and
real-world data. 
\end{abstract}

 
% \begin{keywords}
%     keyword one, keyword two, keyword three
% \end{keywords}

\section{Introduction}

Two important and closely related problems in statistical learning are the
problems of estimating a partial correlation network and the inverse covariance
matrix, also known as the precision matrix, from data. Partial correlation
networks, which generalize the Gaussian graphical model, are used to model the
relationships between variables while conditioning on all other variables, and
are useful for inferring causal relationships between variables. Partial
correlation networks are used in a plethora of applications, such as in the
analysis of gene expression data, where the goal is to infer the regulatory
relationships between genes \citep{delaFuente04}, and psychological data, where
networks are used to model the relationships between psychological variables
such as mood and attitude \citep{Epskamp18}. The precision matrix, from which
we can obtain the partial correlation network, is also of interest in its own
right, as it also appears in linear discriminant analysis \citep{Hastie09} and
in Markowitz portfolio selection \citep{Markowitz52}. However, due to the
high-dimensionality of the problem, estimating a precision or partial
correlation matrix is often challenging as the number of parameters are on the
order of the squared number of features. For this reason, classical methods
such as using the inverse of the sample covariance matrix, are known to perform
poorly whenever the number of observation is not extremely large. Additionally
they produce estimates which are almost surely dense. This makes regularization
crucial, since in many applications we typically only have a moderate number of
observations, and in particular we are most often seeking a sparse estimate
which gives rise to a more parsimonious and interpretable network model.

\paragraph{Related work.} Gaussian graphical models, and by extension sparse
precision matrix estimation, have been studied extensively in the literature
with the \emph{covariance selection} problem originally being due to
\cite{Dempster72}. Today there are several methods for estimating the precision
matrix, which can be divided into three main categories: methods based upon
maximum likelihood estimation, methods based upon approximate inversion, and
methods based upon what we shall refer to as \emph{partial regression}. T/he
partial regression approach exploits a connection between the precision matrix
and linear regression, and is given its name due to its close relation to
partial correlations.

Perhaps the most popular method of estimation is $\ell_1$-regularized maximum
likelihood estimation (commonly known as the \emph{graphical lasso}). This
method was introduced independently by \cite{Yuan07} and \cite{dAspremont08}
(having slight variations), where the former work also provided a consistency
result in the high-dimensional setting. There has additionally been much work
on the problem of numerically solving the graphical lasso problem, such as the
work of \cite{Friedman08}, who also coined the name. 

\cite{Cai11} provide a method of the second kind, based on minimizing the
$\ell_1$-norm of the estimate under the constraint that the estimate is
sufficiently close to an inverse of the sample covariance matrix in the
$\ell_\infty$-norm. Robust extensions of this method are provided by
\cite{Wang17} and \cite{Zhao14}, with the former focusing on the case of
contaminated data and the latter on the case of data drawn from a heavy-tailed
distribution.

The present work is most closely related to the works of \cite{Meinshausen06},
\cite{vandeGeer14} and \cite{Yuan10}, who opt for estimation via partial
regression. \citeauthor{Meinshausen06} specifically consider the sparsity
pattern estimation problem in a high-dimensional setting, and use the lasso
\citep{Tibshirani96} to estimate the neighborhood of each node in the graphical
model. Their work provides consistency results of their proposed method, and has
become one of the most cited works on the subject. The work of
\citeauthor{vandeGeer14} continues in the same vein and estimate the precision
matrix with the purpose of constructing confidence regions for high-dimensional
models. \citeauthor{Yuan10} provides a method that can be boiled down to solving
a sequence of linear programs, opting for the Dantzig selector \citep{Candes07}
in lieu of the lasso, and projecting the estimate onto the space of symmetric
matrices in the operator $\ell_1$-norm. The author also proves that their method
achieves the minimax optimal rate in the operator $\ell_1$-norm over certain
parameter spaces.

Some work has additionally been done on the problem of estimating
high-dimensional partial correlation matrices. \cite{Peng09} provide a
non-convex method dubbed SPACE for estimating the partial correlation matrix
via partial regression. \cite{Friedman10} and \cite{Khare15} also propose
pseudo-likelihood methods based on partial regression. \citeauthor{Peng09} and
\citeauthor{Khare15} provide consistency results for their methods, with
identical statistical rates of convergence.

\paragraph{Contributions.} The main contributions of the present work are three-fold,
and are as follows: 
\begin{itemize}

    \item We propose a novel method for simultaneously estimating a sparse
        precision and partial correlation matrix via linear regression, which
        we call \emph{joint partial regression}. We also provide a robust
        extension of the method that uses sparse Huber regression, which is
        well-suited for contaminated data. For sub-Gaussian design, we provide
        statistical estimation error rates in a high-dimensional setting, which
        match the best known rates for precision matrix estimation and improve
        upon the best known rates for partial correlation matrix estimation.

    \item We provide an efficient proximal algorithm for numerically fitting
        the proposed method which is well-suited for large-scale data sets. An
        implementation of the algorithm is also given in the form of a Python
        package written in Rust, which is available at
        \url{https://github.com/samericks/joint-partial-regression}.

    \item We conduct numerical studies on synthetic data, showing that the
        proposed method outperforms the graphical lasso in terms of estimation
        error. We also apply the method to real stock market data, showing that
        the method is capable of capturing the underlying structure of the
        data. 

\end{itemize}


\paragraph{Notation.} We write $\symm^p$ for the set of real $p\times p$
matrices, and for $\Omega\in\symm^p$ we write $\Omega\succeq 0$  if $\Omega$ is
positive semi-definite. If $\Omega_1$ and $\Omega_2$ are both in $\symm^p$ we
write $\Omega_1\succeq\Omega_2$ if $\Omega_1 - \Omega_2 \succeq 0$. We write
$\|\Omega\|_{\ell_q} = \max_{\|\nu\|_q \leq 1} \|\Omega \nu\|_q$ for the
operator $\ell_q$-norm, and the Frobenius norm is defined as $\|\Omega\|_{\rm
F} = \sqrt{\Tr(\Omega^2)}$. The cardinality function $\card$ gives the number
of elements if given a set argument, and the number of non-zero entries if
given a vector argument. 

\section{The precision matrix and partial correlation}\label{sec:precision-partial-correlation} 

Likely the most common use of the precision matrix is in the context of Gaussian
graphical models, where the connection between the precision matrix and the
conditional independence structure of jointly Gaussian random variables is
exploited. The basis of this connection is in fact a connection that the
precision matrix also has to linear regression. Suppose $Z\in\reals^p$ is a
square-integrable mean-zero random vector with non-singular covariance $\Sigma$
and inverse $\Omega = \Sigma^{-1}$. Then the best linear unbiased predictor of
a component $Z_j$ given all other components
\[
    Z_{\setminus j} = (Z_1, \dots, Z_{j-1}, Z_{j+1}, \dots, Z_p)
\]
is $Z_{\setminus j}^\top \theta_j$, where
\[
    \theta_j = \Sigma_{\setminus j, \setminus j}^{-1} \Sigma_{\setminus j, j}.
\]
Here $\Sigma_{\setminus j, \setminus j}$ is the sub-matrix of $\Sigma$ obtained
by removing the $j$-th row and column, and $\Sigma_{\setminus j, j}$ is the $j$-th
column of $\Sigma$ with the $j$-th element removed. Likewise, the residual
$\varepsilon_j = Z_j - Z_{\setminus j}^\top \theta_j$ has mean zero and
variance $\tau_j^2 = \Sigma_{jj} - \Sigma_{j, \setminus j} \Sigma_{\setminus j,
\setminus j}^{-1} \Sigma_{\setminus j, j}$. Using Schur complements, we have
that
\[
    \begin{aligned}
      \Omega_{jj} &= (\Sigma_{jj} - \Sigma_{j, \setminus j} \Sigma_{\setminus j, \setminus j}^{-1} \Sigma_{\setminus j, j})^{-1}, \\
      \Omega_{\setminus j, j} &= -(\Sigma_{jj} - \Sigma_{j, \setminus j} \Sigma_{\setminus j, \setminus j}^{-1} \Sigma_{\setminus j, j})^{-1} \Sigma_{\setminus j, \setminus j}^{-1} \Sigma_{\setminus j, j},
    \end{aligned}
\]
and thus
\[
    \begin{aligned}
      \Omega_{jj} &= 1 / \tau_j^2, \\
      \Omega_{\setminus j, j} &= -\theta_j / \tau_j^2. 
    \end{aligned}
\]
In the Gaussian case we find that $\varepsilon_j$ is independent of $Z_{\setminus
j}$, which then implies that $Z_j$ is \emph{conditionally independent} of $Z_k$,
$j\neq k$, given $Z_{\setminus \{j, k\}}$ if and only if $\theta_{jk} = 0$, \ie,
$\Omega_{jk} = 0$. More generally, the \emph{partial correlation coefficient}
between $Z_j$ and $Z_k$ given $Z_{\setminus \{j, k\}}$ is given by the negative
correlation between $\varepsilon_j$ and $\varepsilon_k$, 
\[
    \rho_{jk \mid \setminus\{j, k\}} = -\corr(\varepsilon_j, \varepsilon_k) = -\frac{\E(\varepsilon_j \varepsilon_k)}{\sqrt{\E(\varepsilon_j^2) \E(\varepsilon_k^2)}},
\]
which can be written in terms of the precision matrix and the linear regression
as
\[
    \rho_{jk \mid \setminus\{j, k\}} = -\frac{\Omega_{jk}}{\sqrt{\Omega_{jj} \Omega_{kk}}} = \frac{\tau_k}{\tau_j} \theta_{jk},
\]
respectively. Thus we define the \emph{partial correlation matrix} $Q$ as
\[
    Q = - T \Omega T,
\]
where $T = \diag(\tau_1, \dots, \tau_p)$. Note that some works \citep{Schäfer04,
Marrelec06} define the partial correlation matrix as $-T \Omega T + 2I$, so that
the diagonal elements are ones. 

\section{Proposed method}

In light of the connection between the precision matrix and linear regression
described in \S\ref{sec:precision-partial-correlation}, a way to estimate the
precision matrix is to form estimates of the linear regression coefficients
$\hat\theta_j$ and the residual variances $\hat \tau_j^2$ for each $j$ and then
compute an estimate of the precision matrix $\widehat{\Omega}$ via 
\[
    \begin{aligned}
        \widehat{\Omega}_{jj} &= 1 / \hat\tau_j^2, \\
        \widehat{\Omega}_{\setminus j, j} &= -\hat\theta_j / \hat\tau_j^2,
    \end{aligned}
\]
and an estimate of the partial correlation matrix $\widehat{Q}$ via
\[
    \widehat{Q} = -\widehat{T} \widehat{\Omega} \widehat{T},
\]
where $\widehat{T} = \diag(\hat\tau_1, \dots, \hat\tau_p)$. An obvious approach,
since we are interested in sparsity, is to use the lasso. Suppose we have access
to a design matrix $X \in \reals^{n \times p}$, where each row contains a sample
of the random vector $Z$ and each column corresponds to a feature of the data.
We can then estimate the linear regression coefficients by solving the lasso
problem
\[
    \hat \theta_j = \argmin_{\theta \in \mathbf{R}^{p-1}} \left\{\frac{1}{2n} \|X_j - X_{\setminus j} \theta\|_2^2 + \lambda \|\theta\|_1 \right\}
\]
and subsequently estimate the residual variances by $\hat \tau_j^2 = (1/n) \|X_j
- X_{\setminus j} \hat \theta_j\|_2^2$. However, this approach does not result
in a positive semi-definite estimate of the precision matrix, let alone a
symmetric one. If we are solely interested in the unweighted partial correlation
network, we could choose to include the edge $(j,k)$ only if
$\widehat{\Omega}_{jk}$ and $\widehat{\Omega}_{kj}$ are both non-zero or,
alternatively, if either is non-zero. If we are interested in the partial
correlations themselves we could choose to symmetrize the estimate by averaging
the estimates of $\widehat{\Omega}_{jk}$ and $\widehat{\Omega}_{kj}$, or by
projection onto the cone of positive semi-definite matrices in some norm. We
will however take a different approach, in which we will enforce positive
semi-definiteness while simultaneously estimating the linear regression
coefficients, given initial estimates of the residual variances.

\subsection{Joint partial regression}
We propose the following convex program that regresses the features at the same
time as ensuring positive semi-definiteness,
\begin{equation}\label{eq:cvx-prob}
    \begin{array}{ll}
        \text{minimize} & \displaystyle \sum_{j=1}^{p} \left( \frac{1}{2n} \|X_j - X_{\setminus j} \theta_j\|_2^2 + \lambda \|\theta_j\|_1 \right)\\
        \text{subject to} & \Omega_{jj} = 1 / \hat\tau_j^2, \quad \Omega_{\setminus j, j} = -\theta_j / \hat\tau_j^2, \quad j = 1, \dots, p, \\
        & \Omega \succeq 0, \quad Q = -\widehat{T} \Omega \widehat{T}.
    \end{array}
\end{equation}
Here $\Omega\in\R^{p\times p}$ and $\theta_j\in\R^{p-1}$, $j=1,2,\dots,p$, are
the optimization variables, and the data are the data matrix $X$, the estimates
$\hat\tau_j^2$ of the residual variances, and the regularization parameter
$\lambda \geq 0$. Note that the equality $Q = -\widehat{T}\Omega\widehat{T}$ is
not a real constraint as $Q$ does not appear in the objective function or other
constraints, but simply expresses that $Q$ is a by-product of the optimization
problem in the stated way. We call the solutions $\widehat{\Omega}$ and
$\widehat{Q}$ the \emph{joint partial regression} estimates of the precision
matrix and the partial correlation matrix, respectively. In order to obtain the
initial estimates of the residual variances we can use the aforementioned lasso
estimates of the linear regression coefficients to compute the sample variances
$\hat \tau_j^2 = (1/n) \|X_j - X_{\setminus j} \hat \theta_j\|_2^2$. We
summarize the proposed procedure in Algorithm~\ref{algo:jpr}.

% \begin{equation}
%     \begin{array}{ll}
%         \text{minimize} &  \displaystyle \frac{1}{2n} \|X - X \Theta\|_F^2 + \lambda\|\Theta\|_1 \\[0.5em]
%         \text{subject to} & \widehat{T}^{-2} - \widehat{T}^{-1} \Theta \widehat{T} \succeq 0, \quad \diag(\Theta) = 0.
%     \end{array}
% \end{equation}
Note moreover that applying the derivation in
\S\ref{sec:precision-partial-correlation} to the sample covariance matrix
reveals that Algorithm~\ref{algo:jpr} exactly retrieves the inverse sample
covariance matrix when $\lambda = 0$.

\begin{algorithm}[ht]
    \SetAlgoLined 
    
    \textbf{Input:} {Data matrix $X \in \reals^{n \times p}$, penalty parameter
    $\lambda$.} \\

    \caption{\sc Joint partial regression}
    
    \BlankLine
    
    \textbf{for} {$j = 1, \dots, p$} \textbf{do} {
    \begin{quote} \it lasso regression
        \[
            \hat\theta_j = \argmin_{\theta \in \R^{p-1}} \left\{\frac{1}{2n} \|X_j - X_{\setminus j} \theta\|_2^2 + \lambda \|\theta\|_1\right\},
        \]
        and compute the estimate $\hat\tau_j^2 = (1/n) \|X_j - X_{\setminus
        j}\hat\theta_j\|_2^2$ of the residual variance.
    \end{quote}
    } \textbf{end}
    
    \textit{Solve the convex program \eqref{eq:cvx-prob} with the estimated residual
    variances $\hat\tau_j^2$ and regularization parameter $\lambda$ to obtain the
    estimates $\widehat{\Omega}$ and $\widehat{Q}$ of the precision matrix and
    partial correlation matrix, respectively.}
    \label{algo:jpr}
\end{algorithm}


\subsection{Extensions}

A benefit of our proposed method is that the convex program \eqref{eq:cvx-prob}
can easily be modified. For example, we need not be limited to using quadratic
loss, and may opt for another convex loss $\ell \colon \reals^n \to \reals$. In
the case of contaminated or heavy-tailed data, we can benefit from taking $\ell$
as the Huber loss,
\begin{equation}\label{eq:huber-loss}
    \ell(z) = \frac{1}{n} \sum_{i=1}^{n} \phi_\rho (z_i), \quad  \phi_\rho(r) = \begin{cases}
        r^2 / 2, & \text{if } |r| \leq \rho, \\
        \rho (|r| - \rho / 2), & \text{if } |r| > \rho.
    \end{cases}
\end{equation}
We may also use different regularization parameters $\lambda_1, \dots,
\lambda_p$ for each linear regression, rather than just a single $\lambda$. In
practice, choosing these parameters via cross-validation does in fact not
become computationally infeasible, since in the initial regression step we may
simply perform $p$ model selection procedures for the separate lasso
regressions in order to choose the parameters, and subsequently solve the main
optimization problem only once.

Another possible modification to \eqref{eq:cvx-prob} is to constrain the
eigenvalues of the precision matrix estimates to lie within $[\alpha, \beta]$
for some choice of $\alpha$ and $\beta$ with $0 \leq \alpha \leq \beta \leq
\infty$. In this way we can control the condition number of our estimate, which
is desirable if we would like to produce an estimate of the covariance matrix
via inversion. We thus have the generalized joint partial regression problem
\begin{equation}\label{eq:cvx-prob-general}
    \begin{array}{ll}
        \text{minimize} & \displaystyle \sum_{j=1}^{p} \left( \ell(X_j - X_{\setminus j} \theta_j) + \lambda_j \|\theta_j\|_1 \right) \\
        \text{subject to} & \Omega_{jj} = 1 / \hat\tau_j^2, \quad \Omega_{\setminus j, j} = -\theta_j / \hat\tau_j^2, \quad j = 1, \dots, p \\
        & \alpha I \preceq \Omega \preceq \beta I, \quad Q = -\widehat{T} \Omega \widehat{T}.
    \end{array}
\end{equation}

We can also choose another method than sample variance for estimating the residual variances.
For example, \cite{Fan12} suggested
\[
    \hat\tau_j^2 = \frac{1}{n - \hat s_j} \|X_j - X_{\setminus j}\hat \theta_j\|_2^2, \quad \hat s_j = \card(\hat\theta_j),
\]
which may be more suitable in the $p \gg n$ case. Under the contaminated data
assumption we propose using Huber loss and a simple truncated mean squared
estimator, where the residuals with magnitude larger than the threshold $\rho$
(the residuals that the Huber regression ``rejected'') are left out. That is,
for the residual $r_j = X_j - X_{\setminus j} \hat \theta_j$ we estimate the
residual variance  as 
\begin{equation}\label{eq:robust-variance-estimator}
    \hat\tau_j^2 = \frac{1}{n_j} \sum_{i=1}^{n} r_{ji}^2 \mathbf{1}(|r_{ji}| \leq \rho),
\end{equation}
where $n_j = \sum_{j=1}^{p} \mathbf{1}(|r_{ji}| \leq \rho)$ is the number of
residuals not omitted.



% \begin{algorithm}[ht]
%     \SetAlgoLined 
    
%     \textbf{Input:} {Data matrix $X \in \reals^{n \times p}$, penalty parameters
%     $\lambda_1, \dots, \lambda_p$, eigenvalue limits $\alpha$ and $\beta$.} \\

%     % \KwResult{Estimate $\widehat{\Omega} $ of the precision matrix.}
%     \caption{\sc Generalized joint partial regression}
    
%     \BlankLine
    
%     \textbf{for} {$j = 1, \dots, p$} \textbf{do} {
%     \begin{quote} \it sparse regression
%         \[
%             \hat\theta_j = \argmin_{\theta \in \R^{p-1}} \left\{\ell(X_j - X_{\setminus j} \theta) + \lambda_j \|\theta\|_1\right\},
%         \]
%         and compute the estimate $\hat\tau_j^2$ of the residual variance.
%     \end{quote}
%     } \textbf{end}
    
%     \it Solve the convex program \eqref{eq:cvx-prob-general} with the estimated
%     residual variances $\hat\tau_j^2$, regularization parameters $\lambda_1,
%     \dots, \lambda_p$, and limits $\alpha$ and $\beta$ to obtain the estimates
%     $\widehat{\Omega}$ and $\widehat{Q}$ of the precision matrix and partial
%     correlation matrix, respectively.

%     \label{algo:jpr-general}
% \end{algorithm}



\section{Theoretical results}

In this section we provide theoretical guarantees for the estimation error of
the proposed method that hold with high probability under given assumptions. 
Each data point, a random vector in $\reals^p$, is assumed to have a true,
unknown, covariance matrix that we denote by $\Sigma^\star$. The corresponding
true precision matrix is $\Omega^\star = (\Sigma^\star)^{-1}$. Our first
assumption concerns how the dimensionality of the problem grows. Indeed, the
theoretical results below are asymptotic in nature, assuming that both $n$ and
$p$ tend to infinity. This first assumption specifies how fast $p$ can grow
with $n$, and also bounds the sparsity of the true precision matrix.

\begin{assumption}\label{assumption:dimensionality}    
The dimensionality is such that $p/n \leq 1 - \delta$ for some $\delta \in (0,
1)$, and the degree
\[
    d = \max_j \sum_{k\neq j} \mathbf{1}(\Omega_{jk}^\star \neq 0)
\]
of the partial correlation network is such that $d\sqrt{\log (p)/n}\leq M$ for
some constant $M>0$.  
\end{assumption}

Notice that the dependence on $n$ and $p$ is in many cases not made explicit;
for example, in the above assumption both $\Omega^\star$ and $d$ tacitly depend
on $n$ and $p$. We remark that the term $d\sqrt{\log(p)/n}$ that appears in the
assumption has been shown by \cite{Yuan10} to be the minimax optimal error rate
in the operator $\ell_1$-norm for precision matrix estimation. It is also the
error rate of the graphical lasso in the spectral norm as shown by
\cite{Ravikumar11}. Hence the boundedness assumption on that term is
comparatively weak.

Our second assumption is that the data is \emph{sub-Gaussian}. A real-valued
random variable $Y$ is said to be sub-Gaussian if there exists a constant $C$
such that the tail probabilities satisfy 
\[
    \prob(|Y| \geq t) \leq 2 \exp(-t^2 / C^2), \quad \text{ for all } t \geq 0.
\] 
For sub-Gaussian random variables we also define the \emph{sub-Gaussian norm}
$\|\cdot\|_{\psi_2}$ by
\[
    \|Y\|_{\psi_2} = \inf\{t > 0 \colon \E(\exp(Y^2 / t^2)) \leq 2\}.
\]
With this in mind, we state the following assumption.

\begin{assumption}\label{assumption:sub-gaussianity}

The rows of the design matrix $X \in \reals^{n \times p}$ are $n$ i.i.d.\
samples from a random vector with covariance matrix $\Sigma^\star$, and each
$X_{ij}$ is sub-Gaussian with associated norm $\|X_{ij}\|_{\psi_2} \leq K$ for
some $K > 0$. \end{assumption} Since the bound $K$ is for all $p$, $i$ and $j$,
we may say that the elements of $X$ are \textit{uniformly sub-Gaussian}. 

Finally we assume that the eigenvalues of true covariance matrix do not
degenerate to zero or infinity, and that the maximum absolute row sum of the
precision matrix is bounded.

\begin{assumption}\label{assumption:bounded-eigenvalues}    
The precision matrix $\ \Omega^\star = (\Sigma^\star)^{-1}$ has bounded operator
$\ell_1$-norm and its eigenvalues are uniformly bounded away from $0$ and
$\infty$, \ie, there exists constants $\kappa \in (1,\infty)$ and $L \in
(0,\infty)$ such that
\[
    \Omega^\star \in \{\Omega \succeq 0 \colon 1/\kappa \leq \lambda_\mathrm{min}(\Omega) \leq \lambda_\mathrm{max}(\Omega) \leq \kappa, \ \|\Omega\|_{\ell_1} \leq L\}.
\]
\end{assumption}

The boundedness assumption on the maximum absolute row sum of $\Omega^\star$
was also used by \cite{Yuan10}. In our analysis it is only used to ensure that
the residuals $\varepsilon_j$ have bounded sub-Gaussian norms, and it would not
be necessary in the case of Gaussian data since then the tail probabilities can
be controlled via the variances $\tau_j^2$ which are bounded due to the
eigenvalue assumption.

Defining
\[
    s = \card\{(j,k) \colon \Omega^\star_{jk} \neq 0,\ j > k\}    
\]
as number of non-zero off-diagonal elements of the precision matrix
$\Omega^\star$ (or, equivalently, the partial correlation matrix $Q^\star$), we
can now state the main result of this section using the assumptions above.

\begin{theorem}\label{thm:est-rate}
Under
Assumptions~\ref{assumption:dimensionality}--\ref{assumption:bounded-eigenvalues}
there exist positive constants $c$, $C_1$ and $C_2$ such that
Algorithm~\ref{algo:jpr} with $\lambda = c\sqrt{\log(p)/n}$ outputs an estimate
$\widehat{Q}$ of the partial correlation matrix that satisfies
\begin{equation}\label{eq:Q-estimation-error}
    \|\widehat{Q} - Q^\star\|_{\rm F} \leq C_1 \sqrt{\frac{s\log p}{n}}    
\end{equation}
and an estimate $\widehat{\Omega}$ of the precision matrix that satisfies
\begin{equation}\label{eq:Omega-estimation-error}
    \|\widehat{\Omega} - \Omega^\star\|_{\rm F} \leq C_2 \sqrt{\frac{(s+p)\log p}{n}}
\end{equation}
with probability at least $1 - 6/p$.
\end{theorem}

The statistical rate of convergence \eqref{eq:Q-estimation-error} for the
partial correlation matrix estimate is an improvement over the rates for the
SPACE method of \cite{Peng09} and the quasi-likelihood method of
\cite{Khare15}. Although they are similarly $\sqrt{s} \lambda$, they require a
choice of regularization parameter $\lambda$ that is asymptotically larger than
ours, unless $s$ is bounded in which case they achieve a similar rate. The rate
\eqref{eq:Omega-estimation-error} for the precision matrix estimate matches
that of the graphical lasso proved by \cite{Rothman08}. To the best of our
knowledge, these are the best known Frobenius norm rates for partial
correlation estimation and precision matrix estimation, respectively.

An inspection of the proof of Theorem~\ref{thm:est-rate} reveals that we can in
fact obtain an explicit bound on the Frobenius error of the partial correlation
matrix using quantities that, apart from $s$, can be computed in practice.

\begin{corollary}
Under the assumptions of Theorem~\ref{thm:est-rate}, if $\lambda =
c\sqrt{\log(p)/n}$ with $c$ sufficiently large, the estimate $\widehat{Q}$
produced by Algorithm~\ref{algo:jpr} satisfies
\[
    \|\widehat{Q} - Q^\star\|_{\rm F} \leq \frac{4}{\lambda_\mathrm{min}(\widehat{\Sigma})}\left(\frac{\max_j \hat \tau_j}{\min_j \hat \tau_j}\right)^3 \sqrt{s}\lambda 
\]
with probability at least $1 - 6/p$, where $\widehat{\Sigma} = (1/n) X^\top X$
is the sample covariance matrix.
\end{corollary}

\section{A proximal splitting algorithm}\label{sec:algorithm}

To numerically compute the joint partial regression estimate, consider the
equivalent problem to \eqref{eq:cvx-prob-general}, 
\[
    \begin{array}{ll}
        \text{minimize} & \displaystyle f(\Omega) + g(\Omega) + h(\Omega)
    \end{array}
\]
where
\[
    f(\Omega) = \sum_{j=1}^{p} \ell(X_j + \hat\tau_j^2X_{\setminus j} \Omega_{\setminus j , j}),
\]
is the sum of the partial regression losses,
\[
    g(\Omega) = \begin{cases}
        0 & \text{if } \alpha I \preceq \Omega \preceq \beta I, \\
        \infty & \text{otherwise}
    \end{cases}
\]
is the indicator function for the set $\mathcal{S} = \{\Omega \in \symm^p \colon
\alpha I \preceq \Omega \preceq \beta I \}$, and
\[ 
    h(\Omega) = \sum_{j=1}^{p} \left(\lambda_j \hat\tau_j^2 \|\Omega_{\setminus j, j}\|_1 + \delta_{\{1/\hat\tau_j^2\}}(\Omega_{jj}) \right), % + \delta_{\symm^p}(\Omega), % \lambda \|\Omega\|_1 + \delta_{D}(\Omega)
\]
is the sum of the $\ell_1$-regularization terms and the indicator functions of
the diagonal elements. Assuming $\ell$ is $L$-smooth for some $L\geq 0$, the
general joint partial regression problem \eqref{eq:cvx-prob-general} can be
solved via the PD3O algorithm \citep{Yan18}. In this instance, the PD3O
algorithm has iteration updates 
\[
    \begin{aligned}
        \Omega^{(k+1)} &= \prox_{\gamma g}\left(\Omega^{(k)} - \gamma U^{(k)} - \gamma \nabla f(\Omega^{(k)}) \right), \\
            U^{(k+1)} &= \prox_{\eta h^*}\left(U^{(k)} + \eta (2 \Omega^{(k+1)} - \Omega^{(k)}) + \gamma \eta (\nabla f(\Omega^{(k)}) - \nabla f(\Omega^{(k+1)})) \right).
        % \left[\begin{matrix} \Omega^{(k+1)}\\ U^{(k+1)}\end{matrix}\right] &= \gamma_k \left[\begin{matrix} \Omega^{(k+1/2)}\\ U^{(k+1/2)}\end{matrix}\right] + (1 - \gamma_k) \left[\begin{matrix} \Omega^{(k)}\\ U^{(k)}\end{matrix}\right].
    \end{aligned}
\]
The iterates $\Omega^{(k)}$ and $U^{(k)}$ are guaranteed to converge to a primal
and a dual solution of the problem, respectively, if $\gamma$ and $\eta$ are
chosen as positive step sizes satisfying $\gamma < 2 / L$ and $\gamma \eta \leq
1$.

\paragraph{Primal update.} The primal iterate update is a projection onto the
set $\mathcal{S}$,
\[
    \Omega^{(k+1)} = \Pi_{\mathcal{S}}\left(\Omega^{(k)} - \gamma U^{(k)} - \gamma \nabla f(\Omega^{(k)}) \right).
\]
Importantly, note that the matrix which is being projected is not in general
symmetric. The projection of a matrix $A \in \reals^{n \times n}$ onto 
$\mathcal{S}$ is by definition given by
\[
    \Pi_{\mathcal{S}}(A) = \argmin_{\Omega \in \mathcal{S}} \|\Omega - A\|_{\rm F}^2.
\]
Due to symmetry, for any $\Omega \in \mathcal{S}$ the squared distance 
$\|\Omega - A\|_{\rm F}^2$ can be written as 
\begin{equation}\label{eq:projection}
    \sum_{j=1}^{p}(\Omega_{jj} - A_{jj})^2  + 2\sum_{j=1}^{p}\sum_{k<j}^{p} (\Omega_{jk} - (A_{jk} + A_{kj}) / 2)^2 = \|\Omega - (A + A^\top) / 2\|_{\rm F}^2
\end{equation}
up to a term constant in $\Omega$. Let $Q \Lambda Q^\top = (A+
A^\top) / 2$ be the eigenvalue decomposition of the symmetric part of $A$. The
Frobenius norm is invariant under orthogonal transformations, so
\eqref{eq:projection} can equivalently be written as
\[
    \|Q^\top(\Omega - (A + A^\top) / 2)Q\|_{\rm F}^2 = \|Q^\top\Omega Q - \Lambda\|_{\rm F}^2.
\]
Clearly, this expression is minimized by choosing $\Omega$ such that $Q^\top
\Omega Q$ is diagonal, with the diagonal elements being the projection of the
eigenvalues of ($A + A^\top) / 2$ onto the interval $[\alpha, \beta]$. Thus the
projection $\Pi_{\mathcal{S}}(A)$ is given by projecting the symmetric part of
$A$, 
\begin{align*}
    \Pi_{\mathcal{S}}(A) &= Q \Lambda_{[\alpha, \beta]} Q^\top
\end{align*}
where $\Lambda_{[\alpha, \beta]}$ is a diagonal matrix with entries
$(\Lambda_{[\alpha, \beta]})_{jj} = \min(\max(\Lambda_{jj}, \alpha), \beta)$. 

The gradient that appears in the primal step is given by
\[
    \begin{aligned}
        \nabla f(\Omega)_{jj} &= 0 \\
        \nabla f(\Omega)_{\setminus j, j} &= \hat \tau_j^2 X_{\setminus j}^\top \nabla \ell(X_{j} + \hat\tau_j^2 X_{\setminus j} \Omega_{\setminus j, j}),
    \end{aligned}
\]
and depends on the loss function we choose. If we take the quadratic loss as in
\eqref{eq:cvx-prob}, then for $i=1, \dots, n$, 
\[
    \nabla \ell(z)_i = \frac{z_i}{n}.
\]
For the Huber loss \eqref{eq:huber-loss}, we have instead 
\[
    \nabla \ell(z)_i = \frac{1}{n} \begin{cases}
        z_i, & \text{if } |z_i| \leq \rho,\\
        \rho \sign(z_i), & \text{otherwise}.
    \end{cases}
    % \nabla \phi_\rho(X_{j} + \hat\tau_j^2 X_{\setminus j} \Omega_{\setminus j, j})_i = \begin{cases}
    %     X_{ij} + \hat\tau_j^2 X_{i, \setminus j} \Omega_{\setminus j, j}, & \text{if } |X_{ij} + \hat\tau_j^2 X_{i,\setminus j} \Omega_{\setminus j, j}| \leq \rho,\\
    %     \rho \sign(X_{ij} + \hat\tau_j^2 X_{i,\setminus j} \Omega_{\setminus j, j}), & \text{otherwise}.
    % \end{cases}
    % \frac{1}{n} \sum_{i=1}^{n} \hat\tau_j^2 X_{ik} \begin{cases}
    %     X_{ij} + \hat\tau_j^2 X_{ik} \Omega_{kj}, & \text{if } |X_{ij} + \hat\tau_j^2 X_{i,\setminus j} \Omega_{\setminus j, j}| \leq \rho,\\
    %     \rho \sign(X_{ij} + \hat\tau_j^2 X_{i,\setminus j} \Omega_{\setminus j, j}), & \text{otherwise}.
    % \end{cases}
\]
For two points $\Omega$ and $\widetilde{\Omega}$ in $\reals^{n\times n}$, with either
loss function, $f$ satisfies
\[
    \begin{aligned}
        \| \nabla f(\Omega) - \nabla f(\widetilde{\Omega})\|_{\rm F} 
    & \leq \frac{1}{n} \left(\sum_{j=1}^{p} \|\hat\tau_j^4 X_{\setminus j}^\top X_{\setminus j}(\Omega_{\setminus j, j} - \widetilde{\Omega}_{\setminus j, j})\|_2^2\right)^{1/2} \\
                                                        &\leq \frac{1}{n} \max_{1\leq j\leq p} \hat\tau_j^4 \lambda_\text{max}(X_{\setminus j}^\top X_{\setminus j}) \|\Omega - \widetilde{\Omega}\|_{\rm F}. 
    \end{aligned}
\]
So $f$ is $L$-smooth with $L = \max_{1\leq j\leq p} \hat\tau_j^4 \lambda_\text{max}(X_{\setminus j}^\top X_{\setminus j}) / n$.

\paragraph{Dual update.} By the Moreau identity, we can write the dual iterate
update with the proximal operator of $h / \eta$ instead of the Fenchel
conjugate $\eta h^*$,
\begin{align*}
    V^{(k+1)} &= U^{(k)} + \eta (2 \Omega^{(k+1)} - \Omega^{(k)}) + \gamma\eta(\nabla f(\Omega^{(k)}) - \nabla f(\Omega^{(k+1)})), \\
    U^{(k+1)} &= V^{(k+1)} - \eta \prox_{h / \eta}(V^{(k+1)} / \eta).
    % &= U^{(k)} + \eta (2 \Omega^{(k+1)} - \Omega^{(k)}) - \eta \Pi_{\mathcal{S}}\left((U^{(k)} + \eta (2 \Omega^{(k+1)} - \Omega^{(k)}) / \eta)\right).
\end{align*}
The proximal operator of $h / \eta$ is given by 
\[
\begin{aligned}
    \prox_{h / \eta}(V)_{jj} &= 1 / \hat\tau_j^2, \\
    \prox_{h / \eta}(V)_{kj} &= \sign(V_{kj})(V_{kj} - \hat\tau_j^2 \lambda_j / \eta)_+,
\end{aligned}
\]
for $j = 1, \dots, p$ and $k \neq j$.

\paragraph{Termination criterion.} To stop the algorithm within a finite number
of steps, we terminate when the iterates are approximately stationary. That is,
given a tolerance $\epsilon^\text{tol} > 0$, the termination criterion is
\[
    \max(\|\Omega^{(k+1)} - \Omega^{(k)}\|_{\rm F}, \|U^{(k+1)} - U^{(k)}\|_{\rm F}) \leq \epsilon^\text{tol}.
\]

\paragraph{Computational complexity.} The computational cost of the projection
in the primal update is that of an eigenvalue decomposition, which is $O(p^3)$.
In the case of the quadratic loss the gradient computation is $O(p^3)$ if we
cache the products $X_{\setminus j}^\top X_j$ and $X_{\setminus j}^\top
X_{\setminus j}$. This means that apart from an initial matrix multiplication,
the computational cost of the algorithm is independent of the number of samples
$n$. For the Huber loss, however, the gradient computation is $O(np^2)$, as we
have to check the residual for each observation. The computational cost of the
dual iterate update is $O(p^2)$, meaning the total per iteration cost of the
algorithm is $O(p^3)$ when using the quadratic loss (matching the graphical
lasso), and $O(p^3 + np^2)$ when using the Huber loss. Compared to the SPACE
method, which has an iteration cost of $O(\min(p^3, np^2))$, our algorithm is
slightly more expensive for $n < p$. However, note that the SPACE method does
not necessarily produce a negative semi-definite estimate, and that enforcing
any eigenvalue constraints will necessarily incur a cost of $O(p^3)$ due to the
necessary eigenvalue decomposition.

\section{Numerical experiments} 

We study the performance of the proposed method
and compare it to existing estimation methods on synthetic data, and test the
proposed method on real stock market data. In \S\ref{sec:synthetic-data} we
evaluate the performance of the estimators on synthetic data from a known
distribution, meaning the true precision matrix is known. In
\S\ref{ssec:timings} we provide a comparison of computation times in CPU
seconds, and in \S\ref{ssec:stock-market-data} we estimate the partial
correlation network of the returns of stocks on the Nasdaq Stock Market.

\subsection{Synthetic data}\label{sec:synthetic-data}
We begin by evaluating the performance of the joint partial regression method on
synthetic data for which the distribution and the true precision matrix are known.
We compare the performance of the proposed method with the graphical lasso,
as well as an oracle estimator which maximizes the Gaussian likelihood with the 
knowledge of the true sparsity pattern. 

\paragraph{Data generation.} For different numbers of features $p$ we generate
data sets of $n=500$ samples from a $p$-dimensional Gaussian distribution
$\mathcal{N}(0, \Sigma)$. We generate the precision matrix $\Omega =
\Sigma^{-1}$ according to three different models. In the first model we
generate an Erdös--Rényi network  with edge probability 0.05. That is, the
adjacency matrix $A \in \reals^{p\times p}$ is generated as
\[
    A_{jk} = A_{kj} = \begin{cases}
        0 & \text{with probability } 0.95,\\
        1 & \text{with probability } 0.05. \\
    \end{cases}
\]
In the second model we generate a simple path network, with the adjacency
matrix $A$ having ones on the secondary diagonals. We denote this model by
\emph{AR(1)}, since it corresponds to a first order autoregressive model. In
the third model we generate a hub network as in \cite{Peng09}, where one node
has a high degree of connectivity (connected to 20\% of all other nodes), and
the rest of the nodes have between one and three connections. Such a model is
interesting in the context of gene regulatory networks, where a single gene may
regulate a large number of other genes.

In all three models we generate the precision matrix by randomly flipping the
signs of the adjacency matrix and setting the diagonal elements of matrix to
$\Omega_{jj} = 1 + \sum_{k\neq j}|A_{jk}|$ to guarantee positive definiteness
via strict diagonal dominance. 

\paragraph{Regularization parameter selection.} We select the regularization
parameters for the graphical lasso and joint partial regression via 5-fold
cross-validation. For joint partial regression we use different regularization
parameters $\lambda_j$ for each feature, and select them separately via
cross-validation in the initial regression step.

\paragraph{Results.} We compare the average Frobenius and operator $\ell_2$-norm
error of the three methods over 100 generated data sets. The average Frobenius
errors are shown in Figure~\ref{fig:fro-error}, and the average operator
$\ell_2$-norm are shown in Figure~\ref{fig:l2-error}. The results show that the
joint partial regression method outperforms the graphical lasso almost uniformly
across all models and problem sizes, with Figure~\ref{fig:hub_l2} even showing
joint partial regression as closer to the oracle estimator than it is to the
graphical lasso. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/er_fro.pdf}
        \caption{Erdös--Rényi model.}
        \label{fig:er_fro}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ar_fro.pdf}
        \caption{$\text{AR}(1)$ model.}
        \label{fig:ar_fro}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hub_fro.pdf}
        \caption{Hub network model.}
        \label{fig:hub_fro}
    \end{subfigure}
    \caption{Average Frobenius error versus number of features with $\pm 2
    \times \text{SE}$ bands.}
    \label{fig:fro-error}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/er_l2.pdf}
        \caption{Erdös--Rényi model.}
        \label{fig:er_l2}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/ar_l2.pdf}
        \caption{$\text{AR}(1)$ model.}
        \label{fig:ar_l2}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hub_l2.pdf}
        \caption{Hub network model.}
        \label{fig:hub_l2}
    \end{subfigure}
    \caption{Average operator $\ell_2$-error versus number of features with $\pm 2
    \times \text{SE}$ bands.}
    \label{fig:l2-error}
\end{figure}

\subsection{Timing comparisons}\label{ssec:timings} We compare the average
timings of our implementation of joint partial regression and an implementation
of the graphical lasso over $10$ datasets of $n=500$ Gaussian samples with
different numbers of features $p$. The timings are shown in
Table~\ref{tab:timings}. Both methods are run with $\lambda=10^{-2}$, which
gives them a similar level of sparsity. 

Joint partial regression is implemented in a Python package written in Rust,
using the proximal algorithm derived in \S\ref{sec:algorithm} along with FISTA
\citep{Beck09} for the initial regression step. The graphical lasso is
implemented in the Python package Scikit-learn \citep{sklearn}, and uses the
algorithm proposed by \cite{Friedman08} coupled with a coordinate descent
algorithm written in Cython. 

Note that the termination criteria of the methods are disanalogous, since the
graphical lasso uses a dual residual while joint partial regression uses the
difference of iterates. Hence we compare timings both when the methods are run
until the tolerance $\epsilon^\text{tol} = 10^{-3}$ and until the 100th
iteration is reached, respectively. All tests were performed on an Intel Core
i9 3.2 GHz processor.


%     Joint partial regression Graphical lasso
% 100                 0.114062         1.00625
% 200                   0.6375         5.90625
% 400                 5.089062        29.80625
%     Joint partial regression Graphical lasso
% 100                 0.023598        0.227473
% 200                 0.038069        0.705897
% 400                 0.091431        3.736814

\begin{table}[ht]
    \centering
    \caption{Average timings (CPU seconds) with $\pm \times \mathrm{SE}$ for
        joint partial regression and graphical lasso on ten random datasets of
        $p$ features and $n=500$ observations. }
    \label{tab:timings}
    
    \begin{subtable}{\textwidth}\label{tab:timings-tol}
        \centering
        \subcaption{Termination after tolerance $\epsilon^\text{tol} = 10^{-3}$ is
        reached.}
        \begin{tabular}{lll}
            \toprule
            $p$ & Joint partial regression & Graphical lasso \\
            \midrule
                100 & $\mathbf{0.11 \pm 0.024}$  & $1.01 \pm 0.227$ \\
                200 & $\mathbf{0.64 \pm 0.038}$  & $5.91 \pm 0.706$ \\
                400 & $\mathbf{5.09 \pm 0.091}$  & $29.81 \pm 3.737$ \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}\label{tab:timings-max_iter}
        \centering
        \subcaption{Termination after 100 iterations.}
%     Joint partial regression Graphical lasso
% 100                 0.309375        1.670313
% 200                 1.332812        7.298438
% 400                 6.978125       42.759375
%     Joint partial regression Graphical lasso
% 100                 0.021853        0.278173
% 200                 0.094761         0.97721
% 400                  0.49873        1.915569

       \begin{tabular}{lll}
            \toprule
            $p$ & Joint partial regression & Graphical lasso \\ 
            \midrule
            100 & $\mathbf{0.31 \pm 0.022}$ & $1.67 \pm 0.278$ \\
            200 & $\mathbf{1.33 \pm 0.095}$ & $7.29 \pm 0.977$ \\
            400 & $\mathbf{6.98 \pm 0.499}$ & $42.76 \pm 1.916$ \\
            \bottomrule
        \end{tabular}
    \end{subtable}
\end{table}

% Joint partial regression Graphical lasso
% 100                 2.995392   (0.01813)     4.785711 (1.167691)
% 200                12.088668  (0.101409)     14.633857 (3.791807)
% 400                53.571677  (1.301821)     52.437889 (14.734311)
% 800               382.618956  (9.204173)    261.662792 (75.443892)
                     

\subsection{Stock market data}\label{ssec:stock-market-data} 
To illustrate the performance of the proposed method on real data, we estimate
the partial correlation network of stocks on the Nasdaq Stock Market. The data
consists of the daily returns of 399 out of the 500 highest market
capitalization stocks listed on stock exchanges in the United States. The data
spans from the 11th of February 2013 to the 7th of February 2018, and contains
1258 observations. The network, estimated using the proposed method, is shown in
Figure~\ref{fig:asset-networks}. Here the nodes represent the stocks and the
color indicates the sector to which the stock belongs. We can clearly see
structure relating to sector belonging in the network. The four largest
connected components of the network are overwhelmingly comprised of stocks from
the finance (magenta), utilities (red), real estate (green), and energy (orange)
sectors, respectively.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/sp500_network3.png}
    \caption{Stock market network as estimated by the proposed method.}
    \label{fig:asset-networks}
\end{figure}


\section{Discussion}
We propose a method for estimating high-dimensional sparse precision matrices
and partial correlation matrices based on jointly regressing each feature on
the remaining features. The method is shown to match the statistical rate of
convergence for precision matrix estimation of the graphical lasso, and to
improve upon the rate for the partial correlation matrix estimation of the
SPACE method. We also derive an efficient algorithm for solving the joint
partial regression problem based on the PD3O algorithm. The method was shown to
perform well on both synthetic and real data, outperforming the graphical lasso
in terms of both estimation error and computation time. 




% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\vskip 0.2in
\input{references}

\newpage

\appendix

\section{Proof of Theoretical Results}

\subsection{Technical lemmas}

\begin{lemma}\label{lemma:reg-param}    
Set $\theta_j^\star = -\tau_j^2 \Omega_{\setminus j, j}^\star$ and let
\[
    \hat \theta_j = \argmin_{\theta_j} \left\{\frac{1}{2n}\|X_j - X_{\setminus j} \theta_j\|_2^2 + \lambda \|\theta_j\|_1 \right\}
\]
be the lasso estimate of $\theta_j^\star$. Suppose that
Assumptions~\ref{assumption:sub-gaussianity}--\ref{assumption:dimensionality}
holds, the design matrix satisfies
\[
    \gamma_\ell \|\nu\|_2^2 \leq \frac{1}{n} \|X \nu\|_2^2 \leq \gamma_u \|\nu\|_2^2 \quad \text{for all } \nu \in \reals^p,
\]
for some $\gamma_\ell, \gamma_u \in (0, \infty)$, and $\lambda = c \sqrt{\log(p)
/ n}$ for some sufficiently large constant $c > 0$. Then, with probability at
least $1 - 4/p$, the estimates $\hat \tau_j^2 = (1/n) \|X_j - X_{\setminus j}
\hat \theta_j\|_2^2$ of the residual variances satisfy
\begin{equation}\label{eq:var-estimation-error}
    \max_j |\hat \tau_j^2 - \tau_j^2| \lesssim \sqrt{\frac{\log p}{n}}, 
\end{equation}
and 
\begin{equation}\label{eq:var-estimates-bound}
    \tau_\ell \leq \hat \tau_j \leq \tau_u \quad \text{for all } j = 1, \dots, p,
\end{equation}
for some constants $\tau_\ell, \tau_u \in (0, \infty)$, and the regularization
parameter satisfies 
\begin{equation}\label{eq:reg-param-ineq}
    \begin{aligned}
        \lambda \geq \max\bigg(&\max_j \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty + \gamma_u \kappa |\hat \tau_j^2 - \tau_j^2|\right) \\
        &\max_j \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty + \kappa \frac{\gamma_u}{\tau_\ell} \max_k \left|\frac{\hat \tau_j}{\hat \tau_k} - \frac{\tau_j}{\tau_k}\right| \right)\bigg).                
    \end{aligned}
\end{equation}
\end{lemma}

\begin{proof}
Recall that $X_j = X_{\setminus j} \theta_j^\star + \epsilon_j$, where
$\epsilon_j$ is an $n$-dimensional random variable with zero mean. From
\cite{Bickel09}, we have that if $\lambda \geq (2/n) \|X_{\setminus j}^\top
\epsilon_j\|_\infty$, then 
\begin{equation}\label{eq:lasso-error-1}
    \|\hat \theta_j - \theta_j^\star\|_2 \leq \frac{3}{\gamma_\ell} \sqrt{s_j} \lambda, \quad \|\hat \theta_j - \theta_j^\star\|_1 \leq \frac{12}{\gamma_\ell}  s_j \lambda,
\end{equation}
and 
\begin{equation}\label{eq:lasso-error-2}
    \frac{1}{n}\|X_{\setminus j}(\hat \theta - \theta^\star)\|_2^2 \leq \frac{9}{\gamma_\ell} s_j \lambda^2.
\end{equation}
where $s_j = \card(\theta_j)$. Note that $X_k$ for $k \neq j$ and $\epsilon_j$
are uncorrelated, so $\E(X_k^\top \epsilon_j) = 0$. Moreover, $\epsilon_j = X_j
- X_{\setminus j} \theta_j$,
\[
    \|\epsilon_{ij}\|_{\psi_2} \leq \|X_{ij}\|_{\psi_2} + \sum_{k\neq j} \|X_{ik} \theta_{jk}^\star\|_{\psi_2} \lesssim (1 + \|\theta_j^\star\|_1) K \leq (1 + \kappa \|\Omega^\star\|_{\ell_1}) K
\]
by the triangle inequality of the sub-Gaussian norm. So, due to
Assumption~\ref{assumption:bounded-eigenvalues}, for all $i$ and $j$,
$\epsilon_{ij}$ is sub-Gaussian with norm bounded by $(1 + \kappa L) K$, and
thus for all $k\neq j$, $X_{ik} \epsilon_{ij}$ is \emph{sub-exponential},
meaning there exists a constant $C$ such that for every $t>0$,
\[
    \prob\left(\frac{1}{n}\left|\sum_{i=1}^{n} X_{ik} \epsilon_{ij}\right| \geq t\right) \leq 2 \exp(-t/C).
\]
The \emph{sub-exponential norm} of a random variable $Y$ is defined as
\[
    \|Y\|_{\psi_1} = \inf\left\{t > 0 \colon \E(\exp(Y/t)) \leq 2\right\}.
\]
Then, by \citep[Lemma~2.7.7]{Vershynin18}, $\|X_{ik} \epsilon_{ij}\|_{\psi_1}
\leq \|X_{ik}\|_{\psi_2} \|\epsilon_{ij}\|_{\psi_2} \leq K'$, where $K' = (1 +
\kappa L) K^2$. By Bernstein's inequality \citep[Corollary~2.8.3]{Vershynin18},
\[
    \prob\left(\frac{2}{n}|X_k^\top \epsilon_j| \geq \lambda \right) \leq 2 \exp\left(-c_0\left(\frac{\lambda}{K'}\right)^2 n \right),
\]
whenever $\log (p) /n$ is sufficiently small, and where $c_0$ is a universal
constant. Hence,
\[
    \begin{aligned}
        \prob\left(\lambda \leq \max_j \frac{2}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty\right) &\leq \sum_{j=1}^{p} \prob\left(\frac{2}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \geq \lambda\right) \\
        &\leq \sum_{j=1}^{p} \sum_{k\neq j} \prob\left(\frac{2}{n}|X_k^\top \epsilon_j| \geq \lambda\right) \\
        &\leq 2p(p-1) \exp\left(-c_0\left(\frac{\lambda}{K'}\right)^2 n \right) \\
        &= 2 \exp\left(-c_0\left(\frac{\lambda}{K'}\right)^2 n + \log p + \log(p-1)\right).
    \end{aligned}
\]
Then, choosing $\lambda = c\sqrt{\log(p) /n}$ with $c>0$ sufficiently large
ensures that $\lambda \geq (2/n)\|X_{\setminus j}^\top \epsilon_j\|_\infty$
holds for all $j$ with probability at least $1 - 2/p$. So the inequalities
\eqref{eq:lasso-error-1} and \eqref{eq:lasso-error-2} hold with same probability
for this choice of $\lambda$. Turning to the residual variances, we have
\[
    \begin{aligned}
        \hat \tau_j^2 - \tau_j^2 &= \frac{1}{n} \|X_{\setminus j}(\hat \theta_j - \theta_j)\|_2^2 + \frac{2}{n}\braket{\epsilon_j, X_{\setminus j}(\hat \theta_j - \theta_j)} + \left(\frac{1}{n}\|\epsilon_j\|_2^2 - \tau_j^2\right) \\
        &\leq \frac{9}{\gamma_\ell} s_j \lambda^2 + \frac{2}{n}\braket{\epsilon_j, X_{\setminus j}(\hat \theta_j - \theta_j)} + \left(\frac{1}{n}\|\epsilon_j\|_2^2 - \tau_j^2\right). 
    \end{aligned}
\]
Then, noting that 
\[
    \frac{2}{n} |\braket{\epsilon_j, X_{\setminus j}(\hat \theta_j - \theta_j)}| \leq \frac{2}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \|\hat \theta_j - \theta_j\|_1 \leq \frac{12}{\gamma_\ell} s_j \lambda^2, 
\]
where the first inequality is due to Hölder and the second is due to
\eqref{eq:lasso-error-1}, we get the bound
\begin{equation}\label{eq:var-bound}
    |\hat \tau_j^2 - \tau_j^2| \leq \frac{21}{\gamma_\ell} s_j \lambda^2 + \left|\frac{1}{n}\|\epsilon_j\|_2^2 - \tau_j^2\right| = c^2\frac{21}{\gamma_\ell} \frac{s_j \log p}{n} + \left|\frac{1}{n}\|\epsilon_j\|_2^2 - \tau_j^2\right|.    
\end{equation}
The random variable $(1/n)\|\epsilon_j\|_2^2$ is sub-exponential
with mean
$\tau_j^2$ and $\|\epsilon_{ij}\|_{\psi_1} \leq (1 + \kappa L)^2 K^2$, so
another application of Bernstein's inequality yields
\[
    \prob\left(\left|\frac{1}{n}\|\epsilon_j\|_2^2 - \tau_j^2\right| \geq \frac{(1 + \kappa L)^2 K^2}{\sqrt{c_0 / 2}} \sqrt{\frac{\log p}{n}}\right) \leq \sum_{j=1}^{p} 2\exp\left(- 2 \log p\right),
\]
and so,
\[
    \begin{aligned}
        \prob\left(\left|\frac{1}{n}\|\epsilon_j\|_2^2 - \tau_j^2\right| \geq \frac{(1 + \kappa L)^2 K^2}{\sqrt{c_0 / 2}} \sqrt{\frac{\log p}{n}} \text{ for some } j \right) &\leq \sum_{j=1}^{p} 2\exp\left(- 2 \log p\right) \\
        &= 2p \exp(-2 \log p) = 2/p.
    \end{aligned}
\]
Due to Assumption~\ref{assumption:dimensionality}, $s_j\log(p) / n \leq M
\sqrt{\log (p)/n}$, so we have that $|\hat \tau_j^2 - \tau_j^2| \lesssim
\sqrt{\log(p) / n}$ for all $j$ with probability at least $1 - 4/p$, proving
\eqref{eq:var-estimation-error}. Due to this bound, $\hat\tau_j^2 = \tau_j^2 +
O(\sqrt{\log(p)/n})$, so since $\lambda_\mathrm{min}(\Omega^\star) \leq 1 /
\tau_j^2 \leq \lambda_\mathrm{max}(\Omega^\star)$, there exists
$\tau_\ell,\tau_u \in (0, \infty)$ such that $\tau_\ell \leq \hat \tau_j \leq
\tau_u$ for all $j\in \{1, \dots, p\}$, whenever $\log(p)/n$ is small, proving
\eqref{eq:var-estimates-bound}.

Since $\lambda = c\sqrt{\log(p) / n} \geq (2/n) \|X_{\setminus j}^\top
\epsilon_j\|_\infty$ for all $j$, we have that the regularization parameter
satisfies
\[
    \lambda \geq \max_j \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty + \gamma_u \kappa |\hat \tau_j^2 - \tau_j^2|\right)
\]
with the same probability, if $c$ is sufficiently large. Likewise, since
\[
    \sqrt{\frac{\log p}{n}} \gtrsim |\hat \tau_j^2 - \tau_j^2| = |\hat \tau_j + \tau_j| |\hat \tau_j - \tau_j| \geq (\tau_u + \tau_j) |\hat \tau_j - \tau_j|,
\]
it holds that
\[
    \begin{aligned}
        \max_k \left|\frac{\hat \tau_j}{\hat \tau_k} - \frac{\tau_j}{\tau_k}\right| &= \max_k \left|\frac{\hat \tau_j \tau_k - \tau_j \hat \tau_k}{\hat \tau_k \tau_k}\right| \\
        &= \max_k \left|\frac{\left(\tau_j + O(\sqrt{\log(p)/n})\right) \tau_k - \tau_j \left(\tau_k + O(\sqrt{\log(p)/n})\right)}{\hat \tau_k \tau_k}\right| \\
        &\lesssim \sqrt{\frac{\log p}{n}}.
    \end{aligned}
\]
Thus, if $c$ is sufficiently large, then the inequality
\[
    \lambda \geq \max_j \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty + \kappa \frac{\gamma_u}{\tau_\ell} \max_k \left|\frac{\hat \tau_j}{\hat \tau_k} - \frac{\tau_j}{\tau_k}\right| \right)
\]
also holds with the same probability.
\end{proof}

\begin{lemma}\label{lemma:smallest-eigenvalue}
For any vector $\nu \in \reals^p$, the data matrix satisfies
\[
    \lambda_\mathrm{min}(\Sigma)\left(1 - \sqrt{\frac{p + 2 \log p}{n}}\right)^2 \|\nu\|_2^2 \leq \frac{1}{n} \|X \nu\|_2^2 \leq \lambda_\mathrm{max}(\Sigma)\left(1 + \sqrt{\frac{p + 2 \log p}{n}}\right)^2 \|\nu\|_2^2,
\]
with probability at least $1 - 2/p$.
\end{lemma}

\begin{proof}
Let $\Sigma = Q \Lambda Q^\top$ be the eigenvalue decomposition of the
covariance matrix. Then $X = Q \Lambda^{1/2} A$ where $A$ is a matrix of $n$
i.i.d.\ rows of mean-zero, sub-Gaussian and isotropic vectors in $\reals^p$.
Hence, $X^\top X = \Lambda^{1/2} Y^\top Y \Lambda^{1/2}$. Without loss of
generality, take any unit vector $\nu \in \reals^p$. Then 
\[
    \begin{aligned}
        \frac{1}{n}\|X \nu\|_2^2  &= \frac{1}{n} \|Q \Lambda^{1/2}Y \nu\|_2^2 = \frac{1}{n} \|\Lambda^{1/2} Y \nu\|_2^2 \\ 
        &\geq \lambda_\mathrm{min}(\Lambda) \lambda_\mathrm{min}((1/n) Y^\top Y) = \lambda_\mathrm{min}(\Sigma) \lambda_\mathrm{min}((1/n) Y^\top Y).
    \end{aligned}
\]
The upper bound
\[
    \frac{1}{n}\|X \nu\|_2^2 \leq \lambda_\mathrm{max}(\Sigma)\lambda_\mathrm{max}((1/n) Y^\top Y)
\]
follows similarly. A straightforward application of
\cite[Corollary~5.35]{Vershynin11} then gives the result.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:est-rate}}
\begin{proof}
Note that under Assumption~\ref{assumption:dimensionality},
Lemma~\ref{lemma:smallest-eigenvalue} tells us that there exists $\gamma_\ell,
\gamma_u \in (0,\infty)$ such that $\gamma_\ell \|\nu\|_2^2 \leq (1/n)\|X
\nu\|_2^2 \leq \gamma_u \|\nu\|_2^2$ for all $\nu\in\reals^p$ with probability
$1 - 2/p$. Thus, by Lemma~\ref{lemma:reg-param}, inequalities
\eqref{eq:var-estimation-error} and \eqref{eq:reg-param-ineq} hold with
probability at least $1 - 6/p$. Throughout, assume that this event occurs.

Define the matrix $\Omega'$ by 
\begin{equation}
        \Omega_{jj}' = 1 / \hat \tau_j^2, \quad \Omega_{jk}' = \Omega_{jk}^\star, \quad j \neq k.
\end{equation}
Then we have that 
\begin{equation}
    \begin{aligned}
        \lambda_\text{min}(\Omega') &= \lambda_\text{min}(\Omega - \diag(\Omega - \widehat{\Omega})) \\
                                &\geq \lambda_\text{min}(\Omega) - \max_{j=1, \dots, p}|1 / \hat \tau_j^2 - 1 / \tau_j^2|.
    \end{aligned}
\end{equation}
By Assumption~\ref{assumption:bounded-eigenvalues} and
Lemma~\ref{lemma:reg-param}, $\lambda_\text{min}(\Omega') \geq 0$ whenever
$\log(p)/n$ is sufficiently small, meaning $\Omega'$ is feasible in the joint
partial regression problem \eqref{eq:cvx-prob}. To alleviate the notation, let
$\omega^\star_j = \Omega_{\setminus j, j}^\star$, and set $G$ as the objective
function of \eqref{eq:cvx-prob} except with shifted variables, \ie, 
\[
    G(\nu_1, \dots, \nu_p) = \sum_{j=1}^p \left(\frac{1}{2n} \|X_j + \hat\tau_j^2 X_{\setminus j}(\omega^\star_j + \nu_j) \|_2^2 + \lambda \hat\tau^2_j \|\omega^\star_j + \nu_j\|_1\right).
\]
By construction, we have the inequality $G(\hat \nu_1, \dots, \hat \nu_p) \leq
G(0, \dots, 0)$, where $\hat \nu_j = \widehat{\Omega} _{\setminus j, j} -
\omega^\star_j$. Recall that 
\[
    X_j = -\tau_j^2 X_{\setminus j} \omega^\star_j + \epsilon_j, \quad \epsilon_j \sim \mathcal{N}(0, \tau_j^2 I_n),
\]
Expanding some squares, we have that
\begin{equation}
    \begin{aligned}
        G(0, \dots, 0) &= \sum_{j=1}^{p} \left(\frac{1}{2n} \|X_j + \hat \tau_j^2 X_{\setminus j} \omega^\star_j\|_2^2 + \lambda \hat\tau_j^2 \|\omega^\star_j\|_1\right) \\
                        &= \sum_{j=1}^{p} \left(\frac{1}{2n} \|\epsilon_j + (\hat \tau_j^2 - \tau_j^2) X_{\setminus j} \omega^\star_j\|_2^2 + \lambda \hat\tau_j^2 \|\omega^\star_j\|_1\right)                 
    \end{aligned}
\end{equation}
and, 
\begin{equation*}
    \begin{aligned}
        G(\hat \nu_1, \dots, \hat \nu_p) &= \sum_{j=1}^{p} \left(\frac{1}{2n}\|X_j + \hat \tau_j^2 X_{\setminus j}(\omega^\star_j + \hat \nu_j)\|_2^2 + \lambda \hat\tau_j^2 \|\omega^\star_j + \hat \nu_j\|_1\right) \\
            &= \sum_{j=1}^{p} \left(\frac{1}{2n}\|\epsilon_j + (\hat \tau_j^2 - \tau_j^2) X_{\setminus j} \omega^\star_j + \hat \tau_j^2 X_{\setminus j}\hat \nu_j\|_2^2 + \lambda \hat\tau_j^2 \|\omega^\star_j + \hat \nu_j\|_1\right) \\
            &= \sum_{j=1}^{p} \bigg(\frac{1}{2n}\|\epsilon_j + (\hat \tau_j^2 - \tau_j^2) X_{\setminus j} \omega^\star_j\|_2^2 + \frac{\hat \tau_j^2}{n}\braket{\epsilon_j + (\hat \tau_j^2 - \tau_j^2) X_{\setminus j}\omega^\star_j, X_{\setminus j}\hat \nu_j} \\
            &+ \frac{1}{2n}\|\hat \tau_j^2 X_{\setminus j}\hat \nu_j\|_2^2 + \lambda \hat\tau_j^2 \|\omega^\star_j + \hat \nu_j\|_1\bigg)
    \end{aligned}
\end{equation*}
Rearranging some terms and using $(1/n)\|X \nu\|_2^2 \geq \gamma_\ell
\|\nu\|_2^2$, we obtain the inequality
\begin{equation*}
        \sum_{j=1}^{p} \frac{\gamma_\ell \hat \tau_j^2}{2} \|\hat \nu_j\|_2^2 \leq \sum_{j=1}^{p} \left(\lambda \hat\tau_j^2 (\|\omega^\star_j\|_1 - \|\omega^\star_j + \hat \nu_j\|_1) - \frac{\hat \tau_j^2}{n}\braket{\epsilon_j + (\hat \tau_j^2 - \tau_j^2) X_{\setminus j}\omega^\star_j, X_{\setminus j}\hat \nu_j} \right).
\end{equation*}
Using Hölder's inequality and $\lambda_\text{max}((1/n) X^\top X) \leq
\gamma_u$ and $\lambda_\text{min}(\Sigma) \geq 1/\kappa$, we have that
\[
    \begin{aligned}
        \left|\frac{1}{n}\braket{\epsilon_j + (\hat \tau_j^2 - \tau_j^2) X_{\setminus j}\omega^\star_j, X_{\setminus j}\hat \nu_j}\right| &= \left|\frac{1}{n}\braket{\epsilon_j, X_{\setminus j} \hat \nu_j} + (\hat \tau_j^2 - \tau_j^2)\frac{1}{n}\braket{X_{\setminus j}\omega^\star_j, X_{\setminus j}\hat \nu_j} \right|\\
            &\leq \frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \|\hat \nu_j\|_1 + |\hat \tau_j^2 - \tau_j^2| \frac{1}{n} \|X^\top X \omega^\star_j\|_2 \|\hat \nu_j\|_2 \\ 
            &\leq \frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \|\hat \nu_j\|_1 + \gamma_u \kappa |\hat \tau_j^2 - \tau_j^2|  \|\hat \nu_j\|_1 \\
            &= \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty + \gamma_u \kappa|\hat \tau_j^2 - \tau_j^2| \right) \|\hat \nu_j\|_1. 
    \end{aligned}
\]

Let $S_j$ be the indices of the true non-zero elements of $\omega^\star_j$, and
$S_j^c$ be the indices of the zero elements. Then we have
\begin{equation}\label{eq:l1-ineq}
    \|\omega^\star_j + \hat \nu_j\|_1 = \|(\omega^\star_j + \hat \nu_j)_{S_j}\|_1 + \|(\hat \nu_j)_{S_j^c}\|_1 \geq \|\omega^\star_j\|_1 - \|(\hat \nu_j)_{S_j}\|_1  
\end{equation}
where we subtracted $\|(\hat \nu_j)_{S_j^c}\|_1$ and used the reverse triangle
inequality. Using \eqref{eq:l1-ineq} and \eqref{eq:reg-param-ineq}, we have that
\begin{equation}
    \sum_{j=1}^{p} \frac{\gamma_\ell}{2} \|\hat \tau_j^2 \hat \nu_j\|_2^2 \leq \sum_{j=1}^{p}\left(\frac{3}{2} \lambda \hat\tau_j^2 \|(\hat \nu_j)_{S_j}\|_1\right) 
\end{equation}
Thus, we have that 
\begin{equation}
    \left(\frac{\gamma_\ell}{2} \min_j \hat \tau_j^4\right) \|\widehat{\Omega} - \Omega'\|_{\rm F}^2 
   \leq \left(\frac{3}{2} \max_{j} \hat \tau_j^2\right) \lambda \|(\widehat{\Omega} - \Omega')_S\|_1 \leq \left(\frac{3}{2} \max_{j} \hat \tau_j^2\right) \lambda \sqrt{s} \|\widehat{\Omega} - \Omega'\|_{\rm F},
\end{equation}
where $S$ are the indices of the true non-zero entries. By some rearranging,
this gives the bound
\begin{equation}\label{eq:off-diag-bound}
    \|\widehat{\Omega} - \Omega'\|_{\rm F} \leq \left(\frac{3}{\gamma_\ell} \frac{\max_j \hat\tau^2_j}{\min_j \hat\tau^4_j}\right) \sqrt{s} \lambda.    
\end{equation}
By the inequality \eqref{eq:var-estimation-error}, $\hat\tau_j^2 = \tau_j^2 +
O(\sqrt{\log(p)/n})$, so since $\lambda_\mathrm{min}(\Omega^\star) \leq 1 /
\tau_j^2 \leq \lambda_\mathrm{max}(\Omega^\star)$, there exists
$\tau_\ell,\tau_u \in (0, \infty)$ such that $\tau_\ell \leq \hat \tau_j \leq
\tau_u$ for all $j\in \{1, \dots, p\}$, whenever $\log(p)/n$ is small. Hence,
the term $\max_j \hat\tau^2_j / \min_j \hat\tau^4_j$ is bounded, so
\eqref{eq:off-diag-bound} implies that $\|\widehat{\Omega} - \Omega'\|_{\rm F}
\lesssim \sqrt{s\log(p)/n}$. Moreover,
\begin{equation}\label{eq:reci-var-bound}
    |1/\hat \tau_j^2 - 1/\tau_j^2| \leq \frac{1}{\hat \tau_j^2 \tau_j^2} |\hat \tau_j^2 - \tau_j^2| \leq \frac{\lambda_\mathrm{max}(\Omega^\star)}{\tau_\ell^2} |\hat \tau_j^2 - \tau_j^2| \lesssim \sqrt{\frac{\log p}{n}},
\end{equation}
Hence, 
\begin{equation*}
    \|\widehat{\Omega} - \Omega^\star\|_{\rm F}^2 = \|\widehat{\Omega} - \Omega'\|_{\rm F}^2 + \sum_{j=1}^{p} (1 / \hat \tau_j^2 - 1 / \tau_j^2)^2 \lesssim \frac{(s + p)\log p}{n},
\end{equation*}
which is the stated result for the precision matrix. 

Turning to the partial correlation matrix, we define the function $H$ by
\[
    H(\nu_1, \dots, \nu_p) = \sum_{j=1}^{p} \left(\frac{1}{2n} \|X_j - X_{\setminus j} \widehat{R}_j (q^\star_j + \nu_j)\|_2^2 + \lambda \|\widehat{R}_j(q^\star_j + \nu_j)\|_1\right),
\]
where $q_j^\star = Q_{\setminus j, j}^\star$ and $\widehat{R}_j = \diag(\hat
\tau_j / \hat \tau_1, \dots, \hat \tau_j / \hat \tau_{j-1}, \hat \tau_j / \hat
\tau_{j+1}, \dots, \hat \tau_j / \hat \tau_p)$. Like before, we have that
\begin{equation}
    \begin{aligned}
        H(0, \dots, 0) &= \sum_{j=1}^{p} \left(\frac{1}{2n} \|X_j - X_{\setminus j} \widehat{R}_j q^\star_j\|_2^2 + \lambda \hat\tau_j^2 \|\widehat{R}_j q^\star_j\|_1\right) \\
                        &= \sum_{j=1}^{p} \left(\frac{1}{2n} \|\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j\|_2^2 + \lambda \|\widehat{R}_j q^\star_j\|_1\right)                 
    \end{aligned}
\end{equation}
where $R_j = \diag(\tau_j / \tau_1, \dots, \tau_j / \tau_{j-1}, \tau_j /
\tau_{j+1}, \dots \tau_j / \tau_p)$. Likewise, 
\begin{equation*}
    \begin{aligned}
        H(\tilde \nu_1, \dots, \tilde \nu_p) &= \sum_{j=1}^{p} \left(\frac{1}{2n}\|X_j - \hat X_{\setminus j} \widehat{R}_j (q^\star_j + \tilde \nu_j)\|_2^2 + \lambda \|\widehat{R}_j(q^\star_j + \tilde \nu_j)\|_1\right) \\
            &= \sum_{j=1}^{p} \left(\frac{1}{2n}\|\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j - X_{\setminus j}\widehat{R}_j \tilde \nu_j\|_2^2 + \lambda \|\widehat{R}_j(q^\star_j + \tilde \nu_j)\|_1\right) \\
            &= \sum_{j=1}^{p} \bigg(\frac{1}{2n}\|\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j\|_2^2 - \frac{1}{n}\braket{\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j, X_{\setminus j} \widehat{R}_j \hat\nu_j} \\
            &\quad + \frac{1}{2n}\| X_{\setminus j} \widehat{R}_j \tilde \nu_j\|_2^2 + \lambda \|\widehat{R}_j(q^\star_j + \tilde \nu_j)\|_1\bigg)
    \end{aligned}
\end{equation*}
The true partial correlation matrix $Q^\star$ is always feasible in the joint
partial regression problem, so for $\tilde \nu_j = \widehat{Q}_{\setminus j, j}
- q_j^\star$, we have that $H(\tilde \nu_1, \dots, \tilde \nu_p) \leq H(0,
\dots, 0)$. By rearranging terms, we have that
\[
    \begin{aligned}
        \sum_{j=1}^{p} \frac{\gamma_\ell}{2} \|\widehat{R}_j \tilde \nu_j\|_2^2 \leq \sum_{j=1}^{p} \Big(&\lambda (\|\widehat{R}_j  q^\star_j\|_1 - \|\widehat{R}_j (q^\star_j + \tilde \nu_j)\|_1) \\
        &+ \frac{1}{n}\braket{\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j, X_{\setminus j} \widehat{R}_j\tilde \nu_j} \Big).        
    \end{aligned}
\]
Due to $\widehat{R}_j$ being diagonal, it holds that
$\lambda_\mathrm{min}(\widehat{R}_j)= \min_{k\neq j} \hat \tau_j / \hat \tau_k
\geq \tau_\ell / \tau_u$ and $\|\widehat{R}_j\|_{\ell_1} =
\lambda_\mathrm{max}(\widehat{R}_j)= \max_{k\neq j} \hat \tau_j / \hat \tau_k
\leq \tau_u / \tau_\ell$. Thus, we get the inequality
\begin{equation}\label{eq:partial-corr-ineq}
    \begin{aligned}
        \frac{\gamma_\ell (\tau_\ell/ \tau_u)^2}{2} \sum_{j=1}^{p} \|\tilde \nu_j\|_2^2 \leq \sum_{j=1}^{p} \Big(& \lambda (\tau_u / \tau_\ell) (\|q^\star_j\|_1 - \|q^\star_j + \tilde \nu_j\|_1) \\
        & + \frac{1}{n}\braket{\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j, X_{\setminus j} \widehat{R}_j\tilde \nu_j} \Big).        
    \end{aligned}
\end{equation}
Once again using Hölder's inequality, we have that the inner product terms satisfy
\[
    \begin{aligned}
        \left|\frac{1}{n}\braket{\epsilon_j - X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j, X_{\setminus j} \widehat{R}_j\tilde \nu_j}\right| &= \left|\frac{1}{n}\braket{\epsilon_j, X_{\setminus j} \widehat{R}_j \tilde \nu_j} - \frac{1}{n}\braket{X_{\setminus j} (\widehat{R}_j - R_j) q^\star_j, X_{\setminus j} \widehat{R}_j \tilde \nu_j} \right|\\
            &\leq \frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \|\widehat{R}_j \tilde \nu_j\|_1 + \frac{1}{n} \|X^\top X (\widehat{R}_j - R_j) q^\star_j\|_2 \|\widehat{R}_j \tilde \nu_j\|_2 \\ 
            &\leq \frac{\tau_u}{\tau_\ell}\left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \|\tilde \nu_j\|_1 + \gamma_u \lambda_\mathrm{max}(\widehat{R}_j - R_j) \|q^\star_j\|_2 \|\tilde \nu_j\|_2 \right) \\ 
            &\leq \frac{\tau_u}{\tau_\ell} \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty \|\tilde \nu_j\|_1 + \kappa \frac{\gamma_u}{\tau_\ell} \max_k \left|\frac{\hat \tau_j}{\hat \tau_k} - \frac{\tau_j}{\tau_k}\right| \|\tilde \nu_j\|_1 \right) \\
            &= \frac{\tau_u}{\tau_\ell} \left(\frac{1}{n}\|X_{\setminus j}^\top \epsilon_j\|_\infty + \kappa \frac{\gamma_u}{\tau_\ell} \max_k \left|\frac{\hat \tau_j}{\hat \tau_k} - \frac{\tau_j}{\tau_k}\right| \right) \|\tilde \nu_j\|_1. 
    \end{aligned}
\]
Thus, using \eqref{eq:reg-param-ineq}, the inequality
\eqref{eq:partial-corr-ineq} implies that
\[
    \sum_{j=1}^{p} \frac{\gamma_\ell}{2} \left(\frac{\tau_\ell}{\tau_u}\right)^2 \|\tilde \nu_j\|_2^2 \leq \frac{\tau_u}{\tau_\ell} \sum_{j=1}^{p} \lambda (\| q^\star_j\|_1 - \|q^\star_j + \tilde \nu_j\|_1 + \|\tilde \nu_j\|_1)
\]
Adapting the inequality \eqref{eq:l1-ineq}, we have that
\[
    \sum_{j=1}^{p} \frac{\gamma_\ell}{2} \left(\frac{\tau_\ell}{\tau_u}\right)^2 \|\tilde \nu_j\|_2^2 \leq 2\frac{\tau_u}{\tau_\ell} \sum_{j=1}^{p} \lambda \|(\tilde \nu_j)_{S_j}\|_1.
\]
By rearranging terms and using the fact that $\sum_j \|\tilde \nu_j\|_2^2 =
\|\widehat{Q} - Q^\star\|_{\rm F}^2$ and $\sum_j \|\tilde \nu_j\|_1 = \|\widehat{Q} -
Q^\star\|_1$, we get that 
\[
    \|\widehat{Q} - Q^\star\|_{\rm F}^2 \leq \frac{4}{\gamma_\ell} \left(\frac{\tau_u}{\tau_\ell}\right)^3 \lambda \|(\widehat{Q} - Q^\star)_S\|_1 \leq \frac{4}{\gamma_\ell} \frac{\tau_\ell}{\tau_u} \lambda \sqrt{s} \|\widehat{Q} - Q^\star\|_{\rm F}.  
\]
Inserting $\lambda = c\sqrt{\log(p)/n}$ and dividing by the Frobenius error, we
arrive at the bound
\[
    \|\widehat{Q} - Q^\star\|_{\rm F} \leq \frac{4c}{\gamma_\ell} \left(\frac{\tau_u}{\tau_\ell}\right)^3 \sqrt{\frac{s\log p}{n}},
\]
completing the proof. \end{proof}

\end{document}
