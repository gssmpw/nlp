%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{paralist}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Use the following line for the initial blind version submitted for review:


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

\usepackage[accepted]{icml2025}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{float}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{icml2025}

\makeatletter
\renewcommand{\ICML@appearing}{}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{constraint}[theorem]{Constraint}
\newtheorem{Problem}[theorem]{Problem}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\begin{document}
\twocolumn[
\icmltitle{Unifying and Optimizing Data Values for Selection via Sequential-Decision-Making}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hongliang Chi}{rpi}
\icmlauthor{Qiong Wu}{att}
\icmlauthor{Zhengyi Zhou}{att}
\icmlauthor{Jonathan Light}{rpi}
\icmlauthor{Emily Dodwell}{att}
\icmlauthor{Yao Ma}{rpi}
\end{icmlauthorlist}

\icmlaffiliation{rpi}{Rensselaer Polytechnic Institute, Troy, NY, United States}
\icmlaffiliation{att}{AT\&T-Chief Data Office, Bedminster, NJ, United States}

\icmlcorrespondingauthor{Hongliang Chi}{chih3@rpi.edu}
\icmlcorrespondingauthor{Qiong Wu}{qw6547@att.com}
\icmlcorrespondingauthor{Zhengyi Zhou}{zz547k@att.com}
\icmlcorrespondingauthor{Jonathan Light}{lij54@rpi.edu}
\icmlcorrespondingauthor{Emily Dodwell}{ed720d@att.com}
\icmlcorrespondingauthor{Yao Ma}{may13@rpi.edu}

\icmlkeywords{Data Valuation, Approximate Dynamic Programming, Graph Structure Learning}
\vskip 0.3in
]
\printAffiliationsAndNotice{}

\begin{abstract}
Data selection has emerged as a crucial downstream application of data valuation. While existing data valuation methods have shown promise in selection tasks, the theoretical foundations and full potential of using data values for selection remain largely unexplored. In this work, we first demonstrate that data values applied for selection can be naturally reformulated as a sequential-decision-making problem, where the optimal data value can be derived through dynamic programming. We show this framework unifies and reinterprets existing methods like Data Shapley through the lens of approximate dynamic programming, specifically as myopic reward function approximations to this sequential problem. Furthermore, we analyze how sequential data selection optimality is affected when the ground-truth utility function exhibits monotonic submodularity with curvature. To address the computational challenges in obtaining optimal data values, we propose an efficient approximation scheme using learned bipartite graphs as surrogate utility models, ensuring greedy selection is still optimal when the surrogate utility is correctly specified and learned. Extensive experiments demonstrate the effectiveness of our approach across diverse datasets. 
\end{abstract}


\section{Introduction}\label{sec:introduction}
Data plays a fundamental role in modern machine learning, with recent advances in deep learning heavily dependent on massive datasets \citep{schmidhuber2015deep, lecun2015deep, hatcher2018survey}. However, not all data contributes equally to model performance, leading to the development of data valuation methods that quantify the contributions of individual training samples.  The dominant approaches to data valuation are grounded in cooperative game theory \citep{shapley1953value, banzhaf1964weighted, schmeidler1969nucleolus}. In this formulation, training samples are treated as players in a cooperative game, where the utility function measures the validation performance of models trained on different data subsets. Data values are then derived by aggregating each sample's marginal contributions across various subset combinations, leading to principled valuation methods such as Data Shapley \citep{ghorbani2019data}, Data Banzhaf \citep{wang2023data}, and Beta Shapley \citep{kwon2021beta}.

These valuation methods provide essential guidance for crucial data-centric tasks like data selection, where the goal is to identify optimal subsets of training data that maximize model performance \citep{ghorbani2019data, schoch2023data, wang2024rethinking}. The standard protocol of existing work \citep{ghorbani2019data, kwon2021beta, tarun2024ecoval} evaluates data values through selection curves: data points are ranked by their assigned values, and model performance is measured as more points are incrementally added in descending order of their values. As shown in \cref{fig:selection}, the effectiveness is evaluated by comparing performance between different methods across different selection sizes (vertical dashed lines), where superior data values should demonstrate both immediate efficiency through steeper initial curves and sustained effectiveness via consistently higher performance across all selection budgets (e.g., \textit{Data Values I} in \cref{fig:selection}), indicating their ability to identify valuable training points early while maintaining their advantage as the dataset grows. However, existing data valuation methods were not explicitly designed to optimize for this selection objective, which accounts for performance across all possible budgets. Despite the clear importance of achieving both efficient initial selection and sustained performance improvements, current methods overlook this fundamental requirement.


\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{curve.pdf}
    \caption{Demonstration of data selection performance curves with different data values. The x-axis shows the selection size (from 0 to dataset size $n$), and y-axis represents test accuracy. Vertical dashed lines indicate different selection budgets. Superior data values (Data Values I) achieve both steeper initial curves and consistently higher performance across all budgets compared to Data Values II.}
    \label{fig:selection}
\end{figure}

In this work, we directly analyze data values from a selection perspective. While existing evaluation protocols implicitly assume that good data values should perform well across different selection sizes, this crucial objective has never been explicitly formalized or optimized. We address this gap by first establishing a clear optimization objective that captures the selection performance of data values across all possible subset sizes. Our key insight is that this optimization problem can be naturally reformulated as a sequential decision-making process, where each selection step builds upon previous choices, and the cumulative rewards directly correspond to selection performance across different subset sizes. We show that this sequential perspective not only provides theoretical clarity but also leads to a practical solution through dynamic programming \citep{bellman1966dynamic}, enabling us to compute optimal data values specifically designed for selection tasks. 
Under this sequential-decision-making perspective, we reveal that existing data valuation methods like Data Shapley \citep{ghorbani2019data} and other semi-values \citep{wang2023data, kwon2021beta} can be understood as specific instances of \textit{myopic cost function approximation} in approximate dynamic programming \cite{powell2007approximate, bertsekas2024course}. 

Our theoretical analysis demonstrates that the effectiveness of such myopic approximations is fundamentally determined by the structural properties of the underlying utility function, particularly its submodularity and curvature, which characterize the degree of diminishing returns and data dependencies in the selection process. This theoretical insight aligns with recent empirical observations \cite{wang2024rethinking} that Data Shapley performs particularly well on heterogeneous datasets, where data points tend to have more independent contributions and thus exhibit lower curvature in the utility function.


While our framework provides a path to optimal data values, its exact computation requires evaluating utilities for all possible subsets, becoming intractable for large datasets. Existing Shapley-based approximation approaches rely on simplified linear utility models and cannot adequately capture complex data dependencies in practical selection tasks. To bridge this gap between theoretical optimality and practical efficiency, we develop a novel approximation scheme based on bipartite coverage models that preserves essential theoretical properties while enabling practical computation of near-optimal values.


Specifically, our contributions are listed as follows:

\begin{compactenum}[\textbullet]
 \item We establish the first systematic framework that unifies data values for sequential data. 

\item We provide new theoretical insights by analyzing existing game-theoretic data valuation methods as myopic approximations under our framework. 

\item We develop an efficient approximation scheme using bipartite surrogate utility models that preserve theoretical guarantees while enabling practical computation. 

\item Through comprehensive experiments, we identify significant performance gaps in existing data valuation methods compared to optimal selection strategies, and demonstrate that our approximation scheme substantially closes this gap while maintaining computational efficiency.
\end{compactenum}


\section{Background and Preliminaries}\label{sec:preliminary}
\subsection{Data Values and Data Selection}
\begin{definition}[Score-based Data Values]
Given a dataset $\mathcal{D} = \{x_1,...,x_n\}$ and a utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}$ that measures the performance of any subset, the goal of data valuation is to learn a value-assignment function $v: \mathcal{D} \rightarrow \mathbb{R}$ that assigns scores to individual data points based on their contribution to the overall utility. 
\end{definition}

The predominant data values leverage solutions from the cooperative game theory, known as \textit{Game-theoretic Data Values}. For a broader review of data valuation methods, we refer readers to Section \ref{sec:related_dv}.

\begin{definition}[Game-theoretic Data Values] \label{def:game_values}
Given a dataset $\mathcal{D}$ and a utility function $U$ that measures model performance on a held-out validation set when trained on different subsets, game-theoretic data values are derived by treating data points as players in a cooperative game, where for each point $x_i$, its value $v(i)$ is computed as a weighted combination of its marginal contributions across different subsets:
\[
v(i) = \sum_{S \subseteq \mathcal{D}\setminus\{i\}} w(S)[U(S \cup \{i\}) - U(S)]
\]
where $w(S)$ represents the weight assigned to subset $S$. Notably, when the weights depend only on subset size and satisfy $\sum_{k=1}^n w(k) = 1$, this formulation yields semi-values \cite{kwon2021beta}, with Data Shapley being a special case where $w(S) = \frac{|S|!(n-|S|-1)!}{n!}$. Another commonly used but non-game-theoretic data value that also fits this definition is the Leave-one-out (LOO) value $v_{LOO}(i) = U(\mathcal{D}) - U(\mathcal{D}\setminus\{i\})$, which measures each point's marginal contribution to the full dataset.
\end{definition}

Data values provide principled approaches for quantifying the importance of training samples, with data selection emerging as a crucial downstream application. The goal of data selection is to identify an optimal subset of training data that maximizes model performance. Recent work \cite{wang2024rethinking} has formulated data selection with a fixed size constraint $S^*_k = \text{argmax}_{S \subseteq \mathcal{D}, |S|=k} U(S)$, where a subset $S$ of size $k$ is selected to maximize the utility function $U$. The standard protocol for data selection using data values is:

\begin{definition}[Data Values for Data Selection]\label{def:selection}
Given a dataset $\mathcal{D}$ and a utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}$, a value function $v: \mathcal{D} \rightarrow \mathbb{R}$ induces a selection strategy through ranking. Let $\pi_v$ denote the permutation that sorts samples in descending order of their values, i.e., $v(\pi_v(1)) \geq v(\pi_v(2)) \geq ... \geq v(\pi_v(n))$. The value-based selection strategy is defined as:
\begin{equation*} \label{eq:selection}
S_k = \{\pi_v(1), ..., \pi_v(k)\}
\end{equation*}
\end{definition}
A key insight of selection via data values is that only the relative ordering of values affects the selection sequence, not their absolute magnitudes. Any strictly monotonic transformation of the values $v$ will result in identical selection sets $S_k$.


\subsection{Sequential-Decision-Making and MDPs}\label{sec:sequential}
Sequential decision-making problems can be systematically modeled through Markov Decision Processes (MDPs) \cite{howard1960dynamic}. An MDP is defined by a tuple $\mathcal{M}=(\mathcal{S}, \mathcal{A}, P, r)$, where $\mathcal{S}$ represents the finite state space, $\mathcal{A}$ denotes the set of possible actions, $P(s'|s,a)$ specifies the probability of transitioning to state $s'$ when taking action $a$ in state $s$, and $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function. 

For finite-horizon problems with deterministic state transitions, known as finite-horizon Deterministic MDPs, the transition probability $P$ is replaced by a deterministic transition function $T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ for a fixed horizon length $H$. At each step $t=1,\ldots,H$, given the current state $s \in \mathcal{S}$, an action $a \in \mathcal{A}$ is selected, yielding an immediate reward $r(s,a)$, after which the system transitions to the next state. The value function $V^t(s)$ represents the maximum cumulative reward achievable from state $s$ at time $t$: $V^t(s) = \max_{a \in \mathcal{A}} [r(s,a) + V^{t+1}(T(s,a))]$, with boundary condition $V^H(s) = \max_{a \in \mathcal{A}} r(s,a)$. The objective is to find a policy $\sigma: \mathcal{S} \times [H] \rightarrow \mathcal{A}$ that maximizes the cumulative reward $\sum_{t=1}^{H} r(s_t, \sigma(s_t,t))$ subject to the transition dynamics. This optimization can be solved through backward dynamic programming (DP) \citep{bellman1966dynamic}.

\subsection{Approximate Dynamic Programming}
Dynamic programming faces computational challenges due to the curse of dimensionality when state and action spaces grow large. Approximate Dynamic Programming (ADP) \cite{lee2004approximate, powell2007approximate, bertsekas2024course} introduces approximation techniques to address these limitations. The first key strategy in ADP involves parametric function approximation to estimate both value functions and reward functions. Instead of maintaining exact values for each state-action pair $(s,a)$, ADP employs parametric approximations like $\hat{V}(s;\theta)$ for value functions and $\hat{r}(s,a;\theta)$ for finite-horizon undiscounted reward functions, where $\theta$ represents learnable parameters in linear models  \cite{powell2009you} or neural networks \cite{bertsekas1996neuro}. The second strategy involves simplified decision rules at each state that consider only immediate or limited-horizon future rewards, rather than the full backward induction required by exact dynamic programming. These approximations enable ADP to handle high-dimensional MDPs by trading off exact optimality for computational tractability while maintaining solution quality through careful approximation design.  For a comprehensive discussion of sequential decision-making and ADP methods, we refer readers to Section \ref{sec:related_sdm}.

Among various ADP approaches to solving sequential decision-making problems, myopic reward function approximation \cite{powell2016perspectives, rempel2021review} represents the most computationally efficient strategy by combining both approximation strategies: it uses parameterized reward approximation $\hat{r}(s,a;\theta)$ and adopts the simplest decision rule $\sigma_{myopic} = \text{argmax}_{a \in \mathcal{A}} \hat{r}(s,a;\theta)$. This solution focuses solely on maximizing an approximated immediate reward for the nearest time period, ignoring how current decisions might impact future states.


\begin{figure*}
    \centering
    \includegraphics[width=0.73\linewidth]{frame.pdf}
    \caption{Framework for sequential data selection. Our framework consists of three components: (1) A sequential data decision problem formulating data selection through step-by-step decision-making. (2) Core components of any solution for this sequential problem including reward modeling and decision policies. (3) Selection performance curves showing outcomes from different reward modeling plus decision policy combinations, where exact DP achieves optimal performance (blue), linear ADP yields suboptimal results (gray), and bipartite approximation attains near-optimal result (yellow).}
    \label{fig:framework}    
\end{figure*}



\section{A Unified Framework for Sequential Data Selection}  

When data values are utilized for data selection (Definition~\ref{def:selection}), data points are selected sequentially according to their induced ranking. This process is fundamentally sequential in nature: data points must be chosen one by one, with each selection potentially impacting the value of the remaining points. Moreover, through this ranking mechanism, a single value assignment implicitly provides solutions for data selection problems with any budget constraint $k \leq |\mathcal{D}|$, which is formalized in the following observation.

\begin{observation}[Solutions for Multiple Selection Problems]\label{obs:solution_multiplicity}
\textit{Any value assignment $v: \mathcal{D} \rightarrow \mathbb{R}$ over a dataset $\mathcal{D}$ implicitly generates solutions for all subset selection problems with cardinality constraints $k \in [1,|\mathcal{D}|]$ through its induced ranking.}
\end{observation}

According to this observation, an ideal data value should select satisfying subsets for different budget sizes $k$ in a progressive way. Existing evaluation protocols implicitly follow this intuition. As shown in Figure~\ref{fig:selection}, the quality of data values is typically evaluated through qualitative comparison of performance curves. Superior data values should demonstrate both immediate efficiency through steeper initial curves and sustained effectiveness via consistently higher performance across all selection budgets. Such qualitative comparison can be quantitatively characterized through the metric $\sum_{k=1}^{|\mathcal{D}|} U(S_k)$, where $S_k$ represents the subset of size $k$ selected according to the value-based ranking (Definition~\ref{def:selection}). An ideal data value should maximize this metric. However, existing data values are not explicitly optimized for this objective.

To address this limitation, we formulate the sequential data selection problem that directly optimizes this objective:


\begin{definition}[Sequential Data Selection Problem] \label{prob:sequential}
Given a dataset $\mathcal{D}$ and a utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}$, the goal is to find an optimal selection sequence that maximizes the expected utility across different selection sizes:
\begin{equation} \label{eq:expected}
\begin{aligned}
\max_{\pi} & \quad \mathbb{E}_{k}[U(S_k)] = \frac{1}{|\mathcal{D}|}\sum_{k=1}^{|\mathcal{D}|} U(S_k) \\
\text{s.t.} & \quad S_{k-1} \subset S_k, \quad \forall k \in {2,...,|\mathcal{D}|} \\
& \quad S_k = \{\pi(1), ..., \pi(k)\}
\end{aligned}
\end{equation}
where $k \sim \text{Uniform}(1,|\mathcal{D}|)$, and $\pi$ is a permutation of dataset indices representing the selection order.
\end{definition}

\subsection{Sequential Data Selection as a Deterministic Markov Decision Process}
The nested constraint $S_{k-1} \subset S_k$ reveals the inherent recursive structure of Problem~\ref{prob:sequential}: each selection decision is conditioned on all previous choices, and the marginal contribution of the $k$-th selected sample depends on the composition of the existing subset. This sequential dependency naturally suggests reformulating the problem as a Deterministic Markov Decision Process (DMDP):

\begin{compactenum}
\item \textbf{State} $s_t$: The set of selected samples at step $t$
\item \textbf{Action} $a_t$: The next sample to be selected from remaining samples
\item \textbf{Transition}: $s_{t+1} = s_t \cup {a_t}$
\item \textbf{Reward}: $r_t = U(s_t)$
\end{compactenum}

To solve the sequential data selection problem, which starts from an empty set and sequentially selects data points until all data is selected, we need to analyze what factors determine the selection quality. As illustrated in \textbf{Figure~\ref{fig:framework}}, the solution quality is fundamentally determined by two key factors: (1) \textbf{Reward Modeling}:  How we model and access the environment's reward signals. This could be the ground-truth utility obtained through actual model training and evaluation, or a surrogate that approximates the true utility function. (2) \textbf{Decision Policy}: How we make decisions based on the reward function we used. The combination of these two components determines the quality of the induced ranking, as their interactions directly impact the selection performance across different subset sizes. Through this lens of utility modeling and decision-making, we will analyze how different solutions arise from specific choices of these two components, from exact dynamic programming to various approximation schemes in existing methods.

\subsection{An Exact Solution via Dynamic Programming}\label{sec:exact_dp}
Under the DMDP formulation, our goal is to find an optimal policy that maximizes the cumulative reward starting from any state. Let $V(s)$ denote the optimal value function starting from state $s$, representing the maximum achievable cumulative reward from state $s$ onwards. According to the Bellman optimality equation \citep{bellman1966dynamic}: 
\begin{equation} \label{eq:bellman}
V(s) = \begin{cases}
U(s), & \text{if } |s| = n \\
U(s) + \max_{a \in \mathcal{D}\setminus s} V(s \cup \{a\}), & \text{otherwise}
\end{cases}
\end{equation}

The optimal policy $\sigma^*(s) = {\text{argmax}}_{a \in \mathcal{D}\setminus s} V(s \cup \{a\})$ outputs the action that maximizes the sum of immediate reward and future value, naturally inducing an optimal selection sequence. This solution represents the ideal case where both components achieve optimality: (1) it uses the ground-truth utility function $U(s)$ for reward modeling, and (2) it employs full-lookahead decision policy through immediate reward plus value function maximization as shown in Equation~\eqref{eq:bellman}. However, this exact solution faces significant computational challenges as it requires evaluating utilities for exponentially many subsets, motivating the need for efficient approximation methods.

\subsection{Approximated Solutions via Approximated Dynamic Programming}

The exact solution through dynamic programming, while theoretically optimal, faces significant computational challenges in both key components: obtaining the ground-truth utility $U(s)$ requires model retraining for exponentially many subsets, and computing the optimal value function requires evaluating all possible future states recursively. To address these computational challenges, we can leverage Approximate Dynamic Programming (ADP) to simplify both components. For reward modeling, instead of computing the ground-truth utility through actual model training, we can approximate it with parametric surrogate functions. For decision making, rather than considering all future states through value function optimization, we can adopt myopic policies that only consider immediate rewards. Interestingly, existing data valuation methods can be understood as specific instances of this ADP framework. As we will demonstrate in the next section, methods like Data Shapley implicitly employ linear reward approximation combined with myopic decision-making, providing a theoretical explanation for both their computational efficiency and performance characteristics.

\section{Analyzing Existing Data Values via ADP}\label{sec:adp_analysis}
Having established the optimal sequential selection framework, we now demonstrate how existing data valuation methods can be unified and analyzed through Approximate Dynamic Programming (ADP). This perspective reveals that the ranking sequences induced by data values are in fact specific solutions to our sequential decision-making problem under particular approximation schemes. 

\begin{theorem}[Game-theoretic Data Values as ADP Solutions]\label{thm:adp_solution} 
For game-theoretic data valuation methods (Data Shapley, Beta Shapley and Data Banzhaf) that assign values $v: \mathcal{D} \rightarrow \mathbb{R}$, and induce a data sequence $\pi_v$ ranked by values, this sequence can be equivalently obtained as a solution to our sequential decision MDP under the following ADP framework:
\begin{compactenum}
   \item \textbf{Reward Modeling}: Using a linear surrogate function $\hat{U}(S) = \sum_{i \in S} \theta_i$ as the reward signal, where $\theta_i$ are estimated through weighted least squares with method-specific weights. Different weights correspond to different game-theoretic values. 
   \item \textbf{Decision Policy}: Adopting a myopic policy $\pi_{myopic}(s) = \text{argmax}_{a \in \mathcal{D}\setminus s} \theta(a)$
\end{compactenum}
such that the trajectory generated by this ADP solution exactly matches the data sequence $\pi_v$ induced by the original data values. (Proof in Appendix \ref{app:adp_proof}.)

\end{theorem}

\subsection{Optimality Analysis}
In this subsection, we analyze the optimality of existing game-theoretic data values under different utility functions in Problem~\ref{prob:sequential}. We start with linear utility functions - a special case where data points contribute independently:
Remarkably, despite their approximations, existing data valuation methods achieve optimality when the ground-truth utility exhibits linear structure. Consider linear utility functions, a special case where each data point contributes independently to the overall utility:
\begin{definition}[Linear Utility Function]
A utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}$ is linear if it can be written as $U(S) = \sum_{i\in S} w_i$ for some weights $w_i \in \mathbb{R}$.
\end{definition}
When the utility function is linear,  all semi-value-based methods achieve optimal sequential selection.
\begin{theorem}[Optimality Under Linear Utility]
When the utility function $U$ in Problem~\ref{prob:sequential} is linear, the sequences induced by semi-value based data valuation methods (including Data Shapley, Beta Shapley, and Data Banzhaf) are optimal solutions to Problem~\ref{prob:sequential} (proof in Appendix~\ref{app:linear_proof}).
\end{theorem}

\subsubsection{Optimality Analysis Under Monotonic Sub-modular Utility with Curvature $c$} \label{sec:submodular}
While the prior analysis of linear utility functions provides valuable theoretical insights, such functions represent an idealized case that rarely appears in practical data selection scenarios. Real-world selection tasks typically exhibit more complex utility structures with diminishing returns \cite{huh2022optimal}, wherein the marginal benefit of each additional data point decreases as we select more points. This behavior is naturally captured by monotonic submodular functions, which arise in many practical settings such as coreset selection and data summarization.



\begin{definition}[Monotonic Submodular Function] \label{def:submodular}
A utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}$ is called monotonic submodular if it satisfies two properties: (1) {\bf Monotonicity}: For any $A \subseteq B \subseteq \mathcal{D}$, we have $U(A) \leq U(B)$; and (2) {\bf Submodularity}: For any $A \subseteq B \subseteq \mathcal{D}$ and any element $i \in \mathcal{D} \setminus B$, $U(A \cup \{i\}) - U(A) \geq U(B \cup \{i\}) - U(B)$.
\end{definition}

\begin{definition}[Curvature]
For a monotonic submodular function $U$, let $\Delta_i U(S) = U(S \cup \{i\}) - U(S)$ denote the marginal contribution of element $i$ to set $S$. The curvature $c$ of $U$ is defined as:
$$c = 1 - \min_{i \in \mathcal{D}} \frac{\Delta_i U(\mathcal{D}\setminus\{i\})}{\Delta_i U(\emptyset)}$$
where $c \in [0,1]$ measures the degree of diminishing returns, with $c=0$ corresponding to linear functions and $c=1$ representing maximum diminishing returns.
\end{definition}

\begin{theorem}\label{thm:submodular}
For a monotonic submodular function $U$ with curvature $c$, let $G_k$ denote the set of first $k$ elements selected by ordering elements according to their semi-value scores. Then:
$$U(G_k) \geq (1-c)^2 U(OPT_k)$$
where $OPT_k$ represents the optimal size-$k$ subset. This further extends to our sequential selection objective:
$$\sum_{k=1}^n U(G_k) \geq (1-c)^2 \sum_{k=1}^n U(OPT_k)$$
The proof is provided in Appendix~\ref{app:submodu_proof}.
\end{theorem}



This theorem establishes that all semi-value based methods achieve a $(1-c)^2$ approximation ratio for both fixed-size and sequential selection objectives.  More importantly, this theoretical characterization also reveals a significant limitation as described in the following remark. 

\begin{remark} \label{rem:substitution}
 As $c \to 1$ (high data substitutability), the $(1-c)^2$ guarantee deteriorates quadratically, explaining why traditional data valuation methods often struggle for data with overlapping information.
\end{remark}


This finding aligns with \cite{wang2024rethinking} showing that Data Shapley preforms well on heterogeneous datasets.

\section{Efficient Approximation via Bipartite Surrogate Models}\label{sec:efficient}
While our framework provides theoretical clarity through exact dynamic programming (Section~\ref{sec:exact_dp}), its computational demands make it intractable in practice. Although existing methods like Data Shapley offer linear approximations (Section~\ref{sec:adp_analysis}), they often fail to capture complex data dependencies. To address both challenges, we propose approximating the utility function using bipartite graph structures. Our key insight is that a data point's utility contribution can be effectively modeled through its coverage relationships with the validation set. This approach provides a balance between expressiveness and computational efficiency while maintaining theoretical guarantees for optimal selection under mild conditions.

\subsection{Bipartite Graph Structure Learning} 
Let training data $X_{train} = \{x_1^{train},...,x_n^{train}\}$ and validation data $X_{valid} = \{x_1^{valid},...,x_m^{valid}\}$. We represent the utility dependencies through a weighted bipartite graph $G = (V_{train} \cup V_{valid}, E, w)$, where vertices $V_{train}$ and $V_{valid}$ correspond to training and validation points respectively, edges $E \subseteq V_{train} \times V_{valid}$ capture influence relationships, and weights $w: E \rightarrow \mathbb{R}_+$ quantify their strengths.  The key challenge lies in learning appropriate edge structures that accurately reflect utility dependencies. Our algorithm leverages feature space information to discover inherent utility relationships, based on the insight that training and validation points with similar features often exhibit shared utility patterns. We propose an efficient algorithm (Algorithm \ref{alg:bipartite} in Appendix \ref{app:bipartite}) that leverages feature space information to learn edge connections by adaptively thresholding similarity scores.


\subsection{Sequential Selection with Bipartite Graphs}
With the constructed bipartite graphs from our two approximation approaches, we can determine data values through an iterative maximum coverage process. At each step, we select the training point that covers the maximum number of uncovered validation points based on the learned edge relationships. After each selection, we update the remaining coverage structure by removing both the selected training point and its covered validation points from consideration. This pruning step prevents redundant coverage while ensuring diversity in the selected sequence. The process naturally assigns higher values to points selected earlier in the sequence, as they provide coverage for larger portions of previously uncovered validation points. This selection strategy inherits theoretical guarantees from the submodular nature of the coverage function while maintaining computational efficiency by operating on our pre-computed graph structures rather than requiring repeated model evaluations. 


\subsection{Theoretical Properties}
The bipartite-based approximation preserves key theoretical properties that ensure its effectiveness. As established in Appendix \ref{app:fast_approx}, our approximation maintains both monotonicity and submodularity of the original utility function. These properties, formalized in Theorem \ref{thm:coverage_properties}, guarantee that greedy selection under the approximated utility achieves optimal sequential ordering when the approximation accurately learns the ground-truth utility function (Theorem \ref{thm:greedy_coverage}). 

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.96\linewidth]{upper_bound_results.pdf}
    \caption{Performance comparison between optimal sequential selection (DynamicProgramming, solid gray) and existing data valuation methods across eight datasets. Results reveal performance gaps between existing methods and the optimal policy.}
    \label{fig:upper_bound}
\end{figure*}


\section{Data Values for Selection: From Reward Functions to Value Functions}
Our sequential decision-making framework reveals a fundamental insight: data values for selection should arise from value functions rather than reward functions. Through the optimal policy derived in Section~\ref{sec:exact_dp}, we can define optimal data values as $v^*(i) = n - t^*(i)$, where $t^*(i)$ represents the optimal selection step for sample $i$. This value function perspective fundamentally differs from traditional methods that aggregate marginal rewards independently. Instead, it naturally captures sequential dependencies by considering each sample's role in the optimal selection trajectory. By expressing data values through value functions conditioned on optimal state sequences, we measure a data point's value by its order to the optimal trajectory rather than isolated rewards. This formulation achieves space efficiency by tracking dependencies only along optimal paths instead of the full state space.

\section{Experiment}
For evaluation, we compare our approach with nine baseline data valuation methods including \citep{koh2017understanding}, \texttt{Data Shapley} \citep{ghorbani2019data}, \texttt{Beta Shapley} \citep{kwon2021beta}, and \texttt{Data Banzhaf} \citep{wang2023data} etc., all implemented in OpenDataVal \citep{jiang2023opendataval} and other public dataset sources. We conduct experiments on eight diverse datasets from OpenML \citep{feurer2021openml}, following the standard data valuation evaluation protocol that generates selection curves by iteratively adding points based on their assigned values. All experiments are averaged over 20 independent runs with 1000 model retraining steps except for the \texttt{DynamicProgramming} method (Algorithm \ref{alg:optimal_value}), which is used for optimality verification in sequential selection. Detailed experimental settings and dataset descriptions are provided in Appendix \ref{app:exp_details}.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.93\linewidth]{appro_results.pdf}
    \caption{Evaluation of our proposed bipartite-based method against baselines on eight datasets. Our bipartite approach (solid yellow) demonstrates superior efficiency, requiring significantly fewer samples to achieve comparable accuracy.}
    \label{fig:appr}
\end{figure*}


\subsection{RQ1: How Close are Existing Data Values to Optimal Sequential Selection?}\label{sec:rq1}
Following the experimental settings detailed in Appendix \ref{app:rq1_exp}, Figure \ref{fig:upper_bound} reveals substantial performance gaps between existing methods and optimal sequential selection method \texttt{DynamicProgramming}. Several key findings emerge:

First, the optimal selection \texttt{Dynamic}\texttt{Programming} (solid gray line) consistently outperforms all existing methods across datasets, with particularly pronounced gaps in early selection stages ($k \leq 5$). This indicates that current approaches significantly underperform in identifying the most valuable initial samples. Second, the performance gap varies notably across datasets, suggesting dataset-specific challenges. The gap is particularly prominent in structured datasets like bbc-embeddings and nomao, where optimal selection demonstrates steep initial performance improvements that existing methods fail to match. Third, existing methods exhibit varying patterns of suboptimality. Game-theoretic approaches (\texttt{DataShap}, \texttt{BetaShap}, \texttt{Banzhaf}) tend to perform similarly to each other but consistently fall short of \texttt{DynamicProgramming}. Learning-based methods (\texttt{DVRL}, \texttt{AME}) and influence-based approaches (\texttt{InfluenceSubsample}) show competitive performance in some cases but lack consistency across datasets. Notably, all methods cannot match the optimal strategy's ability to achieve both rapid initial improvement and sustained performance gains.


\subsection{RQ2: How Does Utility Curvature Impact the Performance of Game-theoretic Data Values?}
To empirically validate our theoretical analysis of utility curvature's impact on data valuation methods, we design controlled experiments using message passing mechanisms that systematically vary the degree of data substitutability. Our results demonstrate three key findings that align with the theoretical predictions (detailed analysis in Appendix \ref{app:curvature_exp}): (1) under low curvature (propagation proportion = 0.0), all methods \texttt{DataShap}, \texttt{BetaShap}, and \texttt{Banzhaf} achieve strong performance with mean accuracy above 0.70, confirming the $(1-c)^2$ approximation guarantee predicted by Theorem \ref{thm:submodular}; (2) as curvature increases through higher propagation proportions, we observe significant performance degradation across all methods, with mean accuracy dropping from around 0.74 to 0.59 at maximum substitutability (propagation proportion = 1.0); and (3) the convergence in performance between these game-theoretic methods at high curvature validates our analysis that all these approaches face similar fundamental limitations under strong substitution effects.



\subsection{RQ3: How Effective is Bipartite-based Approximation for Data Selection?}\label{sec:rq4_selection}
Following the experimental settings detailed in Appendix \ref{app:rq3_exp}, the selection curves in Figure~\ref{fig:appr} demonstrate the clear advantages of our bipartite-based approach \texttt{bipartite}.  On structured datasets like bbc-embeddings, our method exhibits remarkably steep initial performance improvements, achieving over 60\% accuracy with just 20 samples, while other methods require 40-60 samples to reach similar performance levels. This pattern of superior early-stage selection is consistently observed across different datasets. For instance, on the MiniBooNE dataset, our method reaches 70\% accuracy using only 15 samples, significantly outperforming both game-theoretic approaches like \texttt{DataShapley} and learning-based methods like \texttt{DVRL}.

The performance advantage is particularly pronounced on complex datasets with heterogeneous feature distributions. In the digits dataset, our method demonstrates exceptional sample efficiency, reaching 80\% accuracy with just 25 samples, while baseline methods require nearly twice as many samples to achieve comparable performance. Similar patterns emerge in the nomao dataset, where our method maintains a consistent lead throughout the selection process, especially in the crucial early stages where efficient selection is most valuable. Even on relatively simpler datasets like 2dplanes and fried, our method shows noticeable improvements in selection efficiency. The curves reveal not only faster initial accuracy gains but also more stable progression, with fewer fluctuations compared to baseline methods. The electricity dataset results further emphasize this stability, where our method maintains a steady performance advantage across all selection sizes. To quantify these observations, we present detailed average performance metrics for the selection curves in Table~\ref{tab:selection_results} (see Appendix \ref{app:selection_performance}). 


\section{Conclusion}
In this work, we establish a comprehensive theoretical framework that unifies data valuation and sequential decision-making, providing new insights into the role of data values in selection tasks. By reformulating data selection as a sequential optimization problem, we demonstrate that existing game-theoretic methods can be understood as myopic linear approximation solutions. Our analysis reveals fundamental limitations of these methods under high utility curvature, explaining their varying performance across different types of datasets. Moreover, we develop a bipartite graph-based approximation scheme that preserves essential theoretical properties while enabling efficient computation. Through extensive experiments, we demonstrate that our approach significantly outperforms existing methods across diverse datasets, particularly in early-stage selection. 


\bibliography{reference}
\bibliographystyle{icml2025}

\appendix
\onecolumn


\section{Related Work}\label{sec:related}
\subsection{Data Valuation}\label{sec:related_dv}

Data valuation seeks to quantify the contribution of individual training samples to model performance. Game-theoretic approaches have dominated this field, beginning with Data Shapley \citep{ghorbani2019data}, which adapts the Shapley value from cooperative game theory to quantify data contributions. This seminal work inspired various extensions, including Beta Shapley \citep{kwon2021beta}, which introduces a family of semi-values through Beta function weighting, and Data Banzhaf \citep{wang2023data}, which provide robust valuation frameworks through binary-weighted marginal contributions. While these general methods are computationally intensive, specialized approaches like \citep{jia2019efficient} achieve near-linear time complexity by exploiting algorithmic structures such as K-Nearest Neighbors, compared to the exponential complexity of model-agnostic approaches. Recent developments address specific challenges in data valuation: \citet{schoch2022cs} proposed CS-Shapley for better handling class-wise contributions in classification tasks, while \citet{chi2024precedence} introduced PC-Winter value to tackle the unique challenges of graph-structured data valuation. Alternative theoretical frameworks, such as the Core \citep{yan2021if}, have also emerged to address coalition stability in data valuation. 

Recent work has focused on improving computational efficiency and valuation accuracy. \citet{wang2021improving} proposed learning data utility functions to avoid repeated model retraining, while \citet{garrido2023shapley} developed DU-Shapley as an efficient proxy through discrete uniform distributions. \citet{tarun2024ecoval} introduced EcoVal, which accelerates valuation by clustering similar data points and propagating values within clusters. P-Shapley \citep{xia2024p} leverages predicted probabilities instead of accuracy for finer-grained utility differentiation. 

Parallel efforts have explored training-free and task-agnostic directions. DAVINZ \citep{wu2022davinz} enables data valuation at network initialization by theoretically deriving domain-aware generalization bounds, while \citet{amiri2023fundamentals} proposed a task-agnostic framework based on statistical properties without validation requirements. Alternative paradigms include reinforcement learning-based approaches like DVRL \citep{yoon2020data}, complexity-gap scoring \citep{nohyun2022data}, and Wasserstein distance-based frameworks \citep{just2023lava}, which offer diverse perspectives beyond traditional game-theoretic methods.


\subsection{Approximate Dynamic Programming}\label{sec:related_sdm}
Markov Decision Process (MDP) provides one of the most fundamental frameworks for modeling sequential decision-making problems since the seminal work of \citet{bellman1957markovian}. While MDPs can be solved optimally through dynamic programming \citep{bellman1966dynamic}, they suffer from the curse of dimensionality as the state and action spaces grow. To address these computational challenges, Approximate Dynamic Programming (ADP) has emerged as a powerful paradigm that decomposes the original problem into more tractable subproblems through various approximation strategies \citep{powell2007approximate, bertsekas2024course}.

Several key developments have shaped modern ADP approaches. \citet{roy2002approximate} established theoretical foundations for approximate linear programming in average-cost dynamic programming, while \citet{de2004constraint} advanced the practical applicability through constraint sampling techniques. \citet{powell2014clearing} unified various competing strategies into a common framework, bridging the gap between communities such as stochastic programming, dynamic programming, and stochastic search. Recent work has focused on specific challenges: \citet{hannah2011approximate} developed methods for continuous, convex decision sets in storage problems, while \citet{petrik2012approximate} introduced distributionally robust ADP with guaranteed convergence properties. The effectiveness of ADP has been demonstrated across diverse applications. In game-playing domains, \citet{gabillon2013approximate} achieved breakthrough performance in Tetris through policy-based ADP, while \citet{perolat2015approximate} extended these methods to two-player zero-sum Markov games. Recent advances continue to push boundaries, with \citet{shetty2024generalized} developing tensor-based approximations for hybrid control systems and \citet{mcmahan2024deterministic} establishing polynomial-time guarantees for constrained reinforcement learning through novel ADP formulations.

\subsection{Parallel Work on Unifying Data Selection Methods}
Recent work by \citet{wang2024advancing} presents a comprehensive tutorial on data selection methods for foundation models, focusing on both heuristic and principled approaches to data curation. While their work provides valuable insights into the practical aspects of data selection in foundation model training pipelines, our work differs in several key aspects: First, we focus specifically on the optimization perspective of data selection with data values, providing theoretical guarantees of optimality under our proposed sequential decision making and approximate dynamic programming framework. Their work takes a broader view, covering various selection strategies without explicit optimization guarantees. Our work presents a novel unifying framework via approximate dynamic programming (ADP) \citep{bertsekas2024course}. This framework reveals that many existing data valuation methods can be interpreted as specific instances of myopic cost function approximation heuristic \citep{powell2016perspectives,rempel2021review}, which is a classical strategy in the ADP literature. This theoretical connection not only provides new insights into why existing methods work but also suggests principled ways to improve them. Third, while both works aim to bridge the gap between heuristic and principled approaches, we focus more on the theoretical foundations of data valuation methods for selection tasks. We provide rigorous analysis of the limitations of data attribution methods and characterize the conditions under which they can achieve optimal performance. Our work thus complements theirs by providing deeper theoretical understanding of data valuation based selection methods, while their tutorial offers broader practical guidance across different selection paradigms. Together, these parallel efforts contribute to advancing both the theoretical foundations and practical applications of data selection methods.


\section{Algorithm for Computing Optimal Data Values}\label{app:optimal_algo}

We present the complete algorithm for computing optimal data values through dynamic programming in Algorithm \ref{alg:optimal_value}. The algorithm proceeds in two phases: (1) backward induction to compute the optimal value function and policy, and (2) forward traversal to derive data values from the optimal selection sequence.

\begin{algorithm}[H]
\centering
\caption{Computing Optimal Data Values for Selection} 
\label{alg:optimal_value}
\begin{algorithmic}
   \STATE {\bfseries Input:} dataset $\mathcal{D}$, utility function $U: 2^{\mathcal{D}} \rightarrow \mathbb{R}$
   \STATE \textcolor{blue}{// Initialize value for full dataset}
   \STATE Initialize $V[\mathcal{D}] \leftarrow U(\mathcal{D})$
   \STATE \textcolor{blue}{// Backward phase: compute optimal value function}
   \FOR{$t=|\mathcal{D}|-1$ {\bfseries to} $0$}
        \FOR{each state $s \subseteq \mathcal{D}$ with $|s| = t$}
            \STATE $V[s] \leftarrow U(s) + \max_{a \in \mathcal{D}\setminus s} V(s \cup \{a\})$ 
            \STATE $\sigma^*[s] \leftarrow \text{argmax}_{a \in \mathcal{D}\setminus s} V(s \cup \{a\})$ 
        \ENDFOR
   \ENDFOR
   \STATE \textcolor{blue}{// Forward phase: derive data values}
   \STATE Initialize $s \leftarrow \emptyset$
   \FOR{$t=1$ {\bfseries to} $|\mathcal{D}|$}
        \STATE $a_t \leftarrow \sigma^*[s]$ \textcolor{blue}{// Select next element}
        \STATE $v^*[a_t] \leftarrow |\mathcal{D}|-t$ \textcolor{blue}{// Assign value}
        \STATE $s \leftarrow s \cup \{a_t\}$
   \ENDFOR
   \STATE {\bfseries Return:} value function $v^*$
\end{algorithmic}
\end{algorithm}


The algorithm's first phase implements backward dynamic programming, computing the optimal value function $V[s]$ and policy $\sigma^*[s]$ for each state $s$. The second phase constructs the optimal selection sequence and assigns values inversely proportional to selection order. While this algorithm yields optimal values, its computational complexity is exponential in the dataset size due to the need to evaluate all possible subset states.



\section{Proof of Game-theoretic Data Values as ADP Solutions}\label{app:adp_proof}
We provide a complete proof of Theorem \ref{thm:adp_solution}, which establishes the equivalence between game-theoretic data valuation methods and specific ADP solutions. The theorem states that for any data values assigned by game-theoretic methods (such as Data Shapley, Beta Shapley, and Data Banzhaf), we can construct an equivalent ADP framework that both recovers the original values and generates identical selection sequences. The core of this equivalence lies in two key components: a linear surrogate function that models the reward signal, and a myopic policy that guides sequential decision-making. Through the perspective of ADP, we show that when the surrogate function $\hat{U}(S) = \sum_{i \in S} \theta_i$ is properly fitted by minimizing $\|U - \hat{U}\|^2$, its coefficients $\theta_i$ exactly recover the original data values $v(i)$. Moreover, the myopic policy $\pi_{myopic}(s) = \text{argmax}_{a \in \mathcal{D}\setminus s} \theta(a)$ generates a selection trajectory that precisely matches the sequence induced by ranking the original values. 

\subsection{Utility Function Approximation Analysis}
Our sequential decision-making framework reveals that existing data valuation methods can be naturally interpreted with linear surrogate modeling. Specifically, these methods implicitly approximate the true utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}$ via a linear model:
$$\hat{U}(S) = \sum_{i \in S} \theta_i = \sum_{i \in \mathcal{D}} \theta_i \cdot \mathbb{I}_{i \in S}$$
where $\theta_i$ represents the coefficient for data point $i$, and $\mathbb{I}_{i \in S}$ is the indicator function that equals 1 if $i \in S$ and 0 otherwise. 

As introduced in \cite{li2024faster}, the coefficients $\theta_i$ can be estimated through a constrained weighted least squares optimization:
$$\min_{\theta \in \mathbb{R}^n} \sum_{\emptyset \neq S \subseteq \mathcal{D}} w(|S|) (U(S) - \hat{U}(S))^2$$
$$\text{subject to } \hat{U}(S) \theta_i = U(\mathcal{D}) - U(\emptyset)$$

Different data valuation methods emerge from specific choices of the weighting function $w(|S|)$:
\begin{itemize}
    \item Data Shapley: $w(S) = \binom{n-1}{|S|-1}^{-1}$
    \item Data Banzhaf: $w(S) = 2^{-|S|}$
    \item Beta Shapley: $w_{\alpha,\beta}(|S|)$ 
\end{itemize}

These weighting schemes are specifically designed to recover the corresponding game-theoretic values in the limit of infinite samples. For instance, the combinatorial weights in Data Shapley ensure that the estimated coefficients converge to the exact Shapley values.

\subsection{Value Recovery and Ordering Preservation}
The equivalence between our ADP framework and game-theoretic data values is established through two key steps:

\begin{lemma}[Value Recovery]\label{lem:value_recovery}
When the number of samples used in estimation approaches infinity, the estimated coefficients converge to the game-theoretic values:
$$\lim_{N \to \infty} \theta_i^N = v(i), \quad \forall i \in \mathcal{D}$$
where $\theta_i^N$ represents the coefficient estimated using $N$ samples.
\end{lemma}

\begin{proof}
The weighted least squares optimization with constraint fundamentally estimates the expected marginal contribution of each data point. When $N \to \infty$, this expectation converges to the exact marginal contributions used in game-theoretic definitions:
$$\theta_i = \sum_{S \subseteq \mathcal{D}\setminus\{i\}} w(|S|)[U(S \cup \{i\}) - U(S)] = v(i)$$
This convergence is guaranteed by the law of large numbers and the specific choice of weighting functions that match the corresponding game-theoretic methods.
\end{proof}

\begin{lemma}[Myopic Policy Equivalence]\label{lem:myopic_equiv}
Under the linear surrogate model, the myopic policy selections match the value-based ordering:
$$\sigma_{myopic}(s_t) = \text{argmax}_{a \in \mathcal{D}\setminus s_t} \hat{U}(s_t \cup \{a\}) - \hat{U}(s_t) = \text{argmax}_{a \in \mathcal{D}\setminus s_t} \theta(a)$$
\end{lemma}

\begin{proof}
For any state $s_t$, the marginal gain of adding element $a$ is:
\begin{align*}
\hat{U}(s_t \cup \{a\}) - \hat{U}(s_t) &= \sum_{i \in \mathcal{D}} \theta_i \cdot \mathbb{I}_{i \in s_t \cup \{a\}} - \sum_{i \in \mathcal{D}} \theta_i \cdot \mathbb{I}_{i \in s_t} \\
&= \theta_a
\end{align*}
Therefore, the myopic policy naturally selects elements in descending order of their coefficients.
\end{proof}

Combining these lemmas, we can conclude that when coefficients are properly estimated, our ADP framework with linear surrogate function and myopic policy exactly recovers both the values ($\theta_i = v(i)$) and selection behavior ($\pi_v(k) = \sigma_{myopic}(s_{k-1})$) of game-theoretic methods, completing the proof of Theorem \ref{thm:adp_solution}.


\section{Proof of Linear Utility Optimality}\label{app:linear_proof}

We establish the optimality of semi-value based methods under linear utility functions, inspired by the analytical steps developed by \citet{wang2024rethinking} for monotone transformed modular functions. Our analysis reveals that linear functions, as a special case, possess a fundamental property: they maintain consistent ordering of marginal contributions across different subset sizes. This property, combined with the weighted averaging nature of semi-values, ensures that greedy selection based on any semi-value achieves optimal sequential selection performance.

\begin{lemma}[Consistent Marginal Contributions]\label{lem:marginal}
For a linear utility function $U(S) = \sum_{i\in S} w_i$, the marginal contribution of any element $i$ remains constant regardless of the subset $S$:
\[
U(S \cup \{i\}) - U(S) = w_i, \quad \forall S \subseteq \mathcal{D}\setminus\{i\}
\]
\end{lemma}

\begin{lemma}[Semi-value Equivalence]\label{lem:semi_equiv}
Under a linear utility function, any semi-value based method assigns values proportional to the weights:
\[
v(i) = c w_i
\]
where $c > 0$ is a constant determined by the specific semi-value weighting scheme.
\end{lemma}

\begin{proof}
For any semi-value based method:
\begin{align*}
v(i) &= \sum_{S \subseteq \mathcal{D}\setminus\{i\}} \alpha_{|S|}[U(S \cup \{i\}) - U(S)] \\
&= \sum_{S \subseteq \mathcal{D}\setminus\{i\}} \alpha_{|S|} w_i \\
&= w_i \sum_{S \subseteq \mathcal{D}\setminus\{i\}} \alpha_{|S|} = c w_i
\end{align*}
where $\sum_{S \subseteq \mathcal{D}\setminus\{i\}} \alpha_{|S|} = c > 0$ by the definition of semi-values.
\end{proof}

\begin{theorem}[Sequential Selection Optimality]\label{thm:seq_opt}
For any linear utility function $U$, selecting elements in descending order of their semi-values achieves optimal cumulative utility:
\[
\sum_{k=1}^n U(S_k) = \max_{\pi} \sum_{k=1}^n U(S_k^\pi)
\]
\end{theorem}

\begin{proof}
The proof follows in three steps:

1) By Lemma~\ref{lem:semi_equiv}, the ordering based on semi-values $v(i)$ is equivalent to ordering based on weights $w_i$.

2) For any size $k$, the subset $S_k$ selected by this ordering contains the $k$ elements with largest weights.

3) Due to linearity, for any size $k$:
\[
U(S_k) = \sum_{i \in S_k} w_i \geq \sum_{i \in S_k^\pi} w_i = U(S_k^\pi)
\]
for any alternative subset $S_k^\pi$ of size $k$. Therefore:
\[
\sum_{k=1}^n U(S_k) \geq \sum_{k=1}^n U(S_k^\pi)
\]
for any alternative sequence $\pi$, establishing optimality.
\end{proof}

This proof shows that all semi-value methods achieve identical selection performance under linear utilities because they preserve the ordering of the underlying weights $w_i$. This explains why different methods like Data Shapley, Beta Shapley, and Data Banzhaf all achieve optimal sequential selection despite their different weighting schemes.


\section{Analysis of Data Valuation Methods Under Submodular Functions}\label{app:submodu_proof}
In this appendix, we provide a complete proof that ranking-based element selection using any standard data valuation method achieves a $(1-c)^2$ approximation ratio for monotone submodular functions with curvature $c$. This result unifies previous analyses and provides tight bounds for Shapley value, Banzhaf value, and Leave-one-out methods, providing theoretical foundations for the results presented in Section 4.4 of the main paper.

\subsection{Preliminaries and Submodular Functions}
Consider a dataset $\mathcal{D}$ and a monotone submodular utility function $U: 2^\mathcal{D} \rightarrow \mathbb{R}_{\geq 0}$ with $U(\emptyset) = 0$. For any sets $A \subseteq B \subseteq \mathcal{D}$ and element $i \in \mathcal{D} \setminus B$, monotone submodularity implies:
\begin{align*}
U(A) &\leq U(B) \tag{monotonicity} \\
U(A \cup \{i\}) - U(A) &\geq U(B \cup \{i\}) - U(B) \tag{submodularity}
\end{align*}

A fundamental property of submodular functions is their curvature, which measures how much marginal contributions decrease as sets grow. For a normalized ($U(\emptyset)=0$) monotone submodular function $U$, its curvature $c \in [0,1]$ is defined as:
\[ c = 1 - \min_{i \in \mathcal{D}} \frac{U(\mathcal{D}) - U(\mathcal{D}\setminus\{i\})}{U(\{i\})} \]
This definition implies that for any element $i$ and set $S \subseteq \mathcal{D}\setminus\{i\}$:
\[ U(S \cup \{i\}) - U(S) \geq (1-c)U(\{i\}) \]

\subsection{Data Valuation Methods and Curvature}
We consider data valuation methods that assign a value $v(i)$ to each element $i \in \mathcal{D}$ as a weighted average of marginal contributions:
\[ v(i) = \sum_{S \subseteq \mathcal{D}\setminus \{i\}} w(S)[U(S \cup \{i\}) - U(S)] \]
where weights $w(S) \geq 0$ satisfy $\sum_S w(S) = 1$. A particularly important class of such valuations are semi-values, which satisfy additional symmetry properties through size-based weights:
\[ w(S) = \beta_{|S|} \text{ for some } \beta_k \geq 0 \text{ with } \sum_{k=0}^{|\mathcal{D}|-1} \binom{|\mathcal{D}|-1}{k}\beta_k = 1 \]
where $\beta_k$ represents the weight assigned to all subsets of size $k$. 

This framework encompasses several important cases from the main paper:
\begin{enumerate}
\item \textbf{Data Shapley}: $w(S) \propto \frac{|S|!(|\mathcal{D}|-|S|-1)!}{|\mathcal{D}|!}$
\item \textbf{Data Banzhaf}: $w(S) = \frac{1}{2^{|\mathcal{D}|-1}}$
\item \textbf{Leave-one-out}: $w(S) = 1$ if $S = \mathcal{D}\setminus\{i\}$, 0 otherwise
\end{enumerate}

We now establish a fundamental relationship between data values and singleton utility values under curvature constraints.

\begin{lemma}[Value-Curvature Relationship]\label{lem:value-curvature}
For any element $i \in \mathcal{D}$, the data value $v(i)$ and singleton value $U(\{i\})$ satisfy:
\[ v(i) \geq (1-c)U(\{i\}) \]
\end{lemma}

\begin{proof}
The proof follows from the curvature property and the structure of data valuations. For any set $S \subseteq \mathcal{D}\setminus\{i\}$, the curvature definition ensures:
\[ U(S \cup \{i\}) - U(S) \geq (1-c)U(\{i\}) \]

The data value $v(i)$ is defined as a weighted average of such marginal contributions:
\[ v(i) = \sum_{S \subseteq \mathcal{D}\setminus \{i\}} w(S)[U(S \cup \{i\}) - U(S)] \]

Since each term in the sum is lower bounded by $(1-c)U(\{i\})$ and the weights $w(S)$ are non-negative and sum to 1, we have:
\begin{align*}
v(i) &= \sum_{S \subseteq \mathcal{D}\setminus \{i\}} w(S)[U(S \cup \{i\}) - U(S)] \\
&\geq \sum_{S \subseteq \mathcal{D}\setminus \{i\}} w(S)[(1-c)U(\{i\})] \\
&= (1-c)U(\{i\})\sum_{S \subseteq \mathcal{D}\setminus \{i\}} w(S) \\
&= (1-c)U(\{i\})
\end{align*}

This bound is tight for certain submodular functions and plays a crucial role in establishing our approximation guarantees.
\end{proof}

\subsection{Approximation Analysis}
Inspired by the curvature-based analysis framework introduced by \citet{balkanski2016power}, we begin by establishing a fundamental relationship between the value of a set constructed through sequential selection and the individual values of its elements. This relationship forms the cornerstone of our approximation guarantee.

\begin{lemma}[Sequential Selection Bound]\label{lem:base-bound}
For any sequence of elements $e_1, \ldots, e_k$ and their corresponding partial sets $S_i = \{e_1, \ldots, e_i\}$ with $S_0 = \emptyset$, we have:
\[ U(S) = \sum_{i \leq k} U_{S_{i-1}}\left(e_i\right) \geq (1-c) \sum_{i \leq k} U\left(e_i\right) \geq (1-c) \sum_{i \leq k} v(i) \]
\end{lemma}

\begin{proof}
The first equality expresses the value of set $S$ as the sum of marginal contributions when elements are added sequentially. For the first inequality, we apply the curvature property to each term: $U_{S_{i-1}}(e_i) \geq (1-c)U(\{e_i\})$. The second inequality follows from our earlier observation that for each element $i$, we have $U(\{e_i\}) \geq v(i)$ by the submodularity.
\end{proof}

This theorem formalizes the approximation guarantee mentioned in Section 4.4 of the main paper:

\begin{theorem}[Submodular Approximation]\label{thm:submod_approx}
Let $U$ be a monotone submodular function with curvature $c$. Let $G_k$ be the set of $k$ elements selected by choosing elements in decreasing order of their data values $v(\cdot)$. Then:
\[ U(G_k) \geq (1-c)^2 U(OPT_k) \]
where $OPT_k$ is an optimal solution to $\max_{|S|\leq k} U(S)$.
\end{theorem}

\begin{proof}
\begin{align*}
U(G_k) &\geq (1-c)\sum_{i \in G_k} v(i) \tag{by Lemma~\ref{lem:base-bound}} \\
&\geq (1-c) \sum_{i \in OPT_k} v(i) \tag{by greedy selection} \\
&\geq (1-c)^2 \sum_{i \in OPT_k} U(\{i\}) \tag{by Lemma~\ref{lem:value-curvature}} \\
&\geq (1-c)^2 U(OPT_k) \tag{by submodularity}
\end{align*}
\end{proof}

\subsection{Sequential Selection Guarantees}
We now extend our analysis to the sequential selection setting, providing theoretical foundations for the results discussed in Section 4.4:

\begin{theorem}[Sequential Selection Guarantee]\label{thm:seq_guarantee}
For any monotone submodular function $U$ with curvature $c$, let $\{G_t\}_{t=1}^k$ be a sequence constructed by selecting elements in descending order of their data values. Then:
\[ \sum_{t=1}^k U(G_t) \geq (1-c)^2 \sum_{t=1}^k U(OPT_t) \]
where $\{OPT_t\}_{t=1}^k$ is an optimal sequence.
\end{theorem}

\begin{proof}
We proceed by showing that the guarantee from Theorem~\ref{thm:submod_approx} extends to each prefix of the sequence. Let $G_t$ denote the set of first $t$ elements selected by our algorithm, and $OPT_t$ be an optimal set of size $t$. From Theorem~\ref{thm:submod_approx}, we know that for each $t \in [k]$:

\[ U(G_t) \geq (1-c)^2 U(OPT_t) \]

Summing this inequality over all $t$ from 1 to $k$ directly yields:

\[ \sum_{t=1}^k U(G_t) \geq (1-c)^2 \sum_{t=1}^k U(OPT_t) \]

This uniform approximation guarantee emerges from the fundamental properties of our selection process. The data values $v(\cdot)$ are computed once at the beginning and remain fixed throughout the selection process. As we proceed, at each step $t$, the set $G_t$ consists of the $t$ elements with the highest data values. 
\end{proof}


\section{Detailed Description of Bipartite Graph Structure Learning algorithm }\label{app:bipartite}

Our bipartite graph learning algorithm (Algorithm \ref{alg:bipartite}) constructs edges between training and validation points based on their feature similarity and label consistency. The algorithm explores multiple distance thresholds to determine optimal edge connectivity, selecting the threshold that best aligns the graph's coverage patterns with empirical model performance. Specifically, for each candidate threshold, it evaluates how well the induced coverage structure predicts the actual model improvements on randomly sampled subsets. The final graph is constructed using the threshold that minimizes this prediction error.


\begin{algorithm}[tb]
\caption{Feature-Based Bipartite Graph Structure Learning}
\label{alg:bipartite}
\begin{algorithmic}
\STATE {\bfseries Input:} Training set $(X_{train}, Y_{train})$, validation set $(X_{valid}, Y_{valid})$
\STATE {\bfseries Parameters:} Number of subset samples $K$, number of thresholds $N_\tau$
\STATE \textcolor{blue}{// Compute distances between training-validation pairs}
\STATE Compute pairwise distances $D_{ij} = \|x_i^{train} - x_j^{valid}\|_2$
\STATE Set $\tau_{min} = \min_{i,j} D_{ij}$, $\tau_{max} = \max_{i,j} D_{ij}$
\STATE \textcolor{blue}{// Sample subsets for evaluation}
\STATE Generate $K$ uniform random subsets $\{R_1,...,R_K\}$ of size $k$ from $X_{train}$
\STATE Pre-compute accuracies $\{a_k\}_{k=1}^K$ on validation set for each subset $R_k$
\FOR{$n=1$ to $N_\tau$}
    \STATE \textcolor{blue}{// Generate threshold by interpolation}
    \STATE $\tau_n = \tau_{min} + \frac{n-1}{N_\tau-1}(\tau_{max} - \tau_{min})$
    \FOR{$k=1$ to $K$}
        \STATE \textcolor{blue}{// Build edges for matching label pairs}
        \STATE Construct edges $E_{\tau_n} = \{(i,j) \mid D_{ij} \leq \tau_n, y_i^{train} = y_j^{valid}\}$
        \STATE \textcolor{blue}{// Compute coverage ratio}
        \STATE $c_{\tau_n}^k = \frac{|\{j \mid \exists i \in R_k: (i,j) \in E_{\tau_n}\}|}{|X_{valid}|}$
    \ENDFOR
    \STATE \textcolor{blue}{// Compute average error}
    \STATE $error_{\tau_n} = \frac{1}{K}\sum_{k=1}^K |c_{\tau_n}^k - a_k|$
\ENDFOR
\STATE \textcolor{blue}{// Select best threshold}
\STATE Select $\tau^* = \text{argmin}_{\tau_n} error_{\tau_n}$
\STATE \textcolor{blue}{// Build final graph}
\STATE Initialize $E_{\tau^*} = \{(i,j) \mid D_{ij} \leq \tau^*, y_i^{train} = y_j^{valid}\}$
\STATE Set weights $w_{ij} = 1$ for all $(i,j) \in E_{\tau^*}$
\STATE {\bfseries Return:} Edge set $E_{\tau^*}$ with weights $w$ 
\end{algorithmic}
\end{algorithm}


\section{Theoretical Analysis of Coverage-Based Data Value Approximation}\label{app:fast_approx}
We present a theoretical framework for efficiently computing optimal data values through a maximum coverage model approximation. This approach maintains the essential theoretical properties that enable greedy optimality while significantly reducing computational complexity.

\subsection{Coverage Model Framework}
Consider the original utility evaluation problem: given a dataset $\mathcal{D}$ with $n$ training points and a validation set $\mathcal{V}$ with $m$ points, computing the utility $U(S)$ for any subset $S \subseteq \mathcal{D}$ typically requires training and evaluating a model, making it computationally expensive for the dynamic programming approach described in Section 4.

We propose approximating this utility function through a carefully constructed bipartite coverage model. Let us first formalize the setup:

\begin{definition}[Coverage-Based Utility Approximation] 
For any training point $i \in [n]$, we associate a coverage pattern $S_i \subseteq [m]$ representing its influence on validation points. The approximated utility for a subset $S \subseteq [n]$ is defined as:
\[
\hat{U}(S) = \left|\bigcup_{i \in S} S_i\right|
\]
\end{definition}

This reformulation transforms our original utility computation into a structured maximum coverage problem that preserves the essential properties required for optimality while enabling efficient computation.

\subsection{Bipartite Graph Construction}
We construct a weighted bipartite graph $G = (U, V, E, w)$ where $U = [n]$ represents the training points, $V = [m]$ represents the validation points, and $E \subseteq U \times V$ with weights $w: E \rightarrow \mathbb{R}_+$.

For any subset $S \subseteq U$, we define the generalized coverage utility:
\[
\hat{U}(S) = \sum_{v \in V} \min\left\{c_v, \sum_{u \in S} w_{uv}\right\}
\]
where $c_v \in \mathbb{R}_+$ represents the capacity constraint for validation point $v$.

\subsection{Theoretical Properties}
The following theorem establishes that our approximation preserves the essential properties that enabled the optimality of greedy selection in the original problem:

\begin{theorem}\label{thm:coverage_properties}
The approximated utility function $\hat{U}: 2^U \rightarrow \mathbb{R}_+$ satisfies:
\begin{enumerate}
    \item Monotonicity: $\forall A \subseteq B \subseteq U: \hat{U}(A) \leq \hat{U}(B)$
    \item Submodularity: $\forall A \subseteq B \subseteq U, e \in U \setminus B:$
    \[
    \hat{U}(A \cup \{e\}) - \hat{U}(A) \geq \hat{U}(B \cup \{e\}) - \hat{U}(B)
    \]
    \item Prefix-sum Structure: For any permutation $\pi$ of $U$:
    \[
    \hat{U}(\pi) = \sum_{k=1}^n \left|\bigcup_{j=1}^k S_{\pi(j)}\right|
    \]
\end{enumerate}
\end{theorem}

\begin{proof}
For monotonicity, consider any $A \subseteq B \subseteq U$. For each $v \in V$:
\[
\sum_{u \in A} w_{uv} \leq \sum_{u \in B} w_{uv}
\]
Since $\min\{c_v, x\}$ is monotone in $x$, we have:
\[
\min\{c_v, \sum_{u \in A} w_{uv}\} \leq \min\{c_v, \sum_{u \in B} w_{uv}\}
\]
Summing over all $v \in V$ yields $\hat{U}(A) \leq \hat{U}(B)$.

For submodularity, fix any $A \subseteq B \subseteq U$ and $e \in U \setminus B$. For any vertex $v \in V$, the marginal gain is:
\[
\Delta_v(e|S) = \min\{c_v, \sum_{u \in S \cup \{e\}} w_{uv}\} - \min\{c_v, \sum_{u \in S} w_{uv}\}
\]
This function exhibits diminishing returns due to the capacity constraint $c_v$. To see this, observe that for any $S \subseteq T$:
\[
\sum_{u \in S} w_{uv} \leq \sum_{u \in T} w_{uv}
\]
Let $x_S = \sum_{u \in S} w_{uv}$ and $x_T = \sum_{u \in T} w_{uv}$. The marginal gain difference is:
\begin{align*}
\Delta_v(e|S) - \Delta_v(e|T) &= [\min\{c_v, x_S + w_{ev}\} - \min\{c_v, x_S\}] \\
&\quad - [\min\{c_v, x_T + w_{ev}\} - \min\{c_v, x_T\}] \geq 0
\end{align*}
This inequality holds because the $\min$ function is concave, implying that increments decrease as the base value increases. Summing over all $v \in V$ establishes submodularity.

The prefix-sum structure follows directly from the construction through the union operation over coverage sets. For any permutation $\pi$ and position $k$:
\[
\left|\bigcup_{j=1}^k S_{\pi(j)}\right| = \sum_{v \in V} \min\{c_v, \sum_{j=1}^k w_{\pi(j)v}\}
\]
\end{proof}

\subsection{Optimality Analysis}
With these properties established, we can now prove that the greedy algorithm achieves optimality under our coverage approximation:

\begin{theorem}\label{thm:greedy_coverage}
Let $\sigma^g$ be the sequence produced by the greedy algorithm using $\hat{U}$. Then for any other sequence $\pi$:
\[
\hat{U}(\sigma^g) \geq \hat{U}(\pi)
\]
\end{theorem}

\begin{proof}
We prove this through an exchange argument. Let $\Delta(e|S)$ denote the marginal gain of adding element $e$ to set $S$ under $\hat{U}$. Consider any sequence $\pi^*$ with alleged higher utility than $\sigma^g$.

First, we define an inversion in $\pi^*$ as a pair of positions $i < j$ where:
\[
\Delta(\pi^*(i)|A_{i-1}) < \Delta(\pi^*(j)|A_{i-1})
\]
where $A_{i-1} = \{\pi^*(1), ..., \pi^*(i-1)\}$.

For any such inversion, swapping the elements at positions $i$ and $i+1$ produces a sequence $\pi'$ with $\hat{U}(\pi') \geq \hat{U}(\pi^*)$. This follows from three properties:

1. By submodularity, moving the higher-gain element earlier preserves its marginal contribution:
\[
\Delta(\pi^*(j)|A_{i-1}) \leq \Delta(\pi^*(j)|A_{i-2})
\]

2. The prefix-sum structure ensures earlier coverage provides more total contribution.

3. Other elements' contributions remain unchanged or increase due to submodularity.

Through repeated application of such swaps, we can transform $\pi^*$ into a sequence $\tilde{\pi}$ with no inversions, maintaining:
\[
\hat{U}(\tilde{\pi}) \geq \hat{U}(\pi^*)
\]

However, a sequence with no inversions must be the greedy sequence $\sigma^g$, as each position contains the highest-gain available element. This leads to contradiction:
\[
\hat{U}(\sigma^g) = \hat{U}(\tilde{\pi}) \geq \hat{U}(\pi^*) > \hat{U}(\sigma^g)
\]

Therefore, no sequence can achieve higher utility than $\sigma^g$ under our coverage approximation.
\end{proof}




\section{Experimental Details}\label{app:exp_details}
We provide detailed descriptions of our experimental setup, including baseline methods, datasets, and evaluation protocols.

\subsection{Target Model}
In our experiments, the target model used for data valuation is the Logistic Regression model from the Scikit-learn machine learning library. The data valuation process involves repeatedly training this Logistic Regression model on different data subsets to evaluate the marginal contributions of each training sample to the model's test performance, thereby deriving the corresponding data value scores. 


\subsection{Baseline Methods}
We evaluate our approach against the following state-of-the-art data valuation methods:

\begin{itemize}
    \item \texttt{Random}: A naive baseline that randomly selects data points
    \item \texttt{LOO} (Leave-one-out): Measures each point's marginal contribution by removing it from the full dataset
    \item \texttt{Influence Function} \citep{koh2017understanding}: Approximates data influence using model gradients
    \item \texttt{Data Shapley} \citep{ghorbani2019data}: Assigns values based on average marginal contributions across different subset sizes
    \item \texttt{Beta Shapley} \citep{kwon2021beta}: Generalizes Shapley value by relaxing the efficiency axiom    
    \item \texttt{Data Banzhaf} \citep{wang2023data}: Captures influence through binary marginal contributions
    \item \texttt{AME} (Average Marginal Effect) \citep{lin2022measuring}: Estimates influence using sparse regression
    \item \texttt{DVRL} (Data Valuation using Reinforcement Learning) \citep{yoon2020data}: Learns data values through policy optimization 
    \item \texttt{Data-OOB} (Data Out-of-Bag) \citep{kwon2023data}: Uses out-of-bag estimates from bagging models to evaluate data utility
\end{itemize}

All baseline methods are implemented using OpenDataVal \citep{jiang2023opendataval}, ensuring standardized implementation and fair comparison.

\subsection{Dataset Details}
Our experiments use eight diverse datasets from OpenML \citep{feurer2021openml}, spanning tabular, text, and image domains:

\begin{itemize}
    \item \textbf{2dplanes} (ID 727): A synthetic dataset for binary classification
    
    \item \textbf{nomao} (ID 1486) \citep{candillier2012design}: A real-world dataset for duplicate detection
    
    \item \textbf{bbc-embedding} \citep{greene2006practical}: Text classification dataset with news articles
    
    \item \textbf{MiniBooNE} (ID 43974) \citep{roe2005boosted}: Particle physics dataset for binary classification
    
    \item \textbf{digits} \citep{xu1992methods}: Handwritten digit recognition dataset
    
    \item \textbf{election} \citep{DVN/42MVDX_2017}: Election outcome prediction dataset
    
    \item \textbf{electricity} (ID 44080) \citep{gama2004learning}: Time series dataset for price movement prediction
    
    \item \textbf{fried} (ID 901): Synthetic regression dataset
\end{itemize}

These datasets are commonly used in data valuation research \cite{yoon2020data, ghorbani2019data, kwon2023data} and represent a diverse range of learning tasks and data characteristics.

\subsection{Evaluation Protocol Details}
For each dataset, we follow a rigorous evaluation protocol:

\begin{enumerate}
    \item \textbf{Data Splitting}: Each dataset $\mathcal{D}$ is randomly split into:
        \begin{itemize}
            \item Training set $\mathcal{D}_{train}$
            \item Validation set $\mathcal{D}_{valid}$ (for computing utility functions)
            \item Test set $\mathcal{D}_{test}$ (for evaluation)
        \end{itemize}
    
    \item \textbf{Value Assignment}: Data valuation methods compute values for each training point using validation set performance
    
    \item \textbf{Sequential Selection}: 
        \begin{itemize}
            \item Rank training points by assigned values in descending order
            \item Iteratively add points following this ranking
            \item Train model on selected subset at each step
            \item Record test accuracy
        \end{itemize}
    
    \item \textbf{Performance Curves}: Plot test accuracy versus selection size $k \in [1,|\mathcal{D}_{train}|]$
    
    \item \textbf{Experimental Control}:
        \begin{itemize}
            \item All methods limited to 1000 model retraining steps
            \item Results averaged over 20 independent runs
            \item Consistent random seeds used across methods
        \end{itemize}
\end{enumerate}

This protocol follows standard practices in data valuation literature \citep{ghorbani2019data,kwon2021beta,tarun2024ecoval} and ensures fair and comprehensive evaluation of different methods.

\subsection{RQ1 Experimental Settings}\label{app:rq1_exp}
To establish performance bounds and quantify the limitations of existing data valuation methods, we conduct experiments with the following specific settings:

\begin{itemize}
    \item \textbf{Data Sampling}: For each dataset:
        \begin{itemize}
            \item Training set: 20 points
            \item Validation set: 100 points
            \item Test set: 500 points
        \end{itemize}
    
    \item \textbf{Random Seeds}: 20 independent trials using seeds from 10 to 200 with step 10
    
    \item \textbf{Methods Compared}:
        \begin{itemize}
            \item Optimal strategy: \texttt{DynamicProgramming} (Algorithm \ref{app:optimal_algo})
            \item Existing methods: \texttt{Random}, \texttt{LOO}, \texttt{Influence}, \texttt{DataShap}, \texttt{BetaShap}, \texttt{Banzhaf}, \texttt{AME}, \texttt{DVRL}, \texttt{DataOob}
        \end{itemize}
        
    \item \textbf{Evaluation}: Test accuracy evaluated by sequentially adding points based on their assigned values
\end{itemize}

The selection curves are generated by plotting test accuracy against selection size $k$, with results averaged over all trials.

\subsection{RQ3 Experimental Settings}\label{app:rq3_exp}
We evaluate our bipartite graph-based approximation approach \texttt{bipartite} with the following settings:

\begin{itemize}
    \item \textbf{Dataset Splitting}:
        \begin{itemize}
            \item Standard datasets:
                \begin{itemize}
                    \item Training set: 50 points
                    \item Validation set: 50 points
                    \item Test set: 500 points
                \end{itemize}
            \item Large datasets (digits and bbc-embeddings):
                \begin{itemize}
                    \item Training set: 100 points
                    \item Validation set: 100 points
                    \item Test set: 1000 points
                \end{itemize}
        \end{itemize}
    
    \item \textbf{Evaluation Protocol}:
        \begin{itemize}
            \item Model retraining steps: 1000
            \item Independent runs: 20
            \item Metric: Test accuracy vs. selection size
        \end{itemize}
\end{itemize}

For quantitative comparison, detailed performance metrics of \texttt{bipartite} and baselines \texttt{Random}, \texttt{LOO}, \texttt{Influence}, \texttt{DataShap}, \texttt{BetaShap}, \texttt{Banzhaf}, \texttt{AME}, \texttt{DVRL}, \texttt{DataOob} are provided in Table~\ref{tab:selection_results}.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{gmm_evolution.pdf}
    \caption{Geometric visualization of increasing utility curvature through message passing: The feature space distribution of a three-class classification dataset evolves as the propagation proportion increases from $0.0$ to $1.0$. Points within each class progressively converge through feature averaging, illustrating the transition from low to high substitutability regimes predicted by our curvature analysis. }
    \label{fig:gmm_evolution}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{curvature.pdf}
\caption{Performance of game-theoretic data values under increasing utility curvature: Selection curves demonstrate the theoretical relationship between curvature and approximation quality. The degradation in mean accuracy aligns with the $(1-c)^2$ bound as substitutability increases (propagation proportion from 0.0 to 1.0, where higher proportion indicates higher curvature).}
    \label{fig:curvature}
\end{figure*}


\section{Experimental Validation of Curvature Impact}\label{app:curvature_exp}
To empirically validate the theoretical guarantees established in Theorem \ref{thm:submodular} and examine the limitations described in Remark \ref{rem:substitution}, we conduct controlled experiments with systematically varying utility curvature. Our experimental design uses message passing mechanisms to precisely control the degree of data point substitutability and consequently the curvature of the utility function.

Our experimental setup consists of a three-class classification problem where we employ a graph-based message passing scheme that updates each point's features by aggregating information from its within-class neighbors. The aggregation proportion increases from $0.1$ to $1.0$, systematically controlling the curvature $c$ of the utility function. As visualized in Figure \ref{fig:gmm_evolution}, when the proportion is small, points maintain distinctive features, corresponding to low curvature ($c$ near $0$). As the proportion increases, points within each class become increasingly similar through feature averaging, leading to high curvature ($c$ approaching $1$). 

The experimental results in Figure \ref{fig:curvature} validate our theoretical analysis in three aspects. First, under low curvature (proportion $\leq 0.3$), all methods achieve strong performance with BetaShapley obtaining a mean accuracy of $0.706$, followed by Banzhaf ($0.720$) and Shapley ($0.738$). This aligns with the $(1-c)^2$ approximation guarantee from Theorem \ref{thm:submodular} when $c$ is small. Second, as curvature increases (proportion $\geq 0.5$), we observe significant performance degradation across all methods, with mean accuracies dropping to approximately $0.59$ at proportion $1.0$. This deterioration empirically confirms the quadratic decay in our theoretical bound as $c$ approaches $1$. Third, the convergence in performance between different methods at high curvature validates our analysis that all game-theoretic approaches face similar fundamental limitations under strong substitution effects. 


\section{Detailed Analysis of Data Selection Performance of \texttt{Bipartite} and Baselines}\label{app:selection_performance}
To quantify the selection performance improvements achieved by our bipartite graph-based approach \texttt{Bipartite}, we present detailed numerical results comparing against baseline methods \texttt{Random}, \texttt{LOO}, \texttt{Influence}, \texttt{DataShap}, \texttt{BetaShap}, \texttt{Banzhaf}, \texttt{AME}, \texttt{DVRL}, and \texttt{DataOob}. For each dataset and method, we compute the mean test accuracy and standard deviation across 20 independent runs, with selection budgets of 50 points (100 for digits and bbc-embeddings). The experimental protocol follows Section~\ref{sec:rq4_selection}, with training sets of size 50 (100 for digits and bbc-embeddings), validation sets of equal size, and test sets of 500 points (1000 for digits and bbc-embeddings).


\begin{table*}[t]
\caption{Mean test accuracy (\textpm standard deviation) of different data selection methods across 20 independent runs. Best mean results for each dataset are shown in \textbf{bold}.}

\label{tab:selection_results}
\centering
\small
\vspace{1em}
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{l|cccccccccc}
\toprule
\texttt{Dataset} & \texttt{Random} & \texttt{LOO} & \texttt{Influence} & \texttt{DataShap} & \texttt{BetaShap} & \texttt{Banzhaf} & \texttt{AME} & \texttt{DVRL} & \texttt{DataOob} & \texttt{Bipartite} \\
\midrule
2dplanes & 0.723 & 0.720 & 0.729 & 0.740 & \textbf{0.747} & 0.729 & 0.724 & 0.705 & 0.734 & 0.745 \\
 & (\textpm 0.098) & (\textpm 0.103) & (\textpm 0.109) & (\textpm 0.106) & (\textpm 0.094) & (\textpm 0.113) & (\textpm 0.117) & (\textpm 0.120) & (\textpm 0.090) & (\textpm 0.069) \\
nomao & 0.812 & 0.806 & 0.788 & 0.815 & 0.815 & 0.782 & 0.778 & 0.788 & 0.682 & \textbf{0.852} \\
 & (\textpm 0.148) & (\textpm 0.171) & (\textpm 0.189) & (\textpm 0.163) & (\textpm 0.158) & (\textpm 0.194) & (\textpm 0.203) & (\textpm 0.179) & (\textpm 0.252) & (\textpm 0.088) \\
bbc-embed & 0.805 & 0.803 & 0.743 & 0.790 & 0.809 & 0.711 & 0.803 & 0.560 & 0.561 & \textbf{0.878} \\
 & (\textpm 0.228) & (\textpm 0.246) & (\textpm 0.284) & (\textpm 0.261) & (\textpm 0.240) & (\textpm 0.312) & (\textpm 0.251) & (\textpm 0.334) & (\textpm 0.337) & (\textpm 0.146) \\  
MiniBooNE & 0.722 & 0.703 & 0.678 & 0.711 & 0.731 & 0.679 & 0.691 & 0.686 & 0.726 & \textbf{0.754} \\
 & (\textpm 0.092) & (\textpm 0.100) & (\textpm 0.107) & (\textpm 0.093) & (\textpm 0.086) & (\textpm 0.109) & (\textpm 0.108) & (\textpm 0.108) & (\textpm 0.077) & (\textpm 0.066) \\
digits & 0.619 & 0.680 & 0.646 & 0.649 & 0.647 & 0.610 & 0.662 & 0.419 & 0.338 & \textbf{0.764} \\
 & (\textpm 0.342) & (\textpm 0.318) & (\textpm 0.346) & (\textpm 0.335) & (\textpm 0.332) & (\textpm 0.361) & (\textpm 0.324) & (\textpm 0.366) & (\textpm 0.349) & (\textpm 0.245) \\
election & 0.566 & 0.537 & 0.555 & 0.580 & 0.578 & 0.539 & 0.586 & 0.563 & 0.263 & \textbf{0.601} \\
 & (\textpm 0.224) & (\textpm 0.245) & (\textpm 0.233) & (\textpm 0.200) & (\textpm 0.198) & (\textpm 0.237) & (\textpm 0.202) & (\textpm 0.218) & (\textpm 0.228) & (\textpm 0.182) \\
electricity & 0.639 & 0.627 & 0.623 & 0.650 & 0.656 & 0.628 & 0.632 & 0.636 & 0.638 & \textbf{0.669} \\
 & (\textpm 0.082) & (\textpm 0.081) & (\textpm 0.085) & (\textpm 0.078) & (\textpm 0.065) & (\textpm 0.087) & (\textpm 0.086) & (\textpm 0.087) & (\textpm 0.072) & (\textpm 0.063) \\
fried & 0.702 & 0.698 & 0.700 & 0.710 & 0.719 & 0.707 & 0.702 & 0.702 & 0.719 & \textbf{0.741} \\
 & (\textpm 0.087) & (\textpm 0.099) & (\textpm 0.108) & (\textpm 0.103) & (\textpm 0.095) & (\textpm 0.106) & (\textpm 0.109) & (\textpm 0.098) & (\textpm 0.085) & (\textpm 0.065) \\
\midrule
Average & 0.255 & 0.255 & 0.253 & 0.043 & 0.251 & 0.251 & 0.250 & 0.041 & 0.251 & \textbf{0.041} \\
Std Dev & 0.139 & 0.139 & 0.138 & 0.042 & 0.139 & 0.139 & 0.138 & 0.043 & 0.139 & \textbf{0.043} \\
\bottomrule
\end{tabular}%
}
\end{table*}

The quantitative results reinforce our observations from the selection curves in Section~\ref{sec:rq4_selection}. Our method \texttt{Bipartite} achieves the highest accuracy across all datasets, with particularly substantial improvements on complex datasets like bbc-embeddings (0.878 accuracy, compared to 0.809 for the best baseline) and digits (0.764 accuracy versus 0.680 for the next best method). Even on datasets where baseline methods perform relatively well, such as nomao and MiniBooNE, our approach \texttt{Bipartite} still maintains a clear performance advantage while demonstrating more stable behavior across different runs. These comprehensive results validate our theoretical analysis that the bipartite graph approximation effectively preserves the essential structure of the utility function while enabling efficient computation. The consistent improvements across diverse datasets demonstrate that our method \texttt{Bipartite} successfully addresses both the computational challenges and modeling limitations of existing approaches, particularly in scenarios requiring efficient early-stage selection.


\end{document}