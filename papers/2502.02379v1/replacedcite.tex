\section{Related Work}
\label{sec:related}

Here, we briefly contextualize our contributions, deferring a deeper discussion of related work to \cref{apx:related-work}.

Relating to \emph{mode perturbations} and \emph{performance separability}, 
____ create GNN experiments to improve reproducibility, and ____ show that GNNs use the graph structure even when it is not conducive to a task (i.e., $\eg$ separably outperforms $\id$). 
\framework is inspired by, and goes beyond, these works, 
providing a general framework for perturbation-based dataset evaluation. 

Connecting with \emph{mode complementarity}, 
researchers have been particularly interested in the effects of \emph{homo- and heterophily}
on node-classification performance  ____. 
While homophily characterizes the \emph{task-dependent} relationship between graph structure and \emph{node labels}, 
mode complementarity assesses the \emph{task-independent} relationship between graph structure and \emph{node features}. 
For node classification, 
____ propose the edge signal-to-noise ratio (ESNR), 
____ develop a subspace alignment measure (SAM), 
and ____ analyze relations between \emph{node features} and \emph{graph structure}~(FvS).
However, with mode complementarity, we craft a score that 
\begin{inparaenum}[(1)]
	\item treats graph structure and node features as equal (unlike ESNR), 
  \item works on graphs without node labels and does not make assumptions about the spaces arising from edge connectivity and node features (unlike SAM, FvS), and 
	\item specifically informs graph-level learning tasks (unlike all of these works).
\end{inparaenum}
 
\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{Figures/mode-performance_best-only_accuracy-auroc_pi-095}
	\caption{\textbf{Comparing \emph{GNN performance} across different versions of the same dataset.} 
		We show the mean (dot) and $95$th percentile interval (bars) of accuracy and AUROC across $100$ runs of the best (as measured by the respective performance mean) among our tuned GAT, GCN, and GIN models,
		for the original version and $5$ perturbations of $13$ graph-learning datasets.
		Black resp. silver horizontal lines show best mean trained resp. untrained performance on the original data. 
		For Reddit-M, the complete-graph and complete-features perturbations failed to train due to memory problems, 
		but the existing results already allow us to conclude that this dataset lacks performance separability.
		\label{fig:performance-separability}}
\end{figure*}

\begin{table*}[!t]
	\centering \small
	\begin{tabular}{llllll}
\toprule
Dataset & Accuracy & AUROC & Structure & Features & Evaluation \\
\midrule
AIDS & cf $>$ cg $>$ rg $>$ o $>$ eg $>$ rf & cf/cg/rg $>$ eg/o $>$ rf & uninformative & uninformative & \texttt{--} \\
COLLAB & o $>$ cg/rg $>$ eg $>$ cf/rf & o $>$ cg/rg $>$ eg $>$ rf $>$ cf & informative & informative & \texttt{++} \\
DD & rg $>$ cg $>$ cf $>$ eg/o $>$ rf & rg $>$ cf/cg/eg/o/rf & uninformative & uninformative & \texttt{--} \\
Enzymes & eg $>$ cg/o $>$ rg $>$ cf $>$ rf & eg $>$ o $>$ cg $>$ rg $>$ cf $>$ rf & uninformative & informative & \texttt{-} \\
IMDB-B & cf/cg/eg/o/rf/rg & o $>$ cg $>$ eg/rg $>$ rf $>$ cf & (un)informative & (un)informative & $\circ$ \\
IMDB-M & cg/eg/o/rg $>$ cf $>$ rf & o $>$ cg/eg/rg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
MUTAG & cf/cg/o/rg $>$ eg $>$ rf & cf/o $>$ cg/eg/rf/rg & (un)informative & uninformative & \texttt{-} \\
MolHIV & o $>$ cf/cg/rg $>$ rf $>$ eg & o $>$ cg $>$ cf $>$ rg $>$ eg $>$ rf & informative & informative & \texttt{++} \\
NCI1 & o $>$ cg $>$ cf $>$ rg $>$ eg $>$ rf & o $>$ cg $>$ cf $>$ rg $>$ eg/rf & informative & informative & \texttt{++} \\
Peptides & o $>$ cg $>$ rg $>$ eg $>$ cf $>$ rf & cg $>$ o $>$ rg $>$ eg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
Proteins & cf $>$ cg/rf $>$ eg/o/rg & cf/cg/eg/o/rf/rg & uninformative & uninformative & \texttt{--} \\
Reddit-B & cf $>$ rg $>$ o $>$ cg $>$ eg/rf & rg $>$ cf $>$ o $>$ cg/eg/rf & uninformative & uninformative & \texttt{--} \\
Reddit-M & rf $>$ rg $>$ o $>$ eg & rg $>$ rf $>$ o $>$ eg & uninformative & uninformative & \texttt{--} \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Measuring \emph{performance separability} between different versions of the same dataset.}
		To quantify the conclusions from \cref{fig:performance-separability} and further account for performance \emph{distributions} (\cref{def:evaluating-tuned-models}), 
		we use permutation tests with $10\,000$ random permutations and the Kolmogorov-Smirnov (KS) statistic as our test statistic at an $\alpha$-level of $0.01$, Bonferroni-corrected for multiple hypothesis testing within each individual dataset. 
		Here, for sets $S_1$ and $S_2$, $S_1 > S_2$ denotes that the elements in the set $S_1$ separably outperform the elements in the set $S_2$ 
		(i.e., the pairwise distances between $s_1$ and $s_2$ are statistically significantly different for all $s_1\in S_1$ and $s_2\in S_2$), 
		and we represent the sets in condensed notation, concatenating elements with ``/
		''. 
		We see that original datasets are often separably outperformed by their perturbed variants. 
	}\label{tab:performance-separability-ks}
\end{table*}