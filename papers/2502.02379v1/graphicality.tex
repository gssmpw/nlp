\documentclass{article}

\PassOptionsToPackage{sort}{natbib}

\usepackage[accepted]{icml2025}

\usepackage         {amsmath}
\usepackage         {amssymb}
\usepackage         {amsthm}
\usepackage[english]{babel}
\usepackage         {booktabs}
\usepackage         {dsfont}
\usepackage         {graphicx}
\usepackage         {mathtools}
\usepackage         {mleftright}
\usepackage         {microtype}
\usepackage         {nicefrac}
\usepackage         {paralist}
\usepackage         {enumitem}
\usepackage         {subcaption}
\usepackage         {thmtools, thm-restate}
\usepackage         {xspace}
\usepackage         {hyperref}
\bibliographystyle{icml2025}

\usepackage{multirow}



\usepackage[capitalize,noabbrev]{cleveref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\crefname{equation}{Eq.}{Eqs.}



\usepackage{tikz}
\usepackage{tikz-cd}

\definecolor{bleu}     {RGB}{ 49,140,231}
\definecolor{cardinal} {RGB}{196, 30, 58}
\definecolor{emerald}  {RGB}{ 80,200,120}
\definecolor{lightgrey}{RGB}{230,230,230}
\definecolor{grey}     {RGB}{180,180,180}




\newcommand{\Todo}[1]{\textcolor{red}{#1}\xspace}
\newcommand{\framework}{\textsc{Rings}\xspace}

\newcommand{\metricspace}{\ensuremath{D}\xspace}
\newcommand{\toyspace}{\ensuremath{Y}\xspace}
\newcommand{\reals}      {\mathds{R}\xspace}
\newcommand{\point}[1]   {\mathbf{#1}\xspace}

\DeclareMathOperator{\comp}        {\mathcal{C}}
\DeclareMathOperator{\diam}        {diam}
\DeclareMathOperator{\diagram}     {Dgm}
\DeclareMathOperator{\distortion}  {dis}
\DeclareMathOperator{\dist}        {d}
\DeclareMathOperator{\distB}       {\dist_{B}}
\DeclareMathOperator{\distF}       {\dist_{F}}
\DeclareMathOperator{\distS}       {\dist_{S}}
\DeclareMathOperator{\distGH}      {\dist_{GH}}
\DeclareMathOperator{\lift}        {\mathcal{L}}
\DeclareMathOperator{\graphicality}{\mathfrak{g}}



\newcommand{\features}{\ensuremath{X}\xspace}
\newcommand{\featured}{\ensuremath{k}\xspace}

\newcommand{\graph}{\ensuremath{G}\xspace}
\newcommand{\nodes}{\ensuremath{V}\xspace}
\newcommand{\edges}{\ensuremath{E}\xspace}
\newcommand{\numnodes}{\ensuremath{n}\xspace}
\newcommand{\numedges}{\ensuremath{m}\xspace}   

\DeclareMathOperator{\complementarity}{\gamma}
\DeclareMathOperator{\distinguishability}{\mathfrak{D}}
\DeclareMathOperator{\Separability}{\xi^Z_{\TestMetric}\xspace}


\DeclareMathOperator{\projection}  {proj}
\DeclareMathOperator{\vietoris}    {VR}

\newcommand{\score}{graphicality\xspace}
\newcommand{\Score}{Graphicality\xspace}




\DeclareMathOperator{\onepointspace}{\{ x\}\xspace}
\DeclareMathOperator{\allones}{\mathbf{1}\xspace}
\DeclareMathOperator{\I}{\mathbf{I}\xspace}
\DeclareMathOperator{\allzeros}{\mathbf{0}\xspace}

\DeclareMathOperator{\PerturbGraph}{\varphi\xspace}
\DeclareMathOperator{\PerturbGraphPrime}{\phi^{'}\xspace}
\DeclareMathOperator{\PerturbDataset}{\Phi \xspace}
\DeclareMathOperator{\PerturbDatasetPrime}{\Phi^{'}\xspace}

\DeclareMathOperator{\id}{\PerturbGraph_{o}\xspace}
\DeclareMathOperator{\ID}{\PerturbDataset{o}\xspace}
\DeclareMathOperator{\ef}{\PerturbGraph_{ef}\xspace}
\DeclareMathOperator{\eg}{\PerturbGraph_{eg}\xspace}
\DeclareMathOperator{\EF}{\PerturbDataset_{ef}\xspace}
\DeclareMathOperator{\EG}{\PerturbDataset_{eg}\xspace}



\DeclareMathOperator{\alledges}{\scalebox{0.85}{$\left( \genfrac{}{}{0pt}{}{V}{2} \right)$}}
\DeclareMathOperator{\cf}{\PerturbGraph_{cf}\xspace}
\DeclareMathOperator{\cg}{\PerturbGraph_{cg}\xspace}
\DeclareMathOperator{\CF}{\PerturbDataset_{cf}\xspace}
\DeclareMathOperator{\CG}{\PerturbDataset_{cg}\xspace}

\DeclareMathOperator{\rf}{\PerturbGraph_{rf}\xspace}
\DeclareMathOperator{\rg}{\PerturbGraph_{rg}\xspace}
\DeclareMathOperator{\RF}{\PerturbDataset{rf}\xspace}
\DeclareMathOperator{\RG}{\PerturbDataset{rg}\xspace}



\DeclareMathOperator{\RandomizeFeatures}{\mathcal{R}_{F}\xspace}
\DeclareMathOperator{\RandomizeStructure}{\mathcal{R}_{S}\xspace}


\DeclareMathOperator{\TunedModel}{\mathcal{M}\xspace}
\DeclareMathOperator{\TunedModelPrime}{\mathcal{M}^{'}\xspace}
\DeclareMathOperator{\PerturbModel}{\TunedModel_{\PerturbGraph}\xspace}


\DeclareMathOperator{\Architecture}{\mathcal{A}\xspace}
\DeclareMathOperator{\TrainData}{\mathcal{D}\xspace}
\DeclareMathOperator{\params}{\theta\xspace}

\DeclareMathOperator{\TestMetric}{f\xspace}
\DeclareMathOperator{\TrainConditions}{\zeta\xspace}


\DeclareMathOperator{\ModePerformance}{\TestMetric(\PerturbModel; \TrainConditions)\xspace}

\DeclareMathOperator{\CompareDist}{\mathbb{D}\xspace}




\DeclareMathOperator{\Lift}{\mathcal{L}\xspace}
\DeclareMathOperator{\LiftStructure}{\Lift_{S}\xspace}
\DeclareMathOperator{\LiftFeatures}{\Lift_{F}\xspace}
\DeclareMathOperator{\StructureDist}{\dist_{S}\xspace}
\DeclareMathOperator{\FeatureDist}{\dist_{F}\xspace}

\DeclareMathOperator{\DiffusionDist}{\dist_{S_t}\xspace}
\DeclareMathOperator{\DiffusionMap}{\Psi_t\xspace}
\DeclareMathOperator{\DiffusionDim}{s\xspace}





\DeclareMathOperator{\Partition}{\mathcal{P}\xspace}
 

\usepackage[textsize=tiny]{todonotes}

\setuptodonotes{
color       = white,
  bordercolor = cardinal,
  textcolor   = cardinal,
}

\hypersetup{
  colorlinks,
  linkcolor = bleu,
  citecolor = bleu,
  urlcolor  = bleu,
}

\makeatletter \tikzstyle{notestyleraw}=[
  draw        = \@todonotes@currentbordercolor,
  fill        = \@todonotes@currentbackgroundcolor,
  text        = \@todonotes@currenttextcolor,
  line width  = 0.5pt,
  text width  = \@todonotes@textwidth-1.6ex-1pt,
  inner sep   = 0.8ex
]

\renewcommand{\ICML@appearing}{Preprint. Under Review.}
\makeatother


\icmltitlerunning{No Metric to Rule them All: Toward Principled Evaluations of Graph-Learning Datasets}

\begin{document}

\twocolumn[
\icmltitle{No Metric to Rule Them All: \\
Toward Principled Evaluations of Graph-Learning Datasets}
\icmlsetsymbol{first}{*}

\begin{icmlauthorlist}
\icmlauthor{Corinna Coupette}{first,aalto,mpi}
\icmlauthor{Jeremy Wayland}{first,helmholtz,tum}
\icmlauthor{Emily Simons}{helmholtz,tum}
\icmlauthor{Bastian Rieck}{helmholtz,tum,fri}
\end{icmlauthorlist}


\icmlaffiliation{helmholtz}{Helmholtz Munich, Germany}
\icmlaffiliation{tum}{TU Munich, Germany}
\icmlaffiliation{fri}{University of Fribourg, Switzerland}
\icmlaffiliation{aalto}{Aalto University, Finland}
\icmlaffiliation{mpi}{Max Planck Institute for Informatics, Germany}

\icmlcorrespondingauthor{Corinna Coupette}{corinna.coupette@aalto.fi}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

 \printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
	Benchmark datasets have proved pivotal to the success of graph learning, and \emph{good} benchmark datasets are crucial to guide the development of the field. 
Recent research has highlighted problems with graph-learning datasets and benchmarking practices—revealing, for example, that methods which ignore the graph structure can outperform graph-based approaches on popular benchmark datasets. 
Such findings raise two questions: (1) What makes a good graph-learning dataset, and (2) how can we evaluate dataset quality in graph learning?
Our work addresses these questions. 
As the classic evaluation setup uses datasets to evaluate models, it does not apply to dataset evaluation.
Hence, we start from first principles. 
Observing that graph-learning datasets uniquely combine two modes—the graph structure and the node features—, we introduce \textsc{Rings}, a flexible and extensible \emph{mode-perturbation framework} to assess the quality of graph-learning datasets based on \emph{dataset ablations}—i.e., 
by quantifying differences between the original dataset and its perturbed representations. 
Within this framework, we propose two measures—\emph{performance separability} and \emph{mode complementarity}—as evaluation tools, each assessing, from a distinct angle, the capacity of a graph dataset to benchmark the power and efficacy of graph-learning methods. 
We demonstrate the utility of our framework for graph-learning dataset evaluation in an extensive set of experiments and derive actionable recommendations for improving the evaluation of graph-learning methods. 
Our work opens new research directions in data-centric graph learning, and it constitutes a first step toward the systematic \emph{evaluation of evaluations}. 
 \end{abstract}

\newpage

\section{Introduction}
\label{sec:introduction}

Over the past decade, graph learning has established itself as a prominent approach to making predictions from relational data, 
with remarkable success in areas from small molecules \cite{stokes2020deep,fang2022geometry} to large social networks \cite{ying2018graph,sharma2024recommenders}.
Despite significant progress on the theory of graph neural networks \cite{morris2024position}, however, 
many empirical intricacies of graph-learning tasks, models, and datasets remain poorly understood. 
For example, 
recent research has revealed that
\begin{inparaenum}[(1)]
	\item purported performance gaps disappear with proper hyperparameter tuning \cite{Tonshoff2023}, 
	\item popular graph-learning datasets occupy a very peculiar part of the space of all possible graphs \cite{palowitch2022graphworld}, 
	\item some graph-learning tasks can be solved without using the graph structure \cite{Errica20a}, and
	\item graph-learning models struggle to ignore the graph structure when the features alone are sufficiently informative for the task at hand \cite{Bechler-Speicher23a}. 
\end{inparaenum}
These findings suggest a need for better infrastructure to assess graph-learning methods, 
supporting rigorous evaluations that paint a realistic picture of the progress made by the community. 

\textbf{Necessity and Challenges of Dataset Evaluation.} Benchmark datasets play a key role in the evaluation of graph-learning methods, 
but the results cited above highlight that not all (collections of) graphs are equally suitable for that purpose. 
This motivates us to \emph{flip the script} on graph-learning evaluation, 
asking how well graph-learning datasets can characterize the capabilities of graph-learning methods, 
rather than how well these methods can solve tasks on graph-learning datasets. 
Our work is guided by two questions:
\begin{enumerate}[nosep,label=\bfseries Q\arabic*]
	\item What characterizes a good graph-learning dataset?
	\item How can we evaluate dataset quality in graph learning?
\end{enumerate}
Addressing these questions is not straightforward. 
First, the classic evaluation setup, which compares performance across models while \emph{holding the dataset constant}, cannot be used to evaluate datasets. 
Second, comparing performance levels across datasets while \emph{holding the model constant} yields measurements that are confounded by model capabilities. 
Third, while performance levels indicate the \emph{difficulty} of a dataset for existing methods, 
these levels provide little information about dataset \emph{quality}:  
A difficult dataset of high quality may guide the field toward methodological innovation, 
but a difficult dataset of low quality may detract from real progress. 
Hence, our work starts from first principles. 

\textbf{Desirable Properties of Graph-Learning Datasets.} 
We observe that attributed graphs combine two types of information,
the \emph{graph structure} and the \emph{node features}. 
Graph-learning methods leverage both of these \emph{modes} to tackle a given learning task.\footnote{Notably, to be amenable to graph learning, even non-attributed graphs need to be assigned node features (e.g., one-hot encodings).
}
This suggests the following \emph{desirable property} for a dataset to reveal powerful insights into the capabilities of graph-learning methods, given a specific task:
\begin{enumerate}[nosep,label=\bfseries P0]
	\item The graph structure and the node features should contain \emph{complementary} \emph{task-relevant} information. \label{item:p0}
\end{enumerate}
Assessing whether this property is present poses theoretical and practical challenges---not only due to the limitations of existing graph-learning methods but also because the relationship between task relevance and complementarity is potentially complicated. 
However, we can identify the following \emph{necessary conditions} for \ref{item:p0} to be satisfied:
\begin{enumerate}[nosep,label=\bfseries P\arabic*]
	\item The graph structure and the node features should both contain \emph{task-relevant} information. \label{item:p1}
	\item The graph structure and the node features should contain \emph{complementary} information. \label{item:p2}
\end{enumerate}
Notably, while \ref{item:p1} is \emph{task-dependent}, 
\ref{item:p2} is \emph{task-independent}.

\textbf{Principled Evaluations via Mode Perturbations.} 
Both \ref{item:p1} and \ref{item:p2} address the \emph{relationship between the different modes} of a graph-learning dataset. 
Therefore, 
to test datasets for these properties, 
we propose \framework (Relevant Information in Node features and Graph Structure), 
a dataset-evaluation framework based on the concept of \emph{mode perturbation}. 
As illustrated in \cref{fig:mode-overview}, a mode perturbation maps an attributed graph $(G,X)$ to an attributed graph $(G',X')$, 
replacing the original edge set or feature set with a modified version 
according to a given transformation (e.g., randomization). 
This allows us to make measurements on both $(G,X)$ and $(G',X')$. 
Given appropriate measures, 
the difference between the resulting measurements can then provide insights into \ref{item:p1} and \ref{item:p2}. 
In analogy to model ablations in the evaluation of graph-learning methods, 
mode perturbations can also be thought of as \emph{dataset ablations}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Figures/mode-overview}
	\caption{\textbf{Overview of our main mode perturbations.}
		Given a ring of cliques on $12$ nodes (top left panel) with $12$-dimensional node features (bottom left panel), we show our input data for the original and perturbed states of the graph structure (top row) and the features (bottom row). 
	}\label{fig:mode-overview}
\end{figure}

\textbf{Our Contributions.}
We make four main contributions:
\begin{enumerate}[nosep,label=\bfseries C\arabic*]
	\item \textbf{Framework.} We develop \framework, a flexible and extensible framework to assess the quality of graph-learning datasets by quantifying differences between the original dataset and its perturbed representations.  \label{item:c1}
	\item \textbf{Measures.} As part of \framework, 
	we introduce two measures for evaluating graph datasets based on mode perturbations: 
	\emph{performance separability}, addressing \ref{item:p1}, and \emph{mode complementarity}, addressing \ref{item:p2}.\label{item:c2}
	\item \textbf{Experiments.} We demonstrate the value of our framework and measures through extensive experiments on real-world graph-classification datasets.\label{item:c3}
	\item \textbf{Recommendations.} Based on  \ref{item:c1}--\ref{item:c3}, we derive concrete recommendations for improving benchmarking and evaluation practices in graph learning. \label{item:c4}
\end{enumerate}
Our work opens new research directions in data-centric graph learning, 
and it constitutes a first step toward our long-term vision of enabling \emph{evaluations of evaluations}:
systematic assessments of the quality of evidence provided for the performance of new graph-learning methods.

\textbf{Structure.}
In \cref{sec:methods}, 
we formally introduce \framework, our mode-perturbation framework for evaluating graph-learning datasets, 
along with our proposed dataset quality measures. 
Having reviewed related work in \cref{sec:related}, 
we demonstrate the practical utility of our framework through extensive experiments in \cref{sec:experiments}.
We discuss conclusions, limitations, and avenues for future work in \cref{sec:discussion}.
Detailed supplementaries are provided in \cref{apx:theory,apx:experiments,apx:datasets,apx:related-work,apx:methods}.

 
\section{The \framework Framework}
\label{sec:methods}

After establishing our notation,
we develop \framework, 
our framework for evaluating graph-learning datasets. 
We do so in three steps, 
intuitively introducing and formally defining \emph{mode perturbations} (\ref{sec:mode-perturbations}), 
\emph{performance separability} (\ref{sec:performance-separability}), 
and \emph{mode complementarity} (\ref{sec:mode-complementarity}). 

\textbf{Preliminaries.}
We work with attributed graphs $(G,X)$, 
where $G=(V,E)$ is a graph with $n = |V|$ nodes and $m = |E|$ edges, 
$\features\subset\reals^{k}$ is the space of $k$-dimensional node features, 
and we assume w.l.o.g.\ that ${|\features| = n}$. 
For graph-level tasks, we have datasets ${\TrainData = \{G_1,\dots,G_N\}}$, 
where $N$ is the total number of graphs. 
Given a set $S$, $2^S$ is its power set, and $\binom{S}{\ell}$ is the set of all $\ell$-element subsets of $S$. 
Multisets are denoted by $\{\{\cdot\}\}$, and the set of positive integers no greater than $\ell$ is written as ${[\ell] = \{i\in\mathbb{Z}\mid 0 < i \leq \ell\}}$. 

\subsection{Mode Perturbations}\label{sec:mode-perturbations}

Given an attributed graph $(G,X)$, 
both data modes---i.e., the graph structure and the node features---are naturally associated with metric spaces that encode pairwise distances between nodes (i.e., distance matrices\footnote{
Deferring further details to \cref{apx:theory}, 
we note that for finite metric spaces, we have $(\toyspace,\dist) \cong \metricspace \in \reals^{n \times n}$, 
i.e., a finite matrix encoding pairwise distances between elements in $\toyspace$. 
We use $D$ in contexts that rely on matrix operations, and $(Y,\dist)$ to emphasize both the original space and the associated metric.
}). 
In our \framework framework, 
we modify these metric spaces to reveal information about the quality of graph-learning datasets. 
This idea is formalized in the notion of \emph{mode perturbation}.

\begin{restatable}[Mode Perturbation]{definition}{ModePerturbationDef}
A mode perturbation $\PerturbGraph$ is a map between attributed graphs
such that $\PerturbGraph\colon (\graph, \features) \mapsto (\graph',\features')$.
\end{restatable}

This definition is very general, allowing $\PerturbGraph$, inter alia, 
to act on \emph{both} the graph structure \emph{and} the node features.
For the purposes of understanding the connection between graph structure and node features in graph-learning datasets, 
however, 
we focus on mode perturbations that modify \emph{either} the graph structure \emph{or} the node features, 
as illustrated in \cref{fig:mode-overview}. 
We start by formalizing our \emph{feature perturbations}.

\begin{restatable}[Feature Perturbations]{definition}{FeaturePerturbsDef} \label{def:feature-perturbations}
Given an attributed graph $(G,X)$ on $n$ nodes with features $X\subset \reals^k$, we define:
	\begin{align*}
		\ef &: (G,\features) \mapsto (G,\allzeros_n)&\text{[empty features]}\\
		\cf &: (G,\features) \mapsto (G, \I_n)&\text{[complete features]}\\
		\rf &: (G,\features) \mapsto (G,\RandomizeFeatures(\features))&\text{[random features]}
	\end{align*}
Here, $\RandomizeFeatures:\reals^{n\times k} \to \reals^{n\times k' }$ randomizes the features $X$.
\end{restatable}

We can define a matching set of \emph{structural perturbations} by modifying our original edge set.
\begin{restatable}[Structural Perturbations]{definition}{StructurePerturbsDef} \label{def:structure-perturbations}
	Given an attributed graph $(G,X)$ with edge set $E$, we define:
	\begin{align*}
		\eg &: (G,X) \mapsto \left((V,\emptyset),X\right)&\text{[empty graph]}\\
		\cg &: (G,X) \mapsto \left(\left(V,\alledges\right),X\right)&\text{[complete graph]} \\
		\rg &: (G,X) \mapsto \left((V,\RandomizeStructure(E)),X\right)&\text{[random graph]}
	\end{align*}
Here, $\RandomizeStructure:2^{\binom{V}{2}} \to 2^{\binom{V}{2}}$ randomizes the edge set $E$. 
\end{restatable}

For consistency, we also define the \emph{original perturbation} $\varphi_{\text{o}}: (G,X)\mapsto (G,X)$ [original].

To modify entire \emph{collections} of graphs, 
we apply mode perturbations element-wise to all graphs in a given collection.

\begin{restatable}[Dataset Perturbation]{definition}{DatasetPerturbDef}\label{def:dataset-perturbation}
	Given a dataset $\TrainData$ of attributed graphs, a \emph{dataset perturbation} is given by 
	$\PerturbDataset(\TrainData) := \{ \PerturbGraph(G,X) : (G,X) \in \TrainData\}$.
\end{restatable}

\subsection{Performance Separability} \label{sec:performance-separability}

By systematically applying mode perturbations to a dataset $\TrainData$, 
we create a \emph{set of datasets} that contains several different versions of $\TrainData$, 
capturing potentially interesting variation. 
This variation can be leveraged to investigate the properties derived in \cref{sec:introduction}. 
First addressing \ref{item:p1}, 
we now introduce \emph{performance separability} as a measure to assess the extent to which both the graph structure and the node features of an attributed graph contain task-relevant information. 

Intuitively, given two perturbations $\PerturbDataset$, $\PerturbDataset'$ of a dataset $\TrainData$ as well as a task to be solved on $\TrainData$, 
performance separability measures the \emph{distance between performance distributions} associated with $\PerturbDataset(\TrainData)$ and $\PerturbDataset'(\TrainData)$. 
For a formal definition, 
we need notation describing these distributions. 

\begin{restatable}[Tuned Model]{definition}{TunedModelDef} \label{def:tuned-model}
A tuned model $\TunedModel$ is a triple
	$\TunedModel\coloneqq (\TrainData,\Architecture,\params)$, 
where $\TrainData$ represents the dataset and associated task, $\Architecture$ is the architecture used during training, and $\params$ denotes the tuned parameters for the specific architecture. 
\end{restatable}

To elucidate the relationship between the graph structure and the node features as it pertains to performance, 
within \framework, 
we tune models not only on datasets $\TrainData$ but also on perturbed datasets $\PerturbDataset(\TrainData)$.  

\begin{restatable}[Tuned Perturbed Model]{definition}{PerturbedModelDef} \label{def:perturbed-model}
For a mode perturbation $\PerturbGraph$, 
$\PerturbModel \coloneqq (\PerturbDataset(\TrainData),\Architecture,\params)$
denotes a model tuned to solve the task associated with $\TrainData$ under mode perturbation~$\PerturbGraph$.	
\end{restatable}

The performance distributions underlying our notion of performance separability can then be defined as follows.

\begin{restatable}[Empirical Performance Distribution]{definition}{PerformanceDistDef} \label{def:evaluating-tuned-models}
For a tuned (perturbed) model $\PerturbModel$, 
given an evaluation metric $\TestMetric$ and a set of initialization conditions $Z$,
the \emph{empirical performance distribution} $P_{\PerturbGraph}^{\TestMetric,Z}$ of $\PerturbModel$ is the distribution associated with performance measurements
\begin{equation}
	\{\{\TestMetric(\PerturbModel; \TrainConditions) \mid \TrainConditions\in Z\}\}\;.
\end{equation} 
\end{restatable}

With \emph{performance separability}, 
we now enable pairwise comparisons between the performance distributions of models 
trained and evaluated on distinct perturbations of $\TrainData$. 

\begin{restatable}[Performance Separability]{definition}{SeparabilityDef} \label{def:performance-separability}
	Fix a dataset $\TrainData$, an evaluation metric $\TestMetric$,
	and initialization conditions $Z$,  
	and let $\PerturbGraph,\PerturbGraph'$ be mode perturbations. 
	We define the \emph{performance separability} $\Separability$ of $\PerturbGraph$ and $\PerturbGraph'$ as
\begin{equation}
	\Separability (\PerturbGraph,\PerturbGraph') \coloneqq \CompareDist(P_{\PerturbGraph}^{\TestMetric,Z},P_{\PerturbGraph'}^{\TestMetric,Z})\;,
\end{equation}
 where $\CompareDist$ is a method comparing distributions.
\end{restatable} 

In our experiments (\cref{experiments:separability}), 
we use the Kolmogorov-Smirnov (KS) statistic with permutation testing to instantiate $\Separability$.  
This allows us to assess whether the performance distributions associated with $\PerturbModel$ and $\mathcal{M}_{\PerturbGraph'}$ are \emph{significantly different}. 
To evaluate \ref{item:p1}, 
we can then interpret a lack of (statistical) performance separability between a model trained on the original data and a model trained on a mode perturbation as evidence 
that the perturbed mode does not contain (non-redundant) task-relevant information.

\subsection{Mode Complementarity} \label{sec:mode-complementarity} 

While performance separability allows us to assess dataset quality in a setting similar to traditional model evaluation, 
it has three main limitations.
First, it is task-dependent due to its focus on \ref{item:p1}, 
and thus risks underestimating datasets whose tasks are simply misaligned with the information contained in them.
Second, it is measured based on, and hence still to some extent confounded by, model capabilities.
And third, it is resource-intensive to compute. 
To address \ref{item:p2} and forego these limitations, 
we propose \emph{mode complementarity}. 

Intuitively, as illustrated in \cref{fig:metric-space-overview}, 
for an attributed graph $(G,X)$, 
mode complementarity measures the distance between a metric space constructed from the graph structure and a metric space constructed from the node features. 
To formalize this,
we define a process for constructing metric spaces from both modes in  $(G,X)$ using lift functions. 

\begin{restatable}[Metric-Space Construction]{definition}{LiftDef} \label{def:metric-space-construction}
For attributed graph $(G,X)$ and metric $\dist$, 
we construct metric spaces as 
\begin{equation}
	\Lift_{\dist} : (G,X) \mapsto (V,\dist)\,,
\end{equation}
i.e., lifts that take in either \emph{structure-based distances} arising from $G$ or \emph{feature-based distances} arising from $X$ and produce a metric space over the node set $V$.
\end{restatable}

\vspace*{-0.5em}In \framework, we combine these lifts with mode perturbations.

\begin{restatable}[Perturbed Metric Space]{definition}{PerturbMetricSpaceDef} \label{def:perturbed-metric-space}
Given an attributed graph $(G,X)$ and an associated metric $\dist$, the $\PerturbGraph$-perturbed metric space of $(G,X)$ under $\dist$ is
\begin{equation}
	\metricspace^{\PerturbGraph}_{\dist} \coloneqq \Lift_{\dist} \circ\PerturbGraph~(G,X)\;,
\end{equation}
which we construe as a distance matrix. 
\end{restatable}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Figures/metric-space-overview}
	\caption{\textbf{Setup for mode-complementarity assessment.}
		For $(G,X)$ from \cref{fig:mode-overview}, 
		we show the metric spaces arising from original graph structure ($D_S^{\text{o}}$), original node features ($D_F^{\text{o}}$), and $3$ feature perturbations ($D_F^{\text{ef}}$, $D_F^{\text{cf}}$, $D_F^{\text{rf}}$), 
		along with the differences between $D_S^{\text{o}}$ and the feature spaces that underlie our mode-complementarity computations. 
		For structural perturbations, we swap the roles of $D^{\ast}_S$ and $D^{\ast}_{F}$. 
		Note that the spaces arising from complete graph and complete (one-hot) features are equivalent, 
		as are the (degenerate) spaces arising from empty graph and empty features. 
	}\label{fig:metric-space-overview}
\end{figure}

This allows us to assess differences between mode perturbations by comparing metric spaces. 

\begin{restatable}[Metric-Space Comparison]{definition}{ComparatorDef} \label{def:metric-space-comparison}
For a fixed set of $n$ points and two metrics $\dist$ and $\dist'$, 
we compare the metric spaces that arise from $\dist$ and $\dist'$ by computing the $L_{p,q}$ norm of the difference of their  $n\times n$ matrix representations, i.e.,
\begin{align} \label{eq:metric-space-comparison}
	\comp_{p,q}(\metricspace_{\dist},\metricspace_{\dist'}) &:= \frac{\| \metricspace_{\dist} - \metricspace_{\dist'} \|_{p,q}}{\sqrt[q]{n^2 - n}}\;.
\end{align}
\end{restatable}

With \emph{mode complementarity}, 
we then measure the distance between the \emph{normalized} metric spaces arising from the graph structure and the node features.

\begin{restatable}[Mode Complementarity]{definition}{ComplementarityDef} \label{def:mode-complementarity}
Given an attributed graph $(G, X)$ 
with structural metric $\dist_S$, derived from $G$, 
and feature metric $\dist_F$, derived from $X$, 
we define the \emph{mode complementarity} of $(G,X)$ as
\begin{equation}
\complementarity^{p,q}(G, X) \coloneqq \comp_{p,q}(\metricspace_S, \metricspace_F) \;,
\end{equation}
where $\comp$ is a comparator from \Cref{def:metric-space-comparison}, 
 \begin{align}
	\metricspace_{S} \coloneqq \metricspace_{\overline{S}} / \text{diam}(\metricspace_{\overline{S}} )&\text{~~for~~} \metricspace_{\overline{S}}\coloneqq \Lift_{\dist_S}(G, X)\;, \\
 	\metricspace_{F} \coloneqq \metricspace_{\overline{F}} / \text{diam}(\metricspace_{\overline{F}} )&\text{~~for~~}\metricspace_{\overline{F}} \coloneqq \Lift_{\dist_F}(G, X)\,,
 \end{align}
 using the lifts from \Cref{def:metric-space-construction}, 
and we leave (degenerate) zero-diameter spaces unchanged. 
\end{restatable}

Note that since mode complementarity takes an $L_{p,q}$ norm of normalized metric spaces, 
we have ${\complementarity^{p,q}(G,X) \in [0,1]}$ by construction. 
To extend mode complementarity to mode perturbations, 
we again leverage function composition.

\begin{restatable}[Perturbed Mode Complementarity]{definition}{PerturbedComplimentarityDef} \label{def:mode-complementarity-under-perturbation}
	Given an attributed graph $(G,X)$ and a mode perturbation $\PerturbGraph$, the mode complementarity of $\PerturbGraph(G,X)$ is
	\begin{equation}
	\complementarity^{p,q} \circ \PerturbGraph~(G,X) = \comp_{p,q}(\metricspace^{\PerturbGraph}_S,\metricspace^{\PerturbGraph}_F)\;.
	\end{equation}
\end{restatable}
For notational clarity, we use a subscript convention to denote the mode complementarity of $(G,X)$ under specific mode perturbations, 
i.e.,  $\complementarity^{p,q}_{\ast} (G,X) \coloneqq \complementarity^{p,q} \circ \PerturbGraph_\ast~(G,X))$.

To assess \ref{item:p2},
we can interpret high \emph{levels} of mode complementarity under a given mode perturbation as evidence that, 
in the metric spaces associated with that perturbation, 
the graph structure and the features contain \emph{complementary} information. 
We can also gain further insights by assessing the \emph{differences} between mode-complementarity levels across mode perturbations. 
Depending on the mode perturbations compared, 
these differences reveal information about the nature of the connection between the graph structure and the node features, 
or about the diversity present in $G$ and $X$.

In  \cref{prop:perturbation-duality} (\cref{apx:theory}), 
we formalize the relationship between empty mode perturbations $\PerturbGraph_{e*}$ and what we call \emph{self-complementarity}, 
showing that $\complementarity^{p,q}_{e\ast}(G,X)$ measures the geometric structure of the unperturbed mode. 
Based on both limiting behaviors of $\complementarity^{p,q}_{e\ast}(G,X)$, which correspond to \emph{uninformative} metric-space structures, we can then define a notion of \emph{mode diversity}.

\begin{restatable}[Mode Diversity]{definition}{DiversityDef}
	Given an attributed graph $(G,X)$,  
	the \emph{mode diversity} of $(G,X)$ for  $\ast\in\{f,g\}$ is 
	\begin{equation}
		\Delta^{p,q}_\ast(G,X) \coloneqq 1 - |1 - 2\complementarity^{p,q}_{e\ast}(G,X)| \in [0,1]\;.
	\end{equation}
\end{restatable}
Intuitively, the mode diversity $\Delta^{p,q}_\ast$ scores the ability of $\dist_\ast$ to produce non-trivial geometric structure. 
Note that $\Delta^{p,q}_\ast(G,X) \to 0$ implies that $\Lift_{\dist_*} \to \allzeros_n$ or $\Lift_{\dist_*} \to \allones_n - \I_n$, our canonical uninformative metric spaces.

In our experiments,
we instantiate $\dist_F$ with the Euclidean distance between node features and $\dist_S$ with a diffusion distance based on the graph structure (explained in \cref{apx:methods}) 
to approximate the computations underlying graph-learning methods. 
We further choose the $L_{1,1}$ norm as our comparator, which yields a favorable duality, 
proved in \cref{apx:theory}, 
between empty and complete perturbations.

\begin{restatable}[Perturbation Duality]{theorem}{PerturbationDualityThm} \label{thm:perturbation-duality}
	Fix an attributed graph $(G,X)$ and corresponding distances $\dist_S$,$\dist_F$ for lifting each mode into a metric space.  
	For $\ast\in\{\text{f},\text{g}\}$, \cref{def:mode-complementarity} of $\complementarity$ yields the equivalence
	\begin{equation}
		\complementarity^{1,1}_{c\ast} (G,X) = 1 - \complementarity^{1,1}_{e\ast} (G,X)\;.
	\end{equation}
\end{restatable}




 
\section{Related Work}
\label{sec:related}

Here, we briefly contextualize our contributions, deferring a deeper discussion of related work to \cref{apx:related-work}.

Relating to \emph{mode perturbations} and \emph{performance separability}, 
\citet{Errica20a} create GNN experiments to improve reproducibility, and \citet{Bechler-Speicher23a} show that GNNs use the graph structure even when it is not conducive to a task (i.e., $\eg$ separably outperforms $\id$). 
\framework is inspired by, and goes beyond, these works, 
providing a general framework for perturbation-based dataset evaluation. 

Connecting with \emph{mode complementarity}, 
researchers have been particularly interested in the effects of \emph{homo- and heterophily}
on node-classification performance  \citep{lim2021large,platonov2023characterizing,platonov2023critical,luan2023when}. 
While homophily characterizes the \emph{task-dependent} relationship between graph structure and \emph{node labels}, 
mode complementarity assesses the \emph{task-independent} relationship between graph structure and \emph{node features}. 
For node classification, 
\citet{dong2023understanding} propose the edge signal-to-noise ratio (ESNR), 
\citet{quian2022quantifying} develop a subspace alignment measure (SAM), 
and \citet{THANG202246} analyze relations between \emph{node features} and \emph{graph structure}~(FvS).
However, with mode complementarity, we craft a score that 
\begin{inparaenum}[(1)]
	\item treats graph structure and node features as equal (unlike ESNR), 
  \item works on graphs without node labels and does not make assumptions about the spaces arising from edge connectivity and node features (unlike SAM, FvS), and 
	\item specifically informs graph-level learning tasks (unlike all of these works).
\end{inparaenum}
 
\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{Figures/mode-performance_best-only_accuracy-auroc_pi-095}
	\caption{\textbf{Comparing \emph{GNN performance} across different versions of the same dataset.} 
		We show the mean (dot) and $95$th percentile interval (bars) of accuracy and AUROC across $100$ runs of the best (as measured by the respective performance mean) among our tuned GAT, GCN, and GIN models,
		for the original version and $5$ perturbations of $13$ graph-learning datasets.
		Black resp. silver horizontal lines show best mean trained resp. untrained performance on the original data. 
		For Reddit-M, the complete-graph and complete-features perturbations failed to train due to memory problems, 
		but the existing results already allow us to conclude that this dataset lacks performance separability.
		\label{fig:performance-separability}}
\end{figure*}

\begin{table*}[!t]
	\centering \small
	\begin{tabular}{llllll}
\toprule
Dataset & Accuracy & AUROC & Structure & Features & Evaluation \\
\midrule
AIDS & cf $>$ cg $>$ rg $>$ o $>$ eg $>$ rf & cf/cg/rg $>$ eg/o $>$ rf & uninformative & uninformative & \texttt{--} \\
COLLAB & o $>$ cg/rg $>$ eg $>$ cf/rf & o $>$ cg/rg $>$ eg $>$ rf $>$ cf & informative & informative & \texttt{++} \\
DD & rg $>$ cg $>$ cf $>$ eg/o $>$ rf & rg $>$ cf/cg/eg/o/rf & uninformative & uninformative & \texttt{--} \\
Enzymes & eg $>$ cg/o $>$ rg $>$ cf $>$ rf & eg $>$ o $>$ cg $>$ rg $>$ cf $>$ rf & uninformative & informative & \texttt{-} \\
IMDB-B & cf/cg/eg/o/rf/rg & o $>$ cg $>$ eg/rg $>$ rf $>$ cf & (un)informative & (un)informative & $\circ$ \\
IMDB-M & cg/eg/o/rg $>$ cf $>$ rf & o $>$ cg/eg/rg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
MUTAG & cf/cg/o/rg $>$ eg $>$ rf & cf/o $>$ cg/eg/rf/rg & (un)informative & uninformative & \texttt{-} \\
MolHIV & o $>$ cf/cg/rg $>$ rf $>$ eg & o $>$ cg $>$ cf $>$ rg $>$ eg $>$ rf & informative & informative & \texttt{++} \\
NCI1 & o $>$ cg $>$ cf $>$ rg $>$ eg $>$ rf & o $>$ cg $>$ cf $>$ rg $>$ eg/rf & informative & informative & \texttt{++} \\
Peptides & o $>$ cg $>$ rg $>$ eg $>$ cf $>$ rf & cg $>$ o $>$ rg $>$ eg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
Proteins & cf $>$ cg/rf $>$ eg/o/rg & cf/cg/eg/o/rf/rg & uninformative & uninformative & \texttt{--} \\
Reddit-B & cf $>$ rg $>$ o $>$ cg $>$ eg/rf & rg $>$ cf $>$ o $>$ cg/eg/rf & uninformative & uninformative & \texttt{--} \\
Reddit-M & rf $>$ rg $>$ o $>$ eg & rg $>$ rf $>$ o $>$ eg & uninformative & uninformative & \texttt{--} \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Measuring \emph{performance separability} between different versions of the same dataset.}
		To quantify the conclusions from \cref{fig:performance-separability} and further account for performance \emph{distributions} (\cref{def:evaluating-tuned-models}), 
		we use permutation tests with $10\,000$ random permutations and the Kolmogorov-Smirnov (KS) statistic as our test statistic at an $\alpha$-level of $0.01$, Bonferroni-corrected for multiple hypothesis testing within each individual dataset. 
		Here, for sets $S_1$ and $S_2$, $S_1 > S_2$ denotes that the elements in the set $S_1$ separably outperform the elements in the set $S_2$ 
		(i.e., the pairwise distances between $s_1$ and $s_2$ are statistically significantly different for all $s_1\in S_1$ and $s_2\in S_2$), 
		and we represent the sets in condensed notation, concatenating elements with ``/
		''. 
		We see that original datasets are often separably outperformed by their perturbed variants. 
	}\label{tab:performance-separability-ks}
\end{table*}


\section{Experiments}
\label{sec:experiments}

We now demonstrate how to use \framework 
to evaluate the quality of graph-learning datasets,  
guided by the principles developed in \cref{sec:introduction}. 
Leveraging our mode-perturbation framework, 
we explore \ref{item:p1} via performance separability and 
\ref{item:p2} via mode complementarity,
before distilling our observations into an actionable taxonomy of recommendations.
For further details 
and additional results, see \cref{apx:experiments}.\footnote{We make all code, data, results, and workflows publicly available. 
	Upon publication, a fully documented reproducibility package will be deposited via Zenodo.
} 

\textbf{Evaluated Datasets.}~In our
main experiments, 
we evaluate $13$ popular graph-classification datasets:
From the \emph{life sciences}, 
we select AIDS, ogbg-molhiv (MolHIV), MUTAG, and NCI1 (small molecules), 
as well as DD, Enzymes, Peptides-func (Peptides), and PROTEINS-full (Proteins) (larger chemical structures). 
From the \emph{social sciences}, 
we take COLLAB, IMDB-B, and IMDB-M (collaboration ego-networks),  
as well as REDDIT-B and REDDIT-M (online interactions). 
See \cref{apx:datasets} for details and references.

\subsection{Performance Separability (\ref{item:p1})}\label{experiments:separability}
\textbf{Expectations.}~
\begin{inparaenum}[(1)]
	\item For a \emph{structure- and feature-based task}, 
	the original dataset should separably outperform all other mode perturbations. 
	\item For a \emph{structure-based task}, 
	the original dataset should separably outperform all structural perturbations. 
	\item For a \emph{feature-based task}, 
	the original dataset should separably outperform all feature perturbations.
	\item If the original dataset is separably outperformed by a structural (feature) perturbation, 
	the structural (feature) mode of the dataset is poorly aligned with the task. 
\end{inparaenum}

\textbf{Measurement.}~To evaluate performance separability, 
we train $3$ standard GNN architectures (GAT, GCN, GIN)
in the \framework framework 
on the original and $5$ perturbations of our $13$ main datasets: 
\{empty, complete, random\} graph, and \{complete, random\} features.\footnote{GNNs cannot train with $\ef$ due to uninformative gradients.
} 
With the setup described in Appendix \cref{tab:gnn_training_overview}, we tune and evaluate a total of $273$ models. 
Although any GNN architecture could be used to evaluate performance separability within \framework, 
using standard architectures allows us to focus on \emph{dataset} evaluation. 

For each version of each dataset, 
 we identify the model with the best performance under evaluation metric $\TestMetric \in$ \{accuracy, AUROC\} as $(\Architecture^\star, \params^\star) \coloneqq \arg\max_{(\Architecture, \params)} \mathbb{E}\left[\ModePerformance \right]$,
using the performance distribution of the top model,  $\TunedModel^{\star}_{\PerturbGraph} \coloneqq (\PerturbDataset(\TrainData),\Architecture^{\star},\params^{\star})$, 
to assess performance separability. 

To compare performance distributions, 
we use permutation tests with the Kolmogorov-Smirnov (KS) statistic, 
Bonferroni correcting for multiple hypothesis testing within each individual dataset and testing differences for significance at an $\alpha$-level of $0.01$ 
(see Appendix \cref{tab:performance-separability-wilcoxon,tab:performance-separability-ks-005} for substantively identical results using different setups).

\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{Figures/complementarity_fixed-random_pi-95}
	\caption{\textbf{Comparing \emph{levels of mode complementarity} across different versions of the same dataset.} We show the mean (dot) and $95$th percentile intervals (bars) of complementarity scores for the original version as well as $4$ deterministic perturbations (top) and $4$ randomized perturbations (bottom) of $13$ graph-learning datasets, computed with $t=1$ diffusion steps.
		Black horizontal lines indicate mean mode complementarities of the original dataset, and the silver horizontal line marks the $0.5$ threshold relevant for assessing mode diversity. 
		Note that $\complementarity_\text{eg} = 1 - \complementarity_\text{cg}$ and  $\complementarity_\text{ef} = 1 - \complementarity_\text{cf}$ by \cref{thm:perturbation-duality}.
	}\label{fig:mode-complementarity}
\end{figure*}

\textbf{Results.}~
We show the performance of the best GNN models in \cref{fig:performance-separability}, for the original and $5$ mode perturbations over our $13$ main datasets and $2$ evaluation statistics,
and summarize the associated statistical performance-separability results in \cref{tab:performance-separability-ks}. 
We find that only $3$ datasets---COLLAB, MolHIV, and NCI1---satisfy the performance-separability relations expected from structure- and feature-based tasks (with Peptides coming close), 
whereas $5$ datasets---AIDS, DD, Proteins, Reddit-B, and Reddit-M---do not satisfy \emph{any} separability requirements, 
highlighting their low quality from the perspective of \ref{item:p1}.
Among our striking results, 
the original dataset is separably outperformed by 
\begin{inparaenum}[(1)]
	\item the random-graph perturbation on DD, 
	\item the empty-graph perturbation on Enzymes, and 
	\item the complete-features and random-graph perturbations on Reddit-B, 
\end{inparaenum}
indicating that the affected modes are poorly aligned with their current task.  

\subsection{Mode Complementarity (\ref{item:p2})}\label{experiments:complementarity}

\textbf{Expectations.}~\begin{inparaenum}[(1)]
	\item For a \emph{structure- and feature-based task}, 
	the original dataset should have high mode complementarity, 
	and both modes should have high mode diversity (indicated by high mode complementarity of the empty and complete perturbations of their dual mode).
	\item For a \emph{structure-based task}, 
	the structural mode should have high mode diversity. 
	\item For a \emph{feature-based task}, 
	the feature mode should have high mode diversity. 
	\item If the original dataset has low mode complementarity, 
	the information contained in structure and features is redundant. 
	\item If both modes have low mode diversity, 
	the dataset contains little insightful variation. 
\end{inparaenum}

\textbf{Measurement.}~We evaluate mode complementarity for the original as well as $4$ fixed and $4$ randomized mode perturbations: 
\{complete, empty, random, shuffled\} $\times$ \{graph, features\} (see \cref{apx:experiments} for detailed descriptions). 
To measure mode complementarity (and the mode diversity derived from it), 
for each graph in a given dataset perturbation, 
we instantiate \cref{def:mode-complementarity} using the Euclidean distance as our feature distance, 
the diffusion distance (see \cref{apx:methods}) for a number of diffusion steps $t\in [10]$ as our graph distance, 
and the $L_{1,1}$ norm as our comparator 
(see \cref{apx:methods} for examples of other choices). 

\begin{table}[!t]
	\centering \small
	\begin{tabular}{lr@{\hspace*{0.8em}}r@{\hspace*{0.8em}}r@{\hspace*{0.8em}}rc@{\hspace*{1em}}c@{\hspace*{1em}}c@{\hspace*{1em}}c}
\toprule
 & \multicolumn{2}{r}{$\Delta_S$} & \multicolumn{2}{r}{$\Delta_F$} & \multicolumn{2}{r}{Structure} & \multicolumn{2}{r}{Features} \\
 Dataset & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\
\midrule
AIDS & 0.52 & 0.07 & 0.81 & 0.14 & $\circ$ & \texttt{-} & \texttt{++} & $\circ$ \\
COLLAB & 0.30 & 0.20 & 0.27 & 0.24 & \texttt{-} & \texttt{++} & \texttt{-} & \texttt{++} \\
DD & 0.62 & 0.09 & 0.13 & 0.03 & \texttt{++} & \texttt{-} & \texttt{--} & \texttt{--} \\
Enzymes & 0.38 & 0.10 & 0.66 & 0.13 & \texttt{-} & \texttt{-} & \texttt{++} & $\circ$ \\
IMDB-B & 0.18 & 0.11 & 0.55 & 0.29 & \texttt{--} & $\circ$ & $\circ$ & \texttt{++} \\
IMDB-M & 0.09 & 0.11 & 0.30 & 0.34 & \texttt{--} & $\circ$ & \texttt{-} & \texttt{++} \\
MUTAG & 0.51 & 0.02 & 0.76 & 0.14 & $\circ$ & \texttt{--} & \texttt{++} & $\circ$ \\
MolHIV & 0.55 & 0.05 & 0.69 & 0.21 & $\circ$ & \texttt{--} & \texttt{++} & \texttt{++} \\
NCI1 & 0.56 & 0.05 & 0.78 & 0.16 & $\circ$ & \texttt{--} & \texttt{++} & \texttt{++} \\
Peptides & 0.61 & 0.01 & 0.71 & 0.23 & \texttt{++} & \texttt{--} & \texttt{++} & \texttt{++} \\
Proteins & 0.36 & 0.12 & 0.76 & 0.08 & \texttt{-} & $\circ$ & \texttt{++} & \texttt{-} \\
Reddit-B & 0.71 & 0.05 & 0.80 & 0.12 & \texttt{++} & \texttt{-} & \texttt{++} & $\circ$ \\
Reddit-M & 0.72 & 0.03 & 0.85 & 0.11 & \texttt{++} & \texttt{--} & \texttt{++} & $\circ$ \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Evaluating mode diversity.}
		We assess the mean ($\mu$) and standard deviation ($\sigma$) of structural diversity ($\Delta_S$) and feature diversity ($\Delta_F$) for $13$ graph-learning datasets.
	}\label{tab:mode-diversity}
\end{table}

\textbf{Results.}~In \cref{fig:mode-complementarity}, we show the mean and $95$th percentile intervals of the mode-complementarity distributions at $t=1$, 
for the original and the $8$ other selected mode perturbations, 
for each of our $13$ main datasets. 
We observe interesting differences between mode-complementarity profiles across datasets. 
For example, 
\begin{inparaenum}[(1)]
	\item COLLAB, IMDB-B, and IMDB-M 
	stand out for their large ranges (likely due to their nature as ego-networks); 
	\item DD attains extreme values with its complete-graph and empty-graph perturbations (likely due to the absence of features in the original dataset, translated into complete-type features by PyTorch-geometric); and 
	\item Peptides has mode complementarities in the random-graph and the complete-graph perturbations that are noticeably higher than that of the original dataset (possibly related to the presence of long-range connections). 
\end{inparaenum} 


Supplementing \cref{fig:mode-complementarity} with mode-diversity scores in \cref{tab:mode-diversity},
we find that feature diversity is more common than structural diversity, and variation in structural diversity is almost always low. 
Notably,
\begin{inparaenum}[(1)]
	\item COLLAB, IMDB-B, and IMDB-M, judged favorably or neutral by performance separability, score poorly on structural and feature diversity, suggesting that their tasks may not reveal much information about the power of graph-learning methods;
	\item conversely, among the datasets judged problematic by performance separability, 
	DD exhibits high levels of structural diversity, 
	indicating its potential for adequately aligned structural tasks; and
	\item Reddit-B and Reddit-M show high levels of structural and feature diversity, 
	indicating untapped potential for graph-learning tasks that are based on both structure \emph{and} features, as well as high quality from the perspective of \ref{item:p2}. 
\end{inparaenum}

To conclude, we explore the relationship between mean mode complementarity and mean (AUROC) performance across all mode perturbations in \cref{fig:mode-complementarity-vs-performance} (see Appendix \cref{fig:mode-complementarity-vs-performance-accuracy}  
and \cref{tab:mode-complementarity-vs-performance-cor} 
for extended results). 
We find that higher mode complementarity is generally associated with higher performance, 
with exceptions for some tasks we previously identified as misaligned (DD and Reddit-M). 
Since evaluating performance separability is comparatively costly, 
this further underscores the value of mode complementarity as a task-independent diagnostic for graph-learning datasets. 


\subsection{Dataset Taxonomy}\label{experiments:taxonomy} 

From our observations on performance separability (\ref{item:p1}) and mode complementarity (\ref{item:p2}), 
we can distill the following actionable dataset taxonomy, where we note evidence from performance separability ($\dagger$) and mode complementarity ($\ddag$):

\begin{table}[!h]
	\centering
	\begin{tabular}{@{\hspace*{0.05em}}l@{\hspace*{0.3em}}l@{\hspace*{0.05em}}}
		\toprule
		Action&Datasets\\
		\midrule
		 Keep ($\dagger|\ddag$)&MolHIV, NCI1, Peptides\\
		 Realign ($\ddag$)&AIDS, DD, MUTAG, Reddit-B, Reddit-M\\
		 Deprecate ($\dagger|\ddag$)&Enzymes, Proteins\\ 
		 Deprecate ($\ddag$)&COLLAB, IMDB-B, IMDB-M\\ 
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Figures/complementarity-vs-performance_test_auroc_best-only}
	\caption{\textbf{Mode complementarity and performance.} 
		We show mean AUROC (y) as a function of mean mode complementarity (x), 
		for the original version and $5$ perturbations of our $13$ main datasets, 
		based on our best-on-average models (as in \cref{fig:performance-separability}). 
		Each marker represents a (dataset, perturbation, $t$) tuple, where $t \in[10]$ is the number of diffusion steps in the diffusion distance. 
		Higher mean mode complementarity is associated with higher AUROC, 
		and datasets differ in the range of their mode-complementarity shifts.
	}\label{fig:mode-complementarity-vs-performance}
\end{figure}
 
\section{Discussion}
\label{sec:discussion}

\textbf{Conclusions.}
We introduced \framework, 
a principled framework for evaluating graph-learning datasets based on \emph{mode perturbations}---i.e., controlled changes to their graph structure and node features.
In \framework, we developed \emph{performance separability} as a task-dependent, 
and \emph{mode complementarity} as a task-independent measure of dataset quality. 
Using our framework to categorize $13$ popular graph-classification datasets,
we identified several benchmarks that require realignment (e.g., by better data modeling) or complete deprecation. 
While some datasets have been observed to be problematic in prior work, 
with \framework, we offer a full perturbation-based evaluation pipeline. We designed our initial set of perturbations from a data-centric perspective, 
but cleverly crafted perturbations could also isolate and confirm the performance separability of any proposed contribution. 
Thus, we hope that \framework will raise the standard of proof for model-centric evaluation in graph learning as well. 

\textbf{Limitations.}
Given that message passing is the predominant paradigm in graph learning, our \emph{performance-separability} experiments target message-passing GNNs, at the exclusion of other architectures.
While \framework is a general framework, our current 
norm-based comparison of metric spaces assumes that nodes and node features are \emph{paired}.
Thus, we presently do not measure aspects like the \emph{metric distortion} arising from different mappings.
Moreover, our notion of \emph{mode complementarity} is deliberately task-independent and model-agnostic. 
This allows us to gain insights into \emph{datasets}, but the inclusion of additional information about models would enable claims about (\emph{model}, \emph{dataset}, \emph{task}) triples. 

\textbf{Future Work.}
We envision \framework to support the design of better datasets and more challenging tasks for graph learning, 
and we hope that the community will use our proposed measures to further scrutinize its datasets.
Beyond these aspirations, we highlight three concrete avenues for future work. 
First, although we provide some initial theoretical results on the behavior of perturbed mode complementarity, 
a comprehensive theoretical analysis of its properties is yet outstanding. 
Second, while our work focused on graph-level tasks, 
node-level and edge-level tasks also merit closer inspection, 
and these tasks might naturally give rise to a \emph{localized} notion of mode complementarity. 
And third, 
studying how mode complementarity changes during training, or how mode-complementarity distributions impact train-eval-test splits, 
could yield interesting insights~into the relationship between mode complementarity and performance. 
 
\clearpage

\section*{Impact Statement}

With \framework, we introduce a principled data-centric framework for evaluating the quality of graph-learning datasets. 
We expect our framework to help improve data and evaluation practices in graph learning,
enabling the community to focus on datasets that truly require resource-intensive graph-learning approaches via mode complementarity and encouraging greater experimental rigor via performance separability. 
Thus, we hope to contribute to reducing the environmental impact of the machine-learning community, and to the development of scientifically sound, responsible AI methods. 

 
\bibliography{main} 





\appendix

\clearpage

{\LARGE\bfseries Appendix}
\label{apx}

In this appendix, we provide the following supplementary materials. 
\begin{compactenum}[A.]
	\item \hyperref[apx:theory]{\textbf{Extended Theory.}} Definitions, proofs, and results omitted from the main text.
	\item \hyperref[apx:methods]{\textbf{Extended Methods.}} Further details on properties and parts of our \framework pipeline. 
	\item \hyperref[apx:datasets]{\textbf{Extended Dataset Descriptions.}}  Extra details, statistics, and observations on the selected benchmarks.
	\item \hyperref[apx:experiments]{\textbf{Extended Experiments.}} Additional details and experiments complementing the discussion in the main paper. 
	\item \hyperref[apx:related-work]{\textbf{Extended Related Work.}} Discussion of additional related work. 
\end{compactenum}





\section{Extended Theory}\label{apx:theory}


\subsection{Metric Spaces}
Here, we introduce metric spaces as well as various related notions that improve our understanding of \emph{mode complementarity}.

\begin{definition}[Metric Space] \label{def:metric-space}
	A \emph{metric space} is a tuple $(\toyspace, \dist)$ that consists of a nonempty set $\toyspace$ together with a function
	$\dist \colon \toyspace \times \toyspace\to \reals$
	which satisfies the axioms of a metric, i.e.,
\begin{inparaenum}[(i)]
		\item $\dist(x,y) \geq 0$ for all $x,y \in \toyspace$ and $\dist(x,y) = 0$ if and only if $x = y$, 
		\item $\dist(x,y) = \dist(y,x)$, and
		\item $\dist(x,y) \leq \dist(x,z) + \dist(z, y)$.
	\end{inparaenum}
\end{definition}


 \begin{definition}[Isometry]
	Given two metric spaces $(X, \dist_X)$ and $(Y, \dist_Y)$, we call
	a function ${f\colon X \to Y}$ an \emph{isometry} if $\dist_Y(f(x), f(y))
	= \dist_X(x, y)$ for all $x, y \in X$. 
\end{definition}



\begin{definition}[Trivial Metric Space]
  We define a \emph{trivial metric space} as any metric space $(Y,\dist)$ in the equivalence class of $\onepointspace$ (the single point space) under isometry. In particular, this corresponds to any finite space paired with a metric $\dist$ such that $\dist(x,y) = 0$ for all $x,y$. For finite spaces of size $n$, we represent this as the $n\times n$ matrix of zeros, $\allzeros_n$.
\end{definition}

\begin{definition}[Discrete Metric Space]
  A metric space $(Y,\dist)$ is a \emph{discrete metric space} if $\dist$ satisfies
\begin{align}
	d(x,y) =
	\begin{cases} 
		1, & \text{if } x \neq y \\ 
		0, & \text{otherwise}\;.
	\end{cases}
\end{align}
For finite spaces of size $n$, we represent this as the $n\times n$ matrix of all ones with zero diagonal, $\allones_n - \I_n$. Note that when normalizing non-degenerate spaces to unit diameter, the discrete metric space is one where all non-equal elements are maximally distant from each other, (e.g. a complete graph).
\end{definition}

We go from attributed graphs to metric spaces using the lift construction presented in the main paper, restated here for completeness.

\LiftDef*


\subsection{Perturbations}
For convenience in referencing our theory, we repeat our definitions here:
\ModePerturbationDef*
\StructurePerturbsDef*
\FeaturePerturbsDef*
\begin{definition}[Randomization Methods]
	Assume we have $n$ nodes in $(G,X)$. Let $\pi:S \to S$ be a permutation over an ordered set.
		\begin{align*}
		 \RandomizeFeatures_{,1} (X,k) &\mapsto \{\mathbf{x_i} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_k) \}   & [\text{random features}] \\
		 \RandomizeFeatures_{,2} (X) &\coloneqq \pi(X) & [\text{shuffled features}] \\
		 \RandomizeStructure_{,1} (G,p) &\coloneqq ER(n, p) & [\text{random graph}] \\
		 \RandomizeStructure_{,2}(G) &\coloneqq (V,\pi(E)) & [\text{shuffled graph}]
		\end{align*}
		where $ ER(n, p)$ represents an \emph{Erdős–Rényi} model, ensuring that each edge $ (u, v) \in \binom{V}{2} $ is included in $E$ independently with probability $p$.
	 \end{definition}
	


\subsection{Complementarity}
\PerturbMetricSpaceDef*



\subsubsection{Comparators}
\begin{definition}[$L_{p,q}$ Norm] \label{def:Lpq-norm}
  For an $n\times n$ pairwise distance matrix $D$ representing a finite metric space $(Y,\dist)$, we define the $L_{p,q}$ norm as
  \begin{equation}
  || \metricspace ||_{p,q}= \left( \frac{\sum_{i=1}^n\left(\sum_{j=1}^n \lvert \dist(i,j) \rvert^p \right)^{q/p}}{n(n-1)} \right)^{1/q}\;.
\end{equation}
\end{definition}

\ComparatorDef*




\ComplementarityDef*

\subsubsection{Complementarity under Perturbation}
\PerturbedComplimentarityDef*


\begin{definition}[Complimentarity for Disconnected Graphs] \label{def:disconnected-complementarity}
  For a disconnected attributed graph $(G,X)$ with $n$ nodes and $C$ connected components, we write $G$ as a union  $\bigcup G_i$. Let $X|_{V_i}$ denote the subset of features corresponding to nodes in $G_i = (V_i,E_i)$. We define the \emph{complementarity for disconnected graphs} as the weighted average of the individual component scores
  \begin{equation}
    \complementarity^{p,q}_{\PerturbGraph} (G,X) \coloneqq  \sum_{i}^C \frac{n_i}{n} \left(\complementarity^{p,q}_{\PerturbGraph} (G_i,X|_{V_i})  \right) \;,
  \end{equation}
  where $n_i = |V_i|$. Axiom (ii) in \cref{def:metric-space}, implies that isolated nodes are then assigned a trivial metric space.
\end{definition}


\begin{lemma}[Empty Perturbations Lift to Trivial Metric Spaces] \label{lemma:empty-to-trivial}
  Let $(G,X)$ be an attributed graph. For any metric $\dist_\ast$, the image of $\Lift_{\dist_\ast}$ as defined in \cref{def:metric-space-construction} under precomposition with $\PerturbGraph_{e\ast}$ is a \emph{trivial metric space}, for $\ast \in \{f,g\}$.
  \end{lemma}
  \begin{proof}
  ($\ast = f$)~ By \Cref{def:feature-perturbations}, we have $\ef(G,X) = (G,\allzeros_n)$. Since $\allzeros_n$ is isometric to $\onepointspace$, any metric must lift $\onepointspace$ to a trivial metric space (by metric-space axiom (ii)).
  
  ($\ast = g$)~ For structural metrics $\StructureDist$, we restrict to those derived from the adjacency matrix $A$ of $G$. By convention, $(V,\emptyset)$ is represented as $\allzeros_n$. Hence, an identical argument holds \emph{mutatis mutandis}, as any metric derived from the zero matrix lifts to a trivial metric space.
  \end{proof}
  
  \begin{lemma}[Complete Perturbations Lift to Discrete Metric Spaces] \label{lemma:complete-to-discrete}
  Let $(G,X)$ be an attributed graph. For any choice of $\dist_\ast$, the image of $\Lift_{\dist_\ast}$ as defined in \cref{def:metric-space-construction} under precomposition with $\PerturbGraph_{c\ast}$ is a \emph{discrete metric space}, for $\ast \in \{f,g\}$.
  \end{lemma}
  \begin{proof}
  ($\ast = f$)~ By \Cref{def:feature-perturbations}, we have $\ef(G,X) = (G,\I_n)$. The identity matrix $\I_n$ consists of standard basis vectors $e_i$ (i.e., the one-hot encodings of nodes). We claim that any metric $\FeatureDist$ over $\I_n$ must be discrete.
  
Toward a contradiction, assume otherwise. W.l.o.g., suppose the metric space is scaled to unit diameter, implying that there exist $i,j$ such that $\FeatureDist(e_i,e_j) < 1$. Since $(\I_n,\FeatureDist)$ is uniform, it is invariant under orthogonal transformations, meaning we can permute the standard basis vectors while preserving distances. Thus, for all $k \neq i$, we have $\FeatureDist(e_i, e_k) = \FeatureDist(e_i, e_j) < 1$, contradicting our assumption. Therefore, $\FeatureDist(x,y) = 1$ for all $x \neq y$, proving discreteness.
  
  ($\ast = g$)~ Here, $\cg(G,X) = \left(V, \binom{V}{2}\right)$. The edge set consists of all two-element subsets of $V$, forming a fully connected graph. Similar to \Cref{lemma:empty-to-trivial}, we restrict to metrics defined over the adjacency matrix $A$ of $G$. By construction, $A = \allones_n - \I_n$, which has the same structure as $\I_n$ under the isometry $f: x_i \mapsto e_i$. Since all elements are maximally distant, any metric $\StructureDist$ must lift to a discrete space.
  \end{proof}




\PerturbationDualityThm*
\begin{proof}
  \begin{align*}
  \complementarity_{cg}^{1,1} (G,X) 
  &= \comp_{1,1}(D^{\cg}_{\StructureDist}, D_{\FeatureDist}) \\
  &= \comp_{1,1}(\allones_n - \I_n, D_{\FeatureDist}) \\
  &= \left\| (\allones_n - \I_n) -  D_{\FeatureDist} \right\|_{1,1} \\
  &= \frac{1}{n^2 - n} \left( \sum_{i=1}^{n} \sum_{j \neq i}^{n} \left| 1 - \FeatureDist(i,j) \right| \right) \\
  \end{align*}
  \begin{align*}
  \phantom{\complementarity_{cg}^{1,1} (G,X) }&= \frac{1}{n^2 - n} \left( n^2 - n - \sum_{i=1}^{n} \sum_{j \neq i}^{n} \FeatureDist(i,j) \right) \\
  &= 1 - \frac{1}{n^2 - n} \left( \sum_{i=1}^{n} \sum_{j=1}^{n} \FeatureDist(i,j) \right) \\
  &= 1 - \left\| D_{\FeatureDist} \right\|_{1,1} \\
  &= 1 - \complementarity_{eg}^{1,1} (G,X) \qedhere
  \end{align*}
  
  \noindent
  The justifications for the above steps are as follows:
  \begin{itemize}[nosep]
      \item Line 1: \cref{def:mode-complementarity-under-perturbation}.
      \item Line 2: \cref{lemma:complete-to-discrete}.
      \item Line 3: \cref{def:metric-space-comparison}.
      \item Line 4: \cref{def:Lpq-norm}.
      \item Line 5: Unit-diameter assumption: \(\FeatureDist \leq 1\).
      \item Line 6: Axiom (ii) of \cref{def:metric-space}.
      \item Line 7: \cref{def:Lpq-norm}.
      \item Line 8: Definition of \(\complementarity_{eg}^{1,1} (G,X)\) and \cref{lemma:empty-to-trivial}.
  \end{itemize}
  Note that a symmetrical argument can be applied to $\complementarity_{cf}^{1,1}$. This concludes the proof.
\end{proof}


\DiversityDef*


\begin{proposition}[Self-Complementarity] \label{prop:perturbation-duality}
	Let $\StructureDist$ and $\FeatureDist$ be metrics over the modes of an attributed graph $(G,X)$. 
	We define  $(\eg,\FeatureDist)$ and $(\ef,\StructureDist)$ as dual pairs that can be used to analyze the \emph{self-complementarity} of a single mode. By construction, for $\ast\in\{\text{f},\text{g}\}$, we have:
	\begin{equation}
		\Lift \circ \PerturbGraph_{e\ast} ~(G,X) \triangleq \allzeros_n.
	\end{equation}
	Thus, the complementarity under empty perturbations, $\complementarity^{p,q}_{e\ast}$, measures the internal structure of the nonzero dual mode:
	\begin{align}
		\complementarity^{p,q}_{e\ast}(G,X) &= \frac{1}{\sqrt[q]{n^2 - n}}|| \metricspace_{\dist_*} ||_{p,q}.
	\end{align}
	Due to the normalization, we obtain ${\complementarity^{p,q}_{e\ast}(G,X) \in [0,1]}$. The limiting behavior of $\complementarity^{p,q}_{e\ast}(G,X)$, then, provides insights into the underlying metric structure of the dual mode:
	\begin{enumerate}[label=(\arabic*), nosep]
		\item If $\complementarity^{p,q}_{e\ast}(G,X) \to 0$, then $\dist_{\ast}(x,y) \approx 0$ for all $x,y$.
		\item If $\complementarity^{p,q}_{e\ast}(G,X) \to 1$, then $\dist_{\ast}(x,y) \approx 1$ for all $x \neq y$.
	\end{enumerate}
\end{proposition}


 
\section{Extended Methods}
\label{apx:methods}
	
	
\subsection{Mode Complementarity}
 The perturbations introduced with our \framework framework are specifically designed to disentangle the relationship between the structural mode and the feature mode of $(G,X)$. 
 Unlike existing methods, \framework allows us to leverage the relationships both \emph{between} and \emph{within} individual modes:

\[
\begin{tikzcd}
  G \arrow[d, dashed, leftrightarrow] \arrow[r, leftrightarrow] \arrow[loop left, dashed] & G' \arrow[d, dashed, leftrightarrow] \arrow[loop right, dashed] \\
  X \arrow[r, leftrightarrow] \arrow[loop left, dashed]                                   & X' \arrow[loop right, dashed] \\
\end{tikzcd}
\]

 
 Here, all arrows indicate the calculation of potential similarity measures. Solid arrows describe settings for which similarity measures are already known, such as \emph{graph kernels} \cite{Borgwardt20, Kriege20}, \emph{graph edit distances} \cite{Gao10a}, or distance metrics on  $\reals^n$. The dashed arrows show our ability to study \emph{complementarity} and \emph{self-complementarity} using metric spaces and principled mode perturbations.








 


	















\subsection{Metric Choices}

As outlined in \Cref{sec:mode-complementarity}, we would like to understand, for a given attributed graph $(G,X)$ the complementarity between the information in the graph structure and the node features. Thus, we use $\complementarity(G,X)$ to score the difference in geometric information contained in the two modes. As per  \cref{def:mode-complementarity}, this requires a choice of a \emph{structural metric} ($\StructureDist$) and a \emph{feature metric} ($\FeatureDist$), which facilitate lifting the modes into a metric space. 

While our experimental results are based on diffusion distance as our structural distance metric ($\dist_S$) and Euclidean distance in $\reals^k$ as our feature-based distance metric ($\dist_F$), it is worth noting that the \framework framework generalizes to other distance metrics. Below, we define the diffusion distance and Euclidean distance, as well as several other distance metrics suitable for the \framework framework.
To build intuition, we additionally depict the behavior of different graph-distance candidates under edge addition on toy data in \cref{fig:synthetic-toy} and illustrate the interplay between metric choices for the real-world Peptides dataset in \cref{fig:peptides-exploration}.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{Figures/synthetic-completion_n-10_N-100_ft-normal_diff-01}
	\caption{\textbf{Mode complementarity under different graph distances and varying density.} 
		We show the evolution of mode complementarity for $5$ graphs on $10$ nodes with normally distributed one-dimensional features 
		as we randomly add edges to increase the density from the minimum for a connected graph ($0.2$) to the maximum ($1.0$), 
		for different choices of graph distance~$\dist_S$. 
		For the diffusion distance and the heat-kernel distance,
		which depend on the parameter $t$, we show the complementarity at $t =1$. 
		The diffusion distance exhibits the smoothest behavior. 
	}\label{fig:synthetic-toy}
\end{figure*}

\subsubsection{Structural Metrics}

\paragraph{Disconnected Graphs}
Here we define the metrics we use to lift a graph structure into a metric space. Although common practice assigns an infinite distance between unconnected nodes, due to our treatment of disconnected graphs in \cref{def:disconnected-complementarity}, we only lift metric spaces within a connected component. Our treatment of isolated nodes assigns trivial metric spaces to each node, and their union we also treat as trivial as seen in \cref{lemma:empty-to-trivial}.


\begin{definition}[Laplacian Variants]
	We define the Laplacian of $G = (V,E)$ as
	\begin{equation} 
	L \coloneqq D - A \;,
	\end{equation}
	where $A$ is the $n\times n$ adjacency matrix specified by $E$, and $D$ is the degree matrix. We also define a \emph{normalized} Laplacian
	\begin{equation}
		\widehat{L} \coloneqq D^{-1/2} L D^{-1/2}\;,
	\end{equation}
	where $D^{-1/2} = \text{diag}\left(\frac{1}{\sqrt{D_{ii}}}\right)$.
\end{definition}

\begin{definition}[Diffusion Distance {[}$\DiffusionDist${]} \citep{COIFMAN20065}] \label{def:diffusion-distance}
	We define our distance over $G$ for $t$ \emph{diffusion steps} as
	\begin{equation}
		d_{S_t}(x,y) = \left\|\DiffusionMap(x) - \DiffusionMap(y)\right\|,
	\end{equation}
	
	where $\{\Psi_t\}$ is a family of diffusion maps computed from the spectrum of $\widehat{L}$. In particular,
	\[
	\DiffusionMap(x) \triangleq 
	\begin{pmatrix}
		\lambda_1^t \psi_1(x) \\
		\lambda_2^t \psi_2(x) \\
		\vdots \\
		\lambda_{\DiffusionDim}^t \psi_{\DiffusionDim}(x)
	\end{pmatrix}\;,
	\]
	where $\lambda_i,\psi_i$ are the $i^{th}$ eigenvalue and eigenvector of $\widehat{L}$. 
\end{definition}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{Figures/complementarity-exploration_Peptides_original_L11}
	\caption{\textbf{Real-world mode complementarity under different graph distances $\dist_S$ and feature distances $\dist_F$.} 
		We show the $95$th percentile of the mode-complementarity distribution for the Peptides dataset, 
		for varying combinations of graph distance $\dist_S$ and feature distance $\dist_F$, 
		using $t = 1$ for the $t$-dependent distances (diffusion and heat kernel).
	}\label{fig:peptides-exploration}
\end{figure}


\begin{definition}[Heat-Kernel Distance \citep{Bai2004HeatKM}]
	To compute the heat-kernel distance between nodes in a graph $G$, we take the spectral decomposition of $\widehat{L}$ as
	\begin{equation}
		\widehat{L} = \Psi\Lambda\Psi^{T} = \sum_{i=1}^{n} \lambda_i \psi_i \psi_{i}^T\;,
	\end{equation}
	where $\Lambda$ is the diagonal matrix with ordered eigenvalues and $\Psi=(\psi_1, \psi_2, ..., \psi_n)$ is the matrix with columns corresponding to the ordered eigenvectors. The heat equation associated with the graph Laplacian, written in terms of the heat kernel $h_t$ and time $t$, is defined as 
	\begin{equation*}
		\frac{\partial h_t}{\partial t} = -\widehat{L}h_t\;.
	\end{equation*}
	Finally, the heat-kernel distance between nodes $u$ and $v$ at time $t$ is then computed as:
	\begin{equation}
		h_t(u, v) = \sum_{i=1}^{n} \exp{-\lambda_i t} \psi_i(u) \psi_i(v)\;.
	\end{equation}
\end{definition}

 \begin{definition}[Resistance Distance \citep{randic1993resistance}]
 	Given a graph $G$ with $n$ nodes, the resistance distance between nodes $u$ and $v$ is given as:
 	\begin{equation}
 		\Omega_{u,v} = \Lambda_{i,i} + \Lambda_{j,j} - \Lambda_{i,j} - \Lambda_{j,i}\;,
 	\end{equation}
 	where 
 	\begin{equation*}
 		\Lambda = \left(L + \frac{1}{n}\allones_n\right)^+\;,
 	\end{equation*}
 	with $+$ denoting the Moore-Penrose pseudoinverse.
 \end{definition}

\begin{definition}[Shortest-Path Distance]
	For a graph $G=(V,E)$, we define the shortest-path distance between two nodes $u,v \in V$ as the minimum number of edges in any path between $u$ and $v$.
\end{definition}

\subsubsection{Feature Metrics}

\begin{definition}[Euclidean Distance {[{$\dist_{F}$}]}] \label{def:euclidean-distance}
	Given our feature space $\features\subset\reals^{k}$, let $x,y \in \reals^k$ be two arbitrary feature vectors. Then the Euclidean distance (also known as the L2 norm) between $x$ and $y$ is defined as
	\begin{equation}
		\dist_F(x, y) = \sqrt{\sum_{i=1}^{k} (x_i - y_i)^2}\;.
	\end{equation}
\end{definition}

\begin{definition}[Cosine Distance]
	Given a feature space $\features\subset\reals^{k}$, let $x,y \in \reals^k$ be two arbitrary feature vectors. Then the cosine distance between $x$ and $y$ is defined as 
	\begin{equation}
		\dist(x, y) = 1 - \frac{x \cdot y}{\|x\| \|y\|}\;,
	\end{equation}
	where $x \cdot y$ is the dot product of $x$ and $y$, and $\|x\|$ is the L2 norm of $x$.
\end{definition}


 


\section{Extended Dataset Descriptions} \label{apx:datasets}

As we seek to evaluate the datasets in their capacity as graph-learning benchmarks, the formulation and properties of these datasets are worth detailed discussion. 
We summarize the basic statistics of these datasets in \cref{tab:dataset-statistics} and now describe the semantics and special characteristics of each dataset in turn. 

\begin{table*}[t]
	\small
	\begin{tabular}{lrrrrrrrrrrrrr}
\toprule
 & \multicolumn{3}{r}{$n$} & \multicolumn{2}{r}{$m$} & \multicolumn{2}{r}{$\delta$} & \multicolumn{2}{r}{$\rho$} & \multicolumn{2}{r}{$\gamma_1$} & \multicolumn{2}{r}{$\gamma_{10}$} \\
 & $N$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\
Dataset &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\midrule
AIDS & 2000 & 15.69 & 13.69 & 32.39 & 30.02 & 2.01 & 0.20 & 0.19 & 0.08 & 0.29 & 0.08 & 0.28 & 0.06 \\
COLLAB & 5000 & 74.49 & 62.31 & 4914.43 & 12879.12 & 37.37 & 43.97 & 0.51 & 0.30 & 0.40 & 0.27 & 0.64 & 0.21 \\
DD & 1178 & 284.32 & 272.12 & 1431.32 & 1388.40 & 4.98 & 0.59 & 0.03 & 0.02 & 0.33 & 0.04 & 0.80 & 0.06 \\
Enzymes & 600 & 32.63 & 15.29 & 124.27 & 51.04 & 3.86 & 0.49 & 0.16 & 0.11 & 0.49 & 0.07 & 0.26 & 0.05 \\
IMDB-B & 1000 & 19.77 & 10.06 & 193.06 & 211.31 & 8.89 & 5.05 & 0.52 & 0.24 & 0.49 & 0.23 & 0.53 & 0.20 \\
IMDB-M & 1500 & 13.00 & 8.53 & 131.87 & 221.63 & 8.10 & 4.82 & 0.77 & 0.26 & 0.73 & 0.30 & 0.75 & 0.28 \\
MUTAG & 188 & 17.93 & 4.59 & 39.59 & 11.40 & 2.19 & 0.11 & 0.14 & 0.04 & 0.51 & 0.07 & 0.48 & 0.02 \\
MolHIV & 41127 & 25.51 & 12.11 & 54.94 & 26.43 & 2.14 & 0.11 & 0.10 & 0.04 & 0.42 & 0.11 & 0.33 & 0.08 \\
NCI1 & 4110 & 29.87 & 13.57 & 64.60 & 29.87 & 2.16 & 0.11 & 0.09 & 0.04 & 0.54 & 0.05 & 0.51 & 0.03 \\
Peptides & 10873 & 151.54 & 84.10 & 308.54 & 171.99 & 2.03 & 0.04 & 0.02 & 0.02 & 0.38 & 0.09 & 0.26 & 0.04 \\
Proteins & 1113 & 39.06 & 45.78 & 145.63 & 169.27 & 3.73 & 0.42 & 0.21 & 0.20 & 0.45 & 0.04 & 0.26 & 0.06 \\
Reddit-B & 2000 & 429.63 & 554.20 & 995.51 & 1246.29 & 2.34 & 0.31 & 0.02 & 0.03 & 0.47 & 0.03 & 0.45 & 0.08 \\
Reddit-M & 4999 & 508.52 & 452.62 & 1189.75 & 1133.65 & 2.25 & 0.20 & 0.01 & 0.01 & 0.48 & 0.02 & 0.43 & 0.07 \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Basic statistics of our $13$ evaluated graph-learning datasets.}
		We show the number of graphs ($N$) as well as the mean and standard deviation of the number of nodes ($n$), 
		the number of edges ($m$), 
		the degree ($\delta$), 
		the density ($\rho$), 
		and mode complementarity at $t\in \{1,10\}$ ($\gamma_1$, $\gamma_{10}$).
	}\label{tab:dataset-statistics}
\end{table*}

\subsection{Social Sciences}

\subsubsection{Professional Collaborations}
In professional collaboration networks, nodes represent individuals and edges connect those with a direct working relationship.

\emph{COLLAB}~\citep{Yanardag2015} is a dataset of physics collaboration networks, where nodes represent physics researchers and edges connect co-authors. Graphs are constructed as ego-networks around a central physics researcher, whose subfield (\emph{high energy physics}, \emph{condensed matter physics}, or \emph{astrophysics}) acts as the classification for the graph as a whole. The data to build these graphs is drawn from three public collaboration datasets based on the arXiv~\citep{leskovec05}.

Following a similar construction, the \emph{IMDB}~\citep{Yanardag2015} datasets capture collaboration in movies. Graphs represent the ego-networks of actors and actresses, and edges connect those who appear in the same movie. The graph is classified according to the genre of the movies from which these shared acting credits are pulled. The task is to predict this genre, a binary classification between \emph{Action} and \emph{Comedy} in \emph{IMDB-B}, and a multi-class classification between \emph{Comedy}, \emph{Romance}, and \emph{Sci-Fi} in \emph{IMDB-M}. (Note that genres are engineered to be mutually exclusive.) As one might expect, we note in \cref{fig:performance-separability} the lower GNN performance for \emph{IMDB-M} as compared to \emph{IMDB-B}, suggesting that the multi-class graph classification is a more challenging task.

\emph{COLLAB} and \emph{IMDB} datasets share two notable features that distinguish them from the remaining datasets. First, graphs are constructed as ego-networks. This means that they have a diameter of at most two, in stark contrast to more structurally complex graphs from other datasets. This lends insight into why \emph{COLLAB} and \emph{IMDB} are classified as having low structural diversity (see \cref{tab:mode-diversity}). Second, node features are constructed from the node degree, rather than providing new information independent of graph structure. Thus, the low diversity scores with high variability that we see in \cref{tab:mode-diversity} are simply a reflection of the node-degree distributions in these datasets. 

\subsubsection{Social Media}

We study two graph-learning datasets with origins in social media: \emph{REDDIT-B} and \emph{REDDIT-M}~\citep{Yanardag2015}.

In the these datasets, a graph models a thread lifted from a subreddit (i.e., a specific discussion community). Nodes represent Reddit users and edges connect users if at least one of them has replied to a comment made by the other (i.e., the users have had at least one interaction). Like \emph{COLLAB} and \emph{IMDB}, node features are constructed from the node degree. However, the \emph{REDDIT} datasets score better on both structural and feature diversity (see \cref{tab:mode-diversity}). \emph{REDDIT}'s higher structural diversity reflects the fact that its graphs are not ego-networks. Its more complex graph structures also account for the higher diversity in node features (i.e., node degrees).

The \emph{REDDIT-B} dataset draws from four popular subreddits, two of which follow a Q\&A structure (\emph{IAmA}, \emph{AskReddit}) and two of which are discussion-based (\emph{TrollXChromosomes}, \emph{atheism}). The task is to classify graphs based on these two styles of thread. \emph{REDDIT-M} graphs are drawn from five subreddits, which also act as the graph class (\emph{worldnews}, \emph{videos}, \emph{AdviceAnimals}, \emph{aww} and \emph{mildlyinteresting}).

Similar to the difference between the \emph{IMDB} binary and multi-class datasets, we observe better GNN performance on the \emph{REDDIT-B} benchmark as compared to \emph{REDDIT-M} (see \cref{fig:performance-separability}), despite the fact that \emph{REDDIT-M} boasts more than twice as many graphs (see \cref{tab:dataset-statistics}).

\subsection{Life Sciences}

\subsubsection{Molecules and Compounds}

Graphs provide an elegant way to model molecules, with nodes representing atoms and edges representing the chemical bonds between them.

The \emph{AIDS} dataset \citep{riesen2008aids}, for example, is built from a repository of molecules in the AIDS Antiviral Screen Database of Active Compounds \citep{aids2004data}. Nodes have four features \citep{morris2020tudataset}, most notably their unique atomic number. This is a binary-classification dataset in which molecules are classified by whether or not they demonstrate activity against the HIV virus. Among the 2\,000 graphs in this dataset, note that the two classes are imbalanced: there are four times as many inactive molecules (1\,600) as active (400). The \emph{MolHIV (ogbg-molhiv)} dataset, introduced by the Open Graph Benchmark \citep{hu2021opengraphbenchmarkdatasets} with over 40,000 molecules from MoleculeNet \citep{wu2018moleculenet}, proposes a similar task. Like \emph{AIDS}, \emph{MolHIV} classifies molecules into two classes based on their ability or inability to suppress the HIV virus. However, nodes in the \emph{MolHIV} dataset have $9$ features (compared to $4$ in \emph{AIDS}), including atomic number, chirality, and formal charge.

\emph{MUTAG} \citep{MUTAG} is a relatively small molecular dataset compared with its peers, having only $188$ graphs \citep{morris2020tudataset}. However, it does have a similar number of node features ($7$). Graphs represent aromatic or heteroaromatic nitro compounds, which are divided into two classes by their mutagenicity towards the bacterium \emph{S. typhimurium}, i.e. their ability to cause mutations its DNA. While small, the dataset contains an ``extremely broad range of molecular structures'' \citep{MUTAG}, which we see reflected in a decent structural-diversity score in \cref{tab:mode-diversity}.

When introducing the dataset, \citet{MUTAG} found that both specific sets of atoms and certain structural features (namely the presence of $3$ or more fused rings), tended to be fairly well correlated with increased mutagenicity. This biologically supports that both modes could be informative, which is generally encouraging for graph-learning datasets. Thus, although we do not observe the desired performance separability in \cref{fig:performance-separability}, we cannot rule out that \emph{MUTAG} could be a good benchmark, given a different task.

\emph{NCI1} \citep{nci1-dataset} is a binary-classification dataset that splits molecules into two classes based on whether or not they have an inhibitary effect on the growth of human lung-cancer cell lines. Nodes have $37$ features, a significant step up from other molecular datasets, which is reflected in the richness of \emph{NCI1}'s feature diversity (see \cref{tab:mode-diversity}).

\subsubsection{Larger Chemical Structures}

The \emph{Peptides} \emph{(Peptides-func)} dataset \citep{Dwivedi2022} classifies peptides by their function. Each graph represents a peptide (i.e., a 1D amino acid chain), with nodes representing heavy (non-hydrogen) atoms and edges representing the bonds between them. \emph{Peptides} is a multi-label-classification dataset, meaning that each peptide may belong to more than one of the $10$ function classes (\emph{Antibacterial}, \emph{Antiviral}, \emph{cell-cell communication}, etc.), with the average being 1.65 classes per peptide. Given the complexity of this task, it is perhaps unsurprising to notice that our tuned GNNs achieve the lowest AURUC on this dataset (see \cref{fig:performance-separability}).

\emph{DD}, \emph{ENZYMES}, and \emph{Proteins} \emph{(Proteins-full)} are all protein datasets.
Each dataset draws upon the framework introduced by \citep{borgwardt2005} to model 3D structures of folded proteins as graphs, with nodes representing amino acids and edges connecting amino acids within a given proximity measured in Angstroms. Both the \emph{Proteins-full} database introduced by \citep{borgwardt2005} and the \emph{DD} database used by \citep{shervashidze11a} are based on data from \citet{dobson2003dd}, which classifies $1\,178$ proteins as either enzymes ($59\%$) or non-enzymes ($41\%$). Thus, these two datasets are binary-classification datasets.

\emph{Proteins-full} only uses $1\,128$ of the $1\,178$ proteins from \citet{dobson2003dd} but encodes $29$ descriptive node features, whereas \emph{DD} encodes none \cite{tudortmund,morris2020tudataset}. This distinction is responsible for {Proteins-full}'s drastically higher feature diversity score and standard deviation, as seen in \cref{tab:mode-diversity}.

\emph{ENZYMES} \citep{borgwardt2005} introduces a multi-class classification task on enzymes. Graphs represent enzymes from the BRENDA database \citep{schomburg04brenda}, which are classified according to the $6$ Enzyme Commission top-level enzyme classes (EC classes). The dataset is roughly half the size of \emph{DD} and \emph{Proteins-full}, with only $600$ enzymes ($100$ from each class). There are $18$ node features, producing the high feature diversity observed in \cref{tab:mode-diversity}.

\subsection{Further Information}

Several efforts have been made to consolidate information about the aforementioned datasets and their peers. In particular, we direct the interested reader to the TUDataset documentation \citep{morris2020tudataset}, TU Dortmund's Benchmark Data Sets for Graph Kernels \citep{tudortmund}, and PyTorch Geometric \citep{fey2019pytorch_geometric}.
 
\section{Extended Experiments}
\label{apx:experiments}

\subsection{Extended Setup}

With the \framework framework, we introduce principled perturbations to graph learning datasets that allow us to evaluate the extent to which graph structure and node features are leveraged in a given task (\emph{performance separability}) and to what extent the information in these modes is complementary or redundant (\emph{mode complementarity}). We do so with a series of experiments, which we expand upon below.

\begin{table}[t]
	\centering \small
	\renewcommand{\arraystretch}{1.05} \setlength{\tabcolsep}{4pt} 

 
\begin{tabular}{@{\hspace*{0.1em}}l@{\hspace*{0.3em}}l@{\hspace*{0.1em}}}
    \toprule
    \textbf{Aspect}                  & \textbf{Details} \\ 
    \midrule
    \textbf{Architectures}
        & $\{$GAT, GCN, GIN$\}$\\ 
    \textbf{Mode Perturbations}       
        & $\{\id, \eg, \cg, \cf, \rf, \rg\}$ \\ 
    \textbf{Tuning Strategy}         
        & 5-fold CV $\times$ 64 consistent $\params$\\ 
        &for each $(\PerturbDataset(D), \Architecture)$\\ 
    \textbf{Selection Metric}        
        & Validation AUROC \\ 
    \textbf{Evaluation Strategy}     
        & Tuned model $(\PerturbDataset(D), \Architecture, \params)$ re-trained \\
		& on distinct CV splits and random seeds, \\
		& then evaluated on $\PerturbDataset(D)$'s test set. \\
    \bottomrule
\end{tabular}
 	\caption{\textbf{GNN setup to evaluate performance separability.} Parameter combinations are specified in \cref{tab:gnn-tuning-params}. $\PerturbDataset_{rg}(\TrainData)$ perturbations were trained using a $\mathcal{R}_{F,1}(\cdot, 10)$ randomization. The top performing parameter combination $\params^\star$ was selected based on validation statistics: primarily based on AUROC, with ties broken by accuracy and then loss. }
	\label{tab:gnn_training_overview}
\end{table}


\begin{table}[t]
    \small
    \centering
    \renewcommand{\arraystretch}{1.2} \setlength{\tabcolsep}{10pt} 
    


\begin{tabular}{ll}
    \toprule
    \textbf{Parameter} & \textbf{Values} \\
    \midrule
    Activation       & \texttt{ReLU} \\
    Batch Size       & \{64, 128\} \\
    Dropout          & \{0.1, 0.5\} \\
    Fold            & \{0, 1, 2, 3, 4\} \\
    Hidden Dim      & \{128, 256\} \\
    Learning Rate (LR) & \{0.01, 0.001\} \\
    Max Epochs      & 200 \\
    Normalization   & \texttt{Batch} \\
    Num Layers      & 3 \\
    Optimizer       & \texttt{Adam} \\
    Readout         & \texttt{Sum} \\
    Seed            & \{0, 42\} \\
    Weight Decay    & \{0.0005, 0.005\} \\
    \bottomrule
\end{tabular}     \caption{\textbf{GNN tuning parameters.} For consistency, all $(\PerturbDataset(\TrainData),\Architecture)$ were tuned across a consistent hyperparameter grid search. 
    }
    \label{tab:gnn-tuning-params}
\end{table}


\subsection{Extended Performance Separability}

We begin with a note on the chosen GNN architectures, 
before examining and supplementing the results of our performance separability evaluations.

\subsubsection{A Note on GNN Architectures}

By far the most computationally expensive part of our experiments is tuning GNNs. While variation in the GNN models is desirable to ensure that our results are not overly dependent on a specific architecture, we were limited in the number of GNN models that we could train and evaluate over all of our datasets and their perturbations. As our goal is to evaluate the intrinsic quality of datasets as graph-learning benchmarks, we are more concerned with consistency across experiments than with using the newest methods, especially given finite computational resources.

In accordance with other ``evaluation of evaluations'' studies \citep{Bechler-Speicher23a,Li_2024}, we opted to use several of the most common GNN architectures, namely GCN~ \citep{kipf2017}, GAT~\citep{GAT2018}, and GIN~\citep{xu2019gin}. As noted by \citet{Li_2024}, GCN and GIN provide a good contrast in methodology, with GCN taking a spectral approach and GIN taking a spatial approach.

For each (dataset, perturbation, architecture) triple, we tune the GNN hyperparameters as outlined in \cref{tab:gnn_training_overview}, using the tuning parameters ($\params$) stated in \cref{tab:gnn-tuning-params}. 
The mean and standard deviation of accuracy and AUROC for our tuned models are tabulated in \cref{tab:performance-results-accuracy} and \cref{tab:performance-results-auroc}, respectively.

\begin{table*}[!t]
\centering \small
\begin{tabular}{llrrrrrrrrrrrr}
\toprule
 &  & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\
 Dataset&  & o & o & cg & cg & eg & eg & rg & rg & cf & cf & rf & rf \\
\midrule
\multirow[t]{3}{*}{AIDS} & GAT & 0.980 & 0.006 & 0.968 & 0.017 & 0.977 & 0.007 & 0.968 & 0.011 & 0.998 & 0.002 & 0.320 & 0.075 \\
 & GCN & 0.982 & 0.005 & 0.950 & 0.016 & 0.983 & 0.004 & 0.837 & 0.058 & 0.997 & 0.003 & 0.340 & 0.051 \\
 & GIN & 0.986 & 0.004 & 0.996 & 0.003 & 0.981 & 0.005 & 0.991 & 0.004 & 0.998 & 0.003 & 0.597 & 0.052 \\
\cline{1-14}
\multirow[t]{3}{*}{COLLAB} & GAT & 0.821 & 0.009 & 0.798 & 0.008 & 0.753 & 0.013 & 0.797 & 0.009 & 0.675 & 0.019 & 0.647 & 0.013 \\
 & GCN & 0.827 & 0.009 & 0.798 & 0.008 & 0.753 & 0.014 & 0.790 & 0.009 & 0.705 & 0.018 & 0.712 & 0.012 \\
 & GIN & 0.797 & 0.021 & 0.768 & 0.028 & 0.743 & 0.015 & 0.769 & 0.022 & 0.712 & 0.018 & 0.696 & 0.021 \\
\cline{1-14}
\multirow[t]{3}{*}{DD} & GAT & 0.580 & 0.085 & 0.747 & 0.029 & 0.598 & 0.086 & 0.696 & 0.090 & 0.721 & 0.045 & 0.579 & 0.049 \\
 & GCN & 0.691 & 0.028 & 0.746 & 0.032 & 0.678 & 0.057 & 0.767 & 0.025 & 0.731 & 0.022 & 0.606 & 0.014 \\
 & GIN & 0.683 & 0.037 & 0.601 & 0.073 & 0.670 & 0.050 & 0.594 & 0.074 & 0.710 & 0.030 & 0.597 & 0.018 \\
\cline{1-14}
\multirow[t]{3}{*}{Enzymes} & GAT & 0.470 & 0.054 & 0.415 & 0.065 & 0.530 & 0.044 & 0.406 & 0.053 & 0.414 & 0.044 & 0.173 & 0.032 \\
 & GCN & 0.563 & 0.036 & 0.566 & 0.033 & 0.615 & 0.029 & 0.448 & 0.041 & 0.410 & 0.038 & 0.169 & 0.035 \\
 & GIN & 0.572 & 0.058 & 0.332 & 0.068 & 0.607 & 0.094 & 0.338 & 0.062 & 0.400 & 0.048 & 0.182 & 0.029 \\
\cline{1-14}
\multirow[t]{3}{*}{IMDB-B} & GAT & 0.719 & 0.027 & 0.725 & 0.042 & 0.712 & 0.024 & 0.699 & 0.029 & 0.649 & 0.035 & 0.540 & 0.032 \\
 & GCN & 0.732 & 0.033 & 0.719 & 0.039 & 0.716 & 0.022 & 0.704 & 0.025 & 0.692 & 0.024 & 0.606 & 0.034 \\
 & GIN & 0.724 & 0.034 & 0.706 & 0.034 & 0.704 & 0.025 & 0.710 & 0.029 & 0.688 & 0.034 & 0.695 & 0.045 \\
\cline{1-14}
\multirow[t]{3}{*}{IMDB-M} & GAT & 0.493 & 0.023 & 0.489 & 0.017 & 0.478 & 0.028 & 0.489 & 0.019 & 0.431 & 0.029 & 0.349 & 0.026 \\
 & GCN & 0.504 & 0.021 & 0.484 & 0.018 & 0.496 & 0.015 & 0.487 & 0.018 & 0.451 & 0.026 & 0.383 & 0.021 \\
 & GIN & 0.475 & 0.047 & 0.476 & 0.032 & 0.484 & 0.025 & 0.489 & 0.019 & 0.452 & 0.031 & 0.398 & 0.041 \\
\cline{1-14}
\multirow[t]{3}{*}{MUTAG} & GAT & 0.707 & 0.051 & 0.677 & 0.083 & 0.667 & 0.042 & 0.688 & 0.038 & 0.755 & 0.087 & 0.624 & 0.079 \\
 & GCN & 0.718 & 0.044 & 0.710 & 0.081 & 0.733 & 0.061 & 0.756 & 0.061 & 0.842 & 0.075 & 0.625 & 0.059 \\
 & GIN & 0.767 & 0.140 & 0.814 & 0.068 & 0.707 & 0.056 & 0.801 & 0.068 & 0.821 & 0.078 & 0.689 & 0.042 \\
\cline{1-14}
\multirow[t]{3}{*}{MolHIV} & GAT & 0.962 & 0.003 & 0.653 & 0.151 & 0.964 & 0.001 & 0.955 & 0.005 & 0.966 & 0.001 & 0.965 & 0.001 \\
 & GCN & 0.967 & 0.002 & 0.966 & 0.001 & 0.963 & 0.002 & 0.966 & 0.001 & 0.965 & 0.001 & 0.965 & 0.001 \\
 & GIN & 0.968 & 0.002 & 0.965 & 0.001 & 0.963 & 0.001 & 0.965 & 0.002 & 0.965 & 0.001 & 0.965 & 0.001 \\
\cline{1-14}
\multirow[t]{3}{*}{NCI1} & GAT & 0.562 & 0.056 & 0.580 & 0.037 & 0.506 & 0.004 & 0.575 & 0.031 & 0.666 & 0.014 & 0.500 & 0.001 \\
 & GCN & 0.720 & 0.019 & 0.664 & 0.012 & 0.563 & 0.022 & 0.639 & 0.014 & 0.670 & 0.013 & 0.501 & 0.002 \\
 & GIN & 0.769 & 0.016 & 0.681 & 0.030 & 0.561 & 0.019 & 0.648 & 0.011 & 0.667 & 0.015 & 0.548 & 0.039 \\
\cline{1-14}
\multirow[t]{3}{*}{Peptides} & GAT & 0.604 & 0.104 & 0.297 & 0.155 & 0.418 & 0.222 & 0.410 & 0.164 & 0.582 & 0.014 & 0.515 & 0.000 \\
 & GCN & 0.671 & 0.006 & 0.673 & 0.008 & 0.651 & 0.007 & 0.633 & 0.013 & 0.610 & 0.010 & 0.515 & 0.000 \\
 & GIN & 0.681 & 0.008 & 0.661 & 0.008 & 0.653 & 0.008 & 0.659 & 0.007 & 0.570 & 0.046 & 0.520 & 0.014 \\
\cline{1-14}
\multirow[t]{3}{*}{Proteins} & GAT & 0.600 & 0.015 & 0.620 & 0.038 & 0.599 & 0.007 & 0.614 & 0.034 & 0.723 & 0.024 & 0.610 & 0.015 \\
 & GCN & 0.597 & 0.006 & 0.627 & 0.033 & 0.603 & 0.023 & 0.595 & 0.006 & 0.714 & 0.024 & 0.607 & 0.011 \\
 & GIN & 0.603 & 0.011 & 0.601 & 0.049 & 0.601 & 0.012 & 0.622 & 0.046 & 0.701 & 0.034 & 0.606 & 0.011 \\
\cline{1-14}
\multirow[t]{3}{*}{Reddit-B} & GAT & 0.730 & 0.029 & 0.595 & 0.080 & 0.520 & 0.032 & 0.678 & 0.061 & 0.700 & 0.064 & 0.518 & 0.024 \\
 & GCN & 0.781 & 0.018 & 0.742 & 0.026 & 0.540 & 0.050 & 0.794 & 0.023 & 0.823 & 0.019 & 0.580 & 0.039 \\
 & GIN & 0.699 & 0.109 &  &  & 0.564 & 0.038 & 0.693 & 0.107 & 0.699 & 0.085 & 0.561 & 0.087 \\
\cline{1-14}
\multirow[t]{3}{*}{Reddit-M} & GAT & 0.422 & 0.021 &  &  & 0.331 & 0.060 & 0.307 & 0.097 &  &  & 0.284 & 0.039 \\
 & GCN & 0.472 & 0.017 &  &  & 0.446 & 0.026 & 0.497 & 0.013 &  &  & 0.454 & 0.018 \\
 & GIN & 0.474 & 0.027 &  &  & 0.420 & 0.041 & 0.422 & 0.051 &  &  & 0.501 & 0.042 \\
\bottomrule
\end{tabular}
 \caption{\textbf{Accuracy of tuned perturbed models underlying our performance-separability computations.} 
	For Reddit-M, the complete-graph and complete-features perturbations failed to train due to memory problems. For Reddit-B, GIN failed for similar reasons.}\label{tab:performance-results-accuracy}
\end{table*}

\begin{table*}[!t]
	\centering \small
	\begin{tabular}{llrrrrrrrrrrrr}
\toprule
 &  & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\
 Dataset&  & o & o & cg & cg & eg & eg & rg & rg & cf & cf & rf & rf \\
\midrule
\multirow[t]{3}{*}{AIDS} & GAT & 0.984 & 0.012 & 0.985 & 0.011 & 0.981 & 0.010 & 0.984 & 0.010 & 0.994 & 0.008 & 0.829 & 0.141 \\
 & GCN & 0.986 & 0.010 & 0.954 & 0.020 & 0.986 & 0.009 & 0.982 & 0.011 & 0.995 & 0.006 & 0.874 & 0.055 \\
 & GIN & 0.987 & 0.011 & 0.991 & 0.011 & 0.984 & 0.010 & 0.994 & 0.006 & 0.994 & 0.008 & 0.813 & 0.160 \\
\cline{1-14}
\multirow[t]{3}{*}{COLLAB} & GAT & 0.945 & 0.005 & 0.929 & 0.004 & 0.905 & 0.003 & 0.929 & 0.003 & 0.828 & 0.015 & 0.824 & 0.010 \\
 & GCN & 0.949 & 0.004 & 0.929 & 0.003 & 0.905 & 0.003 & 0.926 & 0.003 & 0.847 & 0.013 & 0.877 & 0.008 \\
 & GIN & 0.926 & 0.022 & 0.906 & 0.034 & 0.897 & 0.007 & 0.908 & 0.023 & 0.855 & 0.013 & 0.858 & 0.017 \\
\cline{1-14}
\multirow[t]{3}{*}{DD} & GAT & 0.642 & 0.163 & 0.801 & 0.029 & 0.720 & 0.198 & 0.766 & 0.126 & 0.773 & 0.030 & 0.703 & 0.113 \\
 & GCN & 0.749 & 0.028 & 0.797 & 0.034 & 0.770 & 0.072 & 0.830 & 0.026 & 0.763 & 0.021 & 0.762 & 0.024 \\
 & GIN & 0.762 & 0.053 & 0.755 & 0.120 & 0.769 & 0.044 & 0.733 & 0.130 & 0.751 & 0.038 & 0.747 & 0.059 \\
\cline{1-14}
\multirow[t]{3}{*}{Enzymes} & GAT & 0.767 & 0.033 & 0.729 & 0.044 & 0.797 & 0.022 & 0.729 & 0.033 & 0.730 & 0.028 & 0.513 & 0.032 \\
 & GCN & 0.816 & 0.017 & 0.811 & 0.020 & 0.829 & 0.017 & 0.758 & 0.025 & 0.718 & 0.025 & 0.505 & 0.030 \\
 & GIN & 0.828 & 0.032 & 0.671 & 0.047 & 0.840 & 0.053 & 0.682 & 0.044 & 0.722 & 0.033 & 0.534 & 0.031 \\
\cline{1-14}
\multirow[t]{3}{*}{IMDB-B} & GAT & 0.811 & 0.030 & 0.822 & 0.036 & 0.803 & 0.025 & 0.791 & 0.026 & 0.694 & 0.035 & 0.561 & 0.037 \\
 & GCN & 0.837 & 0.028 & 0.821 & 0.037 & 0.803 & 0.023 & 0.793 & 0.026 & 0.741 & 0.037 & 0.644 & 0.042 \\
 & GIN & 0.811 & 0.040 & 0.795 & 0.039 & 0.789 & 0.026 & 0.798 & 0.030 & 0.742 & 0.036 & 0.759 & 0.053 \\
\cline{1-14}
\multirow[t]{3}{*}{IMDB-M} & GAT & 0.687 & 0.024 & 0.673 & 0.019 & 0.668 & 0.022 & 0.683 & 0.020 & 0.597 & 0.022 & 0.527 & 0.023 \\
 & GCN & 0.704 & 0.020 & 0.668 & 0.019 & 0.684 & 0.021 & 0.679 & 0.020 & 0.623 & 0.029 & 0.566 & 0.019 \\
 & GIN & 0.671 & 0.037 & 0.658 & 0.027 & 0.674 & 0.018 & 0.677 & 0.019 & 0.623 & 0.028 & 0.594 & 0.036 \\
\cline{1-14}
\multirow[t]{3}{*}{MUTAG} & GAT & 0.836 & 0.063 & 0.761 & 0.119 & 0.867 & 0.081 & 0.840 & 0.062 & 0.898 & 0.062 & 0.618 & 0.110 \\
 & GCN & 0.846 & 0.048 & 0.744 & 0.075 & 0.814 & 0.068 & 0.825 & 0.058 & 0.908 & 0.066 & 0.585 & 0.092 \\
 & GIN & 0.874 & 0.155 & 0.874 & 0.067 & 0.814 & 0.075 & 0.865 & 0.048 & 0.882 & 0.078 & 0.838 & 0.079 \\
\cline{1-14}
\multirow[t]{3}{*}{MolHIV} & GAT & 0.689 & 0.016 & 0.633 & 0.048 & 0.452 & 0.022 & 0.654 & 0.019 & 0.685 & 0.023 & 0.359 & 0.017 \\
 & GCN & 0.726 & 0.014 & 0.630 & 0.011 & 0.588 & 0.007 & 0.645 & 0.016 & 0.691 & 0.028 & 0.361 & 0.018 \\
 & GIN & 0.777 & 0.013 & 0.696 & 0.022 & 0.590 & 0.019 & 0.646 & 0.023 & 0.670 & 0.028 & 0.409 & 0.028 \\
\cline{1-14}
\multirow[t]{3}{*}{NCI1} & GAT & 0.572 & 0.135 & 0.680 & 0.051 & 0.679 & 0.017 & 0.664 & 0.065 & 0.727 & 0.016 & 0.683 & 0.016 \\
 & GCN & 0.797 & 0.017 & 0.735 & 0.013 & 0.647 & 0.029 & 0.685 & 0.017 & 0.731 & 0.017 & 0.648 & 0.022 \\
 & GIN & 0.843 & 0.014 & 0.741 & 0.031 & 0.640 & 0.034 & 0.701 & 0.022 & 0.722 & 0.016 & 0.656 & 0.024 \\
\cline{1-14}
\multirow[t]{3}{*}{Peptides} & GAT & 0.553 & 0.042 & 0.401 & 0.069 & 0.444 & 0.093 & 0.446 & 0.065 & 0.548 & 0.006 & 0.401 & 0.006 \\
 & GCN & 0.582 & 0.004 & 0.596 & 0.006 & 0.561 & 0.008 & 0.580 & 0.006 & 0.549 & 0.008 & 0.400 & 0.007 \\
 & GIN & 0.591 & 0.005 & 0.571 & 0.012 & 0.565 & 0.004 & 0.573 & 0.007 & 0.536 & 0.008 & 0.447 & 0.027 \\
\cline{1-14}
\multirow[t]{3}{*}{Proteins} & GAT & 0.775 & 0.018 & 0.752 & 0.042 & 0.786 & 0.016 & 0.764 & 0.035 & 0.758 & 0.022 & 0.720 & 0.030 \\
 & GCN & 0.782 & 0.021 & 0.782 & 0.027 & 0.773 & 0.028 & 0.780 & 0.024 & 0.758 & 0.027 & 0.730 & 0.031 \\
 & GIN & 0.782 & 0.028 & 0.735 & 0.092 & 0.736 & 0.061 & 0.751 & 0.046 & 0.742 & 0.034 & 0.765 & 0.032 \\
\cline{1-14}
\multirow[t]{3}{*}{Reddit-B} & GAT & 0.798 & 0.022 & 0.763 & 0.049 & 0.749 & 0.148 & 0.740 & 0.088 & 0.721 & 0.082 & 0.624 & 0.178 \\
 & GCN & 0.846 & 0.014 & 0.817 & 0.019 & 0.639 & 0.195 & 0.875 & 0.016 & 0.862 & 0.015 & 0.797 & 0.040 \\
 & GIN & 0.748 & 0.066 &  &  & 0.734 & 0.097 & 0.783 & 0.051 & 0.727 & 0.059 & 0.773 & 0.039 \\
\cline{1-14}
\multirow[t]{3}{*}{Reddit-M} & GAT & 0.717 & 0.014 &  &  & 0.733 & 0.053 & 0.644 & 0.120 &  &  & 0.717 & 0.036 \\
 & GCN & 0.766 & 0.009 &  &  & 0.759 & 0.013 & 0.805 & 0.006 &  &  & 0.777 & 0.009 \\
 & GIN & 0.770 & 0.018 &  &  & 0.742 & 0.035 & 0.746 & 0.031 &  &  & 0.797 & 0.016 \\
\bottomrule
\end{tabular}
 	\caption{\textbf{AUROC of tuned perturbed models underlying our performance-separability computations.}
	For Reddit-M, the complete-graph and complete-features perturbations failed to train due to memory problems. For Reddit-B, GIN failed for similar reasons.
	}\label{tab:performance-results-auroc}
\end{table*}

\begin{table*}[h]
	\small
	\centering\renewcommand{\arraystretch}{1.2} 
    \begin{tabular}{ll}
        \toprule
        \textbf{Component} & \textbf{Specifications} \\
        \midrule
        \emph{Available CPUs} & Intel Xeon (Haswell, Broadwell, Skylake, Cascade Lake, Sapphire Rapids, Emerald Rapids) \\
                           & Intel Xeon (6134, 6248R, 6142M, 6128, 6136, E5620) \\
                           & Intel Platinum (8280L, 8468, 8562Y+) \\
                           & AMD Opteron (6164 HE, 6234, 6376 (x2), 6272, 6128) \\
                           & AMD EPYC (7742, 7713, 7413, 7262) \\
        \emph{Available GPUs}  & NVIDIA Tesla (K80, P100, V100, A100, H100, H200) \\
                            & NVIDIA Quadro (RTX 8000, RTX 6000) \\
                            & AMD MI100 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Summary of Compute Resources.} Especially when tuning our models used to generate the \emph{performance separability} results as seen in \cref{tab:performance-results-accuracy,tab:performance-results-auroc}, we relied heavily on high performance cluster units at multiple institutions that supported our work with the above hardware specifications.}
\end{table*}

\subsubsection{Performance-Separability Results}

Reasoning first principles, we assert that the most instructive graph-learning datasets should have a separable original mode. In other words, we would like both the features and the graph structure to be informative and necessary to solve a given task. When a perturbed mode either outperforms or is non-separable from the original, then we can say that the perturbed mode is non-essential for the task at hand. 

Consider the \emph{Accuracy} and \emph{AUROC} columns of \cref{tab:performance-separability-ks}. (Overall we find \emph{AUROC} to be more expressive in determining separability, but we include both in our classifications.) Note that when training on $\eg$, we are removing any structural information. Comparing the performance of $\eg$ to that of the original, we can determine whether or not the structural information is being used at all. For example, for the \emph{ENZYMES} dataset, we see that $eg > o$, which implies that the graph structure is unnecessary for solving the current classification task. Conversely, we have some tasks for which only the structural mode is essential. \emph{MUTAG} is one example where $cf$ and $o$ are not separable, implying that node features are not essential and the task can be completed based on the graph structure alone. Given their dependency on a graph mode, we would consider this type of dataset interesting for structural tasks, but we would prefer benchmarks that combine structural \emph{and} feature information. 

Moving on to the \emph{Structure} and \emph{Feature} columns of \Cref{tab:performance-separability-ks}, we formalize the notion of whether or not each mode is informative for each dataset and associated task. Let us consider the structural perturbations \{$\Phi_{eg},\Phi_{rg},\Phi_{rg}$\} and the performances of their tuned models. If the perturbed model performances are separably lower than the original, then we can classify the \emph{structure} as informative (e.g., COLLAB). Similarly, turning to feature perturbations \{$\Phi_{cf},\Phi_{rf}$\} and their associated performances, if the perturbed model performances are separably lower than the original, then the \emph{features} are considered informative (e.g. ENZYMES).
In the case that AUROC and accuracy yield different outcomes for whether or not a mode is informative then this is denoted as \emph{(un)informative}. \vspace*{-1em}

\begin{table}[h!]\label{tab:complementarity-config}
	\centering
	\renewcommand{\arraystretch}{1.2} \setlength{\tabcolsep}{10pt} 
	\begin{tabular}{lr}
    \toprule
    \textbf{Parameter} & \textbf{Values} \\
    \midrule
    \text{Feature Metric} & \texttt{Euclidean} \\
    \text{Perturbations} & $\{ \id, \PerturbGraph_{e\ast}, \PerturbGraph_{c\ast}, \PerturbGraph_{r\ast} \}$ \\
    \text{Structural Metric} & \texttt{Diffusion}\\
    \text{Matrix Norm} & $L_{1,1}$ \\
    \text{Diffusion Steps} & $\{1, \cdots, 10 \}$ \\
    \text{Seed*} & $0, 2^1, 2^2, 2^3, 2^4$ \\
    \bottomrule
\end{tabular} 	\caption{\textbf{Complementarity-calculation setup for randomized mode perturbations.} 
		 We compute complementarity under perturbations $\complementarity^{1,1}_{\PerturbGraph}$ as specified in the above table, with $\ast \in \{f,g\}$. 
		 Without being subject to gradient descent in our calculations, we can also compute $\complementarity^{1,1}_{ef}$. For a definition of \texttt{Diffusion}, see \cref{def:diffusion-distance}  and a note our treatment of disconnected graphs in \cref{apx:methods}. Finally, seed is only varied over the random perturbations $\PerturbGraph_{r\ast}$ and our results are aggregated based on the average complementarity. 
	}
\end{table}

For the judgment depicted in the \emph{Evaluation} column of \cref{tab:performance-separability-ks}, 
we score datasets on a scale from $0$ to $5$ as 
$	1.5 \cdot S + 1 \cdot F$,
where $S,F\in\{0,1,2\}$, mapping ``uninformative'' to $0$, ``(un)informative'' to $1$, and informative to $2$, and giving the structural mode higher weight than the feature mode because we are dealing with \emph{graph}-learning datasets. 
The final evaluation results from binning the scores as
\vspace*{-0.4em}\begin{align*}
	[0,1] \mapsto~\texttt{--},~
		(1,2] \mapsto~\texttt{-},\\
			(2,3] \mapsto~\circ,~
				(3,4] \mapsto~\texttt{+},~
					(4,5] \mapsto~\texttt{++}.
\end{align*}
While we include this categorization to simplify the message conveyed by our results, 
we recommend considering the full, uncondensed separability results when assessing the quality of individual datasets. 



\subsubsection{Performance-Separability Evaluation}

For each dataset, we evaluate the \emph{performance separability} between its perturbed versions using two different statistics in our permutation tests (Kolmogorov-Smirnov and Wilcoxon rank-sum) as well as two different $\alpha$-levels as significance cutoffs ($0.01$ and $0.005$). 
Notably, we see very few differences in the separability results and the associated classification of datasets,
confirming the robustness of our approach.




\begin{table*}[!t]
	\small
	\centering 
	\begin{tabular}{llllll}
\toprule
Dataset & Accuracy & AUROC & Structure & Features & Evaluation \\
\midrule
AIDS & cf $>$ cg $>$ rg $>$ o $>$ eg $>$ rf & cf/cg/eg/o/rg $>$ rf & uninformative & uninformative & \texttt{--} \\
COLLAB & o $>$ cg/rg $>$ eg $>$ cf/rf & o $>$ cg/rg $>$ eg $>$ rf $>$ cf & informative & informative & \texttt{++} \\
DD & rg $>$ cg $>$ cf $>$ eg/o $>$ rf & rg $>$ cf/cg/eg/o/rf & uninformative & uninformative & \texttt{--} \\
Enzymes & eg $>$ cg/o $>$ rg $>$ cf $>$ rf & eg/o $>$ cg $>$ rg $>$ cf $>$ rf & uninformative & informative & \texttt{-} \\
IMDB-B & cf/cg/eg/o/rf/rg & cg/o $>$ eg/rg $>$ cf/rf & uninformative & (un)informative & \texttt{--} \\
IMDB-M & cg/eg/o/rg $>$ cf $>$ rf & o $>$ eg/rg $>$ cg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
MUTAG & cf/cg/eg/o/rg $>$ rf & cf/cg/eg/o/rf/rg & uninformative & uninformative & \texttt{--} \\
MolHIV & o $>$ cf/cg/rg $>$ rf $>$ eg & o $>$ cf/cg $>$ rg $>$ eg $>$ rf & informative & informative & \texttt{++} \\
NCI1 & o $>$ cg $>$ cf $>$ rg $>$ eg/rf & o $>$ cg $>$ cf $>$ rg $>$ eg/rf & informative & informative & \texttt{++} \\
Peptides & o $>$ cg $>$ rg $>$ eg $>$ cf $>$ rf & cg $>$ o $>$ rg $>$ eg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
Proteins & cf $>$ cg/eg/o/rf/rg & cf/cg/eg/o/rf/rg & uninformative & uninformative & \texttt{--} \\
Reddit-B & cf $>$ rg $>$ o $>$ cg $>$ eg/rf & rg $>$ cf $>$ o $>$ cg/eg/rf & uninformative & uninformative & \texttt{--} \\
Reddit-M & rf/rg $>$ o $>$ eg & rg $>$ rf $>$ o $>$ eg & uninformative & uninformative & \texttt{--} \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Measuring \emph{performance separability} between different versions of the same dataset: Wilcoxon rank-sum test.}
		Supplementing \cref{tab:performance-separability-ks}, 
		we use the Wilcoxon rank-sum as a test statistic to replace the KS statistic in our permutation tests, 
		with an otherwise identical setup ($10\,000$ random permutations; $\alpha$-level of $0.01$; Bonferroni correction within each dataset). 
		The results are substantively the same, with the exceptions that IMDB-B gets downgraded from fair to poor, and MUTAG gets downgraded from poor to very poor.
	}\label{tab:performance-separability-wilcoxon}
\end{table*}

\begin{table*}[!t]
	\small
	\centering 
	\begin{tabular}{llllll}
\toprule
Dataset & Accuracy & AUROC & Structure & Features & Evaluation \\
\midrule
AIDS & cf $>$ cg $>$ rg $>$ eg/o $>$ rf & cf/cg/rg $>$ eg/o $>$ rf & uninformative & uninformative & \texttt{--} \\
COLLAB & o $>$ cg/rg $>$ eg $>$ cf/rf & o $>$ cg/rg $>$ eg $>$ rf $>$ cf & informative & informative & \texttt{++} \\
DD & rg $>$ cg $>$ cf $>$ eg/o $>$ rf & rg $>$ cf/cg/eg/o/rf & uninformative & uninformative & \texttt{--} \\
Enzymes & eg $>$ cg/o $>$ rg $>$ cf $>$ rf & eg/o $>$ cg $>$ rg $>$ cf $>$ rf & uninformative & informative & \texttt{-} \\
IMDB-B & cf/cg/eg/o/rf/rg & o $>$ cg $>$ eg/rg $>$ cf/rf & (un)informative & (un)informative & $\circ$ \\
IMDB-M & cg/eg/o/rg $>$ cf $>$ rf & o $>$ cg/eg/rg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
MUTAG & cf/cg/o/rg $>$ eg $>$ rf & cf/o $>$ cg/eg/rf/rg & (un)informative & uninformative & \texttt{-} \\
MolHIV & o $>$ cf/cg/rg $>$ rf $>$ eg & o $>$ cf/cg $>$ rg $>$ eg $>$ rf & informative & informative & \texttt{++} \\
NCI1 & o $>$ cg $>$ cf $>$ rg $>$ eg $>$ rf & o $>$ cg $>$ cf $>$ rg $>$ eg/rf & informative & informative & \texttt{++} \\
Peptides & o $>$ cg $>$ rg $>$ eg $>$ cf $>$ rf & cg $>$ o $>$ rg $>$ eg $>$ cf $>$ rf & (un)informative & informative & \texttt{+} \\
Proteins & cf $>$ cg/eg/o/rf/rg & cf/cg/eg/o/rf/rg & uninformative & uninformative & \texttt{--} \\
Reddit-B & cf $>$ o/rg $>$ cg $>$ eg/rf & rg $>$ cf $>$ o $>$ cg/eg/rf & uninformative & uninformative & \texttt{--} \\
Reddit-M & rf/rg $>$ o $>$ eg & rg $>$ rf $>$ o $>$ eg & uninformative & uninformative & \texttt{--} \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Measuring \emph{performance separability} between different versions of the same dataset: $\mathbf{\alpha} \mathbf{\leq} \mathbf{0.005}$.}
		Supplementing \cref{tab:performance-separability-ks}, 
		we test at an  $\alpha$-level of $0.005$ instead of $0.01$, using an otherwise identical setup ($10\,000$ random permutations; KS statistic; Bonferroni correction within each dataset). 
		The results are substantively identical to \cref{tab:performance-separability-ks}.
	}\label{tab:performance-separability-ks-005}
\end{table*}



\subsection{Extended Mode Complementarity}

In our mode-complementarity computations, 
we need to distinguish fixed modes (original, empty, complete) from randomized modes (random, shuffled). 
To compute mode complementarity for the randomized modes, 
we use five different randomization seeds for each graph, resulting in the setup summarized in \cref{tab:complementarity-config}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{Figures/complementarity_fixed-random_pi-95_ogb}
	\caption{\textbf{Comparing \emph{levels of mode complementarity} across different versions of the same dataset.} We show the mean (dot) and $95$th percentile intervals (bars) of complementarity scores for the original version as well as $4$ deterministic perturbations (top) and $4$ randomized perturbations (bottom) of $7$ OGB datasets, computed with $t=1$ diffusion steps.
		Black horizontal lines indicate mean mode complementarities of the original dataset, and the silver horizontal line marks the $0.5$ threshold relevant for assessing mode diversity. 
		Note that $\complementarity_\text{eg} = 1 - \complementarity_\text{cg}$ and  $\complementarity_\text{ef} = 1 - \complementarity_\text{cf}$ by definition. 
		Contrasting with the variation apparent in \cref{fig:mode-complementarity}, 
		the OGB mol-$\ast$ datasets are remarkably similar in their mode-complementarity profiles. 
	}\label{fig:mode-complementarity-ogb}
\end{figure}

To study how perturbations impact the complementarity of a dataset, we refer to \Cref{fig:mode-complementarity} (see \cref{fig:mode-complementarity-ogb} for a supplementary visualization of mode complementarities computed for $7$ OGB datasets). 
In the top panel, we compare the original complementarity score using the $L_{1,1}$ norm, $\complementarity^{1,1}$, to the complementarity under fixed perturbations $ \complementarity^{1,1}_{\PerturbGraph}$, where $\PerturbGraph \in \{\cg,\eg,\cf,\ef \}$, with the randomized perturbations shown in the bottom panel. Note the symmetry introduced by \Cref{thm:perturbation-duality}, i.e., that $\complementarity^{1,1}_{e\ast} = 1 - \complementarity^{1,1}_{c\ast}$. 
These perturbations, as per \Cref{prop:perturbation-duality}, effectively measure the self-complementarity of both modes, thus providing insights into the diversity of either the graph structure or the features contained in a given dataset. In particular, extreme $\complementarity^{1,1}_{e\ast}$ scores imply that the metric space of the dual mode is uninteresting (i.e., it lacks any geometric diversity in the distance matrix). We see this phenomenon, for example, in datasets with high $\complementarity^{1,1}_{ef}$ such as IMDB-B and IMDB-M, both notably ego-networks. Similarly, we note datasets with extreme $\complementarity^{1,1}_{eg}$, including DD, whose node features are derived artificially from node degree. We formalize this notion of mode diversity in the following section.

\subsubsection{Mode Complementarity and Mode Diversity}

In \Cref{tab:mode-diversity}, we compute $\Delta^{p,q}_\ast$ for each graph in the dataset and then compute the mean $\mu$ and the standard deviation $\sigma$ of these scores. A low mean for a mode is indicative of low diversity across the dataset, meaning we see little variation in the geometric structure of the corresponding metric space. 
To arrive at our symbolic scoring for $\mu$, 
we divide the interval $[0,1]$ into five equal-width bins as $[0,0.2)$, $[0.2,0.4)$, $[0.4,0.6)$, $[0.6,0.8)$, $[0.8,1.0]$. 
We proceed similarly for $\sigma$, using smaller brackets due to its different scale: $[0.0,0.05)$, $[0.05,0.1)$, $[0.1,0.15)$, $[0.15,0.2)$, $[0.2,1.0)$. 
As with our symbolic performance-separability scoring, these categories primarily serve to convey our main message, 
but the numerical values (which do not exhibit discontinuities) should be preferred when conducting in-depth dataset evaluations. 

Simply put, datasets with (very) good diversity are candidates for new tasks that could leverage the diversity seen in these modes. In particular, we are interested in datasets that performed poorly in the performance separability evaluation (see \Cref{tab:performance-separability-ks}) but have high structural diversity. This type of dataset may have potential as a graph-learning benchmark---if remodeled or assigned a new task that better leverages the information in both modes. 
As depicted in the second row of our taxonomy table in \cref{experiments:taxonomy}, these datasets are AIDS, DD, MUTAG, Reddit-B, and Reddit-M.


\subsubsection{Mode Complementarity and Performance}

\begin{figure}[!t]
	\centering
	\includegraphics[width=\linewidth]{Figures/complementarity-vs-performance_test_accuracy_best-only}
	\caption{\textbf{Mode complementarity and performance: Accuracy as a performance measure.} 
		We show the mean accuracy (y) as a function of the mean mode complementarity (x), 
		for the original version and $5$ perturbations of $13$ graph-learning datasets, 
		based on our best-on-average models (as in \cref{fig:performance-separability}). 
		Each marker represents a (dataset, perturbation, $t$) tuple, where $t \in[10]$ is the number of diffusion steps used in the computation of diffusion distances on the graph. 
		Higher mean mode complementarity appears to be associated with higher accuracy, 
		and datasets differ in the range of their mode-complementarity shifts.
	}\label{fig:mode-complementarity-vs-performance-accuracy}
\end{figure}

Our intuition is that datasets with high complementarity (different and high information content across the two modes) should perform better. 
In line with this expectation, we observe a generally positive relationship between mode complementarity and performance when measured by both AUROC and accuracy, as visualized in \Cref{fig:mode-complementarity-vs-performance} and \Cref{fig:mode-complementarity-vs-performance-accuracy}, respectively. This correlation is further quantified in \Cref{tab:mode-complementarity-vs-performance-cor}, 
where we symbolically categorize the observed correlations by creating five equal-width bins in the interval $[-1.0,1.0]$, i.e., $[-1.0,-0.6)$, $[-0.6, -0.2)$, $[-0.2,0.2)$, $[0.2,0.6)$, $[0.6,1.0]$. 
Again, this categorization only serves to communicate our main message (here: that the correlations are mostly positive and substantively robust to different ways of measurement), 
but the numerical values should be consulted for gaining detailed insights. 


Taking a closer look \Cref{fig:mode-complementarity-vs-performance}, we can observe how the relationship between mode complementarity and performance changes across mode perturbations (drawn as different shapes). Note that most datasets categorized as good graph-learning benchmarks (such as Peptides and NCI1) exhibit a stronger positive trend between complementarity and performance among their perturbations. We see an even stronger association with some datasets when the original mode (denoted by a circle) is not only the best performer but also has the highest complementarity.

For a given perturbed dataset, \Cref{fig:mode-complementarity-vs-performance} also shows the changes over different diffusion steps (i.e., the horizontal movement of points with the same color and shape). This has a variable effect on the perturbed $\complementarity^{1,1}_\ast$ score. This variation merits further investigation, but we can already note some initial interesting patterns. For example, the $\complementarity^{1,1}_{\rf}$ scores for MolHIV, Peptides, ENZYMES, MUTAG hardly change over diffusion, while   the $\complementarity^{1,1}_{\cf}$ score is more sensitive to the diffusion process. This may indicate that the metric spaces that arise from the graph modes are more similar to the metric spaces that arise from random features. This would suggest using $\cf$ (as an ``uninteresting'' metric comparison) to pick up more signal over a diffusion process that occurs in GNNs.


\begin{table*}[t]
	\centering\small
	\begin{tabular}{lrrrrrrlllllll}
\toprule
 & \multicolumn{3}{r}{Accuracy Values} & \multicolumn{3}{r}{AUROC Values} & \multicolumn{3}{r}{Accuracy} & \multicolumn{3}{r}{AUROC} & Evaluation \\
 & $p$ & $s$ & $k$ & $p$ & $s$ & $k$ & $p$ & $s$ & $k$ & $p$ & $s$ & $k$ &  \\
Dataset &  &  &  &  &  &  &  &  &  &  &  &  &  \\
\midrule
AIDS & 0.51 & 0.33 & 0.23 & 0.44 & 0.29 & 0.19 & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & $\circ$ & \texttt{+} \\
COLLAB & 0.21 & 0.28 & 0.19 & 0.22 & 0.25 & 0.18 & \texttt{+} & \texttt{+} & $\circ$ & \texttt{+} & \texttt{+} & $\circ$ & \texttt{+} \\
DD & -0.17 & -0.31 & -0.20 & -0.27 & -0.24 & -0.16 & $\circ$ & \texttt{-} & $\circ$ & \texttt{-} & \texttt{-} & $\circ$ & \texttt{-} \\
Enzymes & 0.48 & 0.41 & 0.29 & 0.47 & 0.38 & 0.26 & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} \\
IMDB-B & 0.33 & 0.28 & 0.19 & 0.57 & 0.50 & 0.34 & \texttt{+} & \texttt{+} & $\circ$ & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} \\
IMDB-M & 0.50 & 0.59 & 0.40 & 0.58 & 0.67 & 0.46 & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{++} & \texttt{+} & \texttt{+} \\
MUTAG & 0.32 & 0.29 & 0.20 & 0.11 & 0.12 & 0.08 & \texttt{+} & \texttt{+} & \texttt{+} & $\circ$ & $\circ$ & $\circ$ & \texttt{+} \\
MolHIV & 0.14 & 0.19 & 0.13 & 0.61 & 0.55 & 0.39 & $\circ$ & $\circ$ & $\circ$ & \texttt{++} & \texttt{+} & \texttt{+} & \texttt{+} \\
NCI1 & 0.66 & 0.69 & 0.48 & 0.47 & 0.59 & 0.40 & \texttt{++} & \texttt{++} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{+} & \texttt{++} \\
Peptides & 0.65 & 0.37 & 0.25 & 0.71 & 0.52 & 0.36 & \texttt{++} & \texttt{+} & \texttt{+} & \texttt{++} & \texttt{+} & \texttt{+} & \texttt{++} \\
Proteins & 0.21 & 0.20 & 0.12 & 0.09 & 0.10 & 0.07 & \texttt{+} & $\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$ & $\circ$ \\
Reddit-B & 0.44 & 0.32 & 0.24 & 0.12 & 0.30 & 0.21 & \texttt{+} & \texttt{+} & \texttt{+} & $\circ$ & \texttt{+} & \texttt{+} & \texttt{+} \\
Reddit-M & -0.36 & -0.46 & -0.33 & -0.33 & -0.36 & -0.24 & \texttt{-} & \texttt{-} & \texttt{-} & \texttt{-} & \texttt{-} & \texttt{-} & \texttt{-} \\
\bottomrule
\end{tabular}
 	\caption{\textbf{Mode complementarity and performance: Correlation statistics.} 
		We show the Pearson correlation between mode complementarity and test accuracy resp. AUROC for $13$ graph-learning datasets, 
		taken over $5$ perturbations and $t\in[10]$ diffusion steps, 
		based on our best-on-average models (as in \cref{fig:performance-separability}). 
	}\label{tab:mode-complementarity-vs-performance-cor}
\end{table*}
 


\section{Related Work}
\label{apx:related-work}

Extending the discussion of related work begun in the main text, 
there are three relevant related lines of work followed in the graph-learning community, namely, 
\begin{inparaenum}[(i)]
  \item \emph{data-centric and multiverse approaches in machine learning},
  \item \emph{graph-learning benchmark datasets}, and
  \item \emph{graph-learning evaluations}.
\end{inparaenum}
Overall, we find that \framework provides a unique perspective on the challenges discussed in these fields.

\paragraph{Data-Centric and Multiverse Approaches.}
This category comprises works that assume a data-centric perspective, potentially imbued with the \emph{multiverse} notion, i.e., the notion that \emph{any} data-analysis task necessitates an elaborate analysis of choices and ``non-choices,'' thus resulting not in a \emph{single} outcome but a \emph{multiverse} of outcomes.
This is a novel perspective, originally arising from psychology, gaining traction in general machine-learning applications~\citep{Simson2023, biderman2020, wayland2024mapping, Germani2023}, that serves to highlight the \emph{impact} of different decisions, such as data preprocessing or model selection, on the outcome.
Even more broadly, \citet{Mazumder23a} present a call for \emph{data-centric machine learning}, emphasizing the need for considering foremost the \emph{data}, including its quality and provenance, in the development cycle of machine-learning models.

A crucial aspect of any data-centric approach is the development of suitable measures or metrics for the comparison of graphs or their respective~(latent) representations.
We find several prior works here~\citep{zambon_graph_2020, lin_graph_2023, tang_robust_2023}, but by their design, such measures cannot focus \emph{beyond} the comparison of individual graphs, remaining instead \emph{intrinsic} with respect to a specific dataset.
Our \emph{mode complementarity} overcomes this restriction by adopting a metric-space perspective.
While some aspects of metric spaces based on graph structure have been studied~\citep{taha_normed_2023, sonthalia_tree_2020, chuang_tree_2022, Sanmartn2022}, the focus lies on \emph{embeddings} or \emph{robust shortest paths}, whereas our work is concerned with harnessing metric-space information to provide insights into the \emph{interplay} of graph structure and node features. 
Thus, our framework might also benefit from integrating \emph{metric space magnitude} \citep{limbeck2024metric}, 

\paragraph{Graph-Learning Benchmark Datasets.}
Several publications also present new benchmark datasets, driven either by the observation that existing datasets do not cover a sufficiently ``dense'' part of the graph-learning landscape~\citep{palowitch2022graphworld}, or aiming to present more challenging tasks that serve to explore the limitations of existing architectures~\citep{Dwivedi2022, Akbiyik2023}.
For example, in the context of \emph{node-classification tasks}, new datasets are proposed to assess the performance of GNNs in heterophilous~(or, more precisely, non-homophilous) regimes~\citep{lim2021large, luan2023when, mao2023demystifying, platonov2023critical, platonov2023characterizing, luan2022revisiting}, typically drawing upon graph-level measures from \emph{network science} \cite{newman2018networks,barabasi2016network}.

However, with \emph{mode complementarity}, we seek a score that 
\begin{inparaenum}[(1)]
	\item treats graph structure and node features as equal,
	\item works on graphs without node labels and does not make any assumptions about the spaces arising from edge connectivity and node features, and 
	\item specifically informs graph-level learning tasks such as \emph{graph classification}.
\end{inparaenum}
To the best of our knowledge, we are the first to propose a score fulfilling
these desiderata. 

\paragraph{Graph-Learning Evaluations.}
Previous work on \emph{evaluating} graph learning can be broadly categorized into papers that, like ours, criticize the status quo in terms of dataset usage, the data quality as such, or specific aspects of a given GNN architecture.
For the first category, we find works that criticize a lack of suitable baseline comparison partners~\citep{Cai18a}, issues with hyperparameter tuning and model selection~\citep{Tonshoff2023, Errica20a, Zhao2020}, or problems with data--model mismatch~\citep{Chen2019}.
The second category comprises works that highlight the use of unsuitable datasets~\citep{Li_2024, Bechler-Speicher23a}, a problem that is also of relevance to other areas in machine learning~\citep{Ding2018, fenza_data_2021}.
The third category, dealing with the shortcomings of existing architectures, is of particular relevance to practitioners, since it either inspires new research directions or provides practical guidance concerning which models to use in a specific application.

Among the different shortcomings, issues inherent to the message-passing paradigm are well-studied~\citep{alon_bottleneck_2021, di_giovanni_over-squashing_2023, yang_new_2022}, often leading to improved architectures~\citep{michel_path_2023, chen_sa-mlp_2022, han_structural_2023}, with a recent trend being the development of methods that obviate message passing~\citep{fan_structured_2020}.
Beyond our brief categorization, we also observe interest in general GNN ``explainability'' strategies~\citep{agarwal_evaluating_2023, wang_gnninterpreter_2024, bonabi_mobaraki_demonstration_2023, toyokuni_structural_2023, faber_when_2021, li_explainability_2022, rathee_bagel_2022, xie_task-agnostic_2022}.
However one has to bear in mind that such approaches are often tightly coupled to a \emph{specific} task and a \emph{specific} architecture, which, while valuable, cannot help in overcoming dataset deficiencies.

  


\end{document}
