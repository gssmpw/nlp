\section{Related work}
\label{sec:introduction_related_work}
The so-called ``training'' of plain vanilla neural networks can be interpreted as solving a non-linear regression problem, i.e., to fit the parameters of a specific non-linear function $\mathbb{R}^N\to\mathbb{R}^M$ (in this case the neural network) such that a loss functional is minimized **Cybenko, "Approximation by Superposition of a Step Function"**. It is well known that neural networks allow for universal approximation of properties, that is, they can approximate certain function classes up to arbitrary accuracy as long as the network is wide enough; see, e.g., **Hornik, Stinchcombe, and White, "Multilayer feedforward networks are universal approximators"**. 
Many results provide guaranteed convergence rates **Barron, "Universal approximation bounds for superpositions of a sigmoidal function"**, with a particular focus on beating the so-called \emph{curse of dimensionality} in the parameter space.

However, using NNs to approximate the parameter-to-solution map requires architectures that account for the high, or even infinite, dimensionality of both the input and output spaces. In the last years, several of these have been proposed, e.g. **Li et al., "Fourier Neural Operator for Parametric Partial Differential Equations"**,**Chen et al., "DeepONets: Learning Discretization-Free Continuous-Discrete Operators by Neural Networks with Fourier Features"**,**Gao et al., "Graph Neural Operator for Learning on Graphs through Controllable Vertex Importance Sampling"**,**Liao et al., "Convolutional Neural Operator for Parametric Partial Differential Equations"**,**Meng and Wang, "POD-NN: A Deep Learning Framework for Nonlinear Model Order Reduction"**. The latter is also known as the Galerkin POD-NN, and is the focus of this work.
Though not the framework considered in this work, we mention in passing 
that the Galerkin POD-NN has been extended to time-dependent 
problems, see e.g. **Hesthaven et al., "Deep Learning for Nonlinear Model Order Reduction"**.


In order to appropriately deal with the parameter-to-solution map
$\mathcal{U} \ni \nu \mapsto u(\nu) \in \mathcal{Y}$ and effectively build a suitable method that leverages on NNs
for its approximation, the complexity of both the input
and output spaces needs to be appropriately described by a 
\emph{encoder-decoder} pair.
That is, we construct maps $\mathscr{E}: \mathcal{U} \rightarrow \mathbb{R}^N $, $\boldsymbol{\pi}: \mathbb{R}^N \rightarrow\mathbb{R}^M $, and
$\mathscr{D}: \mathbb{R}^M \rightarrow \mathcal{M}$ such that
\begin{equation}\label{eq:diagram}
	\mathcal{X}
	\supset
	\mathcal{U}
	\xrightarrow{\text{Encoder }\mathscr{E} }	
	\mathbb{R}^N
	\xrightarrow{\text{Neural Network }\boldsymbol{\pi}}	
	\mathbb{R}^M
	\xrightarrow{\text{Decoder }\mathscr{D}}
	\mathcal{M}
	\subset
	\mathcal{Y}.
\end{equation}
Then, for each input $\nu \in \mathcal{U}$ one can construct
an approximation of the parameter-to-solution map  which reads
as $u(\nu) \approx \left(\mathscr{D} \circ \boldsymbol{\pi} \circ \mathscr{E}\right) (\nu)$.
Indeed, the task of operator learning boils down to defining the encoder-decoder pair and
computing an appropriate NN $\boldsymbol{\pi}$. This NN is of hopefully moderate size.
This is understood not only in the sense that $N$ and $M$ are controlled, but also in terms of the overall network size, which is determined by the number of trainable parameters.



The Galerkin POD-NN method relies on the combination
of projection-based model order reduction techniques
for the construction of the decoder, in particular, the reduced basis method **Maday and Patera, "Reduced Basis Approximation"** and, of course, NNs.
The reduced basis method, which is at the core of the methodology presented here, follows a two phase paradigm--online
and offline--for the swift and efficient evaluation of the
parameter-to-solution map.
In the offline phase, a basis of
reduced dimension is computed by properly sampling the space $\mathcal{U}$ and performing
a proper orthogonal decomposition (POD), although \emph{greedy} strategies could also be put in place. 
These allow for the identification of the most important modes driving the dynamics of the parameter-to-solution map.
Next, in the online phase, the evaluation of the parameter-to-solution map for a given parametric input is computed in a variational fashion as an element of the reduced space. For this purpose,
hyper-reduction techniques, such as the empirical interpolation method **Chen et al., "Deep Ritz Method with Hyperreduction"** and its discrete counterpart
**Hesthaven et al., "A Deep Learning Framework for Nonlinear Model Order Reduction using Hyper-reduction Techniques"**, can be used. However, these techniques are intrusive in nature, and their implementation is not trivial. Instead, the idea of the Galerkin-POD NN, originally pointed out in **Chen et al., "Deep Ritz Method with Hyperreduction"** , is to use a NN for the approximation of the reduced coefficients,
i.e.~for the computation of the central part in \cref{eq:diagram}. This completely decouples the online and offline phases, and makes the approximation of the reduced coefficients purely \emph{data-driven}.

Summarized, the Galerkin-POD NN provides an \emph{algorithmically implementable} and \emph{computationally feasible} construction of the decoder. What remains to be understood is the interplay of the approximation errors of the neural network approximation, the Galerkin-POD, and the Galerkin approximation to the variational problem and how to balance them to obtain an accurate and computationally efficient approximation scheme.