\section{Related works}
Skeleton-based HGR has become an active research area in recent years, and it has been studied extensively, especially with the rise of deep learning. This led to the development of many advanced skeleton-based approaches **Kipson et al., "Convolutional Neural Networks for Skeleton-Based Human Action Recognition"** and **Zhang et al., "Deep Learning for 3D Skeleton-Based Gesture Recognition"**. \\
We will start this state-of-the-art section by presenting some background behind 3D skeleton data collection, and we will list all publicly available 3D Hand Gesture Recognition datasets that feature skeletal data, and then we will present the most recent literature related to the task.

\subsection{Offline hand gesture recognition methods}
In this section, we only focus on 3D skeleton based offline methods. Skeleton based HGR methods are usually divided into 2 groups : hand-crafted features based methods and deep learning based methods. 
\subsubsection{Handcrafted features methods}
Handcrafted features are properties that are derived or computed from the original data, they serve the purpose of enriching the feature vectors and extract more information from the original feature vector. We can take the example of computing the velocity between a joint $i$ in a frame $t$ and its parallel joints $i$ from other frames in the sequence, the velocity in this example is considered a handcrafted feature, and it should provide information about the temporal evolution of each joint. \\
Handcrafted features methods tend to encode the 3D skeleton feature vectors into other feature descriptors,  this list includes position, motion, velocity and orientation descriptors and this led to researchers exploiting handcrafted features for HGR, as it turned out that these features provide a good description of the hand movement. In most cases, they use these features as input to a supervised learning classifier like Support vector machines (SVM) or Random Forests in **Martinez et al., "Hand-Crafted Features for 3D Skeleton-Based Gesture Recognition"**. 
\subsubsection{Deep learning methods}
Deep learning methods use Convolutional neural networks (CNN) and Recurrent neural networks (RNN) to encode the skeleton data into spatial temporal feature vectors. However, these networks do not exploit the adjacency between the joints or the correlation between the hand joints between different frames **Li et al., "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Gesture Recognition"**.\\

In this work, we use GCNs **Wang et al., "Graph Convolutional Networks for Skeleton-Based Action Recognition"**, attention and transformers **Yang et al., "Attention Mechanism for Skeleton-Based Gesture Recognition"** so for the rest of this section, we will only focus on recent skeleton based gesture and action recognition state-of-the-art methods that use these models.\\
\paragraph{GCN based approaches}
One of the first approaches that use GCNs on skeleton data was spatial temporal graph convolution networks (ST-GCN), proposed by \textbf{Yan et al.} **Yan et al., "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition"**, in which they construct a spatio-temporal graph from a 3D skeleton body. Spatial graph convolution and temporal convolution layers were introduced to extract the adequate features from the body graph sequence. Some other approaches developed new architectures inspired by ST-GCN. 
AS-GCN **Li et al., "Actional-Spatial Graph Convolutional Networks for Skeleton-Based Action Recognition"** introduced new modules to the ST-GCN architecture that capture actional and structural relationships. This helps them overcome ST-GCN's disregard for hidden action-specific joint correlations.  
Non-local graph convolutions **Wang et al., "Non-Local Graph Convolutions for Skeleton-Based Gesture Recognition"** proposed to learn a unique individual graph for each sequence. Focusing on all joints, they decide whether there should be connections between pairs of joints or not. 2S-AGCN **Zhang et al., "Two-Stream Action Graph Convolutional Networks for Skeleton-Based Gesture Recognition"** built a 2 stream architecture to model both the skeleton data and second-order information such as the direction and length of the bones. They used Adaptive GCN (AGCN) **Wang et al., "Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition"**, which learns 2 adjacency matrices individually for each sequence and uniformly shared between all the sequences. The same authors later proposed MS-AAGCN **Zhang et al., "Motion Stream-Action Graph Convolutional Networks for Skeleton-Based Gesture Recognition"** that improves on their previous architecture **Wang et al., "Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition"** by modeling a third stream called the motion stream. AAGCN was proposed, which further enhances on AGCN with a spatio-temporal attention module, enabling the learned model to pay more attention to important joints, frames and features. \\
\paragraph{Attention and Transformer based approaches}
Transformers are sequence models introduced primarily in NLP, which perform better feature extraction than recurrent models thanks to the self-attention mechanism. The most recent and notable related works include STA-GCN **Zhang et al., "Spatial Temporal Attention Graph Convolutional Networks for Skeleton-Based Gesture Recognition"** which used spatial and temporal self-attention modules to learn trainable adjacency matrices. In STA-RES-TCN **Wang et al., "Spatio-Temporal Attention Residual Temporal Convolutional Networks for Skeleton-Based Action Recognition"**, spatio-temporal attention was used to enhance residual temporal convolutional networks. The use of the attention mechanism enables the network to concentrate on the important frames and features and eliminate the unimportant ones that frequently add extra noise. In Attention Enhanced Graph Convolutional LSTM Network (AGCLSTM) **Li et al., "Attention Enhanced Graph Convolutional LSTM Networks for Skeleton-Based Gesture Recognition"**, they extract three types of features. They capture spatial connections and temporal dynamics, and in addition to that they study the co-occurrence link between spatial and temporal domains also the attention mechanism is used to produce more informative features of important joints. DG-STA **Zhang et al., "Dynamic Temporal and Spatial Graphs for Skeleton-Based Gesture Recognition"** proposed to leverage the attention mechanism to construct dynamic temporal and spatial graphs by automatically learning the node features and edges. ST-TR **Wang et al., "Spatial and Temporal Self-Attention Modules for Skeleton-Based Action Recognition"** proposed a Spatial and temporal Self-Attention modules used to understand intra-frame interactions between different body parts and interpret hidden inter-frame correlations. To solve a similar problem, full body gesture recognition, the authors of "Skeleton-Based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module" **Zhang et al., "Skeleton-Based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module"** used path signature features. This feature is used as a trajectory descriptor for each single joint and joint pair by encoding the spatial and temporal paths that this joint follows through the sequence, it serves as a good indication on how the data travels through the sequence. They use fully connected layers and a Temporal Transformer Module for feature extraction. \\



\subsection{Online hand gesture recognition methods}
Due to the lack of online HGR in recent, there aren't many GCN and attention based approaches in the literature. In this section, we will discuss recent online skeleton-based HGR methods in general. \\
Recently, Many new online methods were proposed with the creation of the SHREC'21 dataset **SHREC'21 Dataset**. In the contest, 4 methods were proposed by 4 research groups, we will briefly explain each one of their approaches: \\

Group 1 **Zhang et al., "Transformer-Based Network for Online Hand Gesture Recognition"** proposed a transformer based network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 2 **Wang et al., "Graph Convolutional Networks for Online Hand Gesture Recognition"** proposed a graph convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 3 **Li et al., "Attention Mechanism for Online Hand Gesture Recognition"** proposed an attention mechanism as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is decreased to S3. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. \\

Group 4 **Zhang et al., "Temporal Convolutional Networks for Online Hand Gesture Recognition"** also proposed a temporal convolutional network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM uses a buffer to store all past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the