\section{Related works}
Skeleton-based HGR has become an active research area in recent years, and it has been studied extensively, especially with the rise of deep learning. This led to the development of many advanced skeleton-based approaches \cite{akremi2022spd,caputo,dhg,sem-mem-wal,res-tcn,HPEV+HMM+FRPV,st-ts-hgr-net,huang-Riemannian,huang-grassman}. \\
We will start this state-of-the-art section by presenting some background behind 3D skeleton data collection, and we will list all publicly available 3D Hand Gesture Recognition datasets that feature skeletal data, and then we will present the most recent literature related to the task.

\subsection{Offline hand gesture recognition methods}
In this section, we only focus on 3D skeleton based offline methods. Skeleton based HGR methods are usually divided into 2 groups : hand-crafted features based methods and deep learning based methods. 
\subsubsection{Handcrafted features methods}
Handcrafted features are properties that are derived or computed from the original data, they serve the purpose of enriching the feature vectors and extract more information from the original feature vector. We can take the example of computing the velocity between a joint $i$ in a frame $t$ and its parallel joints $i$ from other frames in the sequence, the velocity in this example is considered a handcrafted feature, and it should provide information about the temporal evolution of each joint. \\
Handcrafted features methods tend to encode the 3D skeleton feature vectors into other feature descriptors,  this list includes position, motion, velocity and orientation descriptors and this led to researchers exploiting handcrafted features for HGR, as it turned out that these features provide a good description of the hand movement. In most cases, they use these features as input to a supervised learning classifier like Support vector machines (SVM) or Random Forests in \cite{ohn2013joint, de2019heterogeneous, de2016skeleton}. 
\subsubsection{Deep learning methods}
Deep learning methods use Convolutional neural networks (CNN) and Recurrent neural networks (RNN) to encode the skeleton data into spatial temporal feature vectors. However, these networks do not exploit the adjacency between the joints or the correlation between the hand joints between different frames \cite{chen2017motion,nunez2018convolutional,res-tcn}.\\

In this work, we use GCNs \cite{gcn}, attention and transformers \cite{transformer} so for the rest of this section, we will only focus on recent skeleton based gesture and action recognition state-of-the-art methods that use these models.\\
\paragraph{GCN based approaches}
One of the first approaches that use GCNs on skeleton data was spatial temporal graph convolution networks (ST-GCN), proposed by \textbf{Yan et al.} \cite{st-gcn}, in which they construct a spatio-temporal graph from a 3D skeleton body. Spatial graph convolution and temporal convolution layers were introduced to extract the adequate features from the body graph sequence. Some other approaches developed new architectures inspired by ST-GCN. 
AS-GCN \cite{as-gcn} introduced new modules to the ST-GCN architecture that capture actional and structural relationships. This helps them overcome ST-GCN's disregard for hidden action-specific joint correlations.  
Non-local graph convolutions \cite{non-local-gcn} proposed to learn a unique individual graph for each sequence. Focusing on all joints, they decide whether there should be connections between pairs of joints or not. 2S-AGCN \cite{shi2019two} built a 2 stream architecture to model both the skeleton data and second-order information such as the direction and length of the bones. They used Adaptive GCN (AGCN) \cite{li2018adaptive}, which learns 2 adjacency matrices individually for each sequence and uniformly shared between all the sequences. The same authors later proposed MS-AAGCN \cite{shi2020skeleton} that improves on their previous architecture \cite{li2018adaptive} by modeling a third stream called the motion stream. AAGCN was proposed, which further enhances on AGCN with a spatio-temporal attention module, enabling the learned model to pay more attention to important joints, frames and features. \\
\paragraph{Attention and Transformer based approaches}
Transformers are sequence models introduced primarily in NLP, which perform better feature extraction than recurrent models thanks to the self-attention mechanism. The most recent and notable related works include STA-GCN \cite{sta-gcn} which used spatial and temporal self-attention modules to learn trainable adjacency matrices. In STA-RES-TCN \cite{res-tcn}, spatio-temporal attention was used to enhance residual temporal convolutional networks. The use of the attention mechanism enables the network to concentrate on the important frames and features and eliminate the unimportant ones that frequently add extra noise. In Attention Enhanced Graph Convolutional LSTM Network (AGCLSTM) \cite{si2019attention}, they extract three types of features. They capture spatial connections and temporal dynamics, and in addition to that they study the co-occurrence link between spatial and temporal domains also the attention mechanism is used to produce more informative features of important joints. DG-STA \cite{dg-sta} proposed to leverage the attention mechanism to construct dynamic temporal and spatial graphs by automatically learning the node features and edges. ST-TR \cite{st-tr} proposed a Spatial and temporal Self-Attention modules used to understand intra-frame interactions between different body parts and interpret hidden inter-frame correlations. To solve a similar problem, full body gesture recognition, the authors of "Skeleton-Based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module"  \cite{li2019skeleton} used path signature features. This feature is used as a trajectory descriptor for each single joint and joint pair by encoding the spatial and temporal paths that this joint follows through the sequence, it serves as a good indication on how the data travels through the sequence. They use fully connected layers and a Temporal Transformer Module for feature extraction. \\



\subsection{Online hand gesture recognition methods}
Due to the lack of online HGR in recent, there aren't many GCN and attention based approaches in the literature. In this section, we will discuss recent online skeleton-based HGR methods in general. \\
Recently, Many new online methods were proposed with the creation of the SHREC'21 dataset \cite{shrec21}. In the contest, 4 methods were proposed by 4 research groups, we will briefly explain each one of their approaches: \\

Group 1 \cite{shrec21} proposed a transformer based network as their recognition model. For online recognition, they use a sliding window approach coupled with a Finite State Machine to detect when gestures start. The FSM starts at state S1, it uses a buffer to store 10 past frames, and each frame is classified. If even one frame is classified as a gesture, then the state of the FSM is increased to S2 and a check on the beginning of the gesture is performed. The system checks for 10 consecutive windows, if not enough gestures are found, then the FSM empties the buffer and abandons this gesture and resets to the initial state S1. If enough gestures are detected, then the state is increased  to S3. The FSM attempts to detect the end of the gesture and checks if a window does not contain any gestures, then the state is increased to S4. In this state, the FSM checks in the next 25 consecutive windows. if at least one gesture is detected, the state is decreased to S3. Otherwise, if no gesture is detected, the FSM resets to the initial state S1 and the whole process is restarted. \\
Group 2 \cite{shrec21} proposed an image based approach. They project the 3D skeletons on the xy plane to create a sequence of 2D images, and then  they use ResNet-50 \cite{he2016deep} which is a Convolutional Neural Network (CNN) based model as their classification model. To provide temporal information to the network, the recent history of the gesture is traced on the image to finally produce only one image to represent a sequence of 3D skeletons. There are several disadvantages with this method, the lack of a true temporal dimension means that they can't exploit the temporal correlations between the frames. They proposed a second model to solve this issue, by using the ResNet-3D network \cite{hara2018can}, they are able to exploit the temporal dimension of this data. But another disadvantage is that they ignored the z-axis in the data, which can affect the recognition rate for some gestures that exploit all 3 dimensions. Their online recognition is not really continuous, they perform the recognition every 300 frames, which is a very long window that can include multiple gestures, so some short gestures can be ignored. This explains why their methods produce more false positives than the other 3 groups. \\
Group 3 \cite{shrec21} proposed uDeepGRU, a model based on Recurrent Neural Networks (RNNs), that uses a network of Gated Recurrent Units (GRUs). They classify the frames sequentially as they are fed to the network and outputs a predicted label for every frame. They obtained decent results, despite that the network doesn't explicitly exploit the spatial features of the 3D skeletons.\\
Group 4 \cite{shrec21} used st-gcn \cite{st-gcn} as their classification model. For online recognition, they used a sliding window approach, and they use an energy-based pre-segmentation module that calculates the amount of energy accumulated in a window W, to determine if the classification should be performed on W or to ignore it and consider it a "non-gesture" window. This pre-segmentation module helped them reduce the number of false positives exponentially, and they performed the best overall out of all 4 groups.\\

Outside of the SHREC'21 contest, the STRONGER method \cite{emporio2021stronger} was evaluated on the SHREC'21 dataset. They proposed a network of simple 1D CNNs, each coordinate out of (x, y, z) is processed by one 1D CNN. The values of each coordinate from all the frames are concatenated to form a vector that represents the temporal evolution in each dimension, and are then fed to the 1D CNN to extract features. All extracted feature vectors are then concatenated and fed to the classifier to predict the label of the sequence. For online recognition, they also use a sliding window approach and a threshold filtering system to reduce false positives. They learn some thresholds during the training phase, which they use later in the online recognition phase to filter the windows with a low confidence score and consider them as "non-gesture" windows.