\section{Related Work}
\label{sec:related_work}

\subsection{Conventional RGB-D Semantic Segmentation Networks}
\label{sec:RGB-D}
Considering the feature fusion stage of state-of-the-art (SoTA) RGB-D semantic segmentation networks, we can categorize them into three main types: early fusion, intermediate fusion, and late fusion \cite{zhang2021deep}. Early fusion methods typically combine RGB and depth images at the input stage. While this approach is straightforward, it falls short in achieving a comprehensive understanding of the environment \cite{zhang2021deep}. Intermediate fusion techniques \cite{fan2020sne-roadseg} extract and fuse these heterogeneous features from RGB-D pairs using duplex encoders. Late fusion approaches \cite{ha2017mfnet,valada2017adapnet,cheng2017locality} utilize two parallel encoders, with a primary focus on feature fusion within the decoder. Although these strategies have proven to be more effective than single-modal methods, they still face challenges such as insufficient utilization of depth features and high computational costs associated with multiple feature extractors \cite{huang2024roadformer+}. 

\subsection{Feature Extraction and Fusion Strategies}
\label{sec:feature}
Feature extraction and fusion play essential roles in advancing various computer vision tasks \cite{hassan2020learning,zhou2021ecffnet,jiang2013salient,zhang2018exfuse}. Traditional fusion strategies perform effectively when features exhibit relevance and consistency. However, when dealing with multi-scale, multi-level, and multi-modality features, indiscriminate extraction and fusion can introduce redundancies and noise due to the semantic gap \cite{wu2024s}. To address these limitations, the separation-and-aggregation gate (SAGate) \cite{chen2020bi} recalibrates and aggregates heterogeneous features to generate enhanced selective representations specifically for segmentation. Additionally, several studies \cite{fan2020sne-roadseg,zhou2019unet++} have addressed this issue by introducing densely connected skip connections to integrate fine-grained and coarse-grained information. The cross feature module (CFM) \cite{wei2020f3net} incorporates adaptive aggregation mechanisms to selectively retain useful feature maps and refine multi-level features while filtering out background noise. Nevertheless, these models often overlook the semantic gap between hierarchical features extracted at different network depths, which may result in incomplete fusion of high-level and low-level details. Hence, our primary focus in this article is on addressing this critical limitation.


\subsection{Vision Foundation Models}
\label{sec:DINOV2}
Vision foundation models have emerged as a groundbreaking approach in artificial intelligence, demonstrating superior learning capabilities compared to traditional models. The segment anything model (SAM) \cite{kirilloV2023segment} is a recently proposed VFM for image segmentation. Trained on a massive dataset consisting of over 1 billion masks from 11 million images, SAM is built to generalize across diverse segmentation tasks. By performing discriminative self-supervised learning at image and patch levels, DINOv2 \cite{oquab2023dinoV2} learns all-purpose visual features for various downstream tasks. Depth Anything V1 \cite{yang2024depth} inherits profound semantic priors from the pre-trained DINOv2 backbone via a simple feature alignment constraint, achieving superior performance in downstream tasks. Depth Anything V2 \cite{yang2024depth2} produces finer and more robust depth predictions than the first version by mitigating the distributional shift and limited diversity of synthetic data. Larger models naturally excel at integrating vision features, making them well-suited for our RGB-D driving scene parsing tasks. However, they also come with increased memory requirements and higher training costs. To strike a balance between performance, resource efficiency, and experimental versatility, we select Large DINOv2, Large Depth Anything V1, and Large Depth Anything V2 as the backbones for our proposed HFIT.