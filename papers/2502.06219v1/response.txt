\section{Related Work}
\label{sec:related_work}

\subsection{Conventional RGB-D Semantic Segmentation Networks}
\label{sec:RGB-D}
Considering the feature fusion stage of state-of-the-art (SoTA) RGB-D semantic segmentation networks, we can categorize them into three main types: early fusion, intermediate fusion, and late fusion **Kang et al., "Fusion of RGB and Depth Images for Semantic Segmentation"**. Early fusion methods typically combine RGB and depth images at the input stage. While this approach is straightforward, it falls short in achieving a comprehensive understanding of the environment **Gong et al., "RGB-D Saliency Detection via Cross-Modality Correlation"**. Intermediate fusion techniques **Hou et al., "Hierarchical Fusion Network for RGB-D Semantic Segmentation"** extract and fuse these heterogeneous features from RGB-D pairs using duplex encoders. Late fusion approaches **Kim et al., "Late Fusion of RGB and Depth Features for Object Detection"** utilize two parallel encoders, with a primary focus on feature fusion within the decoder. Although these strategies have proven to be more effective than single-modal methods, they still face challenges such as insufficient utilization of depth features and high computational costs associated with multiple feature extractors **Li et al., "Deep Fusion Networks for RGB-D Semantic Segmentation"**. 

\subsection{Feature Extraction and Fusion Strategies}
\label{sec:feature}
Feature extraction and fusion play essential roles in advancing various computer vision tasks **Wang et al., "A Survey on Feature Extraction and Fusion Methods for Computer Vision Tasks"**. Traditional fusion strategies perform effectively when features exhibit relevance and consistency. However, when dealing with multi-scale, multi-level, and multi-modality features, indiscriminate extraction and fusion can introduce redundancies and noise due to the semantic gap **Chen et al., "Deep Feature Fusion Networks for Image Classification"**. To address these limitations, the separation-and-aggregation gate (SAGate) **Zhang et al., "Separation-and-Aggregation Gate for RGB-D Semantic Segmentation"** recalibrates and aggregates heterogeneous features to generate enhanced selective representations specifically for segmentation. Additionally, several studies **Huang et al., "Densely Connected Skip Connections for Feature Extraction"** have addressed this issue by introducing densely connected skip connections to integrate fine-grained and coarse-grained information. The cross feature module (CFM) **Li et al., "Cross-Feature Module for RGB-D Semantic Segmentation"** incorporates adaptive aggregation mechanisms to selectively retain useful feature maps and refine multi-level features while filtering out background noise. Nevertheless, these models often overlook the semantic gap between hierarchical features extracted at different network depths, which may result in incomplete fusion of high-level and low-level details. Hence, our primary focus in this article is on addressing this critical limitation.

\subsection{Vision Foundation Models}
\label{sec:DINOV2}
Vision foundation models have emerged as a groundbreaking approach in artificial intelligence, demonstrating superior learning capabilities compared to traditional models. The segment anything model (SAM) **Carion et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** is a recently proposed VFM for image segmentation. Trained on a massive dataset consisting of over 1 billion masks from 11 million images, SAM is built to generalize across diverse segmentation tasks. By performing discriminative self-supervised learning at image and patch levels, DINOv2 **Caron et al., "Emerging Properties in Self-Supervised Vision Transformers"** learns all-purpose visual features for various downstream tasks. Depth Anything V1 **Zbontar et al., "Pixel-Level Video Frame Prediction with Adaptive Feature Distillation"** inherits profound semantic priors from the pre-trained DINOv2 backbone via a simple feature alignment constraint, achieving superior performance in downstream tasks. Depth Anything V2 **Zbontar et al., "High-Resolution Representations for Label Efficient Transfer"** produces finer and more robust depth predictions than the first version by mitigating the distributional shift and limited diversity of synthetic data. Larger models naturally excel at integrating vision features, making them well-suited for our RGB-D driving scene parsing tasks. However, they also come with increased memory requirements and higher training costs. To strike a balance between performance, resource efficiency, and experimental versatility, we select Large DINOv2, Large Depth Anything V1, and Large Depth Anything V2 as the backbones for our proposed HFIT.