
\section{Experiments}
\label{sec:experiments}

\begin{table*}
\begin{center}
\begin{adjustbox}{width=0.99\linewidth}
\centering
\begin{tabular}{lcccccccccccccccccccccccccccccccccccccccccccccccccccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Method}} & Desc. & 
\multicolumn{2}{c}{Baidu \cite{Sun_2017_Baidu_dataset}} & &
\multicolumn{2}{c}{Eynsham \cite{Cummins_2009_eynsham, Berton_2022_benchmark}} & &
\multicolumn{2}{c}{MSLS val \cite{Warburg_2020_msls}} & &
\multicolumn{2}{c}{Pitts250k \cite{Gronat_2013_cvpr_pitts, Arandjelovic_2018_netvlad}} & &
\multicolumn{2}{c}{Pitts30k \cite{Gronat_2013_cvpr_pitts, Arandjelovic_2018_netvlad}} & &
\multicolumn{2}{c}{SF-XL v1 \cite{Berton_2022_cosPlace}} & &
\multicolumn{2}{c}{SF-XL v2 \cite{Berton_2022_cosPlace}} & &
\multicolumn{2}{c}{SF-XL night \cite{Barbarani_2023_local_features_benchmark}} & &
\multicolumn{2}{c}{SF-XL occlusion \cite{Barbarani_2023_local_features_benchmark}} & &
\multicolumn{2}{c}{Tokyo 24/7 \cite{Torii_2018_tokyo247}} \\
\cline{3-4} \cline{6-7} \cline{9-10} \cline{12-13} \cline{15-16} \cline{18-19} \cline{21-22} \cline{24-25} \cline{27-28} \cline{30-31}
& Dim.
& R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 & & R1 & R10 \\
\midrule
NetVLAD \cite{Arandjelovic_2018_netvlad}       &  4096 & 69.0&95.0 && 77.7&90.5 && 54.5&70.4 && 85.9&95.0 && 85.0&94.4 && 40.1&57.7 && 76.9&91.1 &&  6.7&14.2 &&  9.2&22.4 && 69.8&82.9 \\
AP-GeM \cite{Revaud_2019_ap_gem}       &  2048 & 59.8&90.8 && 68.3&84.0 && 56.0&72.9 && 80.0&93.5 && 80.7&94.1 && 37.9&54.1 && 66.4&84.6 &&  7.5&16.7 &&  5.3&14.5 && 57.5&77.5 \\
CosPlace \cite{Berton_2022_cosPlace}     &  2048 & 52.0&80.4 && 90.0&94.9 && 85.0&92.6 && 92.3&98.4 && 90.9&96.7 && 76.6&85.5 && 88.8&96.8 && 23.6&32.8 && 30.3&44.7 && 87.3&95.6 \\
MixVPR \cite{Alibey_2023_mixvpr}        &  4096 & 71.9&94.7 && 89.6&94.4 && 83.2&91.9 && 94.3&98.9 && 91.6&96.4 && 72.5&80.9 && 88.6&95.0 && 19.5&30.5 && 30.3&38.2 && 87.0&94.0 \\
EigenPlaces \cite{Berton_2023_EigenPlaces}   &  2048 & 69.1&91.9 && 90.7&95.4 && 85.9&93.1 && 94.1&98.7 && 92.5&97.6 && 84.0&90.7 && 90.8&96.7 && 23.6&34.5 && 32.9&52.6 && 93.0&97.5 \\
AnyLoc \cite{Keetha_2023_AnyLoc} & 49152 & \underline{75.6}&\underline{95.2} && 85.0&94.1 && 58.7&74.5 && 89.4&98.0 && 86.3&96.7 &&    -&   - &&    -&   - &&    -&   - &&    -&   - && 87.6&97.5 \\
Salad \cite{Izquierdo_2024_SALAD}         &  8448 & 72.7&93.6 && 91.6&95.9 && 88.2&95.0 && 95.0&\underline{99.2}&& 92.3&97.4 &&\underline{88.7}&\underline{94.4}&&\underline{94.6}&98.2&&\underline{46.1}&\underline{62.4}&&\underline{50.0}&\underline{68.4}&& 94.6&\underline{98.1} \\
CricaVPR \cite{Lu_2024_cricavpr}      & 10752 & 65.6&93.2 && 88.0&94.3 && 76.7&87.2 && 92.6&98.3&& 90.0&96.7 && 62.6&78.9 && 86.3&96.0 && 25.8&40.6 && 27.6&47.4 && 82.9&93.7 \\
CliqueMining \cite{Izquierdo_2024_cliqueM}  &  8448 & 72.9&92.7 &&\underline{91.9}&\underline{96.2} &&\textbf{91.6}&\textbf{95.9} &&\underline{95.3}&\underline{99.2}&&\underline{92.6}&\underline{97.8}&& 85.5&92.6 && 94.5&\underline{98.3}&& \underline{46.1}&60.9 && 44.7&64.5 &&\textbf{96.8}&97.8 \\
MegaLoc (Ours) & 8448 & \textbf{87.7}&\textbf{98.0}&&\textbf{92.6}&\textbf{96.8}&&\underline{91.0}&\underline{95.8}&&\textbf{96.4}&\textbf{99.3}&&\textbf{94.1}&\textbf{98.2}&&\textbf{95.3}&\textbf{98.0}&&\textbf{94.8}&\textbf{98.5}&&\textbf{52.8}&\textbf{73.8}&&\textbf{51.3}&\textbf{75.0}&&\underline{96.5}&\textbf{99.4} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\caption{\textbf{Recall@1 and Recall@10 on multiple VPR datasets.} Best overall results on each dataset are in \textbf{bold}, second best results \underline{underlined}. Results marked with a ``-'' did not fit in 480GB of RAM (2.8M features of 49k dimensions require 560GB for a float32-based kNN).}
\label{tab:vpr}
\end{table*}

\begin{table*}
\begin{center}
\begin{adjustbox}{width=0.99\linewidth}
\centering
\begin{tabular}{lccccccccccccccccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Method}} &
\multicolumn{2}{c}{CAB (Phone)} & &
\multicolumn{2}{c}{HGE (Phone)} & &
\multicolumn{2}{c}{LIN (Phone)} & &
\multicolumn{2}{c}{CAB (HoloLens)} & &
\multicolumn{2}{c}{HGE (HoloLens)} & &
\multicolumn{2}{c}{LIN (HoloLens)} \\
\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12} \cline{14-15} \cline{17-18}
& (1, 0.1) & (5, 1.0) & & (1, 0.1) & (5, 1.0) & & (1, 0.1) & (5, 1.0) & & (1, 0.1) & (5, 1.0) & & (1, 0.1) & (5, 1.0) & & (1, 0.1) & (5, 1.0) \\
\midrule
NetVLAD               & 43.4 & 54.0 && 54.8 & 80.0 && 74.4 & 87.8 && 63.1 & 81.4 && 57.9 & 71.6 && 76.1 & 83.0 \\
AP-GeM                & 39.4 & 52.0 && 58.0 & 81.3 && 69.1 & 82.0 && 62.9 & 82.5 && 65.6 & 76.6 && 80.7 & 91.1 \\
Fusion (NetVLAD+AP-GeM)&41.4 & 53.8 && 56.3 & 82.4 && 76.0 & 89.4 && 63.2 & 83.1 && 63.1 & 75.1 && 78.5 & 87.0 \\
CosPlace              & 29.0 & 37.4 && 54.4 & 81.3 && 63.3 & 75.7 && 56.4 & 77.8 && 55.6 & 69.8 && 80.6 & 91.4 \\
MixVPR                & 40.9 & 50.8 && 59.2 & 83.8 && 77.5 & 89.8 && 65.2 & 84.7 && 63.3 & 74.7 && 83.6 & 92.2 \\
EigenPlaces           & 32.3 & 44.7 && 56.3 & 81.3 && 70.2 & 82.6 && 63.9 & 81.8 && 60.2 & 72.5 && 84.8 & 93.1 \\
AnyLoc                &\textbf{48.0}&\underline{59.8}&& 58.8 & 83.0 && 77.2 & 92.4 && 69.7 & 88.5 && 70.1 & 81.0 && 81.4 & 90.4 \\
Salad                 & 44.2 & 55.6 &&65.3&\underline{92.2}&&\underline{81.7}&\underline{94.0}&& 71.5 & 90.7 &&\underline{75.3}&\underline{85.2}&& 91.3 &\textbf{99.4}\\
CricaVPR              & 40.4 & 52.0 && 63.7 & 89.3 && 80.7 & 93.1 && 73.9 & 90.7 && 72.5 & 81.6 && 89.1 & 98.4 \\
CliqueMining          & 44.2 & 55.6 &&\underline{66.0}& 91.4 && 80.5 & 93.1 &&\underline{74.2}&\underline{90.9}&&\textbf{77.3}&\textbf{86.3}&&\underline{92.0}&98.8 \\
MegaLoc (Ours)        &\underline{47.0}&\textbf{60.4}&&\textbf{67.2}&\textbf{92.9}&&\textbf{83.3}&\textbf{94.9}&&\textbf{77.4}&\textbf{93.4}&&72.9&83.5&&\textbf{92.2}&\underline{99.0}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\caption{\textbf{Results on LaMAR's datasets}, computed on each of the three locations, for both types of queries (HoloLens and Phone), which include both indoor and outdoor. For each location we report the recall at (1°, 10cm) and (5°, 1m), following the LaMAR paper \cite{sarlin2022lamar}.}
\label{tab:lamar}
\end{table*}

\subsection{Implementation details}
\label{sec:implementation_details}
During training, images are resized to 224$\times$224, while for inference we resize them to 322$\times$322, following \cite{Izquierdo_2024_SALAD}.
We use RandAugment \cite{Cubuk_2020_RandAugment} for data augmentation, as in \cite{Alibey_2022_gsvcities}, and AdamW \cite{Loshchilov_2018_AdamW} as optimizer.
Training is performed for 40k iterations.
The loss is simply computed as
$\mathcal{L} = \mathcal{L}_1 + \mathcal{L}_2 + \mathcal{L}_3 + \mathcal{L}_4 + \mathcal{L}_5 + \mathcal{L}_6$, where each $\mathcal{L}_n$ is the multi-similarity loss computed on one of the sub-batches.

\paragraph{The architecture} consists of a DINO-v2-base backbone \cite{Oquab_2023_dinov2} followed by a SALAD \cite{Izquierdo_2024_SALAD} aggregation layer, which has shown state-of-the-art performances over multiple VPR datasets \cite{Izquierdo_2024_SALAD, Izquierdo_2024_cliqueM}.
The SALAD layer is computed with 64 clusters, 256
channels per cluster, a global token of 256 and an MLP dimension of 512. The SALAD layer is followed by a linear projection (from a dimension of 16640 to 8448) and an L2 normalization.

\paragraph{Memory-efficient GPU training} is achieved using PyTorch \cite{Paszke_2019_PyTorch}, by ensuring that the computational graph for each loss stays in memory as little as possible.
In practice (in the code), instead of adding the computational graph for each loss into a single giant graph, we compute each loss and perform the $backward()$ operation independently:
calling $backward()$ in PyTorch not only computes the gradient (which is added to any existing gradient), but also frees the computational graph (hence freeing memory).
The $step()$ (and $zero\_grad()$) method is then called only once (after six $backward()$ calls.
This simple technique reduces the VRAM requirement of training MegaLoc from (roughly) 300GB to 60GB.


\subsection{Results}
\label{sec:results}
We perform experiments on three different types of tasks:
\begin{itemize}
    \item Visual Place Recognition, where the task is to retrieve images that are within 25 meters from the query (\cref{sec:results_vpr});
    \item Visual Localization, where retrieval is part of a bigger pipeline that aims at finding the precise pose of the query given a set of posed images (\cref{sec:results_lamar});
    \item Landmark Retrieval, \ie retrieving images that depict the same landmark as the query (\cref{sec:results_lr}).
\end{itemize}

\subsubsection{Visual Place Recognition}
\label{sec:results_vpr}
We run experiments on a comprehensive set of Visual Place Recognition datasets.
These datasets contain a large variety of domains, including:
outdoor, indoor, street-view, hand-held camera, car-mounted camera, night, occlusions, long-term changes, grayscale.
Results are shown in \cref{tab:vpr}.
While other high-performing VPR models (like SALAD and CliqueMining) achieve very good results (\ie comparable to MegaLoc) on most datasets, MegaLoc vastly outperforms every other model on Baidu, which is an indoor-only dataset.

\begin{table}
\begin{center}
\begin{adjustbox}{width=0.9\linewidth}
\centering
\begin{tabular}{lccccccc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{R-Oxford} & & \multicolumn{3}{c}{R-Paris}\\
\cline{2-4} \cline{6-8}
& E & M & H & & E & M & H\\
\hline
NetVLAD        & 24.1 & 16.1 &  4.7 && 61.2 & 46.3 & 22.0 \\
AP-GeM         & 49.6 & 37.6 & 19.3 && 82.5 & 69.5 & 45.5 \\
CosPlace       & 32.1 & 23.4 & 10.3 && 57.6 & 45.0 & 22.3 \\
MixVPR         & 38.2 & 28.4 & 10.8 && 61.9 & 48.3 & 25.0 \\
EigenPlaces    & 29.4 & 22.9 & 11.8 && 60.9 & 47.3 & 23.6 \\
AnyLoc  &\underline{64.2}&\underline{45.5}& 18.9 &&\underline{82.8}& 68.5 & 48.8 \\
Salad          & 55.2 & 42.3 & 21.4 && 76.6 & 66.2 & 44.8 \\
CricaVPR       & 57.0 & 39.2 & 15.3 && 80.0 & \underline{68.9} & \underline{48.9} \\
CliqueMining   & 52.2 & 41.0 &\underline{22.1}&& 71.8 & 60.5 & 41.2 \\
MegaLoc (Ours)&\textbf{91.0}&\textbf{79.0}&\textbf{62.1}&&\textbf{95.3}&\textbf{89.6}&\textbf{77.1}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\caption{\textbf{Results on Landmark Retrieval datasets}, respectively Revisited Paris 6k \cite{Radenovic_CVPR_2018_roxford_rparis, Philbin_2008_paris6k} and Revisited Oxford 5k \cite{Radenovic_CVPR_2018_roxford_rparis, Philbin_2007_oxford5k}.}
\label{tab:landmark_retrieval}
\end{table}

\begin{figure*}
    \begin{center}
    \includegraphics[width=0.99\linewidth]{images/failure_cases.jpg}
    \end{center}
    \caption{\textbf{Failure cases, grouped in 4 categories.} Each one of the 4 column represent a category of failure cases: for each category we show 5 examples, made of 3 images, namely the query and its top-2 predictions with MegaLoc, which can be in red or green depending if the prediction is correct (\ie within 25 meters).
    The 4 categories that we identified are (1) \textit{very difficult cases}, which are unlikely to be solved any time soon; (2) \textit{difficult cases}, which can probably be solved by slightly better models than the current ones or simple post-processing; (3) \textit{incorrect GPS labels}, which, surprisingly, exist also in Mapillary and Google StreetView data; (4) \textit{predictions just out of the 25m threshold}, which despite being considered negatives in VPR, are actually useful predictions for real-world applications.}
    \label{fig:failure_cases}
\end{figure*}



\subsubsection{Visual Localization}
\label{sec:results_lamar}
Image retrieval is a core tool to solve 3D vision tasks, in pipelines like visual localization (\eg Hierarchical Localization \cite{Sarlin_2019_hloc} and InLoc \cite{Taira_2018_inloc}) and 3D reconstructions (\eg COLMAP \cite{Schoenberger_2016_mvs_colmap, Schoenberger_2016_sfm_colmap} and GLOMAP \cite{pan2024glomap}).
To understand if our method can help this use case, we compute results on the three datasets of Lamar \cite{sarlin2022lamar}, which comprise various challenges, including plenty of visual aliasing from both indoor and outdoor imagery.
To do this, we relied on the official LaMAR codebase\footnote{\url{https://github.com/microsoft/lamar-benchmark}} by simply replacing the retrieval method.
Results are reported in \cref{tab:lamar}.



\subsubsection{Landmark Retrieval}
\label{sec:results_lr}
For the task of Landmark Retrieval we compute results on the most used datasets in literature, namely (the revisited versions of \cite{Radenovic_CVPR_2018_roxford_rparis}) Oxford5k \cite{Philbin_2007_oxford5k} and Paris6k \cite{Philbin_2008_paris6k}.
To do this we relied on the official codebase for the datasets\footnote{\url{https://github.com/filipradenovic/revisitop}}, by simply swapping the retrieval method.
Results, reported in \cref{tab:landmark_retrieval}, show a large gap between MegaLoc and previous VPR models on this task, which can be simply explained by the fact that previous models were only optimized for the standard VPR metric of retrieving images within 25 meters from the query.


\subsubsection{Failure Cases}
We identified a series of 4 main categories of ``failure cases'' that prevent the results from reaching 100\% recalls, and we present them in \cref{fig:failure_cases}.
We note however that, from a practical perspective, the only real failure cases are depicted in the second category/column of \cref{fig:failure_cases}: furthermore, in most similar cases SOTA models (\ie not only MegaLoc, but also other recent ones) can actually retrieve precise predictions, meaning that these failure cases can be likely solvable by some simple post-processing techniques (\eg re-ranking with image matchers, or majority voting).
Finally, another failure case that we noted, is when database images do not cover properly the search area: this is very common in the Mapillary (MSLS) dataset, where database images only show one direction (\eg photos along a road taken from north to south), while the queries are photos facing the other direction. We note however, that in the real world this can be easily solved by collecting database images in multiple directions, which is also common in most test datasets, like Eynsham, Pitts30k, Tokyo 24/7 and SF-XL.