
\section{Introduction}
\label{sec:introduction}

This paper tackles the task of retrieving images from a large database that represent the same place as a given query image.
But what does it mean for two images to be ``from the same place''?
Depending on who you ask, you'll get different answers:
\begin{enumerate}
    \item Landmark Retrieval (\textbf{LR}) folks will tell you that two photos are from the same place if they depict the same landmark, regardless of how close to each other the two photos were taken \cite{Weyand_2020_gldv2};
    \item Visual Place Recognition (\textbf{VPR}) people set a camera pose distance of 25 meters to define if two images are positives (\ie from the same place) \cite{Arandjelovic_2018_netvlad};
    \item Visual Localization (\textbf{VL}) / 3D Vision researchers will tell you that two images need to have their pose as close as possible to be considered the same place.
\end{enumerate}

\begin{figure}
    \begin{center}
    \includegraphics[width=0.99\columnwidth]{images/teaser.jpg}
    \end{center}
    \caption{Qualitative examples of predictions by MegaLoc. Each pair of images represents a query and its top-1 prediction from the SF-XL dataset, searched across the 2.8M database spanning 150 km$^2$ across San Francisco. Predictions in green are correct, red are wrong.}
    \label{fig:teaser}
\end{figure}

Even though image retrieval is a core component in all three tasks, their different definitions and requirement has inevitably led to the development of ad-hoc image retrieval solutions for each of them.
As these three tasks continued to diverge, over the years papers have avoided showing results of their methods on more than one of these tasks: VPR papers don't show results on LR, and LR papers don't show results on VPR.
In the meantime, 3D vision pipelines like COLMAP \cite{Schoenberger_2016_sfm_colmap}, Hierarchical Localization \cite{Sarlin_2019_hloc} and GLOMAP \cite{pan2024glomap} keep using outdated retrieval methods, like RootSIFT with bag-of-words \cite{Arandjelovic_2012_rootSift, Schonberger_2016_retrieval, Csurka_2003_bow} and NetVLAD \cite{Arandjelovic_2018_netvlad}.
In this paper we aim to put an end to this, by training a single model that achieves SOTA (or almost) on all of these tasks, showcasing robustness across diverse domains.
To train this model we do not propose any ``technical novelty'', but we use all the lessons learned from all these three task, putting together a combination of good samplers, datasets, and general training techniques.

``\emph{Why does it matter?}'', you may ask.
Imagine you are doing 3D reconstruction, where image retrieval is a fundamental component, on a collection of diverse scenes (\eg to create datasets like MegaDepth \cite{Li_2018_megadepth}, MegaScenes \cite{Tung_2024_Megascenes}, or for the evergreen Image Matching Challenge \cite{bellavia_2024_image}).
In some cases there would be small scenes (\eg reconstruction of a fountain), requiring a retrieval model that is able to retrieve nearby images (few meters away), which is something VPR models excel at, but LR models underperform (see \cite{Berton_2022_benchmark} Tab. 14).
In other cases however, the scene might be large (\eg a big landmark like a church), with images hundreds of meters away: while LR models are designed for this, VPR models achieve poor results in this situations (see \cref{sec:experiments}).
Given these considerations, we note how neither VPR nor LR provide models for the diverse cases of 3D reconstructions, creating a gap in literature that is filled by MegaLoc.
% 
As another example where a model like MegaLoc is necessary, one can think of Visual Place Recognition (which is also the first step for Visual Localization), where models are evaluated by using a 25 meters threshold (and queries in popular datasets always have at least one positive within 25 meters).
However, in the real world the nearest image to a given query might be 100 meters away, and while ideally we would still want to retrieve it, a VPR model is unlikely to work in such case, as it has been trained to ignore anything further away from the camera.


In this paper we demonstrate that, by leveraging a diverse set of data sources and best practices from LR, VPR and VL, we obtain a single image retrieval model that works well across all these tasks. Our model is called MegaLoc and it is released at \url{https://github.com/gmberton/MegaLoc}
