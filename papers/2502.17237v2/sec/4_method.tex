
\section{Method}
\label{sec:method}

The core idea of this paper is to fuse data from multiple datasets, and train a single model.
We use five datasets containing both outdoor and indoor images and catering to different image localization tasks: GSV-Cities \cite{Alibey_2022_gsvcities}, 
Mapillary Street-Level Sequences (MSLS) \cite{Warburg_2020_msls},
MegaScenes \cite{Tung_2024_Megascenes}, ScanNet \cite{Dai_2017_scannet} and San Francisco eXtra Large (SF-XL) \cite{Berton_2022_cosPlace}. 
At each training iteration, we extract six-sub batches of data, one for each dataset (except SF-XL, from which two sub-batches are sampled) and use a multi-similarity loss \cite{Wang_2019_multi_similarity_loss} computed over each sub-batch.
Each sub-batch is made of 128 images, containing 4 images (called quadruplets) from 32 different places/classes.
Given that these datasets have diverse format, they require different sampling techniques.
In the following paragraphs we explain how data is sampled from each dataset.


\paragraph{San Francisco eXtra Large (SF-XL)} is a dataset of 41M images with GPS and orientation from 12 different years, densely covering the entire city of San Francisco across time.
To select ideal quadruplets for training, we use the sampling technique presented in EigenPlaces \cite{Berton_2023_EigenPlaces}.
This method assures that each class contains images that represent a given place from diverse perspectives, while ensuring that no visual overlap exists between two different places.
EigenPlaces provides two sub-batches, one made of frontal-facing images (\ie with the camera facing straight along the street) and one of lateral-facing images.

\paragraph{Google Street View Cities (GSV-Cities)} is a dataset of 530k images split into 62k places/classes from 40 cities, where each class contains at least 4 images with same orientation and is at least 100 meters from any other class.
Given that GSV-Cities is already split into non-overlapping classes, it is not strictly necessary to apply a particular sampling technique.
We therefore directly feed the GSV-Cities dataset to the multi-similarity loss, as in the original GSV-Cities paper \cite{Alibey_2022_gsvcities}.

\paragraph{Mapillary Street-Level Sequences (MSLS)}
is a dataset of 1.6M images split in contiguous sequences, across 30 different cities over 9 years.
To ideally sample data from the MSLS dataset, we use the mining technique described in the CliqueMining paper \cite{Izquierdo_2024_cliqueM}.
This method ensures that the places selected for each batch depict visually similar (but geographically different) places (\ie hard negatives), so that the loss can be as high as possible and effectively teach the model to disambiguate between similar-looking places.

\paragraph{MegaScenes}
is a collection of 100k 3D structure-from-motion reconstructions, composed of 2M images from Wikimedia Commons.
Simply using each reconstruction as a class, and sampling random images from such class, could lead to images that do not have any visual overlap, \eg two images could show opposites facades of a building, therefore having no visual overlap while belonging to the same 3D reconstruction.
Therefore we make sure that when we sample a set of four images from a given reconstruction, each of these four images should have visual overlap with each other (we define visual overlap as having at least 1\% of 3D points in common in the 3D reconstruction).

\paragraph{ScanNet}
is a dataset of 2.5M views from 1500 scans from 707 indoor places.
To train on ScanNet we use each scene as a class, and select quadruplets so that each pair of images within a quadruplet has visual overlap (\ie less than 10 meters and 30Â° apart);
simultaneously we ensure that no two images from different quadruplets has visual overlap.

