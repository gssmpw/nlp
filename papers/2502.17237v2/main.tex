% CVPR 2025 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

%%%% BEGIN MY CHANGES
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{bm}
\usepackage{soul}
\def\eg{\emph{e.g}\onedot}
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot}
\def\Ie{\emph{I.e}\onedot}

%%%% END MY CHANGES

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{*****} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{MegaLoc: One Retrieval to Place Them All}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Gabriele Berton\\
Polytechnic of Turin\\
{\tt\small bertongabri@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Carlo Masone\\
Polytechnic of Turin\\
}

\begin{document}
\maketitle
\input{sec/0_abstract}    
\input{sec/1_introduction}
\input{sec/4_method}
\input{sec/5_experiments}
\input{sec/6_conclusion}

% \section{Conclusions and limitations}
% \textit{So, is image retrieval for localization solved?}
% Well, almost. While some datasets still show some room for improvement, we note that this is often due to either arguably unsolvable failure cases, wrong labels, and very few cases that can be solved by better models.
% We emphasize however that this has been the case for some time, as previous DINO-v2-based models, like SALAD and CliqueMining, show very high results on classic VPR datasets.
% What is still missing from literature is models like MegaLoc that achieve good results in a variety of diverse tasks and domains.

% \textit{Should you always use MegaLoc?}
% Well, almost, except for at least 3 use-cases.
% MegaLoc has shown great results on a variety of related tasks, and, unlike other VPR models, achieves good results on landmark retrieval, which make it a great option also for retrieval for 3D reconstruction tasks, besides standard VPR and visual localization tasks.
% However, experiments show that MegaLoc is outperformed by CliqueMining in MSLS, which is a dataset made of (almost entirely) forward facing images (\ie photos where the camera is facing the same direction of the street, instead of facing sideways towards the side of the street).
% Another use case where MegaLoc is likely to be suboptimal is in very unusual natural environments, like forests or caves, where instead AnyLoc has been shown to work well \cite{Keetha_2023_AnyLoc}.
% A third and final use case where other models might be preferred to MegaLoc is for embedded systems, where one might opt for more lightweight models, like the ResNet-18 \cite{He_2016_resnet} versions of CosPlace \cite{Berton_2022_cosPlace}, which has 11M parameters instead of MegaLoc's 228M.


{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
% \input{sec/X_suppl}

\end{document}
