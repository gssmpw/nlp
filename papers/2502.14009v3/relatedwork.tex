\section{Related work}
\subsection{Benchmarked methods}
\label{sec:benchmark}
We provide a systematic review of existing state-of-the-art feedforward self-supervised methods from the literature i.e. methods which only require measurements $\y$ from a single acquisition at training and inference, without any ground truth. We formulate their loss functions and summarise all compared methods in \cref{tab:methods} along with their implementations in the DeepInverse \cite{tachella_deepinverse_2023} library. We focus on the design of the loss function since, in general, these loss functions are agnostic to the NN architecture and thus can be independently used with any latest architecture.

\begin{table*}[t]
\centering
\caption{Self-supervised losses reviewed in \cref{sec:benchmark}, implemented in DeepInverse and benchmarked in our experiments.}
\label{tab:methods}
\begin{tabular}{llll}
\hline
Method & Ref & Loss & Implementation \\ \hline
MC & \cite{darestani_accelerated_2021} & \cref{eq:mc} & \verb+dinv.loss.MCLoss()+ \\
SSDU & \cite{yaman_self-supervised_2020} & \cref{eq:splitting} & \verb+dinv.loss.SplittingLoss(eval_split_input=False)+ \\
Noise2Inverse & \cite{hendriksen_noise2inverse_2020} & \cref{eq:n2i} & \verb+dinv.loss.SplittingLoss(eval_split_input=True)+ \\
Weighted-SSDU & \cite{millard_theoretical_2023} & \cref{eq:millard} & \verb+dinv.loss.WeightedSplittingLoss()+ \\
Adversarial & \cite{cole_fast_2021} & \cref{eq:cole} & \verb+dinv.loss.UnsupAdversarialGeneratorLoss()+ \\
UAIR & \cite{pajot_unsupervised_2018} & \cref{eq:uair} & \verb+dinv.loss.UAIRGeneratorLoss()+ \\
VORTEX & \cite{desai_vortex_2022} & \cref{eq:vortex} & \verb+dinv.loss.VORTEXLoss()+ \\
EI & \cite{chen_equivariant_2021} & \cref{eq:ei} & \verb+dinv.loss.EILoss()+ \\
MOI & \cite{tachella_unsupervised_2022} & \cref{eq:moi} & \verb+dinv.loss.MOILoss()+ \\
MO-EI & Ours & \cref{eq:moei} & \verb+dinv.loss.MOEILoss()+ \\ \hline
\end{tabular}
\end{table*}

\subsubsection{Measurement consistency}
The most naive self-supervised loss simply compares the recorrupted estimate $\A\xhat$ to the measurement $\y$:

\begin{equation}
    \L_\text{MC}=\L(\A\xhat,\y)
    \label{eq:mc}
\end{equation}

where $\L$ is any metric such as the L2 norm. This loss originally appeared with a classical regularising term to solve inverse problems with deep learning and traditional regularisation, but these still faced problems in designing the regularisers. Without a regularising term, the loss cannot recover information from the null-space of the operator $\mathcal{N}(\A)$ as infinite solutions $\A^\top\y+\mathbf{v}$ can satisfy measurement consistency (MC) where $\mathbf{v}\in\mathcal{N}(\A)$. This loss function is also used in the presence of strong inductive bias where the regularisation is provided by the architecture, for example from specific architectures \cite{darestani_accelerated_2021} or initialisations \cite{darestani_test-time_2022}, or in cycle consistency losses \cite{oh_unpaired_2020}.

In the noisy scenario, some recent works such as ENSURE \cite{aggarwal_ensure_2023} or GSURE \cite{metzler_unsupervised_2020} generalise Stein's Unbiased Risk Estimator (SURE) to reconstruct from noisy undersampled measurements. However, in the noiseless case considered here $\sigma\rightarrow0$, these methods simply reduce to $\L_\text{MC}$, and so will not be compared.

\subsubsection{Learning from multiple operators}
A popular set of state-of-the-art approaches leverages the fact that the whole measurement space is spanned by multiple imaging operators. Measurement splitting methods, including SSDU and similar papers \cite{yaman_self-supervised_2020,acar_self-supervised_2021,huang_self-supervised_2024,zhou_dual-domain_2022,hu_self-supervised_2021}, randomly split the k-space into two sets at each iteration. One is used as the input and the other to construct the loss at the output:
\begin{equation}
    \L_\text{SSDU}=\L(\M_2\A_i \ft(\M_1\y_i,\M_1\A_i),\M_2\y_i)
    \label{eq:splitting}
\end{equation}

where $\M_1$ is a randomly generated mask, $\M_1+\M_2=\mathbb{I}_m$, and the inference-time estimate is $\xhat_i=\ft(\y_i,\A_i)$. Note that some methods \cite{hu_spicer_2024,gan_deep_2021} require pairs of measurements of the same subject; these are equivalent to SSDU but with 2x less acceleration. Similarly, Artifact2Artifact \cite{gan_deformation-compensated_2022,liu_rare_2020}, when applied to static MRI reconstruction without paired measurements, is also identical to SSDU (see \cref{sec:a2a} for proof). We therefore consider SSDU as the archetypal method to compare.

An inference-time adaptation to SSDU performs a Monte-Carlo averaging over subsampled inputs; this is inspired by Noise2Inverse \cite{hendriksen_noise2inverse_2020}:
\begin{equation}
    \xhat=\frac{1}{N}\Sigma_{i=1}^n \ft(\M_i\y,\M_i\A)
    \label{eq:n2i}
\end{equation}

More recently, \cite{millard_theoretical_2023} frames SSDU as a Bernoulli-noise special case of Noisier2Noise \cite{moran_noisier2noise_2019}, and propose a weighting to the SSDU loss and an informed splitting strategy to improve SSDU:

\begin{align}
    \L_\text{Weighted-SSDU}&=(1-\mathbf{K})^{-1/2}\L_\text{SSDU} \\
    \mathbf{K}&=(\mathbb{I}_n-\tilde{\mathbf{P}}\mathbf{P})^{-1}(\mathbb{I}_n-\mathbf{P})
\label{eq:millard}
\end{align}

where $\mathbf{P}=\mathbb{E}[\M_i],\tilde{\mathbf{P}}=\mathbb{E}[\M_1]$ i.e. the PDFs of the imaging mask and the splitting mask, respectively.

Finally, Multi-Operator Imaging (MOI) \cite{tachella_unsupervised_2022} leverages the fact that the image set is seen through multiple operators $\A_i$ by drawing a random operator $\tilde\A\sim\mathcal{A}$ at each forward pass to learn in the null-space:

\begin{equation}
    \L_\text{MOI}=\L_\text{MC}+\L(\xhat,\ft(\tilde\A\xhat))
    \label{eq:moi}
\end{equation}

\subsubsection{Equivariant imaging}
Equivariant Imaging (EI) \cite{chen_equivariant_2021} is a state-of-the-art method that constrains the set of solutions using group invariance, which has been applied to MRI in \cite{chen_robust_2022}, optical camera imaging in \cite{wang_perspective-equivariance_2024}, and dynamic MRI in \cite{wang_fully_2024}. By leveraging a natural assumption that the (unknown) image set $\mathcal{X}$ is invariant to a group $G$ of transformations $g\x\in\mathcal{X}\forall\x\in\mathcal{X},g\in G$, the image set is observed through multiple transformed operators $\A_i\circ g(\cdot)$ allowing the solver to "see" into the null-space. The assumption is constrained using the following loss:

\begin{equation}
    \L_\text{EI}=\L_\text{MC}+\L(\Tg\xhat,\ft(\A\Tg\xhat))
    \label{eq:ei}
\end{equation}

where $\Tg:G\rightarrow\text{Sym}(\mathcal{X})$ is an action of the group $G$.

\subsubsection{Data augmentation}
EI is related to data augmentation (DA), which, when ground truth $\x_\GT$ is available, can be used to constrain invariance or equivariance on $\x$, applied to MRI in \cite{fabian_data_2021}. VORTEX \cite{desai_vortex_2022} builds on this in the semi-supervised context by performing data augmentation on the measurements when ground truth is not available:

\begin{equation}
\L_\text{VORTEX}=
\begin{cases} 
\L_\text{sup}(\ft(\y),\x_\GT) & \text{if GT exists} \\
\L(\T_2\ft(y,\A),\ft(\T_1\y,\A\T_2^{-1})) & \text{else}
\end{cases}
\label{eq:vortex}
\end{equation}

where $\T_1$ is a random k-space transform (e.g. add noise, phase shift) and $\T_2$ is a random image transform (e.g. shift, rotate). We consider the fully unsupervised case, replacing $\L_\text{sup}$ with $\L_\text{MC}$ to enforce measurement consistency, noting that we do not expect this to learn any new information as data in the null-space remains unobserved. 

\subsubsection{Adversarial losses}
Adversarial losses have been proposed to reconstruct MRI via the dual training of a generator $\ft$ and a discriminator $D$. While unconditional, generative adversarial networks such as AmbientGAN \cite{bora_ambientgan_2018} are not feedforward at inference time, \cite{cole_fast_2021} propose a feedforward method with an adversarial loss for training:

\begin{equation}
    \L_\text{adversarial}=D(\tilde\A\ft(\y),\tilde\y)
    \label{eq:cole}
\end{equation}

where $\tilde\y\sim\mathcal{Y}$ i.e. another randomly sampled "real" measurement from the training dataset $\mathcal{Y}$. $D$'s aim is to distinguish between reconstructed measurements and training measurements, such that $\ft$'s aim is to reconstruct high quality measurements that are distinguishable from "real" measurements. Note that in a non-generative context, it is unclear how this loss can learn the signal model and recover information from the null-space. UAIR \cite{pajot_unsupervised_2018} is a related method using the following loss, originally proposed for inpainting:

\begin{equation}
    \hat\y=\tilde\A \ft(\y),\;\L_\text{UAIR}=D(\hat\y,\y)+\L(\tilde\A\ft(\hat\y),\hat\y)
    \label{eq:uair}    
\end{equation}

\subsubsection{Non-feedforward methods}

Finally, we do not consider ground-truth-free methods that require lengthy iterative procedures or optimisation at inference-time, since these are impractical in real-world application, including Deep Image Prior methods \cite{darestani_accelerated_2021}, generative models e.g. AmbientGAN \cite{bora_ambientgan_2018}, diffusion methods \cite{daras_ambient_2023,kawar_gsure-based_2024,aali_ambient_2024}, implicit neural representation methods \cite{shen_nerp_2024}, or self-supervised plug-and-play training methods such as \cite{shafique_mri_2024}.

\subsection{Other inverse problems}
\label{sec:other_problems}
\subsubsection{Noisy problems}
\label{sec:noisy_problems}

In this paper, we separate the task of solving ill-posed problems with removing measurement noise. To achieve the latter, it is often possible to add a denoising term to the loss function. For example, Robust EI \cite{chen_robust_2022} adds the Stein's Unbiased Risk Estimator (SURE) term, which is adapted by ENSURE \cite{aggarwal_ensure_2023}. Noise2Recon \cite{desai_noise2recon_2022} and Robust SSDU \cite{millard_clean_2024} both add a term similar to Noisier2Noise \cite{moran_noisier2noise_2019} to perform denoising on top of SSDU.

For a comparison of state-of-the-art denoising methods in solving inverse problems, see \cite{tachella_unsure_2025}.

\subsubsection{Single-operator problems}
\label{sec:single_operator}

Many of the methods considered in \cref{sec:benchmark} rely on the assumption that the image set is imaged with multiple operators, and that these operators span the full measurement domain in expectation. However, it is often the case, both in MRI and in general inverse problems, that all measurements are made with a single operator $\A$, for example a constant mask or blur kernel. In this case, we note that only the Equivariant Imaging (EI) losses \cite{chen_equivariant_2021} do not require the assumption and thus can still be used.