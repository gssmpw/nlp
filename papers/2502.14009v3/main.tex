\documentclass{article}

\usepackage[colorlinks=true,bookmarks=false]{hyperref}

\usepackage[accepted]{icml2025}

\usepackage{amsmath,amssymb,graphicx,amsthm}
\usepackage{enumitem,listings}

\usepackage[textsize=tiny]{todonotes}
\usepackage{orcidlink,textcomp}
\usepackage[capitalise]{cleveref}

\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem*{proposition}{Proposition}
\newcommand{\TODO}{\todo[inline, inlinewidth=1.5cm, noinlinepar]{TODO}}
\newcommand{\CITE}{\todo[inline, inlinewidth=1.5cm, noinlinepar]{CITE}}
\def\x{{\mathbf x}}
\def\X{{\mathcal X}}
\def\xhat{{\mathbf{\hat x}}}
\def\y{{\mathbf y}}
\def\A{{\mathbf A}}
\def\M{{\mathbf M}}
\def\T{{\mathbf T}}
\def\Tg{{\mathbf{T}_g}}
\def\Tgi{{\mathbf{T}_{g^{-1}}}}
\def\L{{\mathcal{L}}}
\def\GT{{\text{GT}}}
\def\ft{{f_\theta}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
}
\lstset{style=mystyle}


\icmltitlerunning{Benchmarking Self-Supervised Methods for Accelerated MRI Reconstruction}

\begin{document}
\twocolumn[
\icmltitle{Benchmarking Self-Supervised Methods for Accelerated MRI Reconstruction}
\begin{icmlauthorlist}
\icmlauthor{Andrew Wang\orcidlink{0000-0003-0838-7986}}{idcom}
\icmlauthor{Mike Davies\orcidlink{0000-0003-2327-236X}}{idcom}
\end{icmlauthorlist}
\icmlaffiliation{idcom}{Institute of Imaging, Data \& Communications, School of Engineering, University of Edinburgh, UK}
\icmlcorrespondingauthor{Andrew Wang}{andrew.wang@ed.ac.uk}
\icmlkeywords{magnetic resonance image reconstruction, self-supervised learning, equivariant imaging}

\vskip 0.3in
]
\printAffiliationsAndNotice{}


\begin{abstract}
Reconstructing MRI from highly undersampled measurements is crucial for accelerating medical imaging, but is challenging due to the ill-posedness of the inverse problem. While supervised deep learning approaches have shown remarkable success, they rely on fully-sampled ground truth data, which is often impractical or impossible to obtain. Recently, numerous self-supervised methods have emerged that do not require ground truth, however, the lack of systematic comparison and standard experimental setups have hindered research. We present the first comprehensive review of loss functions from all feedforward self-supervised methods and the first benchmark on accelerated MRI reconstruction without ground truth, showing that there is a wide range in performance across methods. In addition, we propose \textbf{M}ulti-\textbf{O}perator \textbf{E}quivariant \textbf{I}maging (\textbf{MO-EI}), a novel framework that builds on the imaging model considered in existing methods to outperform all state-of-the-art and approaches supervised performance. Finally, to facilitate reproducible benchmarking, we provide implementations of all methods in the \href{https://deepinv.github.io/}{DeepInverse library} and easy-to-use \href{https://andrewwango.github.io/deepinv-selfsup-fastmri}{demo code here}.
\end{abstract}

\section{Introduction}

Ill-posed inverse problems are ubiquitous in scientific and medical imaging and can be written as follows:

\begin{equation}
    \y_i=\eta(A(\x_i))
    \label{eq:inverse_problems}
\end{equation}

where $\y_i\in\mathbb{R}^m$ are measurements indexed by $i$, $\x_i\in\mathcal{X}\in\mathbb{R}^n$ are the underlying signals, $A(\cdot)$ is the forward operator and $\eta(\cdot)$ is the noise operator. The goal is to recover $\xhat=\ft(\y)$ via a solver $\ft$, but $m<n$ in general, leading to an ill-posed problem. In this work, we consider the problem of accelerated MRI reconstruction which is often modelled as a linear problem with Gaussian noise:

\begin{equation}
    \y_{i,c}=\A_{i,c}\x_i+\epsilon,\;\A_i=\M_i\mathbf{F}\mathbf{S}_c,\;\epsilon\sim\mathcal{N}(0,\sigma)
    \label{eq:mri_problem}
\end{equation}

where $\mathbf{F}$ is a Fourier operator, $\y_{i,c}$ is the $(i,c)$-th measurement associated with the $i$-th undersampling mask $\M_i$ and the $c$-th sensitivity map $\mathbf{S}_c$, and $\y$ has dimension $\alpha=m=n/\alpha$ where $\alpha$ is the acceleration rate. We consider the scenario where measurements are single-coil $C=1$ and noiseless $\sigma=0$, thereby removing the effect of these somewhat orthogonal problems, in order to focus on tackling the ill-posedness of the problem, although the analysis easily generalises to the multi-coil scenario, and we discuss the noisy scenario in \cref{sec:noisy_problems}.

We emphasise that in MRI, the image set is often imaged through multiple operators $\A_i\sim\mathcal{A}$, via a random subsampling mask $\M_i\sim\mathcal{M}$. This assumption is often required for methods that learn from multiple operators, but we consider the single-operator case $\A_i=\A\forall i$ in \cref{sec:single_operator}.

The classical method of compressed sensing formulates the problem as regularised optimisation and is solved using variational inference \cite{aggarwal_modl_2019}. However, these methods require hand-crafted priors, lengthy and expensive inference-time optimisation and acceleration rates may be limited.

Deep learning has been used to learn a solver directly from data, and achieved impressive results compared to classical methods. A popular framework which incorporates a learnt prior with classical data consistency is algorithm unrolling \cite{aggarwal_modl_2019}, which performs finite iterations of the optimisation and replaces an element of the prior with a neural network, which is then trained end-to-end by alternating between denoising and data consistency steps to estimate $\xhat=\ft(\y,\A)$. Most methods assume that ground-truth (GT) data is available in order to train with supervised learning $\L_\text{sup}=\L(\xhat,\x_\GT)$. These \textit{feedforward} methods are desirable because they provide fast inference requiring only one forward pass as opposed to methods such as diffusion or other generative models which require lengthy iterations or optimisations at inference time.

However, fully-sampled GT measurements are often difficult or impossible to obtain in real scenarios, for example when imaging moving organs, or expensive, for lengthy high-resolution imaging. Self-supervised (a.k.a. unsupervised) learning is therefore needed to circumvent the need for GT, and numerous methods have been proposed in recent years. However they often fail to compare to existing methods, and it is difficult to draw direct comparisons due to variety in datasets, models, forward physics, or lack of clear implementation, hindering fast research and leading to poor applicability of these methods in practice. Recent surveys such as \cite{ongie_deep_2020,zeng_review_2021} mention various self-supervised methods included here, but lack a principled comparison, do not provide experimental results and mostly compare older methods.

We provide the first systematic survey of state-of-the-art self-supervised feedforward methods, and the first set of experimental results benchmarking these methods on accelerated MRI reconstruction without GT. Additionally, inspired by the benefits of these methods, we propose a new method which combines these into one framework. Finally, to facilitate reproducible benchmarking, we implement all compared methods using the DeepInverse library \cite{tachella_deepinverse_2023}, and provide a \href{https://andrewwango.github.io/deepinv-selfsup-fastmri}{demo notebook here}.

\subsection{Contributions}

\begin{enumerate}
    \item A benchmark comparison of state-of-the-art self-supervised feedforward methods for learning to reconstruct accelerated MRI without ground truth;
    \item A new method, multi-operator equivariant imaging (MO-EI), that significantly outperforms all the above.
\end{enumerate}

\section{Related work}

\subsection{Benchmarked methods}
\label{sec:benchmark}
We provide a systematic review of existing state-of-the-art feedforward self-supervised methods from the literature i.e. methods which only require measurements $\y$ from a single acquisition at training and inference, without any ground truth. We formulate their loss functions and summarise all compared methods in \cref{tab:methods} along with their implementations in the DeepInverse \cite{tachella_deepinverse_2023} library. We focus on the design of the loss function since, in general, these loss functions are agnostic to the NN architecture and thus can be independently used with any latest architecture.

\begin{table*}[t]
\centering
\caption{Self-supervised losses reviewed in \cref{sec:benchmark}, implemented in DeepInverse and benchmarked in our experiments.}
\label{tab:methods}
\begin{tabular}{llll}
\hline
Method & Ref & Loss & Implementation \\ \hline
MC & \cite{darestani_accelerated_2021} & \cref{eq:mc} & \verb+dinv.loss.MCLoss()+ \\
SSDU & \cite{yaman_self-supervised_2020} & \cref{eq:splitting} & \verb+dinv.loss.SplittingLoss(eval_split_input=False)+ \\
Noise2Inverse & \cite{hendriksen_noise2inverse_2020} & \cref{eq:n2i} & \verb+dinv.loss.SplittingLoss(eval_split_input=True)+ \\
Weighted-SSDU & \cite{millard_theoretical_2023} & \cref{eq:millard} & \verb+dinv.loss.WeightedSplittingLoss()+ \\
Adversarial & \cite{cole_fast_2021} & \cref{eq:cole} & \verb+dinv.loss.UnsupAdversarialGeneratorLoss()+ \\
UAIR & \cite{pajot_unsupervised_2018} & \cref{eq:uair} & \verb+dinv.loss.UAIRGeneratorLoss()+ \\
VORTEX & \cite{desai_vortex_2022} & \cref{eq:vortex} & \verb+dinv.loss.VORTEXLoss()+ \\
EI & \cite{chen_equivariant_2021} & \cref{eq:ei} & \verb+dinv.loss.EILoss()+ \\
MOI & \cite{tachella_unsupervised_2022} & \cref{eq:moi} & \verb+dinv.loss.MOILoss()+ \\
MO-EI & Ours & \cref{eq:moei} & \verb+dinv.loss.MOEILoss()+ \\ \hline
\end{tabular}
\end{table*}

\subsubsection{Measurement consistency}
The most naive self-supervised loss simply compares the recorrupted estimate $\A\xhat$ to the measurement $\y$:

\begin{equation}
    \L_\text{MC}=\L(\A\xhat,\y)
    \label{eq:mc}
\end{equation}

where $\L$ is any metric such as the L2 norm. This loss originally appeared with a classical regularising term to solve inverse problems with deep learning and traditional regularisation, but these still faced problems in designing the regularisers. Without a regularising term, the loss cannot recover information from the null-space of the operator $\mathcal{N}(\A)$ as infinite solutions $\A^\top\y+\mathbf{v}$ can satisfy measurement consistency (MC) where $\mathbf{v}\in\mathcal{N}(\A)$. This loss function is also used in the presence of strong inductive bias where the regularisation is provided by the architecture, for example from specific architectures \cite{darestani_accelerated_2021} or initialisations \cite{darestani_test-time_2022}, or in cycle consistency losses \cite{oh_unpaired_2020}.

In the noisy scenario, some recent works such as ENSURE \cite{aggarwal_ensure_2023} or GSURE \cite{metzler_unsupervised_2020} generalise Stein's Unbiased Risk Estimator (SURE) to reconstruct from noisy undersampled measurements. However, in the noiseless case considered here $\sigma\rightarrow0$, these methods simply reduce to $\L_\text{MC}$, and so will not be compared.

\subsubsection{Learning from multiple operators}
A popular set of state-of-the-art approaches leverages the fact that the whole measurement space is spanned by multiple imaging operators. Measurement splitting methods, including SSDU and similar papers \cite{yaman_self-supervised_2020,acar_self-supervised_2021,huang_self-supervised_2024,zhou_dual-domain_2022,hu_self-supervised_2021}, randomly split the k-space into two sets at each iteration. One is used as the input and the other to construct the loss at the output:
\begin{equation}
    \L_\text{SSDU}=\L(\M_2\A_i \ft(\M_1\y_i,\M_1\A_i),\M_2\y_i)
    \label{eq:splitting}
\end{equation}

where $\M_1$ is a randomly generated mask, $\M_1+\M_2=\mathbb{I}_m$, and the inference-time estimate is $\xhat_i=\ft(\y_i,\A_i)$. Note that some methods \cite{hu_spicer_2024,gan_deep_2021} require pairs of measurements of the same subject; these are equivalent to SSDU but with 2x less acceleration. Similarly, Artifact2Artifact \cite{gan_deformation-compensated_2022,liu_rare_2020}, when applied to static MRI reconstruction without paired measurements, is also identical to SSDU (see \cref{sec:a2a} for proof). We therefore consider SSDU as the archetypal method to compare.

An inference-time adaptation to SSDU performs a Monte-Carlo averaging over subsampled inputs; this is inspired by Noise2Inverse \cite{hendriksen_noise2inverse_2020}:
\begin{equation}
    \xhat=\frac{1}{N}\Sigma_{i=1}^n \ft(\M_i\y,\M_i\A)
    \label{eq:n2i}
\end{equation}

More recently, \cite{millard_theoretical_2023} frames SSDU as a Bernoulli-noise special case of Noisier2Noise \cite{moran_noisier2noise_2019}, and propose a weighting to the SSDU loss and an informed splitting strategy to improve SSDU:

\begin{align}
    \L_\text{Weighted-SSDU}&=(1-\mathbf{K})^{-1/2}\L_\text{SSDU} \\
    \mathbf{K}&=(\mathbb{I}_n-\tilde{\mathbf{P}}\mathbf{P})^{-1}(\mathbb{I}_n-\mathbf{P})
\label{eq:millard}
\end{align}

where $\mathbf{P}=\mathbb{E}[\M_i],\tilde{\mathbf{P}}=\mathbb{E}[\M_1]$ i.e. the PDFs of the imaging mask and the splitting mask, respectively.

Finally, Multi-Operator Imaging (MOI) \cite{tachella_unsupervised_2022} leverages the fact that the image set is seen through multiple operators $\A_i$ by drawing a random operator $\tilde\A\sim\mathcal{A}$ at each forward pass to learn in the null-space:

\begin{equation}
    \L_\text{MOI}=\L_\text{MC}+\L(\xhat,\ft(\tilde\A\xhat))
    \label{eq:moi}
\end{equation}

\subsubsection{Equivariant imaging}
Equivariant Imaging (EI) \cite{chen_equivariant_2021} is a state-of-the-art method that constrains the set of solutions using group invariance, which has been applied to MRI in \cite{chen_robust_2022}, optical camera imaging in \cite{wang_perspective-equivariance_2024}, and dynamic MRI in \cite{wang_fully_2024}. By leveraging a natural assumption that the (unknown) image set $\mathcal{X}$ is invariant to a group $G$ of transformations $g\x\in\mathcal{X}\forall\x\in\mathcal{X},g\in G$, the image set is observed through multiple transformed operators $\A_i\circ g(\cdot)$ allowing the solver to "see" into the null-space. The assumption is constrained using the following loss:

\begin{equation}
    \L_\text{EI}=\L_\text{MC}+\L(\Tg\xhat,\ft(\A\Tg\xhat))
    \label{eq:ei}
\end{equation}

where $\Tg:G\rightarrow\text{Sym}(\mathcal{X})$ is an action of the group $G$.

\subsubsection{Data augmentation}
EI is related to data augmentation (DA), which, when ground truth $\x_\GT$ is available, can be used to constrain invariance or equivariance on $\x$, applied to MRI in \cite{fabian_data_2021}. VORTEX \cite{desai_vortex_2022} builds on this in the semi-supervised context by performing data augmentation on the measurements when ground truth is not available:

\begin{equation}
\L_\text{VORTEX}=
\begin{cases} 
\L_\text{sup}(\ft(\y),\x_\GT) & \text{if GT exists} \\
\L(\T_2\ft(y,\A),\ft(\T_1\y,\A\T_2^{-1})) & \text{else}
\end{cases}
\label{eq:vortex}
\end{equation}

where $\T_1$ is a random k-space transform (e.g. add noise, phase shift) and $\T_2$ is a random image transform (e.g. shift, rotate). We consider the fully unsupervised case, replacing $\L_\text{sup}$ with $\L_\text{MC}$ to enforce measurement consistency, noting that we do not expect this to learn any new information as data in the null-space remains unobserved. 

\subsubsection{Adversarial losses}
Adversarial losses have been proposed to reconstruct MRI via the dual training of a generator $\ft$ and a discriminator $D$. While unconditional, generative adversarial networks such as AmbientGAN \cite{bora_ambientgan_2018} are not feedforward at inference time, \cite{cole_fast_2021} propose a feedforward method with an adversarial loss for training:

\begin{equation}
    \L_\text{adversarial}=D(\tilde\A\ft(\y),\tilde\y)
    \label{eq:cole}
\end{equation}

where $\tilde\y\sim\mathcal{Y}$ i.e. another randomly sampled "real" measurement from the training dataset $\mathcal{Y}$. $D$'s aim is to distinguish between reconstructed measurements and training measurements, such that $\ft$'s aim is to reconstruct high quality measurements that are distinguishable from "real" measurements. Note that in a non-generative context, it is unclear how this loss can learn the signal model and recover information from the null-space. UAIR \cite{pajot_unsupervised_2018} is a related method using the following loss, originally proposed for inpainting:

\begin{equation}
    \hat\y=\tilde\A \ft(\y),\;\L_\text{UAIR}=D(\hat\y,\y)+\L(\tilde\A\ft(\hat\y),\hat\y)
    \label{eq:uair}    
\end{equation}

\subsubsection{Non-feedforward methods}

Finally, we do not consider ground-truth-free methods that require lengthy iterative procedures or optimisation at inference-time, since these are impractical in real-world application, including Deep Image Prior methods \cite{darestani_accelerated_2021}, generative models e.g. AmbientGAN \cite{bora_ambientgan_2018}, diffusion methods \cite{daras_ambient_2023,kawar_gsure-based_2024,aali_ambient_2024}, implicit neural representation methods \cite{shen_nerp_2024}, or self-supervised plug-and-play training methods such as \cite{shafique_mri_2024}.

\subsection{Other inverse problems}
\label{sec:other_problems}
\subsubsection{Noisy problems}
\label{sec:noisy_problems}

In this paper, we separate the task of solving ill-posed problems with removing measurement noise. To achieve the latter, it is often possible to add a denoising term to the loss function. For example, Robust EI \cite{chen_robust_2022} adds the Stein's Unbiased Risk Estimator (SURE) term, which is adapted by ENSURE \cite{aggarwal_ensure_2023}. Noise2Recon \cite{desai_noise2recon_2022} and Robust SSDU \cite{millard_clean_2024} both add a term similar to Noisier2Noise \cite{moran_noisier2noise_2019} to perform denoising on top of SSDU.

For a comparison of state-of-the-art denoising methods in solving inverse problems, see \cite{tachella_unsure_2025}.

\subsubsection{Single-operator problems}
\label{sec:single_operator}

Many of the methods considered in \cref{sec:benchmark} rely on the assumption that the image set is imaged with multiple operators, and that these operators span the full measurement domain in expectation. However, it is often the case, both in MRI and in general inverse problems, that all measurements are made with a single operator $\A$, for example a constant mask or blur kernel. In this case, we note that only the Equivariant Imaging (EI) losses \cite{chen_equivariant_2021} do not require the assumption and thus can still be used.

\section{Our method}

Motivated by the fact that adding more, well-targeted regularisation will typically improve the stability and generalisation of the model, we build upon the EI \cite{chen_equivariant_2021} and MOI \cite{tachella_unsupervised_2022} losses to propose a hybrid Multi-Operator Equivariant Imaging (MO-EI) loss function. Let $\mathcal{A}$ be the operator set of order $\lvert\mathcal{A}\rvert={n\choose m}$, as it is isomorphic to the mask index set of $m=n/\alpha$-combinations of the index set $\mathbb{I}_n$. Define the \textit{augmented operator set} $\mathcal{A}_G=\{\A_i\Tg\;\forall \A_i\in\mathcal{A},g\in G\}$ as the orbit space of the set $\mathcal{A}$ under the action of the group $G$, and assume that that these operators image the image set. Since $\lvert\mathcal{A}_G\rvert=\lvert\mathcal{A}\rvert\lvert G\rvert$ is much larger than in MOI or EI, we expect to be able to learn more information in the null-spaces of the operators and thereby improve the performance. We leverage the augmented operator set using the loss function:

\begin{equation}
    \L_\text{MO-EI}=\L_\text{MC}+\L(\Tg\xhat,\ft(\tilde\A_g\xhat)),\;\tilde\A_g\sim\mathcal{A}_G
    \label{eq:moei}
\end{equation}

The loss function is visualised diagrammatically in \cref{fig:framework}.

\begin{remark}[MO-EI generalises MOI and EI]
By letting $G$ be the trivial group, we recover MOI \cite{tachella_unsupervised_2022}, and by letting $\mathcal{A}=\{\A\}$, we recover EI \cite{chen_equivariant_2021}.
\end{remark}

For the group $G$, since we have soft deformable tissue \cite{gan_deformation-compensated_2022}, we expect $\mathcal{X}$ be invariant to the group of $C^1$-diffeomorphisms. As these are very general, we relax the full assumption and instead enforce approximate equivariance to small distortions (see \cref{fig:transforms}) by taking the subset of continuous piecewise-affine-based diffeomorphic transforms from \cite{freifeld_transformations_2017}.

\begin{figure}[h]
  \includegraphics[width=0.52\textwidth]{img/moei.pdf}
  \caption{Our multi-operator equivariant imaging (MO-EI) framework.}
  \label{fig:framework}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.48\textwidth]{img/transforms.png}
  \caption{Diffeomorphic transforms.}
  \label{fig:transforms}
\end{figure}


\section{Experiments}

We compare all methods listed in \cref{tab:methods}, including state-of-the-art methods and our proposed method, using their implementations in the DeepInverse library \cite{tachella_deepinverse_2023}. We train a neural network network using a given loss on simulated measurements at 6x acceleration on brain and knee MRI data from the FastMRI dataset \cite{zbontar_fastmri_2018}. We use a simple unrolled MoDL neural network from \cite{aggarwal_modl_2019} with a residual U-Net \cite{ronneberger_u-net_2015} backbone and 3 unrolled iterations, and the same hyperparameters for all loss functions to provide fair comparisons. In addition, we train a model with supervised learning with access to oracle ground truth data, as a gold-standard comparison. We report PSNR and SSIM defined on the magnitude of the complex data. Further details on implementations of the various benchmarked loss functions, the dataset and experiment setup are in \cref{sec:experiment_details}.

We provide example code for how to train the reconstruction algorithm with the benchmarked methods in \cref{sec:code}, and provide full demo code here \href{https://andrewwango.github.io/deepinv-selfsup-fastmri}{at this URL}.

Finally, we run further experiments to reproduce SSDU \cite{yaman_self-supervised_2020} and its variants \cite{millard_theoretical_2023} in \cref{sec:ssdu} and test the robustness capabilities of our trained models to out-of-domain data in \cref{sec:ood}.

\section{Results}
\label{sec:results}

\begin{table}[t]
\centering
\begin{tabular}{llll}
\hline
Loss & PSNR $\uparrow$ & SSIM $\uparrow$ \\ \hline
Zero-filled & 27.67 & .7862 \\
MC & 27.66 & .7861 \\
SSDU* & 18.27 & .4876 \\
Noise2Inverse* & 18.49 & .4693 \\
Weighted-SSDU & 29.12 & .8054 \\
Adversarial & 18.52 & .4732 \\
UAIR & 14.00 & .3715 \\
VORTEX & 27.75 & .7898 \\
EI & 30.26 & .8523 \\
MOI & 30.29 & .8651 \\
\textbf{MO-EI (ours)} & \textbf{32.14} & \textbf{.8846} \\
(Supervised) & (33.15) & (.9032) \\
\hline
\end{tabular}
\caption{Test set results for 6x accelerated MRI on FastMRI brain dataset. Best unsupervised method in \textbf{bold}. *we note the unexpected poor performance of these methods and further analyse them in \cref{sec:ssdu,tab:test_ssdu_further,fig:test_ssdu_further}.}
\label{tab:test_brain_acc_6}
\end{table}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.999\textwidth]{img/results_brain.png}
  \caption{Sample reconstructions for 6x accelerated brain MRI reconstruction with average test-set PSNR labelled, comparing a subset of methods from the full set of compared methods. See \cref{tab:test_brain_acc_6} for full results.}
  \label{fig:test_brain_acc_6}
\end{figure*}

Numerical results and sample reconstructions are shown for the brain dataset in \cref{tab:test_brain_acc_6,fig:test_brain_acc_6} and for the knee dataset in \cref{tab:test_knee_acc_6,fig:test_knee_acc_6}. Note that all images are normalised to the range $[0,1]$ before plotting. The no-learning zero-filled reconstruction is provided for reference. We provide an ablation of the components of our loss function (compared to MOI and EI) in \cref{sec:ablation}.

As expected, measurement consistency (MC) cannot learn any information and recovers the no-learning performance. While EI and MOI both show good results compared to MC, our method, MO-EI, improves significantly on both these previous state-of-the-art methods, approaches the oracle supervised learning performance, and recovers details in the brain images where EI and MOI have strong artifacts (see insets).

In this setup, the performance of SSDU is very poor and drastically worse than no-learning. However, we note that qualitatively it seems to remove some artifacts (\cref{fig:test_ssdu_further}), and we explore why this is the case in \cref{sec:ssdu} and show how to improve the results. Additionally, the inference-time modification inspired by Noise2Inverse from \cref{eq:n2i} does not seem to improve the results. However, by adding the weighting from \cref{eq:millard}, weighted-SSDU performs much better; however, we notice that artifacts are still present in its reconstructions, reflected in its poorer SSIM.

We note the very poor performance of the adversarial losses, and suggest that this is due to the very sensitive training of $\ft$ and $D$. Due to imbalanced training, the network seems to severely decimate the input far beyond the no-learning reconstruction, and we struggle to reproduce the results from \cite{cole_fast_2021}, which may require heavy hyperparameter tuning and specialised model architectures; UAIR \cite{pajot_unsupervised_2018} does not show results on accelerated MRI and we assume that their results do not transfer to this setting. Finally, VORTEX learns only marginal information compared to MC as expected, see \cref{sec:experiment_details} for a full analysis.

\section{Further experiments}

\subsection{Reproducing SSDU}
\label{sec:ssdu}

We explore the seemingly poor performance of SSDU and attempt to reproduce the performance reported in \cite{yaman_self-supervised_2020}. First, we add 4 more unrolled iterations to the neural network from 3 to 7, increasing the training computational cost by more-than-double. Note that this means these results are no longer directly comparable with other methods reported in \cref{sec:results}. We observe a considerable qualitative performance increase but not quantitatively in \cref{fig:test_ssdu_further,tab:test_ssdu_further}. Then, noting that SSDU is trained on further subsampled measurements $\M_1\y_i$ but tested on just the subsampled measurements $\y_i$, we normalise the model output to the range $[0,1]$. This improves the results, but removes any quantitative information in the pixel values. Finally, we standardise the output to the mean and standard deviation of the ground truth $\x$. While this now shows an improvement over no-learning, this requires ground-truth statistics and thus would no longer be fully self-supervised. We note that the Noise2Inverse inference time procedure also does not help, differing from comparisons observed in dynamic MRI \cite{wang_fully_2024}. We also observe a similar pattern in weighted-SSDU, whose performance improves as the number of unrolled iterations increases.

\begin{table}[t]
\centering
\begin{tabular}{llll}
\hline
Loss & PSNR $\uparrow$ & SSIM $\uparrow$ \\ \hline
Zero-filled & 27.67 & .7862 \\
SSDU (3) & 18.27 & .4876 \\
SSDU (7) & 17.56 & .4970 \\
SSDU (7) normalised & 26.75 & .7525 \\
SSDU (7) standardised & 29.60 & .8342 \\
N2I (7) standardised & 28.27 & .7825 \\
Weighted-SSDU (3) & 29.12 & .8054 \\
Weighted-SSDU (7) & 31.64 & .8381 \\
\hline
\end{tabular}
\caption{Further results for training SSDU \cite{yaman_self-supervised_2020,millard_theoretical_2023} with a larger model (7 unrolled iters), and metrics after normalising output to $[0,1]$ or standardising output (requires ground truth statistics $\bar\x_\GT,\sigma_{\bar\x}$).}
\label{tab:test_ssdu_further}
\end{table}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.999\textwidth]{img/results_brain_ssdu.png}
  \caption{Sample reconstructions for further experiments on SSDU \cite{yaman_self-supervised_2020} on brain data, where $(3)$ represents a model with 3 unrolled iterations.}
  \label{fig:test_ssdu_further}
\end{figure*}

\subsection{Out-of-domain generalisation}
\label{sec:ood}

Now suppose that ground truth data is available. We evaluate the robustness and generalisation of our models trained on brain data to out-of-domain, knee data. We compare a model trained with supervised learning on the brain data, our proposed self-supervised MO-EI method, and a model trained alternating between supervised and self-supervised ("interleaved") losses. The results in \cref{tab:test_brain_knee_acc_6} show that, even when ground truth is available, adding a self-supervised loss during supervised training significantly improves the robustness of the model. Qualitatively, the pure supervised model places strange brain artifacts in the knees in \cref{fig:test_brain_knee_acc_6}, where the interleaved training removes this while retaining a sharp image.

\begin{table}[t]
\centering
\begin{tabular}{llll}
\hline
Loss & PSNR $\uparrow$ & SSIM $\uparrow$ \\ \hline
Supervised & 30.63 & .7759 \\
Self-sup & 29.54 & .7390 \\
Interleaved & 31.55 & .7910 \\
\hline
\end{tabular}
\caption{Out-of-domain generalisation results for 6x accelerated MRI trained on the FastMRI brain dataset and tested on knees.}
\label{tab:test_brain_knee_acc_6}
\end{table}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.999\textwidth]{img/results_brkn_trans.png}
  \caption{Sample reconstructions for out-of-domain generalisation experiments trained on 6x acc. brains and tested on 6x acc. knees with average test-set PSNR labelled.}
  \label{fig:test_brain_knee_acc_6}
\end{figure*}

\section{Conclusion}

In this paper we provide an analysis of state-of-the-art self-supervised feedforward methods to train neural networks to solve the accelerated MRI reconstruction problem without ground-truth. We implement and benchmark these methods on fair experiments using real MRI data. We show that while some methods including EI and MOI \cite{chen_equivariant_2021,tachella_unsupervised_2022} show decent results, visible artifacts are still present in their reconstructions. We demonstrate that they generally outperform measurement-splitting methods including SSDU \cite{yaman_self-supervised_2020}, which we find requires a more expensive model and output standardisation to compete with measurement consistency, which cannot learn any more information than the zero-filled no-learning reconstruction. Furthermore, we propose a new method, multi-operator equivariant imaging (MO-EI), which significantly outperforms all existing methods and approaches the performance of supervised learning. Additionally, in experiments where ground-truth \textit{is} available, we show that adding the self-supervised loss during training considerably improves robustness to out-of-domain data. Finally, we provide easy-to-use implementations of all the benchmarked loss functions using the DeepInverse library \cite{tachella_deepinverse_2023} which allows fast experimentation using cutting edge methods and custom datasets.

\section{Acknowledgements}

We thank Keying Guo for help with code implementation for reproducing \cite{millard_theoretical_2023,millard_clean_2024}.

\bibliography{references}
\bibliographystyle{icml2025}

\newpage
\onecolumn
\appendix
\section*{Supplementary Material}
\section{Equivalence of Artifact2Artifact and SSDU}
\label{sec:a2a}
\begin{proposition}[Unpaired Artifact2Artifact as sum of SSDU losses]
Consider the Artifact2Artifact \cite{liu_rare_2020} scenario where $\y_i^{(k)}$ be independent sets of measurements of the same subject $i$ (e.g. arriving in a stream) such that $\y_i=\bigcup_{k}\y_i^{(k)}\in\mathbb{R}^m$ is the full undersampled measurement from one acquisition $i$ (i.e. there are no more measurements of the same subject). $\A_i^{(k)\top}\y_i^{(k)}$ are then the independent "artifact"-corrupted images. The Artifact2Artifact loss draws a pair $k,l$ randomly at each iteration and constructs the loss
\begin{equation*}
    \L_\text{A2A}^{(k,l)}=\L(\A_i^{(l)}\ft(\y_i^{(k)},\A_i^{(k)}),\y_i^{(l)})
\end{equation*}
where $\A_i^{(k)},\A_i^{(l)}$ are the operators associated with each measurement. We can then write this as the overlapping SSDU loss \citep[Fig.~5]{yaman_self-supervised_2020} by setting $\A_i^{(k)}=\M_k\A_i,\A_i^{(l)}=\M_l\A_i,\y_i^{(k)}=\M_k\y_i,\y_i^{(l)}=\M_l\y_i$ where $\M_k,\M_l$ are two overlapping mask subsampling sets. Then
\begin{equation*}
    \L_\text{A2A}=\sum_k\sum_{l\neq k}\L_\text{SSDU}^{(k,l)}
\end{equation*}

\end{proposition}
%Partition $\y^{(k)},\y^{(l)}$ into disjoint sets $(\y^{(k\setminus l)},\y^{(k\cap l)}),(\y^{(l\setminus k)},\y^{(k\cap l)})$. 

\section{Data and hyperparameter details}
\label{sec:experiment_details}
We use a subset of the FastMRI dataset for our experiments. We take the middle slice of 455 brain volumes, use the provided $320\times320$ root-sum-square reconstructions as ground truth, normalise them to $[0,1]$ and simulate measurements using an undersampled Fourier transform. We randomly create train-test datasets with 80-20 split. The knee dataset is created similarly for 973 slices. For the subsampling masks, we use random 1D Gaussian masking with 6x acceleration and 8\% autocalibration lines kept in the centre. For the denoiser of our unrolled network, we use a residual U-Net with no batch norm of depth 4, with a total of 8.6M parameters. We train with the Adam optimiser at $1e-3$ learning rate, although we tweak this per method down to $1e-5$ to achieve convergence. 

For loss functions involving measurement splitting (i.e. SSDU), we use a 2D Gaussian splitting mask with $\rho=0.6$ following \cite{yaman_self-supervised_2020}, and for the weighted Noisier2Noise variant of SSDU we use an independent 1D Gaussian mask following \cite{millard_theoretical_2023}. For losses involving equivariant imaging, we draw randomly a group transform at each iteration; for rotation the group being $G=\text{SO}(\mathbb{R}^2)$ and for diffeomorphisms $G=\text{Diffeo}(\mathbb{R}^2)$. For VORTEX \cite{desai_vortex_2022}, noting that the results of their various variants are very similar, for $\T_1$ we use random noise and their random phase errors with $\sigma=\alpha=1$ and for $\T_2$ we use random maximum $\pm 10\%$ shifts and random $\pm 15^{\circ}$ rotations. We also observe reduced performance when adding more transformations including full $\pm 360^{\circ}$ rotation, scaling, or shearing. We observe that when using the VORTEX consistency term on its own without any supervised or MC loss, it learns the trivial $\ft(y)=\mathbf{0}$, showing the superfluity of VORTEX. 

For the adversarial losses, we use the same generator $\ft$ and the simple convolutional discriminator with skip connections used in \cite{cole_fast_2021}. 

All components of our experiments are implemented using DeepInverse \cite{tachella_deepinverse_2023}. We provide a demo below and full code at \href{https://andrewwango.github.io/deepinv-selfsup-fastmri}{this URL}.

\section{Python code for running experiments using DeepInverse}
\label{sec:code}

The following Python code can be used to train a reconstruction algorithm with the methods considered in this paper, where the loss functions \verb+loss+ can be replaced by those listed in \cref{tab:methods}.

\begin{lstlisting}[language=Python,mathescape]
import deepinv as dinv
import torch

# Define forward problem $\A$
physics_generator = dinv.physics.generator.GaussianMaskGenerator((320, 320), acceleration=4)
physics = dinv.physics.MRI((320, 320))

# Define FastMRI dataset $\x$
dataset = dinv.datasets.SimpleFastMRISliceDataset("data", train=True, download=True)
train_dataset, test_dataset = torch.utils.data.random_split(dataset, (0.8, 0.2))

# Simulate measurements $(\x,\y)$
dataset_path = dinv.datasets.generate_dataset(
    train_dataset=train_dataset,
    test_dataset=test_dataset,
    physics=physics,
    physics_generator=physics_generator,
    save_physics_generator_params=True,
    save_dir="data"
)

train_dataset = dinv.datasets.HDF5Dataset(dataset_path, split="train", load_physics_generator_params=True)
test_dataset  = dinv.datasets.HDF5Dataset(dataset_path, split="test", load_physics_generator_params=True)

train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_dataset)

# Define reconstruction $\ft$
denoiser = dinv.models.UNet(2, 2, scales=3)
model = dinv.utils.demo.demo_mri_model(denoiser=denoiser, num_iter=3)

# Define loss $\L$
loss = ...

trainer = dinv.Trainer(
    model = model,
    physics = physics,
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3),
    train_dataloader = train_dataloader,
    eval_dataloader = test_dataloader,
    epochs = 100,
    losses = loss,
    metrics = dinv.metric.PSNR(complex_abs=True),
)

trainer.train()
\end{lstlisting}

\section{Ablation}
\label{sec:ablation}

We ablate for the components of our proposed loss function by interpolating between existing methods MOI and EI and our method MO-EI, and report results in \cref{tab:ablation}. We note that the base MOI and EI have similar results, and adding the two independent components of a) MO-EI and b) the diffeomorphic transforms helps, but combining these two provides the best performance in our proposed method, as per our theoretical framework.

\begin{table}[t]
\centering
\begin{tabular}{llll}
\hline
Loss & PSNR $\uparrow$ & SSIM $\uparrow$ \\ \hline
MOI & 30.29 & .8651 \\
EI (rotate) & 30.26 & .8523 \\
EI (diffeo) & 31.26 & .8741 \\
MO-EI (rotate) & 30.62 & .8575 \\
\textbf{MO-EI (diffeo, ours)} & \textbf{32.14} & \textbf{.8846} \\
\hline
\end{tabular}
\caption{Ablation of components of our proposed loss function on brain dataset.}
\label{tab:ablation}
\end{table}

\section{Additional results}
\label{sec:additional_results}

We report results of experiments comparing methods trained on the knee dataset in \cref{fig:test_knee_acc_6,tab:test_knee_acc_6}.

\begin{table}[t]
\centering
\begin{tabular}{llll}
\hline
Loss & PSNR $\uparrow$ & SSIM $\uparrow$ \\ \hline
Zero-filled & 27.98 & .7041 \\
MC & 27.98 & .7040 \\
SSDU & 10.91 & .4131 \\
Noise2Inverse & 11.22 & .3969 \\
Weighted-SSDU & 29.59 & .7743 \\
VORTEX & 27.97 & .7023 \\
EI & 31.08 & .7582 \\
MOI & 31.45 & .7787 \\
\textbf{MO-EI (ours)} & \textbf{31.91} & \textbf{.7848} \\
(Supervised) & (32.68) & (.8141) \\
\hline
\end{tabular}
\caption{Test set results for 6x accelerated MRI on FastMRI knee dataset. Best unsupervised method in \textbf{bold}.}
\label{tab:test_knee_acc_6}
\end{table}

\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.999\textwidth]{img/results_knee.png}
  \caption{Sample reconstructions for 6x accelerated knee MRI reconstruction with average test-set PSNR labelled.}
  \label{fig:test_knee_acc_6}
\end{figure*}


\end{document}
