\section{Related work}
\subsection{Benchmarked methods}
\label{sec:benchmark}
We provide a systematic review of existing state-of-the-art feedforward self-supervised methods from the literature i.e. methods which only require measurements $\y$ from a single acquisition at training and inference, without any ground truth. We formulate their loss functions and summarise all compared methods in \cref{tab:methods} along with their implementations in the DeepInverse **Chen et al., "Deep Image Prior"** library. We focus on the design of the loss function since, in general, these loss functions are agnostic to the NN architecture and thus can be independently used with any latest architecture.

\begin{table*}[t]
\centering
\caption{Self-supervised losses reviewed in \cref{sec:benchmark}, implemented in DeepInverse and benchmarked in our experiments.}
\label{tab:methods}
\begin{tabular}{llll}
\hline
Method & Ref & Loss & Implementation \\ \hline
MC & **Chen et al., "Deep Image Prior"** & \cref{eq:mc} & \verb+dinv.loss.MCLoss()+ \\
SSDU & **Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"**__**Schlemper et al., "DeepADMM: A Deep Neural Network for Accelerated Dynamic MRI Reconstruction"** & \cref{eq:splitting} & \verb+dinv.loss.SplittingLoss(eval_split_input=False)+ \\
Noise2Inverse & **Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"** & \cref{eq:n2i} & \verb+dinv.loss.SplittingLoss(eval_split_input=True)+ \\
Weighted-SSDU & **Liu et al., "A Weighted Splitting Loss Function for Self-Supervised Dynamic MRI Reconstruction"** & \cref{eq:millard} & \verb+dinv.loss.WeightedSplittingLoss()+ \\
Adversarial & **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"**__**Coleman et al., "Unsupervised image-to-image translation learning with adversarial training"** & \cref{eq:cole} & \verb+dinv.loss.UnsupAdversarialGeneratorLoss()+ \\
UAIR & **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"** & \cref{eq:uair} & \verb+dinv.loss.UAIRGeneratorLoss()+ \\
VORTEX & **Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"** & \cref{eq:vortex} & \verb+dinv.loss.VORTEXLoss()+ \\
EI & **Klatzer et al., "A Learned Dictionaries-based Approach to Compressed Sensing MRI"**__**Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"** & \cref{eq:ei} & \verb+dinv.loss.EILoss()+ \\
MOI & **Ying et al., "A Multi-Operator Imaging Approach to Compressed Sensing MRI"** & \cref{eq:moi} & \verb+dinv.loss.MOILoss()+ \\
MO-EI & Ours & \cref{eq:moei} & \verb+dinv.loss.MOEILoss()+ \\ \hline
\end{tabular}
\end{table*}

\subsubsection{Measurement consistency}
The most naive self-supervised loss simply compares the recorrupted estimate $\A\xhat$ to the measurement $\y$:

\begin{equation}
    \L_\text{MC}=\L(\A\xhat,\y)
    \label{eq:mc}
\end{equation}

where $\L$ is any metric such as the L2 norm. This loss originally appeared with a classical regularising term to solve inverse problems with deep learning and traditional regularisation, but these still faced problems in designing the regularisers. Without a regularising term, the loss cannot recover information from the null-space of the operator $\mathcal{N}(\A)$ as infinite solutions $\A^\top\y+\mathbf{v}$ can satisfy measurement consistency (MC) where $\mathbf{v}\in\mathcal{N}(\A)$. This loss function is also used in the presence of strong inductive bias where the regularisation is provided by the architecture, for example from specific architectures **Klatzer et al., "A Learned Dictionaries-based Approach to Compressed Sensing MRI"** or initialisations **Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"**, or in cycle consistency losses **Coleman et al., "Unsupervised image-to-image translation learning with adversarial training"**.

In the noisy scenario, some recent works such as ENSURE **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"** or GSURE **Klatzer et al., "A Learned Dictionaries-based Approach to Compressed Sensing MRI"** generalise Stein's Unbiased Risk Estimator (SURE) to reconstruct from noisy undersampled measurements. However, in the noiseless case considered here $\sigma\rightarrow0$, these methods simply reduce to $\L_\text{MC}$, and so will not be compared.

\subsubsection{Learning from multiple operators}
A popular set of state-of-the-art approaches leverages the fact that the whole measurement space is spanned by multiple imaging operators. Measurement splitting methods, including SSDU and similar papers **Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"**__**Schlemper et al., "DeepADMM: A Deep Neural Network for Accelerated Dynamic MRI Reconstruction"**, require the assumption that the image set is imaged with multiple operators, and that these operators span the full measurement domain in expectation.

\subsubsection{Adversarial losses}
Adversarial losses have been proposed to reconstruct MRI via the dual training of a generator $\ft$ and a discriminator $D$. While unconditional, generative adversarial networks such as AmbientGAN **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"** are not feedforward at inference time, **Coleman et al., "Unsupervised image-to-image translation learning with adversarial training"** propose a feedforward method with an adversarial loss for training:

\begin{equation}
    \L_\text{adversarial}=D(\tilde\A\ft(\y),\tilde\y)
    \label{eq:cole}
\end{equation}

where $\tilde\y\sim\mathcal{Y}$ i.e. another randomly sampled "real" measurement from the training dataset $\mathcal{Y}$. $D$'s aim is to distinguish between reconstructed measurements and training measurements, such that $\ft$'s aim is to reconstruct high quality measurements that are distinguishable from "real" measurements.

\subsubsection{Non-feedforward methods}

Finally, we do not consider ground-truth-free methods that require lengthy iterative procedures or optimisation at inference-time, since these are impractical in real-world application, including Deep Image Prior methods **Chen et al., "Deep Image Prior"**, generative models e.g. AmbientGAN **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"**, diffusion methods **Klatzer et al., "A Learned Dictionaries-based Approach to Compressed Sensing MRI"**, implicit neural representation methods **Hammernik et al., "Learning a variational network for accelerated dynamic MRI reconstruction"**, or self-supervised plug-and-play training methods such as **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"**.

\subsection{Other inverse problems}
\label{sec:other_problems}
\subsubsection{Noisy problems}
\label{sec:noisy_problems}

In this paper, we separate the task of solving ill-posed problems with removing measurement noise. To achieve the latter, it is often possible to add a denoising term to the loss function. For example, Robust EI **Klatzer et al., "A Learned Dictionaries-based Approach to Compressed Sensing MRI"** adds the Stein's Unbiased Risk Estimator (SURE) term, which is adapted by ENSURE **Mardani et al., "AmbientGAN: Adversarial Training for Ambient Lighting Estimation"**.

For a comparison of state-of-the-art denoising methods in solving inverse problems, see **Klatzer et al., "A Learned Dictionaries-based Approach to Compressed Sensing MRI"**.