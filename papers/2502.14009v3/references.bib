
@inproceedings{aali_ambient_2024,
	title = {Ambient {Diffusion} {Posterior} {Sampling}: {Solving} {Inverse} {Problems} with {Diffusion} {Models} {Trained} on {Corrupted} {Data}},
	shorttitle = {Ambient {Diffusion} {Posterior} {Sampling}},
	url = {https://openreview.net/forum?id=qeXcMutEZY},
	abstract = {We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Firstly, we extend the Ambient Diffusion framework to enable training directly from measurements corrupted in the Fourier domain. Subsequently, we train diffusion models for MRI with access only to Fourier subsampled multi-coil measurements at acceleration factors R\$=2, 4, 6, 8\$. Secondly, we propose \${\textbackslash}textit\{Ambient Diffusion Posterior Sampling\}\$ (A-DPS), a reconstruction algorithm that leverages generative models pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling on measurements from a different forward process (e.g. image blurring). For MRI reconstruction in high acceleration regimes, we observe that A-DPS models trained on subsampled data are better suited to solving inverse problems than models trained on fully sampled data. We also test the efficacy of A-DPS on natural image datasets (CelebA, FFHQ, and AFHQ) and show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance.},
	language = {en},
	urldate = {2025-03-10},
	author = {Aali, Asad and Daras, Giannis and Levac, Brett and Kumar, Sidharth and Dimakis, Alex and Tamir, Jon},
	month = oct,
	year = {2024},
}

@article{zhou_dual-domain_2022,
	title = {Dual-domain self-supervised learning for accelerated non-{Cartesian} {MRI} reconstruction},
	volume = {81},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841522001852},
	doi = {10.1016/j.media.2022.102538},
	abstract = {While enabling accelerated acquisition and improved reconstruction accuracy, current deep MRI reconstruction networks are typically supervised, require fully sampled data, and are limited to Cartesian sampling patterns. These factors limit their practical adoption as fully-sampled MRI is prohibitively time-consuming to acquire clinically. Further, non-Cartesian sampling patterns are particularly desirable as they are more amenable to acceleration and show improved motion robustness. To this end, we present a fully self-supervised approach for accelerated non-Cartesian MRI reconstruction which leverages self-supervision in both k-space and image domains. In training, the undersampled data are split into disjoint k-space domain partitions. For the k-space self-supervision, we train a network to reconstruct the input undersampled data from both the disjoint partitions and from itself. For the image-level self-supervision, we enforce appearance consistency obtained from the original undersampled data and the two partitions. Experimental results on our simulated multi-coil non-Cartesian MRI dataset demonstrate that DDSS can generate high-quality reconstruction that approaches the accuracy of the fully supervised reconstruction, outperforming previous baseline methods. Finally, DDSS is shown to scale to highly challenging real-world clinical MRI reconstruction acquired on a portable low-field (0.064 T) MRI scanner with no data available for supervised training while demonstrating improved image quality as compared to traditional reconstruction, as determined by a radiologist study.},
	urldate = {2025-02-22},
	journal = {Medical Image Analysis},
	author = {Zhou, Bo and Schlemper, Jo and Dey, Neel and Mohseni Salehi, Seyed Sadegh and Sheth, Kevin and Liu, Chi and Duncan, James S. and Sofka, Michal},
	month = oct,
	year = {2022},
	keywords = {Accelerated MRI, Dual-domain learning, Low-field portable MRI, Non-Cartesian MRI, Self-supervised learning},
	pages = {102538},
}

@inproceedings{hu_self-supervised_2021,
	address = {Cham},
	title = {Self-supervised {Learning} for {MRI} {Reconstruction} with a {Parallel} {Network} {Training} {Framework}},
	isbn = {978-3-030-87231-1},
	doi = {10.1007/978-3-030-87231-1_37},
	abstract = {Image reconstruction from undersampled k-space data plays an important role in accelerating the acquisition of MR data, and a lot of deep learning-based methods have been exploited recently. Despite the achieved inspiring results, the optimization of these methods commonly relies on the fully-sampled reference data, which are time-consuming and difficult to collect. To address this issue, we propose a novel self-supervised learning method. Specifically, during model optimization, two subsets are constructed by randomly selecting part of k-space data from the undersampled data and then fed into two parallel reconstruction networks to perform information recovery. Two reconstruction losses are defined on all the scanned data points to enhance the network’s capability of recovering the frequency information. Meanwhile, to constrain the learned unscanned data points of the network, a difference loss is designed to enforce consistency between the two parallel networks. In this way, the reconstruction model can be properly trained with only the undersampled data. During the model evaluation, the undersampled data are treated as the inputs and either of the two trained networks is expected to reconstruct the high-quality results. The proposed method is flexible and can be employed in any existing deep learning-based method. The effectiveness of the method is evaluated on an open brain MRI dataset. Experimental results demonstrate that the proposed self-supervised method can achieve competitive reconstruction performance compared to the corresponding supervised learning method at high acceleration rates (4 and 8). The code is publicly available at https://github.com/chenhu96/Self-Supervised-MRI-Reconstruction.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2021},
	publisher = {Springer International Publishing},
	author = {Hu, Chen and Li, Cheng and Wang, Haifeng and Liu, Qiegen and Zheng, Hairong and Wang, Shanshan},
	editor = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
	year = {2021},
	keywords = {Deep learning, Image reconstruction, Parallel network, Self-supervised learning},
	pages = {382--391},
}

@article{huang_self-supervised_2024,
	title = {Self-{Supervised} {Deep} {Unrolled} {Reconstruction} {Using} {Regularization} by {Denoising}},
	volume = {43},
	issn = {1558-254X},
	url = {https://ieeexplore.ieee.org/document/10318101},
	doi = {10.1109/TMI.2023.3332614},
	abstract = {Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable self-supervised learning for MR image reconstruction by combining a self-supervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of Noise2Noise in MR reconstruction by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality among the state-of-the-art approaches utilizing Noise2Noise.},
	number = {3},
	urldate = {2025-02-20},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Huang, Peizhou and Zhang, Chaoyi and Zhang, Xiaoliang and Li, Xiaojuan and Dong, Liang and Ying, Leslie},
	month = mar,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Image reconstruction, Imaging, Iterative methods, Magnetic resonance image reconstruction, Magnetic resonance imaging, Noise reduction, Training, Training data, deep neural network, regularization by denoising, self-supervised},
	pages = {1203--1213},
}

@misc{metzler_unsupervised_2020,
	title = {Unsupervised {Learning} with {Stein}'s {Unbiased} {Risk} {Estimator}},
	url = {http://arxiv.org/abs/1805.10531},
	doi = {10.48550/arXiv.1805.10531},
	abstract = {Learning from unlabeled and noisy data is one of the grand challenges of machine learning. As such, it has seen a flurry of research with new ideas proposed continuously. In this work, we revisit a classical idea: Stein's Unbiased Risk Estimator (SURE). We show that, in the context of image recovery, SURE and its generalizations can be used to train convolutional neural networks (CNNs) for a range of image denoising and recovery problems without any ground truth data. Specifically, our goal is to reconstruct an image \$x\$ from a noisy linear transformation (measurement) of the image. We consider two scenarios: one where no additional data is available and one where we have measurements of other images that are drawn from the same noisy distribution as \$x\$, but have no access to the clean images. Such is the case, for instance, in the context of medical imaging, microscopy, and astronomy, where noise-less ground truth data is rarely available. We show that in this situation, SURE can be used to estimate the mean-squared-error loss associated with an estimate of \$x\$. Using this estimate of the loss, we train networks to perform denoising and compressed sensing recovery. In addition, we also use the SURE framework to partially explain and improve upon an intriguing results presented by Ulyanov et al. in "Deep Image Prior": that a network initialized with random weights and fit to a single noisy image can effectively denoise that image. Public implementations of the networks and methods described in this paper can be found at https://github.com/ricedsp/D-AMP\_Toolbox.},
	urldate = {2025-02-20},
	publisher = {arXiv},
	author = {Metzler, Christopher A. and Mousavi, Ali and Heckel, Reinhard and Baraniuk, Richard G.},
	month = jul,
	year = {2020},
	note = {arXiv:1805.10531 [stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{aggarwal_ensure_2023,
	title = {{ENSURE}: {A} {General} {Approach} for {Unsupervised} {Training} of {Deep} {Image} {Reconstruction} {Algorithms}},
	volume = {42},
	issn = {1558-254X},
	shorttitle = {{ENSURE}},
	url = {https://ieeexplore.ieee.org/document/9961145},
	doi = {10.1109/TMI.2022.3224359},
	abstract = {Image reconstruction using deep learning algorithms offers improved reconstruction quality and lower reconstruction time than classical compressed sensing and model-based algorithms. Unfortunately, clean and fully sampled ground-truth data to train the deep networks is often unavailable in several applications, restricting the applicability of the above methods. We introduce a novel metric termed the ENsemble Stein’s Unbiased Risk Estimate (ENSURE) framework, which can be used to train deep image reconstruction algorithms without fully sampled and noise-free images. The proposed framework is the generalization of the classical SURE and GSURE formulation to the setting where the images are sampled by different measurement operators, chosen randomly from a set. We evaluate the expectation of the GSURE loss functions over the sampling patterns to obtain the ENSURE loss function. We show that this loss is an unbiased estimate for the true mean-square error, which offers a better alternative to GSURE, which only offers an unbiased estimate for the projected error. Our experiments show that the networks trained with this loss function can offer reconstructions comparable to the supervised setting. While we demonstrate this framework in the context of MR image recovery, the ENSURE framework is generally applicable to arbitrary inverse problems.},
	number = {4},
	urldate = {2025-02-20},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Aggarwal, Hemant Kumar and Pramanik, Aniket and John, Maneesh and Jacob, Mathews},
	month = apr,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Image reconstruction, Loss measurement, MRI, Magnetic resonance imaging, Measurement, Noise measurement, SURE, Training, Unsupervised learning, Weight measurement, deep learning, inverse problems},
	pages = {1133--1144},
}

@article{shafique_mri_2024,
	title = {{MRI} {Recovery} with {Self}-{Calibrated} {Denoisers} without {Fully}-{Sampled} {Data}},
	volume = {38},
	issn = {1352-8661},
	url = {http://arxiv.org/abs/2304.12890},
	doi = {10.1007/s10334-024-01207-1},
	abstract = {Objective: Acquiring fully sampled training data is challenging for many MRI applications. We present a self-supervised image reconstruction method, termed ReSiDe, capable of recovering images solely from undersampled data. Materials and Methods: ReSiDe is inspired by plug-and-play (PnP) methods, but unlike traditional PnP approaches that utilize pre-trained denoisers, ReSiDe iteratively trains the denoiser on the image or images that are being reconstructed. We introduce two variations of our method: ReSiDe-S and ReSiDe-M. ReSiDe-S is scan-specific and works with a single set of undersampled measurements, while ReSiDe-M operates on multiple sets of undersampled measurements and provides faster inference. Studies I, II, and III compare ReSiDe-S and ReSiDe-M against other self-supervised or unsupervised methods using data from T1- and T2-weighted brain MRI, MRXCAT digital perfusion phantom, and first-pass cardiac perfusion, respectively. Results: ReSiDe-S and ReSiDe-M outperform other methods in terms of peak signal-to-noise ratio and structural similarity index measure for Studies I and II, and in terms of expert scoring for Study III. Discussion: We present a self-supervised image reconstruction method and validate it in both static and dynamic MRI applications. These developments can benefit MRI applications where the availability of fully sampled training data is limited.},
	number = {1},
	urldate = {2025-02-20},
	journal = {Magnetic Resonance Materials in Physics, Biology and Medicine},
	author = {Shafique, Muhammad and Liu, Sizhuo and Schniter, Philip and Ahmad, Rizwan},
	month = oct,
	year = {2024},
	note = {arXiv:2304.12890 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {53--66},
}

@article{shen_nerp_2024,
	title = {{NeRP}: {Implicit} {Neural} {Representation} {Learning} {With} {Prior} {Embedding} for {Sparsely} {Sampled} {Image} {Reconstruction}},
	volume = {35},
	issn = {2162-2388},
	shorttitle = {{NeRP}},
	url = {https://ieeexplore.ieee.org/document/9788018},
	doi = {10.1109/TNNLS.2022.3177134},
	abstract = {Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses additional challenges due to limited measurements. In this work, we propose a methodology of implicit Neural Representation learning with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression.},
	number = {1},
	urldate = {2025-02-19},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Shen, Liyue and Pauly, John and Xing, Lei},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Computed tomography, Deep learning, Image reconstruction, Imaging, Implicit neural representation, Magnetic resonance imaging, Neural networks, Training, inverse problem, prior embedding, sparsely sampled image reconstruction},
	pages = {770--782},
}

@article{aggarwal_modl_2019,
	title = {{MoDL}: {Model}-{Based} {Deep} {Learning} {Architecture} for {Inverse} {Problems}},
	volume = {38},
	issn = {1558-254X},
	shorttitle = {{MoDL}},
	url = {https://ieeexplore.ieee.org/document/8434321},
	doi = {10.1109/TMI.2018.2865356},
	abstract = {We introduce a model-based image reconstruction framework with a convolution neural network (CNN)-based regularization prior. The proposed formulation provides a systematic approach for deriving deep architectures for inverse problems with the arbitrary structure. Since the forward model is explicitly accounted for, a smaller network with fewer parameters is sufficient to capture the image information compared to direct inversion approaches. Thus, reducing the demand for training data and training time. Since we rely on end-to-end training with weight sharing across iterations, the CNN weights are customized to the forward model, thus offering improved performance over approaches that rely on pre-trained denoisers. Our experiments show that the decoupling of the number of iterations from the network complexity offered by this approach provides benefits, including lower demand for training data, reduced risk of overfitting, and implementations with significantly reduced memory footprint. We propose to enforce data-consistency by using numerical optimization blocks, such as conjugate gradients algorithm within the network. This approach offers faster convergence per iteration, compared to methods that rely on proximal gradients steps to enforce data consistency. Our experiments show that the faster convergence translates to improved performance, primarily when the available GPU memory restricts the number of iterations.},
	number = {2},
	urldate = {2025-02-19},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Aggarwal, Hemant K. and Mani, Merry P. and Jacob, Mathews},
	month = feb,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Deep learning, Image reconstruction, Imaging, Machine learning, Numerical models, Optimization, Training, Training data, convolutional neural network, parallel imaging},
	pages = {394--405},
}

@misc{darestani_accelerated_2021,
	title = {Accelerated {MRI} with {Un}-trained {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.02471},
	doi = {10.48550/arXiv.2007.02471},
	abstract = {Convolutional Neural Networks (CNNs) are highly effective for image reconstruction problems. Typically, CNNs are trained on large amounts of training images. Recently, however, un-trained CNNs such as the Deep Image Prior and Deep Decoder have achieved excellent performance for image reconstruction problems such as denoising and inpainting, {\textbackslash}emph\{without using any training data\}. Motivated by this development, we address the reconstruction problem arising in accelerated MRI with un-trained neural networks. We propose a highly optimized un-trained recovery approach based on a variation of the Deep Decoder and show that it significantly outperforms other un-trained methods, in particular sparsity-based classical compressed sensing methods and naive applications of un-trained neural networks. We also compare performance (both in terms of reconstruction accuracy and computational cost) in an ideal setup for trained methods, specifically on the fastMRI dataset, where the training and test data come from the same distribution. We find that our un-trained algorithm achieves similar performance to a baseline trained neural network, but a state-of-the-art trained network outperforms the un-trained one. Finally, we perform a comparison on a non-ideal setup where the train and test distributions are slightly different, and find that our un-trained method achieves similar performance to a state-of-the-art accelerated MRI reconstruction method.},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Darestani, Mohammad Zalbagi and Heckel, Reinhard},
	month = apr,
	year = {2021},
	note = {arXiv:2007.02471 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
}

@misc{millard_clean_2024,
	title = {Clean self-supervised {MRI} reconstruction from noisy, sub-sampled training data with {Robust} {SSDU}},
	url = {http://arxiv.org/abs/2210.01696},
	doi = {10.48550/arXiv.2210.01696},
	abstract = {Most existing methods for Magnetic Resonance Imaging (MRI) reconstruction with deep learning use fully supervised training, which assumes that a high signal-to-noise ratio (SNR), fully sampled dataset is available for training. In many circumstances, however, such a dataset is highly impractical or even technically infeasible to acquire. Recently, a number of self-supervised methods for MR reconstruction have been proposed, which use sub-sampled data only. However, the majority of such methods, such as Self-Supervised Learning via Data Undersampling (SSDU), are susceptible to reconstruction errors arising from noise in the measured data. In response, we propose Robust SSDU, which provably recovers clean images from noisy, sub-sampled training data by simultaneously estimating missing k-space samples and denoising the available samples. Robust SSDU trains the reconstruction network to map from a further noisy and sub-sampled version of the data to the original, singly noisy and sub-sampled data, and applies an additive Noisier2Noise correction term at inference. We also present a related method, Noiser2Full, that recovers clean images when noisy, fully sampled data is available for training. Both proposed methods are applicable to any network architecture, straight-forward to implement and have similar computational cost to standard training. We evaluate our methods on the multi-coil fastMRI brain dataset with a novel denoising-specific architecture and find that it performs competitively with a benchmark trained on clean, fully sampled data.},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Millard, Charles and Chiew, Mark},
	month = jun,
	year = {2024},
	note = {arXiv:2210.01696 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{pang_recorrupted--recorrupted_2021,
	title = {Recorrupted-to-{Recorrupted}: {Unsupervised} {Deep} {Learning} for {Image} {Denoising}},
	shorttitle = {Recorrupted-to-{Recorrupted}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Pang_Recorrupted-to-Recorrupted_Unsupervised_Deep_Learning_for_Image_Denoising_CVPR_2021_paper.html},
	language = {en},
	urldate = {2025-02-17},
	author = {Pang, Tongyao and Zheng, Huan and Quan, Yuhui and Ji, Hui},
	year = {2021},
	pages = {2043--2052},
}

@misc{tachella_unsure_2025,
	title = {{UNSURE}: self-supervised learning with {Unknown} {Noise} level and {Stein}'s {Unbiased} {Risk} {Estimate}},
	shorttitle = {{UNSURE}},
	url = {http://arxiv.org/abs/2409.01985},
	doi = {10.48550/arXiv.2409.01985},
	abstract = {Recently, many self-supervised learning methods for image reconstruction have been proposed that can learn from noisy data alone, bypassing the need for ground-truth references. Most existing methods cluster around two classes: i) Stein's Unbiased Risk Estimate (SURE) and similar approaches that assume full knowledge of the noise distribution, and ii) Noise2Self and similar cross-validation methods that require very mild knowledge about the noise distribution. The first class of methods tends to be impractical, as the noise level is often unknown in real-world applications, and the second class is often suboptimal compared to supervised learning. In this paper, we provide a theoretical framework that characterizes this expressivity-robustness trade-off and propose a new approach based on SURE, but unlike the standard SURE, does not require knowledge about the noise level. Throughout a series of experiments, we show that the proposed estimator outperforms other existing self-supervised methods on various imaging inverse problems.},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Tachella, Julián and Davies, Mike and Jacques, Laurent},
	month = feb,
	year = {2025},
	note = {arXiv:2409.01985 [stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@article{ongie_deep_2020,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	volume = {1},
	issn = {2641-8770},
	url = {https://ieeexplore.ieee.org/document/9084378},
	doi = {10.1109/JSAIT.2020.2991563},
	abstract = {Recent work in machine learning shows that deep neural networks can be used to solve a wide variety of inverse problems arising in computational imaging. We explore the central prevailing themes of this emerging area and present a taxonomy that can be used to categorize different problems and reconstruction methods. Our taxonomy is organized along two central axes: (1) whether or not a forward model is known and to what extent it is used in training and testing, and (2) whether or not the learning is supervised or unsupervised, i.e., whether or not the training relies on access to matched ground truth image and measurement pairs. We also discuss the tradeoffs associated with these different reconstruction approaches, caveats and common failure modes, plus open problems and avenues for future work.},
	number = {1},
	urldate = {2025-02-17},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	month = may,
	year = {2020},
	note = {Conference Name: IEEE Journal on Selected Areas in Information Theory},
	keywords = {Computational modeling, Deep learning, Image reconstruction, Imaging, Interpolation, Inverse problems, Machine learning, Training, computational imaging, deep neural networks, image reconstruction, image restoration, inverse problems},
	pages = {39--56},
}

@article{zeng_review_2021,
	title = {A review on deep learning {MRI} reconstruction without fully sampled k-space},
	volume = {21},
	issn = {1471-2342},
	url = {https://doi.org/10.1186/s12880-021-00727-9},
	doi = {10.1186/s12880-021-00727-9},
	abstract = {Magnetic resonance imaging (MRI) is an effective auxiliary diagnostic method in clinical medicine, but it has always suffered from the problem of long acquisition time. Compressed sensing and parallel imaging are two common techniques to accelerate MRI reconstruction. Recently, deep learning provides a new direction for MRI, while most of them require a large number of data pairs for training. However, there are many scenarios where fully sampled k-space data cannot be obtained, which will seriously hinder the application of supervised learning. Therefore, deep learning without fully sampled data is indispensable.},
	number = {1},
	urldate = {2025-02-17},
	journal = {BMC Medical Imaging},
	author = {Zeng, Gushan and Guo, Yi and Zhan, Jiaying and Wang, Zi and Lai, Zongying and Du, Xiaofeng and Qu, Xiaobo and Guo, Di},
	month = dec,
	year = {2021},
	pages = {195},
}

@inproceedings{acar_self-supervised_2021,
	title = {Self-supervised {Dynamic} {MRI} {Reconstruction}},
	doi = {10.1007/978-3-030-88552-6_4},
	abstract = {Deep learning techniques have recently been adopted for accelerating dynamic MRI acquisitions. Yet, common frameworks for model training rely on availability of large sets of fully-sampled MRI data to construct a ground-truth for the network output. This heavy reliance is undesirable as it is challenging to collect such large datasets in many applications, and even impossible for high spatiotemporal-resolution protocols. In this paper, we introduce self-supervised training to deep neural architectures for dynamic reconstruction of cardiac MRI. We hypothesize that, in the absence of ground-truth data, elevating complexity in self-supervised models can instead constrain model performance due to the deficiencies in training data. To test this working hypothesis, we adopt self-supervised learning on recent state-of-the-art deep models for dynamic MRI, with varying degrees of model complexity. Comparison of supervised and self-supervised variants of deep reconstruction models reveals that compact models have a remarkable advantage in reliability against performance loss in self-supervised settings.},
	booktitle = {Machine {Learning} for {Medical} {Image} {Reconstruction}},
	publisher = {Springer International Publishing},
	author = {Acar, Mert and Çukur, Tolga and Öksüz, Ilkay},
	year = {2021},
	keywords = {Cardiac MRI, Convolutional Neural Networks, Dynamic reconstruction, Self-supervised learning},
	pages = {35--44},
}

@misc{tachella_deepinverse_2023,
	title = {{DeepInverse}: {A} deep learning framework for inverse problems in imaging},
	url = {https://github.com/deepinv/deepinv},
	author = {Tachella, Julian and Chen, Dongdong and Hurault, Samuel and Terris, Matthieu and Wang, Andrew},
	month = jun,
	year = {2023},
	doi = {10.5281/zenodo.7982256},
}

@misc{noauthor_notitle_nodate,
}

@inproceedings{cole_fast_2021,
	title = {Fast {Unsupervised} {MRI} {Reconstruction} {Without} {Fully}-{Sampled} {Ground} {Truth} {Data} {Using} {Generative} {Adversarial} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9607486},
	doi = {10.1109/ICCVW54120.2021.00444},
	abstract = {Most deep learning (DL) magnetic resonance imaging (MRI) reconstruction approaches rely on supervised training algorithms, which require access to high-quality, fully-sampled ground truth datasets. In MRI, acquiring fully-sampled data is time-consuming, expensive, and, in some cases, impossible due to limitations on data acquisition speed. We present a DL framework for MRI reconstruction that does not require any fully-sampled data using unsupervised generative adversarial networks. We test our proposed method on 2D knee MRI data and 2D+time abdominal dynamic contrast enhanced (DCE) MRI data. In the DCE-MRI dataset, as is the case with many dynamic MRI sequences, ground truth was not possible to acquire and therefore, supervised DL reconstruction was not feasible. We show that our unsupervised method produces reconstructions which are better than compressed sensing in terms of image metrics and the recovery of anatomical structure, with faster inference time. In contrast to most deep learning reconstruction techniques, which are supervised, this method does not need any fully-sampled data. With the proposed method, accelerated imaging and accurate reconstruction can be performed in applications in cases where fully-sampled datasets are difficult to obtain or unavailable.},
	urldate = {2025-02-13},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Cole, Elizabeth K. and Ong, Frank and Vasanawala, Shreyas S. and Pauly, John M.},
	month = oct,
	year = {2021},
	note = {ISSN: 2473-9944},
	keywords = {Computer vision, Conferences, Data acquisition, Deep learning, Magnetic resonance imaging, Measurement, Training},
	pages = {3971--3980},
}

@article{oh_unpaired_2020,
	title = {Unpaired {Deep} {Learning} for {Accelerated} {MRI} {Using} {Optimal} {Transport} {Driven} {CycleGAN}},
	volume = {6},
	issn = {2333-9403},
	url = {https://ieeexplore.ieee.org/document/9173689},
	doi = {10.1109/TCI.2020.3018562},
	abstract = {Recently, deep learning approaches for accelerated MRI have been extensively studied thanks to their high performance reconstruction in spite of significantly reduced run-time complexity. These neural networks are usually trained in a supervised manner, so matched pairs of subsampled, and fully sampled k-space data are required. Unfortunately, it is often difficult to acquire matched fully sampled k-space data, since the acquisition of fully sampled k-space data requires long scan time, and often leads to the change of the acquisition protocol. Therefore, unpaired deep learning without matched label data has become a very important research topic. In this article, we propose an unpaired deep learning approach using a optimal transport driven cycle-consistent generative adversarial network (OT-cycleGAN) that employs a single pair of generator, and discriminator. The proposed OT-cycleGAN architecture is rigorously derived from a dual formulation of the optimal transport formulation using a specially designed penalized least squares cost. The experimental results show that our method can reconstruct high resolution MR images from accelerated k-space data from both single, and multiple coil acquisition, without requiring matched reference data.},
	urldate = {2025-02-13},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Oh, Gyutaek and Sim, Byeongsu and Chung, HyungJin and Sunwoo, Leonard and Ye, Jong Chul},
	year = {2020},
	note = {Conference Name: IEEE Transactions on Computational Imaging},
	keywords = {Accelerated MRI, Acceleration, Deep learning, Gallium nitride, Generative adversarial networks, Image reconstruction, Magnetic resonance imaging, cycleGAN, optimal transport, penalized least squares (PLS), unpaired deep learning},
	pages = {1285--1296},
}

@misc{hu_spicer_2024,
	title = {{SPICER}: {Self}-{Supervised} {Learning} for {MRI} with {Automatic} {Coil} {Sensitivity} {Estimation} and {Reconstruction}},
	shorttitle = {{SPICER}},
	url = {http://arxiv.org/abs/2210.02584},
	doi = {10.48550/arXiv.2210.02584},
	abstract = {Deep model-based architectures (DMBAs) integrating physical measurement models and learned image regularizers are widely used in parallel magnetic resonance imaging (PMRI). Traditional DMBAs for PMRI rely on pre-estimated coil sensitivity maps (CSMs) as a component of the measurement model. However, estimation of accurate CSMs is a challenging problem when measurements are highly undersampled. Additionally, traditional training of DMBAs requires high-quality groundtruth images, limiting their use in applications where groundtruth is difficult to obtain. This paper addresses these issues by presenting SPICE as a new method that integrates self-supervised learning and automatic coil sensitivity estimation. Instead of using pre-estimated CSMs, SPICE simultaneously reconstructs accurate MR images and estimates high-quality CSMs. SPICE also enables learning from undersampled noisy measurements without any groundtruth. We validate SPICE on experimentally collected data, showing that it can achieve state-of-the-art performance in highly accelerated data acquisition settings (up to 10x).},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Hu, Yuyang and Gan, Weijie and Ying, Chunwei and Wang, Tongyao and Eldeniz, Cihat and Liu, Jiaming and Chen, Yasheng and An, Hongyu and Kamilov, Ulugbek S.},
	month = jun,
	year = {2024},
	note = {arXiv:2210.02584 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{gan_deep_2020,
	title = {Deep {Image} {Reconstruction} using {Unregistered} {Measurements} without {Groundtruth}},
	url = {http://arxiv.org/abs/2009.13986},
	doi = {10.48550/arXiv.2009.13986},
	abstract = {One of the key limitations in conventional deep learning based image reconstruction is the need for registered pairs of training images containing a set of high-quality groundtruth images. This paper addresses this limitation by proposing a novel unsupervised deep registration-augmented reconstruction method (U-Dream) for training deep neural nets to reconstruct high-quality images by directly mapping pairs of unregistered and artifact-corrupted images. The ability of U-Dream to circumvent the need for accurately registered data makes it widely applicable to many biomedical image reconstruction tasks. We validate it in accelerated magnetic resonance imaging (MRI) by training an image reconstruction model directly on pairs of undersampled measurements from images that have undergone nonrigid deformations.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Gan, Weijie and Sun, Yu and Eldeniz, Cihat and Liu, Jiaming and An, Hongyu and Kamilov, Ulugbek S.},
	month = sep,
	year = {2020},
	note = {arXiv:2009.13986 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{wang_fully_2024,
	title = {Fully {Unsupervised} {Dynamic} {MRI} {Reconstruction} via {Diffeo}-{Temporal} {Equivariance}},
	url = {http://arxiv.org/abs/2410.08646},
	doi = {10.48550/arXiv.2410.08646},
	abstract = {Reconstructing dynamic MRI image sequences from undersampled accelerated measurements is crucial for faster and higher spatiotemporal resolution real-time imaging of cardiac motion, free breathing motion and many other applications. Classical paradigms, such as gated cine MRI, assume periodicity, disallowing imaging of true motion. Supervised deep learning methods are fundamentally flawed as, in dynamic imaging, ground truth fully-sampled videos are impossible to truly obtain. We propose an unsupervised framework to learn to reconstruct dynamic MRI sequences from undersampled measurements alone by leveraging natural geometric spatiotemporal equivariances of MRI. Dynamic Diffeomorphic Equivariant Imaging (DDEI) significantly outperforms state-of-the-art unsupervised methods such as SSDU on highly accelerated dynamic cardiac imaging. Our method is agnostic to the underlying neural network architecture and can be used to adapt the latest models and post-processing approaches. Our code and video demos are at https://github.com/Andrewwango/ddei.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Wang, Andrew and Davies, Mike},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08646 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{kawar_gsure-based_2024,
	title = {{GSURE}-{Based} {Diffusion} {Model} {Training} with {Corrupted} {Data}},
	url = {http://arxiv.org/abs/2305.13128},
	doi = {10.48550/arXiv.2305.13128},
	abstract = {Diffusion models have demonstrated impressive results in both data generation and downstream tasks such as inverse problems, text-based editing, classification, and more. However, training such models usually requires large amounts of clean signals which are often difficult or impossible to obtain. In this work, we propose a novel training technique for generative diffusion models based only on corrupted data. We introduce a loss function based on the Generalized Stein's Unbiased Risk Estimator (GSURE), and prove that under some conditions, it is equivalent to the training objective used in fully supervised diffusion models. We demonstrate our technique on face images as well as Magnetic Resonance Imaging (MRI), where the use of undersampled data significantly alleviates data collection costs. Our approach achieves generative performance comparable to its fully supervised counterpart without training on any clean signals. In addition, we deploy the resulting diffusion model in various downstream tasks beyond the degradation present in the training set, showcasing promising results.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Kawar, Bahjat and Elata, Noam and Michaeli, Tomer and Elad, Michael},
	month = jun,
	year = {2024},
	note = {arXiv:2305.13128 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{desai_vortex_2022,
	title = {{VORTEX}: {Physics}-{Driven} {Data} {Augmentations} {Using} {Consistency} {Training} for {Robust} {Accelerated} {MRI} {Reconstruction}},
	shorttitle = {{VORTEX}},
	url = {http://arxiv.org/abs/2111.02549},
	doi = {10.48550/arXiv.2111.02549},
	abstract = {Deep neural networks have enabled improved image quality and fast inference times for various inverse problems, including accelerated magnetic resonance imaging (MRI) reconstruction. However, such models require a large number of fully-sampled ground truth datasets, which are difficult to curate, and are sensitive to distribution drifts. In this work, we propose applying physics-driven data augmentations for consistency training that leverage our domain knowledge of the forward MRI data acquisition process and MRI physics to achieve improved label efficiency and robustness to clinically-relevant distribution drifts. Our approach, termed VORTEX, (1) demonstrates strong improvements over supervised baselines with and without data augmentation in robustness to signal-to-noise ratio change and motion corruption in data-limited regimes; (2) considerably outperforms state-of-the-art purely image-based data augmentation techniques and self-supervised reconstruction methods on both in-distribution and out-of-distribution data; and (3) enables composing heterogeneous image-based and physics-driven data augmentations. Our code is available at https://github.com/ad12/meddlr.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Desai, Arjun D. and Gunel, Beliz and Ozturkler, Batu M. and Beg, Harris and Vasanawala, Shreyas and Hargreaves, Brian A. and Ré, Christopher and Pauly, John M. and Chaudhari, Akshay S.},
	month = jun,
	year = {2022},
	note = {arXiv:2111.02549 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Medical Physics},
}

@misc{desai_noise2recon_2022,
	title = {{Noise2Recon}: {Enabling} {Joint} {MRI} {Reconstruction} and {Denoising} with {Semi}-{Supervised} and {Self}-{Supervised} {Learning}},
	shorttitle = {{Noise2Recon}},
	url = {http://arxiv.org/abs/2110.00075},
	doi = {10.48550/arXiv.2110.00075},
	abstract = {Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, supervised DL methods depend on extensive amounts of fully-sampled (labeled) data and are sensitive to out-of-distribution (OOD) shifts, particularly low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose Noise2Recon, a model-agnostic, consistency training method for joint MRI reconstruction and denoising that can use both fully-sampled (labeled) and undersampled (unlabeled) scans in semi-supervised and self-supervised settings. With limited or no labeled training data, Noise2Recon outperforms compressed sensing and deep learning baselines, including supervised networks, augmentation-based training, fine-tuned denoisers, and self-supervised methods, and matches performance of supervised models, which were trained with 14x more fully-sampled scans. Noise2Recon also outperforms all baselines, including state-of-the-art fine-tuning and augmentation techniques, among low-SNR scans and when generalizing to other OOD factors, such as changes in acceleration factors and different datasets. Augmentation extent and loss weighting hyperparameters had negligible impact on Noise2Recon compared to supervised methods, which may indicate increased training stability. Our code is available at https://github.com/ad12/meddlr.},
	urldate = {2025-02-13},
	publisher = {arXiv},
	author = {Desai, Arjun D. and Ozturkler, Batu M. and Sandino, Christopher M. and Boutin, Robert and Willis, Marc and Vasanawala, Shreyas and Hargreaves, Brian A. and Ré, Christopher M. and Pauly, John M. and Chaudhari, Akshay S.},
	month = oct,
	year = {2022},
	note = {arXiv:2110.00075 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{darestani_test-time_2022,
	title = {Test-{Time} {Training} {Can} {Close} the {Natural} {Distribution} {Shift} {Performance} {Gap} in {Deep} {Learning} {Based} {Compressed} {Sensing}},
	url = {https://proceedings.mlr.press/v162/darestani22a.html},
	abstract = {Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution PPP and training on another distribution QQQ, and evaluating both models on QQQ. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.},
	language = {en},
	urldate = {2025-02-13},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Darestani, Mohammad Zalbagi and Liu, Jiayu and Heckel, Reinhard},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {4754--4776},
}

@inproceedings{sen_hyp-nerf_2023,
	title = {{HyP}-{NeRF}: {Learning} {Improved} {NeRF} {Priors} using a {HyperNetwork}},
	shorttitle = {{HyP}-{NeRF}},
	url = {https://openreview.net/forum?id=BExDjNDYkN},
	abstract = {Neural Radiance Fields (NeRF) have become an increasingly popular representation to capture high-quality appearance and shape of scenes and objects. However, learning generalizable NeRF priors over categories of scenes or objects has been challenging due to the high dimensionality of network weight space. To address the limitations of existing work on generalization, multi-view consistency and to improve quality, we propose HyP-NeRF, a latent conditioning method for learning generalizable category-level NeRF priors using hypernetworks. Rather than using hypernetworks to estimate only the weights of a NeRF, we estimate both the weights and the multi-resolution hash encodings resulting in significant quality gains. To improve quality even further, we incorporate a denoise and finetune strategy that denoises images rendered from NeRFs estimated by the hypernetwork and finetunes it while retaining multiview consistency. These improvements enable us to use HyP-NeRF as a generalizable prior for multiple downstream tasks including NeRF reconstruction from single-view or cluttered scenes and text-to-NeRF. We provide qualitative comparisons and evaluate HyP-NeRF on three tasks: generalization, compression, and retrieval, demonstrating our state-of-the-art results.},
	language = {en},
	urldate = {2025-01-16},
	author = {Sen, Bipasha and Singh, Gaurav and Agarwal, Aditya and Agaram, Rohith and Krishna, Madhava and Sridhar, Srinath},
	month = nov,
	year = {2023},
}

@inproceedings{kalogeropoulos_scale_2024,
	title = {Scale {Equivariant} {Graph} {Metanetworks}},
	url = {https://openreview.net/forum?id=8Fxqn1tZM1},
	abstract = {This paper pertains to an emerging machine learning paradigm: learning higher- order functions, i.e. functions whose inputs are functions themselves, particularly when these inputs are Neural Networks (NNs). With the growing interest in architectures that process NNs, a recurring design principle has permeated the field: adhering to the permutation symmetries arising from the connectionist structure of NNs. However, are these the sole symmetries present in NN parameterizations? Zooming into most practical activation functions (e.g. sine, ReLU, tanh) answers this question negatively and gives rise to intriguing new symmetries, which we collectively refer to as scaling symmetries, that is, non-zero scalar multiplications and divisions of weights and biases. In this work, we propose Scale Equivariant Graph MetaNetworks - ScaleGMNs, a framework that adapts the Graph Metanetwork (message-passing) paradigm by incorporating scaling symmetries and thus rendering neuron and edge representations equivariant to valid scalings. We introduce novel building blocks, of independent technical interest, that allow for equivariance or invariance with respect to individual scalar multipliers or their product and use them in all components of ScaleGMN. Furthermore, we prove that, under certain expressivity conditions, ScaleGMN can simulate the forward and backward pass of any input feedforward neural network. Experimental results demonstrate that our method advances the state-of-the-art performance for several datasets and activation functions, highlighting the power of scaling symmetries as an inductive bias for NN processing. The source code is publicly available at https://github.com/jkalogero/scalegmn.},
	language = {en},
	urldate = {2025-01-16},
	author = {Kalogeropoulos, Ioannis and Bouritsas, Giorgos and Panagakis, Yannis},
	month = nov,
	year = {2024},
}

@misc{wang_neural_2024,
	title = {Neural {Network} {Diffusion}},
	url = {http://arxiv.org/abs/2402.13144},
	doi = {10.48550/arXiv.2402.13144},
	abstract = {Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also {\textbackslash}textit\{generate high-performing neural network parameters\}. Our approach is simple, utilizing an autoencoder and a diffusion model. The autoencoder extracts latent representations of a subset of the trained neural network parameters. Next, a diffusion model is trained to synthesize these latent representations from random noise. This model then generates new representations, which are passed through the autoencoder's decoder to produce new subsets of high-performing network parameters. Across various architectures and datasets, our approach consistently generates models with comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models are not memorizing the trained ones. Our results encourage more exploration into the versatile use of diffusion models. Our code is available {\textbackslash}href\{https://github.com/NUS-HPC-AI-Lab/Neural-Network-Diffusion\}\{here\}.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Wang, Kai and Tang, Dongwen and Zeng, Boya and Yin, Yida and Xu, Zhaopan and Zhou, Yukun and Zang, Zelin and Darrell, Trevor and Liu, Zhuang and You, Yang},
	month = dec,
	year = {2024},
	note = {arXiv:2402.13144 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{ericsson_einspace_2024,
	title = {einspace: {Searching} for {Neural} {Architectures} from {Fundamental} {Operations}},
	shorttitle = {einspace},
	abstract = {Neural architecture search (NAS) finds high performing networks for a given task. Yet the results of NAS are fairly prosaic; they did not e.g. create a shift from convolutional structures to transformers. This is not least because the search spaces in NAS often aren’t diverse enough to include such transformations *a priori*. Instead, for NAS to provide greater potential for fundamental design shifts, we need a novel expressive search space design which is built from more fundamental operations. To this end, we introduce `einspace`, a search space based on a parameterised probabilistic context-free grammar. Our space is versatile, supporting architectures of various sizes and complexities, while also containing diverse network operations which allow it to model convolutions, attention components and more. It contains many existing competitive architectures, and provides flexibility for discovering new ones. Using this search space, we perform experiments to find novel architectures as well as improvements on existing ones on the diverse Unseen NAS datasets. We show that competitive architectures can be obtained by searching from scratch, and we consistently find large improvements when initialising the search with strong baselines. We believe that this work is an important advancement towards a transformative NAS paradigm where search space expressivity and strategic search initialisation play key roles.},
	language = {en},
	urldate = {2025-01-15},
	author = {Ericsson, Linus and Espinosa, Miguel and Yang, Chenhongyi and Antoniou, Antreas and Storkey, Amos and Cohen, Shay B. and McDonagh, Steven and Crowley, Elliot J.},
	month = nov,
	year = {2024},
}

@inproceedings{navon_equivariant_2023,
	series = {{ICML}'23},
	title = {Equivariant architectures for learning in deep weight spaces},
	abstract = {Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show how these layers can be implemented using three basic operations: pooling, broadcasting, and fully connected layers applied to the input in an appropriate manner. We demonstrate the effectiveness of our architecture and its advantages over natural baselines in a variety of learning tasks.},
	urldate = {2025-01-15},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Navon, Aviv and Shamsian, Aviv and Achituve, Idan and Fetaya, Ethan and Chechik, Gal and Maron, Haggai},
	month = jul,
	year = {2023},
}

@inproceedings{isallari_gsr-net_2020,
	title = {{GSR}-{Net}: {Graph} {Super}-{Resolution} {Network} for {Predicting} {High}-{Resolution} from {Low}-{Resolution} {Functional} {Brain} {Connectomes}},
	doi = {10.1007/978-3-030-59861-7_15},
	abstract = {Catchy but rigorous deep learning architectures were tailored for image super-resolution (SR), however, these fail to generalize to non-Euclidean data such as brain connectomes. Specifically, building generative models for super-resolving a low-resolution brain connectome at a higher resolution (i.e., adding new graph nodes/edges) remains unexplored —although this would circumvent the need for costly data collection and manual labelling of anatomical brain regions (i.e. parcellation). To fill this gap, we introduce GSR-Net (Graph Super-Resolution Network), the first super-resolution framework operating on graph-structured data that generates high-resolution brain graphs from low-resolution graphs. First, we adopt a U-Net like architecture based on graph convolution, pooling and unpooling operations specific to non-Euclidean data. However, unlike conventional U-Nets where graph nodes represent samples and node features are mapped to a low-dimensional space (encoding and decoding node attributes or sample features), our GSR-Net operates directly on a single connectome: a fully connected graph where conventionally, a node denotes a brain region, nodes have no features, and edge weights denote brain connectivity strength between two regions of interest (ROIs). In the absence of original node features, we initially assign identity feature vectors to each brain ROI (node) and then leverage the learned local receptive fields to learn node feature representations. Specifically, for each ROI, we learn a node feature embedding by locally averaging the features of its neighboring nodes based on their connectivity weights. Second, inspired by spectral theory, we break the symmetry of the U-Net architecture by topping it up with a graph super-resolution (GSR) layer and two graph convolutional network layers to predict a HR (high-resolution) graph while preserving the characteristics of the LR (low-resolution) input. Our proposed GSR-Net framework outperformed its variants for predicting high-resolution brain functional connectomes from low-resolution connectomes. Our Python GSR-Net code is available on BASIRA GitHub at https://github.com/basiralab/GSR-Net.},
	language = {en},
	booktitle = {Machine {Learning} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Isallari, Megi and Rekik, Islem},
	editor = {Liu, Mingxia and Yan, Pingkun and Lian, Chunfeng and Cao, Xiaohuan},
	year = {2020},
	pages = {139--149},
}

@misc{kunz_implicit_2024,
	title = {Implicit {Neural} {Networks} with {Fourier}-{Feature} {Inputs} for {Free}-breathing {Cardiac} {MRI} {Reconstruction}},
	abstract = {Cardiac magnetic resonance imaging (MRI) requires reconstructing a real-time video of a beating heart from continuous highly under-sampled measurements. This task is challenging since the object to be reconstructed (the heart) is continuously changing during signal acquisition. In this paper, we propose a reconstruction approach based on representing the beating heart with an implicit neural network and fitting the network so that the representation of the heart is consistent with the measurements. The network in the form of a multi-layer perceptron with Fourier-feature inputs acts as an effective signal prior and enables adjusting the regularization strength in both the spatial and temporal dimensions of the signal. We study the proposed approach for 2D free-breathing cardiac real-time MRI in different operating regimes, i.e., for different image resolutions, slice thicknesses, and acquisition lengths. Our method achieves reconstruction quality on par with or slightly better than state-of-the-art untrained convolutional neural networks and superior image quality compared to a recent method that fits an implicit representation directly to Fourier-domain measurements. However, this comes at a relatively high computational cost. Our approach does not require any additional patient data or biosensors including electrocardiography, making it potentially applicable in a wide range of clinical scenarios.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Kunz, Johannes F. and Ruschke, Stefan and Heckel, Reinhard},
	month = jan,
	year = {2024},
	note = {arXiv:2305.06822},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{eldeniz_phase2phase_2021,
	title = {{Phase2Phase}: {Respiratory} {Motion}-{Resolved} {Reconstruction} of {Free}-{Breathing} {Magnetic} {Resonance} {Imaging} {Using} {Deep} {Learning} {Without} a {Ground} {Truth} for {Improved} {Liver} {Imaging}},
	volume = {56},
	doi = {10.1097/RLI.0000000000000792},
	abstract = {Objectives 
          Respiratory binning of free-breathing magnetic resonance imaging data reduces motion blurring; however, it exacerbates noise and introduces severe artifacts due to undersampling. Deep neural networks can remove artifacts and noise but usually require high-quality ground truth images for training. This study aimed to develop a network that can be trained without this requirement.
          Materials and Methods 
          This retrospective study was conducted on 33 participants enrolled between November 2016 and June 2019. Free-breathing magnetic resonance imaging was performed using a radial acquisition. Self-navigation was used to bin the k-space data into 10 respiratory phases. To simulate short acquisitions, subsets of radial spokes were used in reconstructing images with multicoil nonuniform fast Fourier transform (MCNUFFT), compressed sensing (CS), and 2 deep learning methods: UNet3DPhase and Phase2Phase (P2P). UNet3DPhase was trained using a high-quality ground truth, whereas P2P was trained using noisy images with streaking artifacts. Two radiologists blinded to the reconstruction methods independently reviewed the sharpness, contrast, and artifact-freeness of the end-expiration images reconstructed from data collected at 16\% of the Nyquist sampling rate. The generalized estimating equation method was used for statistical comparison. Motion vector fields were derived to examine the respiratory motion range of 4-dimensional images reconstructed using different methods.
          Results 
          A total of 15 healthy participants and 18 patients with hepatic malignancy (50 ± 15 years, 6 women) were enrolled. Both reviewers found that the UNet3DPhase and P2P images had higher contrast (P {\textless} 0.01) and fewer artifacts (P {\textless} 0.01) than the CS images. The UNet3DPhase and P2P images were reported to be sharper than the CS images by 1 reviewer (P {\textless} 0.01) but not by the other reviewer (P = 0.22, P = 0.18). UNet3DPhase and P2P were similar in sharpness and contrast, whereas UNet3DPhase had fewer artifacts (P {\textless} 0.01). The motion vector lengths for the MCNUFFT800 and P2P800 images were comparable (10.5 ± 4.2 mm and 9.9 ± 4.0 mm, respectively), whereas both were significantly larger than CS2000 (7.0 ± 3.9 mm; P {\textless} 0.0001) and UNnet3DPhase800 (6.9 ± 3.2; P {\textless} 0.0001) images.
          Conclusions 
          Without a ground truth, P2P can reconstruct sharp, artifact-free, and high-contrast respiratory motion-resolved images from highly undersampled data. Unlike the CS and UNet3DPhase methods, P2P did not artificially reduce the respiratory motion range.},
	number = {12},
	urldate = {2024-08-31},
	journal = {Investigative Radiology},
	author = {Eldeniz, Cihat and Gan, Weijie and Chen, Sihao and Fraum, Tyler J. and Ludwig, Daniel R. and Yan, Yan and Liu, Jiaming and Vahle, Thomas and Krishnamurthy, Uday and Kamilov, Ulugbek S. and An, Hongyu},
	month = dec,
	year = {2021},
}

@article{shimron_implicit_2022,
	title = {Implicit data crimes: {Machine} learning bias arising from misuse of public data},
	volume = {119},
	shorttitle = {Implicit data crimes},
	doi = {10.1073/pnas.2117203119},
	abstract = {Although open databases are an important resource in the current deep learning (DL) era, they are sometimes used “off label”: Data published for one task are used to train algorithms for a different one. This work aims to highlight that this common practice may lead to biased, overly optimistic results. We demonstrate this phenomenon for inverse problem solvers and show how their biased performance stems from hidden data-processing pipelines. We describe two processing pipelines typical of open-access databases and study their effects on three well-established algorithms developed for MRI reconstruction: compressed sensing, dictionary learning, and DL. Our results demonstrate that all these algorithms yield systematically biased results when they are naively trained on seemingly appropriate data: The normalized rms error improves consistently with the extent of data processing, showing an artificial improvement of 25 to 48\% in some cases. Because this phenomenon is not widely known, biased results sometimes are published as state of the art; we refer to that as implicit “data crimes.” This work hence aims to raise awareness regarding naive off-label usage of big data and reveal the vulnerability of modern inverse problem solvers to the resulting bias.},
	number = {13},
	urldate = {2024-10-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Shimron, Efrat and Tamir, Jonathan I. and Wang, Ke and Lustig, Michael},
	month = mar,
	year = {2022},
}

@inproceedings{chen_robust_2022,
	title = {Robust {Equivariant} {Imaging}: a fully unsupervised framework for learning to image from noisy and partial measurements},
	doi = {10.1109/CVPR52688.2022.00556},
	abstract = {Deep networks provide state-of-the-art performance in multiple imaging inverse problems ranging from medical imaging to computational photography. However, most existing networks are trained with clean signals which are often hard or impossible to obtain. Equivariant imaging (EI) is a recent self-supervised learning framework that exploits the group invariance present in signal distributions to learn a reconstruction function from partial measurement data alone. While EI results are impressive, its performance degrades with increasing noise. In this paper, we propose a Robust Equivariant Imaging (REI) framework which can learn to image from noisy partial measurements alone. The proposed method uses Stein's Unbiased Risk Estimator (SURE) to obtain a fully unsupervised training loss that is robust to noise. We show that REI leads to considerable performance gains on linear and nonlinear inverse problems, thereby paving the way for robust unsupervised imaging with deep networks. Code is available at https://github.com/edongdongchen/REI.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chen, Dongdong and Tachella, Julián and Davies, Mike E.},
	month = jun,
	year = {2022},
}

@misc{sultan_deep_2024,
	title = {Deep {Image} prior with {StruCtUred} {Sparsity} ({DISCUS}) for dynamic {MRI} reconstruction},
	url = {http://arxiv.org/abs/2312.00953},
	doi = {10.48550/arXiv.2312.00953},
	abstract = {High-quality training data are not always available in dynamic MRI. To address this, we propose a self-supervised deep learning method called deep image prior with structured sparsity (DISCUS) for reconstructing dynamic images. DISCUS is inspired by deep image prior (DIP) and recovers a series of images through joint optimization of network parameters and input code vectors. However, DISCUS additionally encourages group sparsity on frame-specific code vectors to discover the low-dimensional manifold that describes temporal variations across frames. Compared to prior work on manifold learning, DISCUS does not require specifying the manifold dimensionality. We validate DISCUS using three numerical studies. In the first study, we simulate a dynamic Shepp-Logan phantom with frames undergoing random rotations, translations, or both, and demonstrate that DISCUS can discover the dimensionality of the underlying manifold. In the second study, we use data from a realistic late gadolinium enhancement (LGE) phantom to compare DISCUS with compressed sensing (CS) and DIP, and to demonstrate the positive impact of group sparsity. In the third study, we use retrospectively undersampled single-shot LGE data from five patients to compare DISCUS with CS reconstructions. The results from these studies demonstrate that DISCUS outperforms CS and DIP, and that enforcing group sparsity on the code vectors helps discover true manifold dimensionality and provides additional performance gain.},
	urldate = {2024-09-24},
	publisher = {arXiv},
	author = {Sultan, Muhammad A. and Chen, Chong and Liu, Yingmin and Lei, Xuan and Ahmad, Rizwan},
	month = may,
	year = {2024},
	note = {arXiv:2312.00953},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{wang_perspective-equivariance_2024,
	title = {Perspective-{Equivariance} for {Unsupervised} {Imaging} with {Camera} {Geometry}},
	shorttitle = {Perspective-{Equivariant} {Imaging}},
	doi = {10.48550/arXiv.2403.09327},
	abstract = {Ill-posed image reconstruction problems appear in many scenarios such as remote sensing, where obtaining high quality images is crucial for environmental monitoring, disaster management and urban planning. Deep learning has seen great success in overcoming the limitations of traditional methods. However, these inverse problems rarely come with ground truth data, highlighting the importance of unsupervised learning from partial and noisy measurements alone. We propose perspective-equivariant imaging (EI), a framework that leverages perspective variability in optical camera-based imaging systems, such as satellites or handheld cameras, to recover information lost in ill-posed optical camera imaging problems. This extends previous EI work to include a much richer non-linear class of group transforms and is shown to be an excellent prior for satellite and urban image data, where perspective-EI achieves state-of-the-art results in multispectral pansharpening, outperforming other unsupervised methods in the literature. Code at https://andrewwango.github.io/perspective-equivariant-imaging},
	publisher = {arXiv},
	author = {Wang, Andrew and Davies, Mike},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09327 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{gan_deformation-compensated_2022,
	title = {Deformation-{Compensated} {Learning} for {Image} {Reconstruction} {Without} {Ground} {Truth}},
	volume = {41},
	issn = {1558-254X},
	doi = {10.1109/TMI.2022.3163018},
	abstract = {Deep neural networks for medical image reconstruction are traditionally trained using high-quality ground-truth images as training targets. Recent work on Noise2Noise (N2N) has shown the potential of using multiple noisy measurements of the same object as an alternative to having a ground-truth. However, existing N2N-based methods are not suitable for learning from the measurements of an object undergoing nonrigid deformation. This paper addresses this issue by proposing the deformation-compensated learning (DeCoLearn) method for training deep reconstruction networks by compensating for object deformations. A key component of DeCoLearn is a deep registration module, which is jointly trained with the deep reconstruction network without any ground-truth supervision. We validate DeCoLearn on both simulated and experimentally collected magnetic resonance imaging (MRI) data and show that it significantly improves imaging quality.},
	number = {9},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Gan, Weijie and Sun, Yu and Eldeniz, Cihat and Liu, Jiaming and An, Hongyu and Kamilov, Ulugbek S.},
	month = sep,
	year = {2022},
	keywords = {Convolutional neural networks, Image reconstruction, Imaging, Inverse problems, Magnetic resonance imaging, Noise measurement, Strain, Training, deep learning, image reconstruction, magnetic resonance imaging (MRI)},
	pages = {2371--2384},
}

@inproceedings{gan_deep_2021,
	title = {Deep {Image} {Reconstruction} {Using} {Unregistered} {Measurements} {Without} {Groundtruth}},
	url = {https://ieeexplore.ieee.org/document/9434079},
	doi = {10.1109/ISBI48211.2021.9434079},
	abstract = {One of the key limitations in conventional deep learning based image reconstruction is the need for registered pairs of training images containing a set of high-quality groundtruth images. This paper addresses this limitation by proposing a novel unsupervised deep registration-augmented reconstruction method (U-Dream) for training deep neural nets to reconstruct high-quality images by directly mapping pairs of unregistered and artifact-corrupted images. The ability of U-Dream to circumvent the need for accurately registered data makes it widely applicable to many biomedical image reconstruction tasks. We validate it in accelerated magnetic resonance imaging (MRI) by training an image reconstruction model directly on pairs of undersampled measurements from images that have undergone nonrigid deformations.},
	urldate = {2024-09-07},
	booktitle = {2021 {IEEE} 18th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Gan, Weijie and Sun, Yu and Eldeniz, Cihat and Liu, Jiaming and An, Hongyu and Kamilov, Ulugbek S.},
	month = apr,
	year = {2021},
	keywords = {Biomedical measurement, Image reconstruction, Image registration, Magnetic resonance imaging, Neural networks, Reconstruction algorithms, Task analysis, Training, deep learning, deformable image registration, magnetic resonance imaging},
	pages = {1531--1534},
}

@misc{lyu_state---art_2024,
	title = {The state-of-the-art in {Cardiac} {MRI} {Reconstruction}: {Results} of the {CMRxRecon} {Challenge} in {MICCAI} 2023},
	shorttitle = {The state-of-the-art in {Cardiac} {MRI} {Reconstruction}},
	url = {https://arxiv.org/abs/2404.01082v2},
	abstract = {Cardiac MRI, crucial for evaluating heart structure and function, faces limitations like slow imaging and motion artifacts. Undersampling reconstruction, especially data-driven algorithms, has emerged as a promising solution to accelerate scans and enhance imaging performance using highly under-sampled data. Nevertheless, the scarcity of publicly available cardiac k-space datasets and evaluation platform hinder the development of data-driven reconstruction algorithms. To address this issue, we organized the Cardiac MRI Reconstruction Challenge (CMRxRecon) in 2023, in collaboration with the 26th International Conference on MICCAI. CMRxRecon presented an extensive k-space dataset comprising cine and mapping raw data, accompanied by detailed annotations of cardiac anatomical structures. With overwhelming participation, the challenge attracted more than 285 teams and over 600 participants. Among them, 22 teams successfully submitted Docker containers for the testing phase, with 7 teams submitted for both cine and mapping tasks. All teams use deep learning based approaches, indicating that deep learning has predominately become a promising solution for the problem. The first-place winner of both tasks utilizes the E2E-VarNet architecture as backbones. In contrast, U-Net is still the most popular backbone for both multi-coil and single-coil reconstructions. This paper provides a comprehensive overview of the challenge design, presents a summary of the submitted results, reviews the employed methods, and offers an in-depth discussion that aims to inspire future advancements in cardiac MRI reconstruction models. The summary emphasizes the effective strategies observed in Cardiac MRI reconstruction, including backbone architecture, loss function, pre-processing techniques, physical modeling, and model complexity, thereby providing valuable insights for further developments in this field.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Lyu, Jun and Qin, Chen and Wang, Shuo and Wang, Fanwen and Li, Yan and Wang, Zi and Guo, Kunyuan and Ouyang, Cheng and Tänzer, Michael and Liu, Meng and Sun, Longyu and Sun, Mengting and Li, Qin and Shi, Zhang and Hua, Sha and Li, Hao and Chen, Zhensen and Zhang, Zhenlin and Xin, Bingyu and Metaxas, Dimitris N. and Yiasemis, George and Teuwen, Jonas and Zhang, Liping and Chen, Weitian and Zhao, Yidong and Tao, Qian and Pang, Yanwei and Liu, Xiaohan and Razumov, Artem and Dylov, Dmitry V. and Dou, Quan and Yan, Kang and Xue, Yuyang and Du, Yuning and Dietlmeier, Julia and Garcia-Cabrera, Carles and Hemidi, Ziad Al-Haj and Vogt, Nora and Xu, Ziqiang and Zhang, Yajing and Chu, Ying-Hua and Chen, Weibo and Bai, Wenjia and Zhuang, Xiahai and Qin, Jing and Wu, Lianmin and Yang, Guang and Qu, Xiaobo and Wang, He and Wang, Chengyan},
	month = apr,
	year = {2024},
}

@misc{wang_cmrxrecon_2023,
	title = {{CMRxRecon}: {An} open cardiac {MRI} dataset for the competition of accelerated image reconstruction},
	shorttitle = {{CMRxRecon}},
	url = {https://arxiv.org/abs/2309.10836v1},
	abstract = {Cardiac magnetic resonance imaging (CMR) has emerged as a valuable diagnostic tool for cardiac diseases. However, a limitation of CMR is its slow imaging speed, which causes patient discomfort and introduces artifacts in the images. There has been growing interest in deep learning-based CMR imaging algorithms that can reconstruct high-quality images from highly under-sampled k-space data. However, the development of deep learning methods requires large training datasets, which have not been publicly available for CMR. To address this gap, we released a dataset that includes multi-contrast, multi-view, multi-slice and multi-coil CMR imaging data from 300 subjects. Imaging studies include cardiac cine and mapping sequences. Manual segmentations of the myocardium and chambers of all the subjects are also provided within the dataset. Scripts of state-of-the-art reconstruction algorithms were also provided as a point of reference. Our aim is to facilitate the advancement of state-of-the-art CMR image reconstruction by introducing standardized evaluation criteria and making the dataset freely accessible to the research community. Researchers can access the dataset at https://www.synapse.org/\#!Synapse:syn51471091/wiki/.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Wang, Chengyan and Lyu, Jun and Wang, Shuo and Qin, Chen and Guo, Kunyuan and Zhang, Xinyu and Yu, Xiaotong and Li, Yan and Wang, Fanwen and Jin, Jianhua and Shi, Zhang and Xu, Ziqiang and Tian, Yapeng and Hua, Sha and Chen, Zhensen and Liu, Meng and Sun, Mengting and Kuang, Xutong and Wang, Kang and Wang, Haoran and Li, Hao and Chu, Yinghua and Yang, Guang and Bai, Wenjia and Zhuang, Xiahai and Wang, He and Qin, Jing and Qu, Xiaobo},
	month = sep,
	year = {2023},
}

@misc{zbontar_fastmri_2018,
	title = {{fastMRI}: {An} {Open} {Dataset} and {Benchmarks} for {Accelerated} {MRI}},
	shorttitle = {{fastMRI}},
	url = {https://arxiv.org/abs/1811.08839v2},
	abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
	month = nov,
	year = {2018},
}

@article{lim_3d_2019,
	title = {{3D} dynamic {MRI} of the vocal tract during natural speech},
	volume = {81},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.27570},
	doi = {10.1002/mrm.27570},
	abstract = {Purpose To develop and evaluate a technique for 3D dynamic MRI of the full vocal tract at high temporal resolution during natural speech. Methods We demonstrate 2.4 × 2.4 × 5.8 mm3 spatial resolution, 61-ms temporal resolution, and a 200 × 200 × 70 mm3 FOV. The proposed method uses 3D gradient-echo imaging with a custom upper-airway coil, a minimum-phase slab excitation, stack-of-spirals readout, pseudo golden-angle view order in kx-ky, linear Cartesian order along kz, and spatiotemporal finite difference constrained reconstruction, with 13-fold acceleration. This technique is evaluated using in vivo vocal tract airway data from 2 healthy subjects acquired at 1.5T scanner, 1 with synchronized audio, with 2 tasks during production of natural speech, and via comparison with interleaved multislice 2D dynamic MRI. Results This technique captured known dynamics of vocal tract articulators during natural speech tasks including tongue gestures during the production of consonants “s” and “l” and of consonant–vowel syllables, and was additionally consistent with 2D dynamic MRI. Coordination of lingual (tongue) movements for consonants is demonstrated via volume-of-interest analysis. Vocal tract area function dynamics revealed critical lingual constriction events along the length of the vocal tract for consonants and vowels. Conclusion We demonstrate feasibility of 3D dynamic MRI of the full vocal tract, with spatiotemporal resolution adequate to visualize lingual movements for consonants and vocal tact shaping during natural productions of consonant–vowel syllables, without requiring multiple repetitions.},
	number = {3},
	urldate = {2024-09-06},
	journal = {Magnetic Resonance in Medicine},
	author = {Lim, Yongwan and Zhu, Yinghua and Lingala, Sajan Goud and Byrd, Dani and Narayanan, Shrikanth and Nayak, Krishna Shrinivas},
	year = {2019},
	keywords = {dynamic MRI, dynamic speech imaging, golden-angle stack-of-spirals, lateral production, rapid vocal tract shaping, speech articulation},
	pages = {1511--1520},
}

@article{freifeld_transformations_2017,
	title = {Transformations {Based} on {Continuous} {Piecewise}-{Affine} {Velocity} {Fields}},
	volume = {39},
	url = {https://ieeexplore.ieee.org/abstract/document/7814343},
	doi = {10.1109/TPAMI.2016.2646685},
	abstract = {We propose novel finite-dimensional spaces of well-behaved {\textbackslash}mathbb R{\textasciicircum}n{\textbackslash}rightarrow {\textbackslash}mathbb R{\textasciicircum}n transformations. The latter are obtained by (fast and highly-accurate) integration of continuous piecewise-affine velocity fields. The proposed method is simple yet highly expressive, effortlessly handles optional constraints (e.g., volume preservation and/or boundary conditions), and supports convenient modeling choices such as smoothing priors and coarse-to-fine analysis. Importantly, the proposed approach, partly due to its rapid likelihood evaluations and partly due to its other properties, facilitates tractable inference over rich transformation spaces, including using Markov-Chain Monte-Carlo methods. Its applications include, but are not limited to: monotonic regression (more generally, optimization over monotonic functions); modeling cumulative distribution functions or histograms; time-warping; image warping; image registration; real-time diffeomorphic image editing; data augmentation for image classifiers. Our GPU-based code is publicly available.},
	number = {12},
	urldate = {2024-09-02},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Freifeld, Oren and Hauberg, Søren and Batmanghelich, Kayhan and Fisher, Jonn W.},
	month = dec,
	year = {2017},
	keywords = {Biomedical imaging, Complexity theory, Computational modeling, Computer vision, Distribution functions, Histograms, Spatial transformations, continuous piecewise-affine velocity fields, diffeomorphisms, tessellations, priors, MCMC, Trajectory},
	pages = {2496--2509},
}

@misc{hu_restoration_2023,
	title = {A {Restoration} {Network} as an {Implicit} {Prior}},
	abstract = {Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Hu, Yuyang and Delbracio, Mauricio and Milanfar, Peyman and Kamilov, Ulugbek S.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01391 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{zou_selfcolearn_2022,
	title = {{SelfCoLearn}: {Self}-{Supervised} {Collaborative} {Learning} for {Accelerating} {Dynamic} {MR} {Imaging}},
	volume = {9},
	url = {https://www.mdpi.com/2306-5354/9/11/650},
	doi = {10.3390/bioengineering9110650},
	abstract = {Lately, deep learning technology has been extensively investigated for accelerating dynamic magnetic resonance (MR) imaging, with encouraging progresses achieved. However, without fully sampled reference data for training, the current approaches may have limited abilities in recovering fine details or structures. To address this challenge, this paper proposes a self-supervised collaborative learning framework (SelfCoLearn) for accurate dynamic MR image reconstruction from undersampled k-space data directly. The proposed SelfCoLearn is equipped with three important components, namely, dual-network collaborative learning, reunderampling data augmentation and a special-designed co-training loss. The framework is flexible and can be integrated into various model-based iterative un-rolled networks. The proposed method has been evaluated on an in vivo dataset and was compared to four state-of-the-art methods. The results show that the proposed method possesses strong capabilities in capturing essential and inherent representations for direct reconstructions from the undersampled k-space data and thus enables high-quality and fast dynamic MR imaging.},
	language = {en},
	number = {11},
	urldate = {2024-09-02},
	journal = {Bioengineering},
	author = {Zou, Juan and Li, Cheng and Jia, Sen and Wu, Ruoyou and Pei, Tingrui and Zheng, Hairong and Wang, Shanshan},
	month = nov,
	year = {2022},
	keywords = {co-training loss, collaborative learning, dynamic MR imaging, reunderampling data augmentation, self-supervised learning},
	pages = {650},
}

@article{yoo_time-dependent_2021,
	title = {Time-{Dependent} {Deep} {Image} {Prior} for {Dynamic} {MRI}},
	volume = {40},
	url = {https://ieeexplore.ieee.org/document/9442767},
	doi = {10.1109/TMI.2021.3084288},
	abstract = {We propose a novel unsupervised deep-learning-based algorithm for dynamic magnetic resonance imaging (MRI) reconstruction. Dynamic MRI requires rapid data acquisition for the study of moving organs such as the heart. We introduce a generalized version of the deep-image-prior approach, which optimizes the weights of a reconstruction network to fit a sequence of sparsely acquired dynamic MRI measurements. Our method needs neither prior training nor additional data. In particular, for cardiac images, it does not require the marking of heartbeats or the reordering of spokes. The key ingredients of our method are threefold: 1) a fixed low-dimensional manifold that encodes the temporal variations of images; 2) a network that maps the manifold into a more expressive latent space; and 3) a convolutional neural network that generates a dynamic series of MRI images from the latent variables and that favors their consistency with the measurements in k -space. Our method outperforms the state-of-the-art methods quantitatively and qualitatively in both retrospective and real fetal cardiac datasets. To the best of our knowledge, this is the first unsupervised deep-learning-based method that can reconstruct the continuous variation of dynamic MRI sequences with high spatial resolution.},
	number = {12},
	urldate = {2024-09-02},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Yoo, Jaejun and Jin, Kyong Hwan and Gupta, Harshit and Yerly, Jérôme and Stuber, Matthias and Unser, Michael},
	month = dec,
	year = {2021},
	keywords = {Accelerated MRI, Electronics packaging, Heuristic algorithms, Image reconstruction, Imaging, Magnetic resonance imaging, Manifolds, Unsupervised learning, unsupervised learning},
	pages = {3337--3348},
}

@article{liu_rare_2020,
	title = {{RARE}: {Image} {Reconstruction} {Using} {Deep} {Priors} {Learned} {Without} {Groundtruth}},
	volume = {14},
	url = {https://ieeexplore.ieee.org/document/9103213},
	doi = {10.1109/JSTSP.2020.2998402},
	abstract = {Regularization by denoising (RED) is an image reconstruction framework that uses an image denoiser as a prior. Recent work has shown the state-of-the-art performance of RED with learned denoisers corresponding to pre-trained convolutional neural nets (CNNs). In this work, we propose to broaden the current denoiser-centric view of RED by considering priors corresponding to networks trained for more general artifact-removal. The key benefit of the proposed family of algorithms, called regularization by artifact-removal (RARE), is that it can leverage priors learned on datasets containing only undersampled measurements. This makes RARE applicable to problems where it is practically impossible to have fully-sampled groundtruth data for training. We validate RARE on both simulated and experimentally collected data by reconstructing a free-breathing whole-body 3D MRIs into ten respiratory phases from heavily undersampled k-space measurements. Our results corroborate the potential of learning regularizers for iterative inversion directly on undersampled and noisy measurements.},
	number = {6},
	urldate = {2024-08-31},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Liu, Jiaming and Sun, Yu and Eldeniz, Cihat and Gan, Weijie and An, Hongyu and Kamilov, Ulugbek S.},
	month = oct,
	year = {2020},
	keywords = {Deep learning, Image denoising, Image reconstruction, Imaging inverse problems, Inverse problems, MRI, Magnetic resonance imaging, Noise measurement, deep learning, plug-and-play priors, regularization by denoising},
	pages = {1088--1099},
}

@article{millard_theoretical_2023,
	title = {A {Theoretical} {Framework} for {Self}-{Supervised} {MR} {Image} {Reconstruction} {Using} {Sub}-{Sampling} via {Variable} {Density} {Noisier2Noise}},
	volume = {9},
	url = {https://ieeexplore.ieee.org/document/10194985?denied=},
	doi = {10.1109/TCI.2023.3299212},
	abstract = {In recent years, there has been attention on leveraging the statistical modeling capabilities of neural networks for reconstructing sub-sampled Magnetic Resonance Imaging (MRI) data. Most proposed methods assume the existence of a representative fully-sampled dataset and use fully-supervised training. However, for many applications, fully sampled training data is not available, and may be highly impractical to acquire. The development and understanding of self-supervised methods, which use only sub-sampled data for training, are therefore highly desirable. This work extends the Noisier2Noise framework, which was originally constructed for self-supervised denoising tasks, to variable density sub-sampled MRI data. We use the Noisier2Noise framework to analytically explain the performance of Self-Supervised Learning via Data Undersampling (SSDU), a recently proposed method that performs well in practice but until now lacked theoretical justification. Further, we propose two modifications of SSDU that arise as a consequence of the theoretical developments. Firstly, we propose partitioning the sampling set so that the subsets have the same type of distribution as the original sampling mask. Secondly, we propose a loss weighting that compensates for the sampling and partitioning densities. On the fastMRI dataset we show that these changes significantly improve SSDU's image restoration quality and robustness to the partitioning parameters.},
	urldate = {2024-08-31},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Millard, Charles and Chiew, Mark},
	year = {2023},
	keywords = {Deep learning, Image reconstruction, Imaging, Magnetic resonance imaging, Random variables, Standards, Training, Training data, image reconstruction, magnetic resonance imaging},
	pages = {707--720},
}

@inproceedings{zhou_spatial-frequency_2022,
	title = {Spatial-{Frequency} {Domain} {Information} {Integration} for {Pan}-{Sharpening}},
	doi = {10.1007/978-3-031-19797-0_16},
	abstract = {Pan-sharpening aims to generate high-resolution multi-spectral (MS) images by fusing PAN images and low-resolution MS images. Despite its great advances, most existing pan-sharpening methods only work in the spatial domain and rarely explore the potential solutions in the frequency domain. In this paper, we first attempt to address pan-sharpening in both spatial and frequency domains and propose a Spatial-Frequency Information Integration Network, dubbed as SFIIN. To implement SFIIN, we devise a core building module tailored with pan-sharpening, consisting of three key components: spatial-domain information branch, frequency-domain information branch, and dual domain interaction. To be specific, the first employs the standard convolution to integrate the local information of two modalities of PAN and MS images in the spatial domain, while the second adopts deep Fourier transformation to achieve the image-wide receptive field for exploring global contextual information. Followed by, the third is responsible for facilitating the information flow and learning the complementary representation. We conduct extensive experiments to validate the effectiveness of the proposed network and demonstrate the favorable performance against other state-of-the-art methods.},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	author = {Zhou, Man and Huang, Jie and Yan, Keyu and Yu, Hu and Fu, Xueyang and Liu, Aiping and Wei, Xian and Zhao, Feng},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	keywords = {Pan-sharpening, Spatial-frequency domain},
	pages = {274--291},
}

@misc{zhou_panformer_2022,
	title = {{PanFormer}: a {Transformer} {Based} {Model} for {Pan}-sharpening},
	shorttitle = {{PanFormer}},
	doi = {10.48550/arXiv.2203.02916},
	abstract = {Pan-sharpening aims at producing a high-resolution (HR) multi-spectral (MS) image from a low-resolution (LR) multi-spectral (MS) image and its corresponding panchromatic (PAN) image acquired by a same satellite. Inspired by a new fashion in recent deep learning community, we propose a novel Transformer based model for pan-sharpening. We explore the potential of Transformer in image feature extraction and fusion. Following the successful development of vision transformers, we design a two-stream network with the self-attention to extract the modality-specific features from the PAN and MS modalities and apply a cross-attention module to merge the spectral and spatial features. The pan-sharpened image is produced from the enhanced fused features. Extensive experiments on GaoFen-2 and WorldView-3 images demonstrate that our Transformer based model achieves impressive results and outperforms many existing CNN based methods, which shows the great potential of introducing Transformer to the pan-sharpening task. Codes are available at https://github.com/zhysora/PanFormer.},
	publisher = {arXiv},
	author = {Zhou, Huanyu and Liu, Qingjie and Wang, Yunhong},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02916 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{zhao_loss_2017,
	title = {Loss {Functions} for {Image} {Restoration} {With} {Neural} {Networks}},
	volume = {3},
	doi = {10.1109/TCI.2016.2644865},
	abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is {\textbackslash}ell \_2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
	number = {1},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Zhao, Hang and Gallo, Orazio and Frosio, Iuri and Kautz, Jan},
	month = mar,
	year = {2017},
	keywords = {Image processing, Image quality, Image restoration, Measurement, Neural networks, image restoration, loss functions, neural networks},
	pages = {47--57},
}

@misc{zhang_progressive_2023,
	title = {Progressive {Content}-aware {Coded} {Hyperspectral} {Compressive} {Imaging}},
	doi = {10.48550/arXiv.2303.09773},
	abstract = {Hyperspectral imaging plays a pivotal role in a wide range of applications, like remote sensing, medicine, and cytology. By acquiring 3D hyperspectral images (HSIs) via 2D sensors, the coded aperture snapshot spectral imaging (CASSI) has achieved great success due to its hardware-friendly implementation and fast imaging speed. However, for some less spectrally sparse scenes, single snapshot and unreasonable coded aperture design tend to make HSI recovery more ill-posed and yield poor spatial and spectral fidelity. In this paper, we propose a novel Progressive Content-Aware CASSI framework, dubbed PCA-CASSI, which captures HSIs with multiple optimized content-aware coded apertures and fuses all the snapshots for reconstruction progressively. Simultaneously, by mapping the Range-Null space Decomposition (RND) into a deep network with several phases, an RND-HRNet is proposed for HSI recovery. Each recovery phase can fully exploit the hidden physical information in the coded apertures via explicit \${\textbackslash}mathcal\{R\}\$\$-\$\${\textbackslash}mathcal\{N\}\$ decomposition and explore the spatial-spectral correlation by dual transformer blocks. Our method is validated to surpass other state-of-the-art methods on both multiple- and single-shot HSI imaging tasks by large margins.},
	publisher = {arXiv},
	author = {Zhang, Xuanyu and Chen, Bin and Zou, Wenzhen and Liu, Shuai and Zhang, Yongbing and Xiong, Ruiqin and Zhang, Jian},
	month = mar,
	year = {2023},
	note = {arXiv:2303.09773 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{yuan_multiscale_2018,
	title = {A {Multiscale} and {Multidepth} {Convolutional} {Neural} {Network} for {Remote} {Sensing} {Imagery} {Pan}-{Sharpening}},
	volume = {11},
	doi = {10.1109/JSTARS.2018.2794888},
	abstract = {Pan-sharpening is a fundamental and significant task in the field of remote sensing imagery processing, in which high-resolution spatial details from panchromatic images are employed to enhance the spatial resolution of multispectral (MS) images. As the transformation from low spatial resolution MS image to high-resolution MS image is complex and highly nonlinear, inspired by the powerful representation for nonlinear relationships of deep neural networks, we introduce multiscale feature extraction and residual learning into the basic convolutional neural network (CNN) architecture and propose the multiscale and multidepth CNN for the pan-sharpening of remote sensing imagery. Both the quantitative assessment results and the visual assessment confirm that the proposed network yields high-resolution MS images that are superior to the images produced by the compared state-of-the-art methods.},
	number = {3},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Yuan, Qiangqiang and Wei, Yancong and Meng, Xiangchao and Shen, Huanfeng and Zhang, Liangpei},
	month = mar,
	year = {2018},
	keywords = {Convolutional neural network (CNN), Convolutional neural networks, Feature extraction, Machine learning, Remote sensing, Spatial resolution, Task analysis, multiscale feature learning, pan-sharpening, remote sensing},
	pages = {978--989},
}

@inproceedings{yang_pannet_2017,
	title = {{PanNet}: {A} {Deep} {Network} {Architecture} for {Pan}-{Sharpening}},
	doi = {10.1109/ICCV.2017.193},
	abstract = {We propose a deep network architecture for the pan-sharpening problem called PanNet. We incorporate domain-specific knowledge to design our PanNet architecture by focusing on the two aims of the pan-sharpening problem: spectral and spatial preservation. For spectral preservation, we add up-sampled multispectral images to the network output, which directly propagates the spectral information to the reconstructed image. To preserve spatial structure, we train our network parameters in the high-pass filtering domain rather than the image domain. We show that the trained network generalizes well to images from different satellites without needing retraining. Experiments show significant improvement over state-of-the-art methods visually and in terms of standard quality metrics.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yang, Junfeng and Fu, Xueyang and Hu, Yuwen and Huang, Yue and Ding, Xinghao and Paisley, John},
	month = oct,
	year = {2017},
	keywords = {Machine learning, Mathematical model, Neural networks, Satellites, Spatial resolution, Training},
}

@inproceedings{yang_memory-augmented_2022,
	title = {Memory-augmented {Deep} {Conditional} {Unfolding} {Network} for {Pansharpening}},
	doi = {10.1109/CVPR52688.2022.00183},
	abstract = {Pansharpening aims to obtain high-resolution multispectral (MS) images for remote sensing systems and deep learning-based methods have achieved remarkable success. However, most existing methods are designed in a black-box principle, lacking sufficient interpretability. Additionally, they ignore the different characteristics of each band of MS images and directly concatenate them with panchromatic (PAN) images, leading to severe copy artifacts [9]. To address the above issues, we propose an interpretable deep neural network, namely Memory-augmented Deep Conditional Unfolding Network with two specified core designs. Firstly, considering the degradation process, it formulates the Pansharpening problem as the minimization of a variational model with denoising-based prior and non-local auto-regression prior which is capable of searching the similarities between long-range patches, benefiting the texture enhancement. A novel iteration algorithm with built-in CNNs is exploited for transparent model design. Secondly, to fully explore the potentials of different bands of MS images, the PAN image is combined with each band of MS images, selectively providing the high-frequency details and alleviating the copy artifacts. Extensive experimental results validate the superiority of the proposed algorithm against other state-of-the-art methods.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Yang, Gang and Zhou, Man and Yan, Keyu and Liu, Aiping and Fu, Xueyang and Wang, Fan},
	month = jun,
	year = {2022},
	keywords = {Computer vision, Deep learning, Degradation, Learning systems, Neural networks, Pansharpening, Photogrammetry and remote sensing, Search problems},
	pages = {1778--1787},
}

@inproceedings{yang_panflownet_2023,
	title = {{PanFlowNet}: {A} {Flow}-{Based} {Deep} {Network} for {Pan}-sharpening},
	doi = {10.1109/ICCV51070.2023.01546},
	abstract = {Pan-sharpening aims to generate a high-resolution multispectral (HRMS) image by integrating the spectral information of a low-resolution multispectral (LRMS) image with the texture details of a high-resolution panchromatic (PAN) image. It essentially inherits the ill-posed nature of the super-resolution (SR) task that diverse HRMS images can degrade into an LRMS image. However, existing deep learning-based methods recover only one HRMS image from the LRMS image and PAN image using a deterministic mapping, thus ignoring the diversity of the HRMS image. In this paper, to alleviate this ill-posed issue, we propose a flow-based pan-sharpening network (PanFlowNet) to directly learn the conditional distribution of HRMS image given LRMS image and PAN image instead of learning a deterministic mapping. Specifically, we first transform this unknown conditional distribution into a given Gaussian distribution by an invertible network, and the conditional distribution can thus be explicitly defined. Then, we design an invertible Conditional Affine Coupling Block (CACB) and further build the architecture of PanFlowNet by stacking a series of CACBs. Finally, the PanFlowNet is trained by maximizing the log-likelihood of the conditional distribution given a training set and can then be used to predict diverse HRMS images. The experimental results verify that the proposed PanFlowNet can generate various HRMS images given an LRMS image and a PAN image. Additionally, the experimental results on different kinds of satellite datasets also demonstrate the superiority of our PanFlowNet compared with other state-of-the-art methods both visually and quantitatively. Code is available at Github.},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yang, Gang and Cao, Xiangyong and Xiao, Wenzhe and Zhou, Man and Liu, Aiping and Chen, Xun and Meng, Deyu},
	month = oct,
	year = {2023},
	keywords = {Computer architecture, Learning systems, Neural networks, Satellites, Stacking, Superresolution, Training},
	pages = {16811--16821},
}

@article{yaman_self-supervised_2020,
	title = {Self-supervised learning of physics-guided reconstruction neural networks without fully sampled reference data},
	volume = {84},
	doi = {10.1002/mrm.28378},
	abstract = {Purpose To develop a strategy for training a physics-guided MRI reconstruction neural network without a database of fully sampled data sets. Methods Self-supervised learning via data undersampling (SSDU) for physics-guided deep learning reconstruction partitions available measurements into two disjoint sets, one of which is used in the data consistency (DC) units in the unrolled network and the other is used to define the loss for training. The proposed training without fully sampled data is compared with fully supervised training with ground-truth data, as well as conventional compressed-sensing and parallel imaging methods using the publicly available fastMRI knee database. The same physics-guided neural network is used for both proposed SSDU and supervised training. The SSDU training is also applied to prospectively two-fold accelerated high-resolution brain data sets at different acceleration rates, and compared with parallel imaging. Results Results on five different knee sequences at an acceleration rate of 4 shows that the proposed self-supervised approach performs closely with supervised learning, while significantly outperforming conventional compressed-sensing and parallel imaging, as characterized by quantitative metrics and a clinical reader study. The results on prospectively subsampled brain data sets, in which supervised learning cannot be used due to lack of ground-truth reference, show that the proposed self-supervised approach successfully performs reconstruction at high acceleration rates (4, 6, and 8). Image readings indicate improved visual reconstruction quality with the proposed approach compared with parallel imaging at acquisition acceleration. Conclusion The proposed SSDU approach allows training of physics-guided deep learning MRI reconstruction without fully sampled data, while achieving comparable results with supervised deep learning MRI trained on fully sampled data.},
	language = {en},
	number = {6},
	journal = {Magnetic Resonance in Medicine},
	author = {Yaman, Burhaneddin and Hosseini, Seyed Amir Hossein and Moeller, Steen and Ellermann, Jutta and Uğurbil, Kâmil and Akçakaya, Mehmet},
	year = {2020},
	keywords = {accelerated imaging, convolutional neural networks, deep learning, image reconstruction, parallel imaging, self-supervised learning},
	pages = {3172--3191},
}

@inproceedings{xu_deep_2014,
	title = {Deep {Convolutional} {Neural} {Network} for {Image} {Deconvolution}},
	volume = {27},
	abstract = {Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an deal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Xu, Li and Ren, Jimmy SJ and Liu, Ce and Jia, Jiaya},
	year = {2014},
}

@misc{worrall_deep_2019,
	title = {Deep {Scale}-spaces: {Equivariance} {Over} {Scale}},
	shorttitle = {Deep {Scale}-spaces},
	doi = {10.48550/arXiv.1905.11697},
	abstract = {We introduce deep scale-spaces (DSS), a generalization of convolutional neural networks, exploiting the scale symmetry structure of conventional image recognition tasks. Put plainly, the class of an image is invariant to the scale at which it is viewed. We construct scale equivariant cross-correlations based on a principled extension of convolutions, grounded in the theory of scale-spaces and semigroups. As a very basic operation, these cross-correlations can be used in almost any modern deep learning architecture in a plug-and-play manner. We demonstrate our networks on the Patch Camelyon and Cityscapes datasets, to prove their utility and perform introspective studies to further understand their properties.},
	publisher = {arXiv},
	author = {Worrall, Daniel E. and Welling, Max},
	month = may,
	year = {2019},
	note = {arXiv:1905.11697 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wen_frist_2017,
	title = {{FRIST} - {Flipping} and {Rotation} {Invariant} {Sparsifying} {Transform} {Learning} and {Applications}},
	doi = {10.48550/arXiv.1511.06359},
	abstract = {Features based on sparse representation, especially using the synthesis dictionary model, have been heavily exploited in signal processing and computer vision. However, synthesis dictionary learning typically involves NP-hard sparse coding and expensive learning steps. Recently, sparsifying transform learning received interest for its cheap computation and its optimal updates in the alternating algorithms. In this work, we develop a methodology for learning Flipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better represent natural images that contain textures with various geometrical directions. The proposed alternating FRIST learning algorithm involves efficient optimal updates. We provide a convergence guarantee, and demonstrate the empirical convergence behavior of the proposed FRIST learning approach. Preliminary experiments show the promising performance of FRIST learning for sparse image representation, segmentation, denoising, robust inpainting, and compressed sensing-based magnetic resonance image reconstruction.},
	publisher = {arXiv},
	author = {Wen, Bihan and Ravishankar, Saiprasad and Bresler, Yoram},
	month = oct,
	year = {2017},
	note = {arXiv:1511.06359 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{weir_spacenet_2019,
	title = {{SpaceNet} {MVOI}: {A} {Multi}-{View} {Overhead} {Imagery} {Dataset}},
	doi = {10.1109/ICCV.2019.00108},
	abstract = {Detection and segmentation of objects in overheard imagery is a challenging task. The variable density, random orientation, small size, and instance-to-instance heterogeneity of objects in overhead imagery calls for approaches distinct from existing models designed for natural scene datasets. Though new overhead imagery datasets are being developed, they almost universally comprise a single view taken from directly overhead (“at nadir”), failing to address a critical variable: look angle. By contrast, views vary in real-world overhead imagery, particularly in dynamic scenarios such as natural disasters where first looks are often over 40° off-nadir. This represents an important challenge to computer vision methods, as changing view angle adds distortions, alters resolution, and changes lighting. At present, the impact of these perturbations for algorithmic detection and segmentation of objects is untested. To address this problem, we present an open source Multi-View Overhead Imagery dataset, termed SpaceNet MVOI, with 27 unique looks from a broad range of viewing angles (-32.5° to 54.0°). Each of these images cover the same 665 km2 geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance. We benchmark multiple leading segmentation and object detection models on: (1) building detection, (2) generalization to unseen viewing angles and resolutions, and (3) sensitivity of building footprint extraction to changes in resolution. We find that state of the art segmentation and object detection models struggle to identify buildings in off-nadir imagery and generalize poorly to unseen views, presenting an important benchmark to explore the broadly relevant challenge of detecting small, heterogeneous target objects in visually dynamic contexts.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Weir, Nicholas and Lindenbaum, David and Bastidas, Alexei and Etten, Adam and Kumar, Varun and Mcpherson, Sean and Shermeyer, Jacob and Tang, Hanlin},
	month = oct,
	year = {2019},
	keywords = {Buildings, Computer vision, Image resolution, Image segmentation, Object detection, Satellites, Task analysis},
	pages = {992--1001},
}

@inproceedings{weigert_isotropic_2017,
	title = {Isotropic {Reconstruction} of {3D} {Fluorescence} {Microscopy} {Images} {Using} {Convolutional} {Neural} {Networks}},
	doi = {10.1007/978-3-319-66185-8_15},
	abstract = {Fluorescence microscopy images usually show severe anisotropy in axial versus lateral resolution. This hampers downstream processing, i.e. the automatic extraction of quantitative biological data. While deconvolution methods and other techniques to address this problem exist, they are either time consuming to apply or limited in their ability to remove anisotropy. We propose a method to recover isotropic resolution from readily acquired anisotropic data. We achieve this using a convolutional neural network that is trained end-to-end from the same anisotropic body of data we later apply the network to. The network effectively learns to restore the full isotropic resolution by restoring the image under a trained, sample specific image prior. We apply our method to 3 synthetic and 3 real datasets and show that our results improve on results from deconvolution and state-of-the-art super-resolution techniques. Finally, we demonstrate that a standard 3D segmentation pipeline performs on the output of our network with comparable accuracy as on the full isotropic data.},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} − {MICCAI} 2017},
	author = {Weigert, Martin and Royer, Loic and Jug, Florian and Myers, Gene},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	pages = {126--134},
}

@misc{wang_nerf--_2022,
	title = {{NeRF}--: {Neural} {Radiance} {Fields} {Without} {Known} {Camera} {Parameters}},
	shorttitle = {{NeRF}--},
	doi = {10.48550/arXiv.2102.07064},
	abstract = {Considering the problem of novel view synthesis (NVS) from only a set of 2D images, we simplify the training process of Neural Radiance Field (NeRF) on forward-facing scenes by removing the requirement of known or pre-computed camera parameters, including both intrinsics and 6DoF poses. To this end, we propose NeRF\$--\$, with three contributions: First, we show that the camera parameters can be jointly optimised as learnable parameters with NeRF training, through a photometric reconstruction; Second, to benchmark the camera parameter estimation and the quality of novel view renderings, we introduce a new dataset of path-traced synthetic scenes, termed as Blender Forward-Facing Dataset (BLEFF); Third, we conduct extensive analyses to understand the training behaviours under various camera motions, and show that in most scenarios, the joint optimisation pipeline can recover accurate camera parameters and achieve comparable novel view synthesis quality as those trained with COLMAP pre-computed camera parameters. Our code and data are available at https://nerfmm.active.vision.},
	publisher = {arXiv},
	author = {Wang, Zirui and Wu, Shangzhe and Xie, Weidi and Chen, Min and Prisacariu, Victor Adrian},
	month = apr,
	year = {2022},
	note = {arXiv:2102.07064 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{wang_perspective_2020,
	title = {Perspective {Transformation} {Data} {Augmentation} for {Object} {Detection}},
	volume = {8},
	doi = {10.1109/ACCESS.2019.2962572},
	abstract = {One major reason for the success of convolutional neural networks (CNNs) is the availability of large-scale labeled data. Effective training of CNNs relies on large annotated data. Unfortunately, large amounts of data with corresponding annotations are too expensive to obtain in some real-world applications. One reasonable alternative is to use data augmentation techniques to automatically generate annotated samples. In this paper, a novel data augmentation framework based on perspective transformation is proposed. This method automatically generates new annotated data without extra manual labeling, thus effectively extends the inadequate dataset. Perspective transformation can produce new images captured from any cameras viewpoints. Therefore, our method can mimic images taken at the angle that the camera cannot reach. Extensive experimental results on several datasets have demonstrated that our perspective transformation data augmentation strategy is an effective tool when using deep CNNs on small or imbalance datasets.},
	journal = {IEEE Access},
	author = {Wang, Ke and Fang, Bin and Qian, Jiye and Yang, Su and Zhou, Xin and Zhou, Jie},
	year = {2020},
	pages = {4935--4943},
}

@article{wang_deep_2019,
	title = {Deep learning enables cross-modality super-resolution in fluorescence microscopy},
	volume = {16},
	doi = {10.1038/s41592-018-0239-0},
	abstract = {We present deep-learning-enabled super-resolution across different fluorescence microscopy modalities. This data-driven approach does not require numerical modeling of the imaging process or the estimation of a point-spread-function, and is based on training a generative adversarial network (GAN) to transform diffraction-limited input images into super-resolved ones. Using this framework, we improve the resolution of wide-field images acquired with low-numerical-aperture objectives, matching the resolution that is acquired using high-numerical-aperture objectives. We also demonstrate cross-modality super-resolution, transforming confocal microscopy images to match the resolution acquired with a stimulated emission depletion (STED) microscope. We further demonstrate that total internal reflection fluorescence (TIRF) microscopy images of subcellular structures within cells and tissues can be transformed to match the results obtained with a TIRF-based structured illumination microscope. The deep network rapidly outputs these super-resolved images, without any iterations or parameter search, and could serve to democratize super-resolution imaging.},
	number = {1},
	journal = {Nature Methods},
	author = {Wang, Hongda and Rivenson, Yair and Jin, Yiyin and Wei, Zhensong and Gao, Ronald and Günaydın, Harun and Bentolila, Laurent A. and Kural, Comert and Ozcan, Aydogan},
	month = jan,
	year = {2019},
	keywords = {Fluorescence imaging, Super-resolution microscopy},
	pages = {103--110},
}

@article{walsh_near_2024,
	title = {Near {Real}-{Time} {Social} {Distance} {Estimation} {In} {London}},
	volume = {67},
	doi = {10.1093/comjnl/bxac160},
	abstract = {To mitigate the current COVID-19 pandemic, policy makers at the Greater London Authority, the regional governance body of London, UK, are reliant upon prompt, accurate and actionable estimations of lockdown and social distancing policy adherence. Transport for London, the local transportation department, reports they implemented over 700 interventions such as greater signage and expansion of pedestrian zoning at the height of the pandemic’s first wave with our platform providing key data for those decisions. Large well-defined heterogeneous compositions of pedestrian footfall and physical proximity are difficult to acquire, yet necessary to monitor city-wide activity (busyness) and consequently discern actionable policy decisions. To meet this challenge, we leverage our existing large-scale data processing urban air quality machine learning infrastructure to process over 900 camera feeds in near real-time to generate new estimates of social distancing adherence, group detection and camera stability. In this work, we describe our development and deployment of a computer vision and machine learning pipeline. It provides near immediate sampling and contextualization of activity and physical distancing on the streets of London via live traffic camera feeds. We introduce a platform for inspecting, calibrating and improving upon existing methods, describe the active deployment on real-time feeds and provide analysis over an 18 month period.},
	number = {1},
	journal = {The Computer Journal},
	author = {Walsh, James and Kesa, Oluwafunmilola and Wang, Andrew and Ilas, Mihai and O’Hara, Patrick and Giles, Oscar and Dhir, Neil and Girolami, Mark and Damoulas, Theodoros},
	month = jan,
	year = {2024},
	pages = {95--109},
}

@article{wald_fusion_1997,
	title = {Fusion of satellite images of different spatial resolutions: {Assessing} the quality of resulting images},
	abstract = {Methods have been proposed to produce multispectral images with enhanced spatial resolution using one or more images of the same scene of better spatial resolution. Assuming that the main concern of the user is the quality of the transformation of the multispectral content when increasing the spatial resolution, this paper defines the properties of such enhanced multispectral images. It then proposes both a formal approach and some criteria to provide a quantitative assessment of the spectral quality of these products. Five sets of criteria are defined. They measure the pe\$ormance of a method to synthesize the radiometry in a single spectral band as well as the multispectral information when increasing the spatial resolution. The influence of the type of landscape present in the scene upon the assessment of the quality is underlined, as well as its dependence with scale. The whole approach is illustrated by the case of a SPOT image and three different standard methods to enhance the spatial resolution.},
	journal = {Photogrammetric Engineering and Remote Sensing},
	author = {Wald, L. and Ranchin, T. and Mangolini, Marc},
	year = {1997},
}

@article{vivone_critical_2015,
	title = {A {Critical} {Comparison} {Among} {Pansharpening} {Algorithms}},
	volume = {53},
	doi = {10.1109/TGRS.2014.2361734},
	abstract = {Pansharpening aims at fusing a multispectral and a panchromatic image, featuring the result of the processing with the spectral resolution of the former and the spatial resolution of the latter. In the last decades, many algorithms addressing this task have been presented in the literature. However, the lack of universally recognized evaluation criteria, available image data sets for benchmarking, and standardized implementations of the algorithms makes a thorough evaluation and comparison of the different pansharpening techniques difficult to achieve. In this paper, the authors attempt to fill this gap by providing a critical description and extensive comparisons of some of the main state-of-the-art pansharpening methods. In greater details, several pansharpening algorithms belonging to the component substitution or multiresolution analysis families are considered. Such techniques are evaluated through the two main protocols for the assessment of pansharpening results, i.e., based on the full- and reduced-resolution validations. Five data sets acquired by different satellites allow for a detailed comparison of the algorithms, characterization of their performances with respect to the different instruments, and consistency of the two validation procedures. In addition, the implementation of all the pansharpening techniques considered in this paper and the framework used for running the simulations, comprising the two validation procedures and the main assessment indexes, are collected in a MATLAB toolbox that is made available to the community.},
	number = {5},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Vivone, Gemine and Alparone, Luciano and Chanussot, Jocelyn and Dalla Mura, Mauro and Garzelli, Andrea and Licciardi, Giorgio A. and Restaino, Rocco and Wald, Lucien},
	month = may,
	year = {2015},
	keywords = {Algorithm design and analysis, Benchmarking, Indexes, Principal component analysis, Spatial resolution, Transforms, Vectors, component substitution (CS), multiresolution analysis (MRA), multispectral (MS) pansharpening, quality assessment, very high-resolution optical images},
	pages = {2565--2586},
}

@inproceedings{venkatakrishnan_plug-and-play_2013,
	title = {Plug-and-{Play} priors for model based reconstruction},
	doi = {10.1109/GlobalSIP.2013.6737048},
	abstract = {Model-based reconstruction is a powerful framework for solving a variety of inverse problems in imaging. In recent years, enormous progress has been made in the problem of denoising, a special case of an inverse problem where the forward model is an identity operator. Similarly, great progress has been made in improving model-based inversion when the forward model corresponds to complex physical measurements in applications such as X-ray CT, electron-microscopy, MRI, and ultrasound, to name just a few. However, combining state-of-the-art denoising algorithms (i.e., prior models) with state-of-the-art inversion methods (i.e., forward models) has been a challenge for many reasons. In this paper, we propose a flexible framework that allows state-of-the-art forward models of imaging systems to be matched with state-of-the-art priors or denoising models. This framework, which we term as Plug-and-Play priors, has the advantage that it dramatically simplifies software integration, and moreover, it allows state-of-the-art denoising methods that have no known formulation as an optimization problem to be used. We demonstrate with some simple examples how Plug-and-Play priors can be used to mix and match a wide variety of existing denoising models with a tomographic forward model, thus greatly expanding the range of possible problem solutions.},
	booktitle = {2013 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing}},
	author = {Venkatakrishnan, Singanallur V. and Bouman, Charles A. and Wohlberg, Brendt},
	month = dec,
	year = {2013},
	pages = {945--948},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{ulyanov_deep_2018,
	title = {Deep {Image} {Prior}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	year = {2018},
	pages = {9446--9454},
}

@misc{uezato_guided_2020,
	title = {Guided {Deep} {Decoder}: {Unsupervised} {Image} {Pair} {Fusion}},
	shorttitle = {Guided {Deep} {Decoder}},
	doi = {10.48550/arXiv.2007.11766},
	abstract = {The fusion of input and guidance images that have a tradeoff in their information (e.g., hyperspectral and RGB image fusion or pansharpening) can be interpreted as one general problem. However, previous studies applied a task-specific handcrafted prior and did not address the problems with a unified approach. To address this limitation, in this study, we propose a guided deep decoder network as a general prior. The proposed network is composed of an encoder-decoder network that exploits multi-scale features of a guidance image and a deep decoder network that generates an output image. The two networks are connected by feature refinement units to embed the multi-scale features of the guidance image into the deep decoder network. The proposed network allows the network parameters to be optimized in an unsupervised way without training data. Our results show that the proposed network can achieve state-of-the-art performance in various image fusion problems.},
	publisher = {arXiv},
	author = {Uezato, Tatsumi and Hong, Danfeng and Yokoya, Naoto and He, Wei},
	month = jul,
	year = {2020},
	note = {arXiv:2007.11766 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{tao_3d_2002,
	title = {{3D} {Reconstruction} methods based on {Rational} {Function} {Model}},
	volume = {68},
	abstract = {The rational function model (BFM) is an alternative sensor model allowing users to perform photogrammetric processing. The RFM has been used as a replacement sensor model in some commercial photogrammetric systems due to its capability of maintaining the accuracy of the physical sensor models and its generic characteristic of supporting sensor-independent photogrammetric processing. With RFM parameters provided, end users are able to perform photogrammetric processing including ortho-rectification, 3D reconstruction, and DEM generation with an absence of the physical sensor model. In this research, we investigate two methods for BFM-based 3D reconstruction, the inverse RFM method and the forward RFM method. Detailed derivations of the algorithmic procedure are described. The emphasis is placed on the comparison of these two reconstruction methods. Experimental results show that the forward RFM can achieve a better reconstruction accuracy. Finally, real Ikonos stereo pairs were employed to verify the applicability and the performance of the reconstruction method.},
	journal = {Photogrammetric Engineering and Remote Sensing},
	author = {Tao, Chao and Hu, Yong},
	month = jul,
	year = {2002},
	pages = {705--714},
}

@article{tachella_sensing_2023,
	title = {Sensing {Theorems} for {Unsupervised} {Learning} in {Linear} {Inverse} {Problems}},
	volume = {24},
	abstract = {Solving an ill-posed linear inverse problem requires knowledge about the underlying signal model. In many applications, this model is a priori unknown and has to be learned from data. However, it is impossible to learn the model using observations obtained via a single incomplete measurement operator, as there is no information about the signal model in the nullspace of the operator, resulting in a chicken-and-egg problem: to learn the model we need reconstructed signals, but to reconstruct the signals we need to know the model. Two ways to overcome this limitation are using multiple measurement operators or assuming that the signal model is invariant to a certain group action. In this paper, we present necessary and sufficient sensing conditions for learning the signal model from measurement data alone which only depend on the dimension of the model and the number of operators or properties of the group action that the model is invariant to. As our results are agnostic of the learning algorithm, they shed light into the fundamental limitations of learning from incomplete data and have implications in a wide range set of practical algorithms, such as dictionary learning, matrix completion and deep neural networks.},
	number = {39},
	journal = {Journal of Machine Learning Research},
	author = {Tachella, Julián and Chen, Dongdong and Davies, Mike},
	year = {2023},
	pages = {1--45},
}

@inproceedings{tachella_unsupervised_2022,
	title = {Unsupervised {Learning} to {Solve} {Inverse} {Problems}: {Application} to {Single}-{Pixel} {Imaging}},
	abstract = {In recent years, learning-based approaches have obtained state-of-the-art performance in multiple imaging inverse problems ranging from medical imaging to computational photography. These methods generally require pairs of signals and associated measurements for training. However, in various imaging problems, we usually only have access to compressed measurements of the underlying signals, hindering this learning-based approach. Learning from measurement data only is impossible in general, as the compressed observations do not contain information in the nullspace of the forward sensing operator. The recent equivariant imaging framework overcomes this limitation by exploiting the invariance to transformations (translations, rotations, etc.) present in natural signals. In this paper, we leverage this novel unsupervised learning framework for reconstructing single-pixel imaging data from compressed measurements alone. A series of experiments show that the proposed method performs comparably to the standard supervised approach.},
	language = {en},
	booktitle = {{XXVIIIème} {Colloque} {Francophone} de {Traitement} du {Signal} et des {Images} ({GRETSI} 2022)},
	author = {Tachella, Julian and Chen, Dongdong and Davies, Mike},
	month = sep,
	year = {2022},
}

@misc{szymanowicz_splatter_2024,
	title = {Splatter {Image}: {Ultra}-{Fast} {Single}-{View} {3D} {Reconstruction}},
	shorttitle = {Splatter {Image}},
	doi = {10.48550/arXiv.2312.13150},
	abstract = {We introduce the {\textbackslash}method, an ultra-efficient approach for monocular 3D object reconstruction. Splatter Image is based on Gaussian Splatting, which allows fast and high-quality reconstruction of 3D scenes from multiple images. We apply Gaussian Splatting to monocular reconstruction by learning a neural network that, at test time, performs reconstruction in a feed-forward manner, at 38 FPS. Our main innovation is the surprisingly straightforward design of this network, which, using 2D operators, maps the input image to one 3D Gaussian per pixel. The resulting set of Gaussians thus has the form an image, the Splatter Image. We further extend the method take several images as input via cross-view attention. Owning to the speed of the renderer (588 FPS), we use a single GPU for training while generating entire images at each iteration to optimize perceptual metrics like LPIPS. On several synthetic, real, multi-category and large-scale benchmark datasets, we achieve better results in terms of PSNR, LPIPS, and other metrics while training and evaluating much faster than prior works. Code, models, demo and more results are available at https://szymanowiczs.github.io/splatter-image.},
	publisher = {arXiv},
	author = {Szymanowicz, Stanislaw and Rupprecht, Christian and Vedaldi, Andrea},
	month = apr,
	year = {2024},
	note = {arXiv:2312.13150 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{sun_unsupervised_2022,
	title = {Unsupervised {Spatial}–{Spectral} {Network} {Learning} for {Hyperspectral} {Compressive} {Snapshot} {Reconstruction}},
	volume = {60},
	doi = {10.1109/TGRS.2021.3100393},
	abstract = {Hyperspectral compressive imaging takes advantage of compressive sensing theory to achieve coded aperture snapshot measurement without temporal scanning, and the entire 3-D spatial–spectral data is captured by a 2-D projection during a single integration period. Its core issue is how to reconstruct the underlying hyperspectral image (HSI) using compressive sensing reconstruction algorithms. Due to the diversity in the spectral response characteristics and wavelength range of different spectral imaging devices, previous works are often inadequate to capture complex spectral variations or lack the adaptive capacity to new hyperspectral imagers. In order to address these issues, we propose an unsupervised spatial–spectral network to reconstruct HSIs only from the compressive snapshot measurement. The proposed network acts as a conditional generative model conditioned on the snapshot measurement, and it exploits the spatial–spectral attention module to capture the joint spatial–spectral correlation of HSIs. The network parameters are optimized to make sure that the network output can closely match the given snapshot measurement according to the imaging model, thus the proposed network can adapt to different imaging settings, which can inherently enhance the applicability of the network. Extensive experiments upon multiple datasets demonstrate that our network can achieve better reconstruction results than the state-of-the-art methods.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Sun, Yubao and Yang, Ying and Liu, Qingshan and Kankanhalli, Mohan},
	year = {2022},
	keywords = {Apertures, Encoding, Generative network, Hyperspectral imaging, Image reconstruction, Imaging, TV, Wavelength measurement, hyperspectral compressive imaging, spatial–spectral attention, unsupervised network learning},
	pages = {1--14},
}

@article{sidiropoulos_automatic_2018,
	title = {Automatic {Coregistration} and orthorectification ({ACRO}) and subsequent mosaicing of {NASA} high-resolution imagery over the {Mars} {MC11} quadrangle, using {HRSC} as a baseline},
	volume = {151},
	doi = {10.1016/j.pss.2017.10.012},
	abstract = {This work presents the coregistered, orthorectified and mosaiced high-resolution products of the MC11 quadrangle of Mars, which have been processed using novel, fully automatic, techniques. We discuss the development of a pipeline that achieves fully automatic and parameter independent geometric alignment of high-resolution planetary images, starting from raw input images in NASA PDS format and following all required steps to produce a coregistered geotiff image, a corresponding footprint and useful metadata. Additionally, we describe the development of a radiometric calibration technique that post-processes coregistered images to make them radiometrically consistent. Finally, we present a batch-mode application of the developed techniques over the MC11 quadrangle to validate their potential, as well as to generate end products, which are released to the planetary science community, thus assisting in the analysis of Mars static and dynamic features. This case study is a step towards the full automation of signal processing tasks that are essential to increase the usability of planetary data, but currently, require the extensive use of human resources.},
	journal = {Planetary and Space Science},
	author = {Sidiropoulos, Panagiotis and Muller, Jan-Peter and Watson, Gillian and Michael, Gregory and Walter, Sebastian},
	month = feb,
	year = {2018},
	keywords = {Coregistration, High-resolution imagery, MC11 quadrangle, Mars, Mars orbiters, Mosaicing, Orthorectification},
	pages = {33--42},
}

@misc{shocher_zero-shot_2017,
	title = {"{Zero}-{Shot}" {Super}-{Resolution} using {Deep} {Internal} {Learning}},
	doi = {10.48550/arXiv.1712.06087},
	abstract = {Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the low-resolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce "Zero-Shot" SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method.},
	publisher = {arXiv},
	author = {Shocher, Assaf and Cohen, Nadav and Irani, Michal},
	month = dec,
	year = {2017},
	note = {arXiv:1712.06087 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{selva_video_2023,
	title = {Video {Transformers}: {A} {Survey}},
	doi = {10.1109/TPAMI.2023.3243465},
	abstract = {Transformer models have shown great success handling long-range interactions, making them a promising tool for modeling video. However, they lack inductive biases and scale quadratically with input length. These limitations are further exacerbated when dealing with the high dimensionality introduced by the temporal dimension. While there are surveys analyzing the advances of Transformers for vision, none focus on an in-depth analysis of video-specific designs. In this survey, we analyze the main contributions and trends of works leveraging Transformers to model video. Specifically, we delve into how videos are handled at the input level first. Then, we study the architectural changes made to deal with video more efficiently, reduce redundancy, re-introduce useful inductive biases, and capture long-term temporal dynamics. In addition, we provide an overview of different training regimes and explore effective self-supervised learning strategies for video. Finally, we conduct a performance comparison on the most common benchmark for Video Transformers (i.e., action classification), finding them to outperform 3D ConvNets even with less computational complexity.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Selva, Javier and Johansen, Anders S. and Escalera, Sergio and Nasrollahi, Kamal and Moeslund, Thomas B. and Clapés, Albert},
	year = {2023},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--20},
}

@article{schlemper_deep_2018,
	title = {A {Deep} {Cascade} of {Convolutional} {Neural} {Networks} for {Dynamic} {MR} {Image} {Reconstruction}},
	volume = {37},
	doi = {10.1109/TMI.2017.2760978},
	abstract = {Inspired by recent advances in deep learning, we propose a framework for reconstructing dynamic sequences of 2-D cardiac magnetic resonance (MR) images from undersampled data using a deep cascade of convolutional neural networks (CNNs) to accelerate the data acquisition process. In particular, we address the case where data are acquired using aggressive Cartesian undersampling. First, we show that when each 2-D image frame is reconstructed independently, the proposed method outperforms state-of-the-art 2-D compressed sensing approaches, such as dictionary learning-based MR image reconstruction, in terms of reconstruction error and reconstruction speed. Second, when reconstructing the frames of the sequences jointly, we demonstrate that CNNs can learn spatio-temporal correlations efficiently by combining convolution and data sharing approaches. We show that the proposed method consistently outperforms state-of-the-art methods and is capable of preserving anatomical structure more faithfully up to 11-fold undersampling. Moreover, reconstruction is very fast: each complete dynamic sequence can be reconstructed in less than 10 s and, for the 2-D case, each image frame can be reconstructed in 23 ms, enabling real-time applications.},
	number = {2},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Schlemper, Jo and Caballero, Jose and Hajnal, Joseph V. and Price, Anthony N. and Rueckert, Daniel},
	month = feb,
	year = {2018},
	pages = {491--503},
}

@misc{scanvic_self-supervised_2023,
	title = {Self-{Supervised} {Learning} for {Image} {Super}-{Resolution} and {Deblurring}},
	doi = {10.48550/arXiv.2312.11232},
	abstract = {Self-supervised methods have recently proved to be nearly as effective as supervised methods in various imaging inverse problems, paving the way for learning-based methods in scientific and medical imaging applications where ground truth data is hard or expensive to obtain. This is the case in magnetic resonance imaging and computed tomography. These methods critically rely on invariance to translations and/or rotations of the image distribution to learn from incomplete measurement data alone. However, existing approaches fail to obtain competitive performances in the problems of image super-resolution and deblurring, which play a key role in most imaging systems. In this work, we show that invariance to translations and rotations is insufficient to learn from measurements that only contain low-frequency information. Instead, we propose a new self-supervised approach that leverages the fact that many image distributions are approximately scale-invariant, and that can be applied to any inverse problem where high-frequency information is lost in the measurement process. We demonstrate throughout a series of experiments on real datasets that the proposed method outperforms other self-supervised approaches, and obtains performances on par with fully supervised learning.},
	publisher = {arXiv},
	author = {Scanvic, Jérémy and Davies, Mike and Abry, Patrice and Tachella, Julián},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11232 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{rout_solving_2023,
	title = {Solving {Linear} {Inverse} {Problems} {Provably} via {Posterior} {Sampling} with {Latent} {Diffusion} {Models}},
	abstract = {We present the first framework to solve linear inverse problems leveraging pre-trained {\textbackslash}textit\{latent\} diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to {\textbackslash}textit\{pixel-space\} diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.},
	booktitle = {Thirty-seventh {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Rout, Litu and Raoof, Negin and Daras, Giannis and Caramanis, Constantine and Dimakis, Alex and Shakkottai, Sanjay},
	month = nov,
	year = {2023},
}

@inproceedings{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	doi = {10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@misc{romano_little_2017,
	title = {The {Little} {Engine} that {Could}: {Regularization} by {Denoising} ({RED})},
	shorttitle = {The {Little} {Engine} that {Could}},
	doi = {10.48550/arXiv.1611.02862},
	abstract = {Removal of noise from an image is an extensively studied problem in image processing. Indeed, the recent advent of sophisticated and highly effective denoising algorithms lead some to believe that existing methods are touching the ceiling in terms of noise removal performance. Can we leverage this impressive achievement to treat other tasks in image processing? Recent work has answered this question positively, in the form of the Plug-and-Play Prior (\$P{\textasciicircum}3\$) method, showing that any inverse problem can be handled by sequentially applying image denoising steps. This relies heavily on the ADMM optimization technique in order to obtain this chained denoising interpretation. Is this the only way in which tasks in image processing can exploit the image denoising engine? In this paper we provide an alternative, more powerful and more flexible framework for achieving the same goal. As opposed to the \$P{\textasciicircum}3\$ method, we offer Regularization by Denoising (RED): using the denoising engine in defining the regularization of the inverse problem. We propose an explicit image-adaptive Laplacian-based regularization functional, making the overall objective functional clearer and better defined. With a complete flexibility to choose the iterative optimization procedure for minimizing the above functional, RED is capable of incorporating any image denoising algorithm, treat general inverse problems very effectively, and is guaranteed to converge to the globally optimal result. We test this approach and demonstrate state-of-the-art results in the image deblurring and super-resolution problems.},
	publisher = {arXiv},
	author = {Romano, Yaniv and Elad, Michael and Milanfar, Peyman},
	month = sep,
	year = {2017},
	note = {arXiv:1611.02862 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Mathematics - Numerical Analysis},
}

@article{rizeei_urban_2019,
	title = {Urban {Mapping} {Accuracy} {Enhancement} in {High}-{Rise} {Built}-{Up} {Areas} {Deployed} by {3D}-{Orthorectification} {Correction} from {WorldView}-3 and {LiDAR} {Imageries}},
	volume = {11},
	doi = {10.3390/rs11060692},
	abstract = {Orthorectification is an important step in generating accurate land use/land cover (LULC) from satellite imagery, particularly in urban areas with high-rise buildings. Such buildings generally appear as oblique shapes on very-high-resolution (VHR) satellite images, which reflect a bigger area of coverage than the real built-up area on LULC mapping. This drawback can cause not only uncertainties in urban mapping and LULC classification, but can also result in inaccurate urban change detection. Overestimating volume or area of high-rise buildings has a negative impact on computing the exact amount of environmental heat and emission. Hence, in this study, we propose a method of orthorectfiying VHR WorldView-3 images by integrating light detection and ranging (LiDAR) data to overcome the aforementioned problems. A 3D rational polynomial coefficient (RPC) model was proposed with respect to high-accuracy ground control points collected from the LiDAR data derived from the digital surface model. Multiple probabilities for generating an orthrorectified image from WV-3 were assessed using 3D RCP model to achieve the optimal combination technique, with low vertical and horizontal errors. Ground control point (GCPs) collection is sensitive to variation in number and data collection pattern. These steps are important in orthorectification because they can cause the morbidity of a standard equation, thereby interrupting the stability of 3D RCP model by reducing the accuracy of the orthorectified image. Hence, we assessed the maximum possible scenarios of resampling and ground control point collection techniques to bridge the gap. Results show that the 3D RCP model accurately orthorectifies the VHR satellite image if 20 to 100 GCPs were collected by convenience pattern. In addition, cubic conventional resampling algorithm improved the precision and smoothness of the orthorectified image. According to the root mean square error, the proposed combination technique enhanced the vertical and horizontal accuracies of the geo-positioning process to up to 0.8 and 1.8 m, respectively. Such accuracy is considered very high in orthorectification. The proposed technique is easy to use and can be replicated for other VHR satellite and aerial photos.},
	number = {6},
	journal = {Remote Sensing},
	author = {Rizeei, Hossein Mojaddadi and Pradhan, Biswajeet},
	month = jan,
	year = {2019},
	keywords = {3D RPC, GIS, LiDAR, Orthorectification, Worldview 3, obliqueness, remote sensing},
	pages = {692},
}

@article{rilling_multilattice_2013,
	title = {Multilattice sampling strategies for region of interest dynamic {MRI}},
	volume = {70},
	doi = {10.1002/mrm.24471},
	abstract = {A multilattice sampling approach is proposed for dynamic MRI with Cartesian trajectories. It relies on the use of sampling patterns composed of several different lattices and exploits an image model where only some parts of the image are dynamic, whereas the rest is assumed static. Given the parameters of such an image model, the methodology followed for the design of a multilattice sampling pattern adapted to the model is described. The multi-lattice approach is compared to single-lattice sampling, as used by traditional acceleration methods such as UNFOLD (UNaliasing by Fourier-Encoding the Overlaps using the temporal Dimension) or k-t BLAST, and random sampling used by modern compressed sensing-based methods. On the considered image model, it allows more flexibility and higher accelerations than lattice sampling and better performance than random sampling. The method is illustrated on a phase-contrast carotid blood velocity mapping MR experiment. Combining the multilattice approach with the KEYHOLE technique allows up to 12× acceleration factors. Simulation and in vivo undersampling results validate the method. Compared to lattice and random sampling, multilattice sampling provides significant gains at high acceleration factors. Magn Reson Med 70:392–403, 2013. © 2012 Wiley Periodicals, Inc.},
	number = {2},
	journal = {Magnetic Resonance in Medicine},
	author = {Rilling, Gabriel and Tao, Yuehui and Marshall, Ian and Davies, Mike E.},
	year = {2013},
	keywords = {carotid bloodvelocity mapping, compressed sensing, dynamic MRI, lattice sampling, multilattice, sampling pattern design, sparsity},
	pages = {392--403},
}

@article{renza_new_2013,
	title = {A {New} {Approach} to {Change} {Detection} in {Multispectral} {Images} by {Means} of {ERGAS} {Index}},
	doi = {10.1109/LGRS.2012.2193372},
	abstract = {In this letter, we propose a novel method for unsupervised change detection (CD) in multitemporal Erreur Relative Globale Adimensionnelle de Synthese (ERGAS) satellite images by using the relative dimensionless global error in synthesis index locally. In order to obtain the change image, the index is calculated around a pixel neighborhood (3 {\textbackslash}times 3 window) processing simultaneously all the spectral bands available. With the objective of finding the binary change masks, six thresholding methods are selected. A comparison between the proposed method and the change vector analysis method is reported. The accuracy CD showed in the experimental results demonstrates the effectiveness of the proposed method.},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Renza, Diego and Martinez, Estibaliz and Arquero, Agueda},
	month = jan,
	year = {2013},
	keywords = {Accuracy, Image fusion, Indexes, Local relative dimensionless global error in synthesis (ERGAS), Radiometry, Remote sensing, Satellites, Spatial resolution, multitemporal optical satellite images, thresholding, unsupervised change detection (CD)},
}

@article{qin_convolutional_2019,
	title = {Convolutional {Recurrent} {Neural} {Networks} for {Dynamic} {MR} {Image} {Reconstruction}},
	volume = {38},
	doi = {10.1109/TMI.2018.2863670},
	abstract = {Accelerating the data acquisition of dynamic magnetic resonance imaging leads to a challenging ill-posed inverse problem, which has received great interest from both the signal processing and machine learning communities over the last decades. The key ingredient to the problem is how to exploit the temporal correlations of the MR sequence to resolve aliasing artifacts. Traditionally, such observation led to a formulation of an optimization problem, which was solved using iterative algorithms. Recently, however, deep learning-based approaches have gained significant popularity due to their ability to solve general inverse problems. In this paper, we propose a unique, novel convolutional recurrent neural network architecture which reconstructs high quality cardiac MR images from highly undersampled k-space data by jointly exploiting the dependencies of the temporal sequences as well as the iterative nature of the traditional optimization algorithms. In particular, the proposed architecture embeds the structure of the traditional iterative algorithms, efficiently modeling the recurrence of the iterative reconstruction stages by using recurrent hidden connections over such iterations. In addition, spatio–temporal dependencies are simultaneously learnt by exploiting bidirectional recurrent hidden connections across time sequences. The proposed method is able to learn both the temporal dependence and the iterative reconstruction process effectively with only a very small number of parameters, while outperforming current MR reconstruction methods in terms of reconstruction accuracy and speed.},
	number = {1},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Qin, Chen and Schlemper, Jo and Caballero, Jose and Price, Anthony N. and Hajnal, Joseph V. and Rueckert, Daniel},
	month = jan,
	year = {2019},
	pages = {280--290},
}

@article{park_deep_2022,
	title = {Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy},
	volume = {13},
	doi = {10.1038/s41467-022-30949-6},
	abstract = {Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution, in which the axial resolution is inferior to the lateral resolution. To address this problem, we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target images, our method greatly reduces the effort to be put into practice as the training of a network requires only a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport-driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in the lateral image plane and low-resolution 2D images in other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution but also restores suppressed visual details between the imaging planes and removes imaging artifacts.},
	number = {1},
	journal = {Nature Communications},
	author = {Park, Hyoungjun and Na, Myeongsu and Kim, Bumju and Park, Soohyun and Kim, Ki Hean and Chang, Sunghoe and Ye, Jong Chul},
	month = jun,
	year = {2022},
	keywords = {Confocal microscopy, Imaging and sensing, Light-sheet microscopy, Machine learning},
	pages = {3297},
}

@inproceedings{pajot_unsupervised_2018,
	title = {Unsupervised {Adversarial} {Image} {Reconstruction}},
	abstract = {We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting. Typically, we consider situations where there is little to no background knowledge on the structure of the underlying signal, no access to signal-measurement pairs, nor even unpaired signal-measurement data. The only available information is provided by the observations and the measurement process statistics. We cast the problem as finding the {\textbackslash}textit\{maximum a posteriori\} estimate of the signal given each measurement, and propose a general framework for the reconstruction problem. We use a formulation of generative adversarial networks, where the generator takes as input a corrupted observation in order to produce realistic reconstructions, and add a penalty term tying the reconstruction to the associated observation. We evaluate our reconstructions on several image datasets with different types of corruptions. The proposed approach yields better results than alternative baselines, and comparable performance with model variants trained with additional supervision.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Pajot, Arthur and Bezenac, Emmanuel de and Gallinari, Patrick},
	month = sep,
	year = {2018},
}

@inproceedings{padwick_worldview-2_2010,
	title = {{WorldView}-2 {Pan}-sharpening},
	volume = {2630},
	abstract = {A novel pan-sharpening algorithm designed for sharpening WorldView-2 (WV-2) imagery is presented. The WV-2 satellite was launched by DigitalGlobe on Oct 8 2009. WV-2 has 8 spectral bands covering the range from 400nm1050nm and a panchromatic band covering 450nm-800nm. The proposed pan-sharpening algorithm accepts any number of input bands, and quantitative color comparisons are performed using different band combinations from original multi-spectral file. The pan-sharpening algorithm is found to produce acceptable color recovery and spatial recovery for a wide variety of input scenes.},
	booktitle = {Proceedings of the {ASPRS} 2010 {Annual} {Conference}},
	author = {Padwick, C. and Scientist, P. and Deskevich, Michael P. and Smallwood, Scott R.},
	year = {2010},
	pages = {1--14},
}

@inproceedings{nguyen_self-supervised_2022,
	title = {Self-{Supervised} {Super}-{Resolution} for {Multi}-{Exposure} {Push}-{Frame} {Satellites}},
	doi = {10.1109/CVPR52688.2022.00190},
	abstract = {Modern Earth observation satellites capture multi-exposure bursts of push-frame images that can be super-resolved via computational means. In this work, we propose a super-resolution method for such multi-exposure sequences, a problem that has received very little attention in the literature. The proposed method can handle the signal-dependent noise in the inputs, process sequences of any length, and be robust to inaccuracies in the exposure times. Furthermore, it can be trained end-to-end with self-supervision, without requiring ground truth high resolution frames, which makes it especially suited to handle real data. Central to our method are three key contributions: i) a base-detail decomposition for handling errors in the exposure times, ii) a noise-level-aware feature encoding for improved fusion of frames with varying signal-to-noise ratio and iii) a permutation invariant fusion strategy by temporal pooling operators. We evaluate the proposed method on synthetic and real data and show that it outperforms by a significant margin existing single-exposure approaches that we adapted to the multi-exposure case.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Nguyen, Ngoc Long and Anger, Jérémy and Davy, Axel and Arias, Pablo and Facciolo, Gabriele},
	month = jun,
	year = {2022},
	keywords = {Computational photography, Earth, Encoding, Low-level vision, Pattern recognition, Photogrammetry and remote sensing, Photography, Satellites, Self-\& semi-\& meta- \& unsupervised learning, Signal resolution, Superresolution},
	pages = {1848--1858},
}

@article{nag_multispectral_2017,
	title = {Multispectral {Snapshot} {Imagers} {Onboard} {Small} {Satellite} {Formations} for {Multi}-{Angular} {Remote} {Sensing}},
	volume = {17},
	doi = {10.1109/JSEN.2017.2717384},
	abstract = {Multispectral snapshot imagers are capable of producing 2-D spatial images with a single exposure at selected, numerous wavelengths using the same camera, therefore, operate differently from push broom or whiskbroom imagers. They are payloads of choice in multi-angular, multi-spectral imaging missions that use small satellites flying in controlled formation, to retrieve Earth science measurements dependent on the target's bidirectional reflectance-distribution function. Narrow fields of view are needed to capture images with moderate spatial resolution. This paper quantifies the dependencies of the imager's optical system, spectral elements, and camera on the requirements of the formation mission and their impact on performance metrics, such as spectral range, swath, and signal-to-noise ratio (SNR). All variables and metrics have been generated from a comprehensive, payload design tool. The baseline optical parameters selected (a diameter of 7 cm, a focal length of 10.5 cm, a pixel size of 20 μm, and a field of view of 1.15°) and snapshot imaging technologies are available. The spectral components shortlisted were waveguide spectrometers, acoustooptic tunable filters (AOTF), electronically actuated Fabry-Perot interferometers, and integral field spectrographs. Qualitative evaluation favored AOTFs, because of their low weight, small size, and flight heritage. Quantitative analysis showed that the waveguide spectrometers perform better in terms of achievable swath (10-90 km) and SNR ({\textgreater}20) for 86 wavebands, but the data volume generated will need very high bandwidth communication to downlink. AOTFs meet the external data volume caps well as the minimum spectral (wavebands) and radiometric (SNR) requirements, therefore, are found to be currently feasible and design changes to improve swath suggested.},
	number = {16},
	journal = {IEEE Sensors Journal},
	author = {Nag, Sreeja and Hewagama, Tilak and Georgiev, Georgi T. and Pasquale, Bert and Aslam, Shahid and Gatebe, Charles K.},
	month = aug,
	year = {2017},
	keywords = {BRDF, Imaging, Instruments, Measurement, NASA, Payloads, Satellite broadcasting, Satellites, Small satellites, imager design},
	pages = {5252--5268},
}

@inproceedings{murthy_skysat-1_2014,
	title = {{SkySat}-1: very high-resolution imagery from a small satellite},
	volume = {9241},
	doi = {10.1117/12.2074163},
	abstract = {This paper presents details of the SkySat-1 mission, which is the first microsatellite-class commercial earth- observation system to generate sub-meter resolution panchromatic imagery, in addition to sub-meter resolution 4-band pan-sharpened imagery. SkySat-1 was built and launched for an order of magnitude lower cost than similarly performing missions. The low-cost design enables the deployment of a large imaging constellation that can provide imagery with both high temporal resolution and high spatial resolution. One key enabler of the SkySat-1 mission was simplifying the spacecraft design and instead relying on ground- based image processing to achieve high-performance at the system level. The imaging instrument consists of a custom-designed high-quality optical telescope and commercially-available high frame rate CMOS image sen- sors. While each individually captured raw image frame shows moderate quality, ground-based image processing algorithms improve the raw data by combining data from multiple frames to boost image signal-to-noise ratio (SNR) and decrease the ground sample distance (GSD) in a process Skybox calls \&\#100;igital TDI". Careful qual-ity assessment and tuning of the spacecraft, payload, and algorithms was necessary to generate high-quality panchromatic, multispectral, and pan-sharpened imagery. Furthermore, the framing sensor configuration en- abled the first commercial High-Definition full-frame rate panchromatic video to be captured from space, with approximately 1 meter ground sample distance. Details of the SkySat-1 imaging instrument and ground-based image processing system are presented, as well as an overview of the work involved with calibrating and validating the system. Examples of raw and processed imagery are shown, and the raw imagery is compared to pre-launch simulated imagery used to tune the image processing algorithms.},
	booktitle = {Sensors, {Systems}, and {Next}-{Generation} {Satellites} {XVIII}},
	publisher = {SPIE},
	author = {Murthy, Kiran and Shearn, Michael and Smiley, Byron D. and Chau, Alexandra H. and Levine, Josh and Robinson, M. Dirk},
	month = oct,
	year = {2014},
	pages = {367--378},
}

@misc{moran_noisier2noise_2019,
	title = {{Noisier2Noise}: {Learning} to {Denoise} from {Unpaired} {Noisy} {Data}},
	shorttitle = {{Noisier2Noise}},
	doi = {10.48550/arXiv.1910.11908},
	abstract = {We present a method for training a neural network to perform image denoising without access to clean training examples or access to paired noisy training examples. Our method requires only a single noisy realization of each training example and a statistical model of the noise distribution, and is applicable to a wide variety of noise models, including spatially structured noise. Our model produces results which are competitive with other learned methods which require richer training data, and outperforms traditional non-learned denoising methods. We present derivations of our method for arbitrary additive noise, an improvement specific to Gaussian additive noise, and an extension to multiplicative Bernoulli noise.},
	publisher = {arXiv},
	author = {Moran, Nick and Schmidt, Dan and Zhong, Yu and Coady, Patrick},
	month = oct,
	year = {2019},
	note = {arXiv:1910.11908 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{monga_algorithm_2021,
	title = {Algorithm {Unrolling}: {Interpretable}, {Efficient} {Deep} {Learning} for {Signal} and {Image} {Processing}},
	volume = {38},
	doi = {10.1109/MSP.2020.3016905},
	abstract = {Deep neural networks provide unprecedented performance gains in many real-world problems in signal and image processing. Despite these gains, the future development and practical deployment of deep networks are hindered by their black-box nature, i.e., a lack of interpretability and the need for very large training sets. An emerging technique called algorithm unrolling, or unfolding, offers promise in eliminating these issues by providing a concrete and systematic connection between iterative algorithms that are widely used in signal processing and deep neural networks. Unrolling methods were first proposed to develop fast neural network approximations for sparse coding. More recently, this direction has attracted enormous attention, and it is rapidly growing in both theoretic investigations and practical applications. The increasing popularity of unrolled deep networks is due, in part, to their potential in developing efficient, high-performance (yet interpretable) network architectures from reasonably sized training sets.},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Monga, Vishal and Li, Yuelong and Eldar, Yonina C.},
	month = mar,
	year = {2021},
	pages = {18--44},
}

@article{meng_snapshot_2020,
	title = {Snapshot multispectral endomicroscopy},
	volume = {45},
	doi = {10.1364/OL.393213},
	abstract = {Multispectral endomicroscopy provides tissue functional information in addition to structural information for accurate disease diagnosis. In this Letter, we propose a snapshot multispectral endomicroscope that employs a fiber bundle to deliver an in-body tissue spatial–spectral datastream to an external compressive spectral imager. Equipped with an end-to-end deep-learning-based reconstruction algorithm, we are able to capture tissue multispectral data in video rates and reconstruct high-resolution multispectral images with up to 24 spectral channels in near-real time.},
	number = {14},
	journal = {Optics Letters},
	author = {Meng, Ziyi and Qiao, Mu and Ma, Jiawei and Yu, Zhenming and Xu, Kun and Yuan, Xin},
	month = jul,
	year = {2020},
	keywords = {Fiber bundles, Imaging systems, Imaging techniques, Microlens arrays, Multispectral imaging, Spatial resolution},
	pages = {3897--3900},
}

@article{meng_large-scale_2021,
	title = {A {Large}-{Scale} {Benchmark} {Data} {Set} for {Evaluating} {Pansharpening} {Performance}: {Overview} and {Implementation}},
	doi = {10.1109/MGRS.2020.2976696},
	abstract = {Pansharpening aims to sharpen a lowspatial-resolution (LR) multispectral (MS) image using a high-spatial-resolution (HR) panchromatic (Pan) image to obtain the HR MS image. It has been a fundamental and active research topic in remote sensing, and pansharpening methods have been developed for nearly 40 years. While the performance evaluation of pansharpening methods is still based on a small number of individual images, datadriven pansharpening approaches are attracting increasing attention. However, few publicly available benchmark data sets for pansharpening are available, especially large-scale ones. This has been a serious limitation for the future development of pansharpening methods.},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Meng, Xiangchao and Xiong, Yiming and Shao, Feng and Shen, Huanfeng and Sun, Weiwei and Yang, Gang and Yuan, Qiangqiang and Fu, Randi and Zhang, Hongyan},
	month = mar,
	year = {2021},
	keywords = {Benchmark testing, Multiresolution analysis, Remote sensing, Satellites, Spatial resolution},
}

@article{mann_video_1997,
	title = {Video orbits of the projective group a simple approach to featureless estimation of parameters},
	volume = {6},
	doi = {10.1109/83.623191},
	abstract = {We present direct featureless methods for estimating the eight parameters of an "exact" projective (homographic) coordinate transformation to register pairs of images, together with the application of seamlessly combining a plurality of images of the same scene, resulting in a single image (or new image sequence) of greater resolution or spatial extent. The approach is "exact" for two cases of static scenes: (1) images taken from the same location of an arbitrary three-dimensional (3-D) scene, with a camera that is free to pan, tilt, rotate about its optical axis, and zoom, or (2) images of a flat scene taken from arbitrary locations. The featureless projective approach generalizes interframe camera motion estimation methods that have previously used a camera model (which lacks the degrees of freedom to "exactly" characterize such phenomena as camera pan and tilt) and/or which have relied upon finding points of correspondence between the image frames. The featureless projective approach, which operates directly on the image pixels, is shown to be superior in accuracy and the ability to enhance the resolution. The proposed methods work well on image data collected from both good-quality and poor-quality video under a wide variety of conditions (sunny, cloudy, day, night). These new fully automatic methods are also shown to be robust to deviations from the assumptions of static scene and no parallax.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Mann, S. and Picard, R.W.},
	month = sep,
	year = {1997},
	keywords = {Cameras, Image resolution, Image sequences, Layout, Motion estimation, Orbits, Parameter estimation, Pixel, Robustness, Spatial resolution},
	pages = {1281--1295},
}

@article{ma_pan-gan_2020,
	title = {Pan-{GAN}: {An} unsupervised pan-sharpening method for remote sensing image fusion},
	volume = {62},
	doi = {10.1016/j.inffus.2020.04.006},
	abstract = {Pan-sharpening in remote sensing image fusion refers to obtaining multi-spectral images of high-resolution by fusing panchromatic images and multi-spectral images of low-resolution. Recently, convolution neural network (CNN)-based pan-sharpening methods have achieved the state-of-the-art performance. Even though, two problems still remain. On the one hand, the existing CNN-based strategies require supervision, where the low-resolution multi-spectral image is obtained by simply blurring and down-sampling the high-resolution one. On the other hand, they typically ignore rich spatial information of panchromatic images. To address these issues, we propose a novel unsupervised framework for pan-sharpening based on a generative adversarial network, termed as Pan-GAN, which does not rely on the so-called ground-truth during network training. In our method, the generator separately establishes the adversarial games with the spectral discriminator and the spatial discriminator, so as to preserve the rich spectral information of multi-spectral images and the spatial information of panchromatic images. Extensive experiments are conducted to demonstrate the effectiveness of the proposed Pan-GAN compared with other state-of-the-art pan-sharpening approaches. Our Pan-GAN has shown promising performance in terms of qualitative visual effects and quantitative evaluation metrics.},
	journal = {Information Fusion},
	author = {Ma, Jiayi and Yu, Wei and Chen, Chen and Liang, Pengwei and Guo, Xiaojie and Jiang, Junjun},
	month = oct,
	year = {2020},
	keywords = {Deep learning, Generative adversarial network, Image fusion, Pan-sharpening, Unsupervised learning},
	pages = {110--120},
}

@article{luo_pansharpening_2020,
	title = {Pansharpening via {Unsupervised} {Convolutional} {Neural} {Networks}},
	volume = {13},
	doi = {10.1109/JSTARS.2020.3008047},
	abstract = {Pansharpening is normally utilized to take full advantage of all the available spectral and spatial information that are derived from a low-spatial-resolution multispectral (MS) image and its associated high-spatial-resolution (HR) panchromatic (PAN) image, respectively, producing a fused MS image with high spectral and spatial resolutions. Many methods have been recently developed based on convolutional neural networks (CNNs) for the pansharpening task, but most of them still have some drawbacks: 1) The information cannot efficiently flow in their simple stacked convolutional architectures, thereby hindering the representation ability of the networks. 2) They are commonly trained using supervised learning, which does not only require an extra effort to produce the simulated training data, but can also lead to scale-related problems in the fusion results. In this article, we propose a novel unsupervised CNN-based pansharpening method to overcome these limitations. Specifically, we design an iterative network architecture, in which a PAN-guided strategy and a set of skip connections are adopted to continuously extract and fuse the features from the input, thus enhancing the information reuse and transmission. Besides, we propose a new loss function for unsupervised training in which the relationships between the input MS and PAN images and the fused MS image are used to design the spatial constrains and spectral consistency, respectively. The typical quality index with no-reference is also added to this function to further adjust the spectral and spatial qualities. The designed loss function allows the network to be learned only on input images, without any hand-crafted labels (reference HR MS image). We evaluated the effectiveness of our designed network architecture and the combined loss function, and the experiments testify that our unsupervised strategy can also obtain promising results with minor spectral and spatial distortions compared with other traditional and supervised methods.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Luo, Shuyue and Zhou, Shangbo and Feng, Yong and Xie, Jiangan},
	year = {2020},
	keywords = {Convolutional neural networks (CNNs), Distortion, Feature extraction, Fuses, Indexes, Pansharpening, Remote sensing, Training, unsupervised},
	pages = {4295--4310},
}

@inproceedings{liu_deep_2015,
	title = {Deep {Learning} {Face} {Attributes} in the {Wild}},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	year = {2015},
	pages = {3730--3738},
}

@article{liu_sgd-net_2021,
	title = {{SGD}-{Net}: {Efficient} {Model}-{Based} {Deep} {Learning} with {Theoretical} {Guarantees}},
	volume = {7},
	doi = {10.1109/TCI.2021.3085534},
	abstract = {Deep unfolding networks have recently gained popularity in the context of solving imaging inverse problems. However, the computational and memory complexity of data-consistency layers within traditional deep unfolding networks scales with the number of measurements, limiting their applicability to large-scale imaging inverse problems. We propose SGD-Net as a new methodology for improving the efficiency of deep unfolding through stochastic approximations of the data-consistency layers. Our theoretical analysis shows that SGD-Net can be trained to approximate batch deep unfolding networks to an arbitrary precision. Our numerical results on intensity diffraction tomography and sparse-view computed tomography show that SGD-Net can match the performance of the batch network at a fraction of training and testing complexity.},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Liu, Jiaming and Sun, Yu and Gan, Weijie and Xu, Xiaojian and Wohlberg, Brendt and Kamilov, Ulugbek S.},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	pages = {598--610},
}

@inproceedings{liang_swinir_2021,
	title = {{SwinIR}: {Image} {Restoration} {Using} {Swin} {Transformer}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}},
	author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
	year = {2021},
	pages = {1833--1844},
}

@article{li_incorporating_2022,
	title = {Incorporating the image formation process into deep learning improves network performance},
	volume = {19},
	doi = {10.1038/s41592-022-01652-7},
	abstract = {We present Richardson–Lucy network (RLN), a fast and lightweight deep learning method for three-dimensional fluorescence microscopy deconvolution. RLN combines the traditional Richardson–Lucy iteration with a fully convolutional network structure, establishing a connection to the image formation process and thereby improving network performance. Containing only roughly 16,000 parameters, RLN enables four- to 50-fold faster processing than purely data-driven networks with many more parameters. By visual and quantitative analysis, we show that RLN provides better deconvolution, better generalizability and fewer artifacts than other networks, especially along the axial dimension. RLN outperforms classic Richardson–Lucy deconvolution on volumes contaminated with severe out of focus fluorescence or noise and provides four- to sixfold faster reconstructions of large, cleared-tissue datasets than classic multi-view pipelines. We demonstrate RLN’s performance on cells, tissues and embryos imaged with widefield-, light-sheet-, confocal- and super-resolution microscopy.},
	number = {11},
	journal = {Nature Methods},
	author = {Li, Yue and Su, Yijun and Guo, Min and Han, Xiaofei and Liu, Jiamin and Vishwasrao, Harshad D. and Li, Xuesong and Christensen, Ryan and Sengupta, Titas and Moyle, Mark W. and Rey-Suarez, Ivan and Chen, Jiji and Upadhyaya, Arpita and Usdin, Ted B. and Colón-Ramos, Daniel Alfonso and Liu, Huafeng and Wu, Yicong and Shroff, Hari},
	month = nov,
	year = {2022},
	keywords = {Fluorescence imaging, Machine learning},
	pages = {1427--1437},
}

@article{li_simultaneous_2018,
	title = {Simultaneous {Robot}–{World} and {Hand}–{Eye} {Calibration} without a {Calibration} {Object}},
	volume = {18},
	doi = {10.3390/s18113949},
	abstract = {An extended robot–world and hand–eye calibration method is proposed in this paper to evaluate the transformation relationship between the camera and robot device. This approach could be performed for mobile or medical robotics applications, where precise, expensive, or unsterile calibration objects, or enough movement space, cannot be made available at the work site. Firstly, a mathematical model is established to formulate the robot-gripper-to-camera rigid transformation and robot-base-to-world rigid transformation using the Kronecker product. Subsequently, a sparse bundle adjustment is introduced for the optimization of robot–world and hand–eye calibration, as well as reconstruction results. Finally, a validation experiment including two kinds of real data sets is designed to demonstrate the effectiveness and accuracy of the proposed approach. The translation relative error of rigid transformation is less than 8/10,000 by a Denso robot in a movement range of 1.3 m × 1.3 m × 1.2 m. The distance measurement mean error after three-dimensional reconstruction is 0.13 mm.},
	number = {11},
	journal = {Sensors},
	author = {Li, Wei and Dong, Mingli and Lu, Naiguang and Lou, Xiaoping and Sun, Peng},
	month = nov,
	year = {2018},
	keywords = {Kronecker product, calibration object, hand–eye calibration, robot–world calibration, sparse bundle adjustment},
	pages = {3949},
}

@misc{leong_discovering_2023,
	title = {Discovering {Structure} {From} {Corruption} for {Unsupervised} {Image} {Reconstruction}},
	doi = {10.48550/arXiv.2304.05589},
	abstract = {We consider solving ill-posed imaging inverse problems without access to an image prior or ground-truth examples. An overarching challenge in these inverse problems is that an infinite number of images, including many that are implausible, are consistent with the observed measurements. Thus, image priors are required to reduce the space of possible solutions to more desirable reconstructions. However, in many applications it is difficult or potentially impossible to obtain example images to construct an image prior. Hence inaccurate priors are often used, which inevitably result in biased solutions. Rather than solving an inverse problem using priors that encode the spatial structure of any one image, we propose to solve a set of inverse problems jointly by incorporating prior constraints on the collective structure of the underlying images. The key assumption of our work is that the underlying images we aim to reconstruct share common, low-dimensional structure. We show that such a set of inverse problems can be solved simultaneously without the use of a spatial image prior by instead inferring a shared image generator with a low-dimensional latent space. The parameters of the generator and latent embeddings are found by maximizing a proxy for the Evidence Lower Bound (ELBO). Once identified, the generator and latent embeddings can be combined to provide reconstructed images for each inverse problem. The framework we propose can handle general forward model corruptions, and we show that measurements derived from only a small number of ground-truth images (\${\textbackslash}leqslant 150\$) are sufficient for image reconstruction. We demonstrate our approach on a variety of convex and non-convex inverse problems, including denoising, phase retrieval, and black hole video reconstruction.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Leong, Oscar and Gao, Angela F. and Sun, He and Bouman, Katherine L.},
	month = nov,
	year = {2023},
	note = {arXiv:2304.05589 [cs, eess]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{lehtinen_noise2noise_2018,
	title = {{Noise2Noise}: {Learning} {Image} {Restoration} without {Clean} {Data}},
	abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning - learning to map corrupted observations to clean signals - with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans - all corrupted by different processes - based on noisy data only.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	month = jul,
	year = {2018},
	pages = {2965--2974},
}

@article{lee_motion_2023,
	title = {A {Motion} {Deblurring} {Network} for {Enhancing} {UAV} {Image} {Quality} in {Bridge} {Inspection}},
	volume = {7},
	doi = {10.3390/drones7110657},
	abstract = {Unmanned aerial vehicles (UAVs) have been increasingly utilized for facility safety inspections due to their superior safety, cost effectiveness, and inspection accuracy compared to traditional manpower-based methods. High-resolution images captured by UAVs directly contribute to identifying and quantifying structural defects on facility exteriors, making image quality a critical factor in achieving accurate results. However, motion blur induced by external factors such as vibration, low light conditions, and wind during UAV operation significantly degrades image quality, leading to inaccurate defect detection and quantification. To address this issue, this research proposes a deblurring network using a Generative Adversarial Network (GAN) to eliminate the motion blur effect in UAV images. The GAN-based motion deblur network represents an image inpainting method that leverages generative models to correct blurry artifacts, thereby generating clear images. Unlike previous studies, this proposed approach incorporates deblur and blur learning modules to realistically generate blur images required for training the generative models. The UAV images processed using the motion deblur network are evaluated using a quality assessment method based on local blur map and other well-known image quality assessment (IQA) metrics. Moreover, in the experiment of crack detection utilizing the object detection system, improved detection results are observed when using enhanced images. Overall, this research contributes to improving the quality and accuracy of facility safety inspections conducted with UAV-based inspections by effectively addressing the challenges associated with motion blur effects in UAV-captured images.},
	number = {11},
	journal = {Drones},
	author = {Lee, Jin-Hwan and Gwon, Gi-Hun and Kim, In-Ho and Jung, Hyung-Jo},
	month = nov,
	year = {2023},
	keywords = {UAV inspection, generative adversarial network, image quality enhancement, motion deblurring, object detection},
	pages = {657},
}

@article{lanaras_super-resolution_2018,
	title = {Super-resolution of {Sentinel}-2 images: {Learning} a globally applicable deep neural network},
	volume = {146},
	doi = {10.1016/j.isprsjprs.2018.09.018},
	abstract = {The Sentinel-2 satellite mission delivers multi-spectral imagery with 13 spectral bands, acquired at three different spatial resolutions. The aim of this research is to super-resolve the lower-resolution (20 m and 60 m Ground Sampling Distance - GSD) bands to 10 m GSD, so as to obtain a complete data cube at the maximal sensor resolution. We employ a state-of-the-art convolutional neural network (CNN) to perform end-to-end upsampling, which is trained with data at lower resolution, i.e., from 40-{\textgreater}20 m, respectively 360-{\textgreater}60 m GSD. In this way, one has access to a virtually infinite amount of training data, by downsampling real Sentinel-2 images. We use data sampled globally over a wide range of geographical locations, to obtain a network that generalises across different climate zones and land-cover types, and can super-resolve arbitrary Sentinel-2 images without the need of retraining. In quantitative evaluations (at lower scale, where ground truth is available), our network, which we call DSen2, outperforms the best competing approach by almost 50\% in RMSE, while better preserving the spectral characteristics. It also delivers visually convincing results at the full 10 m GSD. The code is available at https://github.com/lanha/DSen2},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Lanaras, Charis and Bioucas-Dias, José and Galliani, Silvano and Baltsavias, Emmanuel and Schindler, Konrad},
	month = dec,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {305--319},
}

@misc{lai_video_2021,
	title = {Video {Autoencoder}: self-supervised disentanglement of static {3D} structure and motion},
	shorttitle = {Video {Autoencoder}},
	doi = {10.48550/arXiv.2110.02951},
	abstract = {A video autoencoder is proposed for learning disentan- gled representations of 3D structure and camera pose from videos in a self-supervised manner. Relying on temporal continuity in videos, our work assumes that the 3D scene structure in nearby video frames remains static. Given a sequence of video frames as input, the video autoencoder extracts a disentangled representation of the scene includ- ing: (i) a temporally-consistent deep voxel feature to represent the 3D structure and (ii) a 3D trajectory of camera pose for each frame. These two representations will then be re-entangled for rendering the input video frames. This video autoencoder can be trained directly using a pixel reconstruction loss, without any ground truth 3D or camera pose annotations. The disentangled representation can be applied to a range of tasks, including novel view synthesis, camera pose estimation, and video generation by motion following. We evaluate our method on several large- scale natural video datasets, and show generalization results on out-of-domain images.},
	publisher = {arXiv},
	author = {Lai, Zihang and Liu, Sifei and Efros, Alexei A. and Wang, Xiaolong},
	month = oct,
	year = {2021},
	note = {arXiv:2110.02951 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{kunwar_large-scale_2021,
	title = {Large-{Scale} {Semantic} 3-{D} {Reconstruction}: {Outcome} of the 2019 {IEEE} {GRSS} {Data} {Fusion} {Contest}—{Part} {A}},
	volume = {14},
	doi = {10.1109/JSTARS.2020.3032221},
	abstract = {In this article, we present the scientific outcomes of the 2019 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2019 Contest addressed the problem of 3-D reconstruction and 3-D semantic understanding on a large scale. Several competitions were organized to assess specific issues, such as elevation estimation and semantic mapping from a single view, two views, or multiple views. In Part A, we report the results of the best-performing approaches for semantic 3-D reconstruction according to these various setups, whereas 3-D point cloud semantic mapping is discussed in Part B.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Kunwar, Saket and Chen, Hongyu and Lin, Manhui and Zhang, Hongyan and D’Angelo, Pablo and Cerra, Daniele and Azimi, Seyed Majid and Brown, Myron and Hager, Gregory and Yokoya, Naoto and Hänsch, Ronny and Le Saux, Bertrand},
	year = {2021},
	keywords = {3-D reconstruction, Classification, Data Fusion Contest (DFC), Data integration, Earth, Laser radar, Satellites, Semantics, Three-dimensional displays, Training, convolutional neural network (CNN), deep learning, elevation model, height estimation, image analysis and data fusion (IADF), light detection and ranging (LiDAR), multiview, point cloud, semantic labeling, semantic mapping, stereo},
	pages = {922--935},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kerepecky_dual-cycle_2023,
	title = {Dual-{Cycle}: {Self}-{Supervised} {Dual}-{View} {Fluorescence} {Microscopy} {Image} {Reconstruction} using {CycleGAN}},
	doi = {10.1109/ICASSP49357.2023.10095386},
	abstract = {Three-dimensional fluorescence microscopy often suffers from anisotropy, where the resolution along the axial direction is lower than that within the lateral imaging plane. We address this issue by presenting Dual-Cycle, a new framework for joint deconvolution and fusion of dual-view fluorescence images. Inspired by the recent Neuroclear method, Dual-Cycle is designed as a cycle-consistent generative network trained in a self-supervised fashion by combining a dual-view generator and prior-guided degradation model. We validate Dual-Cycle on both synthetic and real data showing its state-of-the-art performance without any external training data.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Kerepecky, Tomas and Liu, Jiaming and Ng, Xue Wen and Piston, David W. and Kamilov, Ulugbek S.},
	month = jun,
	year = {2023},
	pages = {1--5},
}

@article{kamilov_plug-and-play_2023,
	title = {Plug-and-{Play} {Methods} for {Integrating} {Physical} and {Learned} {Models} in {Computational} {Imaging}: {Theory}, algorithms, and applications},
	volume = {40},
	doi = {10.1109/MSP.2022.3199595},
	abstract = {Plug-and-play (PnP) priors constitute one of the most widely used frameworks for solving computational imaging problems through the integration of physical models and learned models. PnP leverages high-fidelity physical sensor models and powerful machine learning methods for prior modeling of data to provide state-of-the-art reconstruction algorithms. PnP algorithms alternate between minimizing a data fidelity term to promote data consistency and imposing a learned regularizer in the form of an image denoiser. Recent highly successful applications of PnP algorithms include biomicroscopy, computerized tomography (CT), magnetic resonance imaging (MRI), and joint ptychotomography. This article presents a unified and principled review of PnP by tracing its roots, describing its major variations, summarizing main results, and discussing applications in computational imaging. We also point the way toward further developments by discussing recent results on equilibrium equations that formulate the problem associated with PnP algorithms.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Kamilov, Ulugbek S. and Bouman, Charles A. and Buzzard, Gregery T. and Wohlberg, Brendt},
	month = jan,
	year = {2023},
	keywords = {Computational modeling, Computed tomography, Imaging, Machine learning algorithms, Magnetic resonance imaging, Signal processing algorithms, Training data},
	pages = {85--97},
}

@article{huang_new_2015,
	title = {A {New} {Pan}-{Sharpening} {Method} {With} {Deep} {Neural} {Networks}},
	volume = {12},
	doi = {10.1109/LGRS.2014.2376034},
	abstract = {A deep neural network (DNN)-based new pansharpening method for the remote sensing image fusion problem is proposed in this letter. Research on representation learning suggests that the DNN can effectively model complex relationships between variables via the composition of several levels of nonlinearity. Inspired by this observation, a modified sparse denoising autoencoder (MSDA) algorithm is proposed to train the relationship between high-resolution (HR) and low-resolution (LR) image patches, which can be represented by the DNN. The HR/LR image patches only sample from the HR/LR panchromatic (PAN) images at hand, respectively, without requiring other training images. By connecting a series of MSDAs, we obtain a stacked MSDA (S-MSDA), which can effectively pretrain the DNN. Moreover, in order to better train the DNN, the entire DNN is again trained by a back-propagation algorithm after pretraining. Finally, assuming that the relationship between HR/LR multispectral (MS) image patches is the same as that between HR/LR PAN image patches, the HR MS image will be reconstructed from the observed LR MS image using the trained DNN. Comparative experimental results with several quality assessment indexes show that the proposed method outperforms other pan-sharpening methods in terms of visual perception and numerical measures.},
	number = {5},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Huang, Wei and Xiao, Liang and Wei, Zhihui and Liu, Hongyi and Tang, Songze},
	month = may,
	year = {2015},
	keywords = {Deep neural networks (DNNs), Image fusion, Image reconstruction, Neural networks, Remote sensing, Spatial resolution, Training, multispectral (MS) image, pan-sharpening, panchromatic (PAN) image},
	pages = {1037--1041},
}

@article{huang_spectral_2022,
	title = {Spectral imaging with deep learning},
	volume = {11},
	doi = {10.1038/s41377-022-00743-6},
	abstract = {The goal of spectral imaging is to capture the spectral signature of a target. Traditional scanning method for spectral imaging suffers from large system volume and low image acquisition speed for large scenes. In contrast, computational spectral imaging methods have resorted to computation power for reduced system volume, but still endure long computation time for iterative spectral reconstructions. Recently, deep learning techniques are introduced into computational spectral imaging, witnessing fast reconstruction speed, great reconstruction quality, and the potential to drastically reduce the system volume. In this article, we review state-of-the-art deep-learning-empowered computational spectral imaging methods. They are further divided into amplitude-coded, phase-coded, and wavelength-coded methods, based on different light properties used for encoding. To boost future researches, we’ve also organized publicly available spectral datasets.},
	number = {1},
	journal = {Light: Science \& Applications},
	author = {Huang, Longqian and Luo, Ruichen and Liu, Xu and Hao, Xiang},
	month = mar,
	year = {2022},
	keywords = {Imaging and sensing, Optical spectroscopy},
	pages = {61},
}

@inproceedings{huang_single_2015,
	title = {Single {Image} {Super}-{Resolution} {From} {Transformed} {Self}-{Exemplars}},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Huang, Jia-Bin and Singh, Abhishek and Ahuja, Narendra},
	year = {2015},
	pages = {5197--5206},
}

@misc{hu_spice_2022,
	title = {{SPICE}: {Self}-{Supervised} {Learning} for {MRI} with {Automatic} {Coil} {Sensitivity} {Estimation}},
	shorttitle = {{SPICE}},
	doi = {10.48550/arXiv.2210.02584},
	abstract = {Deep model-based architectures (DMBAs) integrating physical measurement models and learned image regularizers are widely used in parallel magnetic resonance imaging (PMRI). Traditional DMBAs for PMRI rely on pre-estimated coil sensitivity maps (CSMs) as a component of the measurement model. However, estimation of accurate CSMs is a challenging problem when measurements are highly undersampled. Additionally, traditional training of DMBAs requires high-quality groundtruth images, limiting their use in applications where groundtruth is difficult to obtain. This paper addresses these issues by presenting SPICE as a new method that integrates self-supervised learning and automatic coil sensitivity estimation. Instead of using pre-estimated CSMs, SPICE simultaneously reconstructs accurate MR images and estimates high-quality CSMs. SPICE also enables learning from undersampled noisy measurements without any groundtruth. We validate SPICE on experimentally collected data, showing that it can achieve state-of-the-art performance in highly accelerated data acquisition settings (up to 10x).},
	publisher = {arXiv},
	author = {Hu, Yuyang and Gan, Weijie and Ying, Chunwei and Wang, Tongyao and Eldeniz, Cihat and Liu, Jiaming and Chen, Yasheng and An, Hongyu and Kamilov, Ulugbek S.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02584 [eess]},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{hendriksen_noise2inverse_2020,
	title = {{Noise2Inverse}: {Self}-{Supervised} {Deep} {Convolutional} {Denoising} for {Tomography}},
	volume = {6},
	doi = {10.1109/TCI.2020.3019647},
	abstract = {Recovering a high-quality image from noisy indirect measurements is an important problem with many applications. For such inverse problems, supervised deep convolutional neural network (CNN)-based denoising methods have shown strong results, but the success of these supervised methods critically depends on the availability of a high-quality training dataset of similar measurements. For image denoising, methods are available that enable training without a separate training dataset by assuming that the noise in two different pixels is uncorrelated. However, this assumption does not hold for inverse problems, resulting in artifacts in the denoised images produced by existing methods. Here, we propose Noise2Inverse, a deep CNN-based denoising method for linear image reconstruction algorithms that does not require any additional clean or noisy data. Training a CNN-based denoiser is enabled by exploiting the noise model to compute multiple statistically independent reconstructions. We develop a theoretical framework which shows that such training indeed obtains a denoising CNN, assuming the measured noise is element-wise independent, and zero-mean. On simulated CT datasets, Noise2Inverse demonstrates an improvement in peak signal-to-noise ratio and structural similarity index compared to state-of-the-art image denoising methods, and conventional reconstruction methods, such as Total-Variation Minimization. We also demonstrate that the method is able to significantly reduce noise in challenging real-world experimental datasets.},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Hendriksen, Allard Adriaan and Pelt, Daniël Maria and Batenburg, K. Joost},
	year = {2020},
	keywords = {Deep learning, Image denoising, Image reconstruction, Imaging, Inverse problems, Noise measurement, Noise reduction, Training, image reconstruction, inverse problems, reconstruction algorithms, tomography},
	pages = {1320--1335},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	pages = {770--778},
}

@article{hauptmann_image_2021,
	title = {Image {Reconstruction} in {Dynamic} {Inverse} {Problems} with {Temporal} {Models}},
	doi = {https://doi.org/10.1007/978-3-030-03009-4_83-1},
	abstract = {This paper surveys variational approaches for image reconstruction in dynamic inverse problems. Emphasis is on variational methods that rely on parametrized temporal models. These are encoded here as diffeomorphic deformations with time-dependent parameters or as motion-constrained reconstructions where the motion model is given by a differential equation. The survey also includes recent developments in integrating deep learning for solving these computationally demanding variational methods. Examples are given for 2D dynamic tomography, but methods apply to general inverse problems.},
	journal = {Handbook of Mathematical Models and Algorithms in Computer Vision and Imaging: Mathematical Imaging and Vision},
	author = {Hauptmann, Andreas and Öktem, Ozan and Schönlieb, Carola},
	editor = {Chen, Ke and Schönlieb, Carola-Bibiane and Tai, Xue-Cheng and Younces, Laurent},
	year = {2021},
	keywords = {Deep learning, Image reconstruction, Image registration, Indirect registration, Inverse problems, Regularization, Tomography},
	pages = {1--31},
}

@book{hartley_multiple_2004,
	address = {Cambridge},
	edition = {2},
	title = {Multiple {View} {Geometry} in {Computer} {Vision}},
	url = {https://doi.org/10.1017/CBO9780511811685},
	abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
	publisher = {Cambridge University Press},
	author = {Hartley, Richard and Zisserman, Andrew},
	year = {2004},
}

@inproceedings{hartley_linear_1994,
	title = {Linear pushbroom cameras},
	doi = {10.1007/3-540-57956-7_63},
	abstract = {Modelling and analyzing pushbroom sensors commonly used in satellite imagery is difficult and computationally intensive due to the motion of the orbiting satellite with respect to the rotating earth, and the non-linearity of the mathematical model involving orbital dynamics. The linear pushbroom model) introduced in this paper has the advantage of computational simplicity while at the same time giving very accurate results compared with the full orbiting pushbroom model. The common photogrammetric problems may be solved easily for the linear pushbroom model.},
	booktitle = {Computer {Vision} — {ECCV} '94},
	author = {Hartley, Richard and Gupta, Rajiv},
	editor = {Eklundh, Jan-Olof},
	year = {1994},
	keywords = {Common Root, Fundamental Matrix, Ground Control Point, Sensor Array, View Plane},
}

@article{gupta_cryogan_2021,
	title = {{CryoGAN}: {A} {New} {Reconstruction} {Paradigm} for {Single}-{Particle} {Cryo}-{EM} {Via} {Deep} {Adversarial} {Learning}},
	volume = {7},
	doi = {10.1109/TCI.2021.3096491},
	abstract = {We present CryoGAN, a new paradigm for single-particle cryo-electron microscopy (cryo-EM) reconstruction based on unsupervised deep adversarial learning. In single-particle cryo-EM, the structure of a biomolecule needs to be reconstructed from a large set of noisy tomographic projections with unknown orientations. Current reconstruction techniques are based on a marginalized maximum-likelihood formulation that requires calculations over the set of all possible poses for each projection image, a computationally demanding procedure. Our approach is to seek a 3D structure that has simulated projections that match the real data in a distributional sense, thereby sidestepping pose estimation or marginalization. We prove that, in an idealized mathematical model of cryo-EM, this approach results in recovery of the correct structure. Motivated by distribution matching, we propose CryoGAN, a specialized GAN that consists of a 3D structure, a cryo-EM physics simulator, and a discriminator neural network. During reconstruction, the 3D structure is optimized so that its projections obtained through the simulator resemble real data (to the discriminator). Simultaneously, the discriminator is trained to distinguish real projections from simulated projections. CryoGAN takes as input only real projection images and the distribution of the cryo-EM imaging parameters. It involves neither prior training nor an initial estimation of the 3D structure. CryoGAN currently achieves a 10.8 Å resolution on a realistic synthetic dataset. Preliminary results on experimental β-galactosidase and 80S ribosome data demonstrate the ability of CryoGAN to exploit data statistics under standard experimental imaging conditions. We believe that this paradigm opens the door to a family of novel likelihood-free algorithms for cryo-EM reconstruction.},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Gupta, Harshit and McCann, Michael T. and Donati, Laurène and Unser, Michael},
	year = {2021},
	pages = {759--774},
}

@inproceedings{grill_bootstrap_2020,
	title = {Bootstrap your own latent a new approach to self-supervised learning},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6\% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	year = {2020},
	pages = {21271--21284},
}

@article{grigoroiu_deep_2020,
	title = {Deep learning applied to hyperspectral endoscopy for online spectral classification},
	volume = {10},
	doi = {10.1038/s41598-020-60574-6},
	abstract = {Hyperspectral imaging (HSI) is being explored in endoscopy as a tool to extract biochemical information that may improve contrast for early cancer detection in the gastrointestinal tract. Motion artefacts during medical endoscopy have traditionally limited HSI application, however, recent developments in the field have led to real-time HSI deployments. Unfortunately, traditional HSI analysis methods remain unable to rapidly process the volume of hyperspectral data in order to provide real-time feedback to the operator. Here, a convolutional neural network (CNN) is proposed to enable online classification of data obtained during HSI endoscopy. A five-layered CNN was trained and fine-tuned on a dataset of 300 hyperspectral endoscopy images acquired from a planar Macbeth ColorChecker chart and was able to distinguish between its 18 constituent colors with an average accuracy of 94.3\% achieved at 8.8 fps. Performance was then tested on a set of images simulating an endoscopy environment, consisting of color charts warped inside a rigid tube mimicking a lumen. The algorithm proved robust to such variations, with classification accuracies over 90\% being obtained despite the variations, with an average drop in accuracy of 2.4\% being registered at the points of longest working distance and most inclination. For further validation of the color-based classification system, ex vivo videos of a methylene blue dyed pig esophagus and images of different disease stages in the human esophagus were analyzed, showing spatially distinct color classifications. These results suggest that the CNN has potential to provide color-based classification during real-time HSI in endoscopy.},
	number = {1},
	journal = {Scientific Reports},
	author = {Grigoroiu, Alexandru and Yoon, Jonghee and Bohndiek, Sarah E.},
	month = mar,
	year = {2020},
	keywords = {Applied optics, Scientific data},
	pages = {3947},
}

@article{golbabaee_compressive_2021,
	title = {Compressive {MRI} quantification using convex spatiotemporal priors and deep encoder-decoder networks},
	volume = {69},
	doi = {10.1016/j.media.2020.101945},
	abstract = {We propose a dictionary-matching-free pipeline for multi-parametric quantitative MRI image computing. Our approach has two stages based on compressed sensing reconstruction and deep learned quantitative inference. The reconstruction phase is convex and incorporates efficient spatiotemporal regularisations within an accelerated iterative shrinkage algorithm. This minimises the under-sampling (aliasing) artefacts from aggressively short scan times. The learned quantitative inference phase is purely trained on physical simulations (Bloch equations) that are flexible for producing rich training samples. We propose a deep and compact encoder-decoder network with residual blocks in order to embed Bloch manifold projections through multi-scale piecewise affine approximations, and to replace the non-scalable dictionary-matching baseline. Tested on a number of datasets we demonstrate effectiveness of the proposed scheme for recovering accurate and consistent quantitative information from novel and aggressively subsampled 2D/3D quantitative MRI acquisition protocols.},
	journal = {Medical Image Analysis},
	author = {Golbabaee, Mohammad and Buonincontri, Guido and Pirkl, Carolin M. and Menzel, Marion I. and Menze, Bjoern H. and Davies, Mike and Gómez, Pedro A.},
	month = apr,
	year = {2021},
	keywords = {Compressed sensing, Convex model-based reconstruction, Encoder-decoder network, Magnetic resonance fingerprinting, Residual network},
	pages = {101945},
}

@inproceedings{gao_deep_2022,
	title = {Deep {Image} {Interpolation}: {A} {Unified} {Unsupervised} {Framework} for {Pansharpening}},
	doi = {10.1109/CVPRW56347.2022.00076},
	abstract = {Pansharpening, whose aim is to acquire high resolution multispectral data (HRMS) by the fusion of low resolution multispectral data (LRMS) and panchromatic data (PAN), is a specific mission of spatial-spectral fusion in remote sensing field. In recent years, deep learning methods have proved the most feasible methods for pansharpening task. However, these deep learning methods have difficulty in training in an unsupervised manner and become useless when it comes to the condition where no training dataset is available. In this paper, we propose a universal algorithm called deep image interpolation for pansharpening task. The main idea is achieving high-quality fusion results by interpolating two low-quality multispectral images in a deep neural network. We apply it to two conditions: 1) unsupervised training a network when there are enough datasets; 2) directly optimizing the fusion result where no training datasets are available. Simulation and real-data experiments are conducted on various kinds of satellite data. Quantitative and qualitative evaluation results illustrate that the proposed method outperforms traditional pansharpening methods and even catch up with those supervised methods to some extent.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Gao, Jianhao and Li, Jie and Su, Xin and Jiang, Menghui and Yuan, Qiangqiang},
	month = jun,
	year = {2022},
	keywords = {Data integration, Deep learning, Interpolation, Neural networks, Pansharpening, Satellites, Training},
	pages = {608--617},
}

@misc{fu_mononerf_2023,
	title = {{MonoNeRF}: {Learning} {Generalizable} {NeRFs} from {Monocular} {Videos} without {Camera} {Pose}},
	shorttitle = {{MonoNeRF}},
	doi = {10.48550/arXiv.2210.07181},
	abstract = {We propose a generalizable neural radiance fields - MonoNeRF, that can be trained on large-scale monocular videos of moving in static scenes without any ground-truth annotations of depth and camera poses. MonoNeRF follows an Autoencoder-based architecture, where the encoder estimates the monocular depth and the camera pose, and the decoder constructs a Multiplane NeRF representation based on the depth encoder feature, and renders the input frames with the estimated camera. The learning is supervised by the reconstruction error. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single-image novel view synthesis. More qualitative results are available at: https://oasisyang.github.io/mononerf .},
	publisher = {arXiv},
	author = {Fu, Yang and Misra, Ishan and Wang, Xiaolong},
	month = jun,
	year = {2023},
	note = {arXiv:2210.07181 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@incollection{fleet_optical_2006,
	address = {Boston, MA},
	title = {Optical {Flow} {Estimation}},
	isbn = {978-0-387-28831-4},
	abstract = {This chapter provides a tutorial introduction to gradient-based optical flow estimation. We discuss least-squares and robust estimators, iterative coarse-to-fine refinement, different forms of parametric motion models, different conservation assumptions, probabilistic formulations, and robust mixture models.},
	language = {en},
	booktitle = {Handbook of {Mathematical} {Models} in {Computer} {Vision}},
	publisher = {Springer US},
	author = {Fleet, D. and Weiss, Y.},
	editor = {Paragios, Nikos and Chen, Yunmei and Faugeras, Olivier},
	year = {2006},
	doi = {10.1007/0-387-28831-7_15},
	keywords = {Brightness Constancy, Constraint Line, Motion Model, Optical Flow, Residual Motion},
	pages = {237--257},
}

@article{feng_unsupervised_2024,
	title = {Unsupervised {Spectral} {Demosaicing} {With} {Lightweight} {Spectral} {Attention} {Networks}},
	volume = {33},
	doi = {10.1109/TIP.2024.3364064},
	abstract = {This paper presents a deep learning-based spectral demosaicing technique trained in an unsupervised manner. Many existing deep learning-based techniques relying on supervised learning with synthetic images, often underperform on real-world images, especially as the number of spectral bands increases. This paper presents a comprehensive unsupervised spectral demosaicing (USD) framework based on the characteristics of spectral mosaic images. This framework encompasses a training method, model structure, transformation strategy, and a well-fitted model selection strategy. To enable the network to dynamically model spectral correlation while maintaining a compact parameter space, we reduce the complexity and parameters of the spectral attention module. This is achieved by dividing the spectral attention tensor into spectral attention matrices in the spatial dimension and spectral attention vector in the channel dimension. This paper also presents Mosaic 25 , a real 25-band hyperspectral mosaic image dataset featuring various objects, illuminations, and materials for benchmarking purposes. Extensive experiments on both synthetic and real-world datasets demonstrate that the proposed method outperforms conventional unsupervised methods in terms of spatial distortion suppression, spectral fidelity, robustness, and computational cost. Our code and dataset are publicly available at https://github.com/polwork/Unsupervised-Spectral-Demosaicing.},
	journal = {IEEE Transactions on Image Processing},
	author = {Feng, Kai and Zeng, Haijin and Zhao, Yongqiang and Kong, Seong G. and Bu, Yuanyang},
	year = {2024},
	keywords = {Cameras, Correlation, Distortion, Electronics packaging, Hyperspectral imaging, Spectral demosaicing, Task analysis, Training, spectral attention networks, spectral imaging, unsupervised learning},
	pages = {1655--1669},
}

@article{fanous_ganscan_2022,
	title = {{GANscan}: continuous scanning microscopy using deep learning deblurring},
	volume = {11},
	doi = {10.1038/s41377-022-00952-z},
	abstract = {Most whole slide imaging (WSI) systems today rely on the “stop-and-stare” approach, where, at each field of view, the scanning stage is brought to a complete stop before the camera snaps a picture. This procedure ensures that each image is free of motion blur, which comes at the expense of long acquisition times. In order to speed up the acquisition process, especially for large scanning areas, such as pathology slides, we developed an acquisition method in which the data is acquired continuously while the stage is moving at high speeds. Using generative adversarial networks (GANs), we demonstrate this ultra-fast imaging approach, referred to as GANscan, which restores sharp images from motion blurred videos. GANscan allows us to complete image acquisitions at 30x the throughput of stop-and-stare systems. This method is implemented on a Zeiss Axio Observer Z1 microscope, requires no specialized hardware, and accomplishes successful reconstructions at stage speeds of up to 5000 μm/s. We validate the proposed method by imaging H\&E stained tissue sections. Our method not only retrieves crisp images from fast, continuous scans, but also adjusts for defocusing that occurs during scanning within +/− 5 μm. Using a consumer GPU, the inference runs at {\textless}20 ms/ image.},
	number = {1},
	journal = {Light: Science \& Applications},
	author = {Fanous, Michael John and Popescu, Gabriel},
	month = sep,
	year = {2022},
	keywords = {Optical sensors, Phase-contrast microscopy},
	pages = {265},
}

@inproceedings{fabian_data_2021,
	title = {Data augmentation for deep learning based accelerated {MRI} reconstruction with limited data},
	abstract = {Deep neural networks have emerged as very successful tools for image restoration and reconstruction tasks. These networks are often trained end-to-end to directly reconstruct an image from a noisy or corrupted measurement of that image. To achieve state-of-the-art performance, training on large and diverse sets of images is considered critical. However, it is often difficult and/or expensive to collect large amounts of training images. Inspired by the success of Data Augmentation (DA) for classification problems, in this paper, we propose a pipeline for data augmentation for accelerated MRI reconstruction and study its effectiveness at reducing the required training data in a variety of settings. Our DA pipeline, MRAugment, is specifically designed to utilize the invariances present in medical imaging measurements as naive DA strategies that neglect the physics of the problem fail. Through extensive studies on multiple datasets we demonstrate that in the low-data regime DA prevents overfitting and can match or even surpass the state of the art while using significantly fewer training data, whereas in the high-data regime it has diminishing returns. Furthermore, our findings show that DA improves the robustness of the model against various shifts in the test distribution.},
	urldate = {2024-02-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	author = {Fabian, Zalan and Heckel, Reinhard and Soltanolkotabi, Mahdi},
	month = jul,
	year = {2021},
	pages = {3057--3067},
}

@article{deng_machine_2022,
	title = {Machine {Learning} in {Pansharpening}: {A} benchmark, from shallow to deep networks},
	volume = {10},
	doi = {10.1109/MGRS.2022.3187652},
	abstract = {Machine learning (ML) is influencing the literature in several research fields, often through state-of-the-art approaches. In the past several years, ML has been explored for pansharpening, i.e., an image fusion technique based on the combination of a multispectral (MS) image, which is characterized by its medium/low spatial resolution, and higher-spatial-resolution panchromatic (PAN) data. Thus, ML for pansharpening represents an emerging research line that deserves further investigation. In this article, we go through some powerful and widely used ML-based approaches for pansharpening that have been recently proposed in the related literature. Eight approaches are extensively compared. Implementations of these eight methods, exploiting a common software platform and ML library, are developed for comparison purposes. The ML framework for pansharpening will be freely distributed to the scientific community. Experimental results using data acquired by five commonly used sensors for pansharpening and well-established protocols for performance assessment (both at reduced resolution and at full resolution) are shown. The ML-based approaches are compared with a benchmark consisting of classical and variational optimization (VO)-based methods. The pros and cons of each pansharpening technique, based on the training-by-examples philosophy, are reported together with a broad computational analysis. The toolbox is provided in https://github.com/liangjiandeng/DLPan-Toolbox.},
	number = {3},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {Deng, Liang-jian and Vivone, Gemine and Paoletti, Mercedes E. and Scarpa, Giuseppe and He, Jiang and Zhang, Yongjun and Chanussot, Jocelyn and Plaza, Antonio},
	month = sep,
	year = {2022},
	keywords = {Benchmark testing, Image fusion, Machine learning, Optimization, Pansharpening, Philosophical considerations, Spatial resolution, Task analysis},
	pages = {279--315},
}

@inproceedings{de_franchis_stereo-rectification_2014,
	title = {On stereo-rectification of pushbroom images},
	doi = {10.1109/ICIP.2014.7026102},
	abstract = {Image stereo pairs obtained from pinhole cameras can be stereo-rectified, thus permitting to test and use the many standard stereo matching algorithms of the literature. Yet, it is well-known that pushbroom Earth observation satellites produce image pairs that are not stereo-rectifiable. Nevertheless, we show that by a new and adequate use of the satellite calibration data, one can perform a precise local stereo-rectification of large Earth images. Based on this we built a fully automatic 3D reconstruction chain for the new Pléiades Earth observation satellite. It produces 1/10 pixel accurate Earth image stereo pairs at a high resolution. Examples will be made available online to the computer vision community.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {de Franchis, C. and Meinhardt-Llopis, E. and Michel, J. and Morel, J.-M. and Facciolo, G.},
	month = oct,
	year = {2014},
	keywords = {Approximation methods, Cameras, Earth, Pléiades satellite, Remote sensing, Satellites, Sensors, Stereo vision, epipolar, pushbroom, remote sensing, stereo-rectification},
	pages = {5447--5451},
}

@misc{combettes_proximal_2010,
	title = {Proximal {Splitting} {Methods} in {Signal} {Processing}},
	doi = {10.48550/arXiv.0912.3522},
	abstract = {The proximity operator of a convex function is a natural extension of the notion of a projection operator onto a convex set. This tool, which plays a central role in the analysis and the numerical solution of convex optimization problems, has recently been introduced in the arena of signal processing, where it has become increasingly important. In this paper, we review the basic properties of proximity operators which are relevant to signal processing and present optimization methods based on these operators. These proximal splitting methods are shown to capture and extend several well-known algorithms in a unifying framework. Applications of proximal methods in signal recovery and synthesis are discussed.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
	month = may,
	year = {2010},
	note = {arXiv:0912.3522 [math]},
	keywords = {90C25, 65K05, 90C90, 94A08, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
}

@misc{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	doi = {10.48550/arXiv.1602.07576},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Cohen, Taco S. and Welling, Max},
	month = jun,
	year = {2016},
	note = {arXiv:1602.07576 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ciotola_pansharpening_2022,
	title = {Pansharpening by {Convolutional} {Neural} {Networks} in the {Full} {Resolution} {Framework}},
	volume = {60},
	doi = {10.1109/TGRS.2022.3163887},
	abstract = {In recent years, there has been a growing interest in deep learning-based pansharpening. Thus far, research has mainly focused on architectures. Nonetheless, model training is an equally important issue. A first problem is the absence of ground truths, unavoidable in pansharpening. This is often addressed by training networks in a reduced-resolution domain and using the original data as ground truth, relying on an implicit scale invariance assumption. However, on full-resolution images, results are often disappointing, suggesting such invariance not to hold. A further problem is the scarcity of training data, which causes a limited generalization ability and a poor performance on off-training-test images. In this article, we propose a full-resolution training framework for deep learning-based pansharpening. The framework is fully general and can be used for any deep learning-based pansharpening model. Training takes place in the high-resolution domain, relying only on the original data, thus avoiding any loss of information. To ensure spectral and spatial fidelity, a suitable two-component loss is defined. The spectral component enforces consistency between the pansharpened output and the low-resolution multispectral input. The spatial component, computed at high resolution, maximizes the local correlation between each pansharpened band and the panchromatic input. At testing time, the target-adaptive operating modality is adopted, achieving good generalization with a limited computational overhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1 images show that methods trained with the proposed framework guarantee a pretty good performance in terms of both full-resolution numerical indexes and visual quality.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Ciotola, Matteo and Vitale, Sergio and Mazza, Antonio and Poggi, Giovanni and Scarpa, Giuseppe},
	year = {2022},
	keywords = {Convolutional neural network (CNN), Multiresolution analysis, Pansharpening, Remote sensing, Sensors, Spatial resolution, Task analysis, Training, data fusion, deep learning, image enhancement, multiresolution analysis (MRA), spectral distortion, structural consistency, super resolution, unsupervised learning},
	pages = {1--17},
}

@article{ciotola_fast_2023,
	title = {Fast {Full}-{Resolution} {Target}-{Adaptive} {CNN}-{Based} {Pansharpening} {Framework}},
	volume = {15},
	doi = {10.3390/rs15020319},
	abstract = {In the last few years, there has been a renewed interest in data fusion techniques, and, in particular, in pansharpening due to a paradigm shift from model-based to data-driven approaches, supported by the recent advances in deep learning. Although a plethora of convolutional neural networks (CNN) for pansharpening have been devised, some fundamental issues still wait for answers. Among these, cross-scale and cross-datasets generalization capabilities are probably the most urgent ones since most of the current networks are trained at a different scale (reduced-resolution), and, in general, they are well-fitted on some datasets but fail on others. A recent attempt to address both these issues leverages on a target-adaptive inference scheme operating with a suitable full-resolution loss. On the downside, such an approach pays an additional computational overhead due to the adaptation phase. In this work, we propose a variant of this method with an effective target-adaptation scheme that allows for the reduction in inference time by a factor of ten, on average, without accuracy loss. A wide set of experiments carried out on three different datasets, GeoEye-1, WorldView-2 and WorldView-3, prove the computational gain obtained while keeping top accuracy scores compared to state-of-the-art methods, both model-based and deep-learning ones. The generality of the proposed solution has also been validated, applying the new adaptation framework to different CNN models.},
	number = {2},
	journal = {Remote Sensing},
	author = {Ciotola, Matteo and Scarpa, Giuseppe},
	month = jan,
	year = {2023},
	keywords = {data fusion, multiresolution analysis, pansharpening, super-resolution},
	pages = {319},
}

@article{ciotola_unsupervised_2023,
	title = {Unsupervised {Deep} {Learning}-{Based} {Pansharpening} {With} {Jointly} {Enhanced} {Spectral} and {Spatial} {Fidelity}},
	volume = {61},
	doi = {10.1109/TGRS.2023.3299356},
	abstract = {In latest years, deep learning (DL) has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most DL-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework that can be applied to many existing architectures. Here, we propose a new DL-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes the spectral and spatial quality of the pansharpened data. In addition, thanks to a new fine-tuning strategy, it improves inference-time adaptation to target images. Experiments on a large variety of test images, performed in challenging scenarios, demonstrate that the proposed method compares favorably with the state-of-the-art both in terms of numerical results and visual output. The code is available online at https://github.com/matciotola/Lambda-PNN.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Ciotola, Matteo and Poggi, Giovanni and Scarpa, Giuseppe},
	year = {2023},
	keywords = {Correlation, Deep learning (DL), Image resolution, Multiresolution analysis, Pansharpening, Sensors, Spatial resolution, Training, image enhancement, image fusion, image registration, super resolution},
	pages = {1--17},
}

@inproceedings{chung_parallel_2023,
	title = {Parallel {Diffusion} {Models} of {Operator} and {Image} for {Blind} {Inverse} {Problems}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Chung, Hyungjin and Kim, Jeongsol and Kim, Sehui and Ye, Jong Chul},
	year = {2023},
	pages = {6059--6069},
}

@inproceedings{christie_learning_2020,
	title = {Learning {Geocentric} {Object} {Pose} in {Oblique} {Monocular} {Images}},
	doi = {10.1109/CVPR42600.2020.01452},
	abstract = {An object's geocentric pose, defined as the height above ground and orientation with respect to gravity, is a powerful representation of real-world structure for object detection, segmentation, and localization tasks using RGBD images. For close-range vision tasks, height and orientation have been derived directly from stereo-computed depth and more recently from monocular depth predicted by deep networks. For long-range vision tasks such as Earth observation, depth cannot be reliably estimated with monocular images. Inspired by recent work in monocular height above ground prediction and optical flow prediction from static images, we develop an encoding of geocentric pose to address this challenge and train a deep network to compute the representation densely, supervised by publicly available airborne lidar. We exploit these attributes to rectify oblique images and remove observed object parallax to dramatically improve the accuracy of localization and to enable accurate alignment of multiple images taken from very different oblique viewpoints. We demonstrate the value of our approach by extending two large-scale public datasets for semantic segmentation in oblique satellite images. All of our data and code are publicly available.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Christie, Gordon and Munoz Abujder, Rodrigo Rene Rai and Foster, Kevin and Hagstrom, Shea and Hager, Gregory D. and Brown, Myron Z.},
	month = jun,
	year = {2020},
	keywords = {Image segmentation, Laser radar, Optical imaging, Predictive models, Satellites, Semantics, Task analysis},
	pages = {14500--14508},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	pages = {1597--1607},
}

@inproceedings{chen_equivariant_2021,
	title = {Equivariant {Imaging}: {Learning} {Beyond} the {Range} {Space}},
	doi = {10.1109/ICCV48922.2021.00434},
	abstract = {In various imaging problems, we only have access to compressed measurements of the underlying signals, hindering most learning-based strategies which usually require pairs of signals and associated measurements for training Learning only from compressed measurements is impossible in general, as the compressed observations do not contain information outside the range of the forward sensing operator. We propose a new end-to-end self-supervised framework that overcomes this limitation by exploiting the equivariances present in natural signals. Our proposed learning strategy performs as well as fully supervised methods. Experiments demonstrate the potential of this frame- work on inverse problems including sparse-view X-ray computed tomography on real clinical data and image inpainting on natural images. Code has been made available at: https://github.com/edongdongchen/EI.},
	language = {English},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Chen, Dongdong and Tachella, Julián and Davies, Mike E.},
	month = oct,
	year = {2021},
}

@article{chen_imaging_2023,
	title = {Imaging {With} {Equivariant} {Deep} {Learning}: {From} unrolled network design to fully unsupervised learning},
	volume = {40},
	doi = {10.1109/MSP.2022.3205430},
	abstract = {From early image processing to modern computational imaging, successful models and algorithms have relied on a fundamental property of natural signals: symmetry. Here symmetry refers to the invariance property of signal sets to transformations, such as translation, rotation, or scaling. Symmetry can also be incorporated into deep neural networks (DNNs) in the form of equivariance, allowing for more data-efficient learning. While there have been important advances in the design of end-to-end equivariant networks for image classification in recent years, computational imaging introduces unique challenges for equivariant network solutions since we typically only observe the image through some noisy ill-conditioned forward operator that itself may not be equivariant. We review the emerging field of equivariant imaging (EI) and show how it can provide improved generalization and new imaging opportunities. Along the way, we show the interplay between the acquisition physics and group actions and links to iterative reconstruction, blind compressed sensing, and self-supervised learning.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Chen, Dongdong and Davies, Mike and Ehrhardt, Matthias J. and Schönlieb, Carola-Bibiane and Sherry, Ferdia and Tachella, Julián},
	month = jan,
	year = {2023},
	note = {Conference Name: IEEE Signal Processing Magazine},
	pages = {134--147},
}

@inproceedings{chen_compressive_2020,
	title = {Compressive {MR} {Fingerprinting} {Reconstruction} with {Neural} {Proximal} {Gradient} {Iterations}},
	doi = {10.1007/978-3-030-59713-9_2},
	abstract = {Consistency of the predictions with respect to the physical forward model is pivotal for reliably solving inverse problems. This consistency is mostly un-controlled in the current end-to-end deep learning methodologies proposed for the Magnetic Resonance Fingerprinting (MRF) problem. To address this, we propose PGD-Net, a learned proximal gradient descent framework that directly incorporates the forward acquisition and Bloch dynamic models within a recurrent learning mechanism. The PGD-Net adopts a compact neural proximal model for de-aliasing and quantitative inference, that can be flexibly trained on scarce MRF training datasets. Our numerical experiments show that the PGD-Net can achieve a superior quantitative inference accuracy, much smaller storage requirement, and a comparable runtime to the recent deep learning MRF baselines, while being much faster than the dictionary matching schemes. Code has been released at https://github.com/edongdongchen/PGD-Net.},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	author = {Chen, Dongdong and Davies, Mike E. and Golbabaee, Mohammad},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	keywords = {Compressed Sensing, Deep learning, Learned proximal gradient descent, Magnetic resonance fingerprinting},
	pages = {13--22},
}

@misc{chen_deep_2020,
	title = {Deep {Decomposition} {Learning} for {Inverse} {Imaging} {Problems}},
	doi = {10.48550/arXiv.1911.11028},
	abstract = {Deep learning is emerging as a new paradigm for solving inverse imaging problems. However, the deep learning methods often lack the assurance of traditional physics-based methods due to the lack of physical information considerations in neural network training and deploying. The appropriate supervision and explicit calibration by the information of the physic model can enhance the neural network learning and its practical performance. In this paper, inspired by the geometry that data can be decomposed by two components from the null-space of the forward operator and the range space of its pseudo-inverse, we train neural networks to learn the two components and therefore learn the decomposition, i.e. we explicitly reformulate the neural network layers as learning range-nullspace decomposition functions with reference to the layer inputs, instead of learning unreferenced functions. We empirically show that the proposed framework demonstrates superior performance over recent deep residual learning, unrolled learning and nullspace learning on tasks including compressive sensing medical imaging and natural image super-resolution. Our code is available at https://github.com/edongdongchen/DDN.},
	urldate = {2023-10-18},
	publisher = {arXiv},
	author = {Chen, Dongdong and Davies, Mike E.},
	month = jul,
	year = {2020},
	note = {arXiv:1911.11028 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{daras_ambient_2023,
	title = {Ambient {Diffusion}: {Learning} {Clean} {Distributions} from {Corrupted} {Data}},
	shorttitle = {Ambient {Diffusion}},
	doi = {10.48550/arXiv.2305.19256},
	abstract = {We present the first diffusion-based framework that can learn an unknown distribution using only highly-corrupted samples. This problem arises in scientific applications where access to uncorrupted samples is impossible or expensive to acquire. Another benefit of our approach is the ability to train generative models that are less likely to memorize individual training samples since they never observe clean training data. Our main idea is to introduce additional measurement distortion during the diffusion process and require the model to predict the original corrupted image from the further corrupted image. We prove that our method leads to models that learn the conditional expectation of the full uncorrupted image given this additional measurement corruption. This holds for any corruption process that satisfies some technical conditions (and in particular includes inpainting and compressed sensing). We train models on standard benchmarks (CelebA, CIFAR-10 and AFHQ) and show that we can learn the distribution even when all the training samples have \$90{\textbackslash}\%\$ of their pixels missing. We also show that we can finetune foundation models on small corrupted datasets (e.g. MRI scans with block corruptions) and learn the clean distribution without memorizing the training set.},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Daras, Giannis and Shah, Kulin and Dagan, Yuval and Gollakota, Aravind and Dimakis, Alexandros G. and Klivans, Adam},
	month = may,
	year = {2023},
	note = {arXiv:2305.19256 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning},
}

@article{chambolle_introduction_2016,
	title = {An introduction to continuous optimization for imaging},
	volume = {25},
	doi = {10.1017/S096249291600009X},
	abstract = {A large number of imaging problems reduce to the optimization of a cost function, with typical structural properties. The aim of this paper is to describe the state of the art in continuous optimization methods for such problems, and present the most successful approaches and their interconnections. We place particular emphasis on optimal first-order schemes that can deal with typical non-smooth and large-scale objective functions used in imaging problems. We illustrate and compare the different algorithms using classical non-smooth problems in imaging, such as denoising and deblurring. Moreover, we present applications of the algorithms to more advanced problems, such as magnetic resonance imaging, multilabel image segmentation, optical flow estimation, stereo matching, and classification.},
	journal = {Acta Numerica},
	author = {Chambolle, Antonin and Pock, Thomas},
	month = may,
	year = {2016},
	pages = {161--319},
}

@article{celledoni_equivariant_2021,
	title = {Equivariant neural networks for inverse problems},
	volume = {37},
	doi = {10.1088/1361-6420/ac104f},
	abstract = {In recent years the use of convolutional layers to encode an inductive bias (translational equivariance) in neural networks has proven to be a very fruitful idea. The successes of this approach have motivated a line of research into incorporating other symmetries into deep learning methods, in the form of group equivariant convolutional neural networks. Much of this work has been focused on roto-translational symmetry of R d , but other examples are the scaling symmetry of R d and rotational symmetry of the sphere. In this work, we demonstrate that group equivariant convolutional operations can naturally be incorporated into learned reconstruction methods for inverse problems that are motivated by the variational regularisation approach. Indeed, if the regularisation functional is invariant under a group symmetry, the corresponding proximal operator will satisfy an equivariance property with respect to the same group symmetry. As a result of this observation, we design learned iterative methods in which the proximal operators are modelled as group equivariant convolutional neural networks. We use roto-translationally equivariant operations in the proposed methodology and apply it to the problems of low-dose computerised tomography reconstruction and subsampled magnetic resonance imaging reconstruction. The proposed methodology is demonstrated to improve the reconstruction quality of a learned reconstruction method with a little extra computational cost at training time but without any extra cost at test time.},
	number = {8},
	journal = {Inverse Problems},
	author = {Celledoni, Elena and Ehrhardt, Matthias J. and Etmann, Christian and Owren, Brynjulf and Schönlieb, Carola-Bibiane and Sherry, Ferdia},
	month = jul,
	year = {2021},
	pages = {085006},
}

@article{candes_introduction_2008,
	title = {An {Introduction} {To} {Compressive} {Sampling}},
	volume = {25},
	doi = {10.1109/MSP.2007.914731},
	abstract = {Conventional approaches to sampling signals or images follow Shannon's theorem: the sampling rate must be at least twice the maximum frequency present in the signal (Nyquist rate). In the field of data conversion, standard analog-to-digital converter (ADC) technology implements the usual quantized Shannon representation - the signal is uniformly sampled at or above the Nyquist rate. This article surveys the theory of compressive sampling, also known as compressed sensing or CS, a novel sensing/sampling paradigm that goes against the common wisdom in data acquisition. CS theory asserts that one can recover certain signals and images from far fewer samples or measurements than traditional methods use.},
	number = {2},
	journal = {IEEE Signal Processing Magazine},
	author = {Candes, Emmanuel J. and Wakin, Michael B.},
	month = mar,
	year = {2008},
	pages = {21--30},
}

@inproceedings{cai_mask-guided_2022,
	title = {Mask-guided {Spectral}-wise {Transformer} for {Efficient} {Hyperspectral} {Image} {Reconstruction}},
	doi = {10.1109/CVPR52688.2022.01698},
	abstract = {Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI restoration. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S- MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs. https://github.com/caiyuanhao1998/MST/},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cai, Yuanhao and Lin, Jing and Hu, Xiaowan and Wang, Haoqian and Yuan, Xin and Zhang, Yulun and Timofte, Radu and Van Gool, Luc},
	month = jun,
	year = {2022},
	keywords = {Apertures, Computational modeling, Computational photography, Low-level vision, Memory management, Pattern recognition, Photography, Three-dimensional displays, Transformers},
	pages = {17481--17490},
}

@inproceedings{bosch_multiple_2016,
	title = {A multiple view stereo benchmark for satellite imagery},
	doi = {10.1109/AIPR.2016.8010543},
	abstract = {The availability of public multiple view stereo benchmark datasets has been instrumental in enabling research to advance the state of the art in the field and to apply and customize methods to real-world problems. Until now, no public multiple view stereo benchmark dataset has been available for satellite imaging applications. In this work, we describe a public benchmark dataset for multiple view stereo applied to three-dimensional outdoor scene mapping using commercial satellite imagery. This dataset includes fifty Digital Globe WorldView-3 panchromatic and multispectral images of a 100 square kilometer area near San Fernando, Argentina. We also provide high-resolution airborne lidar ground truth data for a 20 square kilometer subset of this area and performance analysis software to assess accuracy and completeness metrics. We report initial results from available solutions using this benchmark data and encourage continued research by making this benchmark dataset publicly available to the research community.},
	booktitle = {2016 {IEEE} {Applied} {Imagery} {Pattern} {Recognition} {Workshop} ({AIPR})},
	author = {Bosch, Marc and Kurtz, Zachary and Hagstrom, Shea and Brown, Myron},
	month = oct,
	year = {2016},
	keywords = {Benchmark testing, Earth, Laser radar, Measurement, Satellites, Solid modeling, Three-dimensional displays},
	pages = {1--9},
}

@inproceedings{bora_ambientgan_2018,
	title = {{AmbientGAN}: {Generative} models from lossy measurements},
	abstract = {Generative models provide a way to model structure in complex distributions and have been shown to be useful for many tasks of practical interest. However, current techniques for training generative models require access to fully-observed samples. In many settings, it is expensive or even impossible to obtain fully-observed samples, but economical to obtain partial, noisy observations. We consider the task of learning an implicit generative model given only lossy measurements of samples from the distribution of interest. We show that the true underlying distribution can be provably recovered even in the presence of per-sample information loss for a class of measurement models. Based on this, we propose a new method of training Generative Adversarial Networks (GANs) which we call AmbientGAN. On three benchmark datasets, and for various measurement models, we demonstrate substantial qualitative and quantitative improvements. Generative models trained with our method can obtain \$2\$-\$4\$x higher inception scores than the baselines.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Bora, Ashish and Price, Eric and Dimakis, Alexandros G.},
	month = feb,
	year = {2018},
}

@inproceedings{barreto_automatic_2009,
	title = {Automatic {Camera} {Calibration} {Applied} to {Medical} {Endoscopy}},
	doi = {10.5244/C.23.52},
	abstract = {The paper proposes a new calibration algorithm for cameras with lens distortion, that uses a single image of a planar chessboard pattern acquired in general position. The radial distortion is modeled using the first order division model, and the method provides a closed form estimation of the intrinsic parameters and distortion coefficient. The experimental evaluation shows that the calibration accuracy is comparable to state-of-the-art algorithms requiring multiple input images. We believe that our approach is particularly well suited for the the calibration of medical endoscopes in computer aided surgery. Since the lens is mounted on the camera before each usage in the OR, the calibration procedure must be performed by the clinical practitioner with minimum effort. We solve this problem by proposing a fully automatic procedure that requires no human intervention other than acquiring a single calibration image.},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2009},
	author = {Barreto, Joao P. and Roquette, Jose and Sturm, Peter and Fonseca, Fernando},
	year = {2009},
	pages = {52.1--52.10},
}

@article{bacca_computational_2023,
	title = {Computational spectral imaging: a contemporary overview},
	volume = {40},
	doi = {10.1364/JOSAA.482406},
	abstract = {Spectral imaging collects and processes information along spatial and spectral coordinates quantified in discrete voxels, which can be treated as a 3D spectral data cube. The spectral images (SIs) allow the identification of objects, crops, and materials in the scene through their spectral behavior. Since most spectral optical systems can only employ 1D or maximum 2D sensors, it is challenging to directly acquire 3D information from available commercial sensors. As an alternative, computational spectral imaging (CSI) has emerged as a sensing tool where 3D data can be obtained using 2D encoded projections. Then, a computational recovery process must be employed to retrieve the SI. CSI enables the development of snapshot optical systems that reduce acquisition time and provide low computational storage costs compared with conventional scanning systems. Recent advances in deep learning (DL) have allowed the design of data-driven CSI to improve the SI reconstruction or, even more, perform high-level tasks such as classification, unmixing, or anomaly detection directly from 2D encoded projections. This work summarizes the advances in CSI, starting with SI and its relevance and continuing with the most relevant compressive spectral optical systems. Then, CSI with DL will be introduced, as well as the recent advances in combining the physical optical design with computational DL algorithms to solve high-level tasks.},
	number = {4},
	journal = {JOSA A},
	author = {Bacca, Jorge and Martinez, Emmanuel and Arguello, Henry},
	month = apr,
	year = {2023},
	keywords = {Computational imaging, Deep learning, Diffractive optical elements, Information processing, Liquid crystal on silicon, Optical systems},
	pages = {C115--C125},
}

@misc{aygun_saor_2024,
	title = {{SAOR}: {Single}-{View} {Articulated} {Object} {Reconstruction}},
	doi = {10.48550/arXiv.2303.13514},
	abstract = {We introduce SAOR, a novel approach for estimating the 3D shape, texture, and viewpoint of an articulated object from a single image captured in the wild. Unlike prior approaches that rely on pre-defined category-specific 3D templates or tailored 3D skeletons, SAOR learns to articulate shapes from single-view image collections with a skeleton-free part-based model without requiring any 3D object shape priors. To prevent ill-posed solutions, we propose a cross-instance consistency loss that exploits disentangled object shape deformation and articulation. This is helped by a new silhouette-based sampling mechanism to enhance viewpoint diversity during training. Our method only requires estimated object silhouettes and relative depth maps from off-the-shelf pre-trained networks during training. At inference time, given a single-view image, it efficiently outputs an explicit mesh representation. We obtain improved qualitative and quantitative results on challenging quadruped animals compared to relevant existing work.},
	publisher = {arXiv},
	author = {Aygün, Mehmet and Mac Aodha, Oisin},
	month = apr,
	year = {2024},
	note = {arXiv:2303.13514 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{arce_compressive_2014,
	title = {Compressive {Coded} {Aperture} {Spectral} {Imaging}: {An} {Introduction}},
	volume = {31},
	doi = {10.1109/MSP.2013.2278763},
	abstract = {Imaging spectroscopy involves the sensing of a large amount of spatial information across a multitude of wavelengths. Conventional approaches to hyperspectral sensing scan adjacent zones of the underlying spectral scene and merge the results to construct a spectral data cube. Push broom spectral imaging sensors, for instance, capture a spectral cube with one focal plane array (FPA) measurement per spatial line of the scene [1], [2]. Spectrometers based on optical bandpass filters sequentially scan the scene by tuning the bandpass filters in steps. The disadvantage of these techniques is that they require scanning a number of zones linearly in proportion to the desired spatial and spectral resolution. This article surveys compressive coded aperture spectral imagers, also known as coded aperture snapshot spectral imagers (CASSI) [1], [3], [4], which naturally embody the principles of compressive sensing (CS) [5], [6]. The remarkable advantage of CASSI is that the entire data cube is sensed with just a few FPA measurements and, in some cases, with as little as a single FPA shot.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Arce, Gonzalo R. and Brady, David J. and Carin, Lawrence and Arguello, Henry and Kittle, David S.},
	month = jan,
	year = {2014},
	keywords = {Detectors, Optical filters, Optical imaging, Optical sensors, Optical signal processing, Spectroscopy, Tutorials},
	pages = {105--115},
}

@inproceedings{arad_ntire_2022,
	title = {{NTIRE} 2022 {Spectral} {Demosaicing} {Challenge} and {Data} {Set}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Arad, Boaz and Timofte, Radu and Yahel, Rony and Morag, Nimrod and Bernat, Amir and Wu, Yaqi and Wu, Xun and Fan, Zhihao and Xia, Chenjie and Zhang, Feng and Liu, Shuai and Li, Yongqiang and Feng, Chaoyu and Lei, Lei and Zhang, Mingwei and Feng, Kai and Zhang, Xun and Yao, Jiaxin and Zhao, Yongqiang and Ma, Suina and He, Fan and Dong, Yangyang and Yu, Shufang and Qiu, Difa and Liu, Jinhui and Bi, Mengzhao and Song, Beibei and Sun, WenFang and Zheng, Jiesi and Zhao, Bowen and Cao, Yanpeng and Yang, Jiangxin and Cao, Yanlong and Kong, Xiangyu and Yu, Jingbo and Xue, Yuanyang and Xie, Zheng},
	year = {2022},
	pages = {882--896},
}

@inproceedings{arad_ntire_2020,
	title = {{NTIRE} 2020 {Challenge} on {Spectral} {Reconstruction} from an {RGB} {Image}},
	doi = {10.1109/CVPRW50498.2020.00231},
	abstract = {This paper reviews the second challenge on spectral reconstruction from RGB images, i.e., the recovery of whole- scene hyperspectral (HS) information from a 3-channel RGB image. As in the previous challenge, two tracks were provided: (i) a "Clean" track where HS images are estimated from noise-free RGBs, the RGB images are themselves calculated numerically using the ground-truth HS images and supplied spectral sensitivity functions (ii) a "Real World" track, simulating capture by an uncalibrated and unknown camera, where the HS images are recovered from noisy JPEG-compressed RGB images. A new, larger-than-ever, natural hyperspectral image data set is presented, containing a total of 510 HS images. The Clean and Real World tracks had 103 and 78 registered participants respectively, with 14 teams competing in the final testing phase. A description of the proposed methods, alongside their challenge scores and an extensive evaluation of top performing methods is also provided. They gauge the state-of-the-art in spectral reconstruction from an RGB image.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	author = {Arad, Boaz and Timofte, Radu and Ben-Shahar, Ohad and Lin, Yi-Tun and Finlayson, Graham and Givati, Shai and Li, Jiaojiao and Wu, Chaoxiong and Song, Rui and Li, Yunsong and Liu, Fei and Lang, Zhiqiang and Wei, Wei and Zhang, Lei and Nie, Jiangtao and Zhao, Yuzhi and Po, Lai-Man and Yan, Qiong and Liu, Wei and Lin, Tingyu and Kim, Youngjung and Shin, Changyeop and Rho, Kyeongha and Kim, Sungho and Zhu, Zhiyu and Hou, Junhui and Sun, He and Ren, Jinchang and Fang, Zhenyu and Yan, Yijun and Peng, Hao and Chen, Xiaomei and Zhao, Jie and Stiebel, Tarek and Koppers, Simon and Merhof, Dorit and Gupta, Honey and Mitra, Kaushik and Fubara, Biebele Joslyn and Sedky, Mohamed and Dyke, Dave and Banerjee, Atmadeep and Palrecha, Akash and Sabarinathan, Sabarinathan and Uma, K. and Vinothini, D. Synthiya and Bama, B. Sathya and Roomi, S. M. Md Mansoor},
	month = jun,
	year = {2020},
	pages = {1806--1822},
}

@article{alparone_multispectral_2008,
	title = {Multispectral and {Panchromatic} {Data} {Fusion} {Assessment} {Without} {Reference}},
	volume = {74},
	doi = {10.14358/PERS.74.2.193},
	abstract = {This paper introduces a novel approach for evaluating the quality of pansharpened multispectral (MS) imagery without resorting to reference originals. Hence, evaluations are feasible at the highest spatial resolution of the panchromatic (PAN) sensor. Wang and Bovik’s image
quality index (QI) provides a statistical similarity measurement between two monochrome images. The QI values between any couple of MS bands are calculated before and after fusion and used to define a measurement of spectral distortion. Analogously, QI values between each MS band and
the PAN image are calculated before and after fusion to yield a measurement of spatial distortion. The rationale is that such QI values should be unchanged after fusion, i.e., when the spectral information is translated from the coarse scale of the MS data to the fine scale of the PAN
image. Experimental results, carried out on very high-resolution Ikonos data and simulated Pléiades data, demonstrate that the results provided by the proposed approach are consistent and in trend with analysis performed on spatially degraded data. However, the proposed method requires
no reference originals and is therefore usable in all practical cases.},
	number = {2},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Alparone, Luciano and Aiazzi, Bruno and Baronti, Stefano and Garzelli, Andrea and Nencini, Filippo and Selva, Massimo},
	month = feb,
	year = {2008},
	pages = {193--200},
}

@article{alomar_data_2023,
	title = {Data {Augmentation} in {Classification} and {Segmentation}: {A} {Survey} and {New} {Strategies}},
	volume = {9},
	doi = {10.3390/jimaging9020046},
	abstract = {In the past decade, deep neural networks, particularly convolutional neural networks, have revolutionised computer vision. However, all deep learning models may require a large amount of data so as to achieve satisfying results. Unfortunately, the availability of sufficient amounts of data for real-world problems is not always possible, and it is well recognised that a paucity of data easily results in overfitting. This issue may be addressed through several approaches, one of which is data augmentation. In this paper, we survey the existing data augmentation techniques in computer vision tasks, including segmentation and classification, and suggest new strategies. In particular, we introduce a way of implementing data augmentation by using local information in images. We propose a parameter-free and easy to implement strategy, the random local rotation strategy, which involves randomly selecting the location and size of circular regions in the image and rotating them with random angles. It can be used as an alternative to the traditional rotation strategy, which generally suffers from irregular image boundaries. It can also complement other techniques in data augmentation. Extensive experimental results and comparisons demonstrated that the new strategy consistently outperformed its traditional counterparts in, for example, image classification.},
	number = {2},
	journal = {Journal of Imaging},
	author = {Alomar, Khaled and Aysel, Halil Ibrahim and Cai, Xiaohao},
	month = feb,
	year = {2023},
	keywords = {classification, convolutional neural networks, data augmentation, deep learning, image processing, segmentation},
	pages = {46},
}

@book{klein_elementary_1939,
	title = {Elementary {Mathematics} from an {Advanced} {Standpoint}.},
	publisher = {Macmillan, New York},
	author = {Klein, Felix},
	year = {1939},
}

@misc{maxar_worldview-2_nodate,
	title = {{WorldView}-2 {Data} {Sheet}},
	url = {https://resources.maxar.com/data-sheets/worldview-2},
	abstract = {WorldView-2, launched October 2009, is the first high-resolution 8-band multispectral commercial satellite.},
	urldate = {2024-02-16},
	journal = {WorldView-2 Data Sheet},
	author = {{Maxar}},
}

@misc{scott_real_2015,
	title = {Real {Technology}, {Real} {Benefits}, part 1: pointing agility},
	shorttitle = {Real {Technology}, {Real} {Benefits}, part 1},
	url = {https://blog.maxar.com/earth-intelligence/2015/real-technology-real-benefits-part-1-pointing-agility},
	abstract = {Part 1 of Real Technology, Real Benefits explores one aspect of our technology - pointing agility - and how it relate to real-world benefits.},
	language = {en-US},
	urldate = {2024-02-15},
	journal = {Maxar Blog},
	author = {Scott, Walter},
	month = apr,
	year = {2015},
}

@misc{noauthor_perspective_nodate,
	title = {Perspective {Transformation} {Data} {Augmentation} for {Object} {Detection} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/document/8943416},
	urldate = {2023-10-25},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@book{murphy_machine_2012,
	address = {Cambridge, MA},
	series = {Adaptive computation and machine learning series},
	title = {Machine learning: a probabilistic perspective},
	isbn = {9780262018029},
	shorttitle = {Machine learning},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2012},
	keywords = {Machine learning, Probabilities},
}

@article{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	urldate = {2022-05-30},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = may,
	year = {2014},
	note = {arXiv: 1401.4082},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@book{zhang_numerical_2017,
	address = {Cham},
	series = {Applied {Mathematical} {Sciences}},
	title = {Numerical {Methods} for {Stochastic} {Partial} {Differential} {Equations} with {White} {Noise}},
	volume = {196},
	isbn = {9783319575100 9783319575117},
	url = {http://link.springer.com/10.1007/978-3-319-57511-7},
	urldate = {2022-05-22},
	publisher = {Springer International Publishing},
	author = {Zhang, Zhongqiang and Karniadakis, George Em},
	year = {2017},
	doi = {10.1007/978-3-319-57511-7},
}

@book{logg_automated_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computational} {Science} and {Engineering}},
	title = {Automated {Solution} of {Differential} {Equations} by the {Finite} {Element} {Method}},
	volume = {84},
	isbn = {9783642230981 9783642230998},
	url = {http://link.springer.com/10.1007/978-3-642-23099-8},
	urldate = {2022-05-22},
	publisher = {Springer Berlin Heidelberg},
	editor = {Logg, Anders and Mardal, Kent-Andre and Wells, Garth},
	year = {2012},
	doi = {10.1007/978-3-642-23099-8},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2022-05-22},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@article{crank_practical_1947,
	title = {A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type},
	volume = {43},
	issn = {0305-0041, 1469-8064},
	url = {https://www.cambridge.org/core/product/identifier/S0305004100023197/type/journal_article},
	doi = {10.1017/S0305004100023197},
	abstract = {This paper is concerned with methods of evaluating numerical solutions of the non-linear partial differential equation 
             
               
                 
               
             
            where 
             
               
                 
               
             
            subject to the boundary conditions 
             
               
                 
               
             
             
              A, k, q 
              are known constants. 
             
            Equation (1) is of the type which arises in problems of heat flow when there is an internal generation of heat within the medium; if the heat is due to a chemical reaction proceeding at each point at a rate depending upon the local temperature, the rate of heat generation is often defined by an equation such as (2).},
	language = {en},
	number = {1},
	urldate = {2022-05-22},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Crank, J. and Nicolson, P.},
	month = jan,
	year = {1947},
	pages = {50--67},
}

@article{rauch_maximum_1965,
	title = {Maximum likelihood estimates of linear dynamic systems},
	volume = {3},
	issn = {0001-1452, 1533-385X},
	url = {https://arc.aiaa.org/doi/10.2514/3.3166},
	doi = {10.2514/3.3166},
	language = {en},
	number = {8},
	urldate = {2022-05-22},
	journal = {AIAA Journal},
	author = {Rauch, H. E. and Tung, F. and Striebel, C. T.},
	month = aug,
	year = {1965},
	pages = {1445--1450},
}

@article{yildiz_ode2vae_2019,
	title = {{ODE}\${\textasciicircum}2\${VAE}: {Deep} generative second order {ODEs} with {Bayesian} neural networks},
	shorttitle = {{ODE}\${\textasciicircum}2\${VAE}},
	url = {http://arxiv.org/abs/1905.10994},
	abstract = {We present Ordinary Differential Equation Variational Auto-Encoder (ODE\${\textasciicircum}2\$VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE\${\textasciicircum}2\$VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks.},
	urldate = {2022-05-21},
	journal = {arXiv:1905.10994 [cs, stat]},
	author = {Yildiz, Cagatay and Heinonen, Markus and Lahdesmaki, Harri},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.10994},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bauer_digital_2021,
	title = {A digital twin of {Earth} for the green transition},
	volume = {11},
	copyright = {2021 Springer Nature Limited},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/s41558-021-00986-y},
	doi = {10.1038/s41558-021-00986-y},
	abstract = {For its green transition, the EU plans to fund the development of digital twins of Earth. For these twins to be more than big data atlases, they must create a qualitatively new Earth system simulation and observation capability using a methodological framework responsible for exceptional advances in numerical weather prediction.},
	language = {en},
	number = {2},
	urldate = {2022-05-21},
	journal = {Nature Climate Change},
	author = {Bauer, Peter and Stevens, Bjorn and Hazeleger, Wilco},
	month = feb,
	year = {2021},
	keywords = {Business and industry, Climate and Earth system modelling},
	pages = {80--83},
}

@article{aiaa_digital_engineering_integration_committee_digital_2020,
	title = {Digital {Twin}: {Definition} \& {Value}. {AIAA} {Position} {Paper}.},
	author = {{AIAA Digital Engineering Integration Committee}},
	year = {2020},
}

@article{schwantes_modeling_2015,
	title = {Modeling {Molecular} {Kinetics} with {tICA} and the {Kernel} {Trick}},
	volume = {11},
	issn = {1549-9618, 1549-9626},
	url = {https://pubs.acs.org/doi/10.1021/ct5007357},
	doi = {10.1021/ct5007357},
	language = {en},
	number = {2},
	urldate = {2022-05-21},
	journal = {Journal of Chemical Theory and Computation},
	author = {Schwantes, Christian R. and Pande, Vijay S.},
	month = feb,
	year = {2015},
	pages = {600--608},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2022-05-21},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_physics-informed_2018,
	title = {Physics-{Informed} {Generative} {Adversarial} {Networks} for {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1811.02033},
	abstract = {We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.},
	urldate = {2022-05-21},
	journal = {arXiv:1811.02033 [cs, math, stat]},
	author = {Yang, Liu and Zhang, Dongkun and Karniadakis, George Em},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.02033},
	keywords = {Computer Science - Machine Learning, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{yang_highly-scalable_2019,
	title = {Highly-scalable, physics-informed {GANs} for learning solutions of stochastic {PDEs}},
	url = {http://arxiv.org/abs/1910.13444},
	abstract = {Uncertainty quantification for forward and inverse problems is a central challenge across physical and biomedical disciplines. We address this challenge for the problem of modeling subsurface flow at the Hanford Site by combining stochastic computational models with observational data using physics-informed GAN models. The geographic extent, spatial heterogeneity, and multiple correlation length scales of the Hanford Site require training a computationally intensive GAN model to thousands of dimensions. We develop a hierarchical scheme for exploiting domain parallelism, map discriminators and generators to multiple GPUs, and employ efficient communication schemes to ensure training stability and convergence. We developed a highly optimized implementation of this scheme that scales to 27,500 NVIDIA Volta GPUs and 4584 nodes on the Summit supercomputer with a 93.1\% scaling efficiency, achieving peak and sustained half-precision rates of 1228 PF/s and 1207 PF/s.},
	urldate = {2022-05-21},
	journal = {arXiv:1910.13444 [physics, stat]},
	author = {Yang, Liu and Treichler, Sean and Kurth, Thorsten and Fischer, Keno and Barajas-Solano, David and Romero, Josh and Churavy, Valentin and Tartakovsky, Alexandre and Houston, Michael and {Prabhat} and Karniadakis, George},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.13444},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{morton_deep_2018,
	title = {Deep {Dynamical} {Modeling} and {Control} of {Unsteady} {Fluid} {Flows}},
	url = {http://arxiv.org/abs/1805.07472},
	abstract = {The design of flow control systems remains a challenge due to the nonlinear nature of the equations that govern fluid flow. However, recent advances in computational fluid dynamics (CFD) have enabled the simulation of complex fluid flows with high accuracy, opening the possibility of using learning-based approaches to facilitate controller design. We present a method for learning the forced and unforced dynamics of airflow over a cylinder directly from CFD data. The proposed approach, grounded in Koopman theory, is shown to produce stable dynamical models that can predict the time evolution of the cylinder system over extended time horizons. Finally, by performing model predictive control with the learned dynamical models, we are able to find a straightforward, interpretable control law for suppressing vortex shedding in the wake of the cylinder.},
	urldate = {2022-05-21},
	journal = {arXiv:1805.07472 [cs]},
	author = {Morton, Jeremy and Witherden, Freddie D. and Jameson, Antony and Kochenderfer, Mykel J.},
	month = nov,
	year = {2018},
	note = {arXiv: 1805.07472},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science},
}

@article{erichson_physics-informed_2019,
	title = {Physics-informed {Autoencoders} for {Lyapunov}-stable {Fluid} {Flow} {Prediction}},
	url = {http://arxiv.org/abs/1905.10866},
	abstract = {In addition to providing high-profile successes in computer vision and natural language processing, neural networks also provide an emerging set of techniques for scientific problems. Such data-driven models, however, typically ignore physical insights from the scientific system under consideration. Among other things, a physics-informed model formulation should encode some degree of stability or robustness or well-conditioning (in that a small change of the input will not lead to drastic changes in the output), characteristic of the underlying scientific problem. We investigate whether it is possible to include physics-informed prior knowledge for improving the model quality (e.g., generalization performance, sensitivity to parameter tuning, or robustness in the presence of noisy data). To that extent, we focus on the stability of an equilibrium, one of the most basic properties a dynamic system can have, via the lens of Lyapunov analysis. For the prototypical problem of fluid flow prediction, we show that models preserving Lyapunov stability improve the generalization error and reduce the prediction uncertainty.},
	urldate = {2022-05-21},
	journal = {arXiv:1905.10866 [physics]},
	author = {Erichson, N. Benjamin and Muehlebach, Michael and Mahoney, Michael W.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.10866},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
}

@article{lu_extracting_2020,
	title = {Extracting {Interpretable} {Physical} {Parameters} from {Spatiotemporal} {Systems} using {Unsupervised} {Learning}},
	volume = {10},
	issn = {2160-3308},
	url = {http://arxiv.org/abs/1907.06011},
	doi = {10.1103/PhysRevX.10.031056},
	abstract = {Experimental data is often affected by uncontrolled variables that make analysis and interpretation difficult. For spatiotemporal systems, this problem is further exacerbated by their intricate dynamics. Modern machine learning methods are particularly well-suited for analyzing and modeling complex datasets, but to be effective in science, the result needs to be interpretable. We demonstrate an unsupervised learning technique for extracting interpretable physical parameters from noisy spatiotemporal data and for building a transferable model of the system. In particular, we implement a physics-informed architecture based on variational autoencoders that is designed for analyzing systems governed by partial differential equations (PDEs). The architecture is trained end-to-end and extracts latent parameters that parameterize the dynamics of a learned predictive model for the system. To test our method, we train our model on simulated data from a variety of PDEs with varying dynamical parameters that act as uncontrolled variables. Numerical experiments show that our method can accurately identify relevant parameters and extract them from raw and even noisy spatiotemporal data (tested with roughly 10\% added noise). These extracted parameters correlate well (linearly with \$R{\textasciicircum}2 {\textgreater} 0.95\$) with the ground truth physical parameters used to generate the datasets. We then apply this method to nonlinear fiber propagation data, generated by an ab-initio simulation, to demonstrate its capabilities on a more realistic dataset. Our method for discovering interpretable latent parameters in spatiotemporal systems will allow us to better analyze and understand real-world phenomena and datasets, which often have unknown and uncontrolled variables that alter the system dynamics and cause varying behaviors that are difficult to disentangle.},
	number = {3},
	urldate = {2022-05-21},
	journal = {Physical Review X},
	author = {Lu, Peter Y. and Kim, Samuel and Soljačić, Marin},
	month = sep,
	year = {2020},
	note = {arXiv: 1907.06011
version: 3},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	pages = {031056},
}

@article{zhong_pi-vae_2022,
	title = {{PI}-{VAE}: {Physics}-{Informed} {Variational} {Auto}-{Encoder} for stochastic differential equations},
	shorttitle = {{PI}-{VAE}},
	url = {http://arxiv.org/abs/2203.11363},
	abstract = {We propose a new class of physics-informed neural networks, called physics-informed Variational Autoencoder (PI-VAE), to solve stochastic differential equations (SDEs) or inverse problems involving SDEs. In these problems the governing equations are known but only a limited number of measurements of system parameters are available. PI-VAE consists of a variational autoencoder (VAE), which generates samples of system variables and parameters. This generative model is integrated with the governing equations. In this integration, the derivatives of VAE outputs are readily calculated using automatic differentiation, and used in the physics-based loss term. In this work, the loss function is chosen to be the Maximum Mean Discrepancy (MMD) for improved performance, and neural network parameters are updated iteratively using the stochastic gradient descent algorithm. We first test the proposed method on approximating stochastic processes. Then we study three types of problems related to SDEs: forward and inverse problems together with mixed problems where system parameters and solutions are simultaneously calculated. The satisfactory accuracy and efficiency of the proposed method are numerically demonstrated in comparison with physics-informed generative adversarial network (PI-WGAN).},
	urldate = {2022-05-21},
	journal = {arXiv:2203.11363 [cs, stat]},
	author = {Zhong, Weiheng and Meidani, Hadi},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.11363},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chung_recurrent_2016,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2022-05-21},
	journal = {arXiv:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv: 1506.02216},
	keywords = {Computer Science - Machine Learning},
}

@article{watter_embed_2015,
	title = {Embed to {Control}: {A} {Locally} {Linear} {Latent} {Dynamics} {Model} for {Control} from {Raw} {Images}},
	shorttitle = {Embed to {Control}},
	url = {http://arxiv.org/abs/1506.07365},
	abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
	urldate = {2022-05-21},
	journal = {arXiv:1506.07365 [cs, stat]},
	author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
	month = nov,
	year = {2015},
	note = {arXiv: 1506.07365},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{karl_deep_2017,
	title = {Deep {Variational} {Bayes} {Filters}: {Unsupervised} {Learning} of {State} {Space} {Models} from {Raw} {Data}},
	shorttitle = {Deep {Variational} {Bayes} {Filters}},
	url = {http://arxiv.org/abs/1605.06432},
	abstract = {We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
	urldate = {2022-05-21},
	journal = {arXiv:1605.06432 [cs, stat]},
	author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and van der Smagt, Patrick},
	month = mar,
	year = {2017},
	note = {arXiv: 1605.06432},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
}

@article{ward_continuous_2021,
	title = {Continuous calibration of a digital twin: {Comparison} of particle filter and {Bayesian} calibration approaches},
	volume = {2},
	issn = {2632-6736},
	shorttitle = {Continuous calibration of a digital twin},
	url = {https://www.cambridge.org/core/product/identifier/S2632673621000125/type/journal_article},
	doi = {10.1017/dce.2021.12},
	abstract = {Abstract 
            Assimilation of continuously streamed monitored data is an essential component of a digital twin; the assimilated data are used to ensure the digital twin represents the monitored system as accurately as possible. One way this is achieved is by calibration of simulation models, whether data-derived or physics-based, or a combination of both. Traditional manual calibration is not possible in this context; hence, new methods are required for continuous calibration. In this paper, a particle filter methodology for continuous calibration of the physics-based model element of a digital twin is presented and applied to an example of an underground farm. The methodology is applied to a synthetic problem with known calibration parameter values prior to being used in conjunction with monitored data. The proposed methodology is compared against static and sequential Bayesian calibration approaches and compares favourably in terms of determination of the distribution of parameter values and analysis run times, both essential requirements. The methodology is shown to be potentially useful as a means to ensure continuing model fidelity.},
	language = {en},
	urldate = {2022-05-21},
	journal = {Data-Centric Engineering},
	author = {Ward, Rebecca and Choudhary, Ruchi and Gregory, Alastair and Jans-Singh, Melanie and Girolami, Mark},
	year = {2021},
	pages = {e15},
}

@article{febrianto_digital_2021,
	title = {Digital twinning of self-sensing structures using the statistical finite element method},
	url = {http://arxiv.org/abs/2103.13729},
	abstract = {The monitoring of infrastructure assets using sensor networks is becoming increasingly prevalent. A digital twin in the form of a finite element model, as used in design and construction, can help make sense of the copious amount of collected sensor data. This paper demonstrates the application of the statistical finite element method (statFEM), which provides a consistent and principled means for synthesising data and physics-based models, in developing a digital twin of a self-sensing structure. As a case study, an instrumented steel railway bridge of 27.34 m length located along the West Coast Mainline near Staffordshire in the UK is considered. Using strain data captured from fibre Bragg grating (FBG) sensors at 108 locations along the bridge superstructure, statFEM can predict the `true' system response while taking into account the uncertainties in sensor readings, applied loading and finite element model misspecification errors. Longitudinal strain distributions along the two main I-beams are both measured and modelled during the passage of a passenger train. The digital twin, because of its physics-based component, is able to generate reasonable strain distribution predictions at locations where no measurement data is available, including at several points along the main I-beams and on structural elements on which sensors are not even installed. The implications for long-term structural health monitoring and assessment include optimisation of sensor placement, and performing more reliable what-if analyses at locations and under loading scenarios for which no measurement data is available.},
	urldate = {2022-05-21},
	journal = {arXiv:2103.13729 [cs, math]},
	author = {Febrianto, Eky and Butler, Liam and Girolami, Mark and Cirak, Fehmi},
	month = nov,
	year = {2021},
	note = {arXiv: 2103.13729},
	keywords = {Mathematics - Numerical Analysis},
}

@article{niederer_scaling_2021,
	title = {Scaling digital twins from the artisanal to the industrial},
	volume = {1},
	issn = {2662-8457},
	url = {http://www.nature.com/articles/s43588-021-00072-5},
	doi = {10.1038/s43588-021-00072-5},
	language = {en},
	number = {5},
	urldate = {2022-05-21},
	journal = {Nature Computational Science},
	author = {Niederer, Steven A. and Sacks, Michael S. and Girolami, Mark and Willcox, Karen},
	month = may,
	year = {2021},
	pages = {313--320},
}

@article{holloway_generalized_1999,
	title = {A generalized {Korteweg}-de {Vries} model of internal tide transformation in the coastal zone},
	volume = {104},
	issn = {01480227},
	url = {http://doi.wiley.com/10.1029/1999JC900144},
	doi = {10.1029/1999JC900144},
	language = {en},
	number = {C8},
	urldate = {2022-05-21},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Holloway, Peter E. and Pelinovsky, Efim and Talipova, Tatjana},
	month = aug,
	year = {1999},
	pages = {18333--18350},
}

@article{bull_probabilistic_2021,
	title = {Probabilistic {Inference} for {Structural} {Health} {Monitoring}: {New} {Modes} of {Learning} from {Data}},
	volume = {7},
	issn = {2376-7642, 2376-7642},
	shorttitle = {Probabilistic {Inference} for {Structural} {Health} {Monitoring}},
	url = {http://ascelibrary.org/doi/10.1061/AJRUA6.0001106},
	doi = {10.1061/AJRUA6.0001106},
	language = {en},
	number = {1},
	urldate = {2022-05-21},
	journal = {ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering},
	author = {Bull, Lawrence A. and Gardner, Paul and Rogers, Timothy J. and Cross, Elizabeth J. and Dervilis, Nikolaos and Worden, Keith},
	month = mar,
	year = {2021},
	pages = {03120003},
}

@article{girolami_statistical_2021,
	title = {The statistical finite element method ({statFEM}) for coherent synthesis of observation data and model predictions},
	volume = {375},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782520307180},
	doi = {10.1016/j.cma.2020.113533},
	language = {en},
	urldate = {2022-05-21},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Girolami, Mark and Febrianto, Eky and Yin, Ge and Cirak, Fehmi},
	month = mar,
	year = {2021},
	pages = {113533},
}

@article{tschannen_recent_2018,
	title = {Recent {Advances} in {Autoencoder}-{Based} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1812.05069},
	abstract = {Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.},
	urldate = {2022-05-21},
	journal = {arXiv:1812.05069 [cs, stat]},
	author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.05069},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{girin_dynamical_2021,
	title = {Dynamical {Variational} {Autoencoders}: {A} {Comprehensive} {Review}},
	volume = {15},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Dynamical {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2008.12595},
	doi = {10.1561/2200000089},
	abstract = {The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In recent years, a series of papers have presented different extensions of the VAE to process sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state space models. In this paper we perform an extensive literature review of these models. Importantly, we introduce and discuss a general class of models called Dynamical Variational Autoencoders (DVAEs) that encompasses a large subset of these temporal VAE extensions. Then we present in detail seven different instances of DVAE that were recently proposed in the literature, with an effort to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We reimplemented those seven DVAE models and we present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper is concluded with an extensive discussion on important issues concerning the DVAE class of models and future research guidelines.},
	number = {1-2},
	urldate = {2022-05-11},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
	year = {2021},
	note = {arXiv: 2008.12595},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--175},
}

@article{akyildiz_statistical_2021,
	title = {Statistical {Finite} {Elements} via {Langevin} {Dynamics}},
	url = {http://arxiv.org/abs/2110.11131},
	abstract = {The recent statistical finite element method (statFEM) provides a coherent statistical framework to synthesise finite element models with observed data. Through embedding uncertainty inside of the governing equations, finite element solutions are updated to give a posterior distribution which quantifies all sources of uncertainty associated with the model. However to incorporate all sources of uncertainty, one must integrate over the uncertainty associated with the model parameters, the known forward problem of uncertainty quantification. In this paper, we make use of Langevin dynamics to solve the statFEM forward problem, studying the utility of the unadjusted Langevin algorithm (ULA), a Metropolis-free Markov chain Monte Carlo sampler, to build a sample-based characterisation of this otherwise intractable measure. Due to the structure of the statFEM problem, these methods are able to solve the forward problem without explicit full PDE solves, requiring only sparse matrix-vector products. ULA is also gradient-based, and hence provides a scalable approach up to high degrees-of-freedom. Leveraging the theory behind Langevin-based samplers, we provide theoretical guarantees on sampler performance, demonstrating convergence, for both the prior and posterior, in the Kullback-Leibler divergence, and, in Wasserstein-2, with further results on the effect of preconditioning. Numerical experiments are also provided, for both the prior and posterior, to demonstrate the efficacy of the sampler, with a Python package also included.},
	urldate = {2022-05-11},
	journal = {arXiv:2110.11131 [cs, math, stat]},
	author = {Akyildiz, Omer Deniz and Duffin, Connor and Sabanis, Sotirios and Girolami, Mark},
	month = dec,
	year = {2021},
	note = {arXiv: 2110.11131},
	keywords = {Mathematics - Numerical Analysis, Statistics - Computation, Statistics - Machine Learning},
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2022-05-11},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fraccaro_disentangled_2017,
	title = {A {Disentangled} {Recognition} and {Nonlinear} {Dynamics} {Model} for {Unsupervised} {Learning}},
	url = {http://arxiv.org/abs/1710.05741},
	abstract = {This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.},
	urldate = {2022-05-11},
	journal = {arXiv:1710.05741 [cs, stat]},
	author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05741},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{duffin_statistical_2021,
	title = {Statistical finite elements for misspecified models},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2015006118},
	doi = {10.1073/pnas.2015006118},
	abstract = {Significance 
            Science and engineering have benefited greatly from the ability of finite element methods (FEMs) to simulate nonlinear, time-dependent complex systems. The recent advent of extensive data collection from such complex systems now raises the question of how to systematically incorporate these data into finite element models, consistently updating the solution in the face of mathematical model misspecification with physical reality. This article describes general and widely applicable methodology for the coherent synthesis of data with FEM models, providing a data-driven probability distribution that captures all sources of uncertainty in the pairing of FEM with measurements. 
          ,  
             
              We present a statistical finite element method for nonlinear, time-dependent phenomena, illustrated in the context of nonlinear internal waves (solitons). We take a Bayesian approach and leverage the finite element method to cast the statistical problem as a nonlinear Gaussian state–space model, updating the solution, in receipt of data, in a filtering framework. The method is applicable to problems across science and engineering for which finite element methods are appropriate. The Korteweg–de Vries equation for solitons is presented because it reflects the necessary complexity while being suitably familiar and succinct for pedagogical purposes. We present two algorithms to implement this method, based on the extended and ensemble Kalman filters, and demonstrate effectiveness with a simulation study and a case study with experimental data. The generality of our approach is demonstrated in 
               
                SI Appendix 
               
              , where we present examples from additional nonlinear, time-dependent partial differential equations (Burgers equation, Kuramoto–Sivashinsky equation).},
	language = {en},
	number = {2},
	urldate = {2022-05-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Duffin, Connor and Cripps, Edward and Stemler, Thomas and Girolami, Mark},
	month = jan,
	year = {2021},
	pages = {e2015006118},
}
