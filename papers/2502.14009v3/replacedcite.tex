\section{Related work}
\subsection{Benchmarked methods}
\label{sec:benchmark}
We provide a systematic review of existing state-of-the-art feedforward self-supervised methods from the literature i.e. methods which only require measurements $\y$ from a single acquisition at training and inference, without any ground truth. We formulate their loss functions and summarise all compared methods in \cref{tab:methods} along with their implementations in the DeepInverse ____ library. We focus on the design of the loss function since, in general, these loss functions are agnostic to the NN architecture and thus can be independently used with any latest architecture.

\begin{table*}[t]
\centering
\caption{Self-supervised losses reviewed in \cref{sec:benchmark}, implemented in DeepInverse and benchmarked in our experiments.}
\label{tab:methods}
\begin{tabular}{llll}
\hline
Method & Ref & Loss & Implementation \\ \hline
MC & ____ & \cref{eq:mc} & \verb+dinv.loss.MCLoss()+ \\
SSDU & ____ & \cref{eq:splitting} & \verb+dinv.loss.SplittingLoss(eval_split_input=False)+ \\
Noise2Inverse & ____ & \cref{eq:n2i} & \verb+dinv.loss.SplittingLoss(eval_split_input=True)+ \\
Weighted-SSDU & ____ & \cref{eq:millard} & \verb+dinv.loss.WeightedSplittingLoss()+ \\
Adversarial & ____ & \cref{eq:cole} & \verb+dinv.loss.UnsupAdversarialGeneratorLoss()+ \\
UAIR & ____ & \cref{eq:uair} & \verb+dinv.loss.UAIRGeneratorLoss()+ \\
VORTEX & ____ & \cref{eq:vortex} & \verb+dinv.loss.VORTEXLoss()+ \\
EI & ____ & \cref{eq:ei} & \verb+dinv.loss.EILoss()+ \\
MOI & ____ & \cref{eq:moi} & \verb+dinv.loss.MOILoss()+ \\
MO-EI & Ours & \cref{eq:moei} & \verb+dinv.loss.MOEILoss()+ \\ \hline
\end{tabular}
\end{table*}

\subsubsection{Measurement consistency}
The most naive self-supervised loss simply compares the recorrupted estimate $\A\xhat$ to the measurement $\y$:

\begin{equation}
    \L_\text{MC}=\L(\A\xhat,\y)
    \label{eq:mc}
\end{equation}

where $\L$ is any metric such as the L2 norm. This loss originally appeared with a classical regularising term to solve inverse problems with deep learning and traditional regularisation, but these still faced problems in designing the regularisers. Without a regularising term, the loss cannot recover information from the null-space of the operator $\mathcal{N}(\A)$ as infinite solutions $\A^\top\y+\mathbf{v}$ can satisfy measurement consistency (MC) where $\mathbf{v}\in\mathcal{N}(\A)$. This loss function is also used in the presence of strong inductive bias where the regularisation is provided by the architecture, for example from specific architectures ____ or initialisations ____, or in cycle consistency losses ____.

In the noisy scenario, some recent works such as ENSURE ____ or GSURE ____ generalise Stein's Unbiased Risk Estimator (SURE) to reconstruct from noisy undersampled measurements. However, in the noiseless case considered here $\sigma\rightarrow0$, these methods simply reduce to $\L_\text{MC}$, and so will not be compared.

\subsubsection{Learning from multiple operators}
A popular set of state-of-the-art approaches leverages the fact that the whole measurement space is spanned by multiple imaging operators. Measurement splitting methods, including SSDU and similar papers ____, randomly split the k-space into two sets at each iteration. One is used as the input and the other to construct the loss at the output:
\begin{equation}
    \L_\text{SSDU}=\L(\M_2\A_i \ft(\M_1\y_i,\M_1\A_i),\M_2\y_i)
    \label{eq:splitting}
\end{equation}

where $\M_1$ is a randomly generated mask, $\M_1+\M_2=\mathbb{I}_m$, and the inference-time estimate is $\xhat_i=\ft(\y_i,\A_i)$. Note that some methods ____ require pairs of measurements of the same subject; these are equivalent to SSDU but with 2x less acceleration. Similarly, Artifact2Artifact ____, when applied to static MRI reconstruction without paired measurements, is also identical to SSDU (see \cref{sec:a2a} for proof). We therefore consider SSDU as the archetypal method to compare.

An inference-time adaptation to SSDU performs a Monte-Carlo averaging over subsampled inputs; this is inspired by Noise2Inverse ____:
\begin{equation}
    \xhat=\frac{1}{N}\Sigma_{i=1}^n \ft(\M_i\y,\M_i\A)
    \label{eq:n2i}
\end{equation}

More recently, ____ frames SSDU as a Bernoulli-noise special case of Noisier2Noise ____, and propose a weighting to the SSDU loss and an informed splitting strategy to improve SSDU:

\begin{align}
    \L_\text{Weighted-SSDU}&=(1-\mathbf{K})^{-1/2}\L_\text{SSDU} \\
    \mathbf{K}&=(\mathbb{I}_n-\tilde{\mathbf{P}}\mathbf{P})^{-1}(\mathbb{I}_n-\mathbf{P})
\label{eq:millard}
\end{align}

where $\mathbf{P}=\mathbb{E}[\M_i],\tilde{\mathbf{P}}=\mathbb{E}[\M_1]$ i.e. the PDFs of the imaging mask and the splitting mask, respectively.

Finally, Multi-Operator Imaging (MOI) ____ leverages the fact that the image set is seen through multiple operators $\A_i$ by drawing a random operator $\tilde\A\sim\mathcal{A}$ at each forward pass to learn in the null-space:

\begin{equation}
    \L_\text{MOI}=\L_\text{MC}+\L(\xhat,\ft(\tilde\A\xhat))
    \label{eq:moi}
\end{equation}

\subsubsection{Equivariant imaging}
Equivariant Imaging (EI) ____ is a state-of-the-art method that constrains the set of solutions using group invariance, which has been applied to MRI in ____, optical camera imaging in ____, and dynamic MRI in ____. By leveraging a natural assumption that the (unknown) image set $\mathcal{X}$ is invariant to a group $G$ of transformations $g\x\in\mathcal{X}\forall\x\in\mathcal{X},g\in G$, the image set is observed through multiple transformed operators $\A_i\circ g(\cdot)$ allowing the solver to "see" into the null-space. The assumption is constrained using the following loss:

\begin{equation}
    \L_\text{EI}=\L_\text{MC}+\L(\Tg\xhat,\ft(\A\Tg\xhat))
    \label{eq:ei}
\end{equation}

where $\Tg:G\rightarrow\text{Sym}(\mathcal{X})$ is an action of the group $G$.

\subsubsection{Data augmentation}
EI is related to data augmentation (DA), which, when ground truth $\x_\GT$ is available, can be used to constrain invariance or equivariance on $\x$, applied to MRI in ____. VORTEX ____ builds on this in the semi-supervised context by performing data augmentation on the measurements when ground truth is not available:

\begin{equation}
\L_\text{VORTEX}=
\begin{cases} 
\L_\text{sup}(\ft(\y),\x_\GT) & \text{if GT exists} \\
\L(\T_2\ft(y,\A),\ft(\T_1\y,\A\T_2^{-1})) & \text{else}
\end{cases}
\label{eq:vortex}
\end{equation}

where $\T_1$ is a random k-space transform (e.g. add noise, phase shift) and $\T_2$ is a random image transform (e.g. shift, rotate). We consider the fully unsupervised case, replacing $\L_\text{sup}$ with $\L_\text{MC}$ to enforce measurement consistency, noting that we do not expect this to learn any new information as data in the null-space remains unobserved. 

\subsubsection{Adversarial losses}
Adversarial losses have been proposed to reconstruct MRI via the dual training of a generator $\ft$ and a discriminator $D$. While unconditional, generative adversarial networks such as AmbientGAN ____ are not feedforward at inference time, ____ propose a feedforward method with an adversarial loss for training:

\begin{equation}
    \L_\text{adversarial}=D(\tilde\A\ft(\y),\tilde\y)
    \label{eq:cole}
\end{equation}

where $\tilde\y\sim\mathcal{Y}$ i.e. another randomly sampled "real" measurement from the training dataset $\mathcal{Y}$. $D$'s aim is to distinguish between reconstructed measurements and training measurements, such that $\ft$'s aim is to reconstruct high quality measurements that are distinguishable from "real" measurements. Note that in a non-generative context, it is unclear how this loss can learn the signal model and recover information from the null-space. UAIR ____ is a related method using the following loss, originally proposed for inpainting:

\begin{equation}
    \hat\y=\tilde\A \ft(\y),\;\L_\text{UAIR}=D(\hat\y,\y)+\L(\tilde\A\ft(\hat\y),\hat\y)
    \label{eq:uair}    
\end{equation}

\subsubsection{Non-feedforward methods}

Finally, we do not consider ground-truth-free methods that require lengthy iterative procedures or optimisation at inference-time, since these are impractical in real-world application, including Deep Image Prior methods ____, generative models e.g. AmbientGAN ____, diffusion methods ____, implicit neural representation methods ____, or self-supervised plug-and-play training methods such as ____.

\subsection{Other inverse problems}
\label{sec:other_problems}
\subsubsection{Noisy problems}
\label{sec:noisy_problems}

In this paper, we separate the task of solving ill-posed problems with removing measurement noise. To achieve the latter, it is often possible to add a denoising term to the loss function. For example, Robust EI ____ adds the Stein's Unbiased Risk Estimator (SURE) term, which is adapted by ENSURE ____. Noise2Recon ____ and Robust SSDU ____ both add a term similar to Noisier2Noise ____ to perform denoising on top of SSDU.

For a comparison of state-of-the-art denoising methods in solving inverse problems, see ____.

\subsubsection{Single-operator problems}
\label{sec:single_operator}

Many of the methods considered in \cref{sec:benchmark} rely on the assumption that the image set is imaged with multiple operators, and that these operators span the full measurement domain in expectation. However, it is often the case, both in MRI and in general inverse problems, that all measurements are made with a single operator $\A$, for example a constant mask or blur kernel. In this case, we note that only the Equivariant Imaging (EI) losses ____ do not require the assumption and thus can still be used.