@article{ABL,
  title={Anti-backdoor learning: Training clean models on poisoned data},
  author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14900--14912},
  year={2021}
}

@article{ANP,
  title={Adversarial neuron pruning purifies backdoored deep models},
  author={Wu, Dongxian and Wang, Yisen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16913--16925},
  year={2021}
}

@article{CL,
  title={Poison frogs! targeted clean-label poisoning attacks on neural networks},
  author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{DST,
  title={Effective backdoor defense by exploiting sensitivity of poisoned samples},
  author={Chen, Weixin and Wu, Baoyuan and Wang, Haoqian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9727--9737},
  year={2022}
}

@article{Dynamic,
  title={Input-aware dynamic backdoor attack},
  author={Nguyen, Tuan Anh and Tran, Anh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3454--3464},
  year={2020}
}

@inproceedings{FP,
  title={Fine-pruning: Defending against backdooring attacks on deep neural networks},
  author={Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
  booktitle={International symposium on research in attacks, intrusions, and defenses},
  pages={273--294},
  year={2018},
  organization={Springer}
}

@inproceedings{FTrojan,
  title={An invisible black-box backdoor attack through frequency domain},
  author={Wang, Tong and Yao, Yuan and Xu, Feng and An, Shengwei and Tong, Hanghang and Wang, Ting},
  booktitle={European Conference on Computer Vision},
  pages={396--413},
  year={2022},
  organization={Springer}
}

@article{I_BAU,
  title={Adversarial unlearning of backdoors via implicit hypergradient},
  author={Zeng, Yi and Chen, Si and Park, Won and Mao, Z Morley and Jin, Ming and Jia, Ruoxi},
  journal={arXiv preprint arXiv:2110.03735},
  year={2021}
}

@inproceedings{SIG,
  title={A new backdoor attack in cnns by training set corruption without label poisoning},
  author={Barni, Mauro and Kallas, Kassem and Tondi, Benedetta},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)},
  pages={101--105},
  year={2019},
  organization={IEEE}
}

@article{blended,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}

@article{gu2017badnets,
  title={Badnets: Identifying vulnerabilities in the machine learning model supply chain},
  author={Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={arXiv preprint arXiv:1708.06733},
  year={2017}
}

@article{huang2022backdoor,
  title={Backdoor defense via decoupling the training process},
  author={Huang, Kunzhe and Li, Yiming and Wu, Baoyuan and Qin, Zhan and Ren, Kui},
  journal={arXiv preprint arXiv:2202.03423},
  year={2022}
}

