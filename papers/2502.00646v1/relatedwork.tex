\section{Related Works}
\textbf{Backdoor Attack.} The objective of a backdoor attack is to stealthily perform a successful attack on the model, and trigger design along with poisoning methods targeting the model are key aspects of achieving this goal.  Backdoor attacks were first introduced by \cite{gu2017badnets}, which applied a fixed pixel patch in the corner of an image. To enhance stealthiness, Blended\cite{blended} was proposed, which mixes a transparent trigger with the original image. SIG\cite{SIG} stealthily embeds superimposed signals, such as ramp or sinusoidal signals, into the background of images. Dynamic\cite{Dynamic} backdoor attacks consider sample-wise triggers generated to confuse defenses and improve the chances of bypassing detection. Clean Label Attack\cite{CL} poisons the data without altering the labels. FTrojan\cite{FTrojan} focuses on applying backdoor attacks in the frequency domain. 

\noindent\textbf{Backdoor Defense.} Backdoor defenses can mainly be categorized into two types: post-process and in-process. Post-process methods typically mitigate the backdoor effect by pruning neurons or fine-tuning the model, such as FP\cite{FP}, ANP\cite{ANP}, and I-BAU\cite{I_BAU}. In-process methods, on the other hand, focus on safely training the model after the data has been poisoned. This process involves isolating or suppressing backdoor injection during training, with methods such as ABL\cite{ABL}, DBD \cite{huang2022backdoor} and DST\cite{DST}.