[
  {
    "index": 0,
    "papers": [
      {
        "key": "gu2017badnets",
        "author": "Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth",
        "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "blended",
        "author": "Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn",
        "title": "Targeted backdoor attacks on deep learning systems using data poisoning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "SIG",
        "author": "Barni, Mauro and Kallas, Kassem and Tondi, Benedetta",
        "title": "A new backdoor attack in cnns by training set corruption without label poisoning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Dynamic",
        "author": "Nguyen, Tuan Anh and Tran, Anh",
        "title": "Input-aware dynamic backdoor attack"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "CL",
        "author": "Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom",
        "title": "Poison frogs! targeted clean-label poisoning attacks on neural networks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "FTrojan",
        "author": "Wang, Tong and Yao, Yuan and Xu, Feng and An, Shengwei and Tong, Hanghang and Wang, Ting",
        "title": "An invisible black-box backdoor attack through frequency domain"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "FP",
        "author": "Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth",
        "title": "Fine-pruning: Defending against backdooring attacks on deep neural networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ANP",
        "author": "Wu, Dongxian and Wang, Yisen",
        "title": "Adversarial neuron pruning purifies backdoored deep models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "I_BAU",
        "author": "Zeng, Yi and Chen, Si and Park, Won and Mao, Z Morley and Jin, Ming and Jia, Ruoxi",
        "title": "Adversarial unlearning of backdoors via implicit hypergradient"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ABL",
        "author": "Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun",
        "title": "Anti-backdoor learning: Training clean models on poisoned data"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "huang2022backdoor",
        "author": "Huang, Kunzhe and Li, Yiming and Wu, Baoyuan and Qin, Zhan and Ren, Kui",
        "title": "Backdoor defense via decoupling the training process"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "DST",
        "author": "Chen, Weixin and Wu, Baoyuan and Wang, Haoqian",
        "title": "Effective backdoor defense by exploiting sensitivity of poisoned samples"
      }
    ]
  }
]