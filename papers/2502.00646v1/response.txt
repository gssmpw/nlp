\section{Related Works}
\textbf{Backdoor Attack.} The objective of a backdoor attack is to stealthily perform a successful attack on the model, and trigger design along with poisoning methods targeting the model are key aspects of achieving this goal.  Backdoor attacks were first introduced by Gu et al., "Backdoors in Deep Neural Networks" which applied a fixed pixel patch in the corner of an image. To enhance stealthiness, Blended Attack was proposed, which mixes a transparent trigger with the original image. SIGAIN stealthily embeds superimposed signals, such as ramp or sinusoidal signals, into the background of images. Dynamic Backdoor attacks consider sample-wise triggers generated to confuse defenses and improve the chances of bypassing detection. Clean Label Attack poisons the data without altering the labels. FTrojan focuses on applying backdoor attacks in the frequency domain. 

\noindent\textbf{Backdoor Defense.} Backdoor defenses can mainly be categorized into two types: post-process and in-process. Post-process methods typically mitigate the backdoor effect by pruning neurons or fine-tuning the model, such as FPNN, ANPWN, and I-BAUDS. In-process methods, on the other hand, focus on safely training the model after the data has been poisoned. This process involves isolating or suppressing backdoor injection during training, with methods such as ABLearn, DBDnet, and DSTL.