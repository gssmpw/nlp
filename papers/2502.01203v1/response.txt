\section{Related Works}
% \todoa{More related works}
\textbf{Multiple References:} Inspired by model soups **Vandal, "Model Soups: A Recipe for Transfer Learning"**, ____ propose a reference soup policy, achieved by averaging two independently trained supervised fine-tuned models, including the reference model. However, their approach lacks theoretical guarantees, particularly regarding its applicability to alignment tasks. More recently, **Sachan, "Multiple Reference Models for Alignment Tasks"** introduced the concept of multiple reference models for alignment. Due to the challenges in deriving a closed-form solution for the RLHF objective under multiple referencing constraints, the authors proposed a lower-bound approximation. In this work, we address this gap by deriving the closed-form solution for the multiple reference model scenario under reverse KL-regularization.

\textbf{Theoretical Foundation of RLHF:} Several works have studied the theoretical underpinnings of reverse KL-regularized RLHF, particularly in terms of sample complexity **Bachman et al., "On the Design of Neural Network Architectures for RLHF"**. Among these, **Dvijal, "Reverse KL-Regularized RLHF: Analysis and Bounds on Sub-Optimality Gap"** analyze reverse KL-regularized RLHF, demonstrating the effect of reverse KL-regularization and establishing an upper bound on sub-optimality gap with $O(1/n)$ sample complexity (convergence rate)  where $n$ represents the size of preference dataset. More detailed comparison with these works is provided in Section~\ref{sec:disc}. However, to the best of our knowledge, the RLHF framework incorporating multiple reference models has not yet been studied.

\textbf{Forward KL-regularization and Alignment:} The forward KL-regularization for Direct Preference Optimization (DPO) proposed by **Guo et al., "Forward KL-Regularized DPO: A Novel Approach to Preference Learning"**. The application of forward KL-regularization for alignment from demonstrations is shown in **Kumar, "Forward KL-Regularization for Alignment Tasks"**. The forward KL-regularization in stochastic decision problems is also studied by **Mehra et al., "Forward KL-Regularized RLHF: A New Perspective on Preference Learning"**. To the best of our knowledge, the forward KL-regularized RLHF is not studied from a theoretical perspective.