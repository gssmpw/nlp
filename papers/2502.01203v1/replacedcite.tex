\section{Related Works}
% \todoa{More related works}
\textbf{Multiple References:} Inspired by model soups ____, ____ propose a reference soup policy, achieved by averaging two independently trained supervised fine-tuned models, including the reference model. However, their approach lacks theoretical guarantees, particularly regarding its applicability to alignment tasks. More recently, ____ introduced the concept of multiple reference models for alignment. Due to the challenges in deriving a closed-form solution for the RLHF objective under multiple referencing constraints, the authors proposed a lower-bound approximation. In this work, we address this gap by deriving the closed-form solution for the multiple reference model scenario under reverse KL-regularization.

\textbf{Theoretical Foundation of RLHF:} Several works have studied the theoretical underpinnings of reverse KL-regularized RLHF, particularly in terms of sample complexity ____. Among these, ____ analyze reverse KL-regularized RLHF, demonstrating the effect of reverse KL-regularization and establishing an upper bound on sub-optimality gap with $O(1/n)$ sample complexity (convergence rate)  where $n$ represents the size of preference dataset. More detailed comparison with these works is provided in Section~\ref{sec:disc}. However, to the best of our knowledge, the RLHF framework incorporating multiple reference models has not yet been studied. 

\textbf{Forward KL-regularization and Alignment:} The forward KL-regularization for Direct Preference Optimization (DPO) proposed by ____. The application of forward KL-regularization for alignment from demonstrations is shown in ____. The forward KL-regularization in stochastic decision problems is also studied by ____. To the best of our knowledge, the forward KL-regularized RLHF is not studied from a theoretical perspective.