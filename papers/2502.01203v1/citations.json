[
  {
    "index": 0,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chegini2024salsa",
        "author": "Chegini, Atoosa and Kazemi, Hamid and Mirzadeh, Iman and Yin, Dong and Horton, Maxwell and Nabi, Moin and Farajtabar, Mehrdad and Alizadeh, Keivan",
        "title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "le2024multi",
        "author": "Le, Hung and Tran, Quan and Nguyen, Dung and Do, Kien and Mittal, Saloni and Ogueji, Kelechi and Venkatesh, Svetha",
        "title": "Multi-Reference Preference Optimization for Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhao2024sharp",
        "author": "Zhao, Heyang and Ye, Chenlu and Gu, Quanquan and Zhang, Tong",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      },
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      },
      {
        "key": "song2024importance",
        "author": "Song, Yuda and Swamy, Gokul and Singh, Aarti and Bagnell, Drew and Sun, Wen",
        "title": "The importance of online data: Understanding preference fine-tuning via coverage"
      },
      {
        "key": "zhan2023provable",
        "author": "Zhan, Wenhao and Uehara, Masatoshi and Kallus, Nathan and Lee, Jason D and Sun, Wen",
        "title": "Provable offline preference-based reinforcement learning"
      },
      {
        "key": "ye2024theoretical",
        "author": "Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong",
        "title": "A theoretical analysis of nash learning from human feedback under general kl-regularized preference"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhao2024sharp",
        "author": "Zhao, Heyang and Ye, Chenlu and Gu, Quanquan and Zhang, Tong",
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wangbeyond",
        "author": "Wang, Chaoqi and Jiang, Yibo and Yang, Chenghao and Liu, Han and Chen, Yuxin",
        "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sun2024inverse",
        "author": "Sun, Hao and van der Schaar, Mihaela",
        "title": "Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "cohen2017data",
        "author": "Cohen, Samuel N",
        "title": "Data-driven nonlinear expectations for statistical uncertainty in decisions"
      }
    ]
  }
]