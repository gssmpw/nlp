%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage{fullpage}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
\usepackage{algorithm}
\usepackage{geometry}

\geometry{
  a4paper,
  % total={170mm,257mm},
  left=20mm,
  top=20mm,
  right=20mm,
  bottom=20mm,
}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{algpseudocode}
\usepackage{booktabs}  % For \toprule, \midrule, \bottomrule
\usepackage{makecell}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% \usepackage{icml2025}
\usepackage{natbib}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{enumitem}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\renewcommand \thepart{}
\renewcommand \partname{}
\algnewcommand{\LeftComment}[1]{\(\triangleright\) {\color{LightCyan}{#1}}}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pifont}
% \usepackage{pmb}
\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}
% \usepackage
% \usepackage[sort,round]{natbib}	
% \usepackage{tikz}
% \usetikzlibrary{trees, shadows, shapes.geometric}
% for textual citations


\newcommand{\ahmad}[1]{{\noindent \textit{\small\textcolor{red}{Ahmad: #1}}}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother

% \makeatletter
% \newcommand{\maketitlepageapp}{%
% %   \begin{titlepage}
%     \let\thanks\@gobble
%     \let\footnote\@gobble
%     \@maketitle
%     % \thispagestyle{empty}
% %   \end{titlepage}%
% }
% \makeatother


\definecolor{royalblue}{rgb}{0.25, 0.41, 0.88}
\hypersetup{
  colorlinks,
  citecolor=royalblue,
  linkcolor=royalblue,
  urlcolor=royalblue}  

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newreptheorem{theorem}{Theorem}
\newreptheorem{lemma}{Lemma}
\newreptheorem{proposition}{Proposition}
\newreptheorem{corollary}{Corollary}



% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}
% \newcommand\todoa{}{\color{red} \textbf{GA:}}
\newcommand{\red}{\color{red}}
\newcommand{\mrd}{\mathrm{d}}
\newcommand{\armin}{\color{black}}
\newcommand{\blue}{\color{black}}
\newcommand{\lse}{\mathrm{LSE}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}
\newcommand{\arenyi}{$\alpha$-R\'enyi Regularization}
\newcommand{\KLr}{\mathrm{KL}}
\newcommand{\LA}{h}
\newcommand{\alKL}{\mathrm{D}_{\alpha}}
\newcommand{\mbE}{\mathbb{E}}
\newcommand{\pirefbetay}{\bar{\pi}_{\pmb{\beta},\mathrm{ref}}(y|x)}
\newcommand{\pirefbetacdot}{\bar{\pi}_{\pmb{\beta},\mathrm{ref}}(\cdot|x)}
\newcommand{\thetas}{\theta^{\star}}
\newcommand{\thetah}{\widehat{\theta}}
\newcommand{\pirefalpha}{\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(\cdot|x)}
\newcommand{\pif}{\tilde{\pi}}
% \newcommand{\mbE}{\mathbb{E}}
% \newcommand{\mrd}{\mathrm{d}}
\usepackage{tcolorbox}

\newcommand{\ga}[1]{\textcolor{blue}{#1}}
\newcommand{\ym}[1]{\textcolor{magenta}{#1}}
\newcommand{\amira}[1]{\textcolor{purple}{#1}}


\usepackage{xspace}
\usepackage[colorinlistoftodos, color=blue!30!white %   ,shadow
 %,disable
]{todonotes}                                        
\setlength{\marginparwidth}{12ex}
\newcommand{\todoa}[2][]{\todo[size=tiny,color=blue!20!white,#1]{GA: #2}\xspace}
\newcommand{\todob}[2][]{\todo[size=tiny,color=gray!20!white,#1]{AM: #2}\xspace}
\newcommand{\todom}[2][]{\todo[size=tiny,color=gray!20!white,#1]{YM: #2}\xspace}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\title{Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models}

\author{
  Gholamali Aminian$^{1}$
  \and
  Amir R. Asadi$^{2}$
  \and
   Idan Shenfeld $^{3}$
   \and 
   Youssef Mroueh $^{4}$
}

\begin{document}
\maketitle
\renewcommand\thefootnote{$^1$}\footnotetext{The Alan Turing Institute.}
\renewcommand\thefootnote{$^2$}\footnotetext{Statistical Laboratory, University of Cambridge.}
\renewcommand\thefootnote{$^3$}\footnotetext{Massachusetts Institute of Technology.}
\renewcommand\thefootnote{$^4$}\footnotetext{IBM Research.}
% \twocolumn[
% \icmltitle{Theoretical Analysis of KL-regularized RLHF with Multiple Reference Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{RLHF, Theoretical Analysis}

% \vskip 0.3in
% ]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent methods for aligning large language models (LLMs) with human feedback predominantly rely on a single reference model, which limits diversity, model overfitting, and underutilizes the wide range of available pre-trained models. Incorporating multiple reference models has the potential to address these limitations by broadening perspectives, reducing bias, and leveraging the strengths of diverse open-source LLMs. However, integrating multiple reference models into reinforcement learning with human feedback (RLHF) frameworks poses significant theoretical challenges, particularly in reverse KL-regularization, where achieving exact solutions has remained an open problem. This paper presents the first \emph{exact solution} to the multiple reference model problem in reverse KL-regularized RLHF. We introduce a comprehensive theoretical framework that includes rigorous statistical analysis and provides sample complexity guarantees. Additionally, we extend our analysis to forward KL-regularized RLHF, offering new insights into sample complexity requirements in multiple reference scenarios. Our contributions lay the foundation for more advanced and adaptable LLM alignment techniques, enabling the effective use of multiple reference models. This work paves the way for developing alignment frameworks that are both theoretically sound and better suited to the challenges of modern AI ecosystems.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}

Large language models (LLMs) have revolutionized natural language processing (NLP) by demonstrating remarkable capabilities in understanding and generating human language. Powered by vast datasets and advanced neural architectures, these models have set new benchmarks across various NLP tasks, including machine translation and conversational agents. Despite these advancements, aligning LLMs with human values and preferences from a \textit{single reference model} remains a critical challenge. Such misalignment can lead to undesirable behaviors, including the generation of biased or inappropriate content, which undermines the reliability and safety of these models \citep{gehman2020realtoxicityprompts}.

However, relying on a single reference model for alignment introduces several limitations. First, it offers a narrow perspective by restricting the diversity of knowledge and linguistic patterns available for fine-tuning, akin to teaching a language student solely through one teacher's style—consistent but lacking the richness of multiple viewpoints. Second, it risks overfitting to the reference model, biasing the optimization process toward its quirks or deficiencies rather than enabling generalization to broader human preferences; for instance, a translation model trained on one corpus may struggle with dialects or uncommon syntactic structures. Third, such an approach is inefficient in utilizing the wealth of pre-trained models available in modern AI ecosystems, which excel in different domains and capture unique linguistic nuances, leaving valuable collective intelligence untapped. Therefore, incorporating multiple LLMs as reference models produces a model that reflects the characteristics of all reference models while satisfying human preferences. This approach is particularly relevant as the open-source community continues to release diverse pre-trained and fine-tuned LLMs of varying scales, trained on a wide range of datasets \citep{jiang2023mistral,penedo2023refinedweb}. 

Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal framework for addressing alignment challenges in LLMs with a single reference model. By fine-tuning LLMs based on human feedback, RLHF steers models towards more human-aligned behaviors, enhancing truthfulness, helpfulness, and harmlessness while maintaining their ability to generate accurate and high-probability outputs \citep{wirth2017survey,christiano2017deep}. In RLHF, reward-based methods use a trained reward model to evaluate (prompt, response) pairs. These methods treat the language model as a policy that takes a prompt $x$ and generates a response $y$ conditioned on $x$, optimizing this policy to align with human feedback. 

While RLHF with multiple reference models has demonstrated practical utility \citep{le2024multi}, its theoretical underpinnings remain largely unexplored. A critical gap in current understanding is the lack of an exact solution for reverse KL-regularized RLHF when incorporating multiple reference models. This theoretical limitation has prevented the study of sample complexity of bounds on both optimality and sub-optimality gaps in the reverse KL-regularized framework. Addressing this problem is crucial for advancing the alignment of LLMs with human preferences in increasingly complex and diverse settings.

In this work, we provide the solutions for RLHF with multiple reference models when regularized via Reverse KL divergence (RKL) or forward KL divergence (FKL). In addition, we provide a statistical analysis of these scenarios. Our main contributions are as follows:
\begin{itemize}
    \item We propose a comprehensive mathematical framework for reverse KL-regularized RLHF with multiple reference models and provide the exact solution for this problem and calculate the maximum objective value. 
    \item We provide theoretical guarantees for the proposed multiple reference models scenario under reverse KL-regularization. In particular, we study the sample complexity\footnote{The sample complexity provides insight into how quickly bounds converge as the dataset size increases.} of reverse KL-regularized RLHF under multiple reference models. 
    \item We also study the multiple reference models scenario under forward KL-regularized RLHF and analyze its sample complexity.
\end{itemize}

\section{Related Works}
% \todoa{More related works}
\textbf{Multiple References:} Inspired by model soups \citep{wortsman2022model}, \citet{chegini2024salsa} propose a reference soup policy, achieved by averaging two independently trained supervised fine-tuned models, including the reference model. However, their approach lacks theoretical guarantees, particularly regarding its applicability to alignment tasks. More recently, \citet{le2024multi} introduced the concept of multiple reference models for alignment. Due to the challenges in deriving a closed-form solution for the RLHF objective under multiple referencing constraints, the authors proposed a lower-bound approximation. In this work, we address this gap by deriving the closed-form solution for the multiple reference model scenario under reverse KL-regularization.

\textbf{Theoretical Foundation of RLHF:} Several works have studied the theoretical underpinnings of reverse KL-regularized RLHF, particularly in terms of sample complexity \citep{zhao2024sharp,xiong2024iterative,song2024importance,zhan2023provable,ye2024theoretical}. Among these, \citet{zhao2024sharp} analyze reverse KL-regularized RLHF, demonstrating the effect of reverse KL-regularization and establishing an upper bound on sub-optimality gap with $O(1/n)$ sample complexity (convergence rate)  where $n$ represents the size of preference dataset. More detailed comparison with these works is provided in Section~\ref{sec:disc}. However, to the best of our knowledge, the RLHF framework incorporating multiple reference models has not yet been studied. 

\textbf{Forward KL-regularization and Alignment:} The forward KL-regularization for Direct Preference Optimization (DPO) proposed by \citet{wangbeyond}. The application of forward KL-regularization for alignment from demonstrations is shown in \citep{sun2024inverse}. The forward KL-regularization in stochastic decision problems is also studied by \citet{cohen2017data}. To the best of our knowledge, the forward KL-regularized RLHF is not studied from a theoretical perspective. 

\section{Preliminaries}\label{sec:Preliminaries}

\paragraph{Notations:} %Throughout the paper,
Upper-case letters denote random variables (e.g., $Z$), lower-case letters denote the realizations of random variables (e.g., $z$), and calligraphic letters denote sets (e.g., $\mathcal{Z}$). 
All logarithms are in the natural base. The set of 
probability distributions (measures) over a space $\mathcal{X}$ with finite variance is denoted by $\mathcal{P}(\mathcal{X})$. The KL-divergence between two
 probability distributions on $\mathbb{R}^d$ 
 with densities $p(x)$ and $q(x),$ 
 so that $q(x) > 0$ when $p(x) > 0$,  
is $\KLr(p\|q) := \int_{\mathbb{R}^d} p(x)\log(p(x)/q(x))\mrd x$ (with $0/0:=0$). The entropy of a distribution $p(x)$ is denoted by $H(p)=-\int_{\mathbb{R}^d} p(x)\log(p(x))$.

 We define the Escort and Generalized Escort distributions \citep{bercher2012simple} ( a.k.a. geometric transformation).
\begin{definition}[Escort and Generalized Escort Distributions] Given a discrete probability measure $P$ defined on a set $\mathcal{A}$, and any $\lambda\in [0,1]$, we define the escort distribution $(P)^{\lambda}$ for all $a\in \mathcal{A}$ as 
\begin{equation*}
	(P)^{\lambda}(a):=\frac{(P(a))^{\lambda}}{\sum_{x\in \mathcal{A}}(P(x))^{\lambda}}.
\end{equation*}
Given two discrete probability measures $P$ and $Q$ defined on a set $\mathcal{A}$, and any $\lambda\in [0,1]$, we define the tilted distribution $(P,Q)^{\lambda}$ as the following generalized escort distribution:
\begin{equation*}
	(P,Q)^{\lambda}(a):= \frac{P^{\lambda}(a)Q^{1-\lambda}(a)}{\sum_{x\in \mathcal{A}}P^{\lambda}(x)Q^{1-\lambda}(x)}.
\end{equation*}
\end{definition}

Next, we introduce the functional derivative, see~\cite{cardaliaguet2019master}. 
\begin{definition}{\citep{cardaliaguet2019master}}
\label{def:flatDerivative}
A functional $U:\mathcal P(\mathbb R^n) \to \mathbb R$ %is said to 
admits a functional derivative
if there is a map $\frac{\delta U}{\delta m} : \mathcal P(\mathbb R^n) \times \mathbb R^n \to \mathbb R$ which is continuous on $\mathcal P(\mathbb R^n)$ and, for all
$m, m' \in\mathcal P(\mathbb R^n)$, it holds that
\begin{align*}
&U(m') - U(m) =\!\int_0^1 \int_{\mathbb{R}^n} \frac{\delta U}{\delta m}(m_\lambda,a) \, (m'
-m)(da)\,\mrd \lambda,
\end{align*}
where $m_\lambda=m + \lambda(m' - m)$.
\end{definition}
We also define the sensitivity of a policy $\pi_r(y|x)$, which is a function of reward function $r(x,y)$, with respect to the reward function as
\begin{equation}
    \frac{\partial \pi}{\partial r}(r):=\lim_{\Delta r\rightarrow 0}\frac{\pi_r(y|x)-\pi_{r+\Delta r}(y|x)}{\Delta r}.
\end{equation}


\section{Problem Formulation}
Following prior works \citep{ye2024theoretical,zhao2024sharp}, we consider the problem of aligning a policy $\pi$ with human preferences. Given an input (prompt) $x \in \mathcal{X}$ which is samples from $\rho(x)$, is the finite space of input texts, the policy $\pi\in\Pi$, where $\Pi$ is the set of policies, models a conditional probability distribution $\pi(y|x)$ over the finite space of output texts $y \in \mathcal{Y}$. From a given $\pi$ and $x$, we can sample an output (response) $y \sim \pi(\cdot|x)$.

\paragraph{Preference Dataset:} Preference data is generated by sampling two outputs $(y, y')|x$ from $\pi_{\mathrm{ref}}$ as the reference policy (model), and presenting them to an agent, typically a human, for rating to indicate which one is preferred. For example, $y \succ y'$ denotes that $y$ is preferred to $y'$. A preference dataset is then denoted as $D = \{y_i^w, y_i^l,x^i\}_{i=1}^n$, where $n$ is the number of data points, $y_w$ and $y_l$ denote the preferred (chosen) and dispreferred (rejected) outputs, respectively. 

We assume that there exists a true model of the agent's preference $p^*(y \succ y'|x)$, which assigns the probability of $y$ being preferred to $y'$ given $x$ based on the latent reward model which is unknown. 

\subsection{RLHF from One Reference Model} Using the dataset $\mathcal{D}$, our goal is to find a policy $\pi$ that maximizes the expected preference while being close to a reference policy $\pi_{\mathrm{ref}}$. In this approach, Bradley-Terry model \cite{bradley1952rank} is employed
as the preference model:
\begin{equation}
p\Big(y\succ y'|x\Big)=\sigma\Big(r_{\theta}\Big(x,y\Big)-r_{\theta}\Big(x,y'\Big)\Big),
\end{equation}
where $\sigma$ denotes the sigmoid function and $r:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}$
is a reward model parameterized by $\theta,$ which assigns a scalar
score to indicate the suitability of output $y$ for input $x$. In \cite{christiano2017deep}, the reward model is trained
on $D$ to maximize the log-likelihood (MLE) estimator:
\begin{equation}
\mathcal{L}_{R}(\theta,D)=\sum_{i=1}^n\frac{1}{n}\log\sigma\Big(r_{\theta}\Big(x_i,y_{w}^i\Big)-r_{\theta}\Big(x_i,y_{l}^i\Big)\Big).
\end{equation}
Given a trained reward model $r_{\thetah}(x,y)$ where $\thetah =\mathop{\arg \max}_{\theta\in\Theta}\mathcal{L}_{R}(\theta,D)$, we can consider the regularized optimization objective which is regularized via reverse KL-regularized or forward KL-regularized.

\textbf{Reverse KL-regularized RLHF:} A crucial component of RLHF is the use of a reference model to compute a Reverse Kullback-Leibler (KL) divergence penalty. This penalty ensures that the fine-tuning process does not deviate excessively from the original model, mitigating the risk of generating nonsensical responses \citep{ziegler2019fine}. The reverse KL-regularized optimization objective for $(\gamma>0)$ can represented as:
\begin{align}\label{eq:pref1-rlhf}
 & \underset{\pi}{\max}\mathbb{E}_{Y\sim \pi(\cdot|x)}\Big[r_{\thetah}\Big(x,Y\Big)\Big]-\frac{1}{\gamma}\KLr\Big(\pi(\cdot|x)\Big\Vert \pi_{\mathrm{ref}}(\cdot|x)\Big.\Big),
\end{align}
Note that the solution of \eqref{eq:pref1-rlhf} is,
\begin{equation}\label{eq: pithetah}
    \pi_{\thetah}^{\gamma}(y|x):=\frac{\pi_{\mathrm{ref}}(y|x)\exp(\gamma r_{\thetah}(x,y))}{Z(x)},
\end{equation}
where $Z(x)=\mathbb{E}_{Y\sim \pi_{\mathrm{ref}(y|x) }}[\exp(\gamma r_{\thetah}(x,Y))]$ is the normalization factor. Similarly, we can define $\pi_{\thetas}^{\gamma}(y|x)$ using $r_{\thetas}(x,y)$ instead of $r_{\thetah}(x,y)$ in \eqref{eq: pithetah}. This RLHF objective is employed to train LLMs such as Instruct-GPT \cite{ouyang2022training} using PPO \cite{schulman2017proximal}. 

We define $J(\pi_{\thetas}(\cdot|x))=\mbE_{Y\sim\pi_{\thetas}(\cdot|x)}[r_{\thetas}(x,Y)]$ (a.k.a. value function\footnote{We can also consider $\mbE_{X\sim\rho(\cdot)}[J(\pi(\cdot|X))]$. All of our results also holds for expected version of value function.}) and provide an upper bound on optimal gap,
\begin{equation}\label{eq:gap}
    \begin{split}
         &\mathcal{J}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x)):= J(\pi_{\thetas}^\gamma(\cdot|x))-J(\pi_{\thetah}^{\gamma}(\cdot|x)).
    \end{split}
\end{equation}
Furthermore, inspired by \citep{song2024importance,zhao2024sharp}, we consider the following RLHF objective function based on the true reward function,
\begin{equation}\label{eq:rlhf-obj}
\begin{split}
       &J_{\gamma}(\pi_{\mathrm{ref}}(\cdot|x),\pi_{\theta}(\cdot|x)):=\\&\quad\mathbb{E}_{Y\sim \pi_\theta(\cdot|x)}[r_{\thetas}(Y,x)]-\frac{1}{\gamma}\KLr(\pi_{\theta}(\cdot|x)\|\pi_{\mathrm{ref}}(\cdot|x)).
\end{split}
\end{equation}
As studied by \citet{zhao2024sharp,song2024importance,zhan2023provable}, we also aim to study the following sub-optimality gap,
\begin{equation}\label{eq:sub-gap}
\begin{split}
     & \mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x)):=\\&J_{\gamma}(\pi_{\mathrm{ref}}(\cdot|x),\pi_{\thetas}^\gamma(\cdot|x))-J_{\gamma}(\pi_{\mathrm{ref}}(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x)).
\end{split}
\end{equation}

\textbf{Forward KL-regularized RLHF:} Inspired by \citep{wangbeyond}, we can consider the forward KL-regularized optimization objective as,
\begin{align}\label{eq:pref1-rlhf-FKL}
 & \underset{\pi}{\max}\mathbb{E}_{Y\sim \pi(\cdot|x)}\Big[r_{\thetah}\big(x,Y\big)\Big]-\frac{1}{\gamma}\KLr\Big(\pi_{\mathrm{ref}}(\cdot|x) \| \pi(\cdot|x) \Big),
\end{align}
As discussed in \citep{wangbeyond}, this optimization problem has an implicit solution given by:
\begin{equation}
\tilde{\pi}_{\thetah}^{\gamma}(y|x) := \frac{\pi_{\text{ref}}(y|x)}{\gamma(\tilde{Z}_{\thetah}(x)-r_{\thetah}(x,y))}
\end{equation}
where $\tilde{Z}_{\thetah}(x)$ is normalization constant ensuring that $\int_{\mathcal{Y}} \pi_{\theta}^{\gamma}(y|x)dy = 1$. Some properties of $\tilde{Z}_{\thetah}(x)$ are discussed in App.~\ref{app_sec_RLHF_FKL}. 

Similar to \eqref{eq:gap} and \eqref{eq:sub-gap}, for forward KL-regularized RLHF, we can define, 
\begin{align}
       &\tilde{J}_{\gamma}(\pi_{\mathrm{ref}}(\cdot|x),\pi_{\theta}(\cdot|x)):=\\\nonumber&\quad \mathbb{E}_{Y\sim \pi_\theta(\cdot|x)}[r_{\thetas}(Y,x)]-\frac{1}{\gamma}\KLr(\pi_{\mathrm{ref}}(\cdot|x)\|\pi_{\theta}(\cdot|x)),\\
       &\widetilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x)):=\\\nonumber&\quad\tilde{J}_{\gamma}(\pi_{\mathrm{ref}}(\cdot|x),\tilde{\pi}_{\thetas}^\gamma(\cdot|x))-\tilde{J}_{\gamma}(\pi_{\mathrm{ref}}(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x)).
    \end{align}  

\subsection{ Assumptions}
For our analysis, the following assumptions are needed.
\begin{assumption}[Bounded Reward]\label{ass:bounded_reward}
We assume that the true and parametrized reward functions, $r_{\thetas}(x,y)$ and $r_{\thetah}(x,y)$, are non-negative functions and bounded by $R_{\max}$.
\end{assumption}
\begin{assumption}[Finite Class]\label{ass:finite_class}
 We assume that the reward function class, $\mathcal{R}$, is finite, $|\mathcal{R}|<\infty$.
\end{assumption}
Coverage conditions play a fundamental role in understanding the theoretical guarantees of RLHF algorithms. We first introduce the most stringent coverage requirement, known as global coverage \citep{munos2008finite}:

\begin{assumption}[Global Coverage]\label{ass:global-coverage}
    For all policies $\pi$, we require
    \begin{equation}
\max_{x,y:\rho(x)>0} \frac{\pi(y|x)}{\widehat{\pi}_{\mathrm{ref}}(y|x)} \leq C_{\mathrm{GC}},        
    \end{equation}
    where $\widehat{\pi}_{\mathrm{ref}}$ denotes the reference model and $C_{\mathrm{GC}}$ is a finite constant.
\end{assumption}

A key implication of Assumption~\ref{ass:global-coverage} is that it requires substantial coverage: specifically, for any prompt $x$ and token sequence $y$ in the support of $\rho$, we must have $\widehat{\pi}_{\mathrm{ref}}(y|x) \geq \frac{1}{C_{\mathrm{GC}}}$.

While global coverage has been extensively studied in the offline RL literature \citep{uehara2021pessimistic,zhan2022offline}, it imposes strong requirements that may be unnecessarily restrictive for RLHF. A key insight from recent work \citep{zhao2024sharp,song2024importance} is that RLHF algorithms inherently employ reverse KL-regularization, which ensures learned policies remain within a neighborhood of the reference model. This observation motivates a more refined coverage condition:

\begin{assumption}[Local Reverse KL-ball Coverage]\label{ass:kl-coverage}
    Consider $\varepsilon_{\mathrm{rkl}} < \infty$ and any policy $\pi$ satisfying
   \begin{equation}
        \mathbb{E}_{x\sim\rho}[\KLr(\pi(\cdot|x)\|\widehat{\pi}_{\mathrm{ref}}(\cdot|x))] \leq \varepsilon_{\mathrm{rkl}},
   \end{equation}
    we require
    \begin{equation}
        \max_{x,y:\rho(x)>0} \frac{\pi(y|x)}{\widehat{\pi}_{\mathrm{ref}}(y|x)} \leq C_{\varepsilon_{\mathrm{rkl}}},
    \end{equation}
    where $C_{\varepsilon_{\mathrm{rkl}}}$ depends on the KL threshold $\varepsilon_{\mathrm{rkl}}$.
\end{assumption}

Similar to Assumption~\ref{ass:kl-coverage}, we consider the forward KL-ball coverage assumption.
\begin{assumption}[Local Forward KL-ball Coverage]\label{ass:fkl-coverage}
    Consider $\varepsilon_{\mathrm{fkl}} < \infty$ and any policy $\pi$ satisfying
    \[
        \mathbb{E}_{x\sim\rho}[\KLr(\widehat{\pi}_{\mathrm{ref}}(\cdot|x)\|\pi(\cdot|x))] \leq \varepsilon_{\mathrm{fkl}},
    \]
    we require
    \begin{equation}
        \max_{x,y:\rho(x)>0} \frac{\pi(y|x)}{\widehat{\pi}_{\mathrm{ref}}(y|x)} \leq  C_{\varepsilon_{\mathrm{fkl}}},
    \end{equation}
    where $C_{\varepsilon_{\mathrm{fkl}}}$ depends on the KL threshold $\varepsilon_{\mathrm{fkl}}$.
\end{assumption}

The local reverse or forward KL-ball coverage condition offers several advantages. Focusing only on policies within a reverse KL-ball of the reference model provides sharper theoretical guarantees while imposing weaker requirements. This localization aligns naturally with RLHF algorithms, which explicitly constrain the learned policy's divergence from the reference model. For any fixed reference model $\pi_{\mathrm{ref}}$, the reverse or forward KL local coverage constant is always bounded by the global coverage constant: $\max(C_{\varepsilon_{\mathrm{rkl}}},C_{\varepsilon_{\mathrm{fkl}}}) \leq C_{\mathrm{GC}}$. This follows from the fact that KL-constrained policies form a subset of all possible policies. 
% Note that, in Assumption~... \todoa{fkl and rkl assumptions comparison.}



% The local KL-ball coverage assumption has proven particularly effective for analyzing RLHF methods with a single reference model \citep{song2024importance,zhao2024sharp}, providing a theoretical foundation for understanding their convergence properties and performance guarantees.

\section{RLHF from Multiple Reference Models via Reverse KL divergence}\label{sec_RLHF_RKL}
In this section, inspired by \cite{le2024multi}, we are focused on situations involving $K$ reference
policies $\Big\{ \pi_{\mathrm{ref},i}\Big\} _{i=1}^{K}$ where the latent reward model among all reference policies is the same. All proof details are deferred to Appendix~\ref{app_sec_RLHF_RKL}.
\subsection{Exact Solution of RLHF under multiple reference models via RKL}\label{sec:Exact_solution}
Inspired by \citep{le2024multi}, our objective can be formulated as a multiple reference models RLHF objective,
\begin{equation}\label{eq:prefm-rlhf-1}
\underset{\pi}{\max}\underset{Y\sim\pi(\cdot|x)}{{\mathbb{E}}}\big[r\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big),
\end{equation}
where $\alpha_{i}$ are weighting coefficients for each reference
policy and $\sum_{i=1}^{K}\alpha_{i}=1$. This objective
was explored in previous studies, leading to enhancements in pure
RL problems \cite{le2022learning}.

However, addressing this optimization problem in LLMs through reward
learning and RL finetuning pose similar challenges to \eqref{eq:pref1-rlhf}.
Our goal is to derive a closed-form solution for the multi-reference RLHF objective in \eqref{eq:prefm-rlhf-1}. Note that in \citep[Proposition~1]{le2024multi}, a lower bound on RLHF objective in \eqref{eq:prefm-rlhf-1} is proposed, and the solution for this surrogate objective function is derived as follows,
\begin{equation}
    \pi_{\mathrm{L}}\big(y|x\big)=\frac{\widetilde{\pi}_{\mathrm{ref}}\Big(y|x\Big)}{\widehat{Z}_{\mathrm{l}}(x)}\exp\Big(\gamma r\Big(x,y\Big)\Big),
\end{equation}
where $\widetilde{\pi}_{\mathrm{ref}}(y|x)=\Big(\sum_{i=1}^K \frac{\alpha_i}{\pi_{\mathrm{ref},i}(y|x) }\Big)^{-1}$ and $\widehat{Z}_{\mathrm{l}}(x)=\sum_{y}\widetilde{\pi}_{\mathrm{ref}}(y|x)\exp\big(\gamma r(x,y)\big)$.

In contrast, we provide the exact solution of the objective function for multiple reference model \eqref{eq:prefm-rlhf-1}. The following Lemma, for deriving the exact solution of \eqref{eq:prefm-rlhf-1} is needed.

\begin{lemma}[Generalized Escort Distribution]\label{lem: Tilted distribution entropy}
Let $\alpha_i\in [0,1]$ for all $i\in[k]$ and $\sum_{i=1}^k\alpha_i=1$. For any distributions $Q_i$ for all $i\in[k]$ and $P$ over the space $\mathcal{X}$, such that $P \ll Q_i$, we have
\begin{equation}
\begin{split}
    	&\sum_{i=1}^k\alpha_i \KLr(P\|Q_i)=\\&\KLr\Big(P\|(\{Q_i\}_{i=1}^k)^{\pmb{\alpha}}\Big)-\log\left(\sum_{x\in\mathcal{A}}\Pi_{i=1}Q_i^{\alpha_i}(x) \right).
\end{split}
\end{equation}	
\end{lemma}
We now provide the exact solution of the RLHF with multiple references.
\begin{theorem}\label{thm: main}
Consider the following objective function for RLHF with multiple reference models,
\begin{equation*}
\begin{split}
    &\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]\right.\\&\qquad\quad\left.-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\},
\end{split}
\end{equation*}
where $\sum_{i=1}^K \alpha_i=1$ and $\alpha_i\in(0,1)$ for $i\in[K]$. Then, the exact solution of the multiple reference model objective function for RLHF is,
\begin{equation}
\pi_{\thetas}^\gamma\big(y|x\big)=\frac{\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}\Big(y|x\Big)}{\widehat{Z}(x)}\exp\Big(\gamma r_{\thetas}(x,y)\Big),
\end{equation}
where 
\begin{equation}\label{eq:ref-mrm}
    \widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x)= \frac{\prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x)}{F_{\pmb{\alpha}}(x)},
\end{equation}
\begin{equation}\label{eq:Falpha}
F_{\pmb{\alpha}}(x)=\sum_{y\in\mathcal{Y}}\prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x),    
\end{equation} and \begin{equation}
    \widehat{Z}(x)=\sum_{y}\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x)\exp\Big(\gamma r(x,y)\Big).
\end{equation}
The maximum objective value is 
\[\frac{1}{\gamma}\log \left(\sum_y \prod_{i=1}^K \pi_{\mathrm{ref},i}^{\alpha_i}(y|x)\exp\left(\gamma r(x,y) \right)\right). \]
\end{theorem}
Using Theorem~\ref{thm: main}, we can consider the following optimization problem for the multiple reference models scenario.
\begin{equation}\label{eq:rlhf-mrm-main}
\mathbb{E}_{Y\sim\pi\big(\cdot|x\big)}\big[r_{\thetas}(x,y)\big]-\frac{1}{\gamma}\KLr\big(\pi (\cdot | x)\| \widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(\cdot|x)\big),
\end{equation}
where $\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x)$ is defined in \eqref{eq:ref-mrm} as generalized escort reference policy. 
% \todoa{absolutely continuous discussion,}

The RLHF algorithm with two reference models is shown in Algorithm~\ref{alg:kl-rlhf}. 
\begin{algorithm}[h]
\caption{Reverse KL-regularized RLHF with Two Reference Models}
\label{alg:kl-rlhf}
\begin{algorithmic}[1]
\Require $\gamma$, $\alpha$, $\pi_{\mathrm{ref},1}$, $\pi_{\mathrm{ref},2}$ $\Theta$
% \State $\triangleright$ \textit{Use policy $\pi_0$ to achieve sufficient data coverage}
\For{$i = 1,\ldots,m$}
    \State Sample prompt $\tilde{x}_i \sim \rho$ and 2 responses with their preference $\tilde{y}_i^w, \tilde{y}_i^l \sim \widehat{\pi}_{\alpha,\mathrm{ref}}(\cdot|x)\propto\pi_{\mathrm{ref},1}^{\alpha}(\cdot|\tilde{x}_i)\pi_{\mathrm{ref},2}^{1-\alpha}(\cdot|\tilde{x}_i)$.
\EndFor
\State Compute the MLE estimator of the reward function based on $D_n=\{(\tilde{x}_i, \tilde{y}_i^w, \tilde{y}_i^l)\}_{i=1}^n$:
$$
\thetah \leftarrow \arg\max_{\theta} \mathcal{L}(\theta,D_n),
$$
\State Compute the RLHF output based on \eqref{eq:rlhf-mrm-main}:     $\pi_{\thetah}^{\gamma}(\cdot|\cdot) \propto \widehat{\pi}_{\alpha,\mathrm{ref}}(\cdot|x)\exp(\gamma r_{\thetah}( \cdot, \cdot))$.
\end{algorithmic}
\end{algorithm}

% Next, we provide the theoretical analysis of RLHF algorithm with 2 reference models, Algorithm~\ref{alg:kl-rlhf}. 

\subsection{Main Results for RLHF via RKL}
In this section, we provide our main theoretical results for the RLHF algorithm with multiple reference models based on reverse KL-regularization. Using the convexity of reverse KL divergence, we can provide an upper bound on the sub-optimality gap. Furthermore, we assume that Assumption~\ref{ass:kl-coverage} holds under $\pirefalpha$ as reference policy with $C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}}$.
\begin{proposition}\label{prop:func_derv_policy}
    For a given response, $x\in\mathcal{X}$, the following upper bound holds,
    \begin{equation*}
    \begin{split}
         &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\leq\\& \int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))(\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y).
    \end{split}
    \end{equation*}
\end{proposition}
Next, we provide a useful lemma for the sensitivity of policy with respect to reward function. 
\begin{lemma}\label{lem:sensivitiy_policy}
Consider the softmax policy, $\pi_r^\gamma(y|x)\propto\pirefalpha(y|x)\exp(\gamma r(x,y))$. Then, the sensitivity of the policy with respect to reward function is,
\begin{equation}
    \frac{\partial \pi_r^\gamma}{\partial r}(r)=\gamma \pi_r^\gamma(y|x) (1-\pi_r^\gamma(y|x)).
\end{equation}
\end{lemma}
Using Proposition~\ref{prop:func_derv_policy} and Lemma~\ref{lem:sensivitiy_policy}, we can derive the following upper bound on the sub-optimality gap of the RLHF algorithm with multiple reference models.
\begin{theorem}\label{thm:sub-gap}
   Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, the following upper bound holds on the sub-optimality gap with probability at least $(1-\delta)$ for $\delta\in(0,1/2)$,
   \begin{equation}
       \begin{split}
           &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\&\leq \gamma C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} 128 e^{4 R_{\max}}R_{\max}^2\frac{\log(|\mathcal{R}|/\delta)}{n}.
       \end{split}
   \end{equation}
\end{theorem}
% \begin{remark}
%  In \citep{zhao2024sharp}   
% \end{remark}
Using Theorem~\ref{thm:sub-gap}, we can provide the upper bound on the optimal gap under the RLHF algorithm.
\begin{theorem}\label{thm:gap}
  Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, there exists constant $C>0$ such that the following upper bound holds on optimality gap of reverse KL-regularized RLHF with probability at least $(1-\delta)$ for $\delta\in(0,1/2)$,
  \begin{equation}
       \begin{split}
           &\mathcal{J}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\&\leq \gamma C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} 128 e^{4 R_{\max}}R_{\max}^2\frac{\log(|\mathcal{R}|/\delta)}{n}\\&\qquad+C 8R_{\max}e^{2 R_{\max}}\sqrt{\frac{2 C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}}\log(|\mathcal{R}|/\delta)}{n}}.
       \end{split}
   \end{equation}
\end{theorem}
\begin{remark}[Sample Complexity]
    We can observe sample complexity of $O(1/n)$ for the sub-optimality gap and $O(1/\sqrt{n})$ for the optimality gap from Theorem~\ref{thm:sub-gap} and Theorem~\ref{thm:gap}, respectively.
\end{remark}


% \begin{theorem}\label{thm:diffMS}
%     Under Assumption~\ref{ass:bounded_reward}, the following upper bound holds,
%     \begin{equation}
%         \begin{split}
% &\tilde{J}_\gamma(\pi_{\pmb{\alpha},\mathrm{ref}},\pi_{\thetas}^\gamma)-\tilde{J}_\gamma(\pi_{\mathrm{ref},i},\pi_{\thetas,i}^\gamma)\\&\quad\leq \frac{\exp(\gamma R_{\max})-1}{\sqrt{2}\gamma}\sqrt{\KLr(\pi_{\pmb{\alpha},\mathrm{ref}}\|\pi_{\mathrm{ref},i})}.
%         \end{split}
%     \end{equation}
% \end{theorem}
\section{RLHF from Multiple Reference Models via Forward KL Divergence}\label{sec_RLHF_FKL}
In this section, inspired by \citep{wangbeyond}, we extend the RLHF from multiple reference models based on reverse KL-regularization \cite{le2024multi} to forward KL-regularization. Similar, to Section~\ref{sec_RLHF_RKL}, we are focused on situations involving $K$ reference
policies $\Big\{ \pi_{\mathrm{ref},i}\Big\} _{i=1}^{K}$ where the latent reward model among all reference policies is the same. All proof details are deferred to Appendix~\ref{app_sec_RLHF_FKL}.
\subsection{Solution of RLHF under multiple reference models via FKL}\label{fsec:Exact_solution}
Inspired by \citep{le2024multi,wangbeyond}, our objective can be formulated as a multiple reference models RLHF objective,
\begin{equation}\label{eq:prefm-rlhf-1f}
\underset{\pi}{\max}\underset{Y\sim\pi(\cdot|x)}{{\mathbb{E}}}\big[r\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\beta_{i}\KLr\big( \pi_{\mathrm{ref},i}(\cdot | x)\|\pi (\cdot | x)\big)\Big),
\end{equation}
where $\beta_{i}$ are weighting coefficients for each reference
policy and $\sum_{i=1}^{K}\beta_{i}=1$. This objective
was explored in previous studies, leading to enhancements in pure
RL problems \cite{le2022learning}.

However, addressing this optimization problem in LLMs through reward
learning and RL finetuning poses similar challenges to \eqref{eq:pref1-rlhf}.
Our goal is to derive a closed-form solution for the multi-reference RLHF objective in \eqref{eq:prefm-rlhf-1f}. The following Lemma, for deriving the exact solution of \eqref{eq:prefm-rlhf-1f} is needed.

\begin{lemma}[Average Distribution]\label{flem: average distribution}
Let $\beta_i\in [0,1]$ for all $i\in[k]$ and $\sum_{i=1}^k\beta_i=1$. For any distributions $Q_i$ for all $i\in[k]$ and $R$ such that $Q_i \ll P$, we have
\begin{equation*}
    \begin{split}
       & \sum_{i=1}^k\beta_i \KLr(Q_i\|P)=\\&\quad H\Big(\sum_{i=1}^k\beta_iQ_i\Big)-\sum_{i=1}^k \beta_i H(Q_i)+\KLr\Big(\sum_{i=1}^k\beta_i Q_i\|P\Big).
    \end{split}
\end{equation*}
\end{lemma}
We now provide the exact solution of the RLHF with multiple references.
\begin{theorem}\label{fthm: main}
Consider the following objective function for RLHF with multiple reference models,
\begin{equation*}
\underset{\pi}{\max}\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\beta_i\KLr\big( \pi_{\mathrm{ref},i}(\cdot | x)\|\pi (\cdot | x)\big)\Big),
\end{equation*}
where $\sum_{i=1}^K \beta_i=1$ and $\beta_i\in(0,1)$ for $i\in[K]$. Then, the implicit solution of the multiple reference models objective function for RLHF is,
\begin{equation}
\tilde{\pi}_{\thetas}^\gamma\big(y|x\big)=\frac{\bar{\pi}_{\pmb{\beta},\mathrm{ref}}\Big(y|x\Big)}{\gamma\big(\tilde{Z}(x)-r_{\thetas}(x,y)\big)},
\end{equation}
where 
\begin{equation}\label{feq:ref-mrm}
    \bar{\pi}_{\pmb{\beta},\mathrm{ref}}(y|x)= \sum_{i=1}^K \beta_i\pi_{\mathrm{ref},i}(y|x),
\end{equation}
    and $\tilde{Z}(x)$ is the solution to $\int_{y\in\mathcal{Y}} \tilde{\pi}_{\thetas}^\gamma\big(y|x\big)=1$ for a given $x\in\mathcal{X}$.
\end{theorem}
Using Theorem~\ref{fthm: main}, we can consider the following optimization problem for forward KL-regularized RLHF under multiple reference model scenario,
\begin{equation}\label{feq:rlhf-mrm-main}
\mathbb{E}_{Y\sim\pi\big(\cdot|x\big)}\big[r_{\thetas}(x,y)\big]-\frac{1}{\gamma}\KLr\big( \bar{\pi}_{\pmb{\beta},\mathrm{ref}}(\cdot|x)\|\pi (\cdot | x)\big),
\end{equation}
where $\bar{\pi}_{\pmb{\beta},\mathrm{ref}}(y|x)$ is defined in \eqref{feq:ref-mrm} as weighted reference policy. 

The forward KL-regularized RLHF algorithm with two reference models is shown in Algorithm~\ref{falg:kl-rlhf}. 
\begin{algorithm}[h]
\caption{Forward KL-regularized RLHF with Two Reference Models}
\label{falg:kl-rlhf}
\begin{algorithmic}[1]
\Require $\gamma$, $\beta$, $\pi_{\mathrm{ref},1}$, $\pi_{\mathrm{ref},2}$ $\Theta$
% \State $\triangleright$ \textit{Use policy $\pi_0$ to achieve sufficient data coverage}
\For{$i = 1,\ldots,m$}
    \State Sample prompt $\tilde{x}_i \sim \rho$ and 2 responses with their preference \newline$\tilde{y}_i^w, \tilde{y}_i^l \sim \bar{\pi}_{\beta,\mathrm{ref}}(\cdot|x)=\beta\pi_{\mathrm{ref},1}(\cdot|\tilde{x}_i)+(1-\beta)\pi_{\mathrm{ref},2}(\cdot|\tilde{x}_i)$.
\EndFor
\State Compute the MLE estimator of the reward function based on $D_n=\{(\tilde{x}_i, \tilde{y}_i^w, \tilde{y}_i^l)\}_{i=1}^n$:
$$
\thetah \leftarrow \arg\max_{\theta} \mathcal{L}(\theta,D_n)
$$
\State Compute the RLHF output based on \eqref{feq:rlhf-mrm-main}.

\end{algorithmic}
\end{algorithm}

\subsection{Main Results for RLHF with FKL}
This section presents our core theoretical analysis of forward KL-regularized RLHF under the multiple reference model setting. We begin by leveraging KL divergence's convex properties to establish an upper bound on the sub-optimality gap. Throughout this section, we consider $\tilde{\pi}_{\thetah}^{\gamma}(y|x)=\frac{\pirefbetay}{\gamma(\tilde{Z}_{\thetah}(x)-r_{\thetah}(x,y))}$ and $\tilde{\pi}_{\thetas}^{\gamma}(y|x)=\frac{\pirefbetay}{\gamma(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y))}$. Furthermore, we assume that Assumption~\ref{ass:fkl-coverage} holds under $\pirefbetay$ as reference policy with $C_{\pmb{\beta},\varepsilon_{\mathrm{rkl}}}$.
\begin{proposition}\label{prop:func_derv_policy_fkl}
    For a given response, $x\in\mathcal{X}$, the following upper bound holds,
    \begin{equation*}
    \begin{split}
         &\widetilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))\leq\\& \int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y).
    \end{split}
    \end{equation*}
\end{proposition}
Using Proposition~\ref{prop:func_derv_policy_fkl}, we derive an upper bound for the sub-optimality gap in the multiple reference forward KL-regularized RLHF setting.
\begin{theorem}\label{thm:sub-gap-fkl}
   Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, the following upper bound holds on the sub-optimality gap with probability at least $(1-\delta)$ for $\delta\in(0,1)$,
   \begin{equation}
       \begin{split}
           &\tilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))\\&\leq 16 C_{\pmb{\beta},\varepsilon_{\mathrm{fkl}}}  e^{2 R_{\max}}R_{\max}\sqrt{\frac{\log(|\mathcal{R}|/\delta)}{n}}.
       \end{split}
   \end{equation}
\end{theorem}
Using Theorem~\ref{thm:sub-gap-fkl}, we can provide the upper bound on the optimal gap under the multiple reference forward KL-regularized RLHF setting.
\begin{theorem}\label{thm:gap-fkl}
  Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, the following upper bound holds on optimality gap of the multiple reference forward KL-regularized RLHF algorithm with probability at least $(1-\delta)$ for $\delta\in(0,1)$,
  \begin{equation}
       \begin{split}
           &\tilde{\mathcal{J}}(\pif_{\thetas}^\gamma(\cdot|x),\pif_{\thetah}^{\gamma}(\cdot|x))\\&\leq 16 C_{\pmb{\beta},\varepsilon_{\mathrm{fkl}}}  e^{2 R_{\max}}R_{\max}\sqrt{\frac{\log(|\mathcal{R}|/\delta)}{n}}\\&\quad+\frac{\max\big(|\log(C_{\varepsilon,\mathrm{fkl}})|,\log(\gamma R_{\max}+1)\big)}{\gamma}.
       \end{split}
   \end{equation}
\end{theorem}
\begin{remark}[Sample Complexity]
    Choosing $\gamma=n$, we have sample complexity $O(1/\sqrt{n})$ on optimality gap from Theorem~\ref{thm:gap-fkl}. We can also observe the sample complexity of $O(1/\sqrt{n})$ for the sub-optimality gap.
\end{remark}
\section{Discussion}\label{sec:disc}
\begin{table*}[ht!]
\centering
\caption{Comparison of Various Works in Theoretical Foundation of RLHF: Key features include support for RKL sub-optimality gap, RKL optimality gap, FKL sub-optimality gap, and FKL optimality gap and their Sample Complexities for each scenario. }
\resizebox{0.88\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\textbf{Work} & \textbf{\makecell{RKL Sub-optimality Gap\\(Sample Complexity)}} & \textbf{\makecell{RKL Optimality Gap\\(Sample Complexity)}} & \textbf{\makecell{FKL Sub-optimality Gap\\(Sample Complexity)}} & \textbf{\makecell{FKL Optimality Gap\\(Sample Complexity)}} \\
\midrule
\citet{song2024importance} & \makecell{\checkmark\\$O(1/\sqrt{n})$} & \xmark & \xmark & \xmark\\
\cmidrule(lr){1-5}
\citet{zhao2024sharp} & \makecell{\checkmark\\$O(1/n)$} & \xmark & \xmark & \xmark\\
\cmidrule(lr){1-5}
\citet{chang2024dataset} & \xmark & \makecell{\checkmark\\$O(1/\sqrt{n})$} & \xmark & \xmark\\
\cmidrule(lr){1-5}
\citet{xiong2024iterative} & \makecell{\checkmark\\$O(1/\sqrt{n})$}  & \xmark & \xmark & \xmark\\
\cmidrule(lr){1-5}
\textbf{Our Work} & \makecell{\checkmark\\$O(1/n)$} & \makecell{\checkmark\\$O(1/\sqrt{n})$} & \makecell{\checkmark\\$O(1/\sqrt{n})$} & \makecell{\checkmark\\$O(1/\sqrt{n})$} \\
\bottomrule
\end{tabular}}
\label{tab:comparison}
\end{table*}

\textbf{Theoretical Comparison with Single-Reference Models:} Our theoretical results extend to the single-reference model setting, enabling comparison with existing work in this domain. The RKL-regularized RLHF framework and its sub-optimality gap have been investigated by \citet{song2024importance} and \citet{zhao2024sharp}, who established sample complexity bounds. \citet{song2024importance} derived a sub-optimality gap sample complexity of $O(1/\sqrt{n})$, which \citet{zhao2024sharp} later improved to $O(1/n)$, demonstrating the effectiveness of RKL regularization. Note that, in \citep{zhao2024sharp}, it is shown that when the error tolerance $\epsilon$ is sufficiently small, the sample complexity follows an $O(1/\epsilon)$ relationship. This corresponds to $O(1/n)$, where $n$ represents the dataset size. In comparison with \citep{zhao2024sharp}, we proposed an approach based on functional derivative and convexity of KL divergence. Our approach is more general and can be applied to the forward KL-regularized RLHF framework. There are also some works on similar algorithms to RLHF. Additionally, \citet{chang2024dataset} proposed an algorithm integrating offline and online preference datasets in RLHF, analyzing its optimality gap sample complexity under RKL regularization. The general reverse KL-regularized RLHF framework under general preference models is studied by  \citet{xiong2024iterative} and a sample complexity of $O(1/\sqrt{n})$ for sub-optimality gap is derived. To the best of our knowledge, the sample complexity of the optimality gap and sub-optimality gap for forward KL-regularization have not been studied in the literature. Furthermore, in \citep{huang2024correcting}, KL-divergence and $\chi^2$-divergence are considered as regularizers, and the sample complexity on optimality gap for $\chi^2$-DPO are studied. We summarized our comparison with different works related to the theoretical study of RLHF in Table~\ref{tab:comparison}.

\textbf{Coverage Assumption Discussion:} The coverage assumptions for multiple references can differ from the single reference scenario. For the reverse KL-regularized case with reference policy $\pirefalpha$, we have:
\begin{equation}
\begin{split}
& \frac{\pi(y|x)}{\pirefalpha}=  F_{\pmb{\alpha}}(x) \prod_{i=1}^K\Big(\frac{ \pi(y|x)}{\pi_{\mathrm{ref},i}(y|x)}\Big)^{\alpha_i},
\end{split}
\end{equation}
where $F_{\pmb{\alpha}}(x)$ is defined in \eqref{eq:Falpha}. Therefore, we have $\prod_{i=1}^K C_{\mathrm{ref},i}^{\alpha_i}$ as the global coverage assumption, where $C_{\mathrm{ref},i}<\infty$ is the global coverage with respect to the $i$-th reference. Note that, using Hölder's inequality, we can show that $F_{\pmb{\alpha}}(x)\leq 1$. A similar discussion applies to the forward KL-regularization scenario with reference policy $\pirefbetay$. Regarding the local reverse KL-ball coverage assumptions (Assumption~\ref{ass:kl-coverage}), as $\pirefalpha$ is defined on common support among all reference models, then the set of policies with bounded
\begin{equation}
        \mathbb{E}_{x\sim\rho}[\KLr(\pi(\cdot|x)\|\pirefalpha)] \leq \varepsilon_{\pmb{\alpha},\mathrm{rkl}},
   \end{equation}
is smaller than each reference model separately. Similarly to global coverage, we can assume that $C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}}=\prod_{i=1}^K C_{\mathrm{ref},i,\varepsilon_{\mathrm{rkl}}}^{\alpha_i}$.

\textbf{Comparison of RKL with FKL:} The RKL and FKL exhibit fundamentally different characteristics in their optimization behavior. RKL between the reference model and target policy, defined as $\mbE_{\pi_{\thetas}}[\log(\pi_{\thetas}/\pi_{\mathrm{ref}})]$, demonstrates mode-seeking behavior during optimization. When $\pi_{\thetas}$ represents the output policy of RLHF for language model alignment, it may assign zero probability to regions where $\pi_{\mathrm{ref}}$ is positive.
Conversely, FKL, expressed as $\mbE_{\pi_{\mathrm{ref}}}[\log(\pi_{\mathrm{ref}}/\pi_{\thetas})]$, exhibits mass-covering properties. Its mathematical formulation requires $\pi_{\thetas}$ to maintain non-zero probability wherever $\pi_{\mathrm{ref}}$ is positive. This constraint naturally leads FKL to produce distributions that cover the full support of the reference model, thereby promoting diverse outputs. 


\textbf{Reference policy in multiple reference model scenario under FKL and RKL:} In the multiple reference model setting, the generalized escort distribution under reverse KL-regularization covers the intersection of the supports of all reference models. Specifically, responses receive zero probability if they lack positive probability in any single reference model. This leads the generalized escort distribution to assign non-zero probabilities only to responses supported by all reference models simultaneously.
In contrast, when using the average distribution as the reference model in the forward KL scenario, the resulting distribution covers the union of supports across all reference models, encompassing a broader range of possible responses.

Furthermore, we discuss the extension of multiple reference models scenario to DPO, where we propose the DPO objective function for reverse and forward KL-regularization under multiple reference models and derive optimality gap under bounded implicit reward assumptions App.~\ref{app:DPO}.

\section{Conclusion and Future Works}\label{sec_conclusion}
This work develops theoretical foundations for two Reinforcement Learning from Human Feedback (RLHF) frameworks: reverse KL-regularized and forward KL-regularized RLHF. We derive solutions for both frameworks under multiple reference scenarios and establish their sample complexity bounds. Our analysis reveals that while both algorithms share identical sample complexity for the optimality gap, the reverse KL-regularized RLHF achieves superior sample complexity for the sub-optimality gap.

The main limitation of our work lies in the assumption of bounded reward functions. Recent work \citep{mroueh2024information} shows that reward functions can be unbounded and sub-Gaussian. Two promising directions for future research emerge:(a) Extending our analysis to multiple-reference KL-regularized RLHF with unbounded reward functions, (b) Following \citep{wangbeyond}, investigating multiple-reference RLHF regularized by general $f$-divergences, (c) Following \citep{sharifnassab2024soft}, we can extend our analysis to general preference models beyond Bradley-Terry (BT) model.
\section*{Acknowledgements} The authors would like to thank Ahmad Beirami for his valuable comments and insightful feedback on our work. Gholamali Aminian acknowledges the support of the UKRI Prosperity Partnership Scheme (FAIR) under EPSRC Grant EP/V056883/1 and the Alan Turing Institute. Amir R. Asadi is supported by Leverhulme Trust grant ECF-2023-189 and Isaac Newton Trust grant 23.08(b).

\newpage

\bibliography{Refs}
\bibliographystyle{plainnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
% \section*{Appendices}
% \section*{Appendices}
% \addcontentsline{toc}{section}{Appendix} % Add the appendix text to the document TOC

% \newpage
% \clearpage
% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none of which we feel must be specifically highlighted here.


% \bibliography{Refs}
% \bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\onecolumn
\section{Technical Tools}
In this section, we introduce the following technical tools and Lemmata.
\begin{lemma}[Lemma C.2 from \citep{chang2024dataset}]\label{lemma:B2}
Under Assumptions~\ref{ass:bounded_reward} and \ref{ass:finite_class}, we have with probability at least $1-\delta$ that
\begin{equation}
    \mathbb{E}_{Y^l,Y^w\sim\pi_{\mathrm{ref}},\pi_{\mathrm{ref}}}\left[\left(r_{\thetas}(x,Y^l) - r_{\thetas}(x,Y^w) - r_{\thetah}(x,Y^l) + r_{\thetah}(x,Y^w)\right)^2\right] \leq \frac{128 R_{\max}^2 \exp(4 R_{\max}) \log(|\mathcal{R}|/\delta)}{n}.
\end{equation}
\end{lemma}

\begin{lemma}[Lemma E.5 from \citep{huang2024correcting}]\label{lemma:E5}
Under Assumptions~\ref{ass:bounded_reward} and \ref{ass:finite_class}, we have with probability at least $1-\delta$ that
\begin{equation}
    \mathbb{E}_{Y^l,Y^w\sim\pi_{\mathrm{ref}},\pi_{\mathrm{ref}}}\left[\left(r_{\thetas}(x,Y^l) - r_{\thetas}(x,Y^w) - r_{\thetah}(x,Y^l) + r_{\thetah}(x,Y^w)\right)^2\right] \leq \frac{128 R_{\max}^2 \exp(4 R_{\max}) \log(|\mathcal{R}|/\delta)}{n}.
\end{equation}
\end{lemma}


\begin{lemma}[\citep{boucheron2013concentration}]\label{lem:transport}
    Assume that function $f(x)\in[0,B]$ is bounded. Then, we have,
    \begin{equation}
        \mbE_{p(X)}[f(X)]-\mbE_{q(X)}[f(X)]\leq B\sqrt{\frac{\KLr(p(X)\|q(X))}{2}}.
    \end{equation}
\end{lemma}
\begin{lemma}\label{lem:shift}
    Assume that $\tilde{\pi}_r(y|x)\propto \pi_{\mathrm{ref}}(y|x)\exp(\gamma r(x,y))$. Then, $\tilde{\pi}_{r+\Delta}(y|x)=\tilde{\pi}_r(y|x)$, where $\Delta$ is constant.
\end{lemma}
\begin{proof}
    \begin{equation}
        \begin{split}
            \tilde{\pi}_{r+\Delta}(y|x)&=\frac{\pi_{\mathrm{ref}}(y|x)\exp(\gamma (r(x,y)+\Delta))}{\mbE_{Y\sim \pi_{\mathrm{ref}}(Y|x)}\big[\exp(\gamma (r(x,y)+\Delta))\big]}\\&=\frac{\pi_{\mathrm{ref}}(y|x)\exp(\gamma r(x,y))}{\mbE_{Y\sim \pi_{\mathrm{ref}}(Y|x)}\big[\exp(\gamma r(x,y))\big]}.
        \end{split}
    \end{equation}
\end{proof}
\newpage
\section{Proofs and Details of Section~\ref{sec_RLHF_RKL}}\label{app_sec_RLHF_RKL}
\begin{tcolorbox}
\begin{replemma}{lem: Tilted distribution entropy}
Let $\alpha_i\in [0,1]$ for all $i\in[k]$ and $\sum_{i=1}^k\alpha_i=1$. For any distributions $Q_i$ for all $i\in[k]$ and $P$ over the space $\mathcal{X}$, such that $P \ll Q_i$, we have
\begin{equation*}
\begin{split}
    	&\sum_{i=1}^k\alpha_i \KLr(P\|Q_i)=\KLr\Big(P\|(\{Q_i\}_{i=1}^k)^{\pmb{\alpha}}\Big)-\log\left(\sum_{x\in\mathcal{A}}\Pi_{i=1}^kQ_i^{\alpha_i}(x) \right).
\end{split}
\end{equation*}	
\end{replemma}
\end{tcolorbox}
\begin{proof}
    We have
    \begin{align*}
        \sum_{i=1}^k\alpha_i \KLr(P\|Q_i)&=\sum_{i=1}^k\alpha_i \left(\sum_{x\in\mathcal{A}}P(x)\log \left(\frac{P(x)}{Q_i(x)}\right)\right)\\
        &=\sum_{x\in\mathcal{A}}\sum_{i=1}^k P(x)\log \left(\frac{P^{\alpha_i}(x)}{Q_i^{\alpha_i}(x)}\right)\\
        &=\sum_{x\in\mathcal{A}} P(x)\log \left(\frac{P(x)}{\prod_{i=1}^k Q_i^{\alpha_i}(x)}\right)\\
        &=\KLr\left(P\middle\|(\{Q_i\}_{i=1}^k)^{\pmb{\alpha}}\right)-\log\left(\sum_{x\in\mathcal{A}}\Pi_{i=1}^kQ_i^{\alpha_i}(x) \right).
    \end{align*}
\end{proof}
\begin{tcolorbox}
\begin{lemma}\label{lem: Gibbs relative entropy}
  Let ${\cal A}$ be an arbitrary set and function $f: {\cal A}\rightarrow \mathbb{R}$ be such that 
  \begin{align*}
  	\int_{x\in {\cal A}}\exp\left(-\frac{f(x)}{\lambda}\right)Q_X(x)\mathrm{d}x<\infty.
  \end{align*}
  Then for any $P_{X}$ defined on ${\cal A}$ such that $X\sim P_X$, we have
  \begin{equation}
      \nonumber
      \mathbb{E}[f(X)]+\lambda \KLr(P_X\|Q_X)=\lambda \KLr\left(P_{X}\middle\|{P}_{X}^{\rm{Gibbs}}\right)-\lambda \log \left(\int_{x\in {\cal A}}\exp\left(-\frac{f(x)}{\lambda}\right)Q_X(x)\mathrm{d}x\right) ,
  \end{equation}
  where 
  $${P}_{X}^{\rm{Gibbs}}(x):=\frac{\exp\left(-\frac{f(x)}{\lambda}\right)Q_{X}(x)}{\int_{x\in {\cal A}}\exp\left(-\frac{f(x)}{\lambda}\right)Q_X(x)\mathrm{d}x}, \quad x\in {\cal A},$$ is the Gibbs--Boltzmann distribution.
\end{lemma}
\end{tcolorbox}
\begin{proof}
We have 
\begin{align*}
    \mathbb{E}[f(X)]+\lambda \KLr(P_X\|Q_X)&= \int f(x)P_X(x)\mathrm{d}x+ \lambda \int P_X(x)\log\left(\frac{P_X(x)}{Q_X(x)}\right)\\
                                        &= \lambda \int P_X(x)\log\left(\frac{P_X(x)}{\exp\left(-\frac{f(x)}{\lambda}\right)Q_X(x)}\right)\\
                                        &= \lambda \KLr\left(P_{X}\middle\|{P}_{X}^{\rm{Gibbs}}\right)-\lambda \log \left(\int_{x\in {\cal A}}\exp\left(-\frac{f(x)}{\lambda}\right)Q_X(x)\mathrm{d}x\right).
\end{align*}
\end{proof}
\begin{tcolorbox}
    \begin{reptheorem}{thm: main}
Consider the following objective function for RLHF with multiple reference models,
\begin{equation*}
\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\},
\end{equation*}
where $\sum_{i=1}^K \alpha_i=1$ and $\alpha_i\in(0,1)$ for $i\in[K]$. Then, the exact solution of the multiple reference models objective function for RLHF is,
\begin{equation}
\pi_{\thetas}^\gamma\big(y|x\big)=\frac{\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}\Big(y|x\Big)}{\widehat{Z}(x)}\exp\Big(\gamma r_{\thetas}(x,y)\Big),
\end{equation}
where 
\begin{equation*}
    \widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x)= \frac{\prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x)}{F_{\pmb{\alpha}}(x)},
\end{equation*}
\begin{equation*}
F_{\pmb{\alpha}}(x)=\sum_{y\in\mathcal{Y}}\prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x),    
\end{equation*} and \begin{equation*}
    \widehat{Z}(x)=\sum_{y}\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x)\exp\Big(\gamma r(x,y)\Big).
\end{equation*}
The maximum objective value is 
\[\frac{1}{\gamma}\log \left(\sum_y \prod_{i=1}^K \pi_{\mathrm{ref},i}^{\alpha_i}(y|x)\exp\left(\gamma r(x,y) \right)\right). \]
\end{reptheorem}

\end{tcolorbox}
\begin{proof}
 We can write
 \begin{align}
    & \underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\nonumber \\& = \frac{1 }{\gamma}\left(\gamma \underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big] - \Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big) \right)\\
     &=\frac{1}{\gamma}\left(\gamma \underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big] - \KLr(\pi(\cdot|x)\|\widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x))+\log F_{\pmb{\alpha}}(x) \right)\label{eq: multiple ref KL equation 2}\\
     &=\frac{1}{\gamma}\left(-\KLr(\pi(\cdot|x)\|\pi_{\thetas}^\gamma\big(y|x\big))+\log \widehat{Z}(x) +\log F_{\pmb{\alpha}}(x) \right) \label{eq: multiple ref KL equation}\\
     &=\frac{1}{\gamma}\left(-\KLr(\pi(\cdot|x)\|\pi_{\thetas}^\gamma\big(y|x\big))+\log \left(\sum_y \prod_{i=1}^K \pi_{\mathrm{ref},i}^{\alpha_i}(y|x)\exp\left(\gamma r(x,y) \right)\right) \right), \label{eq: multiple ref KL equation 3}
 \end{align}
 where \eqref{eq: multiple ref KL equation 2} follows from Lemma \ref{lem: Tilted distribution entropy} and \eqref{eq: multiple ref KL equation} follows from Lemma \ref{lem: Gibbs relative entropy}.
 Clearly, the right side of \eqref{eq: multiple ref KL equation 3} is maximized when the KL divergence is set to zero. Thus, the maximizing distribution $\pi(\cdot|x)$ is identical to $\pi_{\thetas}^\gamma\big(y|x\big)$, and the maximum objective value is $\frac{1}{\gamma}\log \left(\sum_y \prod_{i=1}^K \pi_{\mathrm{ref},i}^{\alpha_i}(y|x)\exp\left(\gamma r(x,y) \right)\right)$.   
\end{proof}
\begin{tcolorbox}
    \begin{corollary}\label{cor:mw-RLHF-RKL}
    Weighted multiple single reverse KL-regularized RLHF problem is an upper bound on multiple references reverse KL-regularized RLHF problem, i.e.,
    \begin{equation}
    \begin{split}
       &\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\}\\&\leq  \sum_{i=1}^K \alpha_i \underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big( \KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\}.
    \end{split}
\end{equation}
\end{corollary}
\end{tcolorbox}

\begin{proof}
It can be shown that the maximum of objective function in Theorem~\ref{thm: main} is,
\begin{equation}
\begin{split}
    &\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\}\\&\quad=\frac{1}{\gamma}\log\Big(\mathbb{E}_{Y\sim \widehat{\pi}_{\pmb{\alpha},\mathrm{ref}}(y|x)}[\exp\big(\gamma r_{\thetas}\big(x,Y\big)\big)]\Big)+\frac{1}{\gamma}\log F_{\pmb{\alpha}}(x) \\
    &=\frac{1}{\gamma}\log\Big(\sum_y  \prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x) \exp\big(\alpha_i\gamma r_{\thetas}\big(x,y\big)\big)\Big)\\
    &\leq \sum_{i=1}^K \frac{\alpha_i}{\gamma}\log\Big( \sum_y \pi_{\mathrm{ref},i}(y|x) \exp\big(\gamma r_{\thetas}\big(x,y\big)\big) \Big),
    \end{split}
\end{equation}
where the last inequality follows from Hölder's inequality. Note that,
\begin{equation}
    \begin{split}
        \underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big( \KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\}=\frac{1}{\gamma}\log\Big( \sum_y \pi_{\mathrm{ref},i}(y|x) \exp\big(\gamma r_{\thetas}\big(x,Y\big)\big) \Big).
    \end{split}
\end{equation}
Then, we have, 
\begin{equation}
    \begin{split}
       &\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\}\\&\leq  \sum_{i=1}^K \alpha_i \underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big( \KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big).\right\}
    \end{split}
\end{equation}
Therefore, multiple single RLHF problem is an upper bound on multiple reference models RLHF problem.
\end{proof}
\begin{remark}[Choosing $\pmb{\alpha}$]
    The optimum $\pmb{\alpha}$ for a given $x$, can be derived from the following optimization problem,
    \begin{equation}
\begin{split}
    \max_{\pmb{\alpha}}\frac{1}{\gamma}\log\Big(\sum_y  \prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x) \exp\big(\alpha_i\gamma r_{\thetas}\big(x,y\big)\big)\Big).
    \end{split}
\end{equation}

\end{remark}
%\begin{remark}[Choosing $\pmb{\alpha}$]
    % Assuming bounded response space, $|\mathcal{Y}|= C$, we have,
    % \begin{equation}
    % \begin{split}
    % &\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\alpha_{i}\KLr\big(\pi (\cdot | x)\| \pi_{\mathrm{ref},i}(\cdot | x)\big)\Big)\right\}\\
    % &=\frac{1}{\gamma}\log(C) +\frac{1}{\gamma}\log\Big(\frac{1}{C}\sum_y  \prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x) \exp\big(\alpha_i\gamma r_{\thetas}\big(x,Y\big)\big)\Big)\\
    % &\geq \frac{1}{\gamma}\log(C) +\frac{1}{C\gamma}\sum_y\log\Big(  \prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x) \exp\big(\alpha_i\gamma r_{\thetas}\big(x,Y\big)\big)\Big)\\
    % &=\frac{1}{\gamma}\log(C) +\frac{\sum_y r_{\thetas}\big(x,y\big)}{C}+\frac{1}{C\gamma}\sum_y\log\Big(  \prod_{i=1}^K \pi_{\mathrm{ref},i}^{ \alpha_i}(y|x) \Big)
    % \end{split}
    % \end{equation}
    % In order to maximize with respect to $\pmb{\alpha}$, we have,
%\end{remark}
\begin{tcolorbox}
    \begin{repproposition}{prop:func_derv_policy}
    For a given response, $x\in\mathcal{X}$, the following upper bound holds,
    \begin{equation*}
    \begin{split}
         &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\leq \int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))(\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y).
    \end{split}
    \end{equation*}
\end{repproposition}
\end{tcolorbox}
\begin{proof}
Note that $\KLr(\pi(\cdot|x)\|\pirefalpha)$ is a convex function with respect to $\pi(\cdot|x)$. Therefore, $J_{\gamma}(\pirefalpha,\pi(\cdot|x))$ is a concave function with respect to $\pi(\cdot|x)$. First, we compute the functional derivative of $J_{\gamma}(\pirefalpha,\pi(\cdot|x))$ with respect to $\pi(\cdot|x)$,
\begin{equation}
    \frac{\delta J_{\gamma}(\pirefalpha,\pi(\cdot|x))}{\delta \pi} = r_{\thetas}(x,y)-\frac{1}{\gamma}\log(\pi(\cdot|x)/\pirefalpha)+\frac{1}{\gamma}.
\end{equation}
    Therefore, we have,
    \begin{equation}
\begin{split}
     & \mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))=\\&
     J_{\gamma}(\pirefalpha,\pi_{\thetas}^\gamma(\cdot|x))-J_{\gamma}(\pirefalpha,\pi_{\thetah}^{\gamma}(\cdot|x))\\
     &\leq \int_{\mathcal{Y}}\frac{\delta J_{\gamma}(\pirefalpha,\pi_{\thetah}^{\gamma}(y|x))}{\delta \pi} (\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y)\\
     &= \int_{\mathcal{Y}}\Big(r_{\thetas}(x,y)-\frac{1}{\gamma}\log(\pi_{\thetah}^{\gamma}(y|x)/\pirefalpha)+\frac{1}{\gamma}\Big) (\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y)\\
     &=\int_{\mathcal{Y}}\Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)+\frac{1}{\gamma}\log(Z(x))\Big) (\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y)\\
     &=\int_{\mathcal{Y}}\Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)\Big) (\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y).
\end{split}
\end{equation}
It completes the proof.
\end{proof}

\begin{tcolorbox}
    \begin{replemma}{lem:sensivitiy_policy}
Consider the softmax policy, $\pi_r^\gamma(y|x)\propto\pirefalpha(y|x)\exp(\gamma r(x,y))$. Then, the sensitivity of the policy with respect to reward function is,
\begin{equation*}
    \frac{\partial \pi_r^\gamma}{\partial r}(r)=\gamma \pi_r^\gamma(y|x) (1-\pi_r^\gamma(y|x)).
\end{equation*}
\end{replemma}
\end{tcolorbox}
\begin{proof}
  We have $\pi_r^\gamma(y|x)=\frac{\pirefalpha(y|x)\exp(\gamma r(x,y))}{\mbE_{Y\sim \pirefalpha(\cdot|x)}[\exp(\gamma r(x,Y))]}.$ Using Chain rule, we have,
  \begin{equation}
      \begin{split}
           \frac{\partial \pi_r^\gamma}{\partial r}(r)&=\gamma \frac{\pirefalpha(y|x)\exp(\gamma r(x,y))}{\mbE_{Y\sim \pirefalpha(\cdot|x)}[\exp(\gamma r(x,Y))]}-\frac{\gamma\pirefalpha(y|x)^2\exp(2\gamma r(x,y))}{\mbE_{Y\sim \pirefalpha(\cdot|x)}[\exp(\gamma r(x,Y))]^2}\\
           &=\gamma \pi_r^\gamma(y|x)(1-\pi_r^\gamma(y|x)).
      \end{split}
  \end{equation}
\end{proof}

\begin{tcolorbox}
    \begin{reptheorem}{thm:sub-gap}
   Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, the following upper bound holds on the sub-optimality gap with probability at least $(1-\delta)$ for $\delta\in(0,1/2)$,
   \begin{equation*}
       \begin{split}
           &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\&\leq \gamma C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} 128 e^{4 R_{\max}}R_{\max}^2\frac{\log(|\mathcal{R}|/\delta)}{n}.
       \end{split}
   \end{equation*}
\end{reptheorem}
\end{tcolorbox}
\begin{proof}
    Using Proposition~\ref{prop:func_derv_policy}, we have,
     \begin{equation}\label{eq:4}
    \begin{split}
         &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\& \leq\int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))(\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y).
    \end{split}
    \end{equation}
    Note that, as the integral in \eqref{eq:4} is over $\mathcal{Y}$, therefore, we have,
     \begin{equation}\label{eq:2}
    \begin{split}
         &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\& \leq\int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))(\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x)-h(x))(\mrd y),
    \end{split}
    \end{equation}
    where $h(x)$ is an arbitrary function over $\mathcal{X}$.
    Note that $\pi_{\thetas}^\gamma(y|x)$ and $\pi_{\thetah}^{\gamma}(y|x)$ are function of $r_{\thetas}(x,y)$ and $r_{\thetah}(x,y)$, respectively. Furthermore, softmax policies are shift invariant, Lemma~\ref{lem:shift}, i.e., $\pi_{\thetas}^\gamma(y|x)\propto\pirefalpha \exp(\gamma(r_\thetas(x,y)-h(x)))$ where $h(x)$ is a function dependent on $x$. Therefore, we can apply the mean-value theorem to $(\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))(\mrd y)$  with respect to function $r(x,y)$. Therefore, we have for a give $h(x)$,
     \begin{equation}\label{eq:1}
    \begin{split}
      (\pi_{\thetas}^\gamma(y|x)-\pi_{\thetah}^{\gamma}(y|x))&=\frac{\partial \pi(\cdot|x)}{\partial r}(r_\lambda)(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x))\\
      &=\gamma \pi_{r_{\lambda}}(\cdot|x)(1-\pi_{r_{\lambda}}(\cdot|x)(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x)),
    \end{split}
    \end{equation}
    where $r_{\lambda}=\lambda (r_{\thetas}(x,y)-h(x)) + (1-\lambda) r_{\thetah}(x,y)$ for some $\lambda\in[0,1]$ and $\pi_{r_{\lambda}}(\cdot|x)\propto \pirefalpha \exp(\gamma r_{\lambda}(x,y))$. Applying \eqref{eq:1} in \eqref{eq:2}, we have,
      \begin{equation}\label{eq:3}
    \begin{split}
         &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\& \leq\gamma\int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))^2 \pi_{r_{\lambda}}(\cdot|x)(1-\pi_{r_{\lambda}}(\cdot|x))(\mrd y)\\
         &\leq \gamma\int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))^2 \pi_{r_{\lambda}}(\cdot|x)(\mrd y) \\
         &\leq C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} \gamma\int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x))^2 \pirefalpha(\mrd y) 
    \end{split}
    \end{equation}
    Choosing $h(x)=\mbE_{Y^l\sim \pirefalpha}[r_{\thetas}(x,Y^l)-r_{\thetah}(x,Y^l)]$, applying Jensen inequality and Lemma~\ref{lemma:B2}, we have,
      \begin{equation}\label{eq:6}
    \begin{split}
         &\mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\\
         &\leq C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} \gamma\int_{\mathcal{Y}}(r_{\thetas}(x,y^w)-r_{\thetah}(x,y^w)-r_{\thetas}(x,y^l)+r_{\thetah}(x,y^l))^2 \pirefalpha(\mrd y^l) \pirefalpha(\mrd y^w)\\
         &\leq \gamma C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} 128 e^{4 R_{\max}}R_{\max}^2\frac{\log(|\mathcal{R}|/\delta)}{n}.
    \end{split}
    \end{equation}
    It completes the proof.
\end{proof}

\begin{tcolorbox}
    \begin{reptheorem}{thm:gap}
  Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, there exists constant $C>0$ such that the following upper bound holds on the optimality gap of the reverse KL-regularized RLHF with probability at least $(1-\delta)$ for $\delta\in(0,1/2)$,
  \begin{equation*}
       \begin{split}
           &\mathcal{J}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\leq \gamma C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} 128 e^{4 R_{\max}}R_{\max}^2\frac{\log(|\mathcal{R}|/\delta)}{n}+C 8R_{\max}e^{2 R_{\max}}\sqrt{\frac{2 C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}}\log(|\mathcal{R}|/\delta)}{n}}.
       \end{split}
   \end{equation*}
\end{reptheorem}
\end{tcolorbox}
\begin{proof}
    We have the following decomposition of the optimality gap,
    \begin{equation}
        \mathcal{J}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))= \mathcal{J}^{\gamma}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))+\frac{\KLr(\pi_{\thetah}^{\gamma}(\cdot|x)\|\pirefalpha)-\KLr(\pi_{\thetas}^\gamma(\cdot|x)\|\pirefalpha)}{\gamma}.
    \end{equation}
    Now, we provide an upper bound on the second term using Lemma~\ref{lem:sensivitiy_policy} and a similar approach for choosing  $h(x)$ in proof of Theorem~\ref{thm:sub-gap}, we have for some $\lambda\in[0,1]$,
    \begin{equation}
        \begin{split}
          &\KLr(\pi_{\thetah}^{\gamma}(\cdot|x)\|\pirefalpha)-\KLr(\pi_{\thetas}^\gamma(\cdot|x)\|\pirefalpha)\\
          &= \int_{\mathcal{Y}}\frac{\partial \pi}{\partial r}(r_\lambda)\Big( \log\big(\frac{\pi_{r_{\lambda}}(\cdot|x)}{\pirefalpha}\big)+1\Big)(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x))(\mrd y)\\
          &=\gamma\int_{\mathcal{Y}}\pi_{r_{\lambda}}(\cdot|x)(1-\pi_{r_{\lambda}}(\cdot|x))\Big( \log\big(\frac{\pi_{r_{\lambda}}(\cdot|x)}{\pirefalpha}\big)+1\Big)(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x))(\mrd y)\\
          &\leq \gamma\sqrt{\int_{\mathcal{Y}}(1-\pi_{r_{\lambda}}(\cdot|x))^2\Big( \log\big(\frac{\pi_{r_{\lambda}}(\cdot|x)}{\pirefalpha}\big)+1\Big)^2(\mrd y) }\sqrt{\int_{\mathcal{Y}}\pi_{r_{\lambda}}(\cdot|x)^2(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x))^2(\mrd y)},
        \end{split}
    \end{equation}
    where in the last inequality, we apply the Cauchy–Schwarz inequality. Using the fact that $\pi_{r_\lambda}\propto \pirefalpha \exp(\gamma r_{\lambda})$ and Lemma~\ref{lemma:B2}, we have,
    \begin{equation}
        \begin{split}
          &\KLr(\pi_{\thetah}^{\gamma}(\cdot|x)\|\pirefalpha)-\KLr(\pi_{\thetas}^\gamma(\cdot|x)\|\pirefalpha)\\
          &\leq \gamma 8\Big( 2\gamma R_{\max}+1\Big)R_{\max}\exp(2 R_{\max})\sqrt{\frac{2C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}}\log(|\mathcal{R}|/\delta)}{n}}.
        \end{split}
    \end{equation}
    The final result holds by applying the union bound.
\end{proof}
% \subsection{Choosing Optimum $\pmb{\alpha}$}
In the following, we compare the RLHF objective function under the multiple reference model policy, $\pirefalpha$, with $i$-th reference model, $\pi_{\mathrm{ref},i}(\cdot|x)$. For this purpose, we bound the difference between these two RLHF objective functions in different scenarios.
\begin{tcolorbox}
    \begin{proposition}\label{prop:diffMS}
    Under Assumption~\ref{ass:bounded_reward}, the following upper bound holds,
    \begin{equation*}
        \begin{split}
&\tilde{J}_\gamma(\pi_{\pmb{\alpha},\mathrm{ref}},\pi_{\thetas}^\gamma)-\tilde{J}_\gamma(\pi_{\mathrm{ref},i},\pi_{\thetas,i}^\gamma)\leq \frac{\exp(\gamma R_{\max})-1}{\gamma\sqrt{2}}\sqrt{\KLr(\pi_{\pmb{\alpha},\mathrm{ref}}\|\pi_{\mathrm{ref},i})}.
        \end{split}
    \end{equation*}
\end{proposition}
\end{tcolorbox}
\begin{proof}
    Note that, for a policy $\pi_{\mathrm{ref}}$ we have,
    \begin{equation}
\tilde{J}_\gamma(\pi_{\mathrm{ref}},\pi_{\thetas}^\gamma)=\frac{1}{\gamma}\log\Big[ \mbE_{\pi_{\mathrm{ref}}}[\exp(\gamma r_{\thetas}(x,y))]\Big].
    \end{equation}
    Therefore, using the functional derivative, we have,
    \begin{equation}
        \begin{split}
&\tilde{J}_\gamma(\pi_{\pmb{\alpha},\mathrm{ref}},\pi_{\thetas}^\gamma)-\tilde{J}_\gamma(\pi_{\mathrm{ref},i},\pi_{\thetas,i}^\gamma)\\
&=\frac{1}{\gamma}\log\Big[ \mbE_{\pi_{\pmb{\alpha},\mathrm{ref}}}[\exp(\gamma r_{\thetas}(x,y))]\Big]-\frac{1}{\gamma}\log\Big[ \mbE_{\pi_{\mathrm{ref},i}}[\exp(\gamma r_{\thetas}(x,y))]\Big]\\
&=\frac{1}{\gamma}\int_{0}^1\int_{\mathcal{Y}}\frac{\exp(\gamma r_{\thetas}(x,y))}{\mbE_{\pi_{\mathrm{ref},\lambda}}[\exp(\gamma r_{\thetas}(x,y))]}\big( \pi_{\pmb{\alpha},\mathrm{ref}}-\pi_{\mathrm{ref},i}\big)(\mrd y) \mrd \lambda\\
&= \frac{1}{\gamma}\int_{0}^1 \frac{1}{\mbE_{\pi_{\mathrm{ref},\lambda}}[\exp(\gamma r_{\thetas}(x,y))]}\int_{\mathcal{Y}}\exp(\gamma r_{\thetas}(x,y))\big( \pi_{\pmb{\alpha},\mathrm{ref}}-\pi_{\mathrm{ref},i}\big)(\mrd y) \mrd \lambda\\
&\leq \frac{\exp(\gamma R_{\max})-1}{\gamma}\sqrt{\frac{\KLr(\pi_{\pmb{\alpha},\mathrm{ref}}\|\pi_{\mathrm{ref},i})}{2}},
        \end{split}
    \end{equation}
    where $\pi_{\mathrm{ref},\lambda}= \pi_{\mathrm{ref},i}+\lambda\big( \pi_{\pmb{\alpha},\mathrm{ref}}-\pi_{\mathrm{ref},i}\big)$ and the last inequality holds due to Lemma~\ref{lem:transport}.
\end{proof}
% Next, we utilize the result in Proposition~\ref{prop}
\newpage
\section{Proofs and Details of Section~\ref{sec_RLHF_FKL}}\label{app_sec_RLHF_FKL}
\begin{tcolorbox}
    \begin{replemma}{flem: average distribution}
Let $\beta_i\in [0,1]$ for all $i\in[k]$ and $\sum_{i=1}^k\beta_i=1$. For any distributions $Q_i$ for all $i\in[k]$ and $R$ such that $Q_i \ll P$, we have
\begin{equation*}
    \begin{split}
       & \sum_{i=1}^k\beta_i \KLr(Q_i\|P)= H\Big(\sum_{i=1}^k\beta_iQ_i\Big)-\sum_{i=1}^k \beta_i H(Q_i)+\KLr\Big(\sum_{i=1}^k\beta_i Q_i\|P\Big).
    \end{split}
\end{equation*}
\end{replemma}
\end{tcolorbox}
\begin{proof}
    We have,
    \begin{align}
        &\sum_{i=1}^k\beta_i \KLr(Q_i\|P)= \sum_{i=1}^k\beta_i Q_i\log(Q_i)-\beta_i Q_i\log(P)\\
        &=-\sum_{i=1}^k \beta_i H(Q_i)+\big (\sum_{i=1}^k \beta_iQ_i\big )\log\big(\sum_{i=1}^k \beta_iQ_i\big )-\big(\sum_{i=1}^k \beta_iQ_i\big)\log\big(\sum_{i=1}^k \beta_iQ_i\big)-\big(\sum_{i=1}^k \beta_iQ_i\big)\log(P)\\
        &=H\Big(\sum_{i=1}^k\beta_iQ_i\Big) -\sum_{i=1}^k \beta_i H(Q_i) + \big(\sum_{i=1}^k \beta_iQ_i\big)\log(\sum_{i=1}^k \beta_iQ_i)-(\sum_{i=1}^k \beta_iQ_i)\log(P)\\
        &= H\Big(\sum_{i=1}^k\beta_iQ_i\Big) -\sum_{i=1}^k \beta_i H(Q_i) + \KLr\Big(\sum_{i=1}^k\beta_i Q_i\|P\Big).
    \end{align}
\end{proof}

\begin{tcolorbox}
    \begin{reptheorem}{fthm: main}
Consider the following objective function for RLHF with multiple reference models,
\begin{equation*}
\underset{\pi}{\max}\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\beta_i\KLr\big( \pi_{\mathrm{ref},i}(\cdot | x)\|\pi (\cdot | x)\big)\Big),
\end{equation*}
where $\sum_{i=1}^K \beta_i=1$ and $\beta_i\in(0,1)$ for $i\in[K]$. Then, the implicit solution of the multiple reference models objective function for RLHF is,
\begin{equation*}
\tilde{\pi}_{\thetas}^\gamma\big(y|x\big)=\frac{\bar{\pi}_{\pmb{\beta},\mathrm{ref}}\Big(y|x\Big)}{\gamma\big(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y)\big)},
\end{equation*}
where 
\begin{equation*}
    \bar{\pi}_{\pmb{\beta},\mathrm{ref}}(y|x)= \sum_{i=1}^K \beta_i\pi_{\mathrm{ref},i}(y|x),
\end{equation*}
    and $\tilde{Z}_{\thetas}(x)$ is the solution to $\int_{y\in\mathcal{Y}} \tilde{\pi}_{\thetas}^\gamma\big(y|x\big)=1$ for a given $x\in\mathcal{X}$.
\end{reptheorem}
\end{tcolorbox}
\begin{proof}
   Using Lemma~\ref{flem: average distribution}, the objective function of forward KL-regularization under multiple reference model can be represented as,
   \begin{equation*}
\underset{\pi}{\max}\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\KLr\big( \pirefbetacdot\|\pi (\cdot | x)\big),
\end{equation*}
where $\pirefbetay=\sum_{i=1}^K \beta_i\pi_{\mathrm{ref},i}(y|x)$. As the function is a concave function with respect to $\pi (\cdot | x)$, we can compute the derivative with respect to $\pi (\cdot | x)$. Therefore, using the functional derivative under the constraint that $\pi (\cdot | x)$ is a probability measure with Lagrange multiplier, $\tilde{Z}
    _{\thetas}(x)$, we have at optimal solution that,
\begin{equation}\label{eq:f1}
    r_{\thetas}\big(x,y\big)+\frac{1}{\gamma}\frac{\pirefbetay}{\tilde{\pi}_{\thetas}^\gamma(y|x)}-\tilde{Z}
    _{\thetas}(x)=0.
\end{equation}
Solving \eqref{eq:f1} results in the final solution, $\tilde{\pi}_{\thetas}^\gamma(y|x)$. 
\end{proof}
    \begin{corollary}\label{cor:mw-RLHF-FKL}
    Weighted multiple single forward KL-regularized RLHF problem is an upper bound on multiple references forward KL-regularized RLHF problem, i.e.,
    \begin{equation}
    \begin{split}
       &\underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big(\sum_{i=1}^{K}\beta_{i}\KLr\big( \pi_{\mathrm{ref},i}(\cdot | x)\| \pi (\cdot | x)\big)\Big)\right\}\\&\leq  \sum_{i=1}^K \beta_{i} \underset{\pi}{\max}\left\{\underset{Y\sim\pi\big(\cdot|x\big)}{{\mathbb{E}}}\big[r_{\thetas}\big(x,Y\big)\big]-\frac{1}{\gamma}\Big( \KLr\big( \pi_{\mathrm{ref},i}(\cdot | x)\| \pi (\cdot | x)\big)\Big)\right\}.
    \end{split}
\end{equation}
\end{corollary}
\begin{proof}
    It holds due to maximum function property. 
\end{proof}

Assuming, \begin{equation*}
\tilde{\pi}_{\thetas}^\gamma\big(y|x\big)=\frac{\bar{\pi}_{\pmb{\beta},\mathrm{ref}}\Big(y|x\Big)}{\gamma\big(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y)\big)},
\end{equation*}
we can provide the following property of $\tilde{Z}_{\thetas}(x)$, inspired by \citep{cohen2017data}.
\begin{tcolorbox}
\begin{lemma}\label{lem:prop-Z}
     The following property holds for $\tilde{Z}_{\thetas}(x)$,
    \begin{itemize}
        \item For any $x\in\mathcal{X}$ where $\rho(x)>0$, we have $\sup_{y\in\mathcal{Y}}r_{\thetas}(x,y)\leq \tilde{Z}_{\thetas}(x)$.
        \item Under Assumption~\ref{ass:bounded_reward}, we have $\sup_{x\in\mathcal{X}}\tilde{Z}_{\thetas}(x)\leq R_{\max}+\frac{1}{\gamma}$.
    \end{itemize}
\end{lemma}   
\end{tcolorbox}
\begin{proof}
    Using the following representation, 
    \begin{equation*}
\tilde{\pi}_{\thetas}^\gamma\big(y|x\big)=\frac{\bar{\pi}_{\pmb{\beta},\mathrm{ref}}\Big(y|x\Big)}{\gamma\big(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y)\big)},
\end{equation*}
we can conclude that for a given $x\in\mathcal{X}$, $\sup_{y\in\mathcal{Y}}r_{\thetas}(x,y)\leq \tilde{Z}_{\thetas}(x)$. Otherwise, $\tilde{\pi}_{\thetas}^\gamma\big(y|x\big)$ will be negative.

For the second part, let's proceed by contradiction. Suppose there exists some $x \in \mathcal{X}$ such that:
$\tilde{Z}{\thetas}(x) > \sup{y\in\mathcal{Y}}r_{\thetas}(x,y) + \frac{1}{\gamma}$

Under this assumption, we can show that:
\[\int_{y}\tilde{\pi}_{\thetas}^\gamma(y|x)(\mrd y) < 1.\]
This contradicts the fundamental requirement that \[\tilde{\pi}_{\thetas}^\gamma(y|x),\] must be a probability distribution. Therefore, our initial assumption must be false.
Consequently, for all $x \in \mathcal{X}$, we must have:
\[\tilde{Z}{\thetas}(x) \leq \sup_{y\in\mathcal{Y}}r_{\thetas}(x,y) + \frac{1}{\gamma}.\]
Taking the supremum of both sides with respect to $x$ completes the proof.
\end{proof}
\begin{tcolorbox}
    \begin{repproposition}{prop:func_derv_policy_fkl}
    For a given response, $x\in\mathcal{X}$, the following upper bound holds,
    \begin{equation*}
    \begin{split}
         &\widetilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))\leq\\& \int_{\mathcal{Y}}(r_{\thetas}(x,y)-r_{\thetah}(x,y))(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y).
    \end{split}
    \end{equation*}
\end{repproposition}
\end{tcolorbox}
\begin{proof}
    The proof is similar to Proposition~\ref{prop:func_derv_policy}. Note that $\KLr(\pirefbetacdot\|\pi(\cdot|x))$ is a convex function with respect $\pi(\cdot|x)$. Therefore, $\tilde{J}_{\gamma}(\pirefbetacdot,\pi(\cdot|x))$ is a concave function with respect to $\pi(\cdot|x)$. First, we compute the functional derivative of $\tilde{J}_{\gamma}(\pirefbetacdot,\pi(\cdot|x))$ with respect to $\pi(\cdot|x)$,
\begin{equation}
    \frac{\delta \tilde{J}_{\gamma}(\pirefbetacdot,\pi(\cdot|x))}{\delta \pi} = r_{\thetas}(x,y)+\frac{1}{\gamma}\frac{\pirefbetacdot}{\pi(\cdot|x)}.
\end{equation}
    Therefore, we have,
    \begin{equation}
        \begin{split}
          &\widetilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))\leq\int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)+\frac{1}{\gamma}\frac{\pirefbetay}{\tilde{\pi}_{\thetah}^{\gamma}(y|x)}\Big)(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y),
        \end{split}
    \end{equation}
    Using the fact that $\tilde{\pi}_{\thetah}^{\gamma}(y|x)=\frac{\pirefbetay}{\gamma(\tilde{Z}(x)-r_{\thetah}(x,y))}$,
    \begin{equation}
        \begin{split}
             \widetilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))&\leq\int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)+\tilde{Z}(x)\Big)(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y)\\
             &= \int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)\Big)(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y),
        \end{split}
    \end{equation}
    where the last equality follows from the fact that $\tilde{Z}(x)$ is just dependent on $x$.
\end{proof}

\begin{tcolorbox}
    \begin{reptheorem}{thm:sub-gap-fkl}
   Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, the following upper bound holds on the sub-optimality gap with probability at least $(1-\delta)$ for $\delta\in(0,1)$,
   \begin{equation*}
       \begin{split}
           &\tilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))\leq 
            16 C_{\pmb{\beta},\varepsilon_{\mathrm{fkl}}} e^{2 R_{\max}}R_{\max}\sqrt{\frac{\log(|\mathcal{R}|/\delta)}{n}}.
       \end{split}
   \end{equation*}
\end{reptheorem}
\end{tcolorbox}
\begin{proof}
    From Proposition~\ref{prop:func_derv_policy_fkl}, we have,
     \begin{equation}
        \begin{split}
             \widetilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))&\leq \int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)\Big)(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y)
             \\& = \int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x)\Big)(\tilde{\pi}_{\thetas}^\gamma(y|x)-\tilde{\pi}_{\thetah}^{\gamma}(y|x))(\mrd y)
              \\& = \int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x)\Big)\pirefbetay\frac{(\tilde{\pi}_{\thetas}^\gamma(y|x) - \tilde{\pi}_{\thetah}^{\gamma}(y|x))}{\pirefbetay}(\mrd y)
              \\& \leq \sqrt{\int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x)\Big)^2(\pirefbetay)^2(\mrd y)}\sqrt{\int_{\mathcal{Y}}\frac{(\tilde{\pi}_{\thetas}^\gamma(y|x) - \tilde{\pi}_{\thetah}^{\gamma}(y|x))^2}{(\pirefbetay)^2}(\mrd y)}\\
              &\leq \sqrt{\int_{\mathcal{Y}}  \Big(r_{\thetas}(x,y)-r_{\thetah}(x,y)-h(x)\Big)^2\pirefbetay(\mrd y)}\sqrt{\int_{\mathcal{Y}}\frac{(\tilde{\pi}_{\thetas}^\gamma(y|x) - \tilde{\pi}_{\thetah}^{\gamma}(y|x))^2}{(\pirefbetay)^2}(\mrd y)}\\
              &\leq 16  C_{\pmb{\beta},\varepsilon_{\mathrm{fkl}}} e^{2 R_{\max}}R_{\max}\sqrt{\frac{\log(|\mathcal{R}|/\delta)}{n}},
        \end{split}
    \end{equation}
    where the first, second, and last inequalities follow from the Cauchy–Schwarz inequality, $(\pirefbetay)^2\leq \pirefbetay$ and using Assumption~\ref{ass:fkl-coverage} and Lemma~\ref{lemma:B2}, respectively.
\end{proof}

\begin{tcolorbox}
    \begin{reptheorem}{thm:gap-fkl}
  Under Assumption~\ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, there exists constant $D>0$ such that the following upper bound holds on optimality gap of the multiple reference forward KL-regularized RLHF algorithm with probability at least $(1-\delta)$ for $\delta\in(0,1)$,
  \begin{equation*}
       \begin{split}
           &\tilde{\mathcal{J}}(\pif_{\thetas}^\gamma(\cdot|x),\pif_{\thetah}^{\gamma}(\cdot|x))\\&\quad\leq 16 C_{\pmb{\beta},\varepsilon_{\mathrm{fkl}}} e^{2 R_{\max}}R_{\max}\sqrt{\frac{\log(|\mathcal{R}|/\delta)}{n}}+\frac{\max\big(|\log(C_{\varepsilon,\mathrm{fkl}})|,\log(\gamma R_{\max}+1)\big)}{\gamma}
       \end{split}
   \end{equation*}
\end{reptheorem}
\end{tcolorbox}
\begin{proof}
    We have the following decomposition of the optimality gap,
    \begin{equation}
        \begin{split}
            \tilde{\mathcal{J}}(\pif_{\thetas}^\gamma(\cdot|x),\pif_{\thetah}^{\gamma}(\cdot|x))&=\tilde{\mathcal{J}}^{\gamma}(\tilde{\pi}_{\thetas}^\gamma(\cdot|x),\tilde{\pi}_{\thetah}^{\gamma}(\cdot|x))+\frac{\KLr(\pirefbetacdot\|\pif_{\thetah}^{\gamma}(\cdot|x))-\KLr(\pirefbetacdot\|\pif_{\thetas}^\gamma(\cdot|x))}{\gamma}.
        \end{split}
    \end{equation}
For second term, using the fact that, $\tilde{\pi}_{\thetah}^{\gamma}(y|x)=\frac{\pirefbetay}{\gamma(\tilde{Z}_{\thetah}(x)-r_{\thetah}(x,y))}$ and $\tilde{\pi}_{\thetas}^{\gamma}(y|x)=\frac{\pirefbetay}{\gamma(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y))}$, we have,
\begin{equation}\label{eq:f1l}
    \begin{split}
&\frac{\KLr(\pirefbetacdot\|\pif_{\thetah}^{\gamma}(\cdot|x))-\KLr(\pirefbetacdot\|\pif_{\thetas}^\gamma(\cdot|x))}{\gamma}\\&=
\frac{\mbE_{Y\sim \pirefbetacdot}[\log(\gamma(\tilde{Z}_{\thetah}(x)-r_{\thetah}(x,y)))]-\mbE_{Y\sim \pirefbetacdot}[\log(\gamma(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y)))] }{\gamma}\\
&\leq \frac{\big|\mbE_{Y\sim \pirefbetacdot}[\log(\gamma(\tilde{Z}_{\thetah}(x)-r_{\thetah}(x,y)))]\big|+\big|\mbE_{Y\sim \pirefbetacdot}[\log(\gamma(\tilde{Z}_{\thetas}(x)-r_{\thetas}(x,y)))]\big|}{\gamma}\\
&\leq\frac{\max\big(|\log(C_{\varepsilon,\mathrm{fkl}})|,\log(\gamma R_{\max}+1)\big)}{\gamma},
% &\leq \max\big(\frac{|\log(C_{\varepsilon,\mathrm{fkl}})|}{\},\log(\gamma R_{\max}+1)\big)
% \leq \frac{2\varepsilon_{\mathrm{fkl}}}{\gamma}
    \end{split}
\end{equation}
where the last inequality follows from Lemma~\ref{lem:prop-Z}.
The final result holds by combining Theorem~\ref{thm:sub-gap-fkl} with \eqref{eq:f1l}.
    \end{proof}
    \newpage
    \section{Extension to DPO}\label{app:DPO}
    Our current results for reverse KL-regularized RLHF and forward KL-regularized RLHF can be extended to the DPO framework \citep{rafailov2023direct}. In particular, we can derive the following DPO function for reverse KL-regularized under multiple reference models scenario using Theorem~\ref{thm: main},
\begin{align}\label{eq:DPO_RKL}
\pi_{\mathrm{DPO},\thetah}^{\mathrm{RKL}}&=\mathop{\arg\max}_{\pi_{\theta}\in\Pi}\sum_{i=1}^n 
    \log\Big[ \sigma\Big(\frac{1}{\gamma}\log(\frac{\pi_{\theta}(y^w_i|x_i)}{\pi_{\pmb{\alpha},\mathrm{ref}}(y^w_i|x_i)})-\frac{1}{\gamma}\log(\frac{\pi_{\theta}(y^l_i|x_i)}{\pi_{\pmb{\alpha},\mathrm{ref}}(y^l_i|x_i)}) \Big)\Big].
\end{align}
For forward KL-regularized DPO, we can combine Theorem~\ref{fthm: main} with the approach outlined in \citep{wangbeyond}, to derive the DPO function, 
\begin{align}\label{eq:DPO_FKL}
\pi_{\mathrm{DPO},\thetah}^{\mathrm{FKL}}&=\mathop{\arg\max}_{\pi_{\theta}\in\Pi}\sum_{i=1}^n 
    \log\Big[ \sigma\Big(\frac{1}{\gamma}\frac{\pi_{\pmb{\beta},\mathrm{ref}}(y^l_i|x_i)}{\pi_{\theta}(y^l_i|x_i)}-\frac{1}{\gamma}\frac{\pi_{\pmb{\beta},\mathrm{ref}}(y^w_i|x_i)}{\pi_{\theta}(y^w_i|x_i)}\Big)\Big].
\end{align}
However, as discussed in \citep{song2024importance}, DPO can not guarantee any performance under some conditions. In particular, The reverse KL-regularized case can fail under partial coverage conditions, necessitating the Global Coverage Assumption (Assumption~\ref{ass:global-coverage}). The forward KL-regularized case requires an even stronger condition: the ratio of reference to policy must be bounded from below away from zero. Specifically, we should have $0<\inf_{(x,y),\rho(x)>0}\frac{\pi_{\pmb{\beta},\mathrm{ref}}(y|x)}{\pi_{\theta}(y|x)}$ which is a stronger assumption. For this purpose, we consider the implicit bounded reward assumptions.

Our theoretical results for reverse KL-regularized RLHF and forward KL-regularized RLHF can be applied DPO problems \eqref{eq:DPO_RKL} and \eqref{eq:DPO_FKL} under the following assumptions.

\begin{assumption}[(Bounded implicit RKL reward]\label{ass:boundd_implicit_reward_RKL}
    For all $y^w,y^l\in\mathcal{Y}$ and $x\in\mathcal{X}$, there exists a constant $B_{\max}$ such that,
    \begin{equation}
        \begin{split}
            \Big|\frac{1}{\gamma}\log(\frac{\pi_{\theta}(y^w|x)}{\pi_{\pmb{\alpha},\mathrm{ref}}(y^w|x)})-\frac{1}{\gamma}\log(\frac{\pi_{\theta}(y^l|x)}{\pi_{\pmb{\alpha},\mathrm{ref}}(y^l|x)}) \Big|\leq B_{\max}.
        \end{split}
    \end{equation}
    
\end{assumption}

\begin{assumption}[(Bounded implicit FKL reward]\label{ass:bounded_implicit_reward_FKL}
    For all $y^w,y^l\in\mathcal{Y}$ and $x\in\mathcal{X}$, there exists a constant $D_{\max}$ such that,
    \begin{equation}
        \begin{split}
            \Big|\frac{1}{\gamma}\frac{\pi_{\pmb{\beta},\mathrm{ref}}(y^l_i|x_i)}{\pi_{\theta}(y^l_i|x_i)}-\frac{1}{\gamma}\frac{\pi_{\pmb{\beta},\mathrm{ref}}(y^w_i|x_i)}{\pi_{\theta}(y^w_i|x_i)} \Big|\leq D_{\max}.
        \end{split}
    \end{equation}
    
\end{assumption}

\begin{tcolorbox}
    \begin{theorem}\label{thm:gap-DPO}
  Under Assumptions~\ref{ass:boundd_implicit_reward_RKL}, \ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, there exists constant $C>0$ such that the following upper bound holds on the optimality gap of DPO based on reverse KL-regularization with probability at least $(1-\delta)$ for $\delta\in(0,1/2)$,
  \begin{equation*}
       \begin{split}
           &\mathcal{J}(\pi_{\thetas}^\gamma(\cdot|x),\pi_{\thetah}^{\gamma}(\cdot|x))\leq \gamma C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}} 128 e^{4 R_{\max}}B_{\max}^2\frac{\log(|\mathcal{R}|/\delta)}{n}+C 8 B_{\max}e^{2 R_{\max}}\sqrt{\frac{2 C_{\pmb{\alpha},\varepsilon_{\mathrm{rkl}}}\log(|\mathcal{R}|/\delta)}{n}}.
       \end{split}
   \end{equation*}
\end{theorem}
\end{tcolorbox}
\begin{proof}
    The proof is similar to Theorem~\ref{thm:gap} using Lemma~\ref{lemma:E5}.
\end{proof}

\begin{tcolorbox}
    \begin{theorem}\label{thm:gap-fkl-DPO}
  Under Assumptions~\ref{ass:bounded_implicit_reward_FKL},  \ref{ass:bounded_reward}, \ref{ass:finite_class} and \ref{ass:kl-coverage}, the following upper bound holds on optimality gap of DPO based on forward KL-regularization with probability at least $(1-\delta)$ for $\delta\in(0,1)$,
  \begin{equation*}
       \begin{split}
           &\tilde{\mathcal{J}}(\pif_{\thetas}^\gamma(\cdot|x),\pif_{\thetah}^{\gamma}(\cdot|x))\leq 16 C_{\pmb{\beta},\varepsilon_{\mathrm{fkl}}} e^{2 R_{\max}}D_{\max}\sqrt{\frac{\log(|\mathcal{R}|/\delta)}{n}}+\frac{\max\big(|\log(C_{\varepsilon,\mathrm{fkl}})|,\log(\gamma R_{\max}+1)\big)}{\gamma}
       \end{split}
   \end{equation*}
\end{theorem}
\end{tcolorbox}
\begin{proof}
    The proof is similar to Theorem~\ref{thm:gap-fkl} by using Lemma~\ref{lemma:E5}.
\end{proof}


\end{document}

