\section{Related Works}
\subsection{Mean-field theory of neural networks and Wasserstein gradient flows}

The study of Wasserstein gradient flows in neural network training is motivated by the observation that, under mean-field scaling, the evolution of neural network parameters under time-accelerated gradient flow can be equivalently described by the evolution of their distribution via the 2-Wasserstein gradient flow. This framework not only characterizes the training dynamics of two-layer neural networks with a finite number of neurons due to the aforementioned equivalence, but also provides the advantage of describing the training process for infinite-width shallow neural networks. The application of Wasserstein gradient flows in the mean-field regime has been successful in analyzing the convergence of neural network training____. For a more detailed discussion on Wasserstein gradient flows in neural network training, see____. For a broader perspective on Wasserstein gradient flows and optimal transport, refer to____.

\subsection{Barron spaces}

Barron spaces, introduced in____, generalize Barron's seminal work____ in neural network approximation theory. A function $f: X \to \mathbb{R}$ is said to be a Barron function if it admits an integral representation of the form  
\begin{align}\label{intrep}
    f(x) = \int_{\mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}} a \sigma(w^T x + b) \pi(da \otimes dw \otimes db), \quad x \in X,
\end{align}  
for some Borel probability measure $\pi$ on $\mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}$, and if its Barron norm, as defined in____, is finite. Here, $\sigma: \mathbb{R} \to \mathbb{R}$ denotes an activation function. When $\sigma$ is chosen as the ReLU function, Barron functions can be approximated by finite-width two-layer neural networks with a dimension-independent approximation rate in terms of the number of parameters____. Moreover, Barron functions constructed using the ReLU activation function exhibit low complexity in statistical learning theory____. These properties extend to Barron functions with certain other activation functions____. Both finite-width and infinite-width two-layer networks in the mean-field scaling can be expressed in the integral form given in~\eqref{intrep}. This integral representation facilitates the study of parameter evolution under gradient flow by leveraging the 2-Wasserstein gradient flow in the parameter distribution, as discussed in____. For a more comprehensive discussion on Barron spaces, see____.