\section{Related Works}
\subsection{Mean-field theory of neural networks and Wasserstein gradient flows}

The study of Wasserstein gradient flows in neural network training is motivated by the observation that, under mean-field scaling, the evolution of neural network parameters under time-accelerated gradient flow can be equivalently described by the evolution of their distribution via the 2-Wasserstein gradient flow. This framework not only characterizes the training dynamics of two-layer neural networks with a finite number of neurons due to the aforementioned equivalence, but also provides the advantage of describing the training process for infinite-width shallow neural networks. The application of Wasserstein gradient flows in the mean-field regime has been successful in analyzing the convergence of neural network training~\cite{chizat2018global,mei2018mean,rotskoff2018neural,lu2020mean,nitanda2021particle}. For a more detailed discussion on Wasserstein gradient flows in neural network training, see~\cite{chizat2018global,chizat2020implicit,wojtowytsch2020convergence}. For a broader perspective on Wasserstein gradient flows and optimal transport, refer to~\cite{villani2009optimal}.

\subsection{Barron spaces}

Barron spaces, introduced in~\cite{ma2022barron}, generalize Barron's seminal work~\cite{barron1993universal} in neural network approximation theory. A function $f: X \to \mathbb{R}$ is said to be a Barron function if it admits an integral representation of the form  
\begin{align}\label{intrep}
    f(x) = \int_{\mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}} a \sigma(w^T x + b) \pi(da \otimes dw \otimes db), \quad x \in X,
\end{align}  
for some Borel probability measure $\pi$ on $\mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}$, and if its Barron norm, as defined in~\cite{ma2022barron}, is finite. Here, $\sigma: \mathbb{R} \to \mathbb{R}$ denotes an activation function. When $\sigma$ is chosen as the ReLU function, Barron functions can be approximated by finite-width two-layer neural networks with a dimension-independent approximation rate in terms of the number of parameters~\cite{ma2022barron}. Moreover, Barron functions constructed using the ReLU activation function exhibit low complexity in statistical learning theory~\cite{ma2022barron}. These properties extend to Barron functions with certain other activation functions~\cite{li2020complexity}. Both finite-width and infinite-width two-layer networks in the mean-field scaling can be expressed in the integral form given in~\eqref{intrep}. This integral representation facilitates the study of parameter evolution under gradient flow by leveraging the 2-Wasserstein gradient flow in the parameter distribution, as discussed in~\cite{wojtowytsch2020can,wojtowytsch2020convergence}. For a more comprehensive discussion on Barron spaces, see~\cite{barron1993universal,ma2022barron,li2020complexity,weinan2022representation}.