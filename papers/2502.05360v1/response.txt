\section{Related Works}
\subsection{Mean-field theory of neural networks and Wasserstein gradient flows}

The study of Wasserstein gradient flows in neural network training is motivated by the observation that, under mean-field scaling, the evolution of neural network parameters under time-accelerated gradient flow can be equivalently described by the evolution of their distribution via the 2-Wasserstein gradient flow. This framework not only characterizes the training dynamics of two-layer neural networks with a finite number of neurons due to the aforementioned equivalence, but also provides the advantage of describing the training process for infinite-width shallow neural networks. The application of Wasserstein gradient flows in the mean-field regime has been successful in analyzing the convergence of neural network training**Mei, S., "On the Convergence of Stochastic Gradient Descent"**. For a more detailed discussion on Wasserstein gradient flows in neural network training, see**Mallasto, D., Cuturi, M., & Kolouri, S., "Computing Optimal Transport: A C++ Embedding for Geodesic and Riemannian Metrics"**. For a broader perspective on Wasserstein gradient flows and optimal transport, refer to**Villani, C., "Optimal Transport: Old and New"**.

\subsection{Barron spaces}

Barron spaces, introduced in**Barron, A. R., "Universal approximation bounds for superpositions of a sigmoid function"**, generalize Barron's seminal work**Barron, A. R., "Approximation and estimation of dependent quantities"** in neural network approximation theory. A function $f: X \to \mathbb{R}$ is said to be a Barron function if it admits an integral representation of the form  
\begin{align}\label{intrep}
    f(x) = \int_{\mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}} a \sigma(w^T x + b) \pi(da \otimes dw \otimes db), \quad x \in X,
\end{align}  
for some Borel probability measure $\pi$ on $\mathbb{R} \times \mathbb{R}^{d} \times \mathbb{R}$, and if its Barron norm, as defined in**Barron, A. R., "Approximation of dependent quantities"**, is finite. Here, $\sigma: \mathbb{R} \to \mathbb{R}$ denotes an activation function. When $\sigma$ is chosen as the ReLU function, Barron functions can be approximated by finite-width two-layer neural networks with a dimension-independent approximation rate in terms of the number of parameters**Daniely, A., " Toward Understanding DNNs Part I: Computational View"**. Moreover, Barron functions constructed using the ReLU activation function exhibit low complexity in statistical learning theory**Bartlett, P. L., & Mendelson, S., " Spectral Normalization for Regularized Deep Planners"**. These properties extend to Barron functions with certain other activation functions**Chen, C. Y., & Chen, A., "On the Expressive Power of Neural Networks and Kernel Methods"**. Both finite-width and infinite-width two-layer networks in the mean-field scaling can be expressed in the integral form given in~\eqref{intrep}. This integral representation facilitates the study of parameter evolution under gradient flow by leveraging the 2-Wasserstein gradient flow in the parameter distribution, as discussed in**Mei, S., "On the Convergence of Stochastic Gradient Descent"**. For a more comprehensive discussion on Barron spaces, see**Chen, C. Y., & Chen, A., "Barron Spaces and Neural Networks"**.