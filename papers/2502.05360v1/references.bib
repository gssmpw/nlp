%% General Reference Book for CoD
@book{grohs2022mathematical,
  title={Mathematical aspects of deep learning},
  author={Grohs, Philipp and Kutyniok, Gitta},
  year={2022},
  publisher={Cambridge University Press}
}
@book{bach2024learning,
  title={Learning theory from first principles},
  author={Bach, Francis},
  year={2024},
  publisher={MIT press}
}

$ CoD PDE
@book{ames2014numerical,
  title={Numerical methods for partial differential equations},
  author={Ames, William F},
  year={2014},
  publisher={Academic press}
}

% CoD Kernel methods
@article{von2004distance,
  title={Distance-Based Classification with Lipschitz Functions.},
  author={von Luxburg, Ulrike and Bousquet, Olivier},
  journal={J. Mach. Learn. Res.},
  volume={5},
  number={Jun},
  pages={669--695},
  year={2004}
}


%% Others, CoD
@inproceedings{shamir2019exponential,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={2691--2713},
  year={2019},
  organization={PMLR}
}

@article{GU2021110444,
title = {SelectNet: Self-paced learning for high-dimensional partial differential equations},
journal = {Journal of Computational Physics},
volume = {441},
pages = {110444},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110444},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121003399},
author = {Yiqi Gu and Haizhao Yang and Chao Zhou},
keywords = {High-dimensional PDEs, Deep neural networks, Self-paced learning, Selected sampling, Least square method, Convergence},
abstract = {The least squares method with deep neural networks as function parametrization has been applied to solve certain high-dimensional partial differential equations (PDEs) successfully; however, its convergence is slow and might not be guaranteed even within a simple class of PDEs. To improve the convergence of the network-based least squares model, we introduce a novel self-paced learning framework, SelectNet, which quantifies the difficulty of training samples, treats samples equally in the early stage of training, and slowly explores more challenging samples, e.g., samples with larger residual errors, mimicking the human cognitive process for more efficient learning. In particular, a selection network and the PDE solution network are trained simultaneously; the selection network adaptively weighting the training samples of the solution network achieving the goal of self-paced learning. Numerical examples indicate that the proposed SelectNet model outperforms existing models on the convergence speed and the convergence robustness, especially for low-regularity solutions.}
}

@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{SIRIGNANO20181339,
title = {DGM: A deep learning algorithm for solving partial differential equations},
journal = {Journal of Computational Physics},
volume = {375},
pages = {1339-1364},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
author = {Justin Sirignano and Konstantinos Spiliopoulos},
keywords = {Partial differential equations, Machine learning, Deep learning, High-dimensional partial differential equations},
abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method (DGM)” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.}
}

@article{doi:10.1073/pnas.1718942115,
author = {Jiequn Han  and Arnulf Jentzen  and Weinan E },
title = {Solving high-dimensional partial differential equations using deep learning},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {34},
pages = {8505-8510},
year = {2018},
doi = {10.1073/pnas.1718942115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1718942115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1718942115},
abstract = {Partial differential equations (PDEs) are among the most ubiquitous tools used in modeling problems in nature. However, solving high-dimensional PDEs has been notoriously difficult due to the “curse of dimensionality.” This paper introduces a practical algorithm for solving nonlinear PDEs in very high (hundreds and potentially thousands of) dimensions. Numerical results suggest that the proposed algorithm is quite effective for a wide variety of problems, in terms of both accuracy and speed. We believe that this opens up a host of possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships. Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the “curse of dimensionality.” This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black–Scholes equation, the Hamilton–Jacobi–Bellman equation, and the Allen–Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their interrelationships.}}


@article{poggio2017and,
  title={Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
  author={Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017},
  publisher={Springer}
}

% CoD in generalization
@article{li2022robust,
  title={Why robust generalization in deep learning is difficult: Perspective of expressive power},
  author={Li, Binghui and Jin, Jikai and Zhong, Han and Hopcroft, John and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4370--4384},
  year={2022}
}

@article{yu2024generalizability,
  title={Generalizability of Memorization Neural Networks},
  author={Yu, Lijia and Gao, Xiao-Shan and Zhang, Lijun and Miao, Yibo},
  journal={arXiv preprint arXiv:2411.00372},
  year={2024}
}

@article{chen2023deep,
title={Deep Operator Learning Lessens the Curse of Dimensionality for {PDE}s},
author={Ke Chen and Chunmei Wang and Haizhao Yang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=zmBFzuT2DN},
note={}
}

@article{chen2024letdatatalkdataregularized,
      title={Let data talk: data-regularized operator learning theory for inverse problems}, 
      author={Ke Chen and Chunmei Wang and Haizhao Yang},
      year={2024},
      journal={arxiv:2310.09854},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2310.09854}, 
}

% CoD in approximation and generalization
@article{suzuki2018adaptivity,
  title={Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality},
  author={Suzuki, Taiji},
  journal={arXiv preprint arXiv:1810.08033},
  year={2018}
}

@article{JMLR:v25:22-0719,
  author  = {Hao Liu and Haizhao Yang and Minshuo Chen and Tuo Zhao and Wenjing Liao},
  title   = {Deep Nonparametric Estimation of Operators between Infinite Dimensional Spaces},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {24},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v25/22-0719.html}
}

@Article{CiCP-28-5,
author = {Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
title = {Deep Network Approximation Characterized by Number of Neurons},
journal = {Communications in Computational Physics},
year = {2020},
volume = {28},
number = {5},
pages = {1768--1811},
abstract = {<p style="text-align: justify;">This paper quantitatively characterizes the approximation power of deep
feed-forward neural networks (FNNs) in terms of the number of neurons. It is shown
by construction that ReLU FNNs with width&nbsp;$\mathcal{O}$(max{$d⌊N^{1/d}⌋$,$N$+1}) and depth $\mathcal{O}(L)$ can approximate an arbitrary Hölder continuous function of order $α∈(0,1]$ on $[0,1]^d$ with a nearly tight approximation rate $\mathcal{O}(\sqrt{d}N^{−2α/d}L^{−2α/d})$ measured in $L^p$ -norm for
any $N,L∈\mathbb{N}^+$ and $p∈[1,∞]$. More generally for an arbitrary continuous function $f$ on $[0,1]^d$ with a modulus of continuity $ω_f
(·)$, the constructive approximation rate
is $\mathcal{O}(\sqrt{d}ω_f(N^{−2α/d}L^{−2α/d}))$. We also extend our analysis to $f$ on irregular domains or
those localized in an ε-neighborhood of a $d_\mathcal{M}$-dimensional smooth manifold $\mathcal{M}⊆[0,1]^d$ with $d_\mathcal{M}≪d$. Especially, in the case of an essentially low-dimensional domain, we
show an approximation rate $\mathcal{O}(ω_f(\frac{ε}{1−δ}\sqrt{\frac{d}{d_δ}}+ε)+\sqrt{d}ω_f(\frac{\sqrt{d}}{1−δ\sqrt{d_δ}}N^{−2α/d_δ}L^{−2α/d_δ})$ for
ReLU FNNs to approximate $f$ in the $ε$-neighborhood, where $d_δ=\mathcal{O}(d_\mathcal{M}\frac{\rm{ln}(d/δ)}{δ^2})$ for any $δ∈(0,1)$ as a relative error for a projection to approximate an isometry when projecting $\mathcal{M}$ to a $d_δ$-dimensional domain.</p>},
issn = {1991-7120},
doi = {https://doi.org/10.4208/cicp.OA-2020-0149},
url = {https://global-sci.com/article/79740/deep-network-approximation-characterized-by-number-of-neurons}
}



@article{doi:10.1137/20M134695X,
author = {Lu, Jianfeng and Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
title = {Deep Network Approximation for Smooth Functions},
journal = {SIAM Journal on Mathematical Analysis},
volume = {53},
number = {5},
pages = {5465-5506},
year = {2021},
doi = {10.1137/20M134695X},

URL = { 
    
        https://doi.org/10.1137/20M134695X
    
    

},
eprint = { 
    
        https://doi.org/10.1137/20M134695X
    
    

}
,
    abstract = { This paper establishes the optimal approximation error characterization of deep rectified linear unit (ReLU) networks for smooth functions in terms of both width and depth simultaneously. To that end, we first prove that multivariate polynomials can be approximated by deep ReLU networks of width \$\mathcal{O}(N)\$ and depth \$\mathcal{O}(L)\$ with an approximation error \$\mathcal{O}(N^{-L})\$. Through local Taylor expansions and their deep ReLU network approximations, we show that deep ReLU networks of width \$\mathcal{O}(N\ln N)\$ and depth \$\mathcal{O}(L\ln L)\$ can approximate \$f\in C^s([0,1]^d)\$ with a nearly optimal approximation error \$\mathcal{O}(\|f\|\_{C^s([0,1]^d)}N^{-2s/d}L^{-2s/d})\$. Our estimate is nonasymptotic in the sense that it is valid for arbitrary width and depth specified by \$N\in\mathbb{N}^+\$ and \$L\in\mathbb{N}^+\$, respectively. }
}




@article{SHEN2022101,
title = {Optimal approximation rate of ReLU networks in terms of width and depth},
journal = {Journal de Mathématiques Pures et Appliquées},
volume = {157},
pages = {101-135},
year = {2022},
issn = {0021-7824},
doi = {https://doi.org/10.1016/j.matpur.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0021782421001124},
author = {Zuowei Shen and Haizhao Yang and Shijun Zhang},
keywords = {Deep ReLU networks, Optimal approximation, VC-dimension, Bit extraction},
abstract = {This paper concentrates on the approximation power of deep feed-forward neural networks in terms of width and depth. It is proved by construction that ReLU networks with width O(max⁡{d⌊N1/d⌋,N+2}) and depth O(L) can approximate a Hölder continuous function on [0,1]d with an approximation rate O(λd(N2L2ln⁡N)−α/d), where α∈(0,1] and λ>0 are Hölder order and constant, respectively. Such a rate is optimal up to a constant in terms of width and depth separately, while existing results are only nearly optimal without the logarithmic factor in the approximation rate. More generally, for an arbitrary continuous function f on [0,1]d, the approximation rate becomes O(dωf((N2L2ln⁡N)−1/d)), where ωf(⋅) is the modulus of continuity. We also extend our analysis to any continuous function f on a bounded set. Particularly, if ReLU networks with depth 31 and width O(N) are used to approximate one-dimensional Lipschitz continuous functions on [0,1] with a Lipschitz constant λ>0, the approximation rate in terms of the total number of parameters, W=O(N2), becomes O(λWln⁡W), which has not been discovered in the literature for fixed-depth ReLU networks.
Résumé
Cet article se concentre sur la capacité d'approximation des réseaux de neurones à propagation avant en termes de largeur et de profondeur. Il est prouvé par construction que les réseaux ReLU de largeur O(max⁡{d⌊N1/d⌋,N+2}) et profondeur O(L) peuvent approximer une fonction höldérienne sur [0,1]d avec une erreur d'approximation O(λd(N2L2ln⁡N)−α/d), où α∈(0,1] et λ>0 sont respectivement l'exposant et la constante de Hölder. Une telle erreur d'approximation est optimale, à une constante multiplicative près, en termes de largeur et de profondeur séparément, alors que les résultats connus ne sont que presque optimaux sans le facteur logarithmique dans l'erreur d'approximation. Plus généralement, pour une fonction continue arbitraire f sur [0,1]d, l'erreur d'approximation devient O(dωf((N2L2ln⁡N)−1/d)), où ωf(⋅) est le module de continuité. Nous étendons également notre analyse à toute fonction continue f sur un ensemble borné. En particulier, si les réseaux ReLU de profondeur 31 et de largeur O(N) sont utilisés pour approximer les fonctions lipschitziennes d'une variable sur [0,1] avec une constante de Lipschitz λ>0, l'erreur d'approximation en fonction du nombre total de paramètres, W=O(N2), devient O(λWln⁡W), ce qui n'a pas été découvert dans la littérature pour les réseaux ReLU à profondeur fixée.}
}

$ Overcoming CoD
@article{montanelli2019deep,
  title={Deep ReLU networks overcome the curse of dimensionality for bandlimited functions},
  author={Montanelli, Hadrien and Yang, Haizhao and Du, Qiang},
  journal={arXiv preprint arXiv:1903.00735},
  year={2019}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={19},
  pages={1--53},
  year={2017}
}

@article{cabannes2021overcoming,
  title={Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning},
  author={Cabannes, Vivien and Pillaud-Vivien, Loucas and Bach, Francis and Rudi, Alessandro},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30439--30451},
  year={2021}
}

@article{berner2020analysis,
  title={Analysis of the generalization error: Empirical risk minimization over deep artificial neural networks overcomes the curse of dimensionality in the numerical approximation of Black--Scholes partial differential equations},
  author={Berner, Julius and Grohs, Philipp and Jentzen, Arnulf},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={3},
  pages={631--657},
  year={2020},
  publisher={SIAM}
}



%% Approximation Theory, CoD
@article{devore1989optimal,
  title={Optimal nonlinear approximation},
  author={DeVore, Ronald A and Howard, Ralph and Micchelli, Charles},
  journal={Manuscripta mathematica},
  volume={63},
  pages={469--478},
  year={1989},
  publisher={Springer}
}

@article{mhaskar1996neural,
  title={Neural networks for optimal approximation of smooth and analytic functions},
  author={Mhaskar, Hrushikesh N},
  journal={Neural computation},
  volume={8},
  number={1},
  pages={164--177},
  year={1996},
  publisher={MIT Press}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep ReLU networks},
  author={Yarotsky, Dmitry},
  booktitle={Conference on learning theory},
  pages={639--649},
  year={2018},
  organization={PMLR}
}

@article{yarotsky2020phase,
  title={The phase diagram of approximation rates for deep neural networks},
  author={Yarotsky, Dmitry and Zhevnerchuk, Anton},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={13005--13015},
  year={2020}
}

@article{grohs2023lower,
  title={Lower bounds for artificial neural network approximations: A proof that shallow neural networks fail to overcome the curse of dimensionality},
  author={Grohs, Philipp and Ibragimov, Shokhrukh and Jentzen, Arnulf and Koppensteiner, Sarah},
  journal={Journal of Complexity},
  volume={77},
  pages={101746},
  year={2023},
  publisher={Elsevier}
}

%% Over-parametrization
@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}


@article{zou2019improved,
  title={An improved analysis of training over-parameterized deep neural networks},
  author={Zou, Difan and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{chen2022feature,
  title={On feature learning in neural networks with global convergence guarantees},
  author={Chen, Zhengdao and Vanden-Eijnden, Eric and Bruna, Joan},
  journal={arXiv preprint arXiv:2204.10782},
  year={2022}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine learning},
  volume={109},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{oymak2020toward,
  title={Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={84--105},
  year={2020},
  publisher={IEEE}
}

@article{liu2022loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={85--116},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{li2020learning,
  title={Learning over-parametrized two-layer neural networks beyond ntk},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Conference on learning theory},
  pages={2613--2682},
  year={2020},
  organization={PMLR}
}

@inproceedings{zhou2021local,
  title={A local convergence theory for mildly over-parameterized two-layer neural network},
  author={Zhou, Mo and Ge, Rong and Jin, Chi},
  booktitle={Conference on Learning Theory},
  pages={4577--4632},
  year={2021},
  organization={PMLR}
}

@article{soltanolkotabi2018theoretical,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}
}







%% Mean field theory
@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{rotskoff2018neural,
  title={Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={stat},
  volume={1050},
  pages={22},
  year={2018}
}

@inproceedings{lu2020mean,
  title={A mean field analysis of deep resnet and beyond: Towards provably optimization via overparameterization from depth},
  author={Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
  booktitle={International Conference on Machine Learning},
  pages={6426--6436},
  year={2020},
  organization={PMLR}
}

@article{nitanda2021particle,
  title={Particle dual averaging: Optimization of mean field neural network with global convergence rate analysis},
  author={Nitanda, Atsushi and Wu, Denny and Suzuki, Taiji},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19608--19621},
  year={2021}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on learning theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric and others},
  volume={338},
  year={2009},
  publisher={Springer}
}




%% NTKs
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}





%% CoD in numerical integration
@article{hinrichs2017product,
  title={Product rules are optimal for numerical integration in classical smoothness spaces},
  author={Hinrichs, Aicke and Novak, Erich and Ullrich, Mario and Wo{\'z}niakowski, Henryk},
  journal={Journal of Complexity},
  volume={38},
  pages={39--49},
  year={2017},
  publisher={Elsevier}
}

@article{hinrichs2014curse,
  title={The curse of dimensionality for numerical integration of smooth functions},
  author={Hinrichs, Aicke and Novak, Erich and Ullrich, Mario and Wo{\'z}niakowski, H},
  journal={Mathematics of Computation},
  volume={83},
  number={290},
  pages={2853--2863},
  year={2014}
}

%% Main references
@article{wojtowytsch2021kolmogorov,
  title={Kolmogorov width decay and poor approximators in machine learning: Shallow neural networks, random feature models and neural tangent kernels},
  author={Wojtowytsch, Stephan and others},
  journal={Research in the mathematical sciences},
  volume={8},
  number={1},
  pages={1--28},
  year={2021},
  publisher={Springer}
}
@article{wojtowytsch2020can,
  title={Can shallow neural networks beat the curse of dimensionality? a mean field training perspective},
  author={Wojtowytsch, Stephan and Weinan, E},
  journal={IEEE Transactions on Artificial Intelligence},
  volume={1},
  number={2},
  pages={121--129},
  year={2020},
  publisher={IEEE}
}


%% Wasserstein Training for ReLU
@article{wojtowytsch2020convergence,
  title={On the convergence of gradient descent training for two-layer relu-networks in the mean field regime},
  author={Wojtowytsch, Stephan},
  journal={arXiv preprint arXiv:2005.13530},
  year={2020}
}



%% Barron related spaces
@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{ma2022barron,
  title={The Barron space and the flow-induced function spaces for neural network models},
  author={Ma, Chao and Wu, Lei and others},
  journal={Constructive Approximation},
  volume={55},
  number={1},
  pages={369--406},
  year={2022},
  publisher={Springer}
}

@article{li2020complexity,
  title={Complexity measures for neural networks with general activation functions using path-based norms},
  author={Li, Zhong and Ma, Chao and Wu, Lei},
  journal={arXiv preprint arXiv:2009.06132},
  year={2020}
}

@article{weinan2022representation,
  title={Representation formulas and pointwise properties for Barron functions},
  author={Weinan, E and Wojtowytsch, Stephan},
  journal={Calculus of Variations and Partial Differential Equations},
  volume={61},
  number={2},
  pages={46},
  year={2022},
  publisher={Springer New York}
}

@article{wojtowytsch2020banach,
  title={On the banach spaces associated with multi-layer relu networks: Function representation, approximation theory and gradient descent dynamics},
  author={Wojtowytsch, Stephan and others},
  journal={arXiv preprint arXiv:2007.15623},
  year={2020}
}



%% Rademacher Complexity
@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

%% Quadratic Activation 
@article{sarao2020optimization,
  title={Optimization and generalization of shallow neural networks with quadratic activation functions},
  author={Sarao Mannelli, Stefano and Vanden-Eijnden, Eric and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13445--13455},
  year={2020}
}

@inproceedings{du2018power,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon and Lee, Jason},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2018},
  organization={PMLR}
}


%% ReLU^k or RePU activation
@article{yang2024optimal,
  title={Optimal rates of approximation by shallow relu k neural networks and applications to nonparametric regression},
  author={Yang, Yunfei and Zhou, Ding-Xuan},
  journal={Constructive Approximation},
  pages={1--32},
  year={2024},
  publisher={Springer}
}

@article{siegel2022high,
  title={High-order approximation rates for shallow neural networks with cosine and ReLUk activation functions},
  author={Siegel, Jonathan W and Xu, Jinchao},
  journal={Applied and Computational Harmonic Analysis},
  volume={58},
  pages={1--26},
  year={2022},
  publisher={Elsevier}
}

@article{siegel2024sharp,
  title={Sharp bounds on the approximation rates, metric entropy, and n-widths of shallow neural networks},
  author={Siegel, Jonathan W and Xu, Jinchao},
  journal={Foundations of Computational Mathematics},
  volume={24},
  number={2},
  pages={481--537},
  year={2024},
  publisher={Springer}
}


@article{heeringa2024embeddings,
  title={Embeddings between Barron spaces with higher-order activation functions},
  author={Heeringa, Tjeerd Jan and Spek, Len and Schwenninger, Felix L and Brune, Christoph},
  journal={Applied and Computational Harmonic Analysis},
  volume={73},
  pages={101691},
  year={2024},
  publisher={Elsevier}
}

@article{luo2020two,
  title={Two-layer neural networks for partial differential equations: Optimization and generalization theory},
  author={Luo, Tao and Yang, Haizhao},
  journal={arXiv preprint arXiv:2006.15733},
  year={2020}
}

%% Advanced Activation Function
@article{shen2021deep,
  title={Deep network with approximation error being reciprocal of width to power of square root of depth},
  author={Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
  journal={Neural Computation},
  volume={33},
  number={4},
  pages={1005--1036},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{shen2021neural,
  title={Neural network approximation: Three hidden layers are enough},
  author={Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
  journal={Neural Networks},
  volume={141},
  pages={160--173},
  year={2021},
  publisher={Elsevier}
}


%% Nonconvergence or Slow(exponential) convergence results.
@article{cheridito2021non,
  title={Non-convergence of stochastic gradient descent in the training of deep neural networks},
  author={Cheridito, Patrick and Jentzen, Arnulf and Rossmannek, Florian},
  journal={Journal of Complexity},
  volume={64},
  pages={101540},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{yang2023nearly,
title={Nearly Optimal {VC}-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives},
author={Yahong Yang and Haizhao Yang and Yang Xiang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=SE73LzWNjr}
}

@article{yang2025nearlyoptimalapproximationrates,
      title={Nearly Optimal Approximation Rates for Deep Super ReLU Networks on Sobolev Spaces}, 
      author={Yahong Yang and Yue Wu and Haizhao Yang and Yang Xiang},
      year={2025},
      journal={arxiv:2310.10766},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2310.10766}, 
}