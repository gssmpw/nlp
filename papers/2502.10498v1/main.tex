%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage[switch]{lineno}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{subfigure}
\usetikzlibrary{mindmap, shapes.geometric, arrows, positioning}
\definecolor{kaiming-green}{RGB}{57,181,74} % kaiming green
\hypersetup{
    colorlinks=true,
    linkcolor=red,
    filecolor=magenta,      
    urlcolor=magenta,
    citecolor=kaiming-green
}
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{The Role of World Models in Shaping Autonomous Driving: \\A Comprehensive Survey}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Sifan Tu$^1$
\and
Xin Zhou$^1$
\and
Dingkang Liang$^{1}$
\and
Xingyu Jiang$^{1}$ 
\and \\
Yumeng Zhang$^2$
\and
Xiaofan Li$^2$
\And
Xiang Bai$^{1\dagger}$\\
\affiliations
$^1$Huazhong University of Science and Technology,
$^2$Baidu Inc.
\emails
\{sifantu, dkliang, xbai\}@hust.edu.cn
}

\begin{document}

\maketitle
{\let\thefootnote\relax\footnotetext{$\dagger$ Corresponding author.}}
\begin{abstract}
Driving World Model (DWM), which focuses on predicting scene evolution during the driving process, has emerged as a promising paradigm in pursuing autonomous driving. These methods enable autonomous driving systems to better perceive, understand, and interact with dynamic driving environments. In this survey, we provide a comprehensive overview of the latest progress in DWM. We categorize existing approaches based on the modalities of the predicted scenes and summarize their specific contributions to autonomous driving. In addition, high-impact datasets and various metrics tailored to different tasks within the scope of DWM research are reviewed. Finally, we discuss the potential limitations of current research and propose future directions. This survey provides valuable insights into the development and application of DWM, fostering its broader adoption in autonomous driving. The relevant papers are collected at \url{https://github.com/LMD0311/Awesome-World-Model}.

\end{abstract}

\section{Introduction}
World Models aim to predict future observations based on past observations and actions~\cite{ha2018world}. In autonomous driving, the large fields of view and the highly dynamic nature of real-world driving scenarios present unique challenges~\cite{gao2024vista}. Nevertheless, the Driving World Model (DWM) plays a crucial role in addressing complex environments by facilitating accurate scene evolution predictions. Recently, DWM has garnered increasing attention due to its vital role in ensuring safe and reliable autonomous driving.

As illustrated in Fig.~\ref{fig:dwm}, DWM typically involves predicting scene evolution, i.e., generating future scenes given historical observations. Building on this, most approaches can further control scene evolution to comply with given conditions or output responses informed by observations and predictions. Since autonomous driving requires handling diverse data types, DWM has led to variants that predict scenes in different modalities. Some methods leverage semantic information in 2D images~\cite{hu2023gaia,wang2023drivedreamer}, while others learn spatial geometry and precise dynamics in 3D point cloud~\cite{zhang2023learning,yang2024visual} or occupancy~\cite{zheng2024occworld,wang2024occsora}. Additionally, some approaches, which we systematically categorize as scene-free paradigms, prioritize latent state~\cite{li2024enhancing,li2024think2drive} or multi-agent behaviors~\cite{zhang2023trafficbots,hamdan2024carformer}, rather than detailed scenes.

\begin{figure}[t]
\centering 
\includegraphics[width=0.99\linewidth]{image/dwm.pdf} 
    \vspace{-7pt}
\caption{DWM utilizes historical observations as the primary input to predict future scene evolutions. Additionally, many methods also incorporate condition inputs or response outputs.} 
\label{fig:dwm} 
\end{figure}

The path to achieving autonomous driving will present challenges such as high data acquisition costs, the complexity of dynamic environments, and managing rare cases, etc. DWM may provide a practical solution for these issues by improving planning through future state predictions~\cite{yang2024driving,wang2024driving}, enriching training datasets with diverse synthetic data~\cite{zyrianov2024lidardm,yang2024physical}, and enhancing downstream task via a scalable pre-training~\cite{gao2024vista,min2024driveworld}. These developments underscore DWM's versatility and effectiveness in the field.

This paper investigates advancements and specific applications of recent DWM approaches. Specifically, our contributions can be summarized as follows:
\begin{itemize}
\item We provide a comprehensive review of recent progress in DWM, categorizing methods by predicted scene modalities, along with an overview of DWM applications in autonomous driving.
\item We conduct a critical analysis of current limitations and outline promising future research directions, providing valuable insights for the research community and facilitating the continued evolution of DWM.
\end{itemize}


We note that the recent emergence of surveys related to DWM, some of which mainly focus on general world models~\cite{zhu2024sora,ding2024understanding}, while the rest fail to provide a detailed categorization of DWM applications~\cite{zeng2024world,fu2024exploring,guan2024world}. In contrast, this paper not only introduces a systematic categorization of recent methods but also offers a thorough summary of the diverse applications of DWM, with the dual aims of understanding current advancements and exploring future directions.

\begin{table*}[t]
    \scriptsize
    \centering
    \caption{Overview of recent Driving World Models.}
        \vspace{-7pt}
    \setlength{\tabcolsep}{0.5mm}
    \begin{tabular}{l c  c c cc}
    
    \toprule %[2pt] 
    \multirow{2.3}{*}{Method}& \multirow{2.3}{*}{Core Architecture}& \multicolumn{2}{c}{Scene Modality} & \multirow{2.3}{*}{Condition} & \multirow{2.3}{*}{Response}  \\ 
    \cmidrule(lr){3-4}  & & Historical Scene & Predicted Scene &  \\ \midrule
SEM2~\cite{gao2024enhance} & RNN & Image, Point cloud & Image, Point cloud, Mask & Action & Action \\
Iso-Dream~\cite{pan2022iso} & RNN & Image & Image & Action & Action \\
MILE~\cite{hu2022model} & RNN & Image & Image, BEV segmentation & Action & Action \\
TrafficBots~\cite{zhang2023trafficbots} & Transformer & Multi-agent behavior & Multi-agent behavior & Action, Map, Destination, Traffic sign & Action \\
Uniworld~\cite{min2023uniworld} & Transformer & Image & Occupancy & - & - \\
DriveDreamer~\cite{wang2023drivedreamer} & Diffusion & Image & Image & Action, Map, Layout, Text & Action \\
GAIA-1~\cite{hu2023gaia} & Transformer & Image & Image & Action, Text & - \\
Copilot4D~\cite{zhang2023learning} & Diffusion & Point cloud & Point cloud & - & - \\
MUVO~\cite{bogdoll2023muvo} & RNN & Image, Point cloud & Image, Occupancy, Point cloud & Action & Action \\
ADriver-I~\cite{jia2023adriver} & Diffusion & Image & Image & Text & Text \\
OccWorld~\cite{zheng2024occworld} & Transformer & Occupancy & Occupancy & Trajectory & Trajectory \\
Drive-WM~\cite{wang2024driving} & Diffusion & Image & Image & Action, Map, Layout, Text & Trajectory \\
WoVoGen~\cite{lu2024wovogen} & Diffusion & Occupancy, Image &Image & Text, Layout & - \\
ViDAR~\cite{yang2024visual} & Transformer & Image & Point cloud & Action & - \\
Think2Drive~\cite{li2024think2drive} & RNN & Latent state & Latent state & Map, Layout, Sign & Action \\
DriveDreamer-2~\cite{zhao2024drivedreamer} & Diffusion & Image & Image & Text & - \\
LidarDM~\cite{zyrianov2024lidardm} & Diffusion & - & Point cloud & Layout & - \\
DriveWorld~\cite{min2024driveworld} & Transformer & Image & Occupancy & Action & Action \\
Vista~\cite{gao2024vista} & Diffusion & Image & Image & Action, Trajectory, Text, Destination & - \\
OccSora~\cite{wang2024occsora} & Diffusion & - & Occupancy & Trajectory & - \\
LAW~\cite{li2024enhancing} & Transformer & Image & Latent state & Action & Action \\
UnO~\cite{agro2024uno} & Transformer & Point cloud & Occupancy & - & \\
AdaptiveDriver~\cite{vasudevan2024planning} & GCNN, PDM-C & Multi-agent behavior &  Multi-agent behavior & - & Action \\
BEVWorld~\cite{zhang2024bevworld} & Diffusion & Image, Point cloud & Image, Point cloud & Action & - \\
CarFormer~\cite{hamdan2024carformer} & Transformer & Multi-agent behavior & Multi-agent behavior &Action, Trajectory, Destination, Traffic sign & Trajectory \\
Drive-OccWorld~\cite{yang2024driving} & Transformer & Image & Occupancy, Flow & Action, Trajectory, Text & Trajectory \\
NeMo~\cite{huang2024neural} & Transformer & Image & Image, Occupancy & - & Trajectory \\
OccLLaMA~\cite{wei2024occllama} & Transformer & Occupancy & Occupancy & Trajectory, Text & Trajectory, Text \\
RenderWorld~\cite{yan2024renderworld} & Transformer & Occupancy & Occupancy & - & Trajectory \\
%LatentDriver~\cite{xiao2024learning} & Transformer & Vectorized Observation & Latent state & Action & Action \\
Covariate Shift~\cite{popov2024mitigating} & Transformer & Image & Latent state & Action & Action \\
DOME~\cite{gu2024dome} & Diffusion & Occupancy & Occupancy & Trajectory & - \\
DirveDreamer4D~\cite{zhao2024drivedreamer4d} & Diffusion & Image & Image & Map, Layout, Text & - \\
Imagine-2-Drive~\cite{garg2024imagine} & Diffusion & Image & Image & Trajectory & Trajectory \\
ReconDreamer~\cite{ni2024recondreamer} & Diffusion & Image & Image &Map, Layout & - \\
InfinityDrive~\cite{guo2024infinitydrive} & Transformer & Image & Image & Text, Action & - \\
HoloDrive~\cite{wu2024holodrive} & Transformer & Image, Point cloud & Image, Point cloud, Depth & Text, Layout & - \\
DrivePhysica~\cite{yang2024physical} & Diffusion & Image & Image & Action, Map, Layout, Flow, Text & - \\
Doe-1~\cite{zheng2024doe} & Transformer & Image & Image & Trajectory, Text & Trajectory, Text \\
GaussianWorld~\cite{zuo2024gaussianworld} & TransFormer &  Gaussians & Occupancy & Image, Action & - \\
GEM~\cite{hassan2024gem} & Diffusion & Image & Image, Depth & Trajectory, Human pose, DINO features & - \\
DFIT-OccWorld~\cite{zhang2024efficient} & Transformer & Occupancy, Image & Occupancy, Image & Trajectory & Trajectory \\
DrivingGPT~\cite{chen2024drivinggpt} & Transformer & Image & Image & Action & Action \\
DrivingWorld~\cite{hu2024drivingworld} & Transformer & Image & Image & Trajectory & Trajectory \\
ACT-Bench~\cite{arai2024act} & Transformer & Image & Image & Action & - \\
AdaWM~\cite{wang2025adawm} & RNN & Latent state & Latent state & Action & Action \\
% HERMES~\cite{zhou2025hermes} & Transformer & Image & Point cloud & Text, Action & Text \\
AD-L-JEPA~\cite{zhu2025ad} & JEPA & Point cloud & Latent state & - & - \\ 
HERMES~\cite{zhou2025hermes} & Transformer & Image & Point cloud & Text, Action & Text \\
\bottomrule
    \end{tabular}
    \label{tab:overview}
\end{table*}

\section{Driving World Model}
\label{sec:2} 

The predicted scenarios of Driving World Models (DWM) encompass multiple modalities, including images, point clouds, and occupancy, introducing unique advantages and challenges. Recent advancements have increasingly focused on leveraging complementary multi-sensor data to harness the strengths of individual modalities while mitigating their inherent limitations. In addition, scene-free paradigms explore predictions within latent spaces or focus on modeling complex multi-agent interactions. Tab.~\ref{tab:overview} provides an overview of recent approaches. This section presents the latest advancements in DWM from the perspective of various predictive modalities.

\subsection{2D Scene Evolution}
Driving World Models (DWM) utilize advanced generative techniques (e.g., autoregressive transformer and diffusion) to predict photorealistic 2D scene evolution while ensuring physical plausibility. 

GAIA-1~\cite{hu2023gaia} pioneers the formulation of scene evolution prediction as a next-token prediction task and utilizes a diffusion decoder, systematically capturing spatiotemporal dynamics and high-level structures in driving scenarios. By contrast, DriveDreamer~\cite{wang2023drivedreamer} advances conditional diffusion frameworks for multi-modal control and extends the DWM paradigm to synthetic data generation. Building upon these foundational approaches, subsequent DWM research has predominantly focused on enhancing the fidelity, consistency, and controllability of scene evolution prediction.

The fidelity of generated scenes is crucial for ensuring the physically plausible simulation of real-world driving scenarios. Vista~\cite{gao2024vista} employs stable video diffusion and introduces novel loss functions to capture dynamic behaviors while preserving structural integrity, enabling high-resolution and high-fidelity scene generation. Moreover, it ensures rational dynamics by incorporating multiple reference frames.

The predicted scene evolution of DWM should accurately reflect the 3D world, ensuring that each object has a reasonable 3D position and spatial relationships while the entire scene exhibits spatial continuity. By factorizing the joint modeling to predict intermediate views conditioned on adjacent views, Drive-WM~\cite{wang2024driving} significantly improves consistency between views. Farther, DrivePhysica~\cite{yang2024physical} introduces 3D bounding box coordinate conditions to enhance the understanding of spatial relationships, reinforcing spatial consistency and improving the comprehension of occlusion structures. On the other hand, WoVoGen~\cite{lu2024wovogen} predicts explicit world volumes to guide the multi-view video generation, ensuring intra-world and inter-sensor consistency. Similarly, NeMo~\cite{huang2024neural} and GEM~\cite{hassan2024gem} incorporate 3D prediction tasks into their frameworks, while BEVWorld~\cite{zhang2024bevworld} extends these efforts by integrating multi-sensor data inputs.

Temporal consistency plays a pivotal role in enabling DWM to capture the evolving dynamics of scenes effectively. Ensuring coherent transitions between frames in predicted sequences is critical for reflecting realistic scene evolution. Recent advancements address this challenge through architectural innovations. For instance, InfinityDrive~\cite{guo2024infinitydrive} introduces a multi-resolution spatiotemporal modeling framework that progressively expands the temporal receptive field during training. This approach is further enhanced by integrating memory mechanisms, which are designed to preserve long-range temporal dependencies. In parallel, DrivingWorld~\cite{hu2024drivingworld} tackles temporal coherence in autoregressive methods by proposing a suite of techniques, including temporal-aware tokenization, next-state prediction, random token dropout, and balanced attention strategies. Together, these innovations significantly improve the ability to model temporal consistency in dynamic scene understanding.

A sensible DWM would adapt to specific requirements when generating future scenarios. As shown in Tab.~\ref{tab:overview}, control conditions broadly fall into two categories: 1) Low-level conditions, such as action (ego-vehicle information), trajectory (coordinates or displacement), and layouts (objects placements), set exact specifications for ego-vehicle movement and object positioning. 2) High-level conditions, like text (command or description) and destination (the vehicle's arrival location), require not only the fulfillment of specified goals but also the creation of logical intermediate steps. Some works effectively integrate low-level or high-level control conditions to generate future scenarios, yielding reasonable output responses such as driving actions, future trajectories, and text. Among these, GEM~\cite{hassan2024gem} excels in balancing both condition types, producing driving videos that strictly follow the specified trajectory while enabling natural movement toward the desired location. Conversely, DriveDreamer-2~\cite{zhao2024drivedreamer} utilizes a large language model to derive varied low-level conditions from high-level text inputs, significantly enhancing the diversity of generated images and providing a user-friendly generation process. Note that control conditions and output responses are also widely utilized in 3D (Sec.~\ref{sec:3d_scene}) and scene-free (Sec.~\ref{sec:scene_free}) paradigms.

In conclusion, DWM employs generative techniques to synthesize realistic 2D driving scenes with spatiotemporal consistency and physical plausibility. These frameworks effectively advance photorealism, controllability, and stability, enabling reliable predictions of scene evolution.

\subsection{3D Scene Evolution}
\label{sec:3d_scene}
3D data representations inherently preserve structural consistency, detailed geometric information, and precise spatial relationships making the prediction of 3D scene evolution a vital task for Driving World Models (DWM). In practice, two primary types of 3D data are commonly utilized, i.e., occupancy and point cloud. 

\textbf{Occupancy Scene Evolution.} Occupancy provides geometrically consistent volumetric modeling and structured spatial encoding, which make occupancy particularly well-suited for modeling scene evolution and widely adopted in various approaches. 

OccWorld~\cite{zheng2024occworld} uses a spatial-temporal transformer to generate future scene and ego pose tokens from historical observations and enables globally consistent scene predictions through spatial mixing. Subsequently, OccLLaMA~\cite{wei2024occllama} integrates a multi-modal large language model as the core architecture, while RenderWorld~\cite{yan2024renderworld} separately tokenizes air grids and non-air grids for fine-grained 3D scene modeling. Diffusion-based approaches further improve controllability and generation quality. OccSora~\cite{wang2024occsora} provides to predict 4D occupancy scene evolution given arbitrary trajectories. DOME~\cite{gu2024dome} employs a continuous VAE-like tokenizer to preserve intricate spatial information. Noticing the high computational demands of occupancy, recent methods have attempted to improve efficiency. DFIT-OccWorld~\cite{zhang2024efficient} predicts only the dynamic voxel flow while calculating static voxels through pose transformation. Similarly, GaussianWorld~\cite{zuo2024gaussianworld} explicitly models scene evolution in Gaussian space, focusing on the changes rather than reconstructing the entire scene.

Since occupancy cannot be directly obtained from sensors, reconstructing occupancy from images is crucial. DWM extends this 3D prediction task to 4D, i.e., spatiotemporal prediction.
A straightforward approach is to integrate an Img2Occ module, which may lead to the accumulation of errors. Recent advancements, however, aim to directly infer the spatiotemporal evolution of the 3D world from 2D inputs, enabling the synergistic learning of semantics, 3D structures, and temporal dynamics. DriveWorld~\cite{min2024driveworld} propagates static spatial context while predicting dynamic temporal changes in the scene. By learning spatiotemporal representations from multi-view videos, it achieves precise occupancy prediction. Additionally, Drive-OccWorld~\cite{yang2024driving} combines a planner with a DWM, utilizing motion-aware BEV sequences as an intermediary to predict occupancy and flow from multi-view images directly. This integration provides the planner with rich priors, thereby enhancing the safety and accuracy of the planning process.

In addition to reconstructing occupancy from images, some methods derive occupancy pseudo-labels from point clouds, enabling self-supervised training. For instance, UnO~\cite{agro2024uno} produces continuous occupancy fields by sampling positive and negative examples from future LiDAR scans, enabling self-supervised learning on point clouds. Similarly, UniWorld~\cite{min2023uniworld} and NeMo~\cite{huang2024neural} generate occupancy pseudo-labels by voxelizing point clouds. UniWorld fuses multi-frame point clouds for pseudo-labels and learns spatiotemporal dynamics, while NeMo further integrates image prediction and motion flow module to enhance volumetric representations, boosting planning performance.

\textbf{Point Cloud Scene Evolution.} Point clouds, typically captured by LiDAR sensors, offer precise geometric representations of 3D environments. However, their sparse and unstructured nature poses significant challenges for generation tasks, complicating effectively utilizing them for scene modeling and prediction. Copilot4D~\cite{zhang2023learning} employs a VQ-VAE tokenizer to address the complex observation and adopts a parallelized inference acceleration approach by leveraging a modified discrete diffusion. Moreover, LidarDM~\cite{zyrianov2024lidardm} provides layout-aware point cloud video generation by combining static scenes and moving objects.

Rather than a direct point cloud input, visual point cloud forecasting seeks to predict future point cloud evolution using only historical visual images. ViDAR~\cite{yang2024visual} proposes visual point cloud forecasting as a scalable pre-training task and explores the integration of semantics, 3D structures, and temporal dynamics. Recently, HERMES~\cite{zhou2025hermes} has emerged, combining visual point cloud prediction with language tasks to enhance generation and scene understanding. It significantly outperforms ViDAR despite the latter's use of a longer history horizon and an advanced latent rendering module.

The fusion of multi-sensor data has become a key trend in the advancement of autonomous driving systems, as it enables the integration of high-resolution details from 2D data with precise spatial geometry from 3D data. MUVO~\cite{bogdoll2023muvo} combines multi-modal data into a sensor-agnostic geometric representation, enabling accurate scene modeling and evolution prediction through images, occupancy, and point clouds. Similarly, BEVWorld~\cite{zhang2024bevworld} merges images and point clouds into a unified BEV representation and predicts future representations through diffusion, subsequently reconstructing multi-sensor data using a rendering-based approach, enabling self-supervised learning. In contrast, HoloDrive~\cite{wu2024holodrive} employs two separate models and aligns them to jointly generate multi-camera data and LiDAR data, ensuring consistency between 2D and 3D spaces.

Overall, DWM extends generative techniques to 3D scene evolution, leveraging occupancy and structured representations to ensure spatial consistency while integrating generative techniques for dynamic prediction. They enhance scene understanding through multi-sensor fusion, enabling more accurate and robust predictions of complex scene evolution.

\subsection{Scene-free Paradigms}
\label{sec:scene_free}
In addition to the commonly used image, point cloud, and occupancy prediction, some approaches have also explored prediction without detailed scenes. For instance, real-time autonomous driving systems prioritize latent world state transitions, while behavioral simulation frameworks emphasize agent-centric motion dynamics. We systematically categorize these as scene-free paradigms. 

\textbf{Latent State.} Compared to raw sensory data, the latent state provides an efficient representation that seamlessly integrates into decision-making and enhances generalization across diverse driving environments. Reinforcement Learning-based planners~\cite{li2024think2drive,zeng2024world,popov2024mitigating,wang2025adawm} often leverage latent DWM, which provide accurate and dense rewards, efficient parallel training, and interpretable outputs. Supervised learning also benefits from predicting latent states. For instance, LatentDriver~\cite{xiao2024learning} models the predicted latent states and possible actions as a mixture distribution, capturing the stochastic nature of decision-making, while LAW~\cite{li2024enhancing} leverage self-supervised latent feature to enhance end-to-end driving as well as improve efficiency.

\textbf{Multi-agent Behavior.} Multi-agent behavior prediction focuses on forecasting the motions of all agents within the scene.
TrafficBots~\cite{zhang2023trafficbots} explores the behavior realism of bot agents. Conditioned on their corresponding destinations, each agent learns a unique personality and predicts actions from the BEV perspective. Similarly, CarFormer~\cite{hamdan2024carformer} models each object as a self-supervised slot representation, which implicitly contains the necessary information for driving. In contrast, AdaptiveDriver~\cite{vasudevan2024planning} predicts the distinctive behavioral patterns of surrounding agents and subsequently unrolls a corresponding DWM to simulate their behaviors.

Scene-free paradigms extend beyond 2D and 3D representations. Latent states improve efficiency and generalization, while multi-agent behavior-based models capture interactions to mitigate risks. Together, these approaches boost the versatility of autonomous driving systems.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{image/application.pdf}
        \vspace{-5pt}
    \caption{The various applications of DWM. (a) respond to various types of instructions and faithfully simulate the corresponding scenarios. (b) generate diverse data with the same annotation. (c) optimize planning by predicting future scenes. (d) enhance downstream task performance and reduce reliance on annotations through 4D pre-training. }
    \label{fig:application}
\end{figure*}

\section{Application}
\label{sec:3}
The Driving World Model (DWM) is a self-supervised, data-driven approach trained to forecast the evolutions of future scenarios, with some methods requiring minimal annotations. The extensive study of generative tasks enables DWM to function as a simulator and generate data. Additionally, DWM can enhance the capabilities of autonomous driving systems by directly contributing to the decision-making process and training pipeline. In this section, we summarize the applications of DWM, highlighting how World Models contribute to advancing autonomous driving.

\subsection{Simulation} 
Simulation has been a key tool for training and evaluating driving models since the early stages of autonomous driving research, having achieved significant success~\cite{dosovitskiy2017carla}. However, traditional simulators face several challenges, including limited diversity and the gap between the simulated and real-world domains. DWM, with its data-driven generative simulation capabilities, holds the potential to address these challenges. As shown in Fig.~\ref{fig:application}(a), DWM can simulate the driving process conditioned on various input forms, faithfully adhering to the provided instructions. These conditions include but are not limited to actions and captions, making the simulator more user-friendly while granting it greater autonomy to generate refined structures and diverse scene details. 

DWMs have achieved notable progress in fidelity, consistency, and controllability, which are crucial for reliable real-world simulation. Vista~\cite{gao2024vista} offers high-fidelity and highly controllable video simulations, enabling action evaluation. In a related development, GEM~\cite{hassan2024gem} further improves simulation quality and enables precise control over object dynamics, trajectory, and human poses. Similarly, several studies have attempted to simulate driving scene evolution in 3D space~\cite{zyrianov2024lidardm,wang2024occsora,gu2024dome}, showing promising results. Beyond visual realism, recent work has emphasized action fidelity evaluation, illustrated by a newly proposed evaluation framework, ACT-Bench~\cite{arai2024act}, which introduces a robust baseline framework demonstrating strong alignment with action instructions. Complementing these advancements, TrafficBots~\cite{zhang2023trafficbots} investigates the behavior realism of bot agents, further enriching the simulation landscape.

Reinforcement learning-based approaches particularly benefit from the flexibility of DWM-based simulation. By rollouting in the latent state space, Think2Drive~\cite{li2024think2drive} enables efficient parallel training without interacting with heavy physical simulators. Subsequently, Imagine-2-Drive~\cite{garg2024imagine} leverages a high-fidelity DWM to simulate and evaluate multi-step trajectories, significantly improving planning performance.

Combining high-fidelity generative capabilities with faithful controllability, DWMs not only bridge the gap between simulated and real-world domains while enhancing the diversity of simulated scenarios but also enable parallel interaction. These advancements significantly advance simulation in autonomous driving.

\subsection{Data Generation} 
Compared to simulation, which prioritizes the faithful reproduction of conditions, data generation emphasizes data diversity and fidelity, aiming to achieve broader and more comprehensive scenario coverage while mitigating the gap with real-world data. Fig.~\ref{fig:application}(b) demonstrates that DWM can generate diverse driving videos by using the same annotations, which significantly improves the diversity of data annotations.

DWMs have proven effective in augmenting datasets with synthesized data~\cite{wang2023drivedreamer,zhao2024drivedreamer,zyrianov2024lidardm,yang2024physical}. For example, DrivePhysica can synthesize unlimited high-fidelity and diverse driving videos, while LidarDM generates high-quality LiDAR data. Notably, HoloDrive~\cite{wu2024holodrive} demonstrates the potential for synthesizing aligned multi-modal data. Such synthetic data boosts downstream tasks (e.g., 3D detection), demonstrating the controllability and high generation quality of DWMs. While these methods often rely on real-world annotations to synthesize data, recent advancements~\cite{zhao2024drivedreamer4d,ni2024recondreamer} also synthesize driving videos with novel trajectories, mitigating distribution imbalances in real-world datasets.

Given the current limitations in the scale of high-quality autonomous driving datasets, DWM reveals the potential of leveraging synthetic techniques to obtain abundant high-quality driving data, supporting the advancement of autonomous driving research.

\subsection{Anticipative Driving}
Anticipative driving emphasizes enhancing a vehicle's planning capabilities through accurate predictions of future states. By predicting the behavior of surrounding agents and environmental dynamics, ego proactively explores the outcomes of different actions, resulting in improved safety and adaptability in complex driving scenarios.

As shown in Fig.~\ref{fig:application}(c), a typical anticipative driving approach involves leveraging DWM to predict the outcomes of multiple potential actions. These predictions are then evaluated to optimize the final action. For example, Drive-WM~\cite{wang2024driving} integrates DWM predictions with reward functions to select optimal trajectories, while Drive-OccWorld introduces BEV embedding to the planner for further refining. In parallel, ADriver-I~\cite{jia2023adriver} combines a multi-modal large language model and a video diffusion model for jointly predicting vision-action (image and text describing ego-vehicle information) pairs, showing the potential for long-horizon planning. Some approaches further integrate prediction and planning into a unified model, achieving notable performance while offering greater flexibility~\cite{wei2024occllama,chen2024drivinggpt,hu2024drivingworld}.

Given that accurate prediction enhances driving performance, on the other hand, constraining predictions to align with future observations has also proven effective~\cite{li2024enhancing,popov2024mitigating}. Further, AdaWM~\cite{wang2025adawm} uses the discrepancy between predicted and future states to guide fine-tuning, improving adaptability to unfamiliar environments.

The success of anticipative driving indicates that DWM can not only indirectly enhance autonomous driving through simulation and data generation but also actively participate in the decision-making and optimizing processes, enabling the combined effect of prediction and planning.

\subsection{4D Pre-training}
Pre-training has been empirically validated across multiple methodologies. However, traditional pre-training approaches often neglect the 4D dynamic, which is critical for autonomous driving. The target of DWM to predict scene evolution is inherently well-suited for self-supervised learning and large-scale pre-training. As shown in Fig.~\ref{fig:application}(d), DWM leverages large amounts of unlabeled multi-modal data for 4D pre-training, enhancing the performance of a wide range of downstream driving tasks.

Most existing tasks emphasize vision-centric pre-training, aiming to capture spatial and dynamic perception by learning 4D scene evolution from multi-view image data. To leverage large-scale unlabeled image-point cloud pairs, UniWorld~\cite{min2023uniworld} generates occupancy pseudo-labels from point clouds for 4D pre-training. To eliminate the dependency on pseudo-labels, ViDAR~\cite{yang2024visual} introduces a novel approach based on visual point cloud forecasting. By proposing a latent rendering operator, the discriminative 3D geometry of learned BEV representations is preserved, ensuring seamless integration with downstream tasks. Furthermore, NeMo~\cite{huang2024neural} combines RGB reconstruction and occupancy prediction to learn 4D volumetric representations that maintain 3D geometry and semantic information jointly. To capture spatio-temporal dynamics, DriveWorld~\cite{min2024driveworld} separately addresses temporal dynamics and static scenes. Besides, a task prompt is adopted to adapt to various downstream tasks.

Recent approaches have also explored point cloud-centric pre-training. UnO~\cite{agro2024uno} learns geometric structures, dynamics, and semantics by predicting continuous 4D occupancy fields, the novel pseudo-labels produced from future point clouds. In contrast, AD-L-JEPA~\cite{zhu2025ad} performance pre-training via reconstructing masked embedding, enabling simpler implementation and enhanced learned representations. Finally, BEVWorld~\cite{zhang2024bevworld} explores pre-training on multi-sensor data, encoding images and point clouds to a unified BEV representation.

Collectively, DWM typically conducts 4D pre-training via self-supervised learning, capturing spatiotemporal dynamics, reducing reliance on annotations, and delivering performance improvements across a wide range of tasks, thereby paving the way for advancements in autonomous driving.

\section{Evaluation}
Given the absence of standardized benchmarks for Driving World Models (DWM), it is essential to develop a comprehensive understanding of existing resources and evaluation methods to drive progress in this field. In this section, we summarize the primary datasets for autonomous driving, especially those proposed for DWM, and discuss the metrics designed for various benchmarks.
\subsection{Datasets}
\begin{table}[t]
    \scriptsize
    \centering
    \caption{Overview of high-impact and promising datasets.}
        \vspace{-7pt}
    \setlength{\tabcolsep}{0.8mm}
    \begin{tabular}{ccccccc}
    
    \toprule %[2pt] 
    \multirow{2.3}{*}{Dataset} & \multicolumn{3}{c}{Modality} & \multirow{2.3}{*}{\# Cities} & \multirow{2.3}{*}{\# Scenes}& \multirow{2.3}{*}{Duration (h)} \\
    % \vspace{0.5mm}
     \cmidrule(lr){2-4}
 & 2D & 3D & Text & & & \\
    \midrule
    CARLA~\cite{dosovitskiy2017carla} &\ding{51} &\ding{51} &\ding{55} & - & - & - \\
    \midrule
    KITTI~\cite{Geiger2013IJRR} & \ding{51} & \ding{51} & \ding{55} & 1 & 22 & 1.5 \\
    KITTI-360~\cite{Liao2022PAMI} & \ding{51} & \ding{51} & \ding{55} & 1 & 366 & 2.5 \\
    Waymo Open~\cite{sun2020scalability} & \ding{51} & \ding{51} & \ding{55} & 3 & 1k & 6.4 \\
    nuScenes~\cite{caesar2020nuscenes} & \ding{51} & \ding{51} & \ding{55} & 2 & 1k & 5.5 \\
    Arg2 Sensor ~\cite{Argoverse2} & \ding{51} & \ding{51} & \ding{55} & 6 & 1k & 4 \\
    Arg2 LiDAR~\cite{Argoverse2} & \ding{55} & \ding{51} & \ding{55} & 6 & 20k & 167 \\
    OpenScenes~\cite{openscene2023} & \ding{51} & \ding{51} & \ding{55} & 4 & - & 120 \\
    OpenDV-2K~\cite{yang2024generalized} & \ding{51} & \ding{55} & \ding{51} & $\geq$244 & - & 2,059 \\
    DrivingDojo~\cite{wang2024drivingdojo} & \ding{51} & \ding{55} & \ding{51} & 9 & 18k & 150 \\
\bottomrule %[2pt]   
    \end{tabular}
    \label{tab:dataset}
\end{table}

The advancement of autonomous driving heavily relies on high-quality datasets encompassing diverse and comprehensive scenarios. In Tab.~\ref{tab:dataset}, we review the most high-impact datasets in autonomous driving research, emphasizing their scale and diversity. Notably, DrivingDojo~\cite{wang2024drivingdojo} is tailor-made for training Driving World Models with complex driving dynamics. 

\subsection{Metrics}
\begin{table*}[!ht]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{5.0mm}
    \caption{Commonly used metric. $\uparrow$ indicates better performance with higher values, while $\downarrow$ indicates better performance with lower values.}
    \vspace{-7pt}
    \begin{tabular}{lcl}
    
    \toprule %[2pt] 
    Metric & Target & Description \\
    \midrule
    Fréchet Inception Distance (FID) $\downarrow$ & Image &
     Feature distribution distance between the predicted and GT images. \\ 
    Fréchet Video Distance (FVD) $\downarrow$ & Video &
     Feature distribution distance between the predicted and GT videos.\\
    Chamfer Distance (CD) $\downarrow$ & Point Cloud & Average bidirectional nearest neighbor distance between the predicted and GT point clouds.\\
    Intersection over Union (IoU) $\uparrow$ & Occupancy & Ratio of the intersection and union between the predicted and GT occupancy.\\
    Mean IoU (mIoU) $\uparrow$ & Occupancy &Average of IoU values across all categories.\\
    Displacement Error (L2) $\downarrow$ & Open-loop Planning & 
    L2 distance between the predicted and GT trajectory.\\
    Collision Rate (Col.) $\downarrow$ & Open-loop Planning & 
    Rate of collisions that occur with other objects when following the predicted trajectory.\\
    Route Completion (RC) $\uparrow$ & Closed-loop Planning & 
    Percentage of the route completed by ego. \\
    Infraction Score (IS) $\uparrow$ & Closed-loop Planning & 
    Initialized at 1.0 and decreases by a penalty coefficient for each Infraction.\\
    Driving Score (DS) $\uparrow$ & Closed-loop Planning &
    Product of RC and IS.\\
    \bottomrule   

    \end{tabular}
    \label{tab:metric}
\end{table*}

In the context of autonomous driving, Driving World Models (DWM) are primarily trained using video-generation-based methods and applied to various driving-related tasks. The diversity of these tasks makes it challenging for a single metric to assess model performance across all studies comprehensively. As a result, researchers in this field select specialized metrics based on the specific tasks and domains being addressed. Tab. \ref{tab:metric} presents an overview of widely used metrics and their corresponding meanings.

These metrics provide an in-depth evaluation of diverse DWM, guiding further research. However, certain aspects, such as consistency and controllability, remain underexplored. To address these limitations, some studies propose novel metrics. For instance, to assess controllability, a widely adopted approach involves comparing the outputs of a trained detector on generated data either with the corresponding conditions~\cite{zhao2024drivedreamer4d,ni2024recondreamer} or with the prediction of the same detector on the ground truth data~\cite{wang2024driving,hassan2024gem,yang2024physical}. Besides, Key Points Matching (KPM)~\cite{wang2024driving,lu2024wovogen} and average point-to-plane energy~\cite{zyrianov2024lidardm} are introduced to evaluate the consistency of multi-camera videos and LiDAR videos, respectively. 

\section{Limitation and Future Work}
While the research on the Driving World Model (DWM) has advanced significantly, several limitations persist and may hinder its full potential. Furthermore, tailoring DWMs for diverse autonomous driving applications is an ongoing challenge. In this section, we provide an in-depth discussion of the current limitations and outline potential directions for future research and development.

\textbf{Scarce Data.}
% \paragraph{Scarce Data.}
Collecting driving data is costly, especially for long-tail yet safety-critical scenarios, resulting in limited and unevenly distributed datasets. Recent works~\cite{wang2024drivingdojo,yang2024generalized} have progressed scaled and diverse driving video datasets. However, obtaining high-quality 3D data and aligned multi-sensor datasets remains challenging. While DWMs have shown promise in synthesizing data to improve downstream tasks, augmenting data for improving DWM is an open challenge.

\textbf{Efficiency.} 
Generative tasks challenge the inference efficiency of DWM, increasing computational costs and latency, which hinder real-time driving applications. Detailed 4D scene representations further amplify these demands on computation and memory. Recent studies have highlighted the potential of decoupling scenarios as an effective strategy~\cite{zhang2024efficient}. Additionally, exploring more efficient representations is a practical research direction.

\textbf{Reliable Simulation.}
A critical issue is how to ensure the performance of DWMs does not significantly degrade with complex simulations (e.g., long-horizon rollouts and drastic view shifts) and variable driving situations (e.g., diverse traffic and weather). This poses significant challenges to the robustness and generalization capabilities. To address these issues, several studies have proposed partial solutions. For instance, DrivingDojo~\cite{wang2024drivingdojo} provides diverse driving video datasets, AdaptiveDrive~\cite{vasudevan2024planning} develops DWMs adapting to different environments, and InfinityDrive~\cite{guo2024infinitydrive} focus on improving long-term performance. Despite these advancements, this remains a highly challenging and impactful research area that warrants further exploration and innovation in the future.

Another challenge is hallucinations and unrealistic physics (e.g., sudden vehicle appearances and incorrect speed estimations), resulting in dangerous decision-making even under normal situations. DrivePhysica~\cite{yang2024physical} addresses this issue by introducing additional conditions, while multi-modal scene outputs with cross-modal validation present another viable solution.

\textbf{Unified Task.}
Existing DWMs predominantly support predictive tasks, implicitly understanding scenes by predicting scene evolutions rather than through explicit supervision of this critical capability. Incorporating language tasks provides a promising solution to address these limitations. For example, tasks such as image captioning and question answering (QA) promote comprehensive understanding, while causal reasoning enables learning underlying principles governing real-world evolution. Moreover, the seamless integration of predicting and planning, i.e., end-to-end DWM, can fully leverage the potential of frameworks.

\textbf{Multi-sensor Modeling.} Autonomous driving systems predominantly rely on multi-sensor setups, where data from different modalities is complementary. Single-modal-scene-based DWMs are, therefore, less suitable for such systems.  While some studies have made progress in multi-modal data integration~\cite{bogdoll2023muvo,zhang2024bevworld,wu2024holodrive}, this area still holds significant potential for further exploration. Further, given the expense of aligned multi-sensor data, leveraging widely available unaligned and even unpaired multi-sensor data presents a valuable research direction.

\textbf{Attack and Defense.}
Adversarial attacks can easily lead to highly severe accidents, representing a significant threat to driving safety. These attacks involve carefully crafted adversarial patches imperceptible to humans, making them especially difficult to detect and mitigate. Despite their potential impact, there is currently a lack of research specifically addressing adversarial attacks on DWM. Therefore, Investigating such attacks and developing effective defense strategies are of critical practical importance. Such efforts are essential for facilitating the safe and reliable deployment of DWMs in real-world autonomous driving applications.

\section{Conclusion}
Driving World Model (DWM) has been increasingly recognized as a fundamental component in the architecture of autonomous driving systems, aiming to improve decision-making through future evolution prediction. In this paper, we examine DWM's specific contributions, not only beginning with a systematic overview categorized by predicted scene modality but also summarizing DWM applications and their impact on autonomous driving, followed by a review of common datasets and metrics. We further delve into current limitations and point out some promising future research directions to overcome these challenges and advance field exploration in the future. We believe that this survey will provide a rapid overview of key advancements in the field of DWM for early-stage researchers.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}