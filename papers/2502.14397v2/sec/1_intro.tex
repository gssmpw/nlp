


% NEW
% intro逻辑这样：
% image generation发展的非常好，基于pretrain-finetuning路线，业界提出来了很多cusomized image generation方案cite{}，用来解决id学习、风格化、主体一致\cite{}等等任务。但是，while cusomized image generation thrives, a critical gap persistes in customized image editing. 因此，customized image editing任务的价值并没有被充分挖掘出来。

% PhotoDoodle，是一个备受关注，但之前又难以实现的customized image editing任务。艺术家通过在照片上添加装饰性元素与风格化修饰，使背景照片呈现更具个性化或艺术化的效果。艺术家在进行照片涂鸦创作时，常使用局部风格化、装饰性线条、添加新物体以及对现有物体进行装饰。同时，不同艺术家和设计师在编辑图像或创建照片涂鸦时，都体现出各自明显的个人风格。艺术家们也会希望将自己的创作应用至更多的背景照片中，但当前缺乏自动化工具，使得一张照片的生产需要小时级别的时间成本，一方面限制了艺术编辑的使用场景，另一方面限制了大规模生产配对数据的可能性。

% 自动化照片涂鸦的生产流程，主要面临三大挑战：% 1. 生成装饰与背景的和谐统一：不仅位置与内容要合理，还需允许非真实感的呈现卡通或扁平插画风格。% 2. 避免不想要的变化：保证背景的严格一致性。% 3. 从少量艺术家编辑前后图像对中，高效学习其独特的编辑风格与技巧。现有方案在处理这些挑战时，均遇到了阻碍。

% 现有的图像编辑方法大体可分为全局编辑和inpainting：前者常用于整体风格化，但一旦试图修改特定区域时，往往会连带改变原背景， prompt2prompt、Instruct P2P等实现了高一致性的图像编辑，可在一定程度上维持编辑前后的一致性；inpainting类的方法在mask内部进行区域去噪， 可以保证非mask区域不变，但要求用户生产一个合理的mask，这对用户的交互要求过高，不适合照片涂鸦这样需要灵活添加装饰的场景。因此， 我们希望通过建立一个灵活的，instruction引导的而非mask引导的方式实现照片涂鸦。

% 针对这些挑战以及现有方案的缺陷，我们提出了PhotoDoodle方法，用于learning artistic image editing from few shot examples. 模型基于预训练的DiT模型，以背景作为条件生成照片涂鸦，具体采用两阶段训练策略：
% 1. 首先将一个预训练的文生图模型改造成通用图像编辑模型，并在海量图像编辑数据集上习得基础编辑能力。
% 2. 随后，在第一阶段的模型上，用艺术家编辑前后图像的配对数据来训练Edit LoRA，从而捕捉艺术家的编辑技巧与风格。为减少不必要的改动并实现严格一致性，我们还提出了PE复用策略用于隐式对齐。
% 值得强调的是，我们有几个关键洞察：
% * 一是Clean Latent非常重要，即在条件图像部分无需添加噪声，以避免与原图产生额外差异，从而提升生成结果与条件图像的一致性。
% * 二是隐式对齐，条件信息能更好地整合背景信息，同时无需额外增加训练参数，使得模型充分保留预训练T2I model的能力。 

% 综上所述，本文贡献如下：
% 1. 我们提出了一个客制化图像编辑框架，通过少量艺术家“照片涂鸦”图像对，学习与迁移其编辑风格与技巧；
% 2. 借助条件机制扩展与PE复用策略实现隐式对齐，基于DiT实现了高质量且高度一致的图像编辑，并利用EditLoRA高效地学习客制化编辑；
% 3. 我们发布了首个包含6种高质量风格、逾300幅艺术家创作的照片-涂鸦对数据集。大量实验与评估表明，该方法在定制化图像编辑任务上具有显著的性能优势与实用价值。

\section{Introduction}

The rise of diffusion models has started a new chapter for image creation and control. Using the pretrain-finetune approach, the community has achieved remarkable progress in customized image generation \cite{ruiz2023dreambooth, TI, lora}, with applications spanning identity preservation \cite{facechain}, artistic stylization \cite{song2024processpainter, zhang2023inversion,sohn2023styledrop,ahn2024dreamstyler}, and subject coherence \cite{customdiff,ruiz2023dreambooth, TI,lora,ruiz2024hyperdreambooth,jiang2024videobooth,zhu2024multibooth}. These advancements have fueled applications ranging from digital art creation to commercial design.

%However, while customized image generation thrives, a critical gap persists in customized image editing. This problem contrasts sharply with the growing need for precise image editing tools, as current methods mainly focus on creating new content rather than making smart edits based on context. This less explored area of custom image editing presents both technical challenges and chances to create new important applications.
Despite these successes, there remains a critical gap between customized image generation and image editing. Current methods primarily focus on content creation, while intelligent context-aware editing, particularly for artistic enhancement, remains underexplored. This imbalance stands in contrast to the growing demand for precision image editing tools, with current approaches focusing on content generation rather than context-aware editing. As a result, this underexplored frontier of customized image editing is both a technical challenge and a key to unlocking transformative applications.

% 扩散模型的崛起为图像合成与操控开辟了全新纪元。基于预训练-微调框架，研究界在定制化图像生成领域取得了显著进展 \cite{}，其应用涵盖身份特征保留、艺术风格化、多主体一致性等核心任务 \cite{}。尽管定制化图像生成蓬勃发展，定制化图像编辑领域仍存在关键性空白。 这种失衡现象与日益增长的精准化图像修改工具需求形成鲜明对比——当前方法主要聚焦于内容生成，而非上下文感知的精细化编辑。因此，定制化图像编辑这一未充分探索的前沿领域，既是技术挑战，也是开启变革性应用的关键机遇。


%PhotoDoodling, a long-standing challenge in customized image editing, is one of the applications. This task involves artists in improving background photographs through the strategic integration of decorative elements and stylized modifications to achieve personalized or artistic effects. Typical techniques include localized stylization, decorative linework, novel object insertion, and ornamental augmentation of existing subjects. Artists and designers exhibit distinctive personal styles and strategic approaches during the photo doodling creation process, which holds significant value. However, the absence of automated tools forces manual workflows requiring hour-level time investments per image, severely constraining both productivity and the feasibility of generating large-scale paired training data.
%PhotoDoodling is one of the paradigmatic challenges in customized image editing. This task requires artists to enhance background photographs through strategic integration of decorative elements (e.g., stylized linework, ornamental patterns) and context-aware modifications to achieve personalized aesthetics. Conventional workflows involve artistic techniques, such as: (1) local stylization, (2) decorative contour rendering, (3) semantic-aware object insertion, and (4) ornamental augmentation. While these artistic processes demonstrate distinctive personal styles and strategic design thinking, the current reliance on manual execution imposes prohibitive time costs, severely limiting both production scalability and the acquisition of large-scale paired training data essential for machine learning approaches.

% PhotoDoodle代表了一个长期公认但历史上难以捉摸的定制图像编辑挑战。这项任务涉及艺术家通过战略性地整合装饰元素和风格化修改来增强背景照片，以实现个性化或艺术效果。典型技术包括局部风格化、装饰线条、新颖对象插入和对现有主题的装饰增强。艺术家和设计师在照片涂鸦创作过程中展现出独特的个人风格和策略，具有重要价值。然而，自动化工具的缺乏迫使手工工作流程，每张图像需要投入数小时，严重限制了实际应用和生成大规模配对训练数据的可行性。

%Automating PhotoDoodle workflows is intriguing, yet it faces three interconnected challenges. First, achieving harmonious integration demands that generated decorations align not only spatially (e.g., matching perspective) but also semantically with the background, while accommodating non-photorealistic styles such as cartoon or flat illustration. Second, maintaining strict background consistency requires rigorously preventing unintended alterations to the original content, including color shifts or texture distortions. Third, to achieve efficient style acquisition, it is necessary to extract an artist's unique editing patterns from a few photo-doodle pairs, which is technically challenging. These compounded challenges cause existing methods to be incompetent in addressing the problem comprehensively.
PhotoDoodling, as a paradigmatic challenge in customized image editing, demands artists to enhance background photographs through strategic integration of decorative elements (e.g., stylized linework, ornamental patterns) and context-aware modifications for personalized aesthetics. Conventional workflows involve artistic techniques, such as: (1) local stylization, (2) decorative contour rendering, (3) semantic-aware object insertion, and (4) ornamental augmentation. While these processes showcase distinctive artistic signatures and strategic design logic, their manual execution incurs prohibitive time costs, fundamentally constraining both production scalability and the curation of large-scale paired training datasets required for data-driven approaches. However, automating these workflows introduces three interlocked technical barriers: First, harmonious integration demands generated decorations to simultaneously satisfy perspective alignment and semantic coherence with background contexts. Second, strict background preservation requires mechanisms to prevent unintended changes, such as color distribution shifting and texture pattern alternation. Third, efficient style distillation must extract artists' unique editing patterns from sparse pairwise examples (30-50 image pairs). These compounded challenges cause existing methods to be incompetent in addressing the problem in a comprehensive way.
% 自动化照片涂鸦工作流程面临三个相互关联的挑战。首先，实现和谐融合要求生成的装饰不仅在空间上（如透视关系）与背景对齐，还需在语义层面保持一致，同时支持卡通或扁平插画等非写实风格。其次，保持严格的背景一致性需要严格防止对原始内容的非预期修改，包括色彩偏移或纹理失真。第三，实现高效风格学习需要从稀疏的编辑前后图像对中提取艺术家的独特编辑模式——这一任务在当前方法中仍具有技术瓶颈。这些复合挑战使得现有方法在整体解决问题时显得支离破碎。

%The limitations of prevailing image editing paradigms further exacerbate these difficulties. Global editing methods (e.g., Prompt-to-Prompt\cite{p2p}, InstructP2P\cite{brooks2023instructpix2pix}), while effective for consistent style transfer, inadvertently distort background content during localized modifications. Inpainting-based approaches, though capable of preserving non-mask regions through localized denoising, impose impractical demands for pixel-perfect user-defined masks, fundamentally conflicting with PhotoDoodle’s need for flexible creative workflows. To bridge this gap, we introduce an instruction-guided framework that replaces mask dependency with semantic-level control, enabling precise and style-conscious decorative generation while ensuring background consistency.
Prevailing image editing paradigms can hardly deal with these challenges altogether. Global editing methods (e.g., Prompt-to-Prompt\cite{p2p}, InstructP2P\cite{brooks2023instructpix2pix}), while effective for consistent style transfer, inadvertently distort background content during localized modifications. Inpainting-based approaches\cite{zhang2024magicbrush,stylizedneuralpainting}, though capable of preserving unmasked regions through localized editing, impose impractical demands for pixel-perfect user-defined masks, fundamentally conflicting with the need for automatic PhotoDoodling. To bridge this gap, we introduce an instruction-guided image editing framework that discarded mask dependency, enabling precise and style-conscious decorative generation while ensuring background consistency.

% 主流图像编辑范式的局限性进一步加剧了这些困难。全局编辑方法（如Prompt-to-Prompt、InstructP2P）虽然在风格迁移上表现一致，但在局部修改时会无意中扭曲背景内容——这是一种“连锁反应”副作用。基于修复的方法尽管能够通过局部去噪保留非遮罩区域，却对用户定义的遮罩提出了不切实际的高精度要求，这与照片涂鸦所需的灵活创作流程从根本上相冲突。为了弥合这一差距，我们提出了一种指令引导框架，用语义级控制取代遮罩依赖，在确保背景一致性的同时，实现精确且风格感知的装饰生成。

%To address these challenges and bridge the gaps in existing solutions, we propose PhotoDoodle, a framework designed to learn artistic image editing from few-shot examples. Building upon a pre-trained Diffusion Transformer (DiT), our method conditions on background images to generate photo doodles through a two-stage training pipeline. First, we adapt a well-trained text-to-image model into a versatile image editor by fine-tuning it on large-scale editing datasets, thereby establishing the foundational editing model OmniEditor. Then, we train an EditLoRA module using artist-curated before-after pairwise data to learn nuanced editing styles, while introducing a Positional Encoding (PE) Cloning strategy to enforce strict background consistency through implicit feature alignment.
%Our solution, PhotoDoodle, introduces a few-shot artistic image editing framework built upon Diffusion Transformers (DiT). The architecture employs a two-phase training strategy: First, we transform a pre-trained text-to-image DiT model into a universal image editor (OmniEditor) by introducing Positional Encoding (PE) Cloning mechanism and noise-free conditioning paradigm, establishing foundational editing capabilities by finetuning with large-scale image editing dataset. Subsequently, we develop an EditLoRA module that distills artist-specific styles from few-shot pairwise examples. This innovative design enforces implicit feature alignment between input and output images, achieving strict background consistency while permitting flexible decorative generation through learned style parameters.
Our proposed framework, PhotoDoodle, presents a few-shot artistic image editing framwork built upon Diffusion Transformers (DiT), featuring a dual-stage training architecture. In the first phase, we evolve a pre-trained text-to-image DiT model into a universal image editor (OmniEditor) through two key innovations: (1) a Positional Encoding (PE) Cloning mechanism that preserves spatial fidelity by providing coordinate-aware hints, and (2) a noise-free conditioning paradigm that offers non-distorted information of the source image. This foundational stage is trained on 3.5M image editing pairs\cite{ge2024seed}, establishing robust general editing capabilities. The second phase introduces an EditLoRA module that distills artist-specific editing patterns from merely 30-50 exemplar pairs through low-rank adaptation(LoRA), enabling efficient style customization while maintaining base model's capability. This co-designed architecture ensures a balance between artistic flexibility and strict consistency.

% 为解决上述挑战并弥补现有方案的不足，我们提出了PhotoDoodle框架，旨在从少量示例中学习艺术化图像编辑。该方法基于预训练的扩散Transformer（DiT），以背景图像为条件生成照片涂鸦，并通过两阶段训练流程实现：首先，我们将预训练的文本到图像模型转化为通用图像编辑器，通过在大规模编辑数据集上的微调，建立基础编辑能力；随后，利用艺术家提供的编辑前后图像对训练EditLoRA模块，以提炼细腻的编辑风格，同时引入位置编码克隆策略（PE Cloning），通过隐式特征对齐确保严格的背景一致性。

%Our design is guided by two main insights. First, maintaining clean latent conditioning—ensuring that input image tokens are noise-free—is essential for keeping the output coherent with the background by minimizing divergence in the latent space. Second, implicit alignment through position encoding cloning enables the seamless integration of background context without adding extra trainable parameters, thus preserving the full generative capacity of the base T2I model. These insights allow our framework to strike a balance between artistic flexibility and strict consistency.

% 我们的设计受到两个关键发现的驱动。其一，保持纯净潜空间条件（即输入图像不添加噪声）对维持生成结果与背景的一致性至关重要，这通过最小化潜在空间偏差来实现；其二，通过PE克隆的隐式对齐，我们能够在无需引入额外可训练参数的情况下，无缝整合背景信息，从而充分保留基础T2I模型的生成能力。这些洞察共同使我们的框架能够在艺术灵活性与严格一致性之间取得平衡。

In summary, our contributions are threefold:
\begin{itemize}
    %\item We propose a customizable image editing framework that learns and transfers artistic styles from few photo-doodle pairs.
    \item First framework for artistic photo-doodling: A DiT-based architecture to enable few-shot learning of style-specific editing operations while preserving background integrity.
    %\item By extending conditional mechanisms and implementing a PE cloning strategy for implicit alignment, we achieve high-quality and highly consistent image editing built upon Diffusion Transformers (DiTs), while utilizing EditLoRA to efficiently learn customized editing operations.
    \item We propose a noise-free conditioning paradigm with positional encoding cloning for implicit feature alignment, enabling high-fidelity image editing through EditLoRA-enhanced Diffusion Transformers (DiTs) that efficiently learn customized operations while maintaining strict background consistency
  
    \item We collected the first publicly available curated photo-doodle dataset comprising 300+ high-quality pairs across 6 artistic styles, establishing a benchmark for reproducible research.

\end{itemize}


\section{Related work}
\subsection{Diffusion Model and Conditional Generation}


Recent advances in diffusion models have significantly advanced the state of text-conditioned image synthesis, achieving remarkable equilibrium between generation diversity and visual fidelity. Pioneering works such as GLIDE \cite{nichol2021glide}, hierarchical text-to-image models \cite{ramesh2022hierarchical}, and photorealistic synthesis frameworks \cite{saharia2022photorealistic} have systematically addressed key challenges in cross-modal generation tasks. The emergence of Stable Diffusion \cite{rombach2022high}, which implements a Latent Diffusion Model with text-conditioned UNet architecture, has established new benchmarks in text-to-image generation and inspired subsequent innovations including SDXL \cite{podell2023sdxl}, GLiGEN \cite{li2023gligen}, and Ranni \cite{feng2023ranni}. To enhance domain-specific adaptation, parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA) \cite{lora} and DreamBooth \cite{ruiz2023dreambooth} have demonstrated effective customization of pre-trained models. Concurrently, research efforts have focused on precise control over pictorial concepts through multi-concept customization \cite{kumari2023multi}, image prompt integration \cite{ye2023ip，ssr, fast_icassp}, and identity-preserving generation \cite{wang2024instantid}. The introduction of ControlNet \cite{zhang2023adding} further extended controllable generation capabilities to spatial constraints and depth information, with subsequent extended applications on various scenarios\cite{zhang2024stablehair, wang2024stablegarment,zhang2024stablemakeup,guo2025any2anytryon}. Some work \cite{antidreambooth, ringid, antirefernece,  idprotector} also focuses on the security issues of customized generation. Building upon these foundations, our work investigates the novel application of pre-trained diffusion transformers for generating artistic photo-doodles. Unlike previous approaches focusing on photorealism or explicit control modalities, we explore the model's potential in capturing freehand artistic expression while maintaining structural coherence.

% 最近，扩散模型的进展显著推动了文本条件图像生成的领域，达到了生成多样性和视觉逼真度之间的显著平衡。开创性工作如GLIDE \cite{nichol2021glide}、层次化的文本到图像模型 \cite{ramesh2022hierarchical} 和逼真合成框架 \cite{saharia2022photorealistic} 系统地解决了跨模态生成任务中的关键挑战。Stable Diffusion \cite{rombach2022high} 的出现，通过实现一个文本条件的UNet架构的潜在扩散模型，建立了文本到图像生成的新基准，并激发了后续创新，包括SDXL \cite{podell2023sdxl}、GLiGEN \cite{li2023gligen} 和Ranni \cite{feng2023ranni}。为了增强特定领域的适应性，像低秩适应（LoRA） \cite{lora} 和DreamBooth \cite{ruiz2023dreambooth} 等参数高效微调技术展示了有效的预训练模型定制方法。与此同时，研究工作还集中在通过多概念定制 \cite{kumari2023multi}、图像提示集成 \cite{ye2023ip} 和保持身份的生成 \cite{wang2024instantid} 来精确控制图像概念。ControlNet \cite{zhang2023adding} 的引入进一步扩展了可控生成能力，加入了空间约束和深度信息，随后的一些优化提高了推理效率 \cite{ssr, fast_icassp}。还有一些工作关注客制化生成的安全性问题。 基于这些基础，我们的工作探讨了预训练扩散变换器在生成艺术涂鸦方面的新应用。与以往专注于逼真效果或显式控制方式的研究不同，我们探索了模型在捕捉自由手绘艺术表达的同时保持结构一致性的潜力。

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/method.pdf} % Replace with your image file
    \caption{The overall architecture and training prodigim of photodoodle. The ominiEditor and EditLora all follow the lora training prodigm. We use a high rank lora for pre-training the OmniEditor on a large-scale dataset for general-purpose editing and text-following capabilities, and a low rank lora for fine-tuning EditLoRA on a small set of paired stylized images to capture individual artists’ specific styles and strategies for efficient customization. We encode the source image into a condition token and concatenate it with a noised latent token, controlling the generation outcome through MMAttention.}
    \label{fig2}
\end{figure*}

\subsection{Text-guilded Image Editing}

% 近年来，文本引导的图像编辑成为图像处理领域的热门研究方向，主要分为三种类型：全局描述引导、局部描述引导和指令引导。全局描述引导通过文本和图像区域的对齐关系实现编辑，例如 Prompt2Prompt 和 Imagic，可进行细粒度的全局编辑，但需要用户提供详细描述。局部描述引导则依赖掩膜和区域描述，方法如 Blended Diffusion 和 Imagen Editor，通过掩膜实现精确控制，但对用户提出额外要求，尤其在对象移除等复杂任务中增加了使用难度。相比之下，指令引导以简单的指令形式描述编辑需求，例如“将场景变为春季”，如 InstructPix2Pix 和 HIVE，无需精确掩膜或描述，使用更便捷，同时随着指令跟随和图像生成技术的发展，这种方法进一步提升了编辑质量和灵活性。本文基于DiT，在指令引导数据集上预训练了通用的图像编辑模型，并通过LoRA训练实现了客制化的图像编辑。

% In recent years, text-guided image editing has emerged as a prominent research area in image processing, categorized into three types: global description-guided, local description-guided, and instruction-guided editing. Global description-guided methods, like Prompt2Prompt and Imagic, perform fine-grained global editing by aligning text and image regions but require detailed user-provided descriptions. Local description-guided editing relies on masks and regional descriptions, as seen in Blended Diffusion and Imagen Editor, allowing precise control but imposing additional demands on users, particularly in complex tasks like object removal. In contrast, instruction-guided editing uses simple instructions, such as "change the scene to spring," exemplified by InstructPix2Pix and HIVE. It eliminates the need for precise masks or detailed descriptions, making it more user-friendly. With advancements in instruction-following and image generation technologies, this approach has further improved editing quality and flexibility. This paper pre-trains a general image editing model on an instruction-guided dataset using DiT, and achieves customized image editing through LoRA training.

Recent advances in text-guided image editing have established this field as a critical research frontier in visual content manipulation, with current methodologies generally classified into three paradigms: global description-guided, local description-guided, and instruction-guided editing. Global description-guided approaches (e.g., Prompt2Prompt \cite{p2p}, Imagic \cite{kawar2023imagic}, EditWorld \cite{editworld}, ZONE \cite{li2024zone} ), achieve fine-grained manipulation through cross-modal alignment between textual descriptions and image regions, yet demand meticulous text specification for target attributes. Local description-guided methods such as Blended Diffusion \cite{avrahami2022blended} and Imagen Editor \cite{imageneditor} enable pixel-level control via explicit mask annotations and regional text prompts, though their practical application is constrained by the requirement for precise spatial specifications, particularly in complex editing scenarios like object removal. The emerging instruction-guided paradigm, exemplified by InstructPix2Pix \cite{brooks2023instructpix2pix} and HIVE \cite{zhang2024hive}, represents a paradigm shift through its natural language interface that accepts editing commands (e.g., "change the scene to spring"). This approach eliminates the dependency on detailed textual descriptions or precise mask annotations, significantly enhancing user accessibility.

\subsection{Image and video Doodles}
% Image and video doodling refer to the creative process of adding hand-drawn elements or animations to static images or video content. This niche combines elements of graphic design, illustration, and animation, often resulting in a playful and engaging visual style. In the context of academic research, techniques like XXXX utilize advanced machine learning models to automate parts of this process, allowing users to generate complex doodle animations based on simple textual descriptions or sketches. These models not only preserve the artistic intent but also significantly reduce the time and skill required to create engaging multimedia content. Video Doodles extend this concept to dynamic video sequences, making it possible to animate static doodle elements in response to video content, thus adding a layer of interactivity and complexity to the doodling process.

% 图像和视频涂鸦指的是在静态图像或视频内容上添加手绘元素或动画的创造性过程。这一领域结合了平面设计、插画和动画的元素，通常呈现出一种俏皮且引人入胜的视觉风格。在学术研究的背景下，像XXXX这样的技术利用先进的机器学习模型来自动化这一过程的部分内容，允许用户基于简单的文本描述或草图生成复杂的涂鸦动画。这些模型不仅保留了艺术意图，还大大减少了创建引人入胜的多媒体内容所需的时间和技能。视频涂鸦将这一概念扩展到动态视频序列中，使得可以根据视频内容让静态涂鸦元素动起来，从而为涂鸦过程增加了互动性和复杂性。

Image and video doodling involve the creative process of adding hand-drawn elements or animations to static images or video content, blending aspects of graphic design, illustration, and animation to produce playful and engaging visual styles. In academic research, advanced techniques\cite{yu2023designing,videodoodles,belova2021google} have been developed to automate parts of this process. These methods enable users to generate intricate doodle animations from simple textual descriptions, sketches, or keyframes. By preserving artistic intent and reducing the time and expertise required, these models make multimedia content creation more accessible and efficient. Video doodling extends these capabilities to dynamic video sequences, allowing static doodle elements to be seamlessly integrated and animated in response to video motion and context. This innovation not only enhances interactivity but also introduces greater complexity and realism, making it a powerful tool for creative applications in entertainment, education, and storytelling.


