
\section{Experiment}

\subsection{Experiment Setting}
\noindent \textbf{Setup.}  During the OmniEditor pre-training stage, we take the parameters of Flux.1 dev model as the initialization of the DiT architecture, and trained it with the SeedEdit dataset. Images were resized to 768x512. We trained a LoRA rank of 256, a batch size of 128, and a learning rate of $1 \times 10^{-4}$, on 8 H100 GPUs for 330,000 steps. After merge the lora into the base DiT model, we acquired the OmniEditor model for further usage. In the EditLoRA training phase, we fine-tuned the merged model on a paired photo doodle dataset (50 pairs) using a single GPU for 10,000 steps, with a LoRA rank of 128, batch size of 2, and a learning rate of $1 \times 10^{-4}$.


% 预训练阶段，我们基于预训练的DIT架构的Flux 1.0 dev模型，使用SeedEdit数据集，单张图片分辨率为768*512， 拼图后训练分辨率为768*1024, 采用LoRA微调的方式，LoRA Rank 256， batchsize 128，Lr 1e-4，在8张H100上微调330，000 step。 EditLoRA训练阶段，我们先将Omin Editor和base model merge， LoRA merge的权重是 1.0， 然后在配对的photo doodle数据集上（50个），通过LORA微调的方式单卡微调10，000 step，LoRA Rank 64，batchsize 2, 学习率 1e-4。

% During the pretraining stage, we adopted the Flux 1.0 dev model based on the pretrained DiT architecture, using an image resolution of $1024 \times 1024$. We applied LoRA fine-tuning with a rank of 256, a batch size of 32, and a total of 20,000 training steps, enabling the model to acquire general image editing and text-following capabilities. 

% We pre-trained the OmniEditor on the 152K SeedEdit dataset with a batch size of 32 over XX training steps. This extensive pre-training equips the model with robust editing capabilities across diverse scenarios.

% For the EditLoRA training stage, we first merged the OmniEditor with the base model, setting the LoRA merge weight to 1.0. We then fine-tuned the merged model on a custom style dataset (20-50 pairs) for 8,000 steps. This process captures each artist’s unique editing style while preserving the strict background consistency established during pretraining.





% 本文对比的baseline分别为InstructP2P，MagicBrush， 和基于Flux 的SDEdit。 为了公平对比， 我们在通用图像编辑场景和客制化编辑场景进行测试。通用图像编辑测试时，我们用OmniEditor对比上述baseline。 在客制化编辑场景中， 我们用艺术家创作的涂鸦结果训练Flux LoRA，和SDEdit结合使用， 作为baseline， 然后和本文提出的EditLoRA进行对比。

% 和之前的方法相同， 我们在公开的Benchmark，HQ-Edit和Emu Edit上测试本文提出的OmniEditor的性能. 在客制化生成任务中， 本文提出了一个从Pexels网站上手机的Benchmark， 包含人像、动物、建筑等高质量摄影作品50张。


\noindent \textbf{Baseline Methods.}  The baselines compared in this paper include InstructP2P\cite{brooks2023instructpix2pix}, MagicBrush\cite{zhang2024magicbrush}, and SDEdit\cite{meng2021sdedit} based on Flux. For a fair comparison, tests were conducted in both general image editing and customized editing scenarios. For the general image editing tests, OmniEditor was evaluated against the aforementioned baselines. In the customized editing scenario, we trained Flux LoRA using doodle results created by professional artists and used it alongside SDEdit as a baseline. For InstructP2P and MagicBrush, the attention layers were also fine-tuned with the same doodle dataset. Finally, all the trained LoRA models were compared with the proposed EditLoRA model.

\noindent \textbf{Benchmarks.}  As with previous methods, we tested the performance of the proposed OmniEditor on the HQ-Edit benchmark\cite{hui2024hq}. For the customized generation tasks, this paper introduced a new benchmark collected from the Pexels website, consisting of 50 high-quality photographs of portraits, animals, and architecture.


% 图3 展示了PhotoDoodle图像编辑结果，得益于在海量instruction-images数据上训练，我们方法展示出很好的text following能力， 生成的涂鸦和原图非常和谐。 通过EditLoRA在少量艺术家的配对数据上训练，PhotoDoodle可以生成艺术家风格统一的照片涂鸦，同时维持极高的图像一致性，不引入不想要的变化。值得一提的是，我们的方法稳定性和成功率很高，几乎不需要cherry pick。

\subsection{Generation Results}
% Fig. \ref{fig3} shows the image editing results of PhotoDoodle, which demonstrates good text following ability due to training on a massive dataset of instruction-before-after triplets. The generated doodles harmonize well with the original images. Trained on a small set of paired data from artists using EditLoRA, PhotoDoodle can produce artistically consistent photo doodles while maintaining high image consistency without introducing unwanted changes. Notably, our method exhibits stable performance and a high success rate, meeting the demands of a production-ready model and greatly reducing the need for cherry-picking.

Fig. \ref{fig3} and Fig. \ref{fig7} displays the image editing results of PhotoDoodle, which excels in text following due to training on a large dataset of before-after pairs. The generated doodles blend well with the original images. When trained on a limited dataset of artist-paired data using EditLoRA, PhotoDoodle consistently produces artistic doodles while preserving image consistency and avoiding unwanted changes. Notably, our method maintains stable performance and a high success rate, making it suitable for production use and reducing the need for selective sampling.


% 本节，我们展示定性分析结果。 如图4所示， 在通用图像编辑任务中，得益于高质量数据和出色的模型架构设计，Omin editor的结果和SOTA 相比，展现出了更好的一致性， 和更少的不想要的变化。在客制化的图像编辑任务上，和baseline相比，我们的方法优势更显著，体现在生成质量，生成结果和艺术家作品的编辑风格高度相似， 编辑结果没有不想要的变化。


% 本节，我们展示定量分析结果。follow之前工作的评估方式，在通用图像编辑任务上，我们计算CLIP_dir 和 CLIP_img指标， 此外，我们还使用GPT4-o进行评估the alignment between text instruction and editing results。从表1可以看出， 我们的方法Omin Editor在绝大多数指标上取得了最好的结果，最高的CLIP Score 和 GPT Score，和CLIP_image Score, 次高的CLIP_dir Score。 在客制化的图像编辑任务上，和baseline相比， 我们的优势显著，体现在GPT4-o对生成质量和艺术家作品的一致性评估得分上。
\begin{table}[ht]
\centering
\footnotesize % Adjust text size
\caption{Comparison Results in General Image Editing Tasks. The best results are denoted as \textbf{Bold}.}



\begin{tabular}{lp{2.0cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
& \textbf{Methods}   & \textbf{$CLIP\ Score$}↑ & \textbf{$GPT\ Score$}↑  & \textbf{$CLIP_{img}$}↑ \\ \midrule
&Instruct-Pix2Pix    & 0.237    &  38.201                          & 0.806                     \\ 
&Magic Brush         & 0.234    & 36.555                           & 0.811                     \\ 
&SDEdit(FLUX)        & 0.230     & 34.329                              & 0.704                     \\ 
& Ours               & \textbf{0.261}    & \textbf{51.159}                           & \textbf{0.871}                     \\ \midrule
\end{tabular}
\label{tab:comparison_universal}
\end{table}


\subsubsection{Qualitative Evaluation}
% In this section, we present the qualitative analysis results. As demonstrated in Fig. \ref{fig4}, in the general image editing task, the OmniEditor shows superior consistency and fewer undesired changes compared to state-of-the-art (SOTA) methods, which can be attributed to the use of high-quality data and a well-designed model architecture. In custom image editing tasks, our method significantly outperforms baselines, reflecting in the quality of generation and high similarity of the editing style to the artist's original work, with no unwanted alterations.

In this section, we present the results of the qualitative analysis. As illustrated in Fig.~\ref{fig4}, OmniEdito demonstrates superior consistency and minimizes unintended alterations in general image editing tasks compared to state-of-the-art (SOTA) methods. This performance is attributed to the use of high-quality datasets and a thoughtfully designed model architecture. For custom image editing tasks, our method significantly surpasses baseline methods, as evidenced by the high quality of generated outputs and the strong alignment of the editing style with the original artistic intent, while avoiding undesired modifications.



% \begin{table}[ht]
% \centering
% \footnotesize % 调整文字大小
% \caption{Comparison Results in Customized Image Editing Tasks. The best results are denoted as \textbf{Bold}.}

% \begin{tabular}{lp{2.0cm}p{1.5cm}p{1.5cm}p{1.5cm}}
% \toprule
% & \textbf{Methods}   & \textbf{$CLIP\ Score$}↑ & \textbf{$GPT\ Score$}↑  & \textbf{$CLIP_{img}$}↑ \\ \midrule
% &Instruct-Pix2Pix         & 0.249  & 36.359                     & 0.832             \\ 
% &Magic Brush              & 0.247     & 38.478               & \textbf{ 0.885}             \\ 
% &SDEdit(FLUX)             & 0.209    & 21.793                 & 0.624             \\ 
% % &Ours                     & \textbf{0.279}   & \textbf{63.207}                & 0.854             \\  \midrule # in SIGGRAPH
% &Ours                     & \textbf{0.275}   & \textbf{91.141}        & 0.845            \\  \midrule

% \end{tabular}
% \label{tab:comparison_customized}
% \end{table}



\begin{table}[ht]
\centering
\footnotesize % 调整文字大小
\caption{Comparison Results in Customized Image Editing Tasks. The best results are denoted as \textbf{Bold}.}

\begin{tabular}{lp{2.0cm}p{1.5cm}p{1.5cm}p{1.5cm}}
\toprule
& \textbf{Methods}   & \textbf{$CLIP\ Score$}↑ & \textbf{$GPT\ Score$}↑  & \textbf{$CLIP_{img}$}↑ \\ \midrule
&Instruct-Pix2Pix         & 0.249  & 36.359                     & 0.832             \\ 
&Magic Brush              & 0.247     & 38.478               & \textbf{ 0.885}             \\ 
&SDEdit(FLUX)             & 0.209    & 21.793                 & 0.624             \\ 
&Ours                     & \textbf{0.279}   & \textbf{63.207}                & 0.854             \\  \midrule 
% &Ours                     & \textbf{0.275}   & \textbf{91.141}        & 0.845            \\  \midrule

\end{tabular}
\label{tab:comparison_customized}
\end{table}





\subsubsection{Quantitative Evaluation}

% In this section, we present the quantitative analysis results. Following the previous evaluation approaches, we calculate the $CLIP\ Score$ and $CLIP_{img}$ metrics for universal image editing tasks. Additionally, we use GPT4-o to assess the alignment between text instructions and editing results as proposed in HQ-Edit\cite{hui2024hq}. As shown in Table~\ref{tab:comparison_universal}, we outperforms all the other baselines in all of metrics, including the highest $CLIP\ Score$, $GPT\ Score$, and $CLIP_{image}$ Score. In customized image editing tasks, some of the models hardly performs meaningful editing, resulting a high $CLIP_{image}$ score, however, as shown in Table~\ref{tab:comparison_customized}, compared to the baseline, our advantages are still significant, as reflected in great advancement in the $GPT Score$ and $CLIP Score$, which is evaluating the consistency and quality of the generated content with the artist's work.
In this section, we present the quantitative analysis results. Following InstructP2P\cite{brooks2023instructpix2pix}, we compute the $CLIP\ Score$ and $CLIP_{img}$ metrics for both tasks. Furthermore, as proposed in HQ-Edit \cite{hui2024hq}, we employ GPT4-o to evaluate the alignment between text instructions and editing outputs. As shown in Table~\ref{tab:comparison_universal}, our method outperforms all baselines across all metrics in general image editing tasks, achieving the highest $CLIP\ Score$, $GPT\ Score$, and $CLIP_{image}$ Score. In custom image editing tasks, while some models fail to produce meaningful edits, which leads to high $CLIP_{image}$ scores, our method still holds a clear advantage over the baselines. This is evident in the substantial improvements in $GPT Score$ and $CLIP Score$, both of which evaluate the consistency and quality of the generated content in relation to the artist's original work.


\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{image/ablation.pdf} % Replace with your image file
    \caption{Ablation study results.}
    \label{fig5}
 \end{figure}



% \begin{table}[ht]
% \centering
% \scriptsize % 调整文字大小
% \caption{Comparison Results in Universal Image Editing Tasks.}
% \begin{tabular}{lp{2.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
% \toprule
% & \textbf{Methods}   & \textbf{$CLIP\ Score$} & \textbf{$GPT\ Score$} & \textbf{$CLIP_{dir}$} & \textbf{$CLIP_{img}$} \\ \midrule
% &Instruct-Pix2Pix         & 0.237149    &  38.201219                   & 0.131255                   & 0.806216                     \\ 
% &Magic Brush        & 0.233747    & 36.554878               & 0.127838                   & 0.811409                     \\ 
% &SDEdit(FLUX)      & 0.230162     & 34.329268              & \textbf{0.154364}                   & 0.703637                     \\ 
% & Ours               & \textbf{0.261045}    &\textbf{51.158536}              & 0.142719                   & \textbf{0.870749}                     \\ \midrule
% \end{tabular}
% \label{tab:comparison}
% \end{table}





\subsection{Ablation Study}

To demonstrate the effectiveness of the key strategies and modules proposed in this paper, we conducted detailed ablation experiments. We evaluated OmniEditor Pre-training, Position Encoding Cloning, and EditLoRA. As shown in Fig. \ref{fig5}:  Without OmniEditor Pre-training, directly training EditLoRA leads to reduced harmony between the generated sketches and photos, along with weaker text-following capabilities in the output.  Removing Position Encoding Cloning results in decreased consistency in the generated outputs, with unwanted changes occurring in the background.  When EditLoRA is not used, and only the pre-trained OmniEditor is employed for generation, the degree of stylization in the results is significantly reduced.  


% \begin{table}[ht]
% \centering
% \footnotesize % 调整文字大小
% \caption{Comparison Results in Ablation Study. The best results are denoted as \textbf{Bold}.}

% \begin{tabular}{lp{2.0cm}p{1.5cm}p{1.5cm}p{1.5cm}}
% \toprule
% & \textbf{Methods}   & \textbf{$CLIP\ Score$}↑ & \textbf{$GPT\ Score$}↑  & \textbf{$CLIP_{img}$}↑ \\ \midrule
% &Wo PE              & 0.259     & 63.207               & 0.746         \\ 
% &Wo Pretrain             &  0.269    & 85.380                 & 0.844             \\ 
% &Wo EditLora         &   0.267   & 76.576        & 0.829             \\ 
% &Full         &   \textbf{0.275}   & \textbf{91.141}        & \textbf{0.845}             \\  \midrule

% \end{tabular}
% \label{tab:comparison_customized}
% \end{table}


% 为了证明本文提出的关键策略和模块的有效性， 我们进行了详细的消融实验。 我们对， OmniEditor Pre-training、Position Encoding Cloning、EditLoRA 进行消融。 从表X和图X可以看出。 当没有OmniEditor Pre-training， 直接训练EditLoRA时，生成的涂鸦与照片组合的和谐程度降低，生成结果的text following变差。 当取消Position Encoding Cloning， 生成结果的一致性降低，背景区域有时会出现不想要的变化。当不使用EditLoRA，仅采用预训练的 OmniEditor进行生成， 结果风格化程度显著降低。



% \begin{table}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{0pt} 
% \caption{User Preferences for Human-Like Painting Styles Comparison}
% \begin{tabular}{lccc}
% \toprule
% Comparison Method & Anthropomorphic (\%) & Preference (\%)  \\
% \midrule
% Ours vs. LearnToPaint \cite{huang2019learning} & 78.2  & 68.2   \\
% Ours vs. Paint Transformer \cite{liu2021paint} & 80.4  & 65.9   \\
% Ours vs. Intelli-Paint \cite{singh2022intelli}  & 84.5 & 78.4   \\
% Ours vs. Stylized Neural Painting \cite{snp}  & 78.6  & 71.6   \\
% \bottomrule
% \end{tabular}
% \label{tab2}
% \end{table}



% 为了证明我们提出方法的优越性，我们通过电子问卷对30名参与者进行了用户研究。我们在通用和定制化图像编辑场景中评估了用户的偏好。参与者被展示了PhotoDoodle的输出结果与基准方法，并根据三个标准评估他们更喜欢的结果：1）总体偏好，2）与指令的符合度，3）编辑图像与原图之间的一致性。

% 在研究过程中，参与者查看了原始未编辑的图像、提示以及艺术家编辑的参考图像。然后他们需要决定PhotoDoodle（选项A）或某个基准方法（选项B）的表现更好，或者两者效果大致相同。这项用户研究的结果在图6中展示，我们报告了每个标准的百分比得分，突出了我们的方法在紧密符合艺术意图和保持高度一致性编辑中的有效性，同时没有引入不必要的变化。

\subsection{User Study}

To further demonstrate the superiority of our proposed method, we conducted a user study with 30 participants via online questionnaires. We evaluatedd user preferences in both general and customized image editing scenarios. Participants were presented with PhotoDoodle's outputs alongside baseline methods, and asked to evaluate which results they preferred based on three criteria: 1) Overall preference, 2) Instruction following, and 3) Consistency between the edited images and the original images. During the study, participants viewed the original unedited images, the edit instructions, and reference images edited by models. They were then asked to decide whether PhotoDoodle (Option A) or a baseline method (Option B) performed better, or if they were about equally effective. The results of this user study are collected in Fig. \ref{fig6}, where we reported the percentage scores of each criterion, highlighting our method’s effectiveness in aligning closely with artistic intentions and maintaining high consistency in edits without introducing unwanted changes.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{image/userstudy.pdf} % Replace with your image file

    \caption{User study results. The scores demonstrate the percentage of users who prefer ours over others under three evaluation metrics. PhotoDoodle outweighs all other baselines in user study.}

    \label{fig6}
    
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\linewidth]{image/more.pdf} % Replace with your image file

    \caption{More photo doodling results: one adds lines to a photo of clouds, imagining them as animals; the other converts the photo into a monochrome version and decorates it with color blocks.}

    \label{fig7}
\end{figure}

% \begin{table}[ht]
% \centering
% \footnotesize
% \caption{User study results comparing baseline methods with PhotoDoodle. Columns denote the preference and reference consistency rates (\%) for each baseline compared to PhotoDoodle.}
% \label{tab:user_study}
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Metric}       & \textbf{Baseline A} & \textbf{Baseline B} & \textbf{Baseline C} & \textbf{Baseline D} \\
% \midrule
% \textbf{Pref. (\%)}   & X                   & X                   & X                   & X                   \\
% \textbf{Ref. (\%)}    & X                   & X                   & X                   & X                   \\
% \bottomrule
% \end{tabular}
% \end{table}



\section{Limitation and Future Work}
% PhotoDoodle的一个缺点是仍需要收集数十组配对数据（编辑前，编辑后）并通过LoRA训练数千step， 数据收集有时会带来困难， 因为配对的图像并不是随处可得。未来我们会尝试通过Encoder结构从单个图片对中学习涂鸦策略。

One limitation of PhotoDoodle is its dependence on the collection of dozens of paired datasets (pre-edit and post-edit images) and the need for thousands of training steps using LoRA. This data collection process can be challenging, as paired images are not always readily accessible. In the future, we will attempt to learn doodling strategies from single image pairs using an Encoder structure.



\section{Conclusion}
% 本文提出了PhotoDoodle，一种专为“照片涂鸦”设计的新型图像编辑框架。基于预训练的DiT模型，我们采用了两阶段的方法，有效解决了装饰与背景的无缝融合、严格的背景一致性以及从少量数据高效学习风格的挑战。通过引入OmniEditor进行通用编辑，结合EditLoRA捕捉艺术家的特定美学风格，本文方法在多种场景中提供了高质量、可定制的编辑效果。此外，我们发布了一个包含20种风格、超过800个样本的全新数据集，以促进创意图像编辑的进一步研究。大量实验验证了PhotoDoodle的卓越性能和实用价值。
% In this paper, we have introduced PhotoDoodle, a novel image editing framework tailored for photo doodling. Our two-stage approach, built upon a pretrained DiT model, effectively addresses the challenges of seamless decoration, strict background consistency, and efficient style learning from minimal data. By proposing OmniEditor for generalized editing and EditLoRA for capturing an artist’s specific aesthetic, our method offers high-quality, customizable edits across diverse scenarios. Additionally, we release a new dataset of over 300 photo-doodle pairs representing 6 styles, fostering further study in creative image editing. Extensive experiments confirm the superior performance and practicality of PhotoDoodle.

% 在本文中，我们提出了PhotoDoodle，一个基于扩散的艺术图像编辑框架，它能从有限的成对示例中学习可定制的风格。我们的两阶段方法结合了OmniEditor的大规模预训练与高效的EditLoRA微调，实现了精确的装饰生成，同时通过位置编码的重用保持了背景的完整性。主要创新包括一个无噪声的条件化范式，用于严格的一致性，并且只需50对训练样本的参数高效风格适应。我们贡献了第一个专门的photo-doodle数据集，包含六种艺术风格和300多个精选样本。广泛的实验表明，无论是在通用编辑还是定制编辑场景中，我们的方法在和谐装饰与背景以及风格复制精确性方面的性能均显著优于现有方法。

 % In this paper, we present PhotoDoodle, a diffusion-based framework for artistic image editing that learns customizable styles from limited paired examples. Our two-stage approach combines large-scale pretraining of the OmniEditor with efficient EditLoRA fine-tuning, enabling precise decoration generation while preserving background integrity through positional encoding reuse. Key innovations include a noise-free conditioning paradigm for strict consistency and parameter-efficient style adaptation requiring only 50 training pairs. We contribute the first dedicated photo-doodle dataset with six artistic styles and 300+ curated samples. Extensive experiments demonstrate superior performance in harmonizing decorations with backgrounds and style replication accuracy, significantly outperforming existing methods in both generic and customized editing scenarios.

In this paper, we present PhotoDoodle, a diffusion-based framework for artistic image editing that learns unique artistic styles from minimal paired examples. By combining large-scale pretraining of the OmniEditor with efficient EditLoRA fine-tuning, PhotoDoodle enables precise decorative generation while maintaining background integrity through positional encoding cloning. Key innovations, including a noise-free conditioning paradigm and parameter-efficient style adaptation requiring only 50 training pairs, significantly reduce computational barriers. We also contribute a new dataset with six artistic styles and 300+ curated samples, establishing a benchmark for reproducible research. Extensive experiments demonstrate superior performance in style replication and background harmony, outperforming existing methods in both generic and customized editing scenarios.

\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
