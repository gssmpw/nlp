


% NEW
% intro逻辑这样：
% image generation发展的非常好，基于pretrain-finetuning路线，业界提出来了很多cusomized image generation方案cite{}，用来解决id学习、风格化、主体一致\cite{}等等任务。但是，while cusomized image generation thrives, a critical gap persistes in customized image editing. 因此，customized image editing任务的价值并没有被充分挖掘出来。

% PhotoDoodle，是一个备受关注，但之前又难以实现的customized image editing任务。艺术家通过在照片上添加装饰性元素与风格化修饰，使背景照片呈现更具个性化或艺术化的效果。艺术家在进行照片涂鸦创作时，常使用局部风格化、装饰性线条、添加新物体以及对现有物体进行装饰。同时，不同艺术家和设计师在编辑图像或创建照片涂鸦时，都体现出各自明显的个人风格。艺术家们也会希望将自己的创作应用至更多的背景照片中，但当前缺乏自动化工具，使得一张照片的生产需要小时级别的时间成本，一方面限制了艺术编辑的使用场景，另一方面限制了大规模生产配对数据的可能性。

% 自动化照片涂鸦的生产流程，主要面临三大挑战：% 1. 生成装饰与背景的和谐统一：不仅位置与内容要合理，还需允许非真实感的呈现卡通或扁平插画风格。% 2. 避免不想要的变化：保证背景的严格一致性。% 3. 从少量艺术家编辑前后图像对中，高效学习其独特的编辑风格与技巧。现有方案在处理这些挑战时，均遇到了阻碍。

% 现有的图像编辑方法大体可分为全局编辑和inpainting：前者常用于整体风格化，但一旦试图修改特定区域时，往往会连带改变原背景， prompt2prompt、Instruct P2P等实现了高一致性的图像编辑，可在一定程度上维持编辑前后的一致性；inpainting类的方法在mask内部进行区域去噪， 可以保证非mask区域不变，但要求用户生产一个合理的mask，这对用户的交互要求过高，不适合照片涂鸦这样需要灵活添加装饰的场景。因此， 我们希望通过建立一个灵活的，instruction引导的而非mask引导的方式实现照片涂鸦。

% 针对这些挑战以及现有方案的缺陷，我们提出了PhotoDoodle方法，用于learning artistic image editing from few shot examples. 模型基于预训练的DiT模型，以背景作为条件生成照片涂鸦，具体采用两阶段训练策略：
% 1. 首先将一个预训练的文生图模型改造成通用图像编辑模型，并在海量图像编辑数据集上习得基础编辑能力。
% 2. 随后，在第一阶段的模型上，用艺术家编辑前后图像的配对数据来训练Edit LoRA，从而捕捉艺术家的编辑技巧与风格。为减少不必要的改动并实现严格一致性，我们还提出了PE复用策略用于隐式对齐。
% 值得强调的是，我们有几个关键洞察：
% * 一是Clean Latent非常重要，即在条件图像部分无需添加噪声，以避免与原图产生额外差异，从而提升生成结果与条件图像的一致性。
% * 二是隐式对齐，条件信息能更好地整合背景信息，同时无需额外增加训练参数，使得模型充分保留预训练T2I model的能力。 

% 综上所述，本文贡献如下：
% 1. 我们提出了一个客制化图像编辑框架，通过少量艺术家“照片涂鸦”图像对，学习与迁移其编辑风格与技巧；
% 2. 借助条件机制扩展与PE复用策略实现隐式对齐，基于DiT实现了高质量且高度一致的图像编辑，并利用EditLoRA高效地学习客制化编辑；
% 3. 我们发布了首个包含6种高质量风格、逾300幅艺术家创作的照片-涂鸦对数据集。大量实验与评估表明，该方法在定制化图像编辑任务上具有显著的性能优势与实用价值。

\section{Introduction}

The rise of diffusion models has started a new chapter for image creation and control. Using the pretrain-finetune approach, the community has achieved remarkable progress in customized image generation \cite{ruiz2023dreambooth, TI, lora}, with applications spanning identity preservation \cite{facechain}, artistic stylization \cite{song2024processpainter, zhang2023inversion,sohn2023styledrop,ahn2024dreamstyler}, and subject coherence \cite{customdiff,ruiz2023dreambooth, TI,lora,ruiz2024hyperdreambooth,jiang2024videobooth,zhu2024multibooth}. These advancements have fueled applications ranging from digital art creation to commercial design.

Current methods predominantly focus on content creation, while intelligent context-aware editing—particularly for artistic enhancement—remains underexplored. This limitation becomes acute in scenarios requiring precise modifications without compromising original content, such as commercial photo retouching or interactive design workflows.

PhotoDoodling, a long-standing challenge in customized image editing, exemplifies this gap. The task requires: 1. harmonious integration of decorative elements with spatial/semantic context, 2. strict background consistency during non-photorealistic edits, and 3. efficient style acquisition from limited examples. Existing approaches face compounded difficulties: Global editing methods (e.g., Prompt-to-Prompt\cite{p2p}, InstructP2P\cite{brooks2023instructpix2pix}) distort backgrounds during local modifications, while inpainting-based techniques\cite{zhang2024magicbrush} demand precise masks on the to-be-edited area, fundamentally conflicting with creative editing.



Automating PhotoDoodle workflows is intriguing, yet it faces three interconnected challenges. First, achieving harmonious integration demands that generated decorations align not only spatially (e.g., matching perspective) but also semantically with the background, while accommodating non-photorealistic styles such as cartoon or flat illustration. Second, maintaining strict background consistency requires rigorously preventing unintended alterations to the original content, including color shifts or texture distortions. Third, to achieve efficient style acquisition, it is necessary to extract an artist's unique editing patterns from a few photo-doodle pairs, which is technically challenging. These compounded challenges cause existing methods to be incompetent in addressing the problem comprehensively.


The limitations of prevailing image editing paradigms further exacerbate these difficulties. Global editing methods (e.g., Prompt-to-Prompt\cite{p2p}, InstructP2P\cite{brooks2023instructpix2pix}), while effective for consistent style transfer, inadvertently distort background content during localized modifications. Inpainting-based approaches, though capable of preserving non-mask regions through localized denoising, impose impractical demands for pixel-perfect user-defined masks, fundamentally conflicting with PhotoDoodle’s need for flexible creative workflows. To bridge this gap, we introduce an instruction-guided framework that replaces mask dependency with semantic-level control, enabling precise and style-conscious decorative generation while ensuring background consistency.

% 主流图像编辑范式的局限性进一步加剧了这些困难。全局编辑方法（如Prompt-to-Prompt、InstructP2P）虽然在风格迁移上表现一致，但在局部修改时会无意中扭曲背景内容——这是一种“连锁反应”副作用。基于修复的方法尽管能够通过局部去噪保留非遮罩区域，却对用户定义的遮罩提出了不切实际的高精度要求，这与照片涂鸦所需的灵活创作流程从根本上相冲突。为了弥合这一差距，我们提出了一种指令引导框架，用语义级控制取代遮罩依赖，在确保背景一致性的同时，实现精确且风格感知的装饰生成。

To address these challenges and bridge the gaps in existing solutions, we propose PhotoDoodle, a framework designed to learn artistic image editing from few-shot examples. Building upon a pre-trained Diffusion Transformer (DiT), our method conditions on background images to generate photo doodles through a two-stage training pipeline. First, we adapt a well-trained text-to-image model into a versatile image editor by fine-tuning it on large-scale editing datasets, thereby establishing the foundational editing model OmniEditor. Then, we train an EditLoRA module using artist-curated before-after pairwise data to learn nuanced editing styles, while introducing a Positional Encoding (PE) Cloning strategy to enforce strict background consistency through implicit feature alignment.

% 为解决上述挑战并弥补现有方案的不足，我们提出了PhotoDoodle框架，旨在从少量示例中学习艺术化图像编辑。该方法基于预训练的扩散Transformer（DiT），以背景图像为条件生成照片涂鸦，并通过两阶段训练流程实现：首先，我们将预训练的文本到图像模型转化为通用图像编辑器，通过在大规模编辑数据集上的微调，建立基础编辑能力；随后，利用艺术家提供的编辑前后图像对训练EditLoRA模块，以提炼细腻的编辑风格，同时引入位置编码克隆策略（PE Cloning），通过隐式特征对齐确保严格的背景一致性。

Our design is guided by two main insights. First, maintaining clean latent conditioning—ensuring that input image tokens are noise-free—is essential for keeping the output coherent with the background by minimizing divergence in the latent space. Second, implicit alignment through position encoding cloning enables the seamless integration of background context without adding extra trainable parameters, thus preserving the full generative capacity of the base T2I model. These insights allow our framework to strike a balance between artistic flexibility and strict consistency.

% 我们的设计受到两个关键发现的驱动。其一，保持纯净潜空间条件（即输入图像不添加噪声）对维持生成结果与背景的一致性至关重要，这通过最小化潜在空间偏差来实现；其二，通过PE克隆的隐式对齐，我们能够在无需引入额外可训练参数的情况下，无缝整合背景信息，从而充分保留基础T2I模型的生成能力。这些洞察共同使我们的框架能够在艺术灵活性与严格一致性之间取得平衡。

In summary, our contributions are threefold:
\begin{itemize}
    \item We propose a customizable image editing framework that learns and transfers artistic styles from few photo-doodle pairs.
    \item By extending conditional mechanisms and implementing a PE cloning strategy for implicit alignment, we achieve high-quality and highly consistent image editing built upon Diffusion Transformers (DiTs), while utilizing EditLoRA to efficiently learn customized editing operations.
    \item We propose the first dedicated photo-doodle dataset containing 300+ high-quality pairs across 6 artistic styles, establishing a benchmark for reproducible research.
\end{itemize}


\section{Related work}
\subsection{Diffusion Model and Conditional Generation}


Recent advances in diffusion models have significantly advanced the state of text-conditioned image synthesis, achieving remarkable equilibrium between generation diversity and visual fidelity. Pioneering works such as GLIDE \cite{nichol2021glide}, hierarchical text-to-image models \cite{ramesh2022hierarchical}, and photorealistic synthesis frameworks \cite{saharia2022photorealistic} have systematically addressed key challenges in cross-modal generation tasks. The emergence of Stable Diffusion \cite{rombach2022high}, which implements a Latent Diffusion Model with text-conditioned UNet architecture, has established new benchmarks in text-to-image generation and inspired subsequent innovations including SDXL \cite{podell2023sdxl}, GLiGEN \cite{li2023gligen}, and Ranni \cite{feng2023ranni}. To enhance domain-specific adaptation, parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA) \cite{lora} and DreamBooth \cite{ruiz2023dreambooth} have demonstrated effective customization of pre-trained models. Concurrently, research efforts have focused on precise control over pictorial concepts through multi-concept customization \cite{kumari2023multi}, image prompt integration \cite{ye2023ip}, and identity-preserving generation \cite{wang2024instantid}. The introduction of ControlNet \cite{zhang2023adding} further extended controllable generation capabilities to spatial constraints and depth information, with subsequent optimizations improving inference efficiency \cite{ssr, fast_icassp}. Some work \cite{antidreambooth, ringid, antirefernece,  idprotector} also focuses on the security issues of customized generation. Building upon these foundations, our work investigates the novel application of pre-trained diffusion transformers for generating artistic photo-doodles. Unlike previous approaches focusing on photorealism or explicit control modalities, we explore the model's potential in capturing freehand artistic expression while maintaining structural coherence.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/method.pdf} % Replace with your image file
    \caption{The overall architecture and training prodigim of photodoodle. The ominiEditor and EditLora all follow the lora training prodigm. We use a high rank lora for pre-training the OmniEditor on a large-scale dataset for general-purpose editing and text-following capabilities, and a low rank lora for fine-tuning EditLoRA on a small set of paired stylized images to capture individual artists’ specific styles and strategies for efficient customization. We encode the original image into a condition token and concatenate it with a noised latent token, controlling the generation outcome through MMAttention.}
    \label{fig2}
\end{figure*}

\subsection{Text-guilded Image Editing}

% 近年来，文本引导的图像编辑成为图像处理领域的热门研究方向，主要分为三种类型：全局描述引导、局部描述引导和指令引导。全局描述引导通过文本和图像区域的对齐关系实现编辑，例如 Prompt2Prompt 和 Imagic，可进行细粒度的全局编辑，但需要用户提供详细描述。局部描述引导则依赖掩膜和区域描述，方法如 Blended Diffusion 和 Imagen Editor，通过掩膜实现精确控制，但对用户提出额外要求，尤其在对象移除等复杂任务中增加了使用难度。相比之下，指令引导以简单的指令形式描述编辑需求，例如“将场景变为春季”，如 InstructPix2Pix 和 HIVE，无需精确掩膜或描述，使用更便捷，同时随着指令跟随和图像生成技术的发展，这种方法进一步提升了编辑质量和灵活性。本文基于DiT，在指令引导数据集上预训练了通用的图像编辑模型，并通过LoRA训练实现了客制化的图像编辑。

% In recent years, text-guided image editing has emerged as a prominent research area in image processing, categorized into three types: global description-guided, local description-guided, and instruction-guided editing. Global description-guided methods, like Prompt2Prompt and Imagic, perform fine-grained global editing by aligning text and image regions but require detailed user-provided descriptions. Local description-guided editing relies on masks and regional descriptions, as seen in Blended Diffusion and Imagen Editor, allowing precise control but imposing additional demands on users, particularly in complex tasks like object removal. In contrast, instruction-guided editing uses simple instructions, such as "change the scene to spring," exemplified by InstructPix2Pix and HIVE. It eliminates the need for precise masks or detailed descriptions, making it more user-friendly. With advancements in instruction-following and image generation technologies, this approach has further improved editing quality and flexibility. This paper pre-trains a general image editing model on an instruction-guided dataset using DiT, and achieves customized image editing through LoRA training.

Recent advances in text-guided image editing have established this field as a critical research frontier in visual content manipulation, with current methodologies generally classified into three paradigms: global description-guided, local description-guided, and instruction-guided editing. Global description-guided approaches (e.g., Prompt2Prompt \cite{p2p}, Imagic \cite{kawar2023imagic}, EditWorld \cite{editworld} ), achieve fine-grained manipulation through cross-modal alignment between textual descriptions and image regions, yet demand meticulous text specification for target attributes. Local description-guided methods such as Blended Diffusion \cite{avrahami2022blended} and Imagen Editor \cite{imageneditor} enable pixel-level control via explicit mask annotations and regional text prompts, though their practical application is constrained by the requirement for precise spatial specifications, particularly in complex editing scenarios like object removal. The emerging instruction-guided paradigm, exemplified by InstructPix2Pix \cite{brooks2023instructpix2pix} and HIVE \cite{zhang2024hive}, represents a paradigm shift through its natural language interface that accepts editing commands (e.g., "change the scene to spring"). This approach eliminates the dependency on detailed textual descriptions or precise mask annotations, significantly enhancing user accessibility.

\subsection{Image and video Doodles}
% Image and video doodling refer to the creative process of adding hand-drawn elements or animations to static images or video content. This niche combines elements of graphic design, illustration, and animation, often resulting in a playful and engaging visual style. In the context of academic research, techniques like XXXX utilize advanced machine learning models to automate parts of this process, allowing users to generate complex doodle animations based on simple textual descriptions or sketches. These models not only preserve the artistic intent but also significantly reduce the time and skill required to create engaging multimedia content. Video Doodles extend this concept to dynamic video sequences, making it possible to animate static doodle elements in response to video content, thus adding a layer of interactivity and complexity to the doodling process.

% 图像和视频涂鸦指的是在静态图像或视频内容上添加手绘元素或动画的创造性过程。这一领域结合了平面设计、插画和动画的元素，通常呈现出一种俏皮且引人入胜的视觉风格。在学术研究的背景下，像XXXX这样的技术利用先进的机器学习模型来自动化这一过程的部分内容，允许用户基于简单的文本描述或草图生成复杂的涂鸦动画。这些模型不仅保留了艺术意图，还大大减少了创建引人入胜的多媒体内容所需的时间和技能。视频涂鸦将这一概念扩展到动态视频序列中，使得可以根据视频内容让静态涂鸦元素动起来，从而为涂鸦过程增加了互动性和复杂性。

Image and video doodling involve the creative process of adding hand-drawn elements or animations to static images or video content, blending aspects of graphic design, illustration, and animation to produce playful and engaging visual styles. In academic research, advanced techniques\cite{yu2023designing,videodoodles,belova2021google} have been developed to automate parts of this process. These methods enable users to generate intricate doodle animations from simple textual descriptions, sketches, or keyframes. By preserving artistic intent and reducing the time and expertise required, these models make multimedia content creation more accessible and efficient. Video doodling extends these capabilities to dynamic video sequences, allowing static doodle elements to be seamlessly integrated and animated in response to video motion and context. This innovation not only enhances interactivity but also introduces greater complexity and realism, making it a powerful tool for creative applications in entertainment, education, and storytelling.


