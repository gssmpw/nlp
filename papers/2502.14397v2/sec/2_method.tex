
\section{Method}
In this section, we begin by exploring the preliminaries on diffusion transformer as detailed in Section 3.1. Next, Section 3.2 outlines our three-stage system design. We then detail two key innovations—OmniEditor Pre-training (Section 3.3) and EditLoRA for style adaptation (Section 3.4). Finally, Section 3.5 describes how we built the PhotoDoodle dataset. 


% \subsection{Preliminary}
% \textbf{FLUX.1} FLUX.1\cite{flux} is a powerful text-to-image model based on Flow Matching\cite{lipman2022flow} and DiT\cite{dit}. It introduces rotary positional embeddings (RoPE)\cite{su2024roformer} and parallel attention layers\cite{dehghani2023scaling} to improve performance and efficiency. The position embedding plays a crucial role in our try-on tasks for preserving garment-agnostic regions. FLUX.1 implements a three-dimensional RoPE scheme which supports three coordinates, while only the second and third dimensions are used for encoding spatial positions in the latent space:

% \begin{equation}
% \omega_m = \frac{1}{\theta^{2m/d}}, m \in [0, d/2)
% \end{equation}

% where $\theta$ is typically set to 10000. The position encoding applies a rotation matrix:

% \begin{equation}
% \begin{bmatrix} 
% \cos(m\omega \cdot \mathbf{pos}) & -\sin(m\omega \cdot \mathbf{pos}) \\
% \sin(m\omega \cdot \mathbf{pos}) & \cos(m\omega \cdot \mathbf{pos})
% \end{bmatrix}
% \end{equation}

% This rotation is applied to query and key vectors in the attention mechanism, enabling the model to capture relative positional relationships in the latent space.

% \subsection{Preliminary}

% The Diffusion Transformer (DiT) model \cite{}, employed in architectures like FLUX.1 \cite{}, Stable Diffusion 3 \cite{}, and PixArt~\cite{2}, uses a transformer as the denoising network to iteratively refine noisy image tokens.

% A DiT model processes two types of tokens: noisy image tokens $X \in \mathbb{R}^{N \times d}$ and text condition tokens $c_T \in \mathbb{R}^{M \times d}$, where $d$ is the embedding dimension, and $N$ and $M$ are the number of image and text tokens. Throughout the network, these tokens maintain consistent shapes as they pass through multiple transformer blocks.

% In FLUX.1, each DiT block consists of layer normalization followed by Multi-Modal Attention (MMA) \cite{28}, which incorporates Rotary Position Embedding (RoPE) \cite{33} to encode spatial information. For image tokens $X$, RoPE applies rotation matrices based on the token's position $(i,j)$ in the 2D grid:
% \begin{equation}
% X_{i,j} \rightarrow X_{i,j} \cdot R(i,j),
% \end{equation}
% where $R(i,j)$ is the rotation matrix at position $(i,j)$. Text tokens $c_T$ undergo the same transformation with their positions set to $(0,0)$.

% The multi-modal attention mechanism then projects the position-encoded tokens into query $Q$, key $K$, and value $V$ representations. It enables the computation of attention between all tokens:
% \begin{equation}
% \text{MMA}([X; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
% \end{equation}
% where $[X; c_T]$ denotes the concatenation of image and text tokens. This formulation enables bidirectional attention.

\subsection{Preliminary}

% The Diffusion Transformer (DiT) model \cite{dit}, utilized in architectures such as FLUX.1 \cite{flux2023}, Stable Diffusion 3 \cite{sd3}, and PixArt~\cite{chen2023pixartalpha}, employs a transformer-based denoising network to iteratively refine noisy image tokens.

% DiT operates on two categories of tokens: noisy image tokens $X \in \mathbb{R}^{N \times d}$ and text condition tokens $c_T \in \mathbb{R}^{M \times d}$, where $d$ denotes the embedding dimension, and $N$ and $M$ represent the numbers of image and text tokens, respectively. These tokens retain their shapes consistently as they propagate through multiple transformer layers.

% In FLUX.1, each DiT block incorporates layer normalization, followed by Multi-Modal Attention (MMA) \cite{sd3}, which employs Rotary Position Embedding (RoPE) \cite{rope} to encode spatial information. For image tokens $X$, RoPE applies rotation matrices based on the token’s position $(i,j)$ in the 2D grid:
% \begin{equation}
% X_{i,j} \rightarrow X_{i,j} \cdot R(i,j),
% \end{equation}
% where $R(i,j)$ represents the rotation matrix corresponding to position $(i,j)$. Similarly, text tokens $c_T$ are transformed with their positions designated as $(0,0)$.

% The multi-modal attention mechanism projects these position-encoded tokens into query $Q$, key $K$, and value $V$ representations, enabling attention computation across all tokens:
% \begin{equation}
% \text{MMA}([X; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
% \end{equation}
% where $[X; c_T]$ denotes the concatenation of image and text tokens. This approach ensures bidirectional attention between the tokens.

%The Diffusion Transformer (DiT) model \cite{dit}, utilized in architectures such as FLUX.1 \cite{flux2023}, Stable Diffusion 3 \cite{sd3}, and PixArt~\cite{chen2023pixartalpha}, employs a transformer-based denoising network to iteratively refine noisy image tokens.
The Diffusion Transformer (DiT)\cite{dit} powers modern image generators like Stable Diffusion 3\cite{sd3} and PixArt\cite{chen2023pixartalpha}. At its core, DiT uses a special transformer network that denoise the noisy image tokens step-by-step. 


DiT operates on two categories of tokens: noisy image tokens $z \in \mathbb{R}^{N \times d}$ and text condition tokens $c_T \in \mathbb{R}^{M \times d}$, where $d$ denotes the embedding dimension, and $N$ and $M$ represent the numbers of image and text tokens, respectively. These tokens retain their shapes consistently as they propagate through multiple transformer layers.

In FLUX.1, each DiT block incorporates layer normalization, followed by Multi-Modal Attention (MMA) \cite{sd3}, which employs Rotary Position Embedding (RoPE) \cite{rope} to encode spatial information. For image tokens $z$, RoPE applies rotation matrices based on the token’s position $(i,j)$ in the 2D grid:
\begin{equation}
z_{i,j} \rightarrow z_{i,j} \cdot R(i,j),
\end{equation}
where $R(i,j)$ represents the rotation matrix corresponding to position $(i,j)$. Similarly, text tokens $c_T$ are transformed with their positions designated as $(0,0)$.

The multi-modal attention mechanism projects these position-encoded tokens into query $Q$, key $K$, and value $V$ representations, enabling attention computation across all tokens:
\begin{equation}
\text{MMA}([z; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\end{equation}
where $\mathit{Z}=[z; c_T]$ denotes the concatenation of image and text tokens. This approach ensures bidirectional attention between the tokens.



% PhotoDoodle 所提出的整体方案架构如图2所示， 分为两个阶段： 1.  在海量配对图像编辑数据集上 Pre-train OmniEditor ， 以获得通用图像编辑能力和text following能力。 2. 在少量（20-50）风格化图片对上微调  EditLoRA， 以学习特定艺术家的图像编辑风格和策略。


\subsection{Overall Architecture}

The overall architecture of PhotoDoodle is illustrated in Fig. \ref{fig2}. It comprises the following stages:

\noindent \textbf{Pre-training OmniEditor.} The OmniEditor is pre-trained on a large-scale image editing dataset to acquire general-purpose image editing capabilities and strong text-following abilities. This stage ensures the model's capability in diverse editing tasks.

\noindent \textbf{Fine-tuning EditLoRA.} After pre-training, EditLoRA is fine-tuned on a small set of pairwise stylized images (20–50 pairs) to learn the specific editing styles and strategies of individual artists. This stage enables efficient customization for personalized editing needs.

% OmniEditor在一个大规模成对图像编辑数据集上进行预训练，以获取通用的图像编辑能力和强大的文本跟随能力。这一阶段确保模型在多种编辑任务中表现出高效能。

% 预训练后，EditLoRA在一小组成对的风格化图像（20-50对）上进行微调，以学习个别艺术家的具体编辑风格和策略。这一阶段使得针对个性化编辑需求的高效定制成为可能。


\noindent \textbf{Inference.}: During inference, the input source image $I_{src}$ is encoded as condition tokens $c_I$ via VAE. We then randomly sample Gaussian noise as image tokens $z$, cloning the Position Encoding from condition tokens, and concatenate the tokens along the sequence dimension. Subsequently, we apply the flow matching method to predict the target velocity, iterating multiple steps to obtain the predicted image latent representation. Finally, the predicted image tokens are converted by the VAE decoder to achieve the final predicted image. 

% Thanks to the attention mechanism, PhotoDoodle can accelerate inference like LLMs by using kv-cache: storing previous and current key and value states of the input conditions on the GPU, allowing for attention computation without redundant calculations.


% 在推理过程中，我们将输入图像通过VAE编码为condition token，然后我们随机采样一个高斯噪声和condition token拼接，并复用Position Encoding，和然后应用流匹配方法预测目标速度，迭代多个步骤以获得最终的潜在表示。最后，我们使用VAE解码器将潜在表示解码为预测图像。

\subsection{OmniEditor Pre-training}
We denote the pre-edited image as the source image $I_{src}$ and the post-edited image as the target image $I_{tar}$. Previous works, such as SDEdit, model image editing as an adding-denoising problem, they often altering unintended areas. Others like InstructP2P\cite{brooks2023instructpix2pix} redesign core model parts, significantly degrading the capacity of the pretrained t2i models. Unlike them, our approach, PhotoDoodle, conceptualizes image editing as a conditional generation problem and minimizes the modification of pretrained text-to-image DiT. 

Our model leverages the advanced capabilities of the DiT-based pretrained model, and extends it to function as an image editing tool. Both $I_{src}$ and $I_{tar}$ are encoded into their respective latent representations, $c_I$ and $z$, via a VAE. After applying position encoding cloning, the latent tokens are then concatenated along the sequence dimension to perform joint attention. The Multi-modal attention mechanisms are used to provide conditional information for the denoising of the doodle image. 
\begin{equation}
\text{MMA}([z; c_{i}; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\end{equation}
where $\textit{Z}=[z; c_{i}; c_T]$ denotes the concatenation of noised latent tokens, image condition tokens, and text tokens. Here, $c_{I}$ corresponds to $I_{src}$. This formulation enables bidirectional attention, letting the conditional branch and denoise branch interact on demand.

% \noindent \textbf{Position Encoding Cloning.} To ensure consistency and prevent undesired alterations during the editing process, we introduce a novel Position Encoding Cloning strategy. This strategy maintains identical positional encodings for $ l_{0} $ and $ l_{1} $, ensuring precise positional correspondence and enhancing consistency across the editing process.

% Crucially, during the generation of $ I_{1} $ conditioned on $ I_{0} $, $ l_{0} $ remains noise-free. This approach preserves fine details in $ I_{0} $ and prevents the introduction of unwanted modifications. Consequently, during training, noise is only applied to $ l_{1} $, thereby ensuring that the edits are both accurate and consistent.

\noindent \textbf{Position Encoding Cloning.}
Existing approaches to conditional image editing often struggle with pixel-level misalignment between input $I_{src}$ and edited output ($I_{tar}$) that undermine visual coherence. To address this fundamental challenge, we propose a novel Position Encoding(PE) Cloning strategy, motivated by the need for implicit spatial correspondence. 

The PE Cloning is a simple yet powerful stragegy, it simply apply the position encoding calculated on $I_{src}$ on both $I_{src}$ and $I_{tar}$. The identical positional encoding serves as a strong hint so that the DiT is capable of learning correct corresponding easily. By enforcing identical positional encodings between the latent representations $c_I$ and $z$, our method establishes a pixel-perfect coordinate mapping that persists throughout the diffusion process. 
This geometric consistency ensures that every edit respects the original image's spatial structure, eliminating ghosting artifacts and misalignments that plague conventional approaches.

\noindent \textbf{Noise-free Conditioning Paradigm.}  
A critical innovation lies in our noise-free conditioning paradigm. We preserve $c_I$ as a reference during the generation of $I_{tar}$. This design choice achieves two objectives through its operational duality.  
First, by maintaining $c_I$ in a noise-free state, we ensure the retention of high-frequency textures and fine structural details from the original image, thereby preventing degradation during iterative denoising. This preservation mechanism acts as a safeguard against the blurring artifacts commonly observed in conventional approaches. Second, the MM attention machanism is flexible enough to choose either to copy from the source or generate new content via instruction, making the model learns to manipulate only designated target regions.  

Through the combined action of position encodings cloning and MMA mechanism, our framework achieves unprecedented precision in localized editing while maintaining global consistency, a balance previously unattainable in conditional image generation tasks.

% \noindent \textbf{无噪声条件生成范式（Noise-free Conditioning Paradigm）}  
% 我们的关键创新在于无噪声条件生成范式。不同于传统潜在扩散模型同时向 $l_0$ 和 $l_1$ 注入噪声的做法，我们在生成 $I_1$ 的过程中将 $l_0$ 保持为纯净的空间锚点。通过这一设计选择，模型实现了两种协同目标的操作平衡。  

% 首先，保持 $l_0$ 的无噪声状态能够确保保留原始图像的高频纹理和精细结构，从而在迭代去噪过程中防止细节退化。这一保留机制有效避免了传统方法中常见的模糊伪影问题。其次，仅对 $l_1$ 选择性地施加噪声，为编辑过程引入了严格的控制约束：模型仅学习操控指定的目标区域，而将 $l_0$ 作为不可变的几何参考。  

% 通过位置编码克隆实现的几何锚定与选择性噪声调制的协同作用，我们的框架在保持全局一致性的同时，达到了局部编辑前所未有的高精度——这一平衡在条件图像生成任务中是以往难以实现的。

\noindent \textbf{Conditional flow matching loss.} The conditional flow matching loss function is following SD3 \cite{sd3}, which is defined as follows:
\begin{equation}
L_{CFM} = E_{t, p_t(z|\epsilon), p(\epsilon)} \left[ \left\| v_\Theta(z, t, c_I,c_T) - u_t(z|\epsilon) \right\|^2 \right]
\end{equation}

Where $ v_\Theta(z, t, c_I,c_T)$ represents the velocity field parameterized by the neural network's weights, $t$ is timestep, $c_I$ and $c_T$ are image condition tokens extracted from source image $I_{src}$ and text tokens. $u_t(z|\epsilon)$ is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions, and $E$ denotes the expectation, involving integration or summation over time $t$, conditional $z$, and noise $ \epsilon $. 

% \subsection{OmniEditor 预训练}
% 我们将编辑前的图片定义为 $ I_{0} $，编辑后的图片定义为 $ I_{1} $。不同于直接在潜空间去噪的传统图像编辑方法，如SDEdit和InstructP2P，我们的方法PhotoDoodle将图像编辑概念化为条件生成问题。这里，$ I_{1} $的生成是基于 $ I_{0} $的条件。

% 我们的模型利用了基于DiT的预训练模型Flux的先进功能，并将其扩展为图像编辑工具。 $ I_{0} $ 和 $ I_{1} $ 都通过VAE被编码成它们各自的潜在表示，$ l_{0} $ 和 $ l_{1} $，然后拼接在一起以有效利用Flux的上下文学习能力。使用多模态注意力机制为涂鸦图像的去噪提供条件信息。
% \begin{equation}
% \text{MMA}([X; c_I; c_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
% \end{equation}
% 其中 $[X; c_I; c_T]$ 表示噪声令牌、图像令牌和文本令牌的拼接。这种公式支持双向注意力。

% \noindent \textbf{位置编码克隆} 为了确保一致性并防止在编辑过程中出现不希望的变化，我们引入了一种新的位置编码克隆（PEC）策略。这种策略保持 $ l_{0} $ 和 $ l_{1} $ 的位置编码相同，确保精确的位置对应并增强编辑过程中的一致性。

% 关键地，在基于 $ I_{0} $ 条件生成 $ I_{1} $ 时，$ l_{0} $ 保持无噪声。这种方法保留了 $ I_{0} $ 中的细节并阻止了不想要的修改的引入。因此，在训练过程中，噪声仅应用于 $ l_{1} $，从而确保编辑既准确又一致。

% \noindent \textbf{条件流匹配损失} 条件流匹配损失函数，对训练和优化生成模型至关重要，定义如下：
% \begin{equation}
% L_{CFM} = E_{t, p_t(z|\epsilon), p(\epsilon)} \left[ \left\| v_\Theta(z, t) - u_t(z|\epsilon) \right\|^2 \right]
% \end{equation}

% 其中 $ v_\Theta(z, t) $ 表示由神经网络权重参数化的速度场，$ u_t(z|\epsilon) $ 是模型生成的条件向量场，用于映射噪声和真实数据分布之间的概率路径，$ E $ 表示期望，涉及时间 $ t $、条件 $ z $ 和噪声 $ \epsilon $ 的积分或求和。这个期望计算所有条件下平方差的平均值，确保模型的性能在许多实例上平均，提供可靠的生成能力度量。

% \textbf{Conditional flow matching loss} The conditional flow matching loss function, integral to training and optimizing the generative model, is defined as follows:
% \begin{equation}
% LCFM = E_{t, p_t(z|\epsilon), p(\epsilon)} \left[ \left\| v_\Theta(z, t) - u_t(z|\epsilon) \right\|^2 \right]
% \end{equation}

% Where $ v_\Theta(z, t) $ represents the velocity field parameterized by the neural network's weights, $ u_t(z|\epsilon) $ is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions, and $ E $ denotes the expectation, involving integration or summation over time $ t $, conditional $ z $, and noise $ \epsilon $. This expectation calculates the average of the squared differences across all conditions, ensuring that the model's performance is averaged over many instances to provide a reliable measure of its generative capability.

% Note that the optimum of the above objective does not change when introducing a time-dependent weighting. Thus, one can derive various weighted loss functions that provide a signal towards the desired solution but might affect the optimization trajectory. For a unified analysis of different approaches, including classic diffusion formulations, we can write the objective in the following form (following Kingma \& Gao (2023)):
% \begin{equation}
% L_w(x_0) = -\frac{1}{2} E_{t \sim U(t), \epsilon \sim N(0, I)} \left[ w_t \lambda_t' \left\| \epsilon_\Theta(z_t, t) - \epsilon \right\|^2 \right]
% \end{equation}
% where $ w_t = -\frac{1}{2} \lambda_t' b_t^2 $ corresponds to LCFM.



\subsection{EditLoRA }
LoRA \cite{lora} enhances model adaptation by freezing the pre-trained model weights $ W_0 $ and inserting trainable rank decomposition matrices $ A $ and $ B $ into each layer of the model. These matrices, $ A \in \mathbb{R}^{r \times k} $ and $ B \in \mathbb{R}^{d \times r} $, where $ r \ll \min(d, k) $, are used to fit the residual weights adaptively. The forward computation integrates these modifications as follows:
\begin{equation}
y' = y + \Delta y = W_0 x + B A x
\end{equation}
where $ y \in \mathbb{R}^d $ is the output and $ x \in \mathbb{R}^k $ denotes the input. $ B \in \mathbb{R}^{d \times r} $, $ A \in \mathbb{R}^{r \times k} $ with $ r \ll \min(d, k) $. Normally, matrix $ B $ is initialized with zero.

To learn an individual artist's editing style and effectively transfer it from a small number of before-and-after image pairs, we introduce EditLoRA. Inspired by recent low-rank adaptation (LoRA) techniques \cite{makeanything, layertracer}, EditLoRA fine-tunes only a small set of trainable parameters, significantly reducing the risk of overfitting while preserving most of the pretrained model's expressive power. In our work, the general-purpose OmniEditor is trained on a large-scale paired dataset with a higher-rank lora. The EditLoRAs are lower-rank loras that specifically focus on mimicking the style and strategies of single artists in creating photo doodles. EditLoRA's training set consists of before-and-after pairwise data and corresponding text instruction, which differ from the conventional text-image pairs required for learning image generation models.

The EditLora steers the behaviour of the OmniEditor to the specified artist's style. When a new image $I_{src}$ is provided, along with the textual instructions, the model generates $I_{tar}$ that reflects both the previously learned editing capabilities and the distinctive stylistic effects from the artist.


% \subsection{EditLoRA}


% LoRA通过冻结预训练的模型权重 $ W_0 $ 并在模型的每一层中插入可训练的低秩分解矩阵 $ A $ 和 $ B $ 来增强模型适应性。这些矩阵，$ A \in \mathbb{R}^{r \times k} $ 和 $ B \in \mathbb{R}^{d \times r} $，其中 $ r \ll \min(d, k) $，用于适应性地拟合残差权重。前向计算将这些修改整合如下：
% \begin{equation}
% y' = y + \Delta y = W_0 x + B A x
% \end{equation}
% 其中 $ y \in \mathbb{R}^d $ 是输出，$ x \in \mathbb{R}^k $ 是输入，矩阵 $ B $ 通常初始化为零，以从非影响性状态开始。

% 为了学习特定艺术家的编辑风格，并有效地从少量的前后图片对中转移这种风格，我们引入了EditLoRA。受到最近低秩适应（LoRA）技术的启发，EditLoRA只微调一小部分可训练参数，显著降低了过拟合的风险，同时保留了预训练模型的大部分表达能力。基于训练在大规模配对数据集上的通用OmniEditor，EditLoRA专注于模仿单一艺术家在创建照片涂鸦中的风格和策略。与传统的图像生成模型训练不同，EditLoRA的训练集包括编辑前后的图像对，这对学习如何编辑图像至关重要。

% 在微调过程后，OmniEditor通过EditLoRA适应了选定艺术家的风格。当提供一个新图片 $I_{0}$，以及文本指导或参考线索时，模型生成的 $I_{1}$ 既反映了之前学到的编辑能力，也反映了来自艺术家的独特风格线索。


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/R-F.pdf} % Replace with your image file
    \caption{The generated results of PhotoDoodle. PhotoDoodle can mimic the manner and style of artists creating photo doodles, enabling instruction-driven high-quality image editing.}
    \label{fig3}

\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/compare.pdf} % Replace with your image file

    \caption{ Compared to baselines, PhotoDoodle demonstrates superior instruction following, image consistency, and editing effectiveness.}
    \label{fig4}

\end{figure*}


\subsection{PhotoDoodle}

In collaboration with professional artists and designers, we created the first PhotoDoodle dataset. We introduce the dataset containing six high-quality styles and over 300 photo doodle samples. The six styles include cartoon monster, hand-drawn outline, 3D effects, flowing color blocks, flat illustrator, and cloud sketch. Each sample in our dataset consists of a pre-edit photo (e.g., a real-world scene or portrait) and a post-edit photo doodle showing unique modifications by the artist, such as localized stylization, decorative lines, newly added objects, or modifications to existing elements. For each example, we store the raw input image $I_{src}$ and the doodled version $I_{tar}$, along with textual instructions.

% 我们与专业艺术家和设计师合作，收集了多样化的照片涂鸦示例。我们推出了一个包含6种高质量风格和超过300个照片涂鸦样本的新数据集，六个风格分别是cartoon monster, hand-drawn outline, 3D effects, flowing color blocks, flat illustrator, cloud sketch。我们数据集中的每个样本包括：  一张编辑前的照片（例如，真实世界的场景或肖像）。 展示艺术家独特修改的照片涂鸦，如局部风格化、装饰线条、新增对象或修改现有元素。对于每对图片，我们储存了原始输入图像（$I_{0}$）和涂鸦版本（$I_{1}$），并附上的文本标签来指示修改的性质。




% PhotoDoodle 的生成结果。 PhotoDoodle可以模仿艺术家创建照片涂鸦的方式和风格， 实现文本驱动的高质量的图像编辑。
