\section{Related Work and Problem Definition}
% traditional epidemic models and time series models
\textbf{Epidemic Forecasting Models.}
% Traditionally, epidemic forecasting usually employs classic statistical models, including ARIMA ~\cite{}, SEIR ~\cite{}, VAR ~\cite{}, etc. ARIMA model forecasts future infections by analyzing past values and error terms, projecting future data points based on the identified patterns. The SIR model separates the crowd of people based on three statuses (suspected, infected, and recovered) and models the transitions of the three statuses using differential equations. The VAR (Vector Autoregression) model captures the linear inter-dependencies among multiple time series by modeling each variable as a linear function of its own past values and the past values of all other variables in the system. More recently, deep learning models have demonstrated superior performance on time series tasks. Here we categorize them into RNN-based, MLP-based, and transformer-based models. The most commonly used RNN-based models include LSTM and GRU models, which employ gating mechanisms to regulate the flow of information through the network. To improve both efficiency and performance, MLP-based models are also developed, which simply use a linear layer or MLP layers to map the univariate history input to the prediction horizon. Despite some unique designs, Transformer-based models, including Autoformer, informer, Fedformer, etc. all transform the time series data into tokens first and then apply the self-attention mechanism to capture contextualized information. The final prediction is performed by another decoder, e.g. linear head.
Traditionally, epidemic forecasting employs models like ARIMA~\cite{sahai2020arima}, SEIR~\cite{he2020seir}, and VAR~\cite{shang2021regional}. ARIMA predicts infections by analyzing past data and errors, SEIR models population transitions using differential equations, and VAR captures linear inter-dependencies by modeling each variable based on past values. Recently, deep learning models—categorized into RNN-based, MLP-based, and transformer-based—have surpassed these methods. RNN-based models like LSTM~\cite{wang2020time} and GRU~\cite{natarajan2023outbreak} use gating mechanisms to manage information flow. MLP-based models use linear layers~\cite{zeng2023transformers} or multi-layer perceptrons~\cite{borghi2021covid,madden2024deep} for efficient data-to-prediction mapping. Transformer-based models~\cite{wu2021autoformer, zhou2021informer, zhou2022fedformer} apply self-attention to encode time series and generate predictions via a decoder. However, these models are limited in that they typically utilize data from only one type of disease without considering valuable insights and patterns from diverse disease datasets.



% pretrained time series models and epidemic models
\textbf{Pre-trained Time Series Models.}
% To further enhance the performance and enable the few-shot or zero-shot ability of the transformer-based models, pre-training on a large volume of data has become a commonly used strategy. Most of them employ self-supervised learning and aim to reconstruct the masked data ~\cite{zerveas2021transformer, rasul2023lag} or encourage alignment under different contexts~\cite{fraikin2023t, zhang2022self,yue2022ts2vec}. For example, PatchTST~\cite{nie2022time} segments data into patches first and then masks some of the patches. Eventually, the transformer encoder outputs the reconstructed segments. Compared to pre-trained models, large foundational time series models like Moment ~\cite{goswami2024moment} hold a bigger ambition to demonstrate great performance on multiple tasks, e.g. forecasting, imputation, and classification. However, such models require a large sum of data as well as computation resources. Recently, in the epidemic scenario, Kamarthi et al.~\cite{kamarthi2023pems} employed pre-training on multiple different diseases and observed improved performance on the downstream datasets, indicating the potential of pre-training in epidemic forecasting. Nevertheless, further questions including zero-shot ability remain unexplored.
To enhance performance and enable few-shot or zero-shot capabilities, transformer-based models often employ pre-training on large datasets, which typically use masked data reconstruction~\cite{zerveas2021transformer, rasul2023lag} or promote alignment across different contexts~\cite{fraikin2023t, zhang2022self, yue2022ts2vec}. For example, PatchTST~\cite{nie2022time} segments time series into patches, masks some, and reconstructs the masked segments. Larger foundational models like MOMENT~\cite{goswami2024moment} aim to excel in multiple tasks (e.g., forecasting, imputation, classification) but require substantial data and computational resources. In the epidemic context, Kamarthi et al.~\cite{kamarthi2023pems} pre-trained on various diseases, improving downstream performance and highlighting pre-training's potential in epidemic forecasting. Nevertheless, all these models overlook the influence of the environment, and zero-shot ability in epidemic forecasting, along with the factors affecting the pre-training process, remain unanswered. In this study, we introduce environment modeling and conduct a thorough analysis of these questions.
% \wei{it is important that we clearly describe the difference between these works and ours. (i.e., the novelty of ours)}

% % environment inference
% \textbf{Environment Estimation.}

% Time-Series Forecasting for Out-of-Distribution Generalization  Using Invariant Learning

% Graph Out-of-Distribution Generalization via Causal Intervention
