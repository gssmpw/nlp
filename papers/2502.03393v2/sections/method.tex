\section{Proposed Method}
In this paper, we propose a novel Environment-Aware Epidemic Pre-trained Model, using PatchTST as the backbone model. Although it is hard to directly conduct covariate adjustment without exogenous covariates $z$, it is still possible for us to infer the environment from historical data.

\textbf{Assumpiton 1:}
There exists a fixed number of environments (confounders) for an epidemic forecasting problem, and the representations of these environments are shared across time.

\subsection{Pipeline Overview}
To reduce the influence brought by temporal distribution shift, EAPEM applies reversible
instance normalization(RevIN) on the input data: $\mathbf{x}_{\text{norm}} = \text{ReversibleInstanceNorm}(\mathbf{x_{raw}})$. Then, patching is applied and divides the normalized data into $N$ patches with fixed length $L$, yielding $\left\{ x_0, x_1, x_2, ...x_N \right\} = Patching(x_{norm})$.

After that, each patch $x_i$ goes through a learnable linear projection and is added a positional embedding:
$
    \mathbf{x}_i := W x_i + pos(i)
$
. Each encoded patch \( \mathbf{x}_i \) is subsequently input into the encoders of EAPEM. As shown in Figure 2, each block of EAPEM encoder first applies self-attention to the input and acquires the contextualized representation $h$ that captures the correlations among patches: $h=f_{sa}(x)$.

Then, given an environment estimator:

\begin{align}
    z'_i=f_{env}(h_i, Z) = \sum_Z z^kp(z^k_i|h_i) ,
\end{align}

which is the weighted sum of the fixed environment representations, we simply use the dot product to combine the estimated environment with the encoded input. Such formulation models covariate adjustment:

\begin{align}
    m_i=h_i \cdot z'_i=\sum_z (h_i \cdot z_i^k) \cdot p(z_i^k|h_i) \tag{2}
\end{align}

In this way, the original covariate adjustment function is reduced to the categorical distribution $p(y | x) = \sum_z p(y | x, z) p(z)$, and $p(y|x,z)$ is modeled using $x*z$. Lastly, a feedforward neural network $f(m_i)=\sigma(W_f m_i)$ is applied to predict the target y.


\subsection{Patch-Wise Pseudo Environment Estimator}
In this section, we dive into details of how \textbf{$p(z^k_i|h_i)$} is modeled. We adopt the decomposition assumption as follows:

\textbf{Assumpiton 2:} Each sample $h$ can be decomposed into causal factors $h_c$ and spurious factors $h_s$.

Under such an assumption, the weight in Eq. 2 becomes $p(z^k_i | h_c_i, h_s_i)$. Since $h_c_i \perp z^k_i$, we have $p(z^k_i | h_s_i)$, which we model as below:

\begin{align}
     \pi_i^k = \underset{Z}{Softmax}(W_k z_i^k \cdot f_{decomp}(h_i))  \tag{3}
\end{align}

where $h_s_i=f_{decomp}(h_i)$ and $W_{k}$ maps the environment representation $z^k$ to the same dimension as $h_s_i$. In this paper, we use a simple linear function $w_h h_i$ to model $f_{decomp}$. Eventually, we can write our model as follows:

{\small
\begin{align}
    y = \sigma \{W_f \sum_{k=1}[f_{sa}(x) \cdot z^k \cdot \underset{Z}{Softmax}(W_k z^k \cdot W_hf_{sa}(x)) ]\} \tag{4}
\end{align}
}



\subsection{Contrastive Loss}
To enable robust and aligned representations of the same time steps under different contexts, we adopt a hierarchical contrastive loss. Different from the implementation from TS2Vec, we calculate the temporal and instance-wise contrastive loss of both time series embeddings $h$ and estimated environments $z'$ in a patch-wise manner.


\textbf{Instance-wise contrasting.}

{\tiny
\begin{align}
\mathcal{L}_{\text{inst}}^{(j, i)}(h) = -\log \frac{\exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}'_{j,i} \right)}{\sum_{m=1}^B \left( \exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}'_{m,i} \right) + \mathbb{1}_{j \neq m} \exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}_{m,i} \right) \right)} \tag{5}
\end{align}
}

\textbf{Temporal contrasting.}

{\tiny
\begin{align}
\mathcal{L}_{\text{temp}}^{(j, i)}(h) =-\log \frac{\exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}'_{j,i} \right)}{\sum_{i' \in \Omega} \left( \exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}'_{j,i'} \right) + \mathbb{1}_{i \neq i'} \exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}_{j,i'} \right) \right)}. \tag{6}
\end{align}
}

Similarly, the contrastive loss for the estimated environment representations is $\mathcal{L}_{\text{inst}}^{(j, i)}(z')$ and $\mathcal{L}_{\text{temp}}^{(j, i)}(z')$. Therefore, we have the final loss function as shown below:

{\small
\begin{align}
\mathcal{L}_{final} &= \mathcal{L}_{Sup}(x, y)  \nonumber \\
&+ \alpha (\mathcal{L}_{\text{inst}}(h) +\beta (\mathcal{L}_{\text{temp}}(h) \nonumber \\
&+ \gamma \{ 1/l \sum_l [\mathcal{L}_{\text{inst}}(z'^{(l)}) + \mathcal{L}_{\text{temp}}(z'^{(l)})] \} \tag{7}
\end{align}
}

where $\mathcal{L}_{Sup}(x, y)$ is the supervised loss, e.g. Mean Square Error(MSE).


\subsection{Optimization}
\subsubsection{Estimating Environment Representations}
Considering the environments, our optimization goal becomes:
{\small
\begin{align}
\theta^* = \arg\min_\theta \mathbb{E}_{z' \sim p_{tr}(z), (x, y) \sim p(Y, X \mid Z')} \left[ \left\| Y - f_\theta(X) \right\|^2 \right].
\end{align}
}

Unlike..., which learns the latent probability distribution of the environments by approximating the variational lower bound, we aim to acquire the maximum a posteriori (MAP) estimates of the representations of the environments by Expectation Maximization Algorithm ((EM)). Such a strategy does not assume a prior distribution of the environment representations.

\textbf{Expectation Step:} The E Step infers the environment representations. Specifically, we freeze the transformer encoder as well as the environment estimator, only optimizing the learnable environment representations $Z$. We also set $\alpha$, $\beta$, and $\gamma$ to 0 in this step. 

\begin{align}
    Z^{t+1} = \underset{Z}{argmax}[\mathcal{L}_{Sup}(x, y)] \tag{8}
\end{align}


\textbf{Maximization Step:} During the M step, we freeze the learnable environment variables $Z$ to optimize the parameters of the transformer encoder and the environment estimator. This gives 

\begin{align}
    \theta^{t+1} = \underset{\theta}{argmax}[\mathcal{L}_{final}(x, y)] \tag{9}
\end{align}

The pseudo code is shown in Table 1.

\begin{algorithm}
\small
\caption{Hierarchical Contrastive Loss Optimization with EM}
\label{alg:hierarchical_contrastive_em}
\begin{algorithmic}[1]
\State \textbf{Initialize}:
\State \quad Model parameters $\theta$
\State \quad Environment representations $Z$
\State \quad Hyperparameters $\alpha$, $\beta$, $\gamma$
\State \quad Learning rates $\eta_\theta$, $\eta_Z$
\Repeat
    \State \textbf{E-Step: Optimize $Z$}
    \State \quad Freeze $\theta$
    \State \quad Set $\alpha, \beta, \gamma \gets 0$
    \State \quad Compute $\mathcal{L}_{Sup}(x, y)$
    \State \quad Update $Z \gets Z - \eta_Z \nabla_Z \mathcal{L}_{Sup}$
    
    \State \textbf{M-Step: Optimize $\theta$}
    \State \quad Set $\alpha, \beta, \gamma$ to predefined values
    \State \quad Compute embeddings $h = f_\theta(X)$
    \State \quad Estimate environments $z' = g_\phi(Z)$
    
    \State \quad \textbf{Compute Contrastive Losses for $h$}:
\Until{Convergence}
\end{algorithmic}
\end{algorithm}

\subsubsection{Representation Learning}

\textbf{Self-supervised Learning.} We adopt reconstruction with random masking as our self-supervised learning method. The output $h_i$ is transformed into the reconstruction of the current patch $x_i$. In this case, EM algorithm is applied during pre-training and we use the mean of each patch's MSE loss as our final loss: $\mathcal{L}_{Sup} = 1/N \sum_{i=0} MSE(\hat{y_i}, y_i)$.


\textbf{Fine-tuning.} During fine-tuning on the downstream datasets, we fine-tune the whole model using the same EM algorithm. We replace the reconstruction head with a prediction head, which concatenates all the latent representations and maps the input to the prediction target $\hat{y}=W[h_1, h_2, ...]$: $\mathcal{L}_{Sup} = MSE(\hat{y}, y)$.


\textbf{Zero-shot.} For zero-shot forecasting, like the Moment model ~\cite{}, we retain the pre-trained reconstruction head and mask the last patch of the input to perform forecasting: $\hat{y} = \hat{y}_n$.




