\clearpage
\section{Appendix}
\subsection{Theoretical Analysis}
\label{Appendix: theory}
\subsubsection{Derivation for \textit{do}-operation}
We derive a tractable form for $\log p_\Theta(\hat{Y} | do(X))$, leveraging two rules of do-calculus.

\paragraph{Do-Calculus Rules.}
Consider a causal DAG $\mathcal{A}$ with nodes $B$, $D$, and $Z$. Let $\mathcal{A}_B$ denote the intervened graph by removing all arrows entering $B$, and $\mathcal{A}^B$ the graph by removing all arrows leaving $B$. The rules are:

1. \textit{Action/Observation Exchange}:  
   \vspace{-5pt}
   \[
   P(D | do(B), do(Z)) = P(D | do(B), Z),
   \]
   if $(D \perp\!\!\!\perp Z | B)$ in $\mathcal{A}^{Z}_{B}$.

2. \textit{Insertion/Deletion of Actions}:  
   \vspace{-5pt}
   \[
   P(D | do(B), do(Z)) = P(D | do(B)),
   \]
   if $(D \perp\!\!\!\perp Z | B) $ in $ \mathcal{A}^{BZ}$.

We consider a causal graph with variables $Z$, $X$, and $\hat{Y}$, as shown in Figure~\ref{fig: causal_graph}. Starting with the law of total probability:
\begin{equation}
\small
\label{eq:total-prob}
P(\hat{Y} | do(X)) = \int_{z} P(\hat{Y} | do(X), Z = z) P(Z = z | do(X)) dz.
\end{equation}
\textit{Step 1: Action/Observation Exchange.}  
Using $(\hat{Y} \perp\!\!\!\perp X | Z)$ in $\mathcal{A}^X$, we apply the exchange rule:
\begin{equation}
    P(\hat{Y} | do(X), Z = z) = P(\hat{Y} | X, Z = z).
\end{equation}

\textit{Step 2: Insertion/Deletion of Actions.}  
Using $(Z \perp\!\!\!\perp X)$ in $\mathcal{A}^X$, we simplify:
\begin{equation}
    P(Z = z | do(X)) = P(Z = z).
\end{equation}
Substituting these into \eqref{eq:total-prob}, we obtain:
\begin{equation}
P(\hat{Y} | do(X)) = \int_{z} P(\hat{Y} | X, Z = z) P(Z = z) dz.
\end{equation}

The result can be compactly written as:
\begin{equation}
p_\Theta(\hat{Y} | do(X)) = \mathbb{E}_{p_0(Z)} \left[ p_\theta(\hat{Y} | X, Z) \right],
\end{equation}
where $p_0(Z)$ denotes the prior distribution of environments. Do-calculus rules simplify interventional distributions by leveraging independence properties, enabling tractable objectives for causal inference.


\subsubsection{Derivation for Variational Lower Bound}
Below we show the derivation for the variational lower bound in Eq.~\eqref{Eq: lb}.
\begin{equation}
\begin{aligned}
&\quad \log \sum_z p_\Theta (\hat{Y} | X, Z = z) P(Z = z) \\
&= \log \sum_z p_\Theta (\hat{Y} | X, Z = z) p(Z = z) \frac{q_\phi(Z = z | X)}{q_\phi(Z = z | X)} \\
&
\geq \sum_z q^{t+1}_\phi(Z = z | X) \log p_\Theta (\hat{Y} | X, Z = z) \frac{p(Z = z)}{q_\phi(Z = z | X)} \quad \\
& \text{(Jensen's Inequality)}  \\
&= \sum_z q_\phi(Z = z | X) \log p_\Theta (\hat{Y} | X, Z = z) \\
& \quad\quad\quad\quad\quad\quad\quad - \sum_z q_\phi(Z = z | X) \log \frac{q_\phi(Z = z | X)}{p(Z = z)} \\
&= \mathbb{E}_{q_\phi(Z | X)} \left[ \log p_\Theta(\hat{Y} | X, Z) \right] 
- KL\left(q_\phi(Z | X) \parallel p(Z)\right) \\
\end{aligned}
\end{equation}

% \subsubsection{Proof of Theorem~\ref{theorem: e}}
% \begin{theorem}
% The optimization of the environment state $Z$ is learned through maximizing $\mathbb{E}_{p(Z)} [p(Y|Z)] = \mathbb{E}_{p(X)} \mathbb{E}_{q_\phi(Z|X=\mathbf{x})} p_\Theta(Y|X=\mathbf{x}, Z=z) $, which is equivalent to minimizing the expected reconstruction loss: 
% % \wei{do not really understand what this $\leftrightarrow$ means}
% \begin{equation}
% \mathbf{E}^{t+1} = \underset{\mathbf{E}}{\arg\min} \left[  \mathbb{E}_{\mathbf{x} \sim p(X)} \mathcal{L}_{\text{recon}}(\mathbf{x}, \hat{\mathbf{x}}) \right].
% \end{equation}
% \end{theorem}



% \begin{proof}

% In the E-step, we treat $q_\phi$ and $p_\Theta$ as oracles and freeze their parameters, meaning $p(Z|X)=q_\phi(Z|X)$ and $p(Y|X, Z)=p_\Theta(Y|X,Z)$. To find the distribution Z that aligns with the observed outcomes $Y$, we aim to maximize \textbf{the expectation of observing $Y$ given environment distribution $Z$}, which is reduced to the following objective:
% % \frac{1}{|\mathcal{D}_{tr}|}
% % \begin{equation}
% % \begin{aligned}
% %     & \sum_{\mathbf{x}} \mathbb{E}_{p(z)}[p(Y,X=x|Z=z)] \\
% %     &=\sum_{\mathbf{x}} \sum_z p(Z=z) p(Y,X=x|Z=z) \\
% %     &=\sum_{\mathbf{x}} \sum_z p(Z=z) p(X=x|Z=z) \frac{p(Y,X=x|Z=z)}{p(X=x|Z=z)} \\
% %     &=\sum_{\mathbf{x}} \sum_z p(X=x, Z=z) p(Y|X=x, Z=z) \\
% %     &=\sum_{\mathbf{x}} \sum_z p(X=x) p(Z=z|X=x) p(Y|X=x, Z=z) \\
% %     &=\sum_{\mathbf{x}} p(X=x) \mathbb{E}_{p(Z|X)} p(Y|X=x, Z=z) \\
% %     &= \mathbb{E}_{p(X)} \mathbb{E}_{p(Z|X)} p(Y|X=x, Z=z) \\
% % \end{aligned}
% % \end{equation}
% \begin{equation}
% \label{Eq: e-step}
% \begin{aligned}
%     & \mathbb{E}_{p(z)}[p(Y|Z=z)] \\
%     &=\sum_z p(Z=z) p(Y|Z=z) \\
%     &=\sum_{\mathbf{x}} \sum_z p(Z=z) p(Y,X=\mathbf{x}|Z=z) \\
%     &=\sum_{\mathbf{x}} \sum_z p(Z=z) p(X=\mathbf{x}|Z=z) \frac{p(Y,X=\mathbf{x}|Z=z)}{p(X=\mathbf{x}|Z=z)} \\
%     &=\sum_{\mathbf{x}} \sum_z p(X=\mathbf{x}, Z=z) p(Y|X=\mathbf{x}, Z=z) \\
%     &=\sum_{\mathbf{x}} \sum_z p(X=\mathbf{x}) p(Z=z|X=\mathbf{x}) p(Y|X=\mathbf{x}, Z=z) \\
%     &=\mathbb{E}_{p(X)} \mathbb{E}_{q_\phi(Z|X)} p_\Theta(Y|X, Z) \\
% \end{aligned}
% \end{equation}
% By updating the latent representations $\mathbf{E}$, we arrive at a new environment distribution $q^{t+1}_{\phi_{t}}(Z)$, which corresponds to a specific distribution of $Y$ induced by the epidemic predictor.
% % During the E-step, the parameters \( \phi \) and $\Theta$ remain fixed. At the current step, denoted as $t$, we treat \( q_{\phi_t}(Z|X)=p(Z|X) \) and $p_{\Theta_t}(Y|X, Z)=p(Y|X, Z) $. Therefore, $q_{\phi_t}$ defines the environment distribution space and $p_{\Theta_t}$ defines the distribution of the predictions. By updating the latent representations $\mathbf{E}$, we arrive at a new environment distribution $q^{t+1}_{\phi_{t}}(Z)$, which corresponds to a specific distribution of $Y$ given $X$. The primary objective is to identify the environment distribution $Z$ so that given an input $X$, the chance of observed target is maximized, which means maximizing the likelihood of $p(\hat{Y}|X)$. This objective can be expressed as:
% % \begin{equation}
% % \small
% % \begin{aligned}
% % & \quad \mathbb{E}_{p(X)}[p(\hat{Y}|X)]  \\
% % &= \sum_{\mathbf{x}} p(\hat{Y} | X = \mathbf{x}) p(X = \mathbf{x}) \\
% % &= \sum_{\mathbf{x}} p(X = \mathbf{x}) \sum_{Z} p_\Theta(\hat{Y} | X = \mathbf{x}, Z = z) p(Z = z | X = \mathbf{x}) \\
% % &= \sum_{\mathbf{x}} p(X = \mathbf{x}) \sum_{Z} p_\Theta(\hat{Y} | X = \mathbf{x}, Z = z) q_{\phi_{t}}(Z = z | X = \mathbf{x}) \\
% % &= \mathbb{E}_{p(X)} \mathbb{E}_{q_{\phi_t}(Z | X)} \left[ p_\Theta(\hat{Y} | X, Z) \right].
% % \end{aligned}
% % \end{equation}
% When the training set distribution \( p_{\text{tr}}(X) \) is limited, it may fail to accurately represent the true prior \( p(X) \). To address this, minibatch sampling is used to approximate \( p(X) \) with the sampled data distribution \( p_s(X) \). This enhances robustness by exposing the model to diverse subsets of data, reducing dependency on specific patterns, and promoting generalization. Consequently, minibatch sampling mitigates overfitting and improves the model's ability to handle noise and variability, enabling better adaptation to unseen data. In the context of pre-training, where the task involves reconstructing masked inputs, the prediction target becomes \( \hat{X} \). The optimization objective in this case is: $\mathbb{E}_{p_s(X)} \mathbb{E}_{q_{\phi_t}(Z | X)} \left[ p_\Theta(\hat{X} | X, Z) \right]$. Minimizing the reconstruction loss \( \mathcal{L}_{\text{recon}}(\mathbf{x}, \hat{\mathbf{x}}) = \text{MSE}(\hat{\mathbf{x}}, \mathbf{x}) \) ensures that the reconstructed sample \( \hat{\mathbf{x}} \) produced by \( p_\Theta(\hat{X} | X, Z) \) closely matches the ground truth \( \mathbf{x} \). This minimization directly supports maximizing the likelihood \( p(\hat{X}|Z) \), as a lower reconstruction loss corresponds to a higher probability of accurately reconstructing the target distribution, thus achieving the objective of the E-step.
% \end{proof}


\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/theory1.pdf}
        \caption{Previous Method}
        \label{fig:previous_theory}
    \end{subfigure}
    % \hspace{0.04\textwidth} % Adjusts the horizontal space between subfigures
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/theory2.pdf}
        \caption{Ours}
        \label{fig:our_theory}
    \end{subfigure}
    \caption{Comparison between previous approach and our implementation.}
    \label{fig:theory}
\end{figure*}

\subsubsection{Proof of Theorem~\ref{theorem: m}}
In this section, we provide the theoretical analysis of our optimization method, and an over is shown in Figure~\ref{fig:theory}. Before proving Theorem~\ref{theorem: m}, we prove the following theorem:
\begin{theorem}
\label{theorem: kl}
Given the environment estimator before update $q_{\phi_{t}}$ and after update $q_{\phi_{t+1}}$, assuming the training process converges, then minimizing the KL divergence from $q_{\phi_{t+1}}$ to $q_{\phi_{t}}$, i.e., $\text{KL}(q_{\phi_{t+1}} || q_{\phi_{t}})$, is equivalent to applying an L2 norm on the parameters of the environment estimator $q_{\phi}$.

% the L2 norm is applied on the parameter of the environment estimator $q_\phi(Z|X)$, then the KL divergence from $q_{\phi_{t+1}}$ to $q_{\phi_{t}}$: $\text{KL}(q_{\phi_{t+1}} || q_{\phi_{t}})$, is minimized as the training converge.

\end{theorem}
% that the L2 norm on the parameter of our environment estimator serves as an approximation for $q_{\phi_{t+1}}$ to be close to $q_{\phi_{t}}$, which corresponds to minimizing the KL divergence: $\text{KL}(q_{\phi_{t+1}} || q_{\phi_{t}})$. \wei{please write this theorem (the one you are going to prove) down. }
\begin{proof}
\label{proof: l2}
Since $q_\phi$ only involves linear transformations and a softmax function, we argue that minimizing the KL loss between $q_\phi^{t}$ and $q_\phi^{t+1}$ is equivalent to minimizing the difference between $\phi^{t}$ and $\phi^{t+1}$.

Firstly, the logits  $s_i$  are computed as: $s_i = \mathbf{W}_k^{(l)} \mathbf{e}^k$. A change in  $\mathbf{W}_k^{(l)}$ , denoted as  $\Delta \mathbf{W}_k^{(l)}$ , modifies  $s_i$  as $\Delta s_i = (\Delta \mathbf{W}_k^{(l)}) \mathbf{e}^k$. The change in  $s_i$  is linear with respect to  $\Delta \mathbf{W}_k^{(l)}$  and  $\mathbf{e}^k$ . Therefore, a small change in  $\mathbf{W}_k^{(l)}$ leads to proportionally small changes in  $s_i$.

Secondly, the Softmax function introduces nonlinear coupling between logits  $s_i$ , as the output probabilities  $\pi_i$  depend not only on  $s_i$  but also on all other logits  $s_j$ . For a small change in  $s_i$ , the change in  $\pi_i$  can be approximated using the gradient of the Softmax $\frac{\partial \pi_i}{\partial s_i} = \pi_i (1 - \pi_i)$, $\frac{\partial \pi_i}{\partial s_j} = -\pi_i \pi_j \quad \text{for } i \neq j$. Thus, a change in  $\mathbf{W}k^{(l)}$  affects  $\pi_i$  as $\Delta \pi_i \approx \pi_i (1 - \pi_i) \Delta s_i - \sum_{j \neq i} \pi_i \pi_j \Delta s_j$. This implies that the change of the output $\Delta \pi_i$ shrinks proportionally with the change of parameters $\Delta \mathbf{W}_k$.

Lastly, for small parameter changes, we can approximate the KL divergence between the two categorical distributions using a second-order Taylor expansion around \( q_\phi^{t} \). The KL divergence is defined as:
\begin{equation}
\text{KL}\left(q_\phi^{t} \parallel q_\phi^{t+1}\right) = \sum_{i} \pi_i^{t} \log \left( \frac{\pi_i^{t}}{\pi_i^{t+1}} \right).
\end{equation}
Expanding \( \log \left( \frac{\pi_i^{t}}{\pi_i^{t+1}} \right) \) around \( \pi_i^{t} \) using the Taylor series for \( \log(1 + x) \) where \( x = -\frac{\Delta \pi_i}{\pi_i^{t}} \) and keeping terms up to second order, we obtain:
\begin{equation}
\small
\begin{aligned}
\log \left( \frac{\pi_i^{t}}{\pi_i^{t+1}} \right) &= \log \left( 1 + \frac{\pi_i^{t} - \pi_i^{t+1}}{\pi_i^{t}} \right) \\
&= \log \left( 1 - \frac{\Delta \pi_i}{\pi_i^{t}} \right) \approx -\frac{\Delta \pi_i}{\pi_i^{t}} - \frac{1}{2} \left( \frac{\Delta \pi_i}{\pi_i^{t}} \right)^2.
\end{aligned}
\end{equation}
Substituting this into the KL divergence expression and ignoring higher-order terms (since \( \Delta \pi_i \) is small), we get:
\begin{equation}
\small
\begin{aligned}
    \text{KL}\left(q_\phi^{t} \parallel q_\phi^{t+1}\right) &\approx \sum_{i} \pi_i^{t} \left( -\frac{\Delta \pi_i}{\pi_i^{t}} - \frac{1}{2} \left( \frac{\Delta \pi_i}{\pi_i^{t}} \right)^2 \right) \\
    &= -\sum_{i} \Delta \pi_i - \frac{1}{2} \sum_{i} \frac{(\Delta \pi_i)^2}{\pi_i^{t}}.
\end{aligned}
\end{equation}
Since \( \sum_{i} \Delta \pi_i = 0 \) (as probabilities sum to one), the first term vanishes, leaving:
\begin{equation}
\text{KL}\left(q_\phi^{t} \parallel q_\phi^{t+1}\right) \approx \frac{1}{2} \sum_{i} \frac{(\Delta \pi_i)^2}{\pi_i^{t}}.
\end{equation}
Substituting the expression for \( \Delta \pi_i \) from above and noting that \( \Delta s_i \) is linear in \( \Delta \phi \), we observe that the KL divergence is a quadratic function of \( \Delta \phi \). Therefore, for small \( \Delta \phi \), the KL divergence can be approximated as:
\begin{equation}
    \mathcal{L}_{\text{KL}} \approx \frac{1}{2} \sum_{i} \frac{(\Delta \pi_i)^2}{\pi_i^{t}} \propto \| \Delta \phi \|^2 = \| \phi^{t} - \phi^{t+1} \|^2.
\end{equation}
Thus, minimizing the KL divergence is approximately equivalent to minimizing \( \| \phi^{t} - \phi^{t+1} \|^2 \). In our setting, we use weight decay to regularize the model, which indirectly helps control the KL loss. Weight decay adds an L2 penalty to the loss function that encourages smaller parameter values, effectively shrinking the magnitude of the weights during training. The update rule for parameters with weight decay is given by:
$\phi_{t+1} = \phi_t - \alpha \cdot \nabla_\phi \mathcal{L}_{\text{original}} - \alpha \cdot \lambda \phi_t$,
where \( \alpha \) is the learning rate and \( \lambda \) is the weight decay coefficient. The total KL loss can be approximated as:
\begin{equation}
    \mathcal{L}_{\text{kl}} \approx \| \phi^t - \phi^{t+1} \|^2 = \alpha^2 \| \nabla_\phi \mathcal{L}_{\text{original}} + \lambda \phi_t \|^2.
\end{equation}
When the training process converges to a minimum, the task-related gradients (\( \nabla_\phi \mathcal{L}_{\text{original}} \)) become small and nearly zero, and the KL loss becomes dominated by the weight decay term, and the approximation simplifies to: $\mathcal{L}_{\text{kl}} \approx \alpha^2 \lambda^2 \| \phi^t \|^2$. Thus, the L2 norm contributes to minimizing the KL loss.
\end{proof}
Next, we provide proof of the theorem for the M-Step.
\begin{theorem}
Assuming $q^{t+1}_{\phi_{t}}(Z)=p^{t+1}(Z)$ and an L2 norm is applied on $\phi$, the variational lower bound in Eq.~\eqref{Eq: lb} can be approximated as follows:
\begin{equation}
\mathbb{E}_{p(X)} \left[ \mathbb{E}_{q^{t+1}_{\phi_{t}}(Z | X)}  \left[ \log p^{t+1}_{\Theta_{t+1}}(\hat{Y} | X, Z) \right]\right] - C,
\end{equation}
which is equivalent to minimizing the expected reconstruction loss $\mathbb{E}_{\mathbf{x}\sim p(X)}[\mathcal{L}_{recon}(\mathbf{x}, \hat{\mathbf{x}})]$ .
% \wei{$\mathbb{E}_{\mathbf{x}\sim p(X)}[\mathcal{L}_{recon}(\mathbf{x}, \hat{\mathbf{x}})]$ ?}.
% \wei{we can remove the constant here by saying "maximizing the xxx can be approximated as minimizing xxx"}
\end{theorem}

\begin{proof}
In the M-step, $q_\phi(Z | X)$ serves as the estimator for the environment distribution $p^{t+1}(Z)$ defined by $q^{t+1}_{\phi_{t}}(Z)$ in the previous E-step. 
% We apply an L2 norm on $\phi_{t+1}$ to make it approximate $\phi_{t+1}$ during convergence. 
% Similarly to the E-step, we utilize random mini-batch sampling to produce the stochastic distribution $p_s(X)$, which approximates $p(X)$. 
Then, we derive the expected log-likelihood for $ \log p_\Theta (\hat{Y} | do(X)) $ from Eq.~\eqref{Eq: lb} :
{\scriptsize
\begin{equation}
\nonumber
\begin{aligned}
&\quad E_{p(X)}\log{p_\Theta (\hat{Y} | do(X))} \\
& = \sum_{\mathbf{x}} \log{p_\Theta (\hat{Y} | do(X)=\mathbf{x})} p(X=\mathbf{x}) && \\
&= \sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{\mathbf{z}} \log{p_\Theta (\hat{Y} | X=\mathbf{x}, Z=z)} p(Z=z) &&\\
&= \sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{\mathbf{z}} \log{p_\Theta (\hat{Y} | X=\mathbf{x}, Z=z)} p(Z=z) \\
& \hspace{5cm} \cdot \frac{q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x})}{q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x})}  \\ 
&\geq \sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{\mathbf{z}} q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x}) \\
& \hspace{1.5cm}\cdot \log{p_\Theta (\hat{Y} | X=\mathbf{x}, Z=z)} \frac{p(Z=z)}{q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x})}  \\ 
& \text{(Jensen's Inequality)}\\
% \end{aligned}
% \end{equation}}
% {\small
% \begin{equation}
% \begin{aligned}
&=\sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{\mathbf{z}} q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x}) && \\
& \hspace{4.2cm} \cdot \log{p_\Theta (\hat{Y} | X=\mathbf{x}, Z=z)} && \\
& \hspace{2.05cm} + \sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{\mathbf{z}} q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x}) \\
& \hspace{4.35cm} \cdot \log{\frac{p(Z=z)}{q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x})}} &&\\
&= \underbrace{\mathbb{E}_{p(X)} \mathbb{E}_{q^{t+1}_{\phi_{t}}(Z|X)} \log{p_\Theta (\hat{Y} | X, Z)}}_{-\mathcal{L}_{recon}} \\
&+ \underbrace{\sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{z} q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x}) \log{\frac{p(Z=z)}{q^{t+1}_{\phi_{t}}(Z=z|X=\mathbf{x})}}}_{-\mathcal{L}_{reg}}. &&
\end{aligned}
\end{equation}}
The first term, similar to the justification in the E-step, maximizes the predictive power of the model in a batch-wise manner, while the second term serves as a regularization of the environment estimator. Since we assume $q^{t+1}_{\phi_{t}}(Z)=p(Z)$, we prove that the second term can be further reduced:
% \vskip -5em
\begin{equation}
\small
\begin{aligned}
 &-\mathcal{L}_{reg}\\
 &=\sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_{z} q^{t+1}_{\phi_t}(Z=z|X=\mathbf{x})\\ &\hspace{4.8cm} \cdot\log{\frac{p(Z=z)}{q^{t+1}_{\phi_t}(Z=z|X=\mathbf{x})}} \\
 &= \sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_z q^{t+1}_{\phi_t}(Z=z|X=\mathbf{x}) \log{p(Z=z)} \\
 &\hspace{2.45cm}- \sum_{\mathbf{x}} p(X=\mathbf{x}) \sum_z q^{t+1}_{\phi_t}(Z=z|X=\mathbf{x})\\ 
 & \hspace{4.8cm} \cdot\log{q^{t+1}_{\phi_t}(Z=z|X=\mathbf{x})} \\
 &=\sum_z \log{p(Z=z)} \sum_{\mathbf{x}} p(X=\mathbf{x}) q^{t+1}_{\phi_t}(Z=z|X=\mathbf{x})\\
 &\hspace{6.2cm}- H_{\phi_{t}}(Z|X) \\
 &= \sum_z \log{q^{t+1}_{\phi_{t}}(Z=z)} q^{t+1}_{\phi_t}(Z=z) + H_{\phi_{t}}(Z|X) \\
 &= -H_{\phi_{t}}(Z) + H_{\phi_{t}}(Z|X) \\
 &= -I_{\phi_{t}}(Z;X),
\end{aligned}
\end{equation}

where $H_{\phi_{t}}(Z)$ and $H_{\phi_{t}}(Z|X)$ are the entropy and the conditional entropy induced by the estimator $q^{t+1}_{\phi_t}$ respectively, and $I_{\phi_{t}}$ denotes the mutual information. Since $I_{\phi^{t+1}}(Z;X)$ is approximated to a constant $C$ as $p_s(X) \approx p(X)$, we are able to acquire the final lower bound as:
\begin{equation}
\begin{aligned}
&E_{p(X)}\log{p_\Theta (\hat{Y} | do(X))} \\
&\geq \mathbb{E}_{p(X)} \mathbb{E}_{q^{t+1}_\phi(Z|X)} \log{p_\Theta (\hat{Y} | X, Z)} - C
\end{aligned}
\end{equation}
Therefore we only focus on maximizing the first term, which becomes minimizing the reconstruction loss $\mathcal{L}_{recon}$. In practice, we make $q^{t+1}_\phi$ active during this step while posing an L2 norm on its parameters to better optimize the environment estimator as well as switching to a different environment distribution space. During training, as proven in Proof~\ref{proof: l2}, the updated $q^{t+1}_{\phi_{t+1}}$ eventually approximates the $q^{t+1}_{\phi_{t}}$.

\end{proof}


\subsection{Pre-training and Downstream Datasets Details}
\label{Appendix: dataset}
In this study, we utilize a comprehensive collection of 17 distinct diseases from the United States, sourced from \href{https://projecttycho.org/}{Project Tycho}. These diseases encompass both respiratory and non-respiratory categories and serve as the foundation for pre-training three transformer-based models: \textbf{CAPE}, \textbf{PEM}, and \textbf{PatchTST}. The selection criteria for these datasets were meticulously chosen based on the following factors:

\textbf{Temporal Coverage and Geographic Representation}: We prioritized diseases with extensive time series data and coverage across multiple regions to ensure the models are trained on diverse and representative datasets.

\textbf{Consistent Sampling Rate}: All selected datasets maintain a uniform sampling rate, which is crucial for the effective training of transformer models that rely on temporal patterns.

\textbf{Data Quantity}: Diseases with larger datasets in terms of both temporal length and the number of regions were preferred to enhance the robustness and generalizability of the models.


Among the 17 diseases, five are classified as non-respiratory, providing a balanced representation that allows the models to learn from varied disease dynamics. Before the pre-training phase, each disease dataset underwent a normalization process to standardize the data scales, ensuring comparability across different diseases. Subsequently, the datasets were aggregated at the national level based on their corresponding timestamps. The details of the pre-training datasets are summarized in Table~\ref{tab:pretrain_datasets}.

\begin{table}[t!]
    \centering
    \caption{Pre-training Datasets collected from Project Tycho.}
    \label{tab:pretrain_datasets}
    \scriptsize
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Disease} & \textbf{Number of States} & \textbf{Total Length} & \textbf{Non-Respiratory} \\
        \midrule
        Gonorrhea & 39 & 37,824 & Yes &   \\
        Meningococcal Meningitis & 37 & 44,890 & No &    \\
        Varicella & 30 & 33,298 & No &   \\
        Typhoid Fever & 44 & 89,868 & Yes \\
        Acute Poliomyelitis & 47 & 74,070 & Yes &   \\
        Hepatitis B & 31 & 34,322 & Yes &   \\
        Pneumonia & 41 & 68,408 & No &   \\
        Hepatitis A & 38 & 37,303 & Yes &   \\
        Influenza & 42 & 61,622 & No &   \\
        Scarlet Fever & 48 & 129,460 & No &   \\
        Smallpox & 44 & 71,790 & No &   \\
        Tuberculosis & 39 & 95,564 & No &   \\
        Measles & 50 & 151,867 & No &   \\
        Diphtheria & 46 & 112,037 & No &   \\
        Mumps & 41 & 50,215 & No &   \\
        Pertussis & 46 & 109,761 & No &   \\
        Rubella & 7 & 6,274 & No &   \\
        \bottomrule
    \end{tabular}
\end{table}


In addition, we collect seven datasets of different types of diseases from diverse sources for downstream evaluations, which are all normalized without further processing. A summary of the downstream datasets is shown in Figure~\ref{tab:downstream_datasets}.

\begin{table}[t!]
\centering
\caption{Statistics of the downstream datasets for evaluation.}
\label{tab:downstream_datasets}
\scriptsize % reduce font size further
\setlength{\tabcolsep}{3pt} % adjust column separation if needed
\begin{tabular}{lcccc}
\toprule
\textbf{Disease} & \textbf{Number of Regions} & \textbf{Sampling Rate} & \textbf{Respiratory} & \textbf{Total Length} \\
\midrule
ILI USA   & 1   & Weekly & Yes & 966 \\
ILI Japan & 1   & Weekly & Yes & 348 \\
Measles    & 1   & Weekly & Yes & 1{,}108 \\
Dengue     & 23  & Mixed  & No & 10{,}739 \\
RSV        & 13  & Weekly & Yes & 4{,}316 \\
MPox       & 1   & Daily  & No & 876 \\
Covid      & 16  & Daily  & Yes & 12{,}800 \\
\bottomrule
\end{tabular}
\end{table}

All collected diseases can be categorized into \textbf{Respiratory} and \textbf{Non-respiratory} types, which differ in their modes of transmission:

\textbf{Respiratory.}
Respiratory diseases are transmitted primarily through the air via aerosols or respiratory droplets expelled when an infected individual coughs, sneezes, or talks. These diseases predominantly affect the respiratory system, including the lungs and throat.

\textbf{Non-respiratory.}
Non-respiratory diseases are transmitted through various other routes such as direct contact, vectors (e.g., mosquitoes, ticks), contaminated food or water, and sexual activities. These diseases can affect multiple body systems and have diverse transmission pathways unrelated to the respiratory system.

% \clearpage
\subsection{Implementation Details}
\label{Append_B}

\textbf{Settings.}  
We adopt an input length of 36~\cite{wu2023timesnet, wang2024tssurvey} and a patch size of 4 for applicable models. For the environment estimator defined in Eq.~\eqref{eq: env_estimator}, a shared weight \( w_k \) is used for all environment representations. All results are evaluated using Mean Squared Error (MSE). 
% and we measure the improvements with the following equation: $\Delta = \frac{\text{MSE}_{base} - \text{MSE}_{target}}{\text{MSE}_{base}} \times 100$, where $\Delta$ is the improvement of \textit{target} (current model) over \textit{base} (comparison model).


\noindent\textbf{Fine-Tuning.} For downstream tasks, we fine-tune the entire model using MSE loss, still employing EM for optimization. The difference is that we replace the task-specific head $h_\psi$ for pre-training, which is a linear transformation on each patch, with a head for forecasting that takes the concatenated latent representations $\mathbf{x}^{(L)}$ and maps them to the future prediction:
$\hat{\mathbf{y}} = h_\psi(\mathbf{x}^{(L)}) = \mathbf{W}\mathbf{x}^{(L)}$.

\noindent\textbf{Zero-Shot.} Once pre-trained, our CAPE framework can be directly utilized for zero-shot forecasting where the model remains frozen and no parameter is updated. Similar to the MOMENT model~\cite{goswami2024moment}, we retain the pre-trained reconstruction head and mask the last patch of the input to perform forecasting: $\hat{\mathbf{y}} = \hat{\mathbf{x}}_{[T-c: T]}$.

% We stack 4 layers of CAPE encoder in our model, with a hidden size of 512 and 4 attention heads for all layers. For the environment representations, we apply 16 environments, each of which has a size of 512. To allow a fair comparison, we also adopt the same setting for the PatchTST in terms of layers and the hidden size. For the rest of the baselines, we use the architectures as reported by other studies~\cite{wu2023tslib, kamarthi2023pems, panagopoulos2021transfer}.

% We pre-train CAPE, PEM, and PatchTST using one piece of Nvidia A100 GPU. During pre-training, we only used 70\% of the pre-train materials, which means we only used the former 70\% of data for each disease. In addition, a learning rate of 1e-5 and a dropout rate of 0.1 are applied. For the pre-training strategy used by CAPE, we adopt a weight of 0.5 for both $\alpha$ and $\beta$. Following the same setting, we use a piece of Nvidia K80 GPU for fine-tuning the entire model. Then we select the best model for testing based on the validation set. Similarly, for the baseline models, We train all of them until convergence and select the best for testing based on the validation set.

\textbf{Data Splits.} For the ILI USA, Measles, and Dengue datasets, we split the data into 60\% training, 10\% validation, and 30\% test. Other datasets are divided into 40\% training, 20\% validation, and 40\% test. During test, we use the model checkpoint with the best validation performance.

\textbf{Model Details.} We design our model by stacking 4 layers of the CAPE encoder, each with a hidden size of 512 and 4 attention heads. For environment representations, we incorporate 16 distinct environments, each encoded with a size of 512. To ensure a fair comparison, PatchTST is configured with the same number of layers and hidden size as our CAPE-based model. For all other baseline models, we adopt the architectures as reported in previous studies~\cite{wang2024deep, kamarthi2023pems, panagopoulos2021transfer}.

\textbf{Training Details.} For the training process, we pre-train CAPE, PEM, and PatchTST on a single Nvidia A100 GPU. During pre-training, we utilize only 70\% of the available training data, specifically the first 70\% of the dataset for each disease category. We set the learning rate to $1 \times 10^{-5}$ and apply a dropout rate of 0.1 to prevent overfitting. In the CAPE pre-training strategy, we assign a weight of 0.5 to $\alpha$ to balance the contributions of contrast loss to the whole loss function. A detailed illustration of our pre-training strategy is shown in Algorithm~\ref{alg:hierarchical_contrastive_em}. After pre-training, we fine-tune the entire model using a single Nvidia K80 GPU, maintaining the same hyperparameter settings for consistency. The best-performing model is selected based on its performance on the validation set. Similarly, for all baseline models, we train each until convergence and select the optimal model based on validation set performance for the subsequent test.

\begin{algorithm}
\small
\caption{Hierarchical Contrastive Loss Optimization with EM}
\label{alg:hierarchical_contrastive_em}
\begin{algorithmic}[1]
\State \textbf{Input:} $\mathcal{D}_{tr} = \{(\mathbf{x}_i, \mathbf{y}_i)\}_{i=1}^N$, $\alpha$, $\eta_{\theta, \psi, \phi}$, $\eta_{\mathbf{E}}$, $T_{max}$
\State \textbf{Initialize:} $\theta$, $\psi$, $\phi$, $\mathbf{E} \sim \text{Orthogonal}(\mathbf{I})$
\Repeat
    \State \textbf{E-Step: Optimize Environment Representations $\mathbf{E}$}
    \State \quad Freeze $(\theta, \psi)$ and set $\alpha \gets 0$
    \State \quad Sample mini-batch $\mathcal{B} \subseteq \mathcal{D}_{tr}$
    \State \quad Compute reconstruction loss: $\mathcal{L}_{recon}$
    \State \quad Compute gradient: $\nabla_{\mathbf{E}} \mathcal{L}_{recon}$
    \State \quad Update $\mathbf{E}$: $\mathbf{E} \gets \mathbf{E} - \eta_{\mathbf{E}} \nabla_{\mathbf{E}} \mathcal{L}_{recon}$
    
    \State \textbf{M-Step: Optimize Model Parameters $(\theta, \psi, \phi)$}
    \State \quad Freeze $\mathbf{E}$, set $\alpha$ to predefined values
    \State \quad Sample mini-batch $\mathcal{B} \subseteq \mathcal{D}_{tr}$
    \State \quad Compute contrastive loss: $\mathcal{L}_{CL}$
    \State \quad Compute total loss: $\mathcal{L}_{final} = \mathcal{L}_{recon} + \alpha \mathcal{L}_{CL}$
    \State \quad Update parameters:
    \State \quad \quad \quad $(\theta, \psi, \phi) \gets (\theta, \psi, \phi) - \eta_{\theta, \psi, \phi} \nabla_{\theta, \psi, \phi} \mathcal{L}_{final}$
\Until{$t = T_{max}$}
\State \textbf{Output:} $(\theta, \psi, \phi), \mathbf{E}$
\end{algorithmic}
\end{algorithm}

% \clearpage
\subsection{Full Results on Pre-train datasets}
\label{Append_C}
In addition to evaluating the performance of the models on downstream datasets, we also provide the in-domain evaluation results from the pre-training datasets. Recall that we used 70\% data of each disease for pre-training, here we fine-tuned the model on the 70\% of each disease and evaluate both CAPE and the pre-trained PatchTST on the rest 30\% data. As shown in Table~\ref{tab:eapem_patchtst_scores_avg}, CAPE consistently outperforms PatchTST on 13/15 datasets, proving the effectiveness of our method.

\begin{table*}[htbp]
\centering
\caption{Performance of CAPE and pre-trained PatchTST across diseases in the pre-training datasets. The results presented is the average over horizons of {1,2,4,8,16}.}
\label{tab:eapem_patchtst_scores_avg}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llcccccc}
\toprule
 \textbf{Disease} & \textbf{Method} & {Horizon 1} & {Horizon 2} & {Horizon 4} & {Horizon 8} & {Horizon 16} & \textbf{Average}  \\
\midrule

\multirow{2}{*}{\textbf{Mumps}} 
    & CAPE & 0.000284 & 0.000290 & 0.000370 & 0.000451 & 0.000539 & \textbf{\textcolor{red}{0.000387}} \\
    & PatchTST 
        & 0.000280 & 0.000310 & 0.000388 & 0.000508 & 0.000627 & \textbf{0.000423} \\
\midrule

\multirow{2}{*}{\textbf{Meningococcal Meningitis}} 
    & CAPE 
        & 0.063022 & 0.066196 & 0.073552 & 0.093547 & 0.108842 & \textbf{0.081032} \\
    & PatchTST 
        & 0.054611 & 0.061641 & 0.073794 & 0.088404 & 0.096449 & \textbf{\textcolor{red}{0.074980}} \\
\midrule

\multirow{2}{*}{\textbf{Influenza}} 
    & CAPE 
        & 0.367677 & 0.510453 & 0.693110 & 0.903920 & 1.037177 & \textbf{\textcolor{red}{0.702467}} \\
    & PatchTST 
        & 0.392925 & 0.644013 & 0.717147 & 0.851498 & 1.061066 & \textbf{0.733330} \\
\midrule

\multirow{2}{*}{\textbf{Hepatitis B}} 
    & CAPE 
        & 0.071834 & 0.072827 & 0.074606 & 0.077816 & 0.068012 & \textbf{\textcolor{red}{0.073019}} \\
    & PatchTST 
        & 0.074016 & 0.082576 & 0.084535 & 0.085867 & 0.074103 & \textbf{0.080219} \\
\midrule

\multirow{2}{*}{\textbf{Pneumonia}} 
    & CAPE 
        & 0.038916 & 0.052092 & 0.082579 & 0.137004 & 0.191675 & \textbf{\textcolor{red}{0.100453}} \\
    & PatchTST 
        & 0.036961 & 0.074596 & 0.096963 & 0.152206 & 0.174871 & \textbf{0.107119} \\
\midrule

\multirow{2}{*}{\textbf{Typhoid Fever}} 
    & CAPE 
        & 0.004918 & 0.004393 & 0.004552 & 0.005051 & 0.005828 & \textbf{\textcolor{red}{0.004948}} \\
    & PatchTST 
        & 0.007068 & 0.005954 & 0.005906 & 0.006519 & 0.006709 & \textbf{0.006431} \\
\midrule

\multirow{2}{*}{\textbf{Hepatitis A}} 
    & CAPE 
        & 0.347792 & 0.349403 & 0.352361 & 0.360705 & 0.315496 & \textbf{\textcolor{red}{0.345151}} \\
    & PatchTST 
        & 0.331339 & 0.349549 & 0.356113 & 0.381637 & 0.338067 & \textbf{0.351341} \\
\midrule

\multirow{2}{*}{\textbf{SCAPEet Fever}} 
    & CAPE 
        & 4.229920 & 5.258288 & 6.787577 & 10.865951 & 13.724634 & \textbf{\textcolor{red}{8.173274}} \\
    & PatchTST 
        & 8.561295 & 13.564009 & 17.241462 & 19.315905 & 20.373520 & \textbf{15.811238} \\
\midrule

\multirow{2}{*}{\textbf{Gonorrhea}} 
    & CAPE 
        & 0.010826 & 0.010900 & 0.011246 & 0.011483 & 0.011898 & \textbf{\textcolor{red}{0.011271}} \\
    & PatchTST 
        & 0.011297 & 0.012223 & 0.013411 & 0.013438 & 0.013241 & \textbf{0.012722} \\
\midrule

\multirow{2}{*}{\textbf{Smallpox}} 
    & CAPE 
        & 0.063829 & 0.065191 & 0.076199 & 0.098973 & 0.157850 & \textbf{\textcolor{red}{0.092408}} \\
    & PatchTST 
        & 0.070972 & 0.076843 & 0.107076 & 0.124042 & 0.165442 & \textbf{0.108875} \\
\midrule

\multirow{2}{*}{\textbf{Acute Poliomyelitis}} 
    & CAPE 
        & 0.254014 & 0.394454 & 0.355898 & 0.480525 & 0.745428 & \textbf{0.446064} \\
    & PatchTST 
        & 0.094695 & 0.134304 & 0.270908 & 0.392511 & 0.482426 & \textbf{\textcolor{red}{0.274969}} \\
\midrule

\multirow{2}{*}{\textbf{Diphtheria}} 
    & CAPE 
        & 0.006789 & 0.005360 & 0.006557 & 0.010682 & 0.014136 & \textbf{\textcolor{red}{0.008705}} \\
    & PatchTST 
        & 0.011019 & 0.008891 & 0.009036 & 0.013048 & 0.015531 & \textbf{0.011505} \\
\midrule

\multirow{2}{*}{\textbf{Varicella}} 
    & CAPE 
        & 0.000119 & 0.000128 & 0.000154 & 0.000212 & 0.000245 & \textbf{\textcolor{red}{0.000171}} \\
    & PatchTST 
        & 0.000109 & 0.000141 & 0.000169 & 0.000237 & 0.000296 & \textbf{0.000190} \\
\midrule

\multirow{2}{*}{\textbf{Tuberculosis}} 
    & CAPE 
        & 0.178741 & 0.170441 & 0.215367 & 0.177671 & 0.198068 & \textcolor{red}{\textbf{0.188057}} \\
    & PatchTST 
        & 0.189156 & 0.209008 & 0.189944 & 0.204680 & 0.277632 & \textbf{0.214084} \\
\midrule

\multirow{2}{*}{\textbf{Measle}} 
    & CAPE 
        & 0.009626 & 0.010982 & 0.016451 & 0.022407 & 0.042980 & \textbf{\textcolor{red}{0.020489}} \\
    & PatchTST 
        & 0.013008 & 0.012608 & 0.020903 & 0.039835 & 0.063844 & \textbf{0.030039} \\
\bottomrule
\end{tabular}
}
\end{table*}

% \clearpage
\subsection{Full results for few-shot forecasting}
\label{Append_few_shot}

We present the complete few-shot performance across different horizons in Table~\ref{tab: few_shot}. While CAPE does not achieve state-of-the-art average performance on the ILI USA dataset with limited training data, it excels in short-term forecasting when the horizon is smaller.

\begin{table*}[htbp]
    \centering
    \caption{Few-shot learning results with horizons ranging from 1 to 16 future steps. The length of the lookback window is set to 36. Each model is evaluated after being trained on 20\%, 40\%, 60\% and 80\% of the full training data.}
    \label{tab: few_shot}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{ll*{5}{c}*{5}{c}*{5}{c}*{5}{c}*{5}{c}}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Horizon}} & \multicolumn{5}{c}{\textbf{CAPE}} & \multicolumn{5}{c}{\textbf{PatchTST}} & \multicolumn{5}{c}{\textbf{Dlinear}} & \multicolumn{5}{c}{\textbf{MOMENT}} & \multicolumn{5}{c}{\textbf{PEM}} \\
        \cmidrule(lr){3-7} \cmidrule(lr){8-12} \cmidrule(lr){13-17} \cmidrule(lr){18-22} \cmidrule(lr){23-27}
        & & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} & \textbf{100\%} 
        & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} & \textbf{100\%}
        & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} & \textbf{100\%}
        & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} & \textbf{100\%}
        & \textbf{20\%} & \textbf{40\%} & \textbf{60\%} & \textbf{80\%} & \textbf{100\%} \\
        \midrule
        \multirow{6}{*}{ILI USA} 
            & 1  & 1.155 & 0.535 & 0.307 & 0.178 & 0.155 & 1.361 & 0.662 & 0.355 & 0.191 & 0.195 & 1.430 & 1.000 & 0.460 & 0.230 & 0.170 & 2.859 & 1.274 & 0.608 & 0.267 & 0.216 & 1.424 & 0.620 & 0.330 & 0.189 & 0.145 \\
            & 2  & 1.396 & 0.925 & 0.465 & 0.220 & 0.200 & 1.389 & 0.806 & 0.489 & 0.234 & 0.264 & 2.210 & 1.090 & 0.660 & 0.280 & 0.220 & 3.242 & 1.709 & 0.695 & 0.342 & 0.271 & 1.463 & 0.829 & 0.434 & 0.256 & 0.210 \\
            & 4  & 1.770 & 1.154 & 0.640 & 0.306 & 0.270 & 1.923 & 1.215 & 0.656 & 0.387 & 0.385 & 2.500 & 1.670 & 0.720 & 0.380 & 0.310 & 3.910 & 1.901 & 0.891 & 0.399 & 0.356 & 1.889 & 1.186 & 0.625 & 0.393 & 0.312 \\
            & 8  & 2.611 & 1.912 & 0.978 & 0.519 & 0.404 & 2.713 & 1.623 & 0.833 & 0.544 & 0.535 & 3.510 & 1.970 & 0.980 & 0.530 & 0.450 & 4.706 & 2.013 & 1.120 & 0.615 & 0.482 & 2.649 & 1.690 & 0.966 & 0.580 & 0.573 \\
            & 16 & 3.674 & 2.473 & 1.411 & 0.622 & 0.516 & 3.182 & 1.789 & 1.056 & 0.649 & 0.485 & 4.460 & 2.240 & 1.260 & 0.640 & 0.580 & 5.233 & 2.335 & 1.251 & 0.669 & 0.580 & 3.294 & 1.979 & 1.049 & 0.679 & 0.526 \\
            & Avg & \textbf{2.121} & \textbf{1.400} & \textbf{0.760} & \cellcolor{low1}\textbf{0.369} & \cellcolor{low1}\textbf{0.309} 
            & \textbf{2.114} & \textbf{1.219} & \textbf{0.677} & \textbf{0.401} & \textbf{0.373} 
            & \textbf{2.822} & \textbf{1.594} & \textbf{0.816} & \textbf{0.412} & \textbf{0.346} 
            & \textbf{3.990} & \textbf{1.847} & \textbf{0.913} & \textbf{0.459} & \textbf{0.381} 
            & \textbf{2.143} & \textbf{1.261} & \textbf{0.681} & \textbf{0.419} & \textbf{0.353} \\
        \midrule
        \multirow{6}{*}{Dengue} 
            & 1  & 3.254 & 1.384 & 0.489 & 0.384 & 0.218 & 3.700 & 1.580 & 0.657 & 0.389 & 0.203 & 3.600 & 1.470 & 0.550 & 0.350 & 0.220 & 4.585 & 2.480 & 0.689 & 0.423 & 0.383 & 3.383 & 1.613 & 0.558 & 0.350 & 0.206 \\
            & 2  & 4.463 & 2.340 & 0.735 & 0.487 & 0.301 & 5.832 & 2.159 & 0.846 & 0.507 & 0.296 & 7.090 & 2.170 & 0.820 & 0.510 & 0.310 & 6.609 & 2.990 & 0.922 & 0.587 & 0.521 & 5.404 & 2.257 & 0.869 & 0.507 & 0.300 \\
            & 4  & 7.563 & 3.728 & 1.250 & 0.817 & 0.540 & 9.525 & 3.636 & 1.517 & 1.069 & 0.588 & 11.190 & 4.130 & 1.520 & 0.940 & 0.560 & 12.877 & 4.106 & 1.644 & 0.966 & 0.669 & 8.782 & 4.428 & 1.608 & 1.037 & 0.522 \\
            & 8  & 15.526 & 7.276 & 2.836 & 1.922 & 1.193 & 19.052 & 9.530 & 3.597 & 2.133 & 1.296 & 21.910 & 9.690 & 3.470 & 2.160 & 1.250 & 23.298 & 9.229 & 3.625 & 2.135 & 1.235 & 17.023 & 8.117 & 3.323 & 2.249 & 1.295 \\
            & 16 & 35.870 & 17.204 & 6.469 & 3.946 & 2.210 & 30.451 & 19.616 & 7.238 & 4.289 & 2.536 & 35.350 & 24.640 & 7.890 & 4.780 & 3.060 & 31.115 & 18.877 & 7.200 & 4.551 & 3.984 & 29.934 & 18.861 & 7.368 & 4.390 & 2.497 \\
            & Avg & \textbf{13.335} & \cellcolor{low1}\textbf{6.386} & \cellcolor{low1}\textbf{2.356} & \cellcolor{low1}\textbf{1.511} & \cellcolor{low1}\textbf{0.892} 
            & \textbf{13.712} & \textbf{7.304} & \textbf{2.771} & \textbf{1.678} & \textbf{0.984} 
            & \textbf{15.828} & \textbf{8.420} & \textbf{2.850} & \textbf{1.748} & \textbf{1.080} 
            & \textbf{15.697} & \textbf{7.536} & \textbf{2.816} & \textbf{1.733} & \textbf{1.358} 
            & \textbf{12.90} & \textbf{7.055} & \textbf{2.745} & \textbf{1.707} & \textbf{0.964} \\
        \midrule
        \multirow{6}{*}{Measles} 
            & 1  & 0.168 & 0.158 & 0.107 & 0.095 & 0.069 & 0.400 & 0.217 & 0.121 & 0.091 & 0.094 & 0.560 & 0.470 & 0.190 & 0.150 & 0.100 & 1.211 & 0.316 & 0.138 & 0.108 & 0.102 & 0.227 & 0.200 & 0.106 & 0.106 & 0.084 \\
            & 2  & 0.229 & 0.256 & 0.165 & 0.134 & 0.096 & 0.511 & 0.325 & 0.186 & 0.148 & 0.127 & 0.680 & 0.400 & 0.320 & 0.220 & 0.150 & 1.376 & 0.367 & 0.159 & 0.167 & 0.138 & 0.313 & 0.339 & 0.155 & 0.153 & 0.127 \\
            & 4  & 0.371 & 0.399 & 0.267 & 0.198 & 0.155 & 0.663 & 0.510 & 0.297 & 0.243 & 0.205 & 1.050 & 0.920 & 0.360 & 0.310 & 0.240 & 1.444 & 0.516 & 0.278 & 0.228 & 0.196 & 0.497 & 0.451 & 0.258 & 0.240 & 0.196 \\
            & 8  & 0.564 & 0.776 & 0.451 & 0.339 & 0.280 & 1.050 & 1.269 & 0.479 & 0.414 & 0.378 & 1.580 & 1.340 & 0.660 & 0.540 & 0.450 & 1.895 & 1.181 & 0.507 & 0.386 & 0.883 & 0.865 & 1.213 & 0.487 & 0.441 & 0.382 \\
            & 16 & 1.086 & 1.408 & 0.917 & 0.658 & 0.743 & 1.692 & 1.847 & 1.157 & 0.900 & 0.723 & 2.100 & 2.520 & 1.480 & 1.170 & 1.030 & 2.379 & 2.192 & 1.041 & 1.468 & 1.183 & 1.448 & 2.275 & 1.145 & 0.880 & 0.740 \\
            & Avg & \cellcolor{low1}\textbf{0.483} & \cellcolor{low1}\textbf{0.600} & \cellcolor{low1}\textbf{0.381} & \cellcolor{low1}\textbf{0.285} & \cellcolor{low1}\textbf{0.269}
            & \textbf{0.863} & \textbf{0.834} & \textbf{0.448} & \textbf{0.359} & \textbf{0.306} 
            & \textbf{1.194} & \textbf{1.130} & \textbf{0.602} & \textbf{0.478} & \textbf{0.394} 
            & \textbf{1.661} & \textbf{0.915} &  \textbf{0.425} & \textbf{0.471} & \textbf{0.500} 
            & \textbf{0.670} & \textbf{0.896} & \textbf{0.430} & \textbf{0.364} & \textbf{0.306} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}




\subsection{Impact of pre-train ratio on the downstream datasets}
\label{Append_D}

We provide additional evaluations for CAPE on downstream datasets to analyze the impact of the pre-training ratio. As shown in Figure~\ref{fig:pretrain_scale}, increasing the pre-training ratio eventually improves downstream performance across all datasets.


\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pretrain_ratio_Dengue.pdf}
        \caption{Dengue}
        \label{fig:pretrain_dengue}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pretrain_ratio_Measles.pdf}
        \caption{Measles}
        \label{fig:pretrain_measles}
    \end{subfigure}
    
    \vspace{0.5cm} % Adds vertical space between the rows
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pretrain_ratio_ILI_USA.pdf}
        \caption{ILI USA}
        \label{fig:pretrain_ili_usa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/pretrain_ratio_Covid_daily.pdf}
        \caption{COVID}
        \label{fig:pretrain_covid}
    \end{subfigure}
    
    \caption{Downstream performance with different ratios of pre-training datasets. The input length is set to 36 and all MSE results are averaged over \{1,2,4,8,16\} future steps.}
    \label{fig:pretrain_scale}
\end{figure*}


% \clearpage
\subsection{Visualization of the Estimated Environments}
\label{Append_E}

According to $\hat{\mathbf{e}}^{(l)}=\sum_{k=1}^{K}\mathbf{e}_k \pi_{k}^{(l)}$, an aggregated environment is the weighted sum of the learned latent environment representations. Therefore, the estimation shares the same latent space as the fixed representations and we are able to visualize them using t-SNE. As shown in Figure \ref{fig: envs}, we visualize the aggregated environments (Estimated) as well as the learned latent environment (Anchor) from a CAPE model with 8 environments. 
% However, in some cases, a model that is not well trained could show degenerated behavior, outputting the same estimations regardless of any input.


\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/envs/tsne_ILI_USA.pdf}
        \caption{ILI USA}
        \label{fig:env_ILI_USA}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/envs/tsne_Measles.pdf}
        \caption{Measles}
        \label{fig:env_Measles}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/envs/tsne_Dengue.pdf}
        \caption{Dengue}
        \label{fig:env_Dengue}
    \end{subfigure}
    
    
    \caption{Visualization of the estimated environment representations using t-SNE. }
    % \wei{enlarge the fonts of xticks, yticks, legends, xlabel, and ylabel; remove the title on top of the figure. please follow this to revise other figures}
    \label{fig: envs}
\end{figure*}



% \textbf{Clustering}
% \textbf{Correlation Map}

% \clearpage
\subsection{Visualization of Distribution Shift for Downstream Datasets}
\label{Append: ds}

We provide a visualization of the sample distribution used in this study. Each sample has a fixed length of 36, representing the historical infection trajectory. To better understand the distributional differences, we use t-SNE to reduce the data to one dimension and visualize the training and test samples using different colors. As shown in Figure~\ref{fig: ds}, a significant distribution shift is visually apparent across most datasets. To quantitatively assess the distributional differences between the training and test sets, we calculate the Central Moment Discrepancy (CMD) score~\cite{zellinger2017central}. The CMD score measures the discrepancy between the central moments of the two distributions up to a specified order \( K \). For two distributions \( X \) (training set) and \( X_{\text{test}} \) (test set), the CMD score is defined as:

\begin{equation}
\begin{aligned}
&\quad \text{CMD}(X, X_{\text{test}}) \\
&= \| \mu_1(X) - \mu_1(X_{\text{test}}) \|_2 + \sum_{k=2}^{K} \| \mu_k(X) - \mu_k(X_{\text{test}}) \|_2,
\end{aligned}
\end{equation}

where:\( \mu_k(X) \) denotes the \( k \)-th central moment of \( X \), defined as: $\mu_k(X) = \mathbb{E}[(X - \mathbb{E}[X])^k],$ and similarly for \( \mu_k(X_{\text{test}}) \). \( \| \cdot \|_2 \) is the Euclidean norm. \( K \) is the maximum order of moments considered. The CMD score aggregates the differences in the mean (first moment) and higher-order moments (e.g., variance, skewness), providing a robust measure of the distribution shift. In our experiments, we set \( K = 3 \) to capture up to the third-order central moments. This score quantitatively complements the visual observations in Figure~\ref{fig: ds}, offering a more comprehensive understanding of the distributional differences between training and test sets.

\textbf{Impact of Distribution Shifts.}
Distribution shifts between training and test datasets pose significant challenges to the generalizability and robustness of predictive models. When the underlying data distributions differ, models trained on the training set may struggle to maintain their performance on the test set, leading to reduced accuracy and reliability. These discrepancies can arise from various factors, such as temporal changes in infection patterns or geographical variations. In this paper, we assume that the inherent infection pattern of a particular disease remains constant, and the distribution shifts for the disease are primarily caused by the rapidly changing environment, which results in diverse infection patterns. In the context of epidemic modeling, such shifts are especially critical, as they can undermine the model’s ability to accurately predict future infection trends, which is essential for effective public health interventions.



\begin{figure*}[htbp]
\centering
\includegraphics[scale=0.3]{figures/full_ds_1D.pdf}
\caption{ The KDE plot of training set and test set. Each sample contains an infection trajectory of 36 weeks. t-SNE is applied to visualize the distributions of both sets.
% \zw{Not sure if this is necessary, but I added this figure to support challenge 2 about the existence of distribution shift.}
}
\label{fig: ds}
\end{figure*}


% \clearpage
\subsection{Latent space visualization of Measle and Covid datasets from pre-trained models.}
\label{Append_latent}

In order to demonstrate that CAPE effectively disentangles the underlying dynamics of diseases from the influence of the environment, we visualize the output embeddings for the Measles and COVID datasets by projecting them into a two-dimensional space using t-SNE. Specifically, we utilize the pre-trained model without fine-tuning on these two downstream datasets and visualize \( \mathbf{x}^{(L)} \), the final-layer embeddings, as individual data points in the figure. As shown in Figure~\ref{fig: latent}, CAPE (left) visually separates the two datasets more effectively than the pre-trained PatchTST model (right). To quantitatively evaluate the separability of the embeddings, we compute the Davies–Bouldin Index (DBI), which is defined as:

\begin{equation}
\text{DBI} = \frac{1}{K} \sum_{i=1}^K \max_{j \neq i} \left( \frac{\sigma_i + \sigma_j}{\| \mu_i - \mu_j \|} \right),
\end{equation}

where \( K \) is the number of clusters (in this case, two: Measles and COVID), \( \mu_i \) is the centroid of cluster \( i \), \( \sigma_i \) is the average intra-cluster distance for cluster \( i \), defined as: $\sigma_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} \|\mathbf{x} - \mu_i\|$, where \( C_i \) is the set of points in cluster \( i \), $ \| \mu_i - \mu_j \| $ is the Euclidean distance between the centroids of clusters \( i \) and \( j \). The DBI measures the ratio of intra-cluster dispersion to inter-cluster separation. Lower DBI values indicate better separability. As shown in Figure~\ref{fig: latent}, CAPE achieves a significantly lower DBI compared to PatchTST, confirming its superior ability to disentangle the underlying disease dynamics from environmental factors. A more complete result is shown in Figure~\ref{fig: dbi}.





\onecolumn
\subsection{Hyper-parameter Sensitivity Analysis}
\label{Appendix: hp}

While we treat $\hat{\mathbf{E}}_{j,c}^{(l)}=\hat{\mathbf{e}}^{(l)}_c \odot \mathbf{h}^{(l)}_c$ in the contrastive loss and aligned the combined representation of the input and environment, we further applied the contrastive loss on the aggregated environment representations at each layer, denoted as $\Tilde{{\mathbf{E}}}_{j,c}^{(l)}=\hat{\mathbf{e}}^{(l)}_c$ and assigned a hyper-parameter $\beta$ to control for its weight:
% \begin{equation}
% \label{eq: sensitivity}
% \mathcal{L}_{\text{final}} = \mathcal{L}_{\text{recon}}(\mathbf{X}, \hat{\mathbf{X}}) + \alpha \mathcal{L}_{\text{CL}}(\mathbf{X}^{(L)}) + \frac{\beta}{L} \sum_l \mathcal{L}_{\text{CL}}(\hat{\mathbf{E}}^{(l)}),
% \end{equation}

{\small
\begin{equation}
\label{eq: sensitivity}
\begin{aligned}
  \mathcal{L}_{\text{final}} &= \sum_{\mathbf{x} \in \mathbf{X}}\mathcal{L}_{\text{recon}}(\mathbf{x}, \hat{\mathbf{x}}) + \\
  &\alpha \, \mathcal{L}_{\text{CL}}(\hat{\mathbf{E}}^{(L)}, \hat{\mathbf{E}'}^{(L)}) + \frac{\beta}{L} \sum_l \mathcal{L}_{\text{CL}}(\Tilde{{\mathbf{E}}}^{(l)}, \Tilde{{\mathbf{E}}'}^{(l)}), \\
&\mathbf{X} \sim \mathcal{D}'_s \in \mathcal{D}_\text{pre}   
\end{aligned}
\end{equation}}

where $L$ denotes the number of layers. Using Equation~\eqref{eq: sensitivity}, we further fine-tune our pre-trained model by varying $\alpha$ and $\beta$ across the values [1e-3, 1e-2, 1e-1, 1, 10]. The results of this sensitivity analysis are presented in Figure~\ref{fig: hp}.

% \begin{figure}[ht!]
% \centering
% \includegraphics[scale=0.4]{figures/hp_analysis.pdf}
% \caption{ Hyperparameters sensitivity of $\alpha$ and $\beta$.}
% \label{fig: hp}
% \end{figure}


\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hp_analysis_ILI_USA.pdf}
        \caption{ILI USA}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hp_analysis_Measles.pdf}
        \caption{Measles}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hp_analysis_ILI_Japan.pdf}
        \caption{ILI Japan}
    \end{subfigure}
    
    \caption{ Hyperparameters sensitivity of $\alpha$ and $\beta$.}
    \label{fig: hp}
\end{figure*}



\begin{figure*}[htbp]
\centering
\includegraphics[scale=0.3]{figures/example_disentangle.pdf}
\caption{ Output latent space of two pre-trained models without fine-tuning from Measle and Covid datasets. Left: CAPE; Right: Pre-trained PatchTST.}
\label{fig: latent}
\end{figure*}



% \subsection{Visualization of Epidemic Time Series and Prediction Results}
% \label{Append_H}
% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/draw/Measles_h1.pdf}
%         \caption{Measles: horizon=1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/draw/Measles_h2.pdf}
%         \caption{Measles: horizon=2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/draw/Measles_h4.pdf}
%         \caption{Measles: horizon=4}
%     \end{subfigure}

%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/draw/ILI_USA_h1.pdf}
%         \caption{ILI USA: horizon=1}
%         \label{fig: preds_ILI_USA_h1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/draw/ILI_USA_h2.pdf}
%         \caption{ILI USA: horizon=2}
%         \label{fig: preds_ILI_USA_h2}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/draw/ILI_USA_h4.pdf}
%         \caption{ILI USA: horizon=4}
%         \label{fig: preds_ILI_USA_h4}
%     \end{subfigure}


%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/draw/ILI_Japan_h1.pdf}
%     %     \caption{ILI Japan: horizon=1}
%     % \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/draw/ILI_Japan_h2.pdf}
%     %     \caption{ILI Japan: horizon=2}
%     % \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/draw/ILI_Japan_h4.pdf}
%     %     \caption{ILI Japan: horizon=4}
%     % \end{subfigure}


    
%     \caption{Prediction results of Measles and ILI USA, with horizons from 1 to 4 weeks. }
%     \label{fig: preds}
% \end{figure*}


% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_0_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_1_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_2_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_3_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_4_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_5_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_6_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_7_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_8_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_10_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_11_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_12_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_13_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_14_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/covid_daily/r_15_Covid_daily_h2.pdf}
%     \end{subfigure}
%     \hfill

    
%     \caption{Prediction results of the Covid-19 dataset, which consists of multiple regions. Here we visualize the result with a horizon of 2 time steps. }
%     \label{fig: preds_dengue}
% \end{figure*}



% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_1_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_2_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_3_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_4_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_5_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_6_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_7_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_8_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_9_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/dengue/r_10_Dengue_h2.pdf}
%     % \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_11_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_12_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_13_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_14_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_15_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_16_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_17_Dengue_h2.pdf}
%     \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/dengue/r_18_Dengue_h2.pdf}
%     % \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_19_Dengue_h2.pdf}
%     \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.33\textwidth}
%     %     \centering
%     %     \includegraphics[width=\linewidth]{figures/dengue/r_20_Dengue_h2.pdf}
%     % \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.33\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/dengue/r_22_Dengue_h2.pdf}
%     \end{subfigure}
%     \hfill

    
%     \caption{Prediction results of Dengue dataset, which consists of multiple regions. Here we visualize the result with a horizon of 2 time steps. }
%     \label{fig: preds_dengue}
% \end{figure*}



