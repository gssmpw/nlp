% \section{Problem Definition}
% univariate setting

% \wei{Standardize the notations: (1) bold upper-case letters for matrices; (2) bold lower-case letters for vectors; (3) regular letters for scalars (typically lower cases while upper cases are fine). You can take a look at the itransformer or ProGNN paper}

% \wei{Comma or period at the end of equations?  If there are texts following the equation, then use a comma. Otherwise, use a period.}

% \wei{what do we mean by the dimension of a single time point?}
% \textbf{Pre-training for Epidemic Forecasting.}

\underline{\textbf{Problem Definition.}} In this study, we adopt a univariate setting: Given a historical time series input: $\mathbf{x} \in \mathbb{R}^{T \times 1}$, where $T$ is the size of lookback window, the goal of epidemic forecasting is to map $\mathbf{x}$ into target trajectories (e.g. infection rates): $\mathbf{y} \in \mathbb{R}^{h}$, where $h$ denotes the size of the forecast horizon. We define $X$ and $Y$ as the random variables of input $\mathbf{x}$ and target $\mathbf{y}$ respectively.
During pre-training, a representation function \( g_\theta: \mathbb{R}^{T \times 1} \to \mathbb{R}^{T \times d} \), where $d$ denotes the dimension of the latent space and \( \theta \) being the parameter of the model, extracts universal properties from a large collection of epidemic time series datasets \( \mathcal{D}_{\text{pre}} = \{ D_1', D_2', \dots, D_S' \} \). 
% \wei{I have revised the above. Overall, you need to (1) remove the unnecessary notations if they are not utilized in the later paragraphs, (2) what is $t$ in $X_t$; (3) please explain the meaning of $(X_t, V_t)$, and (4) it is pre-trained the function $g$ rather than $g(X)$.} 
Then, a set of self-supervised tasks \( \mathcal{T}_{\text{pre}} = \{ \mathcal{T}_i \}_{i=1}^R \) is defined, where each task \( \mathcal{T}_i \) transforms a sample \( \mathbf{x} \sim \mathcal{D}_{\text{pre}} \) into a pair of new input and label: \( (\mathbf{\tilde{x}}, \mathbf{\tilde{y}}) \), and optimizes a loss \( \mathcal{L}_{\mathcal{T}_i} = \mathbb{E}_{\mathbf{x} \sim \mathcal{D}_{\text{pre}}} [\ell_{\mathcal{T}_i}( h_\psi(g_\theta(\mathbf{\tilde{x}})), \mathbf{\tilde{y}})] \), with \( \ell_{\mathcal{T}_i} \) being the task-specific metric and \( h_\psi \) the task-specific head.
% \wei{$\ell$ should just be a distance function? same for $\ell_\text{down}$}
% Once pre-trained, the learned representation function \( g_\theta \) is then fine-tuned on the downstream epidemic dataset \( \mathcal{D}_{down} = \{(\mathbf{x}_t, \mathbf{y}_t)\}_{t=1}^N \) by minimizing the downstream loss \( \mathcal{L}_{\text{down}} = \mathbb{E}_{(\mathbf{x}_t, \mathbf{y}_t) \sim \mathcal{D}_{\text{down}}} [\ell_{\text{down}}(h_\psi(g_\theta(\mathbf{x}_t)), \mathbf{y}_t)] \), with $\ell_{\text{down}}$ being the metric for downstream tasks. 
% \wei{finetuned together with $h_\psi$? pretrained-stage no $h_\psi$?}




% a large collection of epidemic time series datasets \( \mathcal{D}_{\text{pre}} = \{ D_1', D_2', \dots, D_S' \} \).with \( D_j' = \{ \mathbf{X}_t^{(j)} \}_{t=1}^{N_j} \) and \( \mathbf{X}_t^{(j)} \in \mathbb{R}^{T \times d} \). Then, a set of self-supervised tasks \( \mathcal{T}_{\text{pre}} = \{ \mathcal{T}_i \}_{i=1}^R \) is defined, where each task \( \mathcal{T}_i \) transforms \( \mathbf{X}_t \) into a pair \( (\mathbf{\tilde{X}}_t, \mathbf{V}_t) \) and optimizes a loss \( \mathcal{L}_i = \mathbb{E}_{X_t \sim \mathcal{D}_{\text{pre}}} [\ell(g_\theta(\mathbf{\tilde{X}}_t), \mathbf{V}_t)] \), with \( g_\theta \) being the model and \( \ell \) the task-specific loss. Once pre-trained, the learned representation \( g_\theta(\mathbf{X}_t) \) is then fine-tuned on the downstream epidemic dataset \( \mathcal{D}_{down} = \{(\mathbf{X}_t, \mathbf{y}_t)\}_{t=1}^N \), where \( \mathbf{X}_t \in \mathbb{R}^{T \times d} \) and \( \mathbf{y}_t \in \mathbb{R}^{h} \), by minimizing the downstream loss \( \mathcal{L}_{\text{down}} = \mathbb{E}_{(\mathbf{X}_t, \mathbf{y}_t) \sim \mathcal{D}^t} [\ell_{\text{down}}(h_\psi(g_\theta(\mathbf{X}_t)), \mathbf{y}_t)] \), where \( h_\psi \) is the task-specific head.




% \textbf{Self-supervised Pre-training.}

% The goal of self-supervised pre-training is to learn a representation function \( g_\theta : \mathbb{R}^{T \times d} \to \mathbb{R}^{T \times m} \), parameterized by \( \theta \), that captures universal properties of time-series data from a collection of unlabeled pre-training datasets \( \mathcal{D}_{\text{pre}} = \{ D_1', D_2', \dots, D_S' \} \), where each \( D_j' = \{ X_t^{(j)} \}_{t=1}^{N_j} \) and \( X_t^{(j)} \in \mathbb{R}^{T \times d} \). A set of self-supervised tasks \( \mathcal{T}_{\text{pre}} = \{ \mathcal{T}_i \}_{i=1}^R \) is defined, where each task \( \mathcal{T}_i \) transforms \( X_t \) into a pair \( (\tilde{X}_t, Z_t) \) via \( \phi_i \) and optimizes a loss \( \mathcal{L}_i = \mathbb{E}_{X_t \sim \mathcal{D}_{\text{pre}}} [\ell(g_\theta(\tilde{X}_t), Z_t)] \), with \( g_\theta \) being the model and \( \ell \) the task-specific loss. Once pre-trained, the learned representation \( g_\theta(X_t) \) is then fine-tuned on the downstream dataset \( \mathcal{D}_{down} = \{(X_t, Y_t)\}_{t=1}^N \), where \( X_t \in \mathbb{R}^{T \times d} \) and \( Y_t \in \mathbb{R}^{h} \), by minimizing the downstream loss \( \mathcal{L}_{\text{down}} = \mathbb{E}_{(X_t, Y_t) \sim \mathcal{D}^t} [\ell_{\text{down}}(h_\psi(g_\theta(X_t)), Y_t)] \), where \( h_\psi \) is the task-specific head. This approach leverages \( \mathcal{D}_{\text{pre}} \) for representation learning, enhancing performance on tasks with limited labeled data.

% \textbf{Covariate adjustment.}
% Covariate adjustment is the most widely recognized technique for de-confounding using observational data without relying on parametric assumptions ~\cite{runge2023causal}. This method involves removing confounding influences by controlling for a set of variables $Z$. In general terms, the adjustment can be expressed by the following formula:

% \begin{equation}
% p(y | do(X = x)) = \int p(y | x, z) p(z) dz, \tag{1}
% \end{equation}

% where do(X=x) means assigning value $x$ to variable $X$. For epidemic forecasting, x is the input history time series and y is the output predictions, and z is the corresponding environment. 
