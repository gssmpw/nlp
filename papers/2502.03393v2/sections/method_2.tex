\section{Proposed Method}
% \wei{we should highlight pre-training a few more times in this section. it is not until the last subsection that I realize we are developing a pre-trianing framework} 

% \wei{I feel our focus should be pre-training. Other components are serving for enhancing pre-training instead of using pre-training to enhance other components. }

% \wei{Do you think we should first present our pre-training framework and then detail other components? Following this framework, we then describe the challenges in epidemic pre-trainign and introduce the corresponding solutions in each subsections.}

In this section, we introduce our Covariate-Adjusted Pre-training framework for Epidemic forecasting (CAPE). Enhanced by components that capture temporal dependency and infer pseudo-environments (section~\ref{sec: CA} and~\ref{sec: env_estimate}), CAPE aims to learn the intrinsic disease dynamics through pre-training (section~\ref{sec: contrast}).

% to learn broad patterns and diverse environment representations.




\subsection{Model Design}
\subsubsection{Causal Analysis for Epidemic Forecasting}


\begin{figure}[t]
\centering
\includegraphics[scale=0.35]{figures/causal_graph.pdf}
\caption{ Structural causal model (SCM) for epidemic foresting, where $\mathbf{z}^i$ refers to a confounder, and $X_s$ and $X_c$  refers to the spurious and causal factors of the input respectively. }
% \wei{variables connections with epidemics} 
\label{fig: causal_graph}
\end{figure}

% Given the complex interplay between environments and epidemic dynamics, effectively integrating them into epidemic pre-training presents a challenging question. Since the environments impact both historical infection patterns and future disease spread, we draw inspiration from causal inference~\cite{zhou2023causal, jiao2024causal} and consider the environment as a confounder, i.e., a variable that simultaneously affects both the independent variable (e.g., treatment) and the dependent variable (e.g., outcome). To accurately capture these relationships, we incorporate the environment as a distinct component within a Structural Causal Model (SCM), as illustrated in Figure~\ref{fig: causal_graph}. The SCM effectively represents the underlying generative processes by using directed edges to connect each variable (node), thereby depicting the causal pathways among them.
Given the complex interplay between environments and epidemic dynamics, effectively integrating them into epidemic pre-training presents a challenging question. Since environments impact both historical infection patterns and future disease spread, we draw inspiration from causal inference~\cite{zhou2023causal, jiao2024causal} and treat the environment as a confounderâ€”a variable that simultaneously influences both the independent variable (e.g., historical data) and the dependent variable (e.g., future predictions). To capture these relationships, we incorporate the environment as a distinct component within a Structural Causal Model (SCM), as illustrated in Figure~\ref{fig: causal_graph}. The SCM represents the underlying generative processes by using directed edges to connect each variable (node), thereby depicting the causal pathways among them and ensuring that the model accounts for the confounding effects of the environment. Specifically, we consider the environment as a whole as the confounder, which consists of $k$ discrete values from $\mathbf{z}^1$ to $\mathbf{z}^k$. In addition, we adopt an assumption (section~\ref{sec: env_estimate}) that decomposes the input $\mathbf{X}$ to a spurious factor $\mathbf{X}_s$ and a causal factor $\mathbf{X}_c$. \wei{you should probably merge 4.1.1 and 4.1.2 and reorganize the pargarpahs}

% \wei{we need to briefly mention $X_s, Y_s, X_c$... here like what you did in your assumption 4.2}

% \wei{explain the causal a bit, what are nodes/edges; check how other causal inference paper illustrate this concisely. also we should probably put this figure in introduction as we mentioned it in the intro}
% Next, beyond accounting for the environment during pre-training, the key mechanism to disentangle the correlations between diseases and the environment is required. Since we view the environment as the confounder during modeling, we can treat the correlation introduced by the environment as spurious correlations~\cite{ming2022impact}. To mitigate the effects of these spurious correlations, covariate adjustment~\cite{runge2023causal} serves as a standard tool that controls for confounding variables to isolate the true causal effect of interest. Nonetheless, effective covariate adjustment requires a comprehensive set of valid confounders to accurately control for all potential sources of bias, which may not be observed. While this can be hard to achieve given scarce data for a certain disease and region, pre-training enables the model to utilize a large volume of historical data across diverse diseases and regions, which helps the model to account for the underlying environmental factors.


% \subsubsection{Capturing Temporal Correlations}
% \label{sec: temporal_corr}
% Temporal dependency plays an essential role in epidemic time series data as the observed changes often exhibit lagged effects rooted in historical contexts and intervention strategies, shaping current and future trends. In this study, we incorporate self-attention and additional techniques to effectively capture these temporal patterns.

% First, to mitigate the impact of temporal distribution shifts, we employ Reversible Instance Normalization(RevIN)~\cite{kim2021reversible} on the input data. Then, instead of treating each time point as a token, we apply patching~\cite{nie2022time} to efficiently and effectively learn useful representations for forecasting:

% % is then partitioned into \( N \) patches of fixed length \( L \)~\cite{}:

% \[
% \left\{ \mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N \right\} = \text{Patching}(\text{RevIN}(\mathbf{X})), \tag{1}
% \]

% where $N$ denotes the number of patches. Each patch \( \mathbf{x}_i \) then undergoes a learnable linear projection and is augmented with a positional embedding:
% \[
% \mathbf{x}_i^{(0)} = \mathbf{W}_{p} \mathbf{x}_i + pos(i), \tag{2}
% \]
% where $\mathbf{W_{p}}$ is the projection matrix and \( pos(i) \) denotes the Sinusoidal Positional Encoding for patch \( i \). 
% The resulting encoded patches, \(\mathbf{X}^{(0)}\), are then passed into the CAPE encoders, with \(\mathbf{X}^{(l)}\) denoting the output of the \(l\)-th encoder. As illustrated in Figure~\ref{fig:CAPE}, each encoder block applies a self-attention mechanism that yields a contextualized representation \(\mathbf{h}_i^{(l)}\), effectively capturing inter-patch correlations:

% % The encoded patches \( \mathbf{X}^{(0)} \) are then fed into the CAPE encoders. We denote the output of the $l$ th encoder as $\mathbf{X}^{(l)}$. As illustrated in Figure~\ref{fig:CAPE}, each encoder block applies a self-attention mechanism to the input, producing a contextualized representation \( \mathbf{h}_i^{(l)} \) that captures inter-patch correlations:

% \[
% \small
% \mathbf{h}_i^{(l)} = \operatorname{Softmax}\left( \frac{(\mathbf{x}_i^{(l)} \mathbf{W}_Q^{(l)}) (\mathbf{X}^{(l)} \mathbf{W}_K^{(l)})^\top}{\sqrt{d_k^{(l)}}} \right) (\mathbf{X}^{(l)} \mathbf{W}_V^{(l)}). \tag{3} 
% \]



% To mitigate the impact of temporal distribution shifts, we employ Reversible Instance Normalization~\cite{kim2021reversible} on the input data:
% \[
% \mathbf{X}_{\text{norm}} = \text{RevIN}(\mathbf{X}). \tag{1}
% \]
% Next, the normalized data is partitioned into \( N \) patches of fixed length \( L \) using a patching strategy:
% \[
% \left\{ \mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N \right\} = \text{Patching}(\mathbf{X}_{\text{norm}}). \tag{2}
% \]
% Each patch \( \mathbf{x}_i \) undergoes a learnable linear projection and is augmented with a positional embedding:
% \[
% \mathbf{x}_i^{(0)} = \mathbf{W}_{p} \mathbf{x}_i + pos(i), \tag{3}
% \]
% where $\mathbf{W_{p}}$ is the projection matrix and \( pos(i) \) denotes the positional embedding for patch \( i \).
% The encoded patches \( \mathbf{x}_i^{(0)} \) are then fed into the CAPE encoders. We denote the output of the $l$ th encoder as $\mathbf{x}_i^{(l)}$. As illustrated in Figure~\ref{fig:CAPE}, each encoder block applies a self-attention mechanism to the input, producing a contextualized representation \( \mathbf{h}_i^{(l)} \) that captures inter-patch correlations:

% \[
% \small
% \mathbf{h}_i^{(l)} = \operatorname{Softmax}\left( \frac{(\mathbf{x}_i^{(l)} \mathbf{W}_Q^{(l)}) (\mathbf{X}^{(l)} \mathbf{W}_K^{(l)})^\top}{\sqrt{d_k^{(l)}}} \right) (\mathbf{X}^{(l)} \mathbf{W}_V^{(l)}). \tag{4} 
% \]


\subsubsection{Layer-Wise Covariate Adjustment}
\label{sec: CA}
% \wei{we did not introduce the meaning of $\mathcal{X}$ before.}

Beyond accounting for the environment during pre-training, the key mechanism to disentangle the correlations between diseases and the environment is required. Since we view the environment as the confounder~\cite{ming2022impact}, we can apply covariate adjustment~\cite{runge2023causal}to control for confounding variables and isolate the true causal effect of interest. The adjustment can be expressed as $p(\mathbf{y} | do(\mathbf{X}=\mathbf{x})) = \int p(\mathbf{y} | \mathbf{x}, \mathbf{z}) p(\mathbf{z}) d\mathbf{z}$, where $\mathbf{z}$ refers to the confounder (environment) and $do(\mathbf{X}=\mathbf{x})$ refers to assigning value $\mathbf{x}$ to the input variable $\mathbf{X}$. Nevertheless, it can be hard to identify $\mathbf{z}$ given a limited amount of attributes in the pre-training dataset. Therefore, we adopt the following assumption to further constrain the problem:

% Beyond accounting for the environment during pre-training, it is essential to disentangle the correlations between diseases and the environment. We treat the environment as a confounder, considering its influence as spurious correlations~\cite{ming2022impact}. To mitigate these spurious correlations, covariate adjustment~\cite{runge2023causal} is employed to control confounding variables and isolate the true causal effect
% $p(\mathbf{y} \mid do(\mathcal{X}=\mathbf{x})) = \int p(\mathbf{y} \mid \mathbf{x}, \mathbf{z}) p(\mathbf{z}) \, d\mathbf{z}$,
% where \( \mathbf{z} \) is the confounder and \( do(\mathcal{X}=\mathbf{x}) \) denotes assigning \( \mathbf{x} \) to \( \mathcal{X} \). However, acquiring \( \mathbf{z} \) is challenging due to limited attributes in the pre-training dataset. Therefore, we adopt the following assumption to further constrain the problem:

\begin{assumption}
\label{assumption1}
The epidemic forecasting problem involves a finite set of environments $\mathbf{Z}$, each of which possesses a consistent and distinct representation $\mathbf{z}^i$.
\end{assumption}

Such an assumption is reasonable as the dynamics of epidemics are primarily driven by a limited number of factors, which collectively define distinct environments. By assuming fixed representations for these environments, models can effectively capture and leverage the unique patterns and interactions specific to each environment. Then we are able to reduce the procedure of covariate adjustment to a weighted sum of $p(\mathbf{y}|\mathbf{x},\mathbf{z})$:
\begin{equation}
\label{Eq: 5}
p(\mathbf{y} | \mathbf{x}) = \sum\nolimits_{\mathbf{Z}} p(\mathbf{y} | \mathbf{x}, \mathbf{z}) \cdot p(\mathbf{z})   \tag{1}
\end{equation}
Since multiple layers can be stacked together, we perform the adjustment in a layer-wise manner. Specifically, we process the input $\mathbf{X}$ using patching~\cite{nie2022time} and self-attention to capture the temporal dependency, yielding the contextualized representations $\mathbf{h}_i$ for patch $i$. Given $\mathbf{h}_i^{(l)}$ at the $l$ th layer, we model Eq.~\ref{Eq: 5} as follows:

\vspace{-3mm}
\begin{align}
\label{Eq: 6}
\mathbf{m}_i^{(l)}  = \sum_{k=1}^{K} (\mathbf{h}_i^{(l)} \odot \mathbf{z}^k) \cdot p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}), \tag{2}
\end{align}
\vspace{-3mm}

where \( \mathbf{Z} = \{ \mathbf{z}^1, \mathbf{z}^2, \ldots, \mathbf{z}^K \} \) represents the set of fixed environment representations, and \( p(\mathbf{y} | \mathbf{x}, \mathbf{z}) \) is modeled by a hadamard product between $\mathbf{h}_i^{(l)}$ and $\mathbf{z}^k$. Finally, a feedforward neural network \( \mathbf{x}_i^{(l+1)} = \sigma(\mathbf{W}_f^{(l)} \mathbf{m}_i^{(l)}) \) is applied to acquire the output representations, which serves as the input for the next block. At the end of the model, we acquire the final representation $\mathbf{X^{(L)}} = g_\theta(\mathbf{X})$. Then, a task-specific head is applied to predict the target variable $\mathbf{y}=\mathbf{h}_\psi(\mathbf{X}^{(L)})$, where $h_\psi$ is a linear transformation.




% In order to estimate the environment and perform the adjustment at the same time, we also use the weighted sum of $\mathbf{Z}$ as the inferred environment, since they are in the same semantic space. Therefore, following the self-attention layer, the environment representation \( \mathbf{z}_i^{(l)} \) for each patch is computed as below:

% \[
% \label{Eq: env_estimator}
% \mathbf{e}_i^{(l)} = g_{\phi}^{(l)}(\mathbf{h}_i^{(l)}, \mathbf{Z}) = \sum_{k=1}^{K} \mathbf{z}^k \cdot p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}), \tag{6}
% \]

% where \( \mathbf{Z} = \{ \mathbf{z}^1, \mathbf{z}^2, \ldots, \mathbf{z}^K \} \) represents the set of fixed environment representations, and \( p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}) \) is the probability of environment \( \mathbf{z}^k \) given the contextualized representation \( \mathbf{h}_i^{(l)} \). To perform covariate adjustment, the estimated environment \( \mathbf{e}_i^{(l)} \) is then combined with the encoded input via a hadamard product:
% \[
% \mathbf{m}_i^{(l)} = \mathbf{h}_i^{(l)} \odot \mathbf{e}_i^{(l)} = \sum_{k=1}^{K} (\mathbf{h}_i^{(l)} \odot \mathbf{z}^k) \cdot p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}). \tag{7}
% \]
% This formulation effectively reduces the original covariate adjustment function to Eq.~\ref{Eq: 6}, where \( p(\mathbf{y} | \mathbf{x}, \mathbf{z}) \) is modeled as \( \mathbf{x} \odot \mathbf{z} \). Finally, a feedforward neural network \( \mathbf{x}_i^{(l+1)} = \sigma(\mathbf{W}_f^{(l)} \mathbf{m}_i^{(l)}) \) is applied to acquire the output representations, which also serves as the input for the next block. At the end of the model, we acquire the final representation $g_\theta(\mathbf{X}) = \mathbf{X^{(L)}}$. Then, a task-specific head is applied to predict the target variable $\mathbf{y}=\mathbf{h}_\psi(\mathbf{X}^{(L)})$.



\subsubsection{Pseudo Environment Estimator}
\label{sec: env_estimate}

\wei{briefly explain why we hope to estimate the environment?} In order to estimate the environment and perform the adjustment at the same time, we use the weighted sum of $\mathbf{Z}$ as the inferred environment. Therefore, following the self-attention layer, the environment representation \( \mathbf{z}_i^{(l)} \) for each patch at the $l$ th layer is computed by an environment estimator $g_{\phi}^{(l)}$:

\vspace{-3mm}
\[
\label{Eq: env_estimator}
\mathbf{e}_i^{(l)} = g_{\phi}^{(l)}(\mathbf{h}_i^{(l)}, \mathbf{Z}) = \sum_{k=1}^{K} \mathbf{z}^k \cdot p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}), \tag{3}
\]
\vspace{-3mm}

where \( p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}) \) is the probability of environment \( \mathbf{z}^k \) given the contextualized representation \( \mathbf{h}_i^{(l)} \). To perform covariate adjustment, the estimated environment \( \mathbf{e}_i^{(l)} \) is then combined with the encoded input via $\mathbf{m}_i^{(l)} = \mathbf{h}_i^{(l)} \odot \mathbf{e}_i^{(l)}$ to yield the same output as Eq.~\ref{Eq: 6}.

Next, we detail the modeling of the conditional probability \( p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}) \). To better model the causal and spurious correlations, we adopt the following decomposition assumption from~\cite{mao2022causal}:
 
\begin{assumption}
As shown in Figure~\ref{fig: causal_graph}, each input $\mathbf{X}$ can be decomposed into the spurious factor $\mathbf{X}_s$ and the causal factor $\mathbf{X}_c$. Consequently, we decompose representation \( \mathbf{h}_i^{(l)} \) into two corresponding representations:  \( \mathbf{h}_{c,i}^{(l)} \) and \( \mathbf{h}_{s,i}^{(l)} \).
\end{assumption} 

Under this assumption and given that the causal factor is independent from the environment, which yeilds \( \mathbf{h}_{c,i}^{(l)} \perp \mathbf{z}^k \), the probability \( p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}) \) can be expressed as:
\[
p(\mathbf{z}^k \mid \mathbf{h}_i^{(l)}) = p(\mathbf{z}^k \mid \mathbf{h}_{c,i}^{(l)}, \mathbf{h}_{s,i}^{(l)}) = p(\mathbf{z}^k \mid \mathbf{h}_{s,i}^{(l)}). \tag{4}
\]

Then, we model this conditional probability as a cross-attention score using a softmax function:
\[
\label{eq11}
\pi_i^{k(l)} = \operatorname{Softmax} \left( \mathbf{W}_k^{(l)} \mathbf{z}^k \cdot f_{\text{decomp}}(\mathbf{h}_i^{(l)}) \right), \tag{5}
\]

where \( f_{\text{decomp}}(\mathbf{h}_i^{(l)}) \) denotes the decomposition function extracting spurious factors, modeled as a linear transformation \( f_{\text{decomp}}(\mathbf{h}_i) = \mathbf{W}_h^{(l)} \mathbf{h}_i^{(l)} \), and \( \mathbf{W}_k^{(l)} \) maps the environment representation \( \mathbf{z}^k \) to the same dimensionality as \( \mathbf{h}_{s, i}^{(l)} \). Integrating these components, the final model can be expressed as:

\vspace{-3mm}
\[
\mathbf{X}^{(l+1)} = \sigma \left\{ \mathbf{W}_f^{(l)} \sum_{k=1}^{K} \left( [\mathbf{H}^{(l)} \odot \mathbf{z}^k] \cdot \pi_i^{k(l)} \right) \right\}, \tag{6}
\]
\vspace{-3mm}

where \( \sigma \) represents the activation function.

% The CAPE framework integrates environment inference with temporal data processing to enhance epidemic forecasting. By decomposing representations into causal and spurious factors and modeling environment influences through a weighted sum of fixed environment representations, CAPE effectively adjusts for confounders without requiring explicit exogenous covariates. The combination of reversible normalization, patch-based processing, and self-attention mechanisms ensures robust performance against temporal distribution shifts, making CAPE a powerful tool for epidemic forecasting tasks.


\subsection{Pre-training Objectives for Epidemic Forecasting}
\label{sec: contrast}
To capture a wide range of dynamics from the epidemic time series pile, CAPE applies two self-supervised learning strategies for pre-training. 

% \noindent\textbf{Random Masking.} To capture the inherent characteristics from a vast amount of unlabeled epidemic time series data, we employ a masked time series modeling task~\cite{kamarthi2023pems, goswami2024moment} as shown Figure~\ref{fig:CAPE}(c), which randomly masks input patches by setting them to zero with a probability of 30\%. During training, we utilize Mean Squared Error (MSE) as the reconstruction loss to ensure that the reconstructed data closely matches the original input:

% \[
% \mathcal{L}_{recon} = \frac{1}{N}\sum_{i=1}^{N} MSE(\hat{\mathbf{x}_i}, \mathbf{x}_i) , \tag{9}
% \]

% where $\mathbf{x}_i$ is the $i$ the patch of the original time series and $\hat{\mathbf{x}_i}$ is the reconstructed time series patch given by the model.
% Through this process, the model is able to learn robust representations by predicting the masked segments based on their surrounding context. This not only enhances the model's ability to understand temporal dependencies and patterns within the data but also improves its generalization capabilities when applied to unseen epidemic scenarios. 

\noindent\textbf{Random Masking.} To capture characteristics from large unlabeled epidemic time series data, we use a masked time series modeling task~\cite{kamarthi2023pems, goswami2024moment} (Figure~\ref{fig:CAPE}(c)), which randomly masks input patches by setting them to zero with a 30\% probability. During training, we employ Mean Squared Error (MSE) as the reconstruction loss to ensure that reconstructed data closely matches the original: $\mathcal{L}_{\text{recon}} = \frac{1}{N}\sum_{i=1}^{N} \text{MSE}(\hat{\mathbf{x}}_i, \mathbf{x}_i)$.
where \( \mathbf{x}_i \) is the original time series patch and \( \hat{\mathbf{x}}_i \) is its reconstruction by the model.



\noindent\textbf{Hierarchical Environment Contrasting.} 
% To ensure robust and aligned environment representations of the same time steps under different contexts, we employ a hierarchical contrastive loss on the estimated environment representations during pre-training, as shown in Figure~\ref{fig:CAPE}(b). The contrastive loss includes both the instance level and temporal level, which encourages the representations under different contexts to be similar or dissimilar.
% % \textit{Instance-wise Contrastive Loss.} 
% \textit{Instance-wise contrasting} treats environments from different time series samples (a total of $B$ samples) as negative pairs and encourages them to be encoded to be dissimilar~\cite{yue2022ts2vec}, as shown in the second term of Eq.~\ref{Eq: CL}. This term makes the estimated environment representations from different samples far from each other, yielding diverse environments.
% % \textit{Temporal Contrastive Loss.}
% \textit{Temporal contrasting} creates augmented samples with overlapping areas, as shown in the third term of Eq.~\ref{Eq: CL}. 
% Since the environment is independent of the time series in our causal model and should remain consistent throughout the sequence, the representations of the overlapping regions (a total of $\Omega$) are encouraged to be similar, even under varying contexts\cite{yue2022ts2vec}.
% In our framework, the contrastive loss for both the final time series embeddings \( \mathbf{X}^{L} \) and the estimated environment representations $\mathbf{E}^{(l)} = g_{\phi}^{(l)}(\mathbf{H}^{(l)}, \mathbf{Z})$ are computed in a patch-wise manner. As an example, the contrastive loss for the estimated environments is shown below:
To ensure robust and context-aligned environment representations, we apply a hierarchical contrastive loss during pre-training (Figure~\ref{fig:CAPE}(b)), comprising instance-level and temporal-level components. \textit{Instance-wise contrasting} treats environments from different time series (B samples) as negative pairs, encouraging dissimilar representations~\cite{yue2022ts2vec} and promoting diversity (second term of Eq.~\ref{Eq: CL}). \textit{Temporal contrasting} uses augmented samples with overlapping regions ($\Omega$) to maintain consistent environment representations across sequences despite varying contexts~\cite{yue2022ts2vec} (third term of Eq.~\ref{Eq: CL}). In our framework, contrastive loss is computed patch-wise for both final time series embeddings \( \mathbf{X}^{L} \) and estimated environments \( \mathbf{E}^{(l)} = g_{\phi}^{(l)}(\mathbf{H}^{(l)}, \mathbf{Z}) \). An example of the contrastive loss for estimated environments is shown below:


{
\small
\begin{align}
\label{Eq: CL}
&\mathcal{L}_{\text{CL(j, i)}} = - \mathbf{E}_{j,i} \cdot \mathbf{E'}_{j,i} \nonumber \\
&+ \log \left( \sum_{b\in B} \exp \left( \mathbf{E}_{j,i} \cdot \mathbf{E'}_{b,i} \right) + \mathbb{I}_{j \neq b} \exp \left( \mathbf{E}_{j,i} \cdot \mathbf{E}_{b,i} \right) \right) \nonumber \\
&+ \log \left( \sum_{t \in \Omega} \exp \left( \mathbf{E}_{j,i} \cdot \mathbf{E'}_{j,t} \right) + \mathbb{I}_{j \neq t} \exp \left( \mathbf{E}_{j,i} \cdot \mathbf{E}_{j,t} \right) \right)
\tag{7}
\end{align}
}

% \begin{equation}
% \tiny
% \mathcal{L}_{\text{temp}}^{(j, i)}(\mathbf{h}) = -\log \frac{\exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}'_{j,i} \right)}{\sum_{i' \in \Omega} \left( \exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}'_{j,i'} \right) + \mathbb{I}_{i \neq i'} \exp \left( \mathbf{h}_{j,i} \cdot \mathbf{h}_{j,i'} \right) \right)}. \tag{13}
% \end{equation}

% in each layer and the final time series representation are defined as \( \mathcal{L}_{\text{CL}(j, i)}(\mathbf{E}^{(l)}) \) and \( \mathcal{L}_{\text{CL}(j, i)}(\mathbf{X}^{(L)}) \). Therefore, the final loss function is given by:

\textbf{Pre-training Loss.} Finally, combining the reconstruction loss and the contrastive loss, we have the final loss function for pre-training:
\begin{align}
\mathcal{L}_{final} &= \mathcal{L}_{recon}(\mathbf{X}, \mathbf{y})  + \alpha  \mathcal{L}_{\text{CL}}(\mathbf{X}^{(L)}) \nonumber \\
&+ \beta [ 1/l \sum_l \mathcal{L}_{\text{CL}}(\mathbf{E}^{(l)}) ], \tag{8}
\end{align}
where $\alpha$ and $\beta$ are hyperparameters used to balance the contrastive loss for the time series and the estimated environments.


\subsection{Optimization}
\subsubsection{Optimization for Pre-Training}
% \wei{can you follow the Aditya's ICML'24 paper to write this section? I feel currently we are describing this procedure in a too straightforward way}

Simply optimizing the likelihood $p_\theta(\mathbf{y}|\mathbf{X})$ will mislead the time series model to capture the shortcut predictive relation between the history input $\mathbf{X}$ and the future predictions $\mathbf{y}$~\cite{wu2024graph}, which is why the environment should be considered during optimization:
\begin{equation}
\small
\theta^* = \arg\min_\theta \mathbb{E}_{\mathbf{e} \sim p(E),\ (\mathbf{X}, \mathbf{y}) \sim p(\mathcal{Y}, \mathcal{X} \mid E=\mathbf{e})} \left[ \left\| \mathbf{y} - h_\psi(g_\theta(\mathbf{X})) \right\|^2 \right].  \tag{9}
\end{equation}

However, we may not be able to directly acquire the environment representations without external materials and the corresponding encoder. To address this, we treat these environments as hidden variables and optimize them in a data-driven way. Unlike previous methods that approximate the latent probability distribution of environments via variational lower bounds~\cite{wu2024graph}, our approach uses the Expectation-Maximization (EM) algorithm to obtain maximum a posteriori (MAP) estimates of environment representations:

\textbf{Expectation Step (E-Step):} We freeze the transformer encoder and environment estimator, then optimize only the learnable environment representations $\mathbf{Z}$. Setting hyperparameters $\alpha, \beta = 0$, we solve:
\begin{equation}
\mathbf{Z}^{t+1} = \underset{\mathbf{Z}}{\arg\min} \left[ \mathcal{L}_{\text{recon}}(\mathbf{X}, \mathbf{V}) \right]. \tag{10}
\end{equation}
\vspace{-3mm}

\textbf{Maximization Step (M-Step):} We fix the updated environment variables $\mathbf{Z}^{t+1}$ and optimize the encoder and environment estimator by minimizing $\mathcal{L}_{\text{final}}$. Thus, the representation $g_\theta$ and task-specific head $h_\psi$ are updated as:
\begin{equation}
\theta^{t+1}, \psi^{t+1} = \underset{\theta, \psi}{\arg\min} \left[ \mathcal{L}_{\text{final}}(\mathbf{X}, \mathbf{V}, \mathbf{Z}^{t+1}) \right]. \tag{11}
\end{equation}
\vspace{-3mm}

Although our strategy for learning explicit representations resembles codebook optimization~\cite{dong2023peco}, we use these representations for covariate adjustment instead of input reconstruction. The detailed pseudo-code for the optimization procedure is presented in Appendix~\ref{Append_B}.



% \subsubsection{Representation Learning}

% \textbf{Self-supervised Learning.} We adopt reconstruction with random masking as our self-supervised learning method, as shown in Fig~\ref{fig:CAPE} (c). The output embedding $x_i^{(L)}$ is transformed into the reconstruction of the original input patch $x_i$, denoted as $\hat{x_i}$. In this case, EM algorithm is applied during pre-training and we use the mean of each patch's MSE loss as our final loss: $\mathcal{L}_{Sup} = 1/N \sum_{i=0} MSE(\hat{x_i}, x_i)$.


% \textbf{Fine-tuning.} During fine-tuning on the downstream datasets, we fine-tune the whole model using the same EM algorithm. We replace the reconstruction head with a prediction head, which concatenates all the latent representations and maps the input to the prediction target $\hat{y}=W[x_1^{(L)}, x_2^{(L)}, ...]$: $\mathcal{L}_{Sup} = MSE(\hat{y}, y)$.



\subsubsection{Optimization for Downstream Tasks}

% \wei{not sure why we put the loss here; it should probably be merged with the contrastive learning section; in this section we focus on describing the optimiation for pre-training and finetuning. }
% \noindent\textbf{Self-supervised Learning.} For pre-training, we employ a self-supervised approach based on reconstruction with random masking, as illustrated in Figure~\ref{fig:CAPE}(c). Given an input patch $\mathbf{x}_i$, its output embedding $\mathbf{x}_i^{(L)}$ is fed into a reconstruction head to produce $\hat{\mathbf{x}_i}$. EM Algorithm is applied during this process.
% We apply the EM algorithm during pre-training, and define the supervised loss as:
% $\mathcal{L}_{Sup} = \frac{1}{N}\sum_{i=1}^{N} MSE(\hat{\mathbf{x}_i}, \mathbf{x}_i).$

\noindent\textbf{Fine-tuning.} For downstream tasks, we fine-tune the entire model using MSE loss, still employing EM for optimization. The difference is that the reconstruction head is replaced by a prediction head that takes the concatenated latent representations $[\mathbf{x}_1^{(L)}, \mathbf{x}_2^{(L)}, \ldots ]$ and maps them to the future prediction:
$\hat{\mathbf{y}} = \mathbf{W}[\mathbf{x}_1^{(L)}, \mathbf{x}_2^{(L)}, \ldots]$.
% with the training objective:
% $\mathcal{L}_{Sup} = MSE(\hat{\mathbf{y}}, \mathbf{y})$.


\noindent\textbf{Zero-shot.} For zero-shot forecasting, the model remains frozen and no parameter is updated. Like the implementation from the Moment model~\cite{goswami2024moment}, we retain the pre-trained reconstruction head and mask the last patch of the input to perform forecasting: $\hat{\mathbf{y}} = \hat{\mathbf{x}_n}$.


