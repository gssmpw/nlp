\section{Related Works}
Reasoning capability is one of the most important perspectives where the LLMs are considered as the key step to the artificial general intelligence. Multiple reasoning policies are proposed to improve the reasoning abilities of LLMs, aiming to activate their reasoning capabilities and make them more interpretable and efficient in solving reasoning problems that require multi-step inference and complex reasoning. One of the earliest methods to improve reasoning capabilities was the Chain of Thought (CoT) \cite{wei2022cot} prompting, encouraging LLMs to generate intermediate reasoning steps explicitly. However, it still relies on a relatively simple, linear flow of thought, which can become limiting for tasks involving more complex reasoning. To address this limitation, the Tree of Thought (ToT) \cite{yao2024tree} extends CoT by organizing the reasoning process into a tree-like structure, enabling LLMs to consider multiple reasoning paths and self-evaluate the next step. Recent studies \cite{yu2023metamath, luo2023wizardmath, yue2023mammoth, gou2023tora} developed high-quality math reasoning step datasets using methods such as CoT and ToT to fine-tune LLMs and enhance their reasoning capabilities. These methods enhance LLMs' mathematical reasoning potential by improving the quality of reasoning steps. The O1 Model \cite{openai2024o1, openai2024o1_mini} further boosts reasoning capability through test-time scaling laws \cite{snell2024scaling}, which provide the LLMs with more reasoning tokens during inference. Test-time scaling laws increase in reasoning tokens enhances the modelâ€™s overall reasoning capacity, enabling it to tackle more complex tasks.

In addition to activating the reasoning capability of LLMs through prompts or high-quality reasoning datasets, recent studies \cite{zheng2024processbench, zhang2025lessons, mcaleese2024llm} found that constraining LLMs to generate answers from multiple decoding candidates by reward models can also enhance LLMs reasoning capacity. There are two types of reward models: the Process Reward Model (PRM) and the Outcome Reward Model (ORM). ORM evaluates the whole reasoning process based on the final answer, ignoring intermediate steps. In contrast, PRM assesses the quality of each reasoning step individually. However, LLMs frequently make computational or logical errors in mathematical reasoning tasks. Even when producing correct answers, LLMs may generate plausible but incorrect reasoning steps, compromising the reliability of the models \cite{wang2024math}. Employing reward models to evaluate each step of the reasoning process, enhances the reasoning capability and generative reliability of LLMs \cite{lightman2023let}. PRM has been proven superior to ORM \cite{wu2023fine}, as it not only evaluates the reasoning process but also provides reward signals for each individual reasoning step \cite{uesato2022solving, pan2023let}. This helps generate higher-quality data, thereby strengthening the reasoning capability of LLMs. However, training PRM demands high-quality data, which presents significant challenges in terms of data annotation \cite{luo2024improve}. Recent work suggests that combining the LLM-as-a-judge \cite{zheng2023judging} framework with Monte Carlo estimation provide more accurate and consistent annotations for training PRM, thus enhancing the quality of the data for model training \cite{zhang2025lessons}.

\vspace{-0.08cm}