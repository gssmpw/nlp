\section{Related Work}
% \paragraph{Tabular data tasks with LLMs} \ag{I think this entire subsection can be dropped}
% Early research on tabular data tasks with language models focused on developing custom table representation models Zhang et al., "Learning to Represent Tables"__, leveraging structure-aware representations to capture row-column relationships. However, these models were mostly task-specific and lacked flexibility in general Wang et al., "Structure-Aware Table Representation Learning"__.
% In recent years, pre-trained LLMs have been explored for solving diverse tabular data tasks. While not originally designed for tabular data, these general-purpose LLMs have shown adaptability when prompted or fine-tuned, leveraging their general knowledge and reasoning abilities to solve tabular tasks Guo et al., "Pre-Trained Language Models for Tabular Data Tasks"__. However, they face challenges with complex relational structures and dependencies, often underperforming on advanced tabular tasks that demand deeper structure-aware understanding and sophisticated table reasoning ____.

% \subsection{Benchmarks evaluating LLM performance on Tabular tasks}
% \paragraph{Existing Benchmarks \& Evaluations}
Several recent works systematically evaluate LLMs on tabular data tasks, primarily focusing on question-answering type tasks.
TableBench Chen et al., "TableBench: A Benchmark for Table Question Answering"__ 
tests LLMs over
% investigates the capabilities of popular LLMs in Table Question Answering via a benchmark covering 
four major categories of QA tasks, namely fact-checking, numerical reasoning, data analysis, and visualization.
DataBench Li et al., "DataBench: A Large-Scale Benchmark for Question Answering over Tabular Data"__
examines
% , another large-scale benchmark for question answering over tabular data, facilitates empirical comparisons of various LLMs by assessing 
the reasoning capabilities of LLMs in a tabular context.
TQA-Bench Patel et al., "TQA-Bench: A Multi-Table Question Answering Benchmark"__, a multi-table 
% question answering 
benchmark, evaluates 
% the capabilities of LLMs in performing 
complex question answering 
% tasks 
over relational data. TabIS Zhou et al., "TabIS: Evaluating Table Information Seeking Capabilities of Language Models"__ evaluates the table information-seeking capabilities of LLMs. ____ investigate Table-to-text capabilities 
% of different LLMs Ö¿
in several real-world information-seeking scenarios.
Compared to \benchmark{}, these benchmarks are limited in the tasks they cover and the model capabilities they reflect.
% These benchmarks are limited in the tasks they cover and the model capabilities they reflect; however, this is not the case with our benchmark.

% \paragraph{Robustness of LLMs in Table Tasks}
More recently, there has been an increased focus on also analyzing the robustness of LLMs across table formats and perturbations.
% different table formats and table perturbations for tabular data tasks, 
% in addition to evaluating their performance.
Chen et al., "Robustness of Language Models on Tabular Data Tasks"__ explore the impact of table representation formats and noise operations on self-supervised table structure understanding tasks. Specifically, they consider a set of simple fact-finding and transformation tasks on tables to analyze how GPT-3 model performance varies.
Similarly, Wang et al., "Analyzing Table Structure Understanding Tasks with GPT-3"__ analyzes the capabilities of GPT-3.5 and GPT-4 in understanding tables by designing a specific set of table structure understanding tasks using structured data from various public datasets. %They also examine prompt variants and tabular formats to assess their impact on LLM performance in these tasks.
Several recent works 
Li et al., "Structural Variance and Adversarial Perturbations on Tables"__ explore structural variance and adversarial perturbations on tables, and 
their impact
% its impact specifically 
on Table Question Answering 
performance.
% task performance of LLMs.
% Recent efforts have often been constrained by the data used or the models assessed. In contrast, \benchmark{} provides the first extensive and detailed analysis for an overall assessment of model robustness.
However, existing efforts are often restricted to simple synthetic table understanding tasks or a narrow set of table QA datasets and target models, thus overlooking broader challenges posed by complex table reasoning tasks.
% In contrast, \benchmark{} provides the first extensive and detailed analysis for an overall assessment of model robustness.

% \paragraph{Positioning of our work}

% Most existing benchmarks and evaluations on tabular data tasks predominantly focus on assessing the task performance of LLMs. While some recent efforts have looked at the robustness of LLMs, they are often restricted to simple synthetic table understanding tasks or a narrow set of table QA datasets and target models, overlooking broader challenges posed by complex table reasoning requirements of diverse downstream tabular data tasks. 
% Our work takes a more comprehensive approach by constructing a diverse benchmark of downstream tabular data tasks that assesses performance and robustness across domains, table formats, and perturbations, providing deeper insights into the generalizability and limitations of leading open and proprietary LLMs.
%Our work takes a more comprehensive approach, constructing a diverse benchmark of downstream tabular data tasks that assesses both performance and robustness across domains, table formats and perturbations. By evaluating leading open and proprietary LLMs, we provide deeper insights into their generalizability and limitations for tabular data tasks.

% Most existing benchmarks and evaluations on tabular data tasks predominantly focus on assessing the task performance of LLMs. While some recent efforts have 
% % begun analyzing
% looked at
% the robustness of LLMs, these are often restricted to simple synthetic table understanding tasks, or 
% to narrow sets of table question answering datasets and target models.
% % narrowly focused on select table question-answering datasets and limited models as highlighted above. 
% Although valuable, such studies remain limited in scope, overlooking the broader challenges posed by complex table reasoning requirements of diverse downstream tabular data tasks.
% Our work takes a more comprehensive approach by constructing a benchmark of diverse downstream tabular data tasks, focusing on both performance and robustness, and evaluating leading open and proprietary LLMs. 
% By analyzing performance and robustness across domains, perturbations, and table formats, over a broad spectrum of downstream tabular data tasks, we provide deeper insights into the generalizability and limitations of LLMs for tabular data.



% \begin{figure}[t]
% \centering
% \includegraphics[width=.45\textwidth]{new_figures/Prompt Agreement.pdf}
% \caption{
% Agreement between model rankings based on a single prompt.
% For most datasets we see moderate agreement; this highlights that evaluation based on a single prompt configuration is unreliable, as choosing a different prompt would have resulted in a different ranking. 
% }
% \label{fig:prompt_agreement}
% \end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=.45\textwidth]{new_figures/Agreement between Prompt Subsets in the Same Size_lineplot_100.pdf}
% % \includegraphics[width=.45\textwidth]{new_figures/Agreement of Prompt Subset with Our Ground Truth.pdf}
% \caption{
% The impact of the number of prompts on the model ranking agreement score. 
% % The agreement increases most noticeably within the single-digit range.
% % \sa{kendall w}
% }
% \label{fig:prompt_agreement_func_prompts}
% \end{figure}



% \begin{figure}
% \centering
% \includegraphics[width=.45\textwidth]{new_figures/Agreement as a function of num. of inferences__10_kendall_w.pdf}
% \caption{
% Comparing the effects of adding prompts versus adding examples, while keeping the number of inferences constant. Each approach is presented with permanent selected prompts for the dataset or with varied sampled prompts for each example. While increasing the number of examples or prompts raise the agreement in a similar manner (blue and green), the use of varied prompts for each example make them both have a dramatically higher agreement (orange and red).
% % Testing against more prompts increases the reliability (agreement with the true ranking) much faster.%, with a growing advantage in the range of 10 to 100 inferences.
% }
% \label{fig:prompt_agreement_func_inferences}
% \end{figure}