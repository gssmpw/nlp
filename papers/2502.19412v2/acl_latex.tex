% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{ragged2e} % For better text wrapping in columns

% If the title and author information does not fit in the area allocated, uncomment the following
%
% \setlength\titlebox{5cm}
%
% and set <dim> to something 5cm or larger.

% \documentclass{article}
% \usepackage{graphicx} % Required for inserting images
\usepackage{xcolor}
% \usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs} % For better table lines
\usepackage{array}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{array} % Needed for custom column width
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{mathrsfs}
% \usepackage{adjustbox}

\input{commands.tex} 

\input{mathcros}


\title{The Mighty \benchmark{}: A Benchmark for Table Reasoning and Robustness
}

\author{
    Shir Ashury-Tahan$^{\spadesuit}$$^{\heartsuit}$, 
    Yifan Mai$^{\clubsuit}$,  
    Rajmohan C$^{\spadesuit}$,  
    Ariel Gera$^{\spadesuit}$, 
    Yotam Perlitz$^{\spadesuit}$, \hfill \\
    \textbf{Asaf Yehudai$^{\spadesuit}$,
    Elron Bandel$^{\spadesuit}$,
    Leshem Choshen$^{\spadesuit\diamondsuit}$, 
    Eyal Shnarch$^{\spadesuit}$,} \hfill \\
    \textbf{Percy Liang$^{\clubsuit}$ 
    and Michal Shmueli-Scheuer$^{\spadesuit}$} \hfill \\
    $^{\spadesuit}$IBM Research, 
    $^{\heartsuit}$Bar-Ilan University, 
    $^{\clubsuit}$Stanford University, 
    $^{\diamondsuit}$MIT
    \\
    \href{mailto:shir.ashury.tahan@ibm.com}{shir.ashury.tahan@ibm.com}, 
    \href{mailto:shmueli@ibm.com}{shmueli@il.ibm.com} \hfill
}

\begin{document}

\maketitle

% 




\begin{abstract}
% opening - gaps in tables eval.
Despite its real-world significance, model performance on tabular data remains underexplored, leaving uncertainty about which model to rely on and which prompt configuration to adopt.
To address this gap, we create \benchmark{}, a benchmark for Table Reasoning and Robustness, measuring model performance and robustness on table-related tasks.
% the benchmark versatility
The benchmark includes $10$ datasets that cover different types of table reasoning capabilities across varied domains.
% The benchmark also asses robustness
\benchmark{} goes beyond model performance rankings, and is designed to reflect whether models can handle tabular data consistently and robustly,
across a variety of common table representation formats.
% We specifically test table comprehension across a variety of table representations to better reflect real-world tables, unlike testing a single table format, which can yield arbitrary and inconsistent performance measures in a world where tables appear in diverse forms.
% leaderboard
We present a leaderboard as well as comprehensive analyses of the results of leading models over \benchmark{}.
% We present a leaderboard of benchmark results for a range of leading models.
% The benchmark shows low performances and a lack of robustness 
Our results reveal a striking pattern of brittle model behavior, where even strong models are unable to perform robustly on tabular data tasks.
Although no specific table format leads to consistently better performance, we show that testing over multiple formats is crucial for reliably estimating model capabilities. Moreover, we show that the reliability boost from testing multiple prompts can be equivalent to adding more test examples.
% Surprisingly, while models are greatly affected by the table formatting used, there is no specific one that consistently leads to better model performance.
% evaluation should be done with multiple prompts
% Nevertheless, we show that model ranking with one format tends to be unreliable. Hence, it is imperative to examine multiple table formats to accurately evaluate the model, even at the expense of decreasing the number of instances selected from the dataset.
Overall, our findings show that reasoning over table tasks remains a significant challenge\footnote{ \href{https://crfm.stanford.edu/helm/torr/v1.0.0/}{ToRR leaderboard at HELM}}\footnote{\href{https://github.com/IBM/unitxt/blob/main/prepare/benchmarks/torr.py}{ToRR code at unitxt}}.


\end{abstract}



% \begin{figure*}
% \includegraphics[width=\textwidth]{new_figures/abilities_analysis.png}
% \caption{The selected datasets for \benchmark{} along with their properties. The figure can be divided into two; The columns on the left depict dataset attributes while the ones on the right reflect the required skills to solve it based on our analysis.  
% % We broke down each task into the abilities required to solve it and assessed the level at which each ability is evaluated in the datasets. 
% We used colors to indicate the level of necessity for each ability; Green indicates an essential ability, red a non-essential one, and yellow is a partially needed ability.}
% \label{fig:datasets_abilities}
% \end{figure*}


% \definecolor{weakcolor}{RGB}{245,224,204}
% \definecolor{mediumcolor}{RGB}{255,255,153}
% \definecolor{strongcolor}{RGB}{211,234,194}
% $\checkmark\!\!\times$ % Overlapped check and cross (adjust spacing with \! as needed)


% \begin{table*}[h]
% \centering
% \resizebox{\textwidth}{!}{%
% \small
% % \setlength{\arrayrulewidth}{0.5mm}  % Set thickness of lines
% \begin{tabular}{p{3.3cm} p{2.2cm} p{2cm} p{2cm}*{3}{>{\centering\arraybackslash}m{1.6cm}}} % Final adjusted column widths
% \toprule
% \textbf{Dataset} & \textbf{Task} & \textbf{Domain} & \textbf{Metric} & \textbf{Knowledge Extraction} & \textbf{Textual Reasoning} & \textbf{Numerical Reasoning} \\
% \midrule
% \rowcolor{white} \makecell{FinQA} & \RaggedRight Table QA & Finance & Program 
% % accuracy & 
% & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
% \citep{chen2022finqadatasetnumericalreasoning}&&& accuracy &&&\\
% \midrule
% \rowcolor{white} \makecell{TableBench DA} & \RaggedRight Data Analysis & Diverse & Rouge & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
% \citep{wu2024tablebenchcomprehensivecomplexbenchmark} &&&&&& \\
% \midrule
% \rowcolor{white} \makecell{TableBench NR} & \RaggedRight Table QA & Diverse & Rouge & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
% \citep{wu2024tablebenchcomprehensivecomplexbenchmark} &&&&&& \\
% \midrule
% \rowcolor{white} \makecell{TableBench FC} & \RaggedRight Fact Verification & Diverse & Rouge & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
% \citep{wu2024tablebenchcomprehensivecomplexbenchmark} &&&&&& \\
% \midrule
% \rowcolor{white} \makecell{WikiTQ} & \RaggedRight Table QA & Wikipedia & F1 Strings & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
% \citep{pasupat-liang-2015-compositional} &&&&&& \\
% \midrule
% \rowcolor{white} \makecell{TabFact} & \RaggedRight Fact Verification & Wikipedia & Accuracy & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
% \citep{chen2020tabfactlargescaledatasettablebased} &&&&&& \\
% \midrule
% \rowcolor{white} \makecell{QTSumm} & \RaggedRight Table-to-Text & Wikipedia & Rouge & \textcolor{teal}{\checkmark} & \textcolor{orange}{\textasciitilde} & \textcolor{orange}{\textasciitilde} \\
% \citep{zhao2023qtsummqueryfocusedsummarizationtabular} & QA &&&&& \\
% \midrule
% \rowcolor{white} \makecell{SciGen} & \RaggedRight Table-to-Text & Science & Rouge & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{orange}{\textasciitilde} \\
% \citep{moosavi2021scigen} && (Arxiv) &&&& \\
% \midrule
% \rowcolor{white} \makecell{NumericNLG} & \RaggedRight Table-to-Text & Science & Rouge & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{orange}{\textasciitilde} \\
% \citep{suadaa-etal-2021-towards} &&&&&& \\
% \midrule
% \rowcolor{white} \makecell{TURL} & \RaggedRight Classification & Wikipedia & Exact Match & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{purple}{\boldmath$\times$} \\
% \citep{deng2020turltableunderstandingrepresentation} &&&&&& \\
% \bottomrule
% \end{tabular}
% }
% \parbox{\linewidth}{\small
% \begin{tabular}{p{2.5cm} p{1.5cm} p{1.8cm} p{2.5cm} p{2.2cm}}
%    \\
%    & \textbf{Legend:} & Required \textcolor{teal}{\checkmark} &  Partially Required \textcolor{orange}{\textasciitilde} & Not Required \textcolor{purple}{\boldmath$\times$}\\
% \end{tabular}
% }

% \caption{The selected datasets for \benchmark{} along with their properties (additional details in Appendix~\ref{appendix:benchmark_details}). The columns on the right reflect the required skills to solve it based on our analysis (\S\ref{subsec:selected_datasets}), divided into 3 categories. 
% \ay{I think the table looked better w/o the citation, they can go to the app. or RW}\sa{I think such things can be decided later..}\ay{why?}
% }
% \label{tab:datasets_final_colored}

% \end{table*}

\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{%
\small
% \setlength{\arrayrulewidth}{0.5mm}  % Set thickness of lines
\begin{tabular}{p{2.3cm} p{3cm} p{2cm} p{2.5cm}*{3}{>{\centering\arraybackslash}m{1.6cm}}} % Final adjusted column widths
\toprule
\textbf{Dataset} & \textbf{Task} & \textbf{Domain} & \textbf{Metric} & \textbf{Knowledge Extraction} & \textbf{Textual Reasoning} & \textbf{Numerical Reasoning} \\
\midrule
\rowcolor{white} FinQA & \RaggedRight Table QA & Finance & Program Accuracy
% accuracy & 
& \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
\midrule
\rowcolor{white} TableBench DA & \RaggedRight Data Analysis & Diverse & Rouge & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
\midrule
\rowcolor{white} TableBench NR & \RaggedRight Table QA & Diverse & Rouge & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
\midrule
\rowcolor{white} TableBench FC & \RaggedRight Table QA & Diverse & Rouge & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
\midrule
\rowcolor{white} WikiTQ & \RaggedRight Table QA & Wikipedia & F1 Strings & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
\midrule
\rowcolor{white} TabFact & \RaggedRight Fact Verification & Wikipedia & Accuracy & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
\midrule
\rowcolor{white} QTSumm & \RaggedRight Table-to-Text QA& Wikipedia & Rouge & \textcolor{teal}{\checkmark} & \textcolor{orange}{\textasciitilde} & \textcolor{orange}{\textasciitilde} \\
\midrule
\rowcolor{white} SciGen & \RaggedRight Table-to-Text & Science & Rouge & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{orange}{\textasciitilde} \\
\midrule
\rowcolor{white} NumericNLG & \RaggedRight Table-to-Text & Science & Rouge & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{orange}{\textasciitilde} \\
\midrule
\rowcolor{white} TURL CTA & \RaggedRight Classification & Wikipedia & Exact Match & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{purple}{\boldmath$\times$} \\
\bottomrule
\end{tabular}
}
\parbox{\linewidth}{\small
\begin{tabular}{p{2.5cm} p{1.5cm} p{1.8cm} p{2.5cm} p{2.2cm}}
   \\
   & \textbf{Legend:} & Required \textcolor{teal}{\checkmark} &  Partially Required \textcolor{orange}{\textasciitilde} & Not Required \textcolor{purple}{\boldmath$\times$}\\
\end{tabular}
}

\caption{The selected datasets for \benchmark{} along with their properties. The $3$ columns on the right reflect the required skills to solve each dataset, based on our analysis (\S\ref{sec:bench_construct}). Additional dataset details are in Appendix~\ref{appendix:benchmark_details}.
}
\label{tab:datasets_final_colored}

\end{table*}

 
\section{Introduction}

Tabular data are ubiquitous across real-world use cases and tasks. Hence, the ability to understand and process tables is a crucial skill for Large Language Models (LLMs). Tabular processing capabilities can manifest in a wide range of NLP tasks, including table-to-text~\cite{moosavi2021scigen,suadaa-etal-2021-towards}, table question answering~\cite{pasupat-liang-2015-compositional,wu2024tablebenchcomprehensivecomplexbenchmark} and table fact verification \cite{chen2020tabfactlargescaledatasettablebased,gu2022pasta}. In order to solve such problems, LLMs must correctly parse tabular formats, but must also apply various levels of textual and numerical reasoning over the table contents. Thus, tabular data tasks are a challenging test of LLMs' capabilities and practical utility.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures_main/scheme2.pdf}
    \caption{
    Overview of \benchmark{}. We evaluate LLMs on several tabular reasoning datasets. We apply multiple prompt configurations, consisting of table serializations (methods for representing the table as a string), and an optional perturbation to the table structure.
    Our results explore \textit{model performance} and the effects of \textit{prompt variability}. Our analysis demonstrates that for any number of examples, testing more prompt configurations increases the evaluation reliability.
    % \ay{This is better, yet it can gain from some iteration. A few points: 1. There are many boxes? I guess it is for organizing the figure, but maybe we do not need all of them. 2. For Ser. can we add examples, e.g. json HTML. 3. It is not clear what per. is it? I guess it column swap, but if the colors were adjusted it could be improved. 5. The colors are not visually nice. the red green blue I mean. 6. I preferred the previous model images, those robots seem a bit creepy 7. The y-axis of the right is agreement/reliability but you can not know it from the fig. Maybe instead of prompts->reliability, the prompts are already in the more prompts/examples.}\sa{V}
    % The scheme above summarizes the process and outcomes of our work. It outlines the technical steps involved in creating \benchmark{}, and illustrates the types of results we derived.
    }
    \label{fig:schema}
\end{figure}

While prior work has explored LLM performance on tabular tasks~\citep{ruan2024language, fang2024large, lu2024large, chen2022large}, existing evaluations often fail to capture the complexities of real-world applications. 
Specifically, they do not cover the full breadth of tabular tasks and domains, and, crucially, do not systematically assess the \textit{robustness} of LLMs to variations in table formatting. 
Given that real-world tables appear in diverse, yet semantically equivalent, textual representations~\cite{singha2023tabular, sui2024table, zhao2023robut, bhandari2024robustness}, evaluating LLM capabilities across formats and examining their robustness becomes paramount.

% Although several works examine the performance of modern LLMs on tabular tasks, such efforts are generally quite limited in the scope of tasks, domains, and models that they examine~\citep{ruan2024language, fang2024large, lu2024large, chen2022large}. 
% Moreover, real world applications can feature tables in multiple different -- yet semantically equivalent -- textual formats. 
% Hence, it is particularly important to measure LLMs abilities across many such possibilities as well as examine the \textit{robustness} of LLMs to the variety of tabular formats they may encounter \cite{singha2023tabular, sui2024table, zhao2023robut, bhandari2024robustness}.

In this work, we paint a comprehensive picture of the ability of state-of-the-art LLMs to handle downstream table understanding and table reasoning tasks. To this end, we design a pipeline for evaluating LLMs on tabular tasks, as illustrated in Figure~\ref{fig:schema}, and collect $10$ datasets belonging to $6$ diverse tabular tasks, from multiple domains. Those together amount to our benchmark, \benchmark{}, a broad coverage tabular benchmark, testing different levels of reasoning and table understanding skills. 
We evaluate the performance and robustness of $14$ leading LLMs on \benchmark{}%. %Our benchmark, \benchmark{}, covers % measures the abilities of LLMs over 
% table tasks of varying types and difficulties, thus requiring different levels of reasoning and table understanding skills.
, which highlights significant gaps in model capabilities, even for industry-leading LLMs.

% \raj{the limitation with existing work and motivation can be articulated differently I think for few reasons. > We may not outright state that existing efforts on measuring perf on table tasks are limited with tasks, model etc. considering latest works like Tablebench, Databench etc. > Rather we may put it as these works focus on performance but dont focus on robustness while few other works that focus on robustness dont do it across actual downstream tasks, domain etc. > We are doing it first on *Downstream* Table tasks + Across domains + Across models along both performance & robustness dimensions }

% \raj {Several studies have explored language modeling techniques and the application of LLMs to tabular data tasks (Ruan et al., 2024; Fang et al., 2024; 047 Lu et al., 2024; Chen, 2022). As LLMs are increasingly applied to real-world tabular tasks, it becomes essential to evaluate their adaptability and robustness, as real-world applications often present tables in diverse yet semantically equivalent formats. However, most existing benchmarks and evaluations primarily focus on assessing the task performance of LLMs, often without accounting for their robustness (Wu et al., 2024; Grijalba et al., 2024; Qiu et al., 2024; Pang et al., 2024; Zhao et al., 2023b). While some recent works have attempted to evaluate robustness, these efforts are largely limited to simplified, synthetic table understanding tasks (Singha et al., 2023; Sui et al., 2024) or focus narrowly on specific categories such as table question-answering (Bhandari et al., 2024; Zhao et al., 2023c; Liu et al., 2023), leaving broader challenges in table reasoning and real-world down stream tabular data tasks underexplored.}
% \raj {In this work, we aim to provide a comprehensive assessment of state-of-the-art LLMs in handling downstream table understanding and reasoning tasks, with a strong emphasis on their robustness across diverse formats and perturbations. To achieve this, we curate a diverse set of downstream tabular data tasks spanning multiple domains and evaluate the performance of leading LLMs on these tasks. Our benchmark, TaBen, systematically assesses LLM capabilities across table tasks of varying complexity, demanding a range of reasoning and understanding skills. The results reveal notable gaps in the abilities of even state-of-the-art LLMs, highlighting critical areas for improvement.}

The focus on robustness is inherent to the design of \benchmark{}. Our benchmark examines how models respond to differing prompt configurations -- table serialization formats, as well as input perturbations. Thus, going beyond bottom-line model rankings and performance, we are able to conduct an in-depth analysis of LLM behavior on tabular tasks.  
% Going beyond bottom-line model rankings and performance scores, we conduct an in-depth analysis of LLM behavior on tabular tasks. 
% In particular, \benchmark{} 
% examines the robustness of models to differing table formats, as well as various input perturbations. 
Our analyses reveal a consistent and pervasive pattern of brittle model behavior, one that is exhibited across all models and datasets.
% and across all format variations we examine. 
Furthermore, we demonstrate that robustness measurements 
% -- such as those incorporated in \benchmark{} -- 
are pivotal for obtaining reliable estimates of expected model performance, and for choosing which LLM to use in practice. Thus, our findings illustrate that
the ranking results from existing benchmarks
% do not explore variations in prompt design, their resulting model rankings 
represent an arbitrary sample of the true model task performance; in contrast, in \benchmark{} we obtain a more complete picture of LLM capabilities.


\begin{figure*}
    \centering
    \includegraphics[width=0.903\linewidth]{figures_main/prompt_config_ex.pdf}
    \caption{Examples of prompt configurations within \benchmark{}. 
    Each prompt configuration contains the same instructions, but uses a different formatting of the input tables. On the left, the table is configured in markdown format, while on the right, it is serialized using CSV with row shuffling applied to the original table (\S\ref{sec: table_forms}). While all prompts convey the same information, even state-of-the-art models struggle with solving them consistently (\S\ref{section:results}).
    }
    \label{fig:prompts_example}
\end{figure*}


Our main contributions are as follows:

\begin{enumerate}
    \item We present the first comprehensive benchmark\footnote{We release the leaderboard and complete model inference data, as well as the code for running the benchmark. The leaderboard can be found in \href{https://crfm.stanford.edu/helm/torr/v1.0.0/}{ToRR leaderboard}, and the code is available in \href{https://github.com/IBM/unitxt/blob/main/prepare/benchmarks/torr.py}{ToRR code}.} of downstream tabular data tasks, encompassing diverse tasks and incorporating model robustness measurements. 
    \item We reveal significant gaps for leading LLM in downstream tabular tasks capabilities.
    \item Our analysis demonstrates a high sensitivity of LLMs to table formats and perturbations; at the same time, no single format is associated with consistently better performance.
    \item We show that evaluating multiple table configurations leads to a much more reliable evaluation of model performance.
\end{enumerate}

% \yp{without incurring further compute cost.?}\sa{no}
% even at the expense of evaluating fewer examples.




% \ag{Below is the old intro version -- let's create a related work section based on this, and copy some references to the new intro above}


% Tabular data tasks, such as question answering on tables, table-to-text generation, table-based fact checking and classification are very common in real-world applications. Early research concentrated on developing custom table representation learning models \citep{herzig2020tapas, iida2021tabbie, liu2021tapex, yin2020tabert, wang2021tuta, zhao2022reastap, gu2022pasta}, to handle these tasks. These models were typically trained and fine-tuned to recognize row and column structures through structure-aware representations, capturing the relationships and hierarchies inherent in tabular data. However, they depend on specific data and tasks exhibiting limited flexibility in general \citep{badaro2023transformers}.

% In recent years, pre-trained Large Language Models(LLMs) like GPT-3, GPT-4, Llama-3, etc., have been extensively studied for solving tabular data tasks. While not originally designed for tabular data, these models have shown adaptability when prompted or fine-tuned, leveraging their general knowledge and reasoning abilities to solve tabular tasks. However, they struggle with complex relational structures and dependencies, often falling short on advanced tasks that require deeper, structure-aware understanding and intricate table reasoning \citep{fang2024large, lu2024large, ruan2024language}.

% Serialization of tables into text is a common method used to feed tabular data to LLMs and LLM performance is sensitive to these table formats. 
% Another challenge is that the tables generally exhibit noise, missing values, or structural irregularities making it further difficult for LLMs. There are few works on robustness of LLMs on tabular data tasks, but they either consider select self-supervised table tasks only \citep{singha2023tabular, sui2024table} or focus only on a specific category of task like TableQA \citep{liu2023rethinking, zhao2023robut, bhandari2024robustness} and generally consider best available proprietary models only for analysis. It is unclear how robust LLMs are on actual downstream table reasoning tasks and how they fare against each other.





\section{\benchmark{} Construction} \label{sec:bench_construct}

% In this section, we describe the construction details of our benchmark, \benchmark{}. 

% Our motivation in building \benchmark{} was to explore various reasoning tasks over tables, and to analyze model robustness to varying tabular representations. 

% \subsection{Selected Datasets} \label{subsec:selected_datasets} 
%\benchmark{} aims to measure the reasoning capabilities on tables. Hence, we reviewed dozens of existing table datasets and focused on ones are \textit{challenging} in terms of the reasoning abilities they require. We aimed to focus directly on the capabilities of LLMs to reason over tables; thus, we specifically opted for datasets where input tables are \textit{directly incorporated} in the prompt, and that do not require applying a pipeline with external tools (retrieval, SQL, agents etc.). Based on these two criteria, we selected \temp{} datasets to be included in the benchmark.

% \subsection{Selection of Datasets} \label{subsec:selected_datasets} 

We reviewed numerous existing datasets for downstream tabular data tasks, prioritizing \textit{challenging} ones based on the required reasoning abilities.
% \benchmark{} aims at measuring the reasoning capabilities of LLMs on tables.
% We reviewed numerous existing datasets for downstream tabular data tasks and prioritized those that are \textit{challenging} in terms of the reasoning abilities required. 
Also, we opted for datasets where textual tables can be \textit{directly incorporated} into the prompt, eliminating the need for external tools (e.g., retrieval, SQL queries, agents).
Table~\ref{tab:datasets_final_colored} presents the selected datasets and their attributes. 
% \ag{there is a seperate subsection on metrics}
% For each dataset, we computed all listed metrics but designated the first as the primary metric. 
As can be seen, the datasets are diverse in both the target task and domain. Further details are provided in Appendix~\ref{appendix:benchmark_details}.

For a better understanding of the skills needed to solve each dataset, we performed a qualitative analysis of the nature of the tasks. Specifically, we identify \textit{$3$ key skills} ranked from easiest to hardest: 
% \ay{this is instead of the beginning of #1 and #3}\sa{V}


% To directly and clearly measure LLMs abilities on tables, we looked for datasets that enable table incorporation into the prompt with controllable content, potentially assess reasoning, challenge the models, and are as distinct from each other as possible. 
% \ay{1. what "directly and clearly" add? clear and concise language tends to be more effective in papers. 2. find the right words, "and are as distinct from each other as possible." -> and as diverse as possible. 3. "we looked for datasets that enable table incorporation into the prompt with controllable content" sounds like a technicality, also not sure, what value this adds to the reader.}\sa{V}
% From the identified datasets, we selected \temp{} to test table tasks most broadly and reliably. They vary in their domains, metrics, and required outputs, but also in the reasoning level needed to answer them, as detailed in Figure \ref{fig:datasets_abilities} and in Appendix \ref{appendix:benchmark_details}.
% \ay{1. what are the "identified datasets"? 2. I reinforce AG's suggestion, better to start with a reference to the table, and then go into describing the different dimensions. This can help ground the content in the table, and make it clearer. }\sa{V}

% \subsection{Analyzing the Abilities Assessed by Each Dataset}
% The datasets in \benchmark{} vary in complexity and present different levels of challenge. Together, they encompass the fundamental reasoning abilities\footnote{This analysis is dataset-driven, hence it might be limited to the existing table datasets.} expected of LLMs when working with tables:
% \ay{I think it is our categorization, so I expect it will be described that way as well. Otherwise, cite the relevant paper.}
\begin{enumerate}
    % \item \textbf{Table Understanding} - Comprehension of the structure, content, and relations within a table. This is the most basic ability that is relevant for performing on tables. An example for a task that involves understanding only can be column type classification, because the model should identify the column 
    \item \textbf{Knowledge Extraction} - Extraction of relevant information from the table, such as specific fields, relations, or entities (e.g. \textit{"What was the only year Keene won Class AA?"}; WikiTQ). 
    % \ay{maybe better right rising difficulty in the beginning.}\sa{v}
    \item \textbf{Textual Reasoning} - Deducing conclusions by combining the accompanying text with the data contained in the table (e.g. \textit{"How does the number of examinees affect the pass percentage over the years?"}; TableBench).
    % \ay{1. why form the question? 2. sounds like it can not be different texts within the table. 3. what is DataAnalysis? 4. Use the same structure as #1 starting from the main verb, here integrate, maybe deduce, or similar can be better.}\sa{V}
    \item \textbf{Numerical Reasoning} - Performing calculations on the table, such as aggregating information from multiple cells (e.g. \textit{"What was the percent of the growth of the Priceline group inc. from 2014 to 2015?"}; FinQA).  
\end{enumerate}
% \ay{I think in all the focus needs to be the processing required to answer the question, and not the type of output.}\sa{v}

%As shown in the table, all datasets require knowledge extraction at some level. In contrast, reasoning over the table (textual or numerical) appears to be more rare. 
As shown in Table~\ref{tab:datasets_final_colored}, knowledge extraction ability is a key requirement across all datasets. The level of textual reasoning and numerical reasoning required over the tables varies across datasets 
% for each task. 
and tasks.
This analysis also 
% of required skills also 
demonstrates that our selection of datasets in \benchmark{} covers a range of challenge levels.
% , although our results reflect its advantages for measuring models on tables\footnote{This analysis is based on some examples we sampled from each dataset. Hence it is assessment based, and should mainly reflect the overall picture.}. 
% \sa{TBD => elaborate after getting the final results}



% \begin{table*}[]
% \centering

% \begin{tabular}{lccc}
% \toprule
% Model & Performance & Robustness \\
% \midrule
% claude-3-5-sonnet-20241022 & \textbf{.54} & \textbf{.69}\\
% claude-3-5-haiku-20241022 & .46 & .60 \\
% \hline
% gpt-4o-2024-11-20 & \textbf{.54} & \underline{.67} \\
% gpt-4o-mini-2024-07-18 & .47 & .60 \\
% \hline
% deepseek-v3 & \textbf{.54} & .64  \\
% \hline
% gemini-1.5-pro-002 & \underline{.52} & .62\\
% gemini-1.5-flash-002 & .49 & .62\\
% \hline
% qwen2-72b-instruct & .50 & .61 \\
% \hline
% llama-3.1-405b-instruct & .49 & .57\\
% llama-3.1-70b-instruct & .47 & .56 \\
% llama-3.1-8b-instruct & .31 & .52 \\
% \hline
% mixtral-8x22b-instruct-v0.1 & .45 & .56\\
% mixtral-8x7b-instruct-v0.1 & .39 & .45 \\
% mistral-7b-instruct-v0.3 & .38 & .51 \\
% \end{tabular}


% \caption{The main results of LLMs on \benchmark{}, based on the scores described in \S\ref{subsec:metrics}. Best models are marked with bold, second best are underlined. }

% \label{table:perform}
% \end{table*}

\subsection{Prompt Configurations} \label{sec: table_forms}
% \ym{Use the word "format" instead of "forms" for consistency}\sa{We tend to use config instead. I should make sure it is consistent. thanks!}
In real-world tabular tasks, the tables provided as input to an LLM can be represented in different formats; for instance, in JSON 
% \ym{capitalize JSON throughout}\sa{V}
or HTML format.
%In tabular tasks, one or more tables are given as input to an LLM. Yet the same input table can be formatted in different ways; for instance, in json or HTML format. 
Although these formats encode the same content (column names, cell values etc.), LLMs may perform differently depending on the input format.
Hence, to thoroughly assess a model's ability on tabular tasks, it is important to vary not just the examined tasks, domains, and input examples, but also the structural properties of how a table is presented to the model.

To this end, we manipulate the table format across $2$ dimensions. First, for each input table, we examine $7$ \textbf{serializations} i.e., methods to represent the contents of the table as a string (e.g., using \textit{JSON} or \textit{HTML}). In addition, we explore $4$ structural \textbf{perturbations} which are applied to the tables; for example, shuffling the order of rows, or transposing the rows and columns. An example of the resulting prompts is shown in Figure~\ref{fig:prompts_example}. For details on all prompt configurations, see App.~\ref{appendix:table_forms_details}.
% \raj{Should we atleast highlight the full list of serializers & perturbations considered in our benchmark here itself while Appendix explains them in detail later.}


\subsection{Metrics} \label{subsec:metrics}
% as the general scoring function that maps a model's response and the gold answer to a numerical score.
% \yp{here in general the is an issue as we disregard the different datasets inside \benchmark{}, it seems like we are averaging all examples without first averaging the datasets and only then applying an extra average}\ay{In the second paragraph I changed the opening sentence, is this enough or we need to write it explicitly, e.g. we average across all examples as each dataset has some number of examples.}
% \benchmark{} consists of $N$ examples, denoted as $\{(x_i, y_i)\}_{i=1}^N$, where $x_i$ represents the input, and $y_i$ is the ground-truth response. 
\benchmark{} consists of datasets denoted as $D$, and each dataset $d \in D$ contains examples $\{(x_i, y_i)\}_{i \in d}$, where $x_i$ represents the input, and $y_i$ is the ground-truth response. 
Each input can be represented using one of $c \in C$ prompt configurations (\S\ref{sec: table_forms}), denoted as $x_i^c$.


% \benchmark{} contains the same number of examples from each one of the datasets, and each dataset is associated with a specific evaluation metric (see Table~\ref{tab:datasets_final_colored}). 
Each dataset in \benchmark{} is associated with a specific evaluation metric (see Table~\ref{tab:datasets_final_colored}). All metrics fall within the range $[0.0, 1.0]$, ensuring comparability of aggregated scores across datasets. We denote the score function for an example, as defined by the dataset, as $S$.
% As the metrics have different score ranges, we perform min-max normalization\footnote{Min-max normalization scales data to a fixed range, $[0, 1]$ in our case, while preserving value relationships.} over all the dataset scores. We denote this normalized score function as $S$.

% For simplicity, we denote $s_i^p=Score(M(x_i^p), y_i)$ as the normalized score of the dataset metric on model prediction $M(x_i^p)$ and reference $y_i$. \ay{Now $s_i^p$ is not dependent on M}

\textbf{Model Performance}
The performance of a model $M$ is its ability to solve table-related tasks, 
regardless of the table format.
Let $M(x_i^c)$ denote the output of model $M$ for input $x_i^c$. 
The performance score, $\mathcal{P}$, is the average across prompts and is defined as:
\[
\mathcal{P}_{M} = \frac{1}{|D|} \sum_{d \in D} \frac{1}{|d|} \sum_{i\in d} \frac{1}{|C|} \sum_{c \in C} S(M(x_i^c), y_i)
\]


\textbf{Model Robustness}
% Model robustness measures the consistency of model performance across different configurations. 
A robust model is expected to perform similarly on different prompt configurations of the same example, i.e., to have a low variance over the example performance scores.
% First, we define $Scores(M, i)$, the set of scores a model $M$ got over all configurations $p \in P$ for an example $x_i$:
% \[
% Scores(M, i) = \{Score(M(p(x_i)), y_i) \mid p \in P\}
% \]
Thus, we define the robustness score, $\mathcal{R}$, as the complement of the average score range per example:
\begin{align*}
\mathcal{R}_{M}  = 1 - \frac{1}{|D|} \sum_{d \in D} \frac{1}{|d|} \sum_{i\in d}  \bigg[&\max_{c\in C}S(M(x_i^c), y_i) \\ - &\min_{c\in C}S(M(x_i^c), y_i) \bigg]
\end{align*}


% \begin{equation*}
%     Robust_{M} = \frac{1}{N} \sum_{i=1}^N 
% \bigg[ 1 \\ -\big(\max_{p\in P}(M(x_i^p), y_i) - \min_{p\in P}(M(x_i^p), y_i) \big) \bigg]
% \end{equation*}


% As shown in Table~\ref{tab:datasets_final_colored}, each dataset is associated with a specific evaluation metric. We normalize the resulting scores so that the scores for different datasets are in the same range (see Appendix \ref{appendix:benchmark_details}). 


% % There are a set of  Serializations, $S$, a set of Structural perturbations, $P$.

% We represent the examples in \benchmark{} as $\{x_i\}_{i=1}^N$, and the corresponding tables as $\{t_i\}_{i=1}^N$. 
% Let $P$ represent the config. implemented in the prompt (it should include a serialization and an optional perturbation). 
% Consider a model $M$.
% Given an example, implementing a config. on the example involves applying it to the table included in the example, e.g., for $p \in P$, $p(x_i) \to p(t_i)$.
% Let us denote the $Score$ as a function that gets a prediction and returns a score $Score(M(x_i))$.

% \textbf{Model Performance} The ability of a model to succeed on tables with a randomly selected format. Let $Scores_{M, i}$ be the group of scores of a model $M$ on an example $i$ using all formats $p \in P$:
% $$Scores(M, i) = \{Score(M(p(x_i))) \mid \forall p \in P\}$$

% We define the success of a model on an example to be:

% $$Perform_{M,i} = \frac{\sum Scores(M, i)}{P}$$

% Then, the overall model performance is:
% $$Perform_{M} = \frac{\Sigma_{i=1}^{N}Perform_{M,i}}{N}$$

% \textbf{Model Robustness} The extent to which model performance is consistent across different prompts. We define the robustness of a model on an example:
% $$Robust_{M, i} = $$
% $$1 - \big[ \max(Scores(M, i)) - \min(Scores(M, i)) \big]$$
% Then, the overall robustness score is :
% $$Robust_{M} = \frac{\Sigma_{i=1}^{N}Robust_{M, i}}{N}\\$$





% \begin{multline}
% Scores_{M, i} = \{Score(M(p(x_i))) | \forall p \in P\}\\
% Robust_{M, i} = \\1 - [max(Scores_{M, i}) - min(Scores_{M, i})]\\
% \end{align}



% Let the list of performance of a model Defined as:
% $$Perform_{M} = \{Perform_{M,p} | \forall p \in P\}$$

% $$Robust_{M} = 1 - max(Perform_{M}) - min(Perform_{M})$$


% Let us denote $S^i_a(m)$, as the score a model $m$ receives on the $i$-th example when using prompt $a$.
% We define two metrics for a model over a specific example:
% \begin{itemize}
%     \item \textbf{Model Performance} - The ability of a model to succeed with a randomly selected prompt. This is calculated by the average score for an example 
%     over the prompts: 
%     %$$\Expect_{i,a}S^i_a(m)$$
%     $$\frac{1}{|a|}\sum_a S^i_a(m) $$

%     \item \textbf{Model Robustness} - The extent to which model performance is consistent across different prompts. This is calculated as one minus the difference between the highest and lowest scores obtained per example: 
%     % $$1- \Expect_{i}[\max S^i_a(m) - \min S^i_a(m)]$$
%     $$1- [\max_a S^i_a(m) - \min_a S^i_a(m)]$$
% \end{itemize}

% After performing the calculation for each example, we average the resulting scores across all examples to obtain the final model performance and robustness scores.

% The examples are $\{x_i\}_{i=1}^N$. Each example have a table $\{t_i\}_{i=1}^N$
% There are a set of  Serializations, $S$, a set of Structural perturbations, $P$, and lossy" perturbations, $L$.
% Those get an example table and change its format, its structure, or modify its information, e,g, $s\in S$ $s(t_i)$.
% $M$ a model get an example $x_i$ and predict an answer.
% The score is a function that gets an example, and predicted and gold answers and returns a score.

% \textbf{Model Performance}
% Defined as:
% $Prof_{M,s} = \Sigma_{i=1}^{N}Score(M(s(x_i)))$

% \textbf{Model Robustness}
% We define the group of model performances across all serialization as $Prof_{M}$
% $Prof_{M} = \{Prof_{M,s} | \forall s \in S \}$
% $Rob_{M} = 1 - [max(Prof_{M}) -   min(Prof_{M})]$
% \ay{Is it that or per example?}
% \sa{per examples}




\begin{table*}
\centering
\resizebox{\textwidth}{!}{%

\begin{tabular}{l|cc|cccccccccc}
Model & $\mathcal{P}$ ($\uparrow$) & $\mathcal{R}$ ($\uparrow$) & FinQA & \makecell{Numeric-\\NLG} & QTSumm & SciGen &\makecell{Tab \\ Fact }& \makecell{TableBench \\ DA} & \makecell{TableBench \\FC} & \makecell{TableBench \\NR} &  \makecell{TURL\\ CTA} & WikiTQ \\

% Model & Perform (\(\uparrow\)) & Robust (\(\uparrow\)) & FQA & NNLG & QTS & SG & TURL & TF & TB (DA) & TB (FC) & TB (NR) & WTQ \\
\midrule
claude-3-5-sonnet & \textbf{.49} & \textbf{.70} & .43 & .16 & .36 & .14 & \textbf{.85} & .30 & .69 & \textbf{.42} & \textbf{.67} & \textbf{.91}  \\
claude-3-5-haiku & .42 & .61 & .35 & .16 & .34 & \underline{.15} & .78 & .23 & .62 & .20 & .55 & .85 \\
\specialrule{0.1pt}{1pt}{1pt}
gpt-4o & \textbf{.49} & \underline{.69} & .39 & \textbf{.19} & \textbf{.43} & \textbf{.16} & \underline{.83} & \textbf{.33} & \textbf{.72} & \underline{.39} & .59 & \underline{.90}  \\
gpt-4o-mini & .48 & .61 & .33 & \underline{.18} & .39 & \textbf{.16} & .65 & .26 & .65 & .23 & .54 & .88  \\
\specialrule{0.1pt}{1pt}{1pt}
deepseek-v3 & \textbf{.49} & .66 & \underline{.46} & \underline{.18} & .41 & \textbf{.16} & .81 & \underline{.31} & \underline{.71} & .35 & \underline{.65} & \underline{.90}   \\
\specialrule{0.1pt}{1pt}{1pt}
gemini-1.5-pro & \underline{.48} & .64 & \textbf{.47} & \underline{.18} & .37 & \textbf{.16} & .79 & .28 & .70 & .31 & .61 & .88 \\
gemini-1.5-flash & .45 & .63 & .42 & \textbf{.19} & .34 & \textbf{.16} & .76 & .28 & .69 & .21 & .57 & .88 \\
\specialrule{0.1pt}{1pt}{1pt}
qwen2-72b-i & .45 & .63 & .37 & .16 & \underline{.42} & .14 & .78 & .26 & .67 & .23 & .60 & .86  \\
\specialrule{0.1pt}{1pt}{1pt}
llama-3-1-405b-i & .46 & .60 & .35 & .12 & .41 & .11 & .82 & .29 & .65 & .31 & .61 & \underline{.90} \\
llama-3-1-70b-i & .44 & .58 & .36 & .12 & .41 & .10 & .72 & .29 & .63 & .26 & .59 & \underline{.90}  \\
llama-3-1-8b-i & .28 & .53 & .03 & .10 & .34 & .09 & .55 & .15 & .51 & .09 & .18 & .80   \\
\specialrule{0.1pt}{1pt}{1pt}
mixtral-8x22b-i & .41 & .58 & .27 & \textbf{.19} & .41 & \textbf{.16} & .73  & .23 & .67 & .20 & .54 & .66  \\
mixtral-8x7b-i & .34 & .48 & .19 & .17 & .36 & \textbf{.16} & .64 & .23 & .58 & .12 & .35 & .61  \\
mistral-7b-i & .32 & .54 & .19 & .13 & .38 & .12 & .55 & .21 & .54 & .09 & .30 & .67  
\end{tabular}
}
\caption{
    Main results of LLMs on \benchmark{}: overall performance ($\mathcal{P}$) and robustness ($\mathcal{R}$) scores, along with performance scores for each dataset.
    Best scores are marked with bold, second best are underlined. }
    \label{table:perform}
\end{table*}

% \begin{table*}[]
% % \scriptsize
% \centering
% \resizebox{\textwidth}{!}{%

% \begin{tabular}{l|c|cccccccccc}
% Model & $\mathcal{P}$ ($\mathcal{R}$) 
%     \uparrow & FinQA & \makecell{Numeric-\\NLG} & QTSumm & SciGen & \makecell{TURL\\ CTA} & \makecell{Tab- \\ Fact }& \makecell{TableBench \\ DA} & \makecell{TableBench \\FC} & \makecell{TableBench \\NR} & WikiTQ \\

% % Model & Perform (\(\uparrow\)) & Robust (\(\uparrow\)) & FQA & NNLG & QTS & SG & TURL & TF & TB (DA) & TB (FC) & TB (NR) & WTQ \\
% \midrule
% claude-3-5-sonnet & \textbf{.54} (\textbf{.69}) & .43 & .39 & .37 & .33 & \textbf{.69} & \textbf{.86} & \underline{.31} & .70 & \textbf{.42} & \textbf{.92}  \\
% claude-3-5-haiku & .46 (.60) & .35 & .39 & .34 & .34 & .55 & .79 & .23 & .63 & .20 & .85 \\
% \specialrule{0.1pt}{1pt}{1pt}
% gpt-4o & \textbf{.54} (\underline{.67}) & .40 & \textbf{.45} & \textbf{.43} & \textbf{.38} & .59 & \underline{.83} & \textbf{.33} & \textbf{.73} & \underline{.40} & \underline{.91}  \\
% gpt-4o-mini & .47 (.60) & .34 & .42 & .40 & .36 & .55 & .65 & .26 & .66 & .24 & .88  \\
% \specialrule{0.1pt}{1pt}{1pt}
% deepseek-v3 & \textbf{.54} (.64) & \underline{.46} & \underline{.43} & .41 & \underline{.37} & \underline{.68} & .81 & \underline{.31} & \underline{.71} & .35 & \underline{.91}   \\
% \specialrule{0.1pt}{1pt}{1pt}
% gemini-1.5-pro & \underline{.52} (.62) & \textbf{.47} & \underline{.43} & .38 & .36 & .62 & .80 & .29 & .70 & .31 & .88 \\
% gemini-1.5-flash & .49 (.62) & .43 & \textbf{.45} & .34 & \underline{.37} & .57 & .77 & .28 & .70 & .21 & .88 \\
% \specialrule{0.1pt}{1pt}{1pt}
% qwen2-72b-i & .50 (.61) & .38 & .39 & \underline{.42} & .33 & .65 & .78 & .27 & .68 & .23 & .86  \\
% \specialrule{0.1pt}{1pt}{1pt}
% llama-3-1-405b-i & .49 (.57) & .36 & .28 & \underline{.42} & .25 & .63 & .82 & .30 & .65 & .31 & .90 \\
% llama-3-1-70b-i & .47 (.56) & .37 & .28 & .42 & .23 & .60 & .73 & .3 & .63 & .27 & \underline{.91}  \\
% llama-3-1-8b-i & .31 (.52) & .04 & .25 & .35 & .21 & .19 & .55 & .16 & .52 & .10 & .80   \\
% \specialrule{0.1pt}{1pt}{1pt}
% mixtral-8x22b-i & .45 (.56) & .28 & \textbf{.45} & .41 & \underline{.37} & .56 & .74 & .24 & .68 & .20 & .66  \\
% mixtral-8x7b-i & .39 (.45) & .20 & .41 & .36 & \textbf{.38} & .36 & .65 & .23 & .59 & .12 & .61   \\
% mistral-7b-i & .38 (.51) & .19 & .32 & .38 & .29 & .54 & .55 & .22 & .55 & .10 & .67  
% \end{tabular}



% }
%     \caption{Our main results of LLMs on \benchmark{}, with performance ($\mathcal{P}$) and robustness ($\mathcal{R}$) scores derived from the metrics described in \S\ref{subsec:metrics}. Best models are marked with bold, second best are underlined. }
%     \label{table:perform}
% \end{table*}

% \begin{table}[h]
%     \centering
%     \small
%         \begin{tabular}{lccc}
%         \toprule
%         Model & Performance (\uparrow) & Robustness (\uparrow) \\
%         \midrule
%         claude-3-5-sonnet & \textbf{.54} & \textbf{.69}\\
%         claude-3-5-haiku & .46 & .60 \\
%         \hline
%         gpt-4o & \textbf{.54} & \underline{.67} \\
%         gpt-4o-mini & .47 & .60 \\
%         \hline
%         deepseek-v3 & \textbf{.54} & .64  \\
%         \hline
%         gemini-1.5-pro & \underline{.52} & .62\\
%         gemini-1.5-flash & .49 & .62\\
%         \hline
%         qwen2-72b-i & .50 & .61 \\
%         \hline
%         llama-3.1-405b-i & .49 & .57\\
%         llama-3.1-70b-i & .47 & .56 \\
%         llama-3.1-8b-i & .31 & .52 \\
%         \hline
%         mixtral-8x22b-i & .45 & .56\\
%         mixtral-8x7b-i & .39 & .45 \\
%         mistral-7b-i & .38 & .51 \\
%         \end{tabular}
%     \caption{The main results of LLMs on \benchmark{}, based on the scores described in \S\ref{subsec:metrics}. Best models are marked with bold, second best are underlined. }
%     \label{table:perform}
% \end{table}

% \begin{figure*}[t]
%     \begin{minipage}{0.55\textwidth}
%         \centering
%         \begin{tabular}{lccc}
%         \toprule
%         Model & Performance (\uparrow) & Robustness (\uparrow) \\
%         \midrule
%         claude-3-5-sonnet & \textbf{.54} & \textbf{.69}\\
%         claude-3-5-haiku & .46 & .60 \\
%         \hline
%         gpt-4o & \textbf{.54} & \underline{.67} \\
%         gpt-4o-mini & .47 & .60 \\
%         \hline
%         deepseek-v3 & \textbf{.54} & .64  \\
%         \hline
%         gemini-1.5-pro & \underline{.52} & .62\\
%         gemini-1.5-flash & .49 & .62\\
%         \hline
%         qwen2-72b-i & .50 & .61 \\
%         \hline
%         llama-3.1-405b-i & .49 & .57\\
%         llama-3.1-70b-i & .47 & .56 \\
%         llama-3.1-8b-i & .31 & .52 \\
%         \hline
%         mixtral-8x22b-i & .45 & .56\\
%         mixtral-8x7b-i & .39 & .45 \\
%         mistral-7b-i & .38 & .51 \\
%         \end{tabular}
%     \captionof{table}{The main results of LLMs on \benchmark{}, based on the scores described in \S\ref{subsec:metrics}. Best models are marked with bold, second best are underlined. }
%     \label{table:perform}
%     \end{minipage}
%     \hspace{0.7cm}  % Horizontal space between the figures
%     \begin{minipage}{0.43\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{new_figures/families1.pdf} % replace with your image
%        \caption{Model behavior on \benchmark{} across datasets reflects both average performance and model skills, as analyzed in Table~\ref{tab:datasets_final_colored}.}
%         \label{fig:comp_models_skills}

%     \end{minipage}%
% \end{figure*}


\subsection{Setup} \label{subsec:  Setup}
For each dataset, we sample $100$ examples from the test set. For each example, as mentioned in \S\ref{sec: table_forms}, we represent its table using $7$ different serializations. We also apply $4$ different structural perturbations. As the perturbations are orthogonal to the chosen serialization, this yields a total of $35$ $p \in P$ prompt configurations ($7$ serializations $\times$ $4$ perturbations + $7$ without perturbation). 

We run a total of $14$ models over \benchmark{}. We use 5-shot prompting\footnote{Except for WikiTQ, which includes long examples that we had to truncate to one-shot to fit the context window.}, where each set of 5 shots is randomly sampled for each example, with greedy decoding and limit the maximum token output to $512$. Each model was run on the same set of $100$ examples per dataset\footnote{The TableBench FC dataset consists of $96$ examples. We used $79$ examples in \benchmark{}, with the remaining examples serving as demonstrations.} $\times$ $35$ prompt configurations.

We utilize the \texttt{Unitxt} library \cite{bandel-etal-2024-unitxt} to ensure that \benchmark{} is shareable and reproducible, as also highlighted by \cite{reuel2024betterbenchassessingaibenchmarks}. The modular customization of the library allowed us to manipulate the choice of table serialization and apply perturbations while keeping other aspects of prompt design (e.g., few-shot examples) constant.




\begin{comment}
% \raj{Shoud we move the subsection 2.3 Setup that cover experimental setup out of Benchmark section to Section 3. Rename Section "Results" to "Experimental Evaluation" then 3.1 Setup and 3.2 Results ?}
We utilize the \texttt{Unitxt} library \cite{bandel-etal-2024-unitxt} to ensure that the dataset evaluations are shareable and reproducible. The modular customization of the library allowed us to manipulate the choice of table serialization and apply perturbations while keeping other aspects of prompt design (e.g., few-shot exemplars) constant.

As mentioned in \S\ref{sec: table_forms}, we represent each table using $7$ different serializations, i.e., $7$ different prompt configurations for each example in each dataset. We also apply $4$ different structural perturbations to the tables. As the perturbations are orthogonal to the serialization chosen, this yields a total of $35$ $p \in P$ prompt configurations ($7$ serializations $\times$ $4$ perturbations + $7$ serializations without perturbation).

For each dataset, we sample $100$ examples from the test set, and include a $5$-shot example in the prompt. % in-context learning with $5$ demonstrations.
% \footnote{If the model's context window was too small for the demonstrations, we reduced their number accordingly.}\yp{cite HELM for this practice, some might find it weird, I do :)}. \sa{yifan said it is not true in our case}
We use greedy decoding and limit the maximum token output to $512$.
% \ay{Maybe terms like 5-shot, and greedy decoding are better.}\sa{V}

We run a total of \temp{} models over \benchmark{}. Each model was run on the same set of $100$ examples per dataset $\times$ $35$ prompt configurations for each example.
\end{comment}


\section{Results \& Analysis} \label{section:results}
\benchmark{} offers multiple insights into the capabilities and performance of models on tabular tasks.

We present the high-level results of \benchmark{} in \S\ref{subsec:  capabilities}. 
Next, we analyze advanced aspects of model performance in \S\ref{subsec: perform_aspects}.
% we describe the benchmark properties and the relations between the various datasets included in our benchmark in \S\ref{subsec:  benchmark-characteristics}.
Finally, 
we examine how prompt configurations affect performance
% we show the sensitivity of models to prompt configurations 
(\S\ref{subsec: eval_serializer}).

\subsection{Model Capabilities} \label{subsec:  capabilities}

Table \ref{table:perform} showcases the main results of $14$ open and closed models on \benchmark{}. \textit{claude-3-5-sonnet, gpt-4o} and \textit{deepseek-v3} 
(full model names are in Appendix \ref{appendix:model_names})
outperform others in most of the datasets, however models within the larger size range demonstrate similar performance. These models also have higher robustness scores, and overall, it appears that robustness scores correlate with performance scores, e.g., models with better performance tend to be more robust. Two main trends that \benchmark{} show, are:




\paragraph{Even strong models struggle with tables.} 
The absolute scores of all models are medium-low, at best reaching $0.49$ (see Table \ref{table:perform}); but also, the gap between lower-performing models and higher-performing models is narrow. This indicates that better models are only moderately better at table understanding. 

\paragraph{All models are not robust.} 
For each model and example from the selected datasets, we obtain a set of $35$ scores (\S\ref{subsec:  Setup}) that should all reflect how well the model handles this example and hence expected to mainly agree with each other. Yet in practice, there is a substantial variance within these scores.

Figure~\ref{fig_perform} illustrates this range of scores for each model, aggregated across examples. As can be seen, the minimal score and the maximal score yield entirely different estimates of model performance. Thus, we see that the models exhibit strikingly brittle behavior, and are highly influenced by the choice of configuration. 



\begin{figure}[t]
\includegraphics[width=1\columnwidth]{figures_main/Model_Score_Range.pdf}
\caption{
For each example we obtained $35$ performance scores using different prompt configurations. The example scores are assigned an index in the range $[1, 35]$, ordered from lowest to highest performance. The plot depicts an average aggregation of each index across all examples.
Models exhibit a wide range of scores, reflecting low robustness. 
}
\label{fig_perform}
\end{figure}

\subsection{Model Performance Trends} \label{subsec: perform_aspects}

As can be seen in Table~\ref{table:perform}, performance within model families is directly correlated to the model size. 
% Model size hints for the model ranking within families as can be seen in Table~\ref{table:perform}. 
However, the differences in scores within families tend to be relatively small, and are $0.07$ on average\footnote{Averaging over the differences between pairs of models within the same family.}. 
Across model families, size does not always indicate performance; for example, \textit{qwen2-72b-instruct} outperforms \textit{llama-3.1-405b-instruct} in both performance and robustness. 


Table~\ref{table:perform}
compares model behavior across datasets, highlighting the strengths and weaknesses of each model.
For example, while \textit{claude-3-5-sonnet} outperforms others in classification tasks (e.g., TabFact), \textit{mixtral-8x22b-instruct} performs the worst in them. However, \textit{mixtral-8x22b-instruct} shows better capabilities than \textit{claude-3-5-sonnet} in Table-to-Text tasks (e.g., QTSumm).


The order of models in Table~\ref{table:perform} reflects an overall advantage of closed models over open models. 
% Specifically, \textit{deepseek-v3} stands out as the exception, and have similar performance to closed ones. 
Delving deeper, it appears they outperform in all tasks we examined. Additional analysis can be found in Appendix~\ref{app:perform_analysis}.


% \paragraph{Agreement with Other Benchmarks}

% Another important aspect is comparing the benchmark to other benchmarks, which helps assess its appropriateness \citep{reuel2024betterbenchassessingaibenchmarks} alongside its uniqueness in evaluating model performance. \citealp{perlitz2024llmbenchmarksagreefixing} showed that comparing benchmarks to one another is essential and may indicate for the benchmark validity. 

% We compared \benchmark{} to groups of benchmarks that address similar skills to those shown in Table~\ref{tab:datasets_final_colored}. The first group consists of reasoning benchmarks, while the second includes mathematical ones. We found that \benchmark{} aligns with both at a moderate level, but it correlates more strongly with the reasoning benchmarks than with the mathematical ones, which is consistent with the analysis in Table~\ref{tab:datasets_final_colored}. More details can be found in Appendix \ref{appendix: benchbench}.

\subsection{Performance by Prompt Configuration} \label{sec: prompt_performance}

The notable lack of model robustness observed above raises the question of whether it results from certain configurations outperforming others.
% However, we observe a maximum difference of $0.06$ in overall model performance across serializers, and $0.03$ across perturbations (additional details are provided in Appendix~\ref{app:analysis}).


\paragraph{No serializer leads to superior performance.} \label{subsec: eval_serializer}

To evaluate whether specific serializations give rise to better model performance, we calculate the win-rate of serializers at the example level. Then, we aggregate the results for all models and serializers and find that no serializer consistently outperforms others.

When breaking down the results by model, we do find some weak effects of preferred serializations for specific models, with a maximum difference of $0.06$ in overall model performance across serializers.
Additional details and figures can be found in Appendix~\ref{app:analysis_sec:serializers}.




\paragraph{Perturbations have no consistent effect.} \label{subsec: eval_perturb}

We find that the perturbations outlined in \S\ref{sec: table_forms} do not consistently affect performance, neither decreasing nor increasing model performance. For further details, refer to Appendix~\ref{app:analysis_sec:perturbs}.





\section{Properties of \benchmark{}} \label{subsec:  benchmark-characteristics}

The value of a benchmark hinges on its reliability and validity. This section assesses these properties for \benchmark{}, specifically examining its separability and dataset agreement.

\paragraph{Benchmark and Dataset Separability}
An important feature of benchmark datasets
is their ability to separate models well, as emphasized by \citet{li2024crowdsourceddatahighqualitybenchmarks}. We adopt their "Separability with Confidence" metric, and calculate for each dataset the percentage of model pairs that have non-overlapping confidence intervals of their benchmark scores\footnote{We employed bootstrapping with $1$K randomly selected seeds to sample $100$ examples from each dataset.}.
The results are depicted in Figure \ref{fig:separability} and show varied separability levels across datasets; Some datasets, such as \textit{WikiTQ}, have a high separability score of over $71\%$, indicating greater reliability. In contrast, others, like \textit{TableBench FC}, show a lower separability score of only $38\%$, reflecting less reliable model rankings. This strengthens the need for evaluating models on tables with multiple varied datasets in order to get a reliable result. Indeed, the separability score for the aggregated \benchmark{} is significantly higher, accounting for $79\%$ by applying a similar calculation across all benchmark examples.

\begin{figure}[t]
    \centering
    % \begin{subfigure}[t]{0.4\textwidth}
        \centering
         \includegraphics[width=0.9\linewidth]{figures_main/Dataset_Separability.pdf}
         \caption{The separability score for each dataset in \benchmark{}. This score represents the proportion of model pairs that can be distinguished with confidence, meaning their confidence intervals (CIs; via bootstrapping over $1$K seeds) do not overlap.}
         \label{fig:separability}
    % \end{subfigure}%
\end{figure}

% \ay{Do we want to add \benchmark{} to fig. \ref{fig:separability}?} \sa{I think it is better to keep it with datasets only}

\paragraph{Dataset Agreements}
\benchmark{} aims to cover different skills and difficulty levels of tabular tasks by including diverse datasets as described in \S\ref{sec:bench_construct}. We validate that those datasets indeed measure different model skills. To this end, we calculate model rankings for each dataset using results from all configurations captured in the benchmark and measure the agreement between the rankings derived from the different datasets using \textit{Kendall's tau}.


% As can be seen, most generation datasets tend to agree with each other on the model ranking, ranging from medium to high agreement. In contrast, the two classification datasets (Turl and TabFact) agree with one another but not with the generation datasets. We see that different datasets provide slightly different perspectives on model performance; thus, the full benchmark enables painting a more comprehensive picture of model performance on tables.

As seen in Figure \ref{fig:sepability_agreement}, most datasets tend to agree with each other on model ranking, ranging from medium to high agreement. In contrast, Table-to-Text datasets have a low agreement with others, while specifically \textit{SciGen} and \textit{NumericNLG} align with each other and seem to measure the same aspect as expected.
% Evidently, different datasets provide slightly different perspectives on model performance; thus, the full benchmark enables painting a more comprehensive picture of model performance on tables.



\begin{figure}[t]
    % \begin{subfigure}[t]{0.46\textwidth}
        \centering
         \includegraphics[width=\linewidth]{figures_main/Dataset_Agreement_on_Model_Ranking.pdf}
         \caption{Model ranking agreement between the datasets in \benchmark{}. 
         % It indicates a strong agreement within classification tasks (Turl and TabFact) and within generative tasks (the others) but a weaker agreement between generation and classification.
         }
         \label{fig:sepability_agreement}
    % \end{subfigure}
    % \caption{}
\end{figure}



\begin{figure*}
        % \centering
        % \includegraphics[width=\textwidth]{new_figures/fig_prompt_effect.pdf}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures_main/Prompt_Agreement.pdf}
        \caption{
        }
        \label{fig:prompt_agreement}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures_main/Agreement_between_Prompt_Subsets_Same_Size_lineplot_100.pdf}
        \caption{
        }
        \label{fig:prompt_agreement_func_prompts}
    \end{subfigure}
    \caption{Agreement between model rankings. We sample $30$ prompt configuration sets, calculate the model ranking of each set, and report the agreement (Kendall's W) between rankings. (a) \textit{Agreement between rankings based on a single prompt configuration}. Overall we see low agreement, indicating an unreliable model ranking (i.e., choosing a different prompt would lead to a different ranking). (b) \textit{Agreement between rankings based on multiple prompt configurations.} 
    Adding prompts increases the ranking consistency, with the largest gain between $\sim2$-$8$ prompts. Note that \textit{(a)} is a zoomed-in view of \textit{(b)} where the number of prompt configurations is $1$.}
    \label{fig:prompt_impact}
\end{figure*}


\section{Implications for Reliable Evaluation} \label{section:prompt_impact_on_eval}

A key component in the design of \benchmark{} is the measurement of performance across prompt configurations (\S\ref{sec: table_forms}); The different prompts we test differ only in table structure and hence preserve the same underlying semantics.

In terms of average model performance, the results in \S\ref{sec: prompt_performance} show that the choice of prompt configuration does not have a consistent effect. Nevertheless, the variability introduced by prompt choice is significant for evaluations. 

In the following, we ask how the variability in prompt configurations affects the \textit{evaluation} itself, in terms of obtaining reliable results. Thus, here we utilize \benchmark{} to explore broader questions about desirable evaluation practices.
% Different datasets measure somewhat different qualities, leading to varying model rankings. In contrast, different prompt formats preserve the same underlying semantics while introducing some variability, which is expected to lead to similar model rankings. 
% % While differences between datasets can result in varying model rankings, varying prompts should not have the same effect. This is because datasets are dedicated to testing specific challenges but all prompts are intended to measure the same underlying ability. 
% \ay{Why? if my model was trained on MarkDown it would be better on it than on Json. This is an unjustifiable claim. I get the point it tried to make, but now it does not go through. Maybe, you can say something about what you expect or about what a robust model performance will look like.} \sa{mm can you suggest a way to say it? I'm afraid it may be too much connected to model robustness}
% \ay{Different datasets measure somewhat different qualities leading to varying model rankings. In contrast, different prompt formats might introduce some variability but still preserve the same underlying semantics which is expected to lead to similar model rankings.}\sa{v}
% We define \textbf{Our Ground Truth (GT)} to be the full model ranking based on all $35$ aggregated prompts as detailed in \S\ref{subsec:  Setup}, and use it to analyze the impact of prompts.
% In this section, we set aside the fact that we are dealing with table datasets and focus on prompt analysis. The results presented in \S\ref{section:results} suggest variability without direction across prompts. Hence, it can be used as an example of variability that may be compared to other types of prompt configurations.
The choice of prompt configuration is an example of a design decision made when building a benchmark, one that can be somewhat arbitrary; such decisions
determine the reliability of a benchmark, and the
conclusions that can be drawn from it \cite{perlitz2024efficientbenchmarkinglanguagemodels, reuel2024betterbenchassessingaibenchmarks}. 

Reliability can be measured via the agreement between evaluation results (model rankings) obtained when making different decisions (e.g., choosing different prompt configurations). To measure agreement between rankings we adopt \textbf{Kendall's W} \cite{kendall1939problem} as a main metric. This is a measure of consensus in the rankings provided by a set of raters, in the range $[0.0-1.0]$. 
% The value of Kendall’s W ranges from $0$ to $1$.

\subsection{Using a Single Prompt Config is Unreliable}

Existing benchmarks usually select one prompt format (for example, serialize the table using JSON). We ask to what extent this choice influences the model ranking. Thus, we calculate model rankings based on each of our prompt configurations, and test the similarity between them.
% We measure the extent to which prompts influence the ranking. To do that, we feed each prompt to all of the models and rank the models by their scores. Then, we calculate Kendall's W
% ranking correlation among them. 

The result is depicted in Figure~\ref{fig:prompt_agreement}. The agreement scores are generally low, demonstrating that model ranking order changes dramatically based on the choice of prompt. In other words, if a benchmark uses only a single configuration, the resulting model ranking would not be reliable.

\subsection{Multiple Prompt Configs Increase Reliability}

Figure~\ref{fig:prompt_agreement_func_prompts} depicts the effect of evaluating with multiple prompts. Unsurprisingly, we see that basing the model ranking on more prompts increases the agreement the resulting rankings have with each other. 
The plot also shows that even a relatively small number of prompt configurations can make a large difference and contribute to a more reliable model ranking. For example, increasing the number of prompts from $1$ to $10$ increases Kendall's W score by more than $0.35$ on average.

Another observation from Figure~\ref{fig:prompt_agreement_func_prompts} is that the datasets exhibit differing patterns of the increase in agreement.
% which also indicates for the sensitivity of dataset results to prompt configurations. 
For example, the agreement for \textit{FinQA} increases from $0.35$ to $0.93$ using $11$ prompts, while for \textit{NumericNLG} it increases from $0.29$ to $0.54$, suggesting that the former is more robust to prompt configurations than the latter. 
The full \benchmark{} benchmark (black line in Fig.~\ref{fig:prompt_agreement_func_prompts}) is roughly a lower bound on the prompt robustness of the model ranking across datasets.
% This variation influences the measurement of the combined effect of prompt configuration across all datasets through \benchmark{}, while the agreement serves as the approximate lower bound.

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{figures_main/Agreement_function_num_inferences__1_kendall_w_examples_xaxis.pdf}
\caption{
The improvement in model ranking consistency using different numbers of examples and prompt configurations. 
% The line thickness represents the number of prompt configurations. 
We calculate Kendall's W agreement over $30$ sets of model rankings, where each model ranking is a result of randomly selected examples and prompt configurations.
% For a given number of examples and number of prompts, we sample $30$ sets of examples and prompts. 
For example, for $2$ prompt configurations and $50$ examples, we repeat the following process $30$ times: randomly sample $2$ prompt configurations and $50$ examples, and then calculate the model ranking based on this sample. 
% $30$ samplings of $3$ prompt configurations, and for each prompt configuration sampling of $80$ random examples. 
% Then, we calculate the model ranking according to these instances. 
% Agreement (Kendall's W) is calculated over this set of $30$ rankings.
% Over the full test set ($100$ examples), using $8$ prompts increases the reliability twofold.
% \ay{1. Nice 2. Different colors to each number 3. probably better to use 1:2:4:8:16:32, and not constant jumps 4. Is it average across repatriation? 5. add confidence interval}\sa{1. thanks! 2. V 3. V 4. added 5. it seems very noisy and hard to read the fig with CI}
}
\label{fig:comp_ex_prompts}
\end{figure}
\subsection{Prompt Configs can Substitute Examples}

Figure~\ref{fig:prompt_agreement_func_prompts} shows that more prompts can help increase reliability.
A simple explanation for this is that \textit{adding more data points} helps capture model behavior. 
A common approach for this would be to add more test instances; however, increasing the size of labeled data is not always feasible. 

Thus, we also look at the relation between adding examples and adding prompts.
Figure~\ref{fig:comp_ex_prompts} depicts the ranking agreement as a function of the number of test examples, averaged over each dataset.
We see that both the number of examples and the number of prompts consistently increase reliability. Strikingly, adding prompts can have an equivalent effect to enlarging the test set; for example, a ranking based on $50$ examples and $2$ prompt configurations achieves similar reliability to a ranking based on $100$ examples over a single prompt configuration. 


To conclude, we have shown that measuring multiple prompt configurations provides an added dimension for accurate estimation of model performance. While we demonstrated this for table format, it may be applied in other ways as well \cite{liang2023holisticevaluationlanguagemodels, alzahrani2024benchmarkstargetsrevealingsensitivity, mizrahi2024state}.
This practice increases the reliability of benchmarking results. Moreover, it can help overcome the limitations of smaller test sets.




\section{Related Work}
% \paragraph{Tabular data tasks with LLMs} \ag{I think this entire subsection can be dropped}
% Early research on tabular data tasks with language models focused on developing custom table representation models \citep{herzig2020tapas, iida2021tabbie, liu2021tapex, yin2020tabert, wang2021tuta, zhao2022reastap, gu2022pasta}, leveraging structure-aware representations to capture row-column relationships. However, these models were mostly task-specific and lacked flexibility in general \citep{badaro2023transformers}.
% In recent years, pre-trained LLMs have been explored for solving diverse tabular data tasks. While not originally designed for tabular data, these general-purpose LLMs have shown adaptability when prompted or fine-tuned, leveraging their general knowledge and reasoning abilities to solve tabular tasks \citep{fang2024large, lu2024large, ruan2024language, chen2022large}. However, they face challenges with complex relational structures and dependencies, often underperforming on advanced tabular tasks that demand deeper structure-aware understanding and sophisticated table reasoning \citep{zhang2025survey, deng2024tables, zhao2023investigating}.

% \subsection{Benchmarks evaluating LLM performance on Tabular tasks}
% \paragraph{Existing Benchmarks \& Evaluations}
Several recent works systematically evaluate LLMs on tabular data tasks, primarily focusing on question-answering type tasks.
TableBench~\citep{wu2024tablebenchcomprehensivecomplexbenchmark} 
tests LLMs over
% investigates the capabilities of popular LLMs in Table Question Answering via a benchmark covering 
four major categories of QA tasks, namely fact-checking, numerical reasoning, data analysis, and visualization.
DataBench~\citep{grijalba2024question}
examines
% , another large-scale benchmark for question answering over tabular data, facilitates empirical comparisons of various LLMs by assessing 
the reasoning capabilities of LLMs in a tabular context.
TQA-Bench~\cite{qiu2024tqa}, a multi-table 
% question answering 
benchmark, evaluates 
% the capabilities of LLMs in performing 
complex question answering 
% tasks 
over relational data. TabIS \cite{pang2024uncovering} evaluates the table information-seeking capabilities of LLMs. \citet{zhao2023investigating} investigate Table-to-text capabilities 
% of different LLMs ֿ
in several real-world information-seeking scenarios.
Compared to \benchmark{}, these benchmarks are limited in the tasks they cover and the model capabilities they reflect.
% These benchmarks are limited in the tasks they cover and the model capabilities they reflect; however, this is not the case with our benchmark.

% \paragraph{Robustness of LLMs in Table Tasks}
More recently, there has been an increased focus on also analyzing the robustness of LLMs across table formats and perturbations.
% different table formats and table perturbations for tabular data tasks, 
% in addition to evaluating their performance.
\citet{singha2023tabular} explore the impact of table representation formats and noise operations on self-supervised table structure understanding tasks. Specifically, they consider a set of simple fact-finding and transformation tasks on tables to analyze how GPT-3 model performance varies.
Similarly, \citet{sui2024table} analyzes the capabilities of GPT-3.5 and GPT-4 in understanding tables by designing a specific set of table structure understanding tasks using structured data from various public datasets. %They also examine prompt variants and tabular formats to assess their impact on LLM performance in these tasks.
Several recent works 
\citep{bhandari2024robustness, zhao2023robut, liu2023rethinking} explore structural variance and adversarial perturbations on tables, and 
their impact
% its impact specifically 
on Table Question Answering 
performance.
% task performance of LLMs.
% Recent efforts have often been constrained by the data used or the models assessed. In contrast, \benchmark{} provides the first extensive and detailed analysis for an overall assessment of model robustness.
However, existing efforts are often restricted to simple synthetic table understanding tasks or a narrow set of table QA datasets and target models, thus overlooking broader challenges posed by complex table reasoning tasks.
% In contrast, \benchmark{} provides the first extensive and detailed analysis for an overall assessment of model robustness.

% \paragraph{Positioning of our work}

% Most existing benchmarks and evaluations on tabular data tasks predominantly focus on assessing the task performance of LLMs. While some recent efforts have looked at the robustness of LLMs, they are often restricted to simple synthetic table understanding tasks or a narrow set of table QA datasets and target models, overlooking broader challenges posed by complex table reasoning requirements of diverse downstream tabular data tasks. 
% Our work takes a more comprehensive approach by constructing a diverse benchmark of downstream tabular data tasks that assesses performance and robustness across domains, table formats, and perturbations, providing deeper insights into the generalizability and limitations of leading open and proprietary LLMs.
%Our work takes a more comprehensive approach, constructing a diverse benchmark of downstream tabular data tasks that assesses both performance and robustness across domains, table formats and perturbations. By evaluating leading open and proprietary LLMs, we provide deeper insights into their generalizability and limitations for tabular data tasks.

% Most existing benchmarks and evaluations on tabular data tasks predominantly focus on assessing the task performance of LLMs. While some recent efforts have 
% % begun analyzing
% looked at
% the robustness of LLMs, these are often restricted to simple synthetic table understanding tasks, or 
% to narrow sets of table question answering datasets and target models.
% % narrowly focused on select table question-answering datasets and limited models as highlighted above. 
% Although valuable, such studies remain limited in scope, overlooking the broader challenges posed by complex table reasoning requirements of diverse downstream tabular data tasks.
% Our work takes a more comprehensive approach by constructing a benchmark of diverse downstream tabular data tasks, focusing on both performance and robustness, and evaluating leading open and proprietary LLMs. 
% By analyzing performance and robustness across domains, perturbations, and table formats, over a broad spectrum of downstream tabular data tasks, we provide deeper insights into the generalizability and limitations of LLMs for tabular data.



% \begin{figure}[t]
% \centering
% \includegraphics[width=.45\textwidth]{new_figures/Prompt Agreement.pdf}
% \caption{
% Agreement between model rankings based on a single prompt.
% For most datasets we see moderate agreement; this highlights that evaluation based on a single prompt configuration is unreliable, as choosing a different prompt would have resulted in a different ranking. 
% }
% \label{fig:prompt_agreement}
% \end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=.45\textwidth]{new_figures/Agreement between Prompt Subsets in the Same Size_lineplot_100.pdf}
% % \includegraphics[width=.45\textwidth]{new_figures/Agreement of Prompt Subset with Our Ground Truth.pdf}
% \caption{
% The impact of the number of prompts on the model ranking agreement score. 
% % The agreement increases most noticeably within the single-digit range.
% % \sa{kendall w}
% }
% \label{fig:prompt_agreement_func_prompts}
% \end{figure}



% \begin{figure}
% \centering
% \includegraphics[width=.45\textwidth]{new_figures/Agreement as a function of num. of inferences__10_kendall_w.pdf}
% \caption{
% Comparing the effects of adding prompts versus adding examples, while keeping the number of inferences constant. Each approach is presented with permanent selected prompts for the dataset or with varied sampled prompts for each example. While increasing the number of examples or prompts raise the agreement in a similar manner (blue and green), the use of varied prompts for each example make them both have a dramatically higher agreement (orange and red).
% % Testing against more prompts increases the reliability (agreement with the true ranking) much faster.%, with a growing advantage in the range of 10 to 100 inferences.
% }
% \label{fig:prompt_agreement_func_inferences}
% \end{figure}


\section{Discussion}
In this work, we introduce \benchmark{}, the first comprehensive benchmark for table reasoning and robustness.
\benchmark{} provides a crucial resource for model developers and users seeking a more realistic and nuanced understanding of how LLMs perform in real-world tabular data scenarios.

Our results over a variety of state-of-the-art LLMs reveal a consistent pattern of relatively low performance on table reasoning tasks. 
Furthermore, and perhaps more strikingly, we demonstrate that models exhibit extreme sensitivity to seemingly minor variations in table formatting. As we show, this sensitivity does not reflect LLM preference for particular table formats; rather, the response to format variations reflects a more general phenomenon of LLM sensitivity to prompts.

% how this connects to recent work regarding prompting for other domains.
This finding resonates with a growing body of recent research demonstrating the brittleness of LLMs to variations in input formatting, even in seemingly non-structured aspects of textual inputs~\citep{alzahrani2024benchmarkstargetsrevealingsensitivity}. 
These studies, while focusing on different input modalities, converge on a similar conclusion: LLMs' performance can be surprisingly brittle and inconsistent when faced with even minor input variations.
This brittleness presents a real issue for reliably evaluating LLM performance~\citep{mizrahi2024state}.


% also connects to aspects of reliability
\benchmark{} directly addresses these reliability concerns by 
% not only robustly revealing the aforementioned brittleness effects in the context of table reasoning but also by 
systematically incorporating multiple prompt configurations into the evaluation protocol.
Thus, \benchmark{}
offers a significant step towards mitigating the evaluation reliability issues, and
% they introduce. By systematically incorporating a diverse array of table formatting options into the evaluation protocol, \benchmark{} 
provides a more reliable assessment of LLM capabilities.
Moreover, utilizing multiple prompt configurations can help address the limitations of smaller test sets.

% wrap it up
To conclude, \benchmark{} serves as a crucial benchmark for orienting future advancements in LLM table reasoning. At the same time, it sets an example for robust evaluation methodologies that are more reflective of real-world performance. We encourage the AI community to adopt this benchmark and refine the practices it introduces when developing new benchmarks.


% \benchmark{} serves as a crucial benchmark for orienting future advancements in LLM table reasoning. 
% \textit{
% We anticipate that \benchmark{} will guide the development of more robust and reliable LLMs for table reasoning.
% At the same time, we hope it will set a general example for evaluation methodologies that are less susceptible to input formatting and more reflective of real-world performance.
% }



\section{Limitations}

\paragraph{Table Tasks} \benchmark{} includes tasks in which input tables are directly embedded within the prompt. This may not cover all real-world scenarios where models also need to extract or search table data independently.

\paragraph{Selected Datasets} \benchmark{} relies on datasets and evaluation metrics created by external sources, which may introduce biases or inconsistencies that do not fully align with the intended evaluation goals.

\paragraph{Consistency vs. Reliability} Our analysis relies on the assumption that consistency in model ranking is a valid indicator of reliability. However, this may not always hold, as certain biases or dataset-specific factors could influence rankings independently of true model performance~\cite{perlitz2024llmbenchmarksagreefixing}.

\section*{Acknowledgments}

We thank Ella Rabinovich for her valuable contributions in shaping the metrics used in this work. Her insights and feedback significantly improved the design of \benchmark{} and strengthened the analysis presented in this paper.


\bibliography{references}
% \bibliographystyle{acl_natbib}

\appendix


\clearpage
\section{\benchmark{} Benchmark} \label{appendix:benchmark_details}

In this section, we provide more details about the benchmark and the decisions made as part of its development.


\subsection{Overview of Datasets in \benchmark{}}

\begin{enumerate}

    \item \textbf{FinQA} \citep{chen2022finqadatasetnumericalreasoning} – An expert-annotated question answering (QA) dataset designed to tackle numerical reasoning in real-world financial data. FinQA contains questions that require models to perform complex operations, such as multi-step calculations and logical reasoning over financial reports containing both text and table.

    \item \textbf{TableBench} \citep{wu2024tablebenchcomprehensivecomplexbenchmark} – A dataset designed to evaluate a model's table question answering capabilities across various tasks. It includes 18 distinct sub-tasks grouped into four major categories: Fact Verification (FV), Numerical Reasoning (NR), Data Analysis (DA), and visualizations.
        
    \item \textbf{WikiTableQuestions} \citep{pasupat-liang-2015-compositional} – A dataset designed for question answering over tables sourced from Wikipedia. It often requires reasoning over table data, including operations like aggregation, comparisons, and filtering, to derive accurate answers.
    
    \item \textbf{TabFact} \citep{chen2020tabfactlargescaledatasettablebased} – A large dataset focused on fact verification using tables, containing Wikipedia tables paired with human-annotated statements. The task is to determine whether a given statement is supported, refuted, or unverifiable based on the information in the table often requiring logical and numerical reasoning over table data.

    \item \textbf{QTSumm} \citep{zhao2023qtsummqueryfocusedsummarizationtabular} - A summarization dataset focused on query-based summarization, where summaries are generated based on specific user queries to retrieve relevant information from a table. 

    \item \textbf{Scigen} 
    \citep{moosavi2021scigen} - 
    A dataset designed for reasoning-aware data-to-text generation, featuring tables from scientific articles along with their corresponding descriptions.

    \item \textbf{Numeric nlg}
    \citep{suadaa-etal-2021-towards} - 
    A dataset for table-to-text generation that pairs tables with their corresponding descriptions from scientific papers, with a focus on numerical-reasoning texts.

    \item \textbf{TURL (Table Understanding through Representation Learning)} \citep{deng2020turltableunderstandingrepresentation} – 
    The TURL Column Type Annotation (CTA) dataset, derived from Wikipedia tables, supports semantic type assignment to table columns from a given list of Freebase types. It tests models' ability to understand the meaning of table columns in context.
    %This dataset is designed for various table-based tasks, including relation extraction, entity linking, and table type classification. We focus on the \textit{column annotation} task, where the objective is to annotate each column of a table with its semantic type (e.g., person, location, organization) from a closed long list. The dataset consists of large-scale tables from Wikipedia, and the column annotation task tests models' ability to understand the meaning of table columns in context.
        
\end{enumerate}


\subsection{Evaluation metrics}

We evaluated each of the datasets using the same metrics as reported in the original papers, with some exceptions: (1) For \textbf{WikiTableQuestions}, while the original paper focuses on exact-match for scoring the prediction, we assessed models using F1 score\footnote{We tokenized both the reference and predicted tokens, where true positives are determined by the intersection of these token sets, false positives are the tokens present in the reference set but absent from the predicted set, and false negatives are the tokens in the predicted set that are missing from the reference set.}, to provide a better normalization of model performance. (2) In the case of \textbf{FinQA}, we calculated both execution accuracy and program accuracy, but used the latter as the main metric since execution accuracy tends to overestimate the performance. (3) As for \textbf{Scigen}, since the base paper states that none of the evaluated metrics align with human judgment, we adopted the primary metric of NumericNLG, as they correspond to the exact same task.

% align with human judgment, we adopted the LLM as judge approach and used GPT-4 as the evaluator.


\begin{table*}[h]
    \centering
    \begin{tabular}{|l|p{10cm}|}

    \hline
    \textbf{Format} & \textbf{Example Representation} \\ \hline
    \textbf{HTML} & 
    \texttt{<table><thead>
    \newline<tr><th>Name</th><th>Age</th><th>Sex</th></tr>
    \newline</thead><tbody>
        \newline<tr><td>Sophia</td><td>26</td><td>F</td></tr>
        \newline<tr><td>Aarav</td><td>34</td><td>M</td></tr>
        \newline<tr><td>Oliver</td><td>30</td><td>M</td></tr>
    \newline</tbody>
    </table>
    } \\ \hline

    \textbf{CSV} & 
    \texttt{Name, Age, Sex\newline
Sophia, 26, F\newline
Aarav, 34, M\newline
Oliver, 30, M} \\ \hline

    \textbf{JSON} & 
    \texttt{\{``0": \{``Name": ``Sophia", ``Age": ``26", ``Sex": ``F"\}, ``1": \{``Name": ``Aarav", ``Age": ``34", ``Sex": ``M"\}, ``2": \{``Name": ``Oliver", ``Age": ``30", ``Sex": ``M"\}\}} \\ \hline

    \textbf{Markdown} & 
    \texttt{|Name|Age|Sex|\newline
|---|---|---|\newline
|Sophia|26|F|\newline
|Aarav|34|M|\newline
|Oliver|30|M|} \\ \hline

    \textbf{Indexed Row Major} & 
    \texttt{col : Name | Age | Sex row 1 : Sophia | 26 | F row 2 : Aarav | 34 | M row 3 : Oliver | 30 | M} \\ \hline

    \textbf{DataFrame} & 
    \texttt{pd.DataFrame(\{``Name": [``Sophia", ``Aarav", ``Oliver"], ``Age": [26, 34, 30], ``Sex": [``F", ``M", ``M"]\}, index=[0, 1, 2])} \\ \hline

    \textbf{Concatenation} & 
    \texttt{Name Age Sex Sophia 26 F Aarav 34 M Oliver 30 M} \\ \hline
        
    \end{tabular}
    \caption{Example representations of serialization formats used in \benchmark{}.}
    \label{tab:serialization_formats}
\end{table*}


\begin{table*}
    \centering
    \begin{tabular}{|l|p{6cm}|}

    \hline
    \textbf{Perturbation} & \textbf{Example Transformation} \\ \hline
    \textbf{Row Swapping} & \texttt{Name, Age, Sex\newline
Aarav, 34, M\newline
Oliver, 30, M\newline
Sophia, 26, F} \\ \hline
    \textbf{Column Swapping} & \texttt{Age, Name, Sex\newline
26, Sophia, F\newline
34, Aarav, M\newline
30, Oliver, M} \\ \hline
    \textbf{Transpose} & \texttt{, 0, 1, 2\newline
Name, Sophia, Aarav, Oliver\newline
Age, 26, 34, 30\newline
Sex, F, M, M} \\ \hline
    \textbf{Add Empty Rows} & \texttt{Name, Age, Sex\newline
, ,\newline
Sophia, 26, F\newline
, ,\newline
Aarav, 34, M\newline
Oliver, 30, M} \\ \hline
        
    \end{tabular}
    \caption{Structural perturbations with CSV serializer format.}
    \label{tab:perturbations_example}
\end{table*}
\subsection{Prompt Configurations} \label{appendix:table_forms_details}

\subsubsection{Serializations}

As tables can be presented in various formats during pre-training, models may have biases in how they interpret them as demonstrated by \cite{sui2024table}. Several standard methods exist for converting structured data into text formats that can be embedded in a prompt. We select seven of the most commonly used serializations: \textit{HTML, CSV, JSON, Markdown, Indexed Row Major, Data Frame and Concatenation}. An example representation of each serialization format is provided in Table \ref{tab:serialization_formats}.

\subsubsection{Structural Perturbations} \label{subsec: structural_perturb}

These perturbations present a slightly different structure of the table from the original one, without any change to the content or relations between the table cells. Since the information is preserved but presented in a different form, there should be no significant change in the model performance ideally. The perturbations considered are as follows:
\begin{itemize}
    
    \item \textbf{Row Swapping}: The process of exchanging the positions of rows within a table.
    
    \item \textbf{Column Swapping}: The act of changing the positions of columns in a table.

    \item \textbf{Transpose}: The action of switching the rows and columns of a standard table, transforming rows into columns and vice versa. 

    \item \textbf{Add Empty Rows}: An addition of some empty rows to the table.
\end{itemize}

Refer to Table \ref{tab:perturbations_example} for an example of these structural perturbations with CSV as a base serializer.
\subsection{Challenges}

Building a robust benchmark for table reasoning like \benchmark{} presented several challenges, particularly related to dataset quality and evaluation consistency.

\paragraph{Saturated Data.} 
Many existing table datasets are derived from the same sources, particularly Wikipedia tables, which large language models have likely seen during pre-training. As a result, these datasets may be less challenging, as models can rely on memorization rather than genuine reasoning.

\paragraph{Evaluation Rigor.}
The evaluation metrics used in prior work do not always accurately reflect model performance. Several studies indicate that automatic metrics often fail to align with human evaluations, leading to misleading conclusions about model capabilities. This inconsistency makes it difficult to assess true reasoning ability and robustness.

\paragraph{Unreliable Data.} 
Some datasets rely on automatically aggregated structured data, which can introduce inconsistencies and errors. For instance, a dataset with high potential for benchmarking included web tables, but the extracted table HTML was broken at times, making it unreliable for structured reasoning tasks as such issues can compromise the integrity of the benchmark.







\clearpage
\section{Performance Analysis} \label{app:perform_analysis}

This part of the paper presents additional analyses and graphs illustrating model performance.


\subsection{Model Performance across Datasets}
Figure~\ref{fig:model_performance} compares the performance of all models across each dataset in the benchmark. In general, performance tends to be lower on more challenging datasets- FinQA, TableBench Numerical Reasoning, TableBench Data Analysis as well as Table2Text ones. On the other hand, performance scores are relatively higher on question answering and fact-checking datasets including WikiTQ, TabFact and others.

\begin{figure*}[b]
    \centering
    \includegraphics[width=0.8\linewidth]{figures_appendix/families1.pdf}
    % \includegraphics[width=1.2\linewidth]{new_figures/families1.pdf}
    \caption{Performance comparison of the strongest model from each family.}
    \label{fig:model_performance}
\end{figure*}

\subsection{Open vs. Closed models}

Figure~\ref{fig:open_closed} presents the performance comparison between open-weight and closed-weight models across all the benchmark datasets. As can be seen, closed-weight proprietary models considerably outperform open models across most benchmark datasets.

\begin{figure}
\includegraphics[width=0.45\textwidth]{figures_appendix/open_closed.pdf}
\caption{Comparison of open weight and closed models.}
\label{fig:open_closed}
\end{figure}

\clearpage
\section{Robustness Analysis} \label{app:analysis}

In this section, we present additional analyses of the key components of prompt configurations: serializers and structural perturbations.

We provide both performance scores and win rate graphs. All scores are calculated in a manner similar to that described in \ref{subsec:metrics}. Win rate calculations were performed at the example level: for each model's score on an example, we counted the number of times it was higher than the others, and then normalized it by the total number of wins over this example. For example, given the following scores of $7$ serializers across an example: $[1, 1, 1, 0, 0, 0, 0]$, the counting vector will be $[4, 4, 4, 0, 0, 0, 0]$, and the normalized win rate scores will be $[1/3, 1/3, 1/3, 0, 0, 0, 0]$.

\subsection{Serializers Impact} \label{app:analysis_sec:serializers}

Here, we provide more in-depth analysis of trends concerning serializers. We approach this in steps: starting with the lowest level of aggregation for each model-dataset pair, we then move to model-oriented and dataset-oriented aggregations, and finally conclude with an overall aggregation of the serialization results.

\begin{table*}[b]
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lrrrrrrrrrr}
\toprule
 & FinQA & \makecell{Numeric-\\NLG} & QTSumm & SciGen & \makecell{TURL\\ CTA} & \makecell{Tab \\ Fact }& \makecell{TableBench \\ DA} & \makecell{TableBench \\FC} & \makecell{TableBench \\NR} & WikiTQ \\
%  & FinQA 
% & \shortstack{Numeric-\\NLG} 
% & QTSumm 
% & SciGen 
% & \shortstack{TURL\\ CTA} 
% & \shortstack{Tab \\ Fact }
% & \shortstack{TableBench \\ DA} 
% & \shortstack{TableBench \\ FC} 
% & \shortstack{TableBench \\ NR} 
% & WikiTQ \\
 % & FinQA & NumericNLG & QTSumm & SciGen & TURL CTA & TabFact & TableBench DA & TableBench FC & TableBench NR & WikiTQ \\
 % &  &  &  &  &  &  &  &  &  &  \\
\midrule
claude-3-5-haiku & .04 & .01 & .01 & .01 & .02 & .06 & .02 & .07 & .11 & .05 \\
claude-3-5-sonnet & .02 & .01 & .01 & .00 & .02 & .03 & .01 & .08 & .07 & .02 \\
deepseek-v3 & .04 & .02 & .01 & .00 & .02 & .06 & .02 & .03 & .04 & .04 \\
gemini-1.5-flash & .03 & .01 & .10 & .00 & .02 & .08 & .01 & .04 & .06 & .03 \\
gemini-1.5-pro & .04 & .01 & .03 & .01 & .04 & .05 & .01 & .04 & .05 & .05 \\
gpt-4o & .05 & .00 & .01 & .00 & .04 & .04 & .01 & .04 & .05 & .03 \\
gpt-4o-mini & .06 & .01 & .01 & .01 & .03 & .09 & .01 & .05 & .06 & .03 \\
llama-3-1-405b-i & .04 & .07 & .02 & .06 & .04 & .04 & .05 & .23 & .06 & .03 \\
llama-3-1-70b-i & .02 & .07 & .01 & .03 & .04 & .06 & .04 & .20 & .04 & .04 \\
llama-3-1-8b-i & .03 & .01 & .02 & .01 & .17 & .07 & .07 & .22 & .07 & .02 \\
mistral-7b-i & .06 & .04 & .02 & .04 & .03 & .08 & .01 & .14 & .04 & .04 \\
mixtral-8x22b-i & .04 & .01 & .02 & .01 & .02 & .04 & .01 & .09 & .06 & .06 \\
mixtral-8x7b-i & .05 & .03 & .03 & .02 & .16 & .06 & .02 & .11 & .03 & .09 \\
qwen2-72b-i & .06 & .07 & .01 & .05 & .02 & .03 & .01 & .06 & .03 & .02 \\
\bottomrule
\end{tabular}
    }
    \caption{The largest variations in model scores across different serializers, presented for each dataset.}
    \label{table:serializers_score_diff}
\end{table*}

\subsubsection{Model-Dataset Score Variability}
To measure the extent of variability with respect to serializers, we calculated the score for each model-dataset pair across all serializers and computed the difference between the highest and lowest scores. The result is shown in Table~\ref{table:serializers_score_diff}.
% shows the maximum difference in model performance across serializers for each dataset. 
While on average the drop in score is about $0.05$, there are some exceptional cases were it can be much worst, e.g. \textit{llama-3-1-8b-i} has a drop in score of $0.22$ over \textit{TableBench FC}.


\subsubsection{Serializer Preference across Models}
To identify trends related to models, we aggregate the results across models, as illustrated in Figure \ref{fig:serializers_win_across_models}. This figure shows the serializer preferences of all models within our benchmark, indicating that different models exhibit varying preferences, with no single serializer consistently ranking highest across all models. 
% However, certain model families show a slight inclination toward specific serializers. 
Overall, since preferences are highly model-dependent, selecting the most suitable serializer may require case-by-case tuning.

\subsubsection{Serializer Preference across Datasets}
We explored signals indicating which serializers perform better for different tasks and datasets, as shown in Figure \ref{fig:serializers_win_across_datasets}.
% Figure \ref{fig:serializers_win_across_datasets} presents the win-rate of different serializers for each dataset of our benchmark. 
The results show that no single serializer consistently outperforms others across all datasets. In some cases, the ranking differences among serializers is marginal, while in others, a clear performance gap emerges. This emphasizes the need for dataset-specific evaluation rather than a one-size-fits-all approach.


\subsubsection{Overall Serializer Preferences}

The results presented above show a lack of consistency in preferences. Unsurprisingly, aggregating all the results across models and datasets
% , as shown in Figure~\ref{fig:serializers}, 
demonstrates that no serializer consistently outperforms others.

% Figure \ref{fig:serializers}  shows the win rate of serializers, representing the percentage of times a serializer outperforms others aggregated across all models and datasets. Overall, it appears that no serializer consistently outperforms others.

% \begin{figure}[h!]
% % \includegraphics[width=0.95\columnwidth]{new_figures/Serializer Win Rate.pdf}
% \centering
% \includegraphics[width=0.35\textwidth]{new_figures/Serializer Win Rate.pdf}
% \caption{Win rate of serializers averaged across all models and datasets.%\raj{Figure to be updated}\sa{updated}
% }
% \label{fig:serializers}
% \end{figure}
\begin{figure*}
\includegraphics[width=0.95\textwidth]{figures_appendix/Win_serializer_models.pdf}
\caption{Model-wise win-rate comparison of serializers in \benchmark{}. Some models don't show clear preferences, like \textit{gpt-4o}, while others show a preference for one or more serializers. For example, \textit{llama-3-1-405b-i} prefers DF serialization.
}
\label{fig:serializers_win_across_models}
\end{figure*}

\begin{figure*}
% \includegraphics[width=0.45\textwidth]{new_figures/Serializer winning percents across Dataset.pdf}
\includegraphics[width=0.95\textwidth]{figures_appendix/Win_serializer_datasets.pdf}
\caption{
Dataset-wise win-rate comparison of serializers in \benchmark{}. Surprisingly, the \textit{concat} serialziation, which may render the prompt unreadable due to the lack of detailed separation, doesn't present a dramatically low performance across datasets and tasks.
}
\label{fig:serializers_win_across_datasets}
\end{figure*}


\clearpage
\subsection{Perturbations Impact} 


\begin{figure*}[b]
    \centering
    \includegraphics[width=\textwidth]{figures_appendix/Change_Performance_Prompts_no_abs_Model.pdf}
    \caption{Differences in model score for perturbations with respect to the serializer. Overall, the effect of each perturbation seems to be very low and not consistent.}
    \label{fig:perturb_no_abs_models}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{figures_appendix/Change_Perturbs_Dataset.pdf}
\caption{Mean Absolute Impact of different perturbations on benchmark datasets. The signal of differences in score appears to be a part of the datasets rather than related to perturbations.
}
\label{fig:compare_perturbs_dataset}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{figures_appendix/Change_Perturbs_Model.pdf}
\caption{Mean Absolute Impact on each model performance due to perturbations. The average model robustness score (presented in Table~\ref{table:perform}) appears to correlate with the model's robustness across different perturbations.}
\label{fig:compare_perturbs_model}
\end{figure*}

Here, we present a more detailed analysis of trends associated with the structural perturbations in \benchmark{}. We begin with the low-level aggregations of the differences between scores with and without perturbations, and then we aggregate the overall results.



\subsubsection{Perturbations Impact}
To gain deep insights into the impact of perturbations on performance, we analyzed the score differences introduced by each perturbation across benchmark models. 
We established a baseline using prompts with a specific serializer and no perturbations, and then measured the score deviations caused by each perturbation in comparison to this baseline.

The result, depicted in Figure~\ref{fig:perturb_no_abs_models}, reflects small yet inconsistent variability in model scores. Models exhibit an average difference of $0.03$ in overall performance.

\subsubsection{Granular Look: Absolute Perturbations Impact}

While some perturbations exhibit a strong positive or negative impact on certain examples, their effects vary inconsistently across all datasets and models. 
% Averaging over them, we found the impact of the perturbations to be approximately $0.03$ on model overall score.
A simple average, as we see in Figure~\ref{fig:perturb_no_abs_models}, can be misleading as effects may cancel out. Here we report Mean Absolute Impact, which quantifies the overall effect using absolute score changes ($\Delta$). Let us denote $i \in N$ the number of examples, and $\Delta_i$ the difference between observed and expected scores of perturbation with respect to its baseline. The Mean Absolute Impact is defined as: \[ \text{Mean Absolute Impact}  = \frac{1}{N} \sum_{i=1}^N |\Delta_i| \]

Figure~\ref{fig:compare_perturbs_dataset} shows the Mean Absolute Impact on performance scores due to different perturbations for each benchmark dataset, averaged across all models. 
The impact is higher for Table QA and Fact-checking datasets and lower for Table-to-Text datasets. Similarly Figure~\ref{fig:compare_perturbs_model} displays the Mean Absolute Impact on performance scores for each model, averaged across all benchmark datasets.
The impact of perturbations is more pronounced in smaller models compared to larger models in general. Overall, it seems that both datasets and models are key factors that influence robustness.



\label{app:analysis_sec:perturbs}
\subsubsection{Overall Perturbations Effect}

% While the pertubatios presented potentially large differences from the defied baselines, averaging over them across models shows low 
Figure \ref{fig:augmenter} presents the win-rate of structural perturbations, representing the percentage of times a perturbation outperforms all others aggregated across the benchmark. The win rate results are nearly identical across all perturbations, indicating that no single perturbation consistently leads to under-performance or out-performance of models when aggregated across the benchmark datasets and models. 

\begin{figure}[h]
\includegraphics[width=0.4\textwidth]{figures_appendix/Augmenter_Win_Rate.pdf}
\caption{Win rate of structural perturbations across all models and datasets of \benchmark{}.
}
\label{fig:augmenter}
\end{figure}




\clearpage
% \null
% \clearpage

% \section{Additional Analysis}
% \subsection{"Lossy" Perturbations Impact}
% Here, we additionally consider few "Lossy" perturbations that may slightly alter the content and clarity of the table in a limited experiment setup. These perturbations may alter table header or add some duplicate information to the table. In most cases, this may not affect the samples (for example in extraction questions like: "Who won in the first round?"), while in some select cases it might change the answer (for example when an aggregation is required: "Count the number of rows with condition X") though it is very rare for the benchmark datasets. The perturbations considered are:

% \begin{itemize}
%     \item \textbf{Mask Column Names}: The process of obscuring column names in a table.

%     \item \textbf{Shuffle Column Names}: The random re-arrangement of column names within a table.

%     \item \textbf{Duplicate Rows}: The creation of identical copies of rows within a table.

%     \item \textbf{Duplicate Columns}: The creation of identical copies of columns within a table.

% \end{itemize}

% % \begin{figure}
% % \includegraphics[width=0.45\textwidth]{new_figures/Win rate of Augmenter (with lossy).pdf}
% % \caption{Win rate of perturbation including lossy perturbs\raj{To be updated}}
% % \label{fig:augmenter_including_lossy}
% % \end{figure}

% When comparing the structural perturbations with these "lossy" perturbations, as illustrated in Figure \ref{fig:augmenter_including_lossy}, the win rates are nearly identical across both types too, with structural perturbations yielding slightly higher win rate percentages. This reiterates the finding that the impact of perturbations on performance is not consistent when aggregated across datasets and models considered in our benchmark.


% \newpage
% \begin{figure}
% \includegraphics[width=0.45\textwidth]{new_figures/Win rate of Serializer across models.pdf}
% \caption{}
% \label{fig:serializers_models_win_rate}
% \end{figure}



% \begin{figure}
% \includegraphics[width=0.45\textwidth]
% {new_figures/Win rate of Serializer across datasets.pdf}
% \caption{Win rate of serializers across Datasets}
% \label{fig:serializers_datasets_win_rate}
% \end{figure}




% \newpage
% \section{Model names} \label{appendix:model_names}


% \begin{table}[ht]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Model & Short Name \\
% \hline
% gpt-4o-mini-2024-07-18 & gpt4o-mini\\
% llama-3-1-70b-instruct & llm3.1-70b\\
% qwen2.5-coder-32b-instruct & qw2.5-32b\\
% qwen2.5-72b-instruct & qw2.5-72b\\
% mistral-7b-instruct-v0.3 & mist-7b\\
% mixtral-8x7b-instruct-v0.1 & mixt-8x7b\\
% mixtral-8x22b-instruct-v0.1 & mixt-8x22b\\
% mistral-large-instruct-2407 & mist-large\\
% \hline
% \end{tabular}
% \caption{}
% \end{table}


% \sa{find the right place}
% We first calculated for each model and example a vector that counts for each serialization how many times it wins over others\footnote{We assigned the same count to tied scores. For example, the following scores vector: $[1, 1, 1, 0, 0, 0, 0]$ will become the following win-rate vector: $[1/3, 1/3, 1/3, 0, 0, 0, 0]$}, and normalized it by the total number of winning achieved. Then, we summed the scores for each serializer with respect to example. The results can be seen in \ref{fig:serializers} and demonstrate that there is no dominant winner. We also tried to analyze model preferred serialzier, with a similar metric, and got a preference but with a relatively weak signal. Additional details and figures can be found in Appendix \ref{appendix:additional_analysis}.
% \newpage

% \begin{tikzpicture}[
%     box/.style={rectangle, rounded corners=3pt, draw=black, thick, inner sep=8pt, fill=white},
%     titlebox/.style={rectangle, rounded corners=3pt, draw=black, thick, inner sep=4pt, fill=yellow!20},
%     titlebox2/.style={rectangle, rounded corners=3pt, draw=black, thick, inner sep=4pt, fill=purple!20},
%     sectiontitle/.style={font=\sffamily\Large\bfseries\uppercase, text centered},
%     contenttext/.style={text width=6cm, align=left, font=\sffamily}, % Increased text width to 6cm
%     tabletext/.style={font=\ttfamily\footnotesize, align=left, text width=6cm-16pt} % Increased table text width to match contenttext
% ]

% % Prompt Config 1 Box
% \node[titlebox] (prompt1_title) at (4, 9) {\textbf{Prompt Config 1}}; % Shifted to the left
% \node[box, contenttext] (prompt1_box) at (4, 4.5) {
%     \sectiontitle{INSTRUCTION}
%     \vspace{2pt}
%     Given a table and statement classify the entailment of the statement to one of refuted, entailed. \\
%     You should only output the result. Do not add any explanation or other information.
%     \vspace{5pt}
%     \sectiontitle{TABLE CONFIG}
%     \vspace{2pt}
%     \subsectiontitle{SERIALIZER:} CSV \\
%     \subsectiontitle{PERTURBATION} \text{(optional): Shuffle Rows}
%     \vspace{5pt}
%     \sectiontitle{STATEMENT/QUESTION}
%     \vspace{2pt}
%     racing like a pro be the most viewed episode
%     \vspace{5pt}
%     \sectiontitle{PREDICTION \& EVALUATION}
%     \vspace{2pt}
%     \textbf{ENTAILLED} \textcolor{green!70!black}{\Huge \checkmark}
% };

% % % Table content for Prompt Config 1 - OUTSIDE the box, below it
% % \node[tabletext, anchor=north west, below=0pt of prompt1_box.south west, yshift=-10pt] (table1_content) {
% % \texttt{no in series,no in season,title,directed by,us viewers (millions)} \\
% % \texttt{91,3,"my way home is through you",david jackson,2.72} \\
% % \texttt{89,1,"4 years, 6 months, 2 days",greg prange,3.36} \\
% % \texttt{92,4,"it 's alright, ma (i'm only bleeding)",janice cooke,3.04} \\
% % \texttt{90,2,"racing like a pro",paul johansson,3.57}
% % };

% % Prompt Config 2 Box
% \node[titlebox2] (prompt2_title) at (10, 9) {\textbf{Prompt Config 2}}; % Shifted to the right
% \node[box, contenttext] (prompt2_box) at (10, 4.5) {
%     \sectiontitle{INSTRUCTION}
%     \vspace{2pt}
%     Given a table and statement classify the entailment of the statement to one of refuted, entailed. \\
%     You should only output the result. Do not add any explanation or other information.
%     \vspace{5pt}
%     \sectiontitle{TABLE CONFIG}
%     \vspace{2pt}
%     \subsectiontitle{SERIALIZER:} Markdown \\
%     \subsectiontitle{PERTURBATION} \text{(optional): -}
%     \vspace{5pt}
%     \sectiontitle{STATEMENT/QUESTION}
%     \vspace{2pt}
%     racing like a pro be the most viewed episode
%     \vspace{5pt}
%     \sectiontitle{PREDICTION \& EVALUATION}
%     \vspace{2pt}
%     \textbf{REFUTED} \textcolor{red}{\Huge \textbf{X}}
% };

% % % Table content for Prompt Config 2 - OUTSIDE the box, below it
% % \node[tabletext, anchor=north west, below=0pt of prompt2_box.south west, yshift=-10pt] (table2_content) {
% % \texttt{| no in series | no in season | title                         | directed by    | us viewers (millions) |} \\ \hline
% % \texttt{|--------------|-------------|-------------------------------|----------------|-----------------------|} \\
% % \texttt{| 89           | 11          | 4 years , 6 months , 2 days   | greg prange    | 3.36                  |} \\
% % \texttt{| 90           | 12          | racing like a pro             | paul johansson | 3.57                  |} \\
% % \texttt{| 91           | 13          | my way home is through you     | david jackson  | 2.72                  |} \\
% % \texttt{| 92           | 14          | it 's alright, ma (i'm only bleeding)| janice cooke   | 3.04                  |}
% };

% % Dashed border around everything
% \node[draw=black, dashed, thick, inner xsep=15pt, inner ysep=8pt, fit={(prompt1_title) (prompt1_box) (table1_content) (prompt2_title) (prompt2_box) (table2_content)}] (border) {};

% \end{tikzpicture}


% \begin{table*}[h]
% \centering
% \resizebox{\textwidth}{!}{%
% \small
% % \setlength{\arrayrulewidth}{0.5mm}  % Set thickness of lines
% \begin{tabular}{p{2.2cm} p{1.5cm} p{2.0cm} p{3cm} *{3}{>{\centering\arraybackslash}m{1.6cm}}} % Final adjusted column widths
% \toprule
% \textbf{Dataset} & \textbf{Task} & \textbf{Domain} & \textbf{Metrics} & \textbf{Knowledge Extraction} & \textbf{Textual Reasoning} & \textbf{Numerical Reasoning} \\
% \midrule
% \rowcolor{white} FinQA & \RaggedRight Table QA & Finance & Program accuracy, \newline Execution accuracy & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
% \midrule
% \rowcolor{white} TableBench DA & \RaggedRight Data Analysis & Finance, \newline Sports, etc. & ROUGE & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
% \midrule
% \rowcolor{white} TableBench NR & \RaggedRight Table QA & Finance,\newline Sports, etc. & ROUGE & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark} \\
% \midrule
% \rowcolor{white} TableBench FC & \RaggedRight Fact Verification & Finance,\newline Sports, etc. & ROUGE & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
% \midrule
% \rowcolor{white} WikiTQ & \RaggedRight Table QA & Wikipedia & F1 Strings, \newline Exact Match & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
% \midrule
% \rowcolor{white} TabFact & \RaggedRight Fact Verification & Wikipedia & Accuracy & \textcolor{teal}{\checkmark} & \textcolor{teal}{\checkmark}  & \textcolor{orange}{\textasciitilde}  \\
% \midrule
% \rowcolor{white} QTSumm & \RaggedRight Table-to-Text QA & Wikipedia & ROUGE, BLEU, \newline METEOR, BertScore & \textcolor{teal}{\checkmark} & \textcolor{orange}{\textasciitilde} & \textcolor{orange}{\textasciitilde} \\
% \midrule
% \rowcolor{white} SciGen & \RaggedRight Table-to-Text & Science (Arxiv) & LLM as Judge, BLEU, \newline METEOR, BertScore, \newline MoverScore, BLEURT & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{orange}{\textasciitilde} \\
% \midrule
% \rowcolor{white} NumericNLG & \RaggedRight Table-to-Text & Science & ROUGE, BLEU, \newline METEOR, BertScore, \newline PARENT & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{orange}{\textasciitilde} \\
% \midrule
% \rowcolor{white} TURL & \RaggedRight Classification & Wikipedia & Exact Match & \textcolor{orange}{\textasciitilde} & \textcolor{purple}{\boldmath$\times$} & \textcolor{purple}{\boldmath$\times$} \\
% \bottomrule
% \end{tabular}
% }
% \parbox{\linewidth}{\small
% \begin{tabular}{p{3cm} p{3cm} p{3cm} p{3cm} p{3cm}}
%    \\
%    & \textbf{Legend:} & Required \textcolor{teal}{\checkmark} &  Partially Required \textcolor{orange}{\textasciitilde} & Not Required \textcolor{purple}{\boldmath$\times$}\\
% \end{tabular}
% }

% \caption{The selected datasets for \benchmark{} along with their properties (the first mentioned metric is the primary metric). The columns on the right reflect the required skills to solve it based on our analysis \ref{sec}, divided into 3 categories.
% \ay{1. I shortened the names of the dataset, the DA NR and FC need to appear somewhere, but not in the table, maybe in the caption 2. Shorten the domains, consider changing TableBench to diverse. 3. There are a lot of metrics, we use all of them for each example. If not keep the one we use, in app. talked about all the rest. 4. symbols are better, I think they can be larger 5. also I center them horizontally but not vertically, but this is a small point.}
% }
% \label{tab:datasets_app}

% \end{table*}

% \newpage


\clearpage
\section{Additional Properties of \benchmark{}}

Another perspective on benchmark variability can be gained by examining it at the example level. We present two analysis that relate to example difficulty and can serve for indicating better on this variability in \benchmark{}.

\subsection{Score Distribution in \benchmark{}}

To gain insight into the difficulty of each dataset, we analyze the scores achieved by the models across different prompt configurations, focusing on the score distribution. The distribution patterns show notable differences, even when datasets were evaluated with the same metric, as shown in Figure \ref{fig:example_dist}.
For example, datasets like TableBench, Numeric NLG, and QTSumm, which all use \textit{ROUGE}, exhibited distinct score patterns. This suggests that the dataset itself has a strong and varied impact on model performance. 


\begin{figure}[h]
     \centering
    \includegraphics[width=0.5\textwidth]{figures_appendix/Examples_Dist_Dataset.pdf}
    \caption{Score distribution across \benchmark{} datasets.
    }
    \label{fig:example_dist}
\end{figure}

\subsection{Example Difficulty in \benchmark{}}

We also analyze the difficulty of examples within our benchmark by computing the mean score for each example across all models and prompt configurations. We then examine the distribution of these aggregated mean scores across datasets, as illustrated in Figure \ref{fig:example_dist_difficulty}. The figure indicates that our benchmark encompasses both easy and challenging examples, with the majority falling within a medium difficulty range.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures_appendix/examples_diff_datasets.pdf}
    \caption{Example difficulty distribution for each dataset in \benchmark{}. Some datasets like \textit{FinQA} and \textit{TableBench} ones include examples with different levels of difficulty for the models, while others like Table-to-Text ones seem to include examples with a similar level.
    }
    \label{fig:example_dist_difficulty}
\end{figure}


\newpage
\null
\newpage

\section{Full Names}

\subsection{Model Names}\label{appendix:model_names}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Full Model Name} & \textbf{Short Name} \\
\hline
qwen2-72b-instruct & qwen2-72b-i \\
mixtral-8x22b-instruct-v0.1 & mixtral-8x22b-i \\
mixtral-8x7b-instruct-v0.1 & mixtral-8x7b-i \\
mistral-7b-instruct-v0.3 & mistral-7b-i \\
llama-3-1-70b-instruct & llama-3-1-70b-i \\
llama-3.1-70b-instruct & llama-3-1-70b-i \\
llama-3.1-405b-instruct & llama-3-1-405b-i \\
llama-3.1-8b-instruct & llama-3-1-8b-i \\
gpt-4o-mini-2024-07-18 & gpt-4o-mini \\
gpt-4o-2024-11-20 & gpt-4o \\
gemini-1.5-flash-002 & gemini-1.5-flash \\
gemini-1.5-pro-002 & gemini-1.5-pro \\
claude-3-5-sonnet-20241022 & claude-3-5-sonnet \\
claude-3-5-haiku-20241022 & claude-3-5-haiku \\
\hline
\end{tabular}
\caption{Mapping of model names to short names.}
\label{tab:model_names}
\end{table}

\subsection{Dataset Names}\label{appendix:dataset_names}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\hline
\textbf{Full Dataset Name} & \textbf{Short Name} \\
\hline
TURL - Column Type Annotation & TURL CTA \\
TableBench - Data Analysis & TableBench DA \\
TableBench - Numerical Reasoning & TableBench NR \\
TableBench - Fact Verification & TableBench FC \\
\hline
\end{tabular}
\caption{Mapping of dataset names to short names.}
\label{tab:dataset_names}
\end{table}


\newpage
\null
\newpage

\section{Prompt Example}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures_appendix/prompt_example.pdf}
    % \captionsetup{width=\textwidth} 
    \caption{A prompt example from our benchmark, demonstrating the template and instructions used for \textit{TabFact}. The same prompt structure was applied across all datasets, with specific instructions tailored for each.}
    % \caption{\adjustbox{valign=c}{This is a very long caption that might otherwise exceed the page margin, but it has been resized to fit within the page borders.}}

    \label{fig:enter-label}
\end{figure}

\end{document}
