@article{badaro2023transformers,
  title={Transformers for tabular data representation: A survey of models and applications},
  author={Badaro, Gilbert and Saeed, Mohammed and Papotti, Paolo},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={227--249},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{bhandari2024robustness,
  title={On the Robustness of Language Models for Tabular Question Answering},
  author={Bhandari, Kushal Raj and Xing, Sixue and Dan, Soham and Gao, Jianxi},
  journal={arXiv preprint arXiv:2406.12719},
  url={https://arxiv.org/abs/2406.12719},
  year={2024}
}

@article{chen2022large,
  title={Large language models are few (1)-shot table reasoners},
  author={Chen, Wenhu},
  journal={arXiv preprint arXiv:2210.06710},
  url={https://arxiv.org/abs/2210.06710},
  year={2022}
}

@inproceedings{deng2024tables,
  title={Tables as texts or images: Evaluating the table reasoning ability of {LLM}s and {MLLM}s},
  author={Deng, Naihao and Sun, Zhenjie and He, Ruiqi and Sikka, Aman and Chen, Yulong and Ma, Lin and Zhang, Yue and Mihalcea, Rada},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={407--426},
  year={2024}
}

@article{fang2024large,
  title={Large Language Models on Tabular Data--A Survey},
  author={Fang, Xi and Xu, Weijie and Tan, Fiona Anting and Zhang, Jiani and Hu, Ziqing and Qi, Yanjun and Nickleach, Scott and Socolinsky, Diego and Sengamedu, Srinivasan and Faloutsos, Christos},
  journal={arXiv preprint arXiv:2402.17944},
  url={https://arxiv.org/abs/2402.17944},
  year={2024}
}

@inproceedings{grijalba2024question,
  title={Question Answering over Tabular Data with DataBench: A Large-Scale Empirical Evaluation of {LLM}s},
  author={Grijalba, Jorge Os{\'e}s and Lopez, L Alfonso Urena and Mart{\'\i}nez-C{\'a}mara, Eugenio and Camacho-Collados, Jose},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={13471--13488},
  url={https://aclanthology.org/2024.lrec-main.1179/},
  year={2024}
}

@inproceedings{gu2022pasta,
    title = "{PASTA}: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training",
    author = "Gu, Zihui  and
      Fan, Ju  and
      Tang, Nan  and
      Nakov, Preslav  and
      Zhao, Xiaoman  and
      Du, Xiaoyong",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.331/",
    doi = "10.18653/v1/2022.emnlp-main.331",
    pages = "4971--4983",
}

@article{herzig2020tapas,
  title={TaPas: Weakly supervised table parsing via pre-training},
  author={Herzig, Jonathan and Nowak, Pawe{\l} Krzysztof and M{\"u}ller, Thomas and Piccinno, Francesco and Eisenschlos, Julian Martin},
  journal={arXiv preprint arXiv:2004.02349},
  url={https://arxiv.org/abs/2004.02349},
  year={2020}
}

@article{iida2021tabbie,
  title={Tabbie: Pretrained representations of tabular data},
  author={Iida, Hiroshi and Thai, Dung and Manjunatha, Varun and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2105.02584},
  url={https://arxiv.org/abs/2105.02584},
  year={2021}
}

@article{liu2021tapex,
  title={TAPEX: Table pre-training via learning a neural SQL executor},
  author={Liu, Qian and Chen, Bei and Guo, Jiaqi and Ziyadi, Morteza and Lin, Zeqi and Chen, Weizhu and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2107.07653},
  url={https://arxiv.org/abs/2107.07653},
  year={2021}
}

@article{liu2023rethinking,
  title={Rethinking Tabular Data Understanding with Large Language Models},
  author={Liu, Tianyang and Wang, Fei and Chen, Muhao},
  journal={arXiv preprint arXiv:2312.16702},
  url={https://arxiv.org/abs/2312.16702},
  year={2023}
}

@article{lu2024large,
  title={Large language model for table processing: A survey},
  author={Lu, Weizheng and Zhang, Jiaming and Zhang, Jing and Chen, Yueguo},
  journal={arXiv preprint arXiv:2402.05121},
  url={https://arxiv.org/abs/2402.05121},
  year={2024}
}

@article{pang2024uncovering,
  title={Uncovering Limitations of Large Language Models in Information Seeking from Tables},
  author={Pang, Chaoxu and Cao, Yixuan and Yang, Chunhao and Luo, Ping},
  journal={arXiv preprint arXiv:2406.04113},
  url={https://arxiv.org/abs/2406.04113},
  year={2024}
}

@article{qiu2024tqa,
  title={{TQA-B}ench: Evaluating {LLM}s for Multi-Table Question Answering with Scalable Context and Symbolic Extension},
  author={Qiu, Zipeng and Peng, You and He, Guangxin and Yuan, Binhang and Wang, Chen},
  journal={arXiv preprint arXiv:2411.19504},
  url={https://arxiv.org/abs/2411.19504},
  year={2024}
}

@article{ruan2024language,
  title={Language modeling on tabular data: A survey of foundations, techniques and evolution},
  author={Ruan, Yucheng and Lan, Xiang and Ma, Jingying and Dong, Yizhi and He, Kai and Feng, Mengling},
  journal={arXiv preprint arXiv:2408.10548},
  url={https://arxiv.org/abs/2408.10548},
  year={2024}
}

@inproceedings{sui2024table,
  title={Table meets {LLM}: Can large language models understand structured table data? a benchmark and empirical study},
  author={Sui, Yuan and Zhou, Mengyu and Zhou, Mingjie and Han, Shi and Zhang, Dongmei},
  booktitle={Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
  pages={645--654},
  year={2024}
}

@inproceedings{wang2021tuta,
  title={Tuta: Tree-based transformers for generally structured table pre-training},
  author={Wang, Zhiruo and Dong, Haoyu and Jia, Ran and Li, Jia and Fu, Zhiyi and Han, Shi and Zhang, Dongmei},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1780--1790},
  url={https://doi.org/10.1145/3447548.3467434},
  year={2021}
}

@misc{wu2024tablebenchcomprehensivecomplexbenchmark,
      title={TableBench: A Comprehensive and Complex Benchmark for Table Question Answering}, 
      author={Xianjie Wu and Jian Yang and Linzheng Chai and Ge Zhang and Jiaheng Liu and Xinrun Du and Di Liang and Daixin Shu and Xianfu Cheng and Tianzhen Sun and Guanglin Niu and Tongliang Li and Zhoujun Li},
      year={2024},
      eprint={2408.09174},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.09174}, 
}

@article{yin2020tabert,
  title={TaBERT: Pretraining for joint understanding of textual and tabular data},
  author={Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2005.08314},
  url={https://arxiv.org/abs/2005.08314},
  year={2020}
}

@article{zhang2025survey,
  title={A survey of table reasoning with large language models},
  author={Zhang, Xuanliang and Wang, Dingzirui and Dou, Longxu and Zhu, Qingfu and Che, Wanxiang},
  journal={Frontiers of Computer Science},
  volume={19},
  number={9},
  pages={199348},
  year={2025},
  publisher={Springer}
}

@article{zhao2022reastap,
  title={ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples},
  author={Zhao, Yilun and Nan, Linyong and Qi, Zhenting and Zhang, Rui and Radev, Dragomir},
  journal={arXiv preprint arXiv:2210.12374},
  url={https://arxiv.org/abs/2210.12374},
  year={2022}
}

@article{zhao2023investigating,
  title={Investigating Table-to-Text Generation Capabilities of {LLM}s in Real-World Information Seeking Scenarios},
  author={Zhao, Yilun and Zhang, Haowei and Si, Shengyun and Nan, Linyong and Tang, Xiangru and Cohan, Arman},
  journal={arXiv preprint arXiv:2305.14987},
  url={https://arxiv.org/abs/2305.14987},
  year={2023}
}

@article{zhao2023robut,
  title={Robut: A systematic study of table {QA} robustness against human-annotated adversarial perturbations},
  author={Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2306.14321},
  url={https://arxiv.org/abs/2306.14321},
  year={2023}
}

