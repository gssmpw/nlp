\section{Evaluation}
% TODO: fraction of physical nodes - physical node utilization - PDA?




We carry out simulations to showcase the efficacy of our suggested method in relation to VN acceptance ratio, revenue, cost, and utilization compared with other centralized algorithms. We employ the random graph model~\cite{randomGraph} for the formation of physical and virtual networks. 
% All the algorithms are executed in Python $3.6.9$. The computations are performed on a computer furnished with an Intel$^{\tiny{\textregistered}}$ Core\texttrademark,$i5-9500T$ processor operating at $2.2 - 3.7$~GHz and equipped with $8$~GB of RAM.




\subsection{Parameters for Simulation}
In this subsection, we present and clarify the parameters employed during the evaluation.

\cat{Physical Network} For the physical network modeling, we opt for a typical real-world setup featuring $1.2$ terabytes of RAM and a hundred processing and graphics cores similar to those of, respectively, Dell\texttrademark\ PowerEdge\texttrademark\ R$910$, two Intel$^{\tiny{\textregistered}}$ Xeon$^{\tiny{\textregistered}}$ Scalable processors post Hyperthreading, and a single Nvidia$^{\tiny{\textregistered}}$ GeForce$^{\tiny{\textregistered}}$ GT $330$. We then apply a normal distribution to instill diversity and heterogeneity. As a result, we consider a network of $100$ servers, where there is a $40\%$ likelihood of a direct physical link between each pair of servers. Each physical link is distinguished by its bandwidth (in Mbps), which is randomly selected from the normal distributions $\mc{N}(100, 400)$. Every physical node is defined by its CPU power (number of cores), memory capacity (in GBytes), and GPU power (number of cores), with these values chosen from the normal distributions $\mc{N}(100, 400)$, $\mc{N}(1200, 300)$, and $\mc{N}(100, 400)$, respectively.

\cat{Virtual Networks} The count of virtual nodes in each VNR is randomly picked from the interval $[4, 10]$. The probability of a virtual link existing between two virtual nodes stands at $0.7$. The bandwidth demand of virtual links adheres to the normal distributions $\mc{N}(10, 4)$. The CPU, memory, and GPU demand of each virtual node are derived from the normal distributions $\mc{N}(10, 4)$, $\mc{N}(30, 9)$, and $\mc{N}(10, 4)$, respectively. Every VN possesses a lifetime that is also randomly selected from a normal distribution of $\mc{N}(100, 900)$. VNs arrive according to the arrival rate and persist in the physical network for the span of their lifetime. The VN arrival rate is set at $2$ per unit of time and the simulation runs for $2000$ time units.

\cat{Algorithms for Comparison}
In conjunction with \ourAlg, we have executed the following algorithms to make a comparative study.
\begin{itemize}[leftmargin=*]
    \item \textbf{FirstFit}: This is an algorithm that embeds virtual nodes into the first available physical node with adequate capacity.
    \item \textbf{BestFit}: This algorithm opts for the physical node boasting maximum CPU capacity and fills it with the demands of the virtual node.
    \item \textbf{GRC}~\cite{grc}: An algorithm based on node-ranking.
    \item \textbf{NeuroViNE}~\cite{neurovine}: This algorithm uses a search space reduction mechanism. It extracts pertinent subgraphs by means of a Hopefield network, and then employs GRC to embed VN into candidate subgraphs.
\end{itemize}
The parameters $\alpha$, $\beta$, and $L$ are set to $30$, $3$, and $5$ respectively. The optimization parameters $X$ and $Y$ are both set to $1$.


\subsection{Benchmarks}
In this subsection, we benchmark our algorithm against centralized algorithms in terms of performance to demonstrate that {\ourAlg} can be utilized in real-world applications with acceptable results. It's important to note that this algorithm is decentralized, thus it benefits from all the advantages detailed in section~\ref{sec:motivation}.


\begin{figure}[t]
	\centering
	\includegraphics[width=0.97\linewidth]{figures/ar.pdf}
	\caption{Acceptance Ratio over Time}
	\label{fig:ar}
\end{figure}
\cat{Acceptance Ratio}
The long-term acceptance ratio stands as a significant metric influencing the system's profitability. As depicted in Figure~\ref{fig:ar}, the acceptance ratio of varying algorithms over a lengthy simulation of approximately $2000$ episodes is displayed. It should be noted that during this period, the acceptance ratios attain a state of equilibrium and maintain consistency due to a stationary virtual network arrival process. \ourAlg\ enhances the acceptance ratio by roughly $12\%$ in comparison to NeuroViNE and by $17\%$ in relation to other algorithms.


\begin{figure}[t]
    \begin{subfigure}{0.49\linewidth}
	\centering
	\includegraphics[width=\linewidth]{figures/cost.pdf}
	\caption{Cost of Embedding}
	\label{fig:cost}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
	\centering
	\includegraphics[width=\linewidth]{figures/revenue.pdf}
	\caption{Revenue of Embedding}
	\label{fig:revenue}
    \end{subfigure}
    \caption{Financial Performance of \ourAlg}
    \label{fig:fperformance}
\end{figure}
\cat{Revenue and Cost}
Figures~\ref{fig:cost} and \ref{fig:revenue}, respectively, compare the cost and revenue of different algorithms. Computations are based on the equations~\eqref{eq_rev} and \eqref{eq_cost}.
Given that \ourAlg\ accommodates more VNs over the same duration, it generates a higher revenue. Even with a higher revenue, our proposed algorithm results in a lower cost compared to the First Fit, Best Fit, and GRC methods. NeuroViNE manages a low cost because it typically embeds smaller virtual networks, and as a result, it cannot generate high revenue. Furthermore, \ourAlg\ achieves the maximum revenue-to-cost ratio (\ie\ $1.94$), while NeuroViNE, GRC, Best Fit, and First Fit achieve ratios of $1.6$, $1.37$, $1.34$, and $1.08$, respectively.


\begin{figure}[t]
    \begin{subfigure}{0.49\linewidth}
	\centering
	\includegraphics[width=\linewidth]{figures/cpu_util.pdf}
	\caption{Average CPU Utilization}
	\label{fig:cpu-util}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\linewidth}
        \centering
	\includegraphics[width=\linewidth]{figures/link_util.pdf}
	\caption{Average Link Utilization}
	\label{fig:link-util}
    \end{subfigure}
    \caption{Utilization of \ourAlg}
    \label{fig:utilization}
\end{figure}
\cat{Utilization}
Figure~\ref{fig:cpu-util} and \ref{fig:link-util} represent CPU and link utilization in comparison to other methodologies. \ourAlg~ outperforms others in terms of link utilization. This is due to \ourAlg~ employing a BFS-like algorithm for embedding and restricting the depth of BFS, which results in the usage of fewer links. \ourAlg~ also shows slightly lower CPU utilization than other methods as the leaders are chosen randomly. However, CPU utilization can be enhanced by augmenting the number of leaders in each embedding round.


\subsection{Complexity}
% In this subsection, we discuss the complexity of our algorithm in terms of communication and computation. 

\cat{Computation Complexity}
The computational complexity of {\ourAlg} is determined by the local embedding algorithm, which is executed for each leader in the embedding process. The local embedding algorithm has a time complexity equivalent to the BFS algorithm, with a maximum number of inspected nodes denoted as $\alpha\times|V_v|$. Consequently, the overall computational complexity of {\ourAlg} can be expressed as $O(L\times\alpha\times|V_v|)$, which demonstrates its efficiency compared to many centralized methods.

\cat{Communication Complexity}
The decentralized nature of {\ourAlg} introduces a communication complexity to the algorithm. In the worst-case scenario, the communication complexity is determined by the number of messages exchanged between the servers. Specifically, it involves sending $2L - 1$ \textit{EMBEDDING} messages when the second-to-last server possesses the best local embedding and $L$ \textit{EMBEDDED} messages for notifying all the servers. Thus, the communication complexity of {\ourAlg} can be shown as $O(3L - 1)$.