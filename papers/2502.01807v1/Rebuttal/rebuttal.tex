R1
--
* To achieve measurable comparisons with related works, we can enhance the arguments by highlighting two primary properties of our method, i.e., parallelism and flexible option elimination. Then, we will increase the cohesion of the introduction by moving the related works to a separate section. 
* Regarding the evaluations, we used two important and recent **pre-processing algorithms** (`GRC` and `DeepViNE`) to achieve a full-view comparison, where the chosen simulation parameters are extracted from real-world systems: E.g., a server with two Intel Xeon processors provides close to 100 cores. Dell PowerEdge R910 Server supports up to 2 Terabytes of RAM. Low-end Nvidia GeForce GPUs have about 700 cores. Suitable references for the parameters we used in the paper are added.
* Regarding the conclusion, we can amend it to capture the drawbacks and advantages such as the speedup and the re-training overhead.

R2
--
* We will add more background and citations to make the paper even more self-contained. We will re-write the description of Fig. 6. 

R3
--
* The number of layers in the encoder **does not limit** the diameter of VNs. Only, we believe that the results are better when they match. Thus,  our method applies to VNs with arbitrary diameter. We will add sentences to Sect. IV-B to clarify this ambiguity.
* Regarding ambiguities in the problem definition, we will rewrite the section to increase clarity. However, we briefly explain that Constraints (1) and (2) are correct:
  * **C1:** Subscript `r` stands for different types of resources. Also, the capacity of each resource and the demand for each resource, respectively, are considered with functions `f_r(m)` and `g_r(n)`. 
  * **C2:** That inequality ensures that the left-hand side variable can be `1` when at least one variable in the summation is `1`. Also, time is implicitly considered with the superscript `t` in `L^t`. We will change it to include time explicitly.
* The freshness mechanism is implemented in the `re_cluster` subroutine, while its description was omitted to save space. We will add more detail.
* Regarding the concerns about the scalability, note that the bottleneck typically is the size of the physical network with thousands of servers. Fig. 3 shows the scalability analysis in this regard. We will add numbers to show that the size of VNs has a smaller effect.
* The tradeoff of scalability and efficiency is seemingly missing because we computed the number of clusters automatically with the `elbow method` and reported the final result. We will include one of the **intermediate results** to show this tradeoff. 

R4
--
* Regarding the contribution, we emphasize that GNN is a promising and viable tool to model the problems in the communication field of research. To our knowledge, no previous study has used this tool that creates a better solution for future self-driving and autonomous networks.
* Regarding the testbed, our method, similar to previous studies (`GRC`, `NeuroViNE`, and 2 suggested papers), focuses on the algorithmic aspects of the problem. Our evaluation `methodology`, also, resembles those works. With our simulation parameters extracted from real systems (see our response to R1), we expect the testbed and simulation results to be close. 
* Regarding the missing references, to focus on network virtualization papers, we opted to not include papers from NFV literature (e.g., 2 suggested papers). However, to highlight the differences, we will refer to them and present a comparison.