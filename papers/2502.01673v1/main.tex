%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{float}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage[xcdraw, table, dvipsnames]{xcolor} \usepackage[xcdraw, table, dvipsnames]{xcolor} 
\usepackage{colortbl} % Add this package in the preamble for colors
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}



% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{adjustbox}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multilingual State Space Models for Structured Question Answering in Indic Languages}

\begin{document}

\twocolumn[
\icmltitle{Multilingual State Space Models for Structured Question Answering in Indic Languages}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Arpita Vats}{equal,Boston University}
\icmlauthor{Rahul Raja}{equal,Carnegie Mellon University}
\icmlauthor{Mrinal Mathur}{equal,Georgia Tech University}
\icmlauthor{Vinija Jain}{Stanford University}
\icmlauthor{Aman Chadha}{Stanford University}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{Boston University}{Boston University, Boston, USA}
\icmlaffiliation{Carnegie Mellon University}{Carnegie Mellon University Pennsylvania, USA}
\icmlaffiliation{Georgia Tech University}{Georgia Tech University Atlanta, USA}
\icmlaffiliation{Stanford University}{Stanford University, Palo Alto, USA}

\icmlcorrespondingauthor{Arpita Vats}{arpita.vats09@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs) to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. Furthermore, we propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
In recent years, the field of Natural Language Processing (NLP) has witnessed significant advancements with the development of models capable of handling complex linguistic structures and long-range dependencies. State Space Models (SSMs)~\cite{goel2022s4} have emerged as a promising alternative to traditional architectures like Transformers ~\cite{vaswani2017attention}, particularly in tasks requiring efficient long-sequence modeling. Models such as Mamba ~\cite{gu2024mambalineartimesequencemodeling} and its variants have demonstrated the ability to manage long-context language modeling with constant memory usage, addressing some limitations inherent in Transformer-based models ~\cite{vannguyen2024} ~\cite{bourdois2022ssm}.
Despite these advancements, the development of question-answering (QA) systems for low-resource languages, particularly Indic languages, remains a challenging endeavor. India has a population of 1.4 billion and is home to 122 languages and 270 mother tongues. Indian languages fall in the minority as far as NLP models are concerned. One of the Indian languages—Hindi—is spoken by 577.8 million people worldwide ~\cite{sabane2024}. Likewise, Marathi, another Indian language, is ranked 11th on the list of most spoken languages. However, the scarcity of annotated datasets and neural models has hindered progress in building effective QA systems for these languages. Recent efforts have been made to address this gap by introducing large-scale QA datasets for Indic languages, such as the Indic QA dataset ~\cite{singh2024indicqabenchmarkmultilingual}, which provides a substantial resource for developing systems tailored to the linguistic nuances of Indic languages.
This paper explores the various SSM models, to build efficient and contextually aware QA systems for Indic languages. SSMs are particularly suited for this task due to their ability to model both long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages.
The main contributions of this paper are:
\begin{itemize}[noitemsep, topsep=2pt]
    \item This work represents the first application of SSMs to question-answering tasks in Indic languages, setting a foundational benchmark for future research in this domain.
    \item We evaluate multiple SSM architectures on diverse datasets representing various Indic languages, conducting a detailed comparative analysis to highlight their strengths and limitations.
    \item Through extensive experiments, we demonstrate how SSMs effectively capture the linguistic subtleties of Indic languages, leading to improved question interpretation, context alignment, and answer generation.
\end{itemize}

\section{Related Works}
\label{sec:related}

The field of QA and NLP has seen rapid advancements, particularly with the introduction of models leveraging state-space representations and multi-modal learning. This section reviews key advancements related to SSMs, Indic language processing, and multi-modal approaches to contextual understanding.

\subsection{Advances in State Space Models}
Traditional SSMs, such as the Kalman Filter ~\cite{kalman1960new} and Hidden Markov Models (HMMs) ~\cite{rabiner1986hmm}, have been widely used for decades. While effective for many tasks, these models struggle with long-range dependencies and high-dimensional data ~\cite{bourdois2022ssm}. To overcome these limitations, SSMs have emerged, combining the foundational principles of traditional SSMs with the expressive power of deep learning ~\cite{sarrof2024expressivecapacitystatespace}. These approaches allow for scalability and adaptability in modern machine learning applications ~\cite{goel2022s4}.SSMs have emerged as a robust alternative to transformers in sequence modeling, excelling particularly in areas where transformers face inherent limitations ~\cite{patro2024heracleshybridssmtransformermodel}. Mathematically, an SSM can be represented as follows:
\begin{equation}
    {
        \mathbf{x}_{t+1} = \mathbf{A} \mathbf{x}_t + \mathbf{B} \mathbf{u}_t + \mathbf{w}_t, \quad \mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q})
    } 
    \label{eq:ssm_equations}
\end{equation}

The ~\cref{eq:ssm_equations} ~\cite{gu2022structured} describes a framework where the state vector \( \mathbf{x}_t \) at time \( t \) represents latent variables encapsulating the system's underlying dynamics. The state evolution from \( t \) to \( t+1 \) is governed by the state transition matrix \( \mathbf{A} \), while the input matrix \( \mathbf{B} \) determines the effect of the control input \( \mathbf{u}_t \) on the state transitions. The control input \( \mathbf{u}_t \) serves as an external influence, allowing modifications to the state dynamics at time \( t \). Additionally, process noise \( \mathbf{w}_t \) is modeled as a Gaussian random variable with zero mean (\( \mathcal{N}(0, \mathbf{Q}) \)) and covariance matrix \( \mathbf{Q} \), capturing uncertainties and randomness in the state transitions. The covariance matrix \( \mathbf{Q} \) quantifies the variability of the process noise \( \mathbf{w}_t \), ensuring the model accounts for inherent stochasticity.\\
Transformers rely on self-attention mechanisms to capture relationships across sequences ~\cite{vaswani2017attention}. While effective, their quadratic complexity with respect to sequence length leads to significant computational overhead and memory usage for long contexts ~\cite{taha2025logarithmicmemorynetworkslmns}. In contrast, SSMs are designed with a focus on efficiency and scalability, leveraging state transitions and compact latent representations to model temporal dependencies ~\cite{shakhadri2025sambaasrstateoftheartspeechrecognition}. This design enables SSMs to process sequences with linear computational complexity, making them more efficient for handling long-range dependencies in applications such as natural language processing, time-series forecasting, and audio processing ~\cite{liu2024masvspeakerverificationglobal}. SSMs have emerged as a significant development in sequence modeling, providing an effective approach to capture temporal dynamics in various tasks ~\cite{abreu2024qs5quantizedstatespace}. SSMs are grounded in their ability to model dynamic systems through latent state representations, which evolve over time according to predefined transition dynamics and are observed through noisy measurements ~\cite{wang2024statespacemodelnewgeneration}.Recent advancements in SSMs have led to the creation of hybrid architectures and enhanced methodologies that address key limitations in sequence modeling. For instance,Mamba ~\cite{gu2023mamba} (Linear-Time Sequence Modeling with Selective State Spaces)~\cite{gu2024mambalineartimesequencemodeling} introduces a mechanism where the parameters of the SSM are functions of the input, allowing selective propagation or forgetting of information. This selective capability improves the model's handling of discrete data and enables linear scaling with sequence length, outperforming traditional transformer models of similar size across diverse modalities such as language and audio. Another contribution is the Samba ~\cite{ren2024sambasimplehybridstate},  a hybrid architecture that fuses selective SSMs with sliding window attention, enabling efficient handling of unlimited context. By compressing sequences into recurrent hidden states while leveraging attention for precise recall, Samba demonstrates a remarkable ability to manage long sequences with minimal computational overhead. Similarly, models like Hymba ~\cite{dong2024hymbahybridheadarchitecturesmall} and Jamba ~\cite{lieber2024jamba} integrate SSMs with attention mechanisms within hybrid architectures to achieve a balance between efficient summarization of long-range contexts and high-resolution recall. These models exemplify how combining the strengths of SSMs and transformers ~\cite{vaswani2017attention} can yield state-of-the-art results, especially in tasks requiring both scalability and accuracy.
Another critical area where SSMs surpass transformers is in inference speed and memory efficiency. Attention-free SSM models like Falcon Mamba ~\cite{zuo2024} have proven to be significantly faster during inference while being lighter in memory usage for processing long sequences. For example, Falcon Mamba can match or exceed the performance of leading transformer-based models, such as Mistral 7B ~\cite{jiang2023mistral7b} and Llama ~\cite{touvron2023llamaopenefficientfoundation}, while requiring far fewer computational resources. This makes SSMs particularly suitable for deployment in resource-constrained environments or applications where real-time processing is critical. Additionally, transformers often struggle with handling highly structured or continuous data such as audio and sensor streams. In these domains, the state-transition mechanisms of SSMs enable them to natively and effectively capture temporal dynamics, offering a clear advantage over transformers, which often require additional architectural modifications to achieve comparable results.

\subsection{Indic Language Question Answering}
Indic languages, with their diverse scripts, complex grammar, and linguistic variations, present unique challenges for QA systems ~\cite{dani2024reviewmarathinaturallanguage}. Features such as free word order, inflectional patterns, and compound words complicate NLP tasks. While initiatives like the IndicQA dataset ~\cite{sabane2024} have advanced question answering in these languages, the scarcity of annotated datasets and neural models remains a significant challenge. Techniques such as transfer learning and domain adaptation have shown promise, with multilingual models fine-tuned on task-specific data achieving notable improvements in Hindi and Marathi QA tasks ~\cite{jin2022low}.

Adopting the SQuAD format ~\cite{rajpurkar2016squad}, a widely used benchmark in QA research, facilitates the creation of structured datasets for Indic languages. This format includes context passages, questions, and corresponding answers with exact character positions, ensuring consistency and compatibility across QA frameworks. Translating and adapting the SQuAD format for Indic languages has proven effective, as demonstrated by the performance improvements of the IndicQA dataset when aligned with this structure ~\cite{singh2024indicqabenchmarkmultilingual}.

To address data scarcity, data augmentation techniques such as back-translation have been used to generate synthetic SQuAD-style datasets ~\cite{Khan_2024}. Fine-tuning multilingual pre-trained models like XLM-R ~\cite{conneau2020unsupervised} and mT5 ~\cite{xue2021mt5} on such datasets has further improved performance in Indic QA tasks . The structured nature of the SQuAD format allows for robust evaluation of fact-based, reasoning-based, and opinion-based questions, making it particularly suitable for Indic languages ~\cite{UPADHYAY2024100088}. Aligning Indic-language datasets with this format enhances model performance and facilitates better comparisons across multilingual QA research.

\section{Methodology}
\label{sec:methods}
Our methodology focuses on developing a question-answering framework tailored for Indic languages by leveraging the capabilities of SSMs. Specifically, we experimented with various models, including Mamba, Mamba-2,Falcon Mamba, Jamba, Zamba, Samba, and Hymba, to address the linguistic diversity and complexity of Indic languages. Each model was evaluated for its ability to process questions, answers, and context efficiently while preserving the grammatical and semantic nuances of the respective languages. The overall framework for this system is illustrated in ~\cref{fig_framework}. 
\begin{figure*}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=11.5cm]{img/QA-Page.drawio.pdf}}
  % \vspace{2.0cm}
  \caption{\label{fig_framework} Workflow of the fine-tuned SSM model for multilingual question-answering tasks, illustrating the complete framework.}
\end{minipage}
\end{figure*}
\subsection{Dataset Preprocessing}
Dataset preprocessing is a critical step in preparing the data for the SSM-based QA model. The preprocessing pipeline involves tokenization, vocabulary construction, encoding as mentioned in ~\cref{sec:tokenization}, and data transformation as mentioned in ~\cref{sec:transformation}. 
% The complete pipeline is illustrated in ~\cref{fig_dataset}.
\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{img/dataset.drawio.png}}
  % \vspace{2.0cm}
  \caption{\label{fig_dataset}Illustration of the data preprocessing pipeline. }
\end{minipage}
\end{figure}
\subsubsection{Tokenization}
\label{sec:tokenization}
For our data preprocessing pipeline, we employed IndicNLPtokenizer ~\cite{kunchukuttan2020indicnlp}, which is specifically designed to address the unique grammatical structures and linguistic characteristics of Indic languages. This tokenizer is particularly effective for handling the complex and diverse scripts of languages such as Hindi and Marathi. By maintaining consistent tokenization across the context (context about the question), question, and answer in our dataset, it enabled uniformity and alignment throughout the preprocessing pipeline. The tokenizer was trained on our corpus to capture language-specific patterns and generate a custom vocabulary tailored to Indic languages. This vocabulary included frequently occurring tokens and special tokens such as padding (\texttt{<pad>}), unknown tokens (\texttt{<unk>}), start-of-sentence (\texttt{<sos>}), and end-of-sentence (\texttt{eos}) markers, which are essential for enabling sequence alignment, handling missing or unknown words, and defining sentence boundaries during both training and inference. These functionalities ensure that the model can process variable-length inputs, manage incomplete data, and correctly interpret input sequences. After constructing the vocabulary, we performed encoding, where the starting position (\texttt{answer\_start}) of the answer, representing the character index of the answer within the context, was mapped to its corresponding tokenized representation. This process generated the start (\texttt{token\_start}) and end (\texttt{token\_end}) token positions, which define the answer span within the tokenized sequence. These fields are essential for enabling the model to accurately learn and predict answer spans, ensuring precise alignment between the original text and its tokenized format for effective training and evaluation. The overall pipeline is illustrated in ~\cref{fig_dataset}. 

\subsubsection{Dataset Transformation}
\label{sec:transformation}
After tokenization, the dataset was transformed into a structured format to maintain consistency across all samples. This structured representation preserved essential components such as the processed context, question, and answer, along with metadata necessary for accurate span prediction. The metadata included the starting position of the answer in the original text and its corresponding positions in the tokenized sequence, ensuring precise alignment between the raw text and its tokenized version. To handle variable-length sequences, padding was applied using a designated padding token from the vocabulary, ensuring uniform sequence lengths throughout the dataset. Additionally, attention masks were generated to differentiate actual tokens from padded positions, facilitating efficient batch processing during training and inference. These transformations were applied consistently across the training, validation, and test datasets, ensuring coherence throughout the model's learning process. By structuring the data in this way, the model could effectively process question-answer pairs while preserving the necessary information for accurate span-based predictions.

\subsection{Training}
\label{sec:training}
After data preprocessing, the next step was training. Initially, we evaluated the model’s performance without any fine-tuning, using only tokenization mentioned in ~\cref{sec:tokenization}. These baseline inferences provided insights into the model’s capabilities for handling Hindi and Marathi text.

Subsequently, we fine-tuned the SSM model for question answering in Indic languages, we designed a  training pipeline to enhance the performance for question answering in Indic languages. The fine-tuning process employed a resource-efficient strategy using low-rank adaptation (LoRA)~\cite{hu2021loralowrankadaptationlarge}. LoRA employs a strategy of updating only the parameters in the projection layers of attention modules and the embeddings layer, leaving the rest of the model unchanged. By focusing on these key components, the fine-tuning process effectively adapts the model to new tasks. This method uses low-rank matrices to efficiently represent the updates, significantly reducing the number of trainable parameters. Consequently, computational overhead is minimized, enabling fine-tuning on resource-constrained hardware while achieving notable performance improvements for the target Indic languages. The training dataset consisted of conversational question-answer pairs, which was augmented with conversational question-answer pairs structured to include system, user, and assistant messages. The system message set the context for the model's role as an AI assistant answering questions based on a provided reference. The user message consisted of the reference text (context) and the specific question, while the assistant message contained the corresponding answer. This format ensured the model was trained to generate contextually appropriate and linguistically accurate responses in Hindi and Marathi, aligning with the conversational structure required for question-answering tasks.
The implementation details of the fine-tuning process, including hyperparameters and training configurations, are discussed later in ~\cref{sec:implementation_details}

\subsection{Models}
\textbf{Mamba}: Mamba is a linear-time sequence model designed to efficiently handle long sequences by incorporating input-dependent parameterization through a selective propagation mechanism. This unique approach allows Mamba to outperform Transformers of similar size across various modalities, including language, audio and genomics. In language modeling tasks, Mamba demonstrates a performance improvement of 5-10\% in accuracy compared to Transformers, depending on the specific dataset and task requirements. Furthermore, Mamba is computationally efficient, processing long sequences 2-4 times faster than equivalent Transformer-based models. These advantages make Mamba particularly effective for tasks involving extensive text data, such as question answering, and well-suited for low-resource languages like Hindi and Marathi.

\textbf{Mamba\-2}: Mamba-2, with 2.7B parameters trained on 300B tokens, introduces significant advancements in handling very long sequences.  In benchmarks where Mamba struggles, Mamba-2 demonstrates robust performance across all settings. Notably, even when state sizes are controlled (e.g., \(N = 16\)), Mamba-2 significantly outperforms Mamba, highlighting the impact of its architectural enhancements. For example, Mamba achieves an error rate of 11.76 in a sequential setup, while Mamba-2 reduces this to 11.49 using parallelism. Furthermore, increasing the state size (\(N = 16 \to 64 \to 256\)) consistently improves performance, as larger states allow the model to better memorize key-value pairs. \\
\textbf{Falcon Mamba}: Falcon Mamba 7B, is a pure Mamba-based large language model trained on 5.8 trillion tokens.Falcon Mamba 7B surpasses leading open-weight Transformer-based models, including Mistral 7B ~\cite{jiang2023mistral}, Llama ~\cite{touvron2023llama}.Additionally, the model’s architecture enables significantly faster inference and requires less memory for generating long sequences, making it both high-performing and efficient in handling large-scale language modeling tasks.\\
\textbf{Jamba}: Jamba is a hybrid language model that interleaves Transformer and Mamba layers, combining the strengths of both architectures. Incorporating a mixture-of-experts (MoE) ~\cite{lo2024closerlookmixtureofexpertslarge} approach, Jamba increases model capacity while maintaining manageable active parameter usage. It achieves state-of-the-art performance on standard language model benchmarks and exhibits strong results with context lengths up to 256K tokens.\\
\textbf{Zamba}: Zamba is a compact 7B parameter model that combines a Mamba backbone with a single shared attention module. This architecture leverages the benefits of attention at minimal parameter cost, resulting in faster inference and reduced memory requirements for long-sequence generation. Zamba is trained on 1 trillion tokens from openly available datasets and stands as the best non-Transformer model at this scale.\\
\textbf{Hymba}: Hymba  is a family of small language models featuring a hybrid-head parallel architecture that integrates Transformer attention mechanisms with SSMs. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Hymba achieves state-of-the-art results for sub-2B parameter models, surpassing models like Llama-3.2-3B in performance, cache size reduction, and throughput.\\
\textbf{Samba}: A simple hybrid architecture that layer-wise combines Mamba with Sliding Window Attention (SWA) ~\cite{beltagy2020longformerlongdocumenttransformer}. SAMBA ~\cite{ren2024sambasimplehybridstate} selectively compresses sequences into recurrent hidden states while maintaining the ability to recall recent memories precisely. It significantly outperforms state-of-the-art models across various benchmarks and demonstrates efficient extrapolation to context lengths of up to 1 million tokens.
\begin{table*}[ht!]
\caption{Comprehensive Comparison of Model Architectures}
\label{tbl:model_comparison}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c|c}
\toprule
\textbf{Model} & \textbf{Parameters (B)} & \textbf{Architecture Type} & \textbf{Context Window (Tokens)} & \textbf{Training Data (T Tokens)} & \textbf{Inference Complexity} & \textbf{Inference Speed (Tokens/s)} \\
\midrule
\texttt{Mamba} & 3 & SSM-based & 1M & Not specified & $O(n)$ & 50K \\
\texttt{Mamba-2} & 3 & Enhanced SSM & 1M & Not specified & $O(n)$ & 75K \\
\texttt{Falcon} & 2.7 & Lightweight Transformer & 512K & 1.5 & $O(n \log n)$ & 100K \\
\texttt{Jamba} & 52 (12 active) & Hybrid Transformer-SSM with MoE & 256K & Not specified & $O(n)$ & 45K \\
\texttt{Samba} & 3.8 & Hybrid SSM-Attention & 256K & 3.2 & $O(n)$  & 48K \\
\texttt{Hymba} & 1.5 & Hybrid-head Transformer-SSM & Not specified & Not specified & $O(n)$ & 40K \\
\texttt{Zamba} & 7 & SSM-Transformer Hybrid & Not specified & 1 & $O(n)$ & 55K \\
\bottomrule
\end{tabular}
}
\end{table*}
\subsection{Prompting Strategies for Indic Language based SSMs}
SSMs like Mamba and Mamba-2  require careful prompt engineering to maximize the question-answering capabilities in low-resource languages such as Hindi and Marathi. Also because of their lexicals , the tokens are not of constant length. Our approach combines the architectural strengths of SSMs with linguistic consideration for Devanagari script based languages. 

The time-invariant nature of SSMs enables effective zero-shot prompting through structured template design. 
This structure leverages Mamba-2's selective compression of contextual states while preserving script-specific grapheme cluster. 

For Few-shot prompting \citet{ye2022unreliability}, The model maintains separate state trajectories for example patterns through its recurrent SSM block. 

\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=5.0cm]{img/test.pdf}}
  % \vspace{2.0cm}
  \caption{\label{fig_hindi_example}This example shows the one shot prompt in Hindi where we ask Question and give a scenario and then ask the question based on it.}
\end{minipage}
\end{figure}
We provide examples of prompts with both positive and negative samples in Appendix ~\cref{fig_sampleResults}, showcasing their use in few-shot prompting, while examples for one-shot prompting are illustrated in ~\cref{fig_hindi_example}. Our Question-Answering pipeline is based on the few-shot prompting framework ~\cite{reynolds2021prompt}. This approach demonstrates how leveraging paradigms from different prompting techniques leads to improved results during fine-tuning, as evidenced in the subsequent ~\cref{sec:experiments}.

We tend to select the best answer in P where $P \in K; K\ge0$, are the number of samples generated for that query. These queries are converted into positive and negative sample. These selection of positive and negative samples are done using \texttt{infomax} algorithm \citet{velivckovic2018deep} to retrieve the best answer out of $P$ samples, which makes the system stochastic and more deterministic. 

\subsection{Evaluation Metrics}
To comprehensively evaluate model performance in question-answering tasks, we employed multiple metrics capturing different aspects of accuracy and relevance. EM provides a strict precision measure by evaluating the percentage of predictions that exactly match the ground truth, making it ideal for extractive QA tasks~\cite{maalouly2022exactmatchingalgorithmsrelated}. F1 Score balances precision and recall, rewarding partial matches to better reflect token-level similarity, which is especially useful for paraphrased responses~\cite{powers2020evaluationprecisionrecallfmeasure}. To assess deeper semantic relationships, BERTScore~\cite{zhang2020bertscoreevaluatingtextgeneration} leverages contextual embeddings from a pre-trained model, capturing meaning beyond exact word matches. ROUGE evaluates n-gram recall and overlap, ensuring that key phrases from the reference answer are retained even if the phrasing differs~\cite{lin2004rouge}. Lastly, BLEU quantifies n-gram precision while incorporating a brevity penalty to prevent overly short predictions, making it particularly effective for assessing fluency and word-level accuracy in extractive QA settings~\cite{papineni2002bleu}. These metrics collectively provide a robust assessment of model effectiveness in handling both exact and approximate answer matching in Hindi and Marathi QA tasks.


\section{Experiments}
\label{sec:experiments}
In our experimentation, we fine-tuned several models—Falcon Mamba, Mamba, Mamba-2, Jamba, Zamba, Samba, and Hymba—on a Hindi and Marathi SQuAD-style question-answering dataset specifically designed for these languages. This section provides a detailed account of the fine-tuning process, including the experimental setup, hyperparameter configurations, and training strategies employed. We also discuss the dataset details, model architecture adaptations, and evaluation metrics used to assess performance.
\subsection{Dataset}
We utilized the IndicQA dataset, which was constructed by aggregating multiple open-source datasets to develop a SQuAD-style resource for Hindi and Marathi. This dataset includes a significant component of approximately 28,000 samples in these languages, making it one of the largest publicly available datasets in Hindi and Marathi ~\cite{sabane2024}.The datasets are summarized in ~\cref{tbl:dataset}.
\begin{table}[ht!]
  \caption{Data information.}
  \label{tbl:dataset}
  \centering
  \resizebox{0.9\columnwidth}{!}{%
  \begin{threeparttable}
  \begin{tabular}{l|r|r}
    \toprule
    & \emph{Train Set (\#sent)} & \emph{Test Set (\#sent)} \\
    \midrule
    Hindi Dataset & 21,000  & 7,000\\
    Marathi Dataset & 18,500 & 7,000 \\
    \bottomrule
  \end{tabular}
  \end{threeparttable}
  }
\end{table}

\subsection{Implementation Details}
\label{sec:implementation_details}
The fine-tuning process for the models used task-specific configurations to optimize performance and efficiency. LoRA was the primary method for fine-tuning across all models, with a rank of 8, a scaling factor (\(\alpha\)) of 32 to control the impact of low-rank updates on the model’s original weights, and a dropout rate of 0.1. This enabled efficient parameter updates by modifying key layers like projection layers and embeddings while keeping most parameters frozen. The models were trained on Hindi and Marathi question-answer pairs, formatted in a conversational template with system, user, and assistant messages, and tokenized to align inputs and outputs with a maximum sequence length of 2048 tokens.

For Mamba and Mamba-2, the fine-tuning configuration included a learning rate of \(2 \times 10^{-4}\), a batch size of 4 (with gradient accumulation for an effective batch size of 32), 3 epochs, 100 warmup steps, and mixed precision (FP16). The Falcon Mamba model was fine-tuned with start and end predictors, cross-entropy loss, and the Adam optimizer, using a learning rate of \(1 \times 10^{-4}\), a batch size of 4, and 10 epochs. The Jamba model employed a LoRA configuration with an increased rank of 16 to enhance capacity and processed sequences with a context window of up to 256K tokens. The Zamba model was optimized for long sequences, using a maximum sequence length of 4096 tokens. The Samba model focused on computational efficiency, handling sequences up to 256K tokens with a batch size of 8 (effective batch size of 64 with gradient accumulation), excelling in modeling complex conversational patterns. Finally, the Hymba model incorporated learnable meta tokens and partial sliding window attention, trained with a batch size of 4 and a learning rate of \(3 \times 10^{-4}\).
Evaluations for all models were conducted at regular intervals, with checkpoints saved every 500 steps. At the end of training, the best-performing model was selected. The validation graphs for all models are provided in ~\cref{fig_evaluation}.

\begin{figure}[ht]
% \begin{minipage}[b]
  \centering
  % \centerline{\includegraphics[width=9.0cm]{img/Training loss.png}}
  \centerline{\includegraphics[width=0.95\linewidth]{img/Validation_loss.png}}
  % \vspace{2.0cm}
  \caption{\label{fig_evaluation} Validation plots for SSM Models}
% \end{minipage}
\end{figure}


\subsection{Results}
This section evaluates model performance on Hindi and Marathi datasets, comparing results before and after fine-tuning. Additionally, ~\cref{fig_sampleResults} includes sample model responses to highlight qualitative differences in predictions.
\subsubsection{Results without fine-tuning}
~\cref{tbl:combined_table_finetuning} presents model performance before fine-tuning, showing variations across different architectures. Zamba and Samba performed relatively well in Hindi and Marathi, respectively, while Falcon Mamba struggled the most with exact span localization. Mamba-2 demonstrated better alignment with reference answers, due to its superior ability to model long-range dependencies and capture contextual nuances. In contrast, Hymba struggled significantly, due to limitations in handling the structural complexity of Indic languages. Models that performed better exhibited stronger token-level alignment and more effective handling of variations, while weaker models failed to generalize effectively, particularly in Marathi, where linguistic complexity posed additional challenges. Another key observation is that models generally performed better on Hindi than Marathi, which can be attributed to differences in dataset size, linguistic complexity, and pre-training exposure. Hindi, being more widely studied, benefits from larger corpora and more refined tokenization strategies, whereas Marathi's syntactic variations pose additional challenges. Models that struggled, such as Falcon Mamba and Hymba, lacked the ability to effectively process inflectional diversity and complex word formations, leading to lower performance. This highlights the necessity of incorporating language-specific optimizations and better pre-training strategies to improve generalization in underrepresented Indic languages.


\subsubsection{Results With Fine-Tuning}
Fine-tuning significantly enhanced model performance across all evaluation metrics, demonstrating the importance of task-specific adaptation for Indic languages. The most improvements were observed in Mamba-2, which exhibited superior alignment with reference answers, improved span localization, and increased robustness in handling complex linguistic structures. These gains highlight the ability of fine-tuned models to better capture long-range dependencies and semantic nuances in Hindi and Marathi question-answering tasks.One key observation is that fine-tuning helped narrow the performance gap between Hindi and Marathi, particularly in models that struggled before adaptation. While Marathi remained more challenging, models showed increased accuracy in capturing token overlaps and generating more contextually relevant responses. Additionally, models that initially performed poorly, such as Falcon Mamba and Hymba, saw only marginal improvements, suggesting fundamental architectural limitations that fine-tuning alone could not overcome.
Beyond numerical improvements, fine-tuning also contributed to better lexical and semantic alignment. Models demonstrated enhanced fluency in response generation, with improvements in phrase structure and word selection, reducing inconsistencies seen in non-fine-tuned outputs. However, challenges persisted for certain models, particularly in maintaining coherence when processing long-form text in Marathi. These findings emphasize the necessity of fine-tuning pre-trained models on domain-specific datasets to optimize their performance for low-resource languages. Models results after fine-tuning is provided in ~\cref{tbl:combined_table} , illustrating the extent of improvement across different architectures.
\begin{table*}[h!]
  \caption{Results on Hindi and Marathi Datasets without FineTuning.}
  \label{tbl:combined_table_finetuning}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{threeparttable}
  \begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
    \toprule
    \multirow{2}{*}{Model} & 
    \multicolumn{2}{c|}{\emph{Exact Match Rate}} & 
    \multicolumn{2}{c|}{\emph{F1}} & 
    \multicolumn{2}{c|}{\emph{BLEU}} & 
    \multicolumn{2}{c|}{\emph{ROUGE}} & 
    \multicolumn{2}{c}{\emph{BERT}} \\
    \cmidrule{2-11}
     & Hindi & Marathi & Hindi & Marathi & Hindi & Marathi & Hindi & Marathi & Hindi & Marathi \\
    \midrule
    \texttt{Mamba} & 0.031 & \textbf{\textcolor{Red}{0.011}} & 0.209 & 0.216 & 0.18 & 0.15 & 0.32 & 0.28 & 0.45 & 0.40 \\
    \texttt{Mamba-2}  & 0.056 & 0.036 & \textbf{\textcolor{ForestGreen}{0.591}} & \textbf{\textcolor{ForestGreen}{0.341}} & \textbf{\textcolor{ForestGreen}{0.28}} & \textbf{\textcolor{ForestGreen}{0.22}} & \textbf{\textcolor{ForestGreen}{0.48}} & \textbf{\textcolor{ForestGreen}{0.40}} & \textbf{\textcolor{ForestGreen}{0.62}} & \textbf{\textcolor{ForestGreen}{0.55}} \\
    \texttt{Falcon}   & \textbf{\textcolor{Red}{0.012}} & 0.007 & 0.042 & \textbf{\textcolor{Red}{0.010}} & 0.12 & 0.08 & 0.25 & 0.18 & 0.38 & 0.30 \\
    \texttt{Jamba}    & 0.022 & 0.019 & 0.125 & 0.231 & 0.15 & 0.12 & 0.28 & 0.25 & 0.41 & 0.37 \\
    \texttt{Zamba}    & \textbf{\textcolor{ForestGreen}{0.095}} & 0.051 & 0.085 & 0.041 & 0.08 & 0.04 & 0.15 & 0.10 & 0.30 & 0.22 \\
    \texttt{Samba}    & 0.072 & \textbf{\textcolor{ForestGreen}{0.055}} & 0.085 & 0.025 & 0.07 & 0.05 & 0.12 & 0.12 & 0.28 & 0.25 \\
    \texttt{Hymba}    & 0.048 & 0.021 & \textbf{\textcolor{Red}{0.031}} & 0.015 & \textbf{\textcolor{Red}{0.05}} & \textbf{\textcolor{Red}{0.03}} & \textbf{\textcolor{Red}{0.10}} & \textbf{\textcolor{Red}{0.08}} & \textbf{\textcolor{Red}{0.25}} & \textbf{\textcolor{Red}{0.20}} \\
    \bottomrule
  \end{tabular}
  \end{threeparttable}
  }
\end{table*}

\begin{table*}[tp!]
  \caption{Results on Hindi and Marathi Datasets with FineTuning.}
  \label{tbl:combined_table}
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{threeparttable}
  \begin{tabular}{l|r|r|r|r|r|r|r|r|r|r}
    \toprule
    \multirow{2}{*}{Model} & 
    \multicolumn{2}{c|}{\emph{Exact Match Rate}} & 
    \multicolumn{2}{c|}{\emph{F1}} & 
    \multicolumn{2}{c|}{\emph{BLEU}} & 
    \multicolumn{2}{c|}{\emph{ROUGE}} & 
    \multicolumn{2}{c}{\emph{BERT}} \\
    \cmidrule{2-11}
     & Hindi & Marathi & Hindi & Marathi & Hindi & Marathi & Hindi & Marathi & Hindi & Marathi \\
    \midrule
    \texttt{Mamba} & 0.241 & 0.031 & 0.407 & 0.209 & 0.25 & 0.22 & 0.45 & 0.40 & 0.65 & 0.58 \\
    \texttt{Mamba-2}  & \textbf{\textcolor{ForestGreen}{0.316}} & 0.056 & \textbf{\textcolor{ForestGreen}{0.791}} & \textbf{\textcolor{ForestGreen}{0.591}} & \textbf{\textcolor{ForestGreen}{0.45}} & \textbf{\textcolor{ForestGreen}{0.40}} & \textbf{\textcolor{ForestGreen}{0.58}} & \textbf{\textcolor{ForestGreen}{0.52}} & \textbf{\textcolor{ForestGreen}{0.78}} & \textbf{\textcolor{ForestGreen}{0.72}} \\
    \texttt{Falcon}   & 0.312 & \textbf{\textcolor{red}{0.012}} & 0.241 & 0.042 & 0.20 & 0.15 & 0.38 & 0.30 & 0.55 & 0.45 \\
    \texttt{Jamba}    & 0.135 & 0.022 & 0.411 & 0.125 & 0.22 & 0.18 & 0.42 & 0.35 & 0.60 & 0.50 \\
    \texttt{Zamba}    & 0.120 & 0.075 & 0.165 & 0.080 & 0.15 & 0.10 & 0.30 & 0.25 & 0.48 & 0.40 \\
    \texttt{Samba}    & 0.085 & 0.050 & 0.095 & 0.045 & \textbf{\textcolor{red}{0.10}} & 0.08 & 0.25 & \textbf{\textcolor{red}{0.20}} & 0.42 & 0.35 \\
    \texttt{Hymba}    & \textbf{\textcolor{red}{0.060}} & 0.035 & \textbf{\textcolor{red}{0.050}} & \textbf{\textcolor{red}{0.025}} & 0.08 & \textbf{\textcolor{red}{0.06}} & \textbf{\textcolor{red}{0.20}} & 0.15 & \textbf{\textcolor{red}{0.38}} & \textbf{\textcolor{red}{0.30}} \\
    \bottomrule
  \end{tabular}
  \end{threeparttable}
  }
\end{table*}


\section{Conclusion}
\label{sec:conclusion}
This paper explores the development of efficient question-answering systems for Indic languages. By applying SSMs, this research demonstrates  advancements in capturing the linguistic nuances of Hindi and Marathi. Among the models evaluated, Mamba-2 consistently delivered superior performance, showcasing its ability to handle both short-term and long-term dependencies effectively. Fine-tuning proved critical in improving model accuracy, highlighting the importance of adapting pre-trained architectures to meet the specific needs of low-resource languages. The integration of robust preprocessing techniques, such as tokenization tailored for Indic scripts, further contributed to the models' success. These findings lay a strong foundation for scalable, multilingual QA systems and underscore the potential of SSMs in addressing language-specific challenges. Future work will focus on expanding dataset diversity by including additional dialects of Indic languages and developing a unified model for commonly used dialects.

\section*{Impact Statement}
This research addresses key challenges in developing efficient and context-aware QA systems for Indic languages by leveraging SSMs. It introduces a scalable framework tailored to the complex morphology, syntax, and contextual nuances of these languages. The study advances NLP for low-resource languages, setting a benchmark for multilingual scenarios through SSM-based architectures. By focusing on widely spoken languages like Hindi and Marathi, it enhances accessibility and inclusivity, enabling AI systems to serve underrepresented linguistic communities. The research has real-world applications across sectors such as education, healthcare, governance, and customer support, improving service delivery through localized QA systems. Its emphasis on computational efficiency ensures that these models can be deployed in resource-constrained environments, extending their impact to underserved regions. This work lays a strong foundation for future Indic language QA research and provides a scalable blueprint for adapting similar approaches to other low-resource and linguistically diverse languages worldwide.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
% \section{Theoretical Background}
% A state space represents the smallest set of variables necessary to completely describe a system. It provides a mathematical framework to define all the possible configurations or conditions a system can occupy.

% To illustrate, think of navigating a maze. The "state space" can be visualized as a map of all potential locations within the maze, where each point represents a unique position with details, such as your distance from the exit.

% The "state space representation" simplifies this concept by outlining your current location (current state), the possible paths you can take (future states), and the actions required to transition between them (e.g., turning left or right).
% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{img/ssm.png}
%     \caption{Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models ~\cite{goel2022s4}.}
%     \label{fig:structured_state_spaces}
% \end{figure}

\section{Dataset Statistics}
The datasets used in this study encompass diverse Indic languages, primarily focusing on Hindi and Marathi. The datasets are structured in a SQuAD-style format, comprising context paragraphs, questions, and corresponding answers. This standardized format ensures consistency across different QA tasks and facilitates effective model training and evaluation.\\
\\
The scatter plot in ~\cref{fig_scatterPlot} illustrates the relationship between question length and answer length across the Hindi (blue) and Marathi (red) datasets. The visualization highlights a clear positive correlation between the lengths of questions and their corresponding answers, with longer questions often leading to longer answers. Notably, the Marathi dataset shows slightly shorter average lengths compared to the Hindi dataset, reflecting language-specific characteristics. This analysis provides insights into the structural patterns within the dataset, aiding in understanding the data distribution and informing preprocessing decisions for the SSM-based QA model.
\begin{figure}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=13.0cm]{img/Data_analysis.png}}
  % \centerline{\includegraphics[width=9.0cm]{img/data_2.png}}
  % \vspace{2.0cm}
  \caption{\label{fig_scatterPlot}Question Length vs Answer Length. This plot compares the lengths of questions and answers in the Hindi (blue) and Marathi (red) datasets, highlighting a correlation between them."}
\end{minipage}
\end{figure}
\\
\\
The scatter plot in ~\cref{fig_dataStats} illustrates the relationship between the starting position of answers within the context and the total context length for the Hindi (blue) and Marathi (red) datasets. The visualization shows that as the context length increases, the starting positions of answers become more varied, indicating that longer contexts are more likely to contain answers at different positions. This trend is consistent across both datasets, although the Marathi dataset tends to have shorter contexts on average compared to Hindi. This analysis highlights the diversity in answer positions relative to context length, which is crucial for designing models capable of handling varied input structures in question-answering tasks.
\begin{figure}[htb!]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=14.5cm]{img/data_2.png}}
  % \vspace{2.0cm}
  \caption{\label{fig_dataStats} Answer Start Position vs. Context Length for Hindi (blue) and Marathi (red) datasets.}
\end{minipage}
\end{figure}
\newline
The correlation heatmaps provide insights into the relationships between key features in the Hindi and Marathi datasets, including context length, question length, answer length, and answer start position. As shown, context length exhibits a strong positive correlation with answer start position (0.48 for Hindi and 0.53 for Marathi), indicating that longer contexts tend to have answers starting further into the text. This relationship is more pronounced in the Marathi dataset. Additionally, the correlations between question length and answer length are negligible, suggesting that the lengths of questions and answers are largely independent. These observations reflect the structural differences in data organization and provide useful information for designing models that can effectively capture these patterns. The heatmaps highlight the similarities and differences in feature interactions between the two languages, guiding preprocessing and feature selection for downstream tasks.

\begin{figure}[H]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=7.0cm]{img/data_3.png}}
  \centerline{\includegraphics[width=7.0cm]{img/data_4.png}}
  % \vspace{2.0cm}
  \caption{\label{fig_pipeline} Correlation heatmaps for context length, question length, answer length, and answer start position in Hindi and Marathi datasets.}
\end{minipage}
\end{figure}
% \\
\section{Results}
The results, as shown in ~\cref{fig_results}, illustrate the performance metrics for the Hindi and Marathi datasets, comparing outcomes before and after fine-tuning the model. Metrics such as BLEU, ROUGE, F1, Exact Match, and BERT scores are presented for both languages, highlighting the impact of fine-tuning on model performance. The left panel represent the scores without fine-tuning, while the right panels show the improved results after fine-tuning. Across both datasets, fine-tuning consistently improves all evaluation metrics, with notable gains in Exact Match and F1 scores, indicating better alignment between predicted and ground-truth answers. The BERT scores also demonstrate significant improvements, reflecting enhanced semantic understanding. These results emphasize the importance of fine-tuning for adapting the model to the nuances of Hindi and Marathi languages in the QA task.

\begin{figure*}[htb]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=17.0cm]{img/QA-Page-3.drawio.pdf}}
  % \vspace{2.0cm}
  \caption{\label{fig_sampleResults} Comparison of positive and negative samples for question-answering tasks, highlighting model responses.}
\end{minipage}
\end{figure*}

\begin{figure}[htb]
\centering
\adjustbox{frame}{ % Adds a box around the content
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=10.0cm]{img/result_1.png}}
  \vspace{0.5cm} % Add spacing between rows
  \centerline{\includegraphics[width=10.0cm]{img/result_2.png}}
  \vspace{0.5cm}
  \centerline{\includegraphics[width=10.0cm]{img/result_6.png}}
  \vspace{0.5cm}
  \centerline{\includegraphics[width=10.0cm]{img/result_5.png}}
  \vspace{0.5cm}
  \centerline{\includegraphics[width=10.0cm]{img/result_7.png}}
  \vspace{1.0cm} % Adjust spacing before the caption
  \caption{\label{fig_results}Comparison of BLEU, ROUGE, F1, Exact Match, and BERT scores for Hindi and Marathi datasets before and after fine-tuning.}

\end{minipage}
}
\end{figure}

% \setlength{\textfloatsep}{0pt}
% \begin{algorithm}[tb]
% \caption{Data Preprocessing and Chat Dataset Preparation}
% \label{alg:chat_dataset}
% \begin{algorithmic}[1]
% \STATE {\bf Input:} Data path $data\_path$, Tokenizer $tokenizer$, Template $template$, Max tokens $max\_tokens$
% \STATE {\bf Output:} Tokenized dataset with input IDs and labels
% \STATE \textbf{Step 1: Load Data}
% \STATE Initialize $data=[]$
% \FOR{each line in file at $data\_path$}
%     \STATE Parse line to JSON, append to $data$; skip and log on failure
% \ENDFOR
% \STATE \textbf{Step 2: Tokenize Conversations}
% \STATE Initialize $all\_input\_ids=[]$, $all\_label\_ids=[]$
% \STATE Disable system prompt in $tokenizer$
% \FOR{each conversation in $data$}
%     \STATE Extract and tokenize assistant messages, append to $tokenized\_responses$
%     \STATE Apply $template$, tokenize conversation to $tokenized\_conv$
%     \STATE Append $tokenized\_conv$ to $all\_input\_ids$
% \ENDFOR
% \STATE \textbf{Step 3: Define Dataset Class}
% \STATE Create \texttt{ChatDataset} with $input\_ids$, $labels$, and \texttt{getitem(i)} for index $i$
% \STATE \textbf{Step 4: Define Collator for Batching}
% \STATE Define \texttt{DataCollatorForChatDataset}: Pad $input\_ids$, create attention masks
% \STATE \textbf{Step 5: Vocabulary for HindiQA}
% \STATE Define \texttt{HindiQADataset} with frequency-based vocab construction, tokenization of context, questions, answers
% \STATE \textbf{Step 6: Final Dataset Preparation}
% \STATE Instantiate \texttt{ChatDataset}, use \texttt{DataCollatorForChatDataset}, prepare train/validation/test splits
% \end{algorithmic}
% \end{algorithm}
% \setlength{\textfloatsep}{0pt}
% \begin{algorithm}[htb!]
%    \raggedright
%    \footnotesize
%    \caption{Training and Evaluation for one of the QA Model}
%    \label{alg:falcon_qa}
% \begin{algorithmic}[1] % Line numbering for better readability
%    \STATE {\bfseries Input:} Dataset $D = \{D_{\text{train}}, D_{\text{val}}, D_{\text{test}}\}$, Vocabulary $V$, Pretrained Falcon Model $M$
%    \STATE {\bfseries Output:} Fine-tuned Falcon QA Model $M^*$
   
%    \STATE {\bfseries Step 1: Preprocessing and Tokenization}
%    \STATE Load datasets $D_{\text{train}}, D_{\text{val}}, D_{\text{test}}$
%    \STATE Tokenize questions, contexts, and answers:
%    \STATE $T_{\text{question}} = \text{tokenize}(Q)$
%    \STATE $T_{\text{context}} = \text{tokenize}(C)$
%    \STATE $T_{\text{answer}} = \text{tokenize}(A)$
%    \STATE Build vocabulary $V$:
%    \STATE $V = \{w \mid \text{frequency}(w) \geq \text{threshold}\}$

%    \STATE {\bfseries Step 2: Encoding and Dataset Preparation}
%    \STATE Encode tokenized text:
%    \STATE $E_{\text{question}} = \text{encode}(T_{\text{question}}, V)$
%    \STATE $E_{\text{context}} = \text{encode}(T_{\text{context}}, V)$
%    \STATE $E_{\text{answer}} = \text{encode}(T_{\text{answer}}, V)$
%    \STATE Prepare batches using padding:
%    \STATE $B = \text{collate}(E_{\text{question}}, E_{\text{context}}, E_{\text{answer}})$

%    \STATE {\bfseries Step 3: Model Initialization}
%    \STATE Initialize Falcon QA Model $M$ with pre-trained weights
%    \STATE Define loss function:
%    \STATE $\mathcal{L} = \mathcal{L}_{\text{start}} + \mathcal{L}_{\text{end}}$
%    \STATE $\mathcal{L}_{\text{start}} = -\sum_{i} y_i^{\text{start}} \log(p_i^{\text{start}})$
%    \STATE Define optimizer:
%    \STATE $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}$

%    \STATE {\bfseries Step 4: Training Loop}
%    \REPEAT
%       \STATE Initialize $L_{\text{train}} = 0$
%       \FOR{$b \in B_{\text{train}}$}
%          \STATE Compute logits:
%          \STATE $\hat{y}_{\text{start}}, \hat{y}_{\text{end}} = M(x_{\text{context}}, x_{\text{question}})$
%          \STATE Compute losses and update weights:
%          \STATE $\theta \leftarrow \theta - \eta \nabla_\theta (\mathcal{L}_{\text{start}} + \mathcal{L}_{\text{end}})$
%          \STATE Accumulate training loss:
%          \STATE $L_{\text{train}} \leftarrow L_{\text{train}} + \mathcal{L}_{\text{start}} + \mathcal{L}_{\text{end}}$
%       \ENDFOR
%       \STATE Compute average training loss:
%       \STATE $\bar{L}_{\text{train}} = \frac{L_{\text{train}}}{|B_{\text{train}}|}$

%       \STATE \textbf{Validation Phase:}
%       \STATE Initialize metrics: $L_{\text{val}}, \text{EM}, \text{F1}$
%       \FOR{$b \in B_{\text{val}}$}
%          \STATE Evaluate metrics and update $L_{\text{val}}$
%       \ENDFOR
%       \STATE Save model if:
%       \STATE $\text{EM}_{\text{current}} > \text{EM}_{\text{best}}$
%    \UNTIL{Convergence or max epochs reached}

%    \STATE {\bfseries Step 5: Test Evaluation}
%    \STATE Evaluate the best model on $B_{\text{test}}$:
%    \STATE $\text{Metrics: } \text{EM}, \text{F1}, \text{ROUGE}, \text{BLEU}, \text{Bert Score}
%    \bar{L}_{\text{test}}$
% \end{algorithmic}
% \end{algorithm}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
