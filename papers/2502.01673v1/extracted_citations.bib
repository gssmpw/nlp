@inproceedings{Khan_2024,
   title={IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages},
   url={http://dx.doi.org/10.18653/v1/2024.acl-long.843},
   DOI={10.18653/v1/2024.acl-long.843},
   booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Khan, Mohammed and Mehta, Priyam and Sankar, Ananth and Kumaravelan, Umashankar and Doddapaneni, Sumanth and B, Suriyaprasaad and G, Varun and Jain, Sparsh and Kunchukuttan, Anoop and Kumar, Pratyush and Dabre, Raj and Khapra, Mitesh},
   year={2024},
   pages={15831–15879} }

@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval}
}

@misc{abreu2024qs5quantizedstatespace,
      title={Q-S5: Towards Quantized State Space Models}, 
      author={Steven Abreu and Jens E. Pedersen and Kade M. Heckel and Alessandro Pierro},
      year={2024},
      eprint={2406.09477},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.09477}, 
}

@misc{bourdois2022ssm,
  author       = {Louis Bourdois},
  title        = {Get on the SSM Train},
  year         = {2022},
  howpublished = {\url{https://huggingface.co/blog/lbourdois/get-on-the-ssm-train}},
  note         = {Accessed: 2025-01-10}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={8440--8451},
  year={2020}
}

@misc{dani2024reviewmarathinaturallanguage,
      title={A Review of the Marathi Natural Language Processing}, 
      author={Asang Dani and Shailesh R Sathe},
      year={2024},
      eprint={2412.15471},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15471}, 
}

@misc{dong2024hymbahybridheadarchitecturesmall,
      title={Hymba: A Hybrid-head Architecture for Small Language Models}, 
      author={Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov},
      year={2024},
      eprint={2411.13676},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.13676}, 
}

@article{goel2022s4,
  title        = {S4: Structured State Space for Sequence Modeling},
  author       = {Goel, Karan and Rush, Alexander M. and Vaswani, Ashish},
  journal      = {arXiv preprint arXiv:2111.00396},
  year         = {2022},
  url          = {https://arxiv.org/abs/2111.00396}
}

@article{gu2022structured,
  title={Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models},
  author={Gu, Albert and others},
  journal={arXiv preprint arXiv:2206.11893},
  year={2022}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@inproceedings{jin2022low,
  title={Low Resource Style Transfer via Domain Adaptive Meta Learning},
  author={Jin, Di and Jin, Zhijing and He, Junxian and Yang, Zichao and Li, Zhiting and Neubig, Graham},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2993--3004},
  year={2022}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, Rudolf E},
  journal={Journal of Basic Engineering},
  volume={82},
  number={1},
  pages={35--45},
  year={1960},
  publisher={American Society of Mechanical Engineers Digital Collection}
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}

@misc{liu2024masvspeakerverificationglobal,
      title={MASV: Speaker Verification with Global and Local Context Mamba}, 
      author={Yang Liu and Li Wan and Yiteng Huang and Ming Sun and Yangyang Shi and Florian Metze},
      year={2024},
      eprint={2412.10989},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2412.10989}, 
}

@misc{patro2024heracleshybridssmtransformermodel,
      title={Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis}, 
      author={Badri N. Patro and Suhas Ranganath and Vinay P. Namboodiri and Vijay S. Agneeswaran},
      year={2024},
      eprint={2403.18063},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.18063}, 
}

@article{rabiner1986hmm,
  title={An introduction to hidden Markov models},
  author={Rabiner, Lawrence R and Juang, B. H.},
  journal={IEEE ASSP Magazine},
  volume={3},
  number={1},
  pages={4--16},
  year={1986},
  publisher={IEEE}
}

@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2383--2392},
  year={2016}
}

@misc{ren2024sambasimplehybridstate,
      title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}, 
      author={Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
      year={2024},
      eprint={2406.07522},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07522}, 
}

@misc{sabane2024,
      title={Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi}, 
      author={Maithili Sabane and Onkar Litake and Aman Chadha},
      year={2024},
      eprint={2308.09862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.09862}, 
}

@misc{sarrof2024expressivecapacitystatespace,
      title={The Expressive Capacity of State Space Models: A Formal Language Perspective}, 
      author={Yash Sarrof and Yana Veitsman and Michael Hahn},
      year={2024},
      eprint={2405.17394},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17394}, 
}

@misc{shakhadri2025sambaasrstateoftheartspeechrecognition,
      title={Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models}, 
      author={Syed Abdul Gaffar Shakhadri and Kruthika KR and Kartik Basavaraj Angadi},
      year={2025},
      eprint={2501.02832},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.02832}, 
}

@misc{singh2024indicqabenchmarkmultilingual,
      title={INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages}, 
      author={Abhishek Kumar Singh and Rudra Murthy and Vishwajeet kumar and Jaydeep Sen and Ganesh Ramakrishnan},
      year={2024},
      eprint={2407.13522},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.13522}, 
}

@misc{taha2025logarithmicmemorynetworkslmns,
      title={Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments}, 
      author={Mohamed A. Taha},
      year={2025},
      eprint={2501.07905},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.07905}, 
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5998--6008},
  year={2017}
}

@misc{wang2024statespacemodelnewgeneration,
      title={State Space Model for New-Generation Network Alternative to Transformers: A Survey}, 
      author={Xiao Wang and Shiao Wang and Yuhe Ding and Yuehang Li and Wentao Wu and Yao Rong and Weizhe Kong and Ju Huang and Shihao Li and Haoxiang Yang and Ziwen Wang and Bo Jiang and Chenglong Li and Yaowei Wang and Yonghong Tian and Jin Tang},
      year={2024},
      eprint={2404.09516},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.09516}, 
}

@inproceedings{xue2021mt5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Sumit and Raffel, Colin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages={483--498},
  year={2021}
}

@misc{zuo2024,
      title={Falcon Mamba: The First Competitive Attention-free 7B Language Model}, 
      author={Jingwei Zuo and Maksim Velikanov and Dhia Eddine Rhaiem and Ilyas Chahed and Younes Belkada and Guillaume Kunsch and Hakim Hacid},
      year={2024},
      eprint={2410.05355},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.05355}, 
}

