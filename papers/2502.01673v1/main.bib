@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})

@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW = {CVPRW})
@String(CSVT = {IEEE TCSVT})
@misc{bourdois2022ssm,
  author       = {Louis Bourdois},
  title        = {Get on the SSM Train},
  year         = {2022},
  howpublished = {\url{https://huggingface.co/blog/lbourdois/get-on-the-ssm-train}},
  note         = {Accessed: 2025-01-10}
}
@article{goel2022s4,
  title        = {S4: Structured State Space for Sequence Modeling},
  author       = {Goel, Karan and Rush, Alexander M. and Vaswani, Ashish},
  journal      = {arXiv preprint arXiv:2111.00396},
  year         = {2022},
  url          = {https://arxiv.org/abs/2111.00396}
}
@misc{Jamba,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19887}, 
}
@misc{zuo2024,
      title={Falcon Mamba: The First Competitive Attention-free 7B Language Model}, 
      author={Jingwei Zuo and Maksim Velikanov and Dhia Eddine Rhaiem and Ilyas Chahed and Younes Belkada and Guillaume Kunsch and Hakim Hacid},
      year={2024},
      eprint={2410.05355},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.05355}, 
}
@misc{sabane2024,
      title={Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi}, 
      author={Maithili Sabane and Onkar Litake and Aman Chadha},
      year={2024},
      eprint={2308.09862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.09862}, 
}
@misc{vannguyen2024,
      title={Taipan: Efficient and Expressive State Space Language Models with Selective Attention}, 
      author={Chien Van Nguyen and Huy Huu Nguyen and Thang M. Pham and Ruiyi Zhang and Hanieh Deilamsalehy and Puneet Mathur and Ryan A. Rossi and Trung Bui and Viet Dac Lai and Franck Dernoncourt and Thien Huu Nguyen},
      year={2024},
      eprint={2410.18572},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.18572}, 
}
@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, Rudolf E},
  journal={Journal of Basic Engineering},
  volume={82},
  number={1},
  pages={35--45},
  year={1960},
  publisher={American Society of Mechanical Engineers Digital Collection}
}
@article{rabiner1986hmm,
  title={An introduction to hidden Markov models},
  author={Rabiner, Lawrence R and Juang, B. H.},
  journal={IEEE ASSP Magazine},
  volume={3},
  number={1},
  pages={4--16},
  year={1986},
  publisher={IEEE}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5998--6008},
  year={2017}
}
@misc{ren2024sambasimplehybridstate,
      title={Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling}, 
      author={Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
      year={2024},
      eprint={2406.07522},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07522}, 
}

@misc{dong2024hymbahybridheadarchitecturesmall,
      title={Hymba: A Hybrid-head Architecture for Small Language Models}, 
      author={Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov},
      year={2024},
      eprint={2411.13676},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.13676}, 
}
@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
@inproceedings{rajpurkar2016squad,
  title={SQuAD: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2383--2392},
  year={2016}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={8440--8451},
  year={2020}
}

@article{ortiz2020oscar,
  title={OSCAR: Open Super-large Crawled Aligned Raw Text Corpus},
  author={Ortiz Su{\'a}rez, Pedro Javier and Romary, Laurent and Sagot, Beno{\^\i}t},
  journal={arXiv preprint arXiv:1911.12237},
  year={2020}
}

@inproceedings{xue2021mt5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Sumit and Raffel, Colin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  pages={483--498},
  year={2021}
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}
@article{lin2004rouge,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Lin, Chin-Yew},
  journal={Text Summarization Branches Out},
  pages={74--81},
  year={2004}
}
@inproceedings{papineni2002bleu,
  title={BLEU: a Method for Automatic Evaluation of Machine Translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)},
  pages={311--318},
  year={2002}
}
@article{gu2022structured,
  title={Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models},
  author={Gu, Albert and others},
  journal={arXiv preprint arXiv:2206.11893},
  year={2022}
}
@inproceedings{jin2022low,
  title={Low Resource Style Transfer via Domain Adaptive Meta Learning},
  author={Jin, Di and Jin, Zhijing and He, Junxian and Yang, Zichao and Li, Zhiting and Neubig, Graham},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2993--3004},
  year={2022}
}
@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}
@misc{singh2024indicqabenchmarkmultilingual,
      title={INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages}, 
      author={Abhishek Kumar Singh and Rudra Murthy and Vishwajeet kumar and Jaydeep Sen and Ganesh Ramakrishnan},
      year={2024},
      eprint={2407.13522},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.13522}, 
}
@misc{dao2024transformersssmsgeneralizedmodels,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}
@misc{child2019generatinglongsequencessparse,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1904.10509}, 
}
@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}
@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}
@misc{kunchukuttan2020indicnlp,
author = "Anoop Kunchukuttan",
title = "{The IndicNLP Library}",
year = "2020",
howpublished={\url{https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf}}
}
@inproceedings{Dabre_2022,
   title={IndicBART: A Pre-trained Model for Indic Natural Language Generation},
   url={http://dx.doi.org/10.18653/v1/2022.findings-acl.145},
   DOI={10.18653/v1/2022.findings-acl.145},
   booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
   publisher={Association for Computational Linguistics},
   author={Dabre, Raj and Shrotriya, Himani and Kunchukuttan, Anoop and Puduppully, Ratish and Khapra, Mitesh and Kumar, Pratyush},
   year={2022} }


@article{zuo2024falcon,
  title={Falcon mamba: The first competitive attention-free 7b language model},
  author={Zuo, Jingwei and Velikanov, Maksim and Rhaiem, Dhia Eddine and Chahed, Ilyas and Belkada, Younes and Kunsch, Guillaume and Hacid, Hakim},
  journal={arXiv preprint arXiv:2410.05355},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}


@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@misc{lo2024closerlookmixtureofexpertslarge,
      title={A Closer Look into Mixture-of-Experts in Large Language Models}, 
      author={Ka Man Lo and Zeyu Huang and Zihan Qiu and Zili Wang and Jie Fu},
      year={2024},
      eprint={2406.18219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18219}, 
}
@misc{Zamba,
      title={Zamba: A Compact 7B SSM Hybrid Model}, 
      author={Paolo Glorioso and Quentin Anthony and Yury Tokpanov and James Whittington and Jonathan Pilault and Adam Ibrahim and Beren Millidge},
      year={2024},
      eprint={2405.16712},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16712}, 
}
@misc{beltagy2020longformerlongdocumenttransformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.05150}, 
}
@misc{powers2020evaluationprecisionrecallfmeasure,
      title={Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation}, 
      author={David M. W. Powers},
      year={2020},
      eprint={2010.16061},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.16061}, 
}
@misc{maalouly2022exactmatchingalgorithmsrelated,
      title={Exact Matching: Algorithms and Related Problems}, 
      author={Nicolas El Maalouly},
      year={2022},
      eprint={2203.13899},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/2203.13899}, 
}
@misc{liu2024masvspeakerverificationglobal,
      title={MASV: Speaker Verification with Global and Local Context Mamba}, 
      author={Yang Liu and Li Wan and Yiteng Huang and Ming Sun and Yangyang Shi and Florian Metze},
      year={2024},
      eprint={2412.10989},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2412.10989}, 
}
@misc{shakhadri2025sambaasrstateoftheartspeechrecognition,
      title={Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models}, 
      author={Syed Abdul Gaffar Shakhadri and Kruthika KR and Kartik Basavaraj Angadi},
      year={2025},
      eprint={2501.02832},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.02832}, 
}
@misc{wang2024statespacemodelnewgeneration,
      title={State Space Model for New-Generation Network Alternative to Transformers: A Survey}, 
      author={Xiao Wang and Shiao Wang and Yuhe Ding and Yuehang Li and Wentao Wu and Yao Rong and Weizhe Kong and Ju Huang and Shihao Li and Haoxiang Yang and Ziwen Wang and Bo Jiang and Chenglong Li and Yaowei Wang and Yonghong Tian and Jin Tang},
      year={2024},
      eprint={2404.09516},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.09516}, 
}

@article{lieber2024jamba,
  title={Jamba: A hybrid transformer-mamba language model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}
@misc{sarrof2024expressivecapacitystatespace,
      title={The Expressive Capacity of State Space Models: A Formal Language Perspective}, 
      author={Yash Sarrof and Yana Veitsman and Michael Hahn},
      year={2024},
      eprint={2405.17394},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17394}, 
}
@misc{patro2024heracleshybridssmtransformermodel,
      title={Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis}, 
      author={Badri N. Patro and Suhas Ranganath and Vinay P. Namboodiri and Vijay S. Agneeswaran},
      year={2024},
      eprint={2403.18063},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.18063}, 
}
@misc{taha2025logarithmicmemorynetworkslmns,
      title={Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments}, 
      author={Mohamed A. Taha},
      year={2025},
      eprint={2501.07905},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.07905}, 
}
@misc{abreu2024qs5quantizedstatespace,
      title={Q-S5: Towards Quantized State Space Models}, 
      author={Steven Abreu and Jens E. Pedersen and Kade M. Heckel and Alessandro Pierro},
      year={2024},
      eprint={2406.09477},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.09477}, 
}
@misc{dani2024reviewmarathinaturallanguage,
      title={A Review of the Marathi Natural Language Processing}, 
      author={Asang Dani and Shailesh R Sathe},
      year={2024},
      eprint={2412.15471},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15471}, 
}
@inproceedings{Khan_2024,
   title={IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages},
   url={http://dx.doi.org/10.18653/v1/2024.acl-long.843},
   DOI={10.18653/v1/2024.acl-long.843},
   booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Khan, Mohammed and Mehta, Priyam and Sankar, Ananth and Kumaravelan, Umashankar and Doddapaneni, Sumanth and B, Suriyaprasaad and G, Varun and Jain, Sparsh and Kunchukuttan, Anoop and Kumar, Pratyush and Dabre, Raj and Khapra, Mitesh},
   year={2024},
   pages={15831–15879} }
@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval}
}
@misc{sennrich2016neuralmachinetranslationrare,
      title={Neural Machine Translation of Rare Words with Subword Units}, 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1508.07909}, 
}
@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}
@article{ye2022unreliability,
  title={The unreliability of explanations in few-shot prompting for textual reasoning},
  author={Ye, Xi and Durrett, Greg},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={30378--30392},
  year={2022}
}

@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended abstracts of the 2021 CHI conference on human factors in computing systems},
  pages={1--7},
  year={2021}
}

@article{velivckovic2018deep,
  title={Deep graph infomax},
  author={Veli{\v{c}}kovi{\'c}, Petar and Fedus, William and Hamilton, William L and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R Devon},
  journal={arXiv preprint arXiv:1809.10341},
  year={2018}
}


@article{glorioso2024zamba,
  title={Zamba: A Compact 7B SSM Hybrid Model},
  author={Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
  journal={arXiv preprint arXiv:2405.16712},
  year={2024}
}
@misc{brown2020languagemodelsfewshotlearners,
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}