\section{Related Works}
\label{sec:related}

The field of QA and NLP has seen rapid advancements, particularly with the introduction of models leveraging state-space representations and multi-modal learning. This section reviews key advancements related to SSMs, Indic language processing, and multi-modal approaches to contextual understanding.

\subsection{Advances in State Space Models}
Traditional SSMs, such as the Kalman Filter ~\cite{kalman1960new} and Hidden Markov Models (HMMs) ~\cite{rabiner1986hmm}, have been widely used for decades. While effective for many tasks, these models struggle with long-range dependencies and high-dimensional data ~\cite{bourdois2022ssm}. To overcome these limitations, SSMs have emerged, combining the foundational principles of traditional SSMs with the expressive power of deep learning ~\cite{sarrof2024expressivecapacitystatespace}. These approaches allow for scalability and adaptability in modern machine learning applications ~\cite{goel2022s4}.SSMs have emerged as a robust alternative to transformers in sequence modeling, excelling particularly in areas where transformers face inherent limitations ~\cite{patro2024heracleshybridssmtransformermodel}. Mathematically, an SSM can be represented as follows:
\begin{equation}
    {
        \mathbf{x}_{t+1} = \mathbf{A} \mathbf{x}_t + \mathbf{B} \mathbf{u}_t + \mathbf{w}_t, \quad \mathbf{w}_t \sim \mathcal{N}(0, \mathbf{Q})
    } 
    \label{eq:ssm_equations}
\end{equation}

The ~\cref{eq:ssm_equations} ~\cite{gu2022structured} describes a framework where the state vector \( \mathbf{x}_t \) at time \( t \) represents latent variables encapsulating the system's underlying dynamics. The state evolution from \( t \) to \( t+1 \) is governed by the state transition matrix \( \mathbf{A} \), while the input matrix \( \mathbf{B} \) determines the effect of the control input \( \mathbf{u}_t \) on the state transitions. The control input \( \mathbf{u}_t \) serves as an external influence, allowing modifications to the state dynamics at time \( t \). Additionally, process noise \( \mathbf{w}_t \) is modeled as a Gaussian random variable with zero mean (\( \mathcal{N}(0, \mathbf{Q}) \)) and covariance matrix \( \mathbf{Q} \), capturing uncertainties and randomness in the state transitions. The covariance matrix \( \mathbf{Q} \) quantifies the variability of the process noise \( \mathbf{w}_t \), ensuring the model accounts for inherent stochasticity.\\
Transformers rely on self-attention mechanisms to capture relationships across sequences ~\cite{vaswani2017attention}. While effective, their quadratic complexity with respect to sequence length leads to significant computational overhead and memory usage for long contexts ~\cite{taha2025logarithmicmemorynetworkslmns}. In contrast, SSMs are designed with a focus on efficiency and scalability, leveraging state transitions and compact latent representations to model temporal dependencies ~\cite{shakhadri2025sambaasrstateoftheartspeechrecognition}. This design enables SSMs to process sequences with linear computational complexity, making them more efficient for handling long-range dependencies in applications such as natural language processing, time-series forecasting, and audio processing ~\cite{liu2024masvspeakerverificationglobal}. SSMs have emerged as a significant development in sequence modeling, providing an effective approach to capture temporal dynamics in various tasks ~\cite{abreu2024qs5quantizedstatespace}. SSMs are grounded in their ability to model dynamic systems through latent state representations, which evolve over time according to predefined transition dynamics and are observed through noisy measurements ~\cite{wang2024statespacemodelnewgeneration}.Recent advancements in SSMs have led to the creation of hybrid architectures and enhanced methodologies that address key limitations in sequence modeling. For instance,Mamba ~\cite{gu2023mamba} (Linear-Time Sequence Modeling with Selective State Spaces)~\cite{gu2024mambalineartimesequencemodeling} introduces a mechanism where the parameters of the SSM are functions of the input, allowing selective propagation or forgetting of information. This selective capability improves the model's handling of discrete data and enables linear scaling with sequence length, outperforming traditional transformer models of similar size across diverse modalities such as language and audio. Another contribution is the Samba ~\cite{ren2024sambasimplehybridstate},  a hybrid architecture that fuses selective SSMs with sliding window attention, enabling efficient handling of unlimited context. By compressing sequences into recurrent hidden states while leveraging attention for precise recall, Samba demonstrates a remarkable ability to manage long sequences with minimal computational overhead. Similarly, models like Hymba ~\cite{dong2024hymbahybridheadarchitecturesmall} and Jamba ~\cite{lieber2024jamba} integrate SSMs with attention mechanisms within hybrid architectures to achieve a balance between efficient summarization of long-range contexts and high-resolution recall. These models exemplify how combining the strengths of SSMs and transformers ~\cite{vaswani2017attention} can yield state-of-the-art results, especially in tasks requiring both scalability and accuracy.
Another critical area where SSMs surpass transformers is in inference speed and memory efficiency. Attention-free SSM models like Falcon Mamba ~\cite{zuo2024} have proven to be significantly faster during inference while being lighter in memory usage for processing long sequences. For example, Falcon Mamba can match or exceed the performance of leading transformer-based models, such as Mistral 7B ~\cite{jiang2023mistral7b} and Llama ~\cite{touvron2023llamaopenefficientfoundation}, while requiring far fewer computational resources. This makes SSMs particularly suitable for deployment in resource-constrained environments or applications where real-time processing is critical. Additionally, transformers often struggle with handling highly structured or continuous data such as audio and sensor streams. In these domains, the state-transition mechanisms of SSMs enable them to natively and effectively capture temporal dynamics, offering a clear advantage over transformers, which often require additional architectural modifications to achieve comparable results.

\subsection{Indic Language Question Answering}
Indic languages, with their diverse scripts, complex grammar, and linguistic variations, present unique challenges for QA systems ~\cite{dani2024reviewmarathinaturallanguage}. Features such as free word order, inflectional patterns, and compound words complicate NLP tasks. While initiatives like the IndicQA dataset ~\cite{sabane2024} have advanced question answering in these languages, the scarcity of annotated datasets and neural models remains a significant challenge. Techniques such as transfer learning and domain adaptation have shown promise, with multilingual models fine-tuned on task-specific data achieving notable improvements in Hindi and Marathi QA tasks ~\cite{jin2022low}.

Adopting the SQuAD format ~\cite{rajpurkar2016squad}, a widely used benchmark in QA research, facilitates the creation of structured datasets for Indic languages. This format includes context passages, questions, and corresponding answers with exact character positions, ensuring consistency and compatibility across QA frameworks. Translating and adapting the SQuAD format for Indic languages has proven effective, as demonstrated by the performance improvements of the IndicQA dataset when aligned with this structure ~\cite{singh2024indicqabenchmarkmultilingual}.

To address data scarcity, data augmentation techniques such as back-translation have been used to generate synthetic SQuAD-style datasets ~\cite{Khan_2024}. Fine-tuning multilingual pre-trained models like XLM-R ~\cite{conneau2020unsupervised} and mT5 ~\cite{xue2021mt5} on such datasets has further improved performance in Indic QA tasks . The structured nature of the SQuAD format allows for robust evaluation of fact-based, reasoning-based, and opinion-based questions, making it particularly suitable for Indic languages ~\cite{UPADHYAY2024100088}. Aligning Indic-language datasets with this format enhances model performance and facilitates better comparisons across multilingual QA research.