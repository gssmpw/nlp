[
  {
    "index": 0,
    "papers": [
      {
        "key": "kalman1960new",
        "author": "Kalman, Rudolf E",
        "title": "A new approach to linear filtering and prediction problems"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "rabiner1986hmm",
        "author": "Rabiner, Lawrence R and Juang, B. H.",
        "title": "An introduction to hidden Markov models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bourdois2022ssm",
        "author": "Louis Bourdois",
        "title": "Get on the SSM Train"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "sarrof2024expressivecapacitystatespace",
        "author": "Yash Sarrof and Yana Veitsman and Michael Hahn",
        "title": "The Expressive Capacity of State Space Models: A Formal Language Perspective"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "goel2022s4",
        "author": "Goel, Karan and Rush, Alexander M. and Vaswani, Ashish",
        "title": "S4: Structured State Space for Sequence Modeling"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "patro2024heracleshybridssmtransformermodel",
        "author": "Badri N. Patro and Suhas Ranganath and Vinay P. Namboodiri and Vijay S. Agneeswaran",
        "title": "Heracles: A Hybrid SSM-Transformer Model for High-Resolution Image and Time-Series Analysis"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gu2022structured",
        "author": "Gu, Albert and others",
        "title": "Structured State Spaces: Combining Continuous-Time, Recurrent, and Convolutional Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "taha2025logarithmicmemorynetworkslmns",
        "author": "Mohamed A. Taha",
        "title": "Logarithmic Memory Networks (LMNs): Efficient Long-Range Sequence Modeling for Resource-Constrained Environments"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shakhadri2025sambaasrstateoftheartspeechrecognition",
        "author": "Syed Abdul Gaffar Shakhadri and Kruthika KR and Kartik Basavaraj Angadi",
        "title": "Samba-ASR: State-Of-The-Art Speech Recognition Leveraging Structured State-Space Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024masvspeakerverificationglobal",
        "author": "Yang Liu and Li Wan and Yiteng Huang and Ming Sun and Yangyang Shi and Florian Metze",
        "title": "MASV: Speaker Verification with Global and Local Context Mamba"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "abreu2024qs5quantizedstatespace",
        "author": "Steven Abreu and Jens E. Pedersen and Kade M. Heckel and Alessandro Pierro",
        "title": "Q-S5: Towards Quantized State Space Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wang2024statespacemodelnewgeneration",
        "author": "Xiao Wang and Shiao Wang and Yuhe Ding and Yuehang Li and Wentao Wu and Yao Rong and Weizhe Kong and Ju Huang and Shihao Li and Haoxiang Yang and Ziwen Wang and Bo Jiang and Chenglong Li and Yaowei Wang and Yonghong Tian and Jin Tang",
        "title": "State Space Model for New-Generation Network Alternative to Transformers: A Survey"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "gu2024mambalineartimesequencemodeling",
        "author": "Albert Gu and Tri Dao",
        "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ren2024sambasimplehybridstate",
        "author": "Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen",
        "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "dong2024hymbahybridheadarchitecturesmall",
        "author": "Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov",
        "title": "Hymba: A Hybrid-head Architecture for Small Language Models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "lieber2024jamba",
        "author": "Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others",
        "title": "Jamba: A hybrid transformer-mamba language model"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zuo2024",
        "author": "Jingwei Zuo and Maksim Velikanov and Dhia Eddine Rhaiem and Ilyas Chahed and Younes Belkada and Guillaume Kunsch and Hakim Hacid",
        "title": "Falcon Mamba: The First Competitive Attention-free 7B Language Model"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "jiang2023mistral7b",
        "author": "Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L\u00e9lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed",
        "title": "Mistral 7B"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "touvron2023llamaopenefficientfoundation",
        "author": "Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\u00e9e Lacroix and Baptiste Rozi\u00e8re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "dani2024reviewmarathinaturallanguage",
        "author": "Asang Dani and Shailesh R Sathe",
        "title": "A Review of the Marathi Natural Language Processing"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "sabane2024",
        "author": "Maithili Sabane and Onkar Litake and Aman Chadha",
        "title": "Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "jin2022low",
        "author": "Jin, Di and Jin, Zhijing and He, Junxian and Yang, Zichao and Li, Zhiting and Neubig, Graham",
        "title": "Low Resource Style Transfer via Domain Adaptive Meta Learning"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "rajpurkar2016squad",
        "author": "Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy",
        "title": "SQuAD: 100,000+ questions for machine comprehension of text"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "singh2024indicqabenchmarkmultilingual",
        "author": "Abhishek Kumar Singh and Rudra Murthy and Vishwajeet kumar and Jaydeep Sen and Ganesh Ramakrishnan",
        "title": "INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "Khan_2024",
        "author": "Khan, Mohammed and Mehta, Priyam and Sankar, Ananth and Kumaravelan, Umashankar and Doddapaneni, Sumanth and B, Suriyaprasaad and G, Varun and Jain, Sparsh and Kunchukuttan, Anoop and Kumar, Pratyush and Dabre, Raj and Khapra, Mitesh",
        "title": "IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning Datasets for Indian Languages"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "conneau2020unsupervised",
        "author": "Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin",
        "title": "Unsupervised cross-lingual representation learning at scale"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "xue2021mt5",
        "author": "Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Sumit and Raffel, Colin",
        "title": "mT5: A massively multilingual pre-trained text-to-text transformer"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "UPADHYAY2024100088",
        "author": "Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi",
        "title": "A comprehensive survey on answer generation methods using NLP"
      }
    ]
  }
]