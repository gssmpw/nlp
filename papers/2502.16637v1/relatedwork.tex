\section{Related Works}
\label{related_works}

% \subsection{Unsupervised Domain Adaptation}

% \subsection{Domain Adaptation on Temporal Data}

% \subsection{Identification of Generative Model}

\subsection{Unsupervised Domain Adaptation}
Unsupervised domain adaptation \cite{cai2019learning,kong2022partial,shui2021aggregating,stojanov2021domain,wen2019bayesian,zhang2013domain} aims to leverage the knowledge from a labeled source domain to an unlabeled target domain, by training a model to domain-invariant representations \cite{bousmalis2016domain}. Researchers have adopted different directions to tackle this problem. For example, Long et al. \cite{long2017deep} trained the model to minimize a similarity measure, i.e., maximum mean discrepancy (MMD), to guide learning domain-invariant representations. Tzeng et al. \cite{tzeng2014deep} used an adaptation layer and a domain confusion loss. Another direction is to assume the stability of conditional distributions across domains and extract the label-wise domain-invariant representation \cite{chen2019joint,chen2019progressive,kang2020contrastive}. For instance, Xie et al. \cite{xie2018learning} constrained the label-wise domain discrepancy, and Shu et al. \cite{shu2018dirt} considered that the decision boundaries should not cross high-density data regions, so they propose the virtual adversarial domain adaptation model. Another type of assumption is the target shift \cite{lipton2018detecting,roberts2022unsupervised,wen2020domain,zhang2013domain}, which assumes $p(Y|\rvu)$ varies across different domains. 

Besides, several methods address the domain adaptation problem from a causality perspective. Specifically, Zhang et al. \cite{zhang2013domain} proposed the target shift, conditional shift, and generalized target shift assumptions, based on the premise that $p(Y)$ and $P(X|Y)$ vary independently. Cai et al. \cite{cai2019learning} leveraged the data generation process to extract the disentangled semantic representations. Building on causality analysis, Petar et al. \cite{stojanov2021domain} highlighted the significance of incorporating domain-specific knowledge for learning domain-invariant representation. Recently, Kong et al. \cite{kong2022partial} addressed the multi-source domain adaptation by identifying the latent variables, and Li et al. \cite{li2024subspace} further relaxed the identifiability assumptions.


\subsection{Domain Adaptation on Temporal Data}
In recent years, domain adaptation for time series data has garnered significant attention. Da Costa et al. \cite{da2020remaining} is one of the earliest time series domain adaptation works, where authors adopted techniques originally designed for non-time series data to this domain, incorporating recurrent neural networks as feature extractors to capture domain-invariant representations. Purushotham et al. \cite{purushotham2022variational} further refined this approach by employing variational recurrent neural networks \cite{chung2015recurrent} to enhance extracting domain-invariant features. However, such methods face challenges in effectively capturing domain-invariant information due to the intricate dependencies between time points.
Subsequently, Cai et al. \cite{cai2021time} proposed the Sparse Associative Structure Alignment (SASA) method, based on the assumption that sparse associative structures among variables remain stable across domains. This method has been successfully applied to adaptive time series classification and regression tasks. Additionally, Jin et al. \cite{jin2022domain} introduced the Domain Adaptation Forecaster (DAF), which leverages statistical relationships from relevant source domains to improve performance in target domains. Li et al. \cite{li2023transferable} hypothesized that causal structures are consistent across domains, leading to the development of the Granger Causality Alignment (GCA) approach. This method uncovers underlying causal structures while modeling shifts in conditional distributions across domains.

Our work also relates to domain adaptation for video data, which could be considered a form of high-dimensional time series data. Video data offers a robust benchmark for evaluating the performance of our method. Unsupervised domain adaptation for video data has recently attracted substantial interest. For instance, Chen et al. \cite{chen2019temporal} proposed a Temporal Attentive Adversarial Adaptation Network (TA3N), which integrates a temporal relation module to enhance temporal alignment. Choi et al. \cite{choi2020shuffle} proposed the SAVA method, which leverages self-supervised clip order prediction and attention-based alignment across clips. In addition, Pan et al. proposed the Temporal Co-attention Network (TCoN) \cite{pan2020adversarial}, which employs a cross-domain co-attention mechanism to identify key frames shared across domains, thereby improving alignment. 

Luo et al. \cite{luo2020adversarial} focused on domain-agnostic classification using a bipartite graph network topology to model cross-domain correlations. Rather than relying on adversarial learning, Sahoo et al. \cite{sahoo2021contrast} developed CoMix, an end-to-end temporal contrastive learning framework that employs background mixing and target pseudo-labels. More recently, Chen et al. \cite{chen2022multi} introduced multiple domain discriminators for multi-level temporal attentive features to achieve superior alignment, while Turrisi et al.~\cite{da2022dual} utilized a dual-headed deep architecture that combines cross-entropy and contrastive losses to learn a more robust target classifier. Additionally, Wei et al.~\cite{wei2023unsupervised} employed contrastive and adversarial learning to disentangle dynamic and static information in videos, leveraging shared dynamic information across domains for more accurate prediction.


\subsection{Granger Causal Discovery}
Several works have been raised to infer causal structures for time series data based on Granger causality \cite{diks2006new,granger1969investigating,marcinkevivcs2021interpretable,lowe2020amortized,lin2024root,gong2023causal}. Previously, several researchers used the vector autoregressive (\textbf{VAR}) model \cite{lozano2009grouped,hamilton2020time} with the sparsity constraint like Lasso or Group Lasso \cite{yuan2006model,tibshirani1996regression} to learn Granger causality. Recently, several works have inferred Granger causality with the aid of neural networks. For instance, Tank et al. \cite{tank2021neural} developed a neural network-based autoregressive model with sparsity penalties applied to network weights. Inspired by the interpretability of self-explaining neural networks, Marcinkevivcs et al. \cite{marcinkevivcs2021interpretable} introduced a generalized vector autoregression model to learn Granger causality. Li et al. \cite{li2023transferable} considered the Granger causal structure as latent variables. Cheng et al. \cite{cheng2023cuts,cheng2024cuts+} proposed a neural Granger causal discovery algorithm to discover Granger causality from irregular time series data. Lin et al. \cite{lin2024root} used a neural architecture with contrastive learning to learn Granger causality. However, these methods usually consider the Granger causal structures over low-dimension observed time series data, which can hardly address the time series data with high dimension or latent causal relationships. To address this limitation, we identify the latent variables and infer the latent Granger causal structures behind high-dimensional time series data.

\subsection{Identifiability of Generative Model}
To achieve causal representation \cite{rajendran2024learning,mansouri2023object,wendong2024causal} for time series data, many researchers leverage Independent Component Analysis (ICA) to recover latent variables with identifiability guarantees \cite{yao2023multi,scholkopf2021toward,Liu2023CausalTriplet,gresele2020incomplete}. Conventional methods typically assume a linear mixing function from the latent variables to the observed variables \cite{comon1994independent,hyvarinen2013independent,lee1998independent,zhang2007kernel}. To relax the linear assumption, researchers achieve the identifiability of latent variables in nonlinear ICA by using different types of assumptions, such as auxiliary variables or sparse generation processes \cite{zheng2022identifiability,hyvarinen1999nonlinear,hyvarinen2023identifiability,khemakhem2020ice,li2023identifying}. Aapo et al. \cite{hyvarinen2017nonlinear} first achieved identifiability for methods employing auxiliary variables by assuming the latent sources follow an exponential family distribution and introducing auxiliary variables, such as domain indices, time indices, and class labels. To further relax the exponential family assumption, Zhang et al. \cite{kong2022partial,xie2022multi, kong2023identification,yan2023counterfactual, xie2022multi} proposed component-wise identifiability results for nonlinear ICA, requiring $2n+1$ auxiliary variables for $n$ latent variables. To seek identifiability in an unsupervised manner, researchers employed the assumption of structural sparsity to achieve identifiability \cite{ng2024identifiability,lachapelle2022disentanglement,zheng2022identifiability, xu2024sparsity}. Recently, Zhang et al. \cite{zhang2024causal} achieved identifiability under distribution shift by leveraging the sparse structures of latent variables. Li et al. \cite{li2024identification} further employed sparse causal influence to achieve identifiability for time series data with instantaneous dependency.


% \clearpage