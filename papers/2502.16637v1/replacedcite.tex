\section{Related Works}
\label{related_works}

% \subsection{Unsupervised Domain Adaptation}

% \subsection{Domain Adaptation on Temporal Data}

% \subsection{Identification of Generative Model}

\subsection{Unsupervised Domain Adaptation}
Unsupervised domain adaptation ____ aims to leverage the knowledge from a labeled source domain to an unlabeled target domain, by training a model to domain-invariant representations ____. Researchers have adopted different directions to tackle this problem. For example, Long et al. ____ trained the model to minimize a similarity measure, i.e., maximum mean discrepancy (MMD), to guide learning domain-invariant representations. Tzeng et al. ____ used an adaptation layer and a domain confusion loss. Another direction is to assume the stability of conditional distributions across domains and extract the label-wise domain-invariant representation ____. For instance, Xie et al. ____ constrained the label-wise domain discrepancy, and Shu et al. ____ considered that the decision boundaries should not cross high-density data regions, so they propose the virtual adversarial domain adaptation model. Another type of assumption is the target shift ____, which assumes $p(Y|\rvu)$ varies across different domains. 

Besides, several methods address the domain adaptation problem from a causality perspective. Specifically, Zhang et al. ____ proposed the target shift, conditional shift, and generalized target shift assumptions, based on the premise that $p(Y)$ and $P(X|Y)$ vary independently. Cai et al. ____ leveraged the data generation process to extract the disentangled semantic representations. Building on causality analysis, Petar et al. ____ highlighted the significance of incorporating domain-specific knowledge for learning domain-invariant representation. Recently, Kong et al. ____ addressed the multi-source domain adaptation by identifying the latent variables, and Li et al. ____ further relaxed the identifiability assumptions.


\subsection{Domain Adaptation on Temporal Data}
In recent years, domain adaptation for time series data has garnered significant attention. Da Costa et al. ____ is one of the earliest time series domain adaptation works, where authors adopted techniques originally designed for non-time series data to this domain, incorporating recurrent neural networks as feature extractors to capture domain-invariant representations. Purushotham et al. ____ further refined this approach by employing variational recurrent neural networks ____ to enhance extracting domain-invariant features. However, such methods face challenges in effectively capturing domain-invariant information due to the intricate dependencies between time points.
Subsequently, Cai et al. ____ proposed the Sparse Associative Structure Alignment (SASA) method, based on the assumption that sparse associative structures among variables remain stable across domains. This method has been successfully applied to adaptive time series classification and regression tasks. Additionally, Jin et al. ____ introduced the Domain Adaptation Forecaster (DAF), which leverages statistical relationships from relevant source domains to improve performance in target domains. Li et al. ____ hypothesized that causal structures are consistent across domains, leading to the development of the Granger Causality Alignment (GCA) approach. This method uncovers underlying causal structures while modeling shifts in conditional distributions across domains.

Our work also relates to domain adaptation for video data, which could be considered a form of high-dimensional time series data. Video data offers a robust benchmark for evaluating the performance of our method. Unsupervised domain adaptation for video data has recently attracted substantial interest. For instance, Chen et al. ____ proposed a Temporal Attentive Adversarial Adaptation Network (TA3N), which integrates a temporal relation module to enhance temporal alignment. Choi et al. ____ proposed the SAVA method, which leverages self-supervised clip order prediction and attention-based alignment across clips. In addition, Pan et al. proposed the Temporal Co-attention Network (TCoN) ____, which employs a cross-domain co-attention mechanism to identify key frames shared across domains, thereby improving alignment. 

Luo et al. ____ focused on domain-agnostic classification using a bipartite graph network topology to model cross-domain correlations. Rather than relying on adversarial learning, Sahoo et al. ____ developed CoMix, an end-to-end temporal contrastive learning framework that employs background mixing and target pseudo-labels. More recently, Chen et al. ____ introduced multiple domain discriminators for multi-level temporal attentive features to achieve superior alignment, while Turrisi et al.____ utilized a dual-headed deep architecture that combines cross-entropy and contrastive losses to learn a more robust target classifier. Additionally, Wei et al.____ employed contrastive and adversarial learning to disentangle dynamic and static information in videos, leveraging shared dynamic information across domains for more accurate prediction.


\subsection{Granger Causal Discovery}
Several works have been raised to infer causal structures for time series data based on Granger causality ____. Previously, several researchers used the vector autoregressive (\textbf{VAR}) model ____ with the sparsity constraint like Lasso or Group Lasso ____ to learn Granger causality. Recently, several works have inferred Granger causality with the aid of neural networks. For instance, Tank et al. ____ developed a neural network-based autoregressive model with sparsity penalties applied to network weights. Inspired by the interpretability of self-explaining neural networks, Marcinkevivcs et al. ____ introduced a generalized vector autoregression model to learn Granger causality. Li et al. ____ considered the Granger causal structure as latent variables. Cheng et al. ____ proposed a neural Granger causal discovery algorithm to discover Granger causality from irregular time series data. Lin et al. ____ used a neural architecture with contrastive learning to learn Granger causality. However, these methods usually consider the Granger causal structures over low-dimension observed time series data, which can hardly address the time series data with high dimension or latent causal relationships. To address this limitation, we identify the latent variables and infer the latent Granger causal structures behind high-dimensional time series data.

\subsection{Identifiability of Generative Model}
To achieve causal representation ____ for time series data, many researchers leverage Independent Component Analysis (ICA) to recover latent variables with identifiability guarantees ____. Conventional methods typically assume a linear mixing function from the latent variables to the observed variables ____. To relax the linear assumption, researchers achieve the identifiability of latent variables in nonlinear ICA by using different types of assumptions, such as auxiliary variables or sparse generation processes ____. Aapo et al. ____ first achieved identifiability for methods employing auxiliary variables by assuming the latent sources follow an exponential family distribution and introducing auxiliary variables, such as domain indices, time indices, and class labels. To further relax the exponential family assumption, Zhang et al. ____ proposed component-wise identifiability results for nonlinear ICA, requiring $2n+1$ auxiliary variables for $n$ latent variables. To seek identifiability in an unsupervised manner, researchers employed the assumption of structural sparsity to achieve identifiability ____. Recently, Zhang et al. ____ achieved identifiability under distribution shift by leveraging the sparse structures of latent variables. Li et al. ____ further employed sparse causal influence to achieve identifiability for time series data with instantaneous dependency.


% \clearpage