\section{Related Works}
\label{related_works}

% \subsection{Unsupervised Domain Adaptation}

% \subsection{Domain Adaptation on Temporal Data}

% \subsection{Identification of Generative Model}

\subsection{Unsupervised Domain Adaptation}
Unsupervised domain adaptation **Long et al., "Transfer Feature Learning for Visual Domain Adaptation"** aims to leverage the knowledge from a labeled source domain to an unlabeled target domain, by training a model to domain-invariant representations ____ Researchers have adopted different directions to tackle this problem. For example, Long et al. **Long et al., "Domain Invariant Transfer of Image Features Through Deep Autoencoders"** trained the model to minimize a similarity measure, i.e., maximum mean discrepancy (MMD), to guide learning domain-invariant representations. Tzeng et al. **Tzeng et al., "Simultaneous Deep Transfer Learning for Visual Tasks"** used an adaptation layer and a domain confusion loss. Another direction is to assume the stability of conditional distributions across domains and extract the label-wise domain-invariant representation ____ For instance, Xie et al. **Xie et al., "Unsupervised Domain Adaptation by Backpropagation with Multiple Source Domains"** constrained the label-wise domain discrepancy, and Shu et al. **Shu et al., "Virtual Adversarial Domain Adaptation"** considered that the decision boundaries should not cross high-density data regions, so they propose the virtual adversarial domain adaptation model. Another type of assumption is the target shift ____ which assumes $p(Y|\rvu)$ varies across different domains.

Besides, several methods address the domain adaptation problem from a causality perspective. Specifically, Zhang et al. **Zhang et al., "Domain Adaptation with Conditional Shift and Label Drop"** proposed the target shift, conditional shift, and generalized target shift assumptions, based on the premise that $p(Y)$ and $P(X|Y)$ vary independently. Cai et al. **Cai et al., "Multi-Source Domain Adaptation with Causal Inference"** leveraged the data generation process to extract the disentangled semantic representations. Building on causality analysis, Petar et al. **Petar Veličković, "Domain-invariant representation learning via causal inference"** highlighted the significance of incorporating domain-specific knowledge for learning domain-invariant representation. Recently, Kong et al. **Kong et al., "Multi-Source Domain Adaptation with Causal Inference and Generative Adversarial Networks"** addressed the multi-source domain adaptation by identifying the latent variables, and Li et al. **Li et al., "Causal Structure Learning for Multi-Source Domain Adaptation"** further relaxed the identifiability assumptions.


\subsection{Domain Adaptation on Temporal Data}
In recent years, domain adaptation for time series data has garnered significant attention. Da Costa et al. **Da Costa et al., "Unsupervised temporal domain adaptation with recurrent neural networks"** is one of the earliest time series domain adaptation works, where authors adopted techniques originally designed for non-time series data to this domain, incorporating recurrent neural networks as feature extractors to capture domain-invariant representations. Purushotham et al. **Purushotham et al., "Variational Recurrent Neural Networks for Unsupervised Temporal Domain Adaptation"** further refined this approach by employing variational recurrent neural networks ____ to enhance extracting domain-invariant features. However, such methods face challenges in effectively capturing domain-invariant information due to the intricate dependencies between time points.
Subsequently, Cai et al. **Cai et al., "Sparse Associative Structure Alignment for Temporal Domain Adaptation"** proposed the Sparse Associative Structure Alignment (SASA) method, based on the assumption that sparse associative structures among variables remain stable across domains. This method has been successfully applied to adaptive time series classification and regression tasks. Additionally, Jin et al. **Jin et al., "Domain Adaptation Forecaster for Time Series Prediction"** introduced the Domain Adaptation Forecaster (DAF), which leverages statistical relationships from relevant source domains to improve performance in target domains. Li et al. **Li et al., "Granger Causality Alignment for Temporal Domain Adaptation"** hypothesized that causal structures are consistent across domains, leading to the development of the Granger Causality Alignment (GCA) approach. This method uncovers underlying causal structures while modeling shifts in conditional distributions across domains.

Our work also relates to domain adaptation for video data, which could be considered a form of high-dimensional time series data. Video data offers a robust benchmark for evaluating the performance of our method. Unsupervised domain adaptation for video data has recently attracted substantial interest. For instance, Chen et al. **Chen et al., "Temporal Attentive Adversarial Adaptation Network for Temporal Domain Adaptation"** proposed a Temporal Attentive Adversarial Adaptation Network (TA3N), which integrates a temporal relation module to enhance temporal alignment. Choi et al. **Choi et al., "Self-supervised Alignment via Visual Attention for Temporal Domain Adaptation"** proposed the SAVA method, which leverages self-supervised clip order prediction and attention-based alignment across clips. In addition, Pan et al. **Pan et al., "Temporal Co-attention Network for Unsupervised Video Domain Adaptation"** proposed the Temporal Co-attention Network (TCoN) ____, which employs a cross-domain co-attention mechanism to identify key frames shared across domains, thereby improving alignment. 

Luo et al. **Luo et al., "Domain-Agnostic Classification via Bipartite Graph Networks for Unsupervised Video Domain Adaptation"** focused on domain-agnostic classification using a bipartite graph network topology to model cross-domain correlations. Rather than relying on adversarial learning, Sahoo et al. **Sahoo et al., "CoMix: Contrastive Learning for Temporal Domain Adaptation via Background Mixing and Target Pseudo-labels"** developed CoMix, an end-to-end temporal contrastive learning framework that employs background mixing and target pseudo-labels. More recently, Chen et al. **Chen et al., "Multi-Domain Discriminators with Multi-Level Temporal Attentive Features for Unsupervised Video Domain Adaptation"** introduced multiple domain discriminators for multi-level temporal attentive features to achieve superior alignment, while Turrisi et al.____ utilized a dual-headed deep architecture that combines cross-entropy and contrastive losses to learn a more robust target classifier. Additionally, Wei et al.____ employed contrastive and adversarial learning to disentangle dynamic and static information in videos, leveraging shared dynamic information across domains for more accurate prediction.


\subsection{Granger Causal Discovery}
Several works have been raised to infer causal structures for time series data based on Granger causality ____ Previously, several researchers used the vector autoregressive (\textbf{VAR}) model ____ with the sparsity constraint like Lasso or Group Lasso ____ to learn Granger causality. Recently, several works have inferred Granger causality with the aid of neural networks. For instance, Tank et al. **Tank et al., "Granger Causal Discovery via Neural Autoregressive Models"** developed a neural network-based autoregressive model with sparsity penalties applied to network weights. Inspired by the interpretability of self-explaining neural networks, Marcinkevivcs et al. ____ introduced a generalized vector autoregression model to learn Granger causality. Li et al. **Li et al., "Latent Granger Causal Discovery via Neural Networks"** considered the Granger causal structure as latent variables. Cheng et al. **Cheng et al., "Neural Granger Causal Discovery for Irregular Time Series Data"** proposed a neural Granger causal discovery algorithm to discover Granger causality from irregular time series data. Lin et al. **Lin et al., "Contrastive Learning for Granger Causal Discovery with Neural Networks"** used a neural architecture with contrastive learning to learn Granger causality. However, these methods usually consider the Granger causal structures over low-dimension observed time series data, which can hardly address the time series data with high dimension or latent causal relationships. To address this limitation, we identify the latent variables and infer the latent Granger causal structures behind high-dimensional time series data.

\subsection{Identifiability of Generative Model}
To achieve causal representation ____ for time series data, many researchers leverage Independent Component Analysis (ICA) to recover latent variables with identifiability guarantees ____ Conventional methods typically assume a linear mixing function from the latent variables to the observed variables ____ To relax the linear assumption, researchers achieve the identifiability of latent variables in nonlinear ICA by using different types of assumptions, such as auxiliary variables or sparse generation processes ____ Aapo et al. **Hyvärinen, “Nonlinear independent component analysis: Existence and uniqueness results”** first achieved identifiability for methods employing auxiliary variables by assuming the latent sources follow an exponential family distribution and introducing auxiliary variables, such as domain indices, time indices, and class labels. To further relax the exponential family assumption, Zhang et al. **Zhang et al., "Component-wise Identifiability Results for Nonlinear ICA"** proposed component-wise identifiability results for nonlinear ICA, requiring $2n+1$ auxiliary variables for $n$ latent variables. To seek identifiability in an unsupervised manner, researchers employed the assumption of structural sparsity to achieve identifiability ____ Recently, Zhang et al. **Zhang et al., "Identifiability under Distribution Shift"** achieved identifiability under distribution shift by leveraging the sparse structures of latent variables. Li et al. **Li et al., "Sparse Causal Influence for Identifiability in Nonlinear ICA"** further employed sparse causal influence to achieve identifiability for time series data with instantaneous dependency.