


\onecolumn 
% \appendix
% { 
% \tableofcontents
% }
% \section{Proof}
% \section{Proof of Theory}

% 虽然这个马尔可夫网络中，包含其他时刻的信息，但是在公式11到公式12的推导过程中，是有错误的，而且在正确的情况下，假设2是不是，zt-2,m有一个维度，使得假设线性独立就行了
% A2假设，中
% \section{Component-wise Identifications}\label{app:the1}
% \begin{theorem}
%     Let the observations be sampled from the data generation process in Figure 1 and Equation (1) and (2). We let $\mathcal{M}_{\rvc}$ be the Markov networks over $\rvc_t=\{\rvz_{t-1},\rvz_t\}$. Then we further make the following assumptions:
%     \begin{itemize}[leftmargin=*]
%         \item \underline{A1 (Smooth and Positive Density):} The probability function of the latent variables $\rvz_t$ is smooth and positive, i.e., $p_{\rvz}$ is smooth and $p_{\rvz}>0$ over $\mathbb{R}^n$.
%         \item \underline{A2 (Sufficient Changes):} Let $\rvc_t=\{\rvz_{t-1},\rvz_t\} \in \mathbb{R}^{2n}$ be the latent variables in the adjacent two timestamps and $c_{t,i},c_{t,j}$ be the $i$-th and $j$-th latent variables of $\rvc_t$. There exist $2(2n+|\mathcal{M}_{\rvc}|)+1$ different values of $\rvz_{t-2,m}$, where $m=1,2,\cdots,n$, such that the vectors $w(i,j,m)$ are linearly independent, and $w(i,j,m)$ is defined as follows 
%         \begin{equation}
%         % \small
%         \begin{split}
%             w(i,j,m)=&\Big(\frac{\partial^3 \log p(\rvc_t|\rvz_{t-2})}{\partial c_{t,1}^2\partial z_{t-2,m}},\cdots,\frac{\partial^3 \log p(\rvc_t|\rvz_{t-2})}{\partial c_n^2\partial z_{t-2,m}},\frac{\partial^2 \log p(c_t|\rvz_{t-2})}{\partial c_{t,1}\partial z_{t-2,m}}, \\&\cdots,\frac{\partial^2 \log p(c_t|\rvz_{t-2})}{\partial c_n\partial z_{t-2,m}}\Big)\oplus\Big(\frac{\partial^3 \log p(\rvc_t|\rvz_{t-2})}{\partial c_{t,i}\partial c_{t,j}\partial z_{t-2,m}}\Big)_{(i,j)\in \mathcal{E}(\mathcal{M}_{\rvc})}
%         \end{split}
%         \end{equation}
%     \end{itemize}
%     Suppose that we learn $(\hat{g},\hat{f},p_{\rvz})$ to match the data generation process shown in Equation (1) and (2) as well as Figure 1. For any two $k,l\in [2n]$ of estimated latent variables $\hat{c}_{t,k}$ and $\hat{c}_{t,l}$ in variable set $\hat{\rvc}=\{\hat{\rvz}_{t-1},\hat{\rvz}_t\}$ that are \textbf{not adjacent in the Markov networks} $\mathcal{M}_{\hat{\rvc}}$ over $\hat{\rvc}_{t}$, we have the following two statements:
    
%     \textbf{(a)} Each ground-truth latent variable $c_{t,i}$ in $\rvc_t=\{\rvz_{t-1},\rvz_{t}\}$ is a function of at most one of $\hat{c}_{t,k}$ and $\hat{c}_{t,l}$.

%     \textbf{(b)} For each pair of ground-truth latent variables $c_{t,i}$ and $c_{t,j}$ that are adjacent in the Markov network $\mathcal{M}_{\rvc}$ over $\rvc_{t}$, at most one of them is a function of $\hat{c}_{t,k}$ or $\hat{c}_{t,l}$.
% \end{theorem}

\appendix
\begin{lemma} \label{lemma: 1}
Suppose there exists invertible function $\hat{\mathbf{g}}$ that maps $\mathbf{x}_t$ to $\hat{\mathbf{z}}_t$, i.e.
\begin{equation}
\label{eq: hatz_xt_invertible_function}
    \hat{\mathbf{z}}_t=\hat{\mathbf{g}}_(\mathbf{x}_t)
\end{equation}
such that the components of $\hat{\mathbf{z}}_t$ are mutually independent conditional on $\hat{\mathbf{z}}_{t-1}$ .Let
\begin{equation}
\label{eq: second_third_derivatives}
 % \small
 \begin{split}
    \mathbf{v}_{t,k} =
    \Big(\frac{\partial^{2}\log p(z_{t,k}|\mathbf{z}_{t-1}) }{\partial z_{t,k}\partial z_{t-1,1}},\frac{\partial^{2}\log p(z_{t,k}|\mathbf{z}_{t-1})}{\partial z_{t,k}\partial z_{t-1,2}},...,
        \frac{\partial^{2}\log p(z_{t,k}|\mathbf{z}_{t-1})}{\partial z_{t,k}\partial z_{t-1,n}}\Big)^{\mathsf{T}},\\
    \mathring{\mathbf{v}}_{t,k}=
\Big(\frac{\partial^{3}\log p(z_{t,k}|\mathbf{z}_{t-1})}{\partial z_{t,k}^{2}\partial z_{t-1,1}},\frac{\partial^{3}\log p(z_{t,k}|\mathbf{z}_{t-1})}{\partial z_{t,k}^{2}\partial z_{t-1,2}},...,
    \frac{\partial^{3}\log p(z_{t,k}|\mathbf{z}_{t-1})}{\partial z_{t,k}^{2}\partial z_{t-1,n}}\Big)^{\mathsf{T}}.
    \end{split}
\end{equation}
If for each value of $\mathbf{z}_t,\mathbf{v}_{t,1},\mathring{\mathbf{v}}_{t,1},\mathbf{v}_{t,2},\mathring{\mathbf{v}}_{t,2},...,,\mathbf{v}_{t,n},\mathring{\mathbf{v}}_{t,n}$, as 2n vector function in $z_{t-1,1},z_{t-1,2},...,z_{t-1,n}$, are linearly independent, then $\mathbf{z}_t$ must be an invertible, component-wise transformation of a permuted version of $\hat{\mathbf{z}}_t$.
\end{lemma}

\begin{proof}
    Combining Equation (\ref{eq: hatz_xt_invertible_function}) and Equation (\ref{eq: second_third_derivatives}) gives $\mathbf{z}_t=\mathbf{g}^{-1}(\hat{\mathbf{g}}^{-1}(\hat{\mathbf{z}}_t))$, where $\mathbf{h}=\mathbf{g}^{-1}\circ \hat{\mathbf{g}}^{-1}$.
 Since both $\hat{\mathbf{g}}$ and $\mathbf{g}$ are invertible, $\mathbf{h}$ is invertible. Let $\mathbf{H}_t$ be the Jacobian matrix of the transformation $h(\hat{\mathbf{z}}_t)$, and denote by $\mathbf{H}_{tki}$ its $(k,i)$th entry.

 First, it is straightforward to see that if the components of $\hat{\mathbf{z}}_t$ are mutually independent conditional on $\hat{\mathbf{z}}_{t-1}$ , then for any $i\neq j$, $\hat{z}_{t,i}$ and $\hat{z}_{t,k}$ are conditionally independent given $\hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t \setminus \{\hat{z}_{t,i},\hat{z}_{t,j}\}) $ .Mutual independence of the components of $\hat{\mathbf{z}}_t$ conditional of $\hat{\mathbf{z}}_{t-1}$ implies that $\hat{z}_{t,i}$ is independent from $\hat{\mathbf{z}}_t \setminus \{\hat{z}_{t,i}, \hat{z}_{t,j} \}$ conditional on $\hat{\mathbf{z}}_{t-1}$ , i.e.,
 $$p(\hat{z}_{t,i} \,|\, \hat{\mathbf{z}}_{t-1}) = p(\hat{z}_{t,i} \,|\, \hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t \setminus \{\hat{z}_{t,i},\hat{z}_{t,j}\})).$$
At the same time, it also implies $\hat{z}_{t,i}$ is independent from $\hat{\mathbf{z}}_{t} \setminus \{\hat{z}_{t,i}\}$ conditional on $\hat{\mathbf{z}}_{t-1}$, i.e.,
    $$p(\hat{z}_{t,i} \,|\, \hat{\mathbf{z}}_{t-1}) = p(\hat{\mathbf{z}}_{t,i} \,|\, \hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t \setminus \{\hat{z}_{t,i}\})).$$
Combining the above two equations gives $ p(\hat{z}_{t,i} \,|\, \hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t \setminus  \{\hat{z}_{t,i}\}))=
p(\hat{z}_{t,i} \,|\, \hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t  \setminus  \{\hat{z}_{t,i},\hat{z}_{t,j}\}))$, i.e., for $i\neq j$ , $\hat{z}_{t,i}$ and $\hat{z}_{t,j}$ are conditionally independent given $\hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t\setminus \{\hat{z}_{t,i}, \hat{z}_{t,j}\})$.
We then make use of the fact that if $\hat{z}_{t,i}$ and $\hat{z}_{t,j}$ are conditionally independent given $\hat{\mathbf{z}}_{t-1} \cup (\hat{\mathbf{z}}_t\setminus \{\hat{z}_{t,i}, \hat{z}_{t,j}\})$ , then
    $$\frac{\partial^2\log p(\hat{\mathbf{z}}_t,\hat{\mathbf{z}}_{t-1})} {\partial \hat{z}_{t,i} \partial \hat{z}_{t,j}} = 0,$$
assuming the cross second-order derivative exists . Since $p(\hat{\mathbf{z}}_t,\hat{\mathbf{z}}_{t-1})=p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1})p(\hat{\mathbf{z}}_{t-1})$ while $p(\hat{\mathbf{z}}_{t-1})$ does not involve $\hat{z}_{t,i}$ or $\hat{z}_{t,j}$ , the above equality is equivalent to
\begin{equation} \label{Eq:iszero}
    \frac{\partial^2\log p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1})} {\partial \hat{z}_{t,i} \partial \hat{z}_{t,j}} = 0
\end{equation}
The Jacobian matrix of the mapping from $(\mathbf{x}_{t-1}, \hat{\mathbf{z}}_t)$ to $(\mathbf{x}_{t-1}, \mathbf{z}_t)$ is $\begin{bmatrix}\mathbf{I} & \mathbf{0} \\ * & \mathbf{H}_t \end{bmatrix}$, where $*$ stands for a matrix, and the (absolute value of the) determinant of this Jacobian matrix is $|\mathbf{H}_t|$. Therefore $p(\hat{\mathbf{z}}_t, \mathbf{x}_{t-1}) = p({\mathbf{z}}_t, \mathbf{x}_{t-1})\cdot |\mathbf{H}_t|$. Dividing both sides of this equation by $p(\mathbf{x}_{t-1})$ gives 
\begin{equation} \label{Eq:J_trans}
 p(\hat{\mathbf{z}}_t \,|\, \mathbf{x}_{t-1}) = p({\mathbf{z}}_t \,|\, \mathbf{x}_{t-1}) \cdot |\mathbf{H}_t|. 
 \end{equation}
  Since $p({\mathbf{z}}_t \,|\, {\mathbf{z}}_{t-1}) = p({\mathbf{z}}_t \,|\, \mathbf{g}({\mathbf{z}}_{t-1})) = p({\mathbf{z}}_t \,|\, {\mathbf{x}}_{t-1})$ and similarly   $p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1}) = p(\hat{\mathbf{z}}_t \,|\, {\mathbf{x}}_{t-1})$, Equation (\ref{Eq:J_trans}) tells us
 \begin{equation}
     \log p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1}) = \log p({\mathbf{z}}_t \,|\, {\mathbf{z}}_{t-1}) + \log |\mathbf{H}_t| = \sum_{k=1}^n \log p(z_{t,k}|\mathbf{z}_{t-1})  + \log |\mathbf{H}_t|.
 \end{equation}
  Its partial derivative w.r.t. $\hat{z}_{t,i}$ is
  \begin{flalign} \nonumber
  \frac{\partial \log p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1})}{\partial \hat{z}_{t,i}} &=  \sum_{k=1}^n \frac{\partial \log p(z_{t,k}|\mathbf{z}_{t-1})  }{\partial z_{t,k}} \cdot \frac{\partial z_{t,k}}{\partial \hat{z}_{t,i}} - \frac{\partial \log |\mathbf{H}_t|}{\partial \hat{z}_{t,i}} \\ \nonumber
  &= \sum_{k=1}^n \frac{\partial \log p(z_{t,k}|\mathbf{z}_{t-1}) }{\partial z_{t,k}} \cdot \mathbf{H}_{tki} - \frac{\partial \log |\mathbf{H}_t|}{\partial \hat{z}_{t,i}}.
 \end{flalign}
   Its second-order cross derivative is
   \begin{flalign} \label{Eq:cross}
  \frac{\partial^2 \log p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1})}{\partial \hat{z}_{t,i} \partial \hat{z}_{t,j}}
  &= \sum_{k=1}^n \Big( \frac{\partial^2 \log p(z_{t,k}|\mathbf{z}_{t-1}) }{\partial z_{t,k}^2 } \cdot \mathbf{H}_{tki}\mathbf{H}_{tkj} + \frac{\partial \log p(z_{t,k}|\mathbf{z}_{t-1}) }{\partial z_{t,k}} \cdot \frac{\partial \mathbf{H}_{tki}}{\partial \hat{z}_{t,j}} \Big)- \frac{\partial^2 \log |\mathbf{H}_t|}{\partial \hat{z}_{t,i} \partial \hat{z}_{t,j}}.
 \end{flalign}
The above quantity is always 0 according to Equation (\ref{Eq:iszero}). Therefore, for each $l=1,2,...,n$ and each value $z_{t-1,l}$,  its partial derivative w.r.t.
 $z_{t-1,l}$ is always 0. That is,
  \begin{flalign}\label{eq:lind-ap}
  \frac{\partial^3 \log p(\hat{\mathbf{z}}_t \,|\, \hat{\mathbf{z}}_{t-1})}{\partial \hat{z}_{t,i} \partial \hat{z}_{t,j} \partial z_{t-1,l}}
  &= \sum_{k=1}^n \Big( \frac{\partial^3 \log p(z_{t,k}|\mathbf{z}_{t-1}) }{\partial z_{t,k}^2 \partial z_{t-1,l}} \cdot \mathbf{H}_{tki}\mathbf{H}_{tkj} + \frac{ \partial^2 \log p(z_{t,k}|\mathbf{z}_{t-1}) }{\partial z_{t,k} \partial z_{t-1,l}}  \cdot \frac{\partial \mathbf{H}_{tki}}{\partial \hat{z}_{t,j} } \Big) \equiv 0,
 \end{flalign}
  where we have made use of the fact that entries of $\mathbf{H}_t$ do not depend on $z_{t-1,l}$. 

If for any value of  $\mathbf{z}_t,\mathbf{v}_{t,1},\mathring{\mathbf{v}}_{t,1},\mathbf{v}_{t,2},\mathring{\mathbf{v}}_{t,2},...,,\mathbf{v}_{t,n},\mathring{\mathbf{v}}_{t,n}$ are linearly independent, to make the above equation hold true, one has to set $\mathbf{H}_{tki}\mathbf{H}_{tkj} = 0$ or $i\neq j$. That is, in each row of $\mathbf{H}_t$ there is only one non-zero entry. Since $h$ is invertible, then $\mathbf{z}_{t}$ must be an invertible, component-wise transformation of a permuted version of $\hat{\mathbf{z}}_t$.
\end{proof}


%这里的W是不是给的有问题，c1到c2n?
\section{Extension to Multiple Lags and Sequence Lengths}
\label{app: Extension to Multiple Lags and Sequence Lengths}
 For the sake of simplicity, we consider only one special case with $\tau=1$ and $L=2$ in Lemma \ref{lemma: 1}. Our identifiability proof can actually be applied for arbitrary lags directly. For instance, in the stationary case in Equation (\ref{eq: second_third_derivatives}), one can simply let
 \begin{equation}
\label{eq: any_tau_derivatives}
 % \small
 \begin{split}
    \mathbf{v}_{t,k} =
    \Big(\frac{\partial^{2}\log p(z_{t,k}|\mathbf{z}_{Hx}) }{\partial z_{t,k}\partial z_{t-\tau,1}},\frac{\partial^{2}\log p(z_{t,k}|\mathbf{z}_{Hx})}{\partial z_{t,k}\partial z_{t-\tau,2}},...,
        \frac{\partial^{2}\log p(z_{t,k}|\mathbf{z}_{Hx})}{\partial z_{t,k}\partial z_{t-\tau,n}}\Big)^{\mathsf{T}},\\
    \mathring{\mathbf{v}}_{t,k}=
\Big(\frac{\partial^{3}\log p(z_{t,k}|\mathbf{z}_{Hx})}{\partial z_{t,k}^{2}\partial z_{t-\tau,1}},\frac{\partial^{3}\log p(z_{t,k}|\mathbf{z}_{Hx})}{\partial z_{t,k}^{2}\partial z_{t-\tau,2}},...,
    \frac{\partial^{3}\log p(z_{t,k}|\mathbf{z}_{Hx})}{\partial z_{t,k}^{2}\partial z_{t-\tau,n}}\Big)^{\mathsf{T}}.
    \end{split}
\end{equation}
, where  $\mathbf{z}_{Hx}$ denotes the lagged latent variables up to maximum time lag $L$. We take derivatives with regard to $z_{ t-\tau,1}, …, z_{ t-\tau,n}$, which can be any latent temporal variables at lag $\tau$, instead of $z_{ t-1,1}, …, z_{t-1,n}$. If there exists one $\tau$ (out of the $L$ lags) that satisfies the condition, then the stationary latent processes are identifiable. 

%For the sake of simplicity, we consider only one special case with $\tau=1$ and $L=2$ in Theorem~\ref{app: Th2}. Our identifiability theorem can be actually extended to arbitrary lags and subsequences easily. For any given $\tau$, and subsequence which is centered at $\rvz_t$ with previous $lo$ and following $hi$ steps, i.e., $\rvc_t=\{\rvz_{t-lo},\cdots,\rvz_{t}\}$. In this case, the vector function $w(i,j,m)$ in Sufficient Variability Assumption should be modified as 
%\begin{equation}
%\small
%\begin{split}
    %w(i,j,m)=
    %&\Big(\frac{\partial^3 \log p(\rvc_t|\rvz_{t-lo-1},\cdots,\rvz_{t-lo-\tau})}{\partial c_{t,1}^2\partial z_{t-lo-1,m}},\cdots,\frac{\partial^3 \log p(\rvc_t|\rvz_{t-lo-1},\cdots,\rvz_{t-lo-\tau})}{\partial c_{t,2n}^2\partial z_{t-lo-1,m}}\Big)\oplus \\
    %&\Big(\frac{\partial^2 \log p(c_t|\rvz_{t-lo-1},\cdots,\rvz_{t-lo-\tau})}{\partial c_{t,1}\partial z_{t-lo-1,m}},\cdots,\frac{\partial^2 \log p(c_t|\rvz_{t-lo-1},\cdots,\rvz_{t-lo-\tau})}{\partial c_{t,2n}\partial z_{t-lo-1,m}}\Big)\oplus \\
   % & \Big(\frac{\partial^3 \log p(\rvc_t|\rvz_{t-lo-1},\cdots,\rvz_{t-lo-\tau})}{\partial c_{t,i}\partial c_{t,j}\partial z_{t-lo-1,m}}\Big)_{(i,j)\in \mathcal{E}(\mathcal{M}_{\rvc_t})}.
%\end{split}
%\end{equation}
%Besides, $2*(lo+1)+2|\mathcal{M}_{\rvc_t}|$ values of linearly independent vector functions in $z_{t',m}$ for $t'\in[t-lo-1,\cdots,t-lo-\tau]$ and $m\in[1,\cdots,n]$ are required as well. The rest part of the theorem remains the same, and the proof can be easily extended in such a setting.

\section{Granger non-causality of Latent Variables}
\label{app: Granger non-causality of Latent Variables}

In this section, we will theoretically analyze the identification of the proposed model. To achieve this, we begin with the definition of the Granger non-causality of latent variables as follows. 
% 这里的公式还没引用
% 对一下符号
\begin{definition}
\label{def: Granger non-causality of Latent Variables}
      Suppose that the object sequences $Z$ are generated according to Equation (1) and Equation (2), we can determine the Granger non-causality of the object $\mathbf{z}_{t-1,i}$ with respect to object $\mathbf{z}_{t,j}$ as follows: For all $\mathbf{z}_{t-1:t-\tau,1},\mathbf{z}_{t-1:t-\tau,2},\cdots,\mathbf{z }_{t-1:t-\tau,n}$,and the same variable with different values $\mathbf{z}_{t-1:t-\tau,i} \neq \mathbf{z}^{'}_{t-1:t-\tau,i}$,if the following condition holds:
\begin{equation}
\phi_j(\mathbf{z}_{t - 1:t - \tau,1}, \cdots, \mathbf{z}_{t - 1:t - \tau,i}, \cdots, \mathbf{z}_{t - 1:t - \tau,n}) = \phi_j(\mathbf{z}_{t - 1:t - \tau,1}, \cdots, \mathbf{z}_{t - 1:t - \tau,i}', \cdots, \mathbf{z}_{t - 1:t - \tau,n})
\end{equation}
that is, $\mathbf{x}_{t,j}$ is invariant to $\mathbf{x}_{t-1:t-\tau,i}$ with $\phi_j$.
\end{definition}

\begin{prop}
    Suppose that the estimated function $f_j$ well models the relationship between $\mathbf{z}_{t}$ and $\mathbf{z}_{t-1}$.Given the ground truth Granger Causal structure $\mathcal{G}=(V, E_V)$ with the maximum lag of 1, where $V$ and $E_V$ denote the nodes and edges, respectively. When the data are generated by Equation (1) and Equation (2), then $\mathbf{z}_{t-1,i} \rightarrow \mathbf{z}_{t,j} \notin E_V$ if and only if $\frac{\partial \mathbf{z}_{t,j}}{\partial  \mathbf{z}_{t-1,i}}=0$.
\end{prop}

\begin{proof}
    $\Rightarrow$If $\mathbf{z}_{t-1,i} \rightarrow \mathbf{z}_{t,j} \notin E_V$ (there is no Granger Causality between $\mathbf{z}_{t-1,i}$ and $\mathbf{z}_{t,j}$ in the ground truth process), then according to Definition \ref{def: Granger non-causality of Latent Variables},there must be $\mathbf{z}_{t,j}\neq \mathbf{z}'_{t,j}$ for all the different values of $\mathbf{z}_{t-1,i}$. If  $\frac{\partial \mathbf{z}_{t,j}}{\partial  \mathbf{z}_{t-1,i}}=0$, then when the input values are different, i.e. $\mathbf{z}_{t-1,i} \neq \mathbf{z}'_{t-1,i}$,the outputs of $\mathbf{f}_j$ are also different, i.e., $\mathbf{z}_{t,j}\neq \mathbf{z}'_{t,j}$ , which results in contradictions.\\
    $\Leftarrow$: Suppose $\mathbf{z}_{t-1,i} \rightarrow \mathbf{z}_{t,j} \in E_V$ (there is Granger Causality between $\mathbf{z}_{t-1,i}$ and $\mathbf{z}_{t,j}$ in the ground truth process), for any $\mathbf{z}_{t,j}$ and $\mathbf{z}_{t,j}'$ in Definition \ref{def: Granger non-causality of Latent Variables}, if $\mathbf{z}_{t,j} \neq \mathbf{z}'_{t,j}$, there must be different input $\mathbf{z}_{t-1,i} \neq \mathbf{z}'_{t-1,i}$. If $\frac{\partial \mathbf{z}_{t,j}}{\partial  \mathbf{z}_{t-1,i}}=0$, then when $\mathbf{z}_{t,j} \neq \mathbf{z}_{t,j}'$, there might be $\mathbf{z}_{t-1,i} = \mathbf{z}_{t-1,i}'$, which results in contradictions.
    
\end{proof}

%\subsection{General Case for Component-wise Identifications}
%\label{app: General Case for Component-wise Identifications}

%In this part, we briefly give the proof for a more general case of our theorem.

%\begin{corollary}
%\label{coroll}
%\textbf{(General Case for Component-wise Identification.)} 
%Suppose that the observations are generated by Equation~\ref{equ:g1} and \ref{equ:g2}, and there exists $\rvc_t=\{\rvz_{t-a},\cdots,\rvz_{t},\cdots,\rvz_{t+b}\}$ with the corresponding Markov network $\mathcal{M}_{\rvc_t}$. Suppose assumptions A1 and A2 hold true, and for any $z_{t,i}\in \rvz_t$, the intimate neighbor set of $z_{t,i}$ is an empty set. 
%When the observational equivalence is achieved with the minimal number of edges of estimated Markov network of $\mathcal{M}_{\hat{\rvc}}$, there must exist a permutation $\pi$ of the estimated latent variables, such that $z_{t,i}$ and $\hat{z}_{t,\pi(i)}$ is one-to-one corresponding, i.e., $z_{t,i}$ is component-wise identifiable.
%\end{corollary}

%\begin{proof}
  %  The proof is similar to that of Theorem~\ref{app: Th2}. The only difference is that given a different subsequence, the variables that are used to make intimate neighbors empty might be different. The rest part of the theorem remains the same.
%\end{proof}

%Here we further discuss the idea behind the Sparse Latent Process. For two latent variables $z_{t,i},z_{t,j}$ that are entangled at some certain timestamp, the contextual information can be utilized to recover these variables. Intuitively speaking, when $z_{t,i}$ is directly affected by some previous variable, says $z_{t-1,k}$, while $z_{t,j}$ is not. In this case, the changes that happen on $z_{t-1,k}$ can be captured, which helps to tell $z_{t,i}$ from $z_{t,j}$. Similarly, if $z_{t,i}$ directly affects $z_{t+1,k}$ while $z_{t,j}$ does not, we can distinguish $z_{t,i}$ from $z_{t,j}$ as well. When all variables are naturally conditionally independent, no contextual information will be needed. One more thing to note is that, even though the sparse latent process is not fully satisfied, as long as some structures mentioned above exist, the corresponding entanglement can be prevented.
 

% \section{Evidence Lower Bound}
% \subsection{Forecasting}
% In the prediction problem, we start from the logarithm of the joint probability of past and future observations$\ln {p(x_{1:t},x_{t+1:l})}$, and through derivation, we finally obtain Equation~\ref{eq:elbo1}.
% \begin{equation}
% \begin{split}
%     \ln {p(x_{1:t},x_{t+1:l})} &= \ln {\frac{p(x_{1:t},x_{t+1:l},z_{1:t},z_{t+1:l})} {p(z_{1:t},z_{t+1:l}|x_{1:t},x_{t+1:l})}} \\
%     &= \ln {\frac{p(x_{1:t},x_{t+1:l},z_{1:t},z_{t+1:l}) q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})} {p(z_{1:t},z_{t+1:l}|x_{1:t},x_{t+1:l})  q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})}}
% \end{split}
% \label{eq:elbo1}
% \end{equation}
% Next, by taking the expectation with respect to \( q(z_{1:t}|x_{1:t}) \) and \( q(z_{t+1:l}|z_{1:t}) \), we arrive at Equation~\ref{eq:elbo2}.
% \begin{equation}
% \small
% \begin{split}
%     \mathbb{E}_{ q(z_{1:t}|x_{1:t})} \mathbb{E}_{q(z_{t+1:l}|z_{1:t})} \ln {p(x_{1:t},x_{t+1:l})}
%  &= \mathbb{E}_{ q(z_{1:t}|x_{1:t})} \mathbb{E}_{q(z_{t+1:l}|z_{1:t})}  \ln {\frac{p(x_{1:t},x_{t+1:l},z_{1:t},z_{t+1:l}) q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})} {p(z_{1:t},z_{t+1:l}|x_{1:t},x_{t+1:l})  q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})}}\\
%  \ln {p(x_{1:t},x_{t+1:l})}&=D_{KL}(q(z_{1:t}|x_{1:t})||p(z_{1:t}|x_{1:t},x_{t+1:l}))+D_{KL}(q(z_{t+1:l}|z_{1:t})||p(z_{t+1:l}|x_{t+1:l},z_{1:t}))\\
%  &\quad~ +\mathbb{E}_{ q(z_{1:t}|x_{1:t})} \mathbb{E}_{q(z_{t+1:l}|z_{1:t})} \ln {\frac{p(x_{1:t},x_{t+1:l},z_{1:t},z_{t+1:l})} { q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})}}
% \end{split}
% \label{eq:elbo2}
% \end{equation}
% Since the KL divergence is strictly positive, we can ultimately deduce the evidence lower bound as shown in Equation~\ref{eq:elbo3}.
% \begin{equation}
% \begin{split}
%  \ln {p(x_{1:t},x_{t+1:l})}&\geq \mathbb{E}_{ q(z_{1:t}|x_{1:t})} \mathbb{E}_{q(z_{t+1:l}|z_{1:t})}  \ln {\frac{p(x_{1:t},x_{t+1:l},z_{1:t},z_{t+1:l})} { q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})}}\\
%  &= \mathbb{E}_{ q(z_{1:t}|x_{1:t})} \mathbb{E}_{q(z_{t+1:l}|z_{1:t})} \ln {\frac{p(z_{1:t}) p(z_{t+1:l}|z_{1:t}) p(x_{1:t}|z_{1:t}) p(x_{t+1:l}|z_{t+1:l})} { q(z_{1:t}|x_{1:t}) q(z_{t+1:l}|z_{1:t})}}\\
%   &= \underbrace{\mathbb{E}_{ q(z_{1:t}|x_{1:t})} \ln {p(x_{1:t}|z_{1:t})}}_{L_{rec}}+\underbrace{\mathbb{E}_{q(z_{t+1:l}|z_{1:t})} \ln{p(x_{t+1:l}|z_{t+1:l})}}_{L_{pre}}\\
%   &\quad~\underbrace{-D_{KL}( q(z_{1:t}|x_{1:t})||p(z_{1:t}))-D_{KL}( q(z_{t+1:l}|z_{1:t})||p(z_{t+1:l}|z_{1:t}))}_{L_{KL}}
% \end{split}
% \label{eq:elbo3}
% \end{equation}


% \subsection{Classification}
% \begin{equation}
% \begin{split}
%     \ln {p(x_{1:t},y)} &= \ln {\frac{p(x_{1:t},y,z_{1:t})} {p(z_{1:t}|x_{1:t},y)}} \\
%     &= \ln {\frac{p(x_{1:t},y,z_{1:t}) q(z_{1:t}|x_{1:t}) } {p(z_{1:t}|x_{1:t},y)  q(z_{1:t}|x_{1:t})}}
% \end{split}
% \label{eq:celbo1}
% \end{equation}
% Next, by taking the expectation with respect to \( q(z_{1:t}|x_{1:t}) \), we arrive at Equation~\ref{eq:celbo2}.
% \begin{equation}
% \small
% \begin{split}
%     \mathbb{E}_{ q(z_{1:t}|x_{1:t})}\ln {p(x_{1:t},y)}
% &= \mathbb{E}_{ q(z_{1:t}|x_{1:t})} \ln {\frac{p(x_{1:t},y,z_{1:t}) q(z_{1:t}|x_{1:t})} {p(z_{1:t}|x_{1:t},y)  q(z_{1:t}|x_{1:t}) }}\\
%  \ln {p(x_{1:t},x_{t+1:l})}&=D_{KL}(q(z_{1:t}|x_{1:t})||p(z_{1:t}|x_{1:t},y))\\
%  &\quad~ +\mathbb{E}_{ q(z_{1:t}|x_{1:t})}  \ln {\frac{p(x_{1:t},y,z_{1:t})} { q(z_{1:t}|x_{1:t}) }}
% \end{split}
% \label{eq:celbo2}
% \end{equation}

% Since the KL divergence is strictly positive, we can ultimately deduce the evidence lower bound as shown in Equation~\ref{eq:celbo3}.
% \begin{equation}
% \small
% \begin{split}
%  \ln {p(x_{1:t},y)}&\geq \mathbb{E}_{ q(z_{1:t}|x_{1:t})}  \ln {\frac{p(x_{1:t},y,z_{1:t})} { q(z_{1:t}|x_{1:t}) }}\\
%  &=\mathbb{E}_{ q(z_{1:t}|x_{1:t})}  \ln {\frac{p(z_{1:t}),p(x_{1:t}|z_{1:t})  p(y|z_{1:t})} { q(z_{1:t}|x_{1:t}) }}\\
%  &=\underbrace{\mathbb{E}_{ q(z_{1:t}|x_{1:t})}  p(y|z_{1:t})}_{L_{CE}}+\underbrace{\mathbb{E}_{ q(z_{1:t}|x_{1:t})}p(x_{1:t}|z_{1:t}) }_{L_{rec}}\\
%  &\quad~\underbrace{-D_{KL}( q(z_{1:t}|x_{1:t})||p(z_{1:t}))}_{L_{KL}}
% \end{split}
% \label{eq:celbo3}
% \end{equation}

\section{Prior Likelihood Derivation} \label{app:prior}

 We first consider the prior of $\ln p(\rvz_{1:T})$. We start with an illustrative example of latent causal processes with two time-delay latent variables, i.e. $\rvz_t=[z_{t,1}, z_{t,2}]$ with maximum time lag $\tau=1$, i.e., $z_{t,i}=f_i(\rvz_{t-1}, \epsilon_{t,i})$ with mutually independent noises. Then we write this latent process as a transformation map $\mathbf{f}$ (note that we overload the notation $f$ for transition functions and for the transformation map):
    \begin{equation}
    \small
\begin{gathered}\nonumber
    \begin{bmatrix}
    \begin{array}{c}
        z_{t-1,1} \\ 
        z_{t-1,2} \\
        z_{t,1}   \\
        z_{t,2}
    \end{array}
    \end{bmatrix}=\mathbf{f}\left(
    \begin{bmatrix}
    \begin{array}{c}
        z_{t-1,1} \\ 
        z_{t-1,2} \\
        \epsilon_{t,1}   \\
        \epsilon_{t,2}
    \end{array}
    \end{bmatrix}\right).
\end{gathered}
\end{equation}
By applying the change of variables formula to the map $\mathbf{f}$, we can evaluate the joint distribution of the latent variables $p(z_{t-1,1},z_{t-1,2},z_{t,1}, z_{t,2})$ as 
\begin{equation}
\small
\label{equ:p1}
    p(z_{t-1,1},z_{t-1,2},z_{t,1}, z_{t,2})=\frac{p(z_{t-1,1}, z_{t-1,2}, \epsilon_{t,1}, \epsilon_{t,2})}{|\text{det }\mathbf{J}_{\mathbf{f}}|},
\end{equation}
where $\mathbf{J}_{\mathbf{f}}$ is the Jacobian matrix of the map $\mathbf{f}$, which is naturally a low-triangular matrix:
\begin{equation}
\small
\begin{gathered}\nonumber
    \mathbf{J}_{\mathbf{f}}=\begin{bmatrix}
    \begin{array}{cccc}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        \frac{\partial z_{t,1}}{\partial z_{t-1,1}} & \frac{\partial z_{t,1}}{\partial z_{t-1,2}} & 
        \frac{\partial z_{t,1}}{\partial \epsilon_{t,1}} & 0 \\
        \frac{\partial z_{t,2}}{\partial z_{t-1, 1}} &\frac{\partial z_{t,2}}{\partial z_{t-1,2}} & 0 & \frac{\partial z_{t,2}}{\partial \epsilon_{t,2}}
    \end{array}
    \end{bmatrix}.
\end{gathered}
\end{equation}
Given that this Jacobian is triangular, we can efficiently compute its determinant as $\prod_i \frac{\partial z_{t,i}}{\epsilon_{t,i}}$. Furthermore, because the noise terms are mutually independent, and hence $\epsilon_{t,i} \perp \epsilon_{t,j}$ for $j\neq i$ and $\epsilon_{t} \perp \rvz_{t-1}$, so we can with the RHS of Equation (\ref{equ:p1}) as follows
\begin{equation}
\label{equ:p2}
\begin{array}{rl}
    p(z_{t-1,1}, z_{t-1,2}, z_{t,1}, z_{t,2}) & = p(z_{t-1,1}, z_{t-1,2}) \times \frac{p(\epsilon_{t,1}, \epsilon_{t,2})}{|\mathbf{J}_{\mathbf{f}}|} \\
    & = p(z_{t-1,1}, z_{t-1,2}) \times \frac{\prod_i p(\epsilon_{t,i})}{|\mathbf{J}_{\mathbf{f}}|} \\
    % & \Downarrow \\
    % \log {p(z_{t,1}, z_{t,2} , z_{t-1,1}, z_{t-1,2})} & = \sum_{i=1}^{2} p(\epsilon_{t,i}) + \sum_{i=1}^{2} \log \left|\frac{\partial \epsilon_{t,i}}{\partial z_{t,i}}\right|
\end{array}
\end{equation}

Finally, we generalize this example and derive the prior likelihood below. Let $\{r_i\}_{i=1,2,3,\cdots}$ be a set of learned inverse transition functions that take the estimated latent causal variables, and output the noise terms, i.e., $\hat{\epsilon}_{t,i}=r_i(\hat{z}_{t,i}, \{ \hat{\rvz}_{t-\tau}\})$. Then we design a transformation $\mathbf{A}\rightarrow \mathbf{B}$ with low-triangular Jacobian as follows:
\begin{equation}
\small
\begin{gathered}
    \underbrace{[\hat{\rvz}_{t-\tau},\cdots,{\hat{\rvz}}_{t-1},{\hat{\rvz}}_{t}]^{\top}}_{\mathbf{A}} \text{  mapped to  } \underbrace{[{\hat{\rvz}}_{t-\tau},\cdots,{\hat{\rvz}}_{t-1},{\hat{\epsilon}}_{t,i}]^{\top}}_{\mathbf{B}}, \text{ with } \mathbf{J}_{\mathbf{A}\rightarrow\mathbf{B}}=
    \begin{bmatrix}
    \begin{array}{cc}
        \mathbb{I}_{n\times \tau} & 0\\
                    * & \text{diag}\left(\frac{\partial r_{i,j}}{\partial {\hat{z}}_{t,j}}\right)
    \end{array}
    \end{bmatrix}.
\end{gathered}
\end{equation}
Similar to Equation (\ref{equ:p2}), we can obtain the joint distribution of the estimated dynamics subspace as:
\begin{equation}
    \log p(\mathbf{A})=\underbrace{\log p({\hat{\rvz}}_{t-\tau},\cdots, {\hat{\rvz}}_{t-1}) + \sum^{n}_{i=1}\log p({\hat{\epsilon}}_{t,i})}_{\text{Because of mutually independent noise assumption}}+\log (|\text{det}(\mathbf{J}_{\mathbf{A}\rightarrow\mathbf{B}})|)
\end{equation}
Finally, we have:
\begin{equation}
\small
    \log p({\hat{\rvz}}_t|\{{\hat{\rvz}}_{t-\tau}\})=\sum_{i=1}^{n} \log {p({\hat{\epsilon}_{t,i}})} + \sum_{i=1}^{n}\log |\frac{\partial r_i}{\partial {\hat{z}}_{t,i}}|
\end{equation} 
Since the prior of $p(\hat{\rvz}_{1:T})=p(\hat{z}_{1:\tau})\prod_{t=\tau+1}^{T} p(\hat{\rvz}_{t}| \{ \hat{\rvz}_{t-\tau}\})$ with the assumption of first-order Markov assumption, we can estimate $\log p(\hat{\rvz}_{1:T})$ as follows:
\begin{equation}
\small
    \log p(\hat{\rvz}_{1:T})=\log {p(\hat{z}_{1:\tau})} \prod_{t=\tau+1}^T\left(\sum_{i=1}^n \log p(\hat{\epsilon}_{t,i}) + \sum_{i=1}^n \log |\frac{\partial r_i}{\partial z_{t,i}}|  \right),
\end{equation}
Where the noise \( p(\hat{\epsilon}_{\tau,i}) \) and the initial latent variables \( p(\hat{\rvz})_{1:\tau} \) are both assumed to follow Gaussian distributions. When $\tau=1$, $\log p(\rvz_{1:T})=\log {p(\hat{z}_{1})}\prod_{t=2}^T\left(\sum_{i=1}^n \log p(\hat{\epsilon}_{t,i}) + \sum_{i=1}^n \log |\frac{\partial r_i}{\partial z_{t,i}}|  \right)$. In a similar manner, the distribution \( p(\hat{\rvz}_{t+1:T}|\hat{\rvz}_{1:t}) \) can be estimated using analogous methods.



% \section{\textcolor{black}{Realworld Dataset Descriptions}}

% \subsection{\textcolor{black}{Human Motion Capture Dataset}}
% \textcolor{black}{We further show the effectiveness of our method on the human motion capture dataset. Human motion has been studied in many fields, e.g., Granger Causality Inference \cite{9376668,zhang2017causal}, the linear and the nonlinear dynamical systems \cite{fox2014joint}. Many researchers use it to study the human motion capture dataset for Granger Causality since the relationships among the joints of humans are a natural causal structure. In this paper, we use the Human3.6M dataset \cite{ionescu2013human3} \footnote{http://vision.imar.ro/human3.6m/description.php}, one of the most popular benchmarks for human motion prediction \cite{barsoum2018hp}, for cross-domain human motion prediction. This dataset contains 15 motions and we choose three of them as three different domains: ``Walking'', ``Greeting'', ``Eating'' and ``Smoking'' the examples of this dataset are shown in Figure 1 (b)(c), and (d). We choose 9 primary sensors which record the three-dimensional coordinate and the dimension of the processed dataset is 27.} 
% \subsection{\textcolor{black}{PPG-DaLiA Dataset}}
% \textcolor{black}{We also consider the PPG-DaLiA dataset,\footnote{https://archive.ics.uci.edu/ml/datasets/PPG-DaLiA} which is publicly available for PPG-based heart rate estimation. This multimodal dataset features physiological and motion data, recorded with both a wrist- and a chest-worn device from 15 volunteers while they perform a wide range of activities. Raw sensor data was recorded with two devices: a chest-worn device and a wrist-worn device. The chest-worn device provides an electrocardiogram, respiration, and three-axis acceleration. All signals are sampled at 700 Hz. The wrist-worn device provides the following sensor data: blood volume pulse, electrodermal activity, body temperature, and three-axis acceleration.}
% \textcolor{black}{Though different activities lead to the variance of heart rate, how the body signals influence the heart rates usually follows the same causal mechanism, so we can employ this dataset to evaluate the performance of the proposed TSCA model. To reduce the computation complexity, we randomly choose four of them as different domains: ``Cycling (C)'', ``Sitting (S)'', ``Working (W)'', and ``Driving (D)''. An example of this dataset is shown in Figure 1 (a). We employ the data collected from all volunteers and use Z-score Normalization to process all the datasets for each domain, respectively.}


% \subsection{\textcolor{black}{Electricity Transformer Temperature Dataset}}
% \textcolor{black}{The ETT (Electricity Transformer Temperature) \footnote{https://github.com/zhouhaoyi/ETDataset} is a crucial indicator in the electric power long-term deployment, so Zhou et.al \cite{zhou2021informer} collect 2-year data from two separated counties
% in China, which consists of seven features,
% including power load features and oil temperature. We employ the ETT-small subset which contains two stations. We consider each station as a domain and use Z-score Normalization to process the dataset in each domain.}
% \subsection{\textcolor{black}{Electricity Load Diagrams Datasets}}
% \textcolor{black}{The Electricity Load Diagrams 2011-2014 dataset \footnote{https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014} was created by Artur Trindade and shared on the UCI Machine Learning Repository. This comprehensive dataset captures the electricity consumption of 370 substations in Portugal from January 2011 to December 2014, with a sampling period of 15 minutes. We use Z-score Normalization to process the dataset and select 90 substations. Since this dataset is not devised for domain adaptation problem, we consider that the domain shift occurs in different seasons and divide them into four different domains based on the months: domain1 (January, February, March), domain2 (April, May, June), domain3 (July, August, September), and domain4 (October, November, December). We employ this dataset to evaluate the performance of the GCA model on high-dimensional multivariate time series datasets and aim to predict the electricity load of the substations.}
% \subsection{\textcolor{black}{PEMS-BAY Dataset}}
% \textcolor{black}{PEMS-BAY \footnote{https://pems.dot.ca.gov/} is a dataset of traffic speeds collected from the California Transportation Agencies (CalTrans) Performance Measurement System (PeMS) \cite{chen2001freeway}. It includes data collected from 325 sensors located throughout the Bay Area, covering a period of 6 months from January 1st, 2017 to May 31st, 2017 \cite{li2017diffusion}. The dataset provides detailed traffic information recorded at a frequency of every 5 minutes. we use Z-score Normalization to process the dataset and select 100 sensors. Since this dataset is not devised for domain adaptation problems, we consider that the domain shift occurs in different seasons and divide them into three domains based on months: domain1 (January), domain2 (February), and domain3 (March). Since the sensors are deployed in the traffic networks, the speed of the sensors is influenced causally. }

\section{More Experiment Results}

This is more experiment results
% \bibliographystyle{IEEEtran}
% \bibliography{ref}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.01\textwidth, trim=0cm  12cm 0cm 0cm, clip]{fig/ett_1_to_2_predict.pdf}
    \caption{Visualization of prediction results across varying forecast lengths for the transition from domain 1 to domain 2 in the ETT dataset. Subfigures (a), (b), (c), and (d) represent forecast lengths of 10, 20, 30, and 40, respectively.}
    \label{fig:diff_len_mase}
\end{figure*}


\begin{table*}[htbp]
\caption{The MAE and MSE on the Electricity Load Diagrams dataset, where R-COAT, iTRANS, and TMixer are the abbreviation of RAINCOAT, iTransformer, and
TimeMixer, respectively.}
\label{tab:eld}
\setlength{\tabcolsep}{3mm}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|c|ccccccccccc@{}}
\toprule
Metric                & Task              & SASA            & GCA    & DAF    & CLUDA  & R-COAT & AdvSKM & iTRANS & TMixer & TSLANet & SegRNN & Ours            \\ \midrule
\multirow{12}{*}{\rotatebox{90}{MSE}} & 1 $\rightarrow$ 2 & 0.1781          & 0.2919 & 0.2142 & 0.2412 & 0.2472 & 0.2143 & 0.1778 & 0.2066 & 0.2551  & 0.2675 & \textbf{0.1452} \\
                      & 1 $\rightarrow$ 3 & 0.1544          & 0.2810 & 0.1772 & 0.2133 & 0.2251 & 0.1879 & 0.1723 & 0.2064 & 0.2464  & 0.2356 & \textbf{0.1410} \\
                      & 1 $\rightarrow$ 4 & 0.1855          & 0.3421 & 0.2043 & 0.2689 & 0.2804 & 0.2205 & 0.2121 & 0.3375 & 0.2763  & 0.3018 & \textbf{0.1586} \\ \cmidrule{2-13} 
                      & 2 $\rightarrow$ 1 & 0.1216          & 0.2062 & 0.1426 & 0.1503 & 0.1523 & 0.1217 & 0.1758 & 0.2742 & 0.2401  & 0.2372 & \textbf{0.1171} \\
                      & 2 $\rightarrow$ 3 & 0.1315          & 0.2097 & 0.1527 & 0.1643 & 0.1625 & 0.1372 & 0.1459 & 0.2249 & 0.2172  & 0.2144 & \textbf{0.1129} \\
                      & 2 $\rightarrow$ 4 & 0.1892          & 0.3018 & 0.2480 & 0.2528 & 0.2285 & 0.2157 & 0.2084 & 0.3372 & 0.3161  & 0.2903 & \textbf{0.1587} \\ \cmidrule{2-13} 
                      & 3 $\rightarrow$ 1 & \textbf{0.1360} & 0.2514 & 0.1550 & 0.1560 & 0.1922 & 0.1473 & 0.1927 & 0.2840 & 0.2612  & 0.2512 & \textbf{0.1384} \\
                      & 3 $\rightarrow$ 2 & 0.1181          & 0.2262 & 0.1305 & 0.1468 & 0.1625 & 0.1190 & 0.1577 & 0.2371 & 0.2281  & 0.2250 & \textbf{0.1113} \\
                      & 3 $\rightarrow$ 4 & 0.1874          & 0.3277 & 0.2166 & 0.2835 & 0.2868 & 0.2505 & 0.2327 & 0.3328 & 0.3054  & 0.2928 & \textbf{0.1839} \\ \cmidrule{2-13} 
                      & 4 $\rightarrow$ 1 & 0.1203          & 0.2496 & 0.1277 & 0.1416 & 0.1589 & 0.1206 & 0.1791 & 0.2380 & 0.2449  & 0.2308 & \textbf{0.1063} \\
                      & 4 $\rightarrow$ 2 & 0.1442          & 0.2591 & 0.1636 & 0.1851 & 0.1829 & 0.1600 & 0.1655 & 0.2140 & 0.2493  & 0.2340 & \textbf{0.1176} \\
                      & 4 $\rightarrow$ 3 & 0.1271          & 0.2153 & 0.1372 & 0.1670 & 0.1734 & 0.1386 & 0.1558 & 0.2779 & 0.2435  & 0.2155 & \textbf{0.1100} \\ \midrule
\multirow{12}{*}{\rotatebox{90}{MAE}} & 1 $\rightarrow$ 2 & 0.3103          & 0.4131 & 0.3508 & 0.3682 & 0.3866 & 0.3494 & 0.3116 & 0.3264 & 0.3794  & 0.3873 & \textbf{0.2821} \\
                      & 1 $\rightarrow$ 3 & 0.2959          & 0.4107 & 0.3228 & 0.3531 & 0.3717 & 0.3343 & 0.3127 & 0.3305 & 0.3760  & 0.3606 & \textbf{0.2816} \\
                      & 1 $\rightarrow$ 4 & 0.3270          & 0.4548 & 0.3498 & 0.3999 & 0.4167 & 0.3667 & 0.3511 & 0.4419 & 0.3972  & 0.4147 & \textbf{0.3007} \\ \cmidrule{2-13} 
                      & 2 $\rightarrow$ 1 & 0.2593          & 0.3396 & 0.2842 & 0.2946 & 0.2954 & 0.2595 & 0.3065 & 0.3883 & 0.3638  & 0.3570 & \textbf{0.2519} \\
                      & 2 $\rightarrow$ 3 & 0.2722          & 0.3491 & 0.2988 & 0.3061 & 0.3105 & 0.2817 & 0.2842 & 0.3569 & 0.3512  & 0.3421 & \textbf{0.2531} \\
                      & 2 $\rightarrow$ 4 & 0.3286          & 0.4238 & 0.3879 & 0.3752 & 0.3662 & 0.3447 & 0.3411 & 0.4391 & 0.4321  & 0.4046 & \textbf{0.2961} \\ \cmidrule{2-13} 
                      & 3 $\rightarrow$ 1 & 0.2744          & 0.3797 & 0.2967 & 0.2976 & 0.3305 & 0.2886 & 0.3238 & 0.4012 & 0.3880  & 0.3760 & \textbf{0.2740} \\
                      & 3 $\rightarrow$ 2 & 0.2532          & 0.3584 & 0.2707 & 0.2860 & 0.3059 & 0.2564 & 0.2960 & 0.3635 & 0.3577  & 0.3508 & \textbf{0.2472} \\
                      & 3 $\rightarrow$ 4 & 0.3240          & 0.4367 & 0.3572 & 0.3975 & 0.4084 & 0.3802 & 0.3650 & 0.4405 & 0.4227  & 0.4090 & \textbf{0.3202} \\ \cmidrule{2-13} 
                      & 4 $\rightarrow$ 1 & 0.2564          & 0.3819 & 0.2672 & 0.2833 & 0.3021 & 0.2592 & 0.3114 & 0.3549 & 0.3734  & 0.3517 & \textbf{0.2391} \\
                      & 4 $\rightarrow$ 2 & 0.2794          & 0.3910 & 0.3028 & 0.3212 & 0.3236 & 0.2952 & 0.3004 & 0.3388 & 0.3742  & 0.3599 & \textbf{0.2527} \\
                      & 4 $\rightarrow$ 3 & 0.2642          & 0.3537 & 0.2802 & 0.3108 & 0.3185 & 0.2831 & 0.2950 & 0.3797 & 0.3744  & 0.3396 & \textbf{0.2458} \\ \bottomrule
\end{tabular}%
}
\end{table*}