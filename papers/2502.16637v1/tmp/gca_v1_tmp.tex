
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%\cdots
%% This is a skeleton file demonstr\appendixating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}

\usepackage{subfigure}


\usepackage{amsmath}
% \usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{amsfonts, amssymb}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[mathscr]{eucal}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,lipsum}
\usepackage{amssymb} 
\usepackage{cuted}%%\stripsep-3pt
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}

% \newtheorem{lem}{Lemma}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}{Corollary}
% \newtheorem{assumption}{Assumption}

% \theoremstyle{plain}
% \newtheorem{thm}{Theorem}[section]
% % \newtheorem{lem}[thm]{Lemma}
% \newtheorem{prop}[thm]{Proposition}
% \newtheorem*{cor}{Corollary}

% \theoremstyle{definition}
% \newtheorem{defn}{Definition}[section]
% \newtheorem{conj}{Conjecture}[section]
% \newtheorem{exmp}{Example}[section]

% \theoremstyle{remark}
% \newtheorem*{rem}{Remark}
% \newtheorem*{note}{Note}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Granger Causality Alignment for Transferable Time-Series Forecasting}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Zijian Li,~\IEEEmembership{Member,~IEEE,}
        Ruichu Cai,~\IEEEmembership{Member,~IEEE,}
        and Tom Z. J. Fu% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem
Zijian Li is with the School of Computing, Guangdong University of Technology, Guangzhou China, 510006.\protect
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: leizigin@gmail.com
\IEEEcompsocthanksitem Ruichu Cai is with the School of Computing, Guangdong University of Technology and and Guangdong Provincial Key Laboratory of Public Finance and Taxation with Big Data Application, Guangzhou China, 510006.
Email: cairuichu@gmail.com\protect
\IEEEcompsocthanksitem Tom Z. J. Fu is with Bigo Technology Pte. Ltd. Email: Tom.fu@adsc.com.sg\protect

}% <-this % stops an unwanted space
\thanks{Manuscript received XX; revised XX; accepted XX. Date of publication XX XX, 2019; date of current version XX XX, 2019. Ruichu Cai and Zijian Li was supported in part by Natural Science Foundation of China (61876043, 61976052), Science and Technology Planning Project of Guangzhou (201902010058) and Guangdong Provincial Science and Technology Innovation Strategy Fund (2019B121203012).}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
This paper focuses on the problem of domain adaptation for time-series forecasting, an easily neglected but challenging problem, since the complex conditional dependencies between different timestamps vary among domains. In fact, these domain-specific conditional dependencies are mainly led by the data offset, the time lags, and the variant data distribution. In order to address this problem, we assume the \textit{Granger Causality} are stable among different domains and use it to model the complex domain-specific conditional dependencies. The compact \textit{Granger Causality} can not only address the offset problem by avoiding directly align the feature like traditional domain adaptation methods but also simultaneously portray domain-invariant and the domain-specific modules of the conditional distribution. This further enlightens us to devise an end-to-end model of transferable time-series forecasting. The proposed method can not only learning the cross-domain \textit{Granger Causality} but also address the cross-domain time-series forecasting problem, it can even provide the interpretability of the predicted result to some extend. We further theoretically analyze the superiority of the proposed methods. Experimental results on both synthetic and real data demonstrate the effectiveness of the proposed method for transferable time-series forecasting. 

\end{abstract}
% 1. 解决什么问题（DA）
% 2. 难点在哪儿，联合分布不知道，可以说什么是变的
% 3. 解决方法，提出motvation（一句话让别人知道大概怎么做，而且是惊人的想法）花两三句话来详细介绍自己的方法
% 4. 这样做的好处有什么，实验结果，代码等等
% no keywords
% 我的版本
% 解决时间序列预测的DA问题
% 难点在于基于多维时间序列的复杂依赖是根据domain变化而变化的，这个变化主要体现在三个方面：Offset，Time Lag, 和条件分布的变化。
% 为了解决这个问题，我们使用稳定的简洁的格兰杰因果关系来建模这种复杂的domain-specific的依赖关系。这种简洁的格兰杰因果关系不但可以通过避免直接对齐数据值从而绕开offset问题，而且通过考虑不同time lag之间的因果图从而可以很好地刻画条件分布的变和不变的部分。
% 通过这样做，我们提供了一种端对端的时间序列预测迁移模型，它不仅仅能够跨领域学习格兰杰因果关系，而且很好地通过学习不同domain的依赖关系来预测多维时间序列，更能为预测结果提过一定的可解释性。我们进一步从理论上分析了基于格兰杰因果关系对齐的时间序列预测迁移模型的优越性，我们在生成数据集和真实数据集上验证了我们的方法。代码公布xxxx


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Time Series, Granger Causality, Transfer Learning
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% 起，科学的第一要义是泛化，在很多多维时间序列的应用也是如此，例如，人体血糖关系中，有关的血糖，胰岛素，胰高血糖素的关系应该在不同年龄，性别，人中都使用。领域自适应的目的就是利用source domain的数据在target domain上做预测，它的一个核心问题在于解决domain shift.
% 
% 承1，考虑到source domain和target domain中包含共享的可以用来预测的信息，一个常见的假设是covariate shift assumption，它假设p(x)是变化的，但是p(y|x)是固定的。基于这个假设，在非时序领域例如CV已经获得了巨大的成功，例如MMD，adversarial。不少研究者直接将这个假设拓展到时序领域，采用RNN之类的特征提取器。但是由于时间序列的依赖关系是很复杂,即使一阶马尔可夫依赖造成很大不同，所以在时间序列数据上p(y|x)也会随着domain变化,i.e. p_S(x_t+1|\phi(x_1,...x_t)) \neq p_T(x_t+1|\phi(x_1,...x_t))，因此直接在数据层面上对齐很难提取领域不变特征。
% 
% 
% 转，
% 
% 合，基于以上的intuition，我们提出了Granger Causality Alignment (GCA) approach for time-series domain adaptation by assuming the full graph of granger causality is stable across different domains.
% 我们GCA方法主要包含两个不同的挑战：
%1.如何学full graph不同summary相同的格兰杰因果结构。
%2.如何结合因果图产生简介的表达？
%3.如何结合因果图刻画不同domain数据的条件分布？
% 为了解决以上问题，我们首先将不同lag的因果图作为一个隐变量，然后使用在变分自动编码器和离散重参数化的框架上重构格兰杰因果结构。然后我们进一步设计了领域敏感的预测器来预测下一个时间步的结果。我们理论上证明了我们提出方法的优越性。我们的方法不仅仅可以学到不同domain上的格兰杰因果关系，而且在生成和真实数据集上超过了现有sota的方法。
Science is all about generalizations. Lots of scientific applications of multivariate time-series also need to transfer the experiment conclusion from the lab or the virtual environments to different real environments. For example, the physiological mechanism in the human body among ``Blood Glucose'', `` Glucagon'' and ``Insulin'' should be held among people of different ages, genders, and even races. Domain adaptation, which leverages the labeled source domain data to make a prediction in the new unlabeled target domain, aims to address the notorious \textit{domain shift} phenomenon and obtain a robust forecasting model.

Various methods have been raised for domain adaptation \cite{cai2019learning,Cai_Chen_Li_Chen_Zhang_Ye_Li_Yang_Zhang_2021,cai2021graph,hao2021semi,shui2021aggregating,li2021causal,pan2009survey,zhang2019bridging}, one of the most popular assumptions is covariate shift in which the marginal distribution $P(X)$ varies with domains while the conditional distribution $P(Y|X)$ is stable across domains. Based on this assumption, many methods based on MDD \cite{long2015learning,pan2010domain} or adversarial training \cite{cai2019learning,ganin2015unsupervised,xie2018learning}, which aim to extract the domain-invariant representation, have achieved great successes in non-time series data. Recently, many researchers expand these ideas to the field of time-series data \cite{Cai_Chen_Li_Chen_Zhang_Ye_Li_Yang_Zhang_2021,da2020remaining,purushotham2016variational} and straightforwardly replace the feature extractors with the recurrent neural networks \cite{chung2015recurrent}. However, because of the complex dependencies of time series, (i.e., even the trivial 1-order Markov dependence result in the dependence between any two-time steps), the conditional distribution of time-series data $P(Y_{t+1}|X_{1}, \cdots X_{t})$ varies sharply with different domains, which makes the conventional covariate shift assumption hard to be satisfied in time series data. Therefore, it is hard to directly extract the domain-invariant representation in the time-series data.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{./fig/motivation3}
	\caption{ The illustration of various associative structures and causal structures among ``Blood Glucose'' (B), ``Glucagon'' (G) and ``Insulin'' (I). (a)-(c) The illustration of suboptimal common substructures of associative structure alignment. (a) The source associative structure. (b) The target associative structure. (c) The aligned associative structure loss some domain-specific substructures since the time lags vary with different domains. (d)-(e) The illustration of various full graphs from different domains. (d) The full graph from the source domain. (e) The full graph from the target domain. (f) The source and the target domain share the same summary graph, because the physiological mechanism is held in each domain. }
	\label{fig:motivation1}
\end{figure}
% (i.e., $P_S(Y_{t+1}|X_{1}, \cdots X_{t}) \neq P_T(Y_{t+1}|X_{1}, \cdots X_{t})$)
In order to address this problem, recently, Cai et.al \cite{Cai_Chen_Li_Chen_Zhang_Ye_Li_Yang_Zhang_2021} firstly consider that the conditional distribution shift in time-series data is brought by the time lags and the offsets of time-series data and assume the associative structures are stable across the source and the target domains. 
% Aiming to devise a feature transformation mapping $\phi(\cdot)$ that make $P_S(Y_{t+1}|\phi(X_{1}, \cdots X_{t})) = P_T(Y_{t+1}|\phi(X_{1}, \cdots X_{t}))$, 
Therefore, they raise the Sparse Associative Structure Alignment method for time-series domain adaptation by aligning the associative structures. However, as shown in Figure \ref{fig:motivation1}, some the aligned associative structure might loss some important substructures since the time lags vary with different domains. 
% However, simply aligning the associative structures may lead to the suboptimal results. 
% First, as shown in Figure \ref{fig:motivation1} (d)-(e), some the aligned associative structure might loss some important substructures (i.e., the red lines) since the time lags vary with different domains.
% Second, though the associative structures aim to remove the redundant relationships, but lots of redundant information are still kept since the causality is not taken into consideration, which is show in Figure \ref{fig:motivation1} (d)-(e). 
Moreover, Cai et.al \cite{Cai_Chen_Li_Chen_Zhang_Ye_Li_Yang_Zhang_2021} implicitly assume the strength among variables of different domains are the same, but this assumption is too strong. 
% Given a comprehensible example, the gravity of a object is caused by its mass, but the gravity changes in different places. In this case, the causal strength varies with the different places.

% \begin{figure}
% 	\centering
% 	\includegraphics[width=\columnwidth]{./fig/motivation2}
% 	\caption{The illustration of various causal structures among ``Blood Glucose'' (B), ``Glucagon'' (G) and ``Insulin'' (I). (a) The full graph from the source domain. (b) The full graph from the target domain. (c) The source and the target domain share the same summary graph, because the physiological mechanism is held in each domain.}
% 	\label{fig:motivation2}
% \end{figure}

Based on the aforementioned arguments, it is not hard to find that the conditional distribution $P(Y_{t+1}|X_{1}, \cdots X_{t})$ usually changes across domains, which makes it a nontrivial task to extract the domain-invariant information. Fortunately, in the view of causal generation process \cite{scholkopf2021toward,zhang2013domain,zhang2015multi,zhang2020domain}, it is convenient to model how the conditional distribution changes across domains. According to the Independent Causal Mechanisms (\textbf{ICM}) Principle \cite{scholkopf2021toward}, we can find that the causal generative processes of any variables are independent, so the conditional distributions of each variables are independent, which provides a useful tool to model the changeability of conditional distributions. This motivation have been pursued to address domain adaptation problem for static data, but it is hard to apply it to time-series domain adaptation, because the time series data from different domains usually generated by different causal mechanism and limited labeled target domain data. In fact, the time-series data from different domains are further controlled by domain-specific full graph \cite{lawrence2021data,peters2017elements} \footnote{The full graph is a DAG whose nodes represent the variables at each point in time, which denotes that the future values cannot be the cause of past values.} and causal strength. Fortunately, in time-series data, though the full graphs vary across domains, the summary graphs \footnote{The summary graph is a ``collapsed'' version of the full graph.} are usually stable. Figure \ref{fig:motivation2} is a simple toy example of physiological mechanism, (a) and (b) denote the different individual physiological mechanism that unroll by timestamps, but the summary graph, which denote the domain-invariant universal physiological mechanism. Therefore, it make it possible to model how the conditional distributions change across domains with the help of stable summary graph and labeled source domain data as well as few labeled target domain data.

Following this intuition, we propose the \textit{\textbf{G}ranger} \textit{\textbf{C}ausality} \textit{\textbf{A}lignment} (\textbf{GCA} in short) approach for semi-supervised time-series domain adaptation by assuming that the summary graph of \textit{Granger Causality} is stable across domains. The proposed \textbf{GCA} method mainly composes of the following three challenges: (1) How to discovery the domain-specific full graph of Granger Causality? (2) How to generate the compact representation for time-series forecasting with the help of Granger Causal structure? (3)How to model different conditional distributions with the help of Granger Causal structure?
In order to answer these question, we first take the Granger Causality with different lags as the latent variables, and then we reconstruct the Grange causal structure under the framework of variational autoencoder. We further devise a domain-sensitive decoder to forecast forecast the future values. We not only theoretically analyze the advances of the \textbf{GCA} method but also outperforms the state-of-the-art domain adaptation methods for time-series data on the synthetic and real-world data.
% 存在问题：
% 1. 格兰杰因果出现得有点突然
% 2. 应该强调时序的因果结构
% 3. 强调时序的可迁移性质
% 4. 推出SASA仅仅是一个特例，再提及SASA的不足

\section{Related Works}\label{related_works}
In this section, we mainly focus on the existing techniques on domain adaptation on non-time series data, time series domain adaptation and Granger Causality.

\subsection{Domain Adaptation on Non-Time Series Data} Domain Adaptation \cite{pan2009survey,zhang2019bridging,ganin2015unsupervised,xie2018learning,long2015learning,zhang2013domain}, which leverages the labeled source domain data and limited or no labeled target domain data to make prediction in the target domain, have application in various fields \cite{hao2021semi,shui2021aggregating,li2021causal,cai2021graph}. Most of the approaches of domain adaptation follow the covariate shift assumption and aim to extra the domain-invariant representation. Technologically, these methods can be grouped into the Maximum Mean Discrepancy based methods and the adversarial training methods. 

In the view of causal generation process, some researchers find that the conditional distribution $P(Y|X)$ usually changes and the covariate shift assumption does not hold, so they address this limitations from the causal view. Kun et.al \cite{zhang2013domain, zhang2015multi} address this problem by assuming only $P(Y)$ or $P(X|Y)$ change and raise target shift and conditional shift. 
Cai et.al \cite{cai2019learning,cai2021graph} are motivated by the causal generation process and raise disentangled semantic representation for unsupervised domain adaptation.Recently, Kun et al. \cite{zhang2020domain} take domain adaptation as the problem of graphical model inference and directly model the changes of conditional distribution. 
In this paper, we focus on domain adaptation on time-series forecasting, which is more challenging because of the complicated conditional dependencies among time-stamps.

\subsection{Domain Adaptation on Time-Series Data}
Time-series data is another type of common dataset and there are several researches about it. Recently, increasing attention is paid to the domain adaptation on time-series data. Da Costa et al. \cite{da2020remaining} straightforwardly extend the idea of domain adaptation for non-time series data and leverage the RNN as the feature extractor to extract the domain-invariant representation. Purushotham et al. \cite{purushotham2016variational} further improve it by using the variational recurrent neural network \cite{chung2015recurrent}. However, the aforementioned method can not well extract the domain-invariant information because of the complicate dependency between timestamps. Recently, Cai et al. \cite{Cai_Chen_Li_Chen_Zhang_Ye_Li_Yang_Zhang_2021} consider that the sparse associate structure of variables are stable across domains, so they propose the sparse associative structure alignment methods for adaptative time series classification and regression task. Though achieving better performance, the sparse associate structure alignment method will not only miss the domain-specific substructures (the domain-invariant substructures are led by the time-lags.) but also be swamped with the variant strength of association. In order to address these problems, we assume that the summary graph of Granger causality is stable across domains and model how the conditional distribution changes.

\subsection{Granger Causality}
Granger Causality \cite{diks2006new, granger1969investigating,Seth:2007, marcinkevivcs2021interpretable,9376668,lowe2020amortized}, which is a set of directed dependencies among multivariate time-series data, is widely used to determine which the past of an univariate time-series data aids in predicting the future evolution of another univariate time-series. Inferin Granger Causality is a traditional research problem and have been applied in several fields \cite{runge2019inferring,chiou2008economic,seth2015granger}. One of the most classical methods for inferring Granger Causalitay is the vector autoregressive (\textbf{VAR}) model \cite{10.1145/1557019.1557085, wei2006time}, which uses the linear time-lag effect as well as some sparsity regularizer like the Lasso \cite{tibshirani1996regression} or the group lasso \cite{yuan2006model}. With the quick growth of the computation power, more Granger Causality inference methods which borrow the expressive power of neural networks have been proposed. Tank et al. \cite{9376668} devise a neural networks based auturegressive model and apply the sparsity penalties on the weights of neural networks. \cite{marcinkevivcs2021interpretable} are motivated by the interpretability of self-explaining neural networks and propose the generalised vector autoregression model to detect the signs of Granger Causality. In this paper, we are motivated by the generation process of time-series data and take the Granger-causal structure as the latent vclearariables. We combine the variational inference framework into the vector autoregression and further used it for semi-supervised domain adaptation of time-series forecasting.


\section{Preliminary}\label{preliminary}
In order to provide a better comprehension of this manuscript, we begin with the brief introduction of Granger Causality as well as the full graph and the summary graph. Then we give the problem definition of semi-supervised domain adaptation for time-series forecasting. 

\subsection{Granger Causality}
Granger causality \cite{diks2006new, granger1969investigating,Seth:2007, marcinkevivcs2021interpretable,9376668,lowe2020amortized,lutkepohl2005new} is a statistic concept of causality that determines where the past of the time-series data aids in predicting the future evolution of another time-series. In this paper, we follow the definition of Granger Causality in \cite{9376668}. 

We first let the Granger Causality follows the structural equation model shown in Equation (\ref{equ:granger_causality})

\begin{equation}\label{equ:granger_causality}
    \bm{z}_{t+1}^i:=g_i\left(\bm{z}^1_{1:t}, \cdots \bm{z}^m_{1:t}, \cdots,\bm{z}^M_{1:t}\right) + \epsilon^i_t ,
\end{equation}

in which $\bm{z}^m_{1:t}=\{z_1^m, z_2^m, \cdots, z_t^m\}$; $\epsilon^i_t$ denotes the additive noise term;and $g_i(\cdot)$ denote the any nonlinear functions. Intuitively, Equation (\ref{equ:granger_causality}) specify that how the future variable $\bm{z}^i$ relies on the past values of $\bm{z}$. Based on the aforementioned definition, Equation (\ref{not_granger_cause}) shows that how $\bm{z}^m$ does not Granger-cause $\bm{z}^i$.
\begin{equation}\label{not_granger_cause}
    g_i\left(\bm{z}^1_{1:t}, \cdots \bm{z}^m_{1:t}, \cdots,\bm{z}^M_{1:t}\right) \neq g_i\left(\bm{z}^1_{1:t}, \cdots \bm{z'}^m_{1:t}, \cdots,\bm{z}^M_{1:t}\right), 
\end{equation}

where $\bm{z'}^m_{1:t} \neq \bm{z}^m_{1:t}$.

\subsection{Full Graph and Summary Graph}
In the field of causal discovery from time-series data, there are two kinds of causal graph: the Full Time Graph and the Summary Graph \cite{lawrence2021data,peters2017elements}. In this paper, we follow the definition in \cite{peters2017elements}. The full time graph, which is shown in Figure \ref{fig:motivation1} (d) and (e),  is the \textbf{DAG} having $z_t^m$ as node and the summary graph, which is shown in Figure \ref{fig:motivation1} (f), is the \textbf{DAG} that contains directed edge from $z^m$ to $z^i$ whenever there is an arrow from  $z_t^m$ to  $z_t^i$. Intuitively, the full time graph represent the variables at each timestamps and the summary graph is a ``collapsed'' version of the full graph.

\subsection{Problem Definition}
In this paper, we focus on the problem of semi-supervised domain adaptation for time-series forecasting. We first let $\bm{x}=\{\bm{z}_1, \bm{z}_2, \cdots, \bm{z}_t, \cdots, \bm{z}_T\}$ denote a input multivariate time-series sample with $T$ timestamps, where $\bm{z}_t = \{\bm{z}_t^1, \bm{z}_t^2, \cdots, \bm{z}_t^{M}\}$ contains $M$ types of variables, and the output is to predict the future sequence $\bm{y}=\{\bm{z}_{T+1}, \bm{z}_{T+2}, \cdots, \bm{z}_{T+\tau}\}$. Note that the predicted sequence can be a subset of the $M$ variables. Then we assume that $P_S(\bm{x}, \bm{y})$ and $P_T(\bm{x}, \bm{y})$ are the different joint distributions from the source and the target domain but are generated from the similar Granger-causal mechanism. Note that we further assume that the summary graph of the Granger causality is shared but the full graph of the Granger causality varies with domains. $(\mathcal{X}_S,\mathcal{Y}_S)$ and $(\mathcal{X}_T, \mathcal{Y}_T)$, which are sampled from $P_S(\bm{x}, \bm{y})$ and $P_T(\bm{x}, \bm{y})$ respectively, denote the source and target domain dataset. We further assume that each source domain sample $\bm{x}_S$ comes with $\bm{y}_S$, while there are very few labeled samples in the target domain. Our goal is to devise a predictor that can predict the future time-time series $\bm{y}_T$ from the target domain given the previous observed time-series $\bm{x}_T$.

\begin{figure}
	\centering
	\includegraphics[width=0.8\columnwidth]{./fig/model1}
	\caption{The illustration of how time-series data iteratively generate via Granger-causal structures with a maximum lag of two. (The parent of $\bm{x}_{t-2}$ are omissive due to the limited space.) $A_1, A_2$ is the substructures of full time graph with lag of one and two. The Granger-causal structures are stable in all the timestamps. $\bm{x}$ is the observed data of different timestamps.}
	\label{fig:generation}
\end{figure}

\begin{figure*}\label{fig:model}
	\centering
	\includegraphics[width=2\columnwidth]{./fig/model2}
	\caption{The framework of the proposed Granger Causality Alignment (GCA) model. (a) The recurrent Granger-causal structures of different lags reconstruction. $\alpha$ denotes the structure domain latent variables that are used to reconstruct the domain-specific Granger Causality. (b) The domain-sensitive Granger Causality inference process that aims to model the domain-specific conditional shift under the Granger Causality. $\beta$ is the conditional distribution latent variables. (c) The summary graph of Granger Causality alignment between the source and the target domain.}
	\label{fig:model}
\end{figure*}

\section{Granger Causality Alignment Model}\label{model}

\subsection{Model Overview}
In order to address the aforementioned task, the following key obstacles need to be cleared away. (1) How to inference the full time graphs of Granger Causality whose summary graph is stable across domains? (2) How to model the domain-specific conditional distributions among timestamps? 

Regarding the first question, we start from the causal generation process of time series data as show in Figure \ref{fig:generation}. Given the full time graph of Granger Causality $A$, we split $A$ into $A_1, A_2, \cdots, A_T$ according to different time lag, i.e. $A_1$ denote the substructure of lag of 1. It is reasonable to split the Granger Causality according to different lags since the each causal mechanism is independent. Given the observed data $\bm{x}_t$ from $t-$th step, it is respectively generated from the $\bm{x}_{t-1}$ and $\bm{x}_{t-2}$ under the substructure of Granger-causal full time graph whose lag is one and two (We assume the maximum lag is two.). And $\bm{x}_{t-1}$ and $\bm{x}_{t-2}$ are iteratively generated from the previous observed data under the same Granger Causality. 

Regarding the second question, based on the aforementioned data generation process, we propose the Granger Causality Alignment approach by first reconstructing the Granger-causal full graph and then respectively modeling the conditional distribution of the source and target domain. The key structure of the proposed method is shown in Figure. \ref{fig:model}.

As shown in the upper part of Figure. \ref{fig:model}, we extend the variational autoencoder for categorical latent variables to the Recurrent Granger Causality Variational Autoencoder (RGC-VAE), which is used to recurrently reconstruct the full time graph of different lags. Since the full time graphs vary with domain, we leverage the structure domain latent variables $\alpha$ to learning the domain-specific substructures. We further devise a ``Domain-sensitive Granger Causality Inference'' architecture which combines the Granger-causal structures, the previous observed data and the conditional distribution latent variables to model how the condition distribution changes across different domains. Finally, we employ the summary graph alignment restriction to make the summary graph from the source and the target domain be as similar as possible.

\subsection{Recurrent Variational Inference Framework for Reconstructing Granger Causality}
In the light of the power of the power of variational autoencoder (VAE) \cite{kingma2013auto,jang2016categorical} in reconstructuring the latent variables whose distribution can be restricted by the Kullback–Leibler divergence, we take the Granger-causal structures as the latent variables and propose an extension of VAE, namely, Recurrent Variational Autoencoder (R-VAE), to iteratively reconstruct the Granger-causal structures of different lags. Based on the data generation process shown in Figure \ref{fig:generation}, we derive the evidence lower bound (ELBO). Essentially, assuming the maximum lag is $k$, the logarithm of joint likelihood $lnP(\bm{z}_t|\bm{z}_{t-1}, \bm{z}_{t-2}, \cdots, \bm{z}_{t-k})$ can be written as follows:
\begin{equation}
\small
    \begin{split}
        lnP(\bm{z}_t&|\bm{z}_{t-1}, \bm{z}_{t-2}, \cdots, \bm{z}_{t-k}) = \mathcal{L}_{ELBO} +  \sum_{j=1}^k{D_{KL}(Q_j||P_j})\\
        &Q_j = Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\\
        &P_j=P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})
    \end{split}
\end{equation}
in which the second and term are the KL divergence between the approximate distribution and the true posterior. And $\mathcal{L}_{ELBO}$ is the variational lower bound, which can be further derived as follows: (More details are shown in Appendix \ref{ELBO}.)
\begin{equation}\label{elbo}
    \begin{split}
        &\mathcal{L}_{ELBO}=\mathbb{E}_{Q_1}\cdots \mathbb{E}_{Q_k}\left[\ln P(\bm{z}_t|\bm{z}_{t-1}, \cdots, \bm{z}_{t-k},A_1, \cdots, A_k)\right] \\-& D_{KL}(Q_1||P_1)-\sum_{j=2}^k \mathbb{E}_{Q_j}D_{KL}(Q_j||P_j),
    \end{split}
\end{equation}
in whic $Q_j = Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})$ is the Granger-causal substructure with the lag $j$ and assumed to follow the Bernoulli distribution. Similar to variational inference for categorical latent variables \cite{jang2016categorical}, we use Gumbel-Softmax trick to estimate the Granger-causal structures.

According to Equation (\ref{elbo}), we also find that the process of reconstruct the Granger-causal substructures with different lags is recurrently. In detail, we must first reconstruct  $A_1$, the Granger-causal structures whose lag is 1 by $D_{KL}(Q_1||P_1)$ then we can reconstruct the Granger-causal structures whose lag is 2 since $Q_2=Q(A_2|A_1,\bm{z}_{t-1},\cdots,\bm{z}_{t-k})$ is condition on $A_1$. And the other substructures follow the same rule.

Technically, for $Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_1,\cdots,\bm{z}_{t-k})$, we implement it the with MLP and the categorical reparameterization trick. In details, we use Equation  (\ref{encoder_imp_1}) as the universal approximator of $Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_1,\cdots,\bm{z}_{t-k})$ to generate $A_j$ from the previous observed data.
\begin{equation}\label{encoder_imp_1}
    A_j=f_j(A_1,\cdots,A_{j-1},\bm{z}_1,\cdots,\bm{z}_{t-k}).
\end{equation}
However, the full time graph may vary with different domains. So we further introduce the structure domain latent variables $\alpha \in \mathbb{R}^{d_{\alpha}}$ to assist the model to reconstruct the domain-specific module of the full time graph. Therefore, we extend Equation (\ref{encoder_imp_1}) to Equation (\ref{encoder_imp_2})
\begin{equation}\label{encoder_imp_2}
    A_j=f_j(A_1,\cdots,A_{j-1},\bm{z}_1,\cdots,\bm{z}_{t-k},\alpha)
\end{equation}

\subsection{Domain-sensitive Granger Causality Inference}
Since the reason that we aim to model the changeability of the conditional distribution is to predict the future value based on the Granger Causality, we further devise the ``Domain-Sensitive Granger Causality Inference'' architecture, which is shown in the bottom-left side of Figure \ref{fig:model}.

The Granger Causality inference process can be separated into two steps: the intro-lag step and the inter-lag step. In the intro-lag step, we aim to calculate the effectiveness of each time lag. In details, given the $j$-th lag Granger-causal full time graph, we first use it to mask the input at $t-j$ timestamp, and then use MLP $g(\bm{z}\odot A_jA_j)$ to calculate $\widetilde{\bm{z}_{t;1}}$, the effectiveness of $A_j$. Given the same structure, the strength still varies with different domain. In order to address this problem, we use the similar method and introduce the conditional distribution latent variables $\beta \in \mathbb{R}^{d_{\beta}}$ to model the different strength of structures. As a result, the function we used to calculate the effectiveness should be $g(\bm{z}\odot A_jA_j,\beta)$.

In the inter-lag step, we aim to aggregate the effectiveness of all the time lags and predict the future data. In detail, we simply use another MLP $G(\widetilde{\bm{z}_{t;1}}, \widetilde{\bm{z}_{t;2}}, \cdots, \widetilde{\bm{z}_{t;k}})$ to predict the future value.

\subsection{Summary Graph Alignment Regularization}
Since the the labeled data from target domain are limited, so it is hard to reconstruct the Granger-causal structure and further exactly predict the future values. Fortunately, the summary graphs are usually stable across domains, so it is reasonable to assume that the summary graphs from the source and the target domain are the same. Motivated by this intuition, we further devise the simple but effective summary graphs Alignment restriction between the source structures and the target ones, which is shown in Equation (\ref{summary_simi}).
\begin{equation}\label{summary_simi}
    \mathcal{L}_r = \frac{1}{M*M}|\frac{1}{k}\sum_{j=1}^k A_j^S-\frac{1}{k}\sum_{j=1}^k A_j^T|,
\end{equation}
in which $A_j^S$ and $A_j^T$ are the structures with lag $j$. With the help of the summary graph alignment regularization, the information from the source domain can be transferred into the target domain.

\subsection{Training and Inference}
By reconstructing the Granger Causality and transferring the source information to the target domain and further predict the future value according to different domain, we summarize the model as follows.

The labeled data from the source domain $(\bm{x}, \bm{y})\in (\mathcal{X}_S, \mathcal{Y}_S)$ and the few labeled data from the target domain $(\bm{x}, \bm{y})\in (\mathcal{X}_T, \mathcal{Y}_T)$ are respectively used as the input of the evident lower bound of the source and the target domain. And we further use all the source and target domain data for summary graph alignment regularization.

Our method is a autoregressive model that need to take all the predicted time-series into the loss function. If the task is to predict an univariate $\bm{z}_j$, it is easy to suffer from suboptimization, since the model might optimize the loss of other variables. Therefore, we further add an extra strengthen loss. We formulate the extra optimization term follow:
\begin{equation}
    \mathcal{L}_e = MSE(\widetilde{\bm{z}_m}, \bm{z}_m), 
\end{equation}
in which $\widetilde{\bm{z}_m}$ is the predicted time-series and $\bm{z}_m$ is the label; And $MSE$ means the mean square error. Under the above objective function, the total loss of the GCA method can be summary as the follow equation:

\begin{equation}
    \begin{split}
        \mathcal{L}_{total}&=\mathcal{L}_{ELBO}^S + \mathcal{L}_{ELBO}^T + \gamma\mathcal{L}_r + \delta\mathcal{L}_d + \lambda \mathcal{L}_e,
        \\
        \mathcal{L}_d&=(\frac{1}{M*M}||\frac{1}{k}\sum_{j=1}^k A_j^S||_1 + (\frac{1}{M*M}||\frac{1}{k}\sum_{j=1}^k A_j^T||_1))
    \end{split}
\end{equation}
in which $\mathcal{L}_d$ is the sparsity-inducing penalty term, and $\gamma, \delta$ and $\lambda$ are the hyper-parameters. 

In the inference phase, we first reconstruct the full time graphs of different lags via Equation (\ref{encoder_imp_2}), and then we use the full time graphs and the observed value to predict the future value. 

\section{Theoretical Insights}
In this section, we will give the theoretical guarantees for semi-supervised domain adaptation time-series forecasting. The detail of the proofs can be found in Appendices B.

There are several works about the generalization theory of domain adaptation \cite{mohri2018foundations,zhang2019bridging,mansour2009domain,cortes2011domain,ben2007analysis}. In this paper, we follow the theoretical framework of Mohri et al. \cite{mohri2018foundations}. First, we give a introduction of the discrepancy distance for measuring the distribution difference of time-series data and then further derive the upper bound for semi-supervised domain adaptation time-series forecasting. We further give a insightful analysis of the purposed method.

As for the time-series forecasting task, we first consider the the loss function $L: \bm{y} \times \bm{y} \rightarrow \mathbb{R}_+$ like \textit{MSE}. For any two hypothesis $h, h' \in \mathcal{H}$ and any data distribution $\mathcal{D}$ over $\bm{x}$, we denote by $\mathcal{L}_{\mathcal{D}}(h, h')$ the expected loss of $h(\bm{x})$ and $h'(\bm{x})$, which is shown as follow:
\begin{equation}
    \mathcal{L}_{\mathcal{D}}(h, h')=\mathbb{E}_{\bm{x}\sim \mathcal{D}}\left[L\left(h(\bm{x}), h'(\bm{x})\right)\right],
\end{equation}
and the \textit{empirical} loss of $h(\bm{x})$ and $h'(\bm{x})$ is :
\begin{equation}
    \mathcal{L}_{\widehat{\mathcal{D}}}(h, h')=\frac{1}{n}\sum_{i=1}^n\left[L\left(h(\bm{x}_{(i)}), h'(\bm{x}_{(i)})\right)\right].
\end{equation}, 
in which $n$ is the data size of $\widehat{\mathcal{D}}$.
With a little abuse of notations, we further let $ \mathcal{L}_{\mathcal{S}}(h, h')$ and $ \mathcal{L}_{\mathcal{T}}(h, h')$ be the expected loss of the source and the target domain.

In order to theoretically analyze the performance of the domain adaptation algorithm, one of the most important mission is to choose a distribution discrepancy measurement. In this paper , we measure the distance between the source and the target domain by using \textit{Discrepancy Distance}, which measure the difference of the expected losses between the source and the target domain by using the supremum of $|\mathcal{L}_S(h, h')-\mathcal{L}_T(h,h')|$. The definition of \textit{Discrepancy Distance} is shown as follow:
\begin{definition}\label{def:dd}
(\textbf{Discrepancy Distance}).Given a hypothesis set $\mathcal{H}$ and the loss function $L$, the \textit{Discrepancy Distance} disc between the source and the target domain distribution $S,T$ over $\bm{x}$ is defined by:
\begin{equation}
    disc(S,T)\triangleq\sup_{h,h'\in\mathcal{H}}|\mathcal{L}_S(h, h')-\mathcal{L}_T(h,h')|.
\end{equation}
Similarly, the empirical discrepancy distance is:
\begin{equation}
    disc(\widehat{S},\widehat{T})\triangleq\sup_{h,h'\in\mathcal{H}}|\mathcal{L}_{\widehat{S}}(h, h')-\mathcal{L}_{\widehat{T}}(h,h')|.
\end{equation}
\end{definition}
It is not hard to find that the aforementioned measurement is symmetrical and in accord with the triangle inequality for any loss function for time-series forecasting. However, it is noted that this measurement is not a \textit{distance} because $disc(S,T)=0$ does not mean $S=t$.

Before introducing the generalization bound of regression, we first give the definition of \textit{Rademacher Complexity}, which is shown as follow:
\begin{definition}
\textbf{Rademacher Complexity.} Let $\mathcal{H}$ be the set of real-value functions defined over a set $X$. Given a dataset $\mathcal{X}$ whose size is $m$ the empirical Rademacher Complexity of $\mathcal{H}$ is defined as follow:
\begin{equation}
    \widehat{\mathfrak{R}}_{\mathcal{X}}(H)=\frac{2}{m}\mathbb{E}_{\sigma}\left[\sup_{h\in \mathcal{H}}\left.|\sum_{i=1}^{m}\sigma_i h(\bm{x}_i)|\right|\mathcal{X}=(\bm{x}_1, \cdots,\bm{x}_m)\right],
\end{equation}
in which $\sigma_=(\sigma_1, \cdots, \sigma_m)$ are independent uniform random variables taking values in $\{-1, +1\}$. And the \textit{Rademacher Complexity} of a hypothesis set $\mathcal{H}$ is defined as the expectation of $\mathfrak{R}_{\mathcal{X}}(H)$ over all the dataset of size $m$, which is shown as follow:
\begin{equation}
    \mathfrak{R}_{m}(H)=\mathbb{E}_S\left[\left.\widehat{\mathfrak{R}}_{\mathcal{X}}(H)\right|\left|S\right|=m\right].
\end{equation}
\end{definition}

\begin{lemma}\label{def:rade_bound}
\textbf{(Rademacher Complexity Regression Bounds)} Let $L: \bm{y} \times \bm{y} \rightarrow \mathbb{R}_+$ be a non-negative loss upper bounded by $M>0$ ($L(\bm{y}, \bm{y'})\leq M$ for all $\bm{y},\bm{y'} \in \mathcal{Y}$) and we denote by $f_{\mathcal{D}}: \mathcal{X}\rightarrow\mathcal{Y}$ the labeling function of domain $\mathcal{D}$. Given any fixed $\bm{y}' \in \mathcal{Y}, \bm{y}\rightarrow L(\bm{y},\bm{y}')$ is $\mu-$Lipschitz for some $\mu>0$, and the \textit{Redemacher Complexity Regression Bounds} are shown as follow:
\begin{equation}
\begin{split}
        % \mathbb{E}_{\mathcal{X}, \mathcal{Y}\sim \mathcal{D}}\left[L(h(\mathcal{X}), \mathcal{Y})\right]\leq
        \mathcal{L}_{\mathcal{D}}(h, f_{\mathcal{D}})\leq\frac{1}{m}\sum_{i=1}^m L(h(\bm{x}_i), \bm{y}_i) +  2\mu\mathfrak{R}_m(\mathcal{H})+M\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}\\
        % \mathbb{E}_{\mathcal{X}, \mathcal{Y}\sim \mathcal{D}}\left[L(h(\mathcal{X}), \mathcal{Y})\right]\leq
        \mathcal{L}_{\mathcal{D}}(h, f_{\mathcal{D}})\leq\frac{1}{m}\sum_{i=1}^m L(h(\bm{x}_i), \bm{y}_i) +  2\mu\mathfrak{R}_{\mathcal{X}}(\mathcal{H})+3M\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}
\end{split}
\end{equation}
\end{lemma}

Based on the aforementioned definition, we proposed the generalization bound of semi-supervised domain adaptation for time-series forecasting.
\begin{theorem}
\label{the:b1}
For any predictor $h \in \mathcal{H}$, we let $h^*$ be the ideal predictor that simultaneously obtain the minimal error on the source and the target domain,
\begin{equation}
    h^*\triangleq\mathop{\arg\min}_{h\in\mathcal{H}}\{\mathcal{L}_{\mathcal{S}}(h,f_S)+\mathcal{L}_{\mathcal{T}}(h,f_T)\},
\end{equation}
then we can obtain the following inequation:
\begin{equation}
    \mathcal{L}_{T}(h,f_T)\leq\mathcal{L}_S(h,f_S) + disc(S,T) +\mathcal{L}_{\widehat{T}}(h,f_T) - \mathcal{L}_{\widehat{T}}(h,h^*) + \lambda
\end{equation}
where $\lambda=\lambda(\mathcal{H},S,T)$ is independent of $h$ and can be considered as a constant. 
\begin{proof}
\begin{equation}
\begin{split}
        &\quad\mathcal{L}_{T}(h,f_T)=\mathcal{L}_{T}(h,f_T) + \mathcal{L}_{S}(h,f_S) - \mathcal{L}_{S}(h,f_S)\\&\leq\mathcal{L}_S(h,f_S) + \mathcal{L}_{T}(h, h^*) + \mathcal{L}_{T}(f_T, h^*)+\mathcal{L}_S(h^*,f_S)\\&\quad-  \mathcal{L}_S(h^*,h)+\mathcal{L}_{\widehat{T}}(h, f_T)-\mathcal{L}_{\widehat{T}}(h, f_T)\\&\leq\mathcal{L}_S(h,f_S) + \left(\mathcal{L}_{T}(h,h^*)-\mathcal{L}_{S}(h,h^*)\right)+\mathcal{L}_{\widehat{T}}(h,f_T) \\&\quad- \mathcal{L}_{\widehat{T}}(h,h^*) + \mathcal{L}_T(h^*,f_T) + \mathcal{L}_S(h^*,f_S) + \mathcal{L}_{\widehat{T}}(h^*,f_T)\\&\leq\mathcal{L}_S(h,f_S) + \sup_{h,h'\in \mathcal{H}}\left|\mathcal{L}_{T}(h,h')-\mathcal{L}_{S}(h,h')\right|+\mathcal{L}_{\widehat{T}}(h,f_T) \\&\quad- \mathcal{L}_{\widehat{T}}(h,h^*) + \mathcal{L}_T(h^*,f_T) + \mathcal{L}_S(h^*,f_S) + \mathcal{L}_{\widehat{T}}(h^*,f_T)\\&=\mathcal{L}_S(h,f_S) + disc(S,T) +\mathcal{L}_{\widehat{T}}(h,f_T) - \mathcal{L}_{\widehat{T}}(h,h^*) + \lambda
\end{split}
\end{equation}
where $\lambda=\mathcal{L}_T(h^*,f_T) + \mathcal{L}_S(h^*,f_S) + \mathcal{L}_{\widehat{T}}(h^*,f_T)$.
\end{proof}
\end{theorem}

\begin{theorem}\textbf{(Generalization  Bound for Semi-Supervised Time-Series Forecasting.)}
For any $\delta>0$ and any predictor $h\in\mathcal{H}$, with probaility $1-\delta$, we have:
\begin{equation}
\begin{split}
    \mathcal{L}_T(h,f_T)\leq&\mathcal{L}_{\widehat{S}}(h,f_S)+\mathcal{L}_{\widehat{T}}(h,f_T)+disc(S,T)+disc(\widehat{S},\widehat{T})\\&+2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda
\end{split}
\end{equation}
\begin{proof}
Our goal is to proof that $\mathcal{L}_S(h.f_S) - \mathcal{L}_{\widehat{S}}(h,f_S)$ less than a non-negative value. Based on Definition \ref{def:rade_bound}, we have:

\begin{equation}
\nonumber
\begin{split}
    \mathcal{L}_S(h,f_S) - \mathcal{L}_{\widehat{S}}(h,f_S)&\leq
\sup_{h\in\mathcal{H}}\left|\mathcal{L}_S(h,f_S)-\mathcal{L}_{\widehat{S}}(h,f_S)\right|\\&\leq2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}},
\end{split}
\end{equation}
where $m$ is the size of the source domain dataset and $\mathcal{L}_S$ is bounded by $M_S$.

Therefore, combining with Theorem \ref{the:b1} and Definition \ref{def:dd}, we can obtain:
\begin{equation}
\nonumber
\begin{split}
    \mathcal{L}_T(h,f_T)\leq& \mathcal{L}_{\widehat{S}}(h,f_S)+disc(S,T)+\mathcal{L}_{\widehat{T}}(h,f_T) \\&+\mathcal{L}_{\widehat{S}}(h,h^*)-\mathcal{L}_{\widehat{T}}(h,h^*)+2\mu\mathfrak{R}_m(\mathcal{H})\\&+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda\\\leq&\mathcal{L}_{\widehat{S}}(h,f_S)+\mathcal{L}_{\widehat{T}}(h,f_T)+disc(S,T)\\&+\sup_{h,h'\in\mathcal{H}}|\mathcal{L}_{\widehat{S}}(h,h^*)-\mathcal{L}_{\widehat{T}}(h,h^*)| \\&+2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda\\=&\mathcal{L}_{\widehat{S}}(h,f_S)+\mathcal{L}_{\widehat{T}}(h,f_T)+disc(S,T)+disc(\widehat{S},\widehat{T})\\&+2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda
\end{split}
\end{equation}
\end{proof}
\end{theorem}
According to the Theorem 
% \begin{theorem}
% (Genearlization Bound of Semi-Supervised Domain Adaptation for time-series Forecasting.) For any $h, h' \in \mathcal{H}$, 
% \end{theorem}

\section{Experiment}
\subsection{Dataset}
In this section, we give a brief introduction of the dataset that we used. For each dataset, we split it into the training set, validation set and the test set. For all the methods, we try five different random seeds and report the mean and variance. We choose the model with the best validation and evaluate the chosen model on the test set. All the code for simulation and prepossessing of the dataset will be released.

\subsubsection{Simulation Dataset}
In this part, we design a series of controlled experiment on the random causal structure with given sample size, variable size. we simulate three different dataset with the following nonlinear function.

\begin{equation}
\begin{split}
    \bm{z}_t=&A_1\cdot\left(\bm{z}_{t-1}+c\cdot Sin\left(\bm{z}_{t-1}\right)\right) +\cdots\\& +A_k\cdot\left(\bm{z}_{t-k}+c\cdot Sin\left(\bm{z}_{t-k}\right)\right) + \phi ,
\end{split}
\end{equation}
in which $c$ is the hyper-parameters of the nonlinear term; $\phi$ is the variance of Gaussian distributions; $A_j$ is the full time graph with $j$ lag. We further employ different sample interval for different domains. The details of dataset is shown in Table 1. And Figure 4 shows the example of the simulated data. Since the value range of this dataset varies with different cities, we use Z-score Normalization for each domain.

Note that we can use the simulated dataset for both time-series forecasting and cross-domain Granger-causal discovery tasks. Since it is very hard to achieve the ground truth Granger-causal structures, we prove the effectiveness of the proposed GCA method and show that the importance of Granger-causal inference.

\begin{table}
\caption{Different setting for the dataset of different domains.}
\centering
\begin{tabular}{c|ccc}
		\hline
		\small{Domain}  & Variance of Noise $\phi$ & Sample interva &c\\
		\hline
		\small{Domain 1}       	           & 1  & 1 & 0.02\\ 
		\small{Domain 2}              	   & 5 & 2 & 0.04\\
		\small{Domain 3}            	   & 10 & 3 & 0.06\\
		\hline
\end{tabular}
\label{tab:params}
\end{table}

\begin{table}
\caption{Features of Air Quality Forecast Dataset.}
	\centering
    \begin{tabular}{c|c}
		\hline
		Feature Type & Feature Name  \\
		\hline
		\multirow{5}{*}{Air quality} & PM10\\
		~ & PM2.5\\
        ~ & NO2 Concentration\\
        ~ & CO Concentration\\
        ~ & O3 Concentration\\
        ~ & SO2 Concentration\\
        \hline
        \multirow{6}*{Meteorology} & Weather\\
        ~ & Temperature\\
        ~ & Pressure\\
        ~ & Humidity\\
        ~ & Wind Speed\\
		\hline
	\end{tabular}
	\label{tab:air_features}
\end{table}

\subsubsection{Air Quality Forecasting Dataset}
The air quality forecast dataset \cite{10.1145/2783258.2788573} is collected in the Urban Air project \footnote{https://www.microsoft.com/en-us/research/project/urban-air/} from 2014/05/01 to 2015/04/30, which contains the air quality data, the meteorological data, and the weather forecast data, etc. This dataset contains four major Chinese cities: Beijing (B), Tianjin (T), Guangzhou (G) and Shenzhen (S). We employ the air quality data as well as meteorological data to predict the PM2.5. The feature of this data is shown in Table 2. We choose several air quality stations that contains a few missing value and take each city as a domain. We use this dataset because the air quality data is common and the sensors in the smart city systems usually contain complex and compact causality. Since the value range of this dataset varies with different cities, we use Z-score Normalization for each domain. 

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{./fig/data}
	\caption{The Sample of three domains, we can find that the data are totally different, but they share the same summary graph.}
	\label{fig:data}
\end{figure}

\subsubsection{Human Motion Capture Dataset}
We further show the effectiveness of our method on the human motion capture dataset. Human motion has been studied in many fields, e.g. Granger Causality Inference \cite{9376668,zhang2017causal}, the linear and the nonlinear dynamical systems \cite{fox2014joint}. Many researches use study the human motion capture dataset for Granger Causality since the relationship among joints of human is a natural causal structure. In this paper, we use the Human3.6M dataset \cite{ionescu2013human3} \footnote{http://vision.imar.ro/human3.6m/description.php}, one of the most popular benchmark for human motion prediction \cite{barsoum2018hp}, for cross-domain human motion prediction. This dataset contains 15 motions and we choose three of them as three different domains: ``Walking'', ``Greeting'' and ``Eating'', the example of this dataset are shown in Figure \ref{fig:human_motion}. We choose 9 primary sensors which record the three-dimensional coordinate and the dimension of processed dataset is 27.

\begin{figure}
\centering
\label{fig:human_motion}
\subfigure[Walking]{
\begin{minipage}[t]{0.30\linewidth}
\centering
\includegraphics[width=1in]{fig/walking}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[Eating]{
\begin{minipage}[t]{0.30\linewidth}
\centering
\includegraphics[width=1in]{fig/eating}
%\caption{fig2}
\end{minipage}%
}%
\subfigure[Greeting]{
\begin{minipage}[t]{0.30\linewidth}
\centering
\includegraphics[width=1in]{fig/greeting}
%\caption{fig2}
\end{minipage}
}
\centering
\caption{The visualized examples of a frame in the human motion capture dataset. (a), (b) and (c) respective denote ``Walking'', ``Eating'' and ``Greeting''.}
\end{figure}

\subsection{Baseline and Model Variants}
Baselines of domain adaptation for time-series forecasting. 
\begin{itemize}
  \item \textbf{LSTM\_S+T:} LSTM\_S+T only uses the labeled source domain data and the labeled target domain data to train a vanilla LSTM model and applies it to the target domain validation and test set.
%   \item \textbf{GCA\_T2T:} Since the proposed method is different from the previous method that is based on LSTM, so we further devise the GCA\_T2T baseline that is expected to provide the upper bound performance of our approach. As for the simulated dataset, we use the ground-truth Granger-Causality. As for the real-world dataset, we employ the inferred Granger-Causality.
  \item \textbf{R-DANN}: R-DANN \cite{da2020remaining} uses the domain adaptation architecture proposed in \cite{ganin2015unsupervised} with Gradient Reversal Layer on LSTM, which is a straightforward solution from time series domain adaptation.
  \item \textbf{RDC}: Deep domain confusion is a domain adaptation method proposed in \cite{tzeng2014deep} which minimizes the distance between the source and the target distribution by using Maximum Mean Discrepancy (MMD). Similar to the aforementioned R-DANN method, we also employ LSTM as the feature extractor for time-series data.
  \item \textbf{VRADA}. VRADA \cite{purushotham2016variational} is a time-series domain adaptation method which combines the Gradient Reversal Layer and VRNN \cite{chung2015recurrent}.
  \item \textbf{SASA}. \textbf{SASA} \cite{Cai_Chen_Li_Chen_Zhang_Ye_Li_Yang_Zhang_2021} is one of the state-of-the-art domain adaptation approaches for time-series data, that extracts and aligns the sparse associative structure.
\end{itemize}

In order to evaluate the modules of the proposed method, we devise the following model variants.
\begin{itemize}
  \item \textbf{GCA-r} In order to evaluate the effectiveness of the summary graph alignment regularization, we further devise a variant that remove the summary graph alignment regularization $\mathcal{L}_r$.
  \item \textbf{GCA-e} In order to evaluate the effectiveness of extra strengthen term, we further devise a variant that remove the strengthen term $\mathcal{L}_e$.
%   \item \textbf{GCA-s} In order to evaluate the effectiveness of domain-sensitive module, we remove the domain-sensitive module.
\end{itemize}

% Baselines of domain adaptation for cross-domain Granger Causality Inference.
% \begin{itemize}
%     \item \textbf{VAR.} The vector autoregressive model is the extension of the autoregressive (AR) model and is used to infer the Granger Causality.
%     \item \textbf{GVAR.} GVAR \cite{marcinkevivcs2021interpretable} infers the multivariate Granger causality under nonlinear dynamics based on an extension of self-explaining neural networks.
%     \item \textbf{cLSTM} and \textbf{cMLP}. The \textbf{cLSTM} and \textbf{cMLP} combine the structured MLP and LSTM with sparsity-inducing penalties on the weights. 
% \end{itemize}


% \subsection{Result}

\subsection{Result on Simulation Dataset}

The Mean Squared Error(MSE) and Mean Absolute Error (MAE) in the simulated dataset is shown in Table. 3. The proposed GCA method significantly outperforms the other baselines on all the tasks with a large room. It is worth mentioning that our Granger-Causality alignment method have a remarkable MSE promotion on most of the task (more than 10\% improvement compared with SASA). For some easy tasks like $1\rightarrow 2$ and $ 2 \rightarrow 1$, the proposed method achieves the largest improvement among all the tasks. For the other harder task like $3\rightarrow 2$ and $ 2 \rightarrow 3$, our method also achieve a very good result. As for the very hard tasks like $1\rightarrow 3$ and $ 3 \rightarrow 1$, in which the value ranges between the source and the target domain are very large, our method also obtain a comparable result. 

In order to study how the inferred Granger-Causality has influence on the performance of the model, we further apply the proposed method to test set under different accuracy of the Granger-Causality. In details, we evaluate the GCA on the test set every epochs and record the Area Under Precision-Recall Curve (AUPRC) of the inferred Granger-Causality. The experiment result is shown in Figure 6. According to the experiment result, we can learn the following lessons. (1) The proposed method can well discover the Granger-Causality among time-series data. (2) As the increasing of the accuracy of the Granger-Causality, the performance of our method increases. The proposed method achieve the best performance when the Granger-Causality is exactly discovered. This phenomenon proves that rationality of GCA. (3) Compared with the other baselines, the proposed GCA method enjoys faster convergence, this is because the GCA approach can avoid the sideeffect of superfluous variables.

\begin{figure}[htbp]
\centering
\label{fig:exp1}
\subfigure[AUPRC and RMSE of GCA.]{
\begin{minipage}[t]{\linewidth}
\centering
\includegraphics[width=\columnwidth]{fig/exp1}
%\caption{fig1}
\end{minipage}%
}%

\subfigure[Convergence of GCA and baselines.]{
\begin{minipage}[t]{\linewidth}
% \centering
\includegraphics[width=0.9\columnwidth]{fig/exp5}
%\caption{fig2}
\end{minipage}%
}%
\centering
\caption{(a) The convergence of the GCA method. We can find that the GCA method achieves the best performance when the Granger Causality is well reconstructed. (b) The convergence of all the methods, we can find that the proposed GCA method converge faster than the other methods. \textit{(best view in color.)}}
\end{figure}

\begin{table*}
% \renewcommand\arraystretch{1.2}
	\centering
	\label{tab:simulate_forecast}
	\caption{The MAE and MSE on simulated datasets for the baselines and the proposed method. The value presented are averages over 5 replicated with different random seeds. Standard deviation is in the subscript.}
	\begin{tabular}{c|c|c|c|c|c|c|c}
	    \hline
	    Task & Metric & GCA & SASA& VRADA& R-DANN& RDC& LSTM\_S+T\\
		\hline
		\multirow{2}*{$1\rightarrow2$} & MSE & $\bm{0.9232_{\pm 0.0004}}$ & $1.0001_{\pm 0.0026}$& $0.9763_{\pm 0.0009}$& $1.0148_{\pm 0.0037}$& $1.0122_{\pm 0.0033}$& $1.0078_{\pm 0.0059}$  \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7648_{\pm 0.0002}}$ & $0.7961_{\pm 0.0009}$& $0.7858_{\pm 0.0003}$& $0.8017_{\pm 0.0018}$& $0.8841_{\pm 0.0015}$& $0.7990_{\pm 0.0023}$ \\
		\hline
		\multirow{2}*{$1\rightarrow3$} & MSE & $\bm{0.8556_{\pm 0.0010}}$ & $0.8734_{\pm 0.0026}$& $0.9005_{\pm 0.0095}$& $0.8884_{\pm 0.0019}$& $0.8841_{\pm 0.0015}$& $0.8739_{\pm 0.0013}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7399_{\pm 0.0005}}$ & $0.7485_{\pm 0.0012}$& $0.7598_{\pm 0.0038}$ & $0.7539_{\pm 0.0005}$& $0.7527_{\pm 0.0007}$& $0.7489_{\pm 0.0007}$\\
		\hline
		\multirow{2}*{$2\rightarrow1$} & MSE & $\bm{0.8323_{\pm 0.0014}}$ & $0.9386_{\pm 0.0012}$& $0.9277_{\pm 0.0063}$& $0.9333_{\pm 0.0065}$& $0.9397_{\pm 0.0038}$& $0.9469_{\pm 0.0066}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7258_{\pm 0.0007}}$ & $0.7741_{\pm 0.0004}$& $0.7682_{\pm 0.0026}$& $0.7713_{\pm 0.0029}$& $0.7740_{\pm 0.0014}$& $0.7778_{\pm 0.0030}$\\
		\hline
		\multirow{2}*{$2\rightarrow3$} & MSE & $\bm{0.8570_{\pm 0.0019}}$ & $0.9084_{\pm 0.0029}$& $0.9229_{\pm 0.0043}$&$0.9136_{\pm 0.0010}$& $0.9125_{\pm 0.0017}$& $0.9178_{\pm 0.0045}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7493_{\pm 0.0008}}$ & $0.7625_{\pm 0.0014}$& $0.7688_{\pm 0.0018}$& $0.7649_{\pm 0.0008}$& $0.7640_{\pm 0.0009}$& $0.7669_{\pm 0.0023}$\\
		\hline
		\multirow{2}*{$3\rightarrow1$} & MSE & $\bm{0.8314_{\pm 0.0004}}$ & $0.8596_{\pm 0.0012}$& $0.9177_{\pm 0.0008}$& $0.8632_{\pm 0.0020}$& $0.8596_{\pm 0.0032}$& $0.8631_{\pm 0.0022}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7255_{\pm 0.0002}}$ & $0.7379_{\pm 0.0005}$& $0.7635_{\pm 0.0003}$& $0.7393_{\pm 0.0007}$& $0.7373_{\pm 0.0012}$& $0.7395_{\pm 0.0007}$\\
		\hline
		\multirow{2}*{$3\rightarrow2$} & MSE & $\bm{0.9208_{\pm 0.0009}}$ & $0.9685_{\pm 0.0008}$& $0.9753_{\pm 0.0007}$& $0.9752_{\pm 0.0052}$& $0.9710_{\pm 0.0019}$& $0.9759_{\pm 0.0042}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7634_{\pm 0.0004}}$ & $0.7829_{\pm 0.0004}$& $0.7855_{\pm 0.0002}$& $0.7859_{\pm 0.0020}$& $0.7841_{\pm 0.0004}$& $0.7863_{\pm 0.0018}$\\
		\hline
		\multirow{2}*{Average} & MSE & $\bm{0.8700_{\pm 0.0003}}$ & $0.9249_{\pm 0.0012}$& $0.9368_{\pm 0.0021}$& $0.9314_{\pm 0.0012}$& $0.9299_{\pm 0.0018}$& $0.9309_{\pm 0.0041}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.7433_{\pm 0.0002}}$ & $0.7670_{\pm 0.0005}$& $0.7719_{\pm 0.0021}$& $0.7695_{\pm 0.0004}$& $0.7688_{\pm 0.0006}$& $0.7693_{\pm 0.0018}$\\
		\hline
	\end{tabular}
\end{table*}

\begin{table*}
% \renewcommand\arraystretch{1.2}
	\label{tab:air}
	\centering
	\caption{The MAE and MSE on Air Quality Forecasting dataset for the baselines and the proposed method. The value presented are averages over 5 replicated with different random seeds. Standard deviation is in the subscript.}
	\begin{tabular}{c|c|c|c|c|c|c|c}
	    \hline
	    Task & Metric & GCA & SASA& VRADA& R-DANN& RDC& LSTM\_S+T\\
		\hline
		\multirow{2}*{$B\rightarrow T$} & MSE & $\bm{0.1732_{\pm 0.0206}}$ & 
		$0.1952_{\pm 0.0256}$& 
		$0.2536_{\pm 0.0145}$& 
		$0.1868_{\pm 0.0142}$& 
		$0.2199_{\pm 0.0650}$& 
		$0.1910_{\pm 0.0140}$  \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2615_{\pm 0.0099}}$ & $0.3037_{\pm 0.0267}$& 
		$0.3698_{\pm 0.0253}$& 
		$0.3036_{\pm 0.0134}$& 
		$0.3369_{\pm 0.0514}$& 
		$0.2975_{\pm 0.0113}$ \\
		\hline
		\multirow{2}*{$G\rightarrow T$} & MSE & $\bm{0.1497_{\pm 0.0128}}$ & 
		$0.1585_{\pm 0.0255}$& 
		$0.2365_{\pm 0.0129}$& 
		$0.1693_{\pm 0.0195}$& 
		$0.1420_{\pm 0.0152}$& 
		$0.1785_{\pm 0.0049}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2587_{\pm 0.0155}}$ & $0.2740_{\pm 0.0113}$& 
		$0.3544_{\pm 0.0032}$& 
		$0.2938_{\pm 0.0160}$& 
		$0.2675_{\pm 0.0153}$& 
		$0.3027_{\pm 0.0060}$\\
		\hline
		\multirow{2}*{$S\rightarrow T$} & MSE & $\bm{0.1667_{\pm 0.0040}}$ & 
		$0.2022_{\pm 0.0159}$& 
		$0.2662_{\pm 0.0100}$& 
		$0.2330_{\pm 0.0111}$& 
		$0.1877_{\pm 0.010}$& 
		$0.2058_{\pm 0.0042}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2596_{\pm 0.0067}}$ & $0.3130_{\pm 0.0155}$& 
		$0.4287_{\pm 0.0667}$& 
		$0.3834_{\pm 0.0243}$& 
		$0.3194_{\pm 0.0141}$& 
		$0.3195_{\pm 0.0070}$\\
		\hline
		\multirow{2}*{$T\rightarrow B$} & MSE & $\bm{0.2228_{\pm 0.0005}}$ & 
		$0.2366_{\pm 0.0120}$& 
		$0.3112_{\pm 0.0267}$&
		$0.3222_{\pm 0.0487}$& 
		$0.3182_{\pm 0.0732}$& 
		$0.2652_{\pm 0.0115}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2801_{\pm 0.0037}}$ & $0.2962_{\pm 0.0136}$& 
		$0.4007_{\pm 0.0284}$& 
		$0.3522_{\pm 0.0307}$& 
		$0.3580_{\pm 0.0393}$&
		$0.3243_{\pm 0.0064}$\\
		\hline
		\multirow{2}*{$G\rightarrow B$} & MSE & $\bm{0.2198_{\pm 0.0068}}$ &
		$0.2388_{\pm 0.0171}$& 
		$0.2414_{\pm 0.0422}$&
		$0.2706_{\pm 0.0277}$&
		$0.2276_{\pm 0.0137}$&
		$0.3010_{\pm 0.0111}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2757_{\pm 0.0281}}$ & $0.3151_{\pm 0.0213}$& 
		$0.3722_{\pm 0.0277}$&
		$0.3672_{\pm 0.0191}$&
		$0.3077_{\pm 0.0109}$& 
		$0.3854_{\pm 0.0155}$\\
		\hline
		\multirow{2}*{$S\rightarrow B$} & MSE & $\bm{0.2276_{\pm 0.0076}}$ &
		$0.2671_{\pm 0.0244}$& 
		$0.4222_{\pm 0.0159}$&
		$0.3017_{\pm 0.0184}$&
		$0.2744_{\pm 0.0204}$& 
		$0.2503_{\pm 0.0108}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2975_{\pm 0.0082}}$ & $0.3273_{\pm 0.0266}$&
		$0.4621_{\pm 0.0129}$&
		$0.3856_{\pm 0.0216}$&
		$0.3499_{\pm 0.0255}$&
		$0.3303_{\pm 0.0139}$\\
		\hline
		\multirow{2}*{$B\rightarrow G$} & MSE & $\bm{0.1853_{\pm 0.0149}}$ &
		$0.2281_{\pm 0.0269}$&
		$0.3135_{\pm 0.0682}$&
		$0.2964_{\pm 0.0376}$&
		$0.2566_{\pm 0.0242}$&
		$0.2452_{\pm 0.0158}$\\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2929_{\pm 0.0175}}$ & $0.3315_{\pm 0.0259}$&
		$0.4256_{\pm 0.0533}$& 
		$0.4147_{\pm 0.0336}$& 
		$0.3682_{\pm 0.0311}$&
		$0.3564_{\pm 0.0138}$\\
		\hline
		\multirow{2}*{$T\rightarrow G$} & MSE & $\bm{0.1831_{\pm 0.0123}}$ &
		$0.2225_{\pm 0.0183}$& 
		$0.2302_{\pm 0.0316}$&
		$0.1941_{\pm 0.0109}$&
		$0.1912_{\pm 0.0132}$&
		$0.1959_{\pm 0.0109}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.3000_{\pm 0.0069}}$ & $0.2892_{\pm 0.0213}$&
		$0.3643_{\pm 0.0299}$&
		$0.3259_{\pm 0.0129}$&
		$0.3101_{\pm 0.0122}$&
		$0.3069_{\pm 0.0099}$\\
		\hline
		\multirow{2}*{$S\rightarrow G$} & MSE & $\bm{0.1683_{\pm 0.0022}}$ &
		$0.2186_{\pm 0.0261}$&
		$0.4187_{\pm 0.0158}$&
		$0.2942_{\pm 0.0023}$&
		$0.2949_{\pm 0.0461}$&
		$0.1862_{\pm 0.0067}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2788_{\pm 0.0122}}$ & $0.3415_{\pm 0.0424}$&
		$0.5227_{\pm 0.0707}$&
		$0.4912_{\pm 0.0936}$&
		$0.4185_{\pm 0.0406}$&
		$0.3116_{\pm 0.0083}$\\
		\hline
		\multirow{2}*{$B\rightarrow S$} & MSE & $\bm{0.1506_{\pm 0.0076}}$ &
		$0.2281_{\pm 0.0269}$&
		$0.2240_{\pm 0.0244}$&
		$0.2530_{\pm 0.0312}$&
		$0.3350_{\pm 0.0502}$&
		$0.1861_{\pm 0.0140}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2728_{\pm 0.0154}}$ & $0.3315_{\pm 0.0259}$&
		$0.3698_{\pm 0.0252}$&
		$0.3957_{\pm 0.0358}$&
		$0.4521_{\pm 0.0323}$&
		$0.2940_{\pm 0.0111}$\\
		\hline
		\multirow{2}*{$T\rightarrow S$} & MSE & $\bm{0.1522_{\pm 0.0047}}$ &
		$0.1685_{\pm 0.0149}$&
		$0.2587_{\pm 0.0691}$&
		$0.2520_{\pm 0.0373}$&
		$0.2532_{\pm 0.0359}$&
		$0.1945_{\pm 0.0218}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2737_{\pm 0.0114}}$ & $0.3027_{\pm 0.0193}$&
		$0.4468_{\pm 0.0578}$&
		$0.3986_{\pm 0.0389}$&
		$0.3916_{\pm 0.0405}$&
		$0.2939_{\pm 0.0265}$\\
		\hline
		\multirow{2}*{$G\rightarrow S$} & MSE & $\bm{0.1484_{\pm 0.0016}}$ &
		$0.1552_{\pm 0.0070}$&
		$0.2914_{\pm 0.0297}$&
		$0.2017_{\pm 0.0449}$&
		$0.1839_{\pm 0.0172}$& 
		$0.1529_{\pm 0.0055}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2723_{\pm 0.0038}}$ & $0.2844_{\pm 0.0086}$& 
		$0.4236_{\pm 0.0262}$&
		$0.3418_{\pm 0.0443}$&
		$0.3300_{\pm 0.0192}$&
		$0.2940_{\pm 0.0060}$\\
		\hline
		\multirow{2}*{Average} & MSE & $\bm{0.1795_{\pm 0.0069}}$ & 
		$0.2067_{\pm 0.0184}$&
		$0.2889_{\pm 0.0297}$& 
		$0.2479_{\pm 0.0252}$&
		$0.2404_{\pm 0.0320}$&
		$0.2128_{\pm 0.0109}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2755_{\pm 0.0116}}$ & $0.3092_{\pm 0.0215}$&
		$0.4173_{\pm 0.0356}$& 
		$0.3711_{\pm 0.0320}$& 
		$0.3508_{\pm 0.0277}$&
		$0.3180_{\pm 0.0113}$\\
		\hline
	\end{tabular}
\end{table*}

\begin{table*}[t]
% \renewcommand\arraystretch{1.1}
	\centering
	\caption{The MAE and MSE on Human Motion Forecasting dataset for the baselines and the proposed method. The value presented are averages over 5 replicated with different random seeds. Standard deviation is in the subscript.}
	\begin{tabular}{c|c|c|c|c|c|c|c}
	    \hline
	    Task & Metric & GCA & SASA& VRADA& R-DANN& RDC& LSTM\_S+T\\
		\hline
		\multirow{2}*{$W\rightarrow G$} & MSE & $\bm{0.1639_{\pm 0.0093}}$ & $0.2316_{\pm 0.0397}$& $0.6770_{\pm 0.0281}$& $0.6186_{\pm 0.0699}$& $0.5160_{\pm 0.0177}$& $0.2295_{\pm 0.0048}$  \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2314_{\pm 0.0108}}$ & $0.3359_{\pm 0.0355}$& $0.6204_{\pm 0.0148}$& $0.5815_{\pm 0.0260}$& $0.5301_{\pm 0.0068}$& $0.3355_{\pm 0.0038}$ \\
		\hline
		\multirow{2}*{$W\rightarrow E$} & MSE & $\bm{0.1401_{\pm 0.0284}}$ & $0.2147_{\pm 0.0202}$& $0.8269_{\pm 0.0174}$& $0.5617_{\pm 0.0262}$& $0.6841_{\pm 0.0086}$& $0.2046_{\pm 0.0059}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2168_{\pm 0.0337}}$ & $0.3288_{\pm 0.0155}$& $0.6998_{\pm 0.0088}$ & $0.5753_{\pm 0.0171}$& $0.6484_{\pm 0.0031}$& $0.3339_{\pm 0.0040}$\\
		\hline
		\multirow{2}*{$G\rightarrow W$} & MSE & $\bm{0.1414_{\pm 0.0037}}$ & $0.1585_{\pm 0.0118}$& $0.5944_{\pm 0.0785}$& $0.4044_{\pm 0.0191}$& $0.3516_{\pm 0.0104}$& $0.168_{\pm 0.0147}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2163_{\pm 0.0059}}$ & $0.2754_{\pm 0.0125}$& $0.5843_{\pm 0.0286}$& $0.4751_{\pm 0.0096}$& $0.4561_{\pm 0.0045}$& $0.2893_{\pm 0.0095}$\\
		\hline
		\multirow{2}*{$G\rightarrow E$} & MSE & $\bm{0.1372_{\pm 0.0093}}$ & $0.1516_{\pm 0.0151}$& $0.6872_{\pm 0.0804}$&$0.4464_{\pm 0.0446}$& $0.5655_{\pm 0.1052}$& $0.1647_{\pm 0.0066}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2101_{\pm 0.0083}}$ & $0.2760_{\pm 0.0141}$& $0.6265_{\pm 0.0435}$& $0.5143_{\pm 0.0279}$& $0.5971_{\pm 0.0612}$& $0.2990_{\pm 0.0063}$\\
		\hline
		\multirow{2}*{$E\rightarrow W$} & MSE & $\bm{0.1821_{\pm 0.0117}}$ & $0.2400_{\pm 0.0205}$& $0.6642_{\pm 0.0182}$& $0.4904_{\pm 0.0228}$& $0.4038_{\pm 0.0185}$& $0.1978_{\pm 0.0080}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2516_{\pm 0.0193}}$ & $0.3487_{\pm 0.0198}$& $0.6133_{\pm 0.0182}$& $0.5305_{\pm 0.0152}$& $0.4755_{\pm 0.0124}$& $0.3183_{\pm 0.0061}$\\
		\hline
		\multirow{2}*{$E\rightarrow G$} & MSE & $\bm{0.1465_{\pm 0.0124}}$ & $0.2225_{\pm 0.0183}$& $0.7707_{\pm 0.1184}$& $0.4896_{\pm 0.0225}$& $0.5394_{\pm 0.0867}$& $0.2411_{\pm 0.0014}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2180_{\pm 0.0193}}$ & $0.3268_{\pm 0.0148}$& $0.6617_{\pm 0.0428}$& $0.5257_{\pm 0.0150}$& $0.5448_{\pm 0.0866}$& $0.3454_{\pm 0.0011}$\\
		\hline
		\multirow{2}*{Average} & MSE & $\bm{0.1541_{\pm 0.0097}}$ & $0.2031_{\pm 0.0093}$& $0.7034_{\pm 0.0352}$& $0.5018_{\pm 0.0131}$& $0.5101_{\pm 0.0192}$& $0.2010_{\pm 0.0069}$ \\
% 		\cline{2-9}
		~ & MAE & $\bm{0.2262_{\pm 0.0146}}$ & $0.3152_{\pm 0.0095}$& $0.6343_{\pm 0.014}$& $0.5337_{\pm 0.0070}$& $0.5420_{\pm 0.0103}$& $0.3202_{\pm 0.0051}$\\
		\hline
	\end{tabular}
\end{table*}

\subsection{Result on Air Quality Forecasting}
In this section, we will show the experiment results on the real-world data. The experiment result on the air quality forecasting is shown in Table \ref{tab:air}. According to the experiment results, we can find that the methods based on relationship modeling (GCA and SASA) outperform the other baselines. Moreover, the proposed GCA method outperform the SASA with a remarkable margin, which prove that the Granger Causality has more advantages than associative structures in modeling time-series data with time-lags. However, we also find that the improvements of some domain adaptation task such as $G\rightarrow T$, $G\rightarrow B$ and $G\rightarrow S$ are not so remarkable. This is because domain Guangzhou (G) contains many missing value data, which make it difficult to reconstruct the real Granger-Causality.


\subsection{Result on Human Motion Foresting}
We also evaluate our method on human motion forecasting, another popular real-world time-series task, the experiment result is shown in Table 5. Compared with the air quality forecasting dataset, the improvement of our method is larger. This is because (1) The human skeleton structure is a very sparse causal structure, this is why the proposed GCA method and SASA can outperform the other methods. What's more, our method can well remove the sideeffect of superfluous variables. (2) The human motion forecasting dataset contain more variables than the air quality forecasting dataset. (3) Different the previous task that predict one time-series, many time-series need to be predicted simultaneously, and it is difficult for the other baselines. 


\subsection{Ablation Study and Visualization}

\subsubsection{The study of the Effectiveness of Extra strengthen term.}
\begin{figure}[htbp]
\label{fig:strengthen}
	\centering
	\includegraphics[width=\columnwidth]{./fig/exp3}
	\caption{The Sample of three domains in the air quality forecasting dataset, we can find that the data are totally different, but they share the same summary graph. \textit{(best view in color.)}}
	\label{fig:data}
\end{figure}

Essentially, the proposed method is a autoregressive model, so all the variables is taken into consideration, which might lead to the suboptimal of the target variable that needs to be predicted. In order to solve this problem, we use an extra strengthen loss. As shown in Figure. \ref{fig:strengthen}, the red and green bars respectively denotes the standard GCA method and the GCA-e method. We can find that (1) Totally, the performance of the proposed GCA method is better than GCA-e, which reflects the effectiveness of the extra strengthen term. (2) Furthermore, the performance of GCA-e is also comparable, which shows the stableness of our method.

\begin{figure}[t]
\label{fig:alignment}
	\centering
	\includegraphics[width=\columnwidth]{./fig/exp4}
	\caption{The Sample of three domains in the air quality forecasting dataset, we can find that the data are totally different, but they share the same summary graph. \textit{(best view in color.)}}
	\label{fig:data}
\end{figure}

\subsubsection{The study of the Effectiveness of the Granger Causality Alignment.} In order to evaluate the effectiveness of the Granger Causality alignment, we remove the summary graph alignment regularization term $\mathcal{L}_r$ and devise the variant model named GCA-r. According to the experiment result shown in Figure. \label{fig:alignment}, we can find that the performance of GCA-r better than that of SASA. This is because the GCA-r model can leverage the source domain data and the limited labeled target domain data to reconstruct and predict the future value, which shows that the inference process with Granger Causality can exclude the obstruction of redundant information. We can also find that the performance of GCA-r is slightly lower than that of GCA, this is because the small size of target domain data and the difference between the source and the target domain lead to the diversity and the inaccuracy Granger Causality of target domain, which further results in the subobtimality of the experiment result. In fact, The restriction of the summary graph of Granger Causality play a key role in transferring the domain-invariant module from the source to the target domain.


\subsubsection{Visualization and Interpretability}
\begin{figure}[t]
\centering
\label{fig:air_causal}
\subfigure[TianJin]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width=\columnwidth]{fig/domain1}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[BeiJing]{
\begin{minipage}[t]{0.45\linewidth}
\centering
\includegraphics[width=\columnwidth]{fig/domain2}
%\caption{fig2}
\end{minipage}%
}%
\centering
\caption{The illustration of visualization of the summary Granger-Causal structure of TianJin $\rightarrow$ BeiJing. The Blue blocks denote the Granger-causal relationship among different variables.}
\end{figure}
\textbf{Visualization of the Air Quality Forecasting dataset.} To further investigate our approach, we extract the Granger-Causal structures and perform the corresponding visualization over the air quality forecasting dataset. Figure \ref{fig:air_causal} shows the Granger-causal structures whose task is TianJin $\rightarrow$ BeiJing. According to the visualization, we can find that the Granger-Causal structures are very sparse, and most of the module are similar, which means that the PM2.5 are generated by the similar causality. The aforemention visualization also provide some interpretability to a certain extent. For example, in Figure \label{fig:air_causal} (b), we can find that the ``humidity'' and ``PM10\_Concentration'' simultaneously have an influence on the generation of PM2.5, while the other variables like ``humidity'' and ``temperature'' have little influence on PM2.5. This experiment result also provide the insightful conclusion that we can mitigate the effect of PM2.5 by controlling the release of PM10.

\textbf{Visualization of the Human Motion Forecasting dataset. }We further conduct visualization on the human motion forecasting dataset, the experiment results shown in Figure. \ref{fig:motion} illustrate the walking motion with 5 steps later. We can find that the visualized 3D motions of the proposed GCA method are well fitted into the ground truth motions, while the other baselines do not fit well. For some joints like the legs and the elbows, our method markedly 
surpass the other methods, which shows that the Granger Causality is beneficial to capture the relationship between different joints and results in better prediction. 

Note that the proposed method aims to leverage the Granger Causality to address time-series domain adaptation problem, so we do not follow the same experiment setting of the standard human motion estimation works that forecast a long term human motion and just predict the future few steps.

\begin{figure}[t]
\label{fig:motion}
\subfigure[GCA]{
\begin{minipage}[t]{0.5\linewidth}
\flushleft
\includegraphics[width=\columnwidth]{fig/gca}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[SASA]{
\begin{minipage}[t]{0.5\linewidth}
\flushleft
\includegraphics[width=\columnwidth]{fig/sasa}
%\caption{fig2}
\end{minipage}%
}%

\subfigure[LSTM\_S+T]{
\begin{minipage}[t]{0.5\linewidth}
\flushleft
\includegraphics[width=\columnwidth]{fig/lstm}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[R-DANN]{
\begin{minipage}[t]{0.5\linewidth}
\flushleft
\includegraphics[width=\columnwidth]{fig/rdann}
%\caption{fig2}
\end{minipage}%
}%

\subfigure[RDC]{
\begin{minipage}[t]{0.5\linewidth}
\flushleft
\includegraphics[width=\columnwidth]{fig/rdc}
%\caption{fig1}
\end{minipage}%
}%
\subfigure[VRADA]{
\begin{minipage}[t]{0.5\linewidth}
\flushleft
\includegraphics[width=\columnwidth]{fig/vrada}
%\caption{fig2}
\end{minipage}%
}%
\caption{The illustration of visualization of the human Motion forecasting of the task Guesting$\rightarrow$Walking. The red lines denote the ground truth motion and the others colours denote the different methods. \textit{(best view in color.)}}
\end{figure}

\subsubsection{Computational Complexity Analysis}
Different from previous LSTM based method that extract the feature on multivariate level, the architecture of the proposed GCA method works on the univariate level. Given $p$ univariate time-series data whose time lag is $k$, the proposed GCA method required $p$ small MLP for recurrent Granger Causality reconstruction and $k \times p$ small MLP for Granger Causality inference networks. Since the hidden size for each univariate is small, we set the dimension of hidden state to 3 in practice, the proposed method do not have a very large parameters. However, it is incontestable that the time complexity of our method compares unfavorable with the LSTM based methods. Since we need to recurrently reconstruct Granger-causal structure. Fortunately, the time lag is small in practice (usually less than 10). Therefore, given a small time lag $k$ and a large $p$, the time cost of the proposed GCA method is acceptable and is even faster than the sparse associative structure alignment model.

\section{Conclusion}
In this paper, we present a Granger Causality Alignment method for semi-supervised time-series forecasting. In our proposal, the proposed Granger Causality alignment method, which simultaneously reconstructs the Granger Causality and infers on the Granger-causal structure, can well transfer domain knowledge via the domain-sensitive module and the Granger Causality alignment. The proposed method not only address the domain adaptation problem for time-series forecasting task, but also provide the insightful interpretability that may be useful in making decision of time-series application. Though effective, the proposed method need to take the time-lag as a prior or hyperparameters when we apply our method on the real-world dataset. Therefore, exploring how to learning the Granger Causality with adaptive time lag would be a future direction.
% 需要提前知道lag，之后需要改进成lag自适应

% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank Jie Qiao and Jiahao Li from the Guangdong University of Technology for the help and supports on this work.

\bibliographystyle{IEEEtran}
\bibliography{ref}

% \vspace{-8ex}
\begin{IEEEbiography}[{\includegraphics[width=1in, height=1.25in, clip, keepaspectratio]{fig/lizijian3}}]{Zijian Li}
received the B.S. degree in computer science from the Guangdong University of Technology, Guangzhou, China, in 2017. 

He is currently a Ph.D. student at the School of Computer, Guangdong University of Technology. He was a research intern at the Advanced Digital Sciences Center, Illinois at Singapore Pte in 2018.3-2018-6, a visiting student at Nanyang Technological University in 2019.9-2019.12. His research interests cover a variety of different topics including causality, transfer learning and their applications.
\end{IEEEbiography}
\vspace{-8ex}

\begin{IEEEbiography}[{\includegraphics[width=1in, height=1.25in, clip, keepaspectratio]{fig/Tom-bio}}]{Tom Z. J. Fu } 
obtained the B.Eng degree in Information Engineering from Shanghai Jiao Tong University in 2006. He received the M.Phil and the Ph.D degrees from the Department of Information Engineering at the Chinese University of Hong Kong in 2008 and 2013, respectively. He is currently leading the network transmission algorithm team of Bigo Technology. Before joining Bigo, he was a senior research scientist and analytics area programme manager of the Advanced Digital Sciences Center, a Singapore-based research center affiliated with the University of Illinois at Urbana-Champaign. His research interests include data driven networking, software defined networking (SDN), Internet measurement and monitoring, Peer-to-Peer content distribution, cloud computing and real-time distributed stream analytics.
\end{IEEEbiography}
\vspace{15ex}

\begin{IEEEbiography}[{\includegraphics[width=1in, height=1.25in, clip, keepaspectratio]{fig/Biogra_Ruichu_Cai}}]{Ruichu Cai} (M'17) received his B.S. degree in Applied Mathematics and Ph.D. degree in Computer Science from South China University of Technology in 2005 and 2010, respectively. He is currently a Professor in the School of Computer, Guangdong University of Technology. 
	
He was a visiting student at National University of Singapore in 2007-2009, and research fellow at the Advanced Digital Sciences Center, Illinois at Singapore Pte in 2013-2014. His research interests cover a variety of different topics including causality, machine learning and their applications.
\end{IEEEbiography}




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%
% \clearpage
\newpage
\appendices
\section{The Variational Lower Bound}\label{ELBO}

\begin{strip}
\nonumber
\small
\begin{align}
& \ln P(\bm{z}_t|\bm{z}_{t-1},\bm{z}_{t-2}, \cdots, \bm{z}_{t-k})=\ln \frac{P(\bm{z}_t, A_1, A_2, \cdots, A_k|\bm{z}_{t-1},\bm{z}_{t-2},\cdots,\bm{z}_{t-k})}{P(A_1, A_2, \cdots, A_k|\bm{z}_{t},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\\=&\nonumber \ln P(\bm{z}_t|\bm{z}_{t-1}, \cdots, \bm{z}_{t-k},A_1, \cdots, A_k) + ln\frac{P(A_1|\bm{z}_{t-1}, \cdots, \bm{z}_{t-k})\prod_{j=2}^{k}P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{P(A_1|\bm{z}_{t},\!\cdots,\bm{z}_{t-k})\prod_{j=2}^{k}P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\\=&\ln P(\bm{z}_t|\bm{z}_{t-1}, \cdots, \bm{z}_{t-k},A_1, \cdots, A_k) + \ln \frac{P(A_1|\bm{z}_{t-1}, \cdots,\bm{z}_{t-k})\prod_{j=2}^kP(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{P(A_1|\bm{z}_{t},\cdots,\bm{z}_{t-k})\prod_{j=2}^k P(A_j|A_1, \cdots,A_{j-1},\bm{z}_{t},\cdots,\bm{z}_{t-k})} \\&+\ln \frac{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j-2}^k Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j-2}^k Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\\=&
\mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots \mathbb{E}_{Q(A_k|\bm{z}_{t-1},\cdots,\bm{z}_{t-k},A_1,\cdots,A_{k-1})}\left[\underbrace{\ln P(\bm{z}_t|\bm{z}_{t-1}, \cdots, \bm{z}_{t-k},A_1, \cdots, A_k)}_{\text{The reconstruction loss }\mathcal{L}_r} \right. \\&+ \left. \ln \frac{P(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})} \right. \\&+ \left. \ln \frac{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{P(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})} \right] \\=&
\mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots \mathbb{E}_{Q(A_k|\bm{z}_{t-1},\cdots,\bm{z}_{t-k},A_1,\cdots,A_{k-1})}\left[\mathcal{L}_r + \ln \frac{P(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\right] \\& + 
% D_{KL}(Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})||P(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})) + 
\sum_{j=1}^k{D_{KL}(Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})||P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})} \\\geq&
\mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots \mathbb{E}_{Q(A_k|\bm{z}_{t-1},\cdots,\bm{z}_{t-k},A_1,\cdots,A_{k-1})}\left[\mathcal{L}_r + \ln \frac{P(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\prod_{j=2}^k Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\right]\\=&
\mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots \mathbb{E}_{Q(A_k|\bm{z}_{t-1},\cdots,\bm{z}_{t-k},A_1,\cdots,A_{k-1})}\mathcal{L}_r + \mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\left[\ln\frac{P(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{QA_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})} \right] \\&+ \sum_{j=2}^k \mathbb{E}_{Q(A_j|A_1\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots\mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z_{t-k}})}\frac{P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}{Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\\=&
\mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots \mathbb{E}_{Q(A_k|\bm{z}_{t-1},\cdots,\bm{z}_{t-k},A_1,\cdots,A_{k-1})}\mathcal{L}_r - D_{KL}(Q(A_1|\bm{z}_{t-1},\bm{z}_{t-2},\cdots,\bm{z}_{t-k})||P(A_1|\bm{z}_{t-1},\bm{z}_{t-2},\cdots,\bm{z}_{t-k})) \\&-\sum_{j=2}^k \mathbb{E}_{Q(A_1|\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\cdots \mathbb{E}_{Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})}\left[D_{KL}\left(Q(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\right.\right.\\||& \left.\left. P(A_j|A_1,\cdots,A_{j-1},\bm{z}_{t-1},\cdots,\bm{z}_{t-k})\right)\right]
\end{align}
\end{strip}
% \lipsum[1]
% \begin{strip}
% \begin{align}
% a&=b+c=b+c=b+c=b+c=b+c=b+c\\  
% &=b+c=b+c=b+c=b+c=b+c=b+c
% \end{align}
% \end{strip}
% \lipsum[1-2]
% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \clearpage
\newpage



% \clearpage
% \newpage
% \section{}
% Appendix two text goes here. 
% \begin{theorem}
% \label{the:b1}
% For any predictor $h \in \mathcal{H}$, we let $h^*$ be the ideal predictor that simultaneously obtain the minimal error on the source and the target domain,
% \begin{equation}
%     h^*\triangleq\mathop{\arg\min}_{h\in\mathcal{H}}\{\mathcal{L}_{\mathcal{S}}(h,f_S)+\mathcal{L}_{\mathcal{T}}(h,f_T)\},
% \end{equation}
% then we can obtain the following inequation:
% \begin{equation}
%     \mathcal{L}_{T}(h,f_T)\leq\mathcal{L}_S(h,f_S) + disc(S,T) +\mathcal{L}_{\widehat{T}}(h,f_T) - \mathcal{L}_{\widehat{T}}(h,h^*) + \lambda
% \end{equation}
% where $\lambda=\lambda(\mathcal{H},S,T)$ is independent of $h$ and can be considered as a constant. 
% \begin{proof}
% \begin{equation}
% \begin{split}
%         &\quad\mathcal{L}_{T}(h,f_T)=\mathcal{L}_{T}(h,f_T) + \mathcal{L}_{S}(h,f_S) - \mathcal{L}_{S}(h,f_S)\\&\leq\mathcal{L}_S(h,f_S) + \mathcal{L}_{T}(h, h^*) + \mathcal{L}_{T}(f_T, h^*)+\mathcal{L}_S(h^*,f_S)\\&\quad-  \mathcal{L}_S(h^*,h)+\mathcal{L}_{\widehat{T}}(h, f_T)-\mathcal{L}_{\widehat{T}}(h, f_T)\\&\leq\mathcal{L}_S(h,f_S) + \left(\mathcal{L}_{T}(h,h^*)-\mathcal{L}_{S}(h,h^*)\right)+\mathcal{L}_{\widehat{T}}(h,f_T) \\&\quad- \mathcal{L}_{\widehat{T}}(h,h^*) + \mathcal{L}_T(h^*,f_T) + \mathcal{L}_S(h^*,f_S) + \mathcal{L}_{\widehat{T}}(h^*,f_T)\\&\leq\mathcal{L}_S(h,f_S) + \sup_{h,h'\in \mathcal{H}}\left|\mathcal{L}_{T}(h,h')-\mathcal{L}_{S}(h,h')\right|+\mathcal{L}_{\widehat{T}}(h,f_T) \\&\quad- \mathcal{L}_{\widehat{T}}(h,h^*) + \mathcal{L}_T(h^*,f_T) + \mathcal{L}_S(h^*,f_S) + \mathcal{L}_{\widehat{T}}(h^*,f_T)\\&=\mathcal{L}_S(h,f_S) + disc(S,T) +\mathcal{L}_{\widehat{T}}(h,f_T) - \mathcal{L}_{\widehat{T}}(h,h^*) + \lambda
% \end{split}
% \end{equation}
% where $\lambda=\mathcal{L}_T(h^*,f_T) + \mathcal{L}_S(h^*,f_S) + \mathcal{L}_{\widehat{T}}(h^*,f_T)$.
% \end{proof}
% \end{theorem}

% \begin{theorem}
% For any $\delta>0$ and any predictor $h\in\mathcal{H}$, with probaility $1-\delta$, we have:
% \begin{equation}
% \begin{split}
%     \mathcal{L}_T(h,f_T)\leq&\mathcal{L}_{\widehat{S}}(h,f_S)+\mathcal{L}_{\widehat{T}}(h,f_T)+disc(S,T)+disc(\widehat{S},\widehat{T})\\&+2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda
% \end{split}
% \end{equation}
% \begin{proof}
% Our goal is to proof that $\mathcal{L}_S(h.f_S) - \mathcal{L}_{\widehat{S}}(h,f_S)$ less than a non-negative value. Based on Definition \ref{def:rade_bound}, we have:

% \begin{equation}
% \nonumber
% \begin{split}
%     \mathcal{L}_S(h,f_S) - \mathcal{L}_{\widehat{S}}(h,f_S)&\leq
% \sup_{h\in\mathcal{H}}\left|\mathcal{L}_S(h,f_S)-\mathcal{L}_{\widehat{S}}(h,f_S)\right|\\&\leq2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}},
% \end{split}
% \end{equation}
% where $m$ is the size of the source domain dataset and $\mathcal{L}_S$ is bounded by $M_S$.

% Therefore, combining with Theorem \ref{the:b1} and Definition \ref{def:dd}, we can obtain:
% \begin{equation}
% \nonumber
% \begin{split}
%     \mathcal{L}_T(h,f_T)\leq& \mathcal{L}_{\widehat{S}}(h,f_S)+disc(S,T)+\mathcal{L}_{\widehat{T}}(h,f_T) \\&+\mathcal{L}_{\widehat{S}}(h,h^*)-\mathcal{L}_{\widehat{T}}(h,h^*)+2\mu\mathfrak{R}_m(\mathcal{H})\\&+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda\\\leq&\mathcal{L}_{\widehat{S}}(h,f_S)+\mathcal{L}_{\widehat{T}}(h,f_T)+disc(S,T)\\&+\sup_{h,h'\in\mathcal{H}}|\mathcal{L}_{\widehat{S}}(h,h^*)-\mathcal{L}_{\widehat{T}}(h,h^*)| \\&+2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda\\=&\mathcal{L}_{\widehat{S}}(h,f_S)+\mathcal{L}_{\widehat{T}}(h,f_T)+disc(S,T)+disc(\widehat{S},\widehat{T})\\&+2\mu\mathfrak{R}_m(\mathcal{H})+M_S\sqrt{\frac{\log\frac{2}{\sigma}}{2m}}+\lambda
% \end{split}
% \end{equation}
% \end{proof}
% \end{theorem}
% \begin{lemma}\label{lemma1}

% \end{lemma}









% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\clearpage

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:



% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


