\input{tabs/results}

\section{Experimental Results}\label{sec:results}
\subsection{RQ1: Unsupervised COOD Performance}
In this subsection, we analyze the experimental results to assess the detection performance of our unsupervised COOD model compared with the unsupervised baselines. According to Table~\ref{tab:results-python} and~\ref{tab:results-java}, we can observe that COOD outperforms all unsupervised baselines on both CSN-Python and CSN-Java. Notably, COOD effectively detect \textit{out-domain} and \textit{misaligned OOD} testing samples, while other unsupervised approaches only work for the \textit{out-domain} scenario. This is because COOD effectively captures alignment information within (comment, code) pairs through a multi-modal contrastive learning objective with InfoNCE loss and uses similarity scores between comments and code to detect OODs. Specifically, COOD outputs low similarity scores for the out-domain data from TLCS by additionally considering the knowledge gap difference in (comment, code) pairs between ID and out-domain data. Also, as the misaligned scenario involves misaligned (comment, code) pairs, their similarity scores are naturally low. In contrast, the unsupervised baselines aggregate misaligned information into classification logits and rely on the confidence of the "aligned" class to detect OODs. As previously discussed in Sec. IV-C, the contrastive losses~\cite{khosla2020supervised, zhou2021contrastive} used by them are not as effective for learning alignment information, leading to inferior performance. Additionally, detecting token-level OOD in \textit{shuffled-comment} and \textit{buggy-code} scenarios proves challenging without seeing OOD samples during training, as all unsupervised methods fail to detect these OODs. 

% Some of the results and analysis could have been explained \input{texs/05_results}in more detail. For instance, in Section VI-A, it would have been better to explain why the proposed COOD method significantly outperforms the unsupervised baselines on the out-domain and misaligned scenarios individually and overall, while illustrating Table III. 

% Explain the evaluation setup and results more clearly. e.g., the analysis of the results in Section VI-A

\subsection{RQ2: Weakly-supervised COOD+ Performance}
We further investigate the performance of our weakly-supervised COOD+ method against several weakly-supervised baselines on CSN-Python and CSN-Java. Table~\ref{tab:results-python} shows that weak supervision on a tiny amount of OOD data enables COOD+ (and EOE) to not only address unsupervised COOD's shortcomings in detecting finer-grained \textit{shuffled-comment} and \textit{buggy-code} OODs, but also enhance performance for the \textit{out-domain} scenario for CSN-Python. This improvement aligns with previous research ~\cite{hendrycks2018deep, liu2020energy, kim-etal-2023-pseudo} which enhances OOD detection by complementing the downstream task objective with an complementary discriminator operating to distinguish IDs from external OODs. While EOE slightly outperforms COOD+ for the \textit{out-domain} and \textit{shuffled-comment} scenarios by utilizing the prediction probabilities from one classification module, our COOD+, which combines the BC and CL modules, delivers consistently high performance across all four scenarios, resulting in superior overall performance. In addition, the BC module can be directly adapted to the overall COOD+ framework without modifying the underlying learning objective, but the outlier exposure-based methods (\eg EOE) typically require additional engineering (\eg determining class-probabilistic distributions~\cite{hendrycks2018deep}, boundaries for energy scores~\cite{liu2020energy}) to equip ML models with OOD detection abilities. Besides, the bug detection method 2P-CuBERT can reasonably detect OODs, but its performance for the \textit{buggy-code} scenario is negatively impacted by the limited amount of training OOD examples.

On the CSN-Java dataset, our COOD+ also achieves the best overall performance compared to all baselines, despite trailing slightly behind EOE for \textit{out-domain} and \textit{shuffled-comment} OODs. While EOE has higher AUROC score than that of COOD+ for the \textit{buggy-code} scenario, it suffers from a high FPR95, indicating a higher margin of error for OOD inference using a threshold of 95\% ID recall. Moreover, similar to CSN-Python, CuBERT fails to detect OODs effectively on CSN-Java either, likely due to the lack of training examples. In summary, the superior performance of our COOD+ model results from the interplay between the CL and BL modules, where contrastive learning captures high-level alignment between NL-PL input pairs that is naturally suitable for \textit{out-domain} and \textit{misaligned} OODs, while the OOD rejection classifier targets lower-level OOD information from \textit{shuffled-comment} and \textit{buggy-code} samples. Furthermore, by utilizing a weakly-supervised contrastive learning objective that jointly optimizes for OOD detection and the code search task, our method enables effective deployment of the code search model in OOD environments, which will be further studied in Sec. VI-D.


\subsection{RQ3: Weakly-Supervised COOD+ Performance with Different Model Components and Encoder Backbone}
In this subsection, we evaluate the effect of using only the CL (COOD+\_CL) or the BC module (COOD+\_BC) against the proposed combined COOD+ model to illustrate how COOD+ generalizes in four OOD scenarios. As shown in Table~\ref{tab:results-python} and~\ref{tab:results-java}, COOD+\_CL performs well in the \textit{out-domain} and \textit{misaligned} scenarios, which is due to its ability to effectively capture high-level (comment, code) alignment information. COOD+\_BC excels in the \textit{out-domain}, \textit{shuffled-comment}, and \textit{buggy-code} scenarios, since it can learn lower-level features from these types of OOD samples. While COOD+\_BC maintains acceptable OOD detection performance with high AUROC ($>$90\%) and low FPR95 ($<$25\%), the CL module remains crucial for overall performance, since without it the overall performance of COOD+ will drop below the EOE baseline. Moreover, removing the BC module has a more negative impact on the OOD detection as COOD+ loses the ability to capture the necessary lower-level OOD information for detecting \textit{shuffled-comment} and \textit{buggy-code} OODs. Note that the standalone CL module performs better than the unsupervised COOD overall, demonstrating that our proposed modification to the original CL objective enhance OOD detection by leveraging the margin-based loss. Thus, the combined model's superior performance validates our design choices. That is, the combined scoring function (cosine similarities from CL and the prediction probabilities from BC) is thoughtfully designed to leverage the advantage of each module for high detection accuracy.

% How do unsupervised model and weakly-supervised models generalize in four OOD scenarios?

\input{tabs/ablation}
Moreover, we compare the detection performance of our COOD+ with various underlying NL-PL pre-trained encoder. Specifically, we compare our choice of GraphCodeBERT~\cite{guo2020graphcodebert} against other NL-PL encoders from the literature including its predecessor, CodeBERT~\cite{feng2020codebert}, and more recent ones such as UniXcoder~\cite{guo2022unixcoder} and ContraBERT~\cite{liu2023contrabert}. As shown in Table~\ref{tab:ablation}, all encoders perform within a 1-2\% difference, indicating that our COOD+ framework is robust across different encoders. This demonstrates our framework's flexibility and effectiveness in detecting OODs when deploying various NL-PL encoders for code-related tasks. Furthermore, we investigate key hyperparameters in COOD+, such as $m$ for margin-based contrastive loss and  $\lambda$ in the overall loss function. The detailed results are available in our online appendix~\cite{cood-tool}.


%%%---margin-based loss----
% \noindent
% \textbf{Effect of Margin-based Loss and its Hyperparameter.} We explore the impact of the margin $m$ in Eq.~\ref{eq:contrast_id} and Eq.~\ref{eq:contrast_ood} on the detection performance. Table ~\ref{tab:ablation} shows the detection performance under different margin parameters. When $m=0$, the proposed model does not use the margin, meaning that it arbitrarily encourages the OOD cosine similarities to be lower than that of ID samples. We can observe from Table~\ref{tab:ablation} that our method has the best performance when $m=0.4$. In addition, our detection method without the margin-based loss (when $m=0$) indeed performs worse, although the decrease in performance is offset by the rejection classifier. Therefore, we can conclude that the proposed margin-based loss plays an important role in multi-modal OOD detection since it enables OOD-aware finetuning for pretrained NL-PL transformer-based models.\\

% \noindent
% \textbf{Effect of Weight in the Objective.} We also evaluate the performance of the proposed model for different $\lambda$ values in the overall objective function (\ref{eq:overall_obj}) \shao{typos here} as it increases from $0.2$, to $0.5$, and to $1$. As observed in Table~\ref{tab:ablation}, our model performs best when $\lambda=0.2$ on two datasets. One possible reason is that the NL-PL encoders are pretrained on multiple PLs, which requires more weight in the contrastive learning module for language-specific representation learning during optimization. In addition, as representations learned by transformer-based encoders transfer well to fully-connected classification components~\cite{feng2020codebert, guo2020graphcodebert}, the binary OOD rejection classifier needs smaller weight updates during back-propagation to perform effectively.\\

% \noindent \textbf{Effect of Different NL-PL Encoders.}


% \noindent 


\subsection{RQ4: Main Task Performance}
We present the code search performance under the impact of OOD instances by using GraphCodeBERT (GCB), COOD/COOD+, and the closest competitor EOE in Table~\ref{tab:maintask}. As described in Sec. V-F, we use the official metric mRR  and follow the same testing scheme as the original GraphCodeBERT code search model for evaluation. From Table~\ref{tab:maintask}, we first observe that our COOD/COOD+ achieves performance comparable to GraphCodeBERT, while the EOE suffers from a significant reduction in performance, as it reformulates code search as binary classification to gain OOD detection ability. This reveals a critical trade-off between OOD detection and downstream task performance. To further validate the importance of OOD detection for code search, we construct outliers based on the CSN-Java and -Python testing dataset, respectively. Given that code search aims to retrieve the most aligned code from a code corpus given an NL query, the outliers are only sampled from three OOD scenarios: \textit{out-domain}, \textit{shuffled-comment} and \textit{buggy-code}, each replacing 5\% ID data of the original testing set. We then show the results when the dataset contains 15\% OOD samples (\ie 15\% outliers), discard OOD samples by filtering the testing set by ground-truth labels (\ie Filtered-GT) or using various OOD detection models (\ie Filtered-OOD-model). Note that the Filtered-GT dataset is the original CSN's subset with 15\% of ID samples removed.

\input{tabs/main_task}

According to Table~\ref{tab:maintask}, the performance of the original GraphCodeBERT code search model drops by 4.84\% and 5.95\% ((69.10-64.99)/69.10) mRR when outliers are present in CSN-Python and -Java, respectively. As a solution to this issue, our COOD/COOD+ detector recover the performance losses by identifying and filtering out the OOD samples without negatively impacting the model's code understanding ability in code search. Specifically, the code search performance of COOD/COOD+ on the Filtered-COOD/COOD+ dataset (70.30\%/73.10\% and 71.02\%/73.18\% on CSN-Python and -Java, respectively) is comparable to or even better than GraphCodeBERT on the Filtered-GT dataset (70.24\% and 69.12\% on CSN-Python and -Java, respectively). This slight improvement is probably because our detectors filter out additional lower-quality testing samples that resemble outliers. Thus, our COOD/COOD+ enhance the trustworthiness and robustness of the GraphCodeBERT, since the model's predictions become more reliable when encountering OOD data. % Also, although both COOD and COOD+ can recover the GraphCodeBERT's performance losses, COOD+ still perform better than COOD on each possible testing subset, especially the Filtered-OOD-model dataset. %Compared to only using 15\% outliers in this experiment, when more OOD instances from diverse OOD scenarios are encountered (\eg 75\% outliers in the OOD detection experiments), we expect the performance difference between COOD+ and COOD to be more significant. 
Note that the original GraphCodeBERT is not equipped with the OOD detection ability, so its corresponding cells for the Filtered-OOD-model in Table~\ref{tab:maintask} are left blank.

% It is not clear how RQ4 (Main Task Performance) is answered using Table V since we do not know what those numbers mean for CSN-Java and CSN-Python. It would have been better if the rationale of RQ4 is explained clearly along with the metrics that have been used to answer this RQ.  it is not clear why RQ4 is addressed and why only COOD+ is considered rather than both COOD and COOD+

% \subsection{RQ5: Overconfident Case}
% \viet{Yanfu: Please move these into where you want} Given an OOD testing sample, DNN models (pre)trained on ID data are prone to predict a higher MSP confidence score than the threshold and wrongly identify it as an
% ID sample~\cite{lee2017training, liang2017enhancing}. This overconfidence issue limits the effectiveness of OOD detection. For NL data, this is caused by the the spurious correlation between OOD and ID features such as entities and syntactic structures~\cite{zheng2020out, wu2022revisit}. Such correlation also occurs in PL data. For example, an OOD PL input with the syntactic structure ``def ... if ... return ... else ... return ..." may receive an ID score if this pattern is commonly used in other ID inputs. To overcome overconfident predictions, previous works explored techniques such as temperature scaling~\cite{liang2017enhancing}, confidence calibration using adversarial samples~\cite{lee2017training, bitterwolf2020certifiably}, or adaptive class-dependent threshold~\cite{wu2022revisit}. In contrast, our proposed COOD+ utilizes a weakly-supervised contrastive learning objective to take advantage of a small number of OOD samples during training and prevent the alignment between OOD pairs. Moreover, we adopt the binary OOD rejection module to discriminate the fused OOD and ID representations. We further verify whether COOD+ overcome the overconfidence issue through the lens of Conformal Prediction~\cite{angelopoulos2023conformal}.

% Conformal Prediction (CP) involves post-processing uncertainty quantification techniques that are model-agnostic, and provide statistical guarantees on the predictions of a trained model~\cite{angelopoulos2023conformal}. The commonly used split CP technique first computes the nonconformity scores, which are OOD scores in our case, on a calibration set independent of the training data. Then, it builds a prediction set for each testing sample $\mathcal{C}_\alpha(t^{test}_l, c^{test}_l)$ satisfying the condition $P(y_l^{test}\in \mathcal{C}_\alpha(t^{test}_l, c^{test}_l))\geq 1-\alpha$, where $\alpha$ is a small error rate (e.g., 0.05) that the user is willing to tolerate. Here, this condition guarantees that the true outcome is covered by the prediction set with probability $1-\alpha$, which is also known as the CP coverage. When CP is applied to the OOD detection scores, all scores have the same statistical guarantee, but better OOD scores will give tighter prediction sets. Conversely, worse scores will give large and uninformative prediction sets, which corresponds to ineffective OOD detection caused by overconfident predictions.

% In our experiment, we apply split CP by reserving 20\% of the testing samples from each testing dataset (CSN-Java and CSN-Python) for CP calibration, and construct the prediction sets with tolerable error rate $\alpha=0.05$ on the remaining testing samples. To assess how effectively COOD+ addresses the overconfidence issue, we compare its average prediction set (P-Set) size (between 1 and 2 for binary predictions) with that of selected baselines including MCL+MSP, the best performing approach using MSP OOD scores, and our proposed COOD. As observed in Table~\ref{tab:conformal}, the proposed COOD+ achieves the smallest prediction sets on both datasets. Specifically, the vast majority of prediction sets obtained by COOD+ contain one value that is 95\% statistically guaranteed to be the true OOD label according to the CP condition, indicating the minimal overconfidence of OOD scores. In contrast, the MCL+MSP method is prone to overconfidence, because it produces large prediction sets (i.e., size 2) including both IDs and overconfident OODs. Additionally, without utilizing OOD samples during training, COOD cannot effectively prevent overconfident predictions. Note that in the CP context, although higher coverage is desired, the main goal is to build the smallest prediction sets given the user-specified error rate of 0.05. Therefore, COOD+ is the most effective at overcoming the overconfidence barrier despite its slightly lower coverage than that of MCL+MSP and COOD.

% \begin{table}[!tb]
% \centering
% \caption{Effectiveness of COOD+ compared to selected methods for overcoming overconfident OOD predictions.}
% \small
% \label{tab:conformal}
% \begin{adjustbox}{width=\linewidth,center}
% \begin{tabular}{c|cc|cc}
% \toprule
% \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{CSN-Java}} & \multicolumn{2}{c}{\textbf{CSN-Python}} \\ \cline{2-5} 
% & \textbf{Coverage↑}    & \textbf{ P-Set Size↓}   & \textbf{Coverage↑}    & \textbf{P-Set Size↓}    \\ \hline
% MCL+MSP                      &  \textbf{97.03}    &   1.891   &   95.00   &      1.834          \\
% COOD                           &   95.66            &      1.593         &   \textbf{95.81}   &  1.576     \\
% COOD+                          & 95.31              & \textbf{1.077}              &       95.35        &      \textbf{1.010}          \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{table}