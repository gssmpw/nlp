\section{Preliminaries}
Below we review some background knowledge and techniques that will be used in the proposed approach.

\subsection{Code Representation Learning} 

Representation learning refers to the process of learning a parametric mapping from the raw input data domain to a low-dimensional latent space, aiming to extract more abstract and useful concepts that can improve performance on a range of downstream tasks. Recently, self-supervised representation learning has become prominent in the ML community due to the success of large pre-trained models. The similarity between NL and PL has led to the development of numerous NL-PL pre-trained models~\cite{guo2020graphcodebert, guo2022unixcoder}, resulting in significant improvements across various code-related tasks such as code search, generation, and clone detection. CodeBERT\cite{feng2020codebert} stands as the first Transformer-based NL-PL pre-trained model, while GraphCodeBERT~\cite{guo2020graphcodebert} further enhances its performance by considering the inherent structure of code (\ie data flow) during pre-training. Subsequent models like UniXcoder~\cite{guo2022unixcoder} and ContraBERT~\cite{liu2023contrabert} further enhance the code understanding capabilities. In our research study, we are concerned with extending self-supervised NL-PL representation learning to the OOD detection task, which is outside the scope of existing pre-trained models, but is crucial for ensuring the trustworthiness of these models in real-world applications. 

\subsection{Multimodal Contrastive Learning}

Contrastive learning~\cite{1640964} is an emerging technique that learns discriminative representations from data that are organized into similar/dissimilar pairs. It has been developed over multiple sub-fields, such as NLP, CV \cite{khosla2020supervised, chen2020simple}, and SE~\cite{jain2020contrastive, guo2022unixcoder}. Recently, researchers have developed multi-modal approaches that combine contrastive learning with multiple data modalities, achieving superior performance over uni-modal modals in various tasks~\cite{radford2021learning, liu2023contrabert}. Due to the prowess of contrastive learning in discriminative tasks (\eg classification and information retrieval), it naturally fits the OOD detection domain, as shown by studies on both uni-modal and multi-modal data~\cite{ming2022delving, qiu2022unsupervised, esmaeilpour2022zero}. Inspired by this, we apply contrastive learning to NL-PL data to learn representative and discriminate features for both ID and OOD instances and transfer this knowledge to train our OOD detector.







