\section{Empirical Evaluation Design} \label{sec:design}
To evaluate the performance of the proposed approaches in four scenarios, we investigate the following research questions:

\begin{enumerate}[label=\textbf{RQ$_\arabic*$:}, ref=\textbf{RQ$_\arabic*$}, wide, labelindent=5pt]\setlength{\itemsep}{0.2em}
    \item \label{rq:baseline}{\textit{How effective is our unsupervised COOD when compared to unsupervised baselines?}}
    \item \label{rq:athena}{\textit{How effective is our weakly-supervised COOD+ when compared to weakly-supervised baselines?}}
    \item \label{rq:athena}{\textit{How effective is our weakly-supervised COOD+ when using different modules or encode backbone?}}
    \item \label{rq:impact}{\textit{Is the main task (Code Search) performance affected by our COOD/COOD+ auxiliary, and to what extent?}}
\end{enumerate}

\subsection{Datasets}
In our experiments, we rely on two benchmark datasets: CodeSearchNet (CSN)~\cite{guo2020graphcodebert} and TLCS~\cite{salza2022effectiveness}. CSN contains bimodal data points consisting of code paired with function-level NL descriptions (\ie first lines of documentation comments) in six PLs (\eg Python, Java) collected from GitHub repositories. While CSN was originally created for a specific downstream task (\ie code search), it has since been widely adopted by large (NL, PL) models~\cite{guo2020graphcodebert, guo2022unixcoder} for pre-training due to the informative nature of bimodal instances. Large NL-PL models are first pre-trained across \textit{all} six languages, and then further fine-tuned for a \textit{specific} PL for some downstream task to enhance performance. For code search, the goal is to retrieve the most relevant code given a NL query, where CSN is widely used to further fine-tune a PL-specific code search model~\cite{feng2020codebert}. 

% Consistent with prior research \cite{guo2020graphcodebert, liu2023contrabert}, we utilize the filtered version of CSN, which removes low-quality samples using handcrafted rules \cite{lu2021codexglue}.

Salza \etal \cite{salza2022effectiveness} used training samples from CSN for pre-training, and created a new dataset sourced from StackOverflow (SO) for fine-tuning the code search model, involving only \textit{three} PLs: Java, Python, and JavaScript. Specifically, they leverage SO user questions as search queries and accepted answers as retrieved code snippets, which differ from GitHub comments and the corresponding code in CSN. We refer to this new dataset as TLCS. Existing work~\cite{Yang2017msr, lotter2018so, Abdalkareem2017OnCR} investigated code clones between SO and GitHub, demonstrating there exists \textit{only} 1-3\% code reuse. Besides code, user questions in SO are typically formulated before code answers, without concrete knowledge of what code answers will be, and are mostly written by end-users. Conversely, in GitHub, method docstrings (\ie comments) are often written following code snippets, and are mostly written by developers. These distinctions cause performance shortfall when directly applying models trained on CSN to TLCS without further fine-tuning or transfer learning~\cite{guo2022unixcoder, huang2021cosqa, salza2022effectiveness}. %In line with CSN where each code snippet is a single function, we further preprocess the TLCS dataset by localizing functions within accepted answers and extracting the longest one as the final corresponding code snippet. Consequently, TLCS contains 20,531 and 27,879 question-answer pairs for Java and Python, respectively. \yanfu{javascript?}

\subsection{OOD Scenarios}

% \input{tabs/dataset.tex}

%%%%-------examples---------------
% \begin{figure*}[!t]
% \caption{Examples of four OOD scenarios from CodeSearchNet-Java dataset, with descriptions for each scenario and "defective" modality highlighted in \textcolor{red}{red} boxes.}
% \begin{center}
% % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}
% \begin{subfigure}[b]{0.23\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/scenario1.pdf}
%     \caption{Out-domain: Code-comment pair from another dataset (Python).}
%     \label{fig:scen1}
% \end{subfigure}
% \hspace{0.5em}
% \begin{subfigure}[b]{0.23\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/scenario2.pdf}
%     \caption{Misaligned: ID comment belongs to another ID code.}
%     \label{fig:scen2}
% \end{subfigure}
% \hspace{0.5em}
% \begin{subfigure}[b]{0.23\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/scenario3.pdf}
%     \caption{Shuffled-text: Different meaning comment due to shuffled tokens. \yanfu{Viet, please update the figure based on the new scenario setting}}
%     \label{fig:scen3}
% \end{subfigure}
% \hspace{0.5em}
% \begin{subfigure}[b]{0.23\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figs/scenario4.pdf}
%     \caption{AST-defective: Code that cannot be parsed into AST.}
%     \label{fig:scen4}
% \end{subfigure}
% \end{center}
% \label{fig:example}
% \end{figure*}


We design four distinct OOD scenarios using the datasets described above, with CSN-Java and CSN-Python as inliers due to their common use for the pre-training of code models. %We excluded CSN-JavaScript because the buggy code generation algorithm we use in scenario 4 doesn't support JavaScript.

\noindent\textbf{Scenario 1: Out-domain.} Following existing ML work~\cite{hendrycks2020pretrained, podolskiy2021revisiting, zhou2021contrastive}, we create an out-of-domain setting by choosing OOD samples from a different dataset than the training data. Thus, samples from TLCS-Java or TLCS-Python are treated as outliers accordingly. Inliers and their corresponding outliers belong to the same PL to ensure approaches don't identify OODs based on syntax differences between PLs but on data domains: GitHub vs. SO. Prior studies~\cite{wang2022enriching} show that CSN queries are longer than SO questions on average, so we sampled TLCS questions and answers to match the length distribution of CSN comments and code, to avoid OOD approaches exploiting spurious cues of query length differences. % By doing this, the query and code length differences between inliers and outliers are within two tokens.	
We didn't consider other code search datasets~\cite{yao2018staqc, li2020hierarchical, rao2021search4code} because they either contain only one of the PLs (Python or Java) or have a smaller dataset size. 

\noindent\textbf{Scenario 2: Misaligned.} In this scenario, we shuffle normal NL-PL pairs so that each code doesn't match its NL description. Although the NL modality sourced from attached comments in code are typically aligned with the PL modality, documentation errors may still occur and not effectively filtered by handcrafted rules~\cite{guo2020graphcodebert}. %Therefore, in order to avoid laborious human annotation and verification, we construct this OOD scenario to leverage DL for automatically detecting documentation errors, which is also expected to benefit bimodal pre-training and the downstream code search task.


\noindent\textbf{Scenario 3: Shuffled-comment.} For (comment, code) pairs, we modify the syntactic information in each comment by shuffling 20\% of selected tokens using a seeded random algorithm~\cite{sinha-etal-2021-masked} with positions of stopwords and punctuations unchanged. No changes are made to the code for this scenario. This scenario is inspired by~\cite{sinha2020unnatural, mai2022self}. \cite{sinha2020unnatural} discovered that NL pre-trained models are insensitive to permuted sentences, which contrasts with human behavior as humans struggle to understand ungrammatical sentences, or interpret a completely different meaning from a few changes in word order. \cite{mai2022self} further introduces syntactic (shuffling) outliers into NL pre-training corpora to enhance OOD robustness and NL understanding performance.

\noindent\textbf{Scenario 4: Buggy-code.} We create buggy code using a \textit{semantic} mutation algorithm which injects more natural and realistic bugs into code than other traditional loose/strict mutators~\cite{richter2022learning}. This simulates buggy programs that the model may encounter during testing, typically absent from the training dataset, and should be taken into account by OOD code detectors according to the OOD definition~\cite{he2022distribution}. We avoid using real bug/vulnerability datasets~\cite{just2014defects4j, zhou2019devign, widyasari2020bugsinpy} due to limitations like the absence of paired comments, lack of support for Python or Java, introduction to a new dataset domain \etc. We generate buggy code for each code in CSN-Java and CSN-Python using~\cite{richter2022learning} to serve as outliers, ensuring the inliers and outliers are from the same dataset domain with the only difference being normal vs. buggy code. We focus on variable-misuse bugs, as only this mutation algorithm is available for both Python and Java in ~\cite{richter2022learning}. Variable-misuses occur when a variable name is used but another was meant in scope, and often remain undetected after compilation and regarded as hard-to-detect by recent bug detection techniques~\cite{he2022distribution, Richter2023how}. Comments remain unchanged for this scenario. %Since the mutation algorithm requires the code snippets to be successfully parsed into Concrete Syntax Trees, not all normal code can be mutated. We finally obtain 90,200/5,935 and 14,920/1,693 buggy code snippets for the original CSN-Java and CSN-Python training/testing sets.  


% We considered four OOD scenarios as comprehensive for code-related tasks involving both NL and PL modalities, or only one of them. While prior works in OOD detection typically focused only on out-domain and shuffled-text OOD scenarios, we included two additional ones: misalignment and buggy code. Other OOD instances that might occur in the PL modality, such as unusual data/control flow, are typically syntactically infeasible and can mostly be detected by leveraging static analysis tools~\cite{tree_sitter} or compilers, and thus are not included in our current design. Note that the buggy code in scenario 4 is semantically incorrect but syntactically feasible, which cannot be detected by static parsers/compilers, necessitating the use of DL models to extract code semantics.

%we initially also considered syntactically-incorrect code which can not parse into AST, but such code can be directly handled by a parser/compiler. Similarly, unusual data/control flow issues are likely to be detected by leveraging static analysis tools \REF. Therefore, we didn’t include these scenarios in our current design. 

\subsection{Model Configurations} 
For the weakly-supervised COOD+, we experiment with either the contrastive learning module (COOD+\_CL) or the binary OOD rejection module (COOD+\_BC) to compare against the combined model.  All models are trained using the Adam optimizer with a learning rate of $1e-5$, and a linear schedule with 10\% warmup steps. The batch size is set to 64, and the number of training epochs is 10. For the COOD+\_CL and COOD+, the margins in the margin-based loss are set to $0.2$. The balancing value $\lambda$ is set to $0.2$ after a grid search. The hidden layer size in the binary OOD rejection module for COOD+ is 384 ($768/2$). We also explore the robustness
and agnosticism of our COOD+ approach to different NL-PL models by replacing the GraphCodeBERT encoder with CodeBERT~\cite{feng2020codebert}, UniXcoder~\cite{guo2022unixcoder}, and ContraBERT~\cite{liu2023contrabert}.



\subsection{OOD detection model training and measurement}
%For evaluation, we focus on measuring their effectiveness using two standard metrics. In real-world deployment scenarios of ML-guided models, retaining as many ID samples as possible is crucial, as these are safely applicable to the main task training and inference, while OOD instances are rejected or handled separately. Since detection models typically output and rank OOD scores to identify OOD examples, setting a threshold is necessary to ensure a high proportion of ID data (e.g., 95\%). 

For unsupervised COOD, we use only ID data for model training, thus involving all training data from CSN-Python and CSN-Java, with 10\% randomly sampled for validation. We avoid using the CSN development dataset for validation due to its smaller size. For weakly-supervised COOD+, we randomly select 1\% of the training data and replaced them with OOD samples generated for each scenario (following~\cite{tian2020few}), resulting in a total of 4\% OOD samples and 96\% ID samples for training. During inference, both COOD and COOD+ utilize the same ratio (20\%) for inliers and outliers from each scenario, which is more convincing than using an imbalanced dataset (\ie tiny number of OOD data). Detailed dataset statistics are provided in our online appendix~\cite{cood-tool}. Since all outliers are randomly selected, we report average OOD detection results across \textit{five} random seeds of the test dataset to ensure evaluation reliability and reproducibility.  
 
Following prior work in ML~\cite{hendrycks2019using, liznerski2022exposing}, we use two standard metrics to measure the effectiveness of our COOD/COOD+ models: the area under the receiver operating characteristic curve (AUROC) and the false positive rate at 95\% (FPR95). AUROC is threshold-independent, calculating the area under the ROC curve over a range of threshold values, representing the trade-off between true positive rate and false positive rate. It quantifies the probability that a positive example (ID sample) receives a higher score than a negative one (OOD sample). Higher AUROC indicates better performance. Additionally, FPR95 corresponds to the false positive rate (FPR) when the true positive rate of ID samples is 95\%. FPR95 is threshold-dependent, where OODs are identified by setting a threshold $\sigma$ with $ P^{OOD}<1-\sigma$ ($P^{ID}>\sigma$) so that a high fraction (95\%) of ID data is above the threshold. It measures the proportion of OOD samples that are mistakenly classified when 95\% of ID samples are correctly recalled based on the threshold. Lower FPR95 indicates better performance.


\subsection{Baselines}
We compare our COOD/COOD+ against various OOD detection baselines, including adaptations of existing unsupervised NLP OOD approaches on NL-PL encoders (1-2), weakly-supervised approaches based on outlier exposure (3), and neural bug detection techniques (4-5). Since unsupervised approaches (1-2) rely on classification outputs for OOD scoring, we reformulate code search as binary classification to fine-tune the encoders similarly to~\cite{feng2020codebert}. (1-2) is supervised for code search, but unsupervised for OOD detection. For weakly-supervised baselines (3-5), we use the same number of OOD samples as COOD+ for a fair comparison. Note that the encoder backbone of (1-3) is also GraphCodeBERT. (4-5) are specifically designed for neural bug detection, thus not requiring other encoder backbone for OOD detection. 

\begin{enumerate}[leftmargin=1.5em] 
    \item \textbf{Supervised Contrastive Learning For Classification (SCL)}~\cite{khosla2020supervised}. This method fine-tunes transformer-based classification models by maximizing similarity of input pairs if they are from the same class and minimize it otherwise. Following \cite{zhou2021contrastive}, we adopts MSP, Energy, and Mahalanobis OOD scoring algorithms for OOD detection.
    
    \item \textbf{Margin-based Contrastive Learning for Classification (MCL)}~\cite{zhou2021contrastive}. This approach fine-tunes transformer-based classification models by minimizing the L2 distances between instances from the same class, and encouraging the L2 distances between instances of different classes to exceed a margin. We also detect OODs by applying MSP, Energy, and Mahalanobis OOD scoring algorithms.
    
    \item \textbf{Energy-based Outlier Exposure (EOE)}~\cite{liu2020energy}. This approach uses a few auxiliary OOD data to fine-tune the classification model with an energy-based margin loss~\cite{liu2020energy}, and then utilize Energy scores for OOD detection. 
    
    \item \textbf{CuBERT}~\cite{kanade2020learning}. CuBERT is pre-trained on a large code corpus using masked language modeling, then fine-tuned for bug detection and repair. We adapt CuBERT for OOD classification by alternatively fine-tuning it on our datasets with comments appended to their corresponding code, as CuBERT only accepts single instance inputs.

    \item \textbf{2P-CuBERT}~\cite{he2022distribution}. This method enhances CuBERT's bug detection accuracy with a two-phase fine-tuning approach. The first phase utilizes contrastive learning on generated synthetic buggy code~\cite{allamanis2021self}. For the second phase, we alternatively fine-tune CuBERT to detect OOD using our datasets. Results are reported only for CSN-Python due to the lack of Java bug generation algorithms in~\cite{he2022distribution}.

\end{enumerate}
%\yanfu{please also include the neural bug detection baseline}

\subsection{Main Task Performance Analysis}
An effective OOD detector, serving as an auxiliary component, should identify and reject OOD samples without negatively impacting the original model's performance on the main downstream task with ID data~\cite{zhou2021contrastive}. Consequently, we validate the effectiveness of our COOD/COOD+ auxiliary on the code search task using the official evaluation benchmark~\cite{guo2020graphcodebert, liu2023contrabert} by calculating the mean reciprocal rank (mRR) for each pair of comment-code data over distractor codes in the testing code corpus. Specifically, we first measure the performance of original GraphCodeBERT code search model on both ID and OOD data, whose performance is expected to be negatively affected with the presence of OOD samples. Then, we utilize our COOD/COOD+ auxiliary to filter the testing dataset by setting a threshold to retain 95\% of ID instances with higher scores (following existing ML work~\cite{ming2022delving} and the FPR95 definition), as real-world deployment typically involves few OODs. Finally, we directly use the fine-tuned encoder in COOD/COOD+ to perform code search but on the retained ID instances, and compare this performance with that on the ground-truth ID instances. If the performance loss is recovered by using COOD/COOD+, we actually enhance the trustworthiness and robustness of the original code search model (as shown in Sec. VI-D). Here trustworthiness and robustness mean that predictions of code models become more reliable when encountering OOD data in real-world deployment. Note that the dataset used for COOD/COOD+ training is the same as that used for PL-specific training of existing SOTA code search models. 


% Therefore, we leveraged our COOD+ to first identify and filter out OOD instances, and then directly used the fine-tuned encoder (\ie GraphCodeBERT) of COOD+ to perform code search on ID instances. Specifically, we filtered the corrupted testing dataset by setting a threshold to retain 95\% of ID instances with higher scores[1], given few OODs present in real-world deployment. For code search evaluation, we followed the official evaluation metric to calculate the mean reciprocal rank (mRR) for each pair of comment-code data over distractor codes in the testing code corpus. Current SOTA models for code search are currently trained and evaluated on the CSN dataset per PL, and our COOD+ model is trained exactly on CSN-Java or CSN-Python data, treating data from different distributions than the training data as outliers. Subsequently, we deploy our COOD+ model to filter out outliers and retain at least 95\%~\cite{ming2022delving} of normal samples. This allows us to assess the extent to which our models can safeguard and support pretrained NL-PL models in OOD deployment settings (\textbf{RQ4}).  \yanfu {highlight robustness and trustworthiness}

% Our evaluation results on code search (RQ4) can support the claim of enhancing trustworthiness and robustness of existing code models in real-world applications. Here trustworthiness and robustness imply that predictions of code models become more reliable. For RQ4 (as detailed in Section V-F), we leveraged our COOD+ to first identify and filter out OOD instances, and then directly used the fine-tuned encoder of COOD+ to perform code search on ID instances. Specifically, we filtered the corrupted testing dataset by setting a threshold to retain 95\% of ID instances with higher scores[1], given few OODs present in real-world deployment. For code search evaluation, we followed the official evaluation metric to calculate the mean reciprocal rank (mRR) for each pair of comment-code data over distractor codes in the testing code corpus. Our experimental results in Table V show that if we didn't perform OOD detection, the performance of the GraphCodeBERT-based code search model will drop by ~10\%, indicating a lack of robustness to OODs. In contrast, if it adopts our COOD+ detector, the performance loss will be recovered.

% Since OOD instances can be seen when models are deployed in real-world, we apply our approach for the code search task under various OOD scenarios to support this claim. Using our COOD+ model, we can distinguish OOD instances from ID ones, filter out the OOD instances, and directly leverage the encoder of our COOD+ model to perform code search. As shown in Table V, this process significantly enhances code search performance. The evaluation metric we use for code search involves taking (NL, PL) pairs as input and calculating the mean reciprocal rank (mRR) for each (comment, code) pair over distractor codes from the entire testing code corpus. The mRR metric is widely used by the existing code search models for evaluation. Table V demonstrates that accepting outliers during inference leads to a performance decline of ~10\% mRR across both datasets. Our COOD+ detector addresses this issue by identifying and filtering out OOD instances, which recover the performance loss and also improve the code search performance by ~4\%. Given that code search is a real world task, the OOD scenario design is realistic, and the datasets (CodeSearchNet and TLCS) used are also sourced from real-world, we claim that our model can enhance the trustworthiness of existing code models in real-world.

% Since OOD instances are common when models are deployed in the real-world, we apply our COOD+ detector to the code search task (RQ4) under various OOD scenarios to support this claim.  Although Sec. V-F (Main Task Analysis) describes how to use our COOD+ model for code search with the evaluation metric, we agree on the need for a clearer explanation of RQ4 and the metric involved. For RQ4, we leverage our COOD+ model to first identify and filter out OOD instances, and then directly use the fine-tuned encoder of our COOD+ to perform code search on ID instances. We follow the official evaluation metric to calculate the mean reciprocal rank (mRR) for each pair of comment-code data over distractor codes in the testing code corpus. By corrupting 15\% of the testing dataset with OOD samples, we show that the performance of a (original) fine-tuned GraphCodeBERT code search model declines by ~10\% (72.20\%-62.32\% on CSN-Java and 73.37\%-63.23\% on CSN-Python) in Table V, indicating a lack of robustness to OODs. Using COOD+ detector, we filter the corrupted testing dataset by setting a threshold to retain 95\% of ID instances with higher scores (following existing work in NLP [1]), given that the real-world deployment usually involves few OODs. We then use COOD+’s encoder to perform code search on the filtered dataset. Table V shows that our model can recover this performance loss and also improve the code search performance by ~3% (75.57%-72.20% on CSN-Java and 76.99%-73.37% on CSN Python).

% Our evaluation results on the code search task (RQ4) can support the claim of trustworthiness and robustness of existing code models in real-world applications. Please note that here trustworthiness and robustness means the predictions of code models are more reliable. For RQ4, we leveraged our COOD+ model to first identify and filter out OOD instances, and then directly used the fine-tuned encoder of our COOD+ to perform code search on ID instances. We followed the official evaluation metric to calculate the mean reciprocal rank (mRR) for each pair of comment-code data over distractor codes in the testing code corpus. Our experimental results in Table V shows that if we did not perform OOD detection, the performance of the GraphCodeBERT-based code search model will drop by ~10%. In contrast, if it adopts our OOD detector, the performance will ==(XXXX, please write the remaining sentences, then remove the following paragraph)= By corrupting 15% of the testing dataset with OOD samples, we showed that the performance of a (original) fine-tuned GraphCodeBERT code search model declines by ~10% (72.20%-62.32% on CSN-Java and 73.37%-63.23% on CSN-Python) in Table V, indicating a lack of robustness to OODs. Using COOD+ detector, we filtered the corrupted testing dataset by setting a threshold to retain 95% of ID instances with higher scores (following existing work in OOD detection [1]), given that the real-world deployment usually involves few OODs. We then utilized COOD+’s encoder to perform code search on the filtered dataset. Table V shows that our model can recover this performance loss and also improve the code search performance by ~3% (75.57%-72.20% on CSN-Java and 76.99%-73.37% on CSN Python).

% Our evaluation results on the code search task (RQ4) can support the claim of trustworthiness and robustness of existing code models in real-world applications. Please note that here trustworthiness and robustness means the predictions of code models are more reliable. For RQ4 as described in Section V-F, we leveraged our COOD+ model to first identify and filter out OOD instances, and then directly used the fine-tuned encoder of our COOD+ to perform code search on ID instances. Specifically, we filtered the corrupted testing dataset by setting a threshold to retain 95% of ID instances with higher scores[1], given that the real-world deployment usually involves few OODs. For code search evaluation, we followed the official evaluation metric to calculate the mean reciprocal rank (mRR) for each pair of comment-code data over distractor codes in the testing code corpus. Our experimental results in Table V shows that if we did not perform OOD detection, the performance of the GraphCodeBERT-based code search model will drop by ~10%, indicating a lack of robustness to OODs. In contrast, if it adopts our COOD+ detector, the performance loss will be recovered.