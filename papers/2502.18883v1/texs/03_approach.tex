\section{Approach}\label{sec:approach} 

In this section, we first formally define the OOD code detection problem for (NL, PL) models (Sec. IV-A), then introduce the overall proposed framework (Sec. IV-B), and finally present details of unsupervised COOD and weakly-supervised COOD+ in Sec. IV-C and Sec. IV-D, respectively.
\subsection{Problem Statement}

Since current state-of-the-art code-related models~\cite{guo2020graphcodebert, guo2022unixcoder} typically extract code semantics by capturing the semantic connection between NL (\ie comment) and PL (\ie code) modalities, we formally defined OOD samples involving these two modalities in the SE context by following the convention in ML~\cite{yang2021generalized, zhou2021contrastive}. Consider a dataset comprising training samples $((t_1, c_1), y_1), ((t_2, c_2), y_2), ...$ from the joint distribution $P((T, C),Y)$ over the space $\mathcal{(T, C)}\times \mathcal{Y}$, and a neural-based code model is trained to learn this distribution. Here, $((t_1, c_1), y_1)$ represents the first input pair of (comment, code) along with its ground-truth prediction in the training corpus. $T$, $C$ and $Y$ are random variables on an input (comment, code) space $\mathcal{(T, C)}$ and a output (semantic) space $\mathcal{Y}$, respectively. OOD code samples refer to instances that typically deviate from the overall training distribution due to distribution shifts. The concept of distribution shift is very \textit{broad}~\cite{yang2021generalized, salehiunified} and can occur in either the marginal distribution $P(T, C)$, or both $P(Y)$ and $P(T, C)$.


We then formally define the OOD code detection task following~\cite{hsu2020generalized, liu2020energy, tian2020few, kim-etal-2023-pseudo} as follows. Given a main code-related task (\eg clone detection, code search, \etc), the objective here is to develop an \textit{auxiliary} scoring function $g: \mathcal{(T, C)} \rightarrow \mathcal{R}$ that assigns higher scores to normal instances where $ ((t, c), y) \in P((T, C),Y)$, and lower scores to OOD instances where $((t, c), y) \notin P((T, C),Y)$. Based on whether to use OOD instances during the main-task training of pre-trained NL-PL models, we define OOD for code in two settings, namely unsupervised and weakly-supervised learning. For the unsupervised setting, only normal data is used in the main-task training. Conversely, weakly-supervised approaches utilize ID and a tiny collection of OOD data (\eg 1\% of ID data)~\cite{tian2020few} in training. In this context, the output space $\mathcal{Y}$ is typically a binary set, indicating normal or abnormal, which is probably unknown during inference. Due to the small number of training OOD data, the OOD samples required by our COOD+ and other existing weakly-supervised approaches~\cite{ruff2020deep, tian2020few} in ML can be generated at minimal cost and feasibly verified by human experts when necessary.

% \textcolor{blue}{In line with prior work~\cite{ming2022delving, DBLP:journals/corr/abs-2104-08812}, a threshold should be set during inference to filter out OOD code instances and retain most ID code instances (e.g., 95\%), considering that real-world deployment usually involves a small number of OODs. Ensuring a high proportion of ID data is crucial to prevent the OOD auxiliary scoring from adversely affecting the performance of the models on their main code-related tasks.}

%By systematically generating OOD data representative of realistic SE scenarios, we aim to show the merits of OOD detection in SE deployment settings. Moreover, weak supervision provides guarantees that the performance of the main downstream task is not adversely affected by the OOD-aware finetuning process~\cite{majhi2021weakly}.

% When OOD samples are drawn from a distribution resulted from distribution shifts on $P(X,Y)$, they do not conform to the patterns, characteristics, or statistical properties of the original $P(X,Y)$ previously used to develop, train, and validate the model, leading to performance and reliability concerns. Hence, it is important to detect when SE models encounter OOD samples for appropriate counter measures, safeguarding their deployment in practical settings.

\subsection{Overview}
\begin{figure*}[!thb]
\begin{center}
 \includegraphics[width=0.98\textwidth]{figs/COOD.pdf}
    \caption{The Overview of Our Proposed COOD and COOD+ Approaches for OOD Detection %The COOD+ approach consists of two main components: a contrastive learning module and a binary OOD rejection network. (1) For unsupervised COOD, only the contrastive learning module is utilized, focusing on maximizing the cosine similarity difference between ID and OOD data. (2)The weakly-supervised COOD includes both the contrastive learning module and the binary OOD rejection network which identifies OOD samples with low ID probabilities using a threshold.
    }
    \label{fig:cad}
\end{center}

\end{figure*}

Overall, there are two versions of our COOD approach: unsupervised COOD and weakly-supervised COOD+. Given a multi-modal (NL, PL) input, the unsupervised COOD learns distinct representations based on a contrastive learning module by utilizing a pre-trained Transformer-based code representation model (\ie GraphCodeBERT~\cite{guo2020graphcodebert}). Then, these representations are mapped to distance-based OOD detection scores in order to indicate whether the test samples are OODs during inference. The weakly-supervised COOD+ further integrates a improved contrastive learning module with a binary OOD rejection module to enhance the detection performance by using a very tiny number of OOD data during model training. The OOD samples are then identified by the detection scores produced by the contrastive learning module as well as the prediction probabilities of the binary OOD rejection module. 

\subsection{Unsupervised COOD}

Our unsupervised COOD approach consists of a contrastive learning (CL) module trained only on ID samples. Specifically, given (comment, code) pairs as input, we fine-tune a comment encoder and a code encoder through a contrastive objective to learn discriminative features, which are expected to help identify OOD samples based on a scoring function. 

The (comment, code) pairs are first converted into the comment and code representations, which are processed by the comment and code encoder, respectively. We use the pre-trained GraphCodeBERT model~\cite{guo2020graphcodebert} as the encoder architecture (\ie backbone). GraphCodeBERT is a Transformer-based model pre-trained on six PLs by taking the (comment, code) pairs as well as the data flow graph of the code as input, which has shown superior performance on code understanding and generation tasks. All the representations of the last hidden states of the GraphCodeBERT encoder are averaged to obtain the sequence-level features of comment and code.


\textbf{Contrastive Learning Module.} To achieve the contrastive learning objective, we fine-tune the base (GraphCodeBERT) encoders with the InfoNCE loss~\cite{oord2018representation}. The comment and code encoders follow the Siamese architecture~\cite{guo2022unixcoder} since they are designed to be identical subnetworks with the same GraphCodeBERT backbones, in which their parameters (\ie weights and biases) are shared during fine-tuning. Parameter sharing can reduce the model size and has shown state-of-the-art performance for the code search task~\cite{shi2023cocosoda}. To extract discriminative features for (comment, code) pairs, we organize them into functionally-similar positive pairs and dissimilar negative (unpaired) pairs. Through a contrastive objective, positive pairs are drawn together, while unpaired comment and code are pulled apart. Specifically, for each positive (comment, code) pair $(t_i, c_i)$ in the batch, the code in each of other pairs and $t_i$ are constructed as in-batch negatives, similarly for the comment side. The loss function then formulates the contrastive learning as a classification task, which maximizes the probability of selecting positives along the diagonal of the similarity matrix (as shown in Fig. \ref{fig:cad}) by taking the \textit{softmax} of projected embedding similarities across the batch.  The loss function can be summarized as follows: 

  
\begin{align}\small
\label{eq:infonce}
\begin{split}
    \mathcal{L}^{CL} &= -\frac{1}{2N}(\sum_{n=1}^{N}\log \frac{e^{sim(v_{t_i},v_{c_i})/\tau}}{\sum_{j=1}^{N}e^{sim(v_{t_i},v_{c_j})/\tau}}\\ &+ \sum_{n=1}^{N}\log \frac{e^{sim(v_{t_i},v_{c_i})/\tau}}{\sum_{j=1}^{N}e^{sim(v_{t_j},v_{c_i})/\tau}})
\end{split}
\end{align}
where $v_{t_i}$ and $v_{c_i}$ represent the extracted features of the comment $t_i$ and the code ${c_i}$. $\tau$ is the temperature hyperparameter, which is set to 0.07 following previous work~\cite{shi2023cocosoda}.  $sim(v_{c_i},v_{t_i})$ and $sim(v_{t_i},v_{c_j})/sim(v_{t_j},v_{c_i})$ represent the cosine similarities between comment and code features for positive and negative pairs, respectively. $N$ is the number of input pairs in the batch. InfoNCE loss is designed for self-supervised learning and learns to distinguish positive pairs from in-batch negatives. Compared to other contrastive losses~\cite{khosla2020supervised, zhou2021contrastive}, it can take advantage of large batch size to automatically construct many diverse in-batch negatives for robustness representation learning, which is more effective to capture the alignment information between comment and code. % Despite~\cite{khosla2020supervised, zhou2021contrastive} being able to learn class-aware representations, they are designed for supervised learning which requires labeled data, and specifically for the code search task, a large amount of negative pairs. 


\textbf{Scoring Function.} Existing OOD detection techniques in ML derive scoring functions based on model's output, which typically map the learned class-probabilistic distributions to OOD detection scores for testing samples. Maximum Softmax Probability (MSP)~\cite{hendrycks2016baseline} is commonly used for OOD scoring. This method uses the maximum classification probability $\max_{l\in L}\textit{softmax}(f(v_{t}, v_{c}))$, where $f(v_{t}, v_{c})$ is the output of the classification model, with low scores indicating low likelihoods of being OOD. However, NL-PL code search models typically utilize the similarity retrieval scores of NL-PL output representations to make predictions. Therefore, to enable simultaneous similarity and OOD inference, we alternatively extract cosine similarity scores of testing NL-PL pairs as OOD detection scores, denoted as $P^{CL}=sim(v_{c},v_{t})$. The underlying intuition behind this scoring metric is that OOD testing samples should receive low retrieval confidence from the model fine-tuned on ID data, which establishes a closer relationship between ID (comment, code) pairs. Hence, this scoring function also assigns higher scores to ID data and lower scores to OOD data similar to previous scoring methods.

\subsection{Weakly-Supervised COOD+}

To further enhance the performance of unsupervised COOD, we extend it to a weakly-supervised detection model, called COOD+, which takes advantage of a few OOD examples. Inspired by~\cite{duong2024general}, our COOD+ combines an improved contrastive learning (CL) and a binary OOD rejection classifier (BC). The improved CL module adopts a margin-based loss~\cite{xue2009svm} which enforces a margin of difference between the cosine similarities of aligned and unaligned (comment, code) pairs, and constrains the cosine similarities of OOD pairs below another margin. The BC module integrates features from both comments and code to calculate the probabilities of OOD pairs. The OOD scoring function is then designed by combining the cosine similarity scores from the CL module and the prediction probabilities from the BC module. Below, we detail each component of our weakly-supervised COOD+.

\textbf{Improved Contrastive Learning (CL) Module.} Given a batch of $N$ input pairs (comprising $N-K$ ID pairs and $K$ OOD pairs), the latent representations are first obtained from the comment and code encoders. Then the margin-based loss is leveraged in the CL module to distinguish representations of ID and OOD data by constraining the cosine similarity. Specifically, the margin-based contrastive loss is first applied to $N-K$ ID code to maximize the difference between aligned (comment, code) pairs and incorrect pairs for each batch: 


%%%
\begin{equation}\scriptsize
\label{eq:contrast_id}
    \hspace{-0.2cm} \mathcal{L}^{ID} = \sum_{i=1}^{N-K} \left(\frac{1}{N}\sum_{j=1, j \neq i}^N \max \left(0,m - s(v_{t_i^+}, v_{c_i^+}) + s(v_{t_j^-}, v_{c_i^+})\right)\right)
\end{equation}


\noindent $s(v_{t_i^+}, v_{c_i^+})$ represents the cosine similarity of representations between each aligned ID pair from all the $N-K$ aligned pairs, and $s(v_{t_j^-},v_{c_i^+})$ represents the cosine similarity of representations between each ID code and all the other $N-1$ comments (\ie the comment is either not aligned with the ID code or from OOD comments). Thus, this margin-based loss encourages the difference between the aligned pairs and the incorrect pairs greater than margin $m$. 

Regarding the $K$ OOD code, we enforce a constraint on the cosine similarity between each OOD code and all the comments, ensuring that the similarity remains below a margin $m$. This constraint is necessary because each OOD code should not align with its corresponding comment, nor with any of the other $K-1$ OOD comments and the $N-K$ ID comments. The loss function is denoted as follows:
\begin{equation}\small
\label{eq:contrast_ood}
\mathcal{L}^{OOD} = \sum_{k=1}^{K}\left(\frac{1}{N}\sum_{i=1}^N \max \left(0,-m + sim(t_j^-, c_k^-)\right)\right),
\end{equation}

\noindent where $sim(t_j^-, c_k^-)$ represents the cosine similarity between each of the $K$ OOD code and all $N$ comments.
Finally, the overall loss for the contrastive module can be expressed as: 


\begin{equation}\small
\label{eq:contrast}
    \mathcal{L}^{CL} = \frac{1}{N}\left(\mathcal{L}^{ID}+ \mathcal{L}^{OOD}\right).
\end{equation}


\textbf{Binary OOD Rejection (BC) Module.}
Besides the CL module, we also introduce a classification module under weakly-supervision for identifying OOD samples. %The key challenge lies in effectively integrating features from both NL and PL to improve classification accuracy.
Inspired by the Replaced Token Detection (RTD) objective utilized in~\cite{feng2020codebert}, we bypass the generation phase since our OOD data are generated prior to training. Therefore, we directly train a rejection network responsible for determining whether (comment, code) pairs are OOD or not, which can be framed as a binary classification problem. Our binary OOD rejection network comprises a 3-layer fully-connected neural network with \textit{Tanh} activation, and the input is based on the concatenation of features from the comment and code encoders: 
$v_i = (v_{t_i}, v_{c_i}, v_{t_i} - v_{c_i}, v_{t_i} + v_{c_i}).$ Apart from utilizing the comment and code features, we also incorporate feature subtraction $v_{t_i} - v_{c_i}$ and aggregation $v_{t_i} + v_{c_i}$.  Additionally, we apply the sigmoid function to the output layer, producing a prediction probability that indicates whether the sample is OOD. We then use binary cross entropy loss for this module:

\begin{equation}\small
    \mathcal{L}^{BC} = \frac{1}{N}\sum_{i=1}^{N-K}(y_i\log p(v_i)+(1-y_i)\log(1-p(v_i))),
\end{equation}


\noindent where $p(v_i)$ is the output probability of the BC module, and $y_i \in [0, 1]$ is the ground-truth label. $y_i = 1$ indicates the input sample is an inlier, while $y_i = 0$ signifies it is an outlier. 

Hence, for weakly-supervised COOD+, we combine the objectives of the CL and the BC modules to jointly train our model, where $\lambda$ is a weight used to balance the loss functions:

\begin{equation}\small
\label{eq:loss}
    \mathcal{L} = \mathcal{L}^{CL} + \lambda \mathcal{L}^{BC}.
\end{equation}

\noindent\textbf{Combined Scoring Function.}
Similar to the unsupervised COOD approach, we utilize the diagonals of the similarity matrix as the OOD detection scores obtained from the CL module. To further improve the detection performance of the weakly-supervised version, we combine these $P^{CL}$ scores with the output probabilities of the BC module, denoted as $P^{BC}$. Here, we convert cosine similarity scores into probabilities using the sigmoid function $P^{CL*}=\sigma(sim(v_{c},v_{t}))$, then use multiplication to create the overall scoring function, yielding $P^{ID}=P^{CL*}\times P^{BC}$. We anticipate that higher scores will be assigned to ID pairs, while lower scores will be assigned to OOD pairs. This combined scoring function aims to enhance the discrimination between inliers and outliers, leading to more effective OOD detection.