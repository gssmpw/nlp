\begin{abstract}

Numerous machine learning (ML) models have been developed, including those for software engineering (SE) tasks, under the assumption that training and testing data come from the same distribution. However, training and testing distributions often differ, as training datasets rarely encompass the entire distribution, while testing distribution tends to shift over time. Hence, when confronted with out-of-distribution (OOD) instances that differ from the training data, a reliable and trustworthy SE ML model must be capable of detecting them to either abstain from making predictions, or potentially forward these OODs to appropriate models handling other categories or tasks. 

In this paper, we develop two types of SE-specific OOD detection models, unsupervised and weakly-supervised OOD detection for code. The unsupervised OOD detection approach is trained solely on in-distribution samples while the weakly-supervised approach utilizes a tiny number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate that our proposed methods significantly outperform the baselines in detecting OOD samples from four different scenarios simultaneously and also positively impact a main code understanding task.




%This work aims to develop multi-modal contrastive learning for out-of-distribution (OOD) detection to enhance the trustworthiness of machine learning for software engineering. OOD detection involves identifying whether a testing sample deviates from the distribution of training data. It is critical for ensuring the trustworthiness of machine learning in high-stake applications, such as autonomous driving and software engineering, where the developed systems must be capable of handling unexpected situations. While OOD detection has been extensively studied in computer vision and natural language processing, it remains underexplored in software engineering. In this paper, we develop two types of OOD detection models, unsupervised OOD detection and weakly-supervised OOD detection. The unsupervised OOD detection method is trained solely on in-distribution samples while the weakly-supervised approach uses a small number of OOD samples to further enhance the detection performance in various OOD scenarios. Extensive experimental results demonstrate our proposed methods significantly outperform the baselines in detecting OOD samples under four different scenarios and also positively impact the main downstream code understanding task. 

% Additionally, we 

% This work delves into the challenge of out-of-distribution (OOD) detection in machine learning (ML) models, particularly within the context of software engineering. Unlike closed-world ML assumptions, real-world scenarios often involve data distributions that change over time or differ between training and testing phases. Detecting OOD instances, which do not fit known categories, is crucial for ensuring model trustworthiness, especially in software engineering, where large systems must handle unexpected situations. Despite OOD d
% etection being a well-explored topic in domains like computer vision and natural language processing, its relevance to code-related tasks is overlooked. In our paper, we propose a novel approach for multi-modal OOD detection in code. We also establish a benchmark with dataset construction and unsupervised baselines. Extensive experimental results show the excellent performance of the proposed approach in detecting OOD samples under four different scenarios, while positively impacting the main downstream code understanding task.
\end{abstract}