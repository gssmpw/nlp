
\section{Discussions}\label{sec:discussions}

\textbf{Analysis of the Overconfidence of MSP with Conformal Prediction.} DNN models pre-trained on ID data are prone to misclassify OOD samples as ID due to overconfident MSP scores~\cite{lee2017training, liang2017enhancing}. This issue arises from spurious correlations between OOD and ID features, such as entities or syntactic patterns in NL and PL data~\cite{zheng2020aug, wu2022revisit}. For example, OOD inputs with common ID syntactic patterns may receive high ID scores. To overcome overconfident predictions, previous work in ML explored techniques like temperature scaling~\cite{liang2017enhancing} and  confidence calibration using adversarial samples~\cite{lee2017training, bitterwolf2020certifiably}. In contrast, COOD+ leverages weakly-supervised contrastive learning with a small set of OOD samples to prevent the alignment of OOD pairs, and adopts a binary OOD rejection module to better differentiate OOD and ID representations. We further verify COOD+'s ability to address overconfidence using Conformal Prediction (CP)~\cite{angelopoulos2023conformal}. Post-hoc CP converts OOD scores into prediction sets with a high user-specified coverage (e.g., 95\%), correcting overconfident thresholds during calibration. This ensures prediction sets conform statistically to the desired coverage. We experimentally demonstrated that COOD+ achieves near-optimal prediction set sizes with 95\% coverage, effectively identifying true OODs with statistical guarantees. In contrast, MCL+MSP, the best MSP-based method, still struggles with overconfident OODs.  Therefore, COOD+ can effectively overcome the overconfidence issue of MSP. Full details of the experiments and result analysis are in the online appendix~\cite{cood-tool}.

% Given an OOD testing sample, DNN models pre-trained on ID data are prone to predict a higher MSP confidence score than the threshold and wrongly identify it as an ID sample~\cite{lee2017training, liang2017enhancing}. This overconfidence issue limits the effectiveness of OOD detection. For NL data, this is caused by the spurious correlation between OOD and ID features such as entities and syntactic structures~\cite{zheng2020out, wu2022revisit}. Such correlation also occurs in PL data. For example, an OOD PL input with the syntactic structure ``def ... if ... return ... else ... return ..." may receive an ID score if this pattern is commonly used in other ID inputs. To overcome overconfident predictions, previous work explored techniques such as temperature scaling~\cite{liang2017enhancing}, confidence calibration using adversarial samples~\cite{lee2017training, bitterwolf2020certifiably}, or adaptive class-dependent threshold~\cite{wu2022revisit}. 

% We further verify whether COOD+ overcome the overconfidence issue through the lens of Conformal Prediction (CP)~\cite{angelopoulos2023conformal}. We provide a detailed description of the experimental design and results analysis in our supplementary material and  online appendix~\cite{cood-tool}. Specifically, Post-hoc CP transforms OOD scores into prediction sets that are guaranteed to contain the ground-truth values of OOD from an independent calibration set with probability of at least a high user-specified percentage or coverage (e.g., 95\%). Consequently, overconfident thresholds for OOD scores can be corrected during the calibration phase after training, and the prediction sets constructed from testing data with calibrated thresholds are statistically guaranteed to conform to the desired coverage (e.g., 95\%) of the ground-truth OOD labels. When CP is applied to the OOD detection scores, all scores have the same statistical guarantee, but better OOD scores will give tighter prediction sets. We demonstrate that COOD+ achieves the smallest average prediction set size very close to 1 (optimal) with 95\% coverage in the online appendix, indicating its ability to identify true OODs with statistical guarantee. Also, as shown in the online appendix, MCL+MSP, the best performing method using MSP scores, still suffers from over confident OODs. Therefore, our proposed COOD+ can effectively overcome the overconfidence issue of MSP.

\textbf{OOD detection with large language models (LLMs).} It's worth noting that transformer-based code models (\eg GraphCodeBERT~\cite{guo2020graphcodebert}) and LLMs share the same underlying transformer architecture. Scaling up transformer-based code models and training them on vast amounts of code data allows LLMs~\cite{roziere2023code} to perform a wide range of code-related tasks, making coding less labor-intensive and more accessible to end-users. Since LLMs are transformer-based, they are also vulnerable to OOD data, with potentially worse performance degradation due to error accumulation during auto-regressive inference. Thus, identifying OOD samples is crucial to knowing when to trust LLM outputs. Our proposed OOD code framework techniques can be applied to these larger transformer-based code models, similarly as demonstrated in our experiments with different encoders in Table~\ref{tab:ablation}.

% Since Large Language Models (LLMs) show great potential in code-related tasks, this paper could be beneficial for adding a paragraph to describe the current role of transformer-based code models in the LLM era and how the findings can be extended to LLMs.
% Could you add a paragraph on the role of transformer-based code models in the LLM era and extend your findings to LLMs?
% Provide a discussion on the impact of using transformer based code models in the LLM era and how the findings in the current paper could be extended.


\textbf{Generalization of COOD/COOD+ to other code-related tasks.} During software development, developers often write comments following code snippets (methods/functions). Therefore, from a realistic perspective, our framework is generalizable to many code understanding tasks beyond code search, such as clone detection and defect detection, that use (comment, code) pairs as input. All that is needed is identifying the ID dataset and out-domain data, as the four OOD scenarios outlined in this paper are relevant to most tasks. For instance, in clone detection, (comment, code) pairs could first be processed by our framework to identify OOD samples before checking for clones. Unfortunately, since existing clone and defect detection datasets typically lack comments, we haven't applied our framework to these tasks. However, the framework has strong potential to enhance these tasks as more realistic bi-modal datasets become available in the future.

% Our COOD/COOD+ framework can be applied to any code-related tasks, particularly code understanding tasks, as long as their input consists of (comment, code) pairs. During software development, developers often write comments following code snippets (methods/functions). Thus, from a realistic perspective, our framework can be generalized to many code understanding tasks, such as clone detection and defect detection, beyond code search. All that is needed is to determine the ID dataset and the out-domain data since all four OOD scenarios are generally relevant to every task. For instance, in clone detection, before checking whether two (comment, code) pairs are clones, we can first input each pair into our framework (after dataset-specific training) to identify whether they are OODs under the four scenarios designed in our paper. Unfortunately, the currently available clone and defect detection datasets only include code without corresponding comments. This is why we haven't applied our framework to these tasks. However, there is every reason to believe that our framework will be useful for these tasks when more realistic bi-modal datasets are available in the future.

% Note that the encoder backbone (i.e., NL-PL pre-trained models) leveraged in our COOD/COOD+ framework has already shown state-of-the-art performance across various downstream tasks. Therefore, these tasks will be negatively impacted when encountering OODs in the real-world deployment due to the vulnerability of NL-PL pre-trained models to OOD data.


% I was hoping to see some novelty in terms of downstream applications in a variety of SE tasks. If the proposed methodology improves the OOD input detection in terms of code, how such detection models may generalise to a variety of deep-learning-based software engineering tasks?

% Secondly, it is not clear whether the type of SE downstream tasks may have influence on the type of OOD data needed. The paper discussion more revolves around the detection of OOD samples only and only RQ5 shows some results on a downstream tasks.

% While the four OOD scenarios are presented, for an arbitrary SE task, I am wondering how to identify OOD data sample ro improve the model performance.  I think the generalisability of the evaluation and approach for multitudes of SE tasks is missing.

% how the findings can be extended to other domains

% practicality of the COOD+ approach
