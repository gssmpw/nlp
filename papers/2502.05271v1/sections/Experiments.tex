
\subsection{Simulation Experiments Setup}
\label{sec:exp_sim_setup}

\begin{figure}[t]
 \centering
\includegraphics[width=.995\linewidth]{figures/simulation_setting.png}
\label{fig:simulation_setting}
 \vspace{-.5em}
\caption{Two simulation settings. Left: simple dynamic environment, without legged control. Right: fully dynamic environment, controlling all the robot's motors.}
 \vspace{-0.2cm}
\end{figure}



\section{Experiments}
 We evaluate \method~through experiments conducted both in simulation and on the real world. In this section, we first provide an overview of our system set. Next, we elaborate on the simulation experiment setup and present the results of the simulation experiments. We then evaluate the learned policy on hardware by comparing its performance to various baselines, including both learning-based methods and a teleoperation based method. Finally, we demonstrate the applicability of the learned object-moving policy to real-world scenarios. Our results can be best watched in the supplementary video.



\subsection{System Setup}

In our experiment, we use the Boston Dynamics Spot robot, a quadrupedal robot equipped with a robotic arm mounted on its body as our platform. It features three actuators per leg and a seven-degree-of-freedom robotic arm, including a gripper motor, resulting in a total of 19 motors. The robot's default height is 610~mm and the robot arm's extension up to 984~mm, the size of robot allows it to manipulate large objects. The objects involved in our experiments span a variety of shapes, including chairs, tables, and standing-stick. We train separate policies to handle each object type. The robot control policy operates at 20 Hz and is trained in Genesis simulator~\citep{Genesis} using Proximal Policy Optimization (PPO)~\citep{PPO}. During training, 4096 environments are simulated in parallel on a single NVIDIA GeForce RTX 3080 Ti GPU for a period of about 4 hours. On hardware, we use the simulation trained policy without further fine-tuning.

For demonstration dataset, we utilize the OMOMO dataset~\citep{OMOMO}, a high-quality motion capture dataset of human-object interactions that provides precise motion data for both humans and objects. The dataset contains all types of object involve in our experiment. During dataset collection for training a policy for a specific object, we aggregate all relevant demonstrations. For example, when training a policy for moving chairs, we include demonstrations of humans moving both wooden chairs and white chairs from the OMOMO dataset. The result demonstration collections contain diverse motion trajectories of humans moving a type of object. To simplify motion retargeting, we exclude demonstrations involving two-arm object manipulation. However, we provide a brief analysis of using two-arm demonstrations for policy learning in Sec.~\ref{sec:2-arms}. The detailed information of the dataset is provided in the appendix.


 
\begin{figure*}[t]
 \centering


\includegraphics[width=.99\linewidth]{figures/simulation_result.png}
\caption{Quantitative results of simulation experiments. Here, `SD' represents simple-dynamics environment while `FD' indicates full-dynamics environment. Our result indicates \textbf{\method} outperform baseline methods in almost every settings.
}
\vspace{-0.5cm}
\label{fig:simulation_barplot}
\end{figure*}




 We conducted our simulation experiment on two types of simulation environments: a simplified dynamics environment and a full dynamics environment. The simplified dynamics environment is designed to align with the hardware interface, where we do not have direct control over Spot's leg motors. In this setup, the robot policy outputs the robot's root linear and angular velocities in the robot’s local coordinate frame, along with the desired arm poses. This results in an action space of 13 dimensions (6 for root velocities and 7 for arm poses). At each environment step, the robot's root velocity is set to the policy output, and the physics simulation proceeds with the desired arm pose as a target. The full dynamics environment, on the other hand, is used to validate our method without the limitations imposed by the hardware interface. In this setup, the policy has access to all motor controls, leading to an action space of 19 dimensions, with each dimension corresponding to the desired pose of an individual motor. 




For the objects in the simulation, we construct custom object models using primitive geometries to approximate the object models provided in the OMOMO dataset. This approach ensures that the final object shapes are similar to those in the dataset while significantly improving computational efficiency. Using the original mesh files from OMOMO, which contain complex triangular faces, would increase computational complexity during object-robot interactions, especially under contact dynamics.



When resetting the simulation environment, the robot's xy-position is set to the origin of the simulation, and the object is placed at a position and orientation that align with the reference demonstrations. To introduce variability, we apply randomization to the robot's initial height, as well as to the object's initial position and orientation, its weight, and its friction coefficient. We use a single geometric model for each object type, meaning that the target object's shape is not randomized. This decision is motivated by two key reasons. First, in our simulation setup, all objects must be loaded at the start of the simulation. During each simulation step, the contact state of every object is computed, even if only one object is manipulated in a given episode. This approach would result in significant computational overhead and slow down the simulation. Second, we argue that geometric differences in the objects are ultimately reflected in dynamic variations once the robot has grasped the object. These dynamic differences can be adequately addressed by randomizing other properties, such as friction coefficients and the object's initial position, which effectively cover the variations caused by shape differences. 





\begin{figure*}[t]
 \centering
\includegraphics[width=.99\linewidth]{figures/hardware_results.png}

 \vspace{-.5em}
\caption{Summary of Hardware Quantitative Results. We compare our proposed \textbf{\method} with two learning-based baselines, \textbf{RL-EE} and \textbf{RL-IK}, as well as two teleoperation methods, \textbf{Teleop-Root} and \textbf{Teleop-Root-Arm}, across six evaluation metrics. The results show that \textbf{\method} outperforms all baseline methods in every evaluation.}
\label{fig:hardware_results}
 \vspace{-0.5cm}
\end{figure*}


We evaluate our method by comparing it against several baseline methods. The methods used in our simulation experiments are as follows:

\begin{itemize} 

\item \textbf{RL}: This baseline uses reinforcement learning to train a robot control policy for manipulating the target object. The reward function is defined based on the difference between the object’s root 2D position and its reference trajectory.

\item \textbf{RL-EE}: This baseline incorporates the demonstrator's end-effector position in the global frame alongside the object’s root trajectory as imitation goals. Compared to RL, RL-EE uses the end-effector position as a heuristic to guide the exploration of the solution space, aiding policy convergence.

\item \textbf{RL-IK}: Similar to RL-EE, this baseline incorporates the demonstrator’s end-effector position but in the robot's local frame. This provides localized guidance during policy training by referencing both the object’s root trajectory and the end-effector’s position relative to the robot.

\item \textbf{\method~(ours)}: \method \ leverages both the object’s movement trajectory and the demonstrator’s body movement as heuristics for policy learning. This approach provides rich information for the robot, enabling it to learn skills for stable object manipulation.

\end{itemize}
These methods are evaluated based on their ability to track reference object trajectories. The tracking score $v^{track}$ is defined as 
\begin{align}
\label{eqn:norm_error_metric}
   v^{track} = \frac{1}{T}\sum_{t=0}^{T} e^{||x^{ho,xy}_t - x^{ro,xy}_t||} .
\end{align}
here, $x^{ho,2d}$ and $x^{ro,2d}_t$ are referenced and 
 experiment object xy-position, heading direction at time instance t and T represent the total time of a single experiment. The closer the object's trajectory is to the reference, the higher the score. We run each method with 5 random seeds, and the evaluation is performed using 10 randomly sampled reference trajectories for each seed.







\subsection{Simulation Comparison Results}
Figure~\ref{fig:simulation_barplot} presents a quantitative evaluation of the simulation results. The results demonstrate that the proposed \method \  outperforms all baselines in both the simplified and full dynamic environments. The \textbf{RL} baseline exhibits the lowest average score with a high variance. This can be attributed to the lack of heuristic guidance, which causes the RL method to struggle in finding effective solutions and often leads to convergence at a suboptimal local minima. The \textbf{RL-EE} baseline improves upon RL by providing the global position of the end-effector, which corresponds to the robot-object contact position. In some chair and table experiments, \textbf{RL-EE} achieve similar results compared to \textbf{\method}. However, since it lacks information about the robot’s arm pose, the robot often adopts unnatural arm configurations that are difficult to transfer to hardware. We will discuss this in the later section. The \textbf{RL-IK} baseline incorporates arm pose information by measuring the position of the end-effector in the robot’s local frame. While this approach provides localized guidance, it performs poorly in certain scenarios, such as when manipulating a chair (e.g., \textbf{\method} achieves an  0.79$\pm$0.04 score versus 0.55$\pm$0.06 for \textbf{RL-IK}). This performance drop occurs because the arm poses measured in the robot’s local frame do not naturally translate into effective arm poses for the robot due to differences in morphology between humans and the robot, hence making the policy difficult to generalize to diverse target object velocity.




\subsection{Hardware Experiments Setup} \label{sec:hardware_setup}



\begin{figure*}[t]
 \centering
\includegraphics[width=.99\linewidth]{figures/hardware_visual_result_3.pdf}

\caption{Robot moving a chair using different methods. \textbf{\method} shows stable chair moving. \textbf{RL-EE} and \textbf{RL-IK} result in robot self-collision and robot-object collision. \textbf{Teleop-Root} and \textbf{Teleop-Root-Arm} cannot stably move the chair, causing the chair to fall off. }
\label{fig:hardware_visual}
 \vspace{-0.3cm}
\end{figure*}


We conduct hardware experiments to evaluate the performance of our method in the real world. Experiments are performed using the Spot robot from Boston Dynamics. We deploy the policy trained in the simple dynamic simulation to the real-world robot without fine-tuning. To provide a comprehensive evaluation of our proposed method, we compare \method~against four baselines, including two learning-based methods and two teleoperation-based methods. These evaluations are conducted on three types of objects: chairs, tables, and racks, focusing on three aspects of the controller: capability, robustness, and controllability. In our hardware evaluation, unlike simulation where the robot is initialized at its default pose, we first teleoperate the robot to have a stable grasp with the target object and evaluate our method from that point, focusing on the performance of the `moving' part.

The two learning-based baselines are \textbf{RL-EE}, \textbf{RL-IK} introduced in section:~\ref{sec:exp_sim_setup}. The two teleoperation baselines are:

\begin{itemize}
    \item \textbf{Teleop-Root}: In this baseline, an operator teleoperates the robot by directly controlling its root linear and angular velocities, while the arm's movements rely on the robot's passive compliance for object interaction.
    \item \textbf{Teleop-Root-Arm}: Extending Teleop-Root, this baseline sets the robot arm's desired position to the initial contact pose, providing a more direct arm control during manipulation.
\end{itemize}


The evaluation focuses on three critical properties of the controller, each capturing a unique dimension of performance. \textbf{Capability (CAP)} measures the robot's ability to achieve maximum velocity while maintaining stability. \textbf{Robustness (ROB)} assesses the controller's performance across varying conditions and object dynamics. \textbf{Controllability (CON)} evaluates the precision and accuracy of trajectory tracking during task execution.


We define two metrics for each of the properties in a total of six metrics:

\begin{itemize}
    \item \textbf{Max Linear Velocity (LIN-CAP)}: Measures the maximum absolute linear velocity at which the robot can operate stably without the object falling off the gripper or colliding with the robot. Stability is defined as maintaining control for 8 seconds. Each method is evaluated over 5 runs, with success requiring at least 4 successful trials.We measured the maximum linear velocity, starting at 0.5 m/s and adjusting by $\pm0.05$ m/s based on stability until the maximum was found.
    \item \textbf{Max Angular Velocity (ANG-CAP)}: Measures the maximum absolute angular velocity under the same stability criteria and measure strategy as LIN-CAP.
    \item \textbf{Object Diversity (DIV-ROB)}: Tests the trained policy on objects with varying weights and textures to assess robustness against different dynamics. For chairs, three variants with different textures and weights are used. For tables, two different tables are tested. For racks, a rack and a standing lamp are used for comparison. Success is measured by the robot moving these objects at medium speed for 8 seconds. For every object, we run 10 trials, and the score is measure by the success rate. $v^{DIV-ROB} = \frac{1}{10 N} 
    \sum_1^N\mathbf{1} $(if success). We list the objects we used for this experiment in the appendix.
    \item \textbf{Initial Condition Sensitivity (IC-ROB)}: Evaluates the sensitivity of the policy to variations in the initial contact condition. Small random perturbations are introduced, and the policy's performance is assessed under these changes. In this experiment, we randomly sampled 3 initial contact poses close to the default contact pose. For each initial contact pose, we run 10 trials, and the score is also measure by the success rate. This leads to a total of 3*10=30 evaluations for a method on a single type of object. 
    \item \textbf{Single Trajectory Tracking (STT-CON)}: Measures the endpoint difference between the robot's planned rolling trajectory and the actual trajectory during single trajectory execution.
    \item \textbf{Multiple Trajectory Tracking (MTT-CON)}: Measures the endpoint difference for a sequence of trajectories generated by rolling out a series of planned trajectories. Compare to STT-CON, the metric measure also measure the motion transition property. The evaluation is the same as we used in Section:\ref{sec:exp_sim_setup}.
\end{itemize}




\begin{figure*}[t]
 \centering
    \begin{minipage}[b]{.99\textwidth}
        \centering
        \includegraphics[width=.9\linewidth]{figures/applications.png}
    \end{minipage}

\caption{Illustration of two applications: Trash Cart Transportation (top) and Chair Rearrangement (bottom). A motion planner determines the object trajectory for each task (right), our trained policy controls the robot to move the object of interest (left).}
\label{fig:applications}
 \vspace{-0.6cm}
\end{figure*}



\subsection{Hardware Comparison Results}
\label{sec:hardware_result}


We summarize our quantitative hardware evaluation in Figure~\ref{fig:hardware_results}. Overall, \method \ outperforms all baselines over all metrics. For capability, \method \ achieves the highest object-moving velocities, with a maximum linear velocity of 0.75 m/s and an angular velocity of 0.5 rad/s, while other methods struggle to maintain stability at such speeds, often resulting in the object falling off. In terms of robustness, our method demonstrates lower sensitivity to variations in object properties (e.g., weight, friction, thickness) and initial contact poses. Such variations introduce differences in inertia and contact forces, which can negatively impact policy performance. However, the learned \method \ policy successfully handles to these dynamic changes, proving its potential for real-world application where the policy need to move diverse objects.
In our experiment, \method policy is the only policy that able to move a large table and a heavy rack with success rate over 50\%. For controllability, our result shows \method \ achieves the best object velocity tracking performance over all types of objects. Other methods suffer either large sim2real gap or unsuitable root arm coordination which lead to inaccurate velocity tracking.

A qualitative comparison between \method \ and learning-based methods is shown in Figure~\ref{fig:hardware_visual}. Compared to the learning-based baselines (\textbf{RL-EE} and \textbf{RL-IK}), \method \ leverages a dynamic chain representation, which leads to better task performance and facilitates sim-to-real transfer.  Although \textbf{RL-EE} achieves good performance in simulation for chair and table manipulation, the learned policy's desired arm pose on hardware often results in self-collisions, causing damage to the robot and leading to objects slipping off. Similarly, \textbf{RL-IK} exhibits poor hardware performance, as the policy frequently causes the object to collide with the robot’s rear legs, significantly degrading overall task execution as well as raising safty concerns to robot.

When compared to teleoperation methods, \method \ also demonstrates clear advantages. Teleoperation heavily depends on the operator's skill level, leading to lower capability and robustness under our experimental settings. Specifically, the \method \ policy controls both the arm and root movements simultaneously while measuring the robot's state at 20~Hz. This allows it to responsively adjust its actions to compensate for disturbances or oscillations—an ability that is nearly impossible for a human operator to achieve with the same level of precision. Additionally, in terms of controllability, teleoperation strategies primarily rely on root-level commands for the robot, whereas \method’s \ policy operates in the object’s coordinate frame. This design enables \method \ to achieve significantly better tracking performance. Figure~\ref{fig:hardware_visual} illustrates failure cases of the teleoperation methods. In particular, \textbf{Teleop-Root} fails to execute the appropriate torque, causing the chair to fall off. Moreover, \textbf{Teleop-Root-Arm} overemphasizes arm pose and root velocity, failing to maintain consistent movement of the chair.





\subsection{Real World Applications}

\label{sec:applications}



We demonstrate the capability of the learned policies to address complex real-world tasks by integrating them with high-level planners. Two applications are presented: \textit{Trash Cart Transportation} and \textit{Chair Rearrangement}. Trash cart transportation is a common task in office and residential environments where a loaded trash cart needs to be transported from one place to another. Chair rearrangement universally important task in various social and public settings, such as dining areas, meeting rooms, and event spaces. In our experiment, we integrate a learned object manipulation policy with a teleoperation-based high-level planner, we present interactive trash cart transportation. By integrating the policy with a heuristic object root velocity planner, we showcase automatic chair rearrangement. 
Snapshots illustrating these tasks are provided in Figure~\ref{fig:applications}.


 
In the \textit{Trash Cart Transportation} experiment, the robot's objective is to drag a large trash cart backward for approximately 10 meters along a hallway and then execute a 90-degree turn to transition to a perpendicular path. The trash cart, measuring approximately 1.8~m × 1.0~m × 1.2~m (L × W × H) and equipped with four wheels, is significantly larger than the robot itself. When manipulating the cart using the trained policy, the cart does not perfectly track the velocity command due to the asymmetrical weight distribution of the loaded cart and variations in contact status during the robot's stepping motion. 

To address this challenge, we employ an interactive control strategy. The user selects commands from a predefined set of temporary trajectories (a ``cookbook'') using keyboard inputs. These commands dynamically guide the robot, ensuring the trash cart follows the desired path. Notably, the policy used in this experiment is the same policy trained for moving chairs, which are relatively lighter than the trash cart.

The results show that the large trash cart is successfully transported along the desired path. The trained policy adapts to the cart's dynamics without fine-tuning. Notably, the robot’s root-arm coordination is evident during the object turning stage, where the robot follows a large arc while simultaneously adjusting the arm pose to guide the cart along a smaller arc. Our findings demonstrate that by combining the learned policy with an interactive control framework, the robot can reliably drag the trash cart along the specified route.




In the \textit{Chair Rearrangement} experiment, the robot has to rearrange three chairs in a dining space of approximately 5~m x 4~m (L × W). The chairs are initially in the same location but each one has a different goal positions and orientations (headings) relative to the global frame. The goal positions and orientations are predetermined. 

To achieve automatic chair rearrangement, we use a rule-based heuristic high-planner planner. This planner runs at 2Hz and commands target object velocities by comparing the object’s current global position and heading with the desired goal state. We describe the implementation of the heuristic planner in more detail in the Appendix. Spot’s onboard sensing system is utilized to determine the global positions of the robot and the chairs, while the end-effector position is used to approximate the objects' positions. For simplicity, we assume that there are no obstacles between the starting positions and the target locations. Once a chair reaches its goal position, the robot is teleoperated to grasp and move the next chair.



Our results show that the robot can automatically place the chair in different goal states. On average, the robot takes approximately 30 seconds to move a chair 5 meters away while adjusting for a heading difference of $\pi/2$ radians. The system achieves an average tracking error of 0.15 meters in position and 0.3 radians in orientation. In addition, the chair used in this experiment has a different shape compared to the one used during training. Specifically, the experimental chair has a thinner back (4 mm in the experiment versus 10 mm in simulation), resulting in reduced contact force between the robot's gripper and the chair. Lower contact force can potentially lead to reduced friction, increasing the likelihood of the chair slipping off the robot. However, the policy successfully adapts to this variation. We argue that this is due to the inclusion of friction randomization during training, enabling the policy to handle low-friction conditions effectively. These results highlight the system’s ability to perform precise and reliable chair rearrangement, underscoring the feasibility of automating such tasks in real-world environments.




\subsection{Two-arm Demonstrations}
\label{sec:2-arms}
Humans often use both arms to manipulate objects, but such demonstrations cannot be directly utilized to train a robot with only one arm, such as Spot. To address this challenge, we propose a simple solution: constructing a single dynamic chain by aggregating the dynamic chains of both arms and using the aggregated dynamic chain as a reference for training the robot policy. The aggregation is performed by selecting the midpoint between corresponding nodes on each chain to form the nodes of the aggregated chain:
\begin{align}
\label{eqn:two_arms}
   x^{i,agg}_t = \frac{1}{2}(x^{i,l}_t + x^{i,r}_t).
\end{align}
Here, $x^{i,agg}_t$, $x^{i,l}_t$, and $x^{i,r}_t$ represent the positions of the $i$-th node on the aggregated dynamic chain, the left-arm dynamic chain, and the right-arm dynamic chain at time instance $t$, respectively. Once the positions of the aggregated nodes are obtained, the motion of the aggregated dynamic chain can be generated and used for training the robot policy. Notably, since both the left-arm and right-arm dynamic chains originate from the object's root and terminate at the human root, the aggregated dynamic chain retains the same starting and ending nodes as the original chains.



We tested this method on a robot moving coffee tables using two-arm demonstrations. We separately evaluated the imitation performance on 10 demonstrations where the human used a symmetric arm pose and 10 demonstrations where the human's arms had different poses. The learning performance was assessed using the same object velocity tracking metric described in Section~\ref{sec:exp_sim_setup}. 

For the symmetric pose experiment setting, the overall score reached $0.73\pm0.10$, which is close to the single-arm demonstration result of $0.81\pm0.09$. However, in the asymmetric setting, the overall score dropped to $0.35\pm0.07$, which is even worse than the RL baseline score of $0.41\pm0.11$. This result suggests that simply merging two asymmetric arm poses may not produce an effective reference dynamic chain. Averaging the positions of the two chains can lead to a reference chain that penetrates objects or exhibits other undesirable behaviors, ultimately degrading the training process.






