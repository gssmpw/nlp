
\section{Preliminary: Imitation Learning}
\label{sec:preliminary}
Our robot learning framework is built upon the imitation learning paradigm, which enables robots to acquire skills by learning from demonstration data. The goal is to teach the robot how to move large objects by replicating human-object interaction behaviors. We start with a demonstration trajectory $\xi = \{p^{ho}_1, p^{ho}_2, \dots, p^{ho}_T \}$, where each aggregated pose $p^{ho}_t = (p^h_t, p^o_t)$ represents the human pose $p^h_t$ and the object pose $p^o_t$ at time step $t$. These trajectories, typically acquired from motion capture systems or online datasets, form the basis for defining a reward signal that evaluates how well the robot can replicate the demonstrated behavior.

The imitation learning process is formulated as a reinforcement learning (RL) problem, where the robot learns a policy $\pi(a_t | s_t)$ to maximize the expected cumulative reward:
\begin{align}
J(\pi) = \mathbb{E}\left[\sum_{t=1}^T r(s_t, a_t)\right],
\end{align}
where $r(s_t, a_t) = r^{imi}_t + r^{reg}_t$ is the reward at time $t$, consisting of an imitation reward $r^{imi}_t$ and a regularization reward $r^{reg}_t$. The imitation reward evaluates how closely the robot's actions align with the demonstrated behaviors, while the regularization reward encourages smooth and efficient actions.

In many scenarios, the demonstrator and the robot share similar morphologies, which allows for straightforward imitation reward designs, such as directly matching poses or velocities. However, in our case, the human demonstrator and the robot have significantly different morphologies, making it challenging to apply such simple imitation reward schemes.


\section{RobotMover with the Dynamic Chain}
\label{sec:method}

We propose RobotMover, a learning framework designed to enable robots to learn how to move large objects by imitating human-object interaction demonstrations. An overview of the framework is presented in Fig:~\ref{fig:overview}.
This framework addresses the challenge of embodiment differences between humans and robots by introducing a shared representation, the Dynamic Chain, which captures the core dynamics of human-object interaction. By leveraging this representation, \method~enables to train robust policies in simulation which can be deployed zero-shot in the real world. In this section, we first formulate the cross-embodiment imitation learning problem and outline the challenges it presents. Next, we introduce the Dynamic Chain and describe how it is used to design an imitation reward that guides the robot's learning. Finally, we present the model representation in our framework.





\begin{figure}[t]
\centering
\includegraphics[width=.995\linewidth]{figures/method_overview.pdf}
\caption{Method Overview. RobotMover enables robots to learn to move large objects by imitating human-object interaction demonstrations. The framework leverages a novel representation, the Dynamic Chain, which captures the interaction dynamics between the agent and object while remaining agnostic to the agent’s embodiment. This representation is used to design an imitation reward that guides the robot’s imitation learning process.}
\label{fig:overview}
\vspace{-0.5cm}
\end{figure}


\subsection{Cross-embodiment Imitation}
While imitation learning is effective for transferring skills, the dynamic and morphological differences between humans and robots pose significant challenges. These differences, such as variations in degrees of freedom and body structure, prevent direct replication of human movements. To address this, we utilize \textit{cross-embodiment imitation learning}, which leverages a shared representation to align the interaction dynamics between human and robot.

The imitation reward function is designed to measure the alignment between the human and robot interaction dynamics:
\begin{align}
\label{eqn:imit_reward}
    r^{imi}_t \propto -\| \Psi^{ho}(p^{ho}_t) - \Psi^{ro}(p^{ro}_t) \|,
\end{align}
where $\Psi^{ho}$ and $\Psi^{ro}$ are projection functions that map the human aggregated pose $p^{ho}_t$ and robot aggregated pose $p^{ro}_t = (p^r_t, p^o_t)$ into a shared representation space. This shared representation bridges the embodiment gap, enabling the robot to evaluate and imitate human-object interactions effectively.

A critical component of cross-embodiment imitation learning is the design of appropriate shared representations and projection functions. 
The representation must be concise enough to be reconstructed from both robot and human poses, while also being expressive enough to capture the gist of the behavior.




\subsection{Dynamic Chain}
\label{Method:Dynamic Chain}


\begin{figure}[t]
 \centering
\includegraphics[width=.995\linewidth]{figures/DC_vs_IG.png}
 \vspace{-.5em}
\caption{Dynamic Chain (left) vs. Interaction Graph (right). The Dynamic Chain offers a simpler representation to describe agent-object interactions, which can better transfer to other morphologies. This allows us to use a human demonstration to guide the reward of a robot policy.  
}
\label{fig:DC_vs_IG}
 \vspace{-0.2cm}
\end{figure}


To effectively capture the semantics of the interaction between an agent and an object while proving a meaningful metric to guide the robot's learning process, inspired by prior works such as Interaction Graph~\citep{InteractionGraph} and spatial descriptors~\cite{ho2010spatial}, we propose the Dynamic Chain (DC), a chain-based spatial descriptor encoding the interaction information in a sequence of nodes connected by edges. To construct the Dynamic Chain, we begin by placing a set of markers on salient locations of both the agent and the object. One end of the chain is anchored at the object’s root, while the other end is anchored at the agent’s root. Additional markers are placed at key points such as the shoulder, elbow, and the contact point between the agent and the object. The state of the agent-object dynamic chain of the is defined as $c^{ao}_t = (x^{ao}_t, q^{ao,i}_t)$ where $x^{ao}_t$ represents the object position, orientation and velocity in the global coordinate and $q^{ho,i}_t$ is the i-th dynamic chain's orientation in the object-plane coordinate. This arrangement captures the movement of the dynamic chain and how forces are transmitted from the agent's core to the object to trigger the object's movement.





Figure~\ref{fig:DC_vs_IG} shows an example of the Dynamic Chain, with edges and nodes connecting the agent's root to the agent's contact point with the object's root. The edges within the agent represent the agent's body and arm pose, while the edge connecting the agent-object contact to the object root shows the pose of the object. The Dynamic Chain encodes both the absolute pose of the agent and objects as well as the relative positioning and dynamic coordination between these two.


Compared to Interaction Graphs~\cite{InteractionGraph}, the Dynamic Chain is a lower-dimensional representation that focuses on capturing the essential dynamics of agent-object interaction. Interaction Graphs, by contrast, include exhaustive connections, such as edges linking a foot to the object, which can result in overly complex and unnecessary representations. This high-dimensional structure introduces challenges when transferring skills to new domains, where simplicity and interpretability are crucial. The Dynamic Chain addresses this limitation by providing a compact, low-dimensional representation that is more adaptable for skill learning and generalization.




\subsection{Reward Design}

The core of our reward design is based on the similarity between the dynamic chain of the human demonstration $c^{ho}=\Psi^{ho}_{dc}(p^{ho}_t)$ and the robot's dynamic chain during object manipulation $c^{ro}=\Psi^{ro}_{dc}(p^{ro}_t)$. The imitation reward is derived by measuring the distance between the two chains, which is computed as the sum of the distances between the global positions of each corresponding node on the normalized chains:
\begin{align}
\label{eqn:norm_error}
   err^c_t = ||x^{ho}_t - x^{ro}_t|| + \sum_{i=0}^{N-1} \alpha^i \| q^{ho,i}_t - q^{ro,i}_t \|, 
\end{align}
Here, $||x^{ho}_t - x^{ro}_t||$ denote the difference between the referenced object movement and object movement triggered by the robot. $\alpha^i$ represents a weighting factor of the corresponding edge. We assign higher importance to edge closer to the object to emphasize precise tracking of critical points. 

However, simply applying the dynamic chain difference for imitation reward calculation can lead to robot's learning a policy which its end-effector frequently detaches from object during manipulation. This can potentially make the learned policy unstable and hard to be transferred to real world robot. Therefore, we only give imitation reward when robot's end-effector contacts the object. The combined imitation reward is then expressed as:
\begin{align}
\label{eqn:imi_2}
   r^{imi}_t = e^{-err^c_t }* \mathbf{1}(f^{ee}>0),
\end{align}


To enhance the overall motion quality, we include height and torque regularization, body rotation and action rate penalties from \citet{rudin2022learning}, which encourage smoother and more efficient robot movements. These terms mitigate abrupt actions and improve the robustness of the learned policy, resulting in more natural and reliable object manipulation.






