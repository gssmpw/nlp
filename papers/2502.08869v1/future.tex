\section{Challenges and Future Directions}\label{sec.future}

% In this section, we discuss future directions that are specific to leveraging vision models for time series analysis.

%\subsection{Fundamental Understanding}

\noindent{\bf Fundamental Understanding.} Given the multiple methods for imaging time series, the existing works usually pick their own choice by intuition. %without sufficient justification of the choice or even awareness of other methods.
There remains a gap in both theoretical and empirical understanding of research questions such as which imaging methods fit what tasks and whether LVMs truly learn patterns from the images that make them more suitable than LLMs in time series modeling. %\hh{when diescribing the related work, sometimes we used the past tense but other times we used the present tense. it might be better we pick one and use it consistently as much as possible. for example, in this same sentence, we have used the present and past tenses together.}
Some existing works evaluate multiple imaging methods, but in limited tasks. For example, %FIRTS \cite{costa2024fusion} fuses GASF, RP and MTF to a 3-channel input to CNNs for a classification task,
ImagenTime \cite{naiman2024utilizing} compares the representation abilities of GAF, STFT, and delay embedding ($\S$\ref{sec.othermethod}) in a time series generation task. However, a %comprehensive
thorough understanding that can guide future developments of LVMs and LMMs on top of different imaging methods is absent. This survey provides an initial comparative discussion of these methods in $\S$\ref{sec.tsimage}. Further investigations with empirical validation and theoretical justification is essential to the synergy between LVMs/LMMs and time series analysis.

% As the first work to thoroughly investigate this field, this survey collects and summarizes different imaging methods for time series.

% Theoretical and empirical study of different image methods for different tasks. Advantage of each method.

% What research questions?

\vspace{0.2cm}

% \subsection{Modeling The Correlation of Variates in MTS}

\noindent{\bf Modeling the Correlation of Variates in MTS.} In $\S$\ref{sec.modelmts}, we discussed the existing methods for imaging MTSs. However, each of them has its limitation. For example, when visualizing a variate-time matrix by a Heatmap image ({\em e.g.}, Fig. \ref{fig.tsimage}(b)), the row a variate locates at matters to the downstream modeling of correlations. This is because vision models only encode the spatial relationships of pixels thus correlated variates should be spatially close to each other. Similarly, Line Plots does not enable explicit modeling of correlated variates by vision models. Stacking $d$ images, one per variate, into a $d$-channel input may disable the chance to use pre-trained LVMs due to their fixed 3-channel RGB input. As such, effective methods at either the imaging step or the modeling step ({\em e.g.}, leveraging graph neural networks (GNNs) on variates) that allow correlation learning from MTSs are in demand.

% GNNs

\vspace{0.2cm}

% \subsection{Advanced Imaging for Time Series}

\noindent{\bf Advanced Imaging for Time Series.} In addition to the basic methods introduced in $\S$\ref{sec.tsimage}, it is promising to explore more advanced image representations. %especially on those leveraging domain knowledge in time series.
For example, InsightMiner \cite{zhang2023insight} adopts Seasonal-Trend decomposition, which is often used to extract components that can serve as inductive bias for time series models. Generalizing it to decompose images such as Spectrogram, GAF, RP into fine-grained representations may further boost vision models' ability in time series analysis. Moreover, mixture of imaging may enable encoding of information from different views, such as frequency (Spectrogram), temporal relationships (GAF) and recurrence patterns (RP). FIRTS \cite{costa2024fusion} stacks a mixture of images in multiple channels for a classification task, but it is limited to images of the same size. Modeling a mixture of arbitrary images by methods such as multi-view learning may enable more flexibility.

\vspace{0.2cm}

% \subsection{Multimodal Time Series Models and Agents}

\noindent{\bf Multimodal Time Series Models and Agents.} As can be seen from Table \ref{tab.taxonomy}, the existing research on multimodal analysis (with vision modality) is much less than unimodal analysis, with a limited scope of time series tasks. Given the existing LLMs for time series such as Time-LLM \cite{jin2024time} and S\textsuperscript{2}IP \cite{pan2024s}, it is appealing to introduce vision modality %to these advanced models
to further boost the performance in wide tasks such as forecasting, classification and anomaly detection. Furthermore, the visual representation of time series provides the foundation for exploring multimodal AI agents \cite{xie2024large} for more intricate and nuanced tasks that requires reasoning and interactions with environments, such as %correlation and
root cause analysis in AI for IT Operations (AIOps).%\hh{what is this? maybe spell it out}.

\vspace{0.2cm}

% \subsection{Vision-based Time Series Foundation Models}

\noindent{\bf Vision-based Time Series Foundation Models.} A foundation model (FM) is a deep learning model trained on vast datasets that is applicable to a wide range of tasks. Recent time series FMs, such as TimesFM \cite{das2024decoder}, MOMENT \cite{goswami2024moment}, Chronos \cite{ansari2024chronos} and Time-MoE \cite{shi2024time}, are mostly built upon LLM architectures and trained on raw time series. Given the potential of image representation, %of time series,
it is promising to explore vision models as a new architecture to revolutionize time series FMs. This research direction not only leverages the advantages of LVMs as introduced in $\S$\ref{sec.introduction} ({\em e.g.}, the %extracted
prior knowledge extracted from the vast pre-training images), but also enables future development of vision-language FMs for time series. %which is a confluence of researches on LLMs and LVMs for time series.

% \subsection{Vision Model based Time Series Generation}

% \subsection{Multimodal Agents for Time Series Analysis}

% \subsection{Privacy and Safety}


% \subsection{Mixture of Different Images}