% \section{Taxonomy}

% As illustrated by Fig. \ref{}, the typical process of vision models based time series analysis has five components: (1) normalization/scaling; (2) time series to image transformation; (3) image modeling; (4) image to time series recovery; and (5) task processing. In the rest of this paper, we will discuss the typical methods for each of these components. The detailed taxonomy of the methods are summarized in Table \ref{tab.taxonomy}.

%Typical step: normalization/scaling, transformation, vision modeling, task-specific head, inverse transformation (for tasks that output time series, e.g., forecasting, generation, imputation, anomaly detection). Normalization is to fit the arbitrary range of time series values to RGB representation.

\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\textwidth]{fig/fig_3.pdf}
% \vspace{-1em}
\caption{An illustration of different methods for imaging time series with a sample (length=336) from the \textit{Electricity} benchmark dataset \protect\cite{nie2023time}. (a)(c)(d)(e)(f) %are univariate methods.
visualize the same variate. (b) visualizes all 321 variates. Filterbank is omitted due to its %high
similarity to STFT.}\label{fig.tsimage}
\vspace{-0.2cm}
\end{figure*}

\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{2.7pt}{
% \begin{tabular}{llllllllllll}
\begin{tabular}{llcccccccccl}
\toprule[1pt]
\multirow{2}{*}{Method} & \multirow{2}{*}{TS-Type} & \multirow{2}{*}{Imaging} & \multicolumn{5}{c}{Imaged Time Series Modeling} & \multirow{2}{*}{TS-Recover} & \multirow{2}{*}{Task} & \multirow{2}{*}{Domain} & \multirow{2}{*}{Code}\\ \cmidrule{4-8}
 & & & Multi-modal & Model & Pre-trained & Fine-tune & Prompt & & & & \\ \midrule
\cite{silva2013time} & UTS & RP & \xmark & \texttt{K-NN} & \xmark & \xmark & \xmark & \xmark & Classification & General & \xmark\\
\cite{wang2015encoding} & UTS & GAF & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Classification & General & \xmark\\
\cite{wang2015imaging} & UTS & GAF & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Multiple & General & \xmark\\
% \multirow{2}{*}{\cite{wang2015imaging}} & \multirow{2}{*}{UTS} & \multirow{2}{*}{GAF} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\texttt{CNN}} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark$^{\flat}$} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & Classification & \multirow{2}{*}{General} & \multirow{2}{*}{\xmark}\\
% & & & & & & & & & \& Imputation & & \\
\cite{ma2017learning} & MTS & Heatmap & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Forecasting & Traffic & \xmark\\
\cite{hatami2018classification} & UTS & RP & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & General & \xmark\\
\cite{yazdanbakhsh2019multivariate} & MTS & Heatmap & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & General & \cmark\textsuperscript{\href{https://github.com/SonbolYb/multivariate_timeseries_dilated_conv}{[1]}}\\
MSCRED \cite{zhang2019deep} & MTS & Other ($\S$\ref{sec.othermethod}) & \xmark & \texttt{ConvLSTM} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Anomaly & General & \cmark\textsuperscript{\href{https://github.com/7fantasysz/MSCRED}{[2]}}\\
\cite{li2020forecasting} & UTS & RP & \xmark & \texttt{CNN} & \cmark & \cmark & \xmark & \xmark & Forecasting & General & \cmark\textsuperscript{\href{https://github.com/lixixibj/forecasting-with-time-series-imaging}{[3]}}\\
\cite{cohen2020trading} & UTS & LinePlot & \xmark & \texttt{Ensemble} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & Finance & \xmark\\
% \cite{du2020image} & UTS & Spectrogram & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & Finance & \xmark\\
\cite{barra2020deep} & UTS & GAF & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & Finance & \xmark\\
% \cite{barra2020deep} & UTS & GAF & \xmark & \texttt{VGG-16} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & Finance & \xmark\\
% \cite{cao2021image} & UTS & RP & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & General & \xmark\\
VisualAE \cite{sood2021visual} & UTS & LinePlot & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Forecasting & Finance & \xmark\\
% VisualAE \cite{sood2021visual} & UTS & LinePlot & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Img-Generation & Finance & \xmark\\
\cite{zeng2021deep} & MTS & Heatmap & \xmark & \texttt{CNN,LSTM} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Forecasting & Finance & \xmark\\
% \cite{zeng2021deep} & MTS & Heatmap & \xmark & \texttt{SRVP} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Forecasting & Finance & \xmark\\
AST \cite{gong2021ast} & UTS & Spectrogram & \xmark & \texttt{DeiT} & \cmark & \cmark & \xmark & \xmark & Classification & Audio & \cmark\textsuperscript{\href{https://github.com/YuanGongND/ast}{[4]}}\\
TTS-GAN \cite{li2022tts} & MTS & Heatmap & \xmark & \texttt{ViT} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Ts-Generation & Health & \cmark\textsuperscript{\href{https://github.com/imics-lab/tts-gan}{[5]}}\\
SSAST \cite{gong2022ssast} & UTS & Spectrogram & \xmark & \texttt{ViT} & \cmark$^{\natural}$ & \cmark & \xmark & \xmark & Classification & Audio & \cmark\textsuperscript{\href{https://github.com/YuanGongND/ssast}{[6]}}\\
MAE-AST \cite{baade2022mae} & UTS & Spectrogram & \xmark & \texttt{MAE} & \cmark$^{\natural}$ & \cmark & \xmark & \xmark & Classification & Audio & \cmark\textsuperscript{\href{https://github.com/AlanBaade/MAE-AST-Public}{[7]}}\\
AST-SED \cite{li2023ast} & UTS & Spectrogram & \xmark & \texttt{SSAST,GRU} & \cmark & \cmark & \xmark & \xmark & EventDetection & Audio & \xmark\\
\cite{jin2023classification} & UTS & %Multiple
LinePlot & \xmark & \texttt{CNN} & \cmark & \cmark & \xmark & \xmark & Classification & Physics & \xmark\\
ForCNN \cite{semenoglou2023image} & UTS & LinePlot & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Forecasting & General & \xmark\\
Vit-num-spec \cite{zeng2023pixels} & UTS & Spectrogram & \xmark & \texttt{ViT} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Forecasting & Finance & \xmark\\
% \cite{wimmer2023leveraging} & MTS & LinePlot & \xmark & \texttt{CLIP,LSTM} & \cmark & \cmark & \xmark & \xmark & Classification & Finance & \xmark\\
ViTST \cite{li2023time} & MTS & LinePlot & \xmark & \texttt{Swin} & \cmark & \cmark & \xmark & \xmark & Classification & General & \cmark\textsuperscript{\href{https://github.com/Leezekun/ViTST}{[8]}}\\
MV-DTSA \cite{yang2023your} & UTS\textsuperscript{*} & LinePlot & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Forecasting & General & \cmark\textsuperscript{\href{https://github.com/IkeYang/machine-vision-assisted-deep-time-series-analysis-MV-DTSA-}{[9]}}\\
TimesNet \cite{wu2023timesnet} & MTS & Heatmap & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Multiple & General & \cmark\textsuperscript{\href{https://github.com/thuml/TimesNet}{[10]}}\\
ITF-TAD \cite{namura2024training} & UTS & Spectrogram & \xmark & \texttt{CNN} & \cmark & \xmark & \xmark & \xmark & Anomaly & General & \xmark\\
\cite{kaewrakmuk2024multi} & UTS & GAF & \xmark & \texttt{CNN} & \cmark & \cmark & \xmark & \xmark & Classification & Sensing & \xmark\\
HCR-AdaAD \cite{lin2024hierarchical} & MTS & RP & \xmark & \texttt{CNN,GNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Anomaly & General & \xmark\\
FIRTS \cite{costa2024fusion} & UTS & Other ($\S$\ref{sec.othermethod}) & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & General & \cmark\textsuperscript{\href{https://sites.google.com/view/firts-paper}{[11]}}\\
% \multirow{2}{*}{FIRTS \cite{costa2024fusion}} & \multirow{2}{*}{UTS} & Spectrogram & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\texttt{CNN}} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark$^{\flat}$} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{Classification} & \multirow{2}{*}{General} & \multirow{2}{*}{\cmark\textsuperscript{\href{https://sites.google.com/view/firts-paper}{[2]}}}\\
%  & & \& GAF,RP,MTF & & & & & & & & & \\
% \cite{homenda2024time} & UTS\textsuperscript{*} & Multiple & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Classification & General & \xmark\\
CAFO \cite{kim2024cafo} & MTS & RP & \xmark & \texttt{CNN,ViT} & \xmark & \cmark$^{\flat}$ & \xmark & \xmark & Explanation & General & \cmark\textsuperscript{\href{https://github.com/eai-lab/CAFO}{[12]}}\\
% \multirow{2}{*}{CAFO \cite{kim2024cafo}} & \multirow{2}{*}{MTS} & \multirow{2}{*}{RP} & \multirow{2}{*}{\xmark} & \texttt{ShuffleNet,ResNet} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\xmark} & Classification & \multirow{2}{*}{General} & \multirow{2}{*}{\cmark}\\
%  & & & & \texttt{MLP-Mixer,ViT} & & & & & \& Explanation & & \\
ViTime \cite{yang2024vitime} & UTS\textsuperscript{*} & LinePlot & \xmark & \texttt{ViT} & \cmark$^{\natural}$ & \cmark & \xmark & \cmark & Forecasting & General & \cmark\textsuperscript{\href{https://github.com/IkeYang/ViTime}{[13]}}\\
ImagenTime \cite{naiman2024utilizing} & MTS & Other ($\S$\ref{sec.othermethod}) & \xmark & %\texttt{Diffusion}
\texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Ts-Generation & General & \cmark\textsuperscript{\href{https://github.com/azencot-group/ImagenTime}{[14]}}\\
TimEHR \cite{karami2024timehr} & MTS & Heatmap & \xmark & \texttt{CNN} & \xmark & \cmark$^{\flat}$ & \xmark & \cmark & Ts-Generation & Health & \cmark\textsuperscript{\href{https://github.com/esl-epfl/TimEHR}{[15]}}\\
VisionTS \cite{chen2024visionts} & UTS\textsuperscript{*} & Heatmap & \xmark & \texttt{MAE} & \cmark & \cmark & \xmark & \cmark & Forecasting & General & \cmark\textsuperscript{\href{https://github.com/Keytoyze/VisionTS}{[16]}}\\ \midrule
InsightMiner \cite{zhang2023insight} & UTS & LinePlot & \cmark & \texttt{LLaVA} & \cmark & \cmark & \cmark & \xmark & Txt-Generation & General & \xmark\\
\cite{wimmer2023leveraging} & MTS & LinePlot & \cmark & \texttt{CLIP,LSTM} & \cmark & \cmark & \xmark & \xmark & Classification & Finance & \xmark\\
% \cite{dixit2024vision} & UTS & Spectrogram & \cmark & \texttt{GPT4o,Gemini} & \cmark & \xmark & \cmark & \xmark & Classification & Audio & \xmark\\
\multirow{2}{*}{\cite{dixit2024vision}} & \multirow{2}{*}{UTS} & \multirow{2}{*}{Spectrogram} & \multirow{2}{*}{\cmark} & \texttt{GPT4o,Gemini} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{\cmark} & \multirow{2}{*}{\xmark} & \multirow{2}{*}{Classification} & \multirow{2}{*}{Audio} & \multirow{2}{*}{\xmark}\\
 & & & & \& \texttt{Claude3} & & & & & & & \\
\cite{daswani2024plots} & MTS & LinePlot & \cmark & \texttt{GPT4o,Gemini} & \cmark & \xmark & \cmark & \xmark & Multiple & General & \xmark\\
TAMA \cite{zhuang2024see} & UTS & LinePlot & \cmark & \texttt{GPT4o} & \cmark & \xmark & \cmark & \xmark & Anomaly & General & \xmark\\
\cite{prithyani2024feasibility} & MTS & LinePlot & \cmark & \texttt{LLaVA} & \cmark & \cmark & \cmark & \xmark & Classification & General & \cmark\textsuperscript{\href{https://github.com/vinayp17/VLM_TSC}{[17]}}\\
\bottomrule[1pt]
\end{tabular}}
\vspace{-0.25cm}
\caption{Taxonomy of vision models on time series. The top panel includes single-modal models. The bottom panel includes multi-modal models. {\bf TS-Type} denotes type of time series. {\bf TS-Recover} denotes %whether time series recovery ($\S$\ref{sec.processing}) has been performed.
recovering time series from predicted images ($\S$\ref{sec.processing}). \textsuperscript{*}: %the model has been %applied on MTSs by %processing %modeling the individual UTSs of each MTS.
the method has been used to model the individual UTSs of an MTS. $^{\natural}$: a new pre-trained model was proposed in the work. $^{\flat}$: %without using a pre-trained model, fine-tune means training from scratch.
when pre-trained models were unused, ``Fine-tune'' refers to train a task-specific model from scratch. %In the
{\bf Model} column: \texttt{CNN} could be regular CNN, ResNet, VGG-Net, %U-Net,
{\em etc.}}\label{tab.taxonomy}
% The code only include verified official code from the authors.
\vspace{-0.3cm}
\end{table*}

\begin{table*}[t]
\centering
\small
\setlength{\tabcolsep}{2.9pt}{
\begin{tabular}{l|l|l|l}\hline
% \toprule[1pt]
\rowcolor{gray!20}
{\bf Method} & {\bf TS-Type} & {\bf Advantages} & {\bf Limitations}\\ \hline
Line Plot ($\S$\ref{sec.lineplot}) & UTS, MTS & matches human perception of time series & limited to MTSs with a small number of variates\\ \hline
Heatmap ($\S$\ref{sec.heatmap}) & UTS, MTS & straightforward for both UTSs and MTSs & the order of variates may affect their correlation learning\\ \hline
Spectrogram ($\S$\ref{sec.spectrogram}) & UTS & encodes the time-frequency space & limited to UTSs; needs a proper choice of window/wavelet\\ \hline
GAF ($\S$\ref{sec.gaf}) & UTS & encodes the temporal correlations in a UTS & limited to UTSs; $O(T^{2})$ time and space complexity\\ \hline% for long time series\\ \hline
% RP ($\S$\ref{sec.rp}) & UTS & flexibility in image size by tuning $m$ and $\tau$ & limited to UTSs; the pattern has a threshold-dependency\\ \hline
RP ($\S$\ref{sec.rp}) & UTS & flexibility in image size by tuning $m$ and $\tau$ & limited to UTSs; information loss after thresholding\\ \hline
% \bottomrule[1pt]
\end{tabular}}
\vspace{-0.2cm}
\caption{Summary of the five primary methods for transforming time series to images. {\bf TS-Type} denotes type of time series.}\label{tab.tsimage}
\vspace{-0.2cm}
\end{table*}

\section{Time Series To Image Transformation}\label{sec.tsimage}

% This section summarizes 5 major methods for imaging time series ($\S$\ref{sec.lineplot}-$\S$\ref{sec.rp}). We also discuss some other methods ($\S$\ref{sec.othermethod}) and how to model MTS with these methods ($\S$\ref{sec.modelmts}).
This section summarizes the methods for imaging time series ($\S$\ref{sec.lineplot}-$\S$\ref{sec.othermethod}) and their extensions to encode MTSs ($\S$\ref{sec.modelmts}).

% This section summarizes 5 major methods for transforming time series to images, including Line Plot, Heatmap, Spetrogram, GAF and RP, and several minor methods. We discuss their pros and cons and how to deal with MTS.

% This section discusses the advantages and limitations of different methods for time series to image transformation (invertible, efficiency, information preservation, MTS, long-range time series, parametric, etc.).

%\subsection{Methods}

\vspace{-0.08cm}

\subsection{Line Plot}\label{sec.lineplot}

Line Plot is a straightforward way for visualizing UTSs for human analysis ({\em e.g.}, stocks, power consumption, {\em etc.}). As illustrated by Fig. \ref{fig.tsimage}(a), the simplest approach is to draw a 2D image with x-axis representing %the time horizon
time steps and y-axis representing %the magnitude of the normalized time series.
time-wise values, %A line is used to connect all values of the series over time.
with a line connecting all values of the series over time. This image can be %represented by either three-channel pixels or single-channel pixels
either three-channel ({\em i.e.}, RGB) or single-channel as the colors may not %provide additional information
be informative %\cite{cohen2020trading,sood2021visual,jin2023classification,zhang2023insight,zhuang2024see}.
\cite{cohen2020trading,sood2021visual,jin2023classification,zhang2023insight}. ForCNN \cite{semenoglou2023image} even uses a single 8-bit integer to represent each pixel for black-white images. So far, there is no consensus on whether other graphical components, such as legend, grids and tick labels, could provide extra benefits in any task. For example, ViTST \cite{li2023time} finds these components are superfluous in a classification task, while TAMA \cite{zhuang2024see} finds grid-like auxiliary lines help enhance anomaly detection.

In addition to the regular Line Plot, MV-DTSA \cite{yang2023your} and ViTime \cite{yang2024vitime} divide an image into $h\times L$ grids, %where $h$ is the number of rows and $L$ is the number of columns,
and %introduced
define a function to map each time step of a UTS to a grid, producing a grid-like Line Plot. Also, we include methods that use Scatter Plot \cite{daswani2024plots,prithyani2024feasibility} in this category because %the only difference between a Scatter Plot and a Line Plot is whether the time-wise values are connected by lines.
a Scatter Plot resembles a Line Plot but doesn't connect %time-wise values
data points with a line. By comparing them, \cite{prithyani2024feasibility} finds a Line Plot could induce better time series classification.

For MTSs, we defer the discussion on Line Plot to $\S$\ref{sec.modelmts}.

% For MTS, some methods use the channel-independence assumption proposed in \cite{nie2023time} and represent each variate in MTS with an individual Line Plot \cite{yang2023your,yang2024vitime}. ViTST \cite{li2023time} also uses an individual Line Plot per variate, but colors different lines and assembles all plots to form a bigger image. The method in \cite{wimmer2023leveraging} plots %the time series of
% all variates in a single Line Plot and distinguish them by %use different
% types of lines ({\em e.g.}, solid, dashed, dotted, {\em etc.}). %to distinguish them.
% However, these methods only work for a small number of variates. For example, in \cite{wimmer2023leveraging}, there are only 4 variates in its financial MTSs.

%\cite{li2023time} space-costly because of blank pixels. scatter plot.

%Invertible with a numeric prediction head \cite{sood2021visual}. It fits tasks such as forecasting, imputation, etc.

\vspace{-0.08cm}

\subsection{Heatmap}\label{sec.heatmap}

As shown in Fig. \ref{fig.tsimage}(b), Heatmap is a 2D visualization of the magnitude of the values in a matrix using color. %The variation of color represents the intensity of each value. %Therefore,
It has been used to %directly
represent the matrix of an MTS, {\em i.e.}, $\mat{X} \in \mathbb{R}^{d\times T}$, as a one-channel $d\times T$ image \cite{li2022tts,yazdanbakhsh2019multivariate}. Similarly, TimEHR \cite{karami2024timehr} represents an {\em irregular} MTS, where the intervals between time steps are uneven, as a $d\times H$ Heatmap image by grouping the uneven time steps into $H$ even time bins. In \cite{zeng2021deep}, a different method is used for visualizing a 9-variate financial %time series.
MTS. It reshapes the 9 variates at each time step to a $3\times 3$ Heatmap image, and uses the sequence of images to forecast future %image
frames, achieving %time series
%MTS
time series forecasting. In contrast, VisionTS \cite{chen2024visionts} uses Heatmap to visualize UTSs. %instead.
Similar to TimesNet \cite{wu2023timesnet}, it first segments a length-$T$ UTS into $\lfloor T/P\rfloor$ length-$P$ subsequences, where $P$ is a parameter representing a periodicity of the UTS. Then the subsequences are stacked into a $P\times \lfloor T/P\rfloor$ matrix, %and duplicated 3 times to produce a 3-channel
with 3 duplicated channels, to produce a grayscale image %which serves as an
input to %a vision foundation model.
an LVM. To encode MTSs, VisionTS adopts the channel independence assumption \cite{nie2023time} and individually models each variate in an MTS.

\vspace{0.2cm}

\noindent{\bf Remark.} Heatmap can be used to visualize matrices of various forms. It is also used for matrices generated by the subsequent methods ({\em e.g.}, Spectrogram, GAF, RP) in this section. In this paper, the name Heatmap refers specifically to images that use color to visualize the (normalized) values in UTS $\mat{x}$ or MTS $\mat{X}$ without performing other transformations.

%\cite{chen2024visionts,karami2024timehr} bin version of TSH \cite{karami2024timehr}, DE and STFT \cite{naiman2024utilizing} (DE can be used for constructing RP), rearrange variates for video version of TSH \cite{zeng2021deep}.

%\vspace{0.2cm}

\subsection{Spectrogram}\label{sec.spectrogram}

A {\em spectrogram} is a visual representation of the spectrum of frequencies of a signal as it varies with time, which are extensively used for analyzing audio signals \cite{gong2021ast}. Since audio signals are a type of UTS, spectrogram can be considered as a method for imaging a UTS. As shown in Fig. \ref{fig.tsimage}(c), a common format is a 2D heatmap image with x-axis representing time steps and y-axis representing frequency, {\em a.k.a.} a time-frequency space. %The color at each point
Each pixel in the image represents the (logarithmic) amplitude of a specific frequency at a specific time point. Typical methods for %transforming a UTS to
producing a spectrogram include {\bf Short-Time Fourier Transform (STFT)} \cite{griffin1984signal}, {\bf Wavelet Transform} \cite{daubechies1990wavelet}, and {\bf Filterbank} \cite{vetterli1992wavelets}.

\vspace{0.2cm}

\noindent{\bf STFT.} %Discrete Fourier transform (DFT) can be used to represent a UTS signal %$\mat{x}=[x_{1}, ..., x_{T}]$
%$\mat{x}\in\mathbb{R}^{1\times T}$ as a sum of sinusoidal components. The output of the transform is a function of frequency $f(w)$, describing the intensity of each constituent frequency $w$ of the entire UTS. 
Discrete Fourier transform (DFT) can be used to describe the intensity $f(w)$ of each constituent frequency $w$ of a UTS signal $\mat{x}\in\mathbb{R}^{1\times T}$. However, $f(w)$ has no time dependency. It cannot provide dynamic information such as when a specific frequency appear in the UTS. STFT addresses this deficiency by sliding a window function $g(t)$ over the time steps in %the UTS,
$\mat{x}$, and computing the DFT within each window by
\begin{equation}\label{eq.stft}
\small
\begin{aligned}
f(w,\tau) = \sum_{t=1}^{T}x_{t}g(t - \tau)e^{-iwt}
\end{aligned}
\end{equation}
where $w$ is frequency, $\tau$ is the position of the window, $f(w,\tau)$ describes the intensity of frequency $w$ at time step $\tau$.

%With a proper selection of the
By selecting a proper window function $g(\cdot)$ ({\em e.g.}, Gaussian/Hamming/Bartlett window), %({\em e.g.}, Gaussian window, Hamming window, Bartlett window), %{\em etc.}),
a 2D spectrogram ({\em e.g.}, Fig. \ref{fig.tsimage}(c)) can be drawn via a heatmap on the squared values $|f(w,\tau)|^{2}$, with $w$ as the y-axis, and $\tau$ as the x-axis. For example, \cite{dixit2024vision} uses STFT based spectrogram as an input to LMMs %\hh{do you mean LVMs? check}
for time series classification.

%Fourier transform is a powerful data analysis tool that represents any complex signal as a sum of sines and cosines and transforms the signal from the time domain to the frequency domain. However, Fourier transform can only show which frequencies are present in the signal, but not when these frequencies appear. The STFT divides original signal into several parts using a sliding window to fix this problem. STFT involves a sliding window for extracting frequency components within the window.

\vspace{0.2cm}

\noindent{\bf Wavelet Transform.} %Like Fourier transform, %\hh{this paragraph needs a citation}
Continuous Wavelet Transform (CWT) uses the inner product to measure the similarity between a signal function $x(t)$ and an analyzing function. %In STFT (Eq.~\eqref{eq.stft}), the analyzing function is a windowed exponential $g(t - \tau)e^{-iwt}$.
%In CWT,
The analyzing function is a {\em wavelet} $\psi(t)$, where the typical choices include Morse wavelet, Morlet wavelet, %Daubechies wavelet, %Beylkin wavelet, 
{\em etc.} %The
CWT compares $x(t)$ to the shifted and scaled ({\em i.e.}, stretched or shrunk) versions of the wavelet, and output a CWT coefficient by
\begin{equation}\label{eq.cwt}
\small
\begin{aligned}
c(s,\tau) = \int_{-\infty}^{\infty}x(t)\frac{1}{s}\psi^{*}(\frac{t - \tau}{s})dt
\end{aligned}
\end{equation}
where $*$ denotes complex conjugate, $\tau$ is the time step to shift, and $s$ represents the scale. In practice, a discretized version of CWT in Eq.~\eqref{eq.cwt} is implemented for UTS $[x_{1}, ..., x_{T}]$.

It is noteworthy that the scale $s$ controls the frequency encoded in a wavelet -- a larger $s$ leads to a stretched wavelet with a lower frequency, and vice versa. As such, by varying $s$ and $\tau$, a 2D spectrogram ({\em e.g.}, Fig. \ref{fig.tsimage}(d)) can be drawn %, often with a heatmap
on $|c(s,\tau)|$, where $s$ is the y-axis and $\tau$ is the x-axis. Compared to STFT, which uses a fixed window size, Wavelet Transform allows variable wavelet sizes -- a larger size %region
for more precise low frequency information. 
%Usually, $s$ and $\tau$ vary dependently -- a larger $s$ leads to a stretched wavelet that shifts slowly, {\em i.e.}, a smaller $\tau$. This property %of CWT
%yields a spectrogram that balances the resolutions of frequency %$s$
%and time, %$\tau$,
%which is an advantage over the fixed time resolution in STFT.
% Thus, both of the methods in %\cite{du2020image}
% \cite{namura2024training} and \cite{zeng2023pixels} choose CWT (with Morlet wavelet) to generate the spectrogram.
Thus, the methods in \cite{du2020image,namura2024training,zeng2023pixels} choose CWT (with Morlet wavelet) to generate the spectrogram.

%A wavelet is a wave-like oscillation that has zero mean and is localized in both time and frequency space.

\vspace{0.2cm}

\noindent{\bf Filterbank.} This method %is relevant to
resembles STFT and is often used in processing audio signals. Given an audio signal, it firstly goes through a {\em pre-emphasis filter} to boost high frequencies, which helps improve the clarity of the signal. Then, STFT is applied on the signal. %with a sliding window $g(t)$ of size $k$ that shifts in a fixed stride $\tau$. %where the adjacent windows may overlap in $k$ time length.
%Finally, filterbank features are computed by applying multiple ``triangle-shaped'' filters spaced on the Mel-scale to the STFT output $f(w, \tau)$. %where Mel-scale is a method to make the filters more discriminative on lower frequencies, %than higher frequencies,
%imitating the non-linear human ear perception of sound.
Finally, multiple ``triangle-shaped'' filters spaced on a Mel-scale are applied to the STFT power spectrum $|f(w, \tau)|^{2}$ to extract frequency bands. The outcome filterbank features $\hat{f}(w, \tau)$ can be used to yield a spectrogram with $w$ as the y-axis, and $\tau$ as the x-axis.

%Filterbank was introduced in AST \cite{gong2021ast} with %$k$=25ms
Filterbank was adopted in AST \cite{gong2021ast} with 
a 25ms Hamming window that shifts every 10ms for classifying audio signals using Vision Transformer (ViT). It then becomes widely used in the follow-up works such as SSAST \cite{gong2022ssast}, MAE-AST \cite{baade2022mae}, and AST-SED \cite{li2023ast}, as summarized in Table \ref{tab.taxonomy}.



%Use MLP to predict TS directly \cite{zeng2023pixels}.

%\vspace{0.2cm}

% \vspace{0.2cm}

\subsection{Gramian Angular Field (GAF)}\label{sec.gaf}

GAF was introduced for classifying UTSs using CNNs %using %image based CNNs
by \cite{wang2015encoding}. It was then extended %with an extension
to an imputation task in \cite{wang2015imaging}. Similarly, \cite{barra2020deep} applied GAF for financial time series forecasting.

Given a UTS $\mat{x}\in\mathbb{R}^{1\times T}$, %$[x_{1}, ..., x_{T}]$,
the first step %before GAF
is to rescale each $x_{t}$ to a value $\tilde{x}_{t}$ %in the interval of
within $[0, 1]$ (or $[-1, 1]$). %by min-max normalization.
This range enables mapping $\tilde{x}_{t}$ to polar coordinates by $\phi_{t}=\text{arccos}(\tilde{x}_{i})$, with a radius $r=t/N$ encoding the time stamp, where $N$ is a constant factor to regularize the span of the polar coordinates. %system. Then,
Two types of GAF, Gramian Sum Angular Field (GASF) and Gramian Difference Angular Field (GADF) are defined as
\begin{equation}\label{eq.gaf}
\small
\begin{aligned}
&\text{GASF:}~~\text{cos}(\phi_{t} + \phi_{t'})=x_{t}x_{t'} - \sqrt{1 - x_{t}^{2}}\sqrt{1 - x_{t'}^{2}}\\
&\text{GADF:}~~\text{sin}(\phi_{t} - \phi_{t'})=x_{t'}\sqrt{1 - x_{t}^{2}} - x_{t}\sqrt{1 - x_{t'}^{2}}
\end{aligned}
\end{equation}
which exploits the pairwise temporal correlations in the UTS. Thus, the outcome is a $T\times T$ matrix $\mat{G}$ with $\mat{G}_{t,t'}$ specified by either type in Eq.~\eqref{eq.gaf}. A GAF image is a heatmap on $\mat{G}$ with both axes representing time, as illustrated by Fig. \ref{fig.tsimage}(e).

% Invertible.

% \vspace{0.2cm}

\subsection{Recurrence Plot (RP)}\label{sec.rp}

%RP \cite{eckmann1987recurrence} is a method to encode a UTS into an image that aims to capture the periodic patterns in the UTS by using its reconstructed {\em phase space}. The phase space of a UTS $[x_{1}, ..., x_{T}]$ can be reconstructed by {\em time delay embedding}, which is a set of new vectors $\mat{v}_{1}$, ..., $\mat{v}_{l}$ with

RP \cite{eckmann1987recurrence} encodes a UTS into an image that captures its periodic patterns by using its reconstructed {\em phase space}. The phase space of %a UTS %$[x_{1}, ..., x_{T}]$
$\mat{x}\in\mathbb{R}^{1\times T}$ can be reconstructed by {\em time delay embedding} -- a set of new vectors $\mat{v}_{1}$, ..., $\mat{v}_{l}$ with
\begin{equation}\label{eq.de}
\small
\begin{aligned}
\mat{v}_{t}=[x_{t}, x_{t+\tau}, x_{t+2\tau}, ..., x_{t+(m-1)\tau}]\in\mathbb{R}^{m\tau},~~~1\le t \le l
\end{aligned}
\end{equation}
where $\tau$ is the time delay, $m$ is the dimension of the phase space, both %of which
are hyperparameters. Hence, $l=T-(m-1)\tau$. With vectors $\mat{v}_{1}$, ..., $\mat{v}_{l}$, an RP image %is constructed by measuring
measures their pairwise distances, results in an $l\times l$ image whose element
\begin{equation}\label{eq.rp}
\small
\begin{aligned}
\text{RP}_{i,j}=\Theta(\varepsilon - \|\mat{v}_{i} - \mat{v}_{j}\|),~~~1\le i,j\le l
\end{aligned}
\end{equation}
where $\Theta(\cdot)$ is the Heaviside step function, $\varepsilon$ is a threshold, and $\|\cdot\|$ is a norm function such as $\ell_{2}$ norm. Eq.~\eqref{eq.rp} %states RP produces a heatmap image on a binary matrix with $\text{RP}_{i,j}=1$ if $\mat{v}_{i}$ and $\mat{v}_{j}$ are sufficiently similar.
generates a binary matrix with $\text{RP}_{i,j}=1$ if $\mat{v}_{i}$ and $\mat{v}_{j}$ are sufficiently similar, producing a black-white image ({\em e.g.}, Fig. \ref{fig.tsimage}(f)).% ({\em e.g.}, a periodic pattern).

An advantage of RP is its flexibility in image size by tuning $m$ and $\tau$. Thus it has been used for time series classification %\cite{cao2021image},
\cite{silva2013time,hatami2018classification}, forecasting \cite{li2020forecasting}, anomaly detection \cite{lin2024hierarchical} and %feature-wise
explanation \cite{kim2024cafo}. Moreover, the method in \cite{hatami2018classification}, and similarly in HCR-AdaAD \cite{lin2024hierarchical}, omit the thresholding in Eq.~\eqref{eq.rp} and uses $\|\mat{v}_{i} - \mat{v}_{j}\|$ to produce continuously valued images %in a classification task
to avoid information loss.


% \vspace{0.2cm}

\subsection{Other Methods}\label{sec.othermethod}

%There are some less commonly used methods. For example, in
Additionally, %there are some peripheral methods. %In addition to GAF,
\cite{wang2015encoding} introduces Markov Transition Field (MTF) for imaging a UTS. %$\mat{x}\in\mathbb{R}^{1\times T}$. 
%MTF first assigns each $x_{t}$ to one of $Q$ quantile bins, then builds a $Q\times Q$ Markov transition matrix $\mat{M}$ {\em s.t.} $\mat{M}_{i,j}$ represents the frequency %with which
%of the case when a point $x_{t}$ in the $i$-th bin is followed by a point $x_{t'}$ in the $j$-th bin, {\em i.e.}, $t=t'+1$. Matrix $\mat{M}$ serves as the input of a heatmap image.
MTF is a matrix $\mat{M}\in\mathbb{R}^{Q\times Q}$ encoding the transition probabilities over time segments, where $Q$ is the number of segments. %Moreover,
ImagenTime \cite{naiman2024utilizing} stacks the delay embeddings $\mat{v}_{1}$, ..., $\mat{v}_{l}$ in Eq.~\eqref{eq.de} to an $l\times m\tau$ matrix for visualizing UTSs. %It also uses a variant of STFT.
% The method in \cite{homenda2024time} introduces five different 2D images by counting, rearranging, replicating the values in a UTS. 
MSCRED \cite{zhang2019deep} uses heatmaps on the $d\times d$ correlation matrices of MTSs with $d$ variates for anomaly detection. 
Furthermore, some methods use a mixture of imaging methods by stacking different transformations. \cite{wang2015imaging} stacks GASF, GADF, MTF to a 3-channel image. %Similarly,
FIRTS \cite{costa2024fusion} builds a 3-channel image by stacking GASF, MTF and RP. %the GASF, MTF, RP representations of each UTS.
%\cite{jin2023classification} combines Line Plot with Constant-Q Transform (CQT) \cite{brown1991calculation}, a method related to wavelet transform ($\S$\ref{sec.spectrogram}), to generate 2-channel images.
The mixture methods encode a UTS with multiple views and were found more robust than single-view images in these works for %time series
classification tasks.

\subsection{How to Model MTS}\label{sec.modelmts}

In the above methods, Heatmap ($\S$\ref{sec.heatmap}) can be %directly
used to visualize the %2D
variate-time matrices, $\mat{X}$, of MTSs ({\em e.g.}, Fig. \ref{fig.structure}(b)), where correlated variates %are better to
should be spatially close to each other. Line Plot ($\S$\ref{sec.lineplot}) can be used to visualize MTSs by plotting all variates in the same image \cite{wimmer2023leveraging,daswani2024plots} or combining all univariate images to compose a bigger %1-channel
image \cite {li2023time}, but these methods only work for a small number of variates. Spectrogram ($\S$\ref{sec.spectrogram}), GAF ($\S$\ref{sec.gaf}), and RP ($\S$\ref{sec.rp}) were designed specifically for UTSs. For these methods and Line Plot, which are not straightforward %for MTS transformation,
in imaging MTSs, the general approaches %to use them %for MTS
include using channel independence assumption to model each variate individually \cite{nie2023time}, %like VisionTS \cite{chen2024visionts},
or stacking the images of $d$ variates to form a $d$-channel image %as did by
\cite{naiman2024utilizing,kim2024cafo}. %\cite{prithyani2024feasibility,naiman2024utilizing,kim2024cafo}.
However, the latter does not fit some vision models pre-trained on RGB images which requires 3-channel inputs (more discussions are deferred to $\S$\ref{sec.processing}).

\vspace{0.2cm}

\noindent{\bf Remark.} As a summary, Table \ref{tab.tsimage} recaps the salient advantages and limitations of the five primary imaging methods that are introduced in this section.

% \hh{can we have a table (e.g., rows are different imaging methods and columns are a few desirable propoerties) or a short paragraph to discuss/summarize/compare the strenths and weakness of different imaging methods for ts? This might bring some structure/comprehension to this section (as opposed to, e.g., some reviewer might complain that what we do here is a laundry list)}

\section{Imaged Time Series Modeling}\label{sec.model}

With image representations, time series analysis can be readily performed with vision models. This section discusses such solutions from %traditional vision models %($\S$\ref{sec.cnns})
%to the recent large vision models %($\S$\ref{sec.lvms})
%and large multimodal models.% ($\S$\ref{sec.lmms}).
the traditional models to the SOTA models.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{fig/fig_2.pdf}
% \vspace{-1em}
\caption{An illustration of different modeling strategies on imaged time series in (a)(b)(c) and task-specific heads in (d).}\label{fig.models}
\vspace{-0.2cm}
\end{figure*}

\subsection{Conventional Vision Models}\label{sec.cnns}

%Similar to
Following traditional %methods on
image classification, \cite{silva2013time} applies a K-NN classifier on the RPs of time series, \cite{cohen2020trading} applies an ensemble of fundamental classifiers such as %linear regression, SVM, Ada Boost, {\em etc.}
SVM and AdaBoost on the Line Plots %images
for time series classification. As an image encoder, %a typical encoder, %of images,
CNNs have been %extensively
widely used for learning image representations. %\cite{he2016deep}.
Different from using 1D CNNs on sequences %UTS or MTS
\cite{bai2018empirical}, %regular
2D or 3D CNNs can be applied on imaged time series as shown in Fig. \ref{fig.models}(a). %to learn time series representations by encoding their image transformations.
For example, %standard
regular CNNs have been used on Spectrograms \cite{du2020image}, tiled CNNs have been used on GAF images \cite{wang2015encoding,wang2015imaging}, dilated CNNs have been used on Heatmap images \cite{yazdanbakhsh2019multivariate}. More frequently, ResNet \cite{he2016deep}, Inception-v1 \cite{szegedy2015going}, and VGG-Net \cite{simonyan2014very} have been used on Line Plots \cite{jin2023classification,semenoglou2023image}, Heatmap images \cite{zeng2021deep}, RP images \cite{li2020forecasting,kim2024cafo}, GAF images \cite{barra2020deep,kaewrakmuk2024multi}, 
% Heatmaps \cite{zeng2021deep}, RPs \cite{li2020forecasting,kim2024cafo}, GAFs \cite{barra2020deep,kaewrakmuk2024multi},
and even a mixture of GAF, MTF and RP images \cite{costa2024fusion}. In particular, for time series generation tasks, %a diffusion model with U-Nets \cite{naiman2024utilizing} and GAN frameworks of CNNs \cite{li2022tts,karami2024timehr} have also been explored.%investigated.
GAN frameworks of CNNs \cite{li2022tts,karami2024timehr} and a diffusion model with U-Nets \cite{naiman2024utilizing} have also been explored.

Due to their small to medium sizes, these models are often trained from scratch using task-specific training data. %per task using the task's training set. %of time series images.
Meanwhile, fine-tuning {\em pre-trained vision models}  %such as those pre-trained on ImageNet, %\cite{deng2009imagenet}, 
have already been found promising in cross-modality knowledge transfer for time series anomaly detection \cite{namura2024training}, forecasting \cite{li2020forecasting} and classification \cite{jin2023classification}.

% \cite{li2020forecasting} uses ImageNet pretrained CNNs.

\subsection{Large Vision Models (LVMs)}\label{sec.lvms}

Vision Transformer (ViT) \cite{dosovitskiy2021image} has %given birth to
inspired the development of %some
modern LVMs %large vision models (LVMs)
such as %DeiT \cite{touvron2021training}, 
Swin \cite{liu2021swin}, BEiT \cite{bao2022beit}, and MAE \cite{he2022masked}. %Given an input image, ViT splits it
As Fig. \ref{fig.models}(b) shows, ViT splits an %input
image into {\em patches} of fixed size, then embeds each patch and augments it with a positional embedding. The %resulting
vectors of patches are processed by a Transformer %encoder
as if they were token embeddings. Compared to CNNs, ViTs are less data-efficient, but have higher capacity. %Consequently,
Thus, %the
{\em pre-trained} ViTs have been explored for modeling %the images of time series.
imaged time series. For example, AST \cite{gong2021ast} fine-tunes DeiT \cite{touvron2021training} on the filterbank spetrogram of audios %signals
for classification tasks and finds %using
ImageNet-pretrained DeiT is remarkably effective in knowledge transfer. The fine-tuning paradigm has also been %similarly
adopted in \cite{zeng2023pixels,li2023time} but with different pre-trained models %initializations
such as Swin by \cite{li2023time}. 
VisionTS \cite{chen2024visionts} %explains
attributes %the superiority of LVMs
LVMs' superiority over LLMs in knowledge transfer %over LLMs %as an outcome of
to the small gap between the pre-trained images and imaged time series. %the patterns learned from the large-scale pre-trained images and the patterns in the images of time series.
It %also
finds that with one-epoch fine-tuning, MAE becomes the SOTA time series forecasters on %many
some benchmark datasets.

Similar to %build
time series foundation models %\cite{das2024decoder,goswami2024moment,ansari2024chronos,shi2024time}, %such as TimesFM \cite{das2024decoder}, MOMENT \cite{goswami2024moment}, Chronos \cite{ansari2024chronos} and Time-MoE \cite{shi2024time},
such as TimesFM \cite{das2024decoder}, %and MOMENT \cite{goswami2024moment}, 
there are some initial efforts in pre-training ViT architectures with imaged time series. Following AST, SSAST \cite{gong2022ssast} introduced a %joint discriminative and generative
%masked spectrogram patch prediction self-supervised learning framework
masked spectrogram patch prediction framework for pre-training ViT on a large dataset -- AudioSet-2M. Then it becomes a backbone of some follow-up works such as AST-SED \cite{li2023ast} for sound event detection. %To be effective for UTSs,
For UTSs, ViTime \cite{yang2024vitime} generates a large set of Line Plots of synthetic UTSs for pre-training ViT, which was found superior over TimesFM in zero-shot forecasting tasks on benchmark datasets.

\subsection{Large Multimodal Models (LMMs)}\label{sec.lmms}

%As Large Multimodal Models (LMMs)
As LMMs %are getting
get growing attentions, some %of the
notable LMMs, such as LLaVA \cite{liu2023visual}, Gemini \cite{team2023gemini}, GPT-4o \cite{achiam2023gpt} and Claude-3 \cite{anthropic2024claude}, have been explored to consolidate the power of LLMs %on time series
and LVMs in time series analysis. 
Since LMMs support multimodal input via prompts, methods in this thread typically prompt LMMs with the textual and imaged representations of time series, %textual representation of time series and their %image transformations, transformed images,
%then instruct LMMs
and instructions on what tasks to perform ({\em e.g.}, Fig. \ref{fig.models}(c)).

InsightMiner \cite{zhang2023insight} is a pioneer work that uses the LLaVA architecture to generate %textual descriptions about
texts describing the trend of each input UTS. It extracts the trend of a UTS by Seasonal-Trend decomposition, encodes the Line Plot of the trend, and concatenates the embedding of the Line Plot with the embeddings of a textual instruction, which includes a sequence of numbers representing the UTS, {\em e.g.}, ``[1.1, 1.7, ..., 0.3]''. The concatenated embeddings are taken by a language model for generating trend descriptions. %It also fine-tunes a few layers with the generated texts to align LLaVA checkpoints with time series domain.
Similarly, \cite{prithyani2024feasibility} adopts the LLaVA architecture, but for MTS classification. An MTS is encoded by %a sequence of
the visual %token
embeddings of the stacked Line Plots of all variates. %meanwhile
%The method also stacks
%The time series of all variate are also stacked in a prompt % of all variates in a prompt
The matrix of the MTS is also verbalized in a prompt 
as the textual modality. %By manipulating token embeddings,
By integrating token embeddings, both %of these %works propose to
methods fine-tune some layers of the LMMs with some synthetic data.

Moreover, zero-shot and in-context learning performance of several commercial LMMs have been evaluated for audio classification \cite{dixit2024vision}, anomaly detection \cite{zhuang2024see}, and some synthetic tasks \cite{daswani2024plots}, where the image %({\em e.g.}, spectrograms, Line Plots)
and textual representations of a query %UTS or MTS
time series are integrated into a prompt. For in-context learning, these methods inject the images of a few example time series and their labels ({\em e.g.}, classes) %({\em e.g.}, classes, normal status)
into an instruction to prompt LMMs for assisting the prediction of the query time series.

\subsection{Task-Specific Heads}\label{sec.task}

%With the image embedding of a time series, the next step is to produce its prediction.
For classification tasks, most of the methods in Table \ref{tab.taxonomy} adopt a fully connected (FC) layer or multilayer perceptron (MLP) to transform an embedding into a probability distribution over all classes. For forecasting tasks, there are two approaches: (1) using a $d_{e}\times W$ MLP/FC layer to directly predict (from the $d_{e}$-dimensional embedding) the time series values in a future time window of size $W$ \cite{li2020forecasting,semenoglou2023image}; (2) predicting the pixel values that represent the future part of the time series and then recovering the time series from the predicted image \cite{yang2023your,chen2024visionts,yang2024vitime} ($\S$\ref{sec.processing} discusses the recovery methods). Imputation and generation tasks resemble forecasting %in the sense of predicting
as they also predict time series values. Thus approach (2) has been used for imputation \cite{wang2015imaging} and generation \cite{naiman2024utilizing,karami2024timehr}. %LMMs have been used for classification, text generation, and anomaly detection. For these tasks,
When using LMMs for classification, text generation, and anomaly detection, most of the methods prompt LMMs to produce the desired outputs in textual answers, circumventing task-specific heads \cite{zhang2023insight,dixit2024vision,zhuang2024see}.

%Forecasting: MLP, FC to predict numerical values using embeddings. Imputation of images (TSH). Classification: MLP, FC using embeddings.

\section{Pre-Processing and Post-Processing}\label{sec.processing}

To be successful in using vision models, some subtle design desiderata %to be considered
include {\bf time series normalization}, {\bf image alignment} and {\bf time series recovery}.

\vspace{0.2cm}

\noindent{\bf Time Series Normalization.} Vision models are usually trained on %images after Gaussian normalization (GN).
standardized images. To be aligned, the images introduced in $\S$\ref{sec.tsimage} should be normalized with a controlled mean and standard deviation, as did by \cite{gong2021ast} on spectrograms. In particular, as Heatmap is built on raw time series values, the commonly used Instance Normalization (IN) \cite{kim2022reversible} can be applied on the time series as suggested by VisionTS \cite{chen2024visionts} since IN share similar merits as Standardization. %although min-max normalization was used by \cite{karami2024timehr,zeng2021deep}.
Using Line Plot requires a proper range of y-axis. In addition to rescaling time series %by min-max or GN
\cite{zhuang2024see}, ViTST \cite{li2023time} introduced several methods to remove extreme values from the plot. GAF requires min-max normalization on its input, as it transforms time series values withtin $[0, 1]$ to polar coordinates ({\em i.e.}, arccos). In contrast, input to RP is usually normalization-free as an $\ell_{2}$ norm is involved in Eq.~\eqref{eq.rp} before thresholding.%for a comparison with a threshold.

\vspace{0.2cm}

\noindent{\bf Image Alignment.} When using pre-trained models, it is imperative to fit the image size to the input requirement of the models. This is especially true for Transformer based models as they use a fixed number of positional embeddings to encode the spacial information of image patches. For 3-channel RGB images such as Line Plot, it is straightforward to meet a pre-defined size by adjusting the resolution when producing the image. For images built upon matrices such as Heatmap, Spectrogram, GAF, RP, the number of channels and matrix size need adjustment. For the channels, one method is to duplicate a matrix to 3 channels \cite{chen2024visionts}, another way is to average the weights of the 3-channel patch embedding layer into a 1-channel layer \cite{gong2021ast}. For the image size, bilinear interpolation is a common method to resize input images \cite{chen2024visionts}. Alternatively, AST \cite{gong2021ast} %use cut and bilinear interpolation on
resizes the positional embeddings instead of the images to fit the model to a desired input size. However, the interpolation in these methods may either alter the time series or the spacial information in positional embeddings.

% single-channel (UTS), RGB channel (UTS), duplicate channels (UTS), multi-channel (MTS).

%Bilinear interpolation.

%Correlated variates are better to be spatially close to each other.

%\subsection{Pre-training}

\vspace{0.2cm}

\noindent{\bf Time Series Recovery.} As stated in $\S$\ref{sec.task}, tasks such as forecasting, imputation and generation requires predicting time series values. For models that predict pixel values of images, post-processing involves recovering time series from the predicted images. Recovery from Line Plots is tricky, it requires locating pixels that %correspond to
represent time series and mapping them back to the original values. This can be done by manipulating a grid-like Line Plot as introduced in \cite{yang2023your,yang2024vitime}, which has a recovery function. In contrast, recovery from Heatmap is straightforward as it directly stores the predicted time series values \cite{zeng2021deep,chen2024visionts}. Spectrogram is underexplored in these tasks and it remains open on how to recover time series from it. The existing work \cite{zeng2023pixels} uses Spectrogram for forecasting only with an MLP head that directly predicts time series. %predicts time series values.
GAF supports accurate recovery by an inverse mapping from polar coordinates to normalized time series \cite{wang2015imaging}. However, RP lost time series information during thresholding (Eq.~\ref{eq.rp}), thus may not fit recovery-demanded tasks without using an {\em ad-hoc} prediction head.


% Line Plot was regarded as matrices with rows and columns for mapping in \cite{sood2021visual}.


%\section{Tasks and Time Series Recovery}

%\subsection{Task-Specific Head}

% \subsection{Time Series Recovery}


