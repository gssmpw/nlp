
Advancements in Large Language Models (LLMs) have enhanced possibilities for model explainability, making natural language explanations preferable over feature attribution methods. Most LLMs can provide explanations for their predictions at minimal cost \citep{Wei2022ChainOT}. However, despite their plausibility, these explanations do not always reflect the modelâ€™s actual reasoning, potentially misleading practitioners \citep{Turpin2023LanguageMD}.

The idea of faithfulness aims to assess how accurately explanations reflect the true reasoning mechanism of the model. While numerous methods have been proposed to measure faithfulness for natural language-based explanations, they are criticized for not adequately considering the model's inner workings, relying instead on simplistic consistency measures~\citep{Parcalabescu2023OnMF}. Furthermore, while many faithfulness metrics have been developed, currently there are no reliable evaluation frameworks for comparing them. To address this gap in the field, we introduce a new evaluation framework, \methodname, along with a new benchmark for comparing various faithfulness metrics. Our framework extends the notion of \textit{diagnosticity} \citep{Chan2022ACS}, which measures how often a faithfulness metric favors faithful explanations over unfaithful ones, and applies it to faithfulness metrics for natural language explanations. We investigate knowledge editing approaches for causally generating faithful and unfaithful explanation pairs and evaluate diagnosticity through three tasks. These tasks include (1) a fact-checking task, (2) an analogy task, (3) an object counting, and (4) a multi-hop reasoning task. Figure \ref{fig:method_overview} shows an overview of our framework. We evaluate a diverse set of faithfulness metrics, including post-hoc explanation-based and chain-of-thought (CoT)-based metrics: Counterfactual Edits \citep{Atanasova2023FaithfulnessTF}, Simulatability, metrics based on corrupting CoT explanations \citep{Lanham2023MeasuringFI}, and CC-SHAP \citep{Parcalabescu2023OnMF}. Our evaluation shows that \textbf{all tested faithfulness metrics often fail to surpass a random baseline} or perform at near-random levels. These results underscore \textbf{the need for a diagnosticity-first approach} in the design of faithfulness metrics. Moreover, our analysis demonstrates that \textbf{metrics producing continuous faithfulness scores are more diagnostic than those with binary scores}. 

Our contributions include (1) a framework for evaluating faithfulness metrics for natural language explanations, (2) a new dataset spanning four tasks, and (3) benchmarking of prominent faithfulness metrics across multiple knowledge editing methods and models to provide insights into their reliability.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/main_figure.pdf}
    \caption{Our framework consists of three stages: (1) \textbf{Knowledge Editing}: applying counterfactual edits to the models; (2) \textbf{Explanation Generation}: generating faithful and unfaithful explanation pairs using the edited models, or synthetically generating such pairs based on the edits; (3) \textbf{Diagnosticity Evaluation}: assessing the chosen faithfulness metric with one of the edited models using the faithful-unfaithful explanation pairs. Diagnostic faithfulness metrics should assign a higher faithfulness score to the faithful explanation than to the unfaithful one.}
    \label{fig:method_overview}
\end{figure}

