\paragraph{Faithfulness} Faithfulness quantifies how well explanations reflect a modelâ€™s true reasoning process. Let $M_{\vtheta}$ denote an LLM parameterized by $\vtheta$ and with a context $\vc$, operating on a token set $\mathcal{V}$ such that $M(\vt^{\text{in}} \mid \vc) = \vt^{\text{out}}$, where $\vt^{\text{in}} = \langle t^{\text{in}}_1 \ldots, t^{\text{in}}_{N_{\text{in}}} \rangle$, $\vt^{\text{out}} = \langle t^{\text{out}}_1 \ldots, t_{N_{\text{out}}} \rangle$ and $\vc = \langle c_1 \ldots, c_{N_c} \rangle$ ; $t^{\text{in}}_i, t^{\text{out}}_i, c_i \in \mathcal{V}$; $N_{\text{in}}$,  $N_{\text{out}}$ and $N_c$ represent the lengths of the input, output and context sequences. The context $\vc$ consists of instructions or prompts. For brevity, we use $M$ to denote a model parameterized by $\vtheta$ with context $\vc$. The input and output sequences can take many forms. For the simplest case $\vt^{\text{in}} = \vx$ and $\vt^{\text{out}} = \vy$ where $( \vx, \vy)$ is an input-output pair for a task. With appropriate prompting, the output can take the form  $\vt^{\text{out}} = \vy \oplus \vvarepsilon $  for post-hoc explanations or $\vt^{\text{out}} = \vvarepsilon \oplus \vy $ for chain-of-thought (CoT) explanations, where $\vvarepsilon$ is the explanation and $\oplus$ denotes sequence concatenation. We define a faithfulness metric as $\mathcal{F}\left(\vx, \vy, \vvarepsilon, M\right) \in \mathbb{R}$, where $\mathcal{F}$ represents how faithfully explanation $\vvarepsilon$ represents the reasoning process for input-output pair $(\vx,\vy)$ for the model $M$. %While explanations can take different forms, such as importance scores, we focus on text-based explanations. 

\subsection{Faithfulness Metrics}

We focus on seven prominent faithfulness metrics: (1)~Counterfactual Edits~\citep{Atanasova2023FaithfulnessTF}, (2)~Simulatability, metrics based on CoT corruptions~\citep{Lanham2023MeasuringFI} (including (3)~Early Answering, (4)~Adding Mistakes, (5)~Paraphrasing, and (6)~Filler Tokens), and (7)~CC-SHAP~\citep{Parcalabescu2023OnMF}. While Simulatability and Counterfactual Edits target post-hoc explanations, the others are tailored for CoT explanations. CC-SHAP is applicable to both types of explanations. Next, we review these metrics.

\paragraph{Counterfactual Edits} \citet{Atanasova2023FaithfulnessTF} propose a faithfulness metric based on the principle that an explanation is unfaithful if the model prediction changes after a counterfactual edit to the input, but the explanation fails to reflect the edit. A limitation of this approach is the need to train a separate neural editor for each model-dataset pair to make such counterfactual interventions. Instead, we follow their random baseline based on the same rationale, where they insert a random adjective before a noun or a random adverb before a verb, as \citet{Parcalabescu2023OnMF} did. In this approach, an explanation is considered unfaithful if the prediction changes after word insertion and the explanation fails to mention the inserted words.

\paragraph{Simulatability} Simulatability assesses faithfulness from the lens of the extent to which whether an explanation enables a simulator to predict the model's output  \citep{DoshiVelez2017TowardsAR, Hase2020EvaluatingEA, Hase2020LeakageAdjustedSC, Wiegreffe2020MeasuringAB, Chan2022FRAMEER}. We follow \citet{Chan2022FRAMEER}'s definition of simulatability as $\mathbbm{1}_S (\vy_i \mid \vx_i, \vvarepsilon_i) - \mathbbm{1}_S (\vy_i \mid \vx_i)$, where $\mathbbm{1}_S(b \mid a)$ is the accuracy of $S$ in predicting $b$ given $a$.

\paragraph{Corrupting CoT} \citet{Lanham2023MeasuringFI} identify four corruption techniques to measure CoT-faithfulness: (1) \textit{Early Answering}, truncating the CoT to get an early answer; (2) \textit{Adding Mistakes}, introducing mistakes into the CoT, and regenerating; (3) \textit{Paraphrasing}, paraphrasing the CoT and regenerating; and (4) \textit{Filler Tokens}, replacing the CoT with ellipses. An explanation is considered unfaithful if the corruption does not alter the original prediction (except for Paraphrasing, where prediction changes signify unfaithfulness). While these metrics were initially proposed as binary measures, we extend them by quantifying faithfulness in terms of changes to prediction scores.

\paragraph{CC-SHAP} \citet{Parcalabescu2023OnMF} assess faithfulness by aligning input contributions to prediction and explanation using SHAP \citep{Lundberg2017AUA} scores. They calculate importance scores for each input token's prediction, then for each token in the explanation, aggregating them. Convergence of these score distributions is then measured. This method is applicable to both post-hoc and Chain-of-Thought (CoT) explanations.



\subsection{Knowledge Editing}
Our framework generates faithful-unfaithful explanation pairs by modifying facts within LLMs using knowledge editing. This is necessary as LLM knowledge can become outdated over time.  
Knowledge editing methods allow updates without altering unrelated knowledge \citep{cohen2024evaluating,Zhang2024ACS, Patil2023CanSI, geva-etal-2023-dissecting, gupta-etal-2023-editing, hartvigsen2023aging,zheng-etal-2023-edit, Meng2022LocatingAE, mitchell2022fast}, using triplets consisting of subject \textit{s}, object \textit{o}, and relation \textit{r}. For instance, they can update ($s = \text{Joe Biden}$, $r = \text{is the president of}$, $o = \text{the United States}$) to ($s = \text{Donald Trump}$, $r = \text{is the president of}$, $o = \text{the United States}$) while keeping other information unchanged. We explore two knowledge editing methods: (1) In-Context Editing (ICE) \citep{Cohen2023EvaluatingTR}, and (2) MEMIT \citep{meng2023memit}. ICE is preferred since it requires no parameter updates. Unlike ICE, MEMIT relies on a rigid subject-object-target template, which limits its use in complex scenarios. Additionally, MEMIT-like methods are sensitive to hyperparameters making them less applicabile, even with reported optimal values for specific models \citep{Wang2023EasyEditAE}. Finally, in-context approaches consistently surpass MEMIT in multi-step reasoning tasks~\cite{Cohen2023EvaluatingTR}.