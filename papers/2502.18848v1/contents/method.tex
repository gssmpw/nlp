Our \methodname framework is inspired by the idea of \textit{diagnosticity}, which evaluates how well faithfulness metrics distinguish between faithful and unfaithful explanations. In \ref{subsec:diagnosticity}, we summarize diagnosticityt as introduced by \citet{Chan2022ACS} for evaluating feature attribution methods. In \ref{subsec:method}, we introduce \methodname, describing how it builds on diagnosticity and extends it to natural language explanations using causal interventions via edited models.

\subsection{Diagnosticity}
\label{subsec:diagnosticity}
Faithfulness has been widely studied~\citep{Jacovi2020TowardsFI}, leading to multiple metrics but no unified evaluation framework. We adopt \textit{diagnosticity}, proposed by \citet{Chan2022ACS}, which measures how often a faithfulness metric prefers faithful over unfaithful explanations. For example, if a model correctly answers ``No" to the question, ``Is Rihanna a researcher?" based on her being a singer, a faithful explanation should reflect this reasoning. An explanation that provides an irrelevant rationale (e.g., the number of albums she has sold) or false information (e.g., assigning her the wrong occupation) would be unfaithful.

Following the notation from \citet{Chan2022ACS}, let $u$ and $v$ be explanations (regardless of form, e.g., language, feature attributions, etc.), with $u \succ v$ denoting that $u$ is more faithful than $v$. A faithfulness metric $\mathcal{F}$ ranks the explanations as $u \succ_{\mathcal{F}} v$ if it assigns a higher faithfulness score to $u$ than $v$. Then, the diagnosticity of the metric $\mathcal{F}$ is:

\begin{equation}
D(\mathcal{F}) = P(u \succ_{\mathcal{F}} v | u \succ v)
\end{equation}

We approximate this as:

\begin{equation}
D(\mathcal{F}) \approx \frac{1}{|Z|} \sum_{(u_i, v_i) \in Z} 
\mathbbm{1} (u_i \succ_{\mathcal{F}} v_i) 
\end{equation}

where $Z$ contains pairs of faithful$(u_i)$ and unfaithful $(v_i)$ explanations for input-output pairs $(\vx_i, \vy_i)$. Since higher faithfulness scores represent more faithful explanations,  we rewrite:

\begin{equation}
D(\mathcal{F}) \approx \frac{1}{|Z|} \sum_{(u_i, v_i) \in Z} 
\mathbbm{1} (\mathcal{F}_{p_i, M}\left(u_i \right) > \mathcal{F}_{ p_i, M}\left(v_i\right))
\end{equation}

where $p_i = (\vx_i, \vy_i)$.

\subsection{Causal Diagnosticity}
\label{subsec:method}
To obtain unfaithful explanations for measuring diagnosticity, \citet{Chan2022ACS} 
use random feature attribution scores. While random scores can work for structured explanations like feature attributions,
this approach is not straightforward for natural language explanations. Random text cannot function as a meaningful explanation and cannot ensure unfaithfulness in a coherent way.

To address this, we introduce \methodname, which generates unfaithful explanations using knowledge editing. Rather than injecting random noise, we modify a modelâ€™s internal knowledge. For example, consider the \texttt{capitalOf} relation for the question ``Is Paris the capital of France?" and a model that correctly associates this to the knowledge $(s = \text{Paris}, r = \text{is the capital of}, o = \text{France})$. By altering the model's knowledge, we create two variations where the subject $s$ is replaced with Berlin or London. Both modified models should answer ``No" to the original question but for different reasons: ``No, because Berlin is the capital of France." and ``No, because London is the capital of France." In particular, each of these two explanations should be unfaithful to the model that generated the other. 

Formally, let $\vy_i$ be the prediction for the input $\vx_i$ while \BModel{} and \OModel{} be the altered models. \BModel{} generates the explanation \bepsilon{i} and \OModel{} generates the explanation \oepsilon{i}. We modify diagnosticity as:

\begin{equation}
\label{eq:causal_diagnosticity}
D(\mathcal{F}) = \frac{1}{|Z|} \sum_{(\bepsilon{i}, \oepsilon{i}) \in Z} 
\mathbbm{1} (\mathcal{F}_{p_i, \BModel{}}\left(\bepsilon{i}\right) > \mathcal{F}_{p_i, \BModel}\left(\oepsilon{i}\right))
\end{equation}

Models \BModel{} and \OModel{} are edited such that \bepsilon{i} is faithful to \BModel{}, while \oepsilon{i} is unfaithful to \BModel{}. They are created by modifying parameters $\vtheta$ or context $\vc$, depending on the knowledge editing method.  The choice of models is flexible: in most cases, either model can be used in Equation \ref{eq:causal_diagnosticity} by swapping 
$\bepsilon{i}$ and $\oepsilon{i}$. However, in some tasks, one explanation may be faithful to both models, restricting arbitrary model selection. For example, in our Analogy task (see Figure \ref{fig:tasks}), the \texttt{capitalOf} relation exists in only one model, while the \texttt{cityOf} relation holds in both. Additionally, the original model $\vtheta$ can be used as long as it satisfies the needed faithfulness relations with the explanation pairs. Nevertheless, we opt to create two edited variants of the models, even when reflecting factual knowledge, to guarantee that all conditions are met.