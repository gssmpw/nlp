
Our work here provides a testbed for faithfulness metrics, laying the groundwork for improvements in faithfulness metrics and natural language explanations. We benchmark popular post-hoc and CoT-based faithfulness metrics across tasks.  Given the shortcomings of existing metrics, future research should prioritize diagnosticity-first approaches and explore contrastive methods that avoid OOD perturbations, such as Paraphrasing or continuous Counterfactual Edits \citep{siegel-etal-2024-probabilities}. Additionally, normalizing faithfulness scores using model-generated counterfactuals that preserve predictions can be a promising direction for robust evaluation.