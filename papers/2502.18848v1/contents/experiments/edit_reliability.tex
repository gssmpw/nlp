\methodname assumes that one explanation in each pair is faithful to the evaluated model, while the other is unfaithful. While synthetic explanations in principle ensure faithfulness or unfaithfulness with respect to the edited model, their practical accuracy depends on the success of the editing method. We assess this by comparing the perplexities of the explanation pairs. The premise here is that faithful explanations should have a lower perplexity than unfaithful ones.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/ppl_comparison.pdf}
    \caption{Frequency of which explanations deemed as faithful have lower perplexity than those deemed as unfaithful, for each task and each model. Higher frequency indicates the higher success in applied edits.}
    \label{fig:ppl_comparison}
\end{figure}

Figure \ref{fig:ppl_comparison} shows the percentage of explanations deemed faithful that have lower perplexity than those deemed unfaithful, broken down by task and model. For FactCheck, the edits show strong success, with scores approaching 1.0, followed by Multi-hop Reasoning and Object Counting. In contrast, edits for the Analogy task underperform, with scores falling below 50\%. This is likely due to conflicting information about widely known facts, such as capital cities. To explore whether this limitation is inherent to ICE, particularly in cases where there is a conflict between the knowledge encoded in the modelâ€™s parameters and the context, we compare the \texttt{qwen2.5-7b} model using both ICE and MEMIT setups across three tasks, as shown in Figure \ref{fig:ppl_comparison_memit_vs_ice}, Appendix \ref{appendix:results}. MEMIT edits show significant improvements in Analogy and Object Counting, but shows 
a nearly 50\% drop in model editing performance for FactCheck.
Overall, these results indicate that the success of knowledge editing methods varies significantly depending on the task.



