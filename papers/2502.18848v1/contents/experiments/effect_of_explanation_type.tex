\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/synth_vs_real.pdf}
    \caption{Diagnosticity scores for each metric on \texttt{qwen-2.5-7b} using model generated and synthetically generated explanations. The scores are averaged across three tasks: Fact Check, Analogy, Object Counting and Multi-hop Reasoning.}
    \label{fig:explanation_types}
\end{figure}

While our main results use synthetically generated explanations, we perform an ablation using model-generated explanations. We evaluate all faithfulness metrics using \texttt{qwen-2.5-7b}, limiting model-generated explanations to 100 tokens. Figure \ref{fig:explanation_types} compares model-generated and synthetic explanations across faithfulness metrics, with diagnosticity scores averaged over four tasks. The results indicate that synthetic explanations generally achieve higher scores than model-generated ones. Although the comparative ranking of faithfulness metrics varies between explanation types, a strong correlation is observed between the two methods: $\rho = 0.90$ for FactCheck, $\rho = 0.97$ for Analogy, $\rho = 0.96$ for Object Counting, and $r=0.99$ for Multi-hop reasoning. Qualitatively, we find that for Analogy and Object Counting, model-generated explanations often fail to articulate the applied edits, aligning with our findings on edit success in Section \ref{subsec:edit_reliability}. Given their higher consistency and lower computational cost, synthetic explanations remain a strong alternative. 
