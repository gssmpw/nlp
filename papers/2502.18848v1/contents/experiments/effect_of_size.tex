\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/size_effect.pdf}
    \caption{Comparison of diagnosticity scores with respect to model size for four metrics using 7B, 32B and 72B \texttt{qwen2.5-instruct} models.}
    \label{fig:size_effect}
\end{figure}


Our main experiments are conducted on relatively small models with 7 billion to 9 billion parameters. We evaluate the impact of model size on diagnosticity by testing Simulatability, Filler Tokens, Adding Mistakes, and Paraphrasing on three models: \texttt{qwen2.5-7b-instruct}, \texttt{qwen2.5-32b-instruct}, and \texttt{qwen2.5-72b-instruct}. For the 32B and 72B models, we use the AWQ \citep{lin2023awq} versions. Since AWQ versions of the larger base models are unavailable, we instead use their instruction-tuned counterparts.  

Figure \ref{fig:size_effect} shows no clear scaling trends in diagnosticity. Simulatability remains stable, while Adding Mistakes is lowest at 32B (for Analogy and Object Counting) highest in FactCheck. Paraphrasing scales well in FactCheck and Object Counting but follows an inverse trend in Analogy. Filler Tokens slightly improves in FactCheck but worsens in Object Counting. While Figure \ref{fig:ppl_comparison_size} in Appendix \ref{appendix:results} suggests edit reliability improves with model size, our results indicate that diagnosticity scaling shows no uniform patterns across different configurations.