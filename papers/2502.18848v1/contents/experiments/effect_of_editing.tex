\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/memit_vs_ike.pdf}
    \caption{Diagnosticity scores for each metric on \texttt{qwen-2.5-7b} using two knowledge editing methods: ICE and MEMIT. The scores are averaged across three tasks: Fact Check, Analogy and Object Counting.}
    \label{fig:edit_methods}
\end{figure}

We replace ICE with MEMIT \citep{meng2023memit}, a locate-and-edit approach enabling bulk edits (details in Appendix \ref{appendix:model_editing}). Since Multi-hop reasoning edits do not align with MEMIT's format, this task is excluded. Figure \ref{fig:edit_methods} compares MEMIT and ICE, averaging diagnosticity scores across three tasks. ICE shows higher diagnosticity scores than MEMIT. A correlation analysis shows a strong agreement between the methods ($\rho = 0.91$ for FactCheck, $\rho = 0.90$ for Analogy, and $\rho = 0.96$ for Object Counting), indicating that the choice of editing method does not significantly alter results, though relative rankings between metrics shift. Full results for MEMIT are in Appendix \ref{appendix:results}.