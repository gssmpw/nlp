\input{tables/main_results}

\paragraph{Experimental Setup} We evaluate the seven metrics described in Section \ref{sec:background} with two different LLMs: \texttt{qwen-2.5-7b} \citep{Yang2024Qwen25TR}, and \texttt{gemma-2-9b-it} \citep{Riviere2024Gemma2I}. For our main experiments, we use ICE as the knowledge editing method and synthetic explanations to ensure faithfulness to the edited model.

Table \ref{tab:diagnosticity} reports diagnosticity scores across the four tasks for both models. Among post-hoc metrics, CC-SHAP outperforms Simulatability and Counterfactual Edits ($p < 0.05$, McNemar’s test\footnote{McNemar’s test used for all testing unless stated}). The Multi-hop Reasoning task is an exception, where differences between Simulatability and Counterfactual Edits are not significant. Among CoT-based metrics, no single best performer emerges, but CC-SHAP excels in FactCheck, while Paraphrasing significantly leads in Object Counting for both models ($p < 0.05$).

When examining discrepancies between models, notable differences emerge in Early Answering and Filler Tokens for the FactCheck task, as well as in CC-SHAP across post-hoc and CoT setups for the Analogy task. These inconsistencies may be attributed to the way these metrics operate. In Early Answering metric, Truncated explanations may result in incomplete or nonsensical sentences, which can be out-of-distribution (OOD) for the model. Consequently, the drop in prediction scores may not solely reflect the unfaithfulness of the altered explanation but rather the model's sensitivity to OOD inputs \citep{Hooker2018ABF}. 

Crucially, none of the metrics exceeds the baseline value of $0.50$ across all models and tasks. Paraphrasing is the most reliable, significantly outperforming ($p < 0.05$, Binomial test) the baseline in 5 out of 8 measurements. The Multi-hop Reasoning task is especially challenging, as all metrics fail to significantly surpass baseline performance. 