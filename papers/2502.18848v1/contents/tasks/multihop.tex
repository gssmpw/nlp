\paragraph{Task} This task extends diagnostic evaluation to complex multi-step reasoning, and multiple facts. Like Fact Check, it ensures identical answers across counterfactual settings but requires multi-hop chains to reach conclusions. Unlike FactCheck, this task requires explanations grounded in multi-step reasoning chains, where even divergent explanation pairs share overlapping intermediate steps. \\ 
\noindent \textbf{Dataset} We construct this dataset using StrategyQA \citep{Geva2021DidAU}, a multi-hop QA benchmark that provides gold-standard fact decompositions for each example. We generate two counterfactual variants for one fact per question, preserving the original answer while altering the reasoning. When facts are interdependent, we propagate modifications to ensure consistency. Next, we generate explanations for each counterfactual set using the original decompositions. We use \texttt{gpt-4o} for generating counterfactuals and explanations, which we manually verify for logical coherence. The final data consists of 200 high-quality examples, balancing complexity with computational feasibility. 