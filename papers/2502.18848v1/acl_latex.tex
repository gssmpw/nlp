% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{ACL/acl}
\usepackage{ACL/acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}


\usepackage{microtype}


\usepackage{inconsolata}


\usepackage{graphicx}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{bbm}
\usepackage{todonotes}
\usepackage{multirow}
\usepackage{booktabs}


\input{math_commands.tex}
\input{custom_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{A Causal Lens for Evaluating Faithfulness Metrics}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Kerem Zaman \quad Shashank Srivastava\\
  UNC Chapel Hill \\
  \texttt{\{kzaman, ssrivastava\}@cs.unc.edu}
  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's internal reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, a unified evaluation framework remains absent. To address this gap, we present \methodname, a framework to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of causal diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate a variety of faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that all tested faithfulness metrics often fail to surpass a random baseline. Our work underscores the need for improved metrics and more reliable interpretability methods in LLMs. \footnote{The code and dataset will be released upon acceptance.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}
\input{contents/introduction}

\section{Background}
\label{sec:background}
\input{contents/background}

\section{Method}
\label{sec:method}
\input{contents/method.tex}

\section{Tasks}
\label{sec:tasks}
\input{contents/tasks}

\section{Experiments}
\label{sec:experiments}
\input{contents/experiments}

\section{Conclusion}
\label{sec:conclusion}
\input{contents/conclusion}

\section*{Limitations}
\label{sec:limitations}
\input{contents/limitations}

\section*{Acknowledgements}
\label{sec:Acknowledgements}
The authors thank Peter Hase for useful pointers in knowledge editing literature, Rakesh R Menon for feedback on the paper's diagrams and All Digital, Valery Zanimanski, BomSymbols and Twemoji team for providing various icons used in Figures \ref{fig:method_overview} and \ref{fig:tasks}. This work was supported by the NSF grant DRL2112635.

\bibliography{custom}

\appendix
\label{sec:appendix}
\input{contents/appendix}

\end{document}
