\begin{figure}[h]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/posthoc_prompt.txt}
\caption{The prompt used for post-hoc explanations.}
%\end{minipage}
\label{fig:posthoc_prompt}
\end{figure}


\begin{figure}[h]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/cot_prompt.txt}
\caption{The prompt used for CoT explanations.}
%\end{minipage}
\label{fig:cot_prompt}
\end{figure}

\begin{figure}[h]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/ccshap_posthoc_prompt.txt}
\caption{The prompt used to generate post-hoc explanations for CC-SHAP.}
%\end{minipage}
\label{fig:ccshap_posthoc_prompt}
\end{figure}

\begin{figure}[h]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/simulator_prompt.txt}
\caption{The prompt used for simulator model.}
%\end{minipage}
\label{fig:simulator_prompt}
\end{figure}

\begin{figure}[h]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/add_mistake_prompt.txt}
\caption{The prompt used for adding mistakes to explanations.}
%\end{minipage}
\label{fig:add_mistake_prompt}
\end{figure}

\begin{figure}[h]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/add_mistake_prompt.txt}
\caption{The prompt used for paraphrasing explanations.}
%\end{minipage}
\label{fig:paraphrase_prompt}
\end{figure}

\paragraph{Predictions and Explanations} We use different prompts based on the explanation type, which can be either post-hoc or CoT, to generate predictions and explanations. After feeding the model with the designated prompt, we obtain the prediction based on the next-token logits, selecting the token with the highest score among those corresponding to the task-specific labels. Given an input prompt $\vx$, a label set $L$, and the logit produced by the model $M_{\theta}$ for label $L_i$ when given $\vx$, denoted as $p_{\theta}(L_i \mid \vx)$, the class scores are computed as follows:

\begin{equation}
\hat{z}_i = \frac{\exp(p_{\theta}(L_i \mid \vx))}{\sum_{L_j \in L} \exp(p_{\theta}(L_j \mid \vx))}
\end{equation}

The predicted class is determined as

\begin{equation} 
\hat{y} = \arg \max_{L_i \in L} \hat{z}_i
\end{equation}.

For the FactCheck task, we set the label set as $L = \{ \text{"yes"}, \text{"no"} \}$, while for other tasks, we use $L = \{ \text{"A"}, \text{"B"} \}$, as they follow a multiple-choice format. 

Figure \ref{fig:posthoc_prompt} illustrates the prompt used to generate post-hoc explanations, where the obtained prediction is inserted into the prompt accordingly. Figure \ref{fig:cot_prompt} presents the prompt used for CoT explanations. After generating the explanation, we append \textit{"The best answer is:"} at the end of the prompt to obtain the final prediction. For the post-hoc variant of CC-SHAP, we use a slightly modified prompt, following \citet{Parcalabescu2023OnMF}, as shown in Figure \ref{fig:ccshap_posthoc_prompt}.

\paragraph{Simulatability} We use \texttt{llama-3.2-3b-instruct} as our simulator model, employing the prompt shown in Figure \ref{fig:simulator_prompt}.

\paragraph{Corrupting CoT} For the continuous variants of methods based on corrupting CoT, we use the prediction scores for the top predicted class before and after corruption, denoted as $\hat{z}_i$ and $\hat{z}^{\prime}_i$, respectively. In the original binary approach for \textit{Early Answering}, \textit{Filler Tokens}, and \textit{Adding Mistakes}, an explanation is considered unfaithful if corruption does not alter the prediction. For these metrics, we instead use the absolute value of the change in prediction after intervention, $\| \hat{z}_i - \hat{z}^{\prime}_i \|$ as the faithfulness score. A greater change following corruption indicates a more faithful explanation. Conversely, in \textit{Paraphrasing}, an explanation is considered unfaithful if corruption does alter the prediction. Therefore, we define the faithfulness score as  $1 - \| \hat{z}_i - \hat{z}^{\prime}_i \|$.

For specific corruption strategies, we follow established implementations:  
\begin{itemize}
    \item \textbf{Early Answering}: We truncate one-third of the explanation, following \citet{Parcalabescu2023OnMF}.  
    \item \textbf{Filler Tokens}: We replace each character with \texttt{"..."}, following \citet{Parcalabescu2023OnMF}.  
    \item \textbf{Adding Mistakes \& Paraphrasing}: We use \texttt{llama-3.2-3b-instruct} as a helper model to introduce corruption.  
\end{itemize} 

Figures \ref{fig:add_mistake_prompt} and \ref{fig:paraphrase_prompt} illustrate the prompts used to generate the intended corruptions for \textit{Adding Mistakes} and \textit{Paraphrasing}, respectively.
