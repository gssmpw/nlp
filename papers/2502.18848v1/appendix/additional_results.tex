Table \ref{tab:binary_vs_continuous} compares the binary and continuous variants of CoT-corruption-based metrics. Table \ref{tab:diagnosticity_memit} reports the diagnosticity scores when the knowledge editing method is switched from ICE to MEMIT, while Table \ref{tab:diagnosticity_model_generated} presents the scores when using model-generated explanations instead of synthetic ones. Table \ref{tab:scaling_diagnosticity} examines how diagnosticity scores vary with model size for selected metrics. Additionally, Figure \ref{fig:ppl_comparison_memit_vs_ice} compares MEMIT and ICE in terms of edit success across three tasks, whereas Figure \ref{fig:ppl_comparison_size} illustrates the edit success of models of different sizes across four tasks.

\input{tables/binary_results}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/ppl_comparison_memit_vs_ice.pdf}
    \caption{Comparison of the edit reliability of two editing methods across three tasks using \texttt{qwen2.5-7b}. A higher frequency indicates greater success in applied edits.}
    \label{fig:ppl_comparison_memit_vs_ice}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figures/ppl_comparison_size.pdf}
    \caption{Comparison of the edit reliability across four tasks using models of varying sizes: \texttt{qwen2.5-7b-instruct}, \texttt{qwen2.5-32b-instruct-awq}, \texttt{qwen2.5-72b-instruct-awq}. A higher frequency indicates greater success in applied edits.}
    \label{fig:ppl_comparison_size}
\end{figure}

\input{tables/diagnosticity_memit_results}


\input{tables/diagnosticity_model_generated_results}

\input{tables/scaling_results}