\subsection{MEMIT}

MEMIT \citep{meng2023memit} is a locate-and-edit-based knowledge editing approach. Unlike previous methods \citep{Zhu2020ModifyingMI, DeCao2021EditingFK, mitchell2022fast, NEURIPS2023_3927bbdc, Meng2022LocatingAE}, MEMIT effectively scales to edit thousands of facts simultaneously. Similar to ROME \citep{Meng2022LocatingAE}, MEMIT leverages causal mediation analysis \citep{Pearl2001DirectAI, Vig2020InvestigatingGB, Meng2022LocatingAE} to identify MLP layers in transformer networks that store factual knowledge and selectively modify them.

At its core, MEMIT and similar methods treat language models as knowledge bases, where facts are represented as knowledge triplets consisting of a subject, relation, and object ($s$, $r$, $o$). Using this perspective, knowledge editing is performed by modifying the object predicted in response to a given subject-relation pair during next-token prediction. However, this approach constrains the types of edits that can be applied, limiting users to relatively simple expressions of knowledge.
\subsection{In-Context Knowledge Editing}
\label{appendix:ice_details}

\begin{figure}[t]
%\begin{minipage}{\textwidth}
\lstinputlisting[language=, caption=, label=lst:promptfc]{others/ice_prompt.txt}
\caption{The prompt used for ICE.}
%\end{minipage}
\label{fig:ice_prompt}
\end{figure}

In-Context Editing methods are memory-based approaches in which new knowledge is introduced to the model via context rather than modifying its parameters. While most memory-based methods, such as IKE \citep{zheng-etal-2023-edit}, MeLLo \citep{Zhong2023MQuAKEAK}, and PokeMQA \citep{Gu2023PokeMQAPK}, do not involve any additional training or parameter updates, some methods require training. For instance, SERAC \citep{mitchell2022memory} trains a separate counterfactual model to process inputs related to updated knowledge without modifying the original model’s parameters.

In this study, we adopt ICE \citep{cohen2024evaluating} as our knowledge editing method, which operates by prepending new facts to the input context. We adapt the prompt template used by \citet{wang-etal-2024-easyedit}, as shown in Figure \ref{fig:ice_prompt}. Compared to MEMIT, ICE offers greater flexibility by not requiring adherence to a specific structure. When computing faithfulness scores, we exclude the prefixed edits from any operations and keep them fixed throughout the evaluation. 

\subsection{Task-based Editing Templates}
\label{appendix:edit_templates}

\input{tables/edit_templates}

Table \ref{tab:edit_templates} shows the templates we use for editing models in each task. For the FactCheck task, there is a variety of prompts where the action or situation of the subject differs, but the target is always located at the end of the prompt. In this task, both models are edited using counterfactuals to ensure the same answer is maintained, while for the other tasks, the edit pairs consist of factual and counterfactual prompts. 

For the Analogy task, we follow \textbf{Template \#1} to edit the model to change the capital of a given country. Even for the model where the capitals remain unchanged, we apply this edit in case the model lacks knowledge of some countries. For both models, we reinforce the \orelation{\texttt{cityOf}} relation by applying \textbf{Template \#2}. 

For the Object Counting task, we use the corresponding template in Table \ref{tab:edit_templates} to edit the model by altering the types of entities. For the \textit{touristic attraction} category, we use \textit{is located in} instead of \textit{is}. Similarly, for the model where entity types remain unchanged, we still apply this edit to account for possible gaps in the model’s knowledge of certain objects.



