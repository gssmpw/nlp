\begin{table*}[t!]
        \centering
        \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
        \textbf{Metric} & \textbf{FactCheck} & \textbf{Analogy} & \textbf{Object Counting} & \textbf{Multi-Hop} \\
        \midrule

        \multicolumn{4}{l}{\textbf{Post-hoc}} \\
        \quad CC-SHAP & \textbf{\underline{0.547}} & \textbf{0.233} & \textbf{0.414} & \textbf{\underline{0.575}}   \\
                \quad Simulatability & 0.083 & 0.010 & 0.018 & 0.000   \\
                \quad Counterfact. Edits & 0.001 & 0.000 & 0.000 & 0.000   \\
                \midrule

        \multicolumn{4}{l}{\textbf{CoT}} \\
        \quad Early Answering & 0.504 & \textbf{\underline{0.582}} & 0.504 & 0.515   \\
                \quad Filler Tokens & 0.475 & \underline{0.544} & 0.496 & 0.490   \\
                \quad Adding Mistakes & 0.478 & 0.506 & 0.473 & 0.470   \\
                \quad Paraphrasing & \textbf{\underline{0.565}} & 0.498 & \textbf{\underline{0.581}} & \textbf{0.520}   \\
                \quad CC-SHAP & 0.510 & 0.353 & 0.447 & 0.480   \\
                \bottomrule
        \end{tabular}
        }
        \caption{Diagnosticity scores of each metric across three tasks using \texttt{qwen2.5-7b} as the model and ICE as the knowledge editing method, \textbf{with model-generated explanations}. Bold numbers indicate the highest scores on each task across the two categories of faithfulness
metrics: post-hoc and CoT. Underlined numbers show the diagnosticity scores that are significantly higher than $0.5$ (Binomial, $p < 0.05$).}
        \label{tab:diagnosticity_model_generated}
    \end{table*}
