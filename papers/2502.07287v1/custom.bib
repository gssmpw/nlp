@article{PAI2021managing,
title={Managing the Risks of AI Research: Six Recommendations for Responsible Publication
},
url={
https://partnershiponai.org/paper/responsible-publication-recommendations/}, author = {Partnership on AI},
year={2021}}

@article{ada2022looking,
  title={Looking before we leap: Expanding ethical review processes for AI and data science research},
  author="{Ada Lovelace Institute}",
  url={https://www.adalovelaceinstitute.org/wp-content/uploads/2022/12/Ada-Lovelace-Institute-Looking-before-we-leap-Dec-2022.pdf},
  year={2022}, month={December},
  publisher={Institute, Ada Lovelace }
}

@article{hatherley2020limits,
  title={Limits of trust in medical AI},
  author={Hatherley, Joshua James},
  journal={Journal of medical ethics},
  volume={46},
  number={7},
  pages={478--481},
  year={2020},
  publisher={Institute of Medical Ethics}
}

@inproceedings{sap2019risk,
  title={The risk of racial bias in hate speech detection},
  author={Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={1668--1678},
  year={2019}
}

@inproceedings{deng2023understanding,
  title={Understanding Practices, Challenges, and Opportunities for User-Engaged Algorithm Auditing in Industry Practice},
  author={Deng, Wesley Hanwen and Guo, Boyuan and Devrio, Alicia and Shen, Hong and Eslami, Motahhare and Holstein, Kenneth},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}

@inproceedings{deng2023investigating,
  title={Investigating Practices and Opportunities for Cross-functional Collaboration around AI Fairness in Industry Practice},
  author={Deng, Wesley Hanwen and Yildirim, Nur and Chang, Monica and Eslami, Motahhare and Holstein, Kenneth and Madaio, Michael},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={705--716},
  year={2023}
}

@inproceedings{cheng2019explaining,
  title={Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders},
  author={Cheng, Hao-Fei and Wang, Ruotong and Zhang, Zheng and O'connell, Fiona and Gray, Terrance and Harper, F Maxwell and Zhu, Haiyi},
  booktitle={Proceedings of the 2019 chi conference on human factors in computing systems},
  pages={1--12},
  year={2019}
}

@article{blodgett2020language,
  title={Language (technology) is power: A critical survey of" bias" in nlp},
  author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  journal={arXiv preprint arXiv:2005.14050},
  year={2020}
}

@article{awad2018moral,
  title={The moral machine experiment},
  author={Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-Fran{\c{c}}ois and Rahwan, Iyad},
  journal={Nature},
  volume={563},
  number={7729},
  pages={59--64},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{chen2023algorithmic,
  title={Algorithmic fairness in artificial intelligence for medicine and healthcare},
  author={Chen, Richard J and Wang, Judy J and Williamson, Drew FK and Chen, Tiffany Y and Lipkova, Jana and Lu, Ming Y and Sahai, Sharifa and Mahmood, Faisal},
  journal={Nature biomedical engineering},
  volume={7},
  number={6},
  pages={719--742},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{barocas2016big,
  title={Big data's disparate impact},
  author={Barocas, Solon and Selbst, Andrew D},
  journal={Calif. L. Rev.},
  volume={104},
  pages={671},
  year={2016},
  publisher={HeinOnline}
}

@inproceedings{zhao2021understanding,
  title={Understanding and evaluating racial biases in image captioning},
  author={Zhao, Dora and Wang, Angelina and Russakovsky, Olga},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={14830--14840},
  year={2021}
}
@inproceedings{sorensen2024valueKaleidoscope,
  title={Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties},
  author={Sorensen, Taylor and Jiang, Liwei and Hwang, Jena and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and Sap, Maarten and Tasioulas, John and Choi, Yejin},
  year={2024},
  url={https://arxiv.org/abs/2309.00779},
  booktitle={AAAI}
}
@article{li2024safetyanalyst,
  title={SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation},
  author={Li, Jing-Jing and Pyatkin, Valentina and Kleiman-Weiner, Max and Jiang, Liwei and Dziri, Nouha and Collins, Anne G. E. and Schaich Borg, Jana and Sap, Maarten and Choi, Yejin and Levine, Sydney},
  journal={arXiv},
  year={2024},
  url={http://arxiv.org/abs/2410.16665}
}
@inproceedings{devos2022toward,
  title={Toward User-Driven Algorithm Auditing: Investigating users’ strategies for uncovering harmful algorithmic behavior},
  author={DeVos, Alicia and Dhabalia, Aditi and Shen, Hong and Holstein, Kenneth and Eslami, Motahhare},
  booktitle={Proceedings of the 2022 CHI conference on human factors in computing systems},
  pages={1--19},
  year={2022}
}

@inproceedings{gordon2022jury,
  title={Jury learning: Integrating dissenting voices into machine learning models},
  author={Gordon, Mitchell L and Lam, Michelle S and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@inproceedings{cheng2019explaining,
  title={Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders},
  author={Cheng, Hao-Fei and Wang, Ruotong and Zhang, Zheng and O'connell, Fiona and Gray, Terrance and Harper, F Maxwell and Zhu, Haiyi},
  booktitle={Proceedings of the 2019 chi conference on human factors in computing systems},
  pages={1--12},
  year={2019}
}

@article{zhang2023deliberating,
author = {Zhang, Angie and Walker, Olympia and Nguyen, Kaci and Dai, Jiajun and Chen, Anqing and Lee, Min Kyung},
title = {Deliberating with AI: Improving Decision-Making for the Future through Participatory AI Design and Stakeholder Deliberation},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579601},
doi = {10.1145/3579601},
abstract = {Research exploring how to support decision-making has often used machine learning to automate or assist human decisions. We take an alternative approach for improving decision-making, using machine learning to help stakeholders surface ways to improve and make fairer decision-making processes. We created "Deliberating with AI", a web tool that enables people to create and evaluate ML models in order to examine strengths and shortcomings of past decision-making and deliberate on how to improve future decisions. We apply this tool to a context of people selection, having stakeholders---decision makers (faculty) and decision subjects (students)---use the tool to improve graduate school admission decisions. Through our case study, we demonstrate how the stakeholders used the web tool to create ML models that they used as boundary objects to deliberate over organization decision-making practices. We share insights from our study to inform future research on stakeholder-centered participatory AI design and technology for organizational decision-making.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {125},
numpages = {32},
keywords = {deliberation, organizational decision-making, participatory algorithm design}
}

@inproceedings{shen2022model,
  title={The model card authoring toolkit: Toward community-centered, deliberation-driven AI design},
  author={Shen, Hong and Wang, Leijie and Deng, Wesley H and Brusse, Ciell and Velgersdijk, Ronald and Zhu, Haiyi},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={440--451},
  year={2022}
}

@inproceedings{shen2021value,
  title={Value cards: An educational toolkit for teaching social impacts of machine learning through deliberation},
  author={Shen, Hong and Deng, Wesley H and Chattopadhyay, Aditi and Wu, Zhiwei Steven and Wang, Xu and Zhu, Haiyi},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={850--861},
  year={2021}
}

@inproceedings{borg2024required,
  title={What Is Required for Empathic AI? It Depends, and Why That Matters for AI Developers and Users},
  author={Borg, Jana Schaich and Read, Hannah},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={1306--1318},
  year={2024}
}

@article{ChatbotTeach,
title={Will Chatbots Teach Your Children?},
url={https://www.nytimes.com/2024/01/11/technology/ai-chatbots-khan-education-tutoring.html}, author = {The New York Times},
year={2024}}

@article{zhai2024effects,
  title={The effects of over-reliance on AI dialogue systems on students' cognitive abilities: a systematic review},
  author={Zhai, Chunpeng and Wibowo, Santoso and Li, Lily D},
  journal={Smart Learning Environments},
  volume={11},
  number={1},
  pages={28},
  year={2024},
  publisher={Springer}
}
@article{googlePAIR,
title={People AI Guidebook},
url={https://pair.withgoogle.com/guidebook/}, author = {People + AI Research},
year={2021}}

@article{googleRAI,
title={An update on our progress in responsible AI innovation},
url={https://blog.google/technology/ai/update-our-progress-responsible-ai-innovation/}, author = {Walker, Kent and Croak, Marian},
year={2021}}

@misc{openAI_research,
title={OpenAI: Our approach to alignment research},
url={https://openai.com/blog/our-approach-to-alignment-research/}, author = {OpenAI},
year={2022}, month = {August}}

@misc{AIML_Data_Society,
title={Data \& Society Announces the Launch of its
Algorithmic Impact Methods Lab},
url={https://datasociety.net/algorithmic-impact-methods-lab/}, author = {Data \& Society},
year={2023}, month = {October}}

@article{buccinca2023aha,
  title={AHA!: Facilitating AI Impact Assessment by Generating Examples of Harms},
  author={Bu{\c{c}}inca, Zana and Pham, Chau Minh and Jakesch, Maurice and Ribeiro, Marco Tulio and Olteanu, Alexandra and Amershi, Saleema},
  journal={arXiv preprint arXiv:2306.03280},
  year={2023}
}

@article{kieslich2023anticipating,
  title={Anticipating Impacts: Using Large-Scale Scenario Writing to Explore Diverse Implications of Generative AI in the News Environment},
  author={Kieslich, Kimon and Diakopoulos, Nicholas and Helberger, Natali},
  journal={arXiv preprint arXiv:2310.06361},
  year={2023}
}


@inproceedings{weidinger2022taxonomy,
  title={Taxonomy of risks posed by language models},
  author={Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={214--229},
  year={2022}
}


@article{hecht2020suggestions,
title={Suggestions for Writing NeurIPS 2020 Broader Impacts Statements
},
url={
https://brenthecht.medium.com/suggestions-for-writing-neurips-2020-broader-impacts-statements-121da1b765bf}, author = {Hecht, Brent},
year={2020}}


@article{bernstein2021ethics,
  title={Ethics and society review: Ethics reflection as a precondition to research funding},
  author={Bernstein, Michael S and Levi, Margaret and Magnus, David and Rajala, Betsy A and Satz, Debra and Waeiss, Quinn},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={52},
  pages={e2117261118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{moss2021assembling,
  title={Assembling accountability: algorithmic impact assessment for the public interest},
  author={Moss, Emanuel and Watkins, Elizabeth Anne and Singh, Ranjit and Elish, Madeleine Clare and Metcalf, Jacob},
  journal={Available at SSRN 3877437},
  year={2021}
}

@inproceedings{metcalf2021algorithmic,
  title={Algorithmic impact assessments and accountability: The co-construction of impacts},
  author={Metcalf, Jacob and Moss, Emanuel and Watkins, Elizabeth Anne and Singh, Ranjit and Elish, Madeleine Clare},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={735--746},
  year={2021}
}

@article{hecht2021s,
  title={It's time to do something: Mitigating the negative impacts of computing through a change to the peer review process},
  author={Hecht, Brent and Wilcox, Lauren and Bigham, Jeffrey P and Sch{\"o}ning, Johannes and Hoque, Ehsan and Ernst, Jason and Bisk, Yonatan and De Russis, Luigi and Yarosh, Lana and Anjum, Bushra and others},
  journal={arXiv preprint arXiv:2112.09544},
  year={2021}
}

@article{selbst2021institutional,
  title={AN INSTITUTIONAL VIEW OF ALGORITHMIC IMPACT},
  author={Selbst, Andrew D},
  journal={Harvard Journal of Law \& Technology},
  volume={35},
  number={1},
  year={2021},
  publisher={HeinOnline}
}

@article{reisman2018algorithmic,
  title={Algorithmic Impact Assessments: A Practical Framework for Public Agency},
  author={Reisman, Dillon and Schultz, Jason and Crawford, Kate and Whittaker, Meredith},
  journal={AI Now},
  year={2018}
}

@article{adashi2018belmont,
  title={The Belmont Report at 40: reckoning with time},
  author={Adashi, Eli Y and Walters, LeRoy B and Menikoff, Jerry A},
  journal={American Journal of Public Health},
  volume={108},
  number={10},
  pages={1345--1348},
  year={2018},
  publisher={American Public Health Association}
}


@article{ashurst2020NeurIPS,
  title={A Guide to Writing the NeurIPS Impact Statement
},
  author={Ashurst, Carolyn and Anderljung, Markus and Prunkl, Carina and Leike, Jan and Gal, Yarin and Shevlane, Toby and Dafoe, Allan},
  url={https://medium.com/@GovAI/a-guide-to-writing-the-neurips-impact-statement-4293b723f832},
  year={2020}, month={May},
  publisher={Centre of the Governance of AI, Medium}
}

@article{deng2025weaudit,
  title={WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI},
  author={Deng, Wesley Hanwen and Wang, Claire and Han, Howard Ziyu and Hong, Jason I and Holstein, Kenneth and Eslami, Motahhare},
  journal={arXiv preprint arXiv:2501.01397},
  year={2025}
}

@inproceedings{wong2021timelines,
  title={Timelines: A world-building activity for values advocacy},
  author={Wong, Richmond Y and Nguyen, Tonya},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2021}
}

@incollection{owen2020responsible,
  title={Responsible research and innovation: From science in society to science for society, with society},
  author={Owen, Richard and Macnaghten, Phil and Stilgoe, Jack},
  booktitle={Emerging technologies: ethics, law and governance},
  pages={117--126},
  year={2020},
  publisher={Routledge}
}

@article{jobin2019global,
  title={The global landscape of AI ethics guidelines},
  author={Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  journal={Nature Machine Intelligence},
  volume={1},
  number={9},
  pages={389--399},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{morgan2012environmental,
  title={Environmental impact assessment: the state of the art},
  author={Morgan, Richard K},
  journal={Impact assessment and project appraisal},
  volume={30},
  number={1},
  pages={5--14},
  year={2012},
  publisher={Taylor \& Francis}
}

% --------------- Impact Assessments Tools ------------------ %
% ----------------------------------------------------------- %

@misc{Neurips2021checklist,
  title={Introducing the NeurIPS 2021 paper checklist},
  author={Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},
  howpublished = {NeurIPS blog},
  url={https://blog.neurips.cc/ 2021/03/26/introducing-the-neurips-2021-paper-checklist/},
  year={2021}
  }

@misc{Neurips2020blog,
  title = {Getting Started with NeurIPS 2020},
  author = {Hsuan-Tien Lin and Maria-Florina Balcan and Raia Hadsell and Marc’Aurelio Ranzato},
  howpublished = {NeurIPS blog},
  url = {https://neuripsconf.medium.com/getting-started-with-neurips-2020-e350f9b39c28},
  year = {2020}
  }

@misc{Neurips2020workshop,
  title = {Navigating the Broader Impacts of AI Research},
  author = {Ashurst, Carolyn and Barocas, Solon and Campbell, Rosie and Raji, Deborah and Russell, Stuart},
  howpublished = {NeurIPS workshop},
  url = {https://ai-broader-impacts-workshop.github.io/},
  year = {2020}
  }

@article{ACL2023ethicspolicy,
  title={ACL 2023 Responsible NLP Research and Ethics Policy
},
  author="{ACL, Ethic Policy}",
  url={https://2023.aclweb.org/calls/main_conference/#ethics-policy},
  year={2023}, month={March}
}

@article{ICWSM,
  title = "AAAI ICWSM Paper Checklist",
  author = "{AAAI ICWSM 2024 Organizing Committee}",
  url = "https://drive.google.com/file/d/1RMsfCq_dVFwVIpzBUT8wUTdym-Qwi-sV/view?usp=sharing",
  year = "2024"
  }

@article{olteanu2023responsible,
  title={Responsible AI Research Needs Impact Statements Too},
  author={Olteanu, Alexandra and Ekstrand, Michael and Castillo, Carlos and Suh, Jina},
  journal={arXiv preprint arXiv:2311.11776},
  year={2023}
}

@article{CVPR2023EthicsGuidelines,
  title={CVPR 2024 Ethics Guidelines for Authors
},
  author="{CVPR, Ethics Guidelines}", url={https://cvpr2023.thecvf.com/Conferences/2023/EthicsGuidelines
},
  year={2023}, month={November}
}

@article{ICML2023EthicsGuidelines,
  title={International Conference on Machine Learning, Publication Ethics
},
  author="{ICML, Publication Ethics}", url={https://icml.cc/Conferences/2023/PublicationEthics
},
  year={2023}, month={November}
}

@article{NAIRRTF2023Strengthening,
  title={Strengthening and Democratizing the U.S. Artificial Intelligence
Innovation Ecosystem},
  author="{National Artificial Intelligence Research Resource Task Force}", url={https://www.ai.gov/wp-content/uploads/2023/01/NAIRR-TF-Final-Report-2023.pdf},
  year={2023}, month={Janurary}
}

@article{NAIRRTF2023FinalReport,
  title={National Artificial Intelligence Research Resource Task Force Releases Final Report
},
  author="{The White House}", url={https://www.whitehouse.gov/ostp/news-updates/2023/01/24/national-artificial-intelligence-research-resource-task-force-releases-final-report/},
  year={2023}, month={Janurary}
}

@article{ESR_Stanford,
  title={Ethics \& Society Review · Stanford University
},
  author="{Center for Advanced Study in the Behavioral Sciences}", url={https://casbs.stanford.edu/ethics-society-review-stanford-university},
  year={2020}
}

@article{AIA_CIO,
    author = "{Chief Information Officers Council, U.S.}",
    title = {Algorithmic Impact Assessment},
     url={https://www.cio.gov/aia-eia-js/#/},
  year={2021}, month={May}
}

@article{AIA_Canada,
    author = "{Government of Canada}",
    title = {Algorithmic Impact Assessment, Government of Canada},
    url={https://open.canada.ca/data/en/dataset/5423054a-093c-4239-85be-fa0b36ae0b2e},
  year={2020}, month={May}
}

@article{PIA_Canada,
    author = "{Government of Canada}",
    title = {Privacy Impact Assessment, Office of teh Privacy Commissioner of Canada},
    url={https://www.priv.gc.ca/en/privacy-topics/privacy-impact-assessments/},
  year={2023}, month={Nov}
}

@article{PIA_GDPR,
    author = "{Data Protection Impact Assessment, General Data Protection Regulation}",
    title = {GDPR, Privacy Impact Assessment},
    url={https://gdpr-info.eu/issues/privacy-impact-assessment/#:~:text=Basically%2C%20a%20data%20protection%20impact,of%20the%20GDPR%20is%20relevant.},
  year={2023}, month={Nov}
}

@article{TarotCards,
    author = {Artefact},
    title = {The Tarot Cards of Tech: Discover the power of predicting impact},
    url={https://tarotcardsoftech.artefactgroup.com/.},
  year={2022}, month={Nov}
}

@article{AIA_NHS,
    author = "{The Ada Lovelace Insitute}",
    title = {Pioneering framework for assessing the impact of medical AI set to be trialled by NHS in world-first pilot},
    url={https://www.adalovelaceinstitute.org/press-release/algorithmic-impact-assessment-nhs/},
  year={2022}, month={Feb}
}

@article{AIA_Adalove,
    author = "{The Ada Lovelace Insitute}",
    title = {Algorithmic impact assessment: a case study in healthcare},
    url={https://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare/},
  year={2022}, month={Feb}
}

@article{RAIIAtemplate_MSFT,
    author = {Microsoft},
    title = {Microsoft Responsible AI Impact Assessment Template},
    url={https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf},
  year={2022}, month={June}
}

@article{RAIIAguide_MSFT,
    author = {Microsoft},
    title = {Microsoft Responsible AI Impact Assessment Guide},
    url={https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf},
  year={2022}, month={June}
}

@inproceedings{deng2022exploring,
  title={Exploring how machine learning practitioners (try to) use fairness toolkits},
  author={Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={473--484},
  year={2022}
}

@inproceedings{smith2023scoping,
  title={Scoping fairness objectives and identifying fairness metrics for recommender systems: The practitioners’ perspective},
  author={Smith, Jessie J and Beattie, Lex and Cramer, Henriette},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={3648--3659},
  year={2023}
}

@inproceedings{sonboli2021fairness,
  title={Fairness and transparency in recommendation: The users’ perspective},
  author={Sonboli, Nasim and Smith, Jessie J and Cabral Berenfus, Florencia and Burke, Robin and Fiesler, Casey},
  booktitle={Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
  pages={274--279},
  year={2021}
}

@inproceedings{deng2023understanding,
  title={Understanding Practices, Challenges, and Opportunities for User-Engaged Algorithm Auditing in Industry Practice},
  author={Deng, Wesley Hanwen and Guo, Boyuan and Devrio, Alicia and Shen, Hong and Eslami, Motahhare and Holstein, Kenneth},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}

@inproceedings{wang2024farsight,
  title={Farsight: Fostering Responsible AI Awareness During AI Application Prototyping},
  author={Wang, Zijie J. and Kulkarni, Chinmay and Wilcox, Lauren and Terry, Michael and Madaio, Michael},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--40},
  year={2024}
}

@article{deng2024supporting,
  title={Supporting Industry Computing Researchers in Assessing, Articulating, and Addressing the Potential Negative Societal Impact of Their Work},
  author={Deng, Wesley Hanwen and Barocas, Solon and Vaughan, Jennifer Wortman},
  journal={arXiv preprint arXiv:2408.01057},
  year={2024}
}

@misc{mun2024participaidemocraticsurveyingframework,
      title={Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits}, 
      author={Jimin Mun and Liwei Jiang and Jenny Liang and Inyoung Cheong and Nicole DeCario and Yejin Choi and Tadayoshi Kohno and Maarten Sap},
      year={2024},
      eprint={2403.14791},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2403.14791}, 
}

@inproceedings{brailsford2024exploring,
  title={Exploring the Association between Moral Foundations and Judgements of AI Behaviour},
  author={Brailsford, Joe and Vetere, Frank and Velloso, Eduardo},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2024}
}

@inproceedings{kieslich2024myfuture,
author = {Kieslich, Kimon and Helberger, Natali and Diakopoulos, Nicholas},
title = {My Future with My Chatbot: A Scenario-Driven, User-Centric Approach to Anticipating AI Impacts},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659026},
doi = {10.1145/3630106.3659026},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2071–2085},
numpages = {15},
keywords = {anticipatory governance, chatbots, impact assessment, scenario-writing},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{davani2024disentangling,
  title={Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates},
  author={Davani, Aida and D{\'\i}az, Mark and Baker, Dylan and Prabhakaran, Vinodkumar},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={2007--2021},
  year={2024}
}

@inproceedings{deng2022exploring,
	address = {Seoul Republic of Korea},
	title = {Exploring {How} {Machine} {Learning} {Practitioners} ({Try} {To}) {Use} {Fairness} {Toolkits}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533113},
	doi = {10.1145/3531146.3533113},
	language = {en},
	urldate = {2022-08-10},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
	month = jun,
	year = {2022},
	pages = {473--484},
}

@inproceedings{deng2023investigating,
  title={Investigating Practices and Opportunities for Cross-functional Collaboration around AI Fairness in Industry Practice},
  author={Deng, Wesley Hanwen and Yildirim, Nur and Chang, Monica and Eslami, Motahhare and Holstein, Kenneth and Madaio, Michael},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={705--716},
  year={2023}
}

@inproceedings{deng2023understanding,
  title={Understanding Practices, Challenges, and Opportunities for User-Engaged Algorithm Auditing in Industry Practice},
  author={Deng, Wesley Hanwen and Guo, Boyuan and Devrio, Alicia and Shen, Hong and Eslami, Motahhare and Holstein, Kenneth},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2023}
}

@inproceedings{kingsley2024investigating,
  title={Investigating What Factors Influence Users’ Rating of Harmful Algorithmic Bias and Discrimination},
  author={Kingsley, Sara and Zhi, Jiayin and Deng, Wesley Hanwen and Lee, Jaimie and Zhang, Sizhe and Eslami, Motahhare and Holstein, Kenneth and Hong, Jason I and Li, Tianshi and Shen, Hong},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={12},
  pages={75--85},
  year={2024}
}

@article{lederman2024language,
  title={Are language models more like libraries or like librarians? Bibliotechnism, the novel reference problem, and the attitudes of LLMs},
  author={Lederman, Harvey and Mahowald, Kyle},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1087--1103},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{bengio2023ai,
  title={AI and catastrophic risk},
  author={Bengio, Yoshua},
  journal={Journal of Democracy},
  volume={34},
  number={4},
  pages={111--121},
  year={2023},
  publisher={Johns Hopkins University Press}
}

@misc{roose2024canai, title={Can A.I. be blamed for a teen’s suicide?}, url={https://www.nytimes.com/2024/10/23/technology/characterai-lawsuit-teen-suicide.html}, journal={The New York Times}, publisher={The New York Times}, author={Roose, Kevin}, year={2024}, month={Oct}} 

@inproceedings{chien2024beyond,
  title={Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation},
  author={Chien, Jennifer and Danks, David},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={933--946},
  year={2024}
}

@article{awad2018moral,
  title={The moral machine experiment},
  author={Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-Fran{\c{c}}ois and Rahwan, Iyad},
  journal={Nature},
  volume={563},
  number={7729},
  pages={59--64},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{graham2008moral,
  title={Moral foundations questionnaire},
  author={Graham, Jesse and Nosek, Brian A and Haidt, Jonathan and Iyer, Ravi and Spassena, Koleva and Ditto, Peter H},
  journal={Journal of Personality and Social Psychology},
  year={2008}
}

@article{kahane2018beyond,
  title={Beyond sacrificial harm: A two-dimensional model of utilitarian psychology.},
  author={Kahane, Guy and Everett, Jim AC and Earp, Brian D and Caviola, Lucius and Faber, Nadira S and Crockett, Molly J and Savulescu, Julian},
  journal={Psychological review},
  volume={125},
  number={2},
  pages={131},
  year={2018},
  publisher={American Psychological Association}
}

@article{spreng2009toronto,
  title={The Toronto Empathy Questionnaire: Scale development and initial validation of a factor-analytic solution to multiple empathy measures},
  author={Spreng*, R Nathan and McKinnon*, Margaret C and Mar, Raymond A and Levine, Brian},
  journal={Journal of personality assessment},
  volume={91},
  number={1},
  pages={62--71},
  year={2009},
  publisher={Taylor \& Francis}
}

@inproceedings{golpayegani2023risk,
author = {Golpayegani, Delaram and Pandit, Harshvardhan J. and Lewis, Dave},
title = {To Be High-Risk, or Not To Be—Semantic Specifications and Implications of the AI Act’s High-Risk AI Applications and Harmonised Standards},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594050},
doi = {10.1145/3593013.3594050},
abstract = {The EU’s proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act’s hierarchy of risks, the AI systems that are likely to incur “high-risk” to health, safety, and fundamental rights are subject to the majority of the Act’s provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the “core concepts” whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act’s application.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {905–915},
numpages = {11},
keywords = {AI Act, harmonised standards, high-risk AI, semantic web, taxonomy},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@article{svensson2006professional,
  title={Professional occupations and status: A sociological study of professional occupations, status and trust},
  author={Svensson, Lennart G},
  year={2006},
  publisher={H{\o}gskolen i Oslo}
}

@misc{evetts2006introduction,
  title={Introduction: Trust and professionalism: Challenges and occupational changes},
  author={Evetts, Julia},
  journal={Current sociology},
  volume={54},
  number={4},
  pages={515--531},
  year={2006},
  publisher={Sage London, Thousand Oaks, CA and New Delhi}
}

@inproceedings{herdel2024exploregen,
  title={ExploreGen: Large language models for envisioning the uses and risks of AI technologies},
  author={Herdel, Viviane and {\v{S}}{\'c}epanovi{\'c}, Sanja and Bogucka, Edyta and Quercia, Daniele},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={584--596},
  year={2024}
}

@article{wang2023measuring,
  title={Measuring user competence in using artificial intelligence: validity and reliability of artificial intelligence literacy scale},
  author={Wang, Bingcheng and Rau, Pei-Luen Patrick and Yuan, Tianyi},
  journal={Behaviour \& information technology},
  volume={42},
  number={9},
  pages={1324--1337},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{graham2011mapping,
  title={Mapping the moral domain.},
  author={Graham, Jesse and Nosek, Brian A and Haidt, Jonathan and Iyer, Ravi and Koleva, Spassena and Ditto, Peter H},
  journal={Journal of personality and social psychology},
  volume={101},
  number={2},
  pages={366},
  year={2011},
  publisher={American Psychological Association}
}

@article{cushman2013action,
  title={Action, outcome, and value: A dual-system framework for morality},
  author={Cushman, Fiery},
  journal={Personality and social psychology review},
  volume={17},
  number={3},
  pages={273--292},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@misc{cheung2024measuring,
  title={Measuring the decision process in (moral) dilemmas: Self-report measures of reliance on rules, cost-benefit reasoning, intuition, \& deliberation},
  author={Cheung, Vanessa and Maier, Maximilian and Lieder, Falk},
  year={2024},
  publisher={OSF}
}

@article{kesebir2010morality,
  title={Morality (in Handbook of social psychology)},
  author={Kesebir, Selin and Haidt, Jonathan},
  journal={HANDBOOK OF SOCIAL PSYCHOLOGY, 5th Ed., S. Fiske, D. Gilbert, \& G. Lindzey, eds., Forthcoming},
  year={2010}
}

@inbook{Simon1966,
author="Simon, Herbert A.",
title="Theories of Decision-Making in Economics and Behavioural Science",
bookTitle="Surveys of Economic Theory: Resource Allocation",
year="1966",
publisher="Palgrave Macmillan UK",
address="London",
pages="1--28",
abstract="Recent years have seen important new explorations along the boundaries between economics and psychology. For the economist, the immediate question about these developments is whether they include new advances in psychology that can fruitfully be applied to economics. But the psychologist will also raise the converse question---whether there are developments in economic theory and observation that have implications for the central core of psychology. If economics is able to find verifiable and verified generalisations about human economic behaviour, then these generalisations must have a place in the more general theories of human behaviour to which psychology and sociology aspire. Influence will run both ways.2",
isbn="978-1-349-00210-8",
doi="10.1007/978-1-349-00210-8_1",
url="https://doi.org/10.1007/978-1-349-00210-8_1"
}

@article{atari2023morality,
  title={Morality beyond the WEIRD: How the nomological network of morality varies across cultures.},
  author={Atari, Mohammad and Haidt, Jonathan and Graham, Jesse and Koleva, Sena and Stevens, Sean T and Dehghani, Morteza},
  journal={Journal of Personality and Social Psychology},
  year={2023},
  publisher={American Psychological Association}
}

@article{solaiman2023evaluating,
  title={Evaluating the social impact of generative ai systems in systems and society},
  author={Solaiman, Irene and Talat, Zeerak and Agnew, William and Ahmad, Lama and Baker, Dylan and Blodgett, Su Lin and Chen, Canyu and Daum{\'e} III, Hal and Dodge, Jesse and Duan, Isabella and others},
  journal={arXiv preprint arXiv:2306.05949},
  year={2023}
}

@article{wongpakaran2013comparison,
  title={A comparison of Cohen’s Kappa and Gwet’s AC1 when calculating inter-rater reliability coefficients: a study conducted with personality disorder samples},
  author={Wongpakaran, Nahathai and Wongpakaran, Tinakon and Wedding, Danny and Gwet, Kilem L},
  journal={BMC medical research methodology},
  volume={13},
  pages={1--7},
  year={2013},
  publisher={Springer}
}

@online{NIST_2021, title={{Artificial Intelligence Risk Management Framework (AI RMF 1.0)}}, url={https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf}, author={National Institute of Standards and Technology}, year={2023}}

%% EU 
@misc{AIAct_2023, url={https://www.europarl.europa.eu/news/en/press-room/20231206IPR15699/artificial-intelligence-act-deal-on-comprehensive-rules-for-trustworthy-ai}, author={European Parliament
}, abstractNote={To ensure a human-centric and ethical development of Artificial Intelligence (AI) in Europe, MEPs endorsed new transparency and risk-management rules for AI systems.}, journal={European Parliament News}, year={2023}, month={Nov}, title={{Artificial Intelligence Act: deal on comprehensive rules for trustworthy AI}}}

@misc{executiveorder2023, 
    title={Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence},
    howpublished = "\url{https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/}",
    year = {2023},
    month = {Oct},
    note = "Accessed: 2024-01-02",
    journal={The White House}, 
    publisher={The United States Government}
} 

@misc{billofrights, 
title={Blueprint for an AI Bill of Rights}, 
howpublished="\url{https://www.whitehouse.gov/ostp/ai-bill-of-rights/}", year={2022}, note = "Accessed: 2024-1-18", journal={The White House}, publisher={The United States Government} }

@inproceedings{dominguez2024mapping,
  title={Mapping the individual, social and biospheric impacts of Foundation Models},
  author={Dom{\'\i}nguez Hern{\'a}ndez, Andr{\'e}s and Krishna, Shyam and Perini, Antonella Maia and Katell, Michael and Bennett, SJ and Borda, Ann and Hashem, Youmna and Hadjiloizou, Semeli and Mahomed, Sabeehah and Jayadeva, Smera and others},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={776--796},
  year={2024}
}

@inproceedings{pistilli2023stronger,
  title={Stronger together: on the articulation of ethical charters, legal tools, and technical documentation in ML},
  author={Pistilli, Giada and Mu{\~n}oz Ferrandis, Carlos and Jernite, Yacine and Mitchell, Margaret},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={343--354},
  year={2023}
}

@inproceedings{wu2024care,
author = {Wu, Yiying and Lee, Jung-Joo and Pillai, Ajit G. and Cho, Janghee and Ahmadpour, Naseem and Roto, Virpi and Sachathep, Thida and Liu, Jiashuo and Sawan, Mouna and Song, Dongjin and \v{C}ai\'{c}, Martina and Cheng, Lucas and Liu, Renxuan and Kettley, Sarah and Soares, Luis and Grace, Kazjon and Astell-Burt, Thomas},
title = {Collective Imaginaries for the Futures of Care Work},
year = {2024},
isbn = {9798400711145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678884.3681838},
doi = {10.1145/3678884.3681838},
abstract = {Care work, encompassing physical, emotional, and developmental support, is essential yet often undervalued reproductive labour within capitalist systems. Advancements in automation, robotics, AI, and mixed reality are poised to alter care work, raising questions to explore their roles in reshaping caregiving practice, the workforce, workplaces, and ecological care systems. This workshop aims to challenge three dominant narratives surrounding emerging technology and future care work: technology as a solution to the care crisis, aggressive technology investment in the growing care economy, and overlook of the "end of (care) jobs" in post-work discourse. Drawing on CSCW and HCI research, this workshop investigates the complex sociotechnical ecologies of care, envisioning future scenarios through participatory speculative design. By fostering dialogue among diverse participants, the workshop aims to explore the potential impacts of emerging technologies on care work, with a focus on holistic employee wellbeing, worker rights, equity, ethical integration, and socio-economic dynamics.},
booktitle = {Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing},
pages = {732–735},
numpages = {4},
keywords = {care work, future of care, future of work, reproductive labour, sociotechnical imaginaries},
location = {San Jose, Costa Rica},
series = {CSCW Companion '24}
}

@inproceedings{kawakami2024ai,
  title={AI Failure Loops in Feminized Labor: Understanding the Interplay of Workplace AI and Occupational Devaluation},
  author={Kawakami, Anna and Taylor, Jordan and Fox, Sarah and Zhu, Haiyi and Holstein, Ken},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  volume={7},
  pages={683--683},
  year={2024}
}

@inproceedings{ming2024labor,
author = {Ming, Joy and Pei, Lucy and Varanasi, Rama Adithya and Kawakami, Anna and Verdezoto, Nervo and Cheon, EunJeong},
title = {Labor, Visibility, and Technology: Weaving Together Academic Insights and On-Ground Realities},
year = {2024},
isbn = {9798400711145},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678884.3681827},
doi = {10.1145/3678884.3681827},
abstract = {Artificial intelligence (AI) and emerging technologies are expanding rapidly. While these technologies have the potential to support different types of work in different domains, they are also disrupting all forms of labor as we know---transforming traditional work and creating new forms of work. Among other facets of transformation, technology-mediated work shapes the visibility of the worker and their work, impacting their agency, earning, and overall occupational well-being. It is important to study and address this phenomenon because the impact of such disruptions are especially felt on the precarious, underserved, and marginalized workers. Our workshop aims to unpack the challenges that artificial intelligence (AI) technologies are creating on workers' technology-mediated work and their overall visibility by weaving together academic insights and on-ground realities.},
booktitle = {Companion Publication of the 2024 Conference on Computer-Supported Cooperative Work and Social Computing},
pages = {708–711},
numpages = {4},
keywords = {community-engaged research, design sprint, echnology-mediated work, research and practice, worker advocacy},
location = {San Jose, Costa Rica},
series = {CSCW Companion '24}
}

@inproceedings{raji2021ethics,
author = {Raji, Inioluwa Deborah and Scheuerman, Morgan Klaus and Amironesei, Razvan},
title = {You Can't Sit With Us: Exclusionary Pedagogy in AI Ethics Education},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445914},
doi = {10.1145/3442188.3445914},
abstract = {Given a growing concern about the lack of ethical consideration in the Artificial Intelligence (AI) field, many have begun to question how dominant approaches to the disciplinary education of computer science (CS)---and its implications for AI---has led to the current "ethics crisis". However, we claim that the current AI ethics education space relies on a form of "exclusionary pedagogy," where ethics is distilled for computational approaches, but there is no deeper epistemological engagement with other ways of knowing that would benefit ethical thinking or an acknowledgement of the limitations of uni-vocal computational thinking. This results in indifference, devaluation, and a lack of mutual support between CS and humanistic social science (HSS), elevating the myth of technologists as "ethical unicorns" that can do it all, though their disciplinary tools are ultimately limited. Through an analysis of computer science education literature and a review of college-level course syllabi in AI ethics, we discuss the limitations of the epistemological assumptions and hierarchies of knowledge which dictate current attempts at including ethics education in CS training and explore evidence for the practical mechanisms through which this exclusion occurs. We then propose a shift towards a substantively collaborative, holistic, and ethically generative pedagogy in AI education.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {515–525},
numpages = {11},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@article{taylor2024marginalized,
author = {Taylor, Jordan and Deng, Wesley Hanwen and Holstein, Kenneth and Fox, Sarah and Zhu, Haiyi},
title = {Carefully Unmaking the “Marginalized User”: A Diffractive Analysis of a Gay Online Community},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3673229},
doi = {10.1145/3673229},
abstract = {HCI scholars are increasingly engaging in research about “marginalized groups,” such as LGBTQ+ people. While normative habitual readings of marginalized people in HCI often highlight real problems, this work has been criticized for flattening heterogeneous experiences and overemphasizing harms. Some have advocated for expanding how we approach research on marginalized people (e.g., assets-based design, the everyday, and joy). Sensitized by unmaking literature, we explore this tension between conditions, experiences, and representations of marginality in HCI scholarship. To do so, we perform a diffractive analysis of posts in a gay online community by bringing two readings of the same data together: a normative habitual reading of marginalization and an expanded reading. By examining the relationship between empirical material and its representations by HCI researchers, we explore how to carefully unmake HCI research, thus maintaining and repairing our research community. We discuss the political and designerly implications of different readings of marginalized people and offer considerations for attending to the processes and afterlives of HCI research.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {81},
numpages = {30},
keywords = {Unmaking, Diffraction, Marginalized Groups, Marginalized Communities, Gay Men, LGBTQ, LGBTQ+ People}
}

@inproceedings{fox2020worker,
author = {Fox, Sarah E. and Khovanskaya, Vera and Crivellaro, Clara and Salehi, Niloufar and Dombrowski, Lynn and Kulkarni, Chinmay and Irani, Lilly and Forlizzi, Jodi},
title = {Worker-Centered Design: Expanding HCI Methods for Supporting Labor},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3375157},
doi = {10.1145/3334480.3375157},
abstract = {HCI has long considered sites of workplace collaboration. From airline cockpits to distributed groupware systems, scholars emphasize the importance of supporting a multitude of tasks and creating technologies that integrate into collaborative work settings. More recent scholarship highlights a growing need to consider the concerns of workers within and beyond established workplace settings or roles of employment, from steelworkers whose jobs have been eliminated with post-industrial shifts in the economy to contractors performing the content moderation that shapes our social media experiences. This one-day workshop seeks to bring together a growing community of HCI scholars concerned with the labor upon which the future of work we envision relies. We will discuss existing methods for studying work that we find both productive and problematic, with the aim of understanding how we might better bridge current gaps in research, policy, and practice. Such conversations will focus on the challenges associated with taking a worker-oriented approach and outline concrete methods and strategies for conducting research on labor in changing industrial, political, and environmental contexts.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {future of work, labor, worker-centered design},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{cheon2023bigtechwork,
author = {Cheon, EunJeong},
title = {Powerful Futures: How a Big Tech Company Envisions Humans and Technologies in the Workplace of the Future},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610103},
doi = {10.1145/3610103},
abstract = {Big tech companies have had increasing control over how we work with technologies and how technologies define the work we do. In this paper, I identify the sociotechnical futures that Amazon-one of the big tech companies-envisions and the future of work that the company is moving toward. I explore the future of fulfillment centers through an analysis of the patents on fulfillment center technologies which Amazon may turn into reality one day. In my analysis, I focus on humans by asking how they are configured in the future of fulfillment centers and, more specifically, how Amazon envisions the role of human labor within work automation and AI systems. The analysis reveals where and how humans are expected to "step in" to operate the future of fulfillment centers. I discuss my findings within and beyond CSCW, highlighting the importance of studying tech companies' imaginaries. I argue that by understanding tech companies' imaginaries, it becomes possible for us to launch effective sociotechnical interventions to negotiate or even resist their specific imaginaries and/or design ways for a more democratic uptake of companies' future technologies. Finally, I articulate practical ways in which patents can be utilized in CSCW research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {312},
numpages = {35},
keywords = {automation, big tech, human labor, human worker, patents, sociotechnical futures, the future of work}
}

@article{alcover2021aging,
  title={“Aging-and-Tech Job Vulnerability”: A proposed framework on the dual impact of aging and AI, robotics, and automation among older workers},
  author={Alcover, Carlos-Maria and Guglielmi, Dina and Depolo, Marco and Mazzetti, Greta},
  journal={Organizational Psychology Review},
  volume={11},
  number={2},
  pages={175--201},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{barocas2021evaluation,
author = {Barocas, Solon and Guo, Anhong and Kamar, Ece and Krones, Jacquelyn and Morris, Meredith Ringel and Vaughan, Jennifer Wortman and Wadsworth, W. Duncan and Wallach, Hanna},
title = {Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462610},
doi = {10.1145/3461702.3462610},
abstract = {Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts---both beneficial and harmful---that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {368–378},
numpages = {11},
keywords = {artificial intelligence, disaggregated evaluations, evaluations, fairness, machine learning},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{bhardwaj2024machine,
  title={Machine learning data practices through a data curation lens: An evaluation framework},
  author={Bhardwaj, Eshta and Gujral, Harshit and Wu, Siyi and Zogheib, Ciara and Maharaj, Tegan and Becker, Christoph},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1055--1067},
  year={2024}
}

@inproceedings{solaiman2023genairelease,
author = {Solaiman, Irene},
title = {The Gradient of Generative AI Release: Methods and Considerations},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3593981},
doi = {10.1145/3593013.3593981},
abstract = {As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {111–122},
numpages = {12},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@article{kuo2024policycraft,
  title={PolicyCraft: Supporting Collaborative and Participatory Policy Design through Case-Grounded Deliberation},
  author={Kuo, Tzu-Sheng and Chen, Quan Ze and Zhang, Amy X and Hsieh, Jane and Zhu, Haiyi and Holstein, Kenneth},
  journal={arXiv preprint arXiv:2409.15644},
  year={2024}
}

@article{michaels2019coding,
  title={Coding the Everyday Discrimination Scale: implications for exposure assessment and associations with hypertension and depression among a cross section of mid-life African American women},
  author={Michaels, Eli and Thomas, Marilyn and Reeves, Alexis and Price, Melisa and Hasson, Rebecca and Chae, David and Allen, Amani},
  journal={J Epidemiol Community Health},
  volume={73},
  number={6},
  pages={577--584},
  year={2019},
  publisher={BMJ Publishing Group Ltd}
}

@inproceedings{birhane2022values,
  title={The values encoded in machine learning research},
  author={Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and Agnew, William and Dotan, Ravit and Bao, Michelle},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={173--184},
  year={2022}
}

@report{ada2023survey,
  title = {How Do People Feel About AI? A Nationally Representative Survey of Public Attitudes to Artificial Intelligence in Britain},
  institution = {Ada Lovelace Institute and The Alan Turing Institute},
  year = {2023}
}

@inproceedings{suresh2024participation,
  title={Participation in the age of foundation models},
  author={Suresh, Harini and Tseng, Emily and Young, Meg and Gray, Mary and Pierson, Emma and Levy, Karen},
  booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1609--1621},
  year={2024}
}

@inproceedings{kramer2018when,
author = {Kramer, Max F. and Schaich Borg, Jana and Conitzer, Vincent and Sinnott-Armstrong, Walter},
title = {When Do People Want AI to Make Decisions?},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278752},
doi = {10.1145/3278721.3278752},
abstract = {AI systems are now or will soon be sophisticated enough to make consequential decisions. Although this technology has flourished, we also need public appraisals of AI systems playing these more important roles. This article reports surveys of preferences for and against AI systems making decisions in various domains as well as experiments that intervene on these preferences. We find that these preferences are contingent on subjects' previous exposure to computer systems making these kinds of decisions, and some interventions designed to mimic previous exposure successfully encourage subjects to be more hospitable to computer systems making these weighty decisions.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {204–209},
numpages = {6},
keywords = {decision-making, folk psychology, intervention, kidney exchange, preferences, survey},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@article{lee2019webuildai,
author = {Lee, Min Kyung and Kusbit, Daniel and Kahng, Anson and Kim, Ji Tae and Yuan, Xinran and Chan, Allissa and See, Daniel and Noothigattu, Ritesh and Lee, Siheon and Psomas, Alexandros and Procaccia, Ariel D.},
title = {WeBuildAI: Participatory Framework for Algorithmic Governance},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359283},
doi = {10.1145/3359283},
abstract = {Algorithms increasingly govern societal functions, impacting multiple stakeholders and social groups. How can we design these algorithms to balance varying interests in a moral, legitimate way? As one answer to this question, we present WeBuildAI, a collective participatory framework that enables people to build algorithmic policy for their communities. The key idea of the framework is to enable stakeholders to construct a computational model that represents their views and to have those models vote on their behalf to create algorithmic policy. As a case study, we applied this framework to a matching algorithm that operates an on-demand food donation transportation service in order to adjudicate equity and efficiency trade-offs. The service's stakeholders--donors, volunteers, recipient organizations, and nonprofit employees--used the framework to design the algorithm through a series of studies in which we researched their experiences. Our findings suggest that the framework successfully enabled participants to build models that they felt confident represented their own beliefs. Participatory algorithm design also improved both procedural fairness and the distributive outcomes of the algorithm, raised participants' algorithmic awareness, and helped identify inconsistencies in human decision-making in the governing organization. Our work demonstrates the feasibility, potential and challenges of community involvement in algorithm design.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {181},
numpages = {35},
keywords = {algorithmic fairness, collective participation, human-centered ai, matching algorithm, participatory algorithm design}
}