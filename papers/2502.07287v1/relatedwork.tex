\section{Related Works}
\label{sec:related-works}
%\jimin{related works section is a bit long right now, shorten}

%\wesley{Working on rewriting the RW}
% New Related Work:
%\maarten{Todo: write a 1-2 sentence ``intro'' to the related work section. Something like "we briefly summarize the background and related work towards assessing acceptability, perceptions, and impact of AI use cases."}

In this section, we briefly summarize the background and related work towards assessing acceptability and impact of AI use cases. In each subsection, we highlight how our work extends prior work.

\subsection{Assessing Impact of AI}

%While incorporating AI can have positive impacts on people's lives, from automating tedious tasks \citep{} to solving humanity's issues such as climate change and cancer \citep{}, AI can also cause harms towards marginalized comminutes through biased output\citep{}, hallucination\citep{}, or the potential of replacing human labor\citep{}. 

Recent years have witnessed increasing calls from academics \cite{kieslich2023anticipating, hecht2021s, bernstein2021ethics, Neurips2020workshop, Neurips2020blog, CVPR2023EthicsGuidelines, ACL2023ethicspolicy, ICML2023EthicsGuidelines, olteanu2023responsible, ESR_Stanford}, government \cite{AIA_Adalove, NAIRRTF2023FinalReport, NAIRRTF2023Strengthening}, civil society \cite{PAI2021managing, ada2022looking, AIA_Adalove, AIML_Data_Society, metcalf2021algorithmic, reisman2018algorithmic}, and industry \cite{RAIIAguide_MSFT, RAIIAtemplate_MSFT, googleRAI, openAI_research, hecht2021s, deng2024supporting} to assess the impact of AI systems designed and developed by AI researchers and practitioners. This effort has particularly highlighted the need to understand the \textit{positive} impact while grappling with the potential \textit{negative} impact of integrating AI into certain products and services that affect people's daily lives \cite{Neurips2020workshop, Neurips2020blog, hecht2021s}. 
%For example, in 2020, the Neural Information Processing Systems (NeurIPS) conference introduced a requirement that authors ``include a section in their submissions discussing the broader impact of their work, including possible societal consequences --- both positive and negative'' \cite{Neurips2020blog}. Other major AI conferences have introduced similar requirements \cite{ACL2023ethicspolicy, CVPR2023EthicsGuidelines, ICML2023EthicsGuidelines}. More recently, FAccT also encouraged researchers working on ethical issues in AI to also include an ``adverse impact statement'' in their submissions to consider the potential negative impact of their own work \cite{olteanu2023responsible}. Many civil society organizations, such as the Ada Lovelace Institute and the Partnership on AI, have also published reports to urge AI researchers and practitioners in anticipating and addressing potential negative impact of their work\citep{}.
%\maarten{this is missing the laundry list of taxonomies that many companies and experts have been creating (e.g., Laura Weidinger, etc.); we need to at least cite those, and articulate the issues with those (expert only, perhaps too focused on the general tech instead of specific use cases)}
In response, researchers in FAccT, HCI, and AI have developed tools and processes to support AI researchers and practitioners in anticipating the impact of AI systems they developed\citep{wang2024farsight, kieslich2023anticipating, buccinca2023aha, deng2024supporting, weidinger2022taxonomy}. For example, many have developed AI impact taxonomies or checklists to help developers categorize AI impact\citep{weidinger2022taxonomy, RAIIAtemplate_MSFT}. \citeauthor{wang2024farsight} and \citeauthor{deng2024supporting} developed tools and templates to support industry AI developers and researchers in assessing the potential negative societal impact of their work, such as job displacement or stereotyping social groups. 

However, this prior work primarily focuses on supporting \textit{AI experts} rather than \textit{diverse lay people}'s impact assessments of potential AI use cases. Our work extends these prior efforts by understanding diverse (and sometimes conflicting) perspectives on both positive and negative impact of AI use cases from lay people, as a crucial step to complement the AI impact assessments conducted by AI researchers and practitioners. \looseness=-1


\subsection{Understanding People's Perceptions of AI Use Cases}
\label{ssec:understanding-}
%\maarten{need to cite our own ParticipAI lol}
Responding to the calls on meaningfully engaging lay people in assessing the impact of specific AI use cases, prior work have started to understand lay people's perceptions of AI use case \cite{buccinca2023aha, kieslich2023anticipating, mun2024participaidemocraticsurveyingframework, kingsley2024investigating}. Among other findings, this prior work revealed a substantial amount of disagreement regarding the desired behavior of AI, primarily due to the subjectivity inherent in certain tasks (e.g., toxicity detection \citep{sap2019risk, blodgett2020language}, image captioning \citep{zhao2021understanding}) and ambiguous ethical implication of decisions made by AI for certain tasks (e.g., self driving cars \citep{awad2018moral}, medical AI \citep{chen2023algorithmic}, predictive analysis \citep{barocas2016big}). Work done by 
\citeauthor{mun2024participaidemocraticsurveyingframework} highlighted that lay people can envision diverse set of harms specific to different AI use cases, complementary to those defined by experts. Another line of work also begins to examine how factors such as demographic backgrounds and previous exposure to discrimination can affect people's sensitivity towards potential AI harms \citep{kingsley2024investigating}. \looseness=-1
%For example, through an online experiment on Prolific, Kingsley et al. surfaced that participants from marginalized gender or sexual orientation groups were more sensitive towards the potential harmful impact \cite{}. 

Our work extends this prior work by examined the \textbf{detailed reasoning processes} of lay people regarding the acceptability and the \textbf{trade-offs between positive and negative impacts} of AI use cases. In particular, we draw on model decision theory framework, such as the moral foundations developed by \citeauthor{graham2008moral}, to design a survey flow (See Figure \ref{fig:survey-flow} to solicit lay people's decision-making processes (and potential moral conflicts) when assessing both the benefits and harms of concrete AI use cases. 

% \maarten{I do think it might be good to have a small section on moral decision making, giving some background on CBR vs. RBR etc.? Unless we discuss it in the methods?}

\subsection{Background: Moral Decision Making}
\label{ssec:moral-decision-making}
% \maarten{I like this section, but I would substantially condense it (max 1 paragraph), and hit the following key points:
% - Explain in 1 sentence why we have background on morality: acceptability judgments are related to moral decision making, perhaps moreso that economic decision making.
% - Many moral psych and decision making experts have studied how people navigate judgments. 
% - In our work, we draw from two main themes that emerged from this research: add 1-3 sentences explaining CBR (more like consequentialism) and RBR (more like deontology) as well as moral foundations
% }
% \jana{I like Maarten's suggestion.  Two additions: (1) I wouldn't say anything about moral vs. economic decision-making, because that's a very controversial topic. (2) I think you can get rid of most of the second paragraph.  What you need to add, though, is a sentence or two about why we choose to use CBR, RBR, and moral foundations as our main frameworks for parsing moral reasoning and judgments (as opposed to other possible frameworks or moral theories).}
Morality, characterized by diverse values across cultures and social groups, aims to suppress selfishness to facilitate social life \citep{kesebir2010morality}. To understand decision-making in AI use cases, we draw on moral psychology and dual system theory. We examine two decision-making systems: cost-benefit reasoning, which assesses outcomes and consequences, and rule-based reasoning, focusing on norms, rules, and virtues \citep{cushman2013action,cheung2024measuring}. These correspond to utilitarian reasoning (maximizing good) and deontological reasoning (duties and rights), respectively. Additionally, we apply moral foundations theory \citep{graham2008moral} to identify values and potential moral conflicts in AI development.

% our interest in more moral systems and introduction to moral values
% Unlike economic decision making which maximizes utility when presented with uncertainty in choices \citep{Simon1966}, moral decision making relies on moral systems to ground reasoning. Morality, while diverse in its definition and represented by diverse values that vary across culture or even within society by class or politics (i.e., moral pluralism), has been characterized as a system with its aim to suppress selfishness to make social life possible \citep{kesebir2010morality}. Thus, to understand how people make decisions about AI use cases, we adopt moral values developed by \citeauthor{graham2008moral}, which could illuminate the relevant values when considering development of an AI use case as a moral decision and identify possible moral conflicts. 

\begin{comment}
% --------------- Old Related Work:

\subsection{Assessing Impact of AI}
%\jimin{focus more on lay people's assessment of impact}
%\wesley{Agreed that we should highlight lay peopleâ€™s AI impact assessment as the key motivation for our study! But I do think it also makes sense to include a brief paragraph mentioning the existing calls for AI impact assessments by researchers and experts (which still lack best practices), before transitioning to the need for engaging more diverse lay people in assessing AI impact.

%I can help writing this paragraph if you think this makes sense at a high level! It will be a shorter version of the "Background" section in one of my previous work on AI impact assessment: https://arxiv.org/pdf/2408.01057}
%\maarten{Yes, esp. if we adopt your new framing which starts with "There have been many calls for better impact assessments of technologies"! Let's do that!}
%\wesley{Will finish reworking this section by 01/17}

The growing usage and adoption of AI and experiences of unintended consequences and harms \citep{roose2024canai} has spurred extensive discussions about possible impact of AI. The scale of negative impacts in both currently present and anticipated harms vary widely from bias \citep{}, hallucinations \citep{}, and representational harms \citep{chien2024beyond} to existential risks \citep{bengio2023ai}, and positive impacts extend from automating tedious tasks \citep{} to solving humanity's issues such as climate change and cancer \citep{}. Many taxonomies have been created to guide and understand the risks of AI \citep{} and various tools have been developed to integrate these taxonomies into practice, from tools for developers \citep{} to forums to report and aggregate harms from AI usage \citep{}. 
% economic impact
% un-interpretable nature of ai has also made the discussion of AI impact more difficult as we do not understand and fully control this system
This uses scenario writing to understand the desirable and undesirable behaviors of AI chat bots \citep{kieslich2024myfuture}. 

AI as cultural technology \citep{lederman2024language}
% eu ai act and stuff? should we add that here?

\wesley{Given that we aim to streamline the related work, I feel like this related work section on moral decision making can be integrated as the first paragraph of 3.1.1 survey design. This can also help the reader to better connect the survey design with our theoretical background. If we think this would be a good idea, I can try moving things around!}

\subsection{Moral Decision Making}
\label{ssec:moral-decision-making}
Many works in decision making literature, especially those concerning psychology of human judgment, have aimed to characterize and have adopted a dual-system theory. The dual system frameworks often distinguish intuition versus deliberation, automaticity versus control, and emotion versus cognition \citep{cushman2013action}, but in our work, we focus on the two distinctions of cost-benefit reasoning, which focuses on the expected values of outcomes and consequences, and rule-based reasoning, which relies on values assigned to the action itself according to norms, rules, and virtues \citep{cushman2013action,cheung2024measuring}. These two types of reasoning also mirror two main reasoning types in moral reasoning: utilitarian reasoning, which aims at maximizing good and deontological reasoning, which is grounded in duties and rights, respectively. 

% our interest in more moral systems and introduction to moral values
Unlike economic decision making which maximizes utility when presented with uncertainty in choices \citep{Simon1966}, moral decision making relies on moral systems to ground reasoning. Morality, while diverse in its definition and represented by diverse values that vary across culture or even within society by class or politics (i.e., moral pluralism), has been characterized as a system with its aim to suppress selfishness to make social life possible \citep{kesebir2010morality}. Thus, to understand how people make decisions about AI use cases, we adopt moral values developed by \citeauthor{graham2008moral}, which could illuminate the relevant values when considering development of an AI use case as a moral decision and identify possible moral conflicts. 
% talk a little about moral dilemmas?

% how this all ties into our work
% - dual system: values in consequences vs action
% - moral value, used when deciding value of action or outcome
% - understanding how reasoning is done and what values are prevalent when considering either the action and outcome can help guide farther discussions about what dimensions matter when making decisions about ai and how people would be able to agree upon a system

\subsection{Factors in Decision Making about AI}
% moral decision making in AI behavior - introduce but highlight that these works did not address use cases
% those that considered use cases were mainly about perceptions of the use cases and not about the reasoning process
There is a significant amount of disagreement on desired behavior of AI due to subjectivity of certain tasks (e.g., toxicity detection \citep{}, image recommendation \citep{}) and ethical implication of decisions made by AI for certain tasks (e.g., self driving cars \citep{}, medical AI \citep{}, predictive analysis \citep{}). 
%Since AI systems rely on human annotated data for training, model behaviors are also heavily influenced by them \citep{}. Many works, thus, 
Prior work have explored the disagreements and decision making factors in data annotation for subjective tasks such as impact of moral and cultural values in annotating offensiveness of AI's output \citep{davani2024disentangling}. More direct assessment of AI's behavior has also been studied and factors that influence its acceptability such as demographic factors \citep{kingsley2024investigating} and moral and cultural values \citep{brailsford2024exploring}. Notably, 
%focusing on decision making in morally challenging scenarios where distribution of well-being and harms are also decided upon by AI, 
\citeauthor{awad2018moral} have also studied acceptability of machine behavior varying descriptive factors, surfacing that .... 

We expand these prior work by focusing not only on reasoning processes of weighing such factors applied to AI use cases but also surfacing which factors are relevant when it comes to a variety of AI usage and domain.

\end{comment}

%As application area of AI has widened and models more generalized (e.g., foundation models), there has been increasing interest to understand decisions regarding use cases. While use case level decision making require more extensive consideration, \citeauthor{mun2024participaidemocraticsurveyingframework} explored demographic factors and AI literacy in acceptability of AI use cases and \citeauthor{kieslich2024myfuture} performed exploratory analysis on demographic factors and AI attitude on impact anticipation. 
% include the moral machine paper
% with the moral machines paper add discussion about ethical dilemmas
% add some more stuff about ai literacy

% \begin{figure}[hbtp]
%     \centering
%     \includegraphics[width=0.5\columnwidth,draft]{figures/survey-flow.pdf}
%     \caption{Caption}
%     \label{fig:survey-flow}
% \end{figure}

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=\linewidth]{figures/particip-ai-p2-figure_v2.pdf}
    \caption{Five professional or personal use cases are presented in a random order. For each use case, we ask multiple-choice questions about its development and confidence levels (Q1, Q2), free-text questions on rationale and decision-switching conditions (Q3, Q4), and multiple-choice questions on usage and confidence (Q5, Q6). These are followed by questions on AI literacy and demographics.}
    \label{fig:survey-flow}
\end{figure}

%