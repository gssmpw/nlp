\section{Conclusion and Discussion}

% \maarten{Dont' forget to add a conclusion paragraph, reminding the reader what your main thesis of the paper was, what study / approach you took:
% - We examined how and why lay people judge various AI use cases as acceptable
% - We developed a survey to collect judgments and reasoning processes of 197 demographically diverse participants with varying levels of experience with AI.
% - We collected judgments of acceptability on existence with rationales (``Should / Should not be developed, because...'') and requirements to switch the decision and usage (``Would / Would not use if...'')
% - On ten different use cases (personal, professional) and varied risk levels.
% \\
% - We found significant variation in the acceptability judgments and reasoning factors of AI use cases depending on domain, risk level, as well as participants' attributes (e.g., AI literacy, gender).
% - We discuss the implications of our findings below.}
We conducted a study to understand how and why laypeople perceive various AI use cases as acceptable or not. To achieve this, we developed a survey that gathered judgments and reasoning processes from 197 participants who were demographically diverse and had varying levels of experience with AI. Participants were asked to provide their judgments on the acceptability of AI use cases, along with rationales for their decisions (e.g., "Should / Should not be developed, because...") and conditions that might change their decisions (e.g., "I would switch my decision if..."). The survey covered ten different AI use cases, spanning both personal and professional domains, and included varying levels of risk. Our findings revealed significant variation in the acceptability judgments and reasoning factors based on the domain, risk level, and participants' attributes, such as AI literacy and gender. We discuss the implications of these findings below. \looseness=-1
% \maarten{This is stylistic preference, but I like to have my Conclusions and discussions together ("Conclusion \& Discussion" section), in that order. Essentially, you conclude the paper by reminding the reader what your main thesis of the paper was, what study / approach you took. Optionally, then you can briefly (in 1 sentence) say what main results we found. Then you say something like "Below we discuss the implications of our results" and start discussing.}
% \maarten{A lot more related work should be discussed imo; we should explain to the reader how our findings corroborate previous existing works, as well as ground our suggestions for future work in other works}

% \jana{This is another stylistic thing, as well as a risk-tolerance thing.  I tend to like to offer more active discussions about possible implications and more concrete recommendations for next steps.  For example, here I would hope we could say more about what we think should be done to better learn or understand people's rules that might lead them to accept or reject AI use cases, how our approach and what we have learned could be incorporated into policy decisions and/or technical tools that help inform whether an AI use case should be pursued, and what the demographic results suggest about whose voices (what intersection of demographic variables) technology developers should make sure to hear from.  In some academic circles, this level of discussion is actually required.  In places like FACCT, though, I have seen people get away with minimal discussion.  So whether we add more or less of this kind of stuff depends on how the team feels about the possible benefits (in particular, adding this kind of extra interpretation can increase how significant and important reviewers will find the results and, in my opinion, can help the paper have a greater impact overall) vs. the possible costs (ie: takes more space and gives Reviewers more to argue about).  I will leave that decision to Maarten.  Either way, I think we should add some more about possible limitations (ex: not a huge participant sample, biased by Prolific participants who tend to be more tech-savvy and AI-tolerant, etc.) and important future directions (in particular, finding ways to classify what features people are considering in their decisions, how the weights on those features impact what kind of decision-making strategy they will use, and whether there are other ways to understand their decision strategies beyond our current classification of rule-based vs. outcome based. Could even cite some of the team's other work, like the HarmAnalyzer paper.)}

\paragraph{Use Case Perceptions and Disagreements}
In our study, we explored the varying acceptability of AI across different use cases. Generally, acceptance was lower in scenarios with higher educational requirements and greater EU AI risk levels. Professional use cases displayed more variability, notably with Elementary School Teacher AI, which was uniquely unacceptable. This underscores the necessity for further research into how AI should be developed and integrated, as well as what skills it should have, particularly in fields where empathy and care are crucial \citep{wu2024care,kawakami2024ai,borg2024required}. In addition, prior FAccT research have also highlighted how AI practitioners desire understanding lay people's perception on AI fairness in specific use cases \citep{sonboli2021fairness, deng2022exploring, smith2023scoping, deng2023understanding}. Drawing from prior HCI and AI research \citep{deng2025weaudit, lee2019webuildai, cheng2019explaining}, future FAccT researchers and practitioners should explore how to meaningfully connect lay people's use case perceptions with AI developers' workflows. \looseness=-1

While prior research has emphasized understanding AI consequences \citep{kieslich2024myfuture} and providing tools and processes to uncover impact \citep{wang2024farsight,buccinca2023aha, deng2024supporting}, our findings reveal a greater presence of rule-based reasoning in contentious use cases, suggesting a need for diverse approaches to understanding AI beyond mere consequence anticipation. Moreover, while care was generally predominant, we observed that fairness gained prominence in Lawyer AI and Government Eligibility Interviewer AI. This variability underscores the importance of considering values in AI evaluation and training \citep{barocas2021evaluation,bhardwaj2024machine}, rather than solely emphasizing functionality, which is the current trend in AI research \citep{birhane2022values}. Additionally, societal impact considerations were more evident in unacceptable use cases, emphasizing the necessity for implementing safety guardrails when deploying AI with significant social implications \citep{solaiman2023genairelease}.

% \paragraph{Disagreements}
% Moreover, in modeling disagreements through observing distributions of participant answers, we observe more bimodal distribution for professional use cases. Our results support the current controversies (e.g., Writer's Guild \citep{}) that professional use cases could bring more disagreement as they integrate into society. Delving deeper into specific use cases, we observed that within professional use cases, IT Support Specialists were more acceptable while Telemarketers were more controversial than other professional uses which require higher level of education to enter (e.g., Lawyer). These results call for future studies to explore further the perception of various domains and their impact on acceptability of AI integration.

% Disagreement b/c using different types of reasoning might inform intervensions  (e.g., farsight focuses on cb but we need more diverse tools)
% our intervention didnt work

\paragraph{Demographics and AI Literacy}
In line with prior work \cite{kingsley2024investigating, mun2024participaidemocraticsurveyingframework}, our results highlighted significant differences among demographic groups and perceived acceptance of use cases, especially for professional use (\S\ref{sec:RQ2}). Non-majority demographic groups, especially non-male gender groups, found both personal and professional use cases less acceptable. Those experiencing high discrimination chronicity also found professional use less acceptable. 
%Thus, inclusion is crucial as 
Our findings provide future FAccT research with valuable empirical insights on AI integration in contexts such as workplace, where marginalized worker's agency, earning, and occupational well-being are disproportionately affected \citep{ming2024labor,alcover2021aging}. 
% Similar concerns were expressed by some of our participants, including P35 who opposed development of Telemarketer AI because it \textit{``overlaps with my industry, and hence serves as a threat to my job security''}. 

Furthermore, our work highlighted a potential polarization on perceptions of AI among workers as those with 40+ hours employment and those who had advanced degrees were more positive towards AI use cases, suggesting that the relationship stakeholders have to AI and jobs might influence acceptability. Such a concern was expressed by one of our participants who opposed development of Telemarketer AI because it \textit{``overlaps with my industry, and hence serves as a threat to my job security''} (P35). Thus, our results corroborate the need to further explore methods to include diverse workers and various stakeholders into the discussion of workplace AI integration and development \citep{fox2020worker,cheon2023bigtechwork}. We also found that frequent AI usage increased acceptance, while understanding AI ethics and limitations decreased acceptance. This suggests that balanced AI awareness and education, encompassing usage, skills, and ethics, could guide and improve decision-making \citep{raji2021ethics}, e.g., through educational interventions targeting AI skills and ethical implication literacy (e.g., \citep{wong2021timelines, shen2021value}). \looseness=-1
% \maarten{Add AI literacy, and say that it could inform possible interventions for better policy / more consensus? Or risks of AI literacy distorting the picture?}

% power-privilege, harder to change rich / poor
% interventions for more informed decisions e.g., not just skills but limitations and ethics
% human centered studies that need to be made about stakeholders and power, including people from the sectors -- put example in discussion

\paragraph{Rationales}
Through analyzing participants' rationales, we observed an interesting pattern with higher cost-benefit reasoning use cases with least disagreement and more prevalence of rule-based reasoning for those with higher disagreement (\S\ref{sec:RQ3}). These results suggest different valuation systems employed by participants perhaps leading to different conclusions and suggests some use cases have beyond simple utilitarian implication for society, which should be explored more carefully in future studies. Building upon our empirical findings, future work could develop tools and interventions which encourage specific types of acceptability reasoning such as rule and value based \cite{sorensen2024valueKaleidoscope} or cost-benefit analyses \cite{li2024safetyanalyst}. \looseness=-1

However, as our study was limited to the two reasoning type categories, expanding this analysis would be essential for future work including finding ways to classify what features people are considering in their decisions, how the weights on those features impact what kind of decision-making strategy they will use, and whether there are other ways to understand their decision strategies beyond our current classification. Future FAccT research can build upon these further understandings to guide policy making and consensus building. For example, in addition to conducting surveys with single participant, future work can explore how group discussions and deliberations shape communities' collective understanding of AI impacts (e.g., \citep{kuo2024policycraft, lee2019webuildai,devos2022toward, gordon2022jury,zhang2023deliberating})



%\maarten{Possibly, also suggest work that studies tools and interventions which encourage specific types of acceptability reasoning such as rule and value based \cite{sorensen2024valueKaleidoscope} or cost-benefit analyses \cite{li2024safetyanalyst}.}

% implication for policy and consensus building
% - tzu sheng's work and some other stuff
% guardrails on deployment


% implication for ai designers / llm researchers, etc.
% consdieration of downstream task, will it actually work, general purpose vs specific purpose models but also evaluations, especially in more high-stakes settings (not just functionality - the idea of safeguarding, how understanding what people find acceptable and not acceptable should be able to do context of acceptability)

% ask Jana if she could draft a section about implication of our work for moral psychology and decision making

% include Somewhere importance of including lay user voices

% How should we define use cases? -- in terms of variation, regulating with regards to risk level might be more useful, but this needs to be define for each domain as the domains have highly variable acceptance, requirements, and failure cases.
% Similar to prior works, professional usages are in general less acceptable and controversial, calling for higher attention for integration, need to define and understand impacts for various stake holders. 

% \jimin{
% - moral dilemma in use cases? disagreement between develop / not develop, which are moral dilemmas? how can we know which are
% - reasoning patterns that are prevelant tools to support reasoning, especially cost to whom and benefit to whom
% - for those that are dilemmas what are the ways people want mitigation, for example, is it context dependent or trust dependent and what kinds of mitigations are possible and from whom?
% - people factors, gender divide around healthcare
% - use case factors, so something about whether education / ai risk levels are reflective and what does that mean for law-makers?
% - empathy, some roles that expect empathy}

%\wesley{Will add a paragraph on channeling impact assessment from lay people to AI developers and researchers to close the loop by the end of Mon 01/20!} \jimin{It might be good to incorporate it into discussion for demographics?}



