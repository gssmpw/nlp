\section{Introduction}
% state of the world
% \jimin{talk about GPT store? are we saying something too obvious? - merge the first two paragraph, make it more direct: lack of input from people is having a negative impact -- people are needed to make the judgments of ai we don't know how people make judgments and influence their decision making
% - condense p1 and add an example there or maybe add an example in p2}

% \wesley{Added a paragraph to reflect what discussed above. I also went over the introduction paragraph and made minor edits.}
%\maarten{
%So there's kind of three ideas that you're hitting as you're funneling the reader into the "In this work we do X" part of the intro here:
%1. It's important to understand the impacts of AI systems and AI use cases. Many of these AI use cases can have negative impacts on stakeholders.
%2. Most technologies have positive AND negative impacts at the same time, so clear cut banning or naively allowing AI uses isn't great; that creates dilemmas.
%3. Most work hasn't really involved lay stakeholders in the process of impact assessment.
%I feel like this is a little sinuous tbh, and we could more smoothly get at the core of our work with the following three paragraphs, potentially:
%1. It's crucial to involve lay stakeholders into the process of anticipating the impacts (harms, benefits) of AI uses.
%2. Since many AI uses likely have positives and negatives, which is a dilemma.
%3. It's super important to understand how people navigate these dilemmas, ie. judge the acceptability of these use cases.
%See Overleaf comment for a GPT-version of this re-structuring.
%Potentially, we could get rid of the dilemma framing if we decide that our cases cannot be considered dilemmas (I fear reviewers might find it distracting and over-dramaticized... I like the framing but it's defs a little bit PR-y to call them dilemmas)
%}

% In recent years, growing attention has been directed toward understanding and anticipating the impact of AI to adequately regulate its development and integration into society. Initiatives like the EU AI Act, the NIST AI Risk Management Framework, and the recent U.S. Executive Order \cite{} underline the importance of scrutinizing whether certain AI use cases should be pursued at all, and they emphasize the need for nuanced categorization of AI use cases for effective regulation. 
% These regulations often differ in approach, focusing on aspects such as risk levels versus functionality and capability \cite{}. Although these efforts aim to regulate AI, concerns persist regarding their adequacy in addressing the broad impacts of AI's growing adoption \citep{}. 
% Moreover, there is a clear need to incorporate perspectives from laypeople, particularly those from marginalized communities, to fully understand the diverse and disparate impacts of AI \citep{}. Thus, policymakers, technologists, and society at large must address the acceptability of specific AI use cases. Additionally, there is a need to develop a comprehensive understanding of the factors that influence these decisions. Such knowledge could foster effective frameworks that resonate with real-world impacts and accommodate demographic differences. This understanding is crucial for guiding policy choices, especially when stakeholder input is insufficient.
\maarten{
todo: rewrite:
- SOTW: AI needs to be governed (at use case level, not the model); AI policy should be democratic, include lay people; governance and its rationales should mirror how people make these judgments(?)
- Big challenge: Bringing people into AI governance will have to grapple with (a) opinions and disagreement among people about acceptability, (b) how and why people foresee impact of AI use cases, and (c) various ways that people reach judgments.
- In this work: We study how people decide whether and why an AI use case should be allowed
}

In recent years, there has been a growing call for adequately regulating AI in its development and integration into society \citep{pistilli2023stronger}. These efforts, as reflected in the EU AI act \citep{AIAct_2023}, NIST AI Risk Management framework \citep{NIST_2021}, and the recent U.S. Executive Order \citep{executiveorder2023}, have resulted in a discussion regarding underline the importance of scrutinizing whether certain AI use cases should be pursued at all. Although these efforts aim to regulate AI, concerns persist regarding their adequacy to actually address the wide and growing impact of AI \citep{dominguez2024mapping}, highlighting the need to integrate lay people, especially those from marginalized community, to understand the diverse and disparate impact of AI \citep{}. 
% Furthermore, as policy makers, technologists, and society in general tries to answer the question of acceptability, a comprehensive understanding of the factors that influence these decisions become necessary to develop effective frameworks that resonate with real-world impacts and accommodate demographic differences.

% need to understand reasoning behind dilemmas
A significant challenge is that the acceptability and impact of many AI use cases is not clear cut, as typically there are reasons both for and against developing an AI technology \citep{}. Furthermore, whether an AI technology should be developed depends not only on the harms caused by developing the technology, but also on the harms caused by \emph{not} developing the technology \citep{mun2024participaidemocraticsurveyingframework}. Thus, choosing a path forward requires resolving such conflicting reasons and incentives, especially across diverse groups of people. Therefore, it is crucial to understand opinions and disagreement among people about acceptability, how and why people foresee impact of AI use cases, and various ways that people reach judgments (e.g, weighing costs and benefits or judging based on a set of rules \citep{}).

% need to understand reasoning behind dilemmas
% In light of these needs, recent works have focused on the harms that AI create from expert perspectives \citep{} and lay user perspectives \citep{}. However, typically there are reasons both for and against developing an AI technology, as demonstrated by recent examples of mental health chatbots \citep{}. Furthermore, whether an AI technology should be developed depends not only on the harms caused by developing the technology, but also on the harms caused by \emph{not} developing the technology, and some use cases result in reasoning not only about cost and benefit of the outcome (i.e., cost-benefit reasoning) but also its inherent meaning or value (e.g., rule-based reasoning). Thus, choosing a path forward requires resolving such conflicting reasons and incentives, and how these choices are made, specifically about AI, by diverse groups of users (e.g., types of reasoning employed to make decisions) and which use cases result in disagreements or dilemmas not just within but across different users require further exploration. 

To this end, this work draws from fields such as moral psychology and human computer interaction (HCI) to explore how users with diverse backgrounds judge the acceptability of specific AI uses as well as how they reason about these use cases us to understand and potentially incorporate diverse lay people's perspectives and reasoning regarding AI use cases to extend the discussion and decisions about AI beyond AI experts \citep{}. In particular, we ask the following research questions:

\begin{itemize}
    \item \textbf{RQ1}: How and why do lay people judge AI uses as acceptable vs. not?
    \item \textbf{RQ2}: How do people reason through the acceptability of AI use cases?
\end{itemize}
To answer these questions, we develop two surveys to collect judgments and reasoning processes of 397 demographically diverse people with varying levels of experience with AI. The first study asked participants to indicate whether they thought the AI in each use case should be developed, to explain why or why not in free text, and to provide a condition that would change their opinion about whether or not the AI application should be developed in free text. Additionally, we collected judgments about whether the participant would use such an AI application. The second study asked participants for judgments both before and after a series of guided questions about the anticipated harms and benefits from the use case,  which were followed by a question about their rationale, similar to the first study.

In exploring RQ1, our work focuses on use case\footnote{We focus on text-based, non-embodied, digital systems, and while we do not specifically discuss the AI user and subject, in our use case description, we follow three of the five concepts used in EU AI Act to describe high risk use cases \citep{golpayegani2023risk}: the domain, purpose, and capabilities.} variations (Study 1), explicit reasoning of harms and benefits (Study 2), and demographic factors (Study 1) in acceptance judgments and confidence. In use case variation, we explore two categories, professional and personal, varied along dimensions of entry level education required and EU AI risk level, respectively (total ten use cases). 

To address RQ2, we collect rationales for participants' decisions and conditions under which they would change their decisions. We analyze decision-making reasoning types (e.g., cost-benefit and rule-based reasoning \citep{}), moral values \citep{}, and concerns expressed in switching conditions in participants' answers. 

\maarten{Todo: write results paragraph.}

\maarten{Todo: write discussion and implication paragraph (short).}

\jimin{integrating Jana's feedback + Wesley's rewrites above. I think we need to have less emphasis on weighing the positive and negative impacts and more focus on acceptance / reasoning part}
\begin{comment}
In recent years, there has been a growing call for integrating diverse lay people into the process of anticipating the impacts of AI systems\citep{}. Engaging lay people, especially those from marginalized community, in reasoning about the potential positive and negative impact of AI use cases towards themselves is crucial to complement the impact assessment conducted by experts such as AI researchers and practitioners\citep{}. Indeed, many prior works from the FAccT, Human-Computer Interaction (HCI), and AI communities has demonstrated that decisions to integrate AI, without the considerations of end users of the AI systems, can lead to negative impacts \citep{} such as (?) clash between stakeholders \citep{}, and general fear \citep{} overlooked by AI experts.  

One challenge for assessing AI impact, as highlighted by prior FAccT research\citep{} and regulatory proposals such as the EU AI act and the NIST AI Risk Management framework \cite{},
is that the very same AI models that positively impact some people's lives can also cause harms to others, often those from marginalized communities, creating an ``AI impact dilemma.'' 
For example, while mental health or companion chat bots can offer possible avenues of relief from loneliness and provide support for those who cannot afford care otherwise, they can have unintended adverse effects on mental health \citep{} and can lead already vulnerable targeted users to further despair \citep{}. Furthermore, now with platforms such as GPT store\footnote{\href{https://openai.com/index/introducing-the-gpt-store/}{https://openai.com/index/introducing-the-gpt-store/}} and Character.AI\footnote{\href{https://book.character.ai/character-book/training-a-character}{https://book.character.ai/character-book/training-a-character}}, not only AI experts, but lay people with limited technical backgrounds can also create customized AI, making their own development and deployment decisions. Therefore, it is critical to understand lay people's perspectives and reasoning on whether certain AI use cases \footnote{By use cases, we mean specific scenarios, applications, or problems that an AI system is designed to solve or assist within real-world contexts.} should be developed at all, and how they should be deployed.

To this end, this work draw from fields such as moral psychology and HCI to explores how lay people judge the \textbf{acceptability of specific AI uses} as well as how they reason about the \textbf{trade-offs between positive and negative impacts} towards themselves. This allows us to understand and potentially incorporate diverse lay people's perspectives and reasoning regarding the ``AI impact dilemma'' to extend the existing tools and frameworks that support only AI experts in anticipating potential impact of AI applications \citep{}. In particular, we ask the following research questions:

%Although previous works have developed tools and frameworks to support AI experts in anticipating %and categorize technological harm \citep{} and assess 
%potential negative impacts of AI applications they developed \citep{}, these efforts often lack input from diverse lay people's regarding the AI impact dilemma --- the \textbf{trade-offs between positive and negative impacts} towards AI users. Furthermore, we still lack understanding on how lay people \textbf{reason about and justify their decision} concerning AI impact under specific use cases. To this end, in this work, we explore the following research questions:

\begin{itemize}
    \item \textbf{RQ1}: How and why do lay people judge AI uses as acceptable vs. not?
    \item \textbf{RQ2}: How do people reason through the acceptability of AI use cases?
\end{itemize}

%in exploring factors that influence public perceptions \citep{brailsford2024exploring}, previous works have identified the tensions between positive and negative perceptions and impacts but overlooked how people make decisions about AI under these tensions, e.g., the way they make trade-offs and the factors that influence decision making.

%this work seek to (1) understand the different factors that influence lay people's acceptability of AI use cases and (2) analyze the different reasoning processes that underlie these judgments. In particular, 

%Thus, to provide a more comprehensive view of differences among judgments regarding AI development (e.g., should or should not be developed) and integration (e.g., would or would not use this AI system), we aim to first, understand the different factors that influence acceptability of AI use cases and second, analyze the different reasoning processes that underlie these judgments.

%\wesley{Put a note here as a reminder for coming back to this after finalizing results/study design.}

To answer these questions, we develop two surveys to collect judgments and reasoning processes of 397 demographically diverse people with varying levels of experience with AI. We collect judgments on ten different use cases, five personal use cases and five professional use cases (Figure \ref{fig:survey-flow}). We explored these two categories of use cases varied along relevant dimensions, EU AI risk level for personal use cases and required education level of entry for professional use (see \S~\ref{sec:study-design}). 
%with 800 demographically diverse participants. 

% \begin{enumerate}
%     \item [\textbf{RQ1}] What factors of the AI use case explain variation in judgments of acceptability? 
%     (e.g., personal vs. professional use, AI risk level, education level of profession) of the AI use case explain variation in judgments of acceptability?
%     \item [\textbf{RQ2}] Does a reasoning intervention such as explicit weighing of harms and benefits impact judgments of acceptability?
%    \item [\textbf{RQ3}] What attributes of people explain variation in judgments of acceptability? 
%     (e.g., AI literacy, moral values, demographics)
%     \item [\textbf{RQ4}] What factors in participants' open-text rationales of judgments explain their decisions of acceptability?
%     (e.g., decision-making reasoning type, moral values) 
%     \textit{Reasoning factors}: when people list the impacts (h/b of (not) developing) of a possible AI use case, what values extracted from their listed impacts explain their judgment of acceptability?
% \end{enumerate}

\end{comment}
%We explored two categories of use cases varied along relevant dimensions, EU AI risk level for personal use cases and required education level of entry for professional use (see \S~\ref{sec:study-design}). 
Our analysis shows general higher acceptance of personal use cases over professional and shows higher disagreement among professional use cases. Additionally, higher EU AI risk level showed correlation with lower acceptability judgment more significant (\textit{RQ1}). Interestingly, reasoning intervention showed no effect on judgment of existence use cases but showed positive marginally significant impact on usage. Surprisingly, however, it had a significant negative effect for only one use case, Customized Lifestyle Coach AI (\textit{RQ2}). \jimin{(TODO demographics) -- Wei Bin}

Furthermore, our results show varying distribution of reasoning types such as cost-benefit reasoning (i.e., weighing the outcomes) and rule-based reasoning (i.e., weighing inherent value in action) for different use cases with cost-benefit reasoning being generally more prevalent. Care and Fairness were most commonly observed moral values in participant rationales. Functionality was the most prevalent concern; however for two professional use cases (Elementary School Teacher AI and Telemarketer AI) usage was a more common concern. 
% \maarten{Should we say a sentence or two on what we say in our discussion? And we need a high-level takeaway sentence as well?}

% \jimin{I think the most interesting finding is the variation within the use cases. I think the difference between a nice pattern of personal usage where we controlled for domain and then professional shows that there are variations within domains and understanding these domains / categorizing use cases can be helpful?}

% To summarize, our contributions are ...