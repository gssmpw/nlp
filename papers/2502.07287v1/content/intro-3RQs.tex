\section{Introduction}
There are growing calls to regulate AI's development and integration into society \citep{pistilli2023stronger}. These efforts, as reflected in the EU AI Act \citep{AIAct_2023}, NIST AI Risk Management framework \citep{NIST_2021}, and recent U.S. Executive Order \citep{executiveorder2023}, have resulted in discussions about whether certain AI use cases should be pursued at all. Despite much progress in this area, it is still not clear how to determine which use cases should be pursued or more heavily regulated. Further, little is known about how lay community members, especially those from marginalized groups, feel about the development of specific AI use cases \citep{ada2023survey,suresh2024participation}. \looseness=-1

One significant challenge when evaluating the acceptability and impact of specific AI use cases\footnote{By use cases, we mean specific scenarios, applications, or problems that an AI system is designed to solve or assist within real-world contexts.} is that there can be both positive and negative effects, depending on the context\citep{mun2024participaidemocraticsurveyingframework}. For instance, while educational AI can provide affordable and accessible personal tutor, it can also lead to over-reliance of students and diminish the goal of education \cite{ChatbotTeach, zhai2024effects}. To develop a generalizable approach to making decisions about AI use cases, ideally we would understand how people resolve these conflicts. More specifically, first, it is essential to understand differences in judgments about \emph{acceptability and likely usage} across use cases, and how such judgments relate to scenario characteristics. Second, we need better understanding of the \emph{personal factors influencing these judgments}, especially as they relate to demographic differences \cite{kingsley2024investigating}. Third, we need to better understand the reasoning strategies participants use when making judgments about AI use cases, and how those strategies do or do not relate to the judgments that are ultimately made. To form governance and policy decisions that anticipate and address disagreements about the development or regulation about specific AI use cases, these understandings are crucial, especially across groups of people with diverse backgrounds, experiences, and familiarity with AI. \looseness=-1

To address these needs, in this work we examine how and why lay people judge various AI use cases as acceptable or unacceptable, asking the following research questions:
\begin{enumerate}
    \item [\textbf{RQ1}] How do judgments of acceptability vary across a set of distinct AI use cases and their characteristics?
    \item [\textbf{RQ2}] What attributes or characteristics of people explain the variation in acceptability judgments? 
    \item [\textbf{RQ3}] How do people reason through acceptability judgments of AI use cases?
\end{enumerate}
To answer these questions, we develop a survey to collect judgments and reasoning processes of 197 demographically diverse participants with varying levels of experience with AI. We ask participants to report whether a certain AI use case should be developed or not, whether they would use such a system, and ask them to provide rationales for their judgment and conditions that would cause them to change their judgments (Figure \ref{fig:survey-flow}). We examine ten different AI use cases\footnote{We focus on text-based, non-embodied, digital systems, and while we do not specifically discuss the AI user and subject, in our use case description, we follow three of the five concepts used in EU AI Act to describe high risk use cases \citep{golpayegani2023risk}: the domain, purpose, and capabilities.}. To account for differences between sectors or domains, we select use cases in two categories, professional and personal use, and vary them by required entry-level education and EU AI risk level (Table~\ref{tab:use-cases}).

To meaningfully differentiate and analyze participants' reasons and reasoning strategies, we borrow concepts from moral psychology and philosophy. We investigate participants' rationales through two distinct but sometimes overlapping reasoning patterns: cost-benefit reasoning, which assesses expected outcomes (e.g., "using AI for this task would save time"), and rule-based reasoning, which evaluates the intrinsic values of the action itself (e.g., "having humans perform this task would be inherently wrong") \citep{cushman2013action,cheung2024measuring}. We further explore the moral foundations reflected in participants' reasoning, with moral foundations theory\footnote{We used the five foundational dimensions: Care, Fairness, Loyalty, Authority, and Purity. Although these dimensions have been updated to encompass a broader range of values beyond WEIRD (White, Educated, Industrialized, Rich, and Democratic) populations \citep{atari2023morality}, we selected this version for survey brevity.} \citep{graham2011mapping,graham2008moral}. Additionally, to understand aspects of AI that raise concerns, we employ three dimensions based on prior studies \citep{solaiman2023evaluating,mun2024participaidemocraticsurveyingframework}: functionality (system capabilities like performance, bias, and privacy), usage (context of system integration, such as supervision, misuse, or unintended use), and societal impact (effects on individuals, communities, and society, such as job loss and over-reliance).

Our empirical results show general higher acceptance of personal use cases over professional. While both categories of use cases show decreased acceptance with increased entry level education and risk, professional use cases display more variability and disagreements across judgments (\textit{RQ1}). Acceptability significantly varied among demographic groups and levels of AI literacy, with lower acceptability observed particularly among non-male participants and those familiar with AI ethics (RQ2). Finally, our results show varying distribution of reasoning types across acceptability decisions with rule-based reasoning being associated with negative acceptance as well as concern for societal impact. Further qualitative analysis reveal rules such as the need for humanness in certain use cases whether it be for empathy or interaction (\textit{RQ3}). \looseness=-1

Our findings shed novel light onto the diversity of people's acceptability and reasoning of AI uses in distinct domains and risk levels. We conclude with a discussion highlighting three key implications: first, diverse methodologies are needed to effectively analyze use cases and their characteristics; second, involving diverse stakeholders is crucial for assessing the acceptability of AI applications, particularly in workplaces; and third, further investigation into human reasoning processes about AI, notably rule-based reasoning, is needed to inform consensus-building in policy making. \looseness=-1
% , highlighting the need for more diverse approaches to understanding use cases and their characteristics, the need for inclusion of diverse stakeholders in determining AI use case acceptability---especially for workplace AI uses---and the need to further explore reasoning processes used by people, especially rule-based reasoning, to provide insights into building consensus for policy making.