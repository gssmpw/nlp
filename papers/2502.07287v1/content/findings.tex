% KEY TAKEAWAYS
% RQ1. 
% 1. Some use cases are more acceptable than others
% 2. Some use cases have bigger disagreement
% - good TODO correlation between stake level and standard deviation / confidence
% 3. TODO we need something with confidence
% RQ2: Reasoning effect
% 1. Weighing harms and benefits seemingly no effect on existence in aggregate - confirming related work or not?
% 2. Near significant effect of reasoning on usage acceptability in aggregate - positive
% Check paired t-test
\section{Findings}
% \maarten{Todo: add a 1-2 sentence "intro" to this section, reminding the reader what the main paper's goal is, and the purpose of this section.}
Our work aims to uncover variations in acceptability of AI use cases and factors and reasoning processes that underlie these judgments. In this section, we discuss our findings about the judgments of the AI use cases (\S\ref{sec:RQ1}), personal factors that may influence the decision such as demographics and AI literacy (\S\ref{sec:RQ2}), and factors in rationales that could uncover reasoning processes that lead to judgments (\S\ref{sec:RQ3}).

\subsection{RQ1. Perceptions and Disagreements of Use Cases}
\label{sec:RQ1}
\subsubsection{Use Case Factors}
\label{sssec:findings-use-case-factors}
\input{figures/use-case-effect-category}
% \maarten{This subsection's content is great, but I do feel like the writing is a little complex and confusing. Can you take a pass to try and make the statements more clear to a reader that isn't as familiar with the exact measurements we took? Try writing such that the text-part is understandable by a reader who doesn't know the exact variable names? But then the variable names etc. that support the claim are in parentheses.
% E.g., instead of "Notably, personal cases had higher mean judgments (DEV: $M=0.68, SD=0.74$; USAGE: $M=0.51, SD=0.86$) than professional cases (DEV: $M=0.18, SD=0.99$; USAGE: $M=0.18, SD=0.98$)." say "Notably, personal cases were deemed more acceptable (DEV: $M=0.68, SD=0.74$; USAGE: $M=0.51, SD=0.86$) than professional use cases (DEV: $M=0.18, SD=0.99$; USAGE: $M=0.18, SD=0.98$)."
% so instead of saying "higher mean" say "more acceptable" etc.}
% \jimin{addressed above}

In our analysis, we investigated the effects of use cases on participants' judgments of two categories—professional and personal—along with ten specific use cases on judgments and confidence about their development and usage. Figure~\ref{fig:use-case-effect-category} illustrates significant differences in acceptability depending on the categories. Notably, personal use cases had higher acceptability ($M_{\texttt{DEV}}=0.68, SD_{\texttt{DEV}}=0.74$; $M_{\texttt{USAGE}}=0.51, SD_{\texttt{USAGE}}=0.86$) than professional use cases ($M_{\texttt{DEV}}=0.18, SD_{\texttt{DEV}}=0.99$; $M_{\texttt{USAGE}}=0.18, SD_{\texttt{USAGE}}=0.98$). These differences are more pronounced when judgments are weighted by confidence. Although levels of confidence did not differ significantly between categories, they were fairly high for both categories, and personal category exhibited slightly lower confidence for usage. 
% \jana{Could also mention that participants' confidence was fairly high for both types of judgments, even though there is variability}

% \input{tables/use-case-effect-post-hoc-judgment}
\input{figures/use-case-effect-jobs}
\paragraph{Professional Use Cases}
% example for talking about stats on the top: Fisher’s repeated measures one-way ANOVA revealed that, across 22 friends to taste each of the three wines, there was a statistically significant difference across persons preference for each wine. The effect size (ω_p=0.02) was medium, as per Field’s (2013) conventions.
Exploring specific use cases within the professional category, we observed that Elementary School Teacher AI ($M_{\texttt{DEV}}=-0.24, SD_{\texttt{DEV}}=0.98$; $M_{\texttt{USAGE}}=-0.14, SD_{\texttt{USAGE}}=1.00$) had the lowest acceptability for both types of judgments followed by Lawyer AI ($M_{\texttt{DEV}}=0.04, SD_{\texttt{DEV}}=1.00$) and Telemarketer AI ($M_{\texttt{USAGE}}=-0.08, SD_{\texttt{USAGE}}=1.00$). Interestingly, IT Support Specialist AI had the highest acceptability ($M_{\texttt{DEV}}=0.66, SD_{\texttt{DEV}}=0.76$; $M_{\texttt{USAGE}}=0.78, SD_{\texttt{USAGE}}=0.63$). 
% \jana{if you mean "significantly" in a statistical sense, should add results of post-hoc test}.
While all other use cases showed higher acceptability for usage over development, Telemarketer AI uniquely had higher acceptance for development over usage ($M_{\texttt{DEV}}=0.26, SD_{\texttt{DEV}}=0.97$; $M_{\texttt{USAGE}}=-0.08, SD_{\texttt{USAGE}}=1.00$). Additionally, confidence on using Telemarketer AI ($M=4.14, SD=0.93$) was significantly ($p<.05$) higher than that of Government Eligibility Interviewer AI ($M=3.79, SD=0.99$), which had the lowest confidence in usage. Near 0 mean for development of Lawyer AI ($0.04$) and usage for Telemarketer AI ($-0.08$) suggest disagreement within judgments. Moreover, Elementary School Teacher AI uniquely unacceptable across both acceptability judgments ($-0.24$, $-0.14$) underscoring a unique characteristic perhaps related to care. See Figure~\ref{fig:use-case-effect-jobs} for further details
% \jana{Note that text in Fig. 3 might be too small...can you make it bigger?}. 

\input{figures/use-case-effect-personal}
\paragraph{Personal Use Cases}
In personal use scenarios, Digital Medical Advice AI ($M_{\texttt{DEV}}=0.34, SD_{\texttt{DEV}}=0.95$; $M_{\texttt{USAGE}}=0.24, SD_{\texttt{USAGE}}=0.98$), reflecting high risk level, consistently had lower acceptance across judgment types, compared to all other use cases. However, Customized Lifestyle Coach AI had the lowest confidence across both judgments ($M_{\texttt{DEV}}=3.75, SD_{\texttt{DEV}}=0.52$; $M_{\texttt{USAGE}}=3.62, SD_{\texttt{USAGE}}=0.78$). Nutrition Optimizer ($M_{\texttt{DEV}}=0.86, SD_{\texttt{DEV}}=0.92$; $M_{\texttt{USAGE}}=0.69, SD_{\texttt{USAGE}}=0.73$) had the highest mean acceptance across the two acceptability judgments. Interestingly, unlike the professional use cases, which had slightly higher acceptance for usage, personal use cases had lower acceptance for usage in general compared to development. Notably, many personal use cases had substantially high acceptance, especially compared to professional use cases. \looseness=-1

\input{tables/use-case-effect-variations}
\subsubsection{Use Case Variations}
When selecting use cases, we used two underlying variations: entry level of education required for professional use cases and EU AI risk levels for personal use cases. As risk levels and required education increased, we observe consistent negative effects on judgments, with personal use cases ($\beta_{\texttt{DEV}} = -0.11, p<.001$; $\beta_{\texttt{USAGE}} = -0.10, p < .001$) showing stronger effects compared to professional scenarios ($\beta_{\texttt{DEV}} = -0.08, p<.01$), where only development judgments were significantly associated. Again, acceptance for personal use cases ($(intercept)_{\texttt{DEV}}=1.02, p<.001$; $(intercept)_{\texttt{USAGE}}=0.80, p<.001$) were higher than professional use cases ( $(intercept)_{\texttt{DEV}}=0.43, p<.001$) as conveyed by the intercepts. Confidence ratings remained consistently high across all conditions (intercepts > 3.97), though they showed a small but significant decrease with increasing risk levels in personal use cases ($\beta_{\texttt{DEV}} = -0.08, p<.001$; $\beta_{\texttt{USAGE}} = -0.09, p<.001$), while professional use cases showed no significant impact on confidence. Thus, while education level required for entry does show some significant effect, the EU AI risk level had consistent effects with higher significance. \looseness=-1
% \jana{well, given that we didn't test them both at the same time in the stimuli, we should be careful about making claims about which one is better or worse.} .  

\subsubsection{Disagreements}
Observing the standard deviation and visualization of the distribution shows further understanding of possible disagreements among use cases. We compare the judgments weighted by confidence to understand not only the differences in judgment but also their strength. Interestingly, the use cases with four highest disagreements in both judgments were all professional uses in order of Telemarketer ($SD_{\texttt{DEV}}=4.08$; $SD_{\texttt{USAGE}}=4.21$), Elementary School Teacher ($SD_{\texttt{DEV}}=3.99$; $SD_{\texttt{USAGE}}=4.09$), Lawyer ($SD_{\texttt{DEV}}=4.00$; $SD_{\texttt{USAGE}}=3.99$), and Government Eligibility Interviewer AI ($SD_{\texttt{DEV}}=3.96$; $SD_{\texttt{USAGE}}=3.89$). These four use cases were followed by Digital Medical Advice AI ($SD_{\texttt{DEV}}=3.80$, $SD_{\texttt{USAGE}}=3.83$). The use cases with the lowest disagreements were surprisingly Nutrition Optimizer ($SD_{\texttt{DEV}}=2.16$; $SD_{\texttt{USAGE}}=3.03$) followed by IT Support Specialist AI ($SD_{\texttt{DEV}}=3.08$; $SD_{\texttt{USAGE}}=2.65$). These results underscore the general controversy of professional usages but also shows that acceptability is highly use case dependent. \looseness=-1
% , given that the use case with highest disagreement was Telemarketer, which has the lowest entry level education required, and the lower disagreement for IT Support Specialist.

% \input{tables/reasoning-main-effects}
% \input{tables/reasoning-effects-by-use-case}

\subsection{RQ2. Impact of Personal Factors on Acceptability Judgment}
\label{sec:RQ2}
\subsubsection{Demographic Factors}
% \input{tables/demographics-factors-questionnaires-effects}
% \input{tables/demographics-factor-anova}
\input{tables/demographics-factor-regression}
% \jana{somewhere in here make sure to mention that IVs in the model didn't correlate more than X so they were all retained in the model...where X is hopefully less than .5!} 
In analyzing the demographic factors influencing judgments on the development and usage of AI use cases, several significant trends emerged from the data. First, we observed that the demographic variables had less than 0.5 correlation, except for age 65+ and Retired employment status. Across both categories of use cases, age was positively associated with confidence in acceptability judgment of usage for age 25-34 ($\beta=0.41$, $p<0.05$) and 55-64 ($\beta = 0.48$, $p<0.05$). Race also had notable influences; specifically, Asian participants exhibited significantly lower confidence in both development and usage judgments ($\beta_{\texttt{DEV}} = -0.37$, $p<0.01$; $\beta_{\texttt{USAGE}} = -0.33$, $p<0.05$), particularly in professional contexts. \looseness=-1

Gender emerged as a crucial determinant, with non-male participants consistently showing negative judgments across both development ($\beta = -0.29$, $p<0.001$) and usage ($\beta = -0.33$, $p<0.001$), indicating potential discrepancies in perception or experience with AI applications. Liberal views, especially among those identifying as strongly liberal, were associated with negative judgments across both categories of use cases ($\beta_{\texttt{DEV}} = -1.16$, $p<0.05$; $\beta_{\texttt{USAGE}} = -1.51$, $p<0.01$), suggesting a skeptical stance towards AI's prevalence and role. Employment hours also contributed, with individuals working 40+ hours per week displaying a positive association with development judgments ($\beta = 0.25$, $p<0.05$), suggesting more exposure or reliance on AI use cases. High experience of discrimination chronicity was significantly related to lower acceptance of development, $\beta = -0.36$, $p<0.05$. These findings highlight the significant role of demographic factors in shaping perceptions and attitudes toward AI technologies. See Table~\ref{tab:demographics-anova} in the Appendix for ANOVA results. \looseness=-1
% \maarten{No effect of AI literacy?}

\input{tables/ai-literacy-effect}
\subsubsection{AI Literacy}
We identified a correlation greater than 0.5 among three AI literacy aspects: awareness, usage, and evaluation. These were aggregated into a single factor, AI Skills. As shown in Table~\ref{tab:ai-literacy-effects}, understanding of AI Ethics was associated with lower acceptability for both personal ($\beta_{\texttt{DEV}}=-0.05$, $p<.001$; $\beta_{\texttt{USAGE}}=-0.23$, $p<.001$) and professional ($\beta_{\texttt{DEV}} = -0.04,p<.05$). However, across both categories of use cases, high Generative AI Usage Frequency resulted in higher acceptance for both professional ($\beta_{\texttt{DEV}}=0.14$, $p<.01$; $\beta_{\texttt{USAGE}}=0.18$, $p<.001$) and usage acceptance ($\beta_{\texttt{DEV}}=0.15$, $p<.001$; $\beta_{\texttt{USAGE}}=0.19$, $p<.001$). Notably, for personal use cases, AI Skills was positively associated with confidence of judgments ($\beta=0.06$, $p<.05$ for both development and usage) and judgment weighted by confidence ($\beta_{\texttt{USAGE}}=0.15$, $p<.05$), while Generative AI Limitation Familiarity was positively associated with confidence for professional usage ($\beta_{\texttt{DEV}}=0.20$, $p<.05$; $\beta_{\texttt{USAGE}}=0.18$, $p<.05$ for usage). These results implicate that different understandings of and experiences with AI can impact judgments of acceptability, corroborating previous findings \citep{kramer2018when}.

\subsection{RQ3. Factors in Participant Rationale}
\label{sec:RQ3}
% \jana{Since the rationale/reasoning results are a main contribution of this paper (assumig we still think that's the case), maybe rephrase this sentence a bit to better reflect that..."complement our analyses of acceptability judgments" makes it sound a bit like these analyses were kind of an afterthought or "nice to have", rather than a critical contribution.} 
To provide deeper insights to our analyses of acceptability judgments, we also examined open-text rationales (Q3) for judgments of development (Q1) and conditions for switching their decisions (Q4). 
% See Appendix~\ref{app:placeholder} for further analysis on Study 2 results. 

\input{figures/reasoning_type_by_use_cases-study1}
\subsubsection{Decision-making Types}
As we defined in \S~\ref{ssec:text-analysis}, we focus on two distinct reasoning types for decision-making, distinguished by consideration of outcome versus consideration of value inherent in action: cost-benefit reasoning (e.g., \textit{``it gives more people access to medical advice and treatment''}, P365) and rule-based reasoning (e.g., should not be developed because \textit{``human interaction is better''}, P249). 
% \jana{could we give a few more examples of each kind of reasoning?} 
As shown in Figure~\ref{fig:reasoning-types-percentage}, generally, participants used more cost-benefit reasoning with highest percentage for IT Support Specialist (91.0\%) and Nutrition Optimizer (92.8\%). This result is particularly interesting as we observed these two use cases to have the lowest disagreement (see \S~\ref{sssec:findings-use-case-factors}) and signifies that perhaps unified reasoning type leads to less disagreement. 

On the other hand, rationales for Elementary School Teacher AI contained most percentage of rule-based reasoning (30.0\%) followed by Lawyer AI (22.0\%). Interestingly, Elementary School Teacher AI and Lawyer AI were the use cases with the lowest development acceptability. These results suggest that less acceptable AI use cases might trigger more rule-based reasoning, which could, however, be due to the lack of consensus and rules around AI, especially those that are positive. this suggests the use of rule-based reasoning might correlate with judging AI use cases to be more unacceptable. These results suggest that the use of rule-based reasoning might correlate with judging AI use cases to be more unacceptable. \looseness=-1
% \jana{This is a very interesting finding.  I would suggest concluding this section with "this suggests the use of rule-based reasoning might correlate with judging AI use cases to be more unacceptable", and then offer some hypothesis or discussion about why in the Discussion.}
% \wesley{super interesting. I think we maybe can conclude with some hypothesis of why this happens?}

% \jimin{probably a good place to add some qualitative stuff}

\input{figures/moral_values_proportions-study1}
\subsubsection{Moral Foundations}
% \jana{NOTE: in this entire section, moral "values" should be replaced with Moral "Foundations".  Also, I started to replace participant numbers with words like "one participant" because I think the participant numberss add extra complexity we don't need, but that is only my personal preference...whatever you choose, just make sure to be consistent throughout.} \jimin{I think for quoting qualitative results, we need participant numbers!} 
Beyond reasoning types, we explored moral foundations to provide insights into what values are relevant for AI use case decisions (Figure ~\ref{fig:moral_values_proportions}). For example, P12 responded that Elementary School Teacher AI use case should be developed because it \textit{``could give elementary schooling to children who are bed ridden...''}, which was annotated with both values of Care (focusing on the well-being and nurturing of bed-ridden children) and Fairness (focusing on fair access to education).
% \jana{Can we give one more example here?} \jimin{Would it help to give more than one? Just want to save space haha}  
Upon analysis, we observe that Care (i.e., dislike of pain of others, feelings of empathy and compassion toward others) was the most prevalent moral value in participants' rationales across the use cases. Of note, Care could impact the acceptability of AI use cases in both directions, as conveyed by P385 who noted that Customized Lifestyle Coach AI should be developed because ``it may help improve some people's health'' but would change their decision if ``it caused harm to even one person.'' While our choice of medical domain in personal usage could have impacted the distribution, care was still the most prevalent considering only professional use cases. 

Participants' rationales evoked fairness value more than care for two use cases, Lawyer AI (0.62) and Government Eligibility Interviewer (0.74) respectively. These results could be due to the characteristics of the use cases, such as their main purpose and function, as noted by P88, Government Eligibility Interviewer should be developed because \textit{``it might be less biased and therefore more fair in it decisions (sic)''}. Authority was most apparent in participant rationales for Lawyer AI (0.37) and Purity for Flavorful Swaps (0.35). 
% \jana{can you give an example for these as well?}

\subsubsection{Switching Conditions}
\input{figures/switching-conditions-proportions-study1}
We have thus far explored firmness of the decision through levels of confidence provided by users. We further explore the flexibility of participants' judgments to understand possible mitigation of disagreements through concerns expressed in conditions under which they would switch their decisions (Figure~\ref{fig:switching_conditions_proportions}). Overall, Functionality (53.3\%; e.g., Medical Adivce AI should not be developed if it ``consistently or had a high percentage of failure to diagnose correctly.'', P373) was the most commonly noted concern that would switch participants decision. This was followed by Societal Impact (42.3\%; e.g., Lawyer AI should not be developed if ``it puts too many human lawyers out of work'', P97), Usage (31.8\%; e.g., Government Eligibility Interviewer should be developed if ``it was only used to read and screen applications but not for making decisions''), Not Applicable (0.02\%; e.g., will not change decision). 
% \jana{can we give an example for each?}  \jana{One more comment: I haven't had a chance to go through the entire Appendix, but after a quick skim, it seems like the classification rates for these labels weren't as high as we might like (but I don't know how acceptable different levels are for NLP folks).  Should we mention that as a caveat here?}

Interestingly, Societal Impact (50.6\%) was more prevalent followed by Functionality (46.4\%) for professional use cases whereas Functionality (54.4\%) was the most frequently mentioned concern for personal use cases. One of the reasons for frequent mention of Societal Impact in professional use cases could be due to concerns of labor replacement for professional use cases: as described by P296, if Elementary School Teacher AI \textit{``was to replace teachers with the ai to save money''}, they would switch decision from should be developed to should not be developed. Moreover, we observe that Societal Impact was the most prevalent concern for those (31\%) who thought the use case was unacceptable across all use cases and was especially prevalent for Elementary School Teacher AI (0.47). These diverging results show the importance of understanding granular concerns by use cases to effectively address the relevant issues. Participants' responses that were identified as being both rule-based and concerned about societal impact expressed rules such as necessity of humanness as expressed by P22, ``a human touch is 100\% necessary'' when discussing Elementary School AI, whether it be due to belief that ``AI would lack empathy'' (P5) or that ``humans need human interactions'' (P44).
% \jana{That said, it does look like social impact is a major reason given for rejecting the majority of use cases while functionality is a major reason for accepting the majority of use cases...is that worth mentioning?}

% \input{tables/reasoning-factors-judgment}
\input{tables/reasoning-factors-judgment-regression}
\subsubsection{Rationale Factors and Judgment}
The results of analyzing the influence of rationale factors on judgments of acceptability indicated several significant effects (Table~\ref{tab:reasoning-factors-judgment-regression}). We first verified that the factors had less than 0.5 correlation with each other. For acceptability of development, use of Cost-benefit reasoning in rationale was associated with increased acceptability ($\beta_{\texttt{DEV}} = 0.33, p<.01$), while Rule-based reasoning was negatively associated with acceptability ($\beta_{\texttt{DEV}} = -0.47, p<.001$). Interestingly, for usage, Cost-benefit reasoning was negatively associated with acceptability ($\beta_{\texttt{USAGE}}=-0.10,p<.05$) but positively for judgment weighted by confidence ($\beta_{\texttt{USAGE}}=2.07,p<.001$). Rule-based reasoning indicated lower usage acceptability for judgment weighted by confidence, consistent with acceptability of development. In the moral value category, Fairness in rationales had positive associations with acceptance ($\beta_{\texttt{DEV}} = 0.47, p<.01$, $\beta_{\texttt{USAGE}} = 0.83, p<.001$; judgment weighted by confidence). 

Analyzing switching conditions showed that participants who answered negatively to the development of use cases mentioned the concerns about Functionality ($\beta_{\texttt{DEV}} = -0.08, \, p<.05$) and Societal Impact to switch their decisions ($\beta_{\texttt{DEV}} = -0.11, \, p<.05$). However, those who were positive towards development of use case indicated emphasis on Usage ($\beta_{\texttt{DEV}} = 0.12, \, p<.001$) as a condition to reverse their decisions. Thus, our findings highlight diverse perspectives on requirements and concerns of AI use cases, especially with varying perception of acceptability. 
% \jana{Can we say anything about whether when people are using cost-benefit analysis and reject the development, they are doing so because they see a greater potential for negative societal impact than those who are also using cost-benefit analysis, but accept the development?  Or another possible angle: does focusing on social impact make it more likely you will use rule-based reasoning vs. cost-benefit reasoning?  The answers to these questions could have really important implications...}