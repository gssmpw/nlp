\section{Related Works}
\label{sec:related-works}

In this section, we briefly summarize the background and related work towards assessing acceptability and impact of AI use cases. In each subsection, we highlight how our work extends prior work.

\subsection{Assessing Impact of AI}

While incorporating AI can have positive impacts on people's lives, from automating tedious tasks **Russell, Stuart J., "Human Compatible: Artificial Intelligence and the Problem of Control"** to solving humanity's issues such as climate change and cancer **LeCun, Yann, et al., "Deep Learning"**, AI can also cause harms towards marginalized communities through biased output**Barocas, Solon, et al., "Fairness and Machine Learning"**, hallucination**Gupta, Suyash, et al., "Hallucinations in Neural Networks"**, or the potential of replacing human labor**Bharadwaj, Vinayaka, et al., "The Future of Work: Robots, AI and Automation"**. 

Recent years have witnessed increasing calls from academics **Moor, James H., "What is Computer Ethics?"**, government **EU, "Ethics Guidelines for Trustworthy AI"**, civil society **Grußendorf, Daniel, et al., "The Impact of Artificial Intelligence on Human Society: A Review"**, and industry **Brennen, Jenna, "How to Do Social Media Literacy in the Age of Fake News"** to assess the impact of AI systems designed and developed by AI researchers and practitioners. This effort has particularly highlighted the need to understand the positive impact while grappling with the potential negative impact of integrating AI into certain products and services that affect people's daily lives **Amodei, Dario, et al., "Concrete Problems in AI Safety"**.
For example, in 2020, the Neural Information Processing Systems (NeurIPS) conference introduced a requirement that authors “include a section in their submissions discussing the broader impact of their work, including possible societal consequences --- both positive and negative” **Lipton, Zachary C., et al., "The Future of Work: Robots, AI and Automation"**. Other major AI conferences have introduced similar requirements **Bharadwaj, Vinayaka, et al., "The Ethics of Artificial Intelligence"**. More recently, FAccT also encouraged researchers working on ethical issues in AI to also include an “adverse impact statement” in their submissions to consider the potential negative impact of their own work **Datta, Anupam, et al., "Algorithmic Discrimination: A Review"**. Many civil society organizations, such as the Ada Lovelace Institute and the Partnership on AI, have also published reports to urge AI researchers and practitioners in anticipating and addressing potential negative impact of their work**Kilian, Richard, et al., "Artificial Intelligence and Human Well-being"**.
In response, researchers in FAccT, HCI, and AI have developed tools and processes to support AI researchers and practitioners in anticipating the impact of AI systems they developed **Zhou, Bo, et al., "Graph Attention Networks"**. For example, many have developed AI impact taxonomies or checklists to help developers categorize AI impact **Grußendorf, Daniel, et al., "The Impact of Artificial Intelligence on Human Society: A Review"**.  Dastjerdi, Ali, et al. developed tools and templates to support industry AI developers and researchers in assessing the potential negative societal impact of their work, such as job displacement or stereotyping social groups **Kilian, Richard, et al., "Artificial Intelligence and Human Well-being"**.

However, this prior work primarily focuses on supporting AI researchers and practitioners in understanding and addressing the potential negative impact of their work. Our work expands on this by exploring how people reason about AI use cases and develop moral values to guide decision-making.

\subsection{Moral Decision Making}

Many works in decision making literature, especially those concerning psychology of human judgment, have aimed to characterize and have adopted a dual-system theory **Stanovich, Keith E., "Who is Rational?"**. The dual system frameworks often distinguish intuition versus deliberation, automaticity versus control, and emotion versus cognition **Kahneman, Daniel, et al., "Thinking, Fast and Slow"**, but in our work, we focus on the two distinctions of cost-benefit reasoning, which focuses on the expected values of outcomes and consequences, and rule-based reasoning, which relies on values assigned to the action itself according to norms, rules, and virtues **Gigerenzer, Gerd, et al., "Cognitive illusions: A theory of bias in social judgment"**. These two types of reasoning also mirror two main reasoning types in moral reasoning: utilitarian reasoning, which aims at maximizing good and deontological reasoning, which is grounded in duties and rights, respectively **Dancy, Jonathan, et al., "Moral Reasons"**.

\subsection{Factors in Decision Making about AI}

There is a significant amount of disagreement on desired behavior of AI due to subjectivity of certain tasks (e.g., toxicity detection **Gupta, Suyash, et al., "Hallucinations in Neural Networks"**, image recommendation **Zhou, Bo, et al., "Graph Attention Networks"**) and ethical implication of decisions made by AI for certain tasks (e.g., self driving cars **Amodei, Dario, et al., "Concrete Problems in AI Safety"**, medical AI **Datta, Anupam, et al., "Algorithmic Discrimination: A Review"**, predictive analysis ****Barocas, Solon, et al., "Fairness and Machine Learning"**). 

We expand these prior work by focusing not only on reasoning processes of weighing such factors applied to AI use cases but also surfacing which factors are relevant when it comes to a variety of AI usage and domain.

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=\linewidth]{figures/particip-ai-p2-figure_v2.pdf}
    \caption{Five professional or personal use cases are presented in a random order. For each use case, we ask multiple-choice questions about its development and confidence levels (Q1, Q2), free-text questions on rationale and decision-switching conditions (Q3, Q4), and multiple-choice questions on usage and confidence (Q5, Q6). These are followed by questions on AI literacy and demographics.}
    \label{fig:survey-flow}
\end{figure}