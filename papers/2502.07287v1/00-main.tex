%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript]{acmart}
\input{utils}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

\sloppy
\setcopyright{none}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

%\setcopyright{acmlicensed}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  %conference title from your rights confirmation emai}{June 03--05,
  %2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


% \usepackage{draftwatermark}
% \SetWatermarkText{Under Submission}
% \SetWatermarkScale{.25} % scale of the watermark
% \SetWatermarkAngle{45} % angle of the watermark
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Diverse Perspectives on AI: Examining People's Acceptability and Reasoning of Possible AI Use Cases}

% Navigating AI Impact Dilemma: Exmaining People's Accepstability and Reasoning of Possible AI Use Cases.



%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jimin Mun}
\email{jmun@andrew.cmu.edu}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{Pennsylvania}
  \country{USA}
}

\author{Wei Bin Au Yeong}
\email{wauyeong@andrew.cmu.edu}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{Pennsylvania}
  \country{USA}
}

\author{Wesley Hanwen Deng}
\email{hanwend@andrew.cmu.edu}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{Pennsylvania}
  \country{USA}
}

\author{Jana Schaich Borg}
\email{js524@duke.edu}
\affiliation{%
  \institution{Duke University}
  \city{Durham}
  \state{North Carolina}
  \country{USA}
}

\author{Maarten Sap}
\email{msap2@andrew.cmu.edu}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \state{Pennsylvania}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mun et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% \begin{abstract}
% \end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{comment}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

    
\end{comment}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
%\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  %Your, Paper \maarten{don't forget to update this}}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\input{content/abstract}
\maketitle

\input{content/intro-3RQs}

\section{Related Works}
\label{sec:related-works}
%\jimin{related works section is a bit long right now, shorten}

%\wesley{Working on rewriting the RW}
% New Related Work:
%\maarten{Todo: write a 1-2 sentence ``intro'' to the related work section. Something like "we briefly summarize the background and related work towards assessing acceptability, perceptions, and impact of AI use cases."}

In this section, we briefly summarize the background and related work towards assessing acceptability and impact of AI use cases. In each subsection, we highlight how our work extends prior work.

\subsection{Assessing Impact of AI}

%While incorporating AI can have positive impacts on people's lives, from automating tedious tasks \citep{} to solving humanity's issues such as climate change and cancer \citep{}, AI can also cause harms towards marginalized comminutes through biased output\citep{}, hallucination\citep{}, or the potential of replacing human labor\citep{}. 

Recent years have witnessed increasing calls from academics \cite{kieslich2023anticipating, hecht2021s, bernstein2021ethics, Neurips2020workshop, Neurips2020blog, CVPR2023EthicsGuidelines, ACL2023ethicspolicy, ICML2023EthicsGuidelines, olteanu2023responsible, ESR_Stanford}, government \cite{AIA_Adalove, NAIRRTF2023FinalReport, NAIRRTF2023Strengthening}, civil society \cite{PAI2021managing, ada2022looking, AIA_Adalove, AIML_Data_Society, metcalf2021algorithmic, reisman2018algorithmic}, and industry \cite{RAIIAguide_MSFT, RAIIAtemplate_MSFT, googleRAI, openAI_research, hecht2021s, deng2024supporting} to assess the impact of AI systems designed and developed by AI researchers and practitioners. This effort has particularly highlighted the need to understand the \textit{positive} impact while grappling with the potential \textit{negative} impact of integrating AI into certain products and services that affect people's daily lives \cite{Neurips2020workshop, Neurips2020blog, hecht2021s}. 
%For example, in 2020, the Neural Information Processing Systems (NeurIPS) conference introduced a requirement that authors ``include a section in their submissions discussing the broader impact of their work, including possible societal consequences --- both positive and negative'' \cite{Neurips2020blog}. Other major AI conferences have introduced similar requirements \cite{ACL2023ethicspolicy, CVPR2023EthicsGuidelines, ICML2023EthicsGuidelines}. More recently, FAccT also encouraged researchers working on ethical issues in AI to also include an ``adverse impact statement'' in their submissions to consider the potential negative impact of their own work \cite{olteanu2023responsible}. Many civil society organizations, such as the Ada Lovelace Institute and the Partnership on AI, have also published reports to urge AI researchers and practitioners in anticipating and addressing potential negative impact of their work\citep{}.
%\maarten{this is missing the laundry list of taxonomies that many companies and experts have been creating (e.g., Laura Weidinger, etc.); we need to at least cite those, and articulate the issues with those (expert only, perhaps too focused on the general tech instead of specific use cases)}
In response, researchers in FAccT, HCI, and AI have developed tools and processes to support AI researchers and practitioners in anticipating the impact of AI systems they developed\citep{wang2024farsight, kieslich2023anticipating, buccinca2023aha, deng2024supporting, weidinger2022taxonomy}. For example, many have developed AI impact taxonomies or checklists to help developers categorize AI impact\citep{weidinger2022taxonomy, RAIIAtemplate_MSFT}. \citeauthor{wang2024farsight} and \citeauthor{deng2024supporting} developed tools and templates to support industry AI developers and researchers in assessing the potential negative societal impact of their work, such as job displacement or stereotyping social groups. 

However, this prior work primarily focuses on supporting \textit{AI experts} rather than \textit{diverse lay people}'s impact assessments of potential AI use cases. Our work extends these prior efforts by understanding diverse (and sometimes conflicting) perspectives on both positive and negative impact of AI use cases from lay people, as a crucial step to complement the AI impact assessments conducted by AI researchers and practitioners. \looseness=-1


\subsection{Understanding People's Perceptions of AI Use Cases}
\label{ssec:understanding-}
%\maarten{need to cite our own ParticipAI lol}
Responding to the calls on meaningfully engaging lay people in assessing the impact of specific AI use cases, prior work have started to understand lay people's perceptions of AI use case \cite{buccinca2023aha, kieslich2023anticipating, mun2024participaidemocraticsurveyingframework, kingsley2024investigating}. Among other findings, this prior work revealed a substantial amount of disagreement regarding the desired behavior of AI, primarily due to the subjectivity inherent in certain tasks (e.g., toxicity detection \citep{sap2019risk, blodgett2020language}, image captioning \citep{zhao2021understanding}) and ambiguous ethical implication of decisions made by AI for certain tasks (e.g., self driving cars \citep{awad2018moral}, medical AI \citep{chen2023algorithmic}, predictive analysis \citep{barocas2016big}). Work done by 
\citeauthor{mun2024participaidemocraticsurveyingframework} highlighted that lay people can envision diverse set of harms specific to different AI use cases, complementary to those defined by experts. Another line of work also begins to examine how factors such as demographic backgrounds and previous exposure to discrimination can affect people's sensitivity towards potential AI harms \citep{kingsley2024investigating}. \looseness=-1
%For example, through an online experiment on Prolific, Kingsley et al. surfaced that participants from marginalized gender or sexual orientation groups were more sensitive towards the potential harmful impact \cite{}. 

Our work extends this prior work by examined the \textbf{detailed reasoning processes} of lay people regarding the acceptability and the \textbf{trade-offs between positive and negative impacts} of AI use cases. In particular, we draw on model decision theory framework, such as the moral foundations developed by \citeauthor{graham2008moral}, to design a survey flow (See Figure \ref{fig:survey-flow} to solicit lay people's decision-making processes (and potential moral conflicts) when assessing both the benefits and harms of concrete AI use cases. 

% \maarten{I do think it might be good to have a small section on moral decision making, giving some background on CBR vs. RBR etc.? Unless we discuss it in the methods?}

\subsection{Background: Moral Decision Making}
\label{ssec:moral-decision-making}
% \maarten{I like this section, but I would substantially condense it (max 1 paragraph), and hit the following key points:
% - Explain in 1 sentence why we have background on morality: acceptability judgments are related to moral decision making, perhaps moreso that economic decision making.
% - Many moral psych and decision making experts have studied how people navigate judgments. 
% - In our work, we draw from two main themes that emerged from this research: add 1-3 sentences explaining CBR (more like consequentialism) and RBR (more like deontology) as well as moral foundations
% }
% \jana{I like Maarten's suggestion.  Two additions: (1) I wouldn't say anything about moral vs. economic decision-making, because that's a very controversial topic. (2) I think you can get rid of most of the second paragraph.  What you need to add, though, is a sentence or two about why we choose to use CBR, RBR, and moral foundations as our main frameworks for parsing moral reasoning and judgments (as opposed to other possible frameworks or moral theories).}
Morality, characterized by diverse values across cultures and social groups, aims to suppress selfishness to facilitate social life \citep{kesebir2010morality}. To understand decision-making in AI use cases, we draw on moral psychology and dual system theory. We examine two decision-making systems: cost-benefit reasoning, which assesses outcomes and consequences, and rule-based reasoning, focusing on norms, rules, and virtues \citep{cushman2013action,cheung2024measuring}. These correspond to utilitarian reasoning (maximizing good) and deontological reasoning (duties and rights), respectively. Additionally, we apply moral foundations theory \citep{graham2008moral} to identify values and potential moral conflicts in AI development.

% our interest in more moral systems and introduction to moral values
% Unlike economic decision making which maximizes utility when presented with uncertainty in choices \citep{Simon1966}, moral decision making relies on moral systems to ground reasoning. Morality, while diverse in its definition and represented by diverse values that vary across culture or even within society by class or politics (i.e., moral pluralism), has been characterized as a system with its aim to suppress selfishness to make social life possible \citep{kesebir2010morality}. Thus, to understand how people make decisions about AI use cases, we adopt moral values developed by \citeauthor{graham2008moral}, which could illuminate the relevant values when considering development of an AI use case as a moral decision and identify possible moral conflicts. 

\begin{comment}
% --------------- Old Related Work:

\subsection{Assessing Impact of AI}
%\jimin{focus more on lay people's assessment of impact}
%\wesley{Agreed that we should highlight lay people’s AI impact assessment as the key motivation for our study! But I do think it also makes sense to include a brief paragraph mentioning the existing calls for AI impact assessments by researchers and experts (which still lack best practices), before transitioning to the need for engaging more diverse lay people in assessing AI impact.

%I can help writing this paragraph if you think this makes sense at a high level! It will be a shorter version of the "Background" section in one of my previous work on AI impact assessment: https://arxiv.org/pdf/2408.01057}
%\maarten{Yes, esp. if we adopt your new framing which starts with "There have been many calls for better impact assessments of technologies"! Let's do that!}
%\wesley{Will finish reworking this section by 01/17}

The growing usage and adoption of AI and experiences of unintended consequences and harms \citep{roose2024canai} has spurred extensive discussions about possible impact of AI. The scale of negative impacts in both currently present and anticipated harms vary widely from bias \citep{}, hallucinations \citep{}, and representational harms \citep{chien2024beyond} to existential risks \citep{bengio2023ai}, and positive impacts extend from automating tedious tasks \citep{} to solving humanity's issues such as climate change and cancer \citep{}. Many taxonomies have been created to guide and understand the risks of AI \citep{} and various tools have been developed to integrate these taxonomies into practice, from tools for developers \citep{} to forums to report and aggregate harms from AI usage \citep{}. 
% economic impact
% un-interpretable nature of ai has also made the discussion of AI impact more difficult as we do not understand and fully control this system
This uses scenario writing to understand the desirable and undesirable behaviors of AI chat bots \citep{kieslich2024myfuture}. 

AI as cultural technology \citep{lederman2024language}
% eu ai act and stuff? should we add that here?

\wesley{Given that we aim to streamline the related work, I feel like this related work section on moral decision making can be integrated as the first paragraph of 3.1.1 survey design. This can also help the reader to better connect the survey design with our theoretical background. If we think this would be a good idea, I can try moving things around!}

\subsection{Moral Decision Making}
\label{ssec:moral-decision-making}
Many works in decision making literature, especially those concerning psychology of human judgment, have aimed to characterize and have adopted a dual-system theory. The dual system frameworks often distinguish intuition versus deliberation, automaticity versus control, and emotion versus cognition \citep{cushman2013action}, but in our work, we focus on the two distinctions of cost-benefit reasoning, which focuses on the expected values of outcomes and consequences, and rule-based reasoning, which relies on values assigned to the action itself according to norms, rules, and virtues \citep{cushman2013action,cheung2024measuring}. These two types of reasoning also mirror two main reasoning types in moral reasoning: utilitarian reasoning, which aims at maximizing good and deontological reasoning, which is grounded in duties and rights, respectively. 

% our interest in more moral systems and introduction to moral values
Unlike economic decision making which maximizes utility when presented with uncertainty in choices \citep{Simon1966}, moral decision making relies on moral systems to ground reasoning. Morality, while diverse in its definition and represented by diverse values that vary across culture or even within society by class or politics (i.e., moral pluralism), has been characterized as a system with its aim to suppress selfishness to make social life possible \citep{kesebir2010morality}. Thus, to understand how people make decisions about AI use cases, we adopt moral values developed by \citeauthor{graham2008moral}, which could illuminate the relevant values when considering development of an AI use case as a moral decision and identify possible moral conflicts. 
% talk a little about moral dilemmas?

% how this all ties into our work
% - dual system: values in consequences vs action
% - moral value, used when deciding value of action or outcome
% - understanding how reasoning is done and what values are prevalent when considering either the action and outcome can help guide farther discussions about what dimensions matter when making decisions about ai and how people would be able to agree upon a system

\subsection{Factors in Decision Making about AI}
% moral decision making in AI behavior - introduce but highlight that these works did not address use cases
% those that considered use cases were mainly about perceptions of the use cases and not about the reasoning process
There is a significant amount of disagreement on desired behavior of AI due to subjectivity of certain tasks (e.g., toxicity detection \citep{}, image recommendation \citep{}) and ethical implication of decisions made by AI for certain tasks (e.g., self driving cars \citep{}, medical AI \citep{}, predictive analysis \citep{}). 
%Since AI systems rely on human annotated data for training, model behaviors are also heavily influenced by them \citep{}. Many works, thus, 
Prior work have explored the disagreements and decision making factors in data annotation for subjective tasks such as impact of moral and cultural values in annotating offensiveness of AI's output \citep{davani2024disentangling}. More direct assessment of AI's behavior has also been studied and factors that influence its acceptability such as demographic factors \citep{kingsley2024investigating} and moral and cultural values \citep{brailsford2024exploring}. Notably, 
%focusing on decision making in morally challenging scenarios where distribution of well-being and harms are also decided upon by AI, 
\citeauthor{awad2018moral} have also studied acceptability of machine behavior varying descriptive factors, surfacing that .... 

We expand these prior work by focusing not only on reasoning processes of weighing such factors applied to AI use cases but also surfacing which factors are relevant when it comes to a variety of AI usage and domain.

\end{comment}

%As application area of AI has widened and models more generalized (e.g., foundation models), there has been increasing interest to understand decisions regarding use cases. While use case level decision making require more extensive consideration, \citeauthor{mun2024participaidemocraticsurveyingframework} explored demographic factors and AI literacy in acceptability of AI use cases and \citeauthor{kieslich2024myfuture} performed exploratory analysis on demographic factors and AI attitude on impact anticipation. 
% include the moral machine paper
% with the moral machines paper add discussion about ethical dilemmas
% add some more stuff about ai literacy

% \begin{figure}[hbtp]
%     \centering
%     \includegraphics[width=0.5\columnwidth,draft]{figures/survey-flow.pdf}
%     \caption{Caption}
%     \label{fig:survey-flow}
% \end{figure}

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=\linewidth]{figures/particip-ai-p2-figure_v2.pdf}
    \caption{Five professional or personal use cases are presented in a random order. For each use case, we ask multiple-choice questions about its development and confidence levels (Q1, Q2), free-text questions on rationale and decision-switching conditions (Q3, Q4), and multiple-choice questions on usage and confidence (Q5, Q6). These are followed by questions on AI literacy and demographics.}
    \label{fig:survey-flow}
\end{figure}

% \section{Study Design and Data Collection}
% \label{sec:study-design}
% To understand the decision making factors and reasoning patterns of diverse population regarding AI use cases, we crafted two studies using ten different use cases. In this section, we discuss survey designs for the two studies (\S~\ref{ssec:survey-design}) and data collection details and participant demographics (\S~\ref{ssec:data-and-demographics}).
% %\maarten{Somewhere, maybe here (or maybe in 3.2?) we should define what we mean by a use case? I liked the way that the farsight paper defined it, maybe we can cite their paper? I'm thinking just a sentence would suffice?} \wesley{+1, I think we can even add a footnote when it first appears.}

% \maarten{The current flow is a little broken-up imo; we first give an overview of the studies, then explain the use cases, then give a detailed breakdown of studies. Why not just give a shorter overview of the study here (1 maybe 2 sentences per study), and then making 3.1 Use cases, and 3.2 Study Design, and 3.3 Data collection?}


% \subsection{Survey Design}
% \label{ssec:survey-design}
% \subsubsection{Overview}
% To understand how people make judgments about AI use cases, we designed two studies that collect acceptability and usage judgments, which were administered to two separate groups of participants. In both studies, we ask participants to make judgments on 1) whether the use case should exist or not and 2) whether the participant would use the application if it existed. However, the two studies differ in their elicitation of the participant's reasoning process. 

% \paragraph{Study 1: Perception of Five AI Use Cases}
% To understand the unprompted, immediate reasoning process of participants, the first study collects use case judgments without further guidance by showing participants five different use case descriptions. After reading a brief description of the use case, the participants are asked to choose whether the application should be developed or not. The reasoning process is collected through an open text question that asks participants to 1) elaborate on their decisions and 2) provide a condition in which they would switch the decision. Thus, the first question asks about the main reasons for their decision and the second question elicits the primary concern when considering the counterfactual decision. 
% % We repeat the same sets for 5 different use cases for each participant to account for the variability in the individuals and to assess the use case factors in judgment within subjects as well as between subjects (RQ4; Q\#).  

% \paragraph{Study 2: Guided Weighing of a Single Use Case}
% To understand the impact to the decision making process of participants when specifically asked to weigh the possible harms and benefits of a use case (\reasoningeffect), the second study utilizes the framework developed by \citet{mun2024participaidemocraticsurveyingframework}, which contains a comprehensive set of questions to ask participants about positive and negative impacts of use case development as well as the impacts of not developing the application. In our survey, we show participants a single, pre-determined use case drawn from the same set of use cases from the first study. While we collect judgments both before and after the explicit harms and benefits consideration, we ask participants to elaborate on their decisions and to write the conditions for switching their decisions once after the questions about harms and benefits.

% \subsubsection{Use Cases}
% \maarten{This paragraph is a little circuitous and long? I would be more to-the-point; Just say we focus on two distinct categories of use cases: personal health and labor replacement. We choose these two domains because [justify why we chose these two individually]. Together, [justify why these two domains can give us insights because they are distinct...].}
% To consider diverse impacts and usage of AI, we first carefully crafted ten use cases. To understand how different characteristics of an AI use case can impact judgments and decision making processes, we first chose two broad areas of AI involvement: AI in personal, everyday usage and AI in labor replacement, which were chosen with aims to understand how different areas of impact (e.g., in private life and society) and the roles of the participants in their interaction with the AI (e.g., AI user and AI subject or indirect stakeholder) impact the decision process. Both personal, everyday use cases and labor replacement use cases were prevalent characteristics of AI use cases discussed by the public \citep{kieslich2024myfuture,mun2024participaidemocraticsurveyingframework}. Additionally, these two areas of focus were chosen due to the familiarity of their functionality (e.g., AI lawyer) compared to other use cases (e.g., AI for climate change). We then crafted five use cases per category by varying EU AI risk level for the personal use cases and required entry level education for the labor replacement use cases. While we explore multiple domains for labor replacement use cases, we chose to focus on health domain for personal use cases reflecting the public interest \citep{mun2024participaidemocraticsurveyingframework,kieslich2024myfuture}. 
% % In our description, we focus on text-based, non-embodied, digital systems, and while we do not specifically discuss the AI user and subject, in our use case description, we follow three of the five concepts used in EU AI Act to describe high risk use cases \citep{golpayegani2023risk}: the domain, purpose, and capabilities.
% % discuss and define AI subject vs AI user

% \input{tables/use-cases}
% \paragraph{Professional Use Case Scenarios} 
% For the first area of focus, AI in labor replacement, we collect jobs listed in the U.S. census bureau\footnote{} and sort them according to entry level education required as stated in the census. Education level has been tightly linked to socioeconomic status \citep{} and occupational status \citep{}, which signals the level of expertise and trust \citep{svensson2006professional,evetts2006introduction}. We select jobs that have a large portion of digital or intellectual components with minimal requirement for embodiment. We select the following five labor roles: Lawyer, Elementary school teacher, IT support specialist, Government support eligibility interviewer, and Telemarketer. See Table~\ref{tab:use-cases} for further details.

% \paragraph{Personal Use Case Scenarios} 
% To comprehensively understand the acceptability of different applications in personal and private life, we varied the risk levels in the use cases following the EU AI Act to high risk, high / limited, limited, limited / low, and low risk. Furthermore, to confirm that the AI risk levels were reflected in the description, we ensured that the categories assigned by GPT-4 following \citeauthor{herdel2024exploregen} agreed with the research team's assignment. For these use cases, we adapted the descriptions of the systems written by participants from prior works \citep{mun2024participaidemocraticsurveyingframework,kieslich2024myfuture}. See Table~\ref{tab:use-cases} for further details.

% \subsubsection{Survey Questions}
% \label{sssec:survey-qs}
% % some things might be better to say in the overview than here?
% \maarten{I'm wondering if we shouldn't just give details for study 1 first in its own paragraph; and then for study 2, explain what you asked, but highlight the diff. Also, overview figures (I'm imagining very wide but narrow) would be very useful here.}
% In both surveys, we ask participants to read one or more descriptions of AI use cases and to make two judgments: 1) ``Do you think a technology like this should exist?'' (Q1) and 2) ``If the <\textit{use case}> exists, would you use its services?'' (Q5). A question to indicate their level of confidence is asked following each question (Q2, Q6). As discussed above, the two surveys differ in eliciting reasoning. The participants are asked to both elaborate on their decisions (Q3) and specify the conditions under which they would switch their decisions (Q4). While these same set of questions are asked for all five use cases for Study 1, in Study 2, we further ask participants to weigh the harms and benefits of the use case in the context of both developing and not developing it. We then again ask participants the same set of judgment questions (Q1-2, Q5-6) along with the same open-text questions to elaborate on their reasoning (Q3, Q4).

% \paragraph{Harms and Benefits (Study 2)} 
% To gather explicit weighing of harms and benefits of a use case, we ask participants to write in free-text the positive and negative impacts, the groups that would be harmed or benefited the most, and the scale of such impacts. We repeat this process for both scenarios of developing and not developing the use case. To minimize the ordering effect, we randomize the order in which participants answer questions about the scenarios, developing and not developing, harms and benefits, and the types of harms of developing (functionality error or misuse).

% \paragraph{AI Literacy and Demographics}
% \maarten{In the content below, you should say explicitly "we use the scale by... " or something like that, for AI literacy as well as experience being discriminated against}
% Following the main survey, we asked participants questions about their AI literacy and demographics, to explore both the demographic factors and AI literacy that affect decision making of use cases (\demofactor). In the AI literacy section of the survey, we asked participants to indicate their familiarity with AI in awareness, usage, evaluation, and ethics as well as frequency of AI usage and knowledge of their shortcomings \citep{wang2023measuring,mun2024participaidemocraticsurveyingframework}. In addition to the demographic information of the participants such as race, sex, age, employment status, income, and level of education, we also collected information about their discrimination experiences \citep{kingsley2024investigating}. 

% \paragraph{Questionnaires}
% To better understand the decision making styles of participants that could inform AI use case decision making (\reasoningfactor), we adopted three questionnaires in our survey: Moral foundations questionnaire \citep{graham2008moral}, Oxford Utilitarianism Scale \citep{kahane2018beyond}, and Toronto empathy questionnaire \citep{spreng2009toronto}. These questionnaires were included at the end of the survey so as not to influence the decisions of the participants in the main portion of the survey. 

% \subsection{Data Collection and Participant Demographics}
% \label{ssec:data-and-demographics}
% \maarten{Say something about pay and IRB approval. Also simplified race sounds weird, maybe say that it's a Prolific category?} 
% \jimin{addressed}
% We used Prolific\footnote{} to recruit participants. To represent diverse sample, we stratified our recruitment by simplified Prolific ethnicity categories (White, Mixed, Asian, Black and Other) and age (18-48, 49-100). We also added criteria for quality such as survey approval rating, previous number of surveys, etc. Our study was approved by IRB, and we paid 12 USD/hour, adjusting post-hoc for older age group, which took longer time to complete the surveys. Our final sample consisted of...

\input{content/methods}
\input{content/findings}
\input{content/discussion}
% \section{Conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{custom}
\input{content/appendix}
\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
