
\section{Methodology}
\label{sec:Methodology}

\begin{figure*}[htbp]
    % \centering
    \includegraphics[width=\textwidth]{figure/main_v3.pdf}
    \caption{(a) Framework of CleanPose. (b) Causal inference based on front-door adjustment is employed for mitigating potential spurious correlations, promoting unbiased pose estimation. Moreover, the proposed (c) residual-based knowledge distillation module can efficiently provide comprehensive category-specific guidance.
    }\label{fig:CleanPose}
    \vspace{-0.4cm} 
\end{figure*}


%-------------------------------------------------------------------------
\subsection{Feature Extractor}
\label{sec:feature_extractor}
Following~\cite{lin2024instance}, we utilize the PointNet++~\cite{qi2017pointnet++} to extract point feature $\mathcal{F}_{P} \in \mathbb{R}^{N \times C_1}$ of input point cloud $\mathcal{P}_{obj}$.
As for the RGB image $\mathcal{I}_{obj}$, we adopt DINOv2~\cite{oquab2024dinov2} as our image feature extractor, which has been proven to extract abundant semantic-aware information from RGB images~\cite{peng2024sam}. We select those pixel features corresponding to $\mathcal{P}_{obj}$ and utilize linear interpolation to propagate the original DINOv2 features into the final RGB features $\mathcal{F}_{I} \in \mathbb{R}^{N \times C_2}$. Eventually, we concatenate $\mathcal{F}_{P}$ and $\mathcal{F}_{I}$ to form $\mathcal{F}_{obj} \in \mathbb{R}^{N \times C}$ as the input for the subsequent networks.


%-------------------------------------------------------------------------
\subsection{Causal Inference of CleanPose}
\label{sec:causal_inference}
{\bf The Adjustment Formulation.} 
To avoid making the pose estimation process overfit to specific hidden confounders, we propose to perform robust modeling according to the causal graph and exploit \emph{intervention}~\cite{pearl2009causality} to alleviate the negative effects raised by confounders. Specifically, we employs \emph{do}-operation~\cite{pearl2016causal}, which provides scientifically sound methods for determining causal effects, as shown in \cref{fig:CleanPose}(b). The specific process of intervention via \emph{do}-operation is as follow.
Based on Bayes's theorem, the typical observational likelihood is as:
\begin{small}
\begin{equation}
\label{equ:typical_likelihood}
    P(\mathcal{Y}|\mathcal{X}) = \sum_{u}P(\mathcal{Y}|\mathcal{X},u)P(u|\mathcal{X}),
\end{equation}
\end{small}
where $P(u|\mathcal{X})$ would bring biased weights. Then we perform the \emph{do}-operation on inputs $\mathcal{X}$, which amounts to removing all edges directed into that variable in the graph model, \ie, from hidden confounders $\mathcal{U}$. According to the invariance and independence rules~\cite{pearl2016causal}, we have:
\begin{small}
\begin{align}
    P(\mathcal{Y}|do(\mathcal{X})) &= \sum_{u}P(\mathcal{Y}|do(\mathcal{X}),u)P(u|do(\mathcal{X})) \label{equ:typical_likelihood_dox_1}\\
    &= \sum_{u}P(\mathcal{Y}|\mathcal{X},u)P(u) \label{equ:typical_likelihood_dox_2}.
\end{align}
\end{small}
In this way, the intervention is realized by blocking the causal path $\mathcal{U} \rightarrow \mathcal{X}$. The \cref{equ:typical_likelihood_dox_2} is also known as the \emph{adjustment formulation}~\cite{pearl2009causality}.

\vspace{0.1cm}
\noindent
{\bf Front-door Adjustment Causal Learning.} In the previous section, we have clarified meaning of the \emph{front-door path} $\mathcal{X} \rightarrow \mathcal{M} \rightarrow \mathcal{Y}$. In COPE task, the DNNs-based models $P(\mathcal{Y}|\mathcal{X}) = \sum_{m}P(\mathcal{Y}|m)P(m|\mathcal{X})$ will choose the suitable knowledge $\mathcal{M}$ from input $\mathcal{X}$ for pose estimation results $\mathcal{Y}$.
Due to the presence of the mediators $\mathcal{M}$, applying adjustment \cref{equ:typical_likelihood_dox_2} solely to the inputs $\mathcal{X}$ is insufficient to fully block the confounding effects~\cite{wang2024vision}. Therefore, we implement continuous intervention in both stages of front-door path~\cite{pearl2016causal}, \ie, by simultaneously applying \emph{do}-operation to $\mathcal{X}$ and $\mathcal{M}$.
First, the effect of $\mathcal{X}$ on $\mathcal{M}$ is identifiable:
\begin{small}
\begin{equation}
\label{equ:x2m_do}
    P(\mathcal{M}|do(\mathcal{X})) = P(\mathcal{M}|\mathcal{X}).
\end{equation}
\end{small}
Then, for the second stage $\mathcal{M} \rightarrow \mathcal{Y}$, we apply the adjustment formulation and have:
\begin{small}
\begin{equation}
\label{equ:m2y_do}
    P(\mathcal{Y}|do(\mathcal{M})) = \sum_{x'}P(\mathcal{Y}|\mathcal{M}, x')P(x').
\end{equation}
\end{small}
Here, $x'$ denotes potential inputs of the whole representation space, different from current inputs $\mathcal{X} = x$. To chain together these two partial effects to obtain the overall effect of $\mathcal{X}$ on $\mathcal{Y}$, we sum over all states $m$ of $\mathcal{M}$ to form overall front-door adjustment as follow:
\begin{small}
\begin{align}
    P(\mathcal{Y}|do(\mathcal{X})) &= \sum_{m}P(\mathcal{Y}|do(m))P(m|do(\mathcal{X})) \label{equ:x2y_do_1} \\
    &= \sum_{x'}P(x')\sum_{m}P(\mathcal{Y}|m, x')P(m|\mathcal{X}) \label{equ:x2y_do_2}.
\end{align}
\end{small}
From the above derivation, we can observe that the front-door adjustment in causal inference involves the intermediate knowledge of current input $\mathcal{X} = x$ (\ie, the current features) as well as the cross-sampling features of other samples from the entire training set. We utilize the bold symbol $\boldsymbol{m}$ and $\boldsymbol{x'}$ to represent these two components respectively. Here, the intermediate knowledge is extracted from current input $\boldsymbol{m} \sim x$. Detailed methods for sampling $\boldsymbol{x'}$ are introduced in subsequent sections.

For convenience, we define the network module as a linear function $f(x,\boldsymbol{x'})$ to model the computation of $x$ and $\boldsymbol{x'}$ to obtain $\mathcal{Y}$. Meanwhile, according to the Bayes' rule and the the definition of Expected values, the front-door adjustment $\mathcal{FD}$(·) of \cref{equ:x2y_do_2} can be expressed as:
\begin{small} 
\begin{align}
    P(\mathcal{Y}|do(\mathcal{X})) &= \mathcal{FD}(x,\boldsymbol{x}') \label{equ:x2y_do_expected_1}\\
    &= \mathbb{E}_{x'}\mathbb{E}_{m|x}[f(x,\boldsymbol{x'})] \label{equ:x2y_do_expected_2},
\end{align}
\end{small}
where the linear function is used as $f(x,\boldsymbol{x'}) = f_x(x) + f_{x'}(x')$. Hence, based on the linear mapping model, the \cref{equ:x2y_do_expected_2} can be formulated as $\mathbb{E}_{x'}[\boldsymbol{x'}] + \mathbb{E}_{m|x}[\boldsymbol{m}]$. Since the closed-form solution of expected values are difficult to obtain in the complex representation space~\cite{wang2024vision}, we employ the query mechanism to achieve the estimation:
\begin{small} 
\begin{align}
    \mathbb{E}_{x'}[\boldsymbol{x}'] &\approx \sum_{x'}P(\boldsymbol{x}'|\boldsymbol{g}_1)\boldsymbol{x}' = \sum_{i} \frac{\exp \left(\boldsymbol{g}_{1} \boldsymbol{x}_{i}^{\prime T}\right)}{\sum_{j} \exp \left(\boldsymbol{g}_{1} \boldsymbol{x}_{j}^{\prime T}\right)} \boldsymbol{x}_{i}^{\prime} \label{equ:query1} \\
    \mathbb{E}_{m|x}[\boldsymbol{m}] &\approx \sum_{m}P(\boldsymbol{m}|\boldsymbol{g}_2)\boldsymbol{m} = \sum_{i} \frac{\exp \left(\boldsymbol{g}_{2} \boldsymbol{m}_{i}^{T}\right)}{\sum_{j} \exp \left(\boldsymbol{g}_{2} \boldsymbol{m}_{j}^{T}\right)} \boldsymbol{m}_{i} \label{equ:query2},
\end{align}
\end{small}
where $\boldsymbol{g}_1 = q_1(x)$ and $\boldsymbol{g}_2 = q_2(x)$ are two embedding functions~\cite{yang2021deconfounded,liu2023cross} that transmit input $x$ into two query sets. Then, the front-door adjustment $\mathcal{FD}$(·) is approximated as follows:
\begin{small} 
\begin{equation}
\label{equ:fd_final}
    \mathcal{FD}(x,\boldsymbol{x}') = \mathbb{E}_{x'}[\boldsymbol{x}'] + \mathbb{E}_{m|x}[\boldsymbol{m}].
\end{equation}
\end{small}
As we realize an approximate estimation through a querying mechanism, multi-head attention~\cite{vaswani2017attention} can be efficiently employed to handle the aforementioned process.

\vspace{0.2cm}
\noindent
{\bf Specific Network Design.} Although the derivation is clear, how to implement the causal inference in networks presents a significant technical challenge. 
Considering the characteristics of COPE, we propose leveraging the keypoint features of object as the intermediate knowledge $\boldsymbol{m}$, since the keypoints that are evenly distributed on the surface can effectively capture structure and pose information of the object.
Specifically, with the fusion input feature $\mathcal{F}_{obj}$, we detect $N_{kpt}$ local keypoints and perform global information aggregation to extract features $\mathcal{F}_{kpt} \in \mathbb{R}^{N_{kpt} \times C}$, following previous keypoint-based methods~\cite{lin2024instance,liu2023net}.
As for the cross-sampling features $\boldsymbol{x'}$, one straightforward way is to construct a memory bank of all features and randomly sample a certain number of features from it as the cross-sampling features. Afterward, the memory bank is updated at the corresponding positions using the features from the current mini-batch~\cite{wu2018unsupervised}. However, this intuitive approach is not applicable to our task, since the memory bank should cover all samples, it may not capture the dynamic variations of features during training. The lack of feature consistency could introduce additional confounders, which will negatively impact the models to focus on correct causal relations.

To this end, we draw inspiration from MoCo~\cite{he2020momentum} and devise a features sampling approach based on a dynamic queue. Specifically, we first utilize the 3D encoders of ULIP-2~\cite{xue2024ulip} to extract a specified number of features for each category, constructing an initial 2D queue with a shape of $N_{c} \times N_{q}$, where $N_{c}$ and $N_{q}$ represent the number of categories and the queue length of each category, respectively.
Next, we randomly sample $N_{s}$ features from the queue to form $\boldsymbol{x'}$, which is denoted as $\mathcal{F}_{samp} \in \mathbb{R}^{N_{s} \times C}$ in the networks. Based on the queue's characteristics, we update the keypoint features $\mathcal{F}_{kpt}$ from the current epoch into the queue via first-in-first-out (FIFO) approach, ensuring feature consistency within the queue.
Following \cref{equ:query1} to \cref{equ:fd_final}, the causality-enhanced features $\mathcal{F}_{f}$ are obtained as follow:
\begin{small} 
\begin{equation}
\label{equ:fd_attn}
    \mathcal{F}_{s} = SA(\mathcal{F}_{kpt}), \mathcal{F}_{c} = CA(\mathcal{F}_{kpt}, \mathcal{F}_{samp})
\end{equation}
\begin{equation}
\label{equ:fd_ln_plus}
    \mathcal{F}_{f} = LN(\mathcal{F}_{s} + \mathcal{F}_{c}),
\end{equation}
\end{small}
where $SA$(·) and $CA$(·) represent multi-head self-attention and cross-attention, respectively. $LN$(·) denotes the layer normalization.
Furthermore, to strengthen the stability of learning by fusing causality-enhanced features with original keypoint features of objects, we introduce an adaptive weight fusion method:
\begin{small} 
\begin{equation}
\label{equ:gate1}
    w_{a} = \sigma(\mathcal{F}_{f}W_{f}+\mathcal{F}_{kpt}W_{k})
\end{equation}
\begin{equation}
\label{equ:gate2}
    \mathcal{F}_{f} \leftarrow w_{a}\odot \mathcal{F}_{f} + (1-w_{a})\odot \mathcal{F}_{kpt},
\end{equation}
\end{small}
where $\sigma$ and $\odot$ mean Sigmoid function and element-wise multiplication. Suppose $W_{f/k} \in \mathbb{R}^{C \times 1}$ is learnable weight parameter.



%-------------------------------------------------------------------------
\subsection{Category Knowledge Distillation}
\label{sec:knowledge_distillation}
As mentioned in above section, implicitly learning category knowledge from a limited dataset only results in limited intra-category generalization.
Inspired by the feature alignment design~\cite{huang2024froster,zhu2024mote,zhu2023fine} to transfer visual-language knowledge to different tasks, we propose a modified residual approach to distill the category knowledge from 3D foundation models into COPE network, as shown in \cref{fig:CleanPose}(c).
Unlike CLIPose~\cite{lin2024clipose} exploits contrastive learning to extract semantic knowledge from text and image modalities, we leverage the pre-trained 3D encoder in ULIP-2~\cite{xue2024ulip}, which possesses strong awareness of point cloud structures and category information.
Specifically, given the input point cloud of a object $\mathcal{P}_{obj}$, we can directly feed it into the frozen 3D Encoder $\Phi_{ULIP}$ of ULIP-2, which can be written as:
\begin{small} 
\begin{equation}
\label{equ:pointbert}
    \mathcal{F}^{ULIP}_{P} = \Phi_{ULIP}(\mathcal{P}_{obj}),
\end{equation}
\end{small}
where $\mathcal{F}^{ULIP}_{P} \in \mathbb{R}^{1 \times C_3}$ denotes the $\verb|[CLS]|$ token in Point Transformer. The feature of $\verb|[CLS]|$ token embeds representation of the whole point cloud. To obtain the representation of our model, we employ a simple average pooling on point features and get $\mathcal{F}_{P}^{avg} \in \mathbb{R}^{1 \times C_1}$, we have:
\begin{small} 
\begin{equation}
\label{equ:avg_pool}
    \mathcal{F}_{P}^{avg} = \mathrm{AvgPool}(\mathcal{F}_{P}).
\end{equation}
\end{small}
Then, we apply a modified residual distillation network on $\mathcal{F}_{P}^{avg}$ to transform with two MLP projectors $K_1$ and $K_2$:
\begin{small} 
\begin{equation}
\label{equ:residual}
    \mathcal{\widehat{F}}_{P}^{avg} = \mathcal{F}_{P}^{avg} + \mu \times K_{2}(\delta(K_{1}(\mathcal{F}_{P}^{avg}))),
\end{equation}
\end{small}
where $K_{1/2} \in \mathbb{R}^{C_1 \times C_1}$, $\delta$ represents GELU function and $\mu$ is a balancing parameter. Moreover, we initialize the parameters of the second layer $K_{2}$ as zeros. Therefore, $\mathcal{\widehat{F}}_{P}^{avg}$ initially contains only $\mathcal{F}_{P}^{avg}$ and is gradually updated, which avoids introducing additional confounders.
Lastly, we can utilize L2 loss to supervise the category knowledge distillation:
\begin{small} 
\begin{equation}
\label{equ:distillation}
    \mathcal{L}_{KD} = \frac{1}{B} \sum_{i}^{B}\left\|\mathcal{F}^{ULIP}_{P}-\psi(\mathcal{\widehat{F}}_{P}^{avg})\right\|_{2},
\end{equation}
\end{small}
where $\psi \in \mathbb{R}^{C_1 \times C_3}$ is a learnable embedding layer and $B$ denotes the batch size. 
In this way, the tuned point cloud features of our model can effectively receive supervision from generalized ones, facilitating comprehensive category knowledge learning and enhancing the model's intra-category generalization.



%-------------------------------------------------------------------------
\subsection{Pose Estimation and Overall Loss Function}
\label{sec:estimation_and_loss}
With the obtained causality-enhanced features, following previous works~\cite{lin2022category,lin2024instance}, we recover the final pose and size $\mathcal{R}, t, s$ via set of keypoint-level correspondences containing global features and points. A simply L1 Loss is used to supervise the predicted pose, in formula:
\begin{small} 
\begin{equation}
\label{equ:pose_rts}
    \mathcal{L}_{pose} = \left\|\mathcal{R}_{gt} - \mathcal{R}\right\|_{2} + \left\|t_{gt} - t\right\|_{2} + \left\|s_{gt} - s\right\|_{2},
\end{equation}
\end{small}
where $\mathcal{R}_{gt}, t_{gt}, s_{gt}$ means the ground truth rotation, translation and size. For more details please refer to supplementary or ~\cite{lin2024instance}. 
Hence, the overall loss function is as follow:
\begin{small} 
\begin{equation}
\label{equ:overall_loss_func}
    \mathcal{L}_{all} = \alpha_{1}\mathcal{L}_{pose} + \alpha_{2}\mathcal{L}_{KD},
\end{equation}
\end{small}
where $\alpha_{1}, \alpha_{2}$ are hyper-parameters to balanced the contribution of each term. We omit some loss terms for for brevity. \emph{Please see supplementary for more details}.

