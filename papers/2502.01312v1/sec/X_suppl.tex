\clearpage
\setcounter{page}{1}
\setcounter{section}{0}
\maketitlesupplementary
\renewcommand\thesection{\Alph{section}}

%-------------------------------------------------------------------------------
\section{Limitation and Broader Impact}
\label{sec:suppl_limitation}
\noindent
{\bf Limitation and future work.} While our method achieves superior results in various challenging benchmarks of category-level pose estimation, there are still several aspects for improvement. First, although the front-door adjustment is effective, the investigate on the application of causal learning methods remains incomplete. Therefore, exploring further use of different causal learning methods such as back-door adjustment and counterfactual reasoning may enhance the performance of CleanPose.
Second, despite the guidance of the causal analysis, the network modules in actual implementation may induce inaccuracy inevitably. Such a flaw introduces a gap between the causal framework and the network design. In future work, we will further study advanced algorithm design strategies.

\vspace{0.1cm}
\noindent
{\bf Broader Impact.} For tasks with parameter regression properties, \eg, category-level pose estimation, the current mainstream approaches focus on exploring advanced network designs to perform data fitting. We believe that relying solely on learning statistical similarity can also introduce spurious correlations into parameter regression models, thereby damaging the model's generalization ability. We hope this work brings new insights for the broader and long-term research on parameter regression tasks.
Besides, adapting foundation models to downstream tasks has become a dominant paradigm in machine learning. Our method also provide novel views for offering knowledge guidance in similar tasks across diverse categories.

\section{More Loss Function Details}
\label{sec:suppl_loss_details}
The backbone of our method is based on AG-Pose~\cite{lin2024instance}. In addition to $\mathcal{L}_{pose}$ (\cref{equ:pose_rts}), there are some additional loss functions to balance keypoints selection and pose prediction. First, to encourage the keypoints to focus on different parts, the diversity loss $\mathcal{L}_{div}$ is used to force the detected keypoints to be away from each other, in detail:
\begin{small} 
\begin{align}
    \mathcal{L}_{div}&=\sum_{i=1}^{N_{kpt}} \sum_{j=1, j \neq i}^{N_{kpt}} \mathbf{d}\left(\mathcal{P}_{kpt}^{(i)}, \mathcal{P}_{kpt}^{(j)}\right) \label{equ:supp_divloss1}\\
    \mathbf{d}\left(\mathcal{P}_{kpt}^{(i)}, \mathcal{P}_{kpt}^{(j)}\right)&=\max \left\{th_{1}-\left\|\mathcal{P}_{kpt}^{(i)}-\mathcal{P}_{kpt}^{(j)}\right\|_{2}, 0\right\} \label{equ:supp_divloss2},
\end{align}
\end{small}
where $th_1$ is a hyper-parameter and is set as 0.01, $\mathcal{P}_{kpt}^{(i)}$ means the $i$-th keypoint.
To encourage the keypoints to locate on the surface of the object and exclude outliers simultaneously, an object-aware chamfer distance loss $\mathcal{L}_{ocd}$ is employed to constrain the distribution of $\mathcal{P}_{kpt}$. In formula:
\begin{small} 
\begin{align}
    \mathcal{L}_{ocd}=\frac{1}{\left|\mathcal{P}_{kpt}\right|} \sum_{x_{i} \in \mathcal{P}_{kpt}} \min _{y_{j} \in \mathcal{P}_{obj}^{'}}\left\|x_{i}-y_{j}\right\|_{2}, \label{equ:supp_ocdloss}
\end{align}
\end{small}
where $\mathcal{P}_{obj}^{'}$ denotes the point cloud of objects without outlier points.
Moreover, we also use MLP to predict the NOCS coordinates of keypoints $\mathcal{P}_{kpt}^{nocs} \in \mathbb{R}^{N_{kpt} \times 3}$. Then, we generate ground truth NOCS of keypoints $\mathcal{P}_{kpt}^{gt}$ by projecting their coordinates under camera space $\mathcal{P}_{kpt}$ into NOCS using the ground truth $\mathcal{R}_{gt},t_{gt},s_{gt}$. And we use the $SmoothL_1$ loss to supervise the NOCS projection:
\begin{small} 
\begin{align}
    \mathcal{P}_{kpt}^{gt}&=\frac{1}{\left\|s_{gt}\right\|_{2}} \mathcal{R}_{gt}\left(\mathcal{P}_{kpt}-t_{gt}\right) \label{equ:supp_nocsgt}\\
    \mathcal{L}_{nocs} &= SmoothL_{1}(\mathcal{P}_{kpt}^{gt},\mathcal{P}_{kpt}^{nocs}). \label{equ:supp_nocsloss}
\end{align}
\end{small}
Hence, the complete form of overall loss (\cref{equ:overall_loss_func}) is as follows:
\begin{small} 
\begin{equation}
\begin{split}
    \mathcal{L}_{all} = \lambda_1\mathcal{L}_{ocd} + \lambda_2\mathcal{L}_{div} + \lambda_3\mathcal{L}_{nocs} \\ + \lambda_4\mathcal{L}_{pose} + \alpha_{2}\mathcal{L}_{KD}, \label{equ:supp_overall_loss}
\end{split}
\end{equation}
\end{small}
where the parameters are set as ($\lambda_1,\lambda_2,\lambda_3,\lambda_4,\alpha_{2}$) = ($1.0,5.0,1.0,0.3,0.01$) according to AG-Pose~\cite{lin2024instance} and following ablations.


\section{More Details of Using ULIP-2}
\label{sec:suppl_use_ulip2}
ULIP-2~\cite{xue2024ulip} is a large-scale 3D foundation model with strong perception capabilities for the point cloud modality. It offers multiple pre-trained versions of point cloud encoders. In our model, there are two key steps that involve the use of different pre-trained encoders of ULIP-2.
(\textbf{i}) In the knowledge distillation process, we leverage the pre-trained PointBert~\cite{yu2022point}, which achieves the best zero-shot classification performance across all versions. Therefore, it can provide comprehensive category knowledge guidance for our model. In the ablation study \cref{tab:ab_dis_encoder}, we also compared it with PointNet++~\cite{qi2017pointnet++}, which is more similar in architecture to our model. The experimental results demonstrate that our distillation method focuses more on category knowledge rather than feature similarity.
(\textbf{ii}) However, during the initial construction of the dynamic queue, we use the pre-trained PointNet++~\cite{qi2017pointnet++}, as the front-door adjustment primarily focuses on the differences between samples. We aim to avoid introducing confounders due to feature discrepancies from different encoders. The additional ablation study results in \cref{tab:supp_queue_encoder} also support our analysis.

\begin{table}[htbp]
    \small
    \centering
    % 将每个子表格在单独的行上排列
    \begin{subtable}[t]{1.0\linewidth}  % 子表格宽度可调节
        \centering
        \setlength\tabcolsep{8pt}%2pt 列宽
        \begin{tabular}{c|cccc}
            \toprule
            $N_{q}$ & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm}\\
            \midrule
            20  & 57.0          & 64.6    &75.1  &84.7   \\
            50 & 60.5          &66.5    &77.9 &\textbf{86.4}   \\
            \rowcolor{mygray}
            80 & \textbf{61.5}          &\textbf{67.4}    &\textbf{78.3} &86.2   \\
            200  &59.4          &66.1    &78.0 &85.9   \\
            500  &  58.8         &65.3    &76.8 &85.8   \\
            1000  & 58.3          &66.8    &76.3 &86.2   \\
            3000  & 57.7          &65.5    &75.6 &85.0   \\
            10000  & 57.0          &65.0    &75.7 &85.4   \\
            \bottomrule
        \end{tabular}
        \caption{Effect of varying queue lengths $N_{q}$}
        \label{tab:supp_ab_causal_Nq}
    \end{subtable}
    
    \vspace{0.2cm}  % 每个子表格之间增加垂直间距

    \begin{subtable}[t]{1.0\linewidth}  % 子表格宽度可调节
        \centering
        \setlength\tabcolsep{9pt}%2pt 列宽
        \begin{tabular}{c|cccc}
            \toprule
            $N_{s}$ & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm}\\
            \midrule
            6  & 59.4          & 65.3    &76.8  &84.3   \\
            \rowcolor{mygray}
            12 & \textbf{61.5}          &\textbf{67.4}    &78.3 &86.2   \\
            18 & 58.7          &65.5    &\textbf{78.8} &\textbf{86.8}   \\
            24  &  58.5         &64.8    &77.8 &85.9   \\
            48  &56.8          &64.5    &76.3 &85.6   \\
            80  & 56.9          &64.4    &76.2 &85.8   \\
            \bottomrule
        \end{tabular}
        \caption{Effect of varying queue lengths $N_{s}$}
        \label{tab:supp_ab_causal_Ns}
    \end{subtable}

    \vspace{0.2cm}  % 每个子表格之间增加垂直间距

    \begin{subtable}[t]{1.0\linewidth}  % 子表格宽度可调节
        \centering
        \setlength\tabcolsep{8pt}%2pt 列宽
        \begin{tabular}{c|cccc}
            \toprule
            $\alpha_{2}$ & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm}\\
            \midrule
            0.005  & 59.3          & 66.8    &78.0  &\textbf{87.4}   \\
            \rowcolor{mygray}
            0.01 & \textbf{61.5}          &\textbf{67.4}    &\textbf{78.3} &86.2   \\
            0.1 & 58.4          &65.1    &77.6 &85.9   \\
            0.5  &  57.3         &63.9    &76.2 &86.0   \\
            1  &  56.9         &63.4    &76.4 &85.4   \\
            \bottomrule
        \end{tabular}
        \caption{Effect of varying balanced coefficient $\alpha_2$}
        \label{tab:supp_ab_kd_a2}
    \end{subtable}
     
    \caption{Additional ablation studies on some hyper-parameters. Default settings are colored in \colorbox{mygray}{gray}.}
    \label{table:supp_ablation_detail}
    \vspace{-0.3cm}
\end{table}

\section{Additional Ablations}
\label{sec:suppl_additional_ablations}
\noindent
{\bf Effect of varying queue lengths $N_q$.} \cref{tab:supp_ab_causal_Nq} ablates the different lengths of dynamic queue $N_q$. The queue that is too short results in insufficient sample diversity, while too long affect memory efficiency and feature consistency.
We observe that the estimation performance achieves the peak at the length of around 80, with slight declines upon further increases.
We speculate that the queue length is closely related to task characteristics and data scale of COPE.
We select $N_q = 80$ in our model to balance between efficiency and accuracy.

\vspace{0.1cm}
\noindent
{\bf Effect of different sampling quantities $N_s$.} In \cref{sec:causal_inference} of main manuscript, we sample $N_s$ features for the specific network design to perform \emph{front-door adjustment}. We study the influence with response to different sampling quantities $N_s$ in \cref{tab:supp_ab_causal_Ns}. We find that a large $N_s$ leads to a slight performance degradation as selected features may not capture the representation space properly. The results demonstrate that $N_s = 12$ yields the most significant performance gains.


\begin{table}[htbp]
    \small
    \centering
    % 将每个子表格在单独的行上排列
    \begin{subtable}[t]{1.0\linewidth}  % 子表格宽度可调节
        \centering
        \setlength\tabcolsep{9pt}%2pt 列宽
        \begin{tabular}{c|cccc}
            \toprule
            Init. & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm}\\
            \midrule
            Random  & 57.0          & 65.7    &74.8  &85.4   \\
            \rowcolor{mygray}
            Extract & \textbf{61.5}   &\textbf{67.4}    &\textbf{78.3} &\textbf{86.2}   \\
            \bottomrule
        \end{tabular}
        \caption{Effect of different queue initialization approaches}
        \label{tab:supp_ab_queue_init}
    \end{subtable}

    \vspace{0.2cm}  % 每个子表格之间增加垂直间距

    \begin{subtable}[t]{1.0\linewidth}  % 子表格宽度可调节
        \centering
        \setlength\tabcolsep{7pt}%2pt 列宽
        \begin{tabular}{c|cccc}
            \toprule
            3D Encoder & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm}\\
            \midrule
            \rowcolor{mygray}
            PointNet++\cite{qi2017pointnet++} & \textbf{61.5}          &\textbf{67.4}    &\textbf{78.3} &86.2   \\
            PointMLP\cite{ma2022rethinking} & 56.1          &64.8   &76.0 &85.2   \\
            PointBert\cite{yu2022point} & 58.8          &65.2    &77.9 &\textbf{86.4}   \\
            \bottomrule
        \end{tabular}
        \caption{Effect of different 3D encoders of ULIP-2~\cite{xue2024ulip} for initial construction of the queue.}
        \label{tab:supp_queue_encoder}
    \end{subtable}

    \vspace{0.2cm}  % 每个子表格之间增加垂直间距

    \begin{subtable}[t]{1.0\linewidth}  % 子表格宽度可调节
        \centering
        \setlength\tabcolsep{7pt}%2pt 列宽
        \begin{tabular}{c|cccc}
            \toprule
            Selector & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm}\\
            \midrule
            \rowcolor{mygray}
            Random  & \textbf{61.5}          &\textbf{67.4}    &\textbf{78.3} &86.2   \\
            K-means & 58.5          &65.1    &78.0 &\textbf{86.5}   \\
            K-means (simi) & 56.7          &64.2    &76.7 &86.0   \\
            \bottomrule
        \end{tabular}
        \caption{Effect of distinct feature selectors}
        \label{tab:supp_queue_sample}
    \end{subtable}

    \caption{Additional ablation studies on confounders queue. Default settings are colored in \colorbox{mygray}{gray}.}
    \label{table:supp_ablation_detail}
    % \vspace{-0.3cm}
\end{table}

\begin{figure}[htbp]
    \centering
    % \small
    \subfloat[Random feature selector]
    {\includegraphics[width=.4\columnwidth]{figure/ns_rand.pdf}}
    \hspace{8pt}
    \subfloat[K-means feature selector]{\includegraphics[width=.51\columnwidth]{figure/ns_kmeans.pdf}}
    \caption{Illustration of different feature selectors in ablations \cref{tab:supp_queue_sample}.
    }
    \label{fig:subfig}
    \vspace{-0.2cm}
\end{figure}

\vspace{0.1cm}
\noindent
{\bf Varying balanced coefficient $\alpha_{2}$ for loss $\mathcal{L}_{KD}$.} In \cref{sec:knowledge_distillation} of main manuscript, we introduce L2 loss to supervise the feature-based distillation and use $\alpha_{2}$ to balanced its contribution in overall loss function. We investigate the impact of different $\alpha_{2}$ in \cref{tab:supp_ab_kd_a2}. We observe that the better performance is achieved when $\alpha_{2}$ is small, possibly because $\mathcal{L}_{KD}$ is comparable in magnitude to the pose loss function, which is favorable for regression. The results show that our method performs well under $\alpha_{2} = 0.01$.

\vspace{0.1cm}
\noindent
{\bf Different queue initialization approaches.} By default, we construct confounders queue with features extracted by 3D encoders of ULIP-2~\cite{xue2024ulip}. Alternatively, we can randomly initialize the queue, which should achieve the same effect ideally. Therefore, we evaluate the performance between two initialization approaches in \cref{tab:supp_ab_queue_init}. The results indicate the degraded performance with “Random” initialization strategy. We speculate that the randomly initialized queue may introduce additional and uncontrollable confounders, limiting the model's optimization potential.


%%%%%%%% per_category %%%%%%%%%%%%%%%
% \begin{table*}[htbp]
%     \small
%     \centering
%     \setlength\tabcolsep{7pt}%2pt
%     % \renewcommand\arraystretch{1.2} % 行高
%     \begin{tabular}{l|ccc|ccc|cccc}%{c|cc|cc|cc|cccccc}
%     \toprule%[1.2pt]
%      Category & $IoU_{25}^*$ & $IoU_{50}^*$   & $IoU_{75}^*$ & $IoU_{25}$ & $IoU_{50}$   & $IoU_{75}$ & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm} \\ 
%      \midrule%[1pt]
%     bottle &51.3 &49.4 &36.9   &57.7  &57.6 & 50.1   &48.2 &58.0 &81.2 &95.8 \\
%     bowl &100.0 &100.0 &93.8   & 100.0 &100.0 & 100.0  &92.8 &97.8 &94.9 &99.9  \\
%     camera &90.9 &83.5 &39.5   &90.9 &90.9 &  86.1  &2.8 &3.1 &33.9 &40.5  \\
%     can &71.3 &71.1 &43.1   &71.4 &71.4 & 71.3   &84.2 &85.8 &96.9 &98.6  \\
%     laptop &86.3 &84.0 &76.1   & 86.3& 85.1& 77.0   &68.9 &90.2 &71.9 &98.5  \\
%     mug &99.6 &99.4 &86.1   & 99.6&99.6 & 98.8   &45.3 &45.6 &91.9 &91.9  \\
%     \midrule%[1pt]
%     Average &83.3 &81.2 &62.6 &84.3 &84.1 &80.6 &61.5 &67.4 &78.3 &86.2 \\
%     \bottomrule%[1.2pt]
%     \end{tabular}
%     \caption{\textbf{Category-wise evaluation of CleanPose on REAL275 dataset.} ‘*’ denotes CATRE~\cite{liu2022catre} IoU metrics.
%     }
%     \label{tab:suppl_per_category_real}
% \end{table*}

\begin{table*}[htbp]
    \small
    \centering
    \setlength\tabcolsep{7pt}%2pt
    % \renewcommand\arraystretch{1.2} % 行高
    \begin{tabular}{l|ccc|cccc}%{c|cc|cc|cc|cccccc}
    \toprule%[1.2pt]
     Category & $IoU_{25}^*$ & $IoU_{50}^*$   & $IoU_{75}^*$ & 5°2\emph{cm} & 5°5\emph{cm} & 10°2\emph{cm}  & 10°5\emph{cm} \\ 
     \midrule%[1pt]
    bottle &51.3 &49.4 &36.9    &75.3 &81.7 &79.9 &87.8 \\
    bowl &100.0 &100.0 &93.8   &92.8 &97.8 &95.0 &99.9  \\
    camera &90.9 &83.5 &39.5   &2.8 &3.1 &33.9 &40.5  \\
    can &71.3 &71.1 &43.1      &84.2 &85.8 &96.9 &98.6  \\
    laptop &86.3 &84.0 &76.1      &68.9 &90.2 &71.9 &98.5  \\
    mug &99.6 &99.4 &86.1      &45.3 &45.6 &91.9 &91.9  \\
    \midrule%[1pt]
    Average &83.3 &81.2 &62.6 &61.5 &67.4 &78.3 &86.2 \\
    \bottomrule%[1.2pt]
    \end{tabular}
    \caption{\textbf{Category-wise evaluation of CleanPose on REAL275 dataset.} ‘*’ denotes CATRE~\cite{liu2022catre} IoU metrics.
    }
    \label{tab:suppl_per_category_real}
\end{table*}


\begin{table*}[htbp]
    \footnotesize
    \centering
    \setlength\tabcolsep{4pt}%2pt
    % \renewcommand\arraystretch{1.2} % 行高
    \begin{tabular}{l|c|c|cccccccccc}
    \toprule%[1.2pt]
     \multirow{2}{*}{Methods} & \multirow{2}{*}{$IoU_{75}$$\uparrow$} & \multicolumn{11}{c}{$IoU_{25}$/$IoU_{50}$$\uparrow$}\\
     \cline{3-13}
     ~ & ~ & Average    & Bottle & Box & Can & Cup  & Remote & Teapot &Cutlery & Glass & Tube & Shoe\\ 
     \midrule%[1pt]
    NOCS\cite{wang2019normalized} &-& 50.0/21.2 & 41.9/5.0  & 43.3/6.5  & 81.9/62.4 & 68.8/2.0  & \textbf{81.8}/\textbf{59.8} & 24.3/0.1  & 14.7/6.0  & 95.4/49.6 & 21.0/4.6  & 26.4/16.5 \\
    FS-Net\cite{chen2021fs} &14.8 &74.9/48.0 & 65.3/45.0 & 31.7/1.2  & 98.3/73.8 & 96.4/68.1 & 65.6/46.8 & 69.9/59.8 & 71.0/51.6 & 99.4/32.4 & 79.7/{\ul46.0} & 71.4/55.4 \\
    GPV-Pose\cite{di2022gpv} &15.2 &74.9/50.7 & 66.8/45.6 & 31.4/1.1  & 98.6/75.2 & 96.7/69.0 & 65.7/46.9 & 75.4/61.6 & 70.9/52.0 & {\ul99.6}/62.7 & 76.9/42.4 & 67.4/50.2 \\
    VI-Net\cite{lin2023vi} &20.4 &80.7/56.4 & 90.6/79.6 & 44.8/12.7 & {\ul99.0}/67.0 & 96.7/72.1 & 54.9/17.1 & 52.6/47.3 & 89.2/76.4 & 99.1/93.7 & \textbf{94.9}/36.0 & 85.2/62.4 \\
    SecondPose\cite{chen2024secondpose} &24.9 &83.7/66.1 & 94.5/{\ul79.8} & \textbf{54.5}/{\ul23.7} & 98.5/93.2 & {\ul99.8}/{\ul82.9} & 53.6/35.4 & 81.0/71.0 & 93.5/74.4 & 99.3/92.5  & 75.6/35.6 & 86.9/73.0 \\
    AG-Pose\cite{lin2024instance} &{\ul53.0} &{\ul88.1}/{\ul76.9} & {\ul97.6}/\textbf{86.0}  & {\ul54.0}/13.9   & 98.3/{\ul96.7} & \textbf{100}/\textbf{99.9}  & 53.9/37.2 & \textbf{99.9}/\textbf{98.5} & {\ul96.0}/\textbf{93.3}   & \textbf{100}/{\ul99.3}  & {\ul81.4}/45.0   & {\ul99.7}/{\ul99.5} \\
    \midrule%[1pt]
    \textbf{CleanPose} &\textbf{53.9} &\textbf{89.2}/\textbf{79.8} & \textbf{99.9}/79.1 & 51.4/\textbf{28.7} & \textbf{99.9}/\textbf{99.7} & \textbf{100}/\textbf{99.9}  & {\ul71.2}/{\ul57.8} & {\ul99.0}/{\ul94.0} & \textbf{97.8}/{\ul91.0} & \textbf{100}/\textbf{99.6}  & 72.7/\textbf{48.4} & \textbf{99.8}/\textbf{99.8}\\
    \bottomrule%[1.2pt]
    \end{tabular}
    \caption{\textbf{Overall and category-wise evaluation of 3D IoU on the HouseCat6D.} $\uparrow$: a higher value indicating better performance, ‘-’ means unavailable statistics. Overall best results are in \textbf{bold} and the second best results are {\ul underlined}.
    }
    \label{tab:suppl_housecat6d}
\end{table*}


\vspace{0.1cm}
\noindent
{\bf Effect of different 3D encoder for initial construction of the queue.}
\cref{tab:supp_queue_encoder} ablates the different 3D encoders of ULIP-2~\cite{xue2024ulip} for initial construction of the dynamic queue. The results exhibit that using PointNet++~\cite{qi2017pointnet++} yields the most performance gains.
As mentioned in \cref{sec:suppl_use_ulip2}, the dynamic queue is utilized in the cross-attention phase of front-door adjustment, thus primarily focusing on the differences between samples. Using encoders with similar architectures helps avoid introducing extra confounders.

\vspace{0.1cm}
\noindent
{\bf Various feature selection strategies.} In \cref{sec:causal_inference} of main manuscript, we randomly sample $N_s$ features from queue by default, as shown in \cref{fig:subfig}(a). Optionally, we can first use K-means to cluster the features of the queue, and then select features from each cluster to form $\mathcal{F}_{samp}$, as illustrated in \cref{fig:subfig}(b). For fair comparison, the number of clusters is set equal to $N_s$. 
We investigate the impact of these two feature selector in \cref{tab:supp_queue_sample}. As shown in the table, we observe that K-means-based feature sampling strategy shows a decline in performance on strict metrics (5°2\emph{cm} and 5°5\emph{cm}). We argue that k-means, which clusters by Euclidean distance, may lose important boundary information, thus affecting the model performance. Moreover, k-means clustering needs to be performed after each queue update, which increases computational load and training costs. Therefore, we added a comparative experiment using similarity-based updates, denoted as ‘K-means (simi)’, where (simi) refers to the ‘Similarity’ defined in \cref{tab:ab_causal_data_update}. In this case, clustering is only performed once during the initial training. However, experimental results also show that such strategy leads to further performance degradation as one clustering loses the diversity of features.



\section{More Experimental Results}
\label{sec:suppl_more_results}
We report category-wise results of REAL275~\cite{wang2019normalized} in \cref{tab:suppl_per_category_real}. Since there is a small mistake in the original evaluation code of NOCS~\cite{wang2019normalized} for the 3D IoU metrics, we present more reasonable CATRE~\cite{liu2022catre} metrics following~\cite{zheng2024georef,liu2024mh6d,chen2024secondpose}. 
% However, considering that some methods~\cite{lin2024instance,zheng2023hs} only report the NOCS~\cite{wang2019normalized} IoU metric, we also add the results under that metric for information reference.
Further, more detailed results of HouseCat6D~\cite{jung2024housecat6d} are shown in \cref{tab:suppl_housecat6d}. As for more restricted metric $IoU_{75}$, our method also demonstrates state-of-the-art performance (\textbf{53.9\%}), further validating the effectiveness of CleanPose in 3D IoU evaluation.
Moreover, in category-wise validation on $IoU_{25}$ and $IoU_{50}$, our approach obtains state-of-the-art (\eg, \emph{Can}, \emph{Cup}, \emph{Glass} and \emph{Shoe}) or competitive results across all categories. 
It is worth mentioning that our method exhibits more stable performance on these two metrics. For instance, compared to the current sota method AG-Pose~\cite{lin2024instance} in the \emph{Box} category, our method achieves the best performance (\textbf{28.7\%}) on $IoU_{50}$ metric when both obtain competitive results on $IoU_{25}$ metric, with a significant reduction of the AG-Pose (13.9\%).









%-------------------------------------------------------------------------------

