\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage{colortbl}
% \usepackage{ulem}
% \useunder{\uline}{\ul}{}
% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\definecolor{mygray}{gray}{.9}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}


\newcommand{\re}[2]{\textcolor{#1}{{\bf #2}}}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
\setcounter{table}{0}
\renewcommand\thetable{Rb\arabic{table}}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{2514} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
% \section{Introduction}
% \re{red}{R1} \re{blue}{R2} \re{green}{R3}
\noindent
We appreciate reviewers for their valuable feedback, acknowledging the \emph{“clarity and novelty”} (\re{green}{rYxx@R3}) of our core idea, as well as the \emph{“detailed formulation provided”} (\re{blue}{qCQv@R2}). We are encouraged they recognize our approach \emph{“well-organized and easy to follow”} (\re{blue}{R2}, \re{green}{R3}), evaluated with \emph{“extensive experiments”} (\re{green}{R3}), and \emph{“achieving {\bf \emph{SOTA}} performance”} in multiple benchmarks (\re{red}{QHjZ@R1}, \re{blue}{R2}, \re{green}{R3}). We sincerely thank the reviewers for their diligent work and hope our response meets their approval.

\noindent
{\bf $\triangleright$ Responses to individual questions for each reviewer.}

\noindent
\re{red}{@R1 \#w1:} {\bf The Clarity of Paper.}  Thanks for your feedback! Key terms such as \emph{“confounders”} and \emph{“front-door path”} are formally introduced in Sec.3.2 (L201-214), and the causal modeling is illustrated in Fig.2. To improve clarity, we will further enhance the explanation of these key concepts by providing more concrete examples from the task domain and clarifying their roles in the methodology. Additionally, we will confirm that the causal modeling process is presented more intuitively to make it easier to follow.

\noindent
\re{red}{@R1 \#w2:} {\bf The Novelty of Method.} We appreciate your concerns. The key novelty and contribution of our work lies in integrating causal theoretical foundation [36, 37] into COPE models. As Reviewer 3 noted: \emph{“this idea is novel and not straightforward”}. To the best of our knowledge, we are the first to introduce causal modeling to enhance COPE models and achieve significant improvements. Therefore, our work is not simply a combination of existing methods, which involves careful exploration and design that addresses specific challenges mitigating confounding effects.


\noindent
\re{red}{@R1 \#w3:} {\bf Fairness of Comparison.} We would address your comment in two aspects: First, one of the contributions in our work is to explore how to better utilize 3D foundation models to enhance generalization, which is closely related to the overall design of our method, rather than introducing “extra knowledge” that benefits the comparison. Second, 3D foundation model only provides supervision during training, meaning it would not increase any computational burden during inference.
% as shown in \cref{tab:ablation_main} (\#1, \#2).
As for the comparison, we followed the domain consensus to report the metric accuracy.
In response to your suggestions, we have added more terms (\eg encoder type, inference latency) in \cref{tab:ablation_main} for comprehensive comparison. Due to space limitations, detailed comparisons and explanations will be included in revision.

\noindent
\re{blue}{@R2 \#w1:} {\bf Efficacy of Causal Learning.} In fact, as shown in Tab. \textcolor{red}{4}, introducing causal inference can already achieves SOTA results in the rigorous metric of 5°2\emph{cm}, 5°5\emph{cm} and 10°2\emph{cm}, surpassing the baseline by 2.7\%, 1.9\% and 2.8\%. Additionally, as confirmed in \cref{tab:ablation_main}, the front-door adjustment only increases the number of parameters by 10\% (246M \vs 223M), while the running time remains nearly unchanged (33 \vs 35 in FPS).
To ensure a fair comparison, we also replace the front-door module with MLPs that have the same number of parameters (\#3). The results further demonstrate the superior effectiveness of causal learning.


\noindent
\re{blue}{@R2 \#w2:} {\bf Comparison with Feature Concat.} In response to your suggestion, we have concatenated $\mathcal{F}^{ULIP}_{P}$, $\mathcal{F}_{I}$ and $\mathcal{F}_{P}$ for comparison in \cref{tab:ablation_main} (\#4 \vs \#2). The results indicate that the proposed knowledge distillation approach is more effective than feature concatenation.

\noindent
\re{blue}{@R2 \#w3:} {\bf Type of ViT and Additional Comparison.} ViT-S/14 (DINOv2). Following your suggestion, we have added the encoder backbone in \cref{tab:ablation_main}, and included additional results with ResNet18 (\#6). Specifically, our method still outperforms AG-Pose with ResNet18 setting (\#6 \vs \#5), further supporting the efficacy of our approach.

\noindent
\re{green}{@R3 \#w1:} {\bf Reflect of Limitation.} We sincerely acknowledge your valuable feedback! Regarding this limitation, since our method selects PointNet++ as 3D encoder, we hypothesize that when the teacher and student models share similar architectures (\eg, both use PointNet++), the distillation may mislead the student model to focus on feature structure similarity rather than transferring category knowledge. We will add the analysis and detailed difference among three backbones in main text or appendix materials.

\noindent
\re{green}{@R3 \#w2:} {\bf Pose Loss.} L1 loss. We will address it.

\noindent
\re{green}{@R3 \#w4:} {\bf Effect of $N_{s}$.} We speculate that a larger sample size may introduce noise and redundant information that affects key features in causal inference. An appropriate sample size can balance valid and redundant information, prompting the model to focus on learning more representative causal correlations. This will be added in appendix.

\noindent
\re{green}{@R3 \#w5:} {\bf Effect of Sampling.} We have conducted additional experiments with 6 different random seeds, as shown in \cref{tab:inference_samp}. The computed variances $\sigma^{2}$ for metrics demonstrate stable performance across different random seeds, indicating the robustness and reliability of our method.
% Further discussion will be added in revision.

\noindent
\re{blue}{@R2 \#w4}, \re{green}{@R3 \#w3:} {\bf Proofreading.} Thanks for reviewers' reminders and corrections. We will add related works' results and address the caption in revision.
\vspace{-0.3cm}

\begin{table}[htbp]
    \small
    \centering
    \setlength\tabcolsep{4pt}%2pt 列宽
    % \renewcommand\arraystretch{0.9} % 行高
    \begin{tabular}{c|l|c|c|c|c|c}
    % \toprule%[1.2pt]
    \hline
   ID & Method & Encoder & Param.$\downarrow$ & Distill. & 5°2\emph{cm}$\uparrow$  & FPS$\uparrow$\\
    % \midrule%[1pt]
    \hline
    1        &AG-Pose      &  ViT-S/14     &\textbf{223M}          & -    &57.0  &\textbf{35}   \\
    \rowcolor{mygray}
    2         &ours     &ViT-S/14        & \underline{246M}        &Default    &	\textbf{61.5} &\underline{33}   \\
    % \hline
    3        &ours$^*$      &  ViT-S/14      & \underline{246M}          &Default    &59.4 &\underline{33}   \\
    4         &ours     &ViT-S/14        & \underline{246M}        &Concat    &59.8 &31   \\
    \hline
    5         & AG-Pose        & resnet18      & \textbf{220M}          & -   &56.2 &\textbf{35}   \\
    6         & ours        & resnet18      & \underline{243M}          & Default   &\underline{60.3} &\underline{33}   \\
    % \bottomrule%[1.2pt]
    \hline
    \end{tabular}
    \vspace{-0.3cm}
    \caption{Additional results. $*$ denotes replacement of causal module with MLPs of the same number of parameters.
    }
    \label{tab:ablation_main}
\end{table}

\vspace{-0.6cm}

\begin{table}[htbp]
    \small
    \centering
    \setlength\tabcolsep{5pt}%2pt 列宽
    % \renewcommand\arraystretch{0.9} % 行高
    \begin{tabular}{c|cccccc|c}
    % \toprule%[1.2pt]
    \hline
   Seed & 1 & 42 & 500 & 1k & 1w  & 10w& $\sigma^{2}\downarrow$ \\
    % \midrule%[1pt]
    \hline
    5°2\emph{cm}              &  61.4     &61.5         & 61.7    &61.4  &61.3 &61.7&0.03   \\
    5°5\emph{cm}     &67.2        & 67.3        &67.5    &	67.2 &67.1 &67.6&0.04  \\
    % \hline
    % 10°2\emph{cm}              &  78.1      & 78.3          &78.0    &78.0 &78.0  &78.0 &0.01 \\
    % \bottomrule%[1.2pt]
    \hline
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Effect of sampling during inference.
    }
    \label{tab:inference_samp}
\end{table}

% \begin{table}[htbp]
%     \small
%     \centering
%     \setlength\tabcolsep{5pt}%2pt 列宽
%     % \renewcommand\arraystretch{1.2} % 行高
%     \begin{tabular}{c|cccccc|c}
%     \toprule%[1.2pt]
%    $N_{s}$(Infer.) & 6 & 12 & 18 & 24 & 48  & 80& $\sigma^2$ \\
%     % \midrule%[1pt]
%     \hline
%     5°2\emph{cm}$\uparrow$              &  61.1     &\textbf{61.5}         & \underline{61.4}    &60.9  &60.1 &59.9&0.03   \\
%     5°5\emph{cm}$\uparrow$     &66.9        & \textbf{67.4}        &\underline{67.2}    &	66.5 &66.0 &65.8&0.03  \\
%     % \hline
%     10°2\emph{cm}$\uparrow$              &  78.0      & \underline{78.3}          &\textbf{78.5}    &77.8 &77.6  &76.7&0.02 \\
%     \bottomrule%[1.2pt]
%     \end{tabular}
%     \vspace{-0.2cm}
%     \caption{Effect of causal learning and knowledge distillation.
%     }
%     \vspace{-0.2cm}
%     \label{tab:inference_Ns}
% \end{table}



\end{document}
