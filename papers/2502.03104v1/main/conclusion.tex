\section{Conclusion and Future Work}
% This study has provided detailed answers to the three main 
% questions raised in the Introduction.
% \begin{enumerate}
%     \item We established that value-basedreward centering is fundamentally equivalent to centering the Bellman error.
%     \item  In the context of linear function approximation, the solution obtained from Bellman error centering converges to the fixed point of centered temporal difference learning.
%     \item Leveraging the concept of Bellman error centering, we developed centered temporal difference learning algorithms, namely on-policy CTD and off-policy CTDC.
%     \item We present convergence proofs for these algorithms under standard theoretical assumptions.
% \end{enumerate}
% \section{Conclusion and Future Work}  
% In this study, we explored the theoretical foundations of reward centering. 
% We proved its equivalence to value-based
%  reward centering and showed that, with linear function approximation, it converges to the 
%    centered TD fixpoint. Based on this, we proposed on-policy CTD and 
%  off-policy CTDC algorithms and established their convergence under standard assumptions.
This paper explores the principles of reward centering, interpreting value-based
reward centering as Bellman error centering. We derive the fixed-point solutions
under tabular value and linear function approximation. Additionally, we present
both on-policy and off-policy algorithms, along with proofs of their
convergence.


Future work includes, but is not limited to,

(1) extensions of the CTD and CTDC algorithms to learning for control, 
especially the episodic problems;

(2) extending Bellman error centering to the $\lambda$ returns;

(3) extending Bellman error centering to the emphatic TD \cite{sutton2016emphatic}
and its convergence analysis;

(4) extending Bellman error centering to policy gradient and actor-critic RL algorithms.
%(5) employ human-aligned large language models.


