\section{Background}
\label{preliminaries}
\subsection{Markov Decision Process}
Reinforcement learning agent keeps interactions with an environment. 
In each interaction $t$, it observes the state $s_t$,
 takes an action $a_t$ to influence the environment, and obtains
 an immediate reward $r_{t+1}$.
 Consider an infinite-horizon Markov Decision Process (MDP), defined by a tuple $\langle S,A,R,P
 \rangle$, where $S=\{1,2,\ldots,N\}$ is a finite set of states of the environment;  $A$
 is a finite set of actions of the agent; 
 $R:S\times A \times S \rightarrow \mathbb{R}$ is a bounded deterministic reward
 function; $P:S\times A\times S \rightarrow [0,1]$ is the transition
 probability distribution \cite{Sutton2018book}.  
 
 A policy is a mapping $\pi:S\times A \rightarrow [0,1]$. The goal of the
 agent is to find an optimal policy $\pi^*$ to maximize the expectation of a
 discounted cumulative rewards over a long period. 
 State value function $V^{\pi}(s)$ for a stationary policy $\pi$ is 
 defined as:  
 \begin{equation}
 V^{\pi}(s)=\mathbb{E}_{\pi}[\sum_{t=0}^{\infty} \gamma^t r_{t+1}|S_0=s],
 \label{valuefunction}
 \end{equation}
 where $\gamma\in (0,1)$
 is a discount factor.
 We have Bellman equation for each state:
 \begin{equation}
 V^{\pi}(s)=\mathbb{E}_{\pi}[ r+\gamma  V^{\pi}(s')],
 \end{equation}
 where $s'$ is the succesor state of state $s$.
 In vector form,
  \begin{equation}
  \begin{split}
 \bm{V}^{\pi}&=\bm{R}^{\pi}+\gamma \bm{\mathbb{P}}^{\pi} \bm{V}^{\pi}\\
 &\dot{=} \bm{\mathcal{T}}^{\pi}\bm{V}^{\pi},
 \end{split}
 \end{equation}
 where $R^{\pi}(s)=\sum_{a}\pi(a|s)\sum_{s'}P(s,a,s')R(s,a,s')$,
 transition probability matrix of states 
 $[\mathbb{P}^{\pi}]_{s,s'}=\sum_{a}\pi(a|s)\sum_{s'}P(s,a,s')$,
 and $\bm{\mathcal{T}}^{\pi}$ is Bellman prediction operator.
 $\bm{V}^{\pi}$ is the fixpoint solution of $\bm{V}=\bm{\mathcal{T}}^{\pi}\bm{V}$.
 
  To deal with large scale state space, 
  linear value function for state $s\in S$ is defined as:
\begin{equation}
 V_{{\bm{\theta}}}(s):= {{\bm{\theta}}}^{\top}{{\bm{\phi}}}(s) = \sum_{i=1}^{m}
\theta_i \phi_i(s),
\label{linearvaluefunction}
\end{equation}
 where ${{{\bm{\theta}}}}:=(\theta_1,\theta_2,\ldots,\theta_m)^{\top}\in
 \mathbb{R}^m$ is a parameter vector, 
 ${{\bm{\phi}}}:=(\phi_1,\phi_2,\ldots,\phi_m)^{\top}\in \mathbb{R}^m$ is a feature
 function defined on state space $S$, and $m\ll |S|$ is the feature size. 
 
However, in the parameter space, 
$\bm{V}_{{\bm{\theta}}}$ cannot be guaranteed to be equal to 
$\bm{\mathcal{T}}^{\pi}\bm{V}_{{\bm{\theta}}}$.
 We typically solve for the TD fixpoint \citep{sutton2008convergent,sutton2009fast}, as follows:
 \begin{equation}
  \bm{V}_{{\bm{\theta}}}=\bm{{\bm{\Pi}}}\bm{\mathcal{T}}^{\pi}\bm{V}_{{\bm{\theta}}},
 \end{equation}
 where projection ${\bm{\Pi}}={\bm{\Phi}}({\bm{\Phi}}^{\top}\textbf{D}{\bm{\Phi}})^{-1}{\bm{\Phi}}^{\top}\textbf{D}$, 
 $\textbf{D}$  is a diagonal matrix of distribution vector $\bm{d}$, each element $\bm{d}_{s}$
  represents the distribution of state $s$.
 In expectation form, it is equal to 
 \begin{equation}
 \mathbb{E}_{\pi}[\delta\bm{\phi}]=0,
 \end{equation}
 where TD error  $\delta=r+\gamma V_{\bm{\theta}}(s')-V_{\bm{\theta}}(s)$. 
 
 In on-policy learning, the target policy $\pi$ and the behavior policy $\mu$
  are the same,
  and the experience is sampled %from $[\mathbb{P}^{\pi}]_{s_t,s_{t+1}}$ 
  as $\langle s_t,a_t,r_{t+1},s_{t+1}\rangle$.
  The update rule of the on-policy TD learning \citep{sutton2016emphatic} is
  \begin{equation}
  \label{td(0)theta}
  \theta_{t+1} = \theta_{t}+\alpha_t\delta_t\bm{\phi}_t,
  \end{equation}
  where $\alpha_t\in (0,1)$ is a learning stepsize,
  and   $\delta_t=r_{t+1}+\gamma V_{\bm{\theta}}(s_{t+1})-V_{\bm{\theta}}(s_t)$.
  
 In off-policy learning, the target policy $\pi$ and the behavior policy $\mu$ are
 different. The update rule of the off-policy TD learning \citep{sutton2016emphatic} is
 \begin{equation}
  \bm{\theta}_{t+1} = \bm{\theta}_{t}+\alpha_t\rho_t\delta_t\bm{\phi}_t,
  \end{equation}
  where $\rho_t=\frac{\pi(a_t|s_t)}{\mu(a_t|s_t)}$.
 


%  \subsection{On-policy Learning and Off-policy Learning}
%  On-policy methods use the same policy for both interacting 
%  with the environment (behavior policy) and learning (target policy), 
%  while off-policy methods allow the behavior policy to differ from the 
%  target policy, enabling greater flexibility and sample efficiency.
% 
%  Typically, the linear TD algorithm, based on the semi-gradient 
%  of the mean squared Bellman error (MSBE) \cite{baird1995residual}, converges in on-policy settings but may diverge in 
%  off-policy cases. To address this, the TDC algorithm \cite{sutton2009fast}, derived from 
%  the mean squared projected Bellman error (MSPBE) \cite{sutton2009fast}, ensures convergence even in off-policy scenarios.
% 
%  \begin{align*}
%     \text{MSBE}({\bm{\bm{\theta}}})&= \| {\bm{{\bm{\Pi}}}} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}}-\bm{W}) \|_{\mu}^2\\
%     &=\mathbb{E}[(\mathbb{E}[\delta_t|S_t])^2].
% \end{align*}  
% \begin{align*}
%     \text{MSPBE}({\bm{\bm{\theta}}})&= \| {\bm{{\bm{\Pi}}}} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}}) \|_{\mu}^2\\
%     &= (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}})^\top {\bm{{\bm{\Pi}}}}^\top \mathbf{D} {\bm{{\bm{\Pi}}}} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}}) \\
%     &= (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}})^\top \mathbf{D} \bm{{\bm{\Phi}}} (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}})\\
%     &= \big(\bm{{\bm{\Phi}}}^\top \mathbf{D} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}})\big)^\top (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}}) \\
%     &= \mathbb{E}[\delta({\bm{\bm{\theta}}}) \bm{{\bm{\bm{\phi}}}}]^\top \mathbb{E}[\bm{{\bm{\bm{\phi}}}} \bm{{\bm{\bm{\phi}}}}^\top]^{-1} \mathbb{E}[\delta({\bm{\bm{\theta}}}) \bm{{\bm{\bm{\phi}}}}],
% \end{align*} 
% where 
% \begin{equation*}
%     \mathbb{E}[{\bm{\bm{\phi}}}{\bm{\bm{\phi}}}^\top] = \sum_s \mu(s) {\bm{\bm{\phi}}}(s) {\bm{\bm{\phi}}}(s)^\top = \bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}},
% \end{equation*}
% where $\mu$ is state-visitation probability distribution vector whose sth component, $\mu(s)$, 
% represents probability of visiting state s, $\mathbf{D}$ is a diagonal matrix whose s diagonal 
% element is $\mu(s)$, and $\bm{{\bm{\Pi}}}$ is projection operator,
% \begin{equation*}
%     \|v\|_\mu^2 = \sum_s \mu(s) v^2(s),
% \end{equation*}
% \begin{align*}
%     \mathbb{E}[\delta({\bm{\bm{\theta}}}){\bm{\bm{\phi}}}]&= \sum_s \mu(s) {\bm{\bm{\phi}}}(s) \big( R(s) + \gamma \sum_{s'} P_{ss'} V_{\bm{\bm{\theta}}}(s') - V_{\bm{\bm{\theta}}}(s)\big)\\
%     &= \bm{{\bm{\Phi}}}^\top \mathbf{D} (\bm{TV}_{\bm{{\bm{\bm{\theta}}}}} - \bm{V}_{\bm{{\bm{\bm{\theta}}}}}),
% \end{align*}
% \begin{align*}
%     {\bm{{\bm{\Pi}}}}^\top \mathbf{D} {\bm{{\bm{\Pi}}}} &= (\bm{{\bm{\Phi}}} (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D})^\top \mathbf{D} (\bm{{\bm{\Phi}}} (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D}) \\
%     &= \mathbf{D}^\top \bm{{\bm{\Phi}}} (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}} (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D} \\
%     &= \mathbf{D}^\top \bm{{\bm{\Phi}}} (\bm{{\bm{\Phi}}}^\top \mathbf{D} \bm{{\bm{\Phi}}})^{-1} \bm{{\bm{\Phi}}}^\top \mathbf{D}.
% \end{align*}

\subsection{Reward Centering}
In on-policy learning, \citet{naik2024reward} 
proposed simple reward centering.
The update rule is
\begin{equation}
V_{t+1}(s_t)=V_{t}(s_t)+\alpha_t \bar{\delta}_t,
\label{src1}
\end{equation}
where the new TD error $\bar{\delta}_t$ is
\begin{equation}
\bar{\delta}_t=\delta_t-\bar{r}_{t} = r_{t+1}-\bar{r}_{t}+\gamma V_{t}(s_{t+1})-V_t(s_t),
\label{src2}
\end{equation}
and $\bar{r}_{t}$ is updated as:
\begin{equation}
\bar{r}_{t+1}=\bar{r}_{t}+\beta_t (r_{t+1}-\bar{r}_{t}),
\label{src3}
\end{equation}
where $\beta_t\in (0,1)$ is a learning stepsize.

In off-policy learning, \citet{naik2024reward} 
proposed value-based reward centering.
The update rule is
\begin{equation}
  \label{rewardcentering1}
V_{t+1}(s_t)=V_{t}(s_t)+\alpha_t \rho_t \bar{\delta}_t,
\end{equation}
where $\bar{r}_{t}$ is updated as:
\begin{equation}
  \label{rewardcentering2}
\bar{r}_{t+1}=\bar{r}_{t}+\beta_t \rho_t\bar{\delta}_t.
\end{equation}

% \citet{naik2024reward} introduces two distinct methodologies: simple reward centering and value-based reward centering.
% The update rule for simple reward centering is:
% \begin{equation*}
%     \widetilde{V}_{t+1}^\gamma\left(s_t\right)\gets\widetilde{V}_{t}^\gamma\left(s_t\right)+\alpha_t \bar{\delta}_t,
%    \label{rewardcentering3}
% \end{equation*}
% \begin{equation}
%     \bar{r}_{t+1}\leftarrow\bar{r}_t+\beta_t(r_{t+1}-\bar{r}_t).
%    \label{rewardcentering4}
% \end{equation}
% The update rule for value-based reward centering is:
% \begin{equation}
%     \widetilde{V}_{t+1}^\gamma\left(s_t\right)\gets\widetilde{V}_{t}^\gamma\left(s_t\right)+\alpha_t\rho_t\bar{\delta}_t,
%    \label{rewardcentering1}
% \end{equation}
% \begin{equation}
%     \bar{r}_{t+1}\leftarrow\bar{r}_t+\eta\alpha_t\rho_t\bar{\delta}_t,
%    \label{rewardcentering2}
% \end{equation}
% where $\widetilde{V}_{t}^\gamma\left(s_t\right)$ is the estimate of the centered discounted value function for state, 
% $\bar{r}_t$ is the estimate of the average reward, 
% $\bar{\delta}_t=r_{t+1}-\bar{r}_t+\gamma\widetilde{V}_{t}^\gamma\left(s_{t+1}\right)-\widetilde{V}_{t}^\gamma\left(s_t\right)$ is the TD error for value-based reward centering, 
% $\rho_t=\frac{\pi(a_t|s_t)}{b(a_t|s_t)}$ is the importance sampling ratio, which is used to correct the difference between the behavior policy $b$ and the target policy $\pi$, 
% $\alpha_t$ is the step-size parameter, and 
% $\eta$ and $\beta_t$ are the step-size parameter for reward centering.
% For on-policy, $\rho_t=1$.
% The focus of this study is on linear estimation. The linear update rule corresponding to (\ref{rewardcentering1}) is as follows:
% \begin{equation}
%     \widetilde{\bm{\bm{\theta}}}_{t+1}\leftarrow\widetilde{\bm{\bm{\theta}}}_{t}+\alpha_t\rho_t\bar{\delta}_t\bm{\bm{\phi}}_t,
%    \label{rewardcentering3}
% \end{equation}
% where $\bar{\delta}_t=r_{t+1}-\bar{r}_t+\gamma\widetilde{\bm{\bm{\theta}}}_{t}^{\top}\phi_{t+1}-\widetilde{\bm{\bm{\theta}}}_{t}^{\top}\bm{\bm{\phi}}_{t}$.
% 
% As noted earlier, (\ref{rewardcentering4}) estimates the average reward, 
% while (\ref{rewardcentering2}) estimates the expectation of Bellman error. 
% Thus, the term "value-based reward centering" appears slightly 
% misleading.
% 



