\section{Experimental Studies}

This section aims to validates the convergence of the algorithms
in the policy evaluation experiments.
We employ  three policy evaluation experiments, 
including {Boyanchian}, {2-state off-policy counterexample} 
and {7-state off-policy counterexample}.
Details are shown in the appendix.
We compare the performance of 
the proposed CTD and CTDC algorithms with  the TD and TDC algorithms.

The vertical axis is unified as root mean squared centered Bellman error (RMSCBE).
Each experiment was trained for 50 iterations, with each 
episode of BoyanChain consisting of 1,000 steps and each 
episode of the other two experiments consisting of 2,000 
steps. We show the shaded regions  as the standard 
deviation (std).

Figure \ref{Boyanchian} shows the learning curses in Boyanchain.
This environment is an on-policy setting. 
All algorithms converge, with the CTD and CTDC 
algorithms exhibiting the fastest convergence and 
the best performance. The convergence of the CTD algorithm 
validates the theoretical proof presented earlier regarding 
its stable convergence under on-policy conditions.

Figure \ref{2statestep} and \ref{7statestep} 
show the learning curses in 2-state counterexample and 
7-state counterexample, respectively.
Both environments are off-policy settings. 
Off-policy TD diverges, while the other 
three algorithms converge.  
The convergence of the CTDC algorithm validates 
the theoretical proof presented earlier regarding 
its stable convergence under off-policy conditions.
In the 2-state counterexample environment, both the
off-policy CTD algorithm and the off-policy CTDC algorithm
 perform exceptionally well.

Figure \ref{2statestep} and \ref{7statestep} show that
 the off-policy TD algorithm diverges 
 whereas off-policy CTD converge stably. This result is 
particularly surprising because we have only proven that CTD converges 
stably in the on-policy setting.

Comparing Equation (\ref{theta}) and Equation (\ref{td(0)theta}), we find that $\omega$ in 
(\ref{theta}) is essentially an adjustment or correction of the TD update. We think 
the presence  of $\omega$ leads to stabilizing gradient estimation and reducing the variance
of gradient estimation.
\begin{figure}[H]
    \vskip 0.2in
    \begin{center}
    \subfigure[Boyanchian]{
        \includegraphics[width=\columnwidth, height=0.68\columnwidth]{main/pic/boyanchain/boyanchain_compare.pdf}
        \label{Boyanchian}
    }
    \subfigure[2-state counterexample]{
        \includegraphics[width=\columnwidth, height=0.68\columnwidth]{main/pic/2_state/2_state_compare.pdf}
        \label{2statestep}
    }
    \subfigure[7-state counterexample]{
        \includegraphics[width=\columnwidth, height=0.68\columnwidth]{main/pic/7_state/7_state_compare.pdf}
        \label{7statestep}
    }
        \caption{Learning curses of three evaluation environments.}
        \label{learningcurvesofbairdexample}
    \end{center}
    \vskip -0.2in
\end{figure}

One may wonder why the off-policy CTD algorithm converges in the 2-state counterexample.
 We provide the following analysis.
\begin{lemma}
    Consider a general two-state off-policy counterexample with state features  
    $
    \bm{\Phi} = (m, n)^{\top}$, where $m\neq 0$, $n\neq 0$, and $m \neq n$.
    The behavior policy is given by  
    $
    \textbf{P}_{\mu}=
    \begin{bmatrix}
    a & 1-a \\
    b & 1-b
    \end{bmatrix},
    $
    where \( 0 \leqslant a \leqslant 1 \) and \( 0 \leqslant b \leqslant 1 \).  
    The target policy is defined as  
    $
    \textbf{P}_{\pi}=
    \begin{bmatrix}
    x & 1-x \\
    y & 1-y
    \end{bmatrix},
    $
    where \( 0 \leqslant x \leqslant 1 \) and \( 0 \leqslant y \leqslant 1 \).  
%     The steady-state distribution under the behavior policy \(\textbf{P}_{\mu}\) is  
%     $
%     \bm{d}_{\mu} = (d,1-d)^{\top}.
%     $
   % Let the discount factor be \( \gamma \). 
    In this setting, $\textbf{A}_{\textbf{off-policy CTD}}$ is  positive definite.
     %matrix if and only if the feature representations of the two states are distinct, i.e., \( m \neq n \).
\end{lemma}
% During the research process, the following conclusions were drawn 
% : In the 2-state counterexample environment, the off-policy 
% CTD algorithm exhibits stable convergence when the features 
% of each state are distinct.
\begin{proof}
    \label{th3proof}   
%     Let the feature $\bm{\Phi}=(m,n)^{\top}$, where $m\neq n$. 
%     The behavior policy 
% $
% \textbf{P}_{\mu}=
% \begin{bmatrix}
% a & 1-a \\
% b & 1-b
% \end{bmatrix}
% $, where $0 \leqslant a \eqslantless 1$ and $0 \leqslant b \leqslant  1$.
% The target policy 
% $
% \textbf{P}_{\pi}=
% \begin{bmatrix}
% x & 1-x \\
% y & 1-y
% \end{bmatrix}
% $, where $0 \leqslant x \eqslantless 1$ and $0 \leqslant y \leqslant  1$.
% The state distribution of
% the policy $\textbf{P}_{\mu}$ is $\bm{d}_{\mu} =(d,1-d)^{\top}$.
% % The state distribution of
% % the second policy is $\bm{d}_1 =(0,1)^{\top}$.
% The discount factor is $\gamma$.

The stationary distribution $\bm{d}_{\mu}$ and 
$\textbf{P}_{\mu}$ satisfy the  invariance $\bm{d}_\mu^{\top}\mathbf{P}_\mu = \bm{d}_\mu^{\top}$. 
% \begin{align*}
%     \bm{d}_\mu^{\top} \mathbf{P}_\mu &= \bm{d}_\mu^{\top},\\
%     \begin{bmatrix} d & 1-d \end{bmatrix} 
%     \begin{bmatrix} a & 1-a \\ b & 1-b \end{bmatrix} 
%     &= \begin{bmatrix} d & 1-d \end{bmatrix},\\
%     \begin{bmatrix} ad+b(1-d) & d(1-a)+(1-d)(1-b) \end{bmatrix} &= \begin{bmatrix} d & 1-d \end{bmatrix},
% \end{align*}
Then, %$d=\frac{b}{1-a+b}$ and 
$\bm{d}_\mu =(\frac{b}{1-a+b},\frac{1-a}{1-a+b})^{\top}$.

Given that 
$\textbf{A}_{\textbf{off-policy CTD}}=\bm{\Phi^{\top}}(\textbf{D}-\bm{d}_{\mu}\bm{d}_{\mu}^{\top})(\bm{I}-\gamma\textbf{P}_{\pi})\bm{\Phi}$.
\begin{equation*}
    \textbf{D}-\bm{d}_{\mu}\bm{d}_{\mu}^{\top}=\frac{(1-a)b}{(1-a+b)^{2}}\begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix},
\end{equation*}
\begin{equation*}
    (\textbf{D}-\bm{d}_{\mu}\bm{d}_{\mu}^{\top})(\bm{I}-\gamma\textbf{P}_{\pi})=\frac{(1-a)b}{(1-a+b)^{2}}(1-x\gamma +y\gamma)\begin{bmatrix} 1 & -1 \\ -1 & 1 \end{bmatrix},
\end{equation*}
where, 
\begin{equation*}
    \textbf{A}_{\textbf{off-policy CTD}}=\frac{(1-a)b}{(1-a+b)^{2}}(1-x\gamma +y\gamma)(m-n)^2.
\end{equation*}
Since $m\neq n$, $\textbf{A}_{\textbf{off-policy CTD}}$ is a positive definite matrix.
\end{proof}
This is the reason why the off-policy CTD algorithm converges
 in the 2-state counterexample.
% When  matrix $\textbf{A}$ is positive
% definite, the algorithm converges. 
% So, CTD exhibits stable convergence in a 2-state counterexample when the feature values are different.
