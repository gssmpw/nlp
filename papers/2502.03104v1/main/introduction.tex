\section{Introduction}
\label{introduction}
Reinforcement learning (RL) has driven transformative advances 
in strategic decision-making (e.g., AlphaGo \citep{silver2016mastering}) and 
human-aligned large language systems 
(e.g., ChatGPT via RLHF \citep{ouyang2022training,carta2023grounding,dai2024safe,guo2025deepseek}). 
However, these breakthroughs incur prohibitive computational costs:
AlphaZero requires billions of environment interactions, while
 RL-based training of state-of-the-art language models demands
  millions of GPU hours \citep{patterson2021carbon}.
   Such challenges necessitate urgent 
   innovations in RL efficiency to enable scalable 
    AI development.
    
%  To address long-term and even continuous reinforcement learning problems, two
%  primary maximization objectives have been designed: average reward and
%  discounted reward. 
 
 To tackle long-term and continuous reinforcement learning problems,
  two primary maximization objectives have been proposed: 
  the average reward criterion and the discounted reward criterion.
 In the context of average reward, 
 \citet{schwartz1993reinforcement} introduced
 R-Learning, employing an adaptive method to estimate average rewards.
 \citet{das1999solving} proposed SMART, which focuses on 
 estimating average rewards  directly. 
 \citet{abounadi2001learning} introduced RVI Q-learning, utilizing the value of a
 reference state to enhance the learning process. 
 \citet{yang2016efficient} proposed
 CSV-learning, which employs a constant shifting values to improve convergence.
 \citet{wan2021learning} removed reference state of RVI Q-learning, proposed
 differential Q-learning and differential TD learning.

Regarding discounted rewards, \citet{perotto2018tuning,grand2024reducing}  highlighted that, under certain
conditions, such as with a large discount factor, Blackwell optimality can be
achieved. \citet{sun2022exploit} demonstrated that reward shifting can effectively
accelerate convergence in deep reinforcement learning. 
\citet{schneckenreither2025average} introduced near-Blackwell-optimal Average Reward Adjusted
Discounted Reinforcement Learning using Laurent Series expansion of the discounted
reward value function. 
\citet{naik2024reward,naik2024reinforcement} proposed the concept of reward centering,
designing simple reward centering and value-based reward centering, and proved
the convergence of tabular Q-learning with reward centering. 
Applying reward centering in tabular Q-learning, 
Q-learning with linear function approximation 
and Deep Q-Networks (DQN) have produced outstanding experimental 
results across all these approaches \citep{naik2024reward}.    
 
However, three issues remain unresolved:
(1) \citet{naik2024reward,naik2024reinforcement}
 pointed out that while reward centering can be combined with 
 other reinforcement learning (RL) algorithms, 
 the specific methods for such integration are 
 not straightforward or trivial. 
 The underlying mechanisms of reward centering warrant further investigation.
(2) Currently, there is only a convergence proof for tabular
 Q-learning with reward centering, 
 leaving the convergence properties of reward centering
  in large state spaces with function approximation still unknown.
(3) If the algorithm converges, what solution it will converge to
 is also an open question.

 In response to these three issues, 
 \textbf{the contributions of this paper} are as follows:
(1) We demonstrated that value-based reward centering is essentially Bellman
error centering. 
(2) Under linear function approximation,  the solution of
Bellman error centering converges to
the centered TD fixpoint.
(3) Building on Bellman error centering, we designed centered temporal difference
learning algorithms, referred to as on-policy CTD and off-policy CTDC, respectively.
(4) We provide convergence proofs under standard assumptions.  



