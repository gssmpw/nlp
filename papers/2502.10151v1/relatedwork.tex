\section{Related Work}
\label{sec:relatedwork}

%The retrieval of digitized documents is almost as old as the field of computing itself.As a result, the comprehensive list of related works is too large to discuss in full.
Information retrieval is a vast field. For conciseness, we only discuss the methods that are most closely related to the contribution of our work, namely: decentralized retrieval and retrieval based on machine learning techniques.
Hereby, we also omit related work such as database technology, which is necessary to store documents that require retrieval.
Our discussion of related work is centered around the following defining characteristics, also shown in \autoref{tab:existingwork}:

\begin{itemize}
   \item \textbf{Semantic Search}: whether documents can be retrieved from users by utilizing semantics, instead of a fixed unique key.
   \item \textbf{Predictive Cache}: whether network connections are constructed (in a network overlay) such that documents are likely to be available from existing connections.
   \item \textbf{Distributed}: whether documents are (sparsely) distributed among users that can form connections and retrieve documents from each other.
   \item \textbf{Training Requirement}: whether the solution must be trained on a data set in order to meet its performance targets of latency and accuracy, and must be retrained to update its index.
\end{itemize}

%\todo{This is hip and new. We do important stuff, even Google is doing these thing. However, nobody does decentralised. Fully self-organising is only a single workshop paper of related work.}

Early peer-to-peer search provided key-value matching from content items to users' search queries that consist of keywords.
For example, Napster allowed users to search their indexing servers for songs, based on song title~\cite{carlsson2001rise}, and
KaZaA would later replace servers with peers~\cite{liang2005kazaa}.
This was similar to the keyword-based search of early web page indexers like Google Search~\cite{brin1998anatomy}.
However, generally, to search for information that is sparsely available from different users, \textbf{distributed} search algorithms are required.
In some use-cases, distributed search algorithms are trivialized, when all information should be spread to all users.
For instance, in blockchain networks like Bitcoin~\cite{nakamoto2008bitcoin} and Ethereum~\cite{wood2014ethereum} information is spread to all users and they can use techniques like flooding or gossip~\cite{vansteen2018distributed}. In our work, we do not make assumptions about search being limited to keyword-based search or to trivial use-cases where all information is spread to all users.

Modern structured networks allow efficient search.
The essence of these networks is to \textbf{predict user searches} and connect users to caches of information.
These localized caches appear in most networking research that structures network topologies around its data, e.g., Content-Centric Networking and Information-Centric Networking~\cite{vasilakos2015information}, and Content-Oriented Networking and Data-Oriented Networking~\cite{cho2008content}.
Typically, a variant of the Publish-Subscribe~\cite{vansteen2018distributed} model is used, in which users subscribe to the information (cache) of a publisher.
In our work, we do not assume content comes from known publishers.

In distributed systems research, content is typically addressed by a pre-known hash.
These hashes allow networks to be structured as a tree, or Distributed Hash Table~\cite{vansteen2018distributed}.
For example, DHTs like Chord~\cite{stoica2003chord} or Kademlia~\cite{maymounkov2002kademlia} use these trees to group users based on the hashes of their user identifiers to the hashes of the data they want to search for.
Our work uses a similar approach for efficient search using users that self-organize in tree topologies.
However, we do not assume users know the hash of the data they are looking for, but instead \textbf{only know the semantics} of that data.
For example, a ``red car'' and not a ``Ferrari 250 GT California Spyder SWB (1959)''.

The idea of using semantics in peer-to-peer overlay networks for search stems from the early 2000's~\cite{tang2003peer}.
In their work on Semantic Overlay Networks (SONs), Tang et al. already point out the scalability limits of approaches that depend on (non-semantic) keywords in a DHT, like that of Li et al.~\cite{li2003feasibility}.
The idea of using ontology trees for efficient semantic search is now more than two decades old~\cite{crespo2004semantic}.
However, these works assumed that a reliable taxonomy is available to create such a tree: this is (still) not the case.

In lieu of a taxonomy, Latent Semantic Indexing (LSI) of data can be used.
These indices fulfill ``adjustable representational richness'', ``explicit representation of both terms and documents'', and ``computational tractability for large datasets''~\cite{deerwester1990indexing}, which we freely interpret as a dynamic, scalable and efficient data structure, such as a tree.
The matrix-based algorithm to fulfill the requirements of LSI by Deerwester et al.~\cite{deerwester1990indexing} would later be misconstrued by many citing works as ``the LSI algorithm'', when it really is only one of the many possible implementations.
That algorithm in particular, used singular value decomposition (SVD), which is one of the many techniques for word embedding~\cite{johnson2024detailed}.
Even so, considering semantic information, using Deerwester et al.'s algorithm, provides superior retrieval accuracy to faster word-frequency-based approaches like TF-IDF~\cite{zhang2011comparative}. 
Our work uses LLMs to construct a Latent Semantic Index using a tree.

More recent approaches to capture the semantics of words, or sequences of words, \textbf{require some form of training}.
For example, GloVe~\cite{pennington2014glove} uses on-line learning to create word vectors (embeddings) based on global word-word co-occurrence counts.
Moving beyond word embeddings, early models for sequence-to-sequence applications, like BERT~\cite{kenton2019bert}, were designed to incorporate positional encoding of words using attention, i.e., transformers~\cite{vaswani2017attention}.
Specifically, BERT was designed to handle NLP tasks and require minimal fine-tuning~\cite{kenton2019bert}.
Later models using transformers, LLMs, became better at more generalized tasks~\cite{min2023recent} (and arguably worse at NLP).
However, LLMs require much less computing resources~\cite{rostam2024achieving}, making them a good fit for consumer hardware.
Most importantly, pre-trained LLM models are widely available.
In our work, we leverage pre-trained models without further training.

A recent influential approach in non-distributed document retrieval based on embeddings is Differentiable Search Index (DSI) by Tay et al.~\cite{tay2022transformer}.
In their work, they train a (sequence-to-sequence) transformer model to retrieve documents.
The model uses its semantic understanding of a user query to retrieve a document identifier, which they refer to as ``Semantic String Docids''.
Hereby, the transformer model is used as a form of latent semantic indexing, though the authors do not mention this.
Furthermore, the paper does not touch on its applicability to a decentralized, or distributed, context.
In their De-DSI approach, Neague et al.~\cite{neague2024dsi} do consider such a context.
In their decentralized approach, they fine-tune a global network by exchanging documents with network peers.
However, efficiency at scale is not yet considered and the time complexity increases as the network grows~\cite{neague2024dsi}.
In our work, we use the semantic understanding of sequence-to-sequence models without any further training and our work does scale with the network size as we use a tree data structure.

%% Papers of interest:
%% https://doi.org/10.1016/j.ijar.2008.11.006 % Semantic hashing
%% https://arxiv.org/pdf/2310.08891 % EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval

%The first gossip learning algorithm from 2012 uses gradient descent and gossip of parameters. etc etc. \footnote{\url{https://arxiv.org/pdf/1109.1396}}.

The state-of-the-art Graph-Diffusion algorithm by Giatsoglou et al.~\cite{giatsoglou2022graph} uses graph diffusion with Personalized PageRank~\cite{gasteiger2018predict} to spread summarized representations of each node’s content across the network. Each node generates a single ``node embedding'' by aggregating embeddings of its local documents. This node embedding, representing the general content of the node, is diffused to neighboring nodes, which then continues to propagate these embeddings further across the network. Through this layered diffusion process, each node acquires generalized information about distant nodes’ content, enabling the decentralized network to guide queries toward relevant content even when the content is stored multiple hops away.
Our work makes use of these findings and uses diffusion, called ``expansion rounds'', to incrementally improve a node's connections to other nodes, to better fit its embedding.

The query forwarding process of Giatsoglou et al.~\cite{giatsoglou2022graph} is controlled by a time-to-live limit, restricting the number of hops a query can travel to prevent indefinite circulation. When a node receives a query, it calculates a relevance score between the query and the stored node embeddings of neighboring nodes, forwarding the query toward those with the highest scores. Experimental results show that this method effectively finds content within a few hops but encounters reduced accuracy as document density and search radius increase. This makes the method suitable for localized content search in P2P networks, though additional refinements would be needed for scaling to larger document volumes and wider search ranges.
We show that our work requires less hops to retrieve information and operates with larger document volumes.