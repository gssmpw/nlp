\section{Related Work}
\label{sec:relatedwork}

%The retrieval of digitized documents is almost as old as the field of computing itself.As a result, the comprehensive list of related works is too large to discuss in full.
Information retrieval is a vast field. For conciseness, we only discuss the methods that are most closely related to the contribution of our work, namely: decentralized retrieval and retrieval based on machine learning techniques.
Hereby, we also omit related work such as database technology, which is necessary to store documents that require retrieval.
Our discussion of related work is centered around the following defining characteristics, also shown in \autoref{tab:existingwork}:

\begin{itemize}
   \item \textbf{Semantic Search}: whether documents can be retrieved from users by utilizing semantics, instead of a fixed unique key.
   \item \textbf{Predictive Cache}: whether network connections are constructed (in a network overlay) such that documents are likely to be available from existing connections.
   \item \textbf{Distributed}: whether documents are (sparsely) distributed among users that can form connections and retrieve documents from each other.
   \item \textbf{Training Requirement}: whether the solution must be trained on a data set in order to meet its performance targets of latency and accuracy, and must be retrained to update its index.
\end{itemize}

%\todo{This is hip and new. We do important stuff, even Google is doing these thing. However, nobody does decentralised. Fully self-organising is only a single workshop paper of related work.}

Early peer-to-peer search provided key-value matching from content items to users' search queries that consist of keywords.
For example, Napster allowed users to search their indexing servers for songs, based on song title**Bennett**, "Content-Based Retrieval in Music Search"**, and
KaZaA would later replace servers with peers**Goldberg, et al., "Fast Looking Up by Proximity in Peer-to-Peer Systems"**.
This was similar to the keyword-based search of early web page indexers like Google Search**Brin, et al., "The Anatomy of a Large-Scale Hypertextual Web Search Engine"**.
However, generally, to search for information that is sparsely available from different users, \textbf{distributed} search algorithms are required.
In some use-cases, distributed search algorithms are trivialized, when all information should be spread to all users.
For instance, in blockchain networks like Bitcoin**Satoshi Nakamoto, "Bitcoin: A Peer-to-Peer Electronic Cash System"** and Ethereum**Wood, et al., "Ethereum: A Next-Generation Smart Contract and Decentralized Application Platform"**, information is spread to all users and they can use techniques like flooding or gossip**Jelasity, et al., "Gossip-Based Agregation in Large Dynamic Networks"**. In our work, we do not make assumptions about search being limited to keyword-based search or to trivial use-cases where all information is spread to all users.

Modern structured networks allow efficient search.
The essence of these networks is to \textbf{predict user searches} and connect users to caches of information.
These localized caches appear in most networking research that structures network topologies around its data, e.g., Content-Centric Networking and Information-Centric Networking**Papadopoulos, et al., "Information-Centric Networking: Evolving the Internet Paradigm"**, and Content-Oriented Networking and Data-Oriented Networking**Ahlgren, et al., "Content-oriented networks for efficient content delivery"**.
Typically, a variant of the Publish-Subscribe**Carzaniga, et al., "Designing distributed hypermedia systems with late binding" model is used, in which users subscribe to the information (cache) of a publisher.
In our work, we do not assume content comes from known publishers.

In distributed systems research, content is typically addressed by a pre-known hash.
These hashes allow networks to be structured as a tree, or Distributed Hash Table**Karger, et al., "Consistent hashing and random trees: Distributed caching protocols"**.
For example, DHTs like Chord**Stoica, et al., "Chord: A Scalable Peer-to-Peer Lookup Service" or Kademlia**Maymounkov, et al., "Kademlia: Decentralized structured data and routing" use these trees to group users based on the hashes of their user identifiers to the hashes of the data they want to search for.
Our work uses a similar approach for efficient search using users that self-organize in tree topologies.
However, we do not assume users know the hash of the data they are looking for, but instead \textbf{only know the semantics} of that data.
For example, a ``red car'' and not a ``Ferrari 250 GT California Spyder SWB (1959)''.

The idea of using semantics in peer-to-peer overlay networks for search stems from the early 2000's**Tang, et al., "Semantic Overlay Networks"**.
In their work on Semantic Overlay Networks (SONs), Tang et al. already point out the scalability limits of approaches that depend on (non-semantic) keywords in a DHT, like that of Li et al.**Li, et al., "A scalable distributed hash table"**.
The idea of using ontology trees for efficient semantic search is now more than two decades old**Koch, et al., "Ontology-based semantic search in peer-to-peer networks"**.
However, these works assumed that a reliable taxonomy is available to create such a tree: this is (still) not the case.

In lieu of a taxonomy, Latent Semantic Indexing (LSI) of data can be used.
These indices fulfill ``adjustable representational richness'', ``explicit representation of both terms and documents'', and ``computational tractability for large datasets''**Deerwester, et al., "Indexing by latent semantic analysis"**, which we freely interpret as a dynamic, scalable and efficient data structure, such as a tree.
The matrix-based algorithm to fulfill the requirements of LSI by Deerwester et al. would later be misconstrued by many citing works as ``the LSI algorithm'', when it really is only one of the many possible implementations.
That algorithm in particular, used singular value decomposition (SVD), which is one of the many techniques for word embedding**Bengio, et al., "A neural probabilistic language model"**.
Even so, considering semantic information, using Deerwester et al.'s algorithm, provides superior retrieval accuracy to faster word-frequency-based approaches like TF-IDF**Salton, et al., "A vector space model for automatic indexing"**. 
Our work uses LLMs to construct a Latent Semantic Index using a tree.

More recent approaches to capture the semantics of words, or sequences of words, \textbf{require some form of training}.
For example, GloVe**Pennington, et al., "GloVe: Global Vectors for Word Representation" uses on-line learning to create word vectors (embeddings) based on global word-word co-occurrence counts.
Moving beyond word embeddings, early models for sequence-to-sequence applications, like BERT**Devlin, et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" were designed to incorporate positional encoding of words using attention, i.e., transformers**Vaswani, et al., "Attention Is All You Need"**.
Specifically, BERT was designed to handle NLP tasks and require minimal fine-tuning**Zhang, et al., "Pre-training enables effective knowledge distillation for sequence-to-sequence models"**.
Later models using transformers, LLMs, became better at more generalized tasks**Brown, et al., "Language Models are Few-Shot Learners" (and arguably worse at NLP).
However, LLMs require much less computing resources**Shazeer, et al., "Prefix-Tuning: Optimizing Neurons for Transfer and Generalization in Transformers", making them a good fit for consumer hardware.
Most importantly, pre-trained LLM models are widely available.
In our work, we leverage pre-trained models without further training.

A recent influential approach in non-distributed document retrieval based on embeddings is Differentiable Search Index (DSI) by Tay et al.**Tay, et al., "Differentiable search index"**.
In their work, they train a (sequence-to-sequence) transformer model to retrieve documents.
The model uses its semantic understanding of a user query to retrieve a document identifier, which they refer to as ``Semantic String Docids''.
Hereby, the transformer model is used as a form of latent semantic indexing, though the authors do not mention this.
Furthermore, the paper does not touch on its applicability to a decentralized, or distributed, context.
In their De-DSI approach, Neague et al.**Neague, et al., "Decentralized Differentiable Search Index"** do consider such a context.
In their decentralized approach, they fine-tune a global network by exchanging documents with network peers.
However, efficiency at scale is not yet achieved and the number of hops to retrieve information is high.
In our work, we achieve less hops to retrieve information and operate with larger document volumes.

       