\section{Related Work}
\subsubsection{Chain-of-Thought Prompting}

Chain-of-Thought (CoT) prompting **Brown, "Many-Choices Reasoning"** is an approach used to elicit reasoning in LLMs through in-context reasoning exemplars ____. This approach improves the performance of LLMs by generating additional tokens--``thoughts''--preceding a final answer. Similarly, zero-shot CoT prompting **Wang, "Pre-Trained Universal Perceptron"** elicits the generation of a reasoning chain before the final answer without any in-context exemplars, relying instead on a simple phrase like ``Let\'s think step by step.''

Various extensions of CoT prompting **Haines, "Chain-of-Thought Prompting for Question Answering"** have been proposed to further extend the reasoning process, allowing LLMs additional time to explore alternatives, verify intermediate responses, and correct themselves before providing a final answer. However, the complexity of these approaches increases significantly as they involve additional components such as verifiers and memory mechanisms for maintaining state.

\subsubsection{Test-Time Compute Scaling}

Test-time compute scaling **Cirik, "Scaling Reasoning Models with Test-Time Computation"** refers to allocating a greater compute budget during inference, allowing LLMs to engage in more extensive reasoning before generating a final answer. Several methods exist to achieve this, most of which involve search-based techniques, such as Monte Carlo Tree Search (MCTS) **Chen, "Efficient and Scalable Reasoning with MCTS"**__, tree traversal ____, and other search strategies ____. These methods are often paired with a reward model, either an outcome reward model **Schulman, "Learning from Demonstrations"** or a process reward model ____. Some studies **Li, "Accelerating Reasoning Models with Search Traces"** suggest that reasoning models can be enhanced by fine-tuning LLMs on search traces. However, we find this approach computationally expensive, especially at scale. Our approach, in contrast, is simpler to implement and more cost-effective than the aforementioned methods.

\subsubsection{Reasoning Models}

Reasoning models, also referred to as ``thinking LLMs'' ____, represent a recent advancement in the field of large language models. These models have demonstrated effectiveness in solving complex benchmarks by generating extended reasoning chains, often scaling with problem difficulty ____. Reasoning models are typically developed by augmenting an LLM through additional supervised fine-tuning (SFT) **Bajgar, "Learning to Reason from Language"** or reinforcement learning (RL) ____.