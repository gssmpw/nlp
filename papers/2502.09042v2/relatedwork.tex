\section{Related Work}
\subsubsection{Chain-of-Thought Prompting}

Chain-of-Thought (CoT) prompting \citep{wei2022chain} is an approach used to elicit reasoning in LLMs through in-context reasoning exemplars \citep{NEURIPS2020_1457c0d6}. This approach improves the performance of LLMs by generating additional tokens--``thoughts''--preceding a final answer. Similarly, zero-shot CoT prompting \citep{kojima2022large} elicits the generation of a reasoning chain before the final answer without any in-context exemplars, relying instead on a simple phrase like ``Let\'s think step by step.''

Various extensions of CoT prompting \citep{yao2023tree, Besta_Blach_Kubicek_Gerstenberger_Podstawski_Gianinazzi_Gajda_Lehmann_Niewiadomski_Nyczyk_Hoefler_2024, zhou2023threadthoughtunravelingchaotic} have been proposed to further extend the reasoning process, allowing LLMs additional time to explore alternatives, verify intermediate responses, and correct themselves before providing a final answer. However, the complexity of these approaches increases significantly as they involve additional components such as verifiers and memory mechanisms for maintaining state.

\subsubsection{Test-Time Compute Scaling}

Test-time compute scaling \citep{snell2024scalingllmtesttimecompute} refers to allocating a greater compute budget during inference, allowing LLMs to engage in more extensive reasoning before generating a final answer. Several methods exist to achieve this, most of which involve search-based techniques, such as Monte Carlo Tree Search (MCTS) \citep{ding-etal-2024-everything,duan2025promptbasedmontecarlotree}, tree traversal \citep{yao2023tree,Besta_Blach_Kubicek_Gerstenberger_Podstawski_Gianinazzi_Gajda_Lehmann_Niewiadomski_Nyczyk_Hoefler_2024,bi2025forestofthoughtscalingtesttimecompute}, and other search strategies \citep{snell2024scalingllmtesttimecompute,wang2024openropensourceframework}. These methods are often paired with a reward model, either an outcome reward model \citep{beeching2024scalingtesttimecompute} or a process reward model \citep{snell2024scalingllmtesttimecompute}. Some studies \citep{qin2024o1replicationjourneystrategic,guan2025rstarmathsmallllmsmaster} suggest that reasoning models can be enhanced by fine-tuning LLMs on search traces. However, we find this approach computationally expensive, especially at scale. Our approach, in contrast, is simpler to implement and more cost-effective than the aforementioned methods.

\subsubsection{Reasoning Models}

Reasoning models, also referred to as ``thinking LLMs'' \citep{wu2024thinkingllmsgeneralinstruction}, represent a recent advancement in the field of large language models. These models have demonstrated effectiveness in solving complex benchmarks by generating extended reasoning chains, often scaling with problem difficulty \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}. Reasoning models are typically developed by augmenting an LLM through additional supervised fine-tuning (SFT) \citep{muennighoff2025s1simpletesttimescaling} or reinforcement learning (RL) \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}.

Nevertheless, many works \citep{skyt12025,bespokestratos,huang2024o1replicationjourney} have also demonstrated the effectiveness of knowledge distillation from a reasoning model. However, this distillation-based approach provides only a shortcut and requires access to an existing reasoning model. To gain a deeper understanding of reasoning models, we argue that developing such models from scratch--without leveraging an existing reasoning model--is a more effective approach. This study adopts that perspective to gain better insights into reasoning models.