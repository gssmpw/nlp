@article{Besta_Blach_Kubicek_Gerstenberger_Podstawski_Gianinazzi_Gajda_Lehmann_Niewiadomski_Nyczyk_Hoefler_2024,
	title        = {{Graph of Thoughts: Solving Elaborate Problems with Large Language Models}},
	author       = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	year         = 2024,
	month        = {Mar.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 38,
	number       = 16,
	pages        = {17682--17690},
	doi          = {10.1609/aaai.v38i16.29720},
	url          = {https://ojs.aaai.org/index.php/AAAI/article/view/29720},
	abstractnote = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (&quot;LLM thoughts&quot;) are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks}
}

@inproceedings{NEURIPS2020_1457c0d6,
	title        = {{Language Models are Few-Shot Learners}},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 33,
	pages        = {1877--1901},
	url          = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}

@misc{beeching2024scalingtesttimecompute,
	title        = {{Scaling test-time compute with open models}},
	author       = {Edward Beeching and Lewis Tunstall and Sasha Rush},
	url          = {https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute},
    year         = {2024}
}

@misc{bespokestratos,
	title        = {{Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation}},
	author       = {Bespoke Labs},
	year         = 2025,
	note         = {Accessed: 2025-01-22},
	howpublished = {www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation}
}

@misc{bi2025forestofthoughtscalingtesttimecompute,
	title        = {{Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning}},
	author       = {Zhenni Bi and Kai Han and Chuanjian Liu and Yehui Tang and Yunhe Wang},
	year         = 2025,
	url          = {https://arxiv.org/abs/2412.09078},
	eprint       = {2412.09078},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
	title        = {{DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}},
	author       = {DeepSeek-AI},
	year         = 2025,
	url          = {https://arxiv.org/abs/2501.12948},
	eprint       = {2501.12948},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@inproceedings{ding-etal-2024-everything,
	title        = {{Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation}},
	author       = {Ding, Ruomeng  and Zhang, Chaoyun  and Wang, Lu  and Xu, Yong  and Ma, Minghua  and Zhang, Wei  and Qin, Si  and Rajmohan, Saravan  and Lin, Qingwei  and Zhang, Dongmei},
	year         = 2024,
	month        = aug,
	booktitle    = {Findings of the Association for Computational Linguistics: ACL 2024},
	publisher    = {Association for Computational Linguistics},
	address      = {Bangkok, Thailand},
	pages        = {1638--1662},
	doi          = {10.18653/v1/2024.findings-acl.95},
	url          = {https://aclanthology.org/2024.findings-acl.95/},
	editor       = {Ku, Lun-Wei  and Martins, Andre  and Srikumar, Vivek},
}

@misc{duan2025promptbasedmontecarlotree,
	title        = {{Prompt-Based Monte Carlo Tree Search for Mitigating Hallucinations in Large Models}},
	author       = {Zhihua Duan and Jialin Wang},
	year         = 2025,
	url          = {https://arxiv.org/abs/2501.13942},
	eprint       = {2501.13942},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}

@misc{guan2025rstarmathsmallllmsmaster,
	title        = {{rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking}},
	author       = {Xinyu Guan and Li Lyna Zhang and Yifei Liu and Ning Shang and Youran Sun and Yi Zhu and Fan Yang and Mao Yang},
	year         = 2025,
	url          = {https://arxiv.org/abs/2501.04519},
	eprint       = {2501.04519},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{huang2024o1replicationjourney,
	title        = {{O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?}},
	author       = {Zhen Huang and Haoyang Zou and Xuefeng Li and Yixiu Liu and Yuxiang Zheng and Ethan Chern and Shijie Xia and Yiwei Qin and Weizhe Yuan and Pengfei Liu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2411.16489},
	eprint       = {2411.16489},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@inproceedings{kojima2022large,
	title        = {{Large Language Models are Zero-Shot Reasoners}},
	author       = {Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=e2TBb5y0yFf},
	editor       = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho}
}

@misc{muennighoff2025s1simpletesttimescaling,
	title        = {{s1: Simple test-time scaling}},
	author       = {Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Cand√®s and Tatsunori Hashimoto},
	year         = 2025,
	url          = {https://arxiv.org/abs/2501.19393},
	eprint       = {2501.19393},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@misc{qin2024o1replicationjourneystrategic,
	title        = {{O1 Replication Journey: A Strategic Progress Report -- Part 1}},
	author       = {Yiwei Qin and Xuefeng Li and Haoyang Zou and Yixiu Liu and Shijie Xia and Zhen Huang and Yixin Ye and Weizhe Yuan and Hector Liu and Yuanzhi Li and Pengfei Liu},
	year         = 2024,
	url          = {https://arxiv.org/abs/2410.18982},
	eprint       = {2410.18982},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}

@misc{skyt12025,
	title        = {{Sky-T1: Train your own O1 preview model within \$450}},
	author       = {NovaSky Team},
	year         = 2025,
	note         = {Accessed: 2025-01-09},
	howpublished = {https://novasky-ai.github.io/posts/sky-t1}
}

@misc{snell2024scalingllmtesttimecompute,
	title        = {{Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}},
	author       = {Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
	year         = 2024,
	url          = {https://arxiv.org/abs/2408.03314},
	eprint       = {2408.03314},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@misc{wang2024openropensourceframework,
	title        = {{OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models}},
	author       = {Jun Wang and Meng Fang and Ziyu Wan and Muning Wen and Jiachen Zhu and Anjie Liu and Ziqin Gong and Yan Song and Lei Chen and Lionel M. Ni and Linyi Yang and Ying Wen and Weinan Zhang},
	year         = 2024,
	url          = {https://arxiv.org/abs/2410.09671},
	eprint       = {2410.09671},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}

@inproceedings{wei2022chain,
	title        = {{Chain of Thought Prompting Elicits Reasoning in Large Language Models}},
	author       = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
	year         = 2022,
	booktitle    = {Advances in Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=_VjQlMeSB_J},
	editor       = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho}
}

@misc{wu2024thinkingllmsgeneralinstruction,
	title        = {{Thinking LLMs: General Instruction Following with Thought Generation}},
	author       = {Tianhao Wu and Janice Lan and Weizhe Yuan and Jiantao Jiao and Jason Weston and Sainbayar Sukhbaatar},
	year         = 2024,
	url          = {https://arxiv.org/abs/2410.10630},
	eprint       = {2410.10630},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

@inproceedings{yao2023tree,
	title        = {{Tree of Thoughts: Deliberate Problem Solving with Large Language Models}},
	author       = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan},
	year         = 2023,
	booktitle    = {Thirty-seventh Conference on Neural Information Processing Systems},
	url          = {https://openreview.net/forum?id=5Xc1ecxO1h}
}

@misc{zhou2023threadthoughtunravelingchaotic,
	title        = {{Thread of Thought Unraveling Chaotic Contexts}},
	author       = {Yucheng Zhou and Xiubo Geng and Tao Shen and Chongyang Tao and Guodong Long and Jian-Guang Lou and Jianbing Shen},
	year         = 2023,
	url          = {https://arxiv.org/abs/2311.08734},
	eprint       = {2311.08734},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}

