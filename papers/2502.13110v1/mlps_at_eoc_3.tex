\documentclass[twoside,11pt]{article}

\usepackage{jmlr2e}

\newtheorem{assumption}[theorem]{Assumption}
% Recommended, but optional, packages for figures and better typesetting:
%\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{adjustbox}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{affol2023} with \usepackage[nohyperref]{affol2023} above.
%\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
%\usepackage{bbold}
\usepackage{amsmath}
\usepackage{yhmath}
\usepackage{stmaryrd}
%\usepackage{amssymb}
%\usepackage{mathtools}
%\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{fillbetween}
\usepgfplotslibrary{groupplots}
\usetikzlibrary{calc}
\usepackage{makecell}
\usepackage{comment}
\usepackage{todonotes}
\usepackage{bbm}
\usepackage{enumerate}

% if you use cleveref..
%\usepackage[capitalize,noabbrev]{cleveref}

%Colors
\definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
\definecolor{mypink2}{RGB}{219, 48, 122}
\definecolor{mypink3}{cmyk}{0, 0.7808, 0.4429, 0.1412}
\definecolor{mygray}{gray}{0.6}

\definecolor{mycolor8}{rgb}{0, 0, 1}
\definecolor{mycolor7}{rgb}{0.15, 0.15, 0.9}
\definecolor{mycolor6}{rgb}{0.3, 0.3, 0.75}
\definecolor{mycolor5}{rgb}{0.45, 0.45, 0.6}
\definecolor{mycolor4}{rgb}{0.6, 0.6, 0.45}
\definecolor{mycolor3}{rgb}{0.75, 0.75, 0.3}
\definecolor{mycolor2}{rgb}{0.9, 0.9, 0.15}
\definecolor{mycolor1}{rgb}{1, 1, 0}

\definecolor{mycolor8_}{rgb}{1, 0, 0}
\definecolor{mycolor7_}{rgb}{0.9, 0.15, 0.15}
\definecolor{mycolor6_}{rgb}{0.75, 0.3, 0.3}
\definecolor{mycolor5_}{rgb}{0.6, 0.45, 0.45}
\definecolor{mycolor4_}{rgb}{0.45, 0.6, 0.6}
\definecolor{mycolor3_}{rgb}{0.3, 0.75, 0.75}
\definecolor{mycolor2_}{rgb}{0.15, 0.9, 0.9}
\definecolor{mycolor1_}{rgb}{0, 1, 1}

\definecolor{mycolor8__}{rgb}{0, 1, 0}
\definecolor{mycolor7__}{rgb}{0.15, 0.9, 0.15}
\definecolor{mycolor6__}{rgb}{0.3, 0.75, 0.3}
\definecolor{mycolor5__}{rgb}{0.45, 0.6, 0.45}
\definecolor{mycolor4__}{rgb}{0.6, 0.45, 0.6}
\definecolor{mycolor3__}{rgb}{0.75, 0.3, 0.75}
\definecolor{mycolor2__}{rgb}{0.9, 0.15, 0.9}
\definecolor{mycolor1__}{rgb}{1, 0, 1}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Ell}{\mathcal{L}}
\newcommand{\Haus}{\mathcal{H}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\dual}[1]{\widehat{\ #1 \ }}
\newcommand{\limiting}[1]{\overset{\scriptscriptstyle\infty}{#1}}

\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\cont}{cont}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\diam}{diam}
\DeclareMathOperator{\exterior}{ext}
\DeclareMathOperator{\relativeinterior}{relint}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\support}{supp}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator*{\bigcircop}{\bigcirc}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}

%\interfootnotelinepenalty=\@M


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{D\'avid Terj\'ek}

% Short headings should be running head and authors last names

\ShortHeadings{MLPs at the EOC: Dynamics of Feature Learning}{Terj\'ek}
\firstpageno{1}

\begin{document}

\title{MLPs at the EOC: Dynamics of Feature Learning}

\author{
  \name D\'avid Terj\'ek
  \email dterjek@renyi.hu \\
  \addr Alfr\'ed R\'enyi Institute of Mathematics \\ Budapest, Hungary
}

\editor{}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Since infinitely wide neural networks in the kernel regime are random feature models, the success of contemporary deep learning lies in the rich regime, where a satisfying theory should explain not only the convergence of gradient descent but the learning of features along the way. Such a theory should also cover phenomena observed by practicioners including the Edge of Stability (EOS) and the catapult mechanism. For a practically relevant theory in the limit, neural network parameterizations have to efficiently reproduce limiting behavior as width and depth are scaled up. While widthwise scaling is mostly settled, depthwise scaling is solved only at initialization by the Edge of Chaos (EOC). During training, scaling up depth is either done by inversely scaling the learning rate or adding residual connections. We propose $(1)$ the Normalized Update Parameterization ($\nu$P) to solve this issue by growing hidden layer sizes depthwise inducing the regularized evolution of preactivations, $(2)$ a hypothetical explanation for feature learning via the cosine of new and cumulative parameter updates and $(3)$ a geometry-aware learning rate schedule that is able to prolong the catapult phase indefinitely. We support our hypotheses and demonstrate the usefulness of $\nu$P and the learning rate schedule by empirical evidence.
\end{abstract}

%\begin{keywords}
%Rich Regime, Feature Learning, Edge of Stability, Catapult, Maximal Update Parametrization.
%\end{keywords}



\section{Introduction} \label{introduction}
Formally introduced in the celebrated work of \citet{Jacotetal2018}, the Neural Tangent Kernel (NTK) has been widely employed to analyze the problem of overparameterized learning. In particular, \citet{Duetal2018, Suetal2019, Oymaketal2019a, Aroraetal2019, Oymaketal2020, Songetal2021, Duetal2019, Zouetal2019, Nguyenetal2020, Nguyen2021, Liuetal2022} used the NTK to prove that training finite width neural networks with gradient descent converges globally by exploiting the lazy training phenomenon detailed in \citet{Chizatetal2019}, which entails that the NTK is almost constant during training.

\citet{Woodworthetal2020} identified the kernel and rich regimes of neural networks, with the parameterization of \citet{Jacotetal2018} being a prime example of a multilayer perceptron (MLP) belonging to the kernel regime, referred to as the Neural Tangent Parameterization (NTP) by \citet{Yangetal2021}. In the kernel regime, lazy training makes wide models behave as random feature models, while in the rich regime, this phenomenon is absent. \citet{Yangetal2021} showed that in the kernel regime limit, feature learning does not happen in the sense that hidden layer activations are constant during training and proposed the Maximal Update Parameterization ($\mu$P) that, being in the rich regime, does admit feature learning in the infinitely wide limit. Deviations of finite width networks in the kernel regime from the corresponding infinitely wide limits have been studied by \citet{Haninetal2018, Haninetal2020, Robertsetal2022}, arguing that these deviations can lead to feature learning. In contrast, the rich regime enables feature learning even in the infinitely wide limit as shown in \citet{Yangetal2021, Bordelonetal2022}. Unfortunately, while the convergence of gradient descent in the kernel regime is well understood, much less is known in the rich regime, where the NTK evolves during training in a nontrivial manner.

The maximum eigenvalue of the Hessian of the loss is referred to as the sharpness. It is well known that during gradient descent, if the learning rate $\eta>0$ is chosen such that sharpness does not exceed $\frac{2}{\eta}$, then the loss decreases monotonically under mild assumptions. \citet{Cohenetal2021} showed that when neural networks are trained using gradient descent, the progressive sharpening phenomenon is present in the sense that sharpness grows until reaching the Edge of Stability (EOS) regime. At the EOS, sharpness hovers above $\frac{2}{\eta}$ while the loss decreases over time in a non-monotonic manner. \citet{Damianetal2023} theoretically showed that in a simplified setting, gradient descent implicitly regularizes sharpness when it exceeds $\frac{2}{\eta}$, arguing that an analogous mechanism is behind the EOS. \citet{Lewkowyczetal2020} proposed a theory showing that in the kernel regime, when training with large learning rates such that sharpness exceeds $\frac{2}{\eta}$ already at initialization, a catapult phase can be observed instead of lazy training, inducing feature learning and reducing the sharpness below $\frac{2}{\eta}$, enabling the convergence of training after this initial phase. \citet{Nocietal2024} found that spectral properties of the Hessian such as the sharpness depend heavily on width in the kernel regime but are largely independent of width in the rich regime.

The study of infinitely deep neural networks led \citet{Pooleetal2016} to the discovery of the Edge of Chaos (EOC). \citet{Schoenholzetal2017} showed that the EOC is the regime where infinitely deep MLPs avoid both exploding and vanishing gradients. Scaling up the depth $l$ of neural networks in the rich regime has been studied in \citet{Jelassietal2023, Chenetal2024}, showing that the learning rate of MLPs has to be scaled by $l^{-\frac{3}{2}}$ in order for the change in preactivations during training to be bounded uniformly for large depths. This issue has been circumvented by \citet{Bordelonetal2024, Yangetal2024} by adding residual connections with multipliers depending on depth, ablating the need for depth dependent learning rates and showing that feature learning is viable in an infinitely wide and deep limit. \citet{Chizatetal2024} analyzed feature learning under gradient flow, starting from some desiderata and arriving at a variant of $\mu$P with learning rates scaled by $l^{-2}$ for all layers but the last, the initialization scale of the last layer scaled by $l^{\frac{1}{2}}$ and its learning rate by $l^{-1}$ for MLPs and a generalization of the parameterization in \citet{Bordelonetal2024, Yangetal2024} for residual networks. Studying the concentration of the NTK at initialization, \citet{mlpsateoc2} proposed an extension of $\mu$P to varying hidden layer sizes and showed that for quantities such as the cosines of hidden layer activations around their infinitely wide limits, the growth of the concentration error across depth can be made logarithmic by growing the hidden layer sizes quadratically.

The organization of the rest of the paper is as follows. After concluding \S~\ref{introduction} by listing our contributions in \S~\ref{contributions}, we collect notations and definitions in \S~\ref{preliminaries}. Then in \S~\ref{fl}, we detail our MLP parameterization in \S~\ref{mlp}, derive parameter space gradients in \S~\ref{gradients}, preactivations during training in \S~\ref{preac}, present our hypotheses concerning the dynamics of feature learning in \S~\ref{cos} and propose our learning rate schedule in \S~\ref{lrs}. We conclude by discussing the limitations of our work in \S~\ref{limitations} along with future directions.

\subsection{Contributions} \label{contributions}
We propose
\begin{itemize}
\item the Normalized Update Parameterization ($\nu$P), an MLP parameterization in the rich regime with regularized evolution of preactivations during training,
\item a hypothetical explanation for feature learning including progressive sharpening, the EOS and the catapult mechanism,
\item an easy-to-compute learning rate schedule aware of the local geometry of the loss landscape that enables prolonging the catapult phase until early stopping and
\item empirical results supporting our hypotheses and demonstrating the usefulness of $\nu$P and our learning rate schedule.
\end{itemize}

\section{Preliminaries}\label{preliminaries}

Given $i, j \in \N$, we define the tuple $[i:j] = (i,i+1,\cdots,j-1,j)$ (which is the empty tuple $()$ if $i > j$). For any $m, n \in \N$, we denote by $m\N+n$ the set $\{mr+n : r \in \N\}$. We denote by $\Vert \cdot \Vert$ the Euclidean and by $\Vert \cdot \Vert_\infty$ the max norm on $\R^n$. Let $G,H$ be Hilbert spaces. The space of bounded linear operators from $G$ to $H$ is denoted $\Ell(G,H)$ and we equip it with the operator norm $\Vert\cdot\Vert$. The adjoint of a linear operator $A \in \Ell(G,H)$ is the unique linear operator $A^* \in \Ell(H,G)$ such that $\langle A x_1, x_2 \rangle = \langle x_1, A^* x_2 \rangle$ for all $x_1 \in G$ and $x_2 \in H$. For Euclidean spaces $G=\R^m$, $H=\R^n$, we denote the space of $n \times m$ matrices $\R^{n \times m} = \Ell(H,G)$. For such matrices, we denote the Frobenius norm by $\Vert \cdot \Vert_F$. We denote by $\Id_n \in \R^{n \times n}$ the identity matrix on $\R^n$. We denote the tensor product of a pair of vectors $x,y \in \R^n$ by $x \otimes y = [ x_{i_1} y_{i_2} : i_1,i_2 \in [1:n]] \in \R^{n\times n}$. Given a matrix $A_1 \in \R^{n \times n}$ and a block matrix $A_2 = [ {A_2}_{i_1,i_2} \in \R^{m \times m} : i_1, i_2 \in [1:n]] \in \R^{nm \times nm}$, define their block Hadamard product $A_1 \boxcircle A_2 \in \R^{nm \times nm}$ as $A_1 \boxcircle A_2 = [ {A_1}_{i_1,i_2}{ A_2}_{i_1,i_2} \in \R^{m \times m} : i_1, i_2 \in [1:n]]$. For $n \in \N+1$, we denote the $n$-dimensional constant $1$ vector by $\mathbbm{1}_n = [ 1 : i \in [1:n]] \in \R^n$. Given $x \in \R^n$, we define the corresponding diagonal matrix $D_x \in \mathbb{S}^n$ as ${D_x}_{i_1,i_2} = x_i$ if $i_1 = i_2 = i$ and $0$ otherwise for all $i_1,i_2 \in [1:n]$. Given $m, n \in \N+1$ and $x \in \R^m$, we define the right multiplier operator $M_{x,n} \in \Ell(\R^{n \times m}, \R^n)$ as $M_{x,n} A = A x$ for all $A \in \R^{n \times m}$. Note that $\Vert M_{x,n} \Vert \leq \Vert x \Vert$ (i.e., the operator norm of $M_{x,n}$ is bounded by the Euclidean norm of $x$) and the adjoint $M_{x_1,n}^* \in \Ell(\R^n, \R^{n \times m})$ is given as $M_{x_1,n}^* x_2 = x_2 \otimes x_1$ for all $x_1 \in \R^m$ and $x_2 \in \R^n$, implying in particular that $M_{x_1,n} M_{x_2,n}^* = \langle x_1, x_2 \rangle \Id_n$ for all $x_1,x_2 \in \R^m$. We denote the cosine (similarity) of vectors $x_1,x_2 \in \R^n$ by $\cos(x_1,x_2) = \Vert x_1 \Vert^{-1} \Vert x_2 \Vert^{-1} \langle x_1, x_2 \rangle$ and the cosine (alignment) of matrices $A_1,A_2 \in \R^{n \times m}$ by $\cos(A_1,A_2) = \Vert A_1 \Vert_F^{-1} \Vert A_2 \Vert_F^{-1} \tr(A_1 A_2^*)$. The infinity and Lipschitz norms of real-valued functions are denoted by $\Vert \cdot \Vert_\infty$ and $\Vert \cdot \Vert_L$, respectively. Given a function $F:G \to H$, we say that it is differentiable if it is Fr\'echet differentiable, i.e., if there exists a bounded linear operator $\partial F(x) \in \Ell(G,H)$, which we refer to as the Jacobian of $F$ at $x$, satisfying $\lim_{y \to x }\frac{\Vert F(y) - F(x) - \partial F(x) (y - x) \Vert}{\Vert y - x \Vert}=0$.

\section{Dynamics of Feature Learning}\label{fl}

\subsection{A Multilayer Perceptron Parameterization}\label{mlp}

Let $l \in \N+2$ be the depth, $\R^{m_0}$ the input space and $\Theta = \Theta_{1:l} = \prod_{k=1}^l \Theta_k$ the parameter space with parameter subspaces $\Theta_k = \R^{m_k \times m_{k-1}}$, input dimension $m_0 \in \N+1$, output dimension $m_l \in \N+1$ and hidden layer widths $m_k = k^r m$ for $k \in [1:l-1]$ with width parameter $m \in \N+1$ and width growth exponent $r \in \N$. Denote parameters as $\theta = \theta_{1:l} = [A_k : k \in [1:l]] \in \Theta$ with layer matrices $A_k \in \Theta_k$. Let $q \in \R$ be the scaling exponent.

Finally, let $a,b \in \R$, let the activation function $\phi : \R \to \R$ be the $(a,b)$-ReLU defined below and initialize the matrices as $A_k \sim \mathcal{N}(0,\sigma^2 m^{-q} \Id_{m_k \times m_{k-1}})$ for $k \in [1:l]$ with $\sigma = (a^2 + b^2)^{-\frac{1}{2}}$ to ensure that the MLP is at the EOC via \citet[Lemma~3]{Hayouetal2019}.
\begin{definition}[$(a,b)$-ReLU]~\\
Given $a,b \in \R$, define the $(a,b)$-ReLU $\phi : \R \to \R$ for all $s \in \R$ as $\phi(s) = as + b\vert s \vert$.
\end{definition}
As $\phi'(s) = a + b \sgn(s)$ for all $s \in \R \setminus \{ 0 \}$, by abuse of notation we define $\phi'(s) = a + b \sgn(s)$ and refer to it as the derivative of $\phi$. Note that $\Vert \phi \Vert_L = \Vert \phi' \Vert_\infty = \vert a \vert + \vert b \vert$.

Define the activations $x_k : \R^{m_0} \times \Theta_{1:k-1} \to \R^{m_{k-1}}$ and the layers $N_k : \R^{m_0} \times \Theta_{1:k} \to \R^{m_k}$ for $k \in [1:l]$ recursively as $x_1(x) = x$, $x_k(x, \theta_{1 : k-1}) = m_{k-1}^{-\frac{1}{2}} \phi( m^{\frac{q}{2}} N_{k-1}(x, \theta_{1 : k-1}) )$ for $k \in [2:l]$ and $N_k(x,\theta_{1:k}) = A_k x_k(x,\theta_{1:k-1})$ for $k \in [1:l]$, respectively and define the MLP $N : \R^{m_0} \times \Theta \to \R^{m_l}$ as $N = N_l$.

The naming below is going to be justified in \S~\ref{preac}.
\begin{definition}[Normalized Update Parameterization ($\nu$P)]~\\
Given $l \in \N+2$, $m,m_0,m_l \in \N+1$, $r \in \N$ and $a,b \in \R$, we refer to the MLP parameterization detailed above with $q=1$ as the Normalized Update Parameterization or $\nu$P.
\end{definition}

\subsection{Gradients in Parameter Space}\label{gradients}

Define the activation derivatives as $x_k'(x,\theta_{1 : k-1}) = m_{k-1}^{-\frac{1}{2}} \phi'( m^{\frac{q}{2}} N_{k-1}(x,\theta_{1 : k-1}) ) \in \R^{m_{k-1}}$ for $k \in [2:l]$. The Jacobian $\partial_\theta N(x,\theta) = [\partial_{\theta_k} N(x,\theta) : k \in [1:l]] \in \Ell(\Theta,\R^{m_l})$ consists of the parameterwise Jacobians $\partial_{\theta_k} N(x,\theta) \in \Ell(\Theta_k, \R^{m_l})$, given as 
\[
\partial_{\theta_k} N(x,\theta) = A_l D_{x_l'(x, \theta_{1:l-1})} m^{\frac{q}{2}} A_{l-1} \cdots D_{x_{k+1}'(x, \theta_{1:k})} m^{\frac{q}{2}} M_{x_k(x, \theta_{1:k-1}),m_k}
\]
for $k \in [1:l-1]$ and $\partial_{\theta_l} N(x,\theta) = M_{x_l(x, \theta_{1:l-1}),m_l}$. For a loss function $\ell : \R^{m_0} \times \R^{m_l} \to \R$, an output space gradient $\nabla \ell(x,N(x,\theta)) \in \R^{m_l}$ (where $\nabla \ell(x,z)$ always denotes the gradient with respect to $z$) is pulled back via the adjoint ${\partial_{\theta_k} N(x,\theta)}^* \in \Ell(\R^{m_l}, \Theta_k)$ to a parameter subspace gradient ${\partial_{\theta_k} N(x,\theta)}^* \nabla \ell(x,N(x,\theta)) \in \Theta_k$. Letting $b_k(x,\theta) = \nabla_{N_k(x,\theta_{1:k})} \ell(x,N(x,\theta)) \in \R^{m_k}$ for $k \in [1:l]$, note that we have
\[
b_k(x,\theta) = D_{x_{k+1}'(x, \theta_{1:k})} m^{\frac{q}{2}} A_{k+1}^* b_{k+1}(x,\theta)
\]
for $k \in [1:l-1]$ and $b_l(x,\theta) = \nabla \ell(x,N(x,\theta))$. With this in hand, we can write
\[
{\partial_{\theta_k} N(x,\theta)}^* \nabla \ell(x,N(x,\theta)) = b_k(x,\theta) \otimes x_k(x, \theta_{1:k-1}).
\]

For a data distribution $\mu \in \mathcal{P}(\R^{m_0})$, denote by $N_\mu(\theta) \in L^2(\mu,\R^{m_l})$ the equivalence class of $N(\cdot,\theta)$ in $L^2(\mu,\R^{m_l})$ and define the loss functional $\ell_\mu : L^2(\mu,\R^{m_l}) \to \R$ as $\ell_\mu(z) = \int \ell(x,z(x)) d\mu(x)$ for all $z \in L^2(\mu,\R^{m_l})$. Then the parameter subspace gradients $\nabla_{k,\mu}(\theta) = \nabla_{\theta_k} \ell_\mu(N_\mu(\theta)) \in \Theta_k$ for $k \in [1:l]$ can be written as
\[
\nabla_{k,\mu}(\theta) = \int {\partial_{\theta_k} N(x,\theta)}^* \nabla \ell(x,N(x,\theta)) d\mu(x) = \int b_k(x,\theta) \otimes x_k(x, \theta_{1:k-1}) d\mu(x).
\]
Given $k \in [1:l]$, defining $X_{k,\mu}(\theta_{1:k-1}) \in \Ell(\R^{m_{k-1}}, L^2(\mu))$ as
\[
(X_{k,\mu}(\theta_{1:k-1}) z) (x) = \langle x_k(x, \theta_{1:k-1}), z \rangle
\]
for all $z \in \R^{m_{k-1}}$ and $\mu$-a.e. $x \in \R^{m_0}$ and $B_{k,\mu}(\theta) \in \Ell(\R^{m_k}, L^2(\mu))$ as
\[
(B_{k,\mu}(\theta) z)(x) = \langle b_k(x,\theta), z \rangle
\]
for all $z \in \R^{m_k}$ and $\mu$-a.e. $x \in \R^{m_0}$, we can write the parameter subspace gradients as
\[
\nabla_{k,\mu}(\theta) = B_{k,\mu}(\theta)^* X_{k,\mu}(\theta_{1:k-1}).
\]
If $\mu = \frac{1}{n} \sum_{i=1}^n \delta_{x_i}$ for a finite set of data $\{ x_i : i \in [1:n] \} \subset \R^{m_0}$, then $L^2(\mu) \cong \R^n$,
\[
\nabla_{k,\mu}(\theta) = \frac{1}{n} \sum_{i=1}^n {\partial_{\theta_k} N(x_i,\theta)}^* \nabla \ell(x_i,N(x_i,\theta)) = \frac{1}{n} \sum_{i=1}^n b_k(x_i,\theta) \otimes x_k(x_i, \theta_{1:k-1}) \in \Theta_k
\]
and we have the matrix representations $X_{k,\mu}(\theta_{1:k-1}) = [\frac{1}{\sqrt{n}} x_k(x_i, \theta_{1:k-1}) : i \in [1:n]] \in \R^{n \times m_{k-1}}$ and $B_{k,\mu}(\theta) = [\frac{1}{\sqrt{n}} b_k(x_i,\theta) : i \in [1:n]] \in \R^{n \times m_k}$. 

\subsection{Evolution of Preactivations}\label{preac}

Let $\theta_0 = [A_{k,0} : k \in [1:l]] \in \Theta$ be the initial parameter and perform gradient descent on the data stream $\mu_t \in \mathcal{P}(\R^{m_0})$ for $t \in \N$ with learning rate schedule $\eta_{k,t} \geq 0$ for $k \in [1:l]$ and $t \in \N$ by letting $\theta_{t+1} = \theta_t - [\eta_{k,t} \nabla_{k,t} : k \in [1:l]]$ for $t \in \N+1$ with $\nabla_{k,t} = \nabla_{k,\mu_t}(\theta_t)$. Denoting the cumulative subparameter updates $\Delta_{k,t} = \sum_{t'=0}^{t-1} \eta_{k,t'} \nabla_{k,t'} \in \Theta_k$ for $k \in [1:l]$ and $t \in \N$, we then have $\theta_t = [A_{k,0} - \Delta_{k,t} : k \in [1:l]]$. Now note that for $k \in [1:l-1]$, $t \in \N$ and $j \in [1:m_k]$ we have
\[
{\nabla_{k,t}}_j = m_k^{-\frac{1}{2}} \int \phi'\left( \left\langle m^{\frac{q}{2}} {A_{k,t}}_j, x_k(x, {\theta_t}_{1:k-1}) \right\rangle \right) \left\langle m^{\frac{q}{2}} {A_{k+1,t}^*}_j, b_{k+1}(x, \theta_t) \right\rangle x_k(x, {\theta_t}_{1:k-1}) d\mu_t(x)
\]
and
\[
{\nabla_{k+1,t}^*}_j
= m_k^{-\frac{1}{2}} \int \phi\left( \left\langle m^{\frac{q}{2}} {A_{k,t}}_j, x_k(x, {\theta_t}_{1:k-1}) \right\rangle \right) b_{k+1}(x, \theta_t) d\mu_t(x).
\]
Denoting the preactivations $z_{k,t,j}(x) = m^{\frac{q}{2}} \langle {A_{k,t}}_j, x_k(x, {\theta_t}_{1:k-1}) \rangle$ for $k \in [1:l-1]$, $t \in \N$, $j \in [1:m_k]$ and $x \in \R^{m_0}$, we have
\[
z_{k,t,j}(x)
= \left\langle m^{\frac{q}{2}} {A_{k,0}}_j, x_k(x, {\theta_t}_{1:k-1}) \right\rangle - \sum_{t'=0}^{t-1} \eta_{k,t'} \left\langle m^{\frac{q}{2}} {\nabla_{k,t'}}_j, x_k(x, {\theta_t}_{1:k-1}) \right\rangle,
\]
\begin{multline*}
\left\langle m^{\frac{q}{2}} {\nabla_{k,t'}}_j, x_k(x, {\theta_t}_{1:k-1}) \right\rangle 
= m^{-\frac{1}{2}(1-q)} k^{-\frac{r}{2}} \int \phi'(z_{k,t',j}(x')) \\
\left\langle m^{\frac{q}{2}} {A_{k+1,t'}^*}_j, b_{k+1}(x', \theta_{t'}) \right\rangle \langle x_k(x', {\theta_{t'}}_{1:k-1}), x_k(x, {\theta_t}_{1:k-1}) \rangle d\mu_{t'}(x')
\end{multline*}
and
\begin{multline*}
\left\langle m^{\frac{q}{2}} {A_{k+1,t'}^*}_j, b_{k+1}(x', \theta_{t'}) \right\rangle 
= \left\langle m^{\frac{q}{2}} {A_{k+1,0}^*}_j, b_{k+1}(x', \theta_{t'}) \right\rangle \\
- m^{-\frac{1}{2}(1-q)} k^{-\frac{r}{2}} \sum_{t''=0}^{t'-1} \eta_{k+1,t''} \int \phi(z_{k,t'',j}(x'')) \langle b_{k+1}(x'', \theta_{t''}), b_{k+1}(x', \theta_{t'}) \rangle d\mu_{t''}(x''),
\end{multline*}
so that
\begin{multline*}
z_{k,t,j}(x)
= \left\langle m^{\frac{q}{2}} {A_{k,0}}_j, x_k(x, {\theta_t}_{1:k-1}) \right\rangle 
- m^{-\frac{1}{2}(1-q)} k^{-\frac{r}{2}} \sum_{t'=0}^{t-1} \eta_{k,t'} \int \phi'(z_{k,t',j}(x')) \\
\langle x_k(x', {\theta_{t'}}_{1:k-1}), x_k(x, {\theta_t}_{1:k-1}) \rangle \left\langle m^{\frac{q}{2}} {A_{k+1,0}^*}_j, b_{k+1}(x',\theta_{t'}) \right\rangle d\mu_{t'}(x') \\
+ m^{-(1-q)} k^{-r} \sum_{t'=0}^{t-1} \sum_{t''=0}^{t'-1} \eta_{k,t'} \eta_{k+1,t''} \int \int \phi'(z_{k,t',j}(x')) \phi(z_{k,t'',j}(x'')) \\
\langle x_k(x', {\theta_{t'}}_{1:k-1}), x_k(x, {\theta_t}_{1:k-1}) \rangle \langle b_{k+1}(x'',\theta_{t''}), b_{k+1}(x',\theta_{t'}) \rangle d\mu_{t''}(x'') d\mu_{t'}(x').
\end{multline*}
Given $k \in [1:l-1]$, $t \in \N$ and $x \in \R^{m_0}$, the preactivations $z_{k,t,j}(x)$ are i.i.d. for $j \in [1:m_k]$. In the limit $m \to \infty$, if $q < 1$ then the preactivations are constant during training, if $q > 1$ then the preactivations blow up in the first gradient descent step and if $q=1$ then the preactivations evolve, enabling feature learning. Considering the kernel regime $q<1$ ordered and the unstable regime $q>1$ chaotic, the rich regime $q=1$ becomes the edge of chaos. In the following, we focus on the $q=1$ setting. At initialization, the width growth exponent $r$ affects the strength of concentration but has no influence on the infinitely wide limit of the NTK as shown in \citet{mlpsateoc2}. In contrast, during training $r$ has a regularizing effect due to the multipliers $k^{-\frac{r}{2}}$ and $k^{-r}$ above, showing that the hidden layer width pattern influences behavior even in the infinitely wide limit as increasing $r$ dampens the evolution of preactivations with stronger dampening for deeper layers. This property justifies the name Normalized Update Parameterization. We expect that the strength of concentration of certain quantities during training benefits from increasing $r$, similarly to what has been proved in \citet{mlpsateoc2} at initialization.

\subsection{Two Cosines}\label{cos}

Assume now that $\mu_t = \frac{1}{n} \sum_{i=1}^n \delta_{x_{t,i}}$ with $\{ x_{t,i} : i \in [1:n] \} \subset \R^{m_0}$ for $t \in \N$. This is the case for full batch gradient descent over a dataset $\{ x_i : i \in [1:n] \} \subset \R^{m_0}$ if $x_{t,i} = x_i$ for all $t \in \N$ and $i \in [1:n]$ and for SGD if the minibatches $\mu_t$ are i.i.d. as $x_{t,1},\cdots,x_{t,n} \sim \mu^{\otimes n}$ for some data distribution $\mu \in \mathcal{P}(\R^{m_0})$. Note that the sole purpose of this assumption is simplicity as the following discussion works for any $\mu_t \in \mathcal{P}(\R^{m_0})$ by considering $\Ell(L^2(\mu_t),L^2(\mu_t))$ equipped with the Hilbert-Schmidt norm $\Vert \cdot \Vert_{HS}$ instead of $\R^{n \times n}$ equipped with the Frobenius norm $\Vert \cdot \Vert_F$.

Denoting the forward and backward Gram matrices $X_{k,t} = X_{k,\mu_t}({\theta_t}_{1:k-1}) X_{k,\mu_t}({\theta_t}_{1:k-1})^* \in \R^{n \times n}$ and $B_{k,t} = B_{k,\mu_t}(\theta_t) B_{k,\mu_t}(\theta_t)^* \in \R^{n \times n}$ for $k \in [1:l]$ and $t \in \N$, note that
\[
\Vert \nabla_{k,t} \Vert_F^2
= \tr(\nabla_{k,t}^* \nabla_{k,t}) 
= \tr(X_{k,t} B_{k,t}) 
= \Vert X_{k,t} \Vert_F \Vert B_{k,t} \Vert_F \cos(X_{k,t}, B_{k,t}),
\]
so that the squared Frobenius norm of the gradient $\nabla_{k,t}$ is determined by the Frobenius norms of the forward and backward Gram matrices $X_{k,t}, B_{k,t}$ and their cosine alignment, which is always nonnegative as the Gram matrices are positive semidefinite.

Expanding the squared Frobenius norm of $\Delta_{k,t}$ as
\[
\Vert \Delta_{k,t} \Vert_F^2
= \sum_{t_1=0}^{t-1} \sum_{t_2=0}^{t-1} \eta_{k,t_1} \eta_{k,t_2} \tr(\nabla_{k,t_1}^* \nabla_{k,t_2})
\]
and noting that
\begin{multline*}
\tr(\nabla_{k,t_1}^* \nabla_{k,t_2})
= \Vert \nabla_{k,t_1} \Vert_F \Vert \nabla_{k,t_2} \Vert_F \cos(\nabla_{k,t_1}, \nabla_{k,t_2}) \\
= \Vert X_{k,t_1} \Vert_F^{\frac{1}{2}} \Vert B_{k,t_1} \Vert_F^{\frac{1}{2}} \Vert X_{k,t_2} \Vert_F^{\frac{1}{2}} \Vert B_{k,t_2} \Vert_F^{\frac{1}{2}}
\cos(X_{k,t_1}, B_{k,t_1})^{\frac{1}{2}} \cos(X_{k,t_2}, B_{k,t_2})^{\frac{1}{2}} \cos(\nabla_{k,t_1}, \nabla_{k,t_2}),
\end{multline*}
we see that the squared Frobenius norm of the cumulative updates is determined by the Frobenius norms of preceding forward and backward Gram matrices, their cosine alignments and the cosine alignments of preceding gradients.
Having
\begin{multline*}
\Vert \Delta_{k,t+1} \Vert_F^2 - \Vert \Delta_{k,t} \Vert_F^2
= \eta_{k,t}^2 \Vert \nabla_{k,t} \Vert_F^2 + 2 \eta_{k,t} \tr(\Delta_{k,t}^* \nabla_{k,t}) \\
= \eta_{k,t}^2 \Vert X_{k,t} \Vert_F \Vert B_{k,t} \Vert_F \cos(X_{k,t}, B_{k,t}) + 2 \eta_{k,t} \Vert \Delta_{k,t} \Vert_F \Vert \nabla_{k,t} \Vert_F \cos(\Delta_{k,t}, \nabla_{k,t})
\end{multline*}
shows that the cosine $\cos(\Delta_{k,t}, \nabla_{k,t})$ can make the squared Frobenius norm of the cumulative updates stagnate or even decrease, as $\cos(\Delta_{k,t}, \nabla_{k,t})$ can take negative values unlike $\cos(X_{k,t}, B_{k,t})$, resulting in a form of implicit regularization.

We argue that the above cosines quantify the information content of $\nabla_{k,t}$ via $\cos(X_{k,t}, B_{k,t})$ and its relationship with the information that has already been stored in $\Delta_{k,t}$ via $\cos(\Delta_{k,t}, \nabla_{k,t})$. Informally, having $\cos(\Delta_{k,t}, \nabla_{k,t})$ close to $1$ means that the information in the update emphasizes known information which can lead to overfitting, having $\cos(\Delta_{k,t}, \nabla_{k,t})$ close to $-1$ means that some of what has been learned is going to be unlearned which can lead to catastrophic forgetting, while having $\cos(\Delta_{k,t}, \nabla_{k,t})$ close to $0$ means that the update contains novel information.

We hypothesize that in the context of the EOS, $\cos(\Delta_{k,t}, \nabla_{k,t})$ close to $1$ is responsible for progressive sharpening, pushing the MLP to the EOS. At the EOS, $\cos(\Delta_{k,t}, \nabla_{k,t})$ then gets close to $0$, eventually stabilizing the training. The informal reason is that with small jumps in parameter space, consecutive gradients tend to point in similar directions, while with large jumps they tend to be orthogonal. While the sharpness $S_t = \Vert \partial_\theta \nabla_\theta \ell_t \Vert$ with $\ell_t = \ell_{\mu_t}(N_{\mu_t}(\theta_t))$ is hard to compute as it is the maximum eigenvalue of the Hessian, a widely used proxy is $\Vert K_t \Vert$, the maximum eigenvalue of the NTK $K_t \in \R^{n m_l \times n m_l}$ at $\theta_t$ with respect to $\mu_t$ defined as
\[
K_t = \left[ \frac{1}{n} \partial_\theta N(x_{t,i_1},\theta_t) {\partial_\theta N(x_{t,i_2},\theta_t)}^* : i_1,i_2 \in [1:n] \right].
\]
We argue that if $\cos(\Delta_{k,t}, \nabla_{k,t})$ is close to $1$, the norms of the activations are growing, while if $\cos(\Delta_{k,t}, \nabla_{k,t})$ is close to $0$, the updates push the hidden representations towards orthogonality. These respectively increase and decrease the maximum eigenvalue of the NTK since it can be written in terms of the forward Gram matrices $X_{k,t}$ as the sum of block Hadamard products $K_t = \sum_{k=1}^l X_{k,t} \boxcircle \hat{B}_{k,t}$ with $\hat{B}_{k,t} \in \R^{nm_l \times nm_l}$ defined as
\begin{multline*}
\hat{B}_{k,t} = \left[ A_l D_{x_l'(x_{t,i_1},{\theta_t}_{1:l-1})} A_{l-1} \cdots A_{k+1} D_{x_{k+1}'(x_{t,i_1},{\theta_t}_{1:k})} 
\right. \\ \left.
D_{x_{k+1}'(x_{t,i_2},{\theta_t}_{1:k})} A_{k+1}^* \cdots A_{l-1}^* D_{x_l'(x_{t,i_2},{\theta_t}_{1:l-1})} A_l^* : i_1,i_2 \in [1:n] \right]
\end{multline*}
for $k \in [1:l-1]$ and $\hat{B}_{l,t} = [\Id_{m_l} : i_1,i_2 \in [1:n]]$ by \citet[Proposition~6]{mlpsateoc2} to which the block Schur product theorem of \citet[Theorem~3.1]{Hornetal1992} applies. Orthogonality of the hidden representations can be quantified e.g. via $\Vert P_{k,t} \Vert_F$ with $P_{k,t} = [\frac{1}{n} \cos(x_k(x_{t,i_1},{\theta_t}_{1:k-1}),x_k(x_{t,i_2},{\theta_t}_{1:k-1})) : i_1,i_2 \in [1:n]]$, since $\Vert P_{k,t} \Vert_F=1$ for parallel and $\Vert P_{k,t} \Vert_F = n^{-\frac{1}{2}}$ for orthogonal activations.

Empirical evidence supporting the above arguments is shown in Figure~\ref{fig:plot_eos}. We can see that for learning rates until the EOS, the loss decreases monotonically, progressive sharpening is present during training, there is no significant change in $\Vert P_{k,t} \Vert_F$ and $\cos(\Delta_{k,t}, \nabla_{k,t})$ is close to $1$. For learning rates beyond the EOS, a catapult phase can be observed. During this phase, the loss oscillates, sharpness and $\Vert P_{k,t} \Vert_F$ decrease significantly and $\cos(\Delta_{k,t}, \nabla_{k,t})$ is close to $0$. This stabilizes training and the behavior during the rest of training is similar to the case of small learning rates. In general, increasing the learning rate until the EOS results in less orthogonal hidden representations and sharper solutions. Then we observe the reverse trend, as increasing the learning rate beyond the EOS yields more orthogonal representations and flatter minima by starting the catapult phase earlier and making it longer. This holds until a certain threshold where training starts to diverge, which happened for $\eta=0.8$ with $r=0$, $\eta=1.5$ with $r=1$ and $\eta=2.6$ with $r=2$. Increasing $r$ indeed gives a form of regularization as higher learning rates are viable with higher $r$, while increasing $r$ also led to significantly flatter solutions. Increasing $r$ leads to smaller loss values as well.

\begin{figure}
\vskip 0.2in
\begin{center}
%\begin{comment}
\centerline{
\begin{tabular}{c}
\begin{subfigure}[b]{1\textwidth}
\begin{tikzpicture}
\begin{groupplot}[group style={group size= 5 by 1, group name=top plots 1}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{1,t},\nabla_{1,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-nabla_delta_alignment_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{2,t},\nabla_{2,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-nabla_delta_alignment_2.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{3,t},\nabla_{3,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-nabla_delta_alignment_3.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{4,t},\nabla_{4,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-nabla_delta_alignment_4.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{5,t},\nabla_{5,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}, legend style={at={(0.5,-0.38)},anchor=north,font=\tiny}, legend columns=1]
\addlegendentry{$\eta=0.1$}
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.2$}
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.3$}
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.4$}
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.5$}
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.6$}
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.7$}
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-nabla_delta_alignment_5.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 4 by 1, group name=mid plots 1}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{2,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, at={($(top plots 1 c1r1.south west) + (0,-2.0cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-p_xx_2_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-p_xx_2_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-p_xx_2_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-p_xx_2_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-p_xx_2_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-p_xx_2_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-p_xx_2_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{3,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-p_xx_3_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-p_xx_3_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-p_xx_3_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-p_xx_3_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-p_xx_3_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-p_xx_3_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-p_xx_3_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{4,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-p_xx_4_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-p_xx_4_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-p_xx_4_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-p_xx_4_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-p_xx_4_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-p_xx_4_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-p_xx_4_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{5,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-p_xx_5_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-p_xx_5_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-p_xx_5_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-p_xx_5_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-p_xx_5_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-p_xx_5_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-p_xx_5_fro.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 2 by 1}, width=0.422\linewidth, height=3cm]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1000, ytick={0,1000}, ylabel={$\Vert K_t \Vert$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, anchor=north west, at={($(mid plots 1 c1r1.south west) + (0,-0.55cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-ntk_lambda_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-ntk_lambda_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-ntk_lambda_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-ntk_lambda_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-ntk_lambda_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-ntk_lambda_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-ntk_lambda_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0.00001, ymax=4, ytick={0.0001,0.001,0.01,0.1,1}, ymode=log, ylabel={$\ell_t$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_02_42_075167-tag-train_loss.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_02_49_707516-tag-train_loss.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.3_a_0.0_b_1.0_samples_1_2025-02-11_13_03_00_167542-tag-train_loss.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_03_08_656439-tag-train_loss.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_03_18_064229-tag-train_loss.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_03_28_858817-tag-train_loss.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_static_lr_0.7_a_0.0_b_1.0_samples_1_2025-02-11_13_03_37_053595-tag-train_loss.csv};
\end{groupplot}
\end{tikzpicture}
\vspace*{-2mm}
\caption{$r=0$, $m=1024$}
\end{subfigure}\\
\begin{subfigure}[b]{1\textwidth}
\begin{tikzpicture}
\begin{groupplot}[group style={group size= 5 by 1, group name=top plots 2}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{1,t},\nabla_{1,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-nabla_delta_alignment_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{2,t},\nabla_{2,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-nabla_delta_alignment_2.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{3,t},\nabla_{3,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-nabla_delta_alignment_3.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{4,t},\nabla_{4,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-nabla_delta_alignment_4.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{5,t},\nabla_{5,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}, legend style={at={(0.5,-0.38)},anchor=north,font=\tiny}, legend columns=1]
\addlegendentry{$\eta=0.2$}
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.4$}
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.6$}
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.8$}
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.0$}
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.2$}
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.4$}
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-nabla_delta_alignment_5.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 4 by 1, group name=mid plots 2}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{2,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, at={($(top plots 2 c1r1.south west) + (0,-2.0cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-p_xx_2_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-p_xx_2_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-p_xx_2_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-p_xx_2_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-p_xx_2_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-p_xx_2_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-p_xx_2_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{3,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-p_xx_3_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-p_xx_3_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-p_xx_3_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-p_xx_3_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-p_xx_3_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-p_xx_3_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-p_xx_3_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{4,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-p_xx_4_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-p_xx_4_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-p_xx_4_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-p_xx_4_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-p_xx_4_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-p_xx_4_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-p_xx_4_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{5,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-p_xx_5_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-p_xx_5_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-p_xx_5_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-p_xx_5_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-p_xx_5_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-p_xx_5_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-p_xx_5_fro.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 2 by 1}, width=0.422\linewidth, height=3cm]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1000, ytick={0,1000}, ylabel={$\Vert K_t \Vert$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, anchor=north west, at={($(mid plots 2 c1r1.south west) + (0,-0.55cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-ntk_lambda_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-ntk_lambda_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-ntk_lambda_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-ntk_lambda_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-ntk_lambda_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-ntk_lambda_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-ntk_lambda_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=.00001, ymax=4, ytick={0.0001,0.001,0.01,0.1,1}, ymode=log, ylabel={$\ell_t$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-11_13_06_17_986453-tag-train_loss.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-11_13_06_31_049135-tag-train_loss.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-11_13_06_45_558694-tag-train_loss.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-11_13_06_58_727200-tag-train_loss.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-11_13_17_36_291019-tag-train_loss.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-11_13_20_31_608765-tag-train_loss.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_static_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-11_13_22_40_562700-tag-train_loss.csv};
\end{groupplot}
\end{tikzpicture}
\vspace*{-2mm}
\caption{$r=1$, $m=512$}
\end{subfigure}\\
\begin{subfigure}[b]{1\textwidth}
\begin{tikzpicture}
\begin{groupplot}[group style={group size= 5 by 1, group name=top plots 3}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{1,t},\nabla_{1,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-nabla_delta_alignment_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{2,t},\nabla_{2,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-nabla_delta_alignment_2.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{3,t},\nabla_{3,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-nabla_delta_alignment_3.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{4,t},\nabla_{4,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-nabla_delta_alignment_4.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{5,t},\nabla_{5,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}, legend style={at={(0.5,-0.38)},anchor=north,font=\tiny}, legend columns=1]
\addlegendentry{$\eta=0.1$}
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.5$}
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.9$}
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.3$}
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.7$}
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=2.1$}
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=2.5$}
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-nabla_delta_alignment_5.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 4 by 1, group name=mid plots 3}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{2,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, at={($(top plots 3 c1r1.south west) + (0,-2.0cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-p_xx_2_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-p_xx_2_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-p_xx_2_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-p_xx_2_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-p_xx_2_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-p_xx_2_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-p_xx_2_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{3,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-p_xx_3_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-p_xx_3_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-p_xx_3_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-p_xx_3_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-p_xx_3_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-p_xx_3_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-p_xx_3_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{4,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-p_xx_4_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-p_xx_4_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-p_xx_4_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-p_xx_4_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-p_xx_4_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-p_xx_4_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-p_xx_4_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{5,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-p_xx_5_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-p_xx_5_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-p_xx_5_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-p_xx_5_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-p_xx_5_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-p_xx_5_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-p_xx_5_fro.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 2 by 1}, width=0.422\linewidth, height=3cm]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1000, ytick={0,1000}, ylabel={$\Vert K_t \Vert$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, anchor=north west, at={($(mid plots 3 c1r1.south west) + (0,-0.55cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-ntk_lambda_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-ntk_lambda_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-ntk_lambda_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-ntk_lambda_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-ntk_lambda_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-ntk_lambda_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-ntk_lambda_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=.00001, ymax=4, ytick={0.0001,0.001,0.01,0.1,1}, ymode=log, ylabel={$\ell_t$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.1_a_0.0_b_1.0_samples_1_2025-02-11_13_09_56_806541-tag-train_loss.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.5_a_0.0_b_1.0_samples_1_2025-02-11_13_10_23_920499-tag-train_loss.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_0.9_a_0.0_b_1.0_samples_1_2025-02-11_13_17_42_154437-tag-train_loss.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.3_a_0.0_b_1.0_samples_1_2025-02-11_13_21_36_236682-tag-train_loss.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_1.7_a_0.0_b_1.0_samples_1_2025-02-11_13_24_58_068716-tag-train_loss.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.1_a_0.0_b_1.0_samples_1_2025-02-11_13_27_47_987354-tag-train_loss.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_static_lr_2.5_a_0.0_b_1.0_samples_1_2025-02-11_13_31_59_813169-tag-train_loss.csv};
\end{groupplot}
\end{tikzpicture}
\vspace*{-2mm}
\caption{$r=2$, $m=256$}
\end{subfigure}
\end{tabular}
}
%\end{comment}
\caption{Training MLPs with $q=1$, $l=5$, $(a,b)=(0,1)$ and $\eta_{k,t} = \eta$ to minimize the classification loss $\ell(x,z) = \log(\langle e^z, \mathbbm{1}_n \rangle) - z_{t(x)}$ on $\mu_t = \frac{1}{n} \sum_{i=1}^n \delta_{x_i}$ where $\{ x_i : i \in [1:n] \}$ is a random subset of MNIST with $n=2^6$ and $t : \R^{m_0} \to [1:10]$ denotes the ground truth label function.}
\label{fig:plot_eos}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{A Learning Rate Schedule}\label{lrs}

Since $\cos(X_{k,t_1}, B_{k,t_1})^{\frac{1}{2}} \cos(X_{k,t_2}, B_{k,t_2})^{\frac{1}{2}} \cos(\nabla_{k,t_1}, \nabla_{k,t_2}) \in [-1,1]$, we have the bound
\[
\Vert \Delta_{k,t} \Vert_F^2
\leq \sum_{t_1=0}^{t-1} \sum_{t_2=0}^{t-1} \eta_{k,t_1} \eta_{k,t_2} \Vert X_{k,t_1} \Vert_F^{\frac{1}{2}} \Vert B_{k,t_1} \Vert_F^{\frac{1}{2}} \Vert X_{k,t_2} \Vert_F^{\frac{1}{2}} \Vert B_{k,t_2} \Vert_F^{\frac{1}{2}}.
\]
This suggests the learning rate schedule 
\[
\eta_{k,t} = \Vert X_{k,t} \Vert_F^{-\frac{1}{2}} \Vert B_{k,t} \Vert_F^{-\frac{1}{2}} \eta_k
\]
for some $\eta_k>0$, yielding $\Vert \Delta_{k,t} \Vert_F^2 = \rho_{k,t} t^2 \eta_k^2$ with the cumulative cosine $\rho_{k,t} \in [0,1]$ defined as
\[
\rho_{k,t} = \frac{1}{t^2} \sum_{t_1=0}^{t-1} \sum_{t_2=0}^{t-1} \cos(X_{k,t_1}, B_{k,t_1})^{\frac{1}{2}} \cos(X_{k,t_2}, B_{k,t_2})^{\frac{1}{2}} \cos(\nabla_k(\theta_{t_1}), \nabla_k(\theta_{t_2})).
\]
This schedule depends on the geometry of the loss landscape, giving more weight to gradients with better alignment between the forward and backward Gram matrices. Implementation is straightforward and computation is cheap as one only has to store the matrices $N_{k,t} = [N_k(x_{t,i},\theta_t) : i \in [1:n]] \in \R^{n \times m_k}$ in the forward pass and tell the automatic differentiation engine to record gradients during backward pass to obtain the matrices $B_{k,\mu_t}(\theta_t)$.

The above learning rate schedule is demonstrated empirically in Figure~\ref{fig:plot_eos_lrschedule}. An immediate consequence is that training does not saturate, with the loss decreasing indefinitely until the bottom of floating point accuracy, resulting in numerical errors. To this end, we employed early stopping, terminating training after the loss value fell below $10^{-4}$. For the smallest learning rates, we observe similar behavior as without the learning rate schedule since the loss decreases monotonically and progressive sharpening is present without $\Vert P_{k,t} \Vert_F$ changing significantly, but $\cos(\Delta_{k,t}, \nabla_{k,t})$ started approaching $0$ towards the end. Increasing the learning rate led to the catapult phase with $\cos(\Delta_{k,t}, \nabla_{k,t})$ oscillating around or slightly above $0$ sooner or even immediately and $\Vert P_{k,t} \Vert_F$ decreasing significantly, regularizing the maximum eigenvalue of the NTK and preventing progressive sharpening. Unlike before, the catapult phase continued until the end of training, suggesting that the learning rate schedule prevents stabilization. We believe this is desirable as stable training sharpens the solutions. Increasing the learning rate until a certain threshold led to early stopping happening sooner, flatter solutions and more orthogonal hidden representations. Increasing the learning rate beyond the threshold, $\cos(\Delta_{k,t}, \nabla_{k,t})$ oscillated below $0$ for some time with the decrease of $\Vert P_{k,t} \Vert_F$ starting only when $\cos(\Delta_{k,t}, \nabla_{k,t})$ ascended to oscillate around $0$, delaying early stopping and resulting in somewhat sharper solutions. As before, increasing $r$ enables using larger learning rates and leads to reaching flatter solutions earlier.

\begin{figure}
\vskip 0.2in
\begin{center}
%\begin{comment}
\centerline{
\begin{tabular}{c}
\begin{subfigure}[b]{1\textwidth}
\begin{tikzpicture}
\begin{groupplot}[group style={group size= 5 by 1, group name=top plots 1}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{1,t},\nabla_{1,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-nabla_delta_alignment_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{2,t},\nabla_{2,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-nabla_delta_alignment_2.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{3,t},\nabla_{3,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-nabla_delta_alignment_3.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{4,t},\nabla_{4,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-nabla_delta_alignment_4.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{5,t},\nabla_{5,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}, legend style={at={(0.5,-0.38)},anchor=north,font=\tiny}, legend columns=1]
\addlegendentry{$\eta=0.2$}
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.4$}
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.6$}
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.8$}
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.0$}
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.2$}
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.4$}
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-nabla_delta_alignment_5.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 4 by 1, group name=mid plots 1}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{2,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, at={($(top plots 1 c1r1.south west) + (0,-2.0cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-p_xx_2_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-p_xx_2_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-p_xx_2_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-p_xx_2_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-p_xx_2_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-p_xx_2_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-p_xx_2_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{3,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-p_xx_3_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-p_xx_3_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-p_xx_3_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-p_xx_3_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-p_xx_3_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-p_xx_3_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-p_xx_3_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{4,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-p_xx_4_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-p_xx_4_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-p_xx_4_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-p_xx_4_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-p_xx_4_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-p_xx_4_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-p_xx_4_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{5,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-p_xx_5_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-p_xx_5_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-p_xx_5_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-p_xx_5_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-p_xx_5_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-p_xx_5_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-p_xx_5_fro.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 2 by 1}, width=0.422\linewidth, height=3cm]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1000, ytick={0,1000}, ylabel={$\Vert K_t \Vert$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, anchor=north west, at={($(mid plots 1 c1r1.south west) + (0,-0.55cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-ntk_lambda_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-ntk_lambda_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-ntk_lambda_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-ntk_lambda_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-ntk_lambda_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-ntk_lambda_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-ntk_lambda_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=.00001, ymax=4, ytick={0.0001,0.001,0.01,0.1,1}, ymode=log, ylabel={$\ell_t$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_26_31_340959-tag-train_loss.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.4_a_0.0_b_1.0_samples_1_2025-02-13_11_29_20_325802-tag-train_loss.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_11_35_22_795936-tag-train_loss.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_11_38_46_858195-tag-train_loss.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_11_40_22_065613-tag-train_loss.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.2_a_0.0_b_1.0_samples_1_2025-02-13_11_42_06_702098-tag-train_loss.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_0_MNIST_n_64_m_1024_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_11_42_16_499144-tag-train_loss.csv};
\end{groupplot}
\end{tikzpicture}
\vspace*{-2mm}
\caption{$r=0$, $m=1024$}
\end{subfigure}\\
\begin{subfigure}[b]{1\textwidth}
\begin{tikzpicture}
\begin{groupplot}[group style={group size= 5 by 1, group name=top plots 2}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{1,t},\nabla_{1,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-nabla_delta_alignment_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{2,t},\nabla_{2,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-nabla_delta_alignment_2.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{3,t},\nabla_{3,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-nabla_delta_alignment_3.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{4,t},\nabla_{4,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-nabla_delta_alignment_4.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{5,t},\nabla_{5,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}, legend style={at={(0.5,-0.38)},anchor=north,font=\tiny}, legend columns=1]
\addlegendentry{$\eta=0.2$}
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.6$}
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.0$}
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.4$}
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.8$}
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=2.2$}
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=2.6$}
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-nabla_delta_alignment_5.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 4 by 1, group name=mid plots 2}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{2,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, at={($(top plots 2 c1r1.south west) + (0,-2.0cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-p_xx_2_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-p_xx_2_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-p_xx_2_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-p_xx_2_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-p_xx_2_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-p_xx_2_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-p_xx_2_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{3,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-p_xx_3_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-p_xx_3_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-p_xx_3_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-p_xx_3_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-p_xx_3_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-p_xx_3_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-p_xx_3_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{4,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-p_xx_4_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-p_xx_4_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-p_xx_4_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-p_xx_4_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-p_xx_4_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-p_xx_4_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-p_xx_4_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{5,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-p_xx_5_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-p_xx_5_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-p_xx_5_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-p_xx_5_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-p_xx_5_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-p_xx_5_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-p_xx_5_fro.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 2 by 1}, width=0.422\linewidth, height=3cm]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1000, ytick={0,1000}, ylabel={$\Vert K_t \Vert$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, anchor=north west, at={($(mid plots 2 c1r1.south west) + (0,-0.55cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-ntk_lambda_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-ntk_lambda_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-ntk_lambda_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-ntk_lambda_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-ntk_lambda_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-ntk_lambda_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-ntk_lambda_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=.00001, ymax=4, ytick={0.0001,0.001,0.01,0.1,1}, ymode=log, ylabel={$\ell_t$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_11_55_15_353737-tag-train_loss.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_0.6_a_0.0_b_1.0_samples_1_2025-02-13_12_00_26_519554-tag-train_loss.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.0_a_0.0_b_1.0_samples_1_2025-02-13_12_03_37_133191-tag-train_loss.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_03_55_475438-tag-train_loss.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_1.8_a_0.0_b_1.0_samples_1_2025-02-13_12_04_14_645540-tag-train_loss.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.2_a_0.0_b_1.0_samples_1_2025-02-13_12_07_08_641592-tag-train_loss.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_1_MNIST_n_64_m_512_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_07_26_713867-tag-train_loss.csv};
\end{groupplot}
\end{tikzpicture}
\vspace*{-2mm}
\caption{$r=1$, $m=512$}
\end{subfigure}\\
\begin{subfigure}[b]{1\textwidth}
\begin{tikzpicture}
\begin{groupplot}[group style={group size= 5 by 1, group name=top plots 3}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{1,t},\nabla_{1,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-nabla_delta_alignment_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-nabla_delta_alignment_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{2,t},\nabla_{2,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-nabla_delta_alignment_2.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-nabla_delta_alignment_2.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{3,t},\nabla_{3,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-nabla_delta_alignment_3.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-nabla_delta_alignment_3.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{4,t},\nabla_{4,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-nabla_delta_alignment_4.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-nabla_delta_alignment_4.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=-1, ymax=1, ytick={-1,0,1}, ylabel={$\cos(\Delta_{5,t},\nabla_{5,t})$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, extra y ticks=0, extra y tick labels=, extra y tick style={grid=major}, ticklabel style={font=\tiny}, legend style={at={(0.5,-0.38)},anchor=north,font=\tiny}, legend columns=1]
\addlegendentry{$\eta=0.2$}
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=0.8$}
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=1.4$}
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=2.0$}
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=2.6$}
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=3.2$}
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-nabla_delta_alignment_5.csv};
\addlegendentry{$\eta=3.8$}
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-nabla_delta_alignment_5.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 4 by 1, group name=mid plots 3}, width=0.23\linewidth]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{2,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, at={($(top plots 3 c1r1.south west) + (0,-2.0cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-p_xx_2_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-p_xx_2_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-p_xx_2_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-p_xx_2_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-p_xx_2_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-p_xx_2_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-p_xx_2_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{3,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-p_xx_3_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-p_xx_3_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-p_xx_3_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-p_xx_3_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-p_xx_3_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-p_xx_3_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-p_xx_3_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{4,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-p_xx_4_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-p_xx_4_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-p_xx_4_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-p_xx_4_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-p_xx_4_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-p_xx_4_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-p_xx_4_fro.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1, ytick={0,1}, ylabel={$\Vert P_{5,t} \Vert_F$}, y label style={at={(axis description cs:0.65,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-p_xx_5_fro.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-p_xx_5_fro.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-p_xx_5_fro.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-p_xx_5_fro.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-p_xx_5_fro.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-p_xx_5_fro.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-p_xx_5_fro.csv};
\end{groupplot}
\begin{groupplot}[group style={group size= 2 by 1}, width=0.422\linewidth, height=3cm]
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=0, ymax=1000, ytick={0,1000}, ylabel={$\Vert K_t \Vert$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}, anchor=north west, at={($(mid plots 3 c1r1.south west) + (0,-0.55cm)$)}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-ntk_lambda_1.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-ntk_lambda_1.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-ntk_lambda_1.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-ntk_lambda_1.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-ntk_lambda_1.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-ntk_lambda_1.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-ntk_lambda_1.csv};
\nextgroupplot[xmin=0, xmax=200, xtick={0,200}, xlabel={$t$}, x label style={at={(axis description cs:0.5,0.37)},anchor=north,font=\tiny}, xtick pos=upper, xticklabel pos=upper, ymin=.00001, ymax=4, ytick={0.0001,0.001,0.01,0.1,1}, ymode=log, ylabel={$\ell_t$}, y label style={at={(axis description cs:0.252,0.5)},rotate=0,anchor=south,font=\tiny}, ytick pos=right, yticklabel pos=right, ticklabel style={font=\tiny}]
\addplot[mycolor1] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.2_a_0.0_b_1.0_samples_1_2025-02-13_12_32_10_606574-tag-train_loss.csv};
\addplot[mycolor2] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_0.8_a_0.0_b_1.0_samples_1_2025-02-13_12_50_32_179968-tag-train_loss.csv};
\addplot[mycolor3] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_1.4_a_0.0_b_1.0_samples_1_2025-02-13_12_56_26_816650-tag-train_loss.csv};
\addplot[mycolor4] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.0_a_0.0_b_1.0_samples_1_2025-02-13_12_57_24_798878-tag-train_loss.csv};
\addplot[mycolor5] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_2.6_a_0.0_b_1.0_samples_1_2025-02-13_12_58_09_547711-tag-train_loss.csv};
\addplot[mycolor6] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.2_a_0.0_b_1.0_samples_1_2025-02-13_13_03_12_726194-tag-train_loss.csv};
\addplot[mycolor7] table [x=Step, y=Value, col sep=comma] {csvs/feature_learning/run-feature_learning_dgd_param_nu_q_1.0_r_2_MNIST_n_64_m_256_l_5_dynamic_False_lr_3.8_a_0.0_b_1.0_samples_1_2025-02-13_13_03_43_909690-tag-train_loss.csv};
\end{groupplot}
\end{tikzpicture}
\vspace*{-2mm}
\caption{$r=2$, $m=256$}
\end{subfigure}
\end{tabular}
}
%\end{comment}
\caption{Training MLPs with $q=1$, $l=5$, $(a,b)=(0,1)$ and $\eta_{k,t} = \Vert X_{k,t} \Vert_F^{-\frac{1}{2}} \Vert B_{k,t} \Vert_F^{-\frac{1}{2}} \eta$ to minimize the classification loss $\ell(x,z) = \log(\langle e^z, \mathbbm{1}_n \rangle) - z_{t(x)}$ on $\mu_t = \frac{1}{n} \sum_{i=1}^n \delta_{x_i}$ where $\{ x_i : i \in [1:n] \}$ is a random subset of MNIST with $n=2^6$ and $t : \R^{m_0} \to [1:10]$ denotes the ground truth label function.}
\label{fig:plot_eos_lrschedule}
\end{center}
\vskip -0.2in
\end{figure}

\section{Limitations and Future Directions}\label{limitations}
The main limitation of this work is that we only treat MLPs, a basic neural network architecture with a narrow range of practical applicability in real-world problems. One aspect of this issue is that the model is parameter-hungry in the sense that as depth grows, even with $r=2$ the hidden layer sizes quickly rise to unrealistic levels. One future direction is to extend $\nu$P to other architectures such as convolutional neural networks and transformers, which could perhaps yield models that can be scaled up with more realistic memory requirements. Another limitation of our work is that even though we presented a number of formulas through straightforward derivations without making any assumptions that would deviate from practice and then deduced hypotheses concerning the dynamics of feature learning, we only supported these by empirical results without theoretically proving anything. We intend to determine and study the infinitely wide limit of $\nu$P to prove that the observed phenomena happens in the limit and then prove quantitatively that finite MLPs concentrate around the limit during training.

\section*{Acknowledgements}
D\'avid Terj\'ek was supported by the Ministry of Innovation and Technology NRDI Office within the framework of the Artificial Intelligence National Laboratory (RRF-2.3.1-21-2022-00004).

\newpage


\bibliography{jmlr_submission}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
%\appendix




\end{document}




