\newpage
\section{Missing Proofs in Section \ref{sec:playerSBNE}}

\subsection*{Proof of Proposition~\ref{prop:posteriorBeliefs}}
\begin{proof}%[Proof of Proposition~\ref{prop:posteriorBeliefs}]
The observer knows her own ability as well as the promotion status of others. Therefore, the joint posterior probability density \(\beta_1(\mathbf{x})\) is given by:
\[
\beta_1(\mathbf{x}) = \frac{\Pr\left[ \bigwedge_{i=2}^{m} X_i = x_i, \bigwedge_{j=m+1}^{n} X_j \leq \min_{k \in [m]}(X_k), X_1 = x_1 \right]}{\Pr\left[ \bigwedge_{j=m+1}^{n} X_j \leq \min_{k \in [m]}(X_k), X_1 = x_1 \right]}.
\]

For the denominator, we apply the law of total probability over \(X_2, \dots, X_m\):
\[
\Pr\left[ \bigwedge_{j=m+1}^n X_j \leq \min_{k \in [m]}(X_k), X_1 = x_1 \right] =
\sum_{(x_2, \dots, x_m) \in \mathcal{D}_X} \Pr\left[ \bigwedge_{j=m+1}^n X_j \leq \min_{k \in [m]}(x_k) \right] \Pr\left[ \bigwedge_{i=1}^m X_i = x_i \right].
\]

Expanding the probabilities:
\[
\sum_{(x_2, \dots, x_m) \in \mathcal{D}_X} \prod_{j=m+1}^n \Pr\left[X_j \leq \min_{k \in [m]}(x_k)\right] \prod_{i=1}^m f(x_i).
\]

Next, we classify the cases based on the minimum ability among the admitted candidates:

\underline{Case 1:} Observer's ability is the lowest among the admitted (\(x^{(1)} \geq x_1\)).

In this case, \(\min_{k \in [m]}(x_k) = x_1\), so:
\[
\prod_{j=m+1}^n \Pr\left[X_j \leq \min_{k \in [m]}(x_k)\right] = \prod_{j=m+1}^n \Pr\left[X_j \leq x_1\right].
\]

Thus, the integral becomes:
\[
\begin{aligned}
& \int_{x^{(1)} \geq x_1} \prod_{j=m+1}^n \Pr\left[X_j \leq x_1\right] \prod_{i=1}^m f(x_i) \, d\mathbf{x_{-1}} \\
= & F^{n-m}(x_1) f(x_1) \int_{x^{(1)} \geq x_1} \prod_{i=1}^m f(x_i) \, d\mathbf{x_{-1}} \\
= & F^{n-m}(x_1) f(x_1) \prod_{i=1}^m \int_{0}^{x_1} f(x_i) \, dx_i \\
= & F^{n-m}(x_1) (1 - F(x_1))^{m-1} f(x_1).
\end{aligned}
\]

\underline{Case 2:} Observer's ability is not the lowest among the admitted (\(x^{(1)} < x_1\)).

In this case, \(\min_{k \in [m]}(x_k) = x^{(1)}\), so:
\[
\prod_{j=m+1}^n \Pr\left[X_j \leq \min_{k \in [m]}(x_k)\right] = \prod_{j=m+1}^n \Pr\left[X_j \leq x^{(1)}\right].
\]

The integral becomes:
\[
\begin{aligned}
& \int_{x^{(1)} < x_1} \prod_{j=m+1}^n \Pr\left[X_j \leq x^{(1)}\right] \prod_{i=1}^m f(x_i) \, d\mathbf{x_{-1}} \\
= & (m-1) \int_{x_2 < x_1} \int_{\mathbf{x_{-1,2}} \geq x_2} \prod_{j=m+1}^n \Pr\left[X_j \leq x_2\right] \prod_{i=1}^m f(x_i) \, d\mathbf{x_{-1,2}} dx_2 \\
= & (m-1) f(x_1) \int_{0}^{x_1} F^{n-m}(x_2) \int_{\mathbf{x_{-1,2}} \geq x_2} \prod_{i=3}^m f(x_i) \, d\mathbf{x_{-1,2}} f(x_2) \, dx_2 \\
= & (m-1) f(x_1) \int_{0}^{x_1} F^{n-m}(x_2) \left(\prod_{i=3}^m \int_{x_2}^1 f(x_i) \, dx_i\right) f(x_2) \, dx_2 \\
= & (m-1) f(x_1) \int_{0}^{x_1} F^{n-m}(x_2) (1 - F(x_2))^{m-2} f(x_2) \, dx_2 \\
= & (m-1) f(x_1) \int_{0}^{F(x_1)} t^{n-m} (1-t)^{m-2} \, dt.
\end{aligned}
\]

Combining the results from both cases, the denominator becomes:
\[
\int_{x^{(1)} < x_1} \prod_{j=m+1}^n \Pr\left[X_j \leq x^{(1)}\right] \prod_{i=1}^m f(x_i) \, d\mathbf{x_{-1}} + \int_{x^{(1)} \geq x_1} \prod_{j=m+1}^n \Pr\left[X_j \leq x_1\right] \prod_{i=1}^m f(x_i) \, d\mathbf{x_{-1}}.
\]

This simplifies to:
\[
= \left[F^{n-m}(x_1)(1 - F(x_1))^{m-1} + (m-1) \int_{0}^{F(x_1)} t^{n-m} (1-t)^{m-2} \, dt\right] f(x_1).
\]

Or equivalantly:
\[
\binom{n-1}{m-1}^{-1}J(F, n, m, x_1) f(x_1).
\]

For the numerator, using the conditional probability formula:
\[
\begin{aligned}
& \Pr\left[\bigwedge_{i=2}^m X_i = x_i, \bigwedge_{j=m+1}^n X_j \leq \min_{k \in [m]}(X_k), X_1 = x_1\right] \\
= & \Pr\left[\bigwedge_{j=m+1}^n X_j \leq \min_{k \in [m]}(x_k) \mid \bigwedge_{i=1}^m X_i = x_i\right] \Pr\left[\bigwedge_{i=1}^m X_i = x_i\right] \\
= & \left(\prod_{j=m+1}^n \Pr\left[X_j \leq \min_{k \in [m]}(x_k)\right]\right) \prod_{i=1}^m f(x_i).
\end{aligned}
\]

We also analyze by cases. When observer's ability is the lowest among the admitted (\(x^{(1)} \geq x_1\)), the numerator becomes: 
\[
    \begin{aligned}
        & \left ( \prod_{j=m+1}^{n} Pr\left [ X_j \leq x_1 \right ] \right ) \prod_{i=1}^mf(x_i) \\
        = & F^{n-m}(x_1)\prod_{i=1}^mf(x_i)  
    \end{aligned}
\]

Similarly, when observer's ability is not the lowest among the admitted (\(x^{(1)} < x_1\)), the numerator becomes $F^{n-m}(x^{(1)})\prod_{i=1}^mf(x_i)$. 

Combining the numerator and denominator, we obtain:
\[
    \beta_1(\mathbf{x}) =   
    \begin{cases} 
    \frac{\binom{n-1}{m-1}F^{n-m}(x^{(1)})\prod_{i=2}^{m}f(x_i)}{J(F,n,m,x_1)} & \text{if } x^{(1)} \leq x_1, \\
    \frac{\binom{n-1}{m-1}F^{n-m}(x_1)\prod_{i=2}^{m}f(x_i)}{J(F,n,m,x_1)} & \text{if } x^{(1)} > x_1.
    \end{cases},
\]

which completes the proof.
\end{proof}

\begin{lemma} \label{lem:betaRep}
Let the incomplete Beta function be defined as 
\(
B_x(a, b) = \int_{0}^{x} t^{a-1}(1-t)^{b-1} \, dt,
\)
and the normalized incomplete Beta function as 
\(
I_x(a, b) = \frac{B_x(a, b)}{B(a, b)}
\),
where \(B(a, b)\) is the Beta function. For a random variable \(X \sim \text{Binomial}(n, p)\), the following equality holds:
\[
\Pr(X \leq k) = I_{1-p}(n-k, k+1) = 1 - I_p(k+1, n-k).
\]
\end{lemma}

\begin{lemma}[Interpretation of the Normalizer] \label{lem:normal} The normalization factor $J(F,n,m,x_i)$ is the prior probability of contestant $i$ advancing into the second stage, i.e., the following equality holds:
\[
    J(F,n,m,x_i) = \sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(x_i)(1-F(x_i))^{j-1},
\]
which also gives that $J(F,n,m,x_i)$ is an increasing function of shortlist capacity $m$.    
\end{lemma}
\begin{proof}
By definition, The expression for \( J(F, n, m, x_i) \) is given as:
\[
J(F, n, m, x_i) = \binom{n-1}{m-1}F^{n-m}(x_i)(1-F(x_i))^{m-1} + \binom{n-1}{m-1}(m-1)\int_0^{F(x_i)}t^{n-m}(1-t)^{m-2} \, dt.
\]

The second term on the right-hand side can be rewritten as:
\[
\begin{aligned}
    & \frac{(n-1)!}{(m-2)!(n-m)!} B_{F(x_i)}(n-m+1, m-1) \\
    = & \left[ \frac{\Gamma(n-m+1)\Gamma(m-1)}{\Gamma(n)} \right]^{-1} B_{F(x_i)}(n-m+1, m-1) \\
    = & \frac{B_{F(x_i)}(n-m+1, m-1)}{B(n-m+1, m-1)} \\
    = & I_{F(x_i)}(n-m+1, m-1),
\end{aligned}
\]

where \( B_{F(x_i)}(n-m+1, m-1) \) is the incomplete beta function. According to the relationship provided by Lemma~\ref{lem:betaRep}, the above expression corresponds to the value of \( \Pr(X \leq m-2) \), where \( X \sim Binomial(n-1, 1-F(x_i)) \). That is:
\[
\sum_{j=0}^{m-2} \binom{n-1}{j} F^{n-1-j}(x_i)(1-F(x_i))^j.
\]

Substituting back into the original expression and re-indexing terms, we obtain:
\[
J(F, n, m, x_i) = \sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(x_i)(1-F(x_i))^{j-1},
\]

which represents the probability that the ability of contestant \( i \) ranks among the top \( m \) contestants.

Since each term in the summation is positive, it follows that \( J(F, n, m, x_i) \) is monotonically increasing respect to shortlist capacity $m$, which completes the proof.
\end{proof}


\subsection*{Proof of Corollary~\ref{prop:marginalBelief}}
\begin{proof}
We perform a case analysis based on the value of \( z \). Since the denominator \( J(F,n,m,x_1) \) is independent of \( z \), we omit it in our discussion and focus on the marginalization of the numerator:

\underline{Case 1:} \( z \leq x_1 \). In this scenario, \( x^{(1)} \leq x_1 \). We further classify based on the relationship between \( z \) and \( x^{(1)} \), dividing the marginalization integral into two parts:

The first part, when \( z = x^{(1)} \), the marginalization of the numerator is:
\[
\begin{aligned}
    & \binom{n-1}{m-1} \int_{\mathbf{x_{-1,2}} \geq z} F^{n-m}(z) \prod_{k=2}^m f(x_k) \, d\mathbf{x_{-1,2}} \\ 
    & = \binom{n-1}{m-1} F^{n-m}(z) \left ( \int_{z}^{1} f(x) \, dx \right )^{m-2} f(z) \\
    & = \binom{n-1}{m-1} F^{n-m}(z) (1-F(z))^{m-2} f(z).
\end{aligned}
\]

The second part, when \( z > x^{(1)} \): Without loss of generality, we reorder the indices so that the competitor corresponding to \( x^{(1)} \) is indexed by $3$. The marginalization expression then becomes:
\[
\begin{aligned}
    & \binom{n-1}{m-1}(m-2) \int_0^z \left ( \underbrace{\int_{x_3}^{1} \cdots \int_{x_3}^{1}}_{m-3} F^{n-m}(x_3) \prod_{k=2}^m f(x_k) \, d\mathbf{x_{-1,2,3}} \right ) \, dx_3 \\
    = & \binom{n-1}{m-1}(m-2) f(z) \int_0^{z} F^{n-m}(x_3) (1-F(x_3))^{m-3} f(x_3) \, dx_3 \\
    = & \binom{n-1}{m-1}(m-2) f(z) \int_0^{F(z)} t^{n-m} (1-t)^{m-3} \, dt.
\end{aligned}
\]

By summing these two parts and factoring out the common terms, we obtain the expression for the marginalized numerator:
\[
\binom{n-1}{m-1} \left ( F^{n-m}(z)(1-F(z))^{m-2} + (m-2) \int_0^{F(z)} t^{n-m} (1-t)^{m-3} \, dt \right ) f(z).
\]

\underline{Case 2:} when \( z \geq x_1 \). Similarly, if the ability of contestant $2$ is not the lowest among the other advancing contestants, we re-index the contestant corresponding to \( x^{(1)} \) to 3. We then classify the discussion based on the relationship between \( x^{(1)} \) and \( x_1 \), splitting the marginalization integral into two region:

The first region, when \( x^{(1)} \leq x_1 \):

In this case, the marginalization of the numerator becomes:
\[
\begin{aligned}
    & \binom{n-1}{m-1}(m-2)\int_0^{x_1} \left( \underbrace{\int_{x_3}^{1} \cdots \int_{x_3}^{1}}_{m-3} F^{n-m}(x_3) \prod_{k=2}^m f(x_k) \, d\mathbf{x_{-1,2,3}}\right) \, dx_3 \\
    = & \binom{n-1}{m-1}(m-2)f(z)\int_0^{F(x_1)} t^{n-m}(1-t)^{m-3} \, dt
\end{aligned}
\]

The second region, when \( x^{(1)} > x_1 \):

We further classify based on the relationship between \( x^{(1)} \) and \( z \). When \( x_1 \leq x^{(1)} < z \), the marginalization of the numerator is given by:
\[
\begin{aligned}
    & \binom{n-1}{m-1}(m-2)\int_{x_1}^{z} \left( \underbrace{\int_{x_3}^{1} \cdots \int_{x_3}^{1}}_{m-3} F^{n-m}(x_1) \prod_{k=2}^m f(x_k) \, d\mathbf{x_{-1,2,3}}\right) \, dx_3 \\
    = & \binom{n-1}{m-1}(m-2)f(z)F^{n-m}(x_1)\int_{x_1}^{z} (1-t)^{m-3}\, dt
\end{aligned}
\]

When \( z \) is the smallest ability among the remaining contestants, i.e., \( z = x^{(1)} \), the marginalization of the numerator becomes:
\[
\begin{aligned}
    & \binom{n-1}{m-1}\int_{\mathbf{x_{-1,2}} \geq z} F^{n-m}(x_1)  \prod_{k=2}^m f(x_k) \, d\mathbf{x_{-1,2}}\\
    = & \binom{n-1}{m-1}f(z)F^{n-m}(x_1)(1-F(z))^{m-2} \\
    = & \binom{n-1}{m-1}(m-2)f(z)F^{n-m}(x_1)\int_z^{1} (1-t)^{m-3} \, dt
\end{aligned}
\]

Adding the two components together, we obtain the marginalization of the second part's numerator: 
\[
\begin{aligned}
    & \binom{n-1}{m-1}(m-2)f(z)F^{n-m}(x_1)\int_0^{1} (1-t)^{m-3} \, dt \\
    = & \binom{n-1}{m-1} F^{n-m}(x_1)(1-F(x_1))^{m-2}
\end{aligned}
\]

In summary, we derive the posterior probability density function:
\[
\beta_1(z) =   
\begin{cases} 
\frac{\binom{n-1}{m-1}\left( F^{n-m}(z)(1-F(z))^{m-2}+(m-2)\int_0^{F(z)}t^{n-m}(1-t)^{m-3}\, dt \right)f(z)}{J(F,n,m,x_1)} & \text{if } z \leq x_1, \\
\frac{\binom{n-1}{m-1}\left(  F^{n-m}(x_1)(1-F(x_1))^{m-2} +(m-2)\int_0^{F(x_1)} t^{n-m}(1-t)^{m-3}\, dt \right) f(z)}{J(F,n,m,x_1)} & \text{if } z > x_1.
\end{cases}
\]

Since each term in the expression is continuous, it is easy to verify that \(\lim_{z \rightarrow x_i^{+}} \beta_1(z) = \beta_1(x_i)\), thus \(\beta_1\) is continuous.

We then compute the expression for \(\Pr_{\beta_i}\). When \(z \leq x_i\), the integral of the numerator of \(\beta_1(z)\) over \((0, z]\) is:
\[
\begin{aligned}
    & \binom{n-1}{m-1} \left[ \int_0^{z} F^{n-m}(t)(1-F(t))^{m-2} f(t)\, dt + (m-2) \int_0^{z} \int_0^{F(t)} p^{n-m}(1-p)^{m-3}\, dp \, f(t) \, dt \right] \\
    = & \binom{n-1}{m-1} \left[ \int_0^{F(z)} t^{n-m}(1-t)^{m-2} \, dt + (m-2) \int_0^{z} B_{F(t)}(n-m+1, m-2) \, dF(t) \right] \\
    = & \binom{n-1}{m-1} \left[ B_{F(z)}(n-m+1, m-1) + (m-2) \int_0^{z} B_{F(t)}(n-m+1, m-2) \, dF(t) \right],
\end{aligned}
\]
where the last equality follows from the definition of incomplete beta function \(B_x(a,b) = \int_0^x t^{a-1}(1-t)^{b-1} \, dt\) as defined in Lemma~\ref{lem:betaRep}.

When \(z > x_i\), only the last term \(f(z)\) in the expression for \(\beta_1(z)\) is related to \(z\), thus:
\[
\begin{aligned}
    \Pr_{\beta_1}(X_2 \leq z) & = \Pr_{\beta_1}( X_2 \leq x_1) + \int_{x_1}^z \beta_1(t) \, dt \\
    & = \Pr_{\beta_1}(X_2 \leq x_1) + \int_{x_1}^z f(t) \, dt \frac{\beta_1(z)}{f(z)} \\
    & = \Pr_{\beta_1}(X_2 \leq x_1) + (F(z) - F(x_1)) \frac{\beta_1(z)}{f(z)}.
\end{aligned}
\]

In summary, we obtain the expression for the posterior cumulative probability distribution:
\[
\Pr_{\beta_1}(X_2 \leq z) =   
\begin{cases} 
\frac{\binom{n-1}{m-1} \left[ B_{F(z)}(n-m+1, m-1) + (m-2) \int_{0}^{z} B_{F(t)}(n-m+1, m-2) \, dF(t) \right]}{J(F,n,m,x_1)} & \text{if } z \leq x_1, \\
\Pr_{\beta_1}(X_2 \leq x_1) + \frac{\binom{n-1}{m-1} (F(z) - F(x_1)) \left[ F^{n-m}(x_1)(1-F(x_1))^{m-2} + (m-2) B_{F(x_1)}(n-m+1, m-2) \right]}{J(F,n,m,x_1)} & \text{if } z > x_1.
\end{cases}
\]

Since \(\beta_1\) is continuous, its integral \(\Pr_{\beta_i}\) is also continuous, completing the proof.
\end{proof}

\begin{lemma}\label{lem:StoDom}
    For two continuous functions \( f(x), g(x) \geq 0 \), if the following conditions are satisfied:
    \begin{enumerate}
        \item \( g(x) \) is non-decreasing.
        \item \( \int_0^{+\infty} f(x) \, dx = \int_0^{+\infty} g(x) \, dx = \int_0^{+\infty} g(x) f(x) \, dx = 1 \),
    \end{enumerate}
    then for all \( a \geq 0 \), the inequality 
    \[
    \int_0^{a} f(x) \, dx \geq \int_0^a g(x) f(x) \, dx
    \]
    holds.
\end{lemma}

\begin{proof}
    Define 
    \[
    \varphi(a) = \int_0^a \big(1 - g(x)\big) f(x) \, dx.
    \]
    
    Differentiating \(\varphi(a)\), we have
    \[
    \varphi'(a) = \big(1 - g(a)\big) f(a).
    \]
    
    Since \( g(x) \) is non-decreasing and \(\int_0^{+\infty} g(x) \, dx = 1\), there exists a finite point \( a' \neq +\infty \) such that \(\varphi'(a) = 0\). Moreover, for \( a < a' \), \(\varphi'(a) > 0\), and for \( a > a' \), \(\varphi'(a) < 0\). This implies that \(\varphi(a)\) is a unimodal function, achieving its minimum only at the endpoints of the domain.

    Evaluating \(\varphi(a)\) at the endpoints, we have:
    \[
    \varphi(0) = 0,
    \]
    and
    \[
    \varphi(+\infty) = \int_0^{+\infty} \big(1 - g(x)\big) f(x) \, dx = \int_0^{+\infty} f(x) \, dx - \int_0^{+\infty} g(x) f(x) \, dx = 1 - 1 = 0.
    \]
    
    Therefore \(\varphi(a)\) is nonnegative for all \( a \geq 0 \), and we conclude that
    \[
    \int_0^a \big(1 - g(x)\big) f(x) \, dx \geq 0.
    \]
    
    This implies
    \[
    \int_0^a f(x) \, dx \geq \int_0^a g(x) f(x) \, dx,
    \]
    which completes the proof.
\end{proof}

\subsection*{Proof of Proposition~\ref{prop:stoDomPos}}
\begin{proof}
    Define the function:
    \[
    q_{x_i}(z) :=
    \begin{cases} 
    \frac{F^{n-m}(z)(1-F(z))^{m-2}+(m-2)\int_0^{F(z)}t^{n-m}(1-t)^{m-3}\, dt}{\binom{n-1}{m-1}^{-1}J(F,n,m,x_i)} & \text{if } z \leq x_i, \\
    \frac{F^{n-m}(x_i)(1-F(x_i))^{m-2} +(m-2)\int_0^{F(x_i)} t^{n-m}(1-t)^{m-3}\, dt}{\binom{n-1}{m-1}^{-1}J(F,n,m,x_i)} & \text{if } z > x_i.
    \end{cases}
    \]

    The posterior belief \(\beta_i\) can be rewritten as \(\beta_i(z) = q_{x_i}(z)f(z)\).

    To apply Lemma~\ref{lem:StoDom}, we will prove that \(q_{x_i}(z)\) is non-decreasing and \(\int_0^{+\infty}q_{x_i}(z)\,dz = 1\).

    First, consider monotonicity. When \(z > x_i\), \(q_{x_i}(z)\) is independent of \(z\), so \(q'_{x_i}(z) = 0\). When \(z \leq x_i\):
    \[
    \begin{aligned}
        q'_{x_i}(z) = & (n-m)F^{n-m-1}(z)(1-F(z))^{m-2}f(z) \\
                      & - (m-2)F^{n-m}(z)(1-F(z))^{m-3}f(z) + (m-2)F^{n-m}(z)(1-F(z))^{m-3}f(z) \\
                    = & (n-m)F^{n-m-1}(z)(1-F(z))^{m-2}f(z) \geq 0,
    \end{aligned}
    \]
    thus \(q_{x_i}'(z) \geq 0\) holds, $q_{x_i}(z)$ is non-decreasing, as desired.

    Next, consider the integral identity. Since the denominator of \(q_{x_i}(z)\) is independent of \(z\), this is equivalent to proving:
    \begin{equation}\label{eq:StoDomInt}
        \int_0^{+\infty}\binom{n-1}{m-1}^{-1}J(F,n,m,x_i)q_{x_i}(z)\,dz=\binom{n-1}{m-1}^{-1}J(F,n,m,x_i).
    \end{equation}

    Expanding the integral based on the piecewise definition of \(q_{x_i}(z)\):
    \[
    \begin{aligned}
        & \int_0^{x_i}\binom{n-1}{m-1}^{-1}J(F,n,m,x_i)q_{x_i}(z)\,dz + \int_{x_i}^{+\infty}\binom{n-1}{m-1}^{-1}J(F,n,m,x_i)q_{x_i}(z)\,dz \\
        = & \int_0^{F(x_i)}t^{n-m}(1-t)^{m-2}\, dt +(m-2)\int_0^{x_i}\int_0^{F(z)}t^{n-m}(1-t)^{m-3}\, dt\, dF(z) \\
        & + F^{n-m}(x_i)(1-F(x_i))^{m-2}+(m-2)(1-F(x_i))\int_0^{F(x_i)}t^{n-m}(1-t)^{m-3}\, dt.
    \end{aligned}
    \]

    Both sides of Equation~\eqref{eq:StoDomInt} are functions of \(x_i\). To prove these functions are equal on \([0,+\infty)\), differentiate both sides with respect to \(x_i\). On the left-hand side:
    \[
    \begin{aligned}
        & F^{n-m}(x_i)(1-F(x_i))^{m-2} + (m-2)\int_0^{F(x_i)}t^{n-m}(1-t)^{m-3}\, dt \\
        & + (n-m)F^{n-m-1}(x_i)(1-F(x_i))^{m-1} - (m-1)F^{n-m}(x_i)(1-F(x_i))^{m-2} \\
        & + (m-2)F^{n-m}(x_i)(1-F(x_i))^{m-2} -(m-2)\int_0^{F(x_i)}t^{n-m}(1-t)^{m-3}\, dt \\
        = & (n-m)F^{n-m-1}(x_i)(1-F(x_i))^{m-1}.
    \end{aligned}
    \]

    For the right-hand side, by definition:
    \[
    J(F, n, m, x_i) = \binom{n-1}{m-1}F^{n-m}(x_i)(1-F(x_i))^{m-1} + \binom{n-1}{m-1}(m-1)\int_0^{F(x_i)}t^{n-m}(1-t)^{m-2} \, dt.
    \]

    Differentiating with respect to \(x_i\) yields:
    \[
    \begin{aligned}
        & (n-m)F^{n-m-1}(x_i)(1-F(x_i))^{m-1} - (m-1)F^{n-m}(x_i)(1-F(x_i))^{m-2} \\
        & + (m-1)F^{n-m}(x_i)(1-F(x_i))^{m-2} \\
        = & (n-m)F^{n-m-1}(x_i)(1-F(x_i))^{m-1}.
    \end{aligned}
    \]

    Thus, for the Equation~\eqref{eq:StoDomInt}, \(\text{LHS}'_{x_i} = \text{RHS}'_{x_i}\). Additionally, at \(x_i = 0\), \(\text{LHS} = \text{RHS} = 0\). This two conditions gives that \(\text{LHS} = \text{RHS}\) holds, i.e., \(\int_0^{+\infty}q_{x_i}(z)\, dz = 1\).

    By Lemma~\ref{lem:StoDom}, we obtain, for all \(z \geq 0\):
    \[
    \begin{aligned}
        \int_0^z q_{x_i}(t)f(t) \,dt & \leq \int_0^{z}f(t)\, dt, \\
        \int_0^{z} \beta_i(t)\, dt & \leq F(z), \\
        \Pr_{\beta_i}(X_j \leq z) & \leq \Pr_{f}(X_j \leq z),
    \end{aligned}
    \]which completes the proof.
\end{proof}


\subsection*{Proof of Proposition~\ref{prop:StoDomAbi}}
\begin{proof}
    For the convenience of discussion, we define:
    \[
    \begin{aligned}
        I(F,n,m,x) :&= \binom{n-1}{m-1}^{-1}J(F,n,m,x) \\
        & = F^{n-m}(x)(1-F(x))^{m-1}+(m-1)\int_0^{F(x)}t^{n-m}(1-t)^{m-2}\, dt.
    \end{aligned}
    \]

    Then, $I(F,n,m,x)$'s derivative respect to $x$ is expressed as:
    \begin{equation}\label{eq:derivativeI}
    \begin{aligned}
        I'(F,n,m,x) = & (n-m)F^{n-m-1}(x)(1-F(x))^{m-1}f(x) \\
        & -(m-1)F^{n-m}(x)(1-F(x))^{m-2}f(x) \\
        & +(m-1)F^{n-m}(x)(1-F(x))^{m-2}f(x) \\
        = &(n-m)F^{n-m-1}(x)(1-F(x))^{m-1}f(x).
    \end{aligned}
    \end{equation}

    With the help of $I(F,n,m,x_j)$, the formula for $q_{x_j}(p)$ can be simplified to:
    \[
    q_{x_j}(p) =
    \begin{cases} 
    \frac{I(F,n-1,m-1,p)}{I(F,n,m,x_j)} & \text{if } p \leq x_j, \\
    \frac{I(F,n-1,m-1,x_j)}{I(F,n,m,x_j)} & \text{if } p > x_j.
    \end{cases}
    \]
    
    By definition, for contestant $j$ (so does contestant $i$):
    \[
    \Pr_{\beta_j}(X_k \leq z) = \int_0^{z}q_{x_j}(p)f(p)\, dp.
    \]

    Since $q_{x_j}(\cdot)$ is piece-wise, we classify the cases based on the value of $z$. 

    \underline{Case 1:} When $z \leq x_j$:

    In this case, only the denominator of $q_{x_j}(p)$ depends on $x_j$. Equation~\eqref{eq:derivativeI} shows that $I'(F,n,m,x)\geq 0$ for all $x\geq0$, $I(F,n,m,x)$ decreases with $x$. Therefore, $q_{x_j}(p) \geq q_{x_i}(p)$ holds for any $p \geq 0$, so we have $\int_0^{z}q_{x_i}(p)f(p)\, dp \leq \int_0^{z}q_{x_j}(p)f(p)\, dp$, i.e.,$\Pr_{\beta_i}(X_k \leq z) \leq \Pr_{\beta_j}(X_k \leq z)$, as desired. 

    \underline{Case 2:} When $z > x_j$:

    In this case, it suffices to prove that $\Pr_{\beta_j}(X_k \leq z)$ is an decreasing function of her ability $x_j$, which, by Leibniz integral rule, is to show that:
    \[
    \int_0^{z}\frac{\partial q_{x_j}(p)}{\partial x_j} f(p)\,dp \leq 0.
    \]

    We then proceed by dividing the integration interval into two parts, $(0,x_j]$ and $(x_j,z]$. Since after taking derivative, the denominator of $\partial q_{x_j}(p)/ \partial x_j$ is the same positive term for both interval and irrelevant to integration variable $z$, so we omit it in the discussion. 

    The first part, $\int_0^{x_j}\frac{\partial q_{x_j}(p)}{\partial x_j} f(p)\,dp$, the numerator of of $\partial q_{x_j}(p)/ \partial x_j$ is:
    \[
    \begin{aligned}
        & -I'(F,n,m,x_i)I(F,n-1,m-1,p) \\
        = & -(n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-1}f(x_j)\left [ F^{n-m}(p)(1-F(p))^{m-2}+\int_0^{F(p)}t^{n-m}(1-t)^{m-3}\,dt\right ]
    \end{aligned}
    \]

    The integration over $(0,x_j]$ then becomes:
    \begin{multline*}
        -(n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-1}f(x_j) \\
        \cdot \left [ \int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\, dt+ \int_0^{x} \int_0^{F(p)}t^{n-m}(1-t)^{m-3}\,dt f(p) \, dp\right ]
    \end{multline*}

    The second part, $\int_{x_j}^{z}\frac{\partial q_{x_j}(p)}{\partial x_j} f(p)\,dp$, the numerator of $\partial q_{x_j}(p)/ \partial x_j$ is: 
    \[
    \begin{aligned}
        & I'(F,n-1,m-1,x_j)I(F,n,m,x_j)-I'(F,n,m,x_j)-I'(F,n-1,m-1,x_j) \\
        = & (n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-2}f(x_j)\left [ F^{n-m}(x_j)(1-F(x_j))^{m-1}+\int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\,dt\right ] \\
        & -(n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-1}f(x_j)\left [ F^{n-m}(x_j)(1-F(x_j))^{m-2}+\int_0^{F(x_j)}t^{n-m}(1-t)^{m-3}\,dt\right ] \\
        = & (n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-2}f(x_j)
        \\
        & \cdot \left [\int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\,dt - (1-F(x_j))\int_0^{F(x_j)}t^{n-m}(1-t)^{m-3}\,dt\right ]
    \end{aligned}
    \]

    The integration over $(x_j,z]$ then becomes:
    \begin{multline*}
    (n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-2}f(x_j) \\
    \cdot \left [ (F(z)-F(x_j))\int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\,dt-(F(z)-F(x_j))(1-F(x_j))\int_0^{F(x_j)}t^{n-m}(1-t)^{m-3}\,dt\right]
    \end{multline*}

    For the terms inside the square brackets, we have:
    \[
    \begin{aligned}
        \leq & (F(z)-F(x_j))\int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\,dt \\
        \leq & (1-F(x_j))\int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\,dt
    \end{aligned}
    \]

    Combining two parts, we obtain that $I^2(F,n,m,x_j)\int_0^{z}\frac{\partial q_{x_j}(p)}{\partial x_j} f(p)\,dp$:
    \[
    \begin{aligned}
        \leq &  (n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-1}f(x_j) \\
        & \cdot \left[ \int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\,dt -\int_0^{F(x_j)}t^{n-m}(1-t)^{m-2}\, dt - \int_0^{x} \int_0^{F(p)}t^{n-m}(1-t)^{m-3}\,dt f(p)\, dt\right] \\
        = & (n-m)F^{n-m-1}(x_j)(1-F(x_j))^{m-1}f(x_j)\int_0^{x} \int_0^{F(p)}t^{n-m}(1-t)^{m-3}\,dt f(p)\, dt \leq 0.
    \end{aligned}
    \]

    Therefore, when $z > x_j$, $\int_0^{z}\frac{\partial q_{x_j}(p)}{\partial x_j} f(p)\,dp \leq 0$, holds for any ability value $x_j$, so we have $\int_0^{z}q_{x_i}(p)f(p)\, dp \leq \int_0^{z}q_{x_j}(p)f(p)\, dp$, i.e.,$\Pr_{\beta_i}(X_k \leq z) \leq \Pr_{\beta_j}(X_k \leq z)$, as desired. 

    Finally, after discussion by cases, we conclude that for all $z\geq0$, it holds that $\Pr_{\beta_i}(X_k \leq z) \leq \Pr_{\beta_j}(X_k \leq z)$, which completes the proof.
\end{proof}


\subsection*{Proof of Proposition~\ref{prop:ThreatenDesc}}
\begin{proof}
    It is suffice to prove $\Pr_{\beta_1}(X_{(l)} \leq x_1) \geq \Pr_{f}(X_{(l)}\leq x_1)$ for the first claim. We start by solving the expression for $\Pr_{\beta_1}(X_{(l)})$. 

    As stated in the proposition, the observer is re-indexed as $1$, we then appoint the top $l$ contestant (which induces a binomial factor $\binom{m-1}{l}$), re-index the $l^{\text{th}}$ strongest contestant as $2$ (which induces a coefficient $l$), and re-index the weakest contestant as $3$ (which induces a coefficient $m-l-1$). 
    
    Since $x^{(1)} \leq X_{(l)} \leq x_1$, therefore we have $\Pr_{\beta_1}(X_{(l)})$ equals: 
    \[
    \binom{m-1}{l}l(m-l-1)\int_0^{x_1}\underbrace{\int_{x_2}^1 \cdots \int_{x_2}^1}_{l-1} \int_0^{x_2} \underbrace{\int_{x_3}^{x_2} \cdots \int_{x_3}^{x_2}}_{m-l-2} \frac{\binom{n-1}{m-1}F^{n-m}(x_3)\prod_{i=2}^{m}f(x_i)}{J(F,n,m,x_1)} \, d\mathbf{x_{-1}}. 
    \]

    We then proceed by dealing with these integration from inside to outside. We omit coefficient and constant denominator $J(F,n,m,x_1)$ in the procedure.

    First, deal with last $m-l-1$ contestants:
    \[
    \begin{aligned}
    & \int_0^{F(x_2)} t^{n-m}(F(x_2)-t)^{m-l-2}\, dt \\
    = & \frac{(n-m)!(m-l-2)!}{(n-l-1)!} F(x_2)^{n-l-1},
    \end{aligned}
    \]where the equality gives by Lemma~\ref{lem:betaUpper}. 

    Then, the integration over top $l$ contestants gives:
    \[
    \begin{aligned}
        \int_0^{F(x_1)}t^{n-l-1}(1-t)^{l-1} \, dt 
    \end{aligned}
    \]

    Combining the coefficients, they becomes:
    \[
    \begin{aligned}
        & \frac{(m-1)!}{l!(m-l-1)!}l(m-l-1)\frac{(n-1)!}{(m-1)!(n-m)!}\frac{(n-m)!(m-l-2)!}{(n-l-1)!} \\
        = & \frac{(n-1)!}{(l-1)!(n-l-1)!} \\
        = & (n-1) \binom{n-2}{l-1}
    \end{aligned}
    \]

    Recovering the denominator, we get:
    \[
    \Pr_{\beta_1}(X_{(l)}) = \frac{(n-1)\binom{n-2}{l-1}\int_0^{F(x_1)} t^{n-l-1}(1-t)^{l-1}\, dt}{J(F,n,m,x_1)}.
    \]

    Specifically, when $m=n$, i.e., without shortlisting, $J(F,n,m,x_1)=1$, then $\Pr_{\beta_1}(X_{(l)})$ exactly correspond to the $l^{\text{th}}$ order-statistics (in our notation, the order is the inverse of the tradition) of the prior distribution, that is, $\Pr_{\beta_1(m)}(X_{(l)} \leq x_1) = \Pr_{f}(X_{(l)}\leq x_1)$ when $m=n$.

    Finally, Lemma~\ref{lem:normal} gives that $J(F,n,m,x_1)$ increases with $m$, then $\Pr_{\beta_1}(X_{(l)})$ decreases with $m$, equivalently: 
    \[
    \Pr_{\beta_1(m)}(X_{(l)} > x_1) \leq \Pr_{\beta_1(m')}(X_{(l)} > x_1) \leq \Pr_{f}(X_{(l)} > x_1),
    \]for all $l<m<m'\leq n$, which completes the proof.
\end{proof}

\begin{lemma}\label{lem:betaUpper}
For any $x > 0$, $a,b \in \mathbb{N}$, the following equality holds:
    \[
    \int_0^{x}t^{a-1}(x-t)^{b-1}\, dt = \frac{(a-1)!(b-1)!}{(a+b-1)!}x^{a+b-1}
    \]
\end{lemma}
\begin{proof}
    \[
    \int_0^{x}t^{a-1}(x-t)^{b-1}\, dt = x^{a+b-1} \int_0^x (\frac{t}{x})^{a-1}(\frac{t}{x})^{b-1}(x^{-1} \, dt ) = x^{a+b-1} \int_{0}^{1}t^{a-1}(1-t)^{b-1}\, dt
    \]
    
    Recall that for beta function $B(a,b)$, it holds that:
    \[
    B(a,b) = \int_{0}^{1}t^{a-1}(1-t)^{b-1}\, dt = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}
    \]

    This gives:
    \[
    \int_0^{x}t^{a-1}(x-t)^{b-1}\, dt = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} x^{a+b-1} = \frac{(a-1)!(b-1)!}{(a+b-1)!}x^{a+b-1}
    \]
    Where last equality holds since $\Gamma(m) = (m-1)!$ for any $m\in \mathbb{N}$, which completes the proof.
\end{proof}


\subsection*{Proof of Proposition~\ref{prop:winProb}}
\begin{proof}
To facilitate the discussion, we re-index the admitted contestants as \(\mathcal{I} = [m]\), and we designate contestant \(i\) with \(1\) in the new index. Since there is a one-to-one mapping between $e_i$ and $\gamma_i$, choosing an effort \(e_i\) is equivalent to reporting an ability \(\gamma_i\). In the following discussion, we focus on the latter interpretation, which simplifies notations.

Contestant \(i\) (with the original index) sees her probability of achieving rank \(l \neq m\) is:
\[
\begin{aligned}
& \binom{m-1}{l-1} \Pr\left [ \bigwedge_{j=2}^{l}X_j > \gamma_i, \bigwedge_{k=l+1}^m X_k < \gamma_i \mid x_i, \mathcal{I} \right ] \\
= & \binom{m-1}{l-1} \underbrace{\int_{\gamma_i}^{1} \cdots \int_{\gamma_i}^{1}}_{l-1} \underbrace{\int_{0}^{\gamma_i} \cdots \int_{0}^{\gamma_i}}_{m-l} \beta_i(\mathbf{x}) \, d\mathbf{x_{-i}}
\end{aligned}
\]

At this point, \(x^{(1)}\) appears among the last \(m-l\) contestants. Since the reported ability \(\gamma_i\) affects the expression of the integrand \(\beta_i(\mathbf{x})\), we classify the cases based on the value of \(\gamma_i\):

\underline{Case 1:} The reported ability is lower than the true ability, i.e., \(\gamma_i \leq x_i\). Here, \(x^{(1)} \leq x_i\), so:
\[
\binom{m-1}{l-1} \underbrace{\int_{\gamma_i}^{1} \cdots \int_{\gamma_i}^{1}}_{l-1} \underbrace{\int_{0}^{\gamma_i} \cdots \int_{0}^{\gamma_i}}_{m-l} \frac{\binom{n-1}{m-1}F^{n-m}(x^{(1)})\prod_{k=2}^{m}f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-i}}
\]

Without loss of generality, assume contestant \(m\) has the lowest ability among the admitted contestants:
\[
\begin{aligned}
& \binom{m-1}{l-1} (m-l) \int_{0}^{\gamma_i} \underbrace{\int_{\gamma_i}^{1} \cdots \int_{\gamma_i}^{1}}_{l-1} \underbrace{\int_{0}^{\gamma_i} \cdots \int_{0}^{\gamma_i}}_{m-l-1} \frac{\binom{n-1}{m-1}F^{n-m}(x^{(1)})\prod_{k=2}^{m}f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-i,m}} \, dx_m \\
= & \binom{m-1}{l-1} (m-l) [1-F(\gamma_i)]^{l-1} \int_{0}^{\gamma_i} \underbrace{\int_{0}^{\gamma_i} \cdots \int_{0}^{\gamma_i}}_{m-l-1} \frac{\binom{n-1}{m-1}F^{n-m}(x_m)\prod_{k=l+1}^{m}f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-[l]\cup{\{m\}}}} \, dx_m \\
= & \binom{m-1}{l-1} (m-l) [1-F(\gamma_i)]^{l-1} \int_{0}^{\gamma_i} \frac{\binom{n-1}{m-1}F^{n-m}(x_m)[F(\gamma_i)-F(x_m)]^{m-l-1}f(x_m)}{J(F,n,m,x_i)} \, dx_m \\
\end{aligned}
\]

Extracting terms independent of the integral, we obtain:
\[
\frac{\binom{n-1}{m-1} \binom{m-1}{l-1} (m-l) [1-F(\gamma_i)]^{l-1} \int_{0}^{F(\gamma_i)} t^{n-m}[F(\gamma_i)-t]^{m-l-1} \, dt}{J(F,n,m,x_i)}
\]

Substituting the equality from Lemma~\ref{lem:betaUpper}:
\[
\begin{aligned}
& \frac{\binom{n-1}{m-1} \binom{m-1}{l-1} (m-l) [1-F(\gamma_i)]^{l-1} \frac{(n-m)!(m-l-1)!}{(n-l)!}F(\gamma_i)^{n-l}}{J(F,n,m,x_i)} \\
= & \frac{\frac{(n-1)!}{(n-m)!(m-1)!} \frac{(m-1)!}{(l-1)!(m-l)!} [1-F(\gamma_i)]^{l-1} \frac{(n-m)!(m-l)!}{(n-l)!}F(\gamma_i)^{n-l}}{J(F,n,m,x_i)} \\
= & \frac{\frac{(n-1)!}{(l-1)!(n-l)!} [1-F(\gamma_i)]^{l-1}F(\gamma_i)^{n-l}}{J(F,n,m,x_i)} \\
= & \frac{\binom{n-1}{l-1}F(\gamma_i)^{n-l}(1-F(\gamma_i))^{l-1}}{J(F,n,m,x_i)}
\end{aligned}
\]

\underline{Case 2:} The reported ability value by the contestant is higher than the actual ability value, i.e., \(\gamma_i > x_i\). We need to decompose the original integral into two additive parts by considering the relationship between \(x_i\) and \(x^{(1)}\):

\begin{enumerate}
\item \underline{First Part:} When \(x^{(1)} > x_i\), the integral over this interval is:
   \[
   \begin{aligned}
       & \underbrace{\int_{\gamma_i}^{1} \cdots \int_{\gamma_i}^{1}}_{l-1} \underbrace{\int_{x_i}^{\gamma_i} \cdots \int_{x_i}^{\gamma_i}}_{m-l} \frac{\binom{n-1}{m-1}F^{n-m}(x_i)\prod_{k=2}^{m}f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-i}} \\
       = & \frac{\binom{n-1}{m-1}[1-F(\gamma_i)]^{l-1}F^{n-m}(x_i)[F(\gamma_i)-F(x_i)]^{m-l}}{J(F,n,m,x_i)}
   \end{aligned}
   \]
\item \underline{Second Part:} When \(x^{(1)} \leq x_i\), similarly, assuming the \(m\)-th contestant has the lowest ability value among those advancing, the integral over this interval can be expressed as:
   \[
   \begin{aligned}
       & (m-l) \int_{0}^{x_i} \underbrace{\int_{x_m}^{\gamma_i} \cdots \int_{x_m}^{\gamma_i}}_{m-l-1} \underbrace{\int_{\gamma_i}^1 \cdots \int_{\gamma_i}^1}_{l-1} \frac{\binom{n-1}{m-1} F^{n-m}(x^{(1)}) \prod_{k=2}^m f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-i}} \, dx_m \\
       = & (m-l) [1-F(\gamma_i)]^{l-1} \int_{0}^{x_i} \underbrace{\int_{x_m}^{\gamma_i} \cdots \int_{x_m}^{\gamma_i}}_{m-l-1} \frac{\binom{n-1}{m-1} F^{n-m}(x_m) \prod_{k=l+1}^m f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-[l]\cup{\{m\}}}} \, dx_m \\
       = & (m-l) [1-F(\gamma_i)]^{l-1} \int_{0}^{x_i} \frac{\binom{n-1}{m-1} F^{n-m}(x_m) [F(\gamma_i) - F(x_m)]^{m-l-1}f(x_m)}{J(F,n,m,x_i)} \, dx_m 
   \end{aligned}
   \]
   
   Extracting the terms independent of the integral, we obtain:
   \[
   \frac{\binom{n-1}{m-1}(m-l)[1-F(\gamma_i)]^{l-1} \int_{0}^{F(x_i)} t^{n-m}[F(\gamma_i)-t]^{m-l-1}\, dt}{J(F,n,m,x_i)} 
   \]
\end{enumerate}

By adding the two parts together, incorporating the binomial coefficient, and factoring out common terms, we derive the final expression:\[
\frac{\binom{n-1}{m-1}\binom{m-1}{l-1}(1-F(\gamma_i))^{l-1}\left[F^{n-m}(x_i)(F(\gamma_i)-F(x_i))^{m-l}+(m-l)\int_{0}^{F(x_i)}t^{n-m}(F(\gamma_i)-t)^{m-l-1}\, dt\right]}{J(F,n,m,x_i)}
\]

contestant \( i \) sees her probability of achieving rank \( l = m \) is given by:
\[
\Pr\left [ \bigwedge_{j=2}^{m} X_j > \gamma_i \mid x_i, \mathcal{I} \right ]
\]

We consider different cases based on the value of \(\gamma_i\):

\underline{Case 1:} When \(\gamma_i > x_i\), we have \(x^{(1)} > x_i\). The expression becomes:
\[
\begin{aligned}
    & \underbrace{\int_{\gamma_i}^{1} \cdots \int_{\gamma_i}^{1}}_{m-1} \frac{\binom{n-1}{m-1} F^{n-m}(x_i) \prod_{k=2}^{m} f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-i}} \\
    = & \binom{n-1}{m-1} F^{n-m}(x_i) [1-F(\gamma_i)]^{m-1}
\end{aligned}
\]

\underline{Case 2:} When \(\gamma_i \leq x_i\), we decompose the integration space into two parts:

\begin{enumerate}
\item \underline{First Part:} When \(x^{(1)} > x_i\), the integral is:
\[
\begin{aligned}
    & \underbrace{\int_{x_i}^{1} \cdots \int_{x_i}^{1}}_{m-1} \frac{\binom{n-1}{m-1} F^{n-m}(x_i) \prod_{k=2}^{m} f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-i}} \\
    = & \binom{n-1}{m-1} F^{n-m}(x_i) [1-F(x_i)]^{m-1}
\end{aligned}
\]
\item \underline{Second Part:} When \(x^{(1)} \leq x_i\), the integral is:
\[
\begin{aligned}
    & (m-1) \int_{\gamma_i}^{x_i} \underbrace{\int_{x_2}^{1} \cdots \int_{x_2}^{1}}_{m-2} \frac{\binom{n-1}{m-1} F^{n-m}(x_i) \prod_{k=2}^{m} f(x_k)}{J(F,n,m,x_i)} \, d\mathbf{x_{-[l]\cup{\{m\}}}} \, dx_m \\
    = & \binom{n-1}{m-1}(m-1) \int_{F(\gamma_i)}^{F(x_i)} t^{n-m} (1-t)^{m-2} \, dt
\end{aligned}
\]
\end{enumerate}

Combining both parts and factoring out common terms, we obtain the final expression:
\[
\frac{\binom{n-1}{m-1} \left [ F^{n-m}(x_i) (1-F(x_i))^{m-1} + (m-1) \int_{F(\gamma_i)}^{F(x_i)} t^{n-m} (1-t)^{m-2} \, dt \right ]}{J(F,n,m,x_i)}
\]

Based on the above discussion, we have derived the expression for \(P_{(i,l)}(\gamma_i \mid x_i)\). Next, we will verify the continuity of \(P_{(i,l)}\). It suffices to prove that the function is right-continuous at \(x_i\).

We then compute the right limit of $P_{(i,l)}$ at \(x_i\). Since the denominator is independent of \(\gamma_i\), we focus on the limit of the numerator:

When \(l = m\), the numerator is:
\[
\binom{n-1}{m-1}F^{n-m}(x_i)(1-F(x_i))^{m-1}
\]

When \(l < m\), the numerator is:
\[
\binom{n-1}{m-1}\binom{m-1}{l-1}(1-F(x_i))^{l-1}(m-l)\int_{0}^{F(x_i)}t^{n-m}(F(x_i)-t)^{m-l-1}\, dt
\]

Substituting the equality from Lemma~\ref{lem:betaUpper}:
\[
\begin{aligned}
    & \binom{n-1}{m-1}\binom{m-1}{l-1}(1-F(x_i))^{l-1}(m-l)\frac{(n-m)!(m-l-1)!}{(n-l)!}F(x_i)^{n-l} \\ 
    = & \frac{(n-1)!}{(n-m)!(m-1)!} \frac{(m-1)!}{(l-1)!(m-l)!}(1-F(x_i))^{l-1}\frac{(n-m)!(m-l)!}{(n-l)!}F(x_i)^{n-l} \\
    = & \binom{n-1}{l-1}(1-F(x_i))^{l-1}F(x_i)^{n-l} 
\end{aligned}
\]

In summary, we obtain:
\[
\begin{aligned}
\lim_{\gamma_i \rightarrow x_i^{+}}P_{(i,l)}(\gamma_i \mid x_i)  & = \begin{cases} 
\frac{\binom{n-1}{l-1}(1-F(x_i))^{l-1}F(x_i)^{n-l} }{J(F,n,m,x_i)} & \text{if } l < m, \\
\frac{\binom{n-1}{m-1}F^{n-m}(x_i)(1-F(x_i))^{m-1}}{J(F,n,m,x_i)} & \text{if } l = m.
\end{cases} \\
& = P_{(i,l)}(x_i \mid x_i) 
\end{aligned}
\]

Therefore, \(P_{(i,l)}(\gamma_i \mid x_i)\) is continuous throughout \(\gamma_i \geq 0\), thus completing the proof.
\end{proof}

\begin{corollary}[Derivative of Winning Probability]\label{coro:DerivProb}
When admitted contestant $i$ reports an ability $\gamma_i$ given her ability $x_i$, the derivative function of her perceived probability of getting rank $l$ respect to $\gamma_i$, denoted as $P'_{(i,l)}(\gamma_i \mid x_i)$, exists. Specifically, the derivative at $x_i$ can be expressed as: 
\[
P'_{(i,l)}(x_i\mid X_i=x_i) =   
\begin{cases} 
\frac{\binom{n-1}{l-1}\left [ (n-l)(1-F(x_i))-(l-1)F(x_i)\right ]F(x_i)^{n-l-1}(1-F(x_i))^{l-2}f(x_i)}{J(F,n,m,x_i)} & \text{if } l < m, \\
- \frac{\binom{n-1}{m-1}(m-1)F^{n-m}(x_i)(1-F(x_i))^{m-2}f(x_i)}{J(F,n,m,x_i)} & \text{if } l = m.
\end{cases}
\]
We denote $P'_{(i,l)}$ as a shorthand for $P'_{(i,l)}(x_i\mid X_i=x_i)$. 
\end{corollary}
\begin{proof}[Proof of Corollary~\ref{coro:DerivProb}]
From Proposition~\ref{prop:winProb}, it is easy to see \( P_{(i,l)} \) has derivatives when \(\gamma_i \neq x_i\). We only need to prove the existence of the derivative at \(\gamma_i = x_i\), which means showing that the left-hand and right-hand derivatives at \(\gamma_i = x_i\) are equal. Since the denominator \( J(F,n,m,x_i) \) is independent of \(\gamma_i\), we omit it in our discussion. We will consider the cases \( l < m \) and \( l = m \) separately.

\underline{Case 1:} \( l < m \) :

When \(\gamma_i \leq x_i\), the derivative of the numerator is:
\[
\begin{aligned}
    & \left[ \binom{n-1}{l-1} F(\gamma_i)^{n-l} (1-F(\gamma_i))^{l-1} \right]' \\
    = & \binom{n-1}{l-1} \left[ (n-l)(1-F(\gamma_i)) - (l-1)F(\gamma_i) \right] F(\gamma_i)^{n-l-1} (1-F(\gamma_i))^{l-2} f(\gamma_i)
\end{aligned}
\]

Substituting \(\gamma_i = x_i\), the numerator of the left-hand derivative \( P'^{-}_{(i,l)}(x_i \mid x_i) \) is:
\[
\binom{n-1}{l-1} \left[ (n-l)(1-F(x_i)) - (l-1)F(x_i) \right] F(x_i)^{n-l-1} (1-F(x_i))^{l-2} f(x_i)
\]

When \(\gamma_i > x_i\), the numerator of the expression for \( P_{(i,l)} \) expands to:
\[
\binom{n-1}{m-1} \binom{m-1}{l-1} F^{n-m}(x_i) (1-F(\gamma_i))^{m-1} + \binom{n-1}{m-1} \binom{m-1}{l-1} (m-l) \int_{0}^{F(x_i)} t^{n-m} (F(\gamma_i)-t)^{m-l-1} \, dt
\]

We handle each part separately.
After differentiating the first part, we obtain:
\begin{multline*}
    \binom{n-1}{m-1} \binom{m-1}{l-1} \left[ (m-l)(1-F(\gamma_i)) - (l-1)(F(\gamma_i)-F(x_i)) \right] \\
    \cdot F^{n-m}(x_i) (1-F(\gamma_i))^{l-2} (F(\gamma_i) - F(x_i))^{m-l-1} f(\gamma_i)
\end{multline*}

This expression is continuous, so the right-hand limit at \(\gamma_i = x_i\) can be obtained by direct substitution. When \( l < m-1 \), \((F(\gamma_i) - F(x_i))^{m-l-1} = 0\), making this part's right-hand limit zero. When \( l = m-1 \), \((F(\gamma_i) - F(x_i))^{m-l-1} = 1\), and the right-hand limit then becomes:
\begin{equation} \label{eq:derEq1}
    \binom{n-1}{m-1} \binom{m-1}{l-1} F^{n-l-1}(x_i) (1-F(x_i))^{l-1} f(x_i)
\end{equation}

In the second part, we extract the term \( F(\gamma_i) \) from the integrand in the last term:
\[
\begin{aligned}
    \int_{0}^{F(x_i)} t^{n-m} (F(\gamma_i) - t)^{m-l-1} \, dt &= F^{n-l}(\gamma_i) \int_{0}^{F(x_i)} \left(\frac{t}{F(\gamma_i)}\right)^{n-m} \left(1 - \frac{t}{F(\gamma_i)}\right)^{m-l-1} \left(F(\gamma_i)^{-1} dt\right) \\
    &= F^{n-l}(\gamma_i) \int_0^{\frac{F(x_i)}{F(\gamma_i)}} t^{n-m} (1-t)^{m-l-1} \, dt
\end{aligned}
\]

Thus, the expression of this part becomes:
\[
\binom{n-1}{m-1} \binom{m-1}{l-1} (m-l) F^{n-l}(\gamma_i) (1-F(\gamma_i))^{l-1} \int_0^{\frac{F(x_i)}{F(\gamma_i)}} t^{n-m} (1-t)^{m-l-1} \, dt
\]

This expression includes the product of three functions of \(\gamma_i\). After differentiation, it turns into the sum of three components:

The first component involves \([F^{n-l}(\gamma_i)]'\):
\[
\binom{n-1}{m-1} \binom{m-1}{l-1} (n-l) (m-l) F^{n-l-1}(\gamma_i) (1-F(\gamma_i))^{l-1} f(\gamma_i) \int_0^{\frac{F(x_i)}{F(\gamma_i)}} t^{n-m} (1-t)^{m-l-1} \, dt
\]

The limit of the last term at $x_i$ is \(B(n-m+1, m-l)\). Substituting into the Lemma~\ref{lem:betaUpper} and combining other coefficients, we have:
\[
\begin{aligned}
    & \frac{(n-1)!}{(m-1)!(n-m)!} \frac{(m-1)!}{(l-1)!(m-l)!} (n-l) (m-l) \frac{(n-m)!(m-l-1)!}{(n-l)!} \\
    = & \frac{(n-1)!}{(l-1)!(n-l)!} (n-l) = \binom{n-1}{l-1}(n-l)
\end{aligned}
\]

Thus, the right-hand limit of this component is:
\[
\binom{n-1}{l-1} (n-l) F^{n-l-1}(x_i) (1-F(x_i))^{l-1} f(x_i)
\]

The second component involves \([(1-F(\gamma_i))^{l-1}]'\):
\[
-\binom{n-1}{m-1} \binom{m-1}{l-1} (m-l)(l-1) F^{n-l}(\gamma_i) (1-F(\gamma_i))^{l-2} f(\gamma_i) \int_0^{\frac{F(x_i)}{F(\gamma_i)}} t^{n-m} (1-t)^{m-l-1} \, dt
\]

Similarly, substituting the value of \(B(n-m+1, m-l)\) and simplifying, the right-hand limit of this component becomes:
\[
-\binom{n-1}{l-1} (l-1) F^{n-l}(x_i) (1-F(x_i))^{l-2} f(x_i)
\]

The third component involves the expression:
\[
\left[ \int_0^{\frac{F(x_i)}{F(\gamma_i)}} t^{n-m}(1-t)^{m-l-1} \, dt \right]'.
\]

By applying the differentiation rule for integrals with variable upper limits, we obtain:
\[
\begin{aligned}
    \left[ \int_0^{\frac{F(x_i)}{F(\gamma_i)}} t^{n-m}(1-t)^{m-l-1} \, dt \right]' 
    &= - \left( \frac{F(x_i)}{F(\gamma_i)} \right)^{n-m} \left( 1-\frac{F(x_i)}{F(\gamma_i)} \right)^{m-l-1} \frac{F(x_i)}{F^2(\gamma_i)} f(\gamma_i).
\end{aligned}
\]

Now consider the behavior of this term under different $l$:
\begin{enumerate}
\item Case \( l < m-1 \):  
   In this case, we have:
   \[
   \lim_{\gamma_i \to x_i^+} \left( 1-\frac{F(x_i)}{F(\gamma_i)} \right)^{m-l-1} = 0.
   \]
   Consequently, the right-hand limit of this component is \( 0 \).
\item Case \( l = m-1 \):
   Here, the factor \( \left( 1-\frac{F(x_i)}{F(\gamma_i)} \right)^{m-l-1} \) simplifies to \( 1 \). Thus, the right-hand limit of the derivative with respect to the upper limit becomes:
   \[
   -\frac{f(x_i)}{F(x_i)}.
   \]
   The corresponding expression for this component's right-hand limit is:
   \begin{equation} \label{eq:derEq2}
   -\binom{n-1}{m-1} \binom{m-1}{l-1} F^{n-l-1}(x_i) (1-F(x_i))^{l-1} f(x_i).
   \end{equation}
\end{enumerate}

In both cases (\( l < m-1 \) and \( l = m-1 \)), the sum of the two parts from Equation~\eqref{eq:derEq1} and Equation~\eqref{eq:derEq2} equals \( 0 \). Therefore, no additional classification of \( l \) is required.

Finally, combining the results from all parts, we have the right-hand limit of the numerator:
\[
\binom{n-1}{l-1} \left[ (n-l)(1-F(\gamma_i)) - (l-1)F(\gamma_i) \right] F(\gamma_i)^{n-l-1}(1-F(\gamma_i))^{l-2} f(\gamma_i).
\]

Thus, we conclude that for \( l < m \), the following equality holds, as desired:
\[
P'^{-}_{(i,l)}(x_i \mid x_i) = P'^{+}_{(i,l)}(x_i \mid x_i)
\]

\underline{Case 2:} \( l = m \):

If \( \gamma_i \leq x_i \), the derivative's numerator is:
\[
\begin{aligned}
    & \binom{n-1}{m-1} \left[ F^{n-m}(x_i)(1-F(x_i))^{m-1} + (m-1) \int_{F(\gamma_i)}^{F(x_i)} t^{n-m}(1-t)^{m-2} \, dt \right]' \\ 
    = & \binom{n-1}{m-1}(m-1) \left[ \int_{0}^{F(x_i)} t^{n-m}(1-t)^{m-2} \, dt - \int_{0}^{F(\gamma_i)} t^{n-m}(1-t)^{m-2} \, dt \right]' \\ 
    = & -\binom{n-1}{m-1}(m-1) \left[ \int_{0}^{F(\gamma_i)} t^{n-m}(1-t)^{m-2} \, dt \right]' \\
    = & -\binom{n-1}{m-1}(m-1) F^{n-m}(\gamma_i)(1-F(\gamma_i))^{m-2} f(\gamma_i)
\end{aligned}
\]

Substituting \(\gamma_i = x_i\) gives the numerator of left-hand derivative \( P'^{-}_{(i,l)}(x_i \mid x_i) \) as:
\[
-\binom{n-1}{m-1}(m-1) F^{n-m}(x_i)(1-F(x_i))^{m-2} f(x_i)
\]

If \( \gamma_i > x_i \), the derivative's numerator is:
\[
\begin{aligned}
    & \left[ \binom{n-1}{m-1} F^{n-m}(x_i)(1-F(\gamma_i))^{m-1} \right]' \\
    = & -\binom{n-1}{m-1}(m-1) F^{n-m}(x_i)(1-F(\gamma_i))^{m-2} f(\gamma_i)
\end{aligned}
\]

Taking the right-hand limit at $x_i$, we obtain:
\[
-\binom{n-1}{m-1}(m-1) F^{n-m}(x_i)(1-F(x_i))^{m-2} f(x_i)
\]

Thus, we have shown that when \( l = m \), \( P'^{-}_{(i,l)}(x_i \mid x_i) = P'^{+}_{(i,l)}(x_i \mid x_i) \).

In conclusion, for any \( l \leq m \), \( P_{(i,l)}(\gamma_i \mid x_i) \) is differentiable everywhere. Therefore, the derivative \( P'_{(i,l)}(\gamma_i \mid x_i) \) exists and at \( \gamma_i = x_i \), and it is given by:
\[
P'_{(i,l)}(x_i \mid X_i = x_i) =   
\begin{cases} 
\frac{\binom{n-1}{l-1} \left[ (n-l)(1-F(x_i))-(l-1)F(x_i) \right] F(x_i)^{n-l-1}(1-F(x_i))^{l-2} f(x_i)}{J(F,n,m,x_i)} & \text{if } l < m, \\
- \frac{\binom{n-1}{m-1}(m-1) F^{n-m}(x_i)(1-F(x_i))^{m-2} f(x_i)}{J(F,n,m,x_i)} & \text{if } l = m.
\end{cases}
\]
This completes the proof.
\end{proof}


\subsection*{Proof of Theorem~\ref{thm:contestantSBNE}}
\begin{proof}
Under a symmetric Bayesian Nash Equilibrium, each individual's effort \( b(x_i) \) is her best response given her ability \( x_i \), i.e., for all \( i \in [n] \):
\[
b(x_i) \in \mathop{\arg \max}_{e_i} u_i := \sum_{l=1}^{m} V_l P_{(i,l)} - \frac{g(e_i)}{x_i}
\]

Since \( e_i \) and \( \gamma_i \) are in one-to-one correspondence, the expression can be rewritten as:
\[
b(x_i) \in \left\{ b(\gamma_i) \mid \mathop{\arg \max}_{\gamma_i} \sum_{l=1}^{m} V_l P_{(i,l)} - \frac{g(b(\gamma_i))}{x_i} \right\}
\]

Taking the derivative of \( u_i \) with respect to \( \gamma_i \) gives the first-order condition:
\[
\sum_{l=1}^{m} V_l P'_{(i,l)}(\gamma_i \mid X_i = x_i) = \frac{g'(b(\gamma_i)) b'(\gamma_i)}{x_i}
\]

In equilibrium, everyone exerts effort according to \( b(x_i) \), so \( \gamma_i = \gamma(e_i) = \gamma(b(x_i)) = x_i \). Substituting this into the equation and moving the denominator on the right to the left side:
\[
\sum_{l=1}^{m} V_l P'_{(i,l)}(x_i \mid X_i = x_i)x_i = g'(b(x_i)) b'(x_i)
\]

Since the equilibrium condition holds for any realization of ability \( x_i > 0 \), we replace \( x_i \) with a variable \( t \) and integrate both sides over \((0, x_i]\):
\[
\begin{aligned}
\int_{0}^{x_i} \sum_{l=1}^{m} V_l P'_{(i,l)}(t \mid X_i = t)t \, dt &  = \int_{0}^{x_i} g'(b(t)) b'(t) \, dt \\
 & = g(b(x_i)) 
\end{aligned}
\]

Applying the inverse of the cost function \( g \): 
\[
b(x_i) = g^{-1}\left( \int_{0}^{x_i} \sum_{l=1}^{m} V_l P'_{(i,l)}(t \mid X_i = t)t \, dt \right)
\]

By plugging the expression for $P'_{(i,j)}$ into the summation \(\sum_{l=1}^{m}V_lP'_{(i,l)}(t|X_i=t)\):
\[
\begin{aligned} 
    & \frac{\sum_{l=1}^{m-1}\binom{n-1}{n-l}(n-l)F^{n-l-1}(t)(1-F(t))^{l-1}V_l - \sum_{l=1}^m\binom{n-1}{l-1}(l-1)F^{n-l}(t)(1-F(t))^{l-2}V_l}{J(F,n,m,t)f^{-1}(t)} \\
    = & \frac{\sum_{l=1}^{m-1}\binom{n-1}{n-l}(n-l)F^{n-l-1}(t)(1-F(t))^{l-1}V_l - \sum_{l=2}^m\binom{n-1}{l-1}(l-1)F^{n-l}(t)(1-F(t))^{l-2}V_l}{J(F,n,m,t)f^{-1}(t)}
\end{aligned}
\]

Combining like terms, we get:
\[
\begin{aligned}
& \frac{\sum_{l=1}^{m-1}\left [ \binom{n-1}{n-l}(n-l)V_l - \binom{n-1}{l}lV_{l+1}\right ]F^{n-l-1}(t)(1-F(t))^{l-1}f(t)}{J(F,n,m,t)} \\
= & \frac{\sum_{l=1}^{m-1}\binom{n-1}{l-1}(n-l)(V_l-V_{l+1})F^{n-l-1}(t)(1-F(t))^{l-1}f(t)}{J(F,n,m,t)}
\end{aligned}
\]

Therefore, the equilibrium effort must satisfy:
\begin{equation}\label{eq:EquEffort}
        b(x_i) = g^{-1}\left(\int_{0}^{x_i}\frac{\sum_{l=1}^{m-1}\binom{n-1}{l-1}(n-l)(V_l-V_{l+1})F^{n-l-1}(t)(1-F(t))^{l-1}f(t)}{J(F,n,m,t)} t\, dt \right)
\end{equation}

Since we consider rank-order prize structures, i.e., \( V_l \geq V_{l+1}\), the integrand is almost strictly increasing everywhere. Therefore, non-negativity and monotonicity of \( b(x_i) \) are guaranteed. Therefore, we obtain the unique symmetric Bayesian Nash Equilibrium, which completes the proof.
\end{proof}


\section{Missing Proofs in Section \ref{sec:optimal design}}

\subsection*{Proof of Corollary~\ref{coro:Consolation}}
\begin{proof}
    The contribution of \( V_m \) to the contestant $i$'s equilibrium effort is given by:
    \[
    V_m \int_{0}^{x_i} t P'_{(i,m)}(t \mid X_i = t) \, dt
    \]
    
    For \( \forall t > 0 \), we have:
    \[
    P'_{(i,m)}(t \mid X_i = t) = - \frac{\binom{n-1}{m-1}(m-1)F^{n-m}(t)(1-F(t))^{m-2}f(t)}{J(F,n,m,t)} \leq 0
    \]
    
    Furthermore, for a non-zero measure subset of the domain, it holds that \( P'_{(i,m)}(t \mid X_i = t) < 0 \).

    Therefore,
    \begin{align*}
    b(x_i) & = g^{-1} \left( \sum_{l=1}^{m} V_l \int_{0}^{x_i} t P'_{(i,l)}(t \mid X_i = t) \, dt \right) \\
           & < g^{-1} \left( \sum_{l=1}^{m-1} V_l \int_{0}^{x_i} t P'_{(i,l)}(t \mid X_i = t) \, dt \right)
    \end{align*}
    Setting \( V_m = 0 \) will increase the effort for each admitted contestant, which completes the proof. 
\end{proof}


\subsection*{Proof of Corollary~\ref{coro:EmptyPrize}}
\begin{proof}
    For \( m > k+1 \), by Lemma~\ref{lem:normal}, we have:
    \[
    0 < \frac{J(F,n,k+1,t)}{J(F,n,m,t)} < 1
    \]

    From Equation~\eqref{eq:EquEffort} and the rank-order prize structure, we know that \(\sum_{l=1}^{m} V_l P'_{(i,l)}(t \mid X_i = t) \geq 0\). Additionally, for \( l \geq k+1 \), \( V_l = 0 \). Therefore, we have:
    \begin{align*}
        \sum_{l=1}^{k+1} V_l P'_{(i,l)}(t \mid X_i = t, k+1) &= \sum_{l=1}^{k} V_l P'_{(i,l)}(t \mid X_i = t, k+1) \\
        &= \sum_{l=1}^{k} V_l P'_{(i,l)}(t \mid X_i = t, m) \frac{J(F,n,m,t)}{J(F,n,k+1,t)} \\
        &= \sum_{l=1}^{m} V_l P'_{(i,l)}(t \mid X_i = t, m) \frac{J(F,n,m,t)}{J(F,n,k+1,t)} \\
        &> \sum_{l=1}^{m} V_l P'_{(i,l)}(t \mid X_i = t, m)
    \end{align*}

    Multiplying both sides by \( t \) and integrating over \( (0, x_i] \), then applying the inverse cost function \( g^{-1} \), we obtain:
    \begin{align*}
        g^{-1}\left( \int_{0}^{x_i} \sum_{l=1}^{k+1} V_l P'_{(i,l)}(t \mid X_i = t, k+1)t \, dt \right) & > g^{-1}\left( \int_{0}^{x_i} \sum_{l=1}^{m} V_l P'_{(i,l)}(t \mid X_i = t, m)t \, dt \right)
    \end{align*}
    Thus, \( b(x_i; k+1) > b(x_i; m) \), meaning that merely providing more advancement slots will decrease the effort of any participant who was originally advancing, which completes the proof.
\end{proof}

\begin{lemma}\label{lem:binomDesc}
    Let $X \sim Binomial(n,p)$, then $\Pr(X = m \mid X \leq m)$ decreases as \(m\) increases.  
\end{lemma}
\begin{proof}
    First, observe that:
    \begin{align*}
    \frac{\Pr(X = m)}{\Pr(X = m+1)} & = \frac{\binom{n}{m} p^{m} (1-p)^{n-m}}{\binom{n}{m+1} p^{m+1} (1-p)^{n-m-1}} \\
                                & = \frac{m+1}{n-m} \cdot \frac{p}{1-p}
    \end{align*}
    This ratio increases as \(m\) increases.

    Therefore, we have:
    \begin{align*}
        \Pr(X \leq m) & = \sum_{j=0}^{m} \Pr(X = j) \\
                    & \leq \frac{m+1}{n-m} \cdot \frac{p}{1-p} \sum_{j=0}^{m+1} \Pr(X = j) \\
                    & = \frac{m+1}{n-m} \cdot \frac{p}{1-p} \cdot \Pr(X \leq m+1)
    \end{align*}
    Since \(\Pr(X = m \mid X \leq m) = \frac{\Pr(X = m)}{\Pr(X \leq m)}\), consider the ratio of consecutive terms:
    \begin{align*}
        \frac{\Pr(X = m \mid X \leq m)}{\Pr(X = m+1 \mid X \leq m+1)} & = \frac{\Pr(X = m)}{\Pr(X = m+1)} \cdot \frac{\Pr(X \leq m+1)}{\Pr(X \leq m)} \\
            & > \left( \frac{m+1}{n-m} \cdot \frac{p}{1-p} \right) \left( \frac{n-m}{m+1} \cdot \frac{1-p}{p} \right) \\
            & = 1 
    \end{align*}
    Since probabilities are non-negative, \(\Pr(X = m \mid X \leq m)\) decreases as \(m\) increases.
\end{proof}

% \begin{proof}[Proof of Remark~\ref{rmk:TrivialZero}]
%     A trivial simple contest involves two cases, either $V_1 = 0$ or $V_1 = \ldots = V_m \neq 0$. In either case, $\forall l \in [m-1]$, we have $V_l - V_{l+1} = 0$. Also, by definition, $g^{-1}(0) = 0$, therefore, by Theorem~\ref{thm:playerSBNE}, under the symmetric Bayesian Nash Equilibrium, $b(x) = 0$ for any realization of $x$. Thus, every contestant exerts zero effort, which completes the proof.
% \end{proof}

\subsection*{Proof of Proposition~\ref{prop:DesignGuideline}}
\begin{proof}
    We first discuss the ex-post utility when the designer only cares about the effort from ranking $i$. The utility is expressed as:
    \[
    \begin{aligned}
        u_d(\vec{x})  & = c_i b(x_{(i)}) \\
             & = c_ig^{-1}\left(\int_{0}^{x_{(1)}}\frac{\sum_{l=1}^{m-1}\binom{n-1}{l-1}(n-l)(V_l-V_{l+1})F^{n-l-1}(t)(1-F(t))^{l-1}f(t)}{J(F,n,m,t)} t\, dt \right)
    \end{aligned}
    \]
    
    Since $g(\cdot)$ is an monotonically increasing function, so does the inverse $g^{-1}(\cdot)$. Also, $c_i$ is a positive constant, then we only have to optimize over the inner expression. 

    The designer's optimization problem then can be written as :
    \[
    \begin{aligned}
        \mathop{\arg \max}_{m,\vec{V}} \quad & \sum_{l=1}^{m-1}(V_l-V_{l+1})\int_{0}^{x_{(i)}}\frac{\binom{n-1}{l-1}(n-l)F^{n-l-1}(t)(1-F(t))^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}f(t) t \, dt \\
        s.t. \quad & \sum_{l=1}^{m} V_l \leq B \\
            & V_l  \geq V_{l+1} \geq 0,
    \end{aligned}
    \]
    where we use Lemma~\ref{lem:normal} to simplifies the expression of $J(F,n,m,t)$. 

    Let $Z_l=l(V_l-V_{l+1})$, we know from Corollary~\ref{coro:Consolation} that the optimal contest design should satisfy $V_m=0$, hence the above optimization problem becomes:
    \begin{equation}\label{eq:ExpostHighestObj}
    \begin{aligned}
        \mathop{\arg \max}_{m,\vec{Z}} \quad & \sum_{l=1}^{m-1}Z_l\int_{0}^{x_{(1)}}\frac{\binom{n-1}{l-1}\frac{n-l}{l}F^{n-l-1}(t)(1-F(t))^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}f(t) t \, dt \\
        s.t. \quad & \sum_{l=1}^{m-1} Z_l \leq B \\
            & Z_l \geq 0,
    \end{aligned}
    \end{equation}
    where it be easily verified that $\sum_{l=1}^{m-1} Z_l = \sum_{l=1}^{m} V_l -mV_m\leq B-0=B$. 

    Since for any given $m$, the coefficient for $Z_l$ is a positive constant that determined by the realization of $i^\text{th}$ order statistics of ability $x_{(1)}$, the optimal solution for the problem must have some $Z_{l^*}=B$, i.e., a non-trivial simple contest with $V_1=V_2=\ldots=V_{l^*}=\frac{B}{l^*}$, as desired. 

    Next, we consider the ex-post utility when the cost function is linear. The utility is given as: 
    \[
    \begin{aligned}
        u_d(\vec{x})  & =\vec{c}\cdot e(\vec{x}) \\
             & = \sum_{i=1}^{m}c_ik^{-1}\int_{0}^{x_i}\frac{\sum_{l=1}^{m-1}\binom{n-1}{l-1}(n-l)(V_l-V_{l+1})F^{n-l-1}(t)(1-F(t))^{l-1}f(t)}{J(F,n,m,t)} t\, dt \\
             & = \sum_{l=1}^{m-1}(V_l-V_{l+1}) \left [ \sum_{i=1}^{m} c_ik^{-1} \left ( \int_{0}^{x_{i}}\frac{\binom{n-1}{l-1}(n-l)F^{n-l-1}(t)(1-F(t))^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}f(t) t \, dt \right ) \right ],
    \end{aligned}
    \]
    which is still a linear combination of $(V_l-V_{l+1})$s. 

    Again, we obtain the optimization problem:
    \[
    \begin{aligned}
        \mathop{\arg \max}_{m,\vec{Z}} \quad & \sum_{l=1}^{m-1}Z_l \left [ \sum_{i=1}^{m} c_ik^{-1} \left ( \int_{0}^{x_{i}}\frac{\binom{n-1}{l-1}\frac{n-l}{l}F^{n-l-1}(t)(1-F(t))^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}f(t) t \, dt \right ) \right ] \\
        s.t. \quad & \sum_{l=1}^{m-1} Z_l \leq B \\
            & Z_l \geq 0,
    \end{aligned}
    \] the optimal solution of which is still a simple contest, as desired. 

    As for the ax-ante utilities, it follows directly that the optimal contest are simple contests, since the linearity of expectation operator and cost function enable us to extract out $(V_l-V_{l+1})$s, the optimization objective of the designer is still a non-negative combination of $Z_l$s, thus the optimal contest must be a non-trivial simple contest. 

    To sum up, we now conclude that if the designer's utility is a non-negative linear combination of admitted contestants' effort under the equilibrium, whether ex-ante or ex-post, as long as the designer only cares about effort of single ranking or the cost function is linear, the optimal contests are always non-trivial simple contests, which completes the proof.  
\end{proof}


\subsection*{Proof of Theorem~\ref{thm:ExpostHighestEffort}}
\begin{proof}
    From Equation~\ref{eq:ExpostHighestObj} in Proposition~\ref{prop:DesignGuideline}, the designer's maximization objective becomes:
    \[
    \begin{aligned}
        & \int_{0}^{x_{1}}\frac{\binom{n-1}{l-1}\frac{n-l}{l}F^{n-l-1}(t)(1-F(t))^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}f(t) t \, dt \\
        = & \int_{0}^{x_{1}}\frac{\binom{n-1}{l}F^{n-l-1}(t)(1-F(t))^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}f(t) t \, dt \\
        = & \int_{0}^{x_{1}}\frac{\binom{n-1}{l}F^{n-l-1}(t)(1-F(t))^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}} \frac{f(t)}{1-F(t)} t \, dt,
    \end{aligned}
    \]
    where the numerator of the first term in the integrand indicate the probability of a contestant with ability $t$ to get rank $l+1$ among all contestants, while the denominator denotes her chance of getting admitted into the contest with shortlist capacity $m$.

    To find the optimal simple contest, we have to identify the best $m$. Since the designer only cares about the highest effort $b(x_1)$, from Corollary~\ref{coro:EmptyPrize}, we know immediately that the optimal contest must satisfy $m^*=l^{*}+1$, where $l^*$ is the number of prize in the optimal contest.

    Substituting the relation of $m$ and $l$ when optimal, the optimization problem now becomes:
    \[
    \begin{aligned}
         \mathop{\arg \max}_{m} &  \int_{0}^{x_{1}}\frac{\binom{n-1}{m-1}F^{n-m}(t)(1-F(t))^{m-1}}{\sum_{j=0}^{m-1}\binom{n-1}{j}F^{n-j-1}(t)(1-F(t))^{j}}\frac{f(t)}{1-F(t)} t \, dt \\
        s.t. & \quad m \in [n]\backslash\{1\}.
    \end{aligned}
    \]

    The first term in the integrand of the objective corresponds to $\Pr(X=m-1\mid X \leq m-1)$, where $X\sim Binomial(n-1,1-F(t))$. It follows from Lemma~\ref{lem:binomDesc} that for all $t \geq0$, this term attains its maximum at $m=2$. So the objective also attains its maximum when $m=2,l=1$.

    Finally, we conclude that the optimal contest that maximize ex-post highest effort is a 2-contestant winner-take-all contest, the corresponding effort (or designer's utility $u_d$) expresses as: 
    $$
    \begin{aligned}
        & g^{-1}\left ( B \int_{0}^{x_{1}} \frac{(n-1)F^{n-2}(t)f(t)t}{F^{n-1}(t)+(n-1)F^{n-2}(t)(1-F(t))} \, dt \right ) \\
        = & g^{-1}\left ( B \int_{0}^{x_{1}} \frac{(n-1)f(t)t}{F(t)+(n-1)(1-F(t))} \, dt \right )
    \end{aligned}
    $$
    which completes the proof.
\end{proof}


\subsection*{Proof of Corollary~\ref{coro:OptimalMaximumEffort}}
\begin{proof}
    To find the optimal contest that maximize ex-ante highest effort, the problem can be written as:
    \[
    \mathop{\arg \max}_{m,\vec{V}} \quad\mathbb{E}_{x \sim X_{(1)}}[b(x;m,\vec{V})],
    \]
    where we use $b(\cdot;m,\vec{V})$ to denote the symmetric Bayesian Nash Equilibrium that uniquely determined by contest configuration $(m,\vec{V})$. 

    Theorem~\ref{thm:ExpostHighestEffort} states that the 2-contestant winner-take-all contest maximize $b(x;m,\vec{V})$ for any given $x$, i.e., for any other contest $(m', \vec{V}')$ and $\forall x\geq0$, we have $b(x;2,Be_1) \geq b(x;m',\vec{V}')$. 

    Therefore, $\mathbb{E}_{x\sim X_{(1)}}[b(x;2,Be_1)]\geq \mathbb{E}_{x\sim X_{(1)}}[b(x;m',\vec{V}')]$, i.e., the 2-contestant winner-take-all contest maximizes ex-ante highest effort, which completes the proof.
\end{proof}

\subsection*{Proof of Theorem \ref{thm: opt for total}}
\begin{proof}
    This theorem can be concluded by Proposition \ref{thm:ConpleteSimpleContest} and \ref{thm:OptAsmLinear}
\end{proof}

\subsection*{Proof of Proposition~\ref{thm:ConpleteSimpleContest}}
\begin{proof}
    Since we consider total effort under linear cost function, Proposition~\ref{prop:DesignGuideline} gives that the optimal contest is a non-trivial simple contest, with capacity $m$ and $l$ equal prizes.

    We will show that when $l$ is fixed, total effort decrease with $m$ for $l+1\leq m\leq n$. 

    We begin by introducing several notations, the first is order statistics. We use $X_{(k)} \sim F_{(k,n)}$ to denote the $k^{\text{th}}$ highest ability among all contestants, then:
    $$f_{(k,n)}(x) = n \binom{n-1}{k-1} (1-F(x))^{k-1} F(x)^{n-k} f(x) $$
    $$F_{(k,n)}(x) = n \binom{n-1}{k-1} \int_0^x (1-F(t))^{k-1} F(t)^{n-k} f(t) \,dt $$.

    Additionally, define:
    \[
    A(m, n, x) = \sum_{j=1}^m \binom{n-1}{j-1} F(x)^{n-j} (1-F(x))^{j-1},
    \]when $m=n$, $A(m, n, x) = 1$; when $m<n$, $\frac{dA}{dx} = (n-m) \binom{n-1}{m-1} F(x)^{n-m-1} (1-F(x))^{m-1} f(x) = f_{(m,n-1)}(x)$, therefore $A(m, n, x) = F_{(m,n-1)}(x) $.

    Then use these notation to rewrite the expression of total effort:
    $$
\begin{aligned}   
S(m,n,l) = &\sum_{i=1}^m n\binom{n-1}{i-1} \int_0^{\infty} \int_0^{x} \frac{\frac{1}{l} \cdot f_{(l,n-1)}(t) \cdot t}{A(m,n,t)}\, dt \cdot F(x)^{n-i}(1-F(x))^{i-1}f(x)\,dx \\= &\sum_{i=1}^m n\binom{n-1}{i-1} \int_0^{\infty} \int_t^{\infty} F(x)^{n-i}(1-F(x))^{i-1}f(x)\,dx \cdot \frac{\frac{1}{l} \cdot f_{(l,n-1)}(t) \cdot t}{A(m,n,t)} \,dt \\=& \frac{n}{l} \int_0^{\infty} \int_t^{\infty} \frac{f_{l,n-1}(t) \cdot t}{A(m,n,t)} \cdot A(m,n,x)f(x)\,dx \,dt \\=& \frac{n}{l} \int_0^{\infty} \frac{f_{l,n-1}(t)t}{A(m,n,t)} \,dt \int_t^{\infty} A(m,n,x)f(x)\, dx \\
=& \frac{n}{l} \int_0^{\infty} f_{l,n-1}(t)t  \int_t^{\infty} \frac{A(m,n,x)f(x)}{A(m,n,t)}\, dx \, dt
\end{aligned}
$$

    Now, define $h(m,t)$ to denote the inner integration:
    \[
    h(m, t) = \frac{\int_t^\infty A(m, n, x) f(x) dx}{A(m, n, t)} = \frac{\int_t^\infty\sum_{j=1}^m \binom{n-1}{j-1} F(x)^{n-j} (1-F(x))^{j-1}f(x)dx}{\sum_{j=1}^m \binom{n-1}{j-1} F(t)^{n-j} (1-F(t))^{j-1} }
    \]

    Consider the fraction of $j^\text{th}$ corresponding terms, we denote as $g(j,t)$:
    \[
    g(j,t)=\frac{\int_t^\infty \binom{n-1}{j-1} F(x)^{n-j} (1-F(x))^{j-1}f(x)dx}{\binom{n-1}{j-1} F(t)^{n-j} (1-F(t))^{j-1} }=\frac{\int_q^1 u^{n-j}(1-u)^{j-1}du}{q^{n-j}(1-q)^{j-1}},
    \]where we let $u=F(x)$ and $q=F(t)$ (symbol $q$ is then override).

    We will show that $g(j,t)$ is decreasing with $j$. 
    $$\frac{g(j,t)}{g(j-1,t)}=\frac{\int_q^1 u^{n-j}(1-u)^{j-1}du}{\int_q^1 u^{n-j+1}(1-u)^{j-2}du}\cdot \frac{q^{n-j+1}(1-q)^{j-2}}{q^{n-j}(1-q)^{j-1}}=\frac{\int_q^1 u^{n-j+1}(1-u)^{j-2}\cdot \frac{1-u}{u}du}{\int_q^1 u^{n-j+1}(1-u)^{j-2}du}\cdot\frac{q}{1-q}$$

    Because $\frac{1-u}{u}$ decrease with $u$, also it holds in the integration interval that $u\ge q$, so we have $\frac{1-u}{u}\le \frac{1-q}{q}$. Furthermore,
    $$\frac{g(j,t)}{g(j-1,t)}\le \frac{\int_q^1 u^{n-j+1}(1-u)^{j-2}\cdot \frac{1-q}{q}du}{\int_q^1 u^{n-j+1}(1-u)^{j-2}du}\cdot\frac{q}{1-q}=1,$$ (the equal sign binds only when integration of $u^{n-j+1}(1-u)^{j-2}$ over $u>q$ is $0$, thus, generally, we can assume that the inequality strictly holds). 

    Hence, $g(m,t)< g(m-1,t) < \ldots < g(1,t)$, showing that  $g(j,t)$ is decreasing with $j$. 

    We now prove by induction that $h(m,t)< h(m-1,t)$.

    \underline{Base Case:}
    First, we verify the base case. 
    $$h(1,t)=g(1,t)=\frac{\frac{1}{n}(1-q^n)}{q^{n-1}}$$
$$
\begin{aligned}
h(2,t)= & \frac{\sum_{j=1}^2 \binom{n-1}{j-1}\int_q^1 u^{n-j}(1-u)^{j-1}du}{\sum_{j=1}^2 \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1} }\\
= &\frac{\sum_{j=1}^2 \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}\cdot g(j,t)}{\sum_{j=1}^2 \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1} } \\ < & g(1,t) =h(1,t)
\end{aligned}
$$

Similarly, we can show that $h(2,t)>g(2,t)$.

    \underline{Inductive Hypothesis:}
    Assume that $h(j,t)< h(j-1,t)$ holds for all $j\leq m-1$.

    \underline{Induction Step:}
    We now prove that $h(m,t)< h(m-1,t)$. 

    $$
\begin{aligned}
& h(m,t)  \\ = &\frac{\sum_{j=1}^{m-1} \binom{n-1}{j-1}\int_q^1 u^{n-j}(1-u)^{j-1}du+\binom{n-1}{m-1}\int_q^1 u^{n-m}(1-u)^{m-1}du}{\sum_{j=1}^{m-1} \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+\binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}} \\ =&\frac{h(m-1,t)\cdot \sum_{j=1}^{m-1} \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+g(m,t)\cdot \binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}}{\sum_{j=1}^{m-1} \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+\binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}} \\ <&\frac{h(m-1,t)\cdot \sum_{j=1}^{m-1} \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+g(m-1,t)\cdot \binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}}{\sum_{j=1}^{m-1} \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+\binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}} \\ <&\frac{h(m-1,t)\cdot \sum_{j=1}^{m-1} \binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+h(m-1,t)\cdot \binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}}{\sum_{j=1}^{m-1}\binom{n-1}{j-1} q^{n-j} (1-q)^{j-1}+\binom{n-1}{m-1} q^{n-m} (1-q)^{m-1}} \\=&h(m-1,t)
\end{aligned}
$$

Similarly, it can be shown that $h(m,t)>g(m,t)$. Thus, by the principle of mathematical induction, $g(j,t)<h(j,t)<h(j-1,t)$ holds for any $j=1,\ldots,m$.

Now the total effort becomes: 
$$
\begin{aligned}
S(m, n, l) & = \frac{n}{l}\int_0^\infty f_{(l,n-1)}(t)t h(m,t)dt \\ &< \frac{n}{l}\int_0^\infty f_{(l,n-1)}(t)t h(m-1,t)dt
\\ &= S(m-1,n,l)
\end{aligned}
$$

Therefore, $S(m,n,l)$ decrease with $m$, also $m>l$, thus, we conclude with $S(m,n,l)<S(l+1,n,l)$. 

So far we have showed that in total effort setting, the optimal simple contest must satisfies $l=m-1$, hence it is a complete simple contest, which complete the proof.
\end{proof}

\subsection*{Proof of Lemma~\ref{lem:betaRepTotalEffort}}
\begin{proof}
    Let $q=F(t)$ ($q$ therefore no longer stand for quantile), we then define $a(j,q)=\int_q^1\binom{n-1}{j-1}u^{n-j}(1-u)^{j-1}\,du$ and $b(j,q)=\binom{n-1}{j-1}q^{n-j}(1-q)^{j-1}$.

    Ex-ante total effort now rewrites as:
    \[
    S(m)=\int_0^1\frac{F^{-1}(q)}{1-q}b(m,q)\frac{\sum_{j=1}^{m}a(j,q)}{\sum_{j=1}^{m}b(j,q)}\,dq
    \]

    We first simplify the summation term in the denominator, i.e., $B(m,q)=\sum_{j=1}^{m}b(j,q)$, after taking derivative, we get:
    $$
\begin{aligned}
B(m,q)' = &(n-1)q^{n-2}+\sum_{j=2}^m(\binom{n-1}{j-1}(n-j)q^{n-j-1}(1-q)^{j-1}-\binom{n-1}{j-1}(j-1)q^{n-j}(1-q)^{j-2}) \\ = & \sum_{j=2}^m (-\binom{n-1}{j-1}(j-1)q^{n-j}(1-q)^{j-2}+\binom{n-1}{j-1-1}(n-j+1)q^{n-j+1-1}(1-q)^{j-1-1})\\&+\binom{n-1}{m-1}(n-m)q^{n-m-1}(1-q)^{m-1} \\= &\binom{n-1}{m-1}(n-m)q^{n-m-1}(1-q)^{m-1}    
\end{aligned}
$$

Thus, we can use integration to express $B(m,q)$:
$$B(m,q)=B(m,0)+\int_0^qB(m,u)'\,du=\int_0^qB(m,u)'\, du=\int_0^q\beta(u,n-m,m)\,du.$$

By definition, $b(m,q)=\frac{1}{n}\beta(q,n-m+1,m)$, so:
$$A(m,q)=\sum_{j=1}^{m}a(j,q)=\int_q^1B(m,u)\, du=\int_q^1\int_0^u \beta(x,n-m,m)\, dx\, du.$$

Rearrange the order of integration. Since the integration region is a trapezoid, the outer integral can be split into two parts:
$$
\begin{aligned} 
A(m,q)&=\int_0^q\int_q^1\beta(x,n-m,m)\,du\,dx+\int_q^1\int_x^1 \beta(x,n-m,m)\,du\,dx  \\&= \int_0^q(1-q)\beta(x,n-m,m)\,dx+\int_q^1(1-x) \beta(x,n-m,m)\,dx  
\end{aligned}
$$

Thereby, total effort can be simplified using beta distribution:
\[
\begin{aligned}
S(m) &=n\int_0^1\frac{F^{-1}(q)}{1-q}b(m,q)\frac{\sum_{j=1}^{m}a(j,q)}{\sum_{j=1}^{m}b(j,q)}dq 
\\&=n\int_0^1\frac{F^{-1}(q)}{1-q}\frac{1}{n}\beta(q,n-m+1,m)\\&\phantom{a}\cdot\frac{\int_0^q(1-q)\beta(x,n-m,m)dx+\int_q^1(1-x) \beta(x,n-m,m)dx}{\int_0^q\beta(u,n-m,m)du}dq \\&=\int_0^1\frac{F^{-1}(q)}{1-q}\beta(q,n-m+1,m)\big((1-q)+\frac{\int_q^1(1-x) \beta(x,n-m,m)dx}{\int_0^q\beta(u,n-m,m)du}\big)dq \\&=\int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq\\&\phantom{a}+\int_0^1\frac{F^{-1}(q)}{1-q}\beta(q,n-m+1,m)\frac{\int_q^1(1-x) \beta(x,n-m,m)dx}{\int_0^q\beta(u,n-m,m)du}dq \\&=\int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq\\ & \phantom{a}+\int_0^1\frac{F^{-1}(q)q}{1-q}\frac{n}{n-m}\beta(q,n-m,m)\frac{\frac{m}{n}\int_q^1\beta(x,n-m,m+1)dx}{\int_0^q\beta(u,n-m,m)du}dq \\&=\int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq
\\ & \phantom{a}+\int_0^1\frac{F^{-1}(q)q}{1-q}\frac{m}{n-m}\beta(q,n-m,m)\frac{\int_q^1\beta(x,n-m,m+1)dx}{\int_0^q\beta(u,n-m,m)du}dq
\\ &= \int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq\\&\phantom{a}+\int_0^1 F^{-1}(q)\frac{q}{1-q}\frac{m}{n-m}\frac{\beta(q,n-m,m)}{\int_0^q\beta(u,n-m,m)du}\int_q^1\beta(x,n-m,m+1)\,dx\,dq,
\end{aligned}
\]which completes the proof.
\end{proof}

\begin{lemma}\label{lem:1}
    Assume $\beta(q,n-m+1,m+1)$ attain it maximum at $q=\mu$, then when $\mid q-\mu\mid > \delta$, we have $\beta(q,n-m+1,m+1)\le \epsilon$, where $\delta = \sqrt{\frac{k(1-k)}{n}\big(\ln n+\ln \frac{1}{\epsilon^2}+\ln \frac{1}{2\pi k(1-k)}\big)}=\Theta(\sqrt{\frac{\ln n}{n}})$.
\end{lemma}
\begin{proof}
    We begin by finding the point where probability mass of beta distribution attain its maximum. Taking derivative: 
$$\beta(q,\alpha,\beta)'=\beta(q,\alpha,\beta)(\frac{\alpha-1}{q}-\frac{\beta-1}{1-q}).$$

    Then it can be concluded that the Beta distribution function first increases and then decreases. At \(\mu = \frac{\alpha - 1}{\alpha + \beta - 2}\), the derivative equals zero, and the function attains its maximum value, $\beta(q,n-m,m)_{\max}\simeq\frac{\sqrt{n}}{\sqrt{2\pi k(1-k)}}$, where $k=n/m$, when $n$ is large.

For the convenience of discussion, we consider $\beta(q,n-m+1,m+1)$, thus, $\mu=\frac{n-m}{n}=1-k$, 
$$
\begin{aligned}
&\beta(\mu-c,n-m+1,m+1) \\&=\frac{(n+1)!}{(n-m)!m!}(\mu-c)^{n-m}(1-\mu+c)^{m} \\&=\frac{(n+1)\sqrt{2\pi n}(\frac{n}{e})^n}{\sqrt{2\pi (n-m)}(\frac{n-m}{e})^{n-m}\sqrt{2\pi (m)}(\frac{m}{e})^{m}}(\mu-c)^{n-m}(1-\mu+c)^{m}\\&=\frac{\sqrt{n}(\frac{1-k-c}{1-k})^{n-m}(\frac{k+c}{k})^m}{\sqrt{2\pi k(1-k)}} \\&=\frac{\sqrt{n}}{\sqrt{2\pi k(1-k)}}\exp\{(n-m)\ln (1-\frac{c}{1-k})+m\ln (1+\frac{c}{k})\} \\&=\frac{\sqrt{n}}{\sqrt{2\pi k(1-k)}}\exp\{n(1-k)(-\frac{c}{1-k}-\frac{1}{2}\big(\frac{c}{1-k})^2\big) + \big( nk (\frac{c}{k}-\frac{1}{2}(\frac{c}{k})^2)\big) \}\\&=\frac{\sqrt{n}}{\sqrt{2\pi k(1-k)}}\exp\{ \frac{-nc^2}{2k(1-k)}\},   
\end{aligned}
$$
where the second equality derives from Sterling's formula, and the second to last equality derives from Taylor expansion.

To satisfy $\beta(\mu-c,n-m+1,m+1)\le \epsilon$, we have to make the following inequality stands:
$$\ln (\frac{\sqrt{n}}{\sqrt{2\pi k(1-k)}})-\frac{nc^2}{2k(1-k)}\le \ln \epsilon$$

Therefore, 
$$c\ge \delta=\sqrt{\frac{k(1-k)}{n}\big(\ln n+\ln \frac{1}{\epsilon^2}+\ln \frac{1}{2\pi k(1-k)}\big)}=\Theta(\sqrt{\frac{\ln n}{n}}).$$


We aim to ensure that the convergence rate is independent of \( k \). To achieve this, it suffices to choose \( c \) greater than the maximum value of the right-hand side (with respect to \( k \)). Since \( k(1-k) \leq \frac{1}{4} \), it follows that:
\[
\frac{k(1-k)}{n} \ln n \leq \frac{1}{4} \ln n.
\]

Additionally, as \( k(1-k) \to 0 \), we have: 
\[
k(1-k) \ln \frac{1}{2\pi k(1-k)} \to 0.
\]

Therefore, there exists a sufficiently large \( N \) such that for all \( n > N \), the maximum value of the right-hand side does not exceed \( \sqrt{\frac{\ln n}{n}} \). Consequently, as long as the deviation of \( q \) from \( \mu \) exceeds \( \delta = \sqrt{\frac{\ln n}{n}} \), it holds that \( \beta(q, n-m+1, m+1) \leq \epsilon \), as desired.

The analysis is similar for the other side. This completes the proof.
\end{proof}

\begin{lemma}\label{lem:2}
    when $n \rightarrow \infty$, we have:
    $$\int_0^1F^{-1}(q)\beta(q,n-m+1,m)\,dq = F^{-1}(1-k).$$
\end{lemma}
\begin{proof}
    Provided that $F^{-1}(q)\le L$ is bounded and continuous. Therefore for any $\epsilon>0$, there exists $\delta_1>0$, such that when $|q-\mu|=|q-1+k|\le \delta_1$, we have $|F^{-1}(q)-F^{-1}(1-k)|\le \epsilon /2$.

    From Lemma~\ref{lem:1}, for $\epsilon_1=\frac{\epsilon}{2L}$, exists $\delta_2=\Theta(\sqrt{\frac{\ln n}{n}})$, such that $\beta(q,n-m+1,m)<\epsilon_1, if |q-\mu|> \delta_2$.

When $n\rightarrow +\infty$, $\frac{\ln n}{n}=0$, therefore exist $N_1$, such that when $n>N_1$, $\delta_2=\Theta(\sqrt{\frac{\ln n}{n}})<\delta_1$. 

Let $\delta=\delta_1$, therefore:
$$
\begin{aligned}
& \int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq \\ &=\int_{\mu-\delta}^{\mu+\delta}F^{-1}(q)\beta(q,n-m+1,m)dq+ \int_0^{\mu-\delta}F^{-1}(q)\beta(q,n-m+1,m)dq \\ & \phantom{a}+\int_{\mu+\delta}^1F^{-1}(q)\beta(q,n-m+1,m)dq  \\ &\le (1-2\delta)\epsilon_1L+ (F^{-1}(1-k)+\frac{\epsilon}{2})\int_{\mu-\delta}^{\mu+\delta}\beta(q,n-m+1,m)dq \le F^{-1}(1-k)+\epsilon    
\end{aligned}
$$

Similarly, we can prove that:
$$\int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq \ge F^{-1}(1-k)-\epsilon $$

Thus, when $n\rightarrow \infty$, it holds that:
$$\int_0^1F^{-1}(q)\beta(q,n-m+1,m)dq = F^{-1}(1-k),$$
which completes the proof.
\end{proof}

\begin{lemma}\label{lem:3}
    When $q<\mu=1-k$ and $n\rightarrow \infty$, it holds that:
    $$\frac{\beta(q,n-m,m)}{n\int_0^q\beta(x,n-m,m)dx}=\frac{1}{q}-\frac{k}{q(1-q)}.$$
\end{lemma}
\begin{proof}
    We start by making discrete approximation for the numerator. Diving $[0,q]$ into $N$ intervals evenly, $q_j=\frac{j}{N}q,j=0,1,2,\cdots,N$, let $n=N^2$, the fraction becomes:
$$
\begin{aligned}
\frac{\beta(q,n-m,m)}{n\int_0^q\beta(x,n-m,m)dx} & =\frac{\sum_{j=1}^{N-1}(\beta(q_{j+1}-\beta(q_j)))}{n\sum_{j=1}^{N-1}\beta(q_{j})\frac{q}{N}}\\ &= \frac{\sum_{j=1}^{N-1}\beta(q_j)'\frac{q}{N}}{n\sum_{j=1}^{N-1}\beta(q_{j})\frac{q}{N}}\\&=\frac{\sum_{j=1}^{N-1}\beta(q_j)(\frac{1}{q_j}-\frac{k}{q_j(1-q_j)})}{\sum_{j=1}^{N-1}\beta(q_{j})}
\end{aligned}
$$

This fraction can be viewed as weighted average over $N-1$ fraction terms, where the $j^{\text{th}}$ fraction term is $\frac{1}{q_j}-\frac{k}{q_j(1-q_j)}$, and its weight is $\frac{\beta(q_{j})}{\sum_{j=1}^{N-1}\beta(q_{j})}$.

We can compare the weight of two consecutive terms:
$$
\begin{aligned}
\frac{\beta(q_{j+1})}{\beta(q_j)} & =1+\frac{\beta(q_{j+1})-\beta(q_j)}{\beta(q_j)}\\ &=1+n\big(\frac{1}{q_j}-\frac{k}{q_j(1-q_j)}\big)\frac{q}{N}\\ &>1+N\big(\frac{1}{q}-\frac{k}{q(1-q)}\big)\\ &=\lambda_N
\end{aligned}
$$

Then the wight of last term:
$$\frac{\beta_{N-1}}{\sum_{j=1}^{N-1}\beta(q_{j})}\ge\frac{\beta_{N-1}}{\beta_{N-1}\sum_{j=1}^{N-1}\frac{1}{\lambda_N^{j-1}}}\ge \frac{1}{1-\frac{1}{\lambda_N}}=\frac{\lambda_N}{\lambda_N-1}$$

When \( q \leq 1-k-\delta \), we have:
\[
\lambda_N = 1 + \frac{n}{N} \frac{1}{q} \left( 1 - \frac{k}{1-q} \right) \geq 1 + \frac{n}{N} \frac{1}{1-k-\delta} \frac{\delta}{k+\delta}.
\]

Using the definition of \(\delta = \sqrt{\frac{\ln n}{n}}\), this can be further bounded as:
\[
\lambda_N \geq 1 + \frac{4 \delta n}{N} = 1 + \frac{4n \sqrt{\frac{\ln n}{n}}}{N} = 1 + 4\sqrt{\ln n}.
\]

As \( N \to \infty \), it follows that \( \lambda_N \to \infty \). Consequently, in the expression:
\[
\frac{\sum_{j=1}^{N-1} \beta(q_j) \left( \frac{1}{q_j} - \frac{k}{q_j(1-q_j)} \right)}{\sum_{j=1}^{N-1} \beta(q_j)},
\]

the weight of the final term approaches 1. Therefore, the entire ratio converges to the value of the final term. Therefore, we have:
\[
\lim_{n \to \infty} \frac{\beta(q, n-m, m)}{n \int_0^q \beta(x, n-m, m) \, dx} = \lim_{N \to \infty} \left( \frac{1}{q_{N-1}} - \frac{k}{q_{N-1}(1-q_{N-1})} \right)=\frac{1}{q} - \frac{k}{q(1-q)}.
\]

Moreover, the rate of convergence is independent of \( k \). The proof is then completed.
\end{proof}

\subsection*{Proof of Lemma~\ref{lem:AsyRep}}
\begin{proof}
    Recall from Lemma~\ref{lem:betaRepTotalEffort} the beta representation of ex-ante total effort:
    \[
    \begin{aligned}
        S(m,n) = & 
        \int_0^1F^{-1}(q)\beta(q,n-m+1,m)\,dq \\
        & +\int_0^1F^{-1}(q)\frac{q}{1-q} \frac{m}{n-m}\frac{\beta(q,n-m,m)}{\int_0^q\beta(x,n-m,m)\,dx}\int_q^1\beta(x,n-m,m+1)\,dx\, dq.
    \end{aligned}
    \]

    Let:
    \[
    \phi(k) = \int_0^{1-k} F^{-1}(q) \frac{q}{1-q} \frac{k}{1-k} \left( \frac{1}{q} - \frac{k}{q(1-q)} \right) \, dq,
    \]then we aim to show that for any \(\epsilon > 0\), there exists \(N > 0\) such that for all \(n > N\), the inequality \(
\left| \frac{S(m,n)}{n} - \phi(k) \right| \leq \epsilon, \forall k \in (0,1), \) holds.

    %Also recall from Lemma~\ref{lem:1}, that $\beta(q,n-m,m)$ is almost concentrated in the $\delta$ neighborhood of $1-k$, and converges to zero elsewhere. It satisfies that $\delta\sim \Theta(\sqrt{\frac{\ln n}{n}})$.

    The first term in \(\frac{S(m,n)}{n}\) is \(\frac{1}{n} \int_0^1 F^{-1}(q) \beta(q, n-m+1, m) \, dq. \) From the asymptotic properties of the beta distribution, as \(n \to \infty\), \(\beta(q, n-m+1, m)\) becomes concentrated near \(q = 1-k\), and the integral's contribution outside this region vanishes. Furthermore, since \(F^{-1}(q)\) is bounded and \(\beta(q, n-m+1, m)\) scales with \(n\), we have:
\[
\lim_{n \to \infty} \frac{1}{n} \int_0^1 F^{-1}(q) \beta(q, n-m+1, m) \, dq = \lim_{n \to \infty} \frac{F^{-1}(1-k)}{n} = 0.
\]
Thus, the first term vanishes asymptotically.

    The second integration term in the $S(m,n)$ is $\frac{1}{n}\int_0^1F^{-1}(q)\frac{q}{1-q} \frac{m}{n-m}\frac{\beta(q,n-m,m)}{\int_0^q\beta(x,n-m,m)\,dx}\int_q^1\beta(x,n-m,m+1)\,dx\, dq$, which can be separated into three parts based on the integration interval.

    \underline{Part 1:} When $q<1-k-\delta$,

    \(\beta(q, n-m, m)\) is negligible because the beta distribution is concentrated near the $\delta$ neighborhood of \(q = 1-k\) (Lemma~\ref{lem:1}) with $\delta = \Theta(\sqrt{\frac{\ln n}{n}})$, the the convergence speed is independent of $k$. Additionally, for \(q < 1-k-\delta\), \(\int_q^1 \beta(x, n-m, m+1) \, dx \approx 1\). This combines with Lemma~\ref{lem:3}, gives:
    \[
    \begin{aligned}
        & \lim_{n \to \infty} \frac{1}{n}\int_0^{1-k-\delta}F^{-1}(q)\frac{q}{1-q} \frac{m}{n-m}\frac{\beta(q,n-m,m)}{\int_0^q\beta(x,n-m,m)\,dx}\int_q^1\beta(x,n-m,m+1)\,dx\, dq \\
        = & \lim_{n \to \infty} \frac{1}{n}\int_0^{1-k-\delta}F^{-1}(q)\frac{q}{1-q} \frac{m}{n-m}\frac{\beta(q,n-m,m)}{\int_0^q\beta(x,n-m,m)\,dx}\, dq \\
        = & \lim_{n \to \infty} \int_0^{1-k-\delta}F^{-1}(q)\frac{q}{1-q} \frac{m}{n-m}\left (\frac{1}{q} -\frac{k}{q(1-q)} \right)\, dq \\
        = & \int_0^{1-k}F^{-1}(q)\frac{q}{1-q} \frac{k}{1-k}\left (\frac{1}{q} -\frac{k}{q(1-q)} \right)\, dq.
    \end{aligned}
    \]

    \underline{Part 2:} When $q\in (1-k-\delta, 1-k+\delta)$, we denote this integration as $S_2$.

%     For \(q \in (1-k-\delta, 1-k+\delta)\), the beta distribution \(\beta(q, n-m, m)\) is concentrated within the small interval of width \(\delta\). Using the bounds for \(\beta(q, n-m, m)\) and \(\int_q^1 \beta(x, n-m, m+1) \, dx\), we get:
%     \[
% S_2 \leq \int_{1-k-\delta}^{1-k+\delta} F^{-1}(q) \frac{q}{1-q} \frac{k}{1-k} \frac{1-k}{kq} \, dq \leq \int_{1-k-\delta}^{1-k+\delta} \frac{F^{-1}(q)}{1-q} \, dq.
% \]

% Since \(F^{-1}(q)\) is bounded, and \(\delta = \Theta\left(\sqrt{\frac{\ln n}{n}}\right)\), it follows that: \(\lim_{n \to \infty} S_2 = 0.\)
\[
\frac{\beta(q, n-m, m)}{n \int_0^q \beta(x, n-m, m) dx} = \frac{n-m}{nq} \frac{b(m, q)}{B(m, q)} \leq \frac{1-k}{kq}.
\]

Now, for the term \( S_2 \), we have the following bounds:
\[
S_2 \leq \int_{1-k-\delta}^{1-k+\delta} F^{-1}(q) \frac{q}{1-q} \frac{k}{1-k} \frac{1-k}{kq} \int_q^1 \beta(x, n-m, m+1) dx.
\]

Simplifying further, we find:
\[
S_2 \leq \int_{1-k-\delta}^{1-k+\delta} \frac{F^{-1}(q)}{1-q} dq \leq \frac{F^{-1}(1-k+\delta)}{k-\delta} \cdot 2\delta.
\]

Since \( \frac{F^{-1}(1-k+\delta)}{k-\delta} \) is bounded, and with \( \delta = \Theta\left(\sqrt{\frac{\ln n}{n}}\right) \), we conclude that: \(\lim_{n \to \infty} S_2 = 0.\)
    
    %The integrand is bounded by $\beta_{\max}*2\delta \sim \Theta(\sqrt{n})\cdot \Theta(\sqrt{\frac{\ln n}{n}})=\Theta(\sqrt{\ln n})$, which can be ignored since the whole integration is $\Theta(n)$.

    \underline{Part 3:} When $q>1-k+\delta$,

\[
\begin{aligned}
    S_3 & \leq \int_{1-k+\delta}^1 F^{-1}(q) \frac{q}{1-q} \frac{m}{n-m} \frac{\beta(q, n-m, m)}{n(1-\epsilon)} \epsilon \, dq 
\\ & = \int_{1-k+\delta}^1 F^{-1}(q) \frac{m}{n-m} \frac{n-m}{m-1} \beta(q, n-m+1, m-1) \frac{\epsilon}{n(1-\epsilon)} \, dq,
\\ & \leq\int_{1-k+\delta}^1 F^{-1}(q) \frac{2\epsilon^2}{n(1-\epsilon)} \, dq.
\end{aligned}
\]

Since \( F^{-1}(q) \) is bounded, we have:
\[
S_3 \leq \frac{2\epsilon^2}{n(1-\epsilon)} \int v f(v) \, dv = \frac{2\epsilon^2}{n(1-\epsilon)} E(v),
\]where \( E(v) \) denotes the expectation of \( v \), and it is assumed to be bounded. 

Taking the limit as \( n \to \infty \), We get
\(\lim_{n \to \infty} S_3 = 0,
\)

    % Then the second integral in $S(m,n)$ reduces to $\int_0^{1-k}F^{-1}(q)\frac{q}{1-q} \frac{k}{1-k}n\left (\frac{1}{q} -\frac{k}{q(1-q)} \right)\, dq$. 
    
    % We also know that the first integration in $S(m,n)$ converges to $F^{-1}(1-k)$, which follows directly from Lemma~\ref{lem:2}. Therefore when $n\rightarrow \infty$, we have: 
    % $$S(m,n)=F^{-1}(1-k)+\int_0^{1-k}F^{-1}(q)\frac{q}{1-q}\frac{k}{1-k}n(\frac{1}{q}-\frac{k}{q(1-q)})dq$$

    % Formally speaking,
    % $$\lim_{n\rightarrow +\infty}\frac{S(m,n)}{n}=\int_0^{1-k}F^{-1}(q)\frac{q}{1-q}\frac{k}{1-k}(\frac{1}{q}-\frac{k}{q(1-q)})dq.$$
    % This completes the proof.
    To sum up, we conclude that \(
\lim_{n \to \infty} \frac{S(m,n)}{n} = \phi(k),
\) which completes the proof.
\end{proof}


\subsection*{Proof of Proposition~\ref{thm:OptAsmLinear}}
\begin{proof}
    Recall from Lemma~\ref{lem:AsyRep} that: 
    $$\lim_{n\rightarrow +\infty}\frac{S(m,n)}{n}=\int_0^{1-k}F^{-1}(q)\frac{q}{1-q}\frac{k}{1-k}(\frac{1}{q}-\frac{k}{q(1-q)})dq,$$
    we then denote $\phi(k):=\lim_{n\rightarrow +\infty}\frac{S(m,n)}{n}$, which is a function of $k=m/n$.

To find the optimal $m^*(n)$, it is suffice to find the $k$ such that $\phi(k)$ attains its maximum. Since $\phi'(0)=\int_0^1F^{-1}(1-q)\frac{1}{q}dq>0$, $\phi'(k_2)<0$ (where $k_2 \approx 0.3162$),  there exist an $0<k^*<k_2$, such that $\phi'(k^*)=0$, and $\phi(k)$ attains maximum. 

Such $k^*$ can be found by solving the following equation:
$$\frac{d \phi}{dk}=\frac{1}{(1-k)^2}\int_k^1 F^{-1}(1-q)(\frac{1}{q}-(2k-k^2)\frac{1}{q^2})dq=0.$$

This is equivalent to:
$$\int_k^1 F^{-1}(1-q)(\frac{1}{q}-(2k-k^2)\frac{1}{q^2})dq=0,$$
which completes the proof. 
\end{proof}


\subsection*{Proof of Theorem~\ref{thm:UniversalBound}}
\begin{proof}
The proof consists of two parts. First, we demonstrate that \(k_2\) is an upper bound, and then we show that this upper bound is tight by constructing a binding distribution.

\underline{Part 1:} \(k_2\) is an upper bound.

Based on Theorem~\ref{thm:OptAsmLinear}, the optimal \(k\) must satisfy:
\[
\int_k^1 F^{-1}(1-q)\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq=0.
\]

\(r(q)=\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\) is zero at \(q=2k-k^2\), and \(r(q)<0\) for \(k \le q < 2k-k^2\), \(r(q)>0\) for \(1 \ge q > 2k-k^2\). We then Let \(q_0=2k-k^2\).

Since \(F^{-1}(1-q)\) is a decreasing function of \(q\), for any \(k\):
\[
\begin{aligned}
&\int_k^1 F^{-1}(1-q)\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq \\
&=\int_k^{q_0} F^{-1}(1-q)\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq+\int_{q_0}^1 F^{-1}(1-q)\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq \\
&\le F^{-1}(1-q_0)\int_k^{q_0}\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq+F^{-1}(1-q_0)\int_{q_0}^1\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq \\
&= F^{-1}(1-q_0)\int_k^1\left(\frac{1}{q}-(2k-k^2)\frac{1}{q^2}\right)dq \\
&= F^{-1}(1-q_0)\left((2-k)(k-1)-\ln k \right)
\end{aligned}
\]

When \(k\) is the solution \(k_2\) of the equation \(\ln k = (2-k)(k-1)\), the right-hand side of the above inequality is zero. This shows that at \(k=k_2\), the derivative of \(\phi(k)\) is non-positive.

Next, we will show that the optimal $k$ will not appear after $k_2$. 

For \(k>k_2\), let \(c(k)=(2-k)(k-1)-\ln k\), then \(c'(k)=-2k+3-\frac{1}{k}=-\frac{1}{k}\left((2k-1)(k-1)\right)\).

Thus, \(c'(k)\) is first negative and then positive, with a zero at \(k=0.5\), implying that \(c(k)\) first decreases and then increases. Since \(c(k_2)=0\) and \(c(1)=0\), for \(k_2< k < 1\), \(c(k)<0\), and $\phi'(k) < F^{-1}(1-q_0)c(k) < 0$, so \(S(n,m)\) will not achieve its maximum for \(k>k_2\).

Therefore, there exists a linear upper bound for the optimal \(m\) in terms of \(n\), i.e., \(\lim_{n \rightarrow \infty} \frac{m^*(n)}{n} \leq k_2\), where \(k_2\) is the non-trivial solution of the equation \(\ln k = (2-k)(k-1)\).

\underline{Part 2:} \(k_2\) is binding.

We now construct a probability distribution such that the corresponding optimal \(k^*\) approaches \(k_2\) asymptotically.

Consider \(f(v)\), which is the probability mass function of a Beta distribution \(f(v; \alpha, \beta)\). As \(\alpha + \beta \to \infty\), \(f(v)\) concentrated around a single point \(\mu = \frac{\alpha - 1}{\alpha + \beta - 2}\). Let \(F^{-1}(1-q)\) be written as \(v(q)\), where \(v(q)\) is a decreasing function of \(q\). The absolute value of the derivative of \(v(q)\) with respect to \(q\) is given by \(|v'(q)| = \frac{1}{f(v)}\). Since \(f(v)\) is uni-modal, increasing and then decreasing as \(q\) varies from \(k\) to 1, \(v'(q)\) is always negative, and \(|v'(q)|\) first decreases and then increases.

By Lemma~\ref{lem:1}, for any \(\epsilon > 0\), if \(|x-\mu| > \delta = \sqrt{\frac{k(1-k)}{n} \left( \ln n + \ln \frac{1}{\epsilon^2} + \ln \frac{1}{2 \pi k(1-k)} \right)} = \Theta\left(\sqrt{\frac{\ln n}{n}}\right)\), then \(f(x) < \epsilon\). This implies that the Beta distribution \(f(v; \alpha, \beta)\) integrates to \(\epsilon_1\) and \(\epsilon_2\) over the intervals \([\mu + \delta, 1]\) and \([0, \mu - \delta]\), respectively, with \(\epsilon_1 + \epsilon_2 < \epsilon\) holds.

So, for \(v(q)\), when \(q > \epsilon_1\), \(v(q) < \mu + \delta\); and when \(q > 1 - \epsilon_2\), \(v(q) < \mu - \delta\). As \(n = \alpha + \beta\) grows large, \(\delta = \Theta\left(\frac{\ln n}{n}\right) \to 0\). Thus, there exists an \(N\) such that when \(\alpha + \beta > N\), \(\delta < \epsilon\) is satisfied.

The integral \(\int_k^1 F^{-1}(1-q)\left(\frac{1}{q} - (2k-k^2)\frac{1}{q^2}\right) dq\) can now be simplified as:
\[
\int_k^{1-\epsilon_2} \mu \left(\frac{1}{q} - (2k-k^2)\frac{1}{q^2}\right) dq = \mu \int_k^{1-\epsilon_2} \left(\frac{1}{q} - (2k-k^2)\frac{1}{q^2}\right) dq.
\]

When \(k = k_2\), this integral approaches 0. This implies that for such a Beta distribution \(f(v; \alpha, \beta)\), \(k^* \to k_2 \approx 31.62\%\), therefore upper bound $k_2$ is binding, which completes the proof. 
\end{proof}

% \begin{proof}[Proof of Proposition~\ref{prop:OptCapExp}]
%     For exponential distribution with parameter $\lambda$, we have:
%     $$F^{-1}(1-q)=-\frac{1}{\lambda}\ln q$$ 

%     Substituting back into the first order condition:
%     $$\int_k^1 F^{-1}(1-q)(\frac{1}{q}-(2k-k^2)\frac{1}{q^2})dq = \frac{1}{2\lambda}((\ln k)^2+(4-2k)\ln k+2(k-2)(k-1))$$

%     The equation can be solved numerically, we get $m^*(n)=9.70\%$, as desired.
% \end{proof}

% \begin{proof}[Proof of Proposition~\ref{prop:OptCapPowerLaw}]
%     It can be derived from the power-law distribution that, $1-F(x)=\int_x^\infty ct^{-\alpha-1}dt=\frac{c}{-\alpha}t^{-\alpha}\big|_x^\infty=\frac{c}{\alpha}x^{-\alpha}$, $c=\alpha \delta^{\alpha}$ and $v(q)=\delta{q^{-\frac{1}{\alpha}}}$.

%     Then the derivative of $S(n,m)/n$ becomes:
%     $$
%     \begin{aligned}
%     \phi'(k) & =\int_k^1 \delta{q^{-\frac{1}{\alpha}}}(\frac{1}{q}-(2k-k^2)\frac{1}{q^2})dq \\ &=\delta[-\alpha q^{-\frac{1}{\alpha}}-(2k-k^2)(-\frac{\alpha}{\alpha+1})q^{-\frac{1}{\alpha}-1}] \big|_k^1 \\ &=\delta(\frac{-\alpha}{\alpha+1})[(\alpha+1)(1-k^{-\frac{1}{\alpha}})-(2k-k^2)(1-k^{-\frac{1}{\alpha}-1})] \\ & =\delta(\frac{-\alpha}{\alpha+1})[\alpha+(1-k)^2-k^{-\frac{1}{\alpha}}(\alpha-1+k)]. 
%     \end{aligned}
%     $$

%     And we have $\phi'(1)=0$, $\phi'(0)=\delta(\frac{-\alpha}{\alpha+1})[\alpha+1+(1-\alpha)0^{-\frac{1}{\alpha}}]$.

%     We discuss separately for different values of $\alpha$. 

%     \underline{Case 1:} When $\alpha<1$, $\phi'(0)\rightarrow -\infty$,

%     $$
%     \begin{aligned}
%     \phi''(k) & =\frac{-\delta\alpha}{\alpha+1}[2k-2+(1-\alpha)(-\frac{1}{\alpha})k^{-\frac{1}{\alpha}-1}-(1-\frac{1}{\alpha})k^{-\frac{1}{\alpha}}] \\ &=\frac{-\delta\alpha}{\alpha+1}(1-k)[(1-\frac{1}{\alpha})k^{-\frac{1}{\alpha}-1}-2].    
%     \end{aligned}
%     $$

%     Since $\alpha<1$, $S''(k)>0$ , therefore $S'(k)<0, \forall k<1$, which implies that there is no linear optimal $m$ for $S(n,m)$. $S$ attain its maximum at $k=0$, since $m\geq2$, so the optimal $m$ is $2$.

%     \underline{Case 2:} When $\alpha=1$:

%     $S'(k)=\frac{-\delta}{2}[(1-k)^2]<0, \forall k<1$, so there is no linear optimal $m$ for $S(n,m)$ in this case. $S$ attain its maximum at $k=0$, since $m\geq2$, so the optimal $m$ is $2$.

%     So far, we have prove that power-law distribution with $\alpha \leq1$, it holds that $m^*=2$, which completes the proof.
% \end{proof}

\section{Missing Proofs in Section \ref{sec: compare}}

\subsection*{Proof of Lemma~\ref{lem:QuantileRep}}
\begin{proof}
    We start from the normal expression of $S(m,n,l)$:
    \begin{align*}
    & \mathbb{E}_{X_{(1)}, \ldots, X_{(m)}} \left [\sum_{i=1}^{m}\int_{0}^{x_{(i)}}\frac{\binom{n-1}{l}F^{n-l-1}(t)(1-F(t))^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}\frac{f(t)}{1-F(t)} t\, dt \right ]\\
    = &\sum_{i\in[n]}\mathbb{E}_{x_i}[\Pr[x_i>=x_{(m)}]\int_{0}^{x_{i}}\frac{\binom{n-1}{l}F^{n-l-1}(t)(1-F(t))^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}\frac{f(t)}{1-F(t)} t\,dt]\\
    = &n\int_0^{+\infty} F_{(m-1,n-1)}(x_i)f(x_i)\int_{0}^{x_{i}}\frac{\binom{n-1}{l}F^{n-l-1}(t)(1-F(t))^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}F^{n-j}(t)(1-F(t))^{j-1}}\frac{f(t)}{1-F(t)} t\,dt\,dx_i\\
    = &n\int_0^{+\infty} F_{(m-1,n-1)}(x_i)f(x_i)\int_{1-F(x_{i})}^{1}\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}q^{-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}v(q)\,dq\,dx_i\\
    = &n\int_0^{1} \sum_{j=1}^{m}\binom{n-1}{j-1}q_i^{j-1}(1-q_i)^{n-j}\int_{q_i}^{1}\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}q^{-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}v(q)\,dq \,dq_i\\
    = &n\int_0^{1} \frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}q^{-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}\int_{0}^{q}\sum_{j=1}^{m}\binom{n-1}{j-1}q_i^{j-1}(1-q_i)^{n-j}\,dq_i\,v(q)\,dq,
\end{align*}where $F_{(m-1,n-1)}(x)$ is cumulative probability function following the proof of Theorem~\ref{thm:ConpleteSimpleContest}.

    Next, we use $G_{(m,l)}(q)$ to denote the distribution-free part:
    \[
    G_{(m,l)}(q)=\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}\int_{0}^{q}\sum_{j=1}^{m}\binom{n-1}{j-1}p^{j-1}(1-p)^{n-j}\,dp.
    \]

    Then, by change of integration sequence, we obtain:
    \[
    \begin{aligned}
        S(l,n,m) = & \int_0^{1} G_{l,m}(q)v(q)\,dq
        \\ = & \int_0^{1} G_{l,m}(q)\int_{q}^1|v'(t)|\, dt\,dq
        \\ = &\int_0^1|v'(q)|\int_0^qG_{l,m}(t)\,dt\,dq,
    \end{aligned}
    \]as desired. The same derivation applies to $S^{(1)}(m,n,l)$, resulting in $S^{(1)}(m,n, l)= n\int_0^1|v'(q)|\int_0^qG^{(1)}_{(m,l)}(t)\,dt\,dq$ and $G_{l,m}^{(1)}(t):=\frac{\binom{n-1}{l}(1-t)^{n-l-1}t^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-t)^{n-j}t^{j-1}}\int_{0}^{t}(1-p)^{n-1}\,dp,$ which completes the proof.
\end{proof}

% \begin{remark}
%     This representation of effort helps us to somehow decouple the contribution of contest structure itself and the distribution, which is helpful in the proofs of worst case analysis. 
% \end{remark}

\begin{lemma}
\label{lemma:1/qint approximation}
    Define $$\zeta(m, n, q) = \sum_{j=1}^m \binom{n-1}{j-1} (1-q)^{n-j}q^{j-1},$$
    For any $1\leq m\leq n$, $q\in[0,1]$, it holds that 
    $$ \frac14\min\{1,\frac{m}{nq}\}\leq \frac1{q}\int_0^{q}\zeta(m, n, t)dt\leq \min\{1,\frac{m}{nq}\}.$$
\end{lemma}
\begin{proof}
    % Firstly, note that $\zeta(m, n, q)=\Pr[\mathrm{Binomial}(n-1,q)\leq m-1]$, and 
    Firstly, we have $\zeta(m,n,q)\in[0,1]$ for all $q\in[0,1]$, and $\int_0^1\zeta(m, n, t)dt=\frac{m}{n}$.
    We immediately have $\frac1{q}\int_0^{q}\zeta(m, n, t)dt\leq\frac1{q}\int_0^{q}1dt\leq=1$ and $\frac1{q}\int_0^{q}\zeta(m, n, t)dt\leq\frac1{q}\int_0^{1}\zeta(m, n, t)dt\leq=\frac{m}{nq}$, so the upper bound holds.

    For the lower bound, observe that $\zeta(m,n,q)$ is non-increasing in $q$ on $[0,1]$. We can then discuss the following three cases:

    \underline{Case 1:} $\zeta(m,n,q)\geq\frac{1}{2}$. 
    In this case, we have $\frac1{q}\int_0^{q}\zeta(m, n, t)dt\geq \zeta(m, n, q)dt\geq\frac12$.

    \underline{Case 2:} $\zeta(m,n,q)<\frac12$ and $m=1$. We can calculate $\zeta(m,n,\frac{m}{n})=(1-\frac1{n})^{n-1}\geq\frac12$, which implies that $q>\frac{m}{n}$. It holds that $\frac1{q}\int_0^{q}\zeta(m, n, t)dt\geq \frac1{q}\int_0^{\frac{m}{n}}\frac12dt=\frac{m}{2nq}$.
    
    \underline{Case 3:} $\zeta(m,n,q)<\frac12$ and $m\geq 2$. Since $\zeta(m, n, q)=\Pr[\mathrm{Binomial}(n-1,q)\leq m-1]$, by the property of binomial distribution we have that $\zeta(m, n, \frac{m-1}{n-1})\geq\frac12$, which implies that $q>\frac{m-1}{n-1}$.
    Therefore, we have $\frac1{q}\int_0^{q}\zeta(m, n, t)dt\geq \frac1{q}\int_0^{\frac{m-1}{n-1}}\frac12dt=\frac1{2q}\frac{m-1}{n-1}>\frac{m}{4nq}$.

    In summary, we have $\frac1{q}\int_0^{q}\zeta(m, n, t)dt\geq \frac14\min\{1,\frac{m}{nq}\}$, which completes the proof.
\end{proof}

\begin{lemma}\label{lem:Hn1}
    $H_{(n,1)}(q)=\Theta(\min(nq^2,\frac1{n}))$
\end{lemma}
\begin{proof}
   Firstly, for any $q\leq \frac1{2n}$, we prove that $\frac12(e^{-\frac12}-\frac12)nq^2\leq H_{(n,1)}(q)\leq \frac12nq^2$. We can calculate
   \begin{align*}
       &H_{(n,1)}(q)=\int_0^q (n-1) (1-t)^{n-2} t \, dt\\
=& -(1-t)^{n-1} t \Big|_0^q - \int_0^q \left( -(1-t)^{n-1} \right) \, dt\\
=& -(1-q)^{n-1} q - \frac{1}{n} (1-t)^n \Big|_0^q\\
=& -(1-q)^{n-1} q - \frac{1}{n} (1-q)^n + \frac{1}{n}\\
=& \frac{1}{n} \left( 1 - (1-q)^n - n(1-q)^{n-1} q \right).
   \end{align*}
    By Taylor expansion of $x^n$ at $x=1$, we have: $(1-q)^n=1-nq+\frac{n(n-1)\xi_1^{n-2}}{2}q^2$ for some $\xi_1\in[1-q,1]$.
    
   Therefore, we can calculate $1 - (1-q)^n - n(1-q)^{n-1} q
   =nq-\frac{n(n-1)\xi_1^{n-2}}{2}q^2-n(1-q)^{n-1} q
   =nq(1-\frac{n-1}{2}\xi_1^{n-2}q-(1-q)^{n-1})$. 
   
   Since $\xi_1\in[1-q,1]$, we have: 
   $$
   \begin{aligned}
       nq(1-(1-q)^{n-1}-\frac{n-1}{2}q) & \leq 1 - (1-q)^n - n(1-q)^{n-1} q\\ & \leq nq(1-(1-q)^{n-1}-\frac{n-1}{2}(1-q)^{n-2}q).
   \end{aligned}
   $$
   
   We have $1-(1-q)^{n-1}=(n-1)\xi_2^{n-2}q$ for some $\xi_2\in[1-q,1]$ by Taylor expansion of $x^{n-1}$ at $1$, so $(1-q)^{n-2}(n-1)q\leq 1-(1-q)^{n-1}\leq (n-1)q$. Then we have:
   
   $$
   \begin{aligned}
       nq((1-q)^{n-2}(n-1)q-\frac{n-1}{2}q) & \leq 1 - (1-q)^n - n(1-q)^{n-1} q \\ &\leq nq((n-1)q-\frac{n-1}{2}(1-q)^{n-2}q).
   \end{aligned}
   $$
   
   Since $q\leq\frac1{2n}$, $(1-q)^{n-2}=(1+\frac{1}{2n-1})^{-(n-2)}\geq e^{-\frac12}>\frac12$:

   For the lower bound, $nq((1-q)^{n-2}(n-1)q-\frac{n-1}{2}q)\geq nq(e^{-\frac12}-\frac12)(n-1)q\geq \frac12(e^{-\frac12}-\frac12)n^2q^2$.
   
   For the upper bound, $nq((n-1)q-\frac{n-1}{2}(1-q)^{n-2}q)\leq nq\frac12(n-1)q\leq\frac12n^2q^2$.

   So we have $\frac12(e^{-\frac12}-\frac12)n^2q^2\leq 1 - (1-q)^n - n(1-q)^{n-1} q\leq\frac12n^2q^2$, and then $\frac12(e^{-\frac12}-\frac12)nq^2\leq H_{(n,1)}(q)\leq\frac12nq^2$.

   Next, for any $q\geq\frac1{2n}$, we prove that $\frac18(e^{-\frac12}-\frac12)\frac{1}{n}\leq H_{(n,1)}(q)\leq \frac1n$.
   For the lower bound, since $H_{(n,1)}(q)$ is increasing in $q$, we have $H_{(n,1)}(q)\geq H_{(n,1)}(\frac1{2n})\geq \frac18(e^{-\frac12}-\frac12)\frac{1}{n}$.
   % We have $\frac1n\left( 1 - (1-q)^n - n(1-q)^{n-1} q\right)=\frac1n(1-\sum_{j=2}^n\binom{n}{j}(1-q)^{n-j}q^j\geq\frac1n\binom{n}{2}(1-q)^{n-2}q^2$, and $H_{(n,1)}(\frac1{2n})\geq \frac1{4n^3}\frac{n(n-1)}{2}(1-\frac1{2n})^{n-2}\geq \frac1{e^{\frac12}16n}$
   For the upper bound, we have $H_{(n,1)}(q)=\frac1n\left( 1 - (1-q)^n - n(1-q)^{n-1} q\right)\leq \frac1n$. We then conclude with $H_{(n,1)}(q)=\Theta(\min(nq^2,\frac1{n}))$.
\end{proof}

\begin{lemma}\label{lem:H21}
    $H_{(2,1)}(q)=\Theta(\min\{nq^2,\frac{\log(nq)+1}{n}\})$.
\end{lemma}
\begin{proof}
We can calculate
\begin{align*}
H_{(2,1)}(q) &= \int_0^q (n-1) (1-t)^{n-2} t \frac{ \frac{1}{t} \int_0^t \left( (1-x)^{n-1} + (n-1) (1-x)^{n-2} x \right) dx }{(1-t)^{n-1} + (n-1) (1-t)^{n-2} t} \, dt \\
&= \int_0^q (n-1) (1-t)^{n-2} t \frac{\Theta\left( \min\left( 1, \frac{2}{nt} \right) \right)}{(1-t)^{n-1} + (n-1) (1-t)^{n-2} t} \, dt\\
&= \int_0^q \frac{\Theta(1) \min\left( 1, \frac{1}{nt} \right) }{\frac{(1-t)}{(n-1)t}  + 1} \, dt\\
&= \Theta(1) \int_0^q \frac{\min\left(t, \frac{1}{n} \right)}{\frac{1}{n-1}(1+(n-2)t)} \, dt \\
&= \Theta(1) \int_0^q \frac{\min\left( t, \frac{1}{n} \right)}{\Theta(1)(\frac{1}{n} + t)} \, dt \\
&= \Theta(1) \int_0^q \frac{\min\left( t, \frac{1}{n} \right)}{\frac{1}{n} + t} \, dt, \\
\end{align*}
where the second equality holds is by Lemma \ref{lemma:1/qint approximation}.

For $q \leq \frac{1}{n}$, $\int_0^q \frac{\min\left( t, \frac{1}{n} \right)}{\frac{1}{n} + t} \, dt=\int_0^q\frac{t}{t+\frac1n}dt=\int_0^q\Theta(n)tdt=\Theta(nq^2)$

For $q \geq \frac{1}{n}$, $\int_{\frac1n}^q \frac{\min\left( t, \frac{1}{n} \right)}{\frac{1}{n} + t} \, dt=\int_{\frac1n}^q\frac{\frac1n}{t+\frac1n}dt=\Theta(1/n)\int_{\frac1n}{q}\frac1tdt=\Theta(\frac{\ln(qn)}{n})$, and then $H_{(2,1)}(q)=H_{(2,1)}(\frac1n)+\int_{\frac1n}^q \frac{\min\left( t, \frac{1}{n} \right)}{\frac{1}{n} + t} \, dt=\Theta(\frac{1}{n}+\frac{\log(nq)}{n})$.
% , \quad H_{(2,1)}(q) &= \Theta\left(\frac{1}{n}\right) + \Theta(1) \int_{\frac{1}{n}}^q \frac{\frac{1}{n}}{t} \, dt \\
           % &= \Theta\left(\frac{1}{n} + \frac{\ln(nq)}{n}\right)
\end{proof}

\subsection*{Proof of Lemma~\ref{lem:bound on n,1}}
\begin{proof}
    From the quantile representation of total effort (Lemma~\ref{lem:QuantileRep}) and analysis on $H_{(n,1)}$ (Lemma~\ref{lem:Hn1}) we have:
    \[
    \begin{aligned}
        S(n,n,1) & = n\int_0^1|v'(q)|H_{(m,1)}(q)\,dq \\
        & = n\int_0^1|v'(q)| \Theta(\min(nq^2,\frac1{n}))\, dq \\
        &= \int_0^1|v'(q)| \Theta(\min(n^2q^2,1))\, dq \\
        &= \int_{0}^{\frac{1}{n}}|v'(q)| \Theta(n^2q^2)\, dq+\int_{\frac{1}{n}}^{1}|v'(q)| \Theta(1)\, dq \\
        &= \Theta(n^2)\int_{0}^{\frac{1}{n}}|v'(q)|q^2\, dq+\Theta(1)\int_{\frac{1}{n}}^{1}|v'(q)|\, dq.
    \end{aligned}
    \]
    
    Since $|v'(q)|$ is bounded, i.e., $L'\leq|v'(q)|\leq L$, then $\int_{\frac{1}{n}}^{1}|v'(q)|\, dq = \Theta(1+n^{-1}) = \Theta(1)$. Similarly, $\int_{0}^{\frac{1}{n}}|v'(q)|q^2\, dq=\Theta(n^{-3})$, Therefore we have:
    \[
    S(n,n,1)= \Theta(n^{2})\Theta(n^{-3})+\Theta(1)\Theta(1)=\Theta(1).
    \]

    As for $S^{(1)}(2,n,1)$, by Lemma \ref{lemma:1/qint approximation} we have 
$H_{l,m}^{(1)}(t)=\Theta(1)\int_0^t\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}\min\{1,\frac{1}{nq}\}dq$.

    Therefore, We have $H_{(n,1)}^{(1)}(t)=\Theta(1)\int_0^t(n-1)(1-q)^{n-2}q\min\{1,\frac{1}{nq}\}dq=\Theta(H_{(n,1)}(t))$. The same procedure immediately applies. Then we conclude that, $S^{(1)}(n,n,1)=\Theta(1)$ and $S(n,n,1)=\Theta(1)$, which completes the proof.
\end{proof}

\begin{lemma}\label{lem:logInt}
    \(\int_{1/n}^{1} \Theta(\log(nq)) \, dq = \Theta(\log n)\) 
\end{lemma}
\begin{proof}
    W.l.o.g. \(\Theta(\log(nq)) = C \log(nq)\), where \(C\) is a constant. Then we have:
    \[
    \int_{1/n}^{1} C \log(nq) \, dq.
    \]

This integral can be solved using change of variables. Let \(u = \log(nq)\). Then \( du = \frac{1}{q} \, dq \Rightarrow dq = q \, du.\)

When \(q = 1/n\), we have \(u = \log(n \cdot 1/n) = \log(1) = 0\). When \(q = 1\), we have \(u = \log(n \cdot 1) = \log(n)\). Substituting these into the integral, we obtain:
\[
\int_{1/n}^{1} C \log(nq) \, dq = \int_{0}^{\log(n)} C u \cdot \frac{e^u}{n} \, du = \frac{C}{n} \int_{0}^{\log(n)} u e^u \, du.
\]

Then integrate by parts. Let \(v = u\) and \(dw = e^u \, du\), so that \(dv = du\) and \(w = e^u\). Using the integration by parts formula \(\int v \, dw = vw - \int w \, dv\), we get \(\
\int u e^u \, du = u e^u - \int e^u \, du = u e^u - e^u + C.\)

Thus, the definite integral becomes:
\[
\int_{0}^{\log(n)} u e^u \, du = \left[ u e^u - e^u \right]_{0}^{\log(n)}.
\]

Evaluating the boundary terms, we have:
\[
\left( \log(n) \cdot n - n \right) - (0 - 1) = n \log(n) - n + 1.
\]

Substituting this result back, the original integral becomes:
\[
\frac{C}{n} (n \log(n) - n + 1) = C (\log(n) - 1 + \frac{1}{n}).
\]

For sufficiently large \(n\), the term \(\frac{1}{n}\) becomes negligible, leaving the dominant term as 
\(
C \log(n) - C.
\). Therefore, the asymptotic bound of the integral is:
\(
\Theta(\log(n))
\), as desired. This completes the proof.
\end{proof}

\subsection*{Proof of Lemma\ref{lem:bound on 2,1}}
\begin{proof}
    We first prove that $S(2,n,1) = \Theta(\log n).$ From the quantile representation of total effort (Lemma~\ref{lem:QuantileRep}) and analysis on $H_{(2,1)}$ (Lemma~\ref{lem:H21}) we have:
    \[
    \begin{aligned}
        S(n,n,1) & = n\int_0^1|v'(q)|H_{(2,1)}(q)\,dq \\
        & = n\int_0^1|v'(q)| \Theta(\min\{nq^2,\frac{\log(nq)+1}{n}\})\, dq \\
        &= \int_{0}^{\frac{1}{n}}|v'(q)| \Theta(n^2q^2)\, dq+\int_{\frac{1}{n}}^{1}|v'(q)| \Theta(\log(nq)+1)\, dq \\
        &= \Theta(n^2)\int_{0}^{\frac{1}{n}}|v'(q)|q^2\, dq+\int_{\frac{1}{n}}^{1}|v'(q)|\Theta(\log(nq))\, dq.
    \end{aligned}
    \]
    
    Since $|v'(q)|$ is bounded, i.e., $L'\leq|v'(q)|\leq L$, then $\int_{\frac{1}{n}}^{1}|v'(q)|\, dq = \Theta(1+n^{-1}) = \Theta(1)$. Also by Lemma~\ref{lem:logInt}, \(\int_{1/n}^{1} \Theta(\log(nq)) \, dq = \Theta(\log n)\), which gives that $\int_{\frac{1}{n}}^{1}|v'(q)|\Theta(\log(nq))\, dq=\Theta(\log n)$. Therefore we have:
    \[
    S(2,n,1)= \Theta(n^{2})\Theta(n^{-3})+\Theta(\log n)=\Theta(\log n).
    \]

    As for $S^{(1)}(2,n,1)$, by Lemma \ref{lemma:1/qint approximation} we have $H_{l,m}^{(1)}(t)=\Theta(1)\int_0^t\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}\min\{1,\frac{1}{nq}\}dq$.

    Therefore, We have $H_{(2,1)}^{(1)}(t)=\Theta(1)\int_0^t\frac{(n-1)(1-q)^{n-2}q}{(1-q)^{n-1}+(n-1)(1-q)^{n-2}q}\min\{1,\frac{1}{nq}\}dq = \Theta(H_{(2,1)}(t))$. The same procedure immediately applies. Then we conclude that, $S^{(1)}(2,n,1)=\Theta(n)$ and $S(2,n,1)=\Theta(n)$, which completes the proof.
\end{proof}

% \begin{corollary}\label{coro:ThetaN}
%     For any given distribution, $S(m^*,n,m^*-1) = \Theta(n)$.
% \end{corollary}

\subsection*{Proof of Lemma~\ref{lem:bound on m,m-1}}
\begin{proof}
Recall from Lemma~\ref{lem:AsyRep} that, as $n\to \infty$:
\[
S(m,n,m-1)=F^{-1}(1-k)+n\int_0^{1-k}F^{-1}(q)\frac{q}{1-q}\frac{k}{1-k}(\frac{1}{q}-\frac{k}{q(1-q)})dq.
\]
Since by Theorem~\ref{thm:OptAsmLinear}, $k^*$ is a constant for arbitrary distributions. Then, asymptotically, $S(m^*,n,m^*-1)$ becomes a linear function of $n$, therefore $S(m^*,n,m^*-1)=\Theta(n)$, which completes the proof.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm: 2,1 vs n,1 max effort}]
This follows directly from Lemma~\ref{lem:bound on 2,1} and Lemma~\ref{lem:bound on n,1}.
\end{proof}

\subsection*{Proof of Theorem~\ref{thm:TotalOPTVAN}}
\begin{proof}
    This follows directly from Lemma~\ref{lem:bound on n,1} and \ref{lem:bound on m,m-1} .
\end{proof}

\subsection*{Proof of Proposition~\ref{prop:TotalTWOVAN}}
\begin{proof}
    This follows directly from Lemma \ref{lem:bound on n,1} and \ref{lem:bound on 2,1}.
\end{proof}

% \begin{lemma}\label{lem:Hfrac}
%     For any $t\in[0,1]$, $\frac{H_{(2,1)}^{(1)}(t)}{H_{(n,1)}^{(1)}(t)}=\frac{H_{(2,1)}(t)}{H_{(n,1)}(t)}=\begin{cases}
%     \Theta(1),&t\in[0,\frac1n],\\
%     \Theta(\log(nt)),&t\in[\frac1n,1].
% \end{cases}$
% \end{lemma}
% \begin{proof}
%     Denote the $H$ function of the quantile representation (Lemma~\ref{lem:QuantileRep}) in maximum individual effort objective as $H_{l,m}^{(1)}(t):=\int_0^t\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}q^{-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}\int_{0}^{q}(1-q_i)^{n-1}dq_idq$, then by Lemma \ref{lemma:1/qint approximation} we have 
% $H_{l,m}^{(1)}(t)=\Theta(1)\int_0^t\frac{\binom{n-1}{l}(1-q)^{n-l-1}q^{l}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}\min\{1,\frac{1}{nq}\}dq$.

% Therefore, We have $H_{(n,1)}^{(1)}(t)=\Theta(1)\int_0^t(n-1)(1-q)^{n-2}q\min\{1,\frac{1}{nq}\}dq$, \\
% and $H_{(2,1)}^{(1)}(t)=\Theta(1)\int_0^t\frac{(n-1)(1-q)^{n-2}q}{(1-q)^{n-1}+(n-1)(1-q)^{n-2}q}\min\{1,\frac{1}{nq}\}dq$.

% For $H_{(2,1)}^{(1)}(t)$, observe that $H_{(2,1)}^{(1)}(t)=\Theta(1)H_{(2,1)}(t)=\Theta(\min\{nq^2,\frac{\log(nq)+1}{n}\})$.

% For $H_{(n,1)}^{(1)}(t)$, observe that $H_{(n,1)}^{(1)}(t)=\Theta(1)\int_0^t(n-1)(1-q)^{n-2}q\min\{1,\frac{1}{nq}\}dq\leq\Theta(1)\int_0^t(n-1)(1-q)^{n-2}qdq=\Theta(1)H_{(n,1)}(t)=\Theta(\min\{nt^2,\frac1n\})$. When $t\leq \frac{1}{n}$, we have $H_{(n,1)}^{(1)}(t)=\Theta(1)\int_0^t(n-1)(1-q)^{n-2}qdq=\Theta(1)H_{(n,1)}(t)=\Theta(nt^2)$. When $t\geq\frac{1}{n}$, by monotonicity we have $H_{(n,1)}^{(1)}(t)\geq H_{(n,1)}^{(1)}(\frac1n)=\Theta(\frac1n)$.

% Therefore, for any $t\in[0,1]$, $\frac{H_{(2,1)}^{(1)}(t)}{H_{(n,1)}^{(1)}(t)}=\frac{H_{(2,1)}(t)}{H_{(n,1)}(t)}=\begin{cases}
%     \Theta(1),&t\in[0,\frac1n],\\
%     \Theta(\log(nt)),&t\in[\frac1n,1].
% \end{cases}$

% Note that the constant in $\Theta$ notation is absolute, i.e., independent of $t$.
% \end{proof}

% \begin{theorem}\label{thm:AprroxHigh}
%     For the maximum individual effort objective, approximation ratio of the 2-player winner-take-all contest (shortlist optimal) and the n-player winner-take-all contest is $\Theta(\log n)$.
% \end{theorem}
% \begin{proof}
%     Recall the quantile representation of highest effort (Lemma~\ref{lem:QuantileRep}):
%     \[
%     S^{(1)}(m,n, l)= n\int_0^1|v'(q)|H^{(1)}_{(m,l)}(q)\,dq.
%     \]
    
%     Lemma~\ref{lem:Hfrac} shows that, for any given $n$, we have: \[
%     \frac{H_{(2,1)}^{(1)}(t)}{H_{(n,1)}^{(1)}(t)}=\begin{cases}
%     \Theta(1),&t\in[0,\frac1n],\\
%     \Theta(\log(nt)),&t\in[\frac1n,1].
% \end{cases}
%     \]
    
%     Since by our assumption, $ L'\leq |v'(q)| \leq L$ is bounded, therefore:
%     \[
%     \begin{aligned}
%         \frac{S^{(1)}(2,n,1)}{S^{(1)}(m,n,1)} & = \frac{n\int_0^1|v'(q)|H^{(1)}_{(2,1)}(q)\,dq}{n\int_0^1|v'(q)|H^{(1)}_{(n,1)}(q)\,dq} \\
%         & \leq \frac{L\cdot H^{(1)}_{(2,1)}(1)}{L' \cdot H^{(1)}_{(n,1)}(0)} = \frac{\Theta(\log n)}{\Theta(1)} = \Theta(\log n),
%     \end{aligned}
%     \]
%     then the approximate ratio is $O(\log n)$. Since $|v'(q)|$ is non-zero every where, the upper bound is achievable. Then the approximation ratio is $\Theta(\log n)$, as desired.
% \end{proof}

% \begin{proposition}
%     For the total effort objective, approximation ratio of the 2-player winner-take-all contest and the n-player winner-take-all contest is $\Theta(\log n)$.
% \end{proposition}
% \begin{proof}
%     Since by Lemma~\ref{lem:Hfrac}, $\frac{H_{(2,1)}(t)}{H_{(n,1)}(t)}=\frac{H_{(2,1)}^{(1)}(t)}{H_{(n,1)}^{(1)}(t)}=\begin{cases}
%     \Theta(1),&t\in[0,\frac1n],\\
%     \Theta(\log(nt)),&t\in[\frac1n,1].
%     \end{cases}$. The same analysis immediately follows from the proof of Theorem~\ref{thm:AprroxHigh}, showing that the approximation ratio is also $\Theta(\log n)$, as desired.
% \end{proof}

% \begin{proposition}\label{prop:Theta1}
%     For any given distribution, $S(n,n,1) = \Theta(1)$.
% \end{proposition}
% \begin{proof}
%     Since $|v'(q)|$ is bounded, from Lemma~\ref{lem:Hn1} we have:
%     \[
%     \begin{aligned}
%         S(n,n,1) & = n\int_0^1|v'(q)|H^{(1)}_{(m,l)}(q)\,dq \\
%         & = n\int_0^1|v'(q)| \Theta(\min(nq^2,\frac1{n}))\, dq \\
%         &= \int_0^1|v'(q)| \Theta(\min(n^2q^2,1))\, dq \\
%         &= \int_0^1|v'(q)| \Theta(1)\, dq \\
%         &= \Theta(1),
%     \end{aligned}
%     \]which completes the proof.
% \end{proof}

% \begin{theorem}
%     For any given distribution, $S(m^*,n,m^*-1) /S(n,n,1) = \Theta(n)$.
% \end{theorem}
% \begin{proof}
%     This ratio follows directly from Proposition~\ref{prop:Theta1} and Corollary~\ref{coro:ThetaN}.
% \end{proof}

\section{Missing Proofs in Section \ref{sec:practicalApp}}
\begin{lemma}\label{lem:IncFracSeq}
    If the following condition holds:
    \[
    \frac{a_1}{b_1} \leq \frac{a_2}{b_2} \leq \ldots \leq \frac{a_n}{b_n}.
    \]
    
    Let $A(x) = \sum_{i=1}^na_ix^{i}$, $B(x)=\sum_{i=1}^n b_ix_i$, then $A(x)/B(x)$ is increasing in $(0,+\infty)$. 
\end{lemma}

\begin{lemma}\label{lem:FracDesc}
    $H_{(m)}(q)/H_{(m')}(q)$ decreases with $q$ in $(0,1]$ for all $2 \leq m < m' \leq n$.
\end{lemma}
\begin{proof}
    We first prove that $H_{(m)}(q)/H_{(m+1)}(q)$ decreases with $q$. 

    It is suffice to prove that derivative of $H_{(m)}(q)/H_{(m+1)}(q)$ is non-positive:
    \[
    \begin{aligned}
        \frac{H'_{(m)}(q)H_{(m+1)}(q)-H_{(m)}(q)H'_{(m+1)}(q)}{(H_{(m+1)}(q))^2} & \leq 0 \\
        \frac{H_{(m+1)}(q)}{H'_{(m+1)}(q)}-\frac{H_{(m)}(q)}{H'_{(m)}(q)} & \leq 0 \\
        \frac{H_{(m+1)}(q)}{G_{(m+1)}(q)} & \leq \frac{H_{(m)}(q)}{G_{(m)}(q)},
    \end{aligned}
    \]or equivalently, $\frac{H_{(m)}(q)}{G_{(m)}(q)}$ decreases with $m$ for any $q\in (0,1]$.

    We have the expression of $\frac{H_{(m)}(q)}{G_{(m)}(q)}$:
    \begin{multline*}
        \frac{\sum_{j=1}^m\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}{\binom{n-1}{m-1}(1-q)^{n-m}q^{m-2}\int_0^q\sum_{j=1}^m\binom{n-1}{j-1}(1-t)^{n-j}t^{j-1}\, dt} \\ \cdot \int_0^q \frac{\binom{n-1}{m-1}(1-x)^{n-m}x^{m-2}\int_0^x\sum_{j=1}^m\binom{n-1}{j-1}(1-t)^{n-j}t^{j-1}\, dt}{\sum_{j=1}^m\binom{n-1}{j-1}(1-x)^{n-j}x^{j-1}}\, dx
    \end{multline*}

    We proceed by prove that $\frac{H_{(m)}(q)}{D_{(m)}(q)} / \frac{H_{(m+1)}(q)}{D_{(m+1)}(q)} \geq 1$. Since the outer integration is hard to handle, we skip it by proving a stronger version of that claim: the inequality holds point-wise for $x$. 

    With some cancellation, this fraction can be simplified to \(D(q)/D(x)\), where:
    \[
        D(q) := \frac{q}{1-q} \frac{\sum_{j=1}^m\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}{\int_0^{q}\sum_{j=1}^{m}\binom{n-1}{j-1}(1-t)^{n-j}t^{j-1}\, dt} \frac{\int_0^{q}\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-t)^{n-j}t^{j-1}\, dt}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}},
    \]
    then it is suffice to prove that $D(q)$ increase with $q$, since $x\leq q$. 

    To simplify expression, we further introduce several notations:
    \[
    \begin{aligned}
        A(m,q) & =\sum_{j=1}^m\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1} = 1-(n-1)\binom{n-2}{m-1} \int_0^{q}(1-t)^{n-m-1}t^{m-1}\,dt, \\
        A'(m,q) & = -(n-1)\binom{n-2}{m-1}(1-q)^{n-m-1}q^{m-1},
    \end{aligned}
    \]$I_A(n,q)=  \int_0^qA(m,t) \, dt$, $C(m,q) = A(m,q)/I_A(m,q)$, $P(q) = \frac{q}{1-q}$, $P'(q)= \frac{1-q+q}{(1-q)^2} = (1-q)^{-2}$ and,
    \[
    C'(m,q) = \frac{A'(m,q)I_A(m,q)-A^2(m,q)}{I^2_A(m,q)}.
    \]

    Then $D(q)=P(q)C(m,q)C^{-1}(m+1,q)$, and the derivative:
    \[
    \begin{aligned}
        D'(q) = & P'(q)C(m,q)C^{-1}(m+1,q)+P(q)C'(m,q)C^{-1}(m+1,q)\\
                & -P(q)C(m,q)C'(m+1,q)C^{-2}(m+1,q).
    \end{aligned}
    \]

    Next, canceling out $C^{-1}(m+1,q)$:
    \[
    P'(q)C(m,q) + P(q)C'(m,q)-P(q)C(m,q)C'(m+1,q)C^{-1}(m+1,q).
    \]

    Substituting the expression for $P$ and $C$, we have:
    \begin{multline*}
        \frac{A(m,q)}{(1-q)^2I_A(m,q)} + \frac{q(A'(m,q)I_A(m,q)-A^2(m,q))}{(1-q)I^2_A(m,q)} \\
        -\frac{qA(m,q)(A'(m+1,q)I_A(m+1,q)-A^2(m+1,q))I_A(m+1,q)}{(1-q)I_A(m,q)I^2_A(m+1,q)A(m+1,q)}
    \end{multline*}

    Canceling out $(1-q)I_A(m,q)$, it becomes:
    \begin{multline*}
        \frac{A(m,q)}{1-q}+\frac{q(A'(m,q)I_A(m,q)-A^2(m,q))}{I_A(m,q)} \\
        - \frac{qA(m,q)(A'(m+1,q)I_A(m+1,q)-A^2(m+1,q))}{I_A(m+1,q)A(m+1,q)}
    \end{multline*}

    Separate the numerators, and move $q$ to denominator, then we get:
    \[
    \frac{A(m,q)}{q(1-q)} + A'(m,q)-\frac{A^2(m,q)}{I_A(m,q)}-\frac{A(m,q)A'(m+1,q)}{A(m+1,q)}+\frac{A(m,q)A(m+1,q)}{I_A(m+1,q)}
    \]

    The third term and the last term can be combined:
    \[
    A(m,q)\left( \frac{A(m+1,q)}{I_A(m+1,q)}-\frac{A(m,q)}{I_A(m,q)}\right) \geq 0,
    \]where $I_A(m,q)/A(m,q)$ decrease with $m$ follows from the proof of Theorem~\ref{thm:ConpleteSimpleContest}.  

    Now, it is suffice to prove the remaining part also non-negative, i.e.:
    \[
    \frac{A(m,q)}{q(1-q)} + A'(m,q) - \frac{A(m,q)A'(m+1,q)}{A(m+1,q)} \geq 0.
    \]

    Re-organizing terms, it is equivalent to show that:
    \[
    \frac{A'(m,q)}{A(m,q)} - \frac{A'(m+1,q)}{A(m+1,q)} + \frac{1}{q(1-q)} \geq 0
    \]

    Since $[\ln q - \ln (1-q)]' = q^{-1}+(1-q)^{-1} = [q(1-q)]^{-1}$, it further becomes:
    \[
    \begin{aligned}
        & [\ln A(m,q)-\ln A(m+1,q)+\ln q-\ln(1-q)]' \\
        = & \left [ \ln \frac{qA(m,q)}{(1-q)A(m+1,q)}\right]'
    \end{aligned}
    \]

    Then the problem finally becomes to show that $\frac{qA(m,q)}{(1-q)A(m+1,q)}$ increase with $q$. 

    Expanding the expressions, we have:
    \[
    \begin{aligned}
        & \frac{\sum_{j=1}^m\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}} \frac{q}{1-q} = \frac{\sum_{j=1}^m\binom{n-1}{j-1}(1-q)^{n-j-1}q^{j}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}} \\
    =& \frac{\sum_{j=2}^{m+1}\binom{n-1}{j-2}(1-q)^{n-j}q^{j-1}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}} = \frac{\sum_{j=2}^{m+1}\frac{j-1}{m-j+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}} \\
    = & \frac{\sum_{j=1}^{m+1}\frac{j-1}{n-j+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{n-j}q^{j-1}} = \frac{\sum_{j=1}^{m+1}\frac{j-1}{n-j+1}\binom{n-1}{j-1}(1-q)^{1-j}q^{j-1}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}(1-q)^{1-j}q^{j-1}}.
    \end{aligned}
    \]

    Let $t=\frac{q}{1-q}$, it becomes:
    \[
    \frac{\sum_{j=1}^{m+1}\frac{j-1}{n-j+1}\binom{n-1}{j-1}t^{j-1}}{\sum_{j=1}^{m+1}\binom{n-1}{j-1}t^{j-1}}.
    \]

    Since $\frac{j-1}{n-j+1}$ increases with $j$, Lemma~\ref{lem:IncFracSeq} shows this fraction increases with $t$ (also $q$), as desired.

    So far, We have proved that $H_{(m)}(q)/H_{(m+1)}(q)$ decreases with $q$. 
    
    For any two function $f(x), g(x) \geq 0$, if $f'(x), g'(x) \leq 0 $, then $[f(x)g(x)]' = f'(x)g(x)+f(x)g'(x) \leq0$. Since non-negative function $H_{(m)}(q)/H_{(m+1)}(q)$ is differentiable and decreasing, we have $[H_{(m)}(q)/H_{(m+2)}(q)]'=[(H_{(m)}(q)/H_{(m+1)}(q))\cdot (H_{(m+1)}(q)/H_{(m+2)}(q))]'\leq0$. Therefore, it can be prove by induction that $H_{(m)}(q)/H_{(m')}(q)$ for any $m'>m$, which completes the proof.
\end{proof}

\begin{proposition}\label{prop:SupM}
    For the total effort objective, given $n$, there exists an $O(n)$ algorithm that finds the largest shortlist capacity $m$ such that the complete simple contest $S(m,n,m-1)$ is the optimal contest under some ability distribution $F(x)$.    
\end{proposition}
\begin{proof}
    Since Theorem~\ref{thm:ConpleteSimpleContest} states that optimal contest is a complete simple contest, i.e., $l = m-1$, we therefore omit $l$ in the following discussion.

Recall the quantile representation of total effort:
\[
    S(m,n, l)= n\int_0^1|v'(q)|\int_0^qG_{(m,l)}(t)\,dt\,dq,
    \]

Then total effort becomes the integration of the multiplication of a function $|v'(q)|$ determined by ability distribution, and a function $H_{(m,l)}(q)=\int_0^qG_{(m,l)}(t)\,dt$ that is completely decided by the contest structure.


Let us focus on the distribution-free part. We can plot $H_{m}(q)$ as a function of $q\in[0,1]$ for $m = 2,\ldots,n$. The example of $n=10$ is shown in Figure~\ref{fig:universal-b}. In this case, we can see that for some $m$ (e.g., $m=3$), $H_{(m)}(q)>H_{(m+1)}(q)$ holds point-wise, thus, $S(m,n) > S(m+1,n)$ stands true for arbitrary distributions, indicating that $m+1$ is a strictly dominated choice. 

\begin{figure}[h]

\begin{subfigure}[ht]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/plot2.pdf}
    \subcaption{$H_{(m)}(q)$ for different $m$}
    \label{fig:universal-b}
    \end{subfigure}
%\hfill
\begin{subfigure}[ht]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/plot3.pdf}
    \subcaption{$|v'(q)|$ s.t. $S(3,10)>S(2,10)$}
    \label{fig:universal-c}
    \end{subfigure}
%\caption{dont know what to say here}
\label{fig:universal}
\end{figure}

Actually, it can be shown that $H_{(m)}(q)/H_{(m')}(q)$ is decreasing with $q$ for $m'>m$ (Lemma~\ref{lem:FracDesc}), then $H_{(m)}(1) > H_{(m')}(1)$ is a suffice and necessary condition for $H_{(m)}(q)>H_{(m')}(q)$ point-wise. On the other hand, if $H_{(m)}(1) < H_{(m')}(1)$, then there exists unique $q' \in(0,1)$ such that $H_{(m)}(q')=H_{(m')}(q')$ and $H_{(m)}(q)<H_{(m')}(q)$ afterwards (e.g., the $q'$ for $m=2$ and $m'=3$ is marked with asterisk in Figure~\ref{fig:universal-b}). Since $v'(q)$ can be any positive function, we can always construct a distribution that satisfies $v'(q)=1$ when $q\geq q'$ and $v'(q)=\epsilon$ elsewhere such that $S(m,n) < S(m',n)$ (See an example distribution that make $m'=3$ better than $m=2$ in Figure~\ref{fig:universal-c}, where we let $q' \approx 0.859$, $\epsilon=0.01$, and $S(2,10) \approx 0.481 < 0.489 \approx S(3,10)$.).

Therefore, we can find the $m$ that maximize $H_{m}(1)$. For $m'>m$, we have $H_{(m)}(q) > H_{(m')}(q)$ point-wise, so the optimal capacity can not be more than $m$. For $m'<m$, we have $H_{(m')}(1) < H_{(m)}(1)$, then we can still construct a distribution that satisfies $v'(q)=1$ when $q \geq \max\{\vec{q'}\}$ and $v'(q) = \epsilon$ elsewhere such that $S(m,n) > S(m',n)$ for all $m'<m$, hence we find an instance making $m$ the optimal capacity. We then conclude that $m$ is the tight upper bound for optimal capacity for given $n$, as desired. Since we enumerate over the value of $m$, the algorithm is $O(n)$. 
\end{proof}
\begin{remark}
    The insight from the construction of worst case distribution (e.g., Figure~\ref{fig:universal-c}) is, when almost all of the population are concentrated near the strongest end of ability, it tends to need larger shortlist capacity to reach optimality. On the other hand, if highest ability only takes up a little probability mass, or equivalently, $|v'(q)|$ is much higher when $q$ is small, it tends to obtain optimality with fewer contestants. An uniform distribution, i.e., $|v'(q)|=1$, whose probability mass is evenly distributed, is right in the middle, with $k^*\approx15\%$, as shown in Example~\ref{exam:OptimalUniform}. 
\end{remark}

\begin{corollary}\label{coro:shortlistAlways}
    All complete simple contests that admit $kn$ contestants ($k\in (0,1)$) will result in $\Theta(n)$ total effort. Specifically, when $n$ is large, letting admission ratio $k=31.62\%$ produces higher total effort than any $k'>k$, where $k$ is the solution of $(2-k)(k-1)-\ln k=0$.
\end{corollary}
\begin{proof}
    Recall from Lemma~\ref{lem:AsyRep} that, as $n\to \infty$:
\[
S(m,n,m-1)=F^{-1}(1-k)+n\int_0^{1-k}F^{-1}(q)\frac{q}{1-q}\frac{k}{1-k}(\frac{1}{q}-\frac{k}{q(1-q)})dq,\]
and the convergence rate is independent of the choice of $k$, therefore for any selected $k$, total effort is a linear function of $n$ asymptotically, then $S(kn,n,kn-1)=\Theta(n)$, as desired. 

Let $\phi(k):=\lim_{n\to \infty}\frac{S(m,n)}{n}$. It is shown in the proof of Theorem~\ref{thm:UniversalBound} that when $k'>k$, $\phi'(k) < 0$, then $S(n,m)$ is decreasing in $[k,k']$, therefore $S(n,km) > S(n,k'm)$ when $n$ is sufficiently large, which completes the proof.
\end{proof}