\section{Towards Practical Applications}\label{sec:practicalApp}

In previous sections, we use asymptotic analysis to characterize and solve the optimal contests, which has a potential application value. In this section, we further provide numerical results to support that our findings fit well even for small $n$, bringing them closer to practical implementation.
In this section, we focus on the total effort objective, the most widely used in practice.

\noindent \textbf{Finding the Optimal Contest.} The optimal contest is a complete simple contest (Proposition~\ref{thm:ConpleteSimpleContest}), and the corresponding shortlist size grows linearly with  $n$. The slope $k$ is determined by the ability distribution and can be identified by solving an equation (Theorem~\ref{thm:OptAsmLinear}, asymptotic).

Figure~\ref{fig:DistributionOpt} illustrates that this linear trend emerges even for very small values of  $n$, with the asymptotic ratio providing a close prediction. Therefore, the optimal contest for any given distribution can be determined efficiently

\begin{figure}[htb]
\centering
\begin{subfigure}[ht]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/AsmUniTex.pdf}
    \subcaption{$F(x)=x$}
    \label{fig:disopt-a}
    \end{subfigure}
%\hfill
          % 子图 (b)
\begin{subfigure}[ht]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/AsmSquareTex.pdf}
    \subcaption{$F(x)=x^2$}
    \label{fig:disopt-b}
    \end{subfigure}
%\hfill
\begin{subfigure}[ht]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/AsmExpTex.pdf}
    \subcaption{$F(x)=1-e^{-x}$}
    \label{fig:disopt-c}
    \end{subfigure}

\caption{The actual optimal size and $m^*$ predicted by asymptotic relation.}
\label{fig:DistributionOpt}
\end{figure}
\noindent \textbf{Universal Upper Bound of the Optimal Size.} There is no distribution such that the optimal shortlist size is larger than $31.62\%n$ (Theorem~\ref{thm:UniversalBound}, asymptotic).
We propose an $O(n)$ algorithm to determine the supremum of the optimal shortlist size across all distributions for any given $n$ (Proposition~\ref{prop:SupM}). Numerical results in Figure~\ref{fig:universal1} confirm that the asymptotic linear trend holds even for very small $n$. Therefore, contest designers can confidently eliminate nearly 68\% of contestants without compromising optimality.


\begin{figure}
    \centering
    \begin{minipage}{0.30\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/UpperBound.pdf}
        \caption{Supremum of optimal $m$.}
        \label{fig:universal1}
    \end{minipage}
    %\hfill
    \begin{minipage}{0.55\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figure/flow_chart.pdf}
        \caption{A flow chart for contest design in practice.}
        \label{fig:flowchart}
    \end{minipage}
\end{figure}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.33\textwidth]{figure/UpperBound.pdf}
%     \caption{The upper bound of optimal $m$.}
%     \label{fig:universal}
% \end{figure}

\noindent \textbf{Performance Enhancement.} For any distribution, the two-player winner-take-all contest is $\Theta(\log n)$ times better (Proposition~\ref{prop:TotalTWOVAN}) and the optimal contest with a shortlist, it is $\Theta(n)$ times better (Theorem~\ref{thm:TotalOPTVAN}, asymptotic), compared to the optimal one without a shortlist. Moreover, even when the distribution is unknown, the designer can still attain a $\Theta(n)$ improvement simply by shortlisting to 31.62\% of the contestants, compared to having no shortlist (Corollary~\ref{coro:shortlistAlways}).

In Figure~\ref{fig:DistributionOpt1}, numerical results demonstrate that the asymptotic approximation ratio holds even for small $n$, and the performance of the asymptotic optimal contest is nearly identical to that of the actual optimal contest. This indicates that our algorithm yields a contest design that is not only near-optimal and highly effective at small scales but also guarantees asymptotic optimality and a strong approximation ratio. 

\begin{figure}[h]
\begin{subfigure}[ht]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/EffortUniTex.pdf}
    \subcaption{$F(x)=x$}
    \label{fig:disopt-a1}
    \end{subfigure}
%\hfill
          % 子图 (b)
\begin{subfigure}[ht]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/EffortSqureTex.pdf}
    \subcaption{$F(x)=x^2$}
    \label{fig:disopt-b1}
    \end{subfigure}
%\hfill
\begin{subfigure}[ht]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/EffortExpTex.pdf}
    \subcaption{$F(x)=1-e^{-x}$}
    \label{fig:disopt-c1}
    \end{subfigure}
    
\caption{Total effort performance of different contest designs.}
\label{fig:DistributionOpt1}
\end{figure}

Finally, we summarize our findings into a practical guideline, as illustrated in Figure~\ref{fig:flowchart}.

\noindent \textbf{Contest Design Cheatsheet.} 
%A complete simple contest with no more than $31.62\%$ of contestant admitted. Small scale? YES $\to$ Find optimal capacity by enumeration in $O(n)$, get $\Theta(n)$ / Use 2-player winner-take-all, get $\Theta(\log n)$. NO $\to$ Distribution known? NO $\to$ Admit $31.62\%n$, then get $\Theta(n)$. YES $\to$ Solve  asymptotic slope $k \leq 31.62\%$, admit $kn$, and get almost-optimal performance, $\Theta(n)$.
Is Distribution known? YES $\to$ Solve for the asymptotic slope $k \leq 31.62\%$, admit $kn$, and achieve almost-optimal performance, get $\Theta(n)$. NO $\to$ Is $n$ small? YES $\to$ Use 2-Contestant winner-take-all, get $\Theta(\log n)$. NO $\to$ Admit $31.62\%n$, get $\Theta(n)$. 

% How can we numerically find the worst distribution that reaching the highest optimal capacity? We rely on another representation of total effort, which help us to somehow decouple the contribution of contest structure itself and the distribution.

% \begin{lemma}[Quantile Representation for Total Effort]\label{lem:QuantileRep}
%     By using quantile $q:=1-F(x)$ and its reverse function $v(q):=F^{-1}(1-q)=x$, Ex-ante total effort of a simple contest expresses as:
%     \[
%     S(m,n, l)= n\int_0^1|v'(q)|\int_0^qG_{(m,l)}(t)\,dt\,dq,
%     \]where $l$ is the number of prizes, $G_{(m,l)}(t)=\frac{\binom{n-1}{l}(1-t)^{n-l-1}t^{l-1}}{\sum_{j=1}^{m}\binom{n-1}{j-1}(1-t)^{n-j}t^{j-1}}\int_{0}^{t}\sum_{j=1}^{m}\binom{n-1}{j-1}p^{j-1}(1-p)^{n-j}\,dp$. We use $H_{(m,l)}(q):=\int_0^qG_{(m,l)}(t)\,dt$ to denote the distribution-free part.  
% \end{lemma}

% Since Theorem~\ref{thm:ConpleteSimpleContest} states that optimal contest is a complete simple contest, i.e., $l = m-1$, we therefore omit $l$ in the following discussion.

% In this representation, total effort becomes the integration of the multiplication of a function $|v'(q)|$ determined by ability level distribution, and a function $H_{(m,l)}(q)=\int_0^qG_{(m,l)}(t)\,dt$ that is completely decided by the contest structure.

% Let us focus on the distribution-free part. We can plot $H_{m}(q)$ as a function of $q\in[0,1]$ for $m = 2,\ldots,n$. The example of $n=10$ is shown in Figure~\ref{fig:universal-b}. In this case, we can see that for some $m$ (e.g., $m=3$), $H_{(m)}(q)>H_{(m+1)}(q)$ holds point-wise, thus, $S(m,n) > S(m+1,n)$ stands true for arbitrary distributions, indicating that $m+1$ is a strictly dominated choice. 

% Actually, it can be shown that $H_{(m)}(q)/H_{(m')}(q)$ is decreasing with $q$ for $m'>m$ (Lemma~\ref{lem:FracDesc}), then $H_{(m)}(1) > H_{(m')}(1)$ is a suffice and necessary condition for $H_{(m)}(q)>H_{(m')}(q)$ point-wise. On the other hand, if $H_{(m)}(1) < H_{(m')}(1)$, then there exists unique $q' \in(0,1)$ such that $H_{(m)}(q')=H_{(m')}(q')$ and $H_{(m)}(q)<H_{(m')}(q)$ afterwards (e.g., the $q'$ for $m=2$ and $m'=3$ is marked with asterisk in Figure~\ref{fig:universal-b}). Since $v'(q)$ can be any positive function, we can always construct a distribution that satisfies $v'(q)=1$ when $q\geq q'$ and $v'(q)=\epsilon$ elsewhere such that $S(m,n) < S(m',n)$ (See an example distribution that make $m'=3$ better than $m=2$ in Figure~\ref{fig:universal-c}, where we let $q' \approx 0.859$, $\epsilon=0.01$, and $S(2,10) \approx 0.481 < 0.489 \approx S(3,10)$.).

% Therefore, we can find the $m$ that maximize $H_{m}(1)$. For $m'>m$, we have $H_{(m)}(q) > H_{(m')}(q)$ point-wise, so the optimal capacity can not be more than $m$. For $m'<m$, we have $H_{(m')}(1) < H_{(m)}(1)$, then we can still construct a distribution that satisfies $v'(q)=1$ when $q \geq \max\{\vec{q'}\}$ and $v'(q) = \epsilon$ elsewhere such that $S(m,n) > S(m',n)$ for all $m'<m$, hence we find an instance making $m$ the optimal capacity. We then conclude that $m$ is the tight upper bound for optimal capacity for given $n$, as desired.

% \begin{remark}
%     The insight from the construction of worst case distribution (e.g., Figure~\ref{fig:universal-c}) is, when almost all of the population are concentrated near the strongest end of ability level, it tends to need larger shortlist capacity to reach optimality. On the other hand, if highest ability only takes up a little probability mass, or equivalently, $|v'(q)|$ is much higher when $q$ is small, it tends to obtain optimality with fewer players. [Mathematical insight]. An uniform distribution, i.e., $|v'(q)|=1$, whose probability mass is evenly distributed, is right in the middle, with $k^*\approx15\%$, as shown in Example~\ref{exam:OptimalUniform}. 
% \end{remark}

% \begin{proposition}[Optimal Capacity for Exponential Distribution]\label{prop:OptCapExp}
%     For exponential distribution, i.e., $F(x)=1-e^{-\lambda x}$ and $f(x)=\lambda e^{-\lambda x}$, when $n \rightarrow \infty$ it holds that:
%     \[
%     \lim_{n \rightarrow \infty} m^*(n)/n = 9.70\%,
%     \]which is independent of the parameter $\lambda$. 
% \end{proposition}

% \begin{proposition}[Optimal Capacity for Power-Law Distribution]\label{prop:OptCapPowerLaw}
%     For power-law distribution $f(x)=cx^{-\alpha-1}, x\ge \delta$ that parametrized by $\alpha \in(0,1]$ and $\delta > 0$. When $n \rightarrow \infty$, $m^*=2$. 
% \end{proposition}
% \begin{remark}
%     Actually, the power-law distribution does not always achieve optimality at extremely small values of \( m \). When \(\alpha > 1\), \( S'(0) \rightarrow +\infty \), and \( S''(k) \) is initially negative and then positive. Consequently, \( S'(k) \) first decreases from \( +\infty \), then increases, eventually reaching \( S'(1) = 0 \). Thus, there exists a point in the interval \( (0,1) \) where \( S'(k) = 0 \), at which \( S \) attains its maximum value. From the condition \( S'(k) = 0 \), it follows that the optimal solution \( k \) satisfies the equation:
% \[
% \ln \frac{\alpha+(1-k)^2}{\alpha-(1-k)} + \frac{1}{\alpha}\ln k = 0.
% \]

% For large values of \(\alpha\), the term \(\ln \frac{\alpha+(1-k)^2}{\alpha-(1-k)}\) can be approximated as \(\ln \left(1+ \frac{(1-k)+(1-k)^2}{\alpha-(1-k)}\right) \simeq \frac{(1-k)(2-k)}{\alpha}\). Therefore, the equation simplifies to:
% \[
% \frac{(1-k)(2-k)}{\alpha} + \frac{1}{\alpha}\ln k = 0.
% \]

% The solution to this equation is approximately \( k_2 \approx 31.65\% \), which reaches the worst ratio given by Theorem~\ref{thm:UniversalBound}. 
% \end{remark}

%\subsection*{Hello} this is a subsection
