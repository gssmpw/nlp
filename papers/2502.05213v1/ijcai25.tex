%%%% ijcai25.tex
\pdfoutput=1
\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{subfigure}
% \usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{bm}

% \usepackage{mathabx}
\usepackage{amssymb}
% Comment out this line in the camera-ready submission


\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark \\ for Large Language Models}


% Single author syntax
\author{
Qihao Lin$^1$\and
Chen Tang$^1$\and
Lan zhang$^1$\and
Junyang zhang$^1$\And
Xiangyang Li$^1$\\
\affiliations
$^1$University of Science and Technology of China\\
\emails
lqh031106@mail.ustc.edu.cn,
chentang1999@mail.ustc.edu.cn,
zhanglan@ustc.edu.cn,
zhangjunyang@mail.ustc.edu.cn,
xiangyangli@ustc.edu.cn
}

\begin{document}

\maketitle

\begin{abstract}
% The potential risks of Large Language Models (LLMs) can be mitigated by embedding multi-bit watermarks. However, existing methods suffer from low efficiency due to the neglect of varying watermark-carrying capacities in texts, specifically, some generated texts are highly deterministic and resistant to watermarking, while others with lower determinism allow more efficient watermark embedding. The fixed-length segmentation also cause the high sensitivity to attacks: random insertions or deletions can cause misalignment of segments, rendering the watermark invalid. 
% We propose an improved approach: (1) estimating the watermark-carrying capacity of text segments based on LLM origin outputs; (2) dynamically allocating segments to ensure balanced capacity for each bit, thereby enhancing embedding efficiency; and (3) introducing a robust extraction scheme that resists text manipulation attacks to a certain extent. This method significantly improves both the efficiency and security of watermark embedding. 
%虽然训练良好的大模型表现出了卓越的性能在各个领域被广泛运用，但同时也带来了一些潜在的风险：如，恶意使用或者版权侵犯。一些研究通过向大语言模型的生成文本内，嵌入水印以追溯生成内容的分发，以对恶意行为或侵权行为进行追责。其中，多比特水印方案通过向给不同用户的输出中嵌入不同的水印，以识别输出内容是由哪个大模型生成并分发给哪个用户。然而，现有的方法选择将水印的每个比特嵌入到定长的tokens中或将整个水印嵌入到整个输出中，而没有考虑文本的水印容量。这会导致当生成文本的水印容量小时（如：代码），严重影响文本语义甚至直接水印嵌入失败。在这篇工作中，我们基于LLM的输出对token的概率分布，推导出了如何计算任意生成文本的水印容量。随后，我们设计DERMARK, 动态的根据生成本文分配tokens给水印的每个bit以嵌入。同时，为了使得水印提取对修改鲁棒，我们通过引入最小化比特损失来避免token变换对水印提取的影响。综合实验结果表明：
Well-trained large language models (LLMs) present significant risks, including potential malicious use and copyright infringement. Current studies aim to trace the distribution of LLM-generated texts by implicitly embedding watermarks. Among these, the single-bit watermarking method can only determine whether a given text was generated by an LLM. In contrast, the multi-bit watermarking method embeds richer information into the generated text, which can identify which LLM generated and distributed a given text to which user. However, existing efforts embed the multi-bit watermark directly into the generated text without accounting for its watermarking capacity. This approach can result in embedding failures when the text's watermarking capacity is insufficient. In this paper, we derive the watermark embedding distribution based on the logits of LLMs and propose a formal inequality to segment the text optimally for watermark embedding. Building on this foundation, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method. DERMARK divides the text into segments of varying lengths for each bit embedding, adaptively matching the text's capacity. It achieves this with negligible overhead and robust performance against text editing by minimizing watermark extraction loss. Comprehensive experiments demonstrate that, compared to the SOTA method, our method reduces the number of tokens required for embedding each bit by 20\%, reduces watermark embedding time by 50\%, and is robust to text editing and watermark erasure attacks.  



% 
% 当今LLM的一些潜在危害可以通过向大模型施加多比特水印来实现，但现有的多比特水印方法往往将水印划分为固定长度，为每个分段施加一个bit水印；这种水印方法没有考虑到对于不同文本的水印承载能力是不同的（例如，有些生成文本确定性较强，水印难以施加；有些则确定性弱，可以相对高效的施加水印），造成了水印嵌入效率偏低；同时定长的水印对攻击极度敏感，随机增删会导致分段错位，使水印无效。
%我们（1）给出了基于LLM输出计算文段水印承载能力的方法，即计算出特定分段水印成功嵌入的概率（2）基于该方法，根据大模型的输出特征，实现了为每个bit动态分配具有相同水印承载能力的分段。实现了对token的科学分配以提高水印嵌入效率（3）提出了一种鲁棒的水印提取方案，该方法可以还原出分段并提取水印，并对一定程度的攻击
\end{abstract}

\section{Introduction}
% 创新点侧重于动态选取的方法，这个是水印嵌入的一个步骤

% dynamic的原理和原理上的创新是什么
% 不懂dynamic segmentation是什么意思
% 一比特水印它需要的token数量的一个变量，不是一个定值，确定这个变量的原理是什么？它需要几个token是由什么决定的？要把原理的东西用自然语言进行表述，formal description，accurate mapping，theoretic analysis，dynamic embedding都很空，没有传达任何实际的做了什么东西，这里面有什么新的技术
% 文本承载水印的能力有什么决定的？是语义吗？
% 单单考虑水印嵌入到文本，是由什么决定其容量的？要把底层的原理讲清楚，然后讲清楚动态啊的设计，是基于它的底层概率分布来吗？
% 目前没有讲任何底层，任何原理，所谓的承载能力到底什么，依据的原理是什么，所有内容都很空洞

% |----------------------
% 目前的主流多bit水印嵌入，其文本的承载能力和模型对每个token 的输出的概率分布有关。
% 然而，大模型巨大的能力带来了潜在的恶意使用的风险，包括spam 和 plagiarism,和版权侵权风险，包括：重分发或再售卖大模型服务的侵害版权的行为发生。因此，急迫需要有一个能有效的对大模型的输出文本进行追溯和声明所有权的手段。
In recent years, large language models (LLMs) such as GPT-4 \cite{achiam2023gpt}, LLaMA \cite{genai2023llama}, and OPT \cite{zhang2022opt} have achieved significant advancements, excelling in tasks such as instruction following and question answering. Training an LLM requires substantial hardware resources, vast amounts of training data, and specialized expert knowledge. Consequently, LLMs are considered valuable intellectual property (IP) of their respective owners.

However, the increasing functionality of highly capable LLMs poses potential risks, including malicious uses such as spreading misinformation \cite{ChenS24} and generating harmful content \cite{perkins2023academic}, as well as copyright infringement, such as redistributing or re-selling purchased LLM services \cite{birch2023model}.
Given these malicious behaviors, there is an urgent need for mechanisms to trace the distribution of LLM-generated texts, safeguarding the IP rights of LLM owners.

% 首先根据prompt把词典划分成红绿两个集合，然后根据需要嵌入的水印bit，给对应的token的logit加bias（如：嵌入1时，给绿色的logit加bias），使其更容易被采样出来。在水印提取阶段，通过计算生成内容的每个segment中的红绿词所占比例，来提取（如：红词占比超过一半，则提取0）.SOTA工作选择设定固定长度的segment，会导致水印嵌入失败。DERMARK设置变化的segment，并有更大的容量。
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{process.pdf}
    \caption{Multi-bit Watermarking. In each token generation, the vocabulary is divided into green tokens and red tokens based on the prompt. Then, the bias is added to the LLM logits based on the embedding bit, thus making the corresponding tokens easier to sample. 
    The LLM-generated text will be divided into segments, and each bit of the watermark will be embedded into one of the segments.
    In the watermark extraction, the watermark bit is extracted by calculating the proportion of green and red tokens in each segment of the LLM-generated text (e.g., 0 if the red tokens make up more than half of the segment). 
    SOTA work chooses to set a fixed-length for each segment, which can lead to embedding failure. While DERMARK sets a variable-length for each segment, resulting in a larger capacity.}
    \label{fig:process}
    \vspace{-0.15in}
\end{figure}

% 大模型水印是一个很好的思路。通过隐式的向模型输出文本中嵌入水印，模型所有者追踪被分发的文本，并通过提取其中的水印来证明所有权并向恶意使用者追责。其中，当前的主流水印方法通过在LLM 推理阶段修改大模型的logit来嵌入水印，由于其高效和语义影响小从众多文本水印方法中脱颖而出。
The watermarking technique for LLMs has emerged as a promising solution \cite{liu2024survey}. By implicitly embedding a watermark into the output text, LLM owners can effectively trace the distribution of their generated content.
% 目前，大部分大模型水印工作致力于判断文本是否由指定大模型生成，称之为单比特水印。这类工作虽然有效的标识了模型的输出，但是缺乏同时对多个大模型的输出进行标识或对同个大模型给不同用户的输出标识的能力。
Currently, most watermarking techniques for LLMs focus on identifying whether a given text was generated by a specific LLM, a method commonly referred to as one-bit watermarking \cite{kirchenbauer2023watermark}. However, these approaches are unable to embed unique watermarks into LLM-generated texts tailored to different users.
% 为此，多bit大模型水印技术被提出，通过修改LLM输出的概率分布来向大模型中嵌入更丰富的水印信息以达到xxx效果。
% modifying the probability distribution of the LLM output over each token
% modifying the probability distribution of each predicted token
As a result, multi-bit watermarking solutions have been proposed to embed rich watermark information (e.g., binary strings \cite{WangYC0LM0024}) into LLM-generated texts by adding bias to the LLM logits, as shown in Fig.\ref{fig:process}. 
This approach enables the accurate identification of malicious use or copyright-infringing content, including determining which user generated it with which LLM.


% 文本低熵：LLM对文本的大部分token都以高概率进行输出；
% 文本高熵：LLM对文本的大部分token都易低概率进行输出；
% 水印容量->熵->对token的输出的概率分布

% 然而，文本可携带水印bit数量的能力是由LLM对预测token的概率分布决定的。当输出文本的大部分token，LLM对其都以高概率进行输出时，该段文本水印容量低；而当输出文本的大部分token，LLM对其预测的概率较低时，则该段文本水印容量高。
% 现有的尝试一股脑的把所有的水印都嵌入到输出文本中，而没有输出文本的水印容量。这会导致当水印bit过多时或文本搭配固定时（如，代码块），严重影响文本语义或水印嵌入失败。
% 直观来看，固定文本长度去嵌入不同bit数量的水印（CODEABLE）和固定水印长度去嵌入到不同的文本中（LLMMARK），都是不合理的。
% 因为LLM每次输出的内容都不一样，导致水印容量也不一样。
% 因此，我们认为，向大语言模型的生成文本中嵌入多比特水印的关键在于，
% 为水印的每一个bit根据文本的熵计算其嵌入所需的token数量，并对水印的每个bit进行逐个嵌入。
The primary factor determining the successful embedding of a multi-bit watermark is the watermark capacity of the LLM-generated text, defined as the maximum number of bits that can be embedded. This capacity is directly influenced by the text's entropy. For example, when the tokens in a text are predicted by the LLM with relatively even probabilities (i.e., high entropy), the watermark capacity is high.
However, existing methods \cite{WangYC0LM0024} overlook the watermark capacity of LLM-generated text and instead divide the text equally into multiple segments based solely on the watermark length, embedding each bit of the watermark into each segment. This approach significantly degrades the semantics of short texts. Furthermore, when the text exhibits low entropy (e.g., code), the watermark frequently fails to embed successfully.
% Intuitively, fixing the length of the text to embed different watermarks \cite{WangYC0LM0024} or fixing the length of the watermark to embed it into different texts \cite{zhang2024remark} is not reasonable, since LLM outputs different text each time, resulting in a different watermark capacity.
Thus, the critical aspect of embedding a multi-bit watermark into LLM-generated text lies in dynamically segmenting the text for each bit embedding based on its entropy and then embedding each bit into the corresponding segment.


% 挑战
Designing such a multi-bit watermarking method is a non-trivial task, presenting three key challenges:
% 1.缺乏动态分配token的有效方法。
% 虽然熵可以大致估计文本的水印容量，但熵值需要多少才能嵌入水印的一个比特是不清楚的。只有知道嵌入一个比特需要文本多少熵，才能动态的为每一个比特分配对应数量的token进行嵌入。根据预实验为熵设定一个阈值是一个可行的办法，但这只是一种模糊的估计，会受到实验条件和主管选取的影响，而带来误差。解决这个挑战需要建立一个根据文本内容到所需token数量的一一映射，是很难实现的
\textbf{(1) Lack of an effective method to segment the LLM-generated text.} While entropy can provide a rough estimate of the text's watermark capacity, it remains unclear how much entropy is required to embed a single watermark bit. This knowledge is essential for dynamically assigning segments to each bit for embedding. Although setting an entropy threshold based on pre-experiments is a feasible approach, it is inherently a subjective estimation, leading to potential errors and embedding failures.
Addressing this challenge requires constructing a one-to-one mapping between the watermark and the segmentation of LLM-generated text, a task that is highly complex and difficult to achieve.
% 2.多bit水印极易受到文本编辑的影响；每个bit信息都分配了不同长度的token数量，而对文本进行增删导致的token数量的变化，可能会使水印检测不出来；
(2) \textbf{Multi-bit watermarks are vulnerable to vanishing due to text editing.} Since each bit of the watermark is embedded within a specific segment, watermark extraction becomes highly sensitive to changes in the segment's tokens. If the LLM-generated text undergoes post-processing (e.g., additions or deletions), resulting in significant alterations to the segment, the watermark may become undetectable.
% 3. watermarks are vulnerable to various attacks. 水印容易受到恶意攻击者的攻击，从而被抹除：
% 输出文本由于加了水印，是有偏的，可能会被攻击者提取出水印嵌入策略从而进行攻击；
% 恶意攻击者可能会通过释义攻击来对输出文本进行释义，从而抹除水印。
(3) \textbf{Watermarks are vulnerable to erasure attacks.} Since the LLM-generated text is accessible to users, attackers may attempt to erase the watermark for malicious purposes or copyright infringement. For instance, because watermarked text exhibits bias, an attacker could infer the watermark embedding strategy by analyzing the LLM-generated text returned over multiple iterations, thereby erasing the watermark. Alternatively, an attacker could remove the watermark by paraphrasing the text, ensuring that it no longer reflects the watermark embedding.

% 还有个问题就是嵌入是嵌多少，如果不是全嵌，提取又要怎么提呢
% 感觉是嵌不了就不嵌；嵌得了就全嵌得了。因为所需bit数量少，所以同等文本，编码效率高

% 在这篇工作中，考虑熵是根据LLM对每个token的概率分布计算得到，
% 我们根据LLM对输出文本中的每个token的概率分布，理论推导出了水印嵌入服从的正态分布.然后以水印成功嵌入为条件，我们推导出了水印的每个比特嵌入需要满足的不等式。该推导结果首次对任意LLM生成文本，可以准确计算该文本的水印容量。基于这个推到结果，我们设计了一个动态高效的多bit水印嵌入方案。在水印嵌入阶段，方案会根据生成文本内容，通过上述不等式，计算所需最短数量的token以嵌入当前比特。随后重复此过程直到所有比特都成功嵌入。而在水印提取阶段，我们设计了基于全局最小损失的水印提取方案，通过在提取阶段引入水印误码率损失，降低因文本编辑带来的token数量的变化对水印提取的影响。此外，我们的方法兼容于目前主流的增强单比特水印鲁棒性的方法。虽然我们是多比特场景，但我们依然可以通过直接兼并他们的方案以实现无偏水印嵌入和对释义攻击鲁棒。
% 首先xxx，然后xxx，最后xxx，我们的主要贡献有xxx
In this work, we theoretically demonstrate that the watermark embedding, which is achieved by modifying the LLM logits, follows a normal distribution based on these logits. 
Then, we derive the inequality that must be satisfied for watermark bit embedding based on this distribution.
Based on this inequality, we can compute the required segment for each bit embedding and determine the watermark capacity of any LLM-generated text.
Subsequently, we design DERMARK, a dynamic and efficient multi-bit watermark embedding method. During the watermark embedding phase, the required segment is dynamically computed using the inequality derived above, based on the LLM logits.
% 此外，在整个水印嵌入过程中，我们对生成文本分段和水印嵌入都是基于大模型输出的logit进行的，引入的有限次计算开销相较于大模型的推理开销，几乎可以忽略不计。
Moreover, in DERMARK, text segmentation and watermark embedding are based on the LLM logits, introducing only a finite computational overhead, which is negligible compared to the LLM inference overhead.
% 通过动态规划，最小化分布差异和颜色差异（提取水印分布和原始分布的差异）来提取水印，从而
In the extraction phase, instead of directly extracting the watermark through the embedding method, we use dynamic programming to minimize the segmentation loss (the difference between the two sides of the derived inequality due to the current segmentation) and the color loss (the difference in the proportion of red and green tokens in each segment), thereby achieving robustness against text editing.
Additionally, DERMARK is compatible with robustness-enhancing one-bit watermarking methods. We leverage the one-bit watermarking method to embed each bit of the multi-bit watermark into each segment, enabling us to achieve robustness against erasure attacks by directly utilizing their improved watermarking techniques.
%根据LLM logits推导出了水印嵌入服从正态分布 这个分布能为水印每个比特嵌入需要使用生成文本中的多少个token提供判断条件

% 高度不够
Our main contributions are threefold as follows:
(1) Based on the LLM logits, we derive that watermark embedding follows a normal distribution. This distribution provides a basis for determining how many tokens in the generated text are required to embed each bit of the watermark.
(2) We design DERMARK, a dynamic, efficient, and robust method for multi-bit watermark embedding. This innovative method performs segmentation based on the text's watermark capacity, and the additional overhead is negligible. Moreover, DERMARK is robust against text editing and various other attacks.
(3) Comprehensive experiments show that DERMARK uses, on average, 2.26 fewer tokens than SOTA for watermark bit embedding, and the additional overhead is negligible compared to the LLM inference overhead. Furthermore, our method is robust to text addition, deletion, and watermark erasure attacks.
% -----------------------------------------related work---------------------------------------------------

\section{Related Work}
% 自2023年，首篇单bit水印被提出后，大量工作被提出，以达到xxxxxx的效果。然而，单bit的工作无法同时标识多个大模型或标识给不同用户的输出。为此，多bit工作被提出，工作1，2，3，4，5分别作了什么事，有什么缺点。
% 为此,我们想要做一个什么样的.
Since 2023, a series of efforts have been dedicated to embedding watermarks into LLM-generated text \cite{liu2024survey}. These works fall into two main categories: one-bit watermarking and multi-bit watermarking.
One-bit watermarking is used to determine whether a given text has been generated by a specified LLM.
However, they cannot be directly extended to multi-bit watermarking scenarios to meet the requirements of watermarking LLM-generated text from different LLMs for different users.
To address the above problem, multi-bit watermarking methods have been proposed to embed 0/1 strings into LLM-generated text. Given the diversity of watermarks, these methods can watermark different LLM-generated texts. Among them, \cite{WangYC0LM0024} proposes adding an additional watermark loss in the inference phase to embed each bit of the watermark into a fixed-length segment. \cite{zhang2024remark} proposes training a model to encode the watermark into the LLM-generated text. However, it is unreliable to embed the watermark directly into a fixed-length segment without considering its capacity. Intuitively, the watermark capacity varies from text to text. However, none of the current multi-bit watermarking methods account for the LLM-generated text's capacity. This can lead to embedding failure when the text has low watermark capacity.



Therefore, we aim to dynamically assign different segments to each bit of the multi-bit watermark based on the watermark capacity of the LLM-generated text and embed each bit into the corresponding segment.

% --------------------------------Problem Formulation-----------------------------------------
\section{Theoretical Analysis}

In this section, we first define the problem of multi-bit watermarking. Next, we provide the necessary notations and preliminaries to facilitate a better understanding of our design. Finally, we theoretically derive the method for dynamically assigning segments to each bit of the watermark.

\subsection{Problem Statement}
In multi-bit watermarking, the objective is to embed the multi-bit watermark into the LLM-generated text during the LLM inference phase. As mentioned previously, instead of directly embedding the watermark into the text, we assign different segments to each bit for embedding. Therefore, the embedding of multi-bit watermarks can be divided into two steps:
\\
\textbf{Step 1. Segmentation:} segment the LLM-generated text per bit for embedding;
\\
\textbf{Step 2. Embedding:} embed each bit of the watermark into the assigned segment.

Note that what we are concerned about is step 1, segmentation, rather than embedding. After segmentation, we leverage the one-bit watermarking method \cite{kirchenbauer2023watermark} to embed each bit into the assigned segment.


\subsection{Notations and Preliminaries}
To better understand the design of our method, we provide a brief description of the processes and notations involved in LLM inference, watermark embedding, and watermark extraction.

In the LLM inference phase, we denote $\mathbf{x}^{p}$ as the prefix prompt, $\mathbf{s}=\{s^{(0)},s^{(1)},\ldots\}$ as the LLM-generated text, $V = \{s_1, \ldots, s_{|V|}\}$ as the vocabulary, where $s^{(t)}$ represents the $t$-th token of the LLM-generated text, and $s_i$ represents the $i$-th token in the vocabulary. The entire input at the $t$-th step for LLM is the combination of $\mathbf{x}^{p}$ and the sequence of tokens $\mathbf{s}^{:t-1} = \{s^{(0)}, \ldots, s^{(t-1)}\}$, where $\mathbf{s}^{:t-1}$ represents tokens generated by the LLM in previous steps. 
The LLM takes these as input and outputs the logits $\mathbf{L}(\mathbf{x}^{p}, \mathbf{s}^{:t-1}) = \{l_1^{(t)}, \ldots, l_{|V|}^{(t)}\}$.
Then the logits are processed by the softmax function to produce a probability distribution $\mathbf{P}(\mathbf{x}^{p}, \mathbf{s}^{:t-1}) = \{P_1^{(t)}, \ldots, P_{|V|}^{(t)}\}$,
% \frac{e^{l_{|V|}^{(t)}}}{\sum_{j=1}^{|V|} e^{l_j^{(t)}}}
where $P_i^{(t)}=e^{l_{i}^{(t)}}/\sum_{j=1}^{|V|} e^{l_j^{(t)}}$ denotes the probability of the $i$-th token in the vocabulary at the $t$-th step.
Finally, the $t$-th token $s^{(t)}$ is sampled based on $\mathbf{P}(\mathbf{x}^{p}, \mathbf{s}^{:t-1})$ through a specific sampling rule, such as probabilistic sampling.
 
Multi-bit watermark embedding is achieved during the LLM inference phase.
For each bit $m_k$ of the multi-bit watermark $m \in \{0,1\}^K$, we assign a segment $S_k$ of consecutive tokens from $s$ for the $m_k$. Subsequently, we adopt the one-bit watermarking strategy to embed $m_k$ into $\mathcal{S}_k$. As shown in Fig.\ref{fig:process}, the watermarking process includes the following steps:

\textbf{1. Vocabulary Partitioning}: A random seed is used to partition the vocabulary into a “green list” $G$ and a “red list” $R$ of equal size. Generally, we assume:
$ G = \{s_{1}, \ldots, s_{|V|/2}\}, \quad R = \{s_{|V|/2+1}, \ldots, s_{|V|}\}.$

\textbf{2. Logits Modification}: If $m_k=1$, add a bias $ \delta $ to logits of green tokens;
% \begin{gather}
%     l_i'^{(t)}=\begin{cases}
%         l_i^{(t)}+\delta, \ if \ m_k=1 \ i \in [1,|V|/2],\\
%         l_i^{(t)}, \  otherwise.
%         \end{cases}
% \end{gather}
If $m_k=0$, add the $ \delta $ to logits of red tokens. 

\textbf{3. Softmax}: The modified logits $\mathbf{L}'(\mathbf{x}^{p}, \mathbf{s}^{:t-1})=\{l_{1}'^{(t)}, \ldots, l_{|V|}'^{(t)}\}$ are processed by the softmax function to generate a watermarked probability distribution $\mathbf{P}'(\mathbf{x}^{p}, \mathbf{s}^{:t-1})$.
Subsequently, $s^{(t)}$ is sampled based on $\mathbf{P}'(\mathbf{x}^{p}, \mathbf{s}^{:t-1})$.

% \textbf{3. Softmax}: The modified logits $\mathbf{L}'(\mathbf{x}^{p}, \mathbf{s}^{:t-1})=\{l_{1}'^{(t)}, \ldots, l_{|V|}'^{(t)}\}$ are processed by the softmax function to a watermarked probability distribution:
% \begin{gather}
%     \mathbf{P}'(\mathbf{x}^{p}, \mathbf{s}^{:t-1}) = \left\{\frac{e^{l_{1}'^{(t)}}}{\sum_{j=1}^{|V|} e^{l_{j}'^{(t)}}},  \ldots, \frac{e^{l_{|V|}'^{(t)}}}{\sum_{j=1}^{|V|} e^{l_{j}'^{(t)}}}\right\}.
% \end{gather}
% Subsequently, $s^{(t)}$ is sampled based on $\mathbf{P}'(\mathbf{x}^{p}, \mathbf{s}^{:t-1})$.
Eventually, once all tokens in $\mathcal{S}_k$ have been generated, $m_k$ is embedded into $S_k$ simultaneously.

% \textbf{4. Output}: Tokens are output according to the sampling mode and the post-watermarked probability distribution $\mathbf{P}'(\mathbf{x}^{prompt}, \mathbf{s}^{:t-1}) $
% \end{enumerate}

% By slightly increasing the probabilities of tokens in $ G $, the watermark is embedded into the generated text. 

In the watermark extraction phase, to extract the $k$-th bit of the watermark, denoted as $m'_k$, we examine the tokens in $S_k$ and calculate the proportion of tokens belonging to $G$ or $R$. If the number of tokens belonging to $G$ exceeds half of the total tokens in $S_k$, then $m'_k = 1$; otherwise, $m'_k = 0$.


\subsection{Theoretical Derivation}
Based on the above notations and processes, we are now ready to derive the mapping from the watermark to the text segmentation.


Since the vocabulary $V$ is partitioned into $G$ and $R$, we first analyze the probability that the next token $s_t$ generated belongs to $G$ or $R$ before watermarking. Let $P^{(t)}_G$ and $P^{(t)}_R$ respectively indicate that the next token $s^{(t)}$ at the $t$-th step belongs to $G$ or $R$, which can be calculated as:
\begin{gather}
    P^{(t)}_G = \frac{\sum_{s_i \in G}e^{l_i^{(t)}}}{\sum_{s_i \in V}e^{l_i^{(t)}}},\quad P^{(t)}_R = \frac{\sum_{s_i \in R}e^{l_i^{(t)}}}{\sum_{s_i \in V}e^{l_i^{(t)}}}.
\end{gather}


Then we analyze the probability that the next token $s^{(t)}$ generated belongs to $G$ or $R$ after watermarking. Let $P_G'^{(t)}$ and $P_R'^{(t)}$ respectively denote the probability of the next token $s^{(t)}$ at the $t$-th step belonging to $G$ or $R$ after watermarking. Then we have the following Lemma:
\begin{lemma}
\label{lemma:1}
If $m_k=1$,
$P_G'^{(t)} = \frac{e^\delta \cdot P_G^{(t)}}{e^\delta \cdot P_G^{(t)} + (1 - P_G^{(t)})}$; Similarly, if $m_k=0$,
$P_R'^{(t)} = \frac{e^\delta \cdot P_R^{(t)}}{e^\delta \cdot P_R^{(t)} + (1 - P_R^{(t)})}$.

\end{lemma}
\begin{proof}
Taking $m_k=1$ as an example.
% After applying Logit Modification and Softmax, we have the following conditional probabilities.

For $s_i\in G$, the predicted probability of $s_i$ is:

$P_{Gi}'^{(t)}(s_i \mid\mathbf{x}^{prompt}, \mathbf{s}^{:t-1}) = \frac{e^{l_{i}^{(t)} + \delta}}{\sum_{s_i \in G}e^{l_{j}^{(t)} + \delta} + \sum_{s_i \in R}e^{l_j^{(t)}}}.$

For $s_i\in R$, the predicted probability of $s_i$ is:

$P_{Ri}'^{(t)}(s_i \mid\mathbf{x}^{prompt}, \mathbf{s}^{:t-1}) = \frac{e^{l_i^{(t)}}}{\sum_{s_i \in G}e^{l_{j}^{(t)} + \delta} + \sum_{s_i \in R}e^{l_j^{(t)}}}.$

It is easy to see that $P_G'^{(t)}$ is the sum of $P_{Gi}'^{(t)}$:  

$P_G'^{(t)} = \sum_{s_i \in G} P_G'^{(t)}(s_i \mid\mathbf{x}^{prompt},  s^{:t-1}) \\
\Rightarrow P_G'^{(t)} = \frac{e^\delta \sum_{s_i \in G} e^{l_i^{(t)}}}{e^\delta \sum_{s_i \in G} e^{l_j^{(t)}} + \sum_{s_i \in R} e^{l_j^{(t)}}}=\frac{e^\delta \cdot P_G^{(t)}}{e^\delta \cdot P_G^{(t)} + (1 - P_G^{(t)})}.$

% $\because P_G^{(t)} = \frac{\sum_{s_i \in G} e^{l_i^{(t)}}}{\sum_{s_i \in V} e^{l_i^{(t)}}}$,

% % $\therefore P_G'^{(t)} = \frac{e^\delta \frac{\sum_{s_i \in G} e^{l_i^{(t)}}}{\sum_{s_i \in V} e^{l_i^{(t)}}}}{e^\delta \frac{\sum_{s_i \in G} e^{l_j^{(t)}}}{\sum_{s_i \in V} e^{l_i^{(t)}}} + (1- \frac{\sum_{s_i \in G} e^{l_j^{(t)}}}{\sum_{s_i \in V} e^{l_i^{(t)}}})}.$

% $\therefore P_G'^{(t)}=\frac{e^\delta \cdot P_G^{(t)}}{e^\delta \cdot P_G^{(t)} + (1 - P_G^{(t)})}.$
\end{proof}


For simplicity, we denote the probability that the next token is the one currently required for watermark embedding as $P'^{(t)}$. 
We then estimate the probability that the next token is the desired token by calculating the expectation of $P'^{(t)}$:
\begin{gather}\label{eq:1}
    \mathbb{E}[P'^{(t)}] =P(m_k=1)P'^{(t)}_G+P(m_k=0)P'^{(t)}_R.
\end{gather}

Subsequently, we begin calculating the proportion of required tokens in a segment. Let $S$ be a segment containing $N$ tokens. Denote $X$ as the number of required tokens in $S$ for the current bit embedding (e.g., $m_k = 1$ and $s^{(t)} \in G$), and let $T = X / N$ represent the ratio of required tokens in $S$. Then, we have:
\begin{lemma}
\label{lemma:2}
\begin{align}
T \sim \mathcal N(\frac{\sum_{t=1}^N\mathbb E [P'^{(t)}]}{N},\frac{ \sum_{t=1}^N\mathbb{E}[P'^{(t)}]-\sum_{t=1}^N\mathbb{E}^2[P'^{(t)}]}{N^2})
\end{align}
\end{lemma}
\begin{proof}
First, each token follows a Bernoulli distribution. Since $G$ and $R$ are divided independently each time, these tokens follow mutually independent Bernoulli distributions. The sum of multiple independent Bernoulli distributions follows a Poisson binomial distribution; thus, $X$ follows a Poisson binomial distribution.

Second, $X$ is the distribution formed by the sum of $N$ independent random variables, where each random variable is independent. According to the Central Limit Theorem, the Poisson binomial distribution can be approximated by a normal distribution $\mathcal{N}(\mu, \sigma^2)$ \cite{tang2023poisson}.
Among them,
\begin{equation}
\mu = \sum_{t=1}^N \mathbb{E}[P'^{(t)}],
\sigma^2 = \sum_{t=1}^N \mathbb{E}[P'^{(t)}] - \sum_{t=1}^N \mathbb{E}^2[P'^{(t)}].
\end{equation}  

Finally, since $X$ approximately follows a normal distribution and $N$ is a constant, $T = X / N$ approximately follows a normal distribution.
% Taking the token sequence with embedded bit 1 as an example, for a segment of N-long tokens, to accurately extract a single-bit watermark, it is necessary for more than half of the tokens in an $N$-length sequence to belong to the $G$. 
% To quantify this relationship, we define the ratio  $T = \frac{X}{N}$. Given that $X$ can be approximated as following  a normal distribution, $T$ approximately follows a normal distribution as well. The mean and variance of  $T$ are given by the following expressions: 
 Among them, 
 \begin{equation*}
     \mathbb E (T)= \mathbb E[\frac{X}{N}] = \frac{\mu}{N} =\frac{\sum_{t=1}^N\mathbb E [P'^{(t)}]}{N},
 \end{equation*}
\begin{equation*}
    \begin{aligned}
      Var(T) =Var(\frac{X}{N}) = \frac{ \sum_{t=1}^N\mathbb{E}[P'^{(t)}]-\sum_{t=1}^N\mathbb{E}^2[P'^{(t)}]}{N^2}.
    \end{aligned}
  \end{equation*}
\end{proof}
According to Lemma \ref{lemma:2}, to embed one bit into the segment $S$, it must hold that $T > \frac{1}{2}$ in $S$. Since $T$ follows a normal distribution, $P(T > \frac{1}{2})$ can be estimated using the confidence level $\alpha$:
\begin{gather}
P(T > \frac{1}{2}) =  \Phi( \frac{\frac{1}{2} -\mathbb E (T) }{\sqrt{Var(T) }}) \geq 1-\alpha,
\end{gather}
\begin{gather}
    \Rightarrow \Phi^{-1}(1- \alpha)) \leq \frac{\frac{1}{2}-\mathbb E (T)}{\sqrt{Var(T) }}. \label{eq:7}
\end{gather}
In conclusion, we have the following Theorem:
\begin{theorem} \label{thm:1}
 When $T$ and $\alpha$ satisfy the inequality Eq.\eqref{eq:7}, the watermark is embedded into the segment $S$.
\end{theorem}
% \begin{equation}
%     \begin{aligned}
%     \label{eq:2}
%       (\Phi^{-1}(\alpha))^2 \frac{ \sum_{t=1}^N\mathbb{E}[P'^{(t)}]-\sum_{t=1}^N\mathbb{E}^2[P'^{(t)}]}{N^2}\leq (\frac{\sum_{t=1}^N\mathbb E [P'^{(t)}]}{N} -\frac{1}{2})^2
%     \end{aligned}
%   \end{equation}
% \begin{proof}
%     When the inequality Eq.\eqref{eq:7} is hold, we have $T>\frac{1}{2}$.
%     This means that the number of desired tokens is more than half of all tokens in $S$.
%     That is, the watermark can be correctly extracted and the embedding is successful.
% \end{proof}

\begin{figure*}
 \centering
    \includegraphics[width=\linewidth]{workflow.pdf}
    \caption{Workflow of DERMARK.}
    \label{fig:workflow}
    \vspace{-0.15in}
\end{figure*}

Theorem \ref{thm:1} clearly demonstrates that, for a fixed confidence level $\alpha$, the length $N$ of the segment required to embed one bit can be calculated using the inequality in Eq.\eqref{eq:7}. Furthermore, we can also compute the maximum number of watermark bits (i.e., capacity) that can be embedded into any LLM-generated text using the same inequality, Eq.\eqref{eq:7}.

Based on the Eq.\eqref{eq:7}, we design DERMARK, a dynamic, efficient, and robust multi-bit watermark embedding method, described as follows.
  

\section{DERMARK}
% In this section, we describe the workflow of DEMAKR, including multi-bit watermark embedding and extraction. 

\subsection{Design Overview}
The workflow of our design is illustrated in Fig.\ref{fig:workflow}. It consists of two main phases: watermark embedding and watermark extraction.
In the watermark embedding phase, different watermarks are dynamically segmented using Eq.\eqref{eq:7} and embedded into the LLM-generated text for various users.
In the watermark extraction phase, the LLM-generated text is segmented through dynamic programming, optimizing segmentation loss and color loss to effectively extract the embedded watermark. 
\subsection{Multi-bit Watermarking}
% 介绍一下方程9是怎么计算的，放在小trick里面
Based on Theorem \ref{thm:1}, we segment the LLM-generated text using Eq.\eqref{eq:7} and embed each bit of the watermark into its corresponding segment. 
% 在每个token的生成过程中，如图2的水印嵌入所示，首先根据当前嵌入的bit是1还是0，给绿词或红词的logits加bias。随后logits被处理得到每个token的概率，并基于特殊的采样规则，如概率采样，采样出一个token。接着，带入方程9中进行判断。如果这个不等式成立，则意味着加入这个token后，该segment已经足够嵌入一个bit了，所以需要进行分段。下一个token生成后，加入到下一个segment中。反之，则意味着加入这个token后，仍然不足以嵌入一个bit，则下一个token生成后，仍然需要加入到当前segment中。
% 值得一提的是我们的方法是伴随着LLM推理过程
Specifically, during each token generation, as illustrated in the watermark embedding process in Fig.\textbackslash{}ref\{fig:workflow\}, a bias is added to the logits of tokens based on the bit currently being embedded. The modified logits are then processed to obtain the output probability for each token, after which a token is sampled. The sampled token is subsequently evaluated using Eq.\eqref{eq:7} to determine whether the inequality holds. If the inequality holds, it indicates that adding this token to the current segment has provided sufficient capacity to embed one bit. In this case, the next generated token should be assigned to a new segment. Conversely, if the inequality does not hold, it means that the current segment still lacks the capacity to embed one bit, and the next generated token should remain in the current segment. Finally, after adding the sampled token, the LLM iteratively generates the next token. As the text is progressively generated, the multi-bit watermark is simultaneously embedded into the text.

 

Nevertheless, the workflow outlined above faces several challenges in real-world applications. To address these limitations and enhance its practicality, we propose the following improvements:

\textbf{(1) Inequality determination}. The key step in determining  Eq.\eqref{eq:7} is to calculate $P(m_k=1) $ and $P(m_k=0)$ in Eq.\eqref{eq:1}. These probabilities are estimated based on the prior distribution of red and green tokens observed within the current segment:
\begin{align}
\label{eq:4}
P_t(m_k=1) = \frac{G_{a:b} + \lambda}{b-a +2\lambda}, P_t(m_k=0) = \frac{R_{a:b} + \lambda }{b-a +2 \lambda}.
\end{align}
Here, $a$ and $b$ denote the positions of the first and current tokens in the segment, respectively. $G_{a:b}$ and $R_{a:b}$ represent the counts of green and red tokens in the segment, respectively. The hyperparameter $\lambda$ is introduced to adjust the estimation. 

\textbf{(2) Redundant tokens}. When the number of tokens in the LLM-generated text exceeds the requirement for embedding the multi-bit watermark, the surplus tokens are utilized for padding. Specifically, the last bit of the watermark is reversed and embedded into these redundant tokens. Since the distribution of redundant tokens differs significantly from that of the last segment, it becomes straightforward to distinguish tokens used for watermark embedding from those that are redundant. 

% \begin{algorithm}[!h]
%     \caption{Text Generation with Dynamic Segment Marking}
%     \label{alg:1}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
    
%     \begin{algorithmic}[1]
%         \REQUIRE Prompt: $\mathbf{x}^{prompt} $, Bit Error Rate: $\alpha$, Binary message: $\mathcal{M}$, Maximum generation length: L%%input
%         \ENSURE A token string that carries  $\mathcal{M} $: $\mathcal{S} =\{s^{(0)}, \ldots, s^{(L)}\}$  %%output
%         \STATE Append the $(1-\mathcal{M}[k-1])$ token to \mathcal{M} 
%         \STATE $i \gets 0$
%         \STATE $P \gets \{\}$
%         \FOR{$t = 0,1,\dots,L$ do}
%             \STATE $k \gets \mathcal{M}[i]$  
%             \STATE Apply the large language model to prior tokens $\{\mathbf{x}^{prompt}, s^{(0 ) },..., s^{(t−1)}\}$ to get a logit vector $\mathbf{L}(\mathbf{x}^{prompt}, \mathbf{s}^{:t})$
%             \STATE Using $s^{(t - 1)}$to seed a randomly partition the vocabulary into two identically sized sets: $G$, $R$.
%              \STATE Based on bit $k$, choose either $G$ or $R$ to apply gain enhancement.
%             \STATE Using soft watermark method, watermark the token  sample the next token, $s^{(t)}$
%             \STATE Substitute $G$, $R$, $\alpha$, and $l^{(t)}$ into \eqref{eq:1} to compute $\mathbb{E}[P'^{(t)}]$.
%             \STATE Add  $\mathbb{E}[P'^{(t)}]$ to set $P$
%             \IF{\eqref{eq:7}is satisfied on $P$}
%                \STATE $P \gets \{\}$
%                \IF{i < k}
%                 \STATE $i \gets  i+1$
%                 \ENDIF
%             \ENDIF
%         \ENDFOR
%         \RETURN Outputs
%     \end{algorithmic}
% \end{algorithm}
% In this algorithm, me let each token substring out satisfy the inequality proved in \ref{sec:3}, and each substring carries the same single-bit watermark information. The generated text of LLM is arranged by multiple token substrings in order, so as to realize the embedding of multi-bit watermark.
% \subsection{Watermark Extraction}
% We provide a watermark extraction method  in \ref{alg:2}

% \begin{algorithm}[!h]
%     \caption{Watermark Extration}
%     \label{alg:2}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%      \begin{algorithmic}[1]
%         \REQUIRE A token string that carries $m$: $\{s^{(-N_p)}, \ldots, s^{(L)}\}$, Bit Error Rate: $\alpha$ %%input
%         \ENSURE Binary message: $\mathcal{M}'$%%output
%         \STATE $P \gets \{\}$
%         \STATE Initialize all counters and index variables to 0 
%         \FOR{$t = 0,1,\dots ,L$ do}
%             \STATE Using $s^{(t - 1)}$to seed a randomly partition the vocabulary into 2 identically sized sets: $V_0$, $V_1$.
%             \IF{$s^{(t)} \in V_0$}
%                 \STATE  $\text{count0} \gets \text{count0} + 1$
%             \ELSE
%                 \STATE $\text{count1} \gets \text{count1} + 1$
%             \ENDIF
%             \STATE Substitute $V_0$, $V_1$, $\alpha$, and $l^{(t)}$ into \eqref{eq:1} to calculate $P^{(t)}$.
%             \STATE Add  $P^{(t)}$ to set $P$
%             \IF{\eqref{eq:2} or \eqref{eq:3} is satisfied on $P$}
%                 \IF{count0 $\leq$ count1}
%                     \STATE  $\mathcal{M}'[i] \gets 0$
%                 \ELSE
%                     \STATE $\mathcal{M}'[i] \gets 1$
%                 \ENDIF
%                 \STATE $i \gets i + 1$
%                 \STATE $P \gets \{\}$
%                 \STATE  $\text{count0} \gets0$, $\text{count1} \gets0$
%             \ENDIF
%         \ENDFOR
%         \RETURN Outputs
%     \end{algorithmic}
% \end{algorithm}

% This algorithm reproduces the result of the segmentation in the process of applying watermark. It extracts the bit of each segment according to the number of tokens belonging to the set of $V_0$ and $V_1$ in each segment, so as to realize the extraction of multi-bit watermark.
\subsection{Watermark Extraction}
% Simply, watermark extraction can be achieved by repeating the watermark embedding process once, segmenting the LLM-generated text and extracting the embedded bit from each segment.
% In practice, however, text may be edited by the user, e.g., additions or deletions. The segmentation principle of the text is extremely sensitive to token changes, which leads to the fact that for the modified text, the watermark may not be extracted by repeating the watermark embedding process.

Since the segmentation inequality in Eq.\eqref{eq:7} is highly sensitive to token changes, even minor text edits can lead to different segmentation outcomes, thereby making the watermark extraction process unreliable. To address this issue and ensure accurate watermark extraction while minimizing the impact of text editing, we focus on identifying an optimal segmentation strategy for watermark extraction.

Assume a segmentation result is represented as $\text{Seg}(\mathcal{S}) = \{\dots,\mathcal{S}^{(a:b)},\dots. \}$.
Intuitively, for any given segmentation result, each segment must satisfy the inequality in Eq.\eqref{eq:7}. Consequently, we define the segmentation loss as the difference between the two sides of the inequality in Eq.\eqref{eq:7} for each segment:
\begin{gather}
    \mathcal{L}_s(a,b) = (f(\mathbb E [P'^{(t)}])-(\Phi^{-1}(\alpha))^2 -\epsilon_s)^2, \\
    f(\mathbb E [P'^{(t)}]) = \frac{(\sum_{t=a}^{b-1}\mathbb E [P'^{(t)}]-\frac{b-a}{2})^2}{ \sum_{t=a}^{b-1}\mathbb{E}[P'^{(t)}]-\sum_{t=a}^{b-1}\mathbb{E}^2[P'^{(t)}]}.
\end{gather}
Here, $a$ and $b$ denote the start and end indices of the segment, respectively, and   $\epsilon_s$ is a hyperparameter introduced to correct for bias deviations in the inequality. 

Furthermore, due to the embedded watermark, each segment will exhibit a noticeable imbalance, with significantly more red or green tokens than the other. To quantify this imbalance, we define the color loss within each segment as the difference in the number of tokens of each color, expressed as follows:
\begin{equation}
        \mathcal{L}_c(a,b) = | \frac{min(G_{a:b},R_{a:b})}{b-a} -\epsilon_d |,
\end{equation}
where $\epsilon_d$ is also a hyper-parameter used to correct for the bias deviation caused by the inequality.

\begin{figure*}[t]
    \centering
    \subfigure[OPT-1.3b across total dataset]{
        \label{fig:opt_total}
        \includegraphics[width=0.23\textwidth]{opt_noattack.pdf}}
    \subfigure[GPT-2 across total dataset]{
        \label{fig:gpt_total}
        \includegraphics[width=0.23\textwidth]{gpt_noattack.pdf}}
    \subfigure[OPT-1.3b across poor dataset]{
        \label{fig:opt_poor}
        \includegraphics[width=0.23\textwidth]{opt_buttom.pdf}}
    \subfigure[GPT-2 across poor dataset]{
        \label{fig:gpt_poor}
        \includegraphics[width=0.23\textwidth]{gpt_buttom.pdf}}
        \vspace{-0.15in}
    \caption{Watermark Capacity Comparison on OPT-1.3b and GPT-2.}
    \label{fig:capacity}
    \vspace{-0.15in}
\end{figure*}

% The extraction method restores each segment by optimizing the loss function. The loss function needs to be characterized simultaneously:
% \begin{enumerate}
%     \item \textbf{Offset Within Segments Relative to Eq.\eqref{eq:7}}: During the watermark embedding process, segmentation is determined by satisfying Eq.\eqref{eq:7}. Therefore, during the extraction process, it is necessary to minimize the offset of each segment relative to this inequality. Additionally, we found that during the actual watermark embedding process, due to the discrete nature of tokens, the inequality on both sides of the segmentation often doesn't match exactly. Possible attacks may also cause a shift in the inequality. Therefore, we added a correction offset $\epsilon_e$ to the loss for this part. %并且我们发现在实际水印嵌入过程中，由于token的离散属性，分段时不等式左右往往不会精确相等，所以我们给该部分的损失添加了一个修正值 
%     $$
%     \mathcal{L}_e(j,i) =  \left( \frac{(\sum_{t=j}^{i-1}\mathbb E [P'^{(t)}]-\frac{i-j}{2})^2}{ \sum_{t=j}^{i-1}\mathbb{E}[P'^{(t)}]-\sum_{t=j}^{i-1}\mathbb{E}^2[P'^{(t)}]} - (\Phi^{-1}(\alpha))^2 -\epsilon_e \right) ^2
%     $$
%     \item \textbf{Token Partition Dispersion Within Segments}:Within a given segment, the actual variance of red and green words in the text should closely match the variance calculated using Equation 5. Therefore, we designed a loss function. Additionally, to align with the purpose of adding an offset variable in the previous loss function, we introduced a correction offset  $\epsilon_d$ for this loss function as well. 
%     $$
%     \mathcal{L}_d(j,i) = | \frac{min(\text{GreenCount}_{j:i},\text{RedCount}_{j:i})}{j-i} -\epsilon_d |
%     $$
% \end{enumerate}
As a result, the watermark extraction loss is the sum of the above two losses described above:
\begin{gather}
    \mathcal{L}(\text{Seg}) = \sum \mathcal{L}(j,i) = \sum ( \beta \cdot \mathcal{L}_s(j,i) + \mathcal{L}_c(j,i)), \label{eq:3}
\end{gather}
where $\beta$ is an adjustable parameter.
% The two losses are combined into a segment loss function using the parameter $\beta$ :
% \begin{gather}
%     \mathcal{L}(j,i) = \beta \cdot \mathcal{L}_e(j,i) + \mathcal{L}_d(j,i) \label{eq:3}
% \end{gather}
% Furthermore, the total loss of a text segment is the sum of the losses from each individual segment. 
% $$\mathcal{L}(\text{Seg}) = \sum_{k=0}^M \mathcal{L}(j_k,i_k)$$
By minimizing this loss, we use dynamic programming to identify an optimal segmentation among multiple segmentation options with a time complexity of $O(N^2) $. The entire watermark extraction process is illustrated in Fig.\ref{fig:workflow}, which involves the following key steps: 
\textbf{1. Color each token.} First, we assign a color to each token, with the token color determined by hashing the previous token. 
\\
\textbf{2. Identify the padding.} Next, we identify the padding by locating all tokens with the watermark embedded. Since padding embeds the inverse of the last bit of the multi-bit watermark, it can be recognized based on the distribution of tokens in different colors. 
% The padding $\mathbf{s}^{end:}$ can be obtained through:
% \begin{gather}
%     end = { \underset {t} { \operatorname {arg\,max} } \, ( max(G_{t:}, R_{t:})-min (G_{:t-1}, R_{:t-1}))}. 
% \end{gather}
\\
\textbf{3. Determine Segmentation.}
Next, we begin segmenting the text using dynamic programming. We initialize the values of $\epsilon_e = 0$ and $\epsilon_d = 0$ and define the total loss $L[k][p]$, which represents the sum of losses for dividing the tokens $\mathbf{s}^{:p}$ into $k$ segments. Additionally, we maintain a predecessor array $\text{prev}[t][b]$, which stores the starting position of the last segment ending at $a$ for $t$ segments.
The optimization process follows the recurrence relation below:
\begin{equation*}
    If \ \quad L[t-1][a] + \text{cost}[a][b] < L[t][b], 
\end{equation*}
\begin{equation*}
    then \ L[t][b] = L[t-1][a] + \text{cost}[a][b], \text{prev}[t][b] = a,
\end{equation*}
where:$ L[t-1][a] $ is the minimum loss for dividing the first $ a $ tokens into $ t-1 $ segments;
$ \text{cost}[a][b] $ is the loss of a single segment spanning from token $ a $ to $ b $;
$ L[t][b] $ is the minimum loss for dividing the first $ b $ tokens into $ t $ segments.
$ \text{prev}[t][b] $ is the starting position $ a $ of the segment ending at $ b $ in the $ t $-th segment.

If the total loss for dividing the first $ a $ tokens into $ t-1 $ segments and adding the loss of the segment from $ a $ to $ b $ is smaller than the current loss for dividing the first $ b $ tokens into $ t $ segments, then $ L[t][b] $ is updated to this smaller value. At the same time, the predecessor of $ b $ is recorded as $ a $, preserving the segmentation information.
By iterating through all possible values of $ b $, $ a $, and $ t $, we ultimately find the minimum value of $ L[k][p] $, which represents the most minor loss for dividing the first $ p $ tokens into $ k $ segments.
\\
\textbf{4. Update $\epsilon$.} Since text editing affects $\epsilon_s$ and $\epsilon_c$, we update them based on the revised segmentation result. Specifically, for each segment $\mathcal{S}^{(a:b)}$, we define:
$\epsilon_s'(a,b) = f(\mathbb{E} [P'^{(t)}]) - (\Phi^{-1}(\alpha))^2$ and
$\epsilon_c'(a,b) = \min(G_{a:b},R_{a:b}) / (b-a)$.
Then, the parameter $\epsilon_s$ is updated to the average value of $\epsilon_s'(a,b)$, and similarly, $\epsilon_c$ is updated to the average value of $\epsilon_c'(a,b)$.
\\
\textbf{5. Iterate Until Convergence.}
Repeat steps 3 and 4 until $\epsilon_s$ and $\epsilon_c$ converge to stable values.
\\
\textbf{6. Extract the Watermark from Segments.}
Extract the watermark bit from each segment based on the proportion of red and green tokens. 

% 该方法对文本编辑鲁棒的原因在于通过损失约束来避免分段错误，并通过\epsilon 放款不等式约束以避免token变化带来的不等式不再成立的影响。
The watermark extraction method ensures robustness to text editing by preventing segmentation errors through loss constraints and relaxing the inequality constraints using $\epsilon$. This helps mitigate the impact of token changes on the determination of inequality in Eq.\eqref{eq:7}.
% \begin{algorithm}[t]
%     \caption{Robust Segmentation}
%     \label{alg:3}
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \begin{algorithmic}[1]
%         \REQUIRE A token string that carries  $\mathcal{M} $: $\mathcal{S} =\{s^{(0)}, \ldots, s^{(L)}\}$, Bit Error Rate: $\alpha$ , bit number:k%%input
%         \ENSURE Segmentation for the Token string:Segments%%output
%         \STATE Calculate the membership of each token $s^{(t)}$ in either $V_0$ or $V_1$, and store the result in bit
%         \STATE Calculate the $P^{(t)}_0$ for each token $s^{(t)}$, and store the result in $P$
%         \STATE Initialize 2D array $\mathcal{L}$ of size $\text{k} \times (\text{L}+1)$ , 1D array $\text{Seg}$ of size $ (\text{L}+1)$
%         \FOR{$p = 1,2,\dots ,\text{k}$ do}
%             \FOR{$q = 1,2,\dots ,\text{L }+ 1$ do}
%                 \STATE $\mathcal{L}[p][q] \gets [\infty]* (L+1)$ 
%             \ENDFOR
%         \ENDFOR
%         \FOR{$t = 1,2,\dots ,k$ do}
%             \FOR{$i = 1,2,\dots ,L$ do}
%                 \FOR{$j = 0,1,\dots ,i-1$ do}
%                     \STATE $\mathcal{L}_e \gets \mathcal{L}_e(j,i)$
%                     \STATE $\mathcal{L}_d \gets \mathcal{L}_d(j,i)$
%                     \IF{$\mathcal{L}[t-1][j]+\mathcal{L}_e + \mathcal{L}_d < \mathcal{L}[t][i]$}
%                         \STATE $\mathcal{L}[t][i] \gets \mathcal{L}[t-1][j]+\mathcal{L}_e + \mathcal{L}_d $
%                         \STATE $\text{Seg}[i] \gets j$
%                     \ENDIF
%                 \ENDFOR
%             \ENDFOR
%         \ENDFOR
%         \STATE $\text{Segments} \gets []$ 
%         \STATE $\text{current} \gets n$
%         \WHILE{$\text{current} > 0$}
%             \STATE $\text{start} \gets \text{Seg}[\text{current}]$
%             \STATE Append $(\text{start}, \text{current})$ to Segments
%             \STATE $\text{current} \gets \text{start}$
%         \ENDWHILE
%         \RETURN Segments
%     \end{algorithmic}
% \end{algorithm}

% ---------------------------------------实验---------------------------------------------------

\section{Experiments}
In this section, we conduct extensive experiments to demonstrate the superior performance of DERMARK in terms of watermark capacity, efficiency, and robustness.

\subsection{Experimental Setup}
% 没讲清楚beta设置的原因
In all experiments, we use the OPT-1.3b \cite{zhang2022opt} and GPT-2 \cite{radford2019language} models as the LLMs for text generation, and the news-like subset of the C4 dataset \cite{raffel2020exploring} as the prompt for evaluation. During the LLM inference phase, texts are randomly sampled from this subset, truncated to 100 tokens as input prompts, and tokens are generated through multinomial sampling, with a repetition penalty of 1.5 applied to enhance text diversity. The hyper-parameter $\alpha$ is set to [0.8, 0.99] to control watermark embedding, $\beta$ is set to 14, and $\lambda$ is defined as $\alpha * \Phi^{-1}(\alpha))^2$. The SOTA multi-bit watermarking method, Balance-Marking, is chosen as the baseline, with GPT-2 serving as its ${LM}_{proxy}$. All experiments were conducted on a server running CentOS Linux. The hardware setup included an Intel Xeon E5-2650 v4 CPU, with an NVIDIA Tesla P40 GPU equipped with 22 GB of memory, running CUDA version 11.3. The experiments utilized Python 3.9.13 as the primary programming environment.


\subsection{Watermark Capacity}
To demonstrate the superior performance of DERMARK in watermark capacity, we first evaluate the performance difference between DERMARK and Balance-Marking on the evaluation dataset. We calculate the watermark capacity by adjusting the value of $\alpha$ in DERMARK. Specifically, for each value of $\alpha$, we use the LLM to generate 500 texts. Then, we extract watermarks from these texts to compute the average watermark detection rate and the average number of tokens per bit for embedding. Similarly, the results for the baseline are obtained by adjusting the segment length for each bit.
The results are shown in Fig.\ref{fig:opt_total} and Fig.\ref{fig:gpt_total}. When achieving the same watermark detection rate, DERMARK uses, on average, 2 fewer tokens per bit for embedding on OPT-1.3B, and 1 fewer token per bit for embedding on GPT-2, compared to the baseline.
% \begin{figure}[t]
%     \centering
%     \subfigure[The watermark capacity on OPT-1.3b.]{
%         \label{opt_noattack.pdf}
%         \includegraphics[width=0.23\textwidth]{opt_noattack.pdf}}
%     \subfigure[The watermark capacity on GPT-2.]{
%         \label{gpt_noattack.pdf}
%         \includegraphics[width=0.23\textwidth]{gpt_noattack.pdf}}
%     \caption{Watermark Capacity Comparison.}
%     \label{fig:capacity}
% \end{figure}
To further demonstrate the superior performance of DERMARK, we construct the top 25\% of the worst-performing datasets, referred to as the "poor" dataset, based on the watermark detection rate of the baseline in the evaluation dataset. We then evaluate the performance of DERMARK on these datasets. The results are shown in Fig.\ref{fig:opt_poor} and Fig.\ref{fig:gpt_poor}. On the poor dataset, DERMARK significantly outperforms the baseline. First, DERMARK uses at least 4 fewer tokens per bit to achieve the same watermark detection rate. Additionally, DERMARK produces more stable results, unlike the baseline, which exhibits numerous outliers. Finally, the baseline performance plateaus after reaching a certain accuracy on this dataset (0.88 for OPT-1.3B and 0.925 for GPT-2).

% % % 1. 水印嵌入效果比
% % % 1. 水印嵌入效果比较
% % Fig.\ref{fig:across different datasets} show the trade off between Token Cost per Bit and Watermark Recovery Rate. The results indicate that the DERMARK method consistently demonstrates stable performance across different watermark recovery rates, approaching the theoretical optimal token cost. In the bottom 25\% (the poorer test subset), the watermark cost (average tokens per bit) of the DERMARK method is significantly lower than that of the Balance-Marking method. This suggests that the DERMARK method performs better on the poorer test subset, possibly due to its dynamic segmentation strategy being better suited to varying text features. In the top 25\% (the excellent test subset), the watermark cost of the DERMARK method is slightly lower than that of the Balance-Marking method. This indicates that the DERMARK method maintains its advantage even in the excellent test subset. The experimental results demonstrate that the DERMARK method maintaining a lower watermark cost for similar recovery rates across the different text characteristics. This confirms that DERMARK’s dynamic segmentation strategy is more adaptable and efficient in handling different types of text data.


\begin{figure*}[t]
    \centering
    \subfigure[Insertion Attack on OPT-1.3b]{
        \label{fig:opt_insert}
        \includegraphics[width=0.23\linewidth]{opt_insert.pdf}
    }
    \subfigure[Insertion Attack on GPT-2]{
        \label{fig:gpt_insert}
        \includegraphics[width=0.23\linewidth]{gpt_insert.pdf}
    }
    \subfigure[Deletion Attack on OPT-1.3b]{
        \label{fig:opt_delete}
        \includegraphics[width=0.23\linewidth]{opt_delete.pdf}
    }
    \subfigure[Deletion Attack on GPT-2]{
        \label{fig:gpt_delete}
        \includegraphics[width=0.23\linewidth]{gpt_delete.pdf}
    }
    \vspace{-0.15in}
    \caption{Comparison of insertion and deletion attacks.}
    \label{fig:attack_comparison}
    \vspace{-0.15in}
\end{figure*}


\subsection{Impact of Watermarking on Text Quality}
% 我们随机选择了500个prompt。对每个delta，我们在每个prompt上使用GPT-2和OPT-1.3b生成文本并使用OPT-2.7b评估生成文本的PPL指标以衡量文本质量 
We explored the impact of watermark strength on text quality by varying the value of $\delta$. A total of 500 prompts were randomly selected. For each value of $\delta$, we used both GPT-2 and OPT-1.3B to generate text for each prompt and evaluated the generated text using the Perplexity (PPL) metric with OPT-2.7B to assess text quality. Lower PPL values indicate higher text quality.
Table \ref{table:combined_ppl} summarizes the text quality data for watermarked text generated by the two methods. The PPL results for both methods exhibit similar trends across varying $\delta$ values.
% While the PPL values differ slightly at specific delta values, they generally intersect, exhibit comparable statistical characteristics, and follow similar patterns as delta increases. 
% Notably,  This observed similarity arises because both methods implement a strategy of selecting specific subsets to apply delta during the watermarking process. 
As a result, the two approaches demonstrate comparable performance in terms of text quality, as measured by the PPL metric.
% Please add the following required packages to your document preamble:

% \begin{table}[]
% \begin{tabular}{@{}rrrrrr@{}}
% \toprule
%                      &               & \multicolumn{2}{r}{OPT-1.3b} & \multicolumn{2}{r}{GPT-2} \\ \midrule
%                      & $\delta  $       & Balance-Marking   & DERMARK   & Balance-Marking  & DERMARK \\
% \multirow{8}{*}{PPL} & 0 & \multicolumn{2}{r}{215}      & \multicolumn{2}{r}{42672} \\
%                      & 0.5           & 216               & 213      & 41379            & 41774  \\
%                      & 0.8           & 228               & 233      & 42236            & 42591  \\
%                      & 1.0             & 232               & 236      & 42174            & 42554  \\
%                      & 1.2           & 243               & 247      & 43289            & 42731  \\
%                      & 1.5           & 224               & 225      & 41736            & 41881  \\ \cmidrule(lr){2-5}
%                      & 1.8           & 253               & 262      & 43121            & 41622  \\
%                      & 2.0             & 289               & 256      & 44438            & 44482  \\ \cmidrule(l){2-6} 
% % \caption{PPL Comparison of Balance-Marking and DERMARK on OPT-1.3b and GPT-2.}
% \label{table:combined_ppl}
% \end{tabular}
% \end{table}

\begin{table}[t]
\centering
\scriptsize % 缩小表格字体
\begin{tabular}{@{}rrrrrrr@{}}
\toprule
\multirow{2}{*}{$\delta$} & \multicolumn{2}{c}{OPT-1.3b}                & \multicolumn{2}{c}{GPT-2}                 \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
                           & Balance-Marking & DERMARK                  & Balance-Marking & DERMARK                  \\ \midrule
0         & \multicolumn{2}{c}{215}                   & \multicolumn{2}{c}{42672}                 \\\cmidrule(lr){2-3} \cmidrule(lr){4-5}
0.5                        & 216             & 213 ($\downarrow 3$)    & 41379           & 41774 ($\uparrow 395$)  \\
0.8                        & 228             & 233 ($\uparrow 5$)      & 42236           & 42591 ($\uparrow 355$)  \\
1.0                        & 232             & 236 ($\uparrow 4$)      & 42174           & 42554 ($\uparrow 380$)  \\
1.2                        & 243             & 247 ($\uparrow 4$)      & 43289           & 42731 ($\downarrow 558$)\\
1.5                        & 224             & 225 ($\uparrow 1$)      & 41736           & 41881 ($\uparrow 145$)  \\
1.8                        & 253             & 262 ($\uparrow 9$)      & 43121           & 41622 ($\downarrow 1499$)\\
2.0                        & 289             & 256 ($\downarrow 33$)   & 44438           & 44482 ($\uparrow 44$)   \\ \bottomrule
\end{tabular}
\caption{Perplexity Comparison. A lower PPL values indicate higher text quality. When $\delta$ = 0, the PPL represents the raw output of the large language model without watermarking.}
\label{table:combined_ppl}
\vspace{-0.15in}
\end{table}




 
\subsection{Efficiency}

We evaluated the text generation and watermark extraction efficiency of both Balance-Marking and DERMARK. For Balance-Marking, the large language model used for text generation also served as the auxiliary model, with a fixed watermark embedding length of 10 tokens per bit. For DERMARK, we set the error rate to 10\%. Both methods were tested with $\delta = 1$ and a fixed prompt length of 100 tokens.


As shown in Fig. \ref{fig:time_cost}, during the watermark embedding phase, the DERMARK watermarking method demonstrates almost identical time overhead compared to the raw outputs of large models, which is significantly lower than that of Balance-Marking. This indicates that DERMARK achieves substantial time optimization during the embedding phase. Although the time overhead of DERMARK during the watermark extraction phase is slightly higher than that of the baseline, DERMARK provides more robust watermark extraction (as described in the next subsection) with only a minimal additional time overhead, which is acceptable in practice. Overall, DERMARK incurs lower time costs in practical applications and is more suitable for large-scale text watermarking tasks. 

\subsection{Robustness}
We demonstrate that DERMARK exhibits robustness both to text modifications and to erasure attacks. 

\textbf{(1) Insertion Attacks}. After inserting 5\% and 10\% of additional tokens into the generated text of OPT-1.3b (Fig. \ref{fig:opt_insert}) and GPT-2 (Fig. \ref{fig:gpt_insert}), DERMARK demonstrates higher watermark detection rates while requiring fewer tokens per bit compared to Balance-Marking. The scatter points and fitted lines show that DERMARK's curve lies below that of Balance-Marking, highlighting its ability to achieve equivalent accuracy with reduced token overhead.

\textbf{(2) Deletion Attacks}. After deleting 5\% and 10\% of tokens from the generated text of OPT-1.3b (Fig. \ref{fig:opt_delete}) and GPT-2 (Fig. \ref{fig:gpt_delete}), DERMARK consistently outperforms Balance-Marking, exhibiting a lower token-per-bit ratio and higher watermark detection rate. This advantage becomes more pronounced at higher detection rates, where efficiency differences are critical. The key takeaway is that DERMARK’s superior efficiency in token utilization enhances its robustness against adversarial attacks, ensuring reliable watermark detection with minimal overhead. This performance advantage highlights DERMARK’s potential as a preferred method for practical applications requiring efficient and resilient text watermarking.

\begin{figure}[t]
    \centering
    \subfigure[Text Generation]{
        \label{fig:tim_generation}
        \includegraphics[width=0.45\linewidth]{tim_generation.pdf}
    }
    \subfigure[Watermark Extraction]{
        \label{fig:tim_extraction}
        \includegraphics[width=0.45\linewidth]{tim_extraction.pdf}
    }
    \vspace{-0.15in}
    \caption{Comparison of time cost in text generation and watermark extraction.}
    \label{fig:time_cost}
    \vspace{-0.15in}
\end{figure}

\textbf{(3) Erasure Attacks}. Our method does not impose specific requirements on the division of red and green sets or the choice of hash functions, ensuring compatibility with existing semantically unbiased approaches. By adjusting the partitioning of the red and green subsets, we can achieve semantic neutrality in watermarked texts \cite{hu2023unbiasedwatermarklargelanguage}. Similarly, our method is compatible with SIR, allowing us to leverage textual semantics to guide the hash function, thereby enhancing robustness against erasure attacks \cite{liu2024semanticinvariantrobustwatermark}.
%因为我们的方法对红绿集的划分和hash函数的选择没有特定要求，因此可以兼容现有的语意无偏的方法，通过改变红绿子集的划分，实现水印文本的无偏性。相似的，我们也可以兼容SIR，使用文本语意来驱动hash函数，来实现对Paraphrasing Attack 的鲁棒性.

% The key takeaway is that DERMARK's efficiency in token utilization makes it more robust against adversarial attacks, ensuring reliable watermark detection with minimal overhead. This performance advantage underscores DERMARK's potential as a preferred method in practical applications requiring efficient and resilient text watermarking.





\section{Conclusion}

In this work, we explore how to assign a segment to each bit of the watermark for embedding and design DERMARK, which introduces negligible additional overhead while meeting the dynamic embedding requirements and ensuring robustness against text editing.
Comprehensive experiments on watermark capacity, efficiency, and robustness demonstrate that our approach provides a practical solution for multi-bit watermark embedding.
% %在这个工作中，我们探索了如何为水印的每个比特分配一段来嵌入，并设计出了DEMARK，在满足水印动态嵌入的需求时，带来的额外开销可以忽略不计，并实现了对文本编辑的鲁棒。在水印容量、效率和鲁棒性的综合实验表面我们的方法给多比特水印嵌入提供了一个可靠的解决方案。
% % 探究了特定文本的水印容量，并基于水印容量设计了动态分段的多比特水印技术。在相同文本质量的影响下，实现了多比特水印的高效嵌入。我们首先分析了水印嵌入的过程，数学证明了特定token串与水印嵌入成功率的映射关系；然后基于该关系，设计了动态分段的多比特水印技术；在水印提取中，我们设计了一个损失函数，并通过最小化损失函数的方式还原文本分段并提取水印。大量实验证明，我们的工作在相同文本质量的情况下，相比于baseline可以实现更高效的水印嵌入，在面对随机插入和删除攻击时，可以实现更准确的水印信息保留。我们的工作同时还只需要占用极少的计算时间。我希望我们的工作可以实现对LLM的高效保护，并激发后续研究
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}


\end{document}

