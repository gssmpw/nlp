\section{3D Point Cloud Anomaly Detection}
In this section, we provide a systematic mathematical formulation of untrained anomaly detection problem based on 3D point cloud data. The problem definition is as follows:

Consider a 3D point cloud sample \( \mathbf{Y} = \{ \mathbf{Y}_i \in \mathbb{R}^3 \mid i = 1, 2, \ldots, N \} \), where \( N \) denotes the total number of points. The Anomaly Detection task aims to partition \( \mathbf{Y} \) into two disjoint subsets \( \mathbf{Y}_0 \) and \( \mathbf{Y}_1 \), satisfying:
\begin{enumerate}[itemsep=1pt]
    \item \( \mathbf{Y}_0 \cup \mathbf{Y}_1 = \mathbf{Y} \)
    \item \( \mathbf{Y}_0 \cap \mathbf{Y}_1 = \emptyset \)
    \item \( \mathbf{Y}_0 \) represents the reference surface points
    \item \( \mathbf{Y}_1 \) represents the anomaly points
\end{enumerate}
Furthermore, we make the following assumptions regarding the distribution of \( \mathbf{Y}_0 \) and \( \mathbf{Y}_1 \):
\begin{enumerate}[itemsep=1pt]
    \item \( \mathbf{Y}_0 \) is sufficiently dense, which is supported by contemporary 3D scanning technologies.
    \item \( \mathbf{Y}_1 \) exhibits sparse distribution characteristics.
\end{enumerate}
The point cloud anomaly detection problem can be formulated using three complementary modeling frameworks: the Classification Framework, the Decomposition Framework, and the Local Geometry Framework.
\subsection{Classification Framework}
From a classification perspective, the problem is formulated as a binary classification task defined by a mapping function \( c: \mathbf{Y} \rightarrow \{0,1\} \), where:
\begin{equation}
    c(\mathbf{Y}_i) = \begin{cases}
        0 & \text{if } \mathbf{Y}_i \in \mathbf{Y}_0 \text{ (reference surface point)} \\
        1 & \text{if } \mathbf{Y}_i \in \mathbf{Y}_1 \text{ (anomaly point)}
    \end{cases}
\end{equation}

The classification problem is solved through a probabilistic approach. The relationships among variables \(\mathcal{C}\), \(\mathbf{Y}\), and the parameter set \(\mathbf{\Theta}\) can be characterized via the joint likelihood function \(p(\mathbf{Y}, \mathcal{C}|\mathbf{\Theta})\). Here, \(\mathbf{\Theta}\) encompasses parameters associated with the reference surface representation.

The optimal classification set \(\mathcal{C}\) is then obtained by maximizing the joint likelihood:
\begin{equation}
    \mathcal{C} = \arg\max\limits_{\mathcal{C}}p(\mathbf{Y}, \mathcal{C}|\mathbf{\Theta})
\end{equation}
The classification framework is suitable for the object surface where point relationship can be modeled. If it is hard to model the statistical relationship between points and point distributions, the following decomposition framework and local-geometry based framework can be considered. 

\subsection{Decomposition Framework}
The data decomposition frameworks assumes the point cloud \(\mathbf{Y}\) can be represented as a superposition of three components:
\begin{equation}
    \mathbf{Y} = \mathbf{X} + \mathbf{A} + \mathbf{E}
\end{equation}
where:
\begin{itemize}[itemsep=1pt]
    \item \(\mathbf{X}\) denotes the reference surface component, representing the underlying regular geometric structure.
    \item \(\mathbf{A}\) represents the anomaly component, capturing structural deviations from the reference surface.
    \item \(\mathbf{E}\) accounts for measurement noise inherent in the data acquisition process.
\end{itemize}
    
As anomaly usually sparsely exists on the surfaces, we propose corresponding solution methods. For the decomposition-based formulation, we develop a sparse learning approach to recover the anomaly component. The details of these solutions are elaborated below:

Given point cloud data with random measurement noise, we propose a decomposition-based solution. The observed point cloud \(\mathbf{Y}\) is decomposed as:
\begin{equation}
    \mathbf{Y} = \mathbf{X} + \mathbf{A} + \mathbf{E}
\end{equation}
where each component represents:
\begin{itemize}[itemsep=1pt]
    \item \(\mathbf{X} = [\mathbf{X}_1,\ldots,\mathbf{X}_N]^T \in \mathbb{R}^{N \times 3}\): the reference surface points.
    \item \(\mathbf{A} = [\mathbf{a}_1,\ldots,\mathbf{a}_N]^T \in \mathbb{R}^{N \times 3}\): the anomaly component with row-sparsity.
    \item \(\mathbf{E} = [\mathbf{e}_1,\ldots,\mathbf{e}_N]^T \in \mathbb{R}^{N \times 3}\): the measurement noise.
\end{itemize}
To recover these components, we formulate a sparse learning optimization problem:
\begin{equation}
    \min_{\mathbf{A}} J(\mathbf{A}) = L(\mathbf{A}; \mathbf{X}, \mathbf{\Theta}) + \lambda p_s(\mathbf{A})
    \label{eq:optimization}
\end{equation}
where:
\begin{itemize}[itemsep=1pt]
    \item \(L(\mathbf{A}; \mathbf{X}, \mathbf{\Theta})\) is a loss function that quantifies the fitting residuals between \(\mathbf{Y}\) and \(\mathbf{X} + \mathbf{A}\), as well as enforces smoothness constraints on the reference surface parameters \(\mathbf{\Theta}\).
    \item \(p_s(\mathbf{A})\) is a penalty term promoting row-sparsity in \(\mathbf{A}\), and non-zero rows indicate the anomaly locations.
    \item \(\lambda\) is a tuning parameter controlling the sparsity level.
    \item \(\mathbf{\Theta}\) represents the estimated parameters of the reference surface.
\end{itemize}

Decomposition framework is useful for the surfaces where anomaly points can be separated from the normal reference surfaces. In addition, decomposition framework models the object surface as a whole, thereby enabling the accurate anomaly detection.  

% \section{Proposed Solution Methods}

\subsection{Local Geometry Framework}
A classical perspective suggests that anomaly detection can be based on neighborhood information. For any vertex, we can select its k-neighborhood and compute its Point Feature Histograms (PFH). These histograms possess invariance to rigid body transformations. After obtaining the Point Feature Histograms, we can employ machine learning methods to train a classifier that distinguishes between normal and anomalous points, thereby completing the task. 

There are various local-geometry based feature learning methods\cite{Du202X}, and key ideas are utilizing local shape descriptors to extract the geometry feature from the neighbourhood. More details can be refereed to  Du et al. 2025. Due to the local modeling of the object surfaces, local-geometry based methods usually cannot detection anomaly boundary very accurately, so the authors suggest to prioritize the first two frameworks. However, when the assumptions of classification framework and decomposition framework fail, local-geometry based framework  can serve as an alternatve, which is more general in real applications.


\section{Examples}
\subsection{Example of Classification Framework}

The Bayesian network structure presented in \cite{tao2023anomaly} establishes a probabilistic framework for anomaly detection in unstructured 3D point cloud data. This review examines its key components and theoretical foundations.

\textbf{Core Components and Relationships}

The network models three fundamental relationships:

1) Point location ($x_i$) depends on point type ($c_i$):
\begin{equation}
p(x_i | C, X, D) = p(x_i | c_i)
\end{equation}

2) Local smoothness ($d_{ij}$) depends on adjacent point types:
\begin{equation}
d_{ij} = \|K_i-K_j\|_F^2
\end{equation}
where $K_i$ represents the structure-aware tensor of point $i$.

3) Point type inference relies on spatial location and neighborhood smoothness:
\begin{equation}
p(c_i | C, X, D) = p(c_i | x_i, \{d_{ij}, c_j, j \in \mathcal{N}_i\})
\end{equation}

\textbf{Joint Distribution}

The network enables factorization of the joint distribution:
\begin{equation}
p(X, D, C) = \prod_i p(x_i | c_i)p(c_i) \prod_{j \in \mathcal{N}_i} p(d_{ij} | c_i, c_j)
\end{equation}
The structure's validity is established through the Markov property, ensuring that each node's distribution depends only on its Markov blanket. This property validates the conditional independence assumptions and enables efficient probabilistic inference.

\subsection{Example of Decomposition Framework }
Recent studies have proposed various methods for anomaly detection based on decomposition. \cite{tao2025pointsgrade} considered a loss function defined as:
\begin{equation}
\min_{\mathbf{A}}J(\mathbf{A})=L(\mathbf{A};\mathbf{H},\mathbf{Y})+\lambda p_{s}(\mathbf{A})
\end{equation}
In this formulation, the loss function $L(\mathbf{A};\mathbf{H},\mathbf{Y})$ is formulated to quantify the fitting residuals between the observed point cloud $\mathbf{Y}$ and the sum of the reference point cloud $\mathbf{X}$ and the anomalous component $\mathbf{A}$. This quantification effectively measures how closely the combination of $\mathbf{X}$ and $\mathbf{A}$ can approximate $\mathbf{Y}$. Moreover, it enforces the smoothness of $\mathbf{X}$ through the graph-based smoothness metric $\mathbf{H}$.

The matrix $\mathbf{H}$ is derived from Graph Signal Processing (GSP) theories. By representing the reference point cloud $\mathbf{X}$ as a graph, $\mathbf{H}$ is constructed in such a way that $\mathbf{HX}$ can capture the smoothness of $\mathbf{X}$. Geometrically, the $i^{\text{th}}$ row of $\mathbf{HX}$, denoted as $(\mathbf{HX})_i$, can be interpreted as the difference between the $i^{\text{th}}$ point and the convex combination of its neighbors. This interpretation approximates the deviation of the $i^{\text{th}}$ point from the local plane. For a smooth $\mathbf{X}$, the value of $\mathbf{HX}$ is approximately zero. This property is integrated into the loss function to guarantee the smoothness of the reference surface.

The penalty term $p_{s}(\mathbf{A})$ is introduced with the aim of promoting the row-sparsity of $\mathbf{A}$. Given that anomalies are typically sparse, a row-sparse $A$ implies that only a small number of points are identified as anomalies. The authors selected the group LOG penalty, defined as $p_{s}(\mathbf{A})=\sum_{i}\log(\sqrt{\left\|a_{i}\right\|_{2}^{2}+\varepsilon}+\left\|a_{i}\right\|_{2})$, for this purpose, where the group LOG penalty can penalize different predictors more evenly. This characteristic enables a better recovery of the sparse anomalous component $\mathbf{A}$. 


\subsection{Example of Local-Geometry based Framework}
This example is sourced from the work presented in \cite{rusu2009fast}. The computational approach it employs unfolds as follows:

\textbf{Neighborhood Selection}
Given a point $p$ in a 3D point cloud, we first select all its neighbors enclosed within a sphere of radius $r$. Let $k$ denote the number of these neighbors, and we refer to this set as the $k$-neighborhood of $p$.

\textbf{Darboux Frame Construction and Feature Calculation}
For each pair of points $p_i$ and $p_j$ ($i \neq j$) in the $k$-neighborhood of $p$ with their estimated normals $n_i$ and $n_j$ (where $p_i$ is the point with a smaller angle between its associated normal and the line connecting the points), we define a Darboux $uvn$ frame:
\begin{align}
\mathbf{u} &= \mathbf{n}_i \\
\mathbf{v} &= (\mathbf{p}_j - \mathbf{p}_i) \times \mathbf{u} \\
\mathbf{w} &= \mathbf{u} \times \mathbf{v}\\
\alpha &= \mathbf{v} \cdot \mathbf{n}_j \\
\phi &= \frac{\mathbf{u} \cdot (\mathbf{p}_j - \mathbf{p}_i)}{\|\mathbf{p}_j - \mathbf{p}_i\|} \\
\theta &= \arctan(\mathbf{w} \cdot \mathbf{n}_j, \mathbf{u} \cdot \mathbf{n}_j)
\end{align}
Previous research sometimes included a fourth featureâ€”the Euclidean distance from $p_i$ to $p_j$. However, recent experiments have demonstrated that its omission does not significantly impact robustness in certain cases, particularly in 2.5D datasets.

\textbf{Histogram Construction}
After computing these features for all point pairs in the $k$-neighborhood of $p$, we quantize these feature values and bin them into a histogram. The resulting histogram constitutes the Point Feature Histogram (PFH) at point $p$, characterizing the local geometric properties in its vicinity.

\textbf{Simplified Point Feature Histogram (SPFH) Calculation}
For each query point $p$, we initially compute only the relationships between itself and its neighbors, termed the Simplified Point Feature Histogram (SPFH).

\textbf{FPFH Computation from SPFH}
Subsequently, for each point $p$, we re-determine its $k$ neighbors. The Fast Point Feature Histogram (FPFH) at point $p$ is computed as:

\begin{equation}
FPFH(p) = SPF(p) + \frac{1}{k}\sum_{i=1}^{k}\frac{1}{\omega_k} \cdot SPF(p_k)
\end{equation}

\noindent
where $\omega_k$ represents the distance between query point $p$ and a neighbor point $p_k$ in a given metric space. This formulation combines the SPFH of point $p$ with a weighted sum of its neighbors' SPFHs, achieving reduced computational complexity compared to PFH while maintaining most of its discriminative power.

\section{Conclusion}
\label{sec:conclusion}

This paper systematically investigates untrained machine learning methods for anomaly detection in 3D point cloud data, addressing critical challenges in scenarios with limited training samples. The significance of untrained methods lies in their ability to operate without pre-trained models or extensive anomaly-free datasets, making them particularly valuable for emerging applications such as personalized manufacturing and personalized medicine where data scarcity is a fundamental constraint. Unlike traditional training-based unsupervised anomaly detection approaches that require substantial computational resources and labeled data, untrained methods achieve remarkable adaptability through the integration of geometric priors and sparse learning principles.

We formally defined the untrained anomaly detection problem through  mathematical formulations, establishing three complementary frameworks to address different application scenarios. The \textit{Classification Framework} provides a probabilistic approach to distinguish anomalies from normal surfaces by maximizing joint likelihood functions. The \textit{Decomposition Framework} separates point clouds into reference surface, anomaly, and noise components through sparse optimization, enabling precise anomaly localization. The \textit{Local Geometry Framework} leverages neighborhood geometric features for anomaly identification when prior knowledge about surface structure is insufficient. These frameworks collectively address key challenges including high-dimensional data representation, single-sample learning, and sparse anomaly detection.

Three concrete implementations demonstrate the practical effectiveness of these frameworks: 1) The decomposition-based approach by \cite{tao2025pointsgrade} achieves sparse anomaly recovery through graph-based smoothness constraints and group LOG penalties; 2) The Bayesian classification framework in \cite{tao2023anomaly} establishes probabilistic relationships between point locations and neighborhood smoothness; 3) The Fast Point Feature Histogram method \cite{rusu2009fast} exemplifies local geometry analysis through Darboux frame construction and feature histogram computation. These examples validate the frameworks' capabilities in handling diverse anomaly types while maintaining computational efficiency.

The positions presented in this paper advance the understanding of untrained anomaly detection mechanisms. By eliminating dependency on training data and predefined feature extractors, the proposed methodologies provide a foundation for developing robust anomaly detection systems in data-scarce environments. Future research should focus on enhancing computational efficiency for real-time applications, improving detection accuracy for sub-surface anomalies, and extending these frameworks to multi-modal data integration scenarios.


\section{Outlook}
\label{sec:outlook}

Future research in untrained anomaly detection should focus on integrating prior knowledge of defect characteristics into detection frameworks. A promising direction lies in treating anomaly detection as an inverse problem, where the goal shifts from merely identifying anomalies to jointly reconstructing normal surfaces and inferring potential defect distributions. If prior information about defect patterns (e.g., typical shapes, spatial frequencies, or material-specific manifestations) can be systematically incorporated, such inverse problem formulations could enable more precise detection while reducing false positives.

Three key directions emerge for advancing this paradigm:
\begin{enumerate}[itemsep=1pt]
    \item Developing defect pattern libraries through physical modeling and industrial data collaboration.
    \item Creating adaptive algorithms that balance prior knowledge and data-driven observations.
    \item Exploring hybrid approaches that combine inverse problem methodologies with emerging deep learning architectures.
\end{enumerate}
This perspective aligns with industrial needs for explainable detection systems and lays the foundation for next-generation quality control solutions. While challenges remain in computational efficiency and theoretical guarantees, the integration of physical knowledge with machine learning principles offers a viable path toward more robust and generalizable anomaly detection systems.

