We also focused on poisoning the DC3 method from Donti et al in \cite{donti2021dc3}. This paper proposes a method to incorporate
feasibility constraints in deep learning in order to make
the process more usable for problems with real physical constraints like OPF, called Deep Constraint Completion and Correction (DC3). In \cite{donti2021dc3}, authors first solve the necessary equality constraints, then infer the remaining ones. To this end, they will
use deep NN to find a partial solution and later infer the remaining solutions using equality constraints. The resulting solution is then moved to the region that also
satisfies the inequality constraints (the feasible region) by taking gradient descent steps along the equality-satisfying region,
and the loss is back-propagated and training continues. As this
paper mentions, convergence to the optima is not guaranteed
in the gradient descent portion, but during test, the proposed
solution should be close to the actual solution. 

We expect a trade off between being close to the optimal value and being within the feasible region due to the walk into the feasible region potentially taking the solution further away from the optimal. The poisoning attack would exploit this inherent weakness by either causing a feasibility violation or keeping the solution far away from the true optima (see Fig. \ref{dc3fig}). In \cite{donti2021dc3}, the number of correction steps for convex problems is selected as ten to balance time, optimality, and feasibility. 

\begin{figure}[ht]
\centerline{\includegraphics[scale=0.059]{DC3_Attack.png}}
\vspace{-.2cm}
\caption{Poisoning the workflow of the DC3 method.}
\label{dc3fig}
\end{figure}
