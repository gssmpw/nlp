
Poisoning attacks generally adopt one of two strategies. In a black box attack, the attacker has limited knowledge about NN and its data, relying mostly on estimations to inflict damage. Conversely, a white box attack assumes that an attacker has access to considerable details of the grid network and its data. In our study, we employ a white box attack approach, where the attacker is well-informed about the dataset and has a rough idea of the network's architecture. This enables us to demonstrate the significant impact on the optimal solutions produced by the method. Our approach modifies the strategy outlined by Chen et al. in their study of evasion attacks on demand prediction \cite{evasion2019demand}. We specifically engineer an attack to maximize the loss, misleading the neural network into predicting higher generation needs than required. This leads to an unnecessary increase in energy production, thus disrupting the supply-demand equilibrium.

In what follows, we will outline our methods of poisoning attacks.
%and explain our development of the attack function, {\color{red}which demonstrates our pre-existing knowledge of grid operations}. 
Then, we will detail the implementation of poisoning attacks on three different OPF proxies: the penalty method \cite{Liu2022Pen}, the DC3 method \cite{Li_2023}, and the \LOOPLC~ method \cite{donti2021dc3}.

% Poisoning attacks have two general strategies. A black box attack is where the attacker has very little knowledge of the NN and data and estimates how to damage it properly. In a white box attack, the attacker has some amount of knowledge of the network/data. In our method, we went with a white box attack where the attacker has knowledge of the data set as well as an estimate of what the network looks like in order to show the impact to the optimal solution generated by the method. Our method was modified from Chen et al.'s work on evasion attacks on demand prediction in \cite{evasion2019demand}. We focused on an attack that maximized the loss by causing the neural network to predict that there needs to be more generation than is actually true. This will cause an imbalance of supply and demand and cause for there to be wasted generation when generators are told to produce more energy than needed. 


% To implement the attack, we used the neural network architecture and the loss function to add perturbations to the training data by calculating the gradient of the loss function, with respect to the input data by using the direction to move each input point to maximize loss. We then add or subtract a set perturbation amount to each data point in order to maximize loss while ensuring that the amount the input points are moved is within a certain bound. This is done in order to avoid detection from those working to protect themselves from adversaries. For our loss function, we use mean squared error (MSE). We find the loss by finding the MSE between a very large valued target and the unattacked optimal generation prediction, in order to achieve the generation maximization attack. The adversarial maximization problem is shown in \eqref{pois}, which is modified from \cite{bai2021recent}. 


