% The penalty method is one of the classical ways to address feasibility violations in ML training. It involves bringing in constraints into the loss function and creating a loss that increases as the neural network results approach violation of the constraints. This incentivizes the network to keep solutions within the feasible region. 

% For the penalty method, since it is entirely NN based, we simply perturbed the input training data to this NN. This implementation is shown in Fig. \ref{penfig}. Adversarial perturbations will exploit the fact that feasibility constraints are not absolutely required to be enforced potentially causing constraint violations in addition to moving the value away from the true optima.

The penalty method is a well-established approach used to manage feasibility violations in ML training. It incorporates constraints directly into the loss function, which is designed to increase as the NN outputs approach the boundaries of these constraints. This design incentivizes the network to produce solutions that remain within the feasible region.

Our approach for introducing perturbations into the penalty method framework is shown in Fig. \ref{penfig}. By exploiting the inherent flexibility of penalty-based methods in enforcing feasibility constraints, adversarial perturbations can cause constraint violations and push solutions further from the optima.

\begin{figure}[h!]
\centerline{\includegraphics[scale=0.085]{Penalty_Attack.png}}
\vspace{-.2cm}
\caption{Poisoning the workflow of Penalty methods. 
%diagram showing where in the pipeline the poisoning attack takes place.
}
\label{penfig}
\end{figure}