

Extensive research has been devoted to developing optimization proxies that expedite the resolution of the OPF problem while ensuring the feasibility of decisions. A commonly employed method is the incorporation of penalty terms aimed at minimizing constraint violations \cite{Liu2022Pen, pan2022deepopf, liu2024teaching}. Although this approach is applicable across a variety of power system optimization problems, it merely penalizes rather than eliminating violations. Alternatively, some methods introduce a repair module that utilizes iterative algorithms to adjust the solutions. These methods range from using commercial solvers to identify the closest feasible solution \cite{zhao2020deepopf}, to optimizing a function that minimizes violations \cite{donti2021dc3}.  Further advancements involve differentiable mapping functions that directly enforce constraint satisfaction \cite{ Li_2023, chen2023end}. For instance, in our previous work \cite{li2023learning}, we introduced an iteration-free optimization proxy that ensures the hard feasibility of solutions through a single feed-forward process. However, research into assessing these methods' performance, such as optimality and feasibility, particularly under adversarial conditions, remains sparse.


% For power systems optimization proxies, a focus on enforcing feasibility constraints is even more pervasive in the literature \cite{Liu2022Pen, donti2021dc3, Li_2023, Gao2024Physics}. This includes adding a penalty in the ML laws functions for violating these constraints or mixing together ML with traditional mathematical solvers. These methods have shown great promise in using ML to generate optimal feasible solutions. However, there is little work done in showing these methods' resilience to adversarial attacks.

% Adversarial attacks, which involve introducing small perturbations to input data to maximize disruption in the output of optimization proxies, specifically within the power system context. Adversarial attacks are categorized into two main types: evasion and poisoning attacks.  Evasion attacks occur during the run/test time of the neural network, where the perturbation is added to input data, compromising the output by fitting the wrong network to the data. These attacks have been extensively studied in their application to the power system. In contrast, poisoning occurs during the training phase, where perturbations are added to the training data, thereby affecting the neural networks' weights. Although less studied, due in part to the assumption that training data is more secure than data used in runtime\cite{agah2024datapois}, they represent a significant threat. Evasion attacks only affect one round of outputs, the outputs coming directly from inputting the perturbed data. Poisoning attacks skew the network structure so that, until there is retraining, every round of outputs are affected. This is a very large impact, and needs to be studied. There are vulnerabilities in the power system that can lead to poisoning attacks. Notably, many OPF optimization proxies rely on MATPOWER data transferred from MATLAB to Python for training \cite{zhao2020deepopf, donti2021dc3, Li_2023}, creating a point of vulnerability where attacks could corrupt the data. Therefore, investigating data poisoning is vital for evaluating the reliability of ML-based optimization proxies.

Adversarial attacks in the power system context typically involve introducing limited perturbations to input data, aiming to maximize disruption in the output of optimization proxies. These attacks are categorized into two primary types: evasion and poisoning attacks. Evasion attacks occur during the operational or test phase of the NN, where perturbations in the input data compromise the output by misleading the neural network. Such attacks have been extensively studied within power systems \cite{Wang2024Evasion}. In contrast, poisoning attacks happen during the training phase, where perturbations to the training data alter the NN's weights, potentially causing long-term disruptions. Poisoning attacks are less explored, partly due to the assumption that training data is more secure than operational data \cite{agah2024datapois}. However, poisoning attacks, unlike evasion attacks that affect only a single output session, can distort the structure of the neural network until retraining occurs, thus impacting all subsequent outputs significantly \cite{abbasi2023brainwash}.

The extensive impact of data poisoning attacks has also led to research on a variety of methods to strengthen the robustness of NNs against these attacks \cite{CompSurvey}. This research includes methods for detecting poisoned data \cite{DetectPois}, or training the network to be more resilient to poisoned data \cite{inbook}. Understanding data poisoning attacks in relation to the power system will allow these robustness strategies to be applied effectively, helping strengthen security of ML methods used in grid operation.

In this paper, we focus on today's common training practices and shed light on their data poisoning vulnerabilities.
Specifically given that many OPF optimization proxies utilize MATPOWER data transferred from MATLAB to Python for training \cite{zhao2020deepopf, donti2021dc3, Li_2023}, there exists a notable intrusion vulnerability. 
%to poisoning at this data transfer point. 
Thus, a thorough study of data poisoning is crucial for assessing the reliability of ML-based optimization proxies.





% {\color{blue} the flow will be easier to follow if: 1. what is Adversarial attacks(your sentence"Adversarial attacks involve adding a small perturbation to input data in order to maximize the loss of the neural network"), 2. Adversarial attacks categories(your sentence "Adversarial attacks fall under two main categories: Evasion attacks and Poisoning attacks"). 3. explain Evasion attacks(your sentence "Evasion attacks are when that perturbation is added to the input data during the run/test time of the NN. This affects the output by exploiting the fact that networks are not perfectly fitted to the data." "Evasion attacks are heavily studied in their application to the power system", 4. explain Poisoning attacks(its difference from  Evasion attacks, its features: your sentence "assumption that a networks data is more secure during training then during runtime \cite{agah2024datapois}.", its importance and research gap: your sentence"Poisoning on the other hand has been studied,", "many of the OPF optimization proxies use MATPOWER data for training, then send that data to matlab from python creating a vulnerability point for attacks to exploit and corrupt the MATPOWER data. Therefore, data poisoning is an important attack method to explore in relation to power grid ML based optimizer reliability.")}

% Adversarial attacks fall under two main categories: Evasion attacks and Poisoning attacks[]. Adversarial attacks involve adding a small perturbation to input data in order to maximize the loss of the neural network. Evasion attacks are when that perturbation is added to the input data during the run/test time of the NN. This affects the output by exploiting the fact that networks are not perfectly fitted to the data. Poisoning attacks are when the perturbation is added to the input data when the network is training affecting the weights of the network during the training process. Evasion attacks are heavily studied in their application to the power system[]. Poisoning on the other hand has been studied, but slightly less so in part due to the assumption that a networks data is more secure during training then during runtime \cite{agah2024datapois}. However, many of the OPF optimization proxies use MATPOWER data for training, then send that data to matlab from python creating a vulnerability point for attacks to exploit and corrupt the MATPOWER data. Therefore, data poisoning is an important attack method to explore in relation to power grid ML based optimizer reliability.
