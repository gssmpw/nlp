Data poisoning attacks are meticulously crafted to manipulate the training data of ML-based optimization proxies. These proxies are designed to map load demands, the inputs, to their respective optimal generation profiles, the outputs. By subtly introducing calculated perturbations into the load demand data, attackers aim to skew the proxy's ability to predict the most efficient (optimal and feasible) power output setting.

The perturbations are guided by the gradient of a Mean Squared Error (MSE) loss function, which measures the deviation between the perturbed (attacked) generation targets and the proxy's predictions. The objective is to manipulate the model into recommending higher than necessary energy outputs, maximizing the loss \cite{bai2021recent}:

\small
\begin{equation}
\texttt{max}_\delta \; L(\theta, x + \delta, y)\label{pois}
\end{equation}
\normalsize

In \eqref{pois}, $L$ is the loss function, $\delta$ is the perturbation, $\theta$ is the model parameters, $x$ is the model input, and $y$ is the output.

A bounding mechanism is applied to increase the likelihood that these perturbations evade detection by protective measures. This mechanism restricts the degree of perturbation applied to each data point while showing {\color{black}pre-existing knowledge of
grid operation (shown by $x_{\text{orig}}$)}:

% The bound is used to avoid detection from defenders against attacks. The bounding clips an adversarial data if the perturbation added to the original input point varies a certain proportional amount from the original input point. This clipping is seen in \eqref{clip}, where $x_{\text{new}}$ is the adversarial data point and $x_{\text{orig}}$, is the original, unperturbed data.

\vspace{-.4cm}

\small
\begin{equation}
x_{\text{new, bounded}} = \texttt{clip}\left(x_{\text{new}}, \, x_{\text{orig}} - \text{L} \cdot |x_{\text{orig}}|, \, x_{\text{orig}} + \text{L} \cdot |x_{\text{orig}}|\right) \label{clip}
\end{equation}
\normalsize

\vspace{.2cm}

The functionality of the clip function is summarized below:
\small
\begin{equation}
\texttt{clip}(x, a, b) = 
\begin{cases}
x = a, & \text{if } x < a \\
x = x, & \text{if } a \leq x \leq b \\
x = b, & \text{if } x > b
\end{cases}
\end{equation}
\normalsize

\noindent where $L$ is the bound that we select to avoid detection, $x_\text{new}$ is the new, perturbed input data that has been attacked. Also, $x_\text{orig}$ is the unattacked, original input data, and $x_\text{new, bounded}$ is the new, attacked data once the bound has been applied. This approach strategically enforces limits on the alterations to ensure the changes do not attract undue attention, while still achieving the intended disruptions to model training. This subtle yet strategic manipulation leads to optimality and feasibility gaps in the grid operation once the model is deployed, demonstrating the critical need for robust defenses against such adversarial threats in the training of ML-based optimization proxies. In the following proxy methods, we perturb the input data at the same location in the data pipeline, however, the way in which that perturbed data will affect the algorithm is different for each case. 