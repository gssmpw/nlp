
The impact of poisoning attacks on neural power system optimizers has been understudied compared to that of evasion attacks. Through this work, we have focused on this gap by implementing poisoning attacks on the latest ML--based optimization proxies focused on the Optimal Power Flow problem. We measured the adverse impact of data poisoning attacks on the optimality and feasibility of the solutions output by ML-based OPF solvers. We have also identified the impacts of different ML-based solver structures and their resilience to poisoning attacks.

We aim to expand this work on multiple fronts. First, we plan to implement poisoning attacks for AC versions of the different ML--based methods. It would also be beneficial to determine the impact of the Poisoning Attack on other methods with different structures or even with some adversarial samples used in training. Additionally, we will implement classical methods to improve the robustness of NN against adversarial attacks, such as adversarial training. %This can help explore ways to make these methods more secure. The understanding from this work and potential future work make it easier to build resilient ML in the future for use in power systems operation. 