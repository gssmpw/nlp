In this paper, we investigate the impact of data poisoning attacks on the optimality and feasibility of ML-based optimization proxies applied to solving the DC-OPF problem.
%, an area that is not sufficiently explored. 
We conduct tests on three distinct types of ML-based optimization proxies: the penalty method \cite{Liu2022Pen}, the DC3 method \cite{Li_2023} (a post-repair approach), and the \LOOPLC~ method \cite{donti2021dc3} (a direct mapping approach). Our research aims to develop more robust ML methods that can withstand adversarial threats, thereby enhancing both the reliability and security of power system operations. In addition, our findings provide valuable insights for power system operators and can facilitate the adoption of neural-based optimizers. This paper is organized as follows: Section \ref{PF} describes the problem formulation for the DC-OPF problem, Section \ref{PA} details our implementation of the data poisoning attack, Section \ref{Res} discusses the results of the attack simulation, and finally Section \ref{conc} provides our conclusion and potential extensions of this work.


% In this paper, we analyze the impact of data poisoning attacks on ML-based optimization proxies in optimality and feasibility applied to ACOPF, which we feel is currently understudied. We test the data poisoning attack implementation on three types of ML-based optimization proxies: penalty method \cite{Liu2022Pen}, DC3 method \cite{Li_2023} (a post-repair method) and \LOOPLC~ method \cite{donti2021dc3} (a direct mapping method). We aim to provide potential solutions for more robust ML methods and safeguard these ML models against adversarial threats, ultimately enhancing the reliability and security of power system operations.
% This paper is organized as follows: Setion II will discuss OPF and the ML-based solvers we used, Section III will give our data poisoning attack implementation method, Section IV will give our experiment set up and results, and Section V will give our concluding thoughts and our suggestions for future work such as work to improve robustness.



% There has been difficulty with ML enforcing the feasibility constraints. This includes adding a penalty in the ML laws functions for violating these constraints or mixing together ML with traditional mathematical solvers. These methods have shown great promise in using ML to generate optimal feasible solutions. One thing that these methods have in common, however, is that many of them use MATPOWER in order to generate training data for the Neural Networks(NN). This consistent use of MATPOWER leaves a vulnerability, in that if this data is corrupted, it will negatively affect these methods. The result of this corrupt data would be inaccurate or infeasible predictions by the network which could lead to wasted money if the generators overproduce, blackouts if the generators under produce, and even transmission lines blowing if the network outputs an infeasible solution.
% The popularity of machine learning has lead to a lot of research in the potential negative impacts of modifying training data with small perturbations, known as data poisoning attacks. They have been extensively studied as a threat to the security of NN's, however, are understudied in the applications of OPF solving ML networks. In this paper, we seek to explore the impact of data poisoning attacks on optimization methods using ML with an emphasis on obeying feasibility. 