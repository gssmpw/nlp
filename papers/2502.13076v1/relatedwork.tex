\section{Literature Review}
\label{s:review}
In this section, we review the related research and discuss the differences between previous works with ours.
\subsection{Keyphrase Extraction and Generation} 
% Existing methods for KG can be categorized into extractive, generative, and integrated ones. 
Methods to obtain keyphrases from documents  can be categorized into extractive and generative paradigms.
Early work focuses on the extractive task \cite{kpe2020,kpeplm}. While extractive methods have salient performance in extracting present keyphrases \cite{kpe2020,kpeplm}, they are incapable of dealing with absent ones which require a comprehensive document understanding \cite{unikeyphrase, promptkp}. Generative methods are designed to tackle this issue by predicting both present and absent keyphrases \cite{kp20k, one2seq, kpgplm}. Generally, these methods fall into three main paradigms: One2One, One2Seq, and One2Set. 
One2One treats each target keyphrase coupled with the document as an independent instance and uses a sequence-to-sequence generative model with a copy mechanism for predicting both present and absent keyphrases \cite{kp20k}. However, One2One can only predict a fixed number of keyphrases. To be more flexible, \cite{one2seq} proposes an One2Seq paradigm, where keyphrases are reordered by a predefined rule, and then concatenated as the input. However, since keyphrases in a document are usually unsorted, imposing a predefined order can potentially introduce unnecessary biases, complicate the training process, and increase sensitivity to the order of keyphrases \cite{one2set, wrone2set}. Addressing these issues, \cite{one2set} proposes a novel One2Set paradigm considering keyphrases as a set instead of a sequence. 
Despite its impressive performance, One2Set still has limitations including overestimation of null ($\varnothing$) tokens that signifies \textit{no corresponding keyphrase}. To solve this problem, \cite{wrone2set} calibrates One2Set with an adaptive instance-level cost weighting strategy and a target re-assignment mechanism. But it demands intricate design and extra elaboration. 

Furthermore, purely generative methods often overlook the semantic relationships between keyphrases and source documents, potentially leading to irrelevant predictions. To constrain generation, \cite{segnet, unikeyphrase} introduce an extractor-generator framework, which leverages present information to guide generation. \cite{promptkp} utilizes keywords to construct prompt templates in a prefix language model. However, these method lacks the capacity for parallel generation due to its reliance on predicting masked tokens, which cannot be expanded to parallel generation. In this work, we also leverage keywords as semantic guides. By adopting the One2Set paradigm, our method enables the parallel generation of keyphrases, making it more efficient than sequential approaches. By directly integrating keywords into the decoder's input, our method achieves more direct and stable constrained generation.

More recently, large language models (LLMs) have been applied to KG tasks. \cite{GPT_keyphrase} and \cite{GPT_survey} demonstrate that ChatGPT can be used for KG, achieving performance comparable to state-of-the-art methods with proper prompt settings. \cite{keyphrase_survey} also validates that ChatGPT outperforms unsupervised keyphrase extraction methods under a zero-shot setting. However, the inference cost of LLMs is high, making it prohibitive to deploy LLMs for KG in large-scale real-world scenarios where the volume of documents is immense or the documents are extremely lengthy, such as in patent analysis. Additionally, \cite{keyphrase_survey} shows that while increasing  parameters of PLMs can be effective, its impact is often overshadowed by the adoption of new training and inference paradigms. Our research addresses this gap by integrating PLMs into the novel One2Set training paradigm. This approach balances enhancing performance through prior knowledge with the implementation costs associated with increasing model parameters.

% Raffel et al. (2019) explored the ability of prompt models to perform generation tasks such as text summarization and machine translation using prompts. Brown et al. (2020) introduce in-context learning for text generation, creating a prompt with manual templates. Schick and Sch¨utze (2020) focus on ﬁxed-prompt LM tuning for few-shot text summarization with manually crafted templates. Li and Liang (2021) investigate ﬁxedprompt LM tuning for text summarization and data-to-text generation in few-shot settings. Dou et al. (2021) employ the prompt-based LM tuning strategy on the text summarization task. However, the prompt-based KG method hasn’t been explored. Besides, none of the above work has systematically investigated the potential that keyphrases can brought to a practical application scenario, such as patent analysis.

\subsection{Patent Analysis based on Deep Learning}
Based on deep learning methods for natural language processing, novel approaches have been developed in the field of patent analysis, including patent classification \cite{patent_bert}, patent retrieval \cite{patent_retrieval}, technology forecasting \cite{patent_tkde}, and information extraction and text generation \cite{patent_generation}. For example, \cite{patent_bert} fine-tunes a pre-trained BERT model for patent classification, while \cite{patent_tkde} proposes an event-based framework for patent application trend prediction using graph learning. However, most existing works focus on specific analytical techniques for patent analysis. The development of integrated analysis frameworks based on deep learning methods remains largely unexplored.

% \subsection{Document Representation of Patents}
Constructing the document representation of patents is the prerequisite of analytical tasks.
\cite{copate} proposes a contrastive learning method to build patent embeddings for analysis. However, the construction of patent embeddings involves semantic compression by simply separating lengthy claims into different parts, thereby neglecting the multi-level features of patents and failing to fully utilize such structural information. Additionally, this document representation is purely numerical and too abstract for experts to explain.
\cite{keyword_patent} extracts keywords from patents to enchance patent classification. \cite{tpe} proposes to extract phrases related to techniques, termed technical phrases, to represent patents. \cite{techpat} constructs a hierarchical framework for patent analysis based on technical phrases. However, these works focus solely on extracting information from words and phrases and do not account for latent information beyond existing texts.
\cite{ai_patent} proposes a novel patent analysis framework based on a language model. However, it primarily focuses on extracting key sentences and phrases from patents, while neglecting extensions to practical downstream patent analysis tasks. Additionally, it requires extra post-training on patents within specific domains, which involves labor-intensive dataset construction and incurs significant costs when transferring to patents across different domains.
Our work is the first to integrate patent structures with both present and absent information in keyphrases, offering a comprehensive representation for patent analysis.