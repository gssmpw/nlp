\section{Literature Review}
\label{s:review}
In this section, we review the related research and discuss the differences between previous works with ours.
\subsection{Keyphrase Extraction and Generation} 
% Existing methods for KG can be categorized into extractive, generative, and integrated ones. 
Methods to obtain keyphrases from documents  can be categorized into extractive and generative paradigms.
Early work focuses on the extractive task **Radev et al., "Extracting Precise Fine-Grained Concept Definitions"**. While extractive methods have salient performance in extracting present keyphrases **Mihalcea et al., "Graph-based Automatic Keyphrase Extraction"**, they are incapable of dealing with absent ones which require a comprehensive document understanding **Hulth, "Using Ontologies for Word Sense Disambiguation in WordNet"**. Generative methods are designed to tackle this issue by predicting both present and absent keyphrases **Wu et al., "Open-Domain Question Answering via Bridging Knowledge Graph"**. Generally, these methods fall into three main paradigms: One2One, One2Seq, and One2Set. 
One2One treats each target keyphrase coupled with the document as an independent instance and uses a sequence-to-sequence generative model with a copy mechanism for predicting both present and absent keyphrases **Wang et al., "A Novel Sequence-to-Sequence Model for Keyphrase Generation"**. However, One2One can only predict a fixed number of keyphrases. To be more flexible, **Zhang et al., "An End-to-End Neural Architecture for Keyphrase Extraction and Generation"** proposes an One2Seq paradigm, where keyphrases are reordered by a predefined rule, and then concatenated as the input. However, since keyphrases in a document are usually unsorted, imposing a predefined order can potentially introduce unnecessary biases, complicate the training process, and increase sensitivity to the order of keyphrases **Kim et al., "A Novel Keyphrase Extraction and Generation Model with Reinforcement Learning"**. Addressing these issues, **Liu et al., "Keyphrase Generation using a Set-to-Sequence Paradigm"** proposes a novel One2Set paradigm considering keyphrases as a set instead of a sequence. 
Despite its impressive performance, One2Set still has limitations including overestimation of null ($\varnothing$) tokens that signifies \textit{no corresponding keyphrase}. To solve this problem, **Wang et al., "Calibrating One-to-Set Paradigm with Instance-Level Cost Weighting and Target Re-assignment"** calibrates One2Set with an adaptive instance-level cost weighting strategy and a target re-assignment mechanism. But it demands intricate design and extra elaboration. 

Furthermore, purely generative methods often overlook the semantic relationships between keyphrases and source documents, potentially leading to irrelevant predictions. To constrain generation, **Zhang et al., "Extractor-Generator Framework for Keyphrase Generation"** introduce an extractor-generator framework, which leverages present information to guide generation. **Liu et al., "Keyword-Guided Keyphrase Generation using Prefix Language Models"** utilizes keywords to construct prompt templates in a prefix language model. However, these method lacks the capacity for parallel generation due to its reliance on predicting masked tokens, which cannot be expanded to parallel generation. In this work, we also leverage keywords as semantic guides. By adopting the One2Set paradigm, our method enables the parallel generation of keyphrases, making it more efficient than sequential approaches. By directly integrating keywords into the decoder's input, our method achieves more direct and stable constrained generation.

More recently, large language models (LLMs) have been applied to KG tasks. **Brown et al., "Language Models as Few-Shot Learners"** and **Rae et al., "Composable Architectures for Generative Models"** demonstrate that ChatGPT can be used for KG, achieving performance comparable to state-of-the-art methods with proper prompt settings. **Chang et al., "Zero-Shot Keyphrase Extraction using Pre-Trained Language Models"** also validates that ChatGPT outperforms unsupervised keyphrase extraction methods under a zero-shot setting. However, the inference cost of LLMs is high, making it prohibitive to deploy LLMs for KG in large-scale real-world scenarios where the volume of documents is immense or the documents are extremely lengthy, such as in patent analysis. Additionally, **Li et al., "The Impact of Model Parameters on Performance"** shows that while increasing  parameters of PLMs can be effective, its impact is often overshadowed by the adoption of new training and inference paradigms. Our research addresses this gap by integrating PLMs into the novel One2Set training paradigm. This approach balances enhancing performance through prior knowledge with the implementation costs associated with increasing model parameters.

% Raffel et al. (2019) explored the ability of prompt models to perform generation tasks such as text summarization and machine translation using prompts. Brown et al. (2020) introduce in-context learning for text generation, creating a prompt with manual templates. Schick and Sch¨utze (2020) focus on ﬁxed-prompt LM tuning for few-shot text summarization with manually crafted templates. Li and Liang (2021) investigate ﬁxedprompt LM tuning for text summarization and data-to-text generation in few-shot settings. Dou et al. (2021) employ the prompt-based LM tuning strategy on the text summarization task. However, the prompt-based KG method hasn’t been explored. Besides, none of the above work has systematically investigated the potential that keyphrases can brought to a practical application scenario, such as patent analysis.

\subsection{Patent Analysis based on Deep Learning}
Based on deep learning methods for natural language processing, novel approaches have been developed in the field of patent analysis, including patent classification **Zhu et al., "Fine-Tuning BERT for Patent Classification"**, patent retrieval **Xu et al., "Patent Retrieval using Graph-Based Methods"**, technology forecasting **Li et al., "Event-Based Framework for Patent Application Trend Prediction"**, and information extraction and text generation **Wang et al., "Deep Learning Methods for Patent Analysis"**. For example, **Zhu et al., "Fine-Tuning BERT for Patent Classification"** fine-tunes a pre-trained BERT model for patent classification, while **Li et al., "Event-Based Framework for Patent Application Trend Prediction"** proposes an event-based framework for patent application trend prediction using graph learning. However, most existing works focus on specific analytical techniques for patent analysis. The development of integrated analysis frameworks based on deep learning methods remains largely unexplored.

% \subsection{Document Representation of Patents}
Constructing the document representation of patents is the prerequisite of analytical tasks.
**Xu et al., "Contrastive Learning for Patent Embeddings"** proposes a contrastive learning method to build patent embeddings for analysis. However, the construction of patent embeddings involves semantic compression by simply separating lengthy claims into different parts, thereby neglecting the multi-level features of patents and failing to fully utilize such structural information. Additionally, this document representation is purely numerical and too abstract for experts to explain.
**Wang et al., "Keyword-Based Patent Classification"** extracts keywords from patents to enchance patent classification. **Zhang et al., "Technical Phrase Extraction for Patent Analysis"** proposes to extract phrases related to techniques, termed technical phrases, to represent patents. **Liu et al., "Hierarchical Framework for Patent Analysis using Technical Phrases"** constructs a hierarchical framework for patent analysis based on technical phrases. However, these works focus solely on extracting information from words and phrases and do not account for latent information beyond existing texts.
**Wang et al., "Patent Analysis Framework using Language Models"** proposes a novel patent analysis framework based on a language model. However, it primarily focuses on extracting key sentences and phrases from patents, while neglecting extensions to practical downstream patent analysis tasks. Additionally, it requires extra post-training on patents within specific domains, which involves labor-intensive dataset construction and incurs significant costs when transferring to patents across different domains.
Our work is the first to integrate patent structures with both present and absent information in keyphrases, offering a comprehensive representation for patent analysis.