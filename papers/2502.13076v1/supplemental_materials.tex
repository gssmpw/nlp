% \documentclass[journal]{IEEEtran}
\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{soul}
\usepackage{color}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{document}

\title{Supplemental Materials of Paper: KAPPA: A Generic Patent Analysis Framework with Keyphrase-Based Document Portraits}
% \author{Xin Xia, Yujin Wang, Jun Zhou, Guisheng Zhong, Chen Zhang
% \thanks{Xin Xia, Yujin Wang and Chen Zhang are with the Department of Industrial Engineering at Tsinghua University, Beijing, China. }% <-this % stops a space
% \thanks{Jun Zhou and Guisheng Zhong are with China Intellectual Property Society, Beijing, China. }
% \thanks{Corresponding author: Chen Zhang.}}
% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
% \markboth{IEEE Transactions on Knowledge and Data Engineering}%
% {Xin Xia, Yujin Wang, Jun Zhou, Guisheng Zhong, Chen Zhang \MakeLowercase{\textit{et al.}}: }
\maketitle
\section{Dataset Statistics}
\begin{table}
  \caption{Statistics of Benchmark Datasets for Keyphrase Prediction}
  \label{tab:datasets}
  \scalebox{1}{
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcccr}
    \toprule
    \textbf{Dataset} & \textbf{$\#$Samples} & \textbf{Avg.$\#$KP} & \textbf{Avg.$\mid$KP$\mid$} & \textbf{Pct.AKP}\\
    \midrule
    KP20k & 20,000 & 5.26 & 2.04 & 37.23\\
    Inspec & 500 & 9.79 & 2.48 & 26.42\\
    Krapivin & 400 & 5.83 & 2.21 & 44.33\\
    NUS & 211 & 10.81 & 2.22 & 45.36\\
    SemEval & 100 & 14.43 & 2.38 & 55.61\\
    % PAI & 523 & 6.64 & 2.42 & 26.39\\
  \bottomrule
\end{tabular*}
}
\end{table}

\section{Implementation Details}
To implement KAPPA, we use T5-base as the PLM backbone of SetPLM and inherit most of the settings in \cite{t5} based on PyTorch. We employ a AdamW optimizer commonly used in T5 finetuing instead of Adam in conventional SetTRANS and set the learning rate to 0.0003 with a batch size of 36. 

Let $\hat{Y}=\left(\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_m\right)$ and $Y=\left(y_2, y_2, \ldots, y_n\right)$ to be the predicted and target keyphrases, respectively. The common practice is to use only top $k$ predictions with the highest scores for evaluation, where $k$ is a pre-defined constant. In this work, we use $F_1@5$ and $F_1@M$ as criteria. $F_1@M$ compares all predictions to ground truths, and $F_1@5$ focuses on the top-5 predictions. When there are fewer than five predictions, we adopt a standard practice in \cite{kp20k, unikeyphrase, one2set}, randomly selecting incorrect keyphrases to fill the vacancies, ensuring a length of five. To eliminate the influence of morphology, the predicted keyphrases are stemmed by applying Porter Stemmer. We remove all padding keywords from the predictions before comparison.
\subsection{Patent Classification}
In particular, we modify the output dimension of the final fully connected layer of BERT to match the number of classes and utilized full fine-tuning for the model. 
The dataset is split into training and testing sets with an 8:2 ratio. We adopted AdamW as the optimizer and used CrossEntropyLoss as the loss function.
\subsection{Technology Recognition}
We treat technology recognition as a special classification task, where the class labels are defined by technologies strongly related to a practical case interested by the patent experts. 
Similar to the classifiction task, we adopt the "\textit{bert-base-uncased}" model as the backbone, with AdamW as the optimizer and CrossEntropyLoss as the loss function. 
\subsection{Patent Summarization}
Specifically, we use the GLM4 model \cite{glm2024chatglm}. To ensure a fair comparison, the prompt specifies that the generated abstract should have an average length of 130 words.

\section{Additional Experiments and Analysis}
\textbf{Analysis of Keyphrase Quality. }
To investigate the modelâ€™s ability to generate diverse keyphrases, we measure the average numbers of unique present and absent keyphrases, and the average duplication ratio of all the predicted keyphrases.

\begin{table}[h]
\centering
\caption{Prediction Diversity on In-Domain and Out-Domain Datasets.}
\begin{threeparttable}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}l|ccc|ccc}
\toprule
\multirow{2}{*}{\textbf{Model}}
& \multicolumn{3}{c|}{\textbf{In-Domain}}
& \multicolumn{3}{c}{\textbf{Out-Domain}}\\ 
&{\#PK}
&{\#AK}
&{\#UK}
&{\#PK}
&{\#AK}
&{\#UK}
\\
\midrule
Oracle\tnote{1}&3.31&1.95& -&3.31&1.95& -
\\
WR-One2Set&4.37&2.43&0.10&4.37&2.43&0.10
\\
SetTRANS &5.01&2.74&0.15&5.01&2.74&0.15
\\
SetPLM  &3.94&2.01&0.05&5.01&2.74&0.15
\\
\bottomrule
\end{tabular*}
\begin{tablenotes}
\footnotesize
\item[1]\textit{ORACLE} refers to the average number of target keyphrases. 
\end{tablenotes}
\end{threeparttable}
\label{tab:diversity}
\end{table}
% \bibliography{reference_kappa}
% \bibliographystyle{IEEEtran}
\newpage
\vfill

\end{document}


