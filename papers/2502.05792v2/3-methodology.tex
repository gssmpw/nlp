\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{quantitative_1_2.jpg}
    \vspace{-5mm}
    \caption{Quantitative comparisons for Scenario 1 and Scenario 2. The simulation setup is illustrated on the left. We compare the prediction accuracy using ADE, and the efficiency and safety in resulting robot plans using Detour and Minimum Distance.}
    % AToM achieves the best accuracy and results in improving efficiency while maintaining safety.}
    \vspace{-5mm}
    \label{fig:quantitative_12}
\end{figure*}

\section{Methodology}
\textbf{Problem Definition.} 
Let the state and control of an agent be $\boldsymbol{x} \in \mathds{R}^{n}$ and $\boldsymbol{u} \in \mathds{R}^{m}$, where $n, m$ are state and control dimensions. 
We use subscripts $H, R, HR$ to represent human, robot, and their joint system, and superscript $k$ to represent timesteps.
The dynamics can be described by $\boldsymbol{x}^{k+1} = \Dynamics(\boldsymbol{x}^{k}, \boldsymbol{u}^{k})$.
The trajectories from timestep $i$ to timestep $j$ can be represented by $\boldsymbol{X}^{i:j} = [\boldsymbol{x}^{i}, \dots, \boldsymbol{x}^{j}]$.
In a long-term interaction setting, we assume that the goal positions and environmental obstacles are known as $\boldsymbol{g}_{HR}$ and $\mathcal{O}$, respectively.
At each timestep $t$, given the observed historical trajectories $\boldsymbol{X}_{HR}^{0:t}$ and known information $\boldsymbol{g}_{HR}$ and $\mathcal{O}$, our goal is to predict future human trajectories $\hat{\boldsymbol{X}}_{H}^{t+1:t+T_{f}}$ with prediction horizon $T_{f}$, which the robot can use for collision-free predictive planning. 
For simplicity, we will omit time superscripts when no confusion is aroused. Our method is planner-agnostic and the robot planning algorithm is outside the scope of this work. 
% We omit the time superscripts for predicted human trajectories $\hat{\boldsymbol{X}}_{H}$ in the rest of the paper for simplicity. Our method is planner-agnostic and the robot planning algorithm is outside the scope of this work. 

% Let the state of the human and the robot be $\boldsymbol{x}_{H} \in \mathds{R}^{n_{H}}$ and $\boldsymbol{x}_{R} \in \mathds{R}^{n_{R}}$, where $n_{H}, n_{R}$ are state dimensions. 
% Let their joint state be $\boldsymbol{x}_{HR} \in \mathds{R}^{n_{H} + n_{R}}$.
% Let the control of the human and the robot be $\boldsymbol{u}_{H} \in \mathds{R}^{m_{H}}$ and $\boldsymbol{u}_{R} \in \mathds{R}^{m_{R}}$, where $m_{H}, m_{R}$ are control dimensions. 
% The dynamics can be described by $\dot{\boldsymbol{x}}_{H} = \Dynamics_{H}(\boldsymbol{x}_{H}, \boldsymbol{u}_{H})$ and $\dot{\boldsymbol{x}}_{R} = \Dynamics_{R}(\boldsymbol{x}_{R}, \boldsymbol{u}_{R})$. 
% In a long-term interaction setting, we assume the goal positions are known for both agents as $g_{H}$ and $g_{R}$. 
% The human and robot share the knowledge of all agents' historical trajectories up to the current step $\boldsymbol{Z}_{HR} = [\boldsymbol{x}_{HR}^{0}, \dots, \boldsymbol{x}_{HR}^{t}]$, as well as the environmental obstacles $\mathcal{O}$. 
% At each step $t$, our goal is to predict future human trajectories $\hat{\boldsymbol{X}}_{H} = [\hat{\boldsymbol{x}}_{H}^{t+1}, \dots, \hat{\boldsymbol{x}}_{H}^{t+T_{pred}}]$ with prediction horizon $T_{pred}$, which the robot can use for collision-free predictive planning. Our method is planner-agnostic and the robot planning algorithm is outside the scope of this work. 

\textbf{Overview.} 
We first present the original nested ToM formulation in robot predictive planning. We then propose our reformulated adaptive ToM (AToM) which detaches the human prediction model from the recursive structure. The human model consists of a game-theoretic solver and a parameter update mechanism, which captures dynamic human behaviours in long-term interactions.

\subsection{Original ToM in Robot Planning}
Following prior works mentioned in Sec. \ref{sec:ToM}, we formulate the original ToM in robot predictive planning task and identify the potential issues with this formulation.

The robot plans with a finite horizon $T_{p}$, where at each timestep t the robot optimises its controls $\boldsymbol{U}_{R} = [\boldsymbol{u}_{R}^{t+1}, \dots, \boldsymbol{u}_{R}^{t+T_{p}}]$ to minimize the cumulative cost:
\begin{equation}
\label{eq:robot_planning}
\begin{aligned}
    \min &\; \Cost_{R}(\boldsymbol{x}_{R}^{t+1:t+T_{p}}, \boldsymbol{x}_{R}^{t}, \hat{\boldsymbol{X}}_{H}, \boldsymbol{g}_{R}, \mathcal{O}), \\
    \text{s.t.} \quad &\boldsymbol{x}_{R}^{k+1} = \Dynamics_{R}(\boldsymbol{x}_{R}^{k}, \boldsymbol{u}_{R}^{k}), \\
                      &\Constraint_{R}(\boldsymbol{x}_{R}^{k}, \boldsymbol{u}_{R}^{k}, \boldsymbol{x}_{H}^{k}, \mathcal{O}) \leq 0, \\
                      &k = t+1, \dots, t+T_{p},
\end{aligned}
\end{equation}
where $\Cost_{R}$ is the cost function that captures the robot's performance goals, 
$\Constraint_{R}$ contains constraints such as collision-avoidance, 
and predicted human trajectories $\hat{\boldsymbol{X}}_{H}$ can be obtained from human prediction models such as the Social Force or prediction neural networks, as described in Sec. \ref{seq:traj_prediction}.

% Human predictions $\hat{X}_{H}$ can be obtained from historical observations and other additional information such as human goal and environmental obstacles: ({\color{blue} can introduce the parameter here and remove equation \eqref{eq:humanpred2}})
% \begin{equation}
% \label{eq:human_pred}
%     \tilde{X}_{H} = f_{pred} (X_{HR}, g_{H}, \mathcal{O}),
% \end{equation}
% where $f_{pred}$ is the prediction method such as the Social Force model or neural networks. 

To obtain $\hat{\boldsymbol{X}}_{H}$ using a ToM model, predicted human actions need to be solved from predicted robot trajectories:
\begin{equation}
\label{eq:original_ToM}
\begin{aligned}
    \boldsymbol{\hat{U}}_{H} = \argmin_{\boldsymbol{u}_H^{t+1:t+T_{f}}} &\Cost_{H}(\boldsymbol{x}_{H}^{t+1:t+T_{f}}, \boldsymbol{x}_{HR}^{t}, \hat{\boldsymbol{X}}_{R}, \boldsymbol{g}_{H}, \mathcal{O}), \\
    \text{s.t.} \quad &\boldsymbol{x}_{H}^{k+1} = \Dynamics_{H}(\boldsymbol{x}_{H}^{k}, \boldsymbol{u}_{H}^{k}), \\
                      &\boldsymbol{\hat{x}}_{R}^{k+1} = \Dynamics_{R}(\boldsymbol{\hat{x}}_{R}^{k}, \boldsymbol{u}_{R}^{k}), \\
                      &\Constraint_{H}(\boldsymbol{x}_{H}^{k}, \boldsymbol{u}_{H}^{k}, \boldsymbol{x}_{R}^{k}, \mathcal{O}) \leq 0, \\
                      &k = t+1, \dots, t+T_{f}. \\
\end{aligned}
\end{equation}
% In the original ToM formulation, future human actions are optimised similarly to robot predictive planning:
% \begin{equation}
% \label{eq:original_ToM}
% \begin{aligned}
%     \min_{u_H^{t+1:t+T_{pred}}} &\sum_{i=t+1}^{t+T_{pred}} J_{H}(x_{H}^{i}, g_{H}, \tilde{X}_{R}, \mathcal{O}), \\
%     \text{s.t.} \quad &x_{H}^{i+1} = x_{H}^{i} + f_{H}(x_{H}^{i}, u_{H}^{i}), \\
%                       &x_{R}^{i+1} = x_{R}^{i} + f_{R}(x_{R}^{i}, u_{R}^{i}), \\
%                       &C(x_{H}^{i}, u_{H}^{i}, x_{R}^{i}) \leq 0, \\
%                       &i = t+1, \dots, t+T_{pred},
% \end{aligned}
% \end{equation}
Note that the predicted robot trajectories are obtained from the robot plans $\boldsymbol{U}_{R}$ solved from Problem \eqref{eq:robot_planning}. 
If we substitute Eq. \eqref{eq:original_ToM} and human dynamics $\Dynamics_{H}$ back to the cost in Problem \eqref{eq:robot_planning}, it results in a recursive optimisation problem for the robot. 
Solving such optimisation is tractable for simple settings as discussed in Sec. \ref{sec:ToM}, but complex and computationally expensive for robot planning with continuous and infinite action space. 
Therefore, we propose a reformulation of the original ToM to detach the human prediction model from the nested optimisation problem.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{quantitative_3.jpg}
%     \vspace{-8mm}
%     \caption{Quantitative comparison between AToM and SF in Scenario 3. We compare the number of steps the robot takes to reach the goal which reflects the efficiency. We mark the rounds where collisions happens using red crosses.}
%     % AToM enables the robot to pass through the doorway safely and efficiently in all rounds of experiments.}
%     \vspace{-5mm}
%     \label{fig:quantitative_3}
% \end{figure}

\subsection{Reformulation and Adaptive ToM (AToM)}
Instead of using the true robot plan as human-predicted robot actions, we argue that the human maintains an internal model of the navigation problem and they predict the optimal actions for all agents based on a dynamic belief about each agent's behavioural pattern. 
We formulate the human internal model as a multi-player general-sum differential game with finite horizon $T_{f}$. 
In a navigation scenario, the strategy for player $i$ is a control sequence $\boldsymbol{U}_{i} = [\boldsymbol{u}_{i}^{t+1}, \dots, \boldsymbol{u}_{i}^{t+T_{f}}]$. 
Each player seeks to minimize its cost while respecting constraints and dynamics:
\begin{equation}
\label{eq:game_cost}
\begin{aligned}
    \min_{U} \quad &\sum_{k=t+1}^{t+T_{f}} 
    (\boldsymbol{x}_{i}^{k} - \boldsymbol{g})^\top Q (\boldsymbol{x}_{i}^{k} - \boldsymbol{g}) 
    + \boldsymbol{u}_{i}^{k}{}^\top R \boldsymbol{u}_{i}^{k} \\
    +  &w_{s}(\sum_{n \neq i} \max (0, \|\boldsymbol{x}^{k}_{i} - \boldsymbol{x}^{k}_{n}\|_{2} - d_{s}))) \\
    + &w_{o}(\max (0, D(\boldsymbol{x}^{k}_{i}, \mathcal{O}) - d_{o}))\\
    \text{s.t.} \quad &\boldsymbol{x}_{min} \leq \boldsymbol{x}_{i}^{k} \leq \boldsymbol{x}_{max}, \\
                &\boldsymbol{u}_{min} \leq \boldsymbol{u}_{i}^{k} \leq \boldsymbol{u}_{max}, \\
                &\boldsymbol{x}_{i}^{k+1} = \Dynamics_{i}(\boldsymbol{x}_{i}^{k}, \boldsymbol{u}_{i}^{k}), \\
                &k = t+1, \dots, t+T_{f}, \\
\end{aligned}
\end{equation}
% \begin{equation}
% \label{eq:game_cost}
% \begin{aligned}
%     \min_{U} \quad & \Cost_{goal}(\boldsymbol{X}, \boldsymbol{g}, \theta_{1}) + \Cost_{speed}(\boldsymbol{X}, \theta_{2}) \\
%                    &+ \Cost_{obs}(\boldsymbol{X}, \mathcal{O}, \theta_{3}) + \Cost_{social}(\boldsymbol{X}_{joint}, \theta_{4}), \\
%     \text{s.t.} \quad &\Constraint(\boldsymbol{X}_{joint}, \boldsymbol{U}, \mathcal{O}, \theta_{5}) \leq 0, \\
%                 &\boldsymbol{x}^{k+1} = \Dynamics(\boldsymbol{x}^{k}, \boldsymbol{u}^{k}), \\
%                 &k = t+1, \dots, t+T_{f}, \\
% \end{aligned}
% \end{equation}
% where the 4 cost components are $\Cost_{goal}$ for goal reaching, $\Cost_{speed}$ for speed regulation, $\Cost_{obs}$ for obstacle avoidance, and $\Cost_{social}$ for social distancing. 
% $\boldsymbol{\theta} = [\theta_{1}, \theta_{2}, \theta_{3}, \theta_{4}, \theta_{5}]^\top$ are the parameters from the costs and constraints 
% such as the preferred speed in $\Cost_{speed}$, and preferred distance from neighbours in $\Cost_{social}$. 
where $Q \geq 0$ and $R > 0$ are the weights for the state and control, $w_{s}$ and $w_{o}$ are the weights for social and obstacle avoidance, $D$ calculates the closest distance to obstacle $\mathcal{O}$, 
$d_{s}$ and $d_{o}$ are the preferred social and obstacle distances, $\boldsymbol{x}_{min}$, $\boldsymbol{x}_{max}$, $\boldsymbol{u}_{min}$, and $\boldsymbol{u}_{max}$ are the state and control limits.
We represent these behavioural parameters for all players using $\boldsymbol{\theta}$.
% $\boldsymbol{\theta} = [w_{s}, d, w_{o}, d_{o}, \boldsymbol{x}_{min}, \boldsymbol{x}_{max}, \boldsymbol{u}_{min}, \boldsymbol{u}_{max}]^\top$ are the parameters from the costs and constraints such as the weights, preferred distance from other agents and obstacles, and limits for the states and controls.
% These parameters model the behaviours of each player in a game-theoretic setting.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{quantitative_3.jpg}
    \vspace{-8mm}
    \caption{Quantitative comparison between AToM and SF in Scenario 3. We compare the number of steps the robot takes to reach the goal which reflects the efficiency. SF leads to collisions in 7 rounds, which we highlighted using red crosses.}
    % We mark the rounds where collisions happen using red crosses. AToM enables the robot to pass through the doorway efficiently and safely in all rounds of experiments.}
    \vspace{-5mm}
    \label{fig:quantitative_3}
\end{figure}


We then find a Nash equilibrium solution $\tilde{\boldsymbol{U}}_{1}, \dots, \tilde{\boldsymbol{U}}_{n}$ for $n$ players with each player's cost defined in Problem \eqref{eq:game_cost}.
At a Nash equilibrium point, no player $i$ can further decrease its cost by unilaterally changing its strategy $\tilde{\boldsymbol{U}}_{i}$ \cite{facchinei2010generalized}. 
Solving this generalized Nash equilibrium problem can be done using existing dynamic game solvers. In this work, we use ILQSolver \cite{fridovich2020efficient} which iteratively solves linear-quadratic approximations of the original differential game in multi-player settings. 
We consider it an algorithmic module $\GameSolver$ parameterized by $\boldsymbol{\theta}$ that takes as input the current joint state of the system, the known goal positions and environmental obstacles, and returns an open-loop joint strategy that satisfies global Nash equilibrium:
\begin{equation}
\label{eq:our_ToM}
    \tilde{\boldsymbol{U}}_{H}, \tilde{\boldsymbol{U}}_{R} = \GameSolver(\boldsymbol{x}_{HR}^{t}, \boldsymbol{g}_{HR}, \mathcal{O}, \hat{\boldsymbol{\theta}}),
\end{equation}
where $\tilde{\boldsymbol{U}}_{H}$ is the predicted human action and equivalent to $\boldsymbol{\hat{U}}_{H}$, 
$\tilde{\boldsymbol{U}}_{R}$ is the human-predicted robot action, 
and $\hat{\boldsymbol{\theta}}$ is the human-predicted behavioural parameters. 
With this reformulation, we can now easily obtain $\hat{\boldsymbol{X}}_{H}$ from Eq. \eqref{eq:our_ToM} and human dynamics $\Dynamics_{H}$, and therefore solve Problem \eqref{eq:robot_planning} without any recursive structure.
% With this reformulation, we can now substitute Eq. \eqref{eq:our_ToM} and human dynamics back to the robot predictive planning problem in Eq. \eqref{eq:robot_planning} without any recursive structure.

Existing game-theoretic robot planners use similar formulations to solve for the optimal robot plans using known cost functions for each agent \cite{tian2022safety, le2021lucidgames, hu2023emergent}. 
A fundamental difference in this work is that our human internal model $\GameSolver$ is parameterized by $\hat{\boldsymbol{\theta}}$, which represents a dynamic human belief instead of the true agent parameters.

After obtaining predicted human trajectories $\hat{\boldsymbol{X}}_{H}$, the downstream robot planner can now perform predictive planning and execute the actual robot plans. 
As we do not assume any leader-follower structure, the human simultaneously performs her/his actions. 
Up to this point, we have obtained four sets of trajectories: 
1) Predicted Human $\hat{\boldsymbol{X}}_{H}$, 
2) Human-predicted Robot $\hat{\boldsymbol{X}}_{R}$, which can be obtained from $\tilde{\boldsymbol{U}}_{R}$
3) Observed Human $\boldsymbol{X}_{H}$, 
4) Observed Robot $\boldsymbol{X}_{R}$. 
These predicted-observed pairs can be used to correct the estimated behavioural parameters $\hat{\boldsymbol{\theta}}$ in the human internal model.

We use Unscented Kalman Filter (UKF) \cite{wan2000unscented} as the update mechanism. We allow $\hat{\boldsymbol{\theta}}$ to evolve using a random walk as the process model $\ProcessModel$. The measurement states are the agents' trajectories and the measurement model $\MeasurementModel$ is therefore the game solver $\GameSolver$ combined with agent dynamics.
\begin{equation}
\begin{aligned}
    \boldsymbol{\theta}^{t+1} &= \ProcessModel(\boldsymbol{\theta}^{t}, \delta^{t}) = \boldsymbol{\theta}^{t} + \delta^{t}, \quad \delta^t \sim \mathcal{N}(0, Q_t), \\
    \boldsymbol{x}^{t+1} &= \MeasurementModel(\boldsymbol{x}^{t}, \boldsymbol{\theta}^{t}) \\
            &= \Dynamics(\boldsymbol{x}^{t}, \GameSolver(\boldsymbol{x}^{t}, \boldsymbol{\theta}^{t})) + \epsilon^{t}, \quad \epsilon^t \sim \mathcal{N}(0, R),
\end{aligned}
\end{equation}
where $Q_{t}$ and $R$ are the covariance for the process model and measurement model.
In this way, our adaptive ToM human model can be adjusted dynamically to improve its prediction accuracy, and to reflect how humans update their beliefs on others. The complete procedure is detailed in Algorithm \ref{alg:our_method}, where $\Sigma$ is the covariance of $\theta$ estimation.
At the observation step, the robot executes the planned action from the predictive planner and the true human motion is observed.

% \vspace{-1mm}
\begin{algorithm}
\caption{Predict-Observe-Update Procedures with AToM}
\label{alg:our_method}
\begin{algorithmic}[1]
\State \textbf{Inputs:} $\boldsymbol{x}_{HR}^{t}, \hat{\boldsymbol{\theta}}^{t}, \Sigma^{t}, \boldsymbol{g}_{HR}, \mathcal{O}$
\State \textbf{for} $k = t, t+1, \dots$ \textbf{do}
    \State \quad $\tilde{\boldsymbol{U}}_{HR} \gets \GameSolver(\boldsymbol{x}_{HR}^{k}, \boldsymbol{g}_{HR}, \mathcal{O}, \hat{\boldsymbol{\theta}}^{k})$ 
    \State \quad $\hat{\boldsymbol{X}}_{HR} \gets \Dynamics_{HR}(\boldsymbol{x}_{HR}^{k}, \tilde{\boldsymbol{U}}_{HR})$\Comment{Predict}
    \State \quad $\boldsymbol{x}_{R}^{k+1} \gets \text{RobotPlanner}(\boldsymbol{x}_{R}^{k}, \hat{\boldsymbol{X}}_{H}, \boldsymbol{g}_{R}, \mathcal{O})$
    % \State \quad $\boldsymbol{x}_{H}^{k+1} \gets \text{Human}$ \Comment{Observation}
    \State \quad observe human state $\boldsymbol{x}_{H}^{k+1}$ \Comment{Observe}
    \State \quad $\hat{\boldsymbol{\theta}}^{k+1}, \Sigma^{k+1} \gets $
    \Statex \quad $\text{UKF}(\boldsymbol{x}_{HR}^{k+1}, \hat{\boldsymbol{x}}_{HR}^{k+1}, \hat{\boldsymbol{\theta}}^{k}, \Sigma^{k}, \ProcessModel, \MeasurementModel)$ \Comment{Update}
\State \textbf{end for}
\end{algorithmic}
\end{algorithm}
% \vspace{-2mm}

% \begin{equation}
% \begin{aligned}
%     \theta^{t+1} &= \ProcessModel(\theta^{t}, \delta^{t}) \\
%                  &= \theta^{t} + \delta^{t}, \quad \delta^t \sim \mathcal{N}(0, Q_t), \\
%     X^{t+1} &= \MeasurementModel(X^{t}, \theta^{t}) \\
%             &= X^{t} + F(X^{t}, GameSolver_{\theta^{t}}(X^{t})).
% \end{aligned}
% \end{equation}
