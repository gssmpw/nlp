\section{Related Work}
\vspace{-2mm}
\label{sec:related}
Popular diffusion-based image generators, such as \textbf{SDXL} \citep{podell2023sdxl}, have made tremendous progress in producing high-quality diverse images. But these methods struggle to fully translate complex scene details from text to image, especially if it involves preserving a specific scene attribute w.r.t.~a reference image. We can observe this limitation in Figure~\ref{fig:teaser}(g) where the image generated based on a text description of a reference image~(image-to-text-to-image~(\textbf{I2T2I})) cannot preserve the scene elements from reference image~(poor fidelity). 

% , but it still faces challenges in preserving the specific context of a reference image. To evaluate this method, given a reference image, we use a MLLM \citep{liu2024improved, chen2024internvl} to extract its detailed description based on a given text guidance. This description is then used as conditioning to generate an image via text-to-image (T2I) SDXL, a process we refer to as image-to-text-to-image (I2T2I) SDXL. Figure~\ref{fig:teaser} (g) shows I2T2I SDXL struggles to translate complex contextual relationships from text into visual representations, particularly in cases where the text contains intricate scene details such as spatial relationships or fine-grained attributes.

\textbf{Image Variation} methods like \citet{xu2023versatile, feng2023diverse, zhang2024real, le2024brush, graikos2024learned, belagali2024gen}, use an image encoder to extract embeddings to condition diffusion models. While this helps to retain high-level semantics and allow changes to style and other low-level features, these methods fail to preserve specific scene attributes of a reference image due to a lack of precise control over which detail to emphasize. Moreover, addition of noise during diffusion process introduces randomness which causes significant deviation from the reference image, again resulting in poor fidelity~(Figure~\ref{fig:teaser}~(g)).

% \textbf{Image Variation} methods like \citet{xu2023versatile, feng2023diverse}, use an image encoder to extract embeddings that condition diffusion models, aiming to retain high-level semantics while allowing changes in style or low-level features. However, these methods often fail to preserve specific contextual information for two main reasons: first, the image embeddings often lack precise control over which details are emphasized, leading to the loss of key context; and second, the addition of noise during the diffusion process introduces randomness that leads to significant deviations from the original context.

\textbf{Image Translation} or Image Editing techniques, such as Boomerang \citep{pan2023boomerang} and ControlNet \citep{zhang2023adding}, add noise to reference input image to create variations around the original reference image in latent space. Since they aim to generate multiple local samples, they fail to produce sufficiently diverse images~(Figure~\ref{fig:teaser}~(g)). Moreover, the added noise during forward diffusion step often distorts or removes the details on specific scene attributes in the reference image.

% They also are prone to losing the details on specific scene attributes in reference image as the added noise during forward diffusion step often distorts/removes critical semantic scene details. 

% \textbf{Image Translation} or Image Editing techniques, such as Boomerang \citep{pan2023boomerang} and ControlNet \citep{zhang2023adding}, add noise to input images to create variations around the original image in latent space. These methods aim to generate multiple local samples, hence they fail to produce sufficiently diverse images. They often fail to preserve the original context as the noise added during the forward diffusion process often distorts or removes critical contextual details.

\textbf{Textual Inversion} methods~\citep{gal2022image, trabucco2023effective, ahn2024dreamstyler, nguyen2024visual} encode new concepts as ``pseudo-words'' in text embedding space to guide image generation. Although effective for style transfer, these methods struggle with tasks where a single reference image is involved, such as TTA, to generate multiple synthetic images. When fine-tuning with just one reference image, these methods lead to images containing irrelevant scene attributes and fail to preserve the attributes from the original reference image (leading to poor fidelity, Figure~\ref{fig:teaser}~(g)).

% \textbf{Textual Inversion} \citep{gal2022image, trabucco2023effective, ahn2024dreamstyler, nguyen2024visual} encode new concepts as ``pseudo-words" in text embedding spaces to guide image generation. Although effective for style transfer, these methods struggle in TTA, where generating multiple synthetic images from a single reference is required. Fine-tuning with just one reference image results in contextually irrelevant outputs, failing to maintain the context of the original input.