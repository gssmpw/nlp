\vspace{-1mm}
\section{Experiment}
\vspace{-1mm}
\subsection{Benchmark Formulation} 
\label{sec:benchmark_formulation}
To evaluate \method's effectiveness as a multimodal context-aligned image generator, we introduce a benchmark focused on two key criteria: \textit{fidelity}, which measures how accurately the generated images preserve scene attributes as per the multimodal context, and \textit{diversity}, which assesses how distinct the generate images are from each other as well as from the reference image.

For \textbf{fidelity}, we build our evaluation framework around three main considerations: \textit{Applicability}, ensuring generated images enhance model performance when combined with real data; \textit{Efficiency}, emphasizing computationally light and resource-efficient protocols; and \textit{Fairness}, promoting standardized test-time evaluation to mitigate biases caused by varying training processes.

Based on these principles, we adopt two evaluation settings: (1) VQA Benchmark for MLLMs using Test-Time Augmentation (TTA)~\citep{shanmugam2021better}, and (2) Human-Object Interaction (HOI) Reasoning using Test-time Prompt Tuning (TPT) from \citet{shu2022testtime}. Both settings leverage real and synthetic data to boost performance while allowing computational efficiency. TTA uses pre-trained MLLMs without requiring additional training for fair comparison, while TPT fine-tunes prompt embeddings for quick convergence.

For \textbf{diversity}, we conduct two experiments using CLIP ViT-G/14 \citep{radford2021learning} features. First, we compute the Euclidean distance between the generated and reference images to quantify how distinct the generated images are from their reference counterparts. Second, we generate images using $20$ different random seeds and compute the average pairwise Euclidean distance across all generated images, providing a measure of intra-set diversity.

\textbf{Datasets and Metrics.} For the VQA benchmark, we fine-tune \method on VQAv2 \citep{goyal2017making} and GQA \citep{hudson2019gqa}, then evaluate using TTA on MME Perception \citep{fu2024mme}, a common benchmark for assessing SOTA MLLMs. 
Our benchmark covers MME Perception tasks related to Existence, Count, Position, Color, and Scene (more discussion on this in Appendix~\ref{appendix:scope}).
We generate synthetic images using the test image as a reference and pair them with corresponding questions as text guidance. We then feed the real~(reference) and generated images to MLLMs along with yes/no questions to extract the logit for the next token \texttt{[yes]/[no]}. We determine the final predicted token by averaging the logits and comparing it to the ground truth (yes/no). Using the paired yes and no questions for each test image, we report Accuracy (ACC), which measures the correctness of individual question predictions, and Accuracy+ (ACC+), which measures the joint correctness of the question pair.
% Synthetic images are generated using the test image as reference and paired with corresponding questions as text guidance.  Both real and generated images are fed to MLLMs along with yes/no questions to extract the logit for the next token \texttt{[yes]/[no]}. The final predicted token is determined by averaging the logits and comparing the result to the ground truth. We assess performance using paired positive (yes) and negative (no) questions for each test image. We report Accuracy (ACC), which measures the correctness of individual question predictions, and Accuracy+ (ACC+), which measures the joint correctness of both questions.

% For HOI Reasoning, we fine-tune \method on Bongard-HOI~\citep{jiang2022bongard} training set and evaluate on corresponding test sets using TPT. The task involves prediction the interaction in a query image by comparing feature similarities to optimized prompts from positive and negative support sets, such as \textit{riding bicycle} vs. \textit{not riding bicycle}.


For HOI Reasoning, we fine-tune \method on Bongard-HOI~\citep{jiang2022bongard} training set and evaluate on associated test sets using TPT from \citet{shu2022testtime}. Following the setup, given a query test image with support sets of positive (e.g., \textit{riding bicycle}) and negative (e.g., \textit{not riding bicycle}) images, we generate synthetic images for support set images as reference using corresponding label as text guidance. We then use the augmented support sets to optimize a pair of prompt embeddings that contrast each other and predict the human-object interaction in query image by comparing its feature similarity to the two optimized prompts. Please refer to \citet{shu2022testtime} for more details.
% \textcolor{blue}{In this setup, we are given a query image along with two support sets including positive (e.g., \textit{riding bicycle}) and negative (e.g., \textit{not riding bicycle}). We generate augmented images from support sets and optimize prompt embeddings for each, ensuring that the optimized prompts contrast with one another. The interaction in the query image is predicted by comparing its feature similarity to the optimized prompts from the positive and negative support sets.}
We use Accuracy as a measure of model's ability to correctly predict these human-object interactions.
% Accuracy reflects the model's ability to correctly classify the interaction based on these comparisons.

% For HOI Reasoning, we fine-tune \method on the Bongard-HOI \citep{jiang2022bongard} training set and evaluate the model on held-out test sets using the TPT strategy \citep{shu2022testtime}. The test involves comparing the feature representations of query images to optimized prompts generated from positive and negative support sets, such as \textit{riding bicycle} vs. \textit{not riding bicycle}. The task is to predict the interaction in the query image based on which optimized prompt yields the highest similarity score. Accuracy measures the model's ability to correctly identify whether the interaction in the query image matches the interaction described by the positive or negative prompts.

\textbf{Method Comparisons.} We compare \method against representative techniques from four groups of SOTA image generation techniques (covered in Section~\ref{sec:related}): (1) customized T2I diffusion models (referred to as I2T2I SDXL), (2) Image Variation, (3) Image Translation/Editing, and (4) Textual Inversion, as well as data augmentation methods like RandAugment \citep{cubuk2020randaugment}.

\textbf{Object-Centric Benchmark.} In addition to scene-aware tasks like VQA and HOI Reasoning, we also evaluate \method on an object-centric benchmark to demonstrate its versatility. Specifically, we fine-tune the UNet denoiser on the ImageNet training set \citep{deng2009imagenet}, and perform TPT \citep{shu2022testtime} using real and generated images on the ImageNet test set and four out-of-distribution (OOD) datasets: ImageNet-A \citep{hendrycks2021natural}, ImageNet-V2 \citep{recht2019imagenet}, ImageNet-R \citep{hendrycks2021many}, and ImageNet-Sketch \citep{wang2019learning}. This enables us to assess the robustness of \method under natural distribution shifts. We use Top-1 accuracy which measures the correctness of classifying test images.

\vspace{-2mm}
\subsection{Implementation Details}
For \method, we use SDXL Base 1.0 which is a standard pre-trained diffusion-based image generation model. We further employ CLIP ViT-G/14 as the image encoder and both CLIP-L/14 \& CLIP-G/14 as the text encoders~\citep{radford2021learning}. We perform LoRA fine-tuning with $11$M trainable parameters~($\approx 0.46\%$ of total $2.6$B parameters) on $8$ NVIDIA A100 80GB GPUs using AdamW~\citep{loshchilov2017decoupled} optimizer, learning rate of \texttt{5e-6}, and gradient accumulation steps of $8$. Please refer to Appendix~\ref{appendix:impl} for more details.

% We implement our \method using PyTorch \citep{paszke2019pytorch} and HuggingFace diffusers \citep{huggingface2023diffusers} libraries. For the generative model, we utilize the SDXL Base $1.0$ which is a standard and common pre-trained diffusion model in natural images domain. In the pipeline, we employ CLIP ViT-G/14 as image encoder and both CLIP-L/14 \& CLIP-G/14 as text encoders \citep{radford2021learning}. We perform LoRA fine-tuning the following modules of SDXL UNet denoiser including $\tt{to\_q, to\_v, query, value, ff.net.0.proj}$ with $r = 8$, which results in $11$M trainable parameters $\approx 0.46\%$ of total $2.6$B parameters. The fine-tuning is done on $8$ NVIDIA A100 80GB GPUs using AdamW \citep{loshchilov2017decoupled} optimizer, a learning rate of $5e-6$, and gradient accumulation steps of $8$.
% \subsection{Experimental Results}
\vspace{-2mm}
\subsection{Comparison with Existing Methods on the Benchmark Formulation}
% \textbf{Evaluation on VQA.} We evaluate the performance of our proposed method \method in generating high contextual fidelity synthetic data for VQA tasks, using the MME Perception benchmark. The evaluation is performed across various context-aware tasks, including Existence, Count, Position, Color, and Scene. We evaluate the performance of current SOTA MLLMs including LLaVA v1.6 \citep{liu2024improved} and InternVL 2.0 \citep{chen2024internvl} in terms of accuracy and accuracy+ metrics when using real data only, TTA with standard augmentation technique RandAugment \citep{cubuk2020randaugment}, synthetic data from other image generation methods, and our proposed \method.

% As shown in Table~\ref{table:vqa}, our method demonstrates significant improvements in accuracy, especially for tasks that require fine-grained understanding of the context such as Count and Position. In evaluation of LLaVA v1.6 7B model, \method shows marked improvements in Position context, with a $+5\%$ increase in ACC and $+13.33\%$ increase in ACC+. For the InternVL 2.0 8B setting, \method consistently improves performance across all tasks. In particular, it achieves the highest improvement in the Count task with a $+13.34\%$ increase in ACC and $+23.33\%$ increase in ACC+.

% In comparison to other state-of-the-art methods such as Image Variation \citep{xu2023versatile}, Image Translation \citep{pan2023boomerang}, Textual Inversion \citep{gal2022image}, and I2T2I SDXL \citep{podell2023sdxl}, \method consistently achieves superior results. These results confirm the capability of \method to generate contextually faithful synthetic data that aids multimodal LLMs in improving their performance on challenging context-aware VQA tasks, reinforcing the importance of context-aware image generation.

\textbf{VQA benchmark.} To evaluate \method for VQA using MME Perception, we experiment with SOTA MLLMs including LLaVA v1.6~\citep{liu2024improved} and InternVL 2.0~\citep{chen2024internvl}. Table~\ref{table:vqa} shows that \method significantly improves both ACC and ACC+ compared to all existing methods on both LLaVA v1.6 and InternVL 2.0 consistently across all tasks. The ability of \method's generated images to enhance MLLM performance on complex VQA tasks validates that \method can generate images faithful to the input multimodal context.
% This demonstrates \method's ability to generate images faithful to the multimodal context that enhance MLLM performance on complex VQA tasks. 
The improvement is particularly prominent for tasks needing a fine-grained understanding of the scene as in Count and Position. For LLaVA v1.6, \method boosts Position ACC by $5\%$ and ACC+ by $13.34\%$, 
while for InternVL 2.0, it significantly improves both Count ACC and Color ACC+ by $13.34\%$ and Count ACC+ by $23.33\%$.
% it gives an improvement of $13.34\%$ on both Count ACC and Color ACC+ and $23.33\%$ on Count ACC+. 
% while for InternVL 2.0, it delivers a $13.34\%$ ACC improvement and $23.33\%$ ACC+ improvement on Count. 
This also highlights \method's generalizability across different MLLMs.  

% \textbf{Evaluation on VQA.} We evaluate \method's ability to generate high-fidelity synthetic data for VQA tasks using the MME Perception benchmark, covering Existence, Count, Position, Color, and Scene tasks. We assess the performance of SOTA MLLMs, including LLaVA v1.6 \citep{liu2024improved} and InternVL 2.0 \citep{chen2024internvl}, comparing accuracy and accuracy+ metrics using real data, TTA with RandAugment, synthetic data from other generation methods, and \method. As shown in Table~\ref{table:vqa}, \method significantly improves accuracy, especially for tasks that require fine-grained understanding of the context such as Count and Position. For LLaVA v1.6, \method boosts Position ACC by $5\%$ and ACC+ by $13.34\%$, while for InternVL 2.0, it delivers a $13.34\%$ ACC and $23.33\%$ ACC+ improvement on Count. Compared to other methods, \method consistently outperforms across all tasks, demonstrating its ability to generate contextually faithful data that enhances MLLM performance on complex VQA tasks.
\input{tables/vqa}



% \textbf{Evaluation on HOI Reasoning.} We evaluate the effectiveness of \method in improving HOI reasoning. We adopt a TPT approach, utilizing both real and synthetic data, and evaluate the performance on the Bongard-HOI dataset using CLIP-ResNet50 \citep{radford2021learning}. As shown in Table~\ref{table:hoi}, \method significantly outperforms other augmentation methods across all test splits, achieving the highest average accuracy of $68.73\%$ $(+2.14\%)$, demonstrating superior generalization over image generation methods like Image Variation \citep{xu2023versatile} and Textual Inversion \citep{gal2022image}, which suffer from large drops in performance. These results highlight \method's ability to generate augmentations of high contextually fidelity, allowing the model to effectively generalize to novel interaction scenarios.

\textbf{HOI Reasoning.} We experiment with CLIP-ResNet50~\citep{radford2021learning} to evaluate \method on Bongard-HOI using TPT. Table~\ref{table:hoi} shows that \method outperforms all existing augmentation/image generation methods consistently across all test splits, achieving the highest average accuracy of $68.73\%$ ($+2.14\%$ over data augmentation-based baseline). While VQA on MME Perception helps to demonstrate \method's effectiveness on spatial scene attributes, \method's superior performance on Bongard-HOI demonstrates the method's ability to generate high-fidelity augmentations also for novel interaction-based scene attributes.

% \textbf{Evaluation on HOI Reasoning.} We evaluate \method on the Bongard-HOI dataset using a TPT approach with real and synthetic data, measured on CLIP-ResNet50 \citep{radford2021learning}. As shown in Table~\ref{table:hoi}, \method outperforms other augmentation methods across all test splits, achieving the highest average accuracy of $68.73\%$ $(+2.14\%)$. This demonstrates its superior generalization compared to methods like Image Variation \citep{xu2023versatile} and Textual Inversion \citep{gal2022image}, which struggle with performance drops. \method's contextual fidelity augmentations enable better generalization to novel interaction scenarios.

\input{tables/hoi}

\textbf{Object-Centric benchmarks.} Table~\ref{table:imagenet} shows the evaluation of \method on object-centric benchmarks, including ImageNet and its OOD variants (ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch) using TPT. We can observe that \method consistently outperforms all other image generation/augmentation methods across all object-centric benchmarks. This validates \method's versatility by being effective on both scene-aware and object-centric tasks.

\input{tables/object_centric}
 
% \textbf{Evaluation on Object-Centric Benchmark.} To showcase the versatility of \method, we evaluate it on object-centric benchmarks, including ImageNet and its OOD variants (ImageNet-A, ImageNet-V2, ImageNet-R, and ImageNet-Sketch) using TPT with real and synthetic data. After fine-tuning \method on the ImageNet train set, we generate synthetic data and perform TPT on the test set and OOD datasets. As shown in Table~\ref{table:imagenet}, \method consistently outperforms other generation methods, improving performance across ImageNet and OOD benchmarks, confirming its effectiveness in both context-aware and object-centric tasks.

\textbf{Qualitative Study.}  Figure~\ref{fig:qualitative} provides qualitative comparison between \method and other image generation methods across different scene-aware tasks from MME Perception and HOI Reasoning benchmarks. \method consistently surpasses all other approaches, achieving higher fidelity w.r.t.~the input multimodal context~(reference image along with text guidance) while maintaining high diversity w.r.t~the reference image. For example, in Figure~\ref{fig:qualitative}~(Row 1), the reference image depicts $4$ people, and \method successfully maintains this count in the generated image. In contrast, I2T2I SDXL, Image Translation, and Textual Inversion generate images with $6$, $5$, and $2$ people, respectively. Image Variation struggles even with image quality, with an estimated people count ranging between $3$ and $7$. Please refer to Appendix~\ref{appendix:visuals} for more qualitative results.

% \textbf{Qualitative Evaluation.} Figure~\ref{fig:qualitative} showcases a qualitative comparison between \method and other image generation methods across context-aware tasks from the MME Perception and HOI Reasoning benchmarks. Our method consistently surpasses current SOTA image generation approaches, excelling in both multimodal context fidelity and image diversity. For example, in the first row, the reference image depicts $4$ people, and \method successfully maintains this count in the generated image. In contrast, I2T2I SDXL, Image Translation, and Textual Inversion generate images with $6$, $5$, and $2$ people, respectively. Image Variation struggles to clearly represent the number of people, with estimates ranging between $3$ and $7$.
\begin{figure}[!t]
\vspace{-1mm}
    \centering
\includegraphics[width=0.9\linewidth]{figures/qualitative_compact_v1.pdf}
    \vspace{-6mm}
    \caption{Generated image comparison between \method and SOTA methods on MME Perception and HOI Reasoning. \method achieves highest fidelity while maintaining high diversity.}
    \label{fig:qualitative}
    \vspace{-4mm}
\end{figure}

\input{tables/diversity}
\input{tables/ablation}
\subsection{Ablation Study}
\textbf{Diversity Analysis.} Table~\ref{table:diversity} shows that \method~(after fine-tuning) achieves the second-highest Euclidean distance score, both w.r.t~the reference image and among generated images, slightly behind I2T2I SDXL, while securing the highest fidelity as shown in Tables~\ref{table:vqa} and~\ref{table:hoi}. Figure~\ref{fig:diversity} illustrates \method's high diversity in generated images across different random seeds, validating its ability to provide high fidelity w.r.t~multimodal context while preserving diversity.

% \textbf{Diversity Analysis.} As shown in Table~\ref{table:diversity}, \method (fine-tuned) secures the second-highest Euclidean distance score, just slightly trailing I2T2I SDXL, demonstrating its ability to balance diversity while maintaining contextual fidelity. Figure~\ref{fig:diversity} further highlights \method's capacity to generate diverse images, ensuring diversity both with respect to the reference images and among the generated outputs, all while preserving contextual fidelity as seen in Table~\ref{table:vqa} and~\ref{table:hoi}.

\begin{figure}[!t]
    \vspace{-1mm}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/diversity_compact.pdf}
    \vspace{-6mm}
    \caption{\method exhibits high diversity across different random seeds while producing high-fidelity images each time w.r.t~multimodal context~(reference image + text guidance).}
    \label{fig:diversity}
    \vspace{-4mm}
\end{figure}

\textbf{Effectiveness of Multimodal Context Rewards.} Table~\ref{table:ablation} shows an ablation to evaluate the impact of Global Semantic $\mathcal{R}_\textrm{global}$ and Fine-Grained Consistency Reward $\mathcal{R}_\textrm{fine-grained}$ of \method. We use LLaVA 1.6 7B for evaluation while considering both LLaVA 1.6 7B and InternVL 2.0 8B as MLLM for Context Description. Table~\ref{table:ablation}~(Row 1 and 5) show that \method achieves the best performance when both rewards are applied. Performance reduces in absence of either reward, especially on tasks requiring detailed multimodal context preservation~(e.g. for Position and Count).
 

% \textbf{Effectiveness of Context Rewards.} We evaluate the impact of Global Semantic $\mathcal{R}_\textrm{global}$ and Fine-Grained Consistency Reward $\mathcal{R}_\textrm{fine-grained}$ on \method's performance across context-aware tasks. As shown in the first and fifth rows of Table~\ref{table:ablation}, \method achieves the highest performance with both rewards applied. Removing either reward reduces performance, especially on tasks requiring detailed context preservation, emphasizing their importance in capturing nuanced attributes.

\textbf{Impact of MLLM as Context Descriptor.} Table~\ref{table:ablation} also helps to analyze the effect of using different MLLMs to obtain Context Description $\mathcal{C}$. We consider LLaVA 1.6 7B and InternVL 2.0 8B for evaluation. Table~\ref{table:ablation}~(Row 1 and 5) show that training \method with $\mathcal{C}$ obtained from LLaVA performs better than that from InternVL for Position and Scene tasks while $\mathcal{C}$ from InternVL achieves better accuracy on Existence and Count tasks. This experiment emphasizes the importance of choosing a strong MLLM to get Context Description to fine-tune \method.

% \textbf{Impact of MLLM as Context Descriptor.} We analyze the impact of different MLLMs, specifically LLaVA and InternVL, as context descriptors on fine-tuning the \method pipeline. Table~\ref{table:ablation} (first and fifth rows) shows that training \method with LLaVA descriptor is better than that of InternVL in terms of Position and Scene tasks while InternVL descriptor achieves the best performance in Existence and Count tasks. This experiment emphasizes the importance of choosing a strong MLLM as a context descriptor to fine-tune \method.

\textbf{Effectiveness of Fine-tuning.} Table~\ref{table:ablation} shows that fine-tuning SDXL in \method leads to better performance consistently across all tasks compared to the baseline without finetuning (\colorbox{lightgray}{gray-shaded} Rows 4, 8). Figure~\ref{fig:training_progress} further illustrates how fine-tuning improves fidelity while preserving diversity~(with reference image having 6 people and text guidance referencing Count attribute, generated image also has 6 people after fine-tuning while remaining visually diverse w.r.t.~reference image).

% \textbf{Effectiveness of Fine-tuning.} We explore the significance of fine-tuning \method. As detailed in the \colorbox{lightgray}{gray-colored} fourth and last rows of Table~\ref{table:ablation}, where the model is tested without fine-tuning, specific tasks such as Position, Color, and Scene exhibit lower accuracy. This demonstrates that the fine-tuning phase is essential for achieving the best performance, especially in maintaining detailed contextual fidelity across all evaluated tasks. Figure~\ref{fig:training_progress}
 % further illustrates the impact, showcasing improved contextual fidelity in the generated test images when fine-tuning is applied.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/trainingprogress.pdf}
    \vspace{-6mm}
    % \caption{Generated test images with different fine-tuning steps, showing improvement in fidelity.}
    \caption{Fine-tuning with Multimodal Context Rewards improves fidelity in generated images.}
    % Image generation showing fidelity improvement with multimodal context rewards fine-tuning.}
    \vspace{-5mm}
    \label{fig:training_progress}
\end{figure}
% \input{ICLR 2025 Template/tables/ablation}