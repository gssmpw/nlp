\section{Introduction}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser_comparison_v1.pdf}
    \vspace{-9mm}
    \caption{\method aligns generated image with multimodal context input (reference image + text guidance) ensuring the synthetic image is diverse w.r.t.~reference image while exhibiting high fidelity (i.e.~preserves the scene attribute from reference image in relation to text guidance). (a-f) By doing so, for VQA and HOI Reasoning, \method enables the answer to the question in the text guidance to remain consistent between the reference and generated images. (g) \method is also able to preserve both diversity and fidelity without the trade-off exhibited by existing methods.} %Examples include: (a) retaining the number of six people when the text refers to a numerical attribute, (b) preserving the green color of a hat, (c) ensuring the existence of an object like a bottle, (d) maintaining the spatial relationship of a dog being above a pool, and (e) keeping the general store in an outdoor setting when referencing scene attributes.}
    \label{fig:teaser}
    \vspace{-6mm}
\end{figure}
In recent years, diffusion models~\citep{ho2020denoising,rombach2022high} have emerged as powerful tools for image generation, offering impressive capabilities in creating high-quality and diverse synthetic data. This synthetic data has proven valuable in various applications, particularly for \textit{object-centric} image classification tasks~\citep{shu2022testtime, feng2023diverse}. However, for \textit{scene-aware} tasks like Visual Question Answering (VQA) \citep{goyal2017making,antol2015vqa} and Human-Object Interaction (HOI) Reasoning \citep{jiang2022bongard, ulutan2020vsgnet}, it is essential that the generated images accurately preserve scene attributes relevant to the task, as specified by the accompanying text queries. These attributes may include numerical attributes (e.g., object count), physical properties (e.g., color), spatial relationships (e.g., relative positions of objects), object interactions (e.g., how objects interact with each other), or scene-level details. Although defining an all-exhaustive list of such attributes is intractable, tasks like VQA and HOI, through their open-ended questions~\citep{antol2015vqa}, enable to capture the relevant semantic attributes effectively.

While existing image generation methods have made great strides in improving the \textit{diversity} (defined as how different the generated images are from a given reference image and from each other), they often fall short in maintaining high \textit{fidelity}. Given a text query as guidance pertaining to scene attributes in relation to a given reference image (together referred to as \textit{multimodal context}), we define \textit{fidelity} as how truthfully the image generator can preserve those attributes in the generated image. Existing methods, including state-of-the-art~(SOTA) diffusion models like Stable Diffusion XL (SDXL)~\citep{podell2023sdxl}, Image Translation techniques like Boomerang~\citep{pan2023boomerang}, Textual Inversion~\citep{gal2022image, trabucco2023effective}, and Image Variation approaches \citep{xu2023versatile, feng2023diverse}, often lose fine-grained scene details as the embeddings do not have a precise control on which scene attributes to capture. This lack of fidelity is particularly problematic in synthetic data generation for \textit{scene-aware} tasks like VQA and HOI Reasoning, where preserving scene elements in relation to text query is critical for high performance.

 
% While existing image generation methods have made great strides in improving the diversity (defined as how different the generated images are from a given reference image and from each other), they often fall short in maintaining high \textit{fidelity}. Given a text query as guidance pertaining to scene attributes in relation to a given reference image (referred to as \textit{multimodal context}), we define \textit{fidelity} as how truthfully the image generator can preserve those attributes in the generated image. State-of-the-art (SOTA) diffusion models like Stable Diffusion XL (SDXL) \citep{podell2023sdxl} struggle to fully translate complex scene details from text to image; Image Translation techniques like Boomerang \citep{pan2023boomerang} introduce noise that can obscure critical details, while Textual Inversion \citep{gal2022image} and DA-Fusion \citep{trabucco2023effective} rely on multiple examples, limiting their effectiveness for single-image testing samples. Image Variation methods \citep{xu2023versatile, feng2023diverse}, while retaining high-level semantics, often lose finer details due to the inability of image embeddings to precisely control which attributes to capture. Additionally, the diffusion process tends to introduce noise that causes deviations from the original context. This lack of fidelity is particularly problematic in context-aware tasks like VQA and HOI Reasoning, where understanding and maintaining detailed context is essential for achieving high performance.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/intro.pdf}
%     \vspace{-5mm}
%     \caption{Comparison}
%     \label{fig:comparison}
% \end{figure}

To achieve high fidelity while preserving diversity in image generation w.r.t~multimodal context, we introduce \method, the first diffusion-based general-purpose image generator designed to produce high-fidelity images guided by a multimodal context while maintaining diversity. Given a multimodal context, containing a reference image and accompanying text guidance, \method generates highly diverse images that differ from the reference image while accurately preserving the scene attributes referenced in the text guidance. 
% This enables \method to maintain consistency with the reference image's specified attributes, even as it introduces diversity in the generated outputs.

 
\method contains a novel Multimodal Context Evaluator that simultaneously maximizes our formulated Global Semantic and Fine-grained Consistency Rewards. Given the multimodal context input, \method crafts an instruction prompt to provide to a Multimodal Large Language Model~(MLLM)~\citep{liu2024improved, chen2024internvl} to obtain a text-based Context Description containing the necessary scene attributes to focus on for image generation. \method then finetunes the SDXL diffusion model using the rewards from the Multimodal Context Evaluator based on the alignment between the generated image and the MLLM Context Description. While SDXL enables generating images with high diversity (i.e.~visually different from the reference image), the rewards encourage the fine-tuning process to align the generated image closely with the scene attributes provided in the multimodal context to achieve high fidelity. Unlike existing methods, \method is able to balance both diversity and fidelity in generated images which is a prerequisite for leveraging synthetic data for scene-aware tasks like VQA and HOI Reasoning. Figure~\ref{fig:teaser}~(a-f) illustrates how, given text guidance~(a question) and a reference image, the generated image remains visually diverse w.r.t~reference image while preserving the specified scene attributes, thus ensuring that the answer to the question in the text guidance remains consistent between the reference and generated images.
 
As the first approach to address the task of multimodal context-aware image generation with high fidelity and diversity, we also introduce a new benchmark formulation to evaluate different methods on this task. Our benchmark leverages the MME Perception benchmark \citep{fu2024mme} to perform Test-Time Augmentation (TTA) \citep{shanmugam2021better,kim2020learning} with real and generated synthetic images to evaluate the ability of a method to maintain fidelity on scene attributes related to spatial existence, count, position, color, and scene. We further leverage Bongard Human-Object Interaction (HOI) \citep{jiang2022bongard} dataset to perform Test-time Prompt Tuning (TPT) \citep{shu2022testtime} to test a method’s ability to maintain fidelity when focusing on sophisticated human-object interactions. Finally, we compute a method’s ability to maintain diversity using feature-based distance metrics between the reference and generated images. Experiments show that \method is able to outperform all existing methods on MME Perception and Bongard HOI (i.e. provide the best fidelity) while achieving high diversity. \method also outperforms all other methods consistently on ImageNet and its OOD variants. This validates its effectiveness on large and diverse datasets from both scene-aware and object-centric tasks. Figure~\ref{fig:teaser}~(g) demonstrates that \method is able to preserve diversity and fidelity without the trade-off exhibited by existing methods.
 
Our contributions are as follows:
\begin{itemize}
    \item We introduce \method, the first diffusion-based image generator to synthesize high fidelity images guided by multimodal context of reference image and text guidance while maintaining high diversity.
    \item We develop a novel Multimodal Context Evaluator that simultaneously maximizes our formulated Global Semantic and Fine-grained Consistency Rewards during end-to-end fine-tuning. This enables the diffusion model to focus on both local and global scene attributes. 
    % the local entities while maintaining the global context.
   \item \method outperforms existing methods on MME Perception and Bongard HOI datasets as part of a newly formulated benchmark that evaluates the ability to generate images with both high fidelity and high diversity given a multimodal context.
\end{itemize}
% \section{Introduction}

% Diffusion models have emerged as powerful tools for image generation, offering impressive capabilities in creating high-quality and diverse synthetic data. However, a significant challenge remains unresolved in the realm of multimodal context-aware image generation: the ability to generate new images that not only maintain diversity but also preserve the specific context of a reference image. In this paper, we define context as various key scene elements, including numerical attributes (e.g., object count), physical properties (e.g., color), spatial relationships (e.g., relative positions of objects), object interactions (e.g., how objects interact with each other), or visual attributes (e.g., scene-level attributes). \textcolor{blue}{We refer to a specific context through a caption or question, which we term the ``text guidance", that highlights the key elements to focus on.} Contextual fidelity requires that the generated images accurately reflect these elements without significant deviation from the original reference image. \textcolor{blue}{Figure~\ref{fig:teaser} demonstrates the ability of our method to generate diverse images that faithfully preserve the contextual elements defined by the text guidance in relation to the provided reference images.} Generating such contextually accurate \textcolor{blue}{and diverse} synthetic data is crucial for downstream tasks like Visual Question Answering (VQA) or Human-Object Interaction (HOI) Reasoning, \textcolor{blue}{ensuring that models can leverage robust features, improving performance, generalization, and adaptability to complex real-world scenarios where data may be limited or costly to obtain.}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/intro.pdf}
%     \caption{Comparison}
%     \label{fig:comparison}
% \end{figure}

% Despite these advancements, current methods remain inadequate for tasks that require both diversity and precise context preservation. SDXL struggles to fully translate complex contextual details from text to image. Techniques like Boomerang and image-to-image translation introduce noise that can obscure critical details, while Textual Inversion and DA-Fusion rely on multiple examples, limiting their effectiveness for single-image testing samples. Image Variation methods, though retaining high-level semantics, often lose finer details because image embeddings do not fully capture context, and the diffusion process adds noise that causes deviations. This gap is particularly problematic in tasks like VQA and HOI Reasoning, where understanding and maintaining detailed context is essential for model performance.

% In this paper, we propose a novel method called \method for multimodal context-aware image generation that effectively balances diversity and contextual fidelity. Our approach integrates image embeddings to maintain high-level semantic features while employing a Multimodal Large Language Model (MLLM) as a context descriptor to guide the model on which specific context to focus. The pre-trained SDXL model still struggles to generate images with accurate contextual fidelity. To address this, we further introduce an end-to-end fine-tuning framework that enhances the SDXL model's ability to improve contextual fidelity while maintaining diversity.

% Our \method fine-tunes the SDXL model by incorporating our defined context rewards derived from global cosine similarity and local image-text matching scores between generated images and MLLM-generated descriptions. We propose to leverage multimodal representation connector in pre-trained VLMs as a context evaluator which assesses the context preservation of generated images. This fine-tuning process aligns the model's outputs more closely with the desired contextual elements, ensuring the generated images retain both the diversity and specific context of the reference images.

% We evaluate our approach on several benchmarks, including the MME Perception benchmark, which applies Test-Time Augmentation (TTA) strategy with real and synthetic images for multimodal context-aware tasks such as existence, count, position, color, and scene. Additionally, we perform Test-time Prompt Tuning (TPT) on the Bongard Human-Object Interaction (HOI) dataset with both real and synthetic data, demonstrating significant improvements in contextual fidelity and model performance.

% Our contributions are as follows:
% \begin{itemize}
%     \item We propose the first multimodal context-aware image generation method \method that effectively balances diversity and contextual fidelity in generated images, allowing it to go beyond object-centric generation to unconstrained images
%     \item We develop a novel end-to-end fine-tuning framework that integrates MLLM as a context descriptor and multimodal representation connector in pre-trained VLMs as a context evaluator, to maximize cosine similarity and image-text matching rewards, enhancing SDXL's ability to generate images that closely align with specific contextual descriptions.
%     \item  Our method is validated through extensive experiments on the MME Perception benchmark and the Bongard HOI dataset, demonstrating significant improvements in tasks such as existence, count, position, color, scene, and HOI reasoning. 
% \end{itemize}


