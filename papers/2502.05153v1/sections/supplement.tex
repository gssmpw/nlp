\appendix
\section*{Appendix}
\section{Discussion: Scope and Ethics}
\label{appendix:scope}
In this work, we evaluate our method on six core scene-aware tasks: existence, count, position, color, scene, and HOI reasoning. We select these tasks as they represent core aspects of multimodal understanding which are essential for many applications. Meanwhile, we do not extend our evaluation to more complex reasoning tasks, such as numerical calculations or code generation, because SOTA diffusion models like SDXL are not yet capable of handling these tasks effectively. Fine-tuning alone cannot overcome the fundamental limitations of these models in generating images that require symbolic logic or complex reasoning. Additionally, we avoid tasks with ethical concerns, such as generating images of specific individuals (e.g., for celebrity recognition task), to mitigate risks related to privacy and misuse. Our goal was to ensure that our approach focuses on technically feasible and responsible AI applications. Expanding to other tasks will require significant advancements in diffusion model capabilities and careful consideration of ethical implications.

\section{Limitations and Future Work}
While our Multimodal Context Evaluator proves effective in enhancing the fidelity of generated images and maintaining diversity, \method is built using pre-trained diffusion models such as SDXL and MLLMs like LLaVA, it inherently shares the limitations of these foundation models. \method still faces challenges with complex reasoning tasks such as numerical calculations or code generation due to the symbolic logic limitations inherent to SDXL. Additionally, during inference, the MLLM context descriptor occasionally generates incorrect information or ambiguous descriptions initially, which can lead to lower fidelity in the generated images. Figure~\ref{fig:failure} further illustrates these observations.

\method currently focuses on single attributes like count, position, and color as part of the multimodal context. This is because this task alone poses significant challenges to existing methods, which \method effectively addresses. A potential direction for future work is to broaden the applicability of \method to synthesize images with multiple scene attributes in the multimodal context as part of compositional reasoning tasks.


\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/failures.pdf}
    \caption{Failure cases of \method. (a) Our method fails due to the symbolic logic limitation of existing pre-trained SDXL. (b) Initially incorrect descriptions generated by MLLMs lead to low fidelity of generated images. (c) Context description generated by MLLMs is ambiguous and does not directly relate to the text guidance, the spoon can be both inside or outside the bowl.}
    \label{fig:failure}
\end{figure}

\section{Prompt Templates}
\label{appendix:prompts}
Figure~\ref{fig:prompt_templates}~(a-c) showcases the prompt templates used by \method to fine-tune diffusion models specifically on each task including VQA, HOI Reasoning, and Object-Centric benchmarks. It's worth noting that we designed the prompt such that it provides detailed instruction to MLLMs on which scene attributes to focus. We also evaluate the effectiveness of our designed prompt templates by fine-tuning \method with a generic prompt as illustrated in Figure~\ref{fig:prompt_templates}~(d). Table~\ref{table:prommpt} indicates that without using our designed prompt template, the MLLM is not properly instructed to generate specific context description thus leading to reduced performance after fine-tuning on MME tasks. We believe that when using a generic prompt, MLLM is not able to receive sufficient grounding about the multimodal context leading to information loss on key scene attributes.
\input{tables/prompt_template}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/prompt_template.pdf}
    \caption{Prompt templates (a-c) used by \method to fine-tune the diffusion model on each task including VQA, HOI Reasoning, and Object Centric benchmarks. The generic prompt (d) is also included to evaluate the effectiveness of prompt template.}
    \label{fig:prompt_templates}
\end{figure}
\section{Inference Pipeline}
\label{appendix:inference}
In the inference pipeline of \method (Figure~\ref{fig:inference}), the text guidance $\mathbf{g}$ includes only the question corresponding to the reference image $\mathbf{x}$. The answer is excluded for fair evaluation. Moreover, we remove Multimodal Context Evaluator, and the generated image $\hat{\mathbf{x}}$ is the final output.
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/inference.pdf}
    \caption{Inference pipeline of \method}
    \label{fig:inference}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/diversity_compact_caption.pdf}
    \vspace{-5mm}
    \caption{Examples of context description from MLLM in the inference pipeline where answers are not included in text guidance.}
    \label{fig:diversity_compact_caption}
\end{figure}



\section{Ablation Study on BLIP-2 QFormer}
Our design choice to leverage BLIP-2 QFormer in \method as the multimodal context evaluator facilitates the formulation of our novel Global Semantic and Fine-grained Consistency Rewards. These rewards enable \method to be effective across all tasks as seen in Table~\ref{table:clip}. On replace with a less powerful multimodal context encoder such as CLIP ViT-G/14, we can only implement the global semantic reward as the cosine similarity between the text features and generated image features. As a result, while the setting can maintain performance on coarse-level tasks such as Scene and Existence, there is a noticeable decline on fine-grained tasks like Count and Position. This demonstrates the effectiveness of our design choices in \method and shows that using less powerful alternatives, without the ability to provide both global and fine-grained alignment, affects the fidelity of generated images.

\input{tables/clip}

\section{Additional Evaluation on MME Artwork}

To explore the method's ability to work on tasks involving more nuanced or abstract text guidance beyond factual scene attributes, we evaluate \method on an additional task of MME Artwork. This task focuses on image style attributes that are more nuanced/abstract such as the following question-answer pair -- Question: ``Does this artwork exist in the form of mosaic?'', Answer: ``No''.

Table~\ref{table:artwork_reasoning} summarizes the evaluation. We can observe that \method outperforms all existing methods on both ACC and ACC+, implying its higher effectiveness in generating images with high fidelity (in this case, image style preservation) compared to existing methods. This provides evidence that \method can generalize to tasks involving abstract/nuanced attributes such as image style. Figure~\ref{fig:artwork} further shows qualitative comparison between image generation methods on the MME Artwork task.

\input{tables/artwork_reasoning}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/artwork.pdf}
    \caption{Qualitative comparison on the Artwork task between image generation method. \method can preserve both diversity and fidelity of the reference image in a more abstract domain.}
    \label{fig:artwork}
\end{figure}


\section{Additional Evaluation on MME Commonsense Reasoning}
We have additionally performed our evaluation to more complex tasks such as Visual Reasoning using the MME Commonsense Reasoning benchmark. Results in Table~\ref{table:artwork_reasoning} highlight \method's ability to generalize effectively across diverse domains and complex reasoning tasks, demonstrating its broader applicability. Figure~\ref{fig:reasoning} further shows qualitative comparison between image generation methods on the MME Commonsense Reasoning task.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/reasoning.pdf}
    \caption{Qualitative comparison on the Commonsense Reasoning task between image generation method. \method can preserve both diversity and fidelity of the reference image in a more abstract domain.}
    \label{fig:reasoning}
\end{figure}
\section{FID Scores}
% \textcolor{blue}{We compute FID scores of traditional augmentation and image generation methods. Table~\ref{table:fid} shows that the data distribution of generated images by RandAugment and Image Translation are closer to the real distribution as these methods only change images minimally. We also want to emphasize that even though the FID metric evaluates the quality of generated images, it can not measure the diversity of generated images. \method with rewards fine-tuning achieves a competitive score. As we showed in the diversity analysis in Table~\ref{table:diversity} in the main paper, \method performs significantly better than these ``minimal change" methods while still achieving a competitive FID score. We believe this is a worldwide trade-off.}

We compute FID scores for \method and the different baselines (traditional augmentation and image generation methods) and tabulate the numbers in Table~\ref{table:fid}. FID is a valuable metric for assessing the quality of generated images and how closely the distribution of generated images matches the real distribution. However, \textit{FID does not account for the diversity among the generated images}, which is a critical aspect of the task our work targets~(i.e., how can we generate high fidelity images, preserving certain scene attributes, while still maintaining high diversity?). We also illustrate the shortcomings of FID for the task in Figure~\ref{fig:fid_diversity} where we compare generated images across methods. We observe that RandAugment and Image Translation achieve lower FID scores than \method~(w/ finetuning) because they compromise on diversity by only minimally changing the input image, allowing their generated image distribution to be much closer to the real distribution. While \method has a higher FID score than RandAugment and Image Translation, Figure~\ref{fig:fid_diversity} shows that it is able to preserve the scene attribute w.r.t.~multimodal context while generating an image that is significantly different from than original input image. Therefore, it accomplishes the targeted task more effectively, with both high fidelity and high diversity.

\input{tables/FID}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/fid_diversity.pdf}
    \caption{While RandAugment and Image Translation achieve lower FID scores, \method balances fidelity and diversity effectively.}
    \label{fig:fid_diversity}
\end{figure}

\section{User Study}
% \textcolor{blue}{We created a survey form with 50 questions (10 questions per MME task). In each survey question, users were shown: a reference image, a related question, and two generated images from different methods (I2T2I SDXL vs. \method). Users are asked to select the generated image(s) that preserve the attribute referred to by the question in relation to reference image. We collected form responses from 70 people. Table~\ref{table:user_study} shows that \method significantly outperforms I2T2I SDXL in terms of fidelity across all tasks on MME benchmark. We have some examples of survey questions in Figure~\ref{fig:user_study_examples}.}

We conduct a user study where we create a survey form with 50 questions (10 questions per MME Perception task). In each survey question, we show users a reference image, a related question, and a generated image each from two different methods (baseline I2T2I SDXL vs \method). We ask users to select the generated images(s) (either one or both or neither of them) that preserve the attribute referred to by the question in relation to the reference image. If an image is selected, it denotes high fidelity in generation. We collect form responses from 70 people for this study. We compute the percentage of total generated images for each method that were selected by the users as a measure of fidelity. Table~\ref{table:user_study} summarizes the results and shows that \method significantly outperforms I2T2I SDXL in terms of fidelity across all tasks on the MME Perception benchmark. We have some examples of survey questions in Figure~\ref{fig:user_study_examples}.

\input{tables/user_study}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/user_study_examples.pdf}
    \caption{Some examples of our survey questions to evaluate the fidelity of generated images from I2T2I SDXL and \method.}
    \label{fig:user_study_examples}
\end{figure}
\section{Training Performance on Bongard HOI Dataset}
% \textcolor{blue}{We conducted an additional experiment by training a CNN baseline ResNet50 \citep{he2016deep} model on the Bongard-HOI training set with traditional augmentation and other image generation methods, using the same number of training iterations. As shown in Table~\ref{table:hoi_training}, \method consistently outperforms other methods across all test splits. However, as discussed in Subsection~\ref{sec:benchmark_formulation}, our primary focus on test-time evaluation ensures fair comparisons by avoiding variability in training behavior caused by differences in model architectures, data distributions, and training configurations.}

Following the existing method \citep{shu2022testtime}, we conduct an additional experiment by training a ResNet50 \citep{he2016deep} model on the Bongard-HOI \citep{jiang2022bongard} training set with traditional augmentation and Hummingbird generated images. We compare the performance with other image generation methods, using the same
number of training iterations. As shown in Table~\ref{table:hoi_training}, \method consistently outperforms all the baselines across all test splits. In the paper, as discussed in Section~\ref{sec:benchmark_formulation}, we focus primarily on test-time evaluation because it eliminates the variability introduced by model training due to multiple external variables such as model architecture, data distribution, and training configurations, and allows for a fairer comparison where the evaluation setup remains fixed.
\input{tables/HOI_training}


\section{Random Seeds Selection Analysis}
We conduct an additional experiment, varying the number of random seeds from $10$ to $100$. The results are presented in the boxplot in Figure~\ref{fig:boxplot}, which shows the distribution of the mean L2 distances of generated image features from Hummingbird across different numbers of seeds.


The figure demonstrates that the difference in the distribution of the diversity (L2) scores across the different numbers of random seeds is statistically insignificant. So while it is helpful to increase the number of seeds for improved confidence, we observe that it stabilizes at 20 random seeds. This analysis suggests that using $20$ random seeds also suffices to capture the diversity of generated images without significantly affecting the robustness of the analysis.

% We conduct an additional experiment where we vary the number of seeds from 10 to 100. We present the results as a boxplot in Appendix K, Figure 15 which shows the distribution of the mean L2 distances of generated image features from Hummingbird across different numbers of seeds.

% The figure demonstrates that the difference in the distribution of the diversity (L2) scores across the different numbers of random seeds is statistically insignificant. So while it is helpful to increase the number of seeds for improved confidence, we observe that it stabilizes at 20 random seeds. This analysis suggests that using 20 random seeds also suffices to capture the diversity of generated images without significantly affecting the robustness of the analysis.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/diversity_boxplot_rectangular.pdf}
    \caption{Diversity analysis across varying numbers of random seeds (10 to 100) using mean L2 distances of generated image features from \method. The box plot demonstrates consistent diversity scores as the number of seeds increases, indicating that performance stabilizes around 20 random seeds.}
    \label{fig:boxplot}
\end{figure}

\section{Further Explanation of Multimodal Context Evaluator}
The Global Semantic Reward, \(\mathcal{R}_\textrm{global}\), ensures alignment between the global semantic features of the generated image \(\mathbf{\hat{x}}\) and the textual context description \(\mathcal{C}\). This reward leverages cosine similarity to measure the directional alignment between two feature vectors, which can be interpreted as maximizing the mutual information \(I(\mathbf{\hat{x}}, \mathcal{C})\) between the generated image \(\mathbf{\hat{x}}\) and the context description \(\mathcal{C}\). Mutual information quantifies the dependency between the joint distribution \(p_{\theta}(\mathbf{\hat{x}}, \mathcal{C})\) and the marginal distributions. In conditional diffusion models, the likelihood \(p_{\theta}(\mathbf{\hat{x}} \vert \mathcal{C})\) of generating \(\mathbf{\hat{x}}\) given \(\mathcal{C}\) is proportional to the joint distribution:
\[
p_{\theta}(\mathbf{\hat{x}} \vert \mathcal{C}) = \frac{p_{\theta}(\mathbf{\hat{x}}, \mathcal{C})}{p(\mathcal{C})} \propto p_{\theta}(\mathbf{\hat{x}}, \mathcal{C}),
\]
where \(p(\mathcal{C})\) is the marginal probability of the context description, treated as a constant during optimization. By maximizing \(\mathcal{R}_\textrm{global}\), which aligns global semantic features, the model indirectly maximizes the mutual information \(I(\mathbf{\hat{x}}, \mathcal{C})\), thereby enhancing the likelihood \(p_{\theta}(\mathbf{\hat{x}} \vert \mathcal{C})\) in the conditional diffusion model.


The Fine-Grained Consistency Reward, $\mathcal{R}_{\textrm{fine-grained}}$, captures detailed multimodal alignment between the generated image $\mathbf{\hat{x}}$ and the textual context description $\mathcal{C}$. It operates at a token level, leveraging bidirectional self-attention and cross-attention mechanisms in the BLIP-2 QFormer, followed by the Image-Text Matching (ITM) classifier to maximize the alignment score.

\textbf{Self-Attention on Text Tokens:}
    Text tokens $\mathcal{T}_{\mathrm{tokens}}$ undergo self-attention, allowing interactions among words to capture intra-text dependencies:
    \begin{equation}
        \mathcal{T}_{\mathrm{attn}} = \tt{SelfAttention}(\mathcal{T}_{\mathrm{tokens}})
    \end{equation}

\textbf{Self-Attention on Image Tokens:}
    Image tokens $\mathcal{Z}$ are derived from visual features of the generated image $\mathbf{\hat{x}}$ using a cross-attention mechanism:
    \begin{equation}
        \mathcal{Z} = \tt{CrossAttention}(\mathcal{Q}_{\mathrm{learned}}, \mathcal{I}_{\mathrm{tokens}}(\mathbf{\hat{x}}))
    \end{equation}
    These tokens then pass through self-attention to extract intra-image relationships:
    \begin{equation}
        \mathcal{Z}_{\mathrm{attn}} = \tt{SelfAttention}(\mathcal{Z})
    \end{equation}

\textbf{Cross-Attention between Text and Image Tokens:}
    The text tokens $\mathcal{T}_{\mathrm{attn}}$ and image tokens $\mathcal{Z}_{\mathrm{attn}}$ interact through cross-attention to focus on multimodal correlations:
    \begin{equation}
        \mathcal{F} = \tt{CrossAttention}(\mathcal{T}_{\mathrm{attn}}, \mathcal{Z}_{\mathrm{attn}})
    \end{equation}

\textbf{ITM Classifier for Alignment:}
    The resulting multimodal features $\mathcal{F}$ are fed into the ITM classifier, which outputs two logits: one for positive match ($j=1$) and one for negative match ($j=0$). The positive class ($j=1$) indicates strong alignment between the image-text pair, while the negative class ($j=0$) indicates misalignment:
    \begin{equation}
        \mathcal{R}_{\textrm{fine-grained}} = \tt{ITM\_Classifier}(\mathcal{F})_{j=1}
    \end{equation}

The ITM classifier predicts whether the generated image and the textual context description match. Maximizing the logit for the positive class $j=1$ corresponds to maximizing the log probability $\log p(\mathbf{\hat{x}}, \mathcal{C})$ of the joint distribution of image and text. This process aligns the fine-grained details in $\mathbf{\hat{x}}$ with $\mathcal{C}$, increasing the mutual information between the generated image and the text features.

\textbf{Improving fine-grained relationships of CLIP.} While the CLIP Text Encoder, at times, struggles to accurately capture spatial features when processing longer sentences in the Multimodal Context Description, \method addresses this limitation by distilling the global semantic and fine-grained semantic rewards from BLIP-2 QFormer into a specific set of UNet denoiser layers, as mentioned in the implementation details under Appendix~\ref{appendix:impl}~(i.e., Q, V transformation layers including $\tt{to\_q, to\_v, query, value}$). This strengthens the alignment between the generated image tokens~(Q) and input text tokens from the Multimodal Context Description~(K, V) in the cross-attention mechanism of the UNet denoiser. As a result, we obtain generated images with improved fidelity, particularly w.r.t.~spatial relationships, thereby helping to mitigate the shortcomings of vanilla CLIP Text Encoder in processing the long sentences of the Multimodal Context Description.

To illustrate further, a Context Description like “the dog under the pool” is processed in three steps: (1) self-attention is applied to the text tokens (K, V), enabling spatial terms like “dog,” “under,” and “pool” to interact; (2) self-attention is applied to visual features represented by the generated image tokens (Q) to extract intra-image relationships (3) cross-attention aligns this text features with visual features. The resulting alignment scores are used to compute the mean and select the positive class for the reward. Our approach to distill this reward into the cross-attention layers therefore ensures that spatial relationships and other fine-grained attributes are effectively captured, improving the fidelity of generated images.


\section{The Choice of Text Encoder in SDXL and BLIP-2 QFormer}

The choice of text encoder in our pipeline is to leverage pre-trained models for their respective strengths. SDXL inherently uses the CLIP Text Encoder for its generative pipeline, as it is designed to process text embeddings aligned with the CLIP Image Encoder. In the Multimodal Context Evaluator, we use the BLIP-2 QFormer, which is pre-trained with a BERT-based text encoder.

\section{Textual Inversion for Data Augmentation}
In our experiments, we applied Textual Inversion for data augmentation as follows: given a reference image, Textual Inversion learns a new text embedding that captures the context of the reference image (denoted as $<$context$>$). This embedding is then used to generate multiple augmented images by employing the prompt: ``a photo of $<$context$>$". This approach allows Textual Inversion to create context-relevant augmentations for comparison in our experiments.

\section{Convergence Curve}
To evaluate convergence, we monitor the training process using the Global Semantic Reward and Fine-Grained Consistency Reward as criteria. Specifically, we observe the stabilization of these rewards over training iterations. Figure~\ref{fig:convergence} presents the convergence curves for both rewards, illustrating their gradual increase followed by stabilization around 50k iterations. This steady state indicates that the model has learned to effectively align the generated images with the multimodal context.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/convergence.pdf}
    \caption{Convergence curves of Global Semantic and Fine-Grained Consistency Rewards}
    \label{fig:convergence}
\end{figure}


\section{Fidelity Evaluation using GPT-4o}
In addition to the results above, we compute additional metrics for fidelity, which measure how well the model preserves scene attributes when generating new images from a reference image. For this, we use GPT-4o (model version: 2024-05-13) as the MLLM oracle for a VQA task on the MME Perception benchmark \citep{fu2024mme}. 
% We use a MLLM as an oracle for a visual question-answering (VQA) task on the MME Perception benchmark \citep{fu2024mme}. In this experiment, we use GPT-4o (model version: 2024-05-13) as the oracle. 
We evaluate \method with and without fine-tuning process.

The MME dataset consists of Yes/No questions, with a positive and a negative question for every reference image. To measure fidelity, we measure the rate at which the oracle's answer remains consistent across the reference and the generated image for every image in the dataset. We run the experiment multiple times and report the average numbers in Table~\ref{table:fidelity_comparison}. We see that fine-tuning the base SDXL with our novel rewards results in an average increase of $2.99\%$ in fidelity.

\input{tables/chatgpt4o}


\section{Implementation Details}
\label{appendix:impl}
We implement \method using PyTorch \citep{paszke2019pytorch} and HuggingFace diffusers \citep{huggingface2023diffusers} libraries. For the generative model, we utilize the SDXL Base $1.0$ which is a standard and commonly used pre-trained diffusion model in natural images domain. In the pipeline, we employ CLIP ViT-G/14 as image encoder and both CLIP-L/14 \& CLIP-G/14 as text encoders \citep{radford2021learning}. We perform LoRA fine-tuning on the following modules of SDXL UNet denoiser including $Q$, $V$ transformation layers, fully-connected layers ($\tt{to\_q, to\_v, query, value, ff.net.0.proj}$) with rank parameter $r = 8$, which results in $11$M trainable parameters $\approx 0.46\%$ of total $2.6$B parameters. The fine-tuning is done on $8$ NVIDIA A100 80GB GPUs using AdamW \citep{loshchilov2017decoupled} optimizer, a learning rate of \texttt{5e-6}, and gradient accumulation steps of $8$.

\section{Additional Qualitative Results}
\label{appendix:visuals}
Figure~\ref{fig:diversity_compact_caption} showcases two examples of context description from MLLM in the inference pipeline where answers are not included in text guidance. Figure~\ref{fig:diversity_full} illustrates additional qualitative results highlighting the diversity and multimodal context fidelity between reference and synthetic images, as well as across images generated by \method with different random seeds. Figure~\ref{fig:qualitative_full} shows additional qualitative comparisons between \method and SOTA image generation methods on VQA and HOI Reasoning tasks.
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/diversity_full.pdf}
    \vspace{-5mm}
    \caption{Diversity and multimodal context fidelity between reference and synthetic image and across generated ones from \method with different random seeds.}
    \label{fig:diversity_full}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/qualitative_full_v1.pdf}
    \vspace{-5mm}
    \caption{Qualitative comparison between \method and other image generation methods on MME Perception and HOI Reasoning benchmarks.}
    \label{fig:qualitative_full}
\end{figure}