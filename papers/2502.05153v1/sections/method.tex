\vspace{-8mm}
\section{Method}

% The proposed end-to-end \method is designed to generate contextually faithful images while retaining diversity in the output. This method leverages SDXL as the generative backbone, enhanced with context descriptors derived from an MLLM to ensure the generated images align closely with the contextual details of the input that we focus on. Our approach integrates both image embeddings and detailed context descriptions to both maintain the high-level semantic features and guide the generation process. In addition, we propose to leverage pre-trained BLIP-2 QFormer as a context evaluator and further fine-tune the SDXL model using our context rewards, since diffusion models often struggle to retain fine-grained contextual details.

% The proposed end-to-end \method is designed to generate contextually faithful images while preserving diversity in the output. Built around the SDXL generative backbone, it is further controlled by context descriptors derived from an MLLM to ensure the generated images remain closely aligned with the essential contextual details of the input. This approach seamlessly integrates image embeddings with detailed context descriptions to maintain high-level semantic features and guide the generation process.

% As a key innovation, \method employs the multimodal representation connector in pre-trained vision-language models (VLMs) as a context evaluator. By fine-tuning the SDXL model using our custom-designed rewards from the evaluator, our method addresses the common issue that diffusion models face: the inability to retain contextual details. These rewards ensure that generated images not only reflect high-level semantic coherence but also capture the intricate details required for contextual fidelity.

% The proposed end-to-end \method is designed to generate contextually faithful images while preserving diversity in the output. Built around the SDXL generative backbone, it is further controlled by the detailed context descriptions derived from an MLLM. Using the descriptions \method generates a set of diverse and contextually aligned images from a reference image. The core innovation of the model is its context evaluator. This VLM-based evaluator fine-tunes the SDXL in an efficient manner with two novel rewards. These rewards ensure that generated images not only reflect high-level semantic coherence but also capture the intricate details required for contextual fidelity. As a result, the model makes a significant leap toward generating contextually aligned images, an area where current diffusion models have struggled to succeed.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/framework-v5.pdf}
    \vspace{-8mm}
    \caption{\textbf{Model Overview}: Given text guidance $\mathbf{g}$ and reference image $\mathbf{x}$~(multimodal context~$\mathcal{M}$), \method crafts an instruction prompt $p$ to feed to MLLM and obtain Context Description $\mathcal{C}$. It them embeds $\mathbf{x}$ and $\mathcal{C}$ via CLIP to feed to UNet Denoiser of SDXL to generate image $\mathbf{\hat{x}}$. To improve the fidelity of $\mathbf{\hat{x}}$ w.r.t.~$\mathcal{M}$ while preserving diversity, \method introduced Multimodal Context Evaluator to simultaneously maximize novel rewards -- Global Semantic and Fine-Grained Consistency Rewards -- to align $\mathbf{\hat{x}}$ with scene attributes provided in $\mathcal{M}$.}
    \label{fig:overview}
    \vspace{-5mm}
\end{figure}



\subsection{Model Overview}
% As depicted in Figure~\ref{fig:overview}, the input to \method begins with a reference image, which is encoded using a CLIP image encoder to extract high-level semantic features and generate the image embedding $\mathcal{I}_\mathbf{e}$. Simultaneously, an MLLM receives a structured instruction prompt to describe the image’s context. During training, both the question and answer are injected into the instruction prompts to ensure the diffusion model is fine-tuned with accurate and high-quality data. However, during inference, the answer is not included in the prompt to prevent any leakage of ground truth answers, ensuring a fair and unbiased evaluation of the model’s performance. The prompt is designed to capture critical context elements, such as spatial relationships, object interactions, or specific attributes. This detailed context description is then passed to a CLIP text encoder, which converts it into a text embedding $\mathcal{T}_\mathbf{e}$. 


% Both the image embedding $\mathcal{I}_\mathbf{e}$ and text embedding $\mathcal{T}_\mathbf{e}$ are fed into the SDXL model to generate a diverse set of images that are expected to align with the original contextual details of the reference image. To further ensure that the generated images maintain contextual fidelity, \method employs a multimodal representation connector in pre-trained VLMs, which acts as a context evaluator. This component calculates two key rewards: the Global Semantic Reward, which captures high-level alignment between the image and text embeddings, and the Fine-Grained Consistency Reward, which ensures the preservation of more detailed contextual elements.

% The \method achieves a cohesive training and generation process, where all components work in concert. The Multimodal Connector remains frozen during training to retain the powerful pre-trained capabilities of VLMs, which provides robust contextual evaluation. The fine-tuning is restricted to the SDXL model using Low-Rank Adaptation (LoRA), while other encoders remain frozen during training for computational efficiency.

Figure~\ref{fig:overview} provides an overview of the end-to-end training setup of \method. Let $\mathcal{M} = \{\mathbf{x}, \mathbf{g}\}$ denote the multimodal context fed as input to \method. $\mathcal{M}$ comprises of the reference image $\mathbf{x}$ and text guidance $\mathbf{g}$ for \method to generate image $\mathbf{\hat{x}}$ that is visually diverse w.r.t.~$\mathbf{x}$ while faithfully preserves the scene attributes in relation to $\mathbf{g}$. Depending on the task, $\mathbf{g}$ can be a question or a set of annotations accompanying $\mathbf{x}$. During training, $\mathbf{g}$ additionally consists of the ground truth (such as answer to the question or correct annotation) which is unavailable during evaluation.  

We feed the multimodal context $\mathcal{M}$ to MLLM to generate a text-based Context Description, $\mathcal{C}$. For this, we need to provide $\mathcal{M}$ as an instruction prompt to MLLM such that it guides the MLLM on which scene attributes to focus on in the reference image $\mathbf{x}$ in relation to the text guidance $\mathbf{g}$. We devise a prompt template $p:\mathbf{g} \rightarrow \mathcal{P}$ to transform $\mathbf{g} \in \mathcal{M}$ into an instruction prompt $\mathcal{P}$. We then feed $\mathcal{P}$ along with $\mathbf{x}$ as the multimodal instruction prompt to the MLLM to obtain $\mathcal{C}$. Figure ~\ref{fig:overview} provides a sample of one such prompt template. Please refer Appendix~\ref{appendix:prompts} for more samples.

$\mathcal{C}$ provides a detailed description of $\mathbf{x}$, capturing the scene attributes specified by $\mathbf{g}$, such as object relationships, their interactions, or other spatial attributes. We pass $\mathcal{C}$ through a CLIP text encoder \citep{radford2021learning} to convert it into a text embedding $\mathcal{T}_\mathbf{e}$. In parallel, we also encode $\mathbf{x}$ using a CLIP image encoder 
% \citep{radford2021learning} 
to generate the image embedding $\mathcal{I}_\mathbf{e}$ comprising the semantic features of $\mathbf{x}$.

We feed the embeddings $\mathcal{I}_\mathbf{e}$ and $\mathcal{T}_\mathbf{e}$ to the UNet denoiser $\boldsymbol{\epsilon}_\theta$ of SDXL \citep{podell2023sdxl} to generate image $\mathbf{\hat{x}}$. While $\mathbf{\hat{x}}$ exhibits diversity (i.e.~visually different from $\mathbf{x}$), we fine-tune the denoiser via LoRA \citep{hu2021lora} to enhance the fidelity of $\mathbf{\hat{x}}$ (i.e.~preserve the scene attributes occurring in $\mathbf{x}$ in relation to $\mathbf{g}$). To facilitate the fine-tuning, we introduce Multimodal Context Evaluator which simultaneously maximizes our two novel rewards -- Global Semantic and Fine-Grained Consistency Rewards -- to align $\mathbf{\hat{x}}$ with the scene attributes provided in the multimodal context~$\mathcal{M}$~(Figure~\ref{fig:qformer}). 


% ------ OLD ------

% % As depicted in Figure~\ref{fig:overview}, the input to \method begins with a reference image and a prompt template containing a question/caption about the image to create an instruction prompt to guide the MLLM which context to focus on. The reference image is encoded using a CLIP image encoder to generate the image embedding $\mathcal{I}_\mathbf{e}$. $\mathcal{I}_\mathbf{e}$ contains high-level semantic features. An MLLM processes the instruction prompt to generate a detailed description of the image. This description captures critical context elements, such as spatial relationships, object interactions, or specific attributes. The description is then passed to a CLIP text encoder, which converts it into a text embedding $\mathcal{T}_\mathbf{e}$.

% Both the image embedding $\mathcal{I}_\mathbf{e}$ and text embedding $\mathcal{T}_\mathbf{e}$ are fed into the SDXL model to generate a diverse set of images that are contextually aligned with the reference image. Our \method incorporates a multi-modal context evaluator for achieving this alignment. The evaluator assesses two novel rewards: the Global Semantic Reward, which captures high-level alignment between the image and text embeddings, and the Fine-Grained Consistency Reward, which ensures the preservation of more detailed contextual elements.

% Other than the SDXL every other component of the \method remains frozen during training to reduce computational overhead. The fine-tuning of the SDXL is done using Low-Rank Adaptation (LoRA).

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/qformer-v5.pdf}
    \vspace{-4mm}
    \caption{\method's Multimodal Context Evaluator leverages pre-trained BLIP-2 QFormer. It simultaneously maximizes our novel Global Semantic and Fine-grained Consistency Rewards to align generated image $\mathbf{\hat{x}}$ with Content Description $\mathcal{C}$ corresponding to multimodal context $\mathcal{M}$.}
    \label{fig:qformer}
    \vspace{-5mm}
\end{figure}
\vspace{-1mm}
\subsection{Fine-tuning with Multimodal Context Rewards}
\textbf{Multimodal Context Evaluator.}~We design our Multimodal Context Evaluator using the multimodal representation connector in pre-trained vision-language models (VLMs). Specifically, we leverage the pre-trained QFormer from BLIP-2 \citep{li2023blip} due to its unique ability to capture both global and local alignment between image-text pairs. We use BLIP-2's image encoder to extract the image tokens $\mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})$ for the image $\mathbf{\hat{x}}$ generated by SDXL~(Figure~\ref{fig:qformer}). In parallel, we use BLIP-2's BERT-based \citep{kenton2019bert} text encoder to extract the text tokens $\mathcal{T}_{\mathrm{tokens}}$~(including the representative token $\mathcal{T}_{\texttt{[CLS]}}$) for the text-based Context Description $\mathcal{C}$ obtained from the MLLM. We then utilize the learned queries $\mathcal{Q}_\mathrm{learned}$ from the pre-trained BLIP-2 QFormer to extract the visual representation token sequence $\mathcal{Z}$ via the Cross-Attention layer as,
\begin{equation}
    \mathcal{Z} = \tt{CrossAttention}\left(\mathcal{Q}_\mathrm{learned}, \mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})\right)
\label{eq:query_output}
\end{equation}

% \textbf{Multimodal Context Evaluator.} We employ the multimodal representation connector in pre-trained vision-language models (VLMs) to design our Multimodal Context Evaluator. Specifically, we leverage the pre-trained QFormer from BLIP-2 \citep{li2023blip} due to its unique ability to capture both global and local alignment between image-text pairs. As illustrated in Figure~\ref{fig:qformer}, we use the BLIP-2 image encoder to extract the image tokens $\mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})$ for the image $\mathbf{\hat{x}}$ generated by SDXL. In parallel, we use the BERT-based \citep{kenton2019bert} text encoder of BLIP-2 to extract the text tokens $\mathcal{T}_{\mathrm{tokens}}$ from the text-based context description $\mathcal{C}$ obtained from the MLLM. $\mathcal{T}_{\mathrm{tokens}}$ include the representative text token $\mathcal{T}_{\texttt{[CLS]}}$ which we leverage for the Global Semantic Reward. We then utilize the learned queries $\mathcal{Q}_\mathrm{learned}$ from the pre-trained BLIP-2 QFormer to extract a sequence of visual representation tokens $\mathcal{Z}$ via the Cross-Attention layer as,
% \begin{equation}
%     \mathcal{Z} = \tt{CrossAttention}\left(\mathcal{Q}_\mathrm{learned}, \mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})\right)
% \label{eq:query_output}
% \end{equation}

\textbf{Global Semantic Reward.} Using the visual representation tokens $\mathcal{Z}$ and the representative text token $\mathcal{T}_{\texttt{[CLS]}}$, we compute cosine similarity between each visual token $\mathcal{Z}_i$ and $\mathcal{T}_{\texttt{[CLS]}}$ and define Global Semantic Reward, $\mathcal{R}_\textrm{global}$, as the maximum of these computed cosine similarities,
\begin{equation}
    \mathcal{R}_\textrm{global} =  \max_i \frac{\mathcal{Z}_i^T\cdot\mathcal{T}_{\texttt{[CLS]}}}{\Vert\mathcal{Z}_i\Vert\Vert\mathcal{T}_{\texttt{[CLS]}}\Vert}
\end{equation}
Through the above formulation, the Global Semantic Reward, $\mathcal{R}_\textrm{global}$, ensures the overall scene context in the generated image $\mathbf{\hat{x}}$ aligns with that in the textual context description $\mathcal{C}$ by maximizing their global feature similarity.
% $\mathcal{R}_\textrm{global}$ enables to maximize the mutual information between $\mathbf{\hat{x}}$ and $\mathcal{C}$. 

\textbf{Fine-Grained Consistency Reward.} While $\mathcal{R}_\textrm{global}$ allows to align the global semantics of the generated image $\mathbf{\hat{x}}$ with that of the textual context description $\mathcal{C}$, it is not sufficient to maintain optimal fidelity as SDXL can still omit capturing key scene attributes specified in $\mathbf{g}$. We therefore introduce Fine-Grained Consistency Reward, $\mathcal{R}_{\textrm{fine-grained}}$, that helps to complement $\mathcal{R}_\textrm{global}$ and captures the multimodal context in $\mathbf{\hat{x}}$ more comprehensively in a fine-grained manner. For this, we leverage the bi-directional self-attention along with the image-text matching~(ITM) classifier of pre-trained BLIP-2 QFormer to attend to all the visual tokens $\mathcal{Z}$ and Context Description text tokens $\mathcal{T}_{\mathrm{tokens}}$ with each other. The ITM classifier outputs two logits: one for the positive match (with index $j=1$) and one for the negative match (with index $j=0$). In the training of \method, positive pairs are defined as the generated image and its corresponding context description within the same training batch. We select the logit corresponding to the positive match $j=1$ of the classifier as $\mathcal{R}_{\textrm{fine-grained}}$,
% The module outputs a binary matching score, of which we assign the confidence score corresponding to the positive class as $\mathcal{R}_{\textrm{fine-grained}}$. 
\begin{equation}
    \mathcal{R}_{\textrm{fine-grained}} = \tt{ITM\_Classifier}(\tt{Bidirectional\_SA}(\mathcal{Z}, \mathcal{T}_{\mathrm{tokens}}))_{j=1},
\end{equation}
$\mathcal{R}_{\textrm{fine-grained}}$ therefore captures the fine-grained multimodal image-text alignment between $\mathbf{\hat{x}}$ and $\mathcal{C}$.

% \textbf{Fine-Grained Consistency Reward.} While $\mathcal{R}_\textrm{global}$ allows to align the global semantics of the generated image $\mathbf{\hat{x}}$ with that of the textual context descriptions $\mathcal{C}$, it is not sufficient to maintain optimal fidelity as SDXL can still omit capturing key scene attributes specified in $\mathbf{g}$. We therefore introduce Fine-Grained Consistency Reward, $\mathcal{R}_{\textrm{fine-grained}}$, that helps to complement $\mathcal{R}_\textrm{global}$ and captures the multimodal context in $\mathbf{\hat{x}}$ more comprehensively in a fine-grained manner. For this, we leverage the bi-directional self-attention alongwith the image-text matching classifier module of pre-trained BLIP-2 QFormer to attend to all the visual tokens $\mathcal{Z}$ and Context Description text tokens $\mathcal{T}_{\mathrm{tokens}}$ with each other. We select the logit corresponding to the positive class $j$ of the classifier as $\mathcal{R}_{\textrm{fine-grained}}$.
% % The module outputs a binary matching score, of which we assign the confidence score corresponding to the positive class as $\mathcal{R}_{\textrm{fine-grained}}$. 
% By doing so, $\mathcal{R}_{\textrm{fine-grained}}$ captures the fine-grained multimodal image-text alignment between $\mathbf{\hat{x}}$ and $\mathcal{C}$,
% \begin{equation}
%     \mathcal{R}_{\textrm{fine-grained}} = \tt{Classifier}(\tt{Bidirectional\ SA}(\mathcal{Z}, \mathcal{T}_{\mathrm{tokens}}))_{j},
% \end{equation}
% where $j$ denotes the positive image-text matching class.

\textbf{Loss Function.}~We combine $\mathcal{R}_\textrm{global}$ and $\mathcal{R}_{\textrm{fine-grained}}$ to compute loss $\mathcal{L}_{\textrm{total}}$ to fine-tune denoiser $\boldsymbol{\epsilon}_\theta$,
% \textbf{Loss Function.} To compute the loss, $\mathcal{L}_{\textrm{total}}$, used to fine-tune denoiser $\boldsymbol{\epsilon}_\theta$, we combine $\mathcal{R}_\textrm{global}$ and $\mathcal{R}_{\textrm{fine-grained}}$ as,
\begin{equation}
    \mathcal{L}_{\textrm{total}} = -(\lambda_1 \mathcal{R}_\textrm{global} +\lambda_2 \mathcal{R}_\textrm{fine-grained}),
\end{equation}
where $\lambda_1$ and $\lambda_2$ are the hyperparameters balancing the contribution from the two rewards.

\setlength{\textfloatsep}{5mm plus 2.0pt minus 2.0pt}
\setlength{\abovecaptionskip}{4mm plus 2.0pt minus 2.0pt}
\textbf{Training Procedure.} During training, we freeze all components of \method except UNet denoiser $\boldsymbol{\epsilon}_\theta$ in SDXL which we fine-tune using LoRA~(Figure~\ref{fig:overview} and~\ref{fig:qformer}).
% \textbf{Training Procedure.} As shown in Figure~\ref{fig:overview} and~\ref{fig:qformer}, all components of \method are frozen except UNet denoiser $\boldsymbol{\epsilon}_\theta$ in SDXL which is fine-tuned using LoRA. 
Additionally, text guidance $\mathbf{g}$ comprises the ground truth corresponding to the reference image in relation to the input text query. Algorithm~\ref{algorithm:prompt-fine-tune} presents an overview of the training procedure. We optimize the UNet denoiser $\boldsymbol{\epsilon}_\theta$ in SDXL using DDIM \citep{song2021score} scheduler, guided by the two rewards $\mathcal{R}_\textrm{global}$ and $\mathcal{R}_{\textrm{fine-grained}}$. For each training step, we extract the image embedding $\mathcal{I}_\mathbf{e}$ of the reference image and text embedding $\mathcal{T}_\mathbf{e}$ of context description $\mathcal{C}$ to condition the generation process. We obtain the generated image $\mathbf{\hat{x}}$ via $25$-steps DDIM and VAE decoder $\mathcal{D}$ from latent $\mathbf{z}_0$ to pixel space $\hat{\mathbf{x}}$, 
which we feed to Multimodal Context Evaluator along with $\mathcal{C}$.
% which is further evaluated by our multimodal context rewards. 
Since each timestep in the denoising process is differentiable, we compute the gradient to update parameters $\theta$ in denoiser $\boldsymbol{\epsilon}_\theta$ through chain rule,
\begin{equation}
\small
\begin{aligned}
&\frac{\partial \mathcal{L}}{\partial \theta}=-\frac{\partial \mathcal{R}}{\partial \hat{\mathbf{x}}} \cdot \frac{\partial \hat{\mathbf{x}}}{\partial \mathbf{z}_0} \cdot \prod_{t=0}^T \frac{\partial\left[\sqrt{\alpha_{t-1}}\frac{\mathbf{z}_t-\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)}{\sqrt{\alpha_t}}+\sqrt{1-\alpha_{t-1}} \cdot \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)\right]}{\partial \theta}
\end{aligned}
\end{equation}
\textbf{Evaluation.} During evaluation, we remove the Multimodal Context Evaluator with final output being the generated image $\mathbf{\hat{x}}$~(Appendix~\ref{appendix:inference}, Figure~\ref{fig:inference}). For fair evaluation, text guidance $\mathbf{g}$ no longer includes the ground truth corresponding to the reference image in relation to the input text query.

\begin{algorithm}[!t]
\small
\caption{Multimodal Context Rewards Fine-tuning}
\label{algorithm:prompt-fine-tune}
\begin{algorithmic}
\Require  Pre-trained UNet denoiser
$\bm{\epsilon}_{\bm\theta}$; original data distribution $\mathbb{P}_\mathrm{data}$; context descriptor $\tt{MLLM}$.
\Ensure $\bm{\epsilon}_{\bm\theta}$ converges and minimizes $\mathcal{L}_\textrm{total}$. 

% \State $\rightarrow$ \textbf{Perform UNet denoiser fine-tuning}.
\State Activate UNet denoiser $\bm{\epsilon}_{\bm\theta}$; freeze context rewards $\mathcal{R}_\textrm{global}$, $\mathcal{R}_\textrm{fine-grained}$, context descriptor $\tt{MLLM}$
\While{$\mathcal{L}_\textrm{total}$ not converged}
\State Sample multimodal context input $\{\mathbf{x}, \mathbf{g}\} \sim \mathbb{P}_\mathrm{data}$; $t=T$
\State $\mathcal{P} \leftarrow p(\mathbf{g})$ \Comment{Create instruction prompt $\mathcal{P}$ from text guidance $\mathbf{g}$ and prompt template $p$}
\State $\mathcal{C} \leftarrow \tt{MLLM} (\mathbf{x}, \mathcal{P})$ \Comment{Extract context description $\mathcal{C}$ from $\tt{MLLM}$}
\State Extract image embedding $\mathcal{I}_\mathbf{e}$ of reference image $\mathbf{x}$
\State Extract text embedding $\mathcal{T}_\mathbf{e}$ of description $\mathcal{C}$
\While{$t>0$} \Comment{perform DDIM denoising}
\State $\mathbf{z}_{t-1} \leftarrow \sqrt{\alpha_{t-1}}\frac{\mathbf{z}_t-\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)}{\sqrt{\alpha_t}}+\sqrt{1-\alpha_{t-1}} \cdot \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)$
\EndWhile
\State $\hat{\mathbf{x}} \leftarrow \mathcal{D}(\mathbf{z}_0)$ \Comment{VAE Decoder decodes latent $\mathbf{z}_0$ to pixel space}
\State $    \mathcal{L}_{\textrm{total}} \leftarrow  -(\lambda_1 \mathcal{R}_\textrm{global} +\lambda_2 \mathcal{R}_\textrm{fine-grained})$
\State Backward $\mathcal{L}_{\textrm{total}}$ and update $\bm{\epsilon}_{\bm\theta}$ for last $K$ steps. 
\EndWhile
\end{algorithmic} 
\end{algorithm}
% \vspace{-mm}


% ------- OLD -------

% To ensure that the generated images align with the context description, \textcolor{blue}{we propose a fine-tuning method that fine-tunes the UNet denoiser $\boldsymbol{\epsilon}_\theta$ in the SDXL model using our defined multimodal context rewards. Particularly, we employ the multimodal representation connector in pre-trained VLMs functioning as a context evaluator. Here we specifically use pre-trained BLIP-2 QFormer and design two novel rewards. From Fig.~\ref{fig:overview}, a generated image $\mathbf{\hat{x}}$ from SDXL and context description $\mathbf{c}$ from MLLM are fed to our context evaluator.} This fine-tuning process maximizes the Global Semantic Reward for capturing high-level global mutual information and the Fine-Grained Consistency Reward for ensuring fine-detail alignment between the generated images and the textual context.


% \textcolor{blue}{As illustrated in Fig.~\ref{fig:qformer}, given a generated image $\mathbf{\hat{x}}$ from SDXL, we use CLIP image encoder to extract its image tokens $\mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})$.} Then we utilize learned queries $\mathcal{Q}_\mathrm{learned}$ from pre-trained BLIP-2 QFormer to extract informative visual representation. The learned queries $\mathcal{Q}_\mathrm{learned}$ interact with image features $\mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})$ using cross-attention layers:
% \begin{equation}
%     \mathcal{Z} = \tt{CrossAttention}\left(\mathcal{Q}_\mathrm{learned}, \mathcal{I}_\mathrm{tokens}(\mathbf{\hat{x}})\right)
% \label{eq:query_output}
% \end{equation}
% \textbf{Global Semantic Reward} ensures that the overall and high-level context in the generated images $\mathbf{\hat{x}}$ aligns with that in the textual context descriptions $\mathbf{c}$. This reward is based on cosine similarity, which maximizes the mutual information between the image and text representation. \textcolor{blue}{Given the context description $\mathbf{c}$ from MLLM, we use text encoder BERT to extract its text tokens $\mathcal{T}_{\mathrm{tokens}}$.} Then using the output query representation $\mathcal{Z}$ from Eq.~\ref{eq:query_output}, we compute cosine similarity between each query output $\mathcal{Z}_i$ and the representative text token $\mathcal{T}_{\texttt{[CLS]}}$, finally select the highest one as the global semantic reward, in order words, image-text similarity:
% \begin{equation}
%     \mathcal{R}_\textrm{global} =  \max_i \frac{\mathcal{Z}_i^T\cdot\mathcal{T}_{\texttt{[CLS]}}}{\Vert\mathcal{Z}_i\Vert\Vert\mathcal{T}_{\texttt{[CLS]}}\Vert}
% \end{equation}
% \textbf{Fine-Grained Consistency Reward} evaluates the fine-grained alignment between image and text representation. Following BLIP-2 QFormer, a bi-directional self-attention is used so that all queries $\mathcal{Z}$ and text tokens $\mathcal{T}_{\mathrm{tokens}}$ can attend to each other. In this way, the output queries $\mathcal{Z}$ can capture fine-grained multimodal information, especially pairwise image-text alignment. \textcolor{blue}{The output is later fed into a two-class linear layer and extract the positive matching score}:
% \begin{equation}
%     \mathcal{R}_{\textrm{fine-grained}} = \tt{Linear}(\tt{Bidirectional\ SA}(\mathcal{Z}, \mathcal{T}_{\mathrm{tokens}}))
% \end{equation}
% \textbf{Loss Function.} We combine the mentioned two rewards to calculate the total loss as below:
% %To fine-tune the SDXL model, we combine the Global Semantic Reward (for high-level semantic alignment) and the Fine-Grained Consistency Reward (for detailed contextual alignment). The total loss is computed as:
% \begin{equation}
%     \mathcal{L}_{\textrm{total}} = -(\lambda_1 \mathcal{R}_\textrm{global} +\lambda_2 \mathcal{R}_\textrm{fine-grained}),
% \end{equation}
% where $\lambda_1$ and $\lambda_2$ are hyperparameters that balance the contributions of the two rewards. %We simply choose $\lambda_1 = \lambda_2 = 1$.

% \textbf{Training Procedure.} During training, the UNet denoiser $\boldsymbol{\epsilon}_\theta$ in the SDXL model is optimized using the DDIM scheduler, guided by the two rewards $\mathcal{R}_\textrm{global}$ and $\mathcal{R}_{\textrm{fine-grained}}$. For each training step, image embedding $\mathcal{I}_\mathbf{e}$ of the reference image and text embedding $\mathcal{T}_\mathbf{e}$ of context description $\mathbf{c}$ are extracted to condition the generation process. The generated image $\mathbf{\hat{x}}$ is obtained via $25$-steps DDIM and VAE decoder $\mathcal{D}$ from latent $\mathbf{z}_0$ to pixel space $\hat{\mathbf{x}}$, which is further evaluated by our context rewards. Since each timestep in the denoising process is differentiable, the gradient to update parameters $\boldsymbol{\theta}$ in UNet denoiser $\boldsymbol{\epsilon}_\theta$ is computed through chain rule:
% \begin{equation}
% \begin{aligned}
% &\frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}}=-\frac{\partial \mathcal{R}}{\partial \hat{\mathbf{x}}} \cdot \frac{\partial \hat{\mathbf{x}}}{\partial \mathbf{z}_0} \cdot \prod_{t=0}^T \frac{\partial\left[\sqrt{\alpha_{t-1}}\frac{\mathbf{z}_t-\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)}{\sqrt{\alpha_t}}+\sqrt{1-\alpha_{t-1}} \cdot \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)\right]}{\partial \boldsymbol{\theta}}
% \end{aligned}
% \end{equation}
% \begin{algorithm}[]
% \small
% \caption{Context Rewards Fine-tuning}
% \label{algorithm:prompt-fine-tune}
% \begin{algorithmic}
% \Require  Pre-trained UNet denoiser
% $\hat{\bm{\epsilon}}_{\bm\theta}$; original data distribution $\mathbb{P}_\mathrm{data}$; context descriptor $\tt{MLLM}$.
% \Ensure $\hat{\bm{\epsilon}}_{\bm\theta}$ converges and maximizes $\mathcal{L}_\textrm{total}$. 

% \State $\rightarrow$ \textbf{Perform UNet denoiser fine-tuning}.
% \State Activate UNet denoiser $\hat{\bm{\epsilon}}_{\bm\theta}$; freeze context rewards $\mathcal{R}_\textrm{global}$, $\mathcal{R}_\textrm{fine-grained}$, context descriptor $\tt{MLLM}$
% \While{$\mathcal{L}_\textrm{total}$ not converged}
% \State Sample reference image $\mathbf{x} \sim \mathbb{P}_\mathrm{data}$; $t=T$; create instruction $\tt{prompt}$
% \State $\mathbf{c} \leftarrow \tt{MLLM} (\mathbf{x}, \tt{prompt})$ \Comment{Extract context description from $\tt{MLLM}$}
% \State Extract image embedding $\mathcal{I}_\mathbf{e}$ of reference image $\mathbf{x}$
% \State Extract text embedding $\mathcal{T}_\mathbf{e}$ of description $\mathbf{c}$
% % \State $\mathbf{c} \leftarrow \mathcal{C}(\mathbf{x})$
% \While{$t>0$} \Comment{perform DDIM denoising}
% \State $\mathbf{z}_{t-1} \leftarrow \sqrt{\alpha_{t-1}}\frac{\mathbf{z}_t-\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)}{\sqrt{\alpha_t}}+\sqrt{1-\alpha_{t-1}} \cdot \boldsymbol{\epsilon}_\theta\left(\mathbf{z}_t, \mathcal{I}_\mathbf{e}, \mathcal{T}_\mathbf{e}, t\right)$
% \EndWhile
% \State $\hat{\mathbf{x}} \leftarrow \mathcal{D}(\mathbf{z}_0)$ \Comment{VAE Decoder decodes latent $\mathbf{z}_0$ to pixel space}
% % \State Forward ${\bm{\epsilon}}_{\bm{\theta}}(t, \mathbf{z}_t, {\mathcal{T}}_\varphi(\mathbf{p}))$, solve Eq.~\ref{eq:ddim} iteratively to get $\hat{\mathbf{x}}$.
% \State $    \mathcal{L}_{\textrm{total}} \leftarrow  -(\lambda_1 \mathcal{R}_\textrm{global} +\lambda_2 \mathcal{R}_\textrm{fine-grained})$
% \State Backward $\mathcal{L}_{\textrm{total}}$ and update $\hat{\bm{\epsilon}}_{\bm\theta}$ for last $K$ steps. 

% % \State $\rightarrow$ \textbf{Perform robust training}.
% % \State Get data $\mathbf{x}$ from $\mathbb D_{train}$, sample $t$, and sample noise $\pmb{\epsilon}$ to optimize Eqn.~\ref{eqn:train_loss} with AdamW~\cite{loshchilov2017decoupled}
% % \State $Loss \leftarrow \mathbb{E}_{t\sim U[0, 1],\mathbf{x},\pmb{\epsilon}\sim\mathcal{N}(\mathbf{0}, \mathbf{I})}[\varpi(\lambda_t)\lvert\lvert\hat{\pmb{\epsilon}}_{t,\theta}(\mathbf{z}_t)-\pmb{\epsilon}\rvert\rvert_2^2] $
% % \State Backpropagate $Loss$
% % \State Update UNet parameters $\theta$ with AdamW~\cite{loshchilov2017decoupled}
% % \State $\rightarrow$ \textbf{Architecture optimization}:
% % \If{perform architecture evolving at this iteration}
% % \State $\rightarrow$ \textbf{Evaluate blocks}:
% % \For{each ${block}[i, j]$}
% % \State {\small $\Delta CLIP \leftarrow$ eval($\hat{\bm{\epsilon}}_{\bm\theta}$, $A^-_{block[i,j]}$, $\mathbb{D}_{val}$)}, 
% % \State {\small $\Delta Latency \leftarrow$ eval($\hat{\bm{\epsilon}}_{\bm\theta}$, $A^-_{block[i,j]}$, $\mathbb{T}$)}
% % \EndFor
% % \State $\rightarrow$ \textbf{Sort actions based on $\frac{\Delta \textit{CLIP}}{\Delta \textit{Latency}}$, execute action, and evolve architecture to get latency $T$}: 
% % % \State \textbf{if} $T$ not satisfied: 
% % \If{latency objective $S$ is not satisfied}
% % \State {\small $\{\hat{A}^-\} \gets {\arg\min}_{A^-} \frac{\Delta \textit{CLIP}}{\Delta \textit{Latency}}$,}
% % % \textbf{else:} 
% % \Else
% % \State {\small $\{\hat{A}^+\} \gets \text{copy}({\arg\max}_{A^-} \frac{\Delta \textit{CLIP}}{\Delta \textit{Latency}} )$,}
% % \State {\small $\hat{\bm{\epsilon}}_{\bm\theta} \gets \text{evolve}( \hat{\bm{\epsilon}}_{\bm\theta}, \{\hat{A}\} )$}
% % \EndIf
% % \EndIf
% \EndWhile

% % \State $\rightarrow$ \textbf{Perform UNet finetuning}.
% % \State Freeze $\mathcal{T}_{\varphi}$ and reward models $\mathcal{R}_i$, activate UNet $\hat{\bm{\epsilon}}_{\bm\theta}$. 
% % \State Repeat the above reward training until converge. 

% % \State Deactivate robust training and fine-tune $\hat{\pmb{\epsilon}}_{\theta}$ with step reduction (Sec.~\ref{sec:step_distillation}) for mobile deployment.

% \end{algorithmic}   

% \end{algorithm}
