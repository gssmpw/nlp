
\begin{abstract}
% Diffusion models have emerged as powerful tools for generating high-quality, diverse synthetic data. Existing methods struggle to preserve the specific contextual details of a reference image, particularly in scenarios that involve multimodal guidance such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning. In this paper, we introduce \textbf{\method}, the first diffusion-based image generation method designed to produce high-fidelity images guided by multimodal context. Given a reference image and text guidance, \method is capable of generating highly diverse images while accurately preserving specified scene attributes such as object interactions, spatial relationships, and other critical visual details. \method employs a novel Multimodal Context Evaluator which simultaneously optimizes a novel Global Semantic and Fine-grained Consistency Rewards to ensure the generated images remain consistent with the contextual attributes of reference image specified by the text guidance. Unlike existing methods, \method maintains a balance between diversity and fidelity, enabling it to generate contextually consistent synthetic data that can be effectively leveraged in VQA and HOI Reasoning tasks. To evaluate the performance of \method, we propose a new benchmark that incorporates the MME Perception dataset for Test-Time Augmentation (TTA) and the Bongard HOI dataset for Test-Time Prompt Tuning (TPT). Experiments demonstrate \method outperforms state-of-the-art methods by achieving high fidelity and diversity in both datasets, validating \method's potential as a robust tool for context-preserving image generation in complex multimodal tasks.

While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes in generated images consistent with a multimodal context, i.e.~a reference image with accompanying text guidance query. To address this, we introduce \textbf{\method}, the first diffusion-based image generator which, given a multimodal context, generates highly diverse images w.r.t. the reference image while ensuring high fidelity by accurately preserving scene attributes, such as object interactions and spatial relationships from the text guidance. \method employs a novel Multimodal Context Evaluator that simultaneously optimizes our formulated Global Semantic and Fine-grained Consistency Rewards to ensure generated images preserve the scene attributes of reference images in relation to the text guidance while maintaining diversity. As the first model to address the task of maintaining both diversity and fidelity given a multimodal context, we introduce a new benchmark formulation incorporating MME Perception and Bongard HOI datasets. Benchmark experiments show \method outperforms all existing methods by achieving superior fidelity while maintaining diversity, validating \method's potential as a robust multimodal context-aligned image generator in complex visual tasks.
\end{abstract}

