\section{Related Work}
\subsection{Textual Embedding Inversion Attacks}
% Let $x$ and $enc(\cdot)$ be text and an encoder,  $\ve= enc(x)$ is the corresponding embedding vector. 
Textual embedding inversion attack aims to learn the inversion function that reconstructs the original textual inputs given their embeddings. 
%$enc^{-1}(\ve)=x$.
% reconstruct text $\hat{\vx}$ from embeddings $enc(\mathbf{x})$, so that $\mathbf{\hat{x}}\approx \mathbf{x}$.
% Textual Embedding Inverison attacks have advanced rapidly, evolving from reconstructing 50\% to 70\% of tokens~\citep{10.1145/3372297.3417270} to achieving near-exact matches with more recent approaches~\citep{li-etal-2023-sentence, morris2023text}. 
\citet{10.1145/3372297.3417270} demonstrates that it is possible to recover over half of the input words from a text embedding without preserving their order.
\citet{li-etal-2023-sentence} starts to treat the inversion attacks as a generation task, generating coherent and contextually similar sentences compared to the original text.
%train attack models assuming leaked data of more than 100k samples.
% average sentence length is low. 11, and it uses a lot of training data.
~\citet{morris2023text} adopts an iterative approach to train the attack model by parameterizing attack and hypothesis embeddings based on decoded text from the previous step, which results in exact matches between original and reconstructed text in certain settings. 
%However, this approach requires almost unlimited access to the victim encoder. Despite its inversion performance, this approach is computationally costly and assumes an unrealistic setting, involving days of model training and 5 million leaked data.
~\citet{huang_transferable_2024} implements adversarial training to align victim embeddings to attack embeddings, making them not differentiable. 
%Although this work uses significantly fewer leaked data samples than prior works, it still leverages at least 8k leaked samples and the performance is lower than 20 in RougeL across the models.
%8k leaked samples were still needed to achieve 20 in RougeL. 
~\citet{chen2024typ,chen2024text} expand inversion attacks beyond English embeddings to multilingual spaces, leveraging linguistic typology to investigate inversion attack performance, finding that certain languages are particularly vulnerable.

However, all existing works in embedding inversion attacks require an enormous amount of data leakage to train the generative attack models, such as 100k samples for~\citet{li-etal-2023-sentence}, 1-5 million for~\citet{morris2023text, chen2024typ,chen2024text} and 8k for~\citet{huang_transferable_2024}.
In comparison, our proposed approach \textbf{\ourmethod} does not require training an attack model on leaked/private embeddings, and the inversion attack succeeds with few leaked data, we additionally experiment on multiple languages.
% \xqk{main reference and must read (\ie EMNLP'23 one) papers.} # done

% \xqk{Prefer not to highlight the scale of samples for training for each work, which makes the attack implausible (e.g., a monitor of network data transmission can avoid such a scale of data leakage). Instead, highlight All existing work requires an enormous amount of training samples, such as xxx for xxx and xxx for xxx. (Two most representative works here.)}



\subsection{Embedding Alignment}
% Embedding alignment has been a critical area in NLP, primarily due to its role in bridging representation spaces across different languages, domains, and modalities. 
Embedding alignment has continuously progressed in NLP with the development of embedding representations and LLMs.
% Early on, a widely adopted approach in this subfield involves first learning monolingual word vectors independently~\citep{10.5555/2999792.2999959}. Subsequently, a mapping from source language embeddings to target language embeddings is learned with a bilingual dictionary~\citep{mikolov2013exploiting,smith2017offline,artetxe-etal-2017-learning}.
% When this mapping is constrained to be an orthogonal linear transformation,
% the optimal alignment between word pairs can be derived in closed form~\citep{artetxe-etal-2016-learning,schonemann1966generalized}.
% In comparison,~\citet{lample2018word} uses the unsupervised approach to align word embedding spaces with a cross-domain similarity adaption to mitigate the hubness problem.
In the early stages, a common approach involved independently training monolingual word vectors~\citep{10.5555/2999792.2999959} and then learning a mapping between source and target language embeddings using a bilingual dictionary~\citep{mikolov2013exploiting,smith2017offline,artetxe-etal-2017-learning}. 
When this mapping is restricted to an orthogonal linear transformation, the optimal word pair alignment can be computed in closed form~\citep{artetxe-etal-2016-learning,schonemann1966generalized}. 
In contrast,~\citet{lample2018word} introduce an unsupervised method for aligning word embedding spaces, incorporating cross-domain similarity adaptation to address the hubness problem.

With the advancement of contextualized embeddings since the emergence of LLMs such as BERT~\citep{devlin2018bert}, the focus shifted to the alignment of contextual word representations~\citep{schuster-etal-2019-cross, aldarmaki-diab-2019-context,wang-etal-2019-cross,alqahtani-etal-2021-using-optimal,cao2020multilingualalignmentcontextualword, jalili-sabet-etal-2020-simalign}.
Moreover, sentence embedding alignment has been operated in lifelong relation extraction with a linear transformation~\citep{wang-etal-2019-sentence}, aligning encoders in different languages to evaluate crosslingual transfer~\citep{conneau2018xnlievaluatingcrosslingualsentence},  building parallel data for machine translation~\citep{krahn2023sentenceembeddingmodelsancient}. 
In comparison, \textbf{\ourmethod} aligns sentence embeddings from different models to conduct embedding inversion attacks, but it can also be applied in embedding alignment in general. 


\subsection{Mitigating Embedding Inversion Attacks}
Most research on textual embedding inversion focuses on attacks~\cite {li-etal-2023-sentence, huang_transferable_2024,chen2024typ}.
While~\citet{10.1145/3372297.3417270} adopt an adversarial training approach to mitigate the risks of inversion attacks, this method is ineffective for defending textual embeddings in black-box settings.
To defend against inversion attacks while maintaining embedding utility in downstream tasks, \citet{morris2023text} propose inserting Gaussian noise as a defense mechanism.
Expanding inversion attacks into multilingual space using the same method,~\citet{chen2024text} find that Gaussian noise effectively protects monolingual embeddings but is less effective for multilingual ones.

Differential privacy (DP) limits the impact of individual element~\citep{dwork2014algorithmic}, and has been shown to preserve the privacy of the extracted representation from text, when applied during model training~\citep{lyu-etal-2020-differentially}.
To ensure sequence-level metric-based local DP, which can be employed during inference, a sentence embedding sanitization pipeline has been developed, maintaining non-private task accuracy and effectively thwarting privacy threats of membership inference attacks~\citep{du2023sanitizing}.
Pertinent to vector DBs, Watermarking EaaS with Linear Transformation (WET) introduces a method that applies linear transformations to embeddings to implant watermarks to counter paraphrasing vulnerabilities~\citep{shetty2024wet}.

In this work, we examine these defenses against \textbf{\ourmethod}, and discuss the potentials and challenges of defending embeddings from inversion attacks.