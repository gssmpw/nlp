
\begin{figure*} [ht]
  \centering
  \includegraphics[width=\textwidth]{figs/pipeline1.png}
  \captionof{figure}{
  Overview of the proposed pipeline. The input RGB image is processed through scene analysis to extract key information, followed by pose-aware generation to create initial 3D models. Physical constraint refinement ensures realistic interactions and spatial relationships, yielding a high-quality, mesh-based 3D scene.
  } \label{fig_overview1}
\end{figure*}.



% Bringing real-world scenes into the digital world has become a pivotal practice in fields like animation, film, gaming, and architecture, enabling the creation of immersive and interactive environments. For example, in ``Avatar (2009)'', James Cameron used groundbreaking 3D scanning technology to create the lush, realistic world of Pandora. Similarly, in the gaming industry, ``The Witcher 3: Wild Hunt'' used real-world locations in Poland to inspire its detailed open-world design, incorporating lifelike terrain and architecture into the game's expansive environment.
Transforming real-world scenes into the digital realm enhances our ability to understand, recreate, and interact with the 3D world around us. This practice is widely embraced in industries such as animation, film, gaming, architecture, and manufacturing. It enables the creation of immersive movie experiences, the digital preservation of historical relics, and the development of interactive environments for gaming.
For example, James Cameron employed groundbreaking 3D scanning technology in \textit{Avatar} (2009) to bring the lush, realistic world of Pandora to life. Similarly, in the gaming industry, \textit{The Witcher 3: Wild Hunt} incorporated lifelike terrain and architectures inspired by real-world locations in Poland, blending authentic cultural and natural elements with imaginative, open-world exploration.

% Traditionally, photogrammetry and 3D scanning are commonly used to capture the physical world in high detail and translate it into digital form~\cite{MVS,chen2018deep,mildenhall2020nerf, muller2022instant,barron2021mip,barron2022mip,kerbl3Dgaussians}.  
% Such techniques require tens to hundreds of images and create models on a per-scene basis, meaning each scene needs to be captured and processed individually in a time-consuming and resource-intensive way. 
% A single-image based approach is more efficient and scalable compared to traditional photogrammetry, as it only requires a single input and reduces the need for extensive data collection and processing.
A widely adopted approach to capture the physical world in high detail and translate it into digital form is photogrammetry~\cite{MVS,chen2018deep,mildenhall2020nerf,barron2021mip,barron2022mip,muller2022instant,kerbl3Dgaussians}. However, this technique typically requires tens to hundreds of images captured from multiple viewpoints for each scene, making the process time-consuming, resource-intensive, and challenging to scale.
In contrast, single-image-based approaches are far more efficient and scalable. They require only a single input image, which can be easily obtained, including from vast online repositories of visual content. This eliminates the need for data collection with expensive scanning devices or labor-intensive multi-view setups, enabling broader accessibility and practicality for large-scale applications.

\paragraph{Scene-level reconstruction}
Scene-level reconstruction from a single image presents a significant challenge, primarily due to the complexity of object variety, occlusions, and spatial relationships. 
\red{Existing approaches leverage different representations, such as point clouds~\cite{yin2023metric3d,bhat2023zoedepth,piccinelli2024unidepth,wang2024moge}, radiance fields~\cite{yu2021pixelnerf, tian2023mononerf, yu2022monosdf}, and 3D Gaussians~\cite{szymanowicz2024splatter,szymanowicz2024flash3d} often relying on depth priors trained from large-scale, single-image depth datasets~\cite{geiger2013vision,dai2017scannet, chang2015shapenet,sun2018pix3d}}. %为什么这些是largescale single image depth
However, methods based on monocular reconstruction frequently lack sufficient detail, resulting in coarse and imprecise representations of the overall scene.

%
Many contemporary methods adopt a scene understanding encoder and geometric regression approach. Notable examples, such as 3D Front~\cite{fu20213d}, train encoders on indoor datasets to aggregate semantic information from images and their segmented labels~\cite{dahnert2021panoptic,gkioxari2022learning,chu2023buol,chen2024single}. \red{These encoders then generate geometries and optimize network performance.} 
While these methods work well in controlled settings, their performance can be influenced by the scope of the datasets and their dependence on predefined structures within scenes.
% open vocab, ATISS

To better film real world to digital, other methods employ retrieval-based approaches to enhance scene quality, they incorporate advanced tools like GPT-4~\cite{achiam2023gpt} SAM~\cite{kirillov2023segment,ren2024grounded}, valuable priors such as depth maps~\cite{yang2024depth} to decompose scenes.
These methods~\cite{langer2022sparc,gumeli2022roca,kuo2021patch2cad,gao2024diffcad,dai2024automated} search for and replace objects in a scene with similar objects from a fixed dataset~\cite{fu20213d,wu20153d}. While effective for enhancing scene realism, these approaches suffer from significant limitations. Specifically, their representational capabilities are restricted to the dataset distribution, often limited to indoor scenes, and only produce roughly similar results. For content outside this distribution, these methods either produce erroneous retrieval results or fail to generate suitable matches, degrading the overall scene quality.


\paragraph{Reconstruction as Generation}
With the continuous advancements in the field, the ability to create high-quality 3D digital assets from various types of open-vocabulary images or text prompts has significantly improved. 3D generation models have become essential tools for creating detailed and realistic digital assets.

% 底下这些不是radiance field吗
% 真是在这两个数据集训练的吗
% genesis哪冒出来的
%\red{Many methods attempt to either represent the scene as a radiance field~\cite{poole2022dreamfusion,hong2023lrm} or 3D Gaussians~\cite{tang2023dreamgaussian,tang2025lgm}. Other approaches reconstruct 3D structures by expanding a single image into multiple multi-view images and recover the geometry through multi-view based reconstruction ~\cite{liu2023syncdreamer,liu2023zero,liu2024one,gao2024cat3d,voleti2025sv3d,wu2024reconfusion,long2024wonder3d}, mostly training on real scene dataset~\cite{lebegue2020co3d, yu2023mvimgnet}. More recent explorations employ video diffusion models to directly generate high-quality video ~\cite{Genesis}. However, these methods are not readily adaptable for direct integration into the graphics pipeline.}

%\red{Many generative methods attempt to either represent the asset as a radiance field~\cite{poole2022dreamfusion,hong2023lrm} or 3D Gaussians~\cite{tang2023dreamgaussian,tang2025lgm}. Other approaches reconstruct 3D structures by expanding a single image into multiple multi-view images and recover the geometry through multi-view based reconstruction ~\cite{liu2023syncdreamer,liu2023zero,liu2024one,gao2024cat3d,voleti2025sv3d,wu2024reconfusion,long2024wonder3d}, mostly training on real scene dataset~\cite{lebegue2020co3d, yu2023mvimgnet}.
%More directly, an increasing number of approaches adopt native 3D representations, bypassing the 2D image generation step and training directly on large-scale 3D datasets~\cite{deitke2023objaverse,deitke2024objaverse} to achieve higher quality and more detailed geometric structures. Examplary methods like~\cite{zhang20233dshape2vecset, zhang2024clay, xiang2024structured,wu2024unique3d,wang2024prolificdreamer}, introduce advanced geometry processing techniques to mine diverse groups of 3D datasets and discuss effective strategies to scale up generation models. However, these approaches focus on an object-level reconstruction, not directly suitable for scene-level generation.
%}
%In contrast to object level generation, scene generation is a more challegning taks due to xxx. 
%Rcent explorations employ video diffusion models to directly generate high-quality video of the scene~\cite{Genesis}, some allow user to navigate xxx and xxx in the scene. However, these methods either uses 3D Gaussian represenations or generate videos as output, which are not readily adaptable for direct integration into the graphics pipeline.

Current 3D generative methods primarily adopt two strategies: (1) 2D-supervised optimization using radiance fields~\cite{poole2022dreamfusion,hong2023lrm} or 3D Gaussians~\cite{tang2023dreamgaussian,tang2025lgm}, and (2) multi-view reconstruction pipelines that synthesize images~\cite{liu2023syncdreamer,liu2023zero,liu2024one,gao2024cat3d,voleti2025sv3d,wu2024reconfusion,long2024wonder3d} trained on real-world datasets~\cite{lebegue2020co3d, yu2023mvimgnet}. While these methods achieve view consistency, they inherit scalability limitations from 2D priors and struggle with high-fidelity geometry.
%
A growing body of work bypasses 2D supervision entirely, training directly on 3D datasets~\cite{deitke2023objaverse,deitke2024objaverse} to produce object-level geometry with advanced processing techniques~\cite{zhang20233dshape2vecset, zhang2024clay, xiang2024structured,wu2024unique3d,wang2024prolificdreamer}. However, such approaches focus on isolated objects and fail to address scene-level challenges, such as modeling spatial hierarchies, inter-object relationships, and environment lighting.

Scene generation remains underdeveloped due to these unresolved complexities. Recent explorations use video diffusion models~\cite{Genesis} to synthesize navigable scenes, but outputs are rasterized videos or 3D Gaussians~\cite{tang2023dreamgaussian,tang2025lgm}, neither of which integrate natively with graphics pipelines requiring editable meshes, UV mappings, or decomposable materials. This gap highlights the need for scene-native representations that balance generative flexibility with industrial compatibility.
%
%Scene generation demands unifying macroscopic layout coherence with microscopic geometric detail, a task ill-served by current approaches. Video diffusion models [21] synthesize dynamic scenes but output rasterized frames incompatible with 3D engines requiring editable meshes or UV mappings. Similarly, 3D Gaussians [3,4], while real-time renderable, lack structural semantics for selective object manipulation. 
%We posit that a hybrid architecture combining 3D-native scene graphs with diffusion-based refinement could bridge this gap, enabling both high-quality synthesis and pipeline integration. Such a system would advance generative graphics by merging the controllability of explicit scene representations with the expressivity of neural priors.
We propose a hybrid architecture merging 3D-native scene graphs (object layouts, materials) with diffusion refinement~\cite{poole2022dreamfusion,hong2023lrm}, bridging scene-generation gaps. By aligning diffusion priors with scene-graph semantics, it synthesizes editable geometry while retaining neural expressivity for details like weathering. Unlike video~\cite{Genesis} or 3D Gaussians~\cite{tang2023dreamgaussian,tang2025lgm}, this approach supports graphics pipeline integration, enabling dynamic editing and applications in VR/game development through animation-ready assets.

% ?你unique3d和prolificdreamer是不要图的吗
% an object-level reconstruction?


% For instance??
For instance, Gen3DSR~\cite{dogaru2024generalizable} employs DreamGaussian~\cite{tang2023dreamgaussian} to generate directly at scene locations, capable of recovering open-vocabulary scenes. 
However, due to a lack of understanding of object occlusions and pose information, it is difficult to recover correct geometric structures for heavily occluded objects. Additionally, since objects are established in the world coordinate system, makes editing and modifying individual objects highly challenging.
%
The most related work to ours is the Midi~\cite{huang2024midi}, which attempts to train on the relationships between objects to reconstruct scenes. However, since direct interactions between objects require additional datasets for training, it's hard to generalize to arbitrary input scene images.

In this paper, we introduce a novel scene reconstruction pipeline from a single image as a generation process, generating each object independently and then composing the full scene by aligning these objects while preserving their physical relationships. The reconstructed objects contain accurate geometric structures and textures, along with consistent spatial dependencies, which ensures a more reliable and realistic scene reconstruction compared to existing methods.

\paragraph{Physics-Aware 3D Modeling}
Generating physically plausible 3D assets is crucial for ensuring realism and functionality in applications such as animation, gaming, and robotics.
While recent 3D generative models excel at creating visually realistic objects, they often fall short in achieving physical plausibility. To address this limitation, physics-aware 3D generative models have been developed to integrate physical principles into the generation process. Some methods use soft-body simulation to animate 3D Gaussians~\cite{xie2024physgaussian,zhong2025reconstruction}, or generate articulated objects with physics-based penalties~\cite{liu2023few}, while others ensure self-supporting structures through rigid-body simulation~\cite{mezghanni2021physically,mezghanni2022physical,chen2024atlas3d} or FEM~\cite{guo2024physically,xu2024precise}. These methods leverage offline or online physical simulations to check the physical validity of generated shapes and in turn guide generation.
However, these approaches are typically confined to individual objects, overlooking the mutual influences between multiple objects within a scene. 

Incorporating physical constraints into scene synthesis is much more challenging due to the inclusion of more complex relationships, e.g., inter-object contact. \citet{yang2024physcene} integrates constraints like object collisions, room layout, and object reachability into their scene-level generation pipeline. However, it is limited to indoor scene synthesis and relies on closed-vocabulary database to perform shape retrieval.
\citet{ni2024phyrecon} addresses the issue of physical implausibility in multi-view neural reconstruction. It leverages both differentiable rendering and physical simulation to learn implicit representations. However, it requires multi-view images as input, focuses on individual objects, and primarily addresses only stability (simulating the dropping of objects).
In contrast, our method operates in an open-vocabulary setting and requires only a single input image. Furthermore, it accounts for more complex inter-object relationships, particularly support and contact, making it more versatile and applicable across diverse scenarios.

The primary inter-object relationship is contact, which plays a crucial role in rigid-body simulations. Collision detection~\cite{baraff1994fast,bergen1997efficient,lin1998collision} is an essential step, followed by contact point generation. The appropriate physical responses to the interaction between objects are computed based on contact points, such as applying forces to separate objects or simulating realistic bounces.
The choice of shape representation significantly affects the performance and complexity of collision detection and contact point generation. Convex polygonal meshes are popular due to the availability of highly efficient collision detection algorithms~\cite{gilbert1988fast,van2003collision}. Besides, signed distance fields (SDFs) have been popular for deformable models~\cite{marchal2004collision}, cloth~\cite{bridson2005simulation} and rigid bodies~\cite{guendelman2003nonconvex}, as well as in trajectory optimization~\cite{ratliff2009chomp,WangXF20}. SDFs provide a continuous representation of the distance between objects, introducing smoothness to the contact point generation process.
Thus, we adopt SDFs as the underlying shape representation in our physics-aware correction module (Sec.~\ref{sec:physics-aware-correction}). 
