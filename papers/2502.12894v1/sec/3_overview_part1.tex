% \section{Overview}


Scene-level reconstruction from a single image is a fundamental challenge in computer graphics, with broad applications in animation, virtual reality, and interactive gaming. Unlike object-level reconstruction, which focuses on isolated objects, scene-level reconstruction emphasizes the arrangement and relations of multiple entities under realistic (or stylized) physics. By capturing per-object structures, spatial relationships, and contextual cues, this holistic approach enables more immersive experiences, compelling narratives, and efficient workflows—benefits that surpass those of single-object reconstructions. Although previous methodologies have explored feed-forward pipelines or retrieval-based approaches using fixed 3D templates~\cite{liu2022towards,dai2024automated}, these methods often struggle to capture nuanced scene semantics and complex object relationships. To address these limitations, we propose a generation-driven scene ``reincarnation'' approach with emphasized object relations to construct high-fidelity, contextually consistent 3D environments from a single, unannotated RGB image whether sourced from real-world photography or synthetic data (see Fig.~\ref{fig_overview1}).

A key insight of our method is the thorough object relation analysis of scene contextual information. First, we perform object segmentation to identify and localize constituent objects within the image. We then obtain preliminary geometric information, i.e., point clouds, and explore semantic and spatial relationships among objects. This contextual backbone informs our subsequent object-wise generation pipeline, ensuring that each reconstructed object retains not only its geometric fidelity but also its correct placement in the broader scene. Finally, we synthesize a coherent 3D environment that respects physical plausibility—achieving structurally sound layouts and realistic interactions among scene elements.

%Our research is guided by two primary questions:

%How can generative models effectively capture complex inter-object relationships to produce realistic, scene-level reconstructions from a single image?
%What strategies for integrating geometric cues and contextual information yield the highest accuracy and plausibility in 3D reconstructions?

%By answering these questions, we demonstrate that generative methods offer a more flexible and robust alternative to feed-forward or retrieval-based techniques. They allow for fine-grained control over object-level detail and global scene composition, thereby streamlining content creation pipelines for animation, game development, and other areas that require accurate and visually compelling 3D models. This work not only illustrates the advantages of a generation-centric framework but also lays the groundwork for future advances in scene-level 3D reconstruction, highlighting the growing importance of context-driven approaches in bridging the gap between 2D imagery and rich, interactive virtual environments.

Our research focuses on two primary objectives: to explore how generative models can effectively capture complex inter-object relationships in order to produce realistic, scene-level reconstructions from a single image; and to identify strategies for integrating geometric cues and contextual information that maximize accuracy and plausibility in 3D reconstructions.
Through this investigation, we demonstrate that generative methods provide a more flexible and robust alternative to traditional feed-forward and retrieval-based techniques. These methods allow for fine-grained control over both object-level details and global scene composition, thus streamlining content creation pipelines for animation, game development, and other fields requiring accurate, visually compelling 3D models.
%
This work highlights the advantages of a generation-centric framework and lays the groundwork for future advancements in scene-level 3D reconstruction. It also underscores the growing importance of context-driven approaches in bridging the gap between 2D imagery and immersive, interactive virtual environments.

% \vspace{16pt}
%\paragraph{Scene decomposition} 
%
\paragraph{Preprocessing}
To facilitate comprehensive scene reconstruction from a single image, we first perform an extensive semantic extraction that provides a robust foundation for subsequent processing. Specifically, we employ Florence-2~\cite{xiao2024florence} to identify objects, generate their descriptions, and localize each object with bounding boxes. We then leverage GPT-4v~\cite{achiam2023gpt} to filter out spurious detections and isolate meaningful constituent objects, allowing for open-vocabulary object identification that is not constrained by predefined categories. Next, we use GroundedSAM-v2~\cite{ren2024grounded} to produce a refined segmentation mask $\{ \bm{M}_i \}$ for each labeled object $\{ \bm{o}_i \}$, thereby obtaining both precise object boundaries and corresponding occlusion masks, which play a crucial auxiliary role in the object generation stage.
Apart from semantic cues, we also integrate geometric information by extracting a scene-level point cloud. Using MoGe~\cite{wang2024moge}, we generate pixel-aligned point clouds $\{ \bm{q}_i \}$ for each object $\{ \bm{o}_i \}, i \in \{1,\dots,N\}$ and a global camera parameter in the scene coordinate system. This additional geometric data is subsequently matched to each object’s segmentation mask, providing a reliable structural reference for the final 3D scene reconstruction.
% Accurate object relationships are also crucial for preserving spatial consistency and depth perception in single-image 3D reconstruction. We use GPT-4v to infer a \red{comprehensive self-designed} scene graph (see Sec.~\ref{sec:physics-aware-correction}.) 
% including semantic labels, spatial positions, relationship types, and force directions.
% These relationships help reduce ambiguity while maintaining the functional and aesthetic coherence of the scene, thereby enabling high-fidelity results, preserving both the structural layout and the underlying physical attributes of the input image. 
% By combining semantic extraction, robust geometric data with accurate relationships, our approach ensures that each object is accurately represented in terms of both appearance, spatial configuration and the underlying physical attributes, creating a strong backbone for downstream scene-level generation and analysis.

