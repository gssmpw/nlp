

\begin{figure} []
  \centering
  \includegraphics[width=0.98\linewidth]{figs/pipeline2.png}
  \captionof{figure}{Network design of our alignment generation model (Sec.~\ref{sec:transformationgen}), occlusion-aware object generation model (Sec.~\ref{sec:objectgen}), and an illustrative figure of the texture generation model.} \label{fig_overview2}
\end{figure}






% \section{Pose-Aware 3D Instance Generation}
% \section{Joint Generation of 3D Object and Transformation}
\section{Perceptive 3D Instance Generation}
\label{sec:method2}

% 为什么我们要一个一个物品生成，为什么不直接生成一个整体场景？
%   （这不是废话吗，生成一个整体场景的mesh有屁用，拆开来一个一个生成才有更多的下游任务）
% 为什么生成单个物品的时候要在Canonical Space？
%   没有数据集先验，生成在世界坐标原地的，几何质量都很差。生成在Canonical Space除了质量更好，更重要的是，Canonical Space实际上是艺术家定义的坐标系和标准，生成在这个空间中就和艺术家一个标准，对于资产标准化、下游应用如编辑、渲染、动画、理解、处理都非常方便。不生成在Canonical Space简直就是反人类。
% 为什么这个section需要部分点云作为输入？
%   因为我们需要和原图更加一致。如果只使用图片而不使用点云，有可能会造成大体上看起来差不多，但是几何形状差很多，比如长度不一样，因此不能完美的放置回场景里。我们希望生成的物品能和图片上完全对应上，深度点云是最精确的控制方式
% 为什么我们不使用icp来把生成好的物品对齐回去？
%   icp没有使用到语义信息，而且效果很差。我们的变换生成器在大数据上训练过，有语义信息
% 
% canonical space 怎么定义的？

In the endeavor to reconstruct high-fidelity 3D scenes from single RGB images, a brute-force approach involves generating the entire scene mesh directly using techniques such as single-image depth estimation or diffusion priors. However, this method inherently struggles to manage occlusions, render invisible components, and accurately represent object relationships due to the complex and intertwined nature of real-world scenes. 
%The intricate configurations and interdependencies among multiple objects often lead to inaccuracies and incomplete reconstructions when relying solely on direct mesh generation. Consequently, these limitations underscore the need for more sophisticated methodologies that can effectively disentangle and model the various elements and their interactions within a scene.
%In the pursuit of reconstructing a high-fidelity 3D scene from a single RGB image, the brute-force way is to generate the entire scene mesh directly using techniques like single image depth or diffusion priors. However, such approach inevitablely fail to handle occlusions、invisible parts and object relations, due to the overly complicated configuration of a scene.
%In the pursuit of reconstructing a high-fidelity 3D scene from a single RGB image, it is imperative to generate and align each object meticulously. 
%Instead of generating an entire scene mesh, our approach focuses on individual object generation and precise transformation alignment. This strategy not only enhances the geometric quality of each asset but also facilitates a wide range of downstream applications such as editing, rendering, and simulation. Moreover, by operating within a canonical space, we ensure that the generated assets adhere to standardized orientations and scales, aligning seamlessly with artist-defined coordinate systems.
Instead of generating an entire scene mesh directly, our approach focuses on individual object generation and then arranges the objects via precise relational alignment, as illustrated in~\ref{fig_overview2}. This strategy offers several advantages: 1. focusing on individual objects ensures higher geometric fidelity and allows for detailed modeling, resulting in more accurate and visually appealing scene components.
%
2. operating within a canonical space ensures that generated assets adhere to standardized orientations and scales, seamlessly integrating with artist-defined coordinate systems and promoting consistency across digital content creation tools.
%
3. the modular approach supports various applications such as editing, rendering, and simulation, enabling independent manipulation of objects for greater flexibility and efficiency.
%
By decomposing scene reconstruction into object-wise generation and alignment, our method improves asset quality and manageability while enhancing the overall coherence and functionality of the 3D environment. This approach addresses challenges like geometric precision and efficient post-processing, advancing single-image 3D scene generation.

Object-wise generation presents significant challenges, primarily due to partial observations of objects within a scene caused by occlusions and limited sensor coverage. Additionally, existing generation methods often fail to coordinate multiple objects cohesively, resulting in inconsistent and unrealistic scenes. To overcome these limitations, we propose an Occlusion-Aware 3D Object Generation framework that integrates partial observations with comprehensive scene understanding. 
Specifically, given an image and its point cloud, our framework generates a high-quality 3D asset that not only resembles the input image but also aligns accurately with the partial point cloud represented in its corresponding canonical space.
Furthermore, we compute a transformation matrix that maps the generated object from its canonical space back to the original scene space, ensuring spatial consistency within the scene.
%This section delves into the core components of our generative pipeline: occlusion-Aware Object Generatio, transformation Generation and joint iterative integration.
% 
%However, object-wise generation also faces considerable challenges. First, we often have only partial observations of each object in the scene due to occlusions and xxx, and the generated objects are not coordinated to produce a consistent scene. To address, we propose an occlusion aware 3D object generation and xxx. Specifically, given the image and the partial point cloud in world space of an object in the scene, we generate a high-quality 3D asset that both resembles the image and aligns with the partial point cloud in canonical space, and a transformation that maps between canonical space and the original world space.
% 
%This section delves into the core components of our generative pipeline: occlusion aware object generation, transformation generation, and their integration through a joint, iterative process.




%\subsection{Object Centric 3D Generation}
%\label{sec:objectgen}
% 为什么生成单个物品的时候要在Canonical Space？
% 为什么需要部分点云作为输入？

%The first module of our pipeline is dedicated to the generation of individual 3D objects. Generating objects one by one, rather than constructing an entire scene mesh, offers significant advantages in terms of quality and usability. When objects are generated independently, each asset can achieve higher geometric fidelity and detail, which is often compromised in holistic scene generation approaches. Additionally, individual object generation aligns with the modular nature of many downstream tasks, allowing for more flexible editing and manipulation.

A critical aspect of our object generation process is the utilization of a large generative model to generate holistic and high-fidelity object meshes from partial image and point cloud observations. To do so, we first follow state-of-art native 3D generative models~\cite{xiang2024structured,zhang20233dshape2vecset,zhang2024clay} to pre-train a large-scale 3D generative model conditioning on textual and image inputs. %
%. By incorporating the partial point cloud, we ensure that the generated geometry is not only visually consistent with the input image but also accurately reflects the underlying depth information. This dual conditioning enables the creation of high-quality 3D assets that maintain structural integrity and geometric precision.
%
%Operating within a canonical space $[-1,1]^3$ is another cornerstone of our design. Without the constraints of a predefined coordinate system, generating objects directly in world space can lead to inconsistencies and subpar geometric quality due to the lack of dataset priors. By standardizing the generation process in a canonical space, we achieve superior asset quality and ensure that each object adheres to a consistent orientation and scale. This alignment with artist-defined standards is crucial for seamless integration into various applications, enhancing the overall usability and interoperability of the generated assets.
%\paragraph{3D Generative Base Model}
%A fundamental component of our pipeline is the generation of high-quality 3D objects that accurately reflect the geometry and appearance depicted in the input image. 
% DetailGen3d withdraw,....  others has cited very nearby?% [CLAY, 3DShape2VecSet, Dora, DetailGen3D]
We build upon existing generative frameworks featuring 3DShape2VecSet representation~\cite{zhang20233dshape2vecset,zhang2024clay}, which prioritize geometry generation by utilizing a Geometry Variational Autoencoder (VAE). This VAE framework encodes uniformly sampled surface point clouds into unordered latent codes and decodes these latent representations into Signed Distance Fields (SDFs). Formally, the VAE encoder $\mathcal{E}$ and decoder $\mathcal{D}$ are defined as:
\begin{equation}
    \bm{Z}=\mathcal{E}(\bm{X}),\ \mathcal{D}(\bm{Z},\bm{p}) = \text{SDF}(\bm{p}),
\end{equation}
where $\bm{X}$ represents the sampled surface point cloud of the geometry, $\bm{Z}$ is the corresponding latent code, and $\text{SDF}(\bm{p})$  denotes the operation of querying the SDF value at point $\bm{p}$ for subsequent mesh extraction via marching cubes.
% 
To effectively incorporate image information into the geometry generation process, we employ DINOv2~\cite{oquab2023dinov2} as our image encoder, following methodologies outlined in \cite{zhang20233dshape2vecset,zhang2024clay, xiang2024structured}, % citet
%  [CLAY, TRELLIS]. 
%DINOv2 excels in capturing rich semantic features from input images, which are crucial for conditioning the generative model. 
The geometry latent diffusion model (LDM) is then formulated as:
\begin{equation}
    \epsilon_\text{obj} (\bm{Z}_t; t,\bm{c}) \rightarrow \bm{Z},
\end{equation}
where $\epsilon$ represents the diffusion transformer model, $\bm{Z}_t$ is noisy geometry latent code at timestep $t$, and $\bm{c}$ denotes the encoded image features from DINOv2. We follow the pre-training process of prior works~\cite{zhang20233dshape2vecset,zhang2024clay} and pre-train the base model on Objaverse~\cite{deitke2023objaverse}. Upon training, our generation model $\epsilon$  is capable of generating detailed 3D geometry solely based on image features.

%\paragraph{Occlusion Adaption}
\subsection{Occlusion-aware 3D Object Generation}
\label{sec:objectgen}

Directly applying 3D generative-based models faces considerable challenges as
real-world scenarios often present challenges such as partial occlusions in the input images, which severely degrade the quality and accuracy of the generated object geometries. To address this issue, we leverage the Masked Auto Encoder (MAE) capabilities of DINOv2. Specifically, during inference, we provide an occlusion mask $\bm{M}$ alongside the input image $\bm{I}$, enabling the encoder to handle missing pixels by inferring latent features for the occluded regions. This is formalized as:
\begin{equation}
    \bm{c}_m=\mathcal{E}_\text{DINOv2}(\bm{I} \odot \bm{M}),    
\end{equation}
where $\bm{M}$ is a binary mask indicating which tokens should be masked and replaced with a [mask] token. During the pretraining phase, DINOv2 is trained with randomly set masks, allowing it to robustly infer missing parts based on the visible regions. Consequently, during inference, even if parts of the object image are occluded, the encoder can effectively reconstruct the necessary features, ensuring that the generative model maintains high-quality and accurate 3D reconstructions.
%
This integration of image conditioning and occlusion handling is pivotal for our pipeline, as it ensures that the generated 3D objects are both visually consistent with the input images and geometrically faithful to the underlying structure. 
%{\color{red} By conditioning the generative model on both robust image features and partial point cloud data, we achieve a harmonious balance between visual fidelity and geometric precision, laying a solid foundation for the subsequent transformation and alignment processes.}


%\paragraph{Partial Point Cloud Conditioning}

%To enhance the fidelity and accuracy of the generated 3D objects, our system incorporates partial point clouds alongside image-based cues. This dual conditioning ensures that the generated geometries not only align visually with the input images but also accurately reflect their underlying scale, shape, and depth. During the training phase, we simulate real-world partial scans or estimated depth map by rendering each 3D asset from multiple viewpoints, thereby obtaining corresponding RGB images, camera parameters, and ground-truth depth maps. These RGB images are then processed using advanced depth estimation techniques such as MoGe [xxx] and DepthAnything, to produce estimated depth map and then projected as partial point clouds. These point clouds are coarsely aligned with the original objects within the canonical $[-1,1]^3$  space to maintain consistency.


\paragraph{Canonical Point Cloud Conditioning} 
Though our object generation model produces visually plausible meshes from input object images, it is challenging to generate pixel-aligned geometry due to the high-level nature of the encoded image condition $\bm{c}$ and the absence of pixel-wise supervision. We address this issue by additionally conditioning our object generation model on observed partial point clouds in canonical coordinates.
%
This dual conditioning ensures that the generated geometries not only align visually with the input images but also accurately reflect their underlying scale, shape, and depth. 
During the conditioning training, we simulate real-world partial scans or estimated depth maps by rendering each 3D asset from multiple viewpoints, thereby obtaining corresponding RGB images, camera parameters, and ground-truth depth maps. These RGB images are then processed using advanced depth estimation techniques, including MoGe~\cite{wang2024moge} and Metric3D~\cite{yang2024depth}, to produce an estimated depth map and then projected as partial point clouds. These point clouds are coarsely aligned with the original objects within the canonical $[-1,1]^3$ space.


To bolster the model's robustness and its ability to generalize across diverse real-world scenarios, we employ a data augmentation strategy that interpolates between ground-truth partial point clouds $\bm{p}_\text{gt}$ (projected from ground truth depth map to simulate accurate depth) and noisier, estimated partial point clouds $\bm{p}_\text{est}$ (projected from estimated depth map and aligned to simulate estimated noisy depth from RGB). This interpolation is mathematically represented as:
%\begin{equation}
 $  \bm{p}_\text{disturb} = \alpha\cdot \bm{p}_\text{gt} +(1-\alpha)\cdot \bm{p}_\text{est}$,
%\end{equation}
where $\alpha\in[0,1]$ is a weighting factor which is sampled uniformly during training. Our object generator, named ``\textit{ObjectGen}'', with partial point cloud conditioning is formulated as
\begin{equation}
    \epsilon (\bm{Z}_t; t,\bm{c},\bm{p}_\text{disturb}) \rightarrow \bm{Z},
\end{equation}
where the conditioning adaptation scheme is based on attention mechanism similar to \cite{zhang20233dshape2vecset,zhang2024clay}. % citet
Additionally, to mimic real-world occlusions and missing data, we randomly mask sets of basic primitives—such as circles and rectangles—in the depth maps from various camera views. This results in partial point clouds with occluded and incomplete regions, further enhancing the model's ability to handle imperfect inputs.
%
A critical design choice in our approach is to maintain the alignment of partial point clouds with the geometry in our training data set. Unlike methods that apply random scaling, translation, or rotation to augmented point clouds, our aligned partial point clouds ensure that the generative model can more effectively conform to the input point clouds's inherent structure. This alignment restricts the model to adhere closely to the actual shapes and scales of objects, thereby facilitating more precise and coherent 3D reconstructions. By conditioning on these well-aligned partial point clouds, our model achieves superior alignment both in overall size and local geometric details, resulting in high-quality and reliable 3D geometry generation.
%\paragraph{Texture Generation}








\subsection{Generative Alignment}
\label{sec:transformationgen}


Each generated 3D object is within a normalized volume and assumes a canonical pose that may not be aligned with the image and scene space point cloud. 
This is because the image conditions use high-level features, such as DINOv2, to achieve better generalization.
Ensuring that each object is correctly transformed and scaled to align with its presentation in the scene is crucial for scene composition.
%
Though traditional alignment methods, such as Iterative Closest Point (ICP)~\cite{arun1987least,best1992method}, can be employed, they often fail to account for semantic context, leading to frequent misalignments and diminished accuracy (see Fig.~\ref{Ablation_icp_dr}). 
% maybe 
Instead, we introduce an alignment generative model conditioned on the scene-space partial point cloud $\bm{q} \in \mathbb{R}^{N\times 3}$ and the canonical-space geometry latent code $\bm{Z}$. Formally, we define our alignment generator ``\textit{AlignGen}'' as:
\begin{equation}
    \epsilon_\text{align} (\bm{p}_t; t, \bm{q}, \bm{Z}) \rightarrow \bm{p},
\end{equation}
where $\epsilon_\text{align}$ is a point cloud diffusion transformer, $\bm{p} \in \mathbb{R}^{N\times 3}$ is the transformed version of the scene-space partial point cloud to the canonical space, aligning with the generated object mesh. $\bm{Z}$ is the generated geometry latent of object corresponding to $\bm{p}$ from the object generation model. 
%
$\bm{p}_t$ is the noised version of $\bm{p}$ at timestep $t$.
%, $\bm{q} \in \mathbb{R}^{N\times 3}$ is the input partial point cloud (in world space) that may have a random transformation, and $\bm{z}$  is the geometry latent code from our object generation module. 
% 
In essence, the generation model maps the scene-space partial point cloud $\bm{q}$ to $\bm{p}$ in the canonical $[-1,1]^3$ space, aligning it with the generated object mesh. We can subsequently recover the similarity transformation (i.e., scaling, rotation, and translation) from $\bm{q}$ and $\bm{p}$ using the Umeyama algorithm~\cite{umeyama1991least} as they are point-wise corresponded. This final step is numerically more stable than directly predicting transformation parameters.% using ICP.%, as it exploits point-wise correspondences between $\bm{T}$ and $\bm{p}$.

% TODO: Add input normalization
In practice, we employ distinct conditioning strategies for the input point cloud $\bm{q}$ and the geometric latent $\bm{Z}$. For $\bm{q}$, we concatenate the input point cloud with the diffusion sample $\bm{p}_t$ along the feature channel dimension, enabling the transformer architecture to learn explicit correspondences between the noisy canonical-frame partial cloud and the world-space partial cloud.
%
For the geometric latent $\bm{Z}$, we apply a cross-attention mechanism to inject it into the point diffusion transformer. This approach ensures that the model effectively incorporates spatial and geometric relationships.
%
Additionally, due to symmetry and replicated geometrical shapes, multiple valid $\bm{p}$ may exist for a given $\bm{q}$ and $\bm{Z}$. Our diffusion model addresses this by sampling multiple noise realizations and aggregating the resulting transformations, to select the most confident and coherent representations.

%In practice, we use distinct conditioning strategies for $\bm{q}$ and $\bm{Z}$ respectively. We concatenate the point clouds $\bm{q}$ with diffusion latent $\bm{p}_t$ along feature channel dimension, allowing the transformer architecture to learn explicit correspondences between the noisy, canonical-frame partial cloud and the world-space partial cloud.
%While for the geometric latent $\bm{Z}$, we use cross attention mechanism to inject into the diffusion transformer.
%Moreover, given $\bm{q}$ and $\bm{Z}$, there may exists multiple valid $\bm{p}$ due to symmetry and replicated geometry shapes.
%Our diffusive model naturally handle these situations by simply sampling multiple noises and averaging more confident and coherent transformations. 
% 
%, reflecting variations in valid object orientation that still satisfy transformation priors and geometric constraints. 
%Hence, we train our transformation generator as a distribution esitmator.
%: 
%During training, for each training model we use VAE to encode it as $\bm{Z}$ and partially sample a set of surface points as $\bm{T}$, and then apply random transformation to synthesize $\bm{p}$.
% 
%By incorporating conditions from geometric latent, our transformation model can infer precise scaling, rotation, and translation parameters, providing better reliability and accuracy. 
% 
%Consequently, the transformation generative model forms a robust bridge between object geometry and real-world placement, paving the way for physically accurate and semantically consistent scene reconstructions.


%\paragraph{Transformation Disambiguity}
%A significant advantage of our transformation generation module is its ability to learn and represent a distribution of possible transformations through the diffusion process. By modeling transformations as a diffusion sampling, our system can generate multiple transformation samples for the same input partial point cloud, capturing the inherent variability in object orientations and scales. This capability is particularly beneficial for objects with symmetrical or non-unique rotational properties, such as circular fountains or square tables, where multiple valid rotations exist without altering the object's essential characteristics.
%
%For instance, consider a cylindrical water bottle that has a fixed up-axis (y-axis) but allows arbitrary rotation around this axis (x and z-axes). Sampling a single transformation in such cases would fail to capture the full range of valid orientations, potentially leading to misalignment or loss of geometric features. By generating multiple transformation samples, our approach ensures that the up-axis remains consistently aligned while allowing variations in the other axes. Averaging these multiple samples helps in accurately determining the dominant orientation and mitigating the impact of any outliers, thereby enhancing the overall alignment accuracy.
%
%To implement this, for each input partial point cloud, we generate up to 10 transformation samples. We then identify and remove outliers from these samples and compute the average transformation based on the remaining valid transformations. This process not only improves the reliability of the transformation estimation but also ensures that the generated 3D objects are accurately and consistently positioned within the scene, aligning with both visual and geometric cues from the input data.




% \subsection{Integration: Joint Iterative Generation}


% % {\color{red}{
% % The true strength of our approach lies in the seamless integration of object generation and transformation alignment through a joint, iterative process. Initially, the object generation module produces a geometry that closely resembles the input image but may not align perfectly with the input partial point cloud due to random transformations inherent in the point cloud data. To rectify this, the transformation generation module estimates the necessary transformations to align the generated geometry with the partial point cloud.

% % This alignment process is not a one-off adjustment but an iterative refinement. Once the initial transformation is applied, the updated partial point cloud and the input image provide a more accurate basis for generating the next iteration of the geometry. By repeatedly alternating between object generation and transformation estimation, the system progressively improves the alignment between the generated assets and the input data. This iterative approach ensures that the final geometry is not only visually consistent with the input image but also geometrically and spatially accurate within the scene.

% % The integration of these two modules addresses the limitations of relying solely on image or point cloud data. By leveraging both visual and depth information in a coordinated manner, our joint generation process achieves a harmonious balance between aesthetic fidelity and geometric precision. This results in 3D assets that are both high in quality and accurately positioned, laying the groundwork for constructing a physically correct and visually coherent scene.
% % }
% % }
% The true strength of our approach lies in the seamless integration of the object generation and transformation alignment modules through a joint, iterative process. This integration ensures that each generated 3D object is not only visually consistent with the input image but also accurately positioned and scaled within the scene. The iterative workflow can be summarized in three key steps:



% A key component of our pipeline is the joint iterative process, which tightly integrates the object generation module and the transformation generation module. This iterative approach progressively refines both the geometry and the alignment, ensuring accurate reconstructions. The process is outlined as follows:

% \paragraph{Step 1: Initial Geometry Generation} The object generation module $\text{ObjectGen}$ (Sec.~\ref{sec:objectgen}) produces a geometry latent code $\bm{z}^{(k)}$ at each iteration. This process is conditioned on the image features $\bm{c}$ (from DINOv2) and the current transformed partial point cloud $\bm{p}^{(k)}$ with conditioning scale factor $\beta^{(k)}$:
% \begin{equation}
%     \bm{z}^{(k)} = \text{ObjectGen}(\bm{c}, \bm{p}^{(k)} \otimes \beta^{(k)}).
% \end{equation}
% $\beta^{(k)}$ is the conditioning scale factor for controlling the impact of transformed partial point cloud conditioning. In the first iteration $k=0$, $\bm{p}^{(0)}$ has no valid value and is set to $q$ and set $ \beta^{(0)}=0$ which means do not use partial point cloud conditioning at the first iteration. $\{\beta^{(k)}\}$ is set to linearly grows from 0 to 1 during iteration.
% The latent code $\bm{z}^{(k)}$ is then decoded into a 3D geometry using the VAE decoder $\mathcal{D}$.

% \paragraph{Step 2: Transformation Estimation} Using the updated geometry latent code $\bm{z}^{(k)}$ and the world-space partial point cloud $\bm{q}$, the transformation generation module $\text{TransformationGen}$ (Sec.~\ref{sec:transformationgen}) predicts a transformed canonical-space partial point cloud $\bm{p}^{(k+1)}$:
% \begin{equation}
%     \bm{p}^{(k+1)} = \text{TransformationGen}(\bm{q}, \bm{z}^{(k)}) \odot \bm{q}.
% \end{equation}

% \paragraph{Step 3: Refinement of Geometry and Alignment} The rigid transformation is applied to update the partial point cloud, improving the alignment between the generated object and the observed scene data. This updated information is fed back into the iterative loop to refine the next generation of geometry and transformations.

% \paragraph{Convergence} This iterative process continues until the geometry and transformation alignments converge, resulting in a high-fidelity object reconstruction that is both visually accurate and spatially coherent.
% The iterative process continues until the change in transformation parameters falls below a predefined threshold or a maximum number of iterations is reached.


%\subsection{Integration: Joint Iterative Generation}
\subsection{Iterative Generation Procedure}

Recall that in our design, the object point cloud is unusable for object generation initially, as it is represented in the scene space, while our object generation model requires canonical-space point cloud for conditioning.
%
%The generated and aligned object xxx coarse, but xxx. Our design enables seamless integration of the object generation and alignment modules through a joint, iterative process. 
Solely depending on image cues for object generation often fails to produce pixel-aligned geometry, mainly because of the high-level semantic conditioning and biases inherent in 3D datasets.
Fortunately, our design enables seamless integration of the object generation and alignment modules through a joint, iterative process. 
This integration ensures that each generated 3D object is not only visually consistent with the input image but also accurately positioned and scaled within the scene. The iterative workflow with step index $k$ can be summarized in three key steps:

\noindent\paragraph{Step 1: Object Generation}
%
For an object image with mask, the \textbf{Object Generation} module (Sec.~\ref{sec:objectgen}) synthesizes the geometry latent code \(\bm{z}^{(k)}\) based on the image features \(\bm{c}\) derived from DINOv2 and the aligned point cloud \(\bm{p}^{(k)}\) in canonical coordinates. We set $\bm{p}^{(0)}$ to scene space point cloud $\bm{q}$ and set the point cloud conditioning scale factor \(\beta^{(k)}\) to progressively increase from 0 to 1 as iteration procedure goes on, allowing the partial point cloud to take influence over time. Formally, this process is represented as:
\begin{equation}
    \bm{z}^{(k)} = \text{ObjectGen}(\bm{c}, \bm{p}^{(k)} \otimes \beta^{(k)}).
\end{equation}
%In the initial iteration (\(k=0\)), \(\bm{p}^{(0)}\) is set to the input partial point cloud \(\bm{p}\) estimated from image with \(\beta^{(0)} = 0\). 
Hence our object generator solely relies on masked image conditioning in the first step. The latent code \(\bm{z}^{(k)}\) is then decoded into a 3D geometry using the VAE decoder \(\mathcal{D}\).

\noindent \paragraph{Step 2: Alignment}
%
Subsequently, the \textbf{Generative Alignment} module (Sec.~\ref{sec:transformationgen}) takes the newly generated geometry latent code \(\bm{z}^{(k)}\) and the partial point cloud \(\bm{q}\) in scene coordinates to predict a transformed canonical-space partial point cloud \(\bm{p}^{(k+1)}\):

\begin{equation}
\bm{p}^{(k+1)} = \text{AlignGen}(\bm{q}, \bm{z}^{(k)}).
\end{equation}

This transformed point cloud \(\bm{p}^{(k+1)}\) serves as an improved alignment reference for the next iteration. By leveraging the generative transformation model, the model ensures that the scaling, rotation, and translation adjustments are both precise and semantically informed.%, surpassing traditional methods like ICP in accuracy and stability.

\noindent \paragraph{Step 3: Refinement}
%
With the updated partial point cloud \(\bm{p}^{(k+1)}\), the system could estimate a new similarity transformation to refine the alignment of the generated geometry within the scene. This updated partial point cloud is then fed back into the \textbf{Object Generation} module for the next iteration, allowing for progressive enhancements in both geometry accuracy and spatial positioning.

%\paragraph{Convergence}
\medskip
This iterative loop—alternating between geometry generation and transformation estimation—continues until convergence criteria are met. Convergence is achieved when the changes in transformation parameters fall below a predefined threshold or when a maximum number of iterations is reached. The result is a high-fidelity 3D object that is both visually accurate and geometrically aligned with the input data.
%
%\medskip
By tightly integrating the \textbf{Object Generation} and \textbf{Alignment Generation} modules within an iterative framework, our approach effectively balances aesthetic fidelity with geometric precision. This joint generation process leverages both visual and depth information, ensuring that each 3D asset is of high quality and accurately positioned. Consequently, the pipeline lays a robust foundation for constructing physically correct and visually coherent 3D scenes, facilitating a wide range of downstream applications such as editing, rendering, and animation.

Once the object geometry is determined, we apply a state-of-the-art texture generation module to create photo-realistic surface details. Following established texture synthesis pipelines~\cite{zhang20233dshape2vecset,zhang2024clay}, we assign UV mappings and train a generative network to paint detailed textures onto the 3D meshes. This module is designed to robustly handle images under various augmentation, ensuring that the final textures match the input appearance even under occlusion or limited visibility.


% While the transformer-based RTS estimation provides initial placement parameters, achieving precise alignment within the 3D scene necessitates further refinement. We introduce an iterative refinement process that evaluates alignment quality and adjusts RTS parameters accordingly to ensure high-fidelity object placement. This process enhances the accuracy and quality of RTS parameter estimation, ensuring that each object's placement meets predefined alignment and quality criteria.


% \paragraph{Refinement Algorithm}
% The refinement process is fully automated, eliminating the need for manual intervention and ensuring consistent and reliable placement of objects within the 3D scene. The iterative loop operates as follows:IoU Computation:
% Calculate the IoU between the point cloud and the current mesh reconstruction to assess alignment quality.RTS Adjustment:If the IoU falls below the predefined threshold, the transformer model predicts a refined set of RTS parameters based on the current alignment status.Reconstruction Update:Apply the refined RTS parameters to the 3D mesh, updating its position and orientation within the scene.Iteration:Repeat the IoU computation and RTS adjustment steps until the IoU meets or exceeds the threshold or a maximum number of iterations is reached.

% To optimize computational efficiency, the iterative refinement loop incorporates adaptive learning rates and early termination criteria, preventing excessive iterations and reducing processing time. Additionally, parallel processing strategies are employed to handle multiple objects simultaneously, further enhancing the efficiency of the refinement process.

% The iterative refinement ensures that each object's placement is both geometrically accurate and contextually coherent, addressing common challenges such as object misalignment and geometric inconsistencies. By systematically refining RTS parameters based on quantitative quality metrics, we achieve high-quality and realistic 3D scene reconstructions that faithfully represent the original input image.