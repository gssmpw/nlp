


%Significant advances have been made on single object generation from either text or image prompts. These range from neural rendering approaches ~\cite{poole2022dreamfusion,wu2024unique3d,wang2024prolificdreamer} that aim to optimize over implicit representations conditioned on input prompts to more recent native 3D generators~\cite{zhang20233dshape2vecset,zhang2024clay,xiang2024structured} that directly create 3D shapes and textures via end-to-end learning.
%Conceptually, these methods can be applied to generate a scene conditioned on an image, by producing individual objects one by one. But their results are far from satisfactory. The first major hurdle lies in estimating object poses: existing methods unanimously assume that the object’s primary orientation is approximately aligned with the camera view where real-world objects are rarely view aligned —they may exhibit strange poses by design or by physics, and can be partially occluded. Yet, few existing methods were explicitly designed to address pose alignment as their primary goal is geometry.

%A more detrimental and at the same time more intriguing problem arises from inter-object spatial relationships, or more precisely, the lack of them. Even with rather accurate pose estimations, the generated/recovered objects can easily penetrate one another, or items that should touch or support each other instead appear floating or disconnected. These errors arise fundamentally from the lack of the spatial and physical relationships that tie objects together as they do to our human society. Although recent methods such as \cite{liu2022towards,zhang2024improving} attempt to implicitly embed such relationships using encoder-decoder architectures, they remain limited to specific domains like indoor environments. Other scene-level generators~\cite{dogaru2024generalizable} generate objects directly in the world coordinate system without modeling their relative poses, let alone physical dependencies. Consequently, the generated scenes often lack practical usability, limiting their effectiveness for downstream tasks such as editing, animation, and simulation.



%To this end, we introduce \methodname, a \emph{C}omponent-\emph{A}ligned 3D \emph{S}cene recons\emph{T}ruction method, featuring compositional reconstruction of a 3D scene by generating pixel-aligned 3D object components from a single RGB image. 3D scenes reconstructed by \methodname consist of separate, high-quality 3D meshes with their similarity transformations (rotation, translation, scale) not only accurately aligned with the reference image but with physically sound interdependencies.
% TODO: emphasize what CAST is capable of and enables
%CAST takes an unannotated and unstructured RGB image as input and preprocess it using 2D foundation models (e.g., Florence-2~\cite{}, Grounded-SAM~\cite{}), to recognize, localize, and segment objects within the scene in an open-vocabulary way. Off-the-shelf monocular depth estimators~\cite{} are then used to obtain a partial 3D point cloud of each segmented object as well as an initial inter-object spatial relationships in terms of their relative transformations and scales, within the context of the image.

%The first core component of \methodname is a \red{\emph{perceptive 3D instance generator}} that consists of an occlusion-aware object generation module and a pose alignment generation module. For the object generation module, we train a large latent diffusion-based generative model based on latent diffusion, to produce complete, high-fidelity object meshes for each component conditioned on the partial image segments obtained during preprocessing and, optionally partial point clouds. Specifically, \methodname employs an occlusion-aware 2D image encoder, capable of robustly inferring occluded regions, to capture high-level image features for image conditions. Besides, we devise a training pipeline that simulates real-world or estimated partial point clouds with occlusions as point-cloud conditioning, providing dense geometric cues and thereby enhancing the model's robustness.

%For the pose alignment module, we propose a transformation generative model conditioned on the partial point cloud in \red{camera} space and the geometry latent code in canonical space produced by the object generation module. This model generates a transformed partial point cloud in canonical space that aligns with the latent complete geometry, enabling the estimation of the similarity transformation between them. Compared to directly regressing poses~\cite{}, our alignment is estimated through generation, allowing us to better capture the multi-modal nature of pose alignment from a single view. These two modules are designed to work iteratively, refining both the geometry and the alignment with each iteration.

%The second core component of \methodname aims to model inter-object spatial relationships. Notice that even when the 3D object and alignment generation modules achieve precise pixel alignment, artifacts such as object penetration or floating will inevitably occur, due to lack of physical constraints. These physical inconsistencies not only make the reconstructed scene perceptually unrealistic but also hinder downstream tasks, particularly physical simulation, by providing poor initial state. To address this, \methodname introduces a physics-aware correction process to optimize object poses based on commonsense knowledge extracted from the reference image. Specifically, GPT-4V~\cite{} is used to identify physical relationships grounded in the image, and optimization is performed based on constraints derived from these relationships, ensuring physical plausibility. The resulting 3D scene closely aligns with the reference image while exhibiting significantly improved physical coherence, which offers highly realistic, production-ready digital assets suitable for editing and rendering.


Humans exist within clear networks of relationships—family, friends, coworkers—that guide our decisions and behaviors. These connections shape our world and give it structure. Similarly, objects in a space also function within their own networks~\cite{latour2005reassembling}, but less noticed. They do not just exist in isolation; their placement, design, and material arise from physical constraints, functional roles, and human design intentions and influence how we move, interact, and perceive space. For example, a chair leans against a table for support, a cup rests on a saucer, and a lamp’s light interacts with surrounding surfaces, casting shadows that shape the overall scene. Recognizing these relationships is critical for accurate scene parsing, modeling, and, more recently, 3D generation, ensuring virtual environments feel as realistic and coherent as the real world.

Significant progress has been made in generating single objects from text or image prompts. Neural rendering approaches~\cite{poole2022dreamfusion,wang2024prolificdreamer} optimize implicit representations, while native 3D generators~\cite{zhang20233dshape2vecset,zhang2024clay,xiang2024structured} directly create 3D shapes and textures via end-to-end learning. While these methods show promise for individual objects, applying them to generate entire scenes by assembling objects sequentially results in notable shortcomings. A key challenge is accurate pose estimation. Existing methods often assume objects are view-aligned, which is rarely the case in real-world scenes. Objects may appear in diverse orientations, constrained by design, physics, or partial occlusion. Yet, most existing methods prioritize geometric fidelity over pose alignment, leaving this critical aspect underexplored.

An even more fundamental issue arises from the lack of inter-object spatial relationships. Even with accurate poses, generated scenes often suffer from physically implausible artifacts: objects penetrate one another, float, or fail to make contact where necessary. These errors stem from the absence of spatial and physical constraints that naturally bind objects together, much as human relationships structure our social world. While some recent methods~\cite{liu2022towards,zhang2024improving} encode spatial relationships implicitly using encoder-decoder architectures, they remain limited to specific domains such as indoor scenes. Other scene-level generators~\cite{dogaru2024generalizable} position objects in a global coordinate system but neglect their relative poses and dependencies, further compromising realism and usability for downstream applications like editing, animation, and simulation.

To this end, we propose \methodname, a \emph{C}omponent-\emph{A}ligned 3D \emph{S}cene recons\emph{T}ruction method for compositional reconstruction of a 3D scene from a single RGB image. \methodname generates high-quality 3D meshes for individual objects, along with their similarity transformations (rotation, translation, scale), ensuring alignment with the reference image and enforcing physically sound interdependencies. \methodname starts by processing an unstructured RGB image using 2D foundation models (e.g., Florence-2~\cite{xiao2024florence}, GroundingDINO~\cite{liu2025grounding}, SAM~\cite{ravi2024sam}, Grounded-SAM~\cite{ren2024grounded}) to recognize, localize, and segment objects in an open-vocabulary manner. Off-the-shelf monocular depth estimators~\cite{wang2024moge} provide partial 3D point clouds and initial estimates of inter-object spatial relationships, including relative transformations and scales.

The first core component of \methodname is our perceptive 3D instance generator with two modules: an occlusion-aware object generation module and a pose alignment generation module. The object generation module employs a latent diffusion-based generative model to produce high-fidelity object meshes conditioned on partial image segments and optional point clouds. This module incorporates an occlusion-aware 2D image encoder capable of inferring occluded regions, ensuring robust feature extraction for image conditions. To improve robustness to real-world point cloud conditioning, we simulate partial point clouds with occluded regions during training, enabling the model to handle occlusion effectively. 
% The pose alignment module estimates similarity transformations by aligning partial point clouds in camera space with complete object geometry in canonical space. Unlike direct pose regression~\cite{kehl2017ssd,labbe2020cosypose}, this module generates transformations iteratively, capturing the multi-modal nature of pose alignment and refining accuracy with each iteration.
The pose alignment module features an alignment generative model that produces a transformed partial point cloud, aligning with the complete geometry implicitly represented in the latent space. The similarity transformation is derived from the generated transformed point cloud and the partial point cloud estimated from the camera. Unlike direct pose regression methods~\cite{kehl2017ssd,labbe2020cosypose}, our method estimates transformations through generation, capturing the multi-modal nature of pose alignment.

The second core component of \methodname addresses inter-object spatial relationships. Despite accurate pixel alignment, physically implausible artifacts such as penetration or floating can occur without explicit modeling of physical constraints. \methodname introduces a physics-aware correction process to ensure spatial and physical coherence. GPT-4v~\cite{achiam2023gpt} is utilized to identifies commonsense physical relationships grounded in the input image, which are then used to optimize object poses based on these constraints. This process ensures that reconstructed scenes exhibit realistic physical interdependencies, making them suitable for applications like simulation, editing, and rendering.



Remarkably, \methodname excels at generating perceptually realistic 3D scenes from a wide range of images, whether they are sourced from indoor or outdoor settings, real-world captured, or AI-generated. Unlike previous approaches~\cite{liu2022towards,dai2024automated}, \methodname supports open-vocabulary reconstruction, even for challenging, in-the-wild images, thanks to our deliberate pipeline design.
Quantitatively, \methodname surpasses strong baselines in the indoor dataset, 3D-Front~\cite{fu20213d}, regarding object- and scene-level geometry quality. It also outperforms on perceptual and physical realism across a diverse set of images, including in-the-wild scenarios, as verified by visual-language models and user studies.

Given only a single image, \methodname can faithfully reconstruct the scene, with detailed geometry, vivid textures of the objects, and more importantly, the spatial and physical interdependencies between them. 
This capability democratizes virtual creation: a single snapshot of a room or outdoor space becomes a fully realized 3D environment, where objects are precisely posed, interact naturally, and account for occlusions. Game developers can integrate real-world setups into immersive landscapes, and filmmakers can effortlessly generate intricate virtual sets—unlocking creative potential.
Beyond entertainment, \methodname paves the way for smarter robots. It can facilitate the real-to-simulation pipeline~\cite{li2024evaluating,torne2024reconciling} by enabling robotics researchers to construct digital replicas from real-world demonstration datasets with more efficient and scalable simulation workflows.






% \methodname can further facilitate the real-to-simulation pipeline~\cite{} for robotics by constructing digital scenes from single images in existing real-world robot demonstration datasets.

% \methodname supports open-vocabulary reconstruction and is capable of generating perceptually realistic scenes even from challenging, in-the-wild images. User studies indicate that our approach is strongly favored in terms of alignment with the input image and physical plausibility. Quantitatively, \methodname outperforms strong baselines on indoor 3D datasets, excelling in pose alignment and geometry quality. Qualitatively, we demonstrate its effectiveness on a diverse range of images, including synthetic renders, AI-generated content, and real-world outdoor scenes. Furthermore, \methodname supports a variety of downstream applications, such as 3D-consistent image editing, generating interactive assets for games, and real-to-sim transfer for robotics, showcasing its versatility and practicality across domains.

% Jiayuan's keywords for applications
% open-vocabulary, focus on in-the-wild, heterougenous source source of images, diverse layouts, indoor, outdoor, real-world, synthetic, AI-generated.
% object-centric -> actor network theory
% (vs. Gen3DSR) production-ready, for existing CG pipelines, canonical space better quality
% single generation -> editting
% relationship -> keep functionality, commonsense
% spatial relationship and physical relationship
% network implies functionality, affordance
% CG pipeline: editting, for film, lower cost for editting scene, rather than reconstruct holistic scene
% gaming: cultural style is kept, (how objects are placed), commonsense, immersive experience (virtual and reality close)
% robotics: objects have their functionality, important for TAMP, as you need to first move the object stacked above, and you need to avoid object leaning against falling down

%In conclusion, \methodname introduces a revolutionary approach to 3D scene reconstruction, blending open-vocabulary capabilities with a powerful understanding of spatial and physical relationships. By focusing on the object-centric perspective, our method constructs a network of interconnected objects that embodies not only their geometric structures but also their functionality and affordances. This enables the generation of realistic scenes from an astonishingly diverse array of input sources—ranging from in-the-wild, real-world images to synthetic, AI-generated content—capturing the nuances of indoor, outdoor, and even culturally distinct layouts. Unlike existing methods such as Gen3DSR, which treat scene reconstruction as a holistic task, our approach allows for production-ready 3D models that integrate seamlessly into existing CG pipelines, offering significantly higher quality and flexibility in canonical space. The capacity for single-generation followed by easy editing opens new doors for efficient scene manipulation in gaming, where the authenticity of object placement and commonsense interactions fosters deeply immersive experiences. Likewise, in robotics, understanding the functional role of each object in relation to others is crucial for precise task and motion planning (TAMP), where proper handling and stability of objects ensure safe, effective interactions. With \methodname, we offer not just an advancement in 3D reconstruction but a pathway to smarter, more intuitive virtual and physical environments, where functionality and aesthetics align to redefine what’s possible in gaming, robotics, and beyond.


% %insights
% In this paper, we introduce CAST, an innovative 3D scene reconsrtuction method designed to generate high-quality 3D instances from a single RGB image. By independently generating each object at the component level, ensuring precise pixel alignment in both geometry and texture, CAST assembles a complete 3D scene without relying on a globally generated model. Although this approach achieves relatively accurate pixel alignment, artifacts such as penetration and floating objects may still occur. To mitigate these issues, CAST incorporates a physical constraint mechanism that fine-tunes the scene based on the mutual relationships among objects, thereby producing more accurate, higher-quality, and physically consistent 3D results.


% %具体怎么做
% In our approach, the reconstruction process is divided into three main steps. First, the scene decomposition module extracts 2D segmentation information of both the background and individual objects from a single RGB image, while also estimating each object's depth. During this stage, GPT-based models are employed to analyze the physical relation graph among objects, thus enhancing the understanding of their interactions.
% Next, guided by the segmentation and depth information, we employ a large-scale 3D generation framework equipped with a transformation generation model to reconstruct high-quality 3D instances. By incorporating occlusion and point cloud priors, we ensure precise pixel-level alignment and enable the recovery of complete 3D shapes, even when objects are partially occluded.
% Finally, to maintain realistic spatial relationships and natural interactions among objects, we introduce physical correction techniques that preserve physical plausibility and dependencies. Through this three-step pipeline, CAST achieves high-quality, physically accurate, and pixel-aligned 3D scene reconstructions from a single RGB image.


%升华，应用
% Our method not only addresses the limitations of existing approaches but also offers crucial support for practical applications, such as virtual reality and robotics, particularly in scenarios where data availability is restricted, thereby showcasing substantial potential. Furthermore, because CAST can generate high-quality 3D digital assets in a canonical space, it greatly facilitates asset standardization and downstream tasks, including editing, rendering, animation, understanding, and processing. This marks a substantial advancement in single-image 3D reconstruction, providing more reliable and efficient solutions for future intelligent devices and real-time applications.

% By uniting precise geometry, pixel-level alignment, and physically grounded constraints, CAST substantially advances the capabilities of single-image scene reconstruction methods, where generating complete, high-fidelity 3D scenes from limited reference images can significantly reduce manual modeling overhead. Moreover, CAST not only simplifies scene creation but also supports fine-grained object editing, allowing designers to reposition, retexture, or animate individual elements without rebuilding the entire scene. Moreover, the physically consistent 3D assets produced by CAST are valuable in real2sim pipelines, especially for embedded AI applications in robotics, where accurate spatial relationships and interactions are essential.







%我们的眼睛就像一扇窗，记录下了世界的平面投影，而我们，作为三维生命，拥有独特的能力，能够从这些平面图像中看到立体的世界，理解其中每个物体的形状、位置以及它们之间复杂的关系，这种能力赋予了我们探索和理解周围环境的力量，从而帮助我们认知这个丰富多彩的世界。AI智能体也应当具备这样的能力，像我们一样，从图像中感知并理解这个立体的世界。然而，当计算机仅凭一张RGB图像时，它看到的只是平面上的颜色和像素，缺乏深度和物体之间的空间依赖。如何从这个静止的二维图像中提取出三维的结构，推断出物体间的关系与相互作用，成为了一项巨大的挑战。如果计算机能够像我们一样，从一张图像中恢复出物体的三维形态，并理解它们之间的空间联系和物理互动，它就能真正“看到”这个世界，像我们一样感知周围的环境。这种能力不仅能为虚拟现实和增强现实带来更加真实的空间感知，还能为机器人和自动驾驶技术提供更精准的环境理解，帮助从静态的图像中创造出动态、层次丰富的三维场景，揭示出物体间错综复杂的物理互动和空间依赖。
%抒情+引入

%随着计算机图形学和三维场景表达技术的不断进步，从真实世界数据创建逼真虚拟环境的能力逐渐得到了拓展。然而，目前大多数现有方法依赖多张图像或深度传感器进行三维重建，这使得其应用受限，特别是在仅有单张RGB图像的情况下。单张图像的场景级别的三维重建在硬件要求、计算资源和多模态输入上都存在局限，这限制了其在实时应用和消费级设备中的实际应用。尽管近年来已有一些研究尝试从单张RGB图像进行三维场景的再现，一些方法\cite{SSR,instintpifu}在大量的现有室内数据集上进行训练后，使用神经网络来训练encoder和decoder，达到了单图infer出整个场景不同种类几何的能力,但是这些方法只能在室内场景进行使用，直接重建整个场景的方法也没有把物件单独区分开，难以编辑，交互，使得没有很好的下游任务.还有大多数基于检索数据库的场景恢复方法，虽然能够最大程度上恢复出与infer场景相似的场景，但通常同样局限于室内场景，并且无法泛化到复杂或动态场景，非常基于数据库的丰富程度。基于生成再组装的模型（如Gen3DSR等）虽然能生成场景，但是大多数方法目前是生成在世界坐标原地的，几何质量都很差，更重要的是，Canonical Space实际上是艺术家定义的坐标系和标准，生成在这个空间中就和艺术家一个标准，对于资产标准化、下游应用如编辑、渲染、动画、理解、处理都非常方便。更重要的是，目前现有的方法往往无法有效处理物体间的物理关系。没有考虑物理关系的保持，这对于实现真实感和互动性至关重要，也直接影响了整个场景是不是可用的。
%

%为了应对这些挑战，CAST: Component-Aligned 3D Scene Transformation from a Single RGB Image 提出了一个创新的三维场景重建方法。我们的目标是从单张RGB图像中生成每个instance，组成完整的三维场景，确保每个物体在几何形状、纹理以及物理关系上与图像像素对齐。我们的方法通过组件级重建的方式，将每个物体独立恢复并与图像像素进行精确对齐，而不是依赖于全局场景的直接生成。通过这种方式，CAST 突破了现有方法的局限，能够生成更高质量、精确且物理上一致的三维生成结果。
%一句话提出我们的做法，为什么能解决问题


%在我们的方案中，重建过程分为三个主要步骤。首先，我们通过场景分析模块，从输入的RGB图像中提取背景和物体的二维分割信息，同时估算每个物体的相对深度。这一阶段还包括利用基于GPT的模型分析物体之间的相对关系，以帮助理解场景中的物体互动。接下来，利用这些分割和深度信息，我们使用带位姿感知的大型三维生成模型来恢复每个物体的三维形态，利用遮挡和点云的先验信息确保它们与输入图像像素精准对齐，这使得即使在物体部分遮挡的情况下，模型也能成功恢复物体的完整三维形态。最后，为了保证物体之间的物理关系与自然交互，我们引入物理仿真技术，以确保每个物体的依赖关系、受力情况和遮挡关系在场景恢复过程中得到准确恢复。通过这一创新的三步重建方法，CAST 实现了从单张RGB图像中生成高质量、物理精确且像素对齐的三维场景重建。我们的方法不仅克服了现有技术的不足，还为虚拟现实、机器人等实际应用提供了重要的支持，尤其是在数据输入受限的情况下，展现了巨大的潜力。CAST 的推出标志着单图像三维重建技术的重要突破，为未来的智能设备和实时应用提供了更为可靠和高效的解决方案。
%具体做法+application



% 1. Background
%我们的眼睛就像一扇窗，记录下了世界的平面投影，而我们，作为三维生命，拥有独特的能力，能够从这些平面图像中看到立体的世界，理解其中每个物体的形状、位置以及它们之间复杂的关系，从而帮助我们认知这个丰富多彩的世界。AI智能体也应当具备这样的能力，像我们一样，从图像中感知并理解这个立体的世界。然而，当计算机仅凭一张RGB图像时，它看到的只是平面上的颜色和像素，缺乏深度和物体之间的空间依赖。如何从这个静止的二维图像中提取出三维的结构，推断出物体间的关系与相互作用，成为了一项巨大的挑战。如果计算机能够像我们一样，从一张图像中恢复出物体的三维形态，并理解它们之间的空间联系和物理互动，它就能真正“看到”这个世界，像我们一样感知周围的环境。这种能力不仅能为虚拟现实和增强现实带来更加真实的空间感知，还能为机器人和自动驾驶技术提供更精准的环境理解，帮助从静态的图像中创造出动态、层次丰富的三维场景，揭示出物体间错综复杂的物理互动和空间依赖。

% 2. problem of related work
%随着计算机视觉和三维重建技术的不断进步，从真实世界数据创建逼真虚拟环境的能力逐渐得到了拓展。然而，目前大多数现有方法依赖多张图像或深度传感器进行三维重建，这使得其应用受限，特别是在仅有单张RGB图像的情况下。单张图像的三维重建在硬件要求、计算资源和多模态输入上都存在局限，这限制了其在实时应用和消费级设备中的实际应用。尽管近年来已有一些研究尝试从单张RGB图像进行三维重建，但大多数方法依然存在精确对齐物体与图像像素、保持物体间关系和物理对齐方面的不足。
% zlw: 目前场景生成/重建方法以retrival为主，不能泛化，限定在室内场景；以gen3dsr为主的生成，单个资产质量很差
% ykx from 345:直接生成一整个场景无用，没有下游任务 （feed forward),(直接生成在原本位置的)不在canonical space的生成质量差，并且不利于下游任务，不利于编辑与修改 (some composition method)

% 3. Our key idea
%为了应对这些挑战，CAST: Component-Aligned 3D Scene Transformation from a Single RGB Image 提出了一个创新的三维场景重建方法。我们的目标是从单张RGB图像中重建完整的三维场景，并确保每个物体在几何形状、纹理以及物理关系上与图像像素对齐。我们的方法通过组件级重建的方式，将每个物体独立恢复并与图像像素进行精确对齐，而不是依赖于全局场景理解。通过这种方式，CAST 突破了现有方法的局限，能够生成更高质量、精确且物理上一致的三维重建结果。
% zlw：生成方法而非重建方法？

% 4. technical pipeline
%在我们的方案中，重建过程分为三个主要步骤。首先，我们通过场景分析模块，从输入的RGB图像中提取背景和物体的二维分割信息，同时估算每个物体的相对深度。这一阶段还包括利用基于GPT的模型分析物体之间的相对关系，以帮助理解场景中的物体互动。接下来，利用这些分割和深度信息，我们使用带位姿感知的大型三维生成模型来恢复每个物体的三维形态，并确保它们与输入图像像素精准对齐。即使在物体部分遮挡的情况下，模型也能成功恢复物体的完整三维形态。最后，为了保证物体之间的物理关系与自然交互，我们引入物理仿真技术，以确保每个物体的依赖关系、受力情况和遮挡关系在重建过程中得到准确恢复。通过这一创新的三步重建方法，CAST 实现了从单张RGB图像中生成高质量、物理精确且像素对齐的三维场景重建。我们的方法不仅克服了现有技术的不足，还为虚拟现实、机器人等实际应用提供了重要的支持，尤其是在数据输入受限的情况下，展现了巨大的潜力。CAST 的推出标志着单图像三维重建技术的重要突破，为未来的智能设备和实时应用提供了更为可靠和高效的解决方案。


% %
% To summarize, our main contributions include:


% \begin{itemize}
%     \item  \textcolor{blue}{TODOTODOTODOTODOTODOTODOT}

    
%     \item \textcolor{blue}{TODOTODOTODOTODOTODOTODOT}


%     \item  \textcolor{blue}{TODOTODOTODOTODOTODOTODOT}
    
% \end{itemize}
