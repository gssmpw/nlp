\section{Scene decomposition}

\begin{figure*} [ht]
  \centering
  \includegraphics[width=\textwidth]{figs/pipeline1.png}
  \captionof{figure}{
  Overview of CAST. The input RGB image is processed through scene analysis to extract key information, followed by perceptive 3D instance generation. Physical-aware correction module using ensures realistic interactions and spatial relationships, yielding a high-quality, mesh-based 3D scene.
  } \label{fig_overview1}
\end{figure*}

Scene-level reconstruction from a single image is a fundamental challenge in computer graphics, with broad applications in animation, virtual reality, and interactive gaming. Unlike object-level reconstruction, which focuses on isolated objects, scene-level reconstruction emphasizes the arrangement and interplay of multiple entities under realistic (or stylized) physics. By capturing per-object structures, spatial relationships, and contextual cues, this holistic approach enables more immersive experiences, compelling narratives, and efficient workflows—benefits that surpass those of single-object reconstructions. Although previous methodologies have explored feed-forward pipelines or retrieval-based approaches using fixed 3D templates [xxx], these methods often struggle to capture nuanced scene semantics and complex object relationships. To address these limitations, we propose a generation-driven scene “reincarnation” approach that constructs high-fidelity, contextually consistent 3D environments from a single, unannotated RGB image—whether sourced from real-world photography or synthetic data.

A key insight of our method is the thorough analysis of scene contextual information. First, we perform object segmentation to identify and localize constituent objects within the image. We then obtain preliminary geometric information, i.e., point clouds, and explore semantic and spatial relationships among objects. This contextual backbone informs our subsequent object-wise generation pipeline, ensuring that each reconstructed object retains not only its geometric fidelity but also its correct placement in the broader scene. Finally, we synthesize a cohesive 3D environment that respects physical plausibility—achieving structurally sound layouts and realistic interactions among scene elements.

Our research is guided by two primary questions:

How can generative models effectively capture complex inter-object relationships to produce realistic, scene-level reconstructions from a single image?
What strategies for integrating geometric cues and contextual information yield the highest accuracy and plausibility in 3D reconstructions?

By answering these questions, we demonstrate that generative methods offer a more flexible and robust alternative to feed-forward or retrieval-based techniques. They allow for fine-grained control over object-level detail and global scene composition, thereby streamlining content creation pipelines for animation, game development, and other areas that require accurate and visually compelling 3D models. This work not only illustrates the advantages of a generation-centric framework but also lays the groundwork for future advances in scene-level 3D reconstruction, highlighting the growing importance of context-driven approaches in bridging the gap between 2D imagery and rich, interactive virtual environments.


%Scene-level reconstruction from a single image is crucial for bringing real-world environments into digital world, with application including animation, xxx and xxx. Compared to object-level reconstruction, scene-level reconstruction captures the big picture—the arrangement of multiple objects and their interplay under realistic (or stylistic) physics. This holistic approach is pivotal for rendering realistic visuals, enabling interactive experiences, and delivering compelling narratives, making it significantly more impactful for many computer graphics applications than focusing solely on individual objects. 
%
%Previous methodologies feed-forward, retrieval template xxxx. 
%In this paper, we present a generation driven scene generation / reincarnation approach. Why generation better? The input to our method is a single, un-annotated RGB image, which can either be a real-world photograph or synthesized image.
%
%To obtain a high-precision scene that closely aligns with the layout and structural composition of the input image, a indespensible step is to analyze the scene contextual information embeded in the input image. 
%
%To maximize the capture of the rich information and facilitate accurate scene-level generation, we first perform object segmentation and acquires preliminary geometric information (e.g., point clouds). We also explores the semantic and spatial relationships among objects within the scene via.... 
%We then coduct object-wise generation xxx, in this processs, we xxx.
%Finally, we place xxx and generate physically plausible xxx.

%This comprehensive analytical process ensures that the generative model can accurately reflect the complex scene of the input image in both structural and semantic aspects, enabling the generation of high-quality scenes.

%我们的方法仅需输入一张未经标定的单一RGB图像，输入图像可来自现实世界中任意拍摄的照片或各种风格的合成数据。为了实现与输入图像在布局和场景结构上高度一致的高精度场景生成，场景信息的提取与分析至关重要。为最大程度地捕捉图像中蕴含的丰富信息，并实现场景级别的精确生成，我们的方法不仅在图像层面对场景进行细致的物体分割，获取几何初步信息（e.g点云）作为先验，还深入挖掘场景中各物体之间的语义和空间关系。这一综合性的分析过程确保了生成模型在结构和语义层面能够准确反映输入图像的复杂场景，从而实现高质量的场景生成。

%Our method requires only a single, uncalibrated RGB image as input, which can originate from any real-world photograph or various styles of synthetic data. To achieve high-precision scene generation that closely aligns with the layout and structural composition of the input image, extraction and analysis of scene information are indispensable. To maximize the capture of the rich information embedded in the image and facilitate accurate scene-level generation, our approach not only performs meticulous object segmentation at the image level and acquires preliminary geometric information (e.g., point clouds) as priors, but also thoroughly explores the semantic and spatial relationships among objects within the scene. This comprehensive analytical process ensures that the generative model can accurately reflect the complex scene of the input image in both structural and semantic aspects, enabling the generation of high-quality scenes.

% \lipsum[1]

\subsection{Semantic and Contextual Reasoning} 
% \lipsum[1-2]

%在场景分析的初步阶段，基于图像的语义分析能够简洁且清晰地提取整个场景的主要信息。我们采用了GPT-4模型，该模型能够识别场景中每一个有意义的组成物体，并且不受限于预定义的类别集合，实现了开放词汇open-vocabulary的识别能力，可以将输入的RGB图像精确地分割成多个具有语义标签的物体区域。随后，利用GroundedSAM-v2，这是一种基于语义和边界框的分割方法，对场景图像中的物体进行进一步提取，获得一系列对应标签的目标物体掩码。该分割方法能够有效避免多个物体粘连导致的分割不清晰问题，从而获得单独且清晰的物体部分。除了获取各个物体的掩码外，由于已经确定了每个物体的所在区域及其他物体的掩码，我们还能够相应地生成每个不同物体的occlusion mask，这对于生成模型如何能够最大程度上分辨遮挡，缺失对于生成结果分布的影响具有极大的作用，为后续的物体生成过程提供了极大的辅助支持。

%TODO 这里可以给一些caption的符号，mask的符号的定义
%In the initial stage of scene analysis, we conduct a semantic extraction process on the input image to obtain the primarily information and lay a foundation for entire scene analysis. 
%
To facilitate comprehensive scene reconstruction from a single image, we first perform an extensive semantic extraction that provides a robust foundation for subsequent processing. Specifically, we employ Florence-2~\cite{xiao2024florence} to identify objects, generate descriptive metadata, and localize each object with bounding boxes. We then leverage GPT-4v~\cite{achiam2023gpt} to filter out spurious detections and isolate meaningful constituent objects, allowing for open-vocabulary object identification that is not constrained by predefined categories. Next, we use GroundedSAM-v2~\cite{TODO} to produce refined segmentation masks for each labeled object, thereby obtaining both precise object boundaries and corresponding occlusion masks, which play a crucial auxiliary role in the object generation stage.
%
Beyond semantic cues, we also integrate geometric information by extracting a scene-level point cloud. Using MoGe~\cite{TODO}, we generate pixel-aligned point clouds, depth maps, normal maps, and camera parameters. This additional geometric data is subsequently matched to each object’s segmentation masks, providing a reliable structural reference for the final 3D scene reconstruction.
%
By combining semantic extraction with robust geometric data, our approach ensures that each object is accurately represented in terms of both appearance and spatial configuration, creating a strong backbone for downstream scene-level generation and analysis.

%To effectively recover the scene, we distill as much information from the input image and lay foundations for subsequent process. We start by conducting a semantic extraction process on the input image to obtain the primarily information and lay a foundation for entire scene analysis. 
%Specifically, we use the Florence-2~\cite{xxx} to extract the objects, descriptions and their corresponding bounding boxes from the input image. We further apply GPT-4v~\cite{TODO} to filter out noise detections, and identify the meaningful constituent objects within the scene. Such approach open-vocabinarily genereate clean object set without being restricted to a predefined set of categories. Subsequently, we use GroundedSAM-v2~\cite{TODO} to obtain a series of refined target object masks corresponding to their labels. 
%To this stage, we obtain individual object part with clear boundaries, and also generate occlusion masks for each object, for providing substantial auxiliary support for the subsequent object generation process.
%Besides the object semantics, we additionally extract the scene point cloud and match the point cloud with each object to provide geometric reference for the final 3D scene. we utilize MoGe~\cite{TODO} as the point cloud generation tool. In addition to pixel-aligned point clouds, MoGe also provides camera parameters, depth maps, and normal maps for each object.
To better address errors at boundaries, we use normal and depth xxx.
The semantics and point cloud provide xxx.
%为后续的纹理映射和物理仿真提供了坚实的基础。这不仅确保了生成的三维模型在视觉属性上的高度一致性，还保证了其在物理属性上的可靠性。

%We employ the GPT-4v model~\cite{TODO} to identify all meaningful constituent objects within the scene, without being restricted to a predefined set of categories. 
%This allows for the precise segmentation of the input RGB image into multiple object regions, each annotated with semantic labels. Subsequently, we use GroundedSAM-v2~\cite{TODO}, a semantics-based segmentation method and bounding boxes, to further extract objects from the scene image, obtaining a series of target object masks corresponding to their labels. 
%
%thereby achieving open-vocabulary recognition.
%In the initial stage of scene analysis, images-based semantic analysis can clearly and effectively extract the primary information of the entire scene. We employ the GPT-4v model~\cite{TODO}, which is capable of identifying every meaningful constituent object within the scene without being restricted to a predefined set of categories, thereby achieving open-vocabulary recognition. This allows for the precise segmentation of the input RGB image into multiple object regions, each annotated with semantic labels. Subsequently, we use GroundedSAM-v2~\cite{TODO}, a semantics-based segmentation method and bounding boxes, to further extract objects from the scene image, obtaining a series of target object masks corresponding to their labels. This segmentation approach effectively avoids the issues of unclear segmentation caused by multiple objects adhering together, thereby obtaining distinct and clear individual object parts. In addition to acquiring individual object masks, since the regions of each object and the masks of other objects have been determined, we are also able to generate occlusion masks for each different object. This plays a significant role in enabling the generative model to effectively distinguish occlusions and understand the impact of occlusions and missing data on the distribution of the generated results, thereby providing substantial auxiliary support for the subsequent object generation process.
%\subsection{Scene Contextual Reasoning}
%\paragraph{Depth Prior Integration} 
%如何获得shape的点云，深度估计？ RGBD的D针对于重建prior的重要性
% \lipsum[1-2]
%为了进一步提升场景生成的几何精度，我们引入了点云先验。点云提供了场景布局的几何线索，这对于准确定位和缩放多个物体，进而重建超越单个物体的复杂场景至关重要。为了更好地解决深度估计边缘不连续所带来的点云误差，我们采用了MoGe作为点云生成工具。除了生成像素对齐的点云外，还能获取单张图片的相机标定、深度图（depth）和法线图（normal）。此外，我们利用基于点云渲染的深度和法线变化作为阈值，从而提取每个实例干净且完整的部分点云（partial point cloud）。通过将这些点云作为场景的几何先验分析，我们的生成模型能够获得详细的空间结构参考。在本研究中，点云先验的集成显著提升了生成模型在复杂场景下的几何生成能力，为后续的纹理映射和物理仿真提供了坚实的基础。这不仅确保了生成的三维模型在视觉属性上的高度一致性，还保证了其在物理属性上的可靠性。



%Furthermore, we employ depth and normal variations based on point cloud rendering as thresholds to extract clean and complete partial point clouds for each instance.

%By using these point clouds as geometric priors for scene analysis, our generative model can obtain detailed spatial structure references. In this study, the integration of point cloud priors significantly enhances the generative model's geometric generation capabilities in complex scenes, providing a solid foundation for the final result. This not only ensures high consistency of the generated 3D models in visual attributes but also guarantees their reliability in physical properties.

%To further enhance the geometric accuracy of scene generation, we introduce point cloud priors. Point clouds provide geometrical clues of scene layout, which are essential for accurately positioning and scaling multiple objects, thereby enabling the reconstruction of complex scenes . To better address point cloud errors caused by discontinuities at the edges of depth estimation, we utilize MoGe~\cite{TODO} as the point cloud generation tool. In addition to generating pixel-aligned point clouds, MoGe also provides camera calibration, depth maps, and normal maps for single images. Furthermore, we employ depth and normal variations based on point cloud rendering as thresholds to extract clean and complete partial point clouds for each instance.

%To further enhance the geometric accuracy of scene generation, we introduce point cloud priors. Point clouds provide geometrical clues of scene layout, which are essential for accurately positioning and scaling multiple objects, thereby enabling the reconstruction of complex scenes . To better address point cloud errors caused by discontinuities at the edges of depth estimation, we utilize MoGe~\cite{TODO} as the point cloud generation tool. In addition to generating pixel-aligned point clouds, MoGe also provides camera calibration, depth maps, and normal maps for single images. Furthermore, we employ depth and normal variations based on point cloud rendering as thresholds to extract clean and complete partial point clouds for each instance.



\subsection{Scene Graph Inference}
%如何问询gpt获得场景的layout图，分析什么在上面的支撑xxxx TODO ykx
% \lipsum[1-2]
% 仅依赖单张图片进行场景还原面临诸多挑战，如物体穿模和几何不一致等问题。为了克服这些难题，我们将物体之间的关系具体定义为Support（支撑）、Hanging（悬挂）和Contact（接触）。Support表示A物体作为主体为B物体提供支撑，Hanging表示A物体作为主体为B物体提供悬挂，Contact则表示两个物体之间存在相互接触且可能移动。首先，利用Set of Mask（SOM）对输入图片中的各个物体进行编号和标记，随后通过GPT-4模型对这些标记后的物体对进行关系分类。这一过程不仅能够准确识别和分类物体之间的关系，还能有效地忽略与主要场景无关的物体，从而精确描述场景中物体的相对位置和作用方式，避免常见的几何错误。

% 此外，我们进一步分析了物体之间作用力的方向，将其分类为垂直（Vertical）、水平（Horizontal）或双向（Both）。这种力方向的信息为后续的场景优化和物理仿真提供了强大的先验支持，确保生成的场景在物理属性上具备合理性和稳定性。通过综合物体的语义标签、空间位置、关系类型以及作用力方向，我们构建了一个详尽且结构化的场景图。这个场景图不仅为仿真提供了更为精准的依据，使得生成结果在结构和物理属性上与输入图像高度一致，从而实现更真实和可信的场景还原。

Accurate object relationships are critical for preserving spatial accuracy and depth perception in single-image 3D reconstruction. These relationships help reduce ambiguity while maintaining the functional and aesthetic coherence of the scene, thereby enabling high-fidelity results. We categorize object relations into three types—support, hanging, and contact—as defined below:
\begin{itemize}
    \item Support: Object A serves as the primary, stationary structure that supports Object B.
    \item Hanging: Object A acts as the stationary anchor from which Object B is suspended.
    \item Contact: Two objects are in mutual contact and may potentially move relative to each other.
\end{itemize}
To systematically analyze these relationships, we use a Set of Mask (SOM) representation that assigns a unique index and label to each object in the image. We then utilize GPT-4 to classify the relationships between object pairs, thereby filtering out irrelevant objects and clearly distinguishing among support, hanging, and contact interactions. Additionally, GPT-4 estimates the direction of forces—categorized as Vertical, Horizontal, or Both—which serves as a robust prior for subsequent scene optimization and physical simulation. This information ensures that the generated 3D environments adhere to plausible physical constraints.

By incorporating semantic labels, spatial positions, relationship types, and force directions, we construct a comprehensive scene graph. This integrated representation preserves both the structural layout and the underlying physical attributes of the input image, ultimately producing more credible and realistic 3D reconstructions.

%Relying solely on a single image for scene reconstruction presents numerous challenges, such as object penetration and collision issues post-reconstruction. To overcome these issues, we define the relationships between objects as Support, Hanging, and Contact. 
%For recover the entire scene with high fidelity, it's crucial to xxx. object penetration and collision issues post-reconstruction. To overcome these issues, we define the relationships between objects as Support, Hanging, and Contact. 
%Understanding object relations ensures spatial accuracy and depth perception, which are crucial for accurately reconstructing a realistic 3D scene from a single image. These relationships help resolve ambiguities and maintain the functional and aesthetic coherence of the scene, resulting in high-fidelity reconstructions. We categorize the object relations in a scene as support, hanging, and contact, which are defined as:

%\begin{itemize}
%    \item Support: signifies that object A serves as the primary, stationary entity providing support to object B. 
%    \item Hanging: indicates that object A acts as the primary, stationary entity from which object B is suspended.
%    \item Contact: denotes that two objects are in mutual contact and may potentially move relative to each other.
%\end{itemize}
%
%To effectively analysze the object relations, we use a Set of Mask (SOM) representation which assigns each object in the image numbers and labels and indexes. Subsequently, we ask the GPT-4 model to classify the relationships between each object pairs to xxx, xxx and xxx. This process identifies and categorizes the relationships between each two objects and effectively disregards irrelevant objects.
%, thereby precisely describing the relative positions and interactions of objects within the scene and avoiding common geometric errors.
%
%Additionally, we ask the GPT-4 model to predict the direction of forces between objects, categorizing them as Vertical, Horizontal, or Both. This information on force direction provides robust prior support for subsequent scene optimization and physical simulation, ensuring that the generated scenes possess reasonable and stable physical properties. 
%
%By integrating the semantic labels, spatial positions, relationship types, and force directions of objects, we construct a detailed and structured scene graph. Consequently, the generated results maintain high consistency with the input image in terms of both structural and physical attributes, enabling more realistic and credible scene reconstruction.


