Scene-level reconstruction/generation is the most challenging and fundamental task, as it oppose to the object-level generation.

, as it involves recovering the geometry, appearances, and layouts of objects from partial observations.

Our method requires only a single, uncalibrated RGB image as input, which can originate from any real-world photograph or various styles of synthetic data. To achieve high-precision scene generation that closely aligns with the layout and structural composition of the input image, extraction and analysis of scene information are indispensable. To maximize the capture of the rich information embedded in the image and facilitate accurate scene-level generation, our approach not only performs meticulous object segmentation at the image level and acquires preliminary geometric information (e.g., point clouds) as priors, but also thoroughly explores the semantic and spatial relationships among objects within the scene. This comprehensive analytical process ensures that the generative model can accurately reflect the complex scene of the input image in both structural and semantic aspects, enabling the generation of high-quality scenes.