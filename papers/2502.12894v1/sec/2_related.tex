

% 从真实场景构建数字场景，再进行演绎、编辑是很重要的。为什么？
% 
Transforming real-world scenes into the digital realm enhances our ability to understand, recreate, and interact with the 3D world around us. This practice is widely embraced in industries such as animation, film, gaming, architecture, and manufacturing. It enables the creation of immersive movie experiences, the digital preservation of historical relics, and the development of interactive environments for gaming.
For example, James Cameron employed groundbreaking 3D scanning technology in \textit{Avatar} (2009) to bring the lush, realistic world of Pandora to life. Similarly, in the gaming industry, \textit{The Witcher 3: Wild Hunt} incorporated lifelike terrain and architectures inspired by real-world locations in Poland, blending authentic cultural and natural elements with imaginative, open-world exploration.


% 广泛使用的真实场景构建数字场景的方法 是多视角重建
% 但是要求很高
% A widely adopted approach to capture the physical world in high detail and translate it into digital form is photogrammetry~\cite{MVS,chen2018deep,mildenhall2020nerf,barron2021mip,barron2022mip,muller2022instant,kerbl3Dgaussians}. However, this technique typically requires tens to hundreds of images captured from multiple viewpoints for each scene, making the process time-consuming, resource-intensive, and challenging to scale.
% % 因此，单目场景重建是很有意义的
% In contrast, single-image-based approaches are far more efficient and scalable. They require only a single input image, which can be easily obtained, including from vast online repositories of visual content. This eliminates the need for data collection with expensive scanning devices or labor-intensive multi-view setups, enabling broader accessibility and practicality for large-scale applications.
Photogrammetry is a widely used method to capture the physical world in high detail and translate it into digital form~\cite{MVS,chen2018deep,mildenhall2020nerf,barron2021mip,barron2022mip,muller2022instant,kerbl3Dgaussians}, but it requires tens to hundreds of images from multiple viewpoints, making it time-consuming, resource-intensive, and hard to scale. In contrast, single-image-based approaches are more efficient and scalable, requiring only one image that can be easily obtained from online repositories, eliminating the need for expensive scanning devices or multi-view setups.


\subsection{Single Image Scene Reconstruction}
% 单视角场景重建是很难的，要处理物体多样性、遮挡、空间关系
% 最直观的方法是单目深度估计，depth foundation model，但是深度点云没法处理不可见部分
% novel view synthesis方法被设计用来弥补这个缺陷，从数据集中学习被遮挡部分的先验

% Scene-level reconstruction from a single image presents significant challenges, primarily due to the complexity of object diversity, occlusions, and the need to preserve spatial relationships between objects.
% A natural starting point is monocular depth estimation, where depth information is inferred from a single image. This approach typically generates a depth point cloud~\cite{yin2023metric3d,bhat2023zoedepth,piccinelli2024unidepth,wang2024moge,yang2024depth}. While depth estimation provides valuable information, it is limited in its ability to handle occlusions or to generate accurate representations of hidden portions of the scene.
% % 
% To address these limitations, novel view synthesis approaches have been developed. These  approaches leverage different representations, such as radiance fields~\cite{yu2021pixelnerf, tian2023mononerf, yu2022monosdf}, and 3D Gaussian~\cite{szymanowicz2024splatter,szymanowicz2024flash3d}, and learn occlusion priors from 3D datasets~\cite{geiger2013vision,dai2017scannet, chang2015shapenet,sun2018pix3d}.
% However, despite these advances, monocular reconstruction methods still often struggle to provide sufficient detail, leading to coarse and imprecise representations of the scene.

Scene-level reconstruction from a single image presents challenges due to object diversity, occlusions, and the need to preserve spatial relationships. A starting point is monocular depth estimation, where depth is inferred from a single image, typically generating a depth point cloud~\cite{yin2023metric3d,bhat2023zoedepth,piccinelli2024unidepth,wang2024moge,yang2024depth}. While it provides valuable information, it struggles with occlusions and hidden portions of the scene. 
To address this, novel view synthesis methods use representations like radiance fields~\cite{yu2021pixelnerf, tian2023mononerf, yu2022monosdf} and 3D Gaussian~\cite{szymanowicz2024splatter,szymanowicz2024flash3d}, learning occlusion priors from 3D datasets~\cite{geiger2013vision,dai2017scannet, chang2015shapenet,sun2018pix3d}. Despite these advances, monocular reconstruction methods still often struggle to provide detailed and precise scene representations.

% 重建出几何的、语义的，而非一整个场景：feedforward方法，局限于室内
Some methods, such as 3DFront~\cite{fu20213d}, focus on directly regressing geometries along with their semantic labels in the scene~\cite{dahnert2021panoptic, gkioxari2022learning, chu2023buol, chen2024single}. These approaches typically rely on scene datasets with ground truth object annotations, such as Matterport3D~\cite{fu20213d} and 3DFront~\cite{fu20213d}, which are often small in scale and limited to indoor room environments. However, the feed-forward nature of these methods leads to the generation of geometries that often lack sufficient detail and quality.


% 重建出资产的：retrieval based方法
% 受限于dataset丰富度

To better film real world to digital, other methods turn to retrieval-based approaches~\cite{langer2022sparc,gumeli2022roca,kuo2021patch2cad,gao2024diffcad,dai2024automated}, which enhance scene quality by searching for and replacing objects in a scene with similar objects from a pre-existing dataset. These methods incorporate advanced tools such as GPT-4~\cite{achiam2023gpt}, SAM~\cite{kirillov2023segment, ren2024grounded}, and depth priors to decompose scenes.
While these methods improve scene realism by integrating real-world objects, they are constrained by the richness and scope of the datasets they rely on. For scenes outside the dataset's domain, retrieval-based methods either produce erroneous results or fail to find suitable replacements, significantly degrading the quality of the reconstructed scene.

\begin{figure*} [ht]
  \centering
  \includegraphics[width=\textwidth]{figs/pipeline1.png}
  \captionof{figure}{
  Overview of the proposed pipeline. The input RGB image is processed through scene analysis to extract key information, followed by pose-aware generation to create initial 3D models. Physical constraint refinement ensures realistic interactions and spatial relationships, yielding a high-quality, mesh-based 3D scene.
  } \label{fig_overview1}
\end{figure*}



%This advancement has being reshaping the single-view reconstruction problem to 3D generation problem, i.e., leveraging 3D generation models have become essential tools for creating detailed and realistic digital assets.
%
\subsection{Reconstruction as Generation}
% 单目场景重建也可以认为是一种生成任务
% 生成单个3d资产是很重要的 对于场景生成来说是很重要的，不再局限于数据集
With the continuous advancements in the field, the ability to create high-quality 3D digital assets from various types of open-vocabulary images or text prompts has significantly improved. This advancement has precipitated a paradigm shift where the single-view reconstruction problem evolves into a generative 3D synthesis framework.
This paradigm change allows for the generation of 3D assets without being confined to a fixed dataset, enabling more flexible and scalable scene reconstruction.

%
% 物体生成方法进展 包括2d方法和 3d原生
Much of the current research in 3D asset generation focuses on distilling 3D geometry from 2D images generative models ~\cite{poole2022dreamfusion,tang2023dreamgaussian,wang2024prolificdreamer}.  More recent developments expand this approach by incorporating multi-view images for supervision~\cite{liu2023syncdreamer,liu2023zero,liu2024one,voleti2025sv3d,long2024wonder3d,wu2024unique3d}, often trained on rendered images from large-scale object datasets like Objaverse~\cite{deitke2023objaverse}, to enhance view consistency during generation.
Some approaches directly regress the shape and appearance of individual objects based on input image~\cite{hong2023lrm,tang2025lgm}. While these methods achieve satisfactory visual results, they frequently fail to reproduce fine geometric details.
%
To improve the quality of 3D geometry, a growing body of work has moved away from 2D supervision entirely, opting instead to train directly on 3D assets~\cite{deitke2023objaverse,deitke2024objaverse}. These methods produce high-quality object-level geometries with advanced processing techniques~\cite{zhang20233dshape2vecset, zhang2024clay, xiang2024structured}. However, such approaches focus on isolated objects and fail to address scene-level challenges, such as modeling spatial hierarchies, inter-object relationships, and environment lighting.
% 场景生成领域是underdeveloped
% 多视角模型 wu2024reconfusion gao2024cat3d \cite{lebegue2020co3d, yu2023mvimgnet}
% 视频、世界模型
% 
% Scene generation remains underdeveloped due to the high computational and representational complexity involved in modeling object relationships, lighting, and materials. While significant progress has been made, current approaches still face limitations in producing fully realized, editable 3D scenes.
% Current paradigms either employ video diffusion models~\cite{ho2022video, ho2022imagen, blattmann2023stable} to produce navigable 2D projections~\cite{bruce2024genie, yu2024wonderjourney}, or leverage diffusion priors to generate volumetric scene approximations through 3D Gaussian splatting~\cite{wu2024reconfusion,gao2024cat3d, luciddreamer2024}.
% While these methods are capable of producing compelling visual outputs, their generated output  are incompatible with traditional production pipelines, lacking key elements like editable meshes, UV mappings, and decomposable physically-based rendering (PBR) materials.
Scene generation remains underdeveloped due to the high computational and representational complexity of modeling object relationships, lighting, and materials. Despite progress, current approaches still struggle to produce fully realized, editable 3D scenes. Existing paradigms either use video diffusion models~\cite{ho2022video, ho2022imagen, blattmann2023stable} to generate navigable 2D projections~\cite{bruce2024genie, yu2024wonderjourney}, or rely on diffusion priors for volumetric scene approximations via 3D Gaussian splatting~\cite{wu2024reconfusion, gao2024cat3d, liang2024luciddreamer}. While these methods yield compelling visuals, they are incompatible with traditional production pipelines, lacking editable meshes, UV mappings, and decomposable PBR materials.


% 两种新方法，不同于之前的建模成一个整体场景或者分物体retrieval
% A more feasible paradigm decomposes scenes into modular components—objects, backgrounds, and environmental elements—generating each independently and reassembling them into a cohesive, editable scene graph. This modular approach allows for greater flexibility and precision in reconstructing complex scenes. For example, Gen3DSR~\cite{dogaru2024generalizable} uses DreamGaussian~\cite{tang2023dreamgaussian} to generate 3D assets at scene locations, enabling open-vocabulary scene reconstruction. However, it struggles with occlusions and pose estimation, making it hard to accurately reconstruct geometries for occluded objects. Furthermore, because objects are fixed in the world coordinate system, editing individual objects is difficult, and the reliance on 2D image-based generative models results in poor geometric details and low-fidelity representations.
A more feasible paradigm decomposes scenes into modular components—objects, backgrounds, and environmental generating and reassembling them into an editable scene graph for greater flexibility and precision. For example, Gen3DSR~\cite{dogaru2024generalizable} uses DreamGaussian~\cite{tang2023dreamgaussian} for open-vocabulary reconstruction. However, it struggles with occlusions, pose estimation, and editing individual objects, while relying on 2D models leads to poor geometric details and low-fidelity representations.
% 介绍midi
% Another recent work, Midi~\cite{huang2024midi}, attempts to learn the spatial relationships between objects in a scene. However, due to the diverse and often complex nature of object interactions, it requires training on scene datasets with ground truth 3D meshes and object annotations. This reliance on specific datasets makes the method difficult to scale and limits its ability to generalize to arbitrary scene images.
Another recent work, Midi~\cite{huang2024midi}, learns spatial relationships between objects in a scene but requires training on datasets with ground truth 3D meshes and annotations. This reliance on specific datasets limits its scalability and generalization to arbitrary scenes.
% 我们的方法
% In this paper, we introduce a novel scene reconstruction pipeline that treats scene generation as an independent process for each object, followed by their alignment to form a cohesive scene. Unlike existing methods, our approach ensures that each object maintains accurate geometric structures and textures while preserving consistent spatial relationships between objects. This allows for more realistic, reliable, and editable scene reconstructions, surpassing the quality and flexibility of prior work.
In this paper, we present a novel scene reconstruction pipeline that generates each object independently and aligns them to form a cohesive scene. Unlike existing methods, our approach preserves accurate geometry, textures, and consistent spatial relationships, resulting in more realistic, reliable, and editable reconstructions, with improved quality and flexibility.
Our approach generates each object independently and aligns them into a cohesive scene, preserving accurate geometry, textures, and consistent spatial relationships, resulting in more realistic, reliable, and editable reconstructions with improved quality and flexibility.


\subsection{Physics-Aware 3D Modeling}
Generating physically plausible 3D assets is crucial for ensuring realism and functionality in applications such as animation, gaming, and robotics.
While recent 3D generative models excel at creating visually realistic objects, they often fall short in achieving physical plausibility. To address this limitation, physics-aware 3D generative models have been developed to integrate physical principles into the generation process. Some methods use soft-body simulation to animate 3D Gaussians~\cite{xie2024physgaussian,zhong2025reconstruction}, or generate articulated objects with physics-based penalties~\cite{liu2023few}, while others ensure self-supporting structures through rigid-body simulation~\cite{mezghanni2021physically,mezghanni2022physical,chen2024atlas3d} or FEM~\cite{guo2024physically,xu2024precise}. These methods leverage offline or online physical simulations to check the physical validity of generated shapes and in turn guide generation.
However, these approaches are typically confined to individual objects, overlooking the mutual influences between multiple objects within a scene. 

%% citet -> cite
Incorporating physical constraints into scene synthesis is much more challenging due to the inclusion of more complex relationships, e.g., inter-object contact. \cite{yang2024physcene} integrates constraints like object collisions, room layout, and object reachability into their scene-level generation pipeline. However, it is limited to indoor scene synthesis and relies on a closed-vocabulary database to perform shape retrieval.
\cite{ni2024phyrecon} addresses the issue of physical implausibility in multi-view neural reconstruction. It leverages both differentiable rendering and physical simulation to learn implicit representations. However, it requires multi-view images as input, focuses on individual objects, and primarily addresses only stability (simulating the dropping of objects).
In contrast, our method operates in an open-vocabulary setting and requires only a single input image. Furthermore, it accounts for more complex inter-object relationships, particularly support and contact, making it more versatile and applicable across diverse scenarios.
%% citet -> cite

% The primary inter-object relationship is contact, which plays a crucial role in rigid-body simulations. Collision detection~\cite{baraff1994fast,bergen1997efficient,lin1998collision} is an essential step, followed by contact point generation. The appropriate physical responses to the interaction between objects are computed based on contact points, such as applying forces to separate objects or simulating realistic bounces.
% The choice of shape representation significantly affects the performance and complexity of collision detection and contact point generation. Convex polygonal meshes are popular due to the availability of highly efficient collision detection algorithms~\cite{gilbert1988fast,van2003collision}. Besides, signed distance fields (SDFs) have been popular for deformable models~\cite{marchal2004collision}, cloth~\cite{bridson2005simulation} and rigid bodies~\cite{guendelman2003nonconvex}, as well as in trajectory optimization~\cite{ratliff2009chomp,WangXF20}. SDFs provide a continuous representation of the distance between objects, introducing smoothness to the contact point generation process.
% Thus, we adopt SDFs as the underlying shape representation in our physics-aware correction module (Sec.~\ref{sec:physics-aware-correction}). 






%


