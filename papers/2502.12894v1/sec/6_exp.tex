

\begin{figure*} []
  \centering
  \includegraphics[width=\textwidth]{figs/compare.png}
  \captionof{figure}{Qualitative comparisons of CAST with state-of-the-art single-image scene reconstruction methods. From left to right: Input image, CAST, ACDC, and Gen3DSR.}
  \label{fig_comparison1}
\end{figure*}


\section{Result}
\label{sec:experiments}


Fig.~\ref{fig_gallery} showcases a range of 3D scenes generated by our method from single-view inputs across a diverse range of open-vocabulary scenarios, featuring detailed indoor environments, close-up captures of objects, and AI-generated imagery. These examples highlight the versatility and robustness of our approach, exhibiting high-fidelity geometry, realistic textures, and convincing scene compositions.

\subsection{Implementation Details}
\paragraph{ObjectGen}
The ObjectGen (Sec.~\ref{sec:objectgen}) model's pretraining follows the methodology outlined in 3DShape2VecSet~\cite{zhang20233dshape2vecset} and CLAY~\cite{zhang2024clay}, where we leverage both a Variational Autoencoder (VAE) and a Latent Diffusion Model (LDM) to generate 3D object geometries. Both the VAE and LDM modules are implemented using a 24-layer transformer, comprising a total of 1.5 billion parameters. The model is trained on the Objaverse~\cite{deitke2023objaverse} dataset, which consists of approximately 500,000 3D assets after filtering. 
% 
The partial point cloud conditioning follows a similar approach to CLAY's adaptation framework. We encode the canonical-space partial point cloud as positional embeddings with a feature dimension of 512, which are injected into the main LDM transformer using cross-attention mechanisms. For each 3D asset, we render 32 views and precompute depth maps using MoGe~\cite{wang2024moge} and Metric3D~\cite{yin2023metric3d}. These depth maps are then lifted into point clouds during training, with random masks applied to simulate occlusions. We sample 2048 points from the unprojected point cloud using Farthest Point Sampling (FPS), which serves as conditioning input for the LDM. To enhance the model's robustness, we randomly interpolate between the ground truth and predicted partial point clouds, allowing the system to handle data of varying quality. The conditioning module is trained on 200K curated data from Objaverse over 3000 epochs with 64 Nvidia A800 GPUs. The AdamW optimizer is used with a learning rate of 1e-5.

\paragraph{AlignGen}
The AlignGen (Sec.~\ref{sec:transformationgen}) module, responsible for generating pose alignment, utilizes a 24-layer transformer with a feature dimension of 512, resulting in a total of 150 million parameters. 
% During training, we randomly sample a partial point cloud from the canonical space and apply a random transformation to this point cloud. This transformed point cloud, along with the geometry latent code $\bm{z}$ from ObjectGen, is used as the conditioning input for AlignGen. 
During training, we randomly sample a partial point cloud in the canonical space from the point clouds lifted from precomputed depth maps and apply a random transformation to this point cloud. The transformed point cloud, along with the geometry latent code $\bm{z}$ from ObjectGen, is used as the conditioning input.
% FPS is utilized to sample 2048 points from the partial point cloud to ensure that the transformer receives an appropriately scaled representation of the data. 
FPS is utilized to sample 2048 points from the partial point cloud to ensure a fixed number of inputs to the transformer.
Training is conducted on the same 200K curated dataset over 1,500 epochs with 64 Nvidia A800 GPUs. The AdamW optimizer is used with a learning rate of 1e-5.


\begin{table}[]
\centering
\caption{Quantitative comparison of scene reconstruction methods across four metrics with CLIP score, GPT-4 ranking, user study of visual quality (VQ), and physical plausibility (PP).}
\label{tab:performance_comparison_visual}
% \small 
\begin{tabular}{lcccc}
\toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{Image-Image Score} & \multicolumn{2}{c}{User study} \\ \cline{2-5} 
\textbf{Method}& CLIP↑ & GPT-4↓ &  VQ↑  & PP↑   \\ \midrule
         % & CLIP↑ & GPT-4↓ &  VQ↑  & PP↑   \\ \midrule
ACDC     & 69.77  & 2.7   &  5.58\% &  22.86\%  \\
Gen3DSR  & 79.84  & 2.175   &  6.35\%  & 5.72\% \\
ours     & \textbf{85.77}  & \textbf{1.125}   &  \textbf{88.07\%} & \textbf{71.42\%} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Comparison}


\begin{table}[]
\centering
\caption{Quantitative comparison of scene reconstruction performance on the 3D-Front indoor dataset. We evaluate different methods based on Chamfer Distance (CD) for shape accuracy, F-Score (FS) for object-level reconstruction quality, and Intersection over Union (IoU) for scene-level overlap. }
\label{tab:performance_comparison_3Dfront}
% \small 
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & CD-S↓ & FS-S↑ & CD-O↓ & FS-O↑ & IoU-B↑ \\
\midrule
ACDC  & 0.104 & 39.46 & 0.072 & 41.99 & 0.541\\
InstPIFU  & 0.092 & 39.12 & 0.103 & 38.29 & 0.436\\
Gen3DSR  & 0.083 & 38.95 & 0.071 & 39.13 & 0.459 \\
ours      & \textbf{0.052} & \textbf{56.18} & \textbf{0.057} & \textbf{56.50} & \textbf{0.603} \\
\bottomrule
\end{tabular}
\end{table}



\paragraph{Qualitative Comparisons}
% We primarily evaluate our method against state-of-the-art techniques in scene reconstruction from a single image. From left to right in Fig.~\ref{fig_comparison1}, we present the results of CAST, the retrieval-based approach ACDC~\cite{dai2024automated}, and the generation-based method Gen3DSR~\cite{dogaru2024generalizable} with each method's performance displayed across both reference and novel views.  The results demonstrate the superior performance of CAST in accurately reconstructing scenes across a diverse range of scenarios, including indoor and outdoor environments, close-up views, and AI-generated imagery.
We first evaluate our method, CAST, against state-of-the-art single-image scene reconstruction techniques on open-vocabulary scenarios. Fig.~\ref{fig_comparison1} illustrates the performance of three methods—(1) the retrieval-based approach ACDC~\cite{dai2024automated}, (2) the generation-based method Gen3DSR~\cite{dogaru2024generalizable}, and (3) our proposed CAST—across both reference and novel views. Our results highlight CAST’s superior ability to accurately reconstruct scenes in diverse settings, including indoor and outdoor environments, close-up perspectives, and AI-generated imagery.
% 
% Existing retrieval-based methods are often constrained to fixed object categories (e.g., furniture), restricting their flexibility and applicability in broader contexts. Meanwhile, generation-based methods, such as Gen3DSR, exhibit a noticeable degradation in performance when dealing with challenging geometries and occlusions, also the performance in novel view decrease rapidly. CAST, on the other hand, excels in handling diverse and unconstrained scenes, producing high-quality reconstructions, highlights the robustness and versatility of our approach in a wide range of real-world and synthetic scenarios.
Existing retrieval-based methods (e.g., ACDC) often rely on fixed object categories (such as furniture) and thus offer limited flexibility. Meanwhile, generation-based approaches like Gen3DSR struggle with challenging geometries and partial occlusions, showing a rapid decrease in performance when rendering novel views. By contrast, CAST demonstrates robust scene reconstructions under varying conditions, underscoring its versatility for a broad range of real-world and generated scenarios.

% To quantify the quality of the generated scene and its similarity to the input image, we employed two visual evaluation methods. First, we use the CLIP score~\cite{taited2023CLIPScore} to measure both the reconstruction quality and the visual similarity between the rendered scene and the input image. To focus on the foreground and minimize the influence of the environment, we remove the background from both the rendered and reference images using a background removal technique. Second, we leverage GPT-4 to provide an additional layer of semantic evaluation by ranking the generated scenes based on reasoning. The model is asked to consider various aspects such as object arrangement, physical relationships, and overall scene realism.

To assess both the visual fidelity and semantic accuracy of the generated scenes, we employ two complementary evaluation methods including CLIP Score~\cite{taited2023CLIPScore} and GPT-4 Reasoning.
% 
We compute the CLIP score between the rendered scene and the input image to measure overall reconstruction quality and visual similarity. To minimize environmental distractions, we remove backgrounds from both the rendered and reference images before computing the score.
% 
We additionally leverage GPT-4 to rank the generated scenes based on various semantic aspects, including object arrangement, physical relationships, and scene realism. This semantic feedback helps identify alignment or contextual errors that might not be apparent through pixel-based scores alone.

% To further evaluate the scene quality and overall performance, we also conducted a comprehensive user study. Our user study primarily compares the performance of ACDC, Gen3DSR, and CAST across two key evaluation aspects: visual quality (VQ) and physical plausibility (PP). For the visual quality experiment, we randomly selected pair-wised reference views, novel views and target views, and asked participants to choose which method's output better matched the input image, both in terms of similarity and overall visual quality. This allowed us to gauge how well each method reconstructed scenes and how accurately it represented the input.
% To mitigate the risk that image similarity might bias users toward selecting the most visually similar scene, we conducted a separate physical plausibility ablation study. In this phase, we did not provide participants with the original input images, thus eliminating any potential bias related to visual resemblance. Instead, we only presented the rendered results and asked users to judge which scene appeared more realistic based on physical reasoning and common sense. The goal of this study was to evaluate how well the generated scenes adhered to physical laws and common sense.
Beyond automated metrics, we also conducted a user study focusing on two key aspects including Visual Quality (VQ) and Physical Plausibility (PP).
We randomly selected paired reference, novel, and target views, asking participants to choose which method’s output best matched the input image in terms of both similarity and overall aesthetics.
To reduce potential biases introduced by visual resemblance, participants in a separate session only viewed rendered results—without the original input images—and judged which scene appeared more realistic based on physical constraints and common sense (e.g., preventing floating objects or improbable contacts).


As shown in Tab.~\ref{tab:performance_comparison_visual}, CAST outperforms both ACDC and Gen3DSR in all four evaluated metrics, confirming its effectiveness in producing scenes that are visually coherent and physically plausible.

\begin{figure} []
  \centering
  \includegraphics[width=\columnwidth]{figs/Ablation_occlusion.png}
  \captionof{figure}{We evaluate the generation performance with and without the occlusion-aware generation module. The RGB and normal renderings of the object highlight the significance of this module in ensuring the completeness and high quality of the generated object.} 
  \label{Ablation_occlusion}
\end{figure}

\paragraph{Quantitative Comparisons}

% Having evaluated our approach qualitatively, we now provide quantitative metrics on a dataset with available ground truth.
% 
% Although CAST has the ability to handle open-vocabulary scenes, such scenes often lack mesh ground truth, making quantitative comparisons challenging. To further evaluate the effectiveness of our method in scene reconstruction and the accuracy of object-level reconstruction on a relatively large dataset, we conduct quantitative experiments using the indoor scene dataset 3DFront~\cite{fu20213d}, which includes ground truth meshes and their corresponding rendered images. 
Although CAST is designed to handle open-vocabulary scenes, many such scenes lack mesh ground truth, which complicates direct quantitative comparisons. To address this, we perform additional evaluations on the 3DFront dataset~\cite{fu20213d}. This dataset offers ground-truth meshes alongside corresponding rendered images, enabling a more precise assessment of both object-level and scene-level reconstructions.
% 
We compare our method with InstPIFu~\cite{liu2022towards}, ACDC~\cite{dai2024automated}, and Gen3DSR~\cite{dogaru2024generalizable}.
% 
We compute Chamfer Distance and F-Score at the object level, as well as IoU, Chamfer Distance, and F-Score at the scene level, to assess both the fidelity of individual object geometries and the accuracy of their spatial layout.
% 
% For a fair comparison, we replace the individual segmentation systems of the other methods with ground truth (GT) masks to eliminate discrepancies in partitioning, ensuring a pairwise comparison against GT meshes. 
To ensure fairness, we replace the segmentation modules in other methods with ground-truth (GT) masks so that any differences stem purely from reconstruction ability rather than object partitioning.


As summarized in Tab.~\ref{tab:performance_comparison_3Dfront}, CAST not only achieves higher object-level generation quality but also surpasses existing approaches in scene layout accuracy. Even within the constraints of an indoor dataset, our method demonstrates robust performance and consistent improvements over competing baselines.



\subsection{Evaluations}

% \begin{figure} []
%   \centering
%   \includegraphics[width=\columnwidth]{figs/Ablation_occlusion.png}
%   \captionof{figure}{We evaluate the generation performance both with and without the occlusion-aware generation module. The RGB and normal renderings of the object highlight the significance of this module in ensuring the completeness and high quality of the generated object.} 
%   \label{Ablation_occlusion}
% \end{figure}

\begin{figure} []
  \centering
  \includegraphics[width=\columnwidth]{figs/Ablation_pcd.png}
  \captionof{figure}{A stack of books with varying lengths and widths directly generated as a single complex object, demonstrating how point cloud conditioning enhances the preservation of scale, dimensions, and local details compared to traditional methods.} 
  \label{Ablation_pointscondition}
\end{figure}

\begin{figure} []
  \centering
  \includegraphics[width=\columnwidth]{figs/Ablation_pose.png}
  \captionof{figure}{Comparative evaluation of pose estimation methods. Our pose alignment module demonstrates superior alignment accuracy compared to Iterative Closest Point (ICP) and differentiable rendering (DR).} 
  \label{Ablation_icp_dr}
\end{figure}

To elucidate the individual contributions of key components in \methodname, we conducted a series of ablation studies. These experiments systematically removed or altered specific components to assess their impact on overall performance. The ablation studies focus on several key design choices: occlusion-aware object generation, point-cloud conditioning, generative alignment, and the physics-aware correction process.
% the occlusion-aware module, the point cloud condition module, the generative alignment module, and the physics-aware correction module.


\paragraph{Ablation on Occlusion-Aware Generation}
Occlusions are a significant challenge in scenes with complex objects. To assess the effectiveness of the Masked Autoencoder (MAE) in handling occlusions, we performed an ablation study comparing generation results with and without MAE components. As shown in Fig.~\ref{Ablation_occlusion}, the results highlight the importance of the occlusion-aware module. Without MAE, the generated objects for partially occluded regions exhibit significant degradation. For example, the spaceship appears fragmented and incomplete, while the cup is depicted as broken with missing parts. In contrast, when MAE conditioning is applied, the model successfully infers and fills the occluded regions, resulting in more accurate and visually coherent generations that align better with the input image. This demonstrates the critical role of the occlusion-aware module in ensuring that occluded objects are reconstructed accurately, improving both the completeness and realism of the final 3D scene.





\paragraph{Ablation on Partial Point Cloud Conditioning}
We conducted an ablation study to investigate the role of canonical space partial point cloud conditioning in our generation process. Although directly generating from the input image can produce visually plausible results. In the absence of pixel-level alignment, the model struggles to maintain correct object quantity and scale, resulting in unsatisfactory generation. To more effectively showcase the importance of point cloud conditioning in generating a single instance, we opted to directly generate a more complex instance structure: a stack of six books with varying lengths and widths. As depicted in Fig.~\ref{Ablation_pointscondition}. When the generation process relies solely on the input image, without the benefit of point cloud conditioning, the results frequently exhibit inaccuracies in both the number and dimensions of the generated objects. By contrast, integrating point cloud conditioning introduces a robust geometric prior that significantly improves the precision of the generated scene. This enhancement ensures that objects with intricate shapes and varying dimensions are reconstructed more accurately, closely resembling their real-world counterparts depicted in the input image. This demonstrates the critical role of geometric priors in enhancing the fidelity of 3D scene generation by preserving the true dimensions and shapes.

 
% \begin{figure} []
%   \centering
%   \includegraphics[width=\columnwidth]{figs/Ablation_pcd.png}
%   \captionof{figure}{The 3D instances generated using the point cloud geometric prior more accurately preserve the dimensions and local details of the six stacked books with different lengths and widths.} 
%   \label{Ablation_pointscondition}
% \end{figure}

\begin{figure} []
  \centering
  \includegraphics[width=\columnwidth]{figs/Ablation_simulator.png}
  \captionof{figure}{Comparison of scene reconstruction with and without relational graph constraints. By integrating relational graph constraints, our method ensures both physical plausibility and accurate alignment with the intended scene, maintaining correct spatial relationships.} 
  \label{Ablation_simu}
\end{figure}


\paragraph{Effectiveness of Alignment Generation}

% To validate the effectiveness of our pose alignment module, we compared it to common pose estimation methods like Iterative Closest Point (ICP)~\cite{arun1987least,best1992method} and differentiable rendering.
% For the ICP method, we initially sampled a point cloud uniformly from the generated mesh, using twice the number of points as in the estimated point cloud to mitigate ambiguity. Both point clouds were normalized by their bounding box sizes to address scale differences. We then employed Open3D~\cite{Zhou2018Open3DAM} to align the sampled point cloud with the estimated one in the normalized space.
% For the differentiable rendering method~\cite{nvdiffrast}, we used the generated mesh and the estimated camera parameters, This approach optimized the rotation, translation, and scaling of each object individually through differentiable rendering, aiming to align the rendered result of each object with its corresponding portion in the input image from the estimated camera viewpoint.
To assess the effectiveness of our pose alignment module, we compared it with common pose estimation methods such as Iterative Closest Point (ICP)~\cite{arun1987least,best1992method} and differentiable rendering~\cite{nvdiffrast}. The generated mesh was provided to different pose estimation methods to align it with the reference RGB image and its corresponding depth prediction.
For the ICP method, we uniformly sampled a point cloud from the generated mesh and normalized both the sampled and estimated point clouds by their bounding boxes to address scale differences. We used the ICP implementation in Open3D~\cite{Zhou2018Open3DAM} to register these two normalized point clouds.
For differentiable rendering, we optimized the rotation and translation parameters to transform the generated mesh so that the rendered image aligned with the reference RGB image.
% Fig.~\ref{Ablation_icp_dr}, our method surpasses both ICP and differentiable rendering~\cite{nvdiffrast} in terms of alignment accuracy. ICP often struggles with accurate pose estimation due to outliers in point clouds, unknown object scales, and symmetrical or repetitive geometries, which can lead to local minima. On the other hand, differentiable rendering's accuracy is significantly affected by occlusions in the RGB input, which disrupts the optimization of object poses and prevents achieving accurate alignment with the input image.
% Thus, our results demonstrate that our transformation estimation module offers superior performance compared to traditional ICP and differentiable rendering methods, highlighting its robustness in accurately estimating object poses from generated meshes and enhancing alignment with input images.
As shown in Fig.~\ref{Ablation_icp_dr}, our method surpasses both ICP and differentiable rendering in alignment accuracy. ICP often struggles with accurate pose estimation due to outliers in point clouds, unknown object scales, and symmetrical or repetitive geometries, which can lead to local minima. Differentiable rendering, on the other hand, is significantly impacted by occlusions in the RGB input, disrupting the optimization of object poses and preventing precise alignment with the input image. Our results show that our pose alignment module outperforms traditional ICP and differentiable rendering methods, demonstrating its robustness in accurately estimating object poses from generated meshes and improving alignment with input images.


% \begin{figure} []
%   \centering
%   \includegraphics[width=\columnwidth]{figs/Ablation_pose.png}
%   \captionof{figure}{Comparative evaluation of pose estimation methods. Our transformation estimation module demonstrates superior alignment accuracy compared to Iterative Closest Point (ICP) and differentiable rendering (DR).} 
%   \label{Ablation_icp_dr}
% \end{figure}



\paragraph{Effect of Physical Consistency Enforcement}
In CAST, physical constraints are essential for achieving realistic object interactions and maintaining spatial coherence within a scene. While we address common challenges such as occlusions and incomplete views, issues like floating objects, penetration, and misaligned spatial relationships still occur. As shown in Fig.~\ref{Ablation_simu}, scenes generated without relational constraints may appear physically inconsistent, when only physical simulation is applied, objects adhere to physical laws, but their relative positioning and overall arrangement can differ significantly from the intended scene (e.g., an onion might fall off a surface, disrupting the original composition).  By incorporating relational graph constraints, our method ensures that the objects not only comply with physical feasibility but also align with the intended scene layout, preserving both the physical plausibility and the desired spatial relationships.


% \begin{figure} []
%   \centering
%   \includegraphics[width=\columnwidth]{figs/Ablation_simulator.png}
%   \captionof{figure}{Comparison of scene reconstruction with and without relational graph constraints. By integrating relational graph constraints, our method ensures both physical plausibility and accurate alignment with the intended scene, maintaining correct spatial relationships.} 
%   \label{Ablation_simu}
% \end{figure}

\begin{table}[]
\centering
\caption{Quantitative ablation study of the MAE module, point cloud conditioning (PCD), and the iterative refinement strategy (iter.). For simplicity, we only display the added key component in each row.}
\label{tab:ablation_occ_iter}
% \small % 调整表格字体大小（可选）
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & CD-S↓ & FS-S↑ & CD-O↓ & FS-O↑ & IoU-B↑ \\
\midrule
Vanilla & 0.079 & 53.38 & 0.069 & 52.83 & 0.515\\
+ MAE  & 0.064 & 53.79& 0.066 &  54.32& 0.548\\
+ PCD   & 0.056 & 53.91 & 0.060 & 54.60  & 0.582 \\
+ iter.     & \textbf{0.052} &  \textbf{56.18}  & \textbf{0.057} & \textbf{56.50} & \textbf{0.603} \\

\bottomrule
\end{tabular}
\end{table}



\paragraph{Quantitative Ablation Study of Different Modules}
% We conducted a quantitative ablation study to evaluate the impact of all module on the overall performance of our method. As shown in Tab.~\ref{tab:ablation_occ_iter}, 
To quantitatively evaluate the contribution of each module to overall performance, we conducted a comprehensive ablation study. As shown in Tab.~\ref{tab:ablation_occ_iter}, we assessed the impact of removing or altering key components on the final scene quality. The results indicate that each component contributes significantly to the overall performance of our method. The quantitative analysis further highlights the importance of each module in achieving high-quality, physically consistent, and realistic scene reconstructions.

\paragraph{Applications}
As shown in Fig.~\ref{fig_application}, CAST transforms a single image into a fully realized 3D scene, enabling a wide range of applications. This ability to reconstruct detailed environments powers physics-based animation by ensuring realistic object interactions. It also supports real-to-simulation workflows in robotics, allowing for accurate scene replication from real-world datasets. In game development, CAST facilitates the creation of immersive environments, where faithfully reconstructed scenes are seamlessly integrated into interactive worlds using Unreal Engine.

\begin{figure} []
  \centering
  \includegraphics[width=\columnwidth]{figs/application.png}
  \captionof{figure}{CAST enables realistic physics-based animations, immersive game environments, and efficient real-to-simulation transitions, driving innovation across various fields.} 
  \label{fig_application}
\end{figure}
% CAST enables realistic physics-based animations, immersive game environments, and efficient  real-to-simulation transitions, driving innovation across various fields.