% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% \usepackage{lipsum}
% \usepackage{xcolor}
% \usepackage{bbding}
\usepackage{bm}
% \usepackage{enumitem}
% \usepackage{multirow}
% \usepackage{xspace}
% \usepackage{natbib}
% \usepackage{authblk}
\newcommand{\methodname}{CAST\xspace}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image}


\author{
% Kaixin Yao\\
% ShanghaiTech University\\
% Kaixin Yao$^{1,2\star}$ \quad Longwen Zhang$^{1,2\star}$ \quad Xinhao Yan$^{1,2}$ \quad Yan Zeng$^{1,2}$ \quad Qixuan Zhang$^{1,2\dagger}$ \\ Lan Xu$^{1\ddag}$ \quad Wei Yang$^{3\ddag}$ \quad Jiayuan Gu$^{1\ddag}$ \quad Jingyi Yu$^{1\ddag}$ \\
Kaixin Yao$^{1,2\star}$ \quad Longwen Zhang$^{1,2\star}$ \quad Xinhao Yan$^{1,2}$ \quad Yan Zeng$^{1,2}$ \quad Qixuan Zhang$^{1,2\dagger}$ \\ Lan Xu$^{1}$ \quad Wei Yang$^{3}$ \quad Jiayuan Gu$^{1}$ \quad Jingyi Yu$^{1}$ \\
  $^1$ShanghaiTech University\quad 
  $^2$Deemos Technology\quad \\
  $^3$Huazhong University of Science and Technology \\
{\tt\small \{yaokx2023,zhanglw2,yanxh,zengyan2024,zhangqx1,xulan1,gujy1,yujingyi\}@shanghaitech.edu.cn} \\
{\tt\small weiyangcs@hust.edu.cn} \\
{\tt \href{https://sites.google.com/view/cast4}{https://sites.google.com/view/cast4}}
% For a paper whose authors are  all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Longwen Zhang\\
% ShanghaiTech University\\
% {\tt\small zhanglw2@shanghaitech.edu.cn}
% \and
% Longwen Zhang\\
% ShanghaiTech University\\
% {\tt\small zhanglw2@shanghaitech.edu.cn}
}
% \maketitle

% \begin{figure*}
% \centering
%   \includegraphics[width=\textwidth]{figs/teaser.png}
%   \caption{CAST brings diverse 3D scenes to life from a single image, where the relationships between objects shaped by their physical roles and interactions come together to form a cohesive and immersive virtual environment.}
%   \label{fig:teaser}
% \end{figure*}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
% \begin{center}
%     {\Large \href{https://sites.google.com/view/cast4}{https://sites.google.com/view/cast4}}
% \end{center}
\includegraphics[width=\linewidth]{figs/teaser.png}
% \vspace{-2em}
\captionof{figure}{CAST brings diverse 3D scenes to life from a single image, where the relationships between objects shaped by their physical roles and interactions come together to form a cohesive and immersive virtual environment.\vspace{1em}}
\label{fig:teaser}
}]

% \def\thefootnote{}\footnotetext{$\star$ Equal contribution. $\dagger$ Project Leader. $\ddag$ Corresponding authors.}
\def\thefootnote{}\footnotetext{$\star$ Equal contribution. $\dagger$ Project Leader.} 
% \thefootnote{$\dagger$}\footnotetext{Project Leader.}

\begin{abstract}
\label{sec:abstract}
    Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. Experimental results demonstrate that CAST significantly improves the quality of single-image 3D scene reconstruction, offering enhanced realism and accuracy in scene recovery tasks. CAST has practical applications in virtual content creation, such as immersive game environments and film production, where real-world setups can be seamlessly integrated into virtual landscapes. Additionally, CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.
\end{abstract}


\section{Introduction}
\label{sec:intro}
    \input{sec/1_intro.tex}

\section{Related Work}
\label{sec:related}
    \input{sec/2_related.tex}
    
\section{Overview}
\input{sec/3_overview_part1.tex}
    \label{sec:method3}

\input{sec/4_method_part2.tex}
\label{sec:method4}

% \section{Physical Simulation and Consistency Enforcement}
% \section{Physical Consistency and Relationship Enforcement}


\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/sec5.png}
  \caption{Physics-aware correction via constraint graph mapped from fine-grained relation graph. Top: Floating surfboard grounded on the van. Bottom: Penetrating guitar and cooler separated.} 
  \label{fig_simu_demonstrate}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{figs/gallery.png}
  \caption{Bringing the vibrant diversity of the real world into the virtual realm, this collection reimagines open-vocabulary scenes as immersive digital environments, capturing the richness and depth of each unique setting. For each scene, the images display as follows: the top-left shows the input image, the top-center displays the rendered geometry, and the right presents the rendered image with realistic textures.}
  \label{fig_gallery}
\end{figure*}

% \input{sec/5_method_part3_jigu.tex}
%-------------------------------
\section{Physics-Aware Correction}
\label{sec:physics-aware-correction}

The pipeline detailed in Sec.~\ref{sec:method2} individually generates each 3D object instance and estimates its similarity transformation (scaling, rotation, and translation) based on a single input image. While our proposed modules achieve high accuracy, the resulting scenes are sometimes not physically plausible. For instance, as illustrated in Fig.~\ref{fig_simu_demonstrate}, one object (e.g., a guitar) may intersect with another (e.g., a cooler), or an object (e.g., a surfboard) may appear to float unnaturally without any support (e.g., from a van). 

To address these issues, we introduce a physics-aware correction process that optimizes the rotation and translation of objects, ensuring the scene adheres to physical constraints consistent with common sense.
The correction process is motivated by physical simulation (Sec.~\ref{sec:rigid-body-sim}) and formulated as an optimization problem (Sec.~\ref{sec:physical-constraints}) based on inter-object relationships represented by a scene graph (Sec.~\ref{sec:scene-relation-graph}) extracted from the image.

\subsection{A Quick Primer to Rigid-Body Simulation}
\label{sec:rigid-body-sim}

We introduce the fundamental principles of physical (rigid-body) simulation, which inspire our problem formulation and make our framework more accessible to downstream applications such as gaming and robotics. For a thorough survey, we refer the readers to \cite{BenderETC12}. % citet

In rigid-body simulations, the world is modeled as an ordinary differential equation (ODE) process. In each simulation step, it begins with the Newton-Euler (differential) equations, which describe the dynamic motion of rigid bodies in the absence of contact. \emph{Collision detection} is conducted to find the contact points between rigid bodies, which are needed to determine contact forces. For contact handling and collision resolution, there are usually several conditions: non-penetration constraints to prevent bodies from overlapping, a friction model ensuring contact forces remain within their friction cones, and complementarity constraints that enforce specific disjunctive relationships among variables. Solvers are used to resolve the system comprising equations and inequalities, subsequently updating the velocity and position of each rigid body.

A straightforward approach to enhance physical plausibility is to utilize an off-the-shelf rigid-body simulator to process the scene, starting from the initial state estimated by the pipeline previously described and obtaining the rest state after simulation. However, this method presents several challenges.
% \\1) \emph{Partial Scene}: Since our primary focus is scene generation from a single image rather than reconstruction, some objects may be missing. Simulating a partial scene under full physical rules can lead to suboptimal results (see Fig.~\ref{fig_simu_demonstrate}).
\\ 1) \emph{Partial Scene}: Some objects may be missing due to the limitations of 2D foundation models, and thus not reconstructed. Simulating a partial scene under full physical rules can lead to suboptimal results (see Fig.~\ref{Ablation_simu}). 
\\2) \emph{Imperfect Geometries}: While our 3D generative model produces high-quality geometries, minor imperfections may still occur. Rigid-body simulators typically require convex decomposition~\cite{mamou2009simple,mamou2016volumetric,wei2022approximate} of objects, which introduces additional complexity and hyperparameters. Overly fine-grained decomposition can result in non-flat, complex surfaces, causing objects to fall or move unexpectedly during simulation. Conversely, coarse decomposition may lead to visually floating objects due to discrepancies between the visual and collision geometries.
\\3) \emph{Initial Penetrations}: Despite the high accuracy of pose estimation, significant inter-object penetrations may exist in the initial state. These penetrations create instability for standard rigid-body solvers and, in some cases, lead to unsolvable scenarios if the solver is not customized for those cases.

Thus, we propose a customized and simplified ``physical simulation'' to optimize the object poses, ensuring that the scene adheres to common-sense physical principles derived from the single image.
\emph{Note that our approach does not model full dynamics. For example, an object may not remain stable in its current pose over time. However, it should be physically plausible at the current time step. We argue that our optimized results can serve as a reliable initialization for subsequent physical simulations.}




\subsection{Problem Formulation and Physical Constraints}
\label{sec:physical-constraints}

We formulate the physics-aware correction process as an optimization problem, aiming to minimize the total cost that represents pairwise constraints on objects.

\begin{equation}
\label{eq:physics-aware-objective}
    \min_{\mathcal{T}=\{T_1,T_2,\dots,T_N\}} \sum_{i,j}C(T_i,T_j;\bm{o}_i,\bm{o}_j)
\end{equation}
where $N$ is the number of objects, $T_i$ is the rigid transformation (rotation and translation) of the i-th object $\bm{o}_i$. $C$ is the cost function representing the  relationship between $\bm{o}_i$ and $\bm{o}_j$. Note that the cost function varies depending on the type of relationship.

Motivated by physical simulation, we categorize the relationships into two types: \emph{contact} and \emph{support}. The relationships are identified with the assistance of a VLM, as detailed in Sec.~\ref{sec:scene-relation-graph}.

1) \emph{Contact} describes whether two objects $\bm{o}_i$ and $\bm{o}_j$ are in contact. Let $D_i(p)$ denote the signed distance function induced by $\bm{o}_i$ at the point $p$, which is used to define the constraint. $D_i(p)=D_j(p)=0$ indicates that $p$ is a contact point of $\bm{o}_i$ and $\bm{o}_j$. When $D_i(p)=0$ ($p$ is a surface point of $\bm{o}_i$), $D_j(p)<0$ indicates inter-object penetration, while $D_j(p)>0$ means the objects are separated. Thus, the cost function can be defined as:

% \begin{equation}
% \label{eq:contact-constraint}
% \begin{split}
%     C(T_i,T_j) = \max(|\min_{p \in \partial \bm{o}_i} D_j(p(T_i))|, \sigma) + \max(|\min_{p \in \partial \bm{o}_j} D_i(p(T_j)|, \sigma)\\
%     \text{if } \bm{o}_i \text{ and } \bm{o}_j \text{ are in contact}
% \end{split}
% \end{equation}
\begin{equation}
\label{eq:contact-constraint}
\begin{split}
    C(T_i, T_j; \bm{o}_i \to \bm{o}_j) = 
    \frac{\sum_{p \in \partial \bm{o}_j} D_i(p(T_j)) \mathbb{I}(D_i(p(T_j))<0)}{\sum_{p \in \partial \bm{o}_j} \mathbb{I}(D_i(p(T_j))<0)} 
    \\
    + \max(\min_{p \in \partial \bm{o}_j}D_i(p(T_j)),0)
    \\
    C(T_i, T_j; \bm{o}_j \to \bm{o}_i) =\frac{\sum_{p \in \partial \bm{o}_i} D_j(p(T_i)) \mathbb{I}(D_j(p(T_i))<0)}{\sum_{p \in \partial \bm{o}_i} \mathbb{I}(D_j(p(T_i))<0)}
    \\
    + \max(\min_{p \in \partial \bm{o}_i}D_j(p(T_i)),0)
    \\
    C(T_i,T_j) = C(T_i, T_j; \bm{o}_i \to \bm{o}_j) + C(T_i, T_j; \bm{o}_j \to \bm{o}_i)
    \\
    \text{if } \bm{o}_i \text{ and } \bm{o}_j \text{ are in contact}
\end{split}
\end{equation}
where $\partial \bm{o}_i$ denotes the surface of $\bm{o}_i$, and $\mathbb{I}$ is the indicator function. The constraint ensures that there is no penetration and at least one contact point between the objects. Note that $p \in \partial \bm{o}_i$ is a function of $T_i$. The contact constraint defined here is bilateral, meaning it applies to both objects.

2) \emph{Support} is a unilateral constraint, which is a special case of \emph{Contact}. If $\bm{o}_i$ supports $\bm{o}_j$, it implies that the pose $T_j$ of $\bm{o}_j$ should be optimized while $\bm{o}_i$ is assumed to be static. This scenario typically occurs when multiple objects are stacked vertically. The cost function for this case is similar to the one in \emph{Contact}, but it only involves one direction:

\begin{equation}
\label{eq:support-constraint}
    C(T_i,T_j) = |\min_{p \in \partial \bm{o}_j} D_i(p(T_j)|,\ \text{if } \bm{o}_i \text{ supports } \bm{o}_j
\end{equation}

% In addition, we simulate the gravitational force acting on the supported object $\bm{o}_j$ to ensure it makes contact with the supporting object $\bm{o}_i$ in a physically realistic manner, minimizing the likelihood of it toppling over.
% \begin{equation}
% \label{eq:support-constraint2}
%     C(:, T_j) = \frac{\sum_{p \in \partial \bm{o}_j} p(T_j) \cdot \vec{g}}{|\partial \bm{o}_j|}
% \end{equation}
% where $g$ is the direction of gravity.

Furthermore, for flat supporting surfaces like the ground or walls, we regularize the SDF values near the contact region, to ensure that objects make close contact with these surfaces. This regularization handles scenarios where objects are partially reconstructed, such as a van with only two wheels, as illustrated in Fig.~\ref{fig_simu_demonstrate}.
\begin{equation}
\label{eq:support-constraint3}
    C(T_i,T_j) = \frac{\sum_{p \in \partial \bm{o}_j} D_i(p(T_j) \mathbb{I}(0<D_i(p)<\sigma)}{\sum_{p \in \partial \bm{o}_j} \mathbb{I}(0<D_i(p)<\sigma)}
\end{equation}
where $\mathbb{I}$ is the indicator function, and $\sigma$ is a threshold to decide whether a point is sufficiently close to the surface.

% For special supporting objects like the ground or a wall as $\bm{o}_i$, we additionally regularize the pose of $\bm{o}_j$, to ensure that the normals of $\bm{o}_j$ near the contact point align closely with the normal of $\bm{o}_j$ -- typically vertical for the ground and horizontal for walls.

% \begin{equation}
% \label{eq:support-constraint2}
%     C_{support}(T_i,T_j) = \frac{1}{|\mathcal{N}(p)|}\sum_{p \in \mathcal{N}(P_{c})} -\vec{N_i}(T_i) \cdot \vec{N_j}(p(T_j))
% \end{equation}
% where $P_{c}$ represents the contact point found in Eq.~\ref{eq:support-constraint}, which achieves the minimum, and $\mathcal{N}(P_{c})$ denotes the neighborhood of $P_{c}$.

\subsection{Scene Relation Graph}
\label{sec:scene-relation-graph}

Physical cues, particularly inter-object relationships, are visually present in the image.  We leverage the strong common-sense reasoning capabilities~\cite{rana2023sayplan,li2024llm,cheng2024navila} of visual-language models, specifically GPT-4v~\cite{achiam2023gpt}, to identify pairwise physical constraints as defined in Sec.~\ref{sec:physical-constraints}. Given an image, we employ the Set of Mark~\cite{yang2023setofmark} (SoM) technique to visually prompt GPT-4v to describe the inter-object relationships, and subsequently extract a \emph{scene relation graph} from the answers.
To address the sampling uncertainty inherent in VLMs, we adopt an ensemble strategy, combining results from multiple trials to produce a robust inferred graph. 
% \red{The detailed prompt design is provided in the Appendix}. 

Instead of directly asking GPT-4v to identify \emph{Support} and \emph{Contact} relationships, we first provide it with more fine-grained physical relationships, such as \emph{Stack} (Object 2 supports Object 1), \emph{Lean} (Object 1 leans against Object 2), and \emph{Hang} (Object 2 supports Object 1 from above). We then map these detailed relationships to the predefined categories of \emph{Support} and \emph{Contact} for further optimization. Specifically, if there are edges pointing toward each other between two nodes, the edge is categorized as \emph{Contact}; otherwise, it is categorized as \emph{Support}. Prompting GPT-4v with these nuanced relationships helps eliminate potential ambiguity in binary relationship classification and facilitates more accurate reasoning by GPT-4v.
An example of the resulting graph is illustrated in Fig.~\ref{fig_simu_demonstrate}.

% \red{Mention the mapping!}
The mapped scene constraint graph is a directed graph where nodes represent object instances and edges denote physical relationships between objects. A \emph{Contact} relationship is represented by a bidirectional edge, while a \emph{Support} relationship is depicted as a directed edge. This graph serves as the foundation for defining the cost functions used in Eq.~\ref{eq:physics-aware-objective}.




\subsection{Optimization with Physics-Aware Relation Graph}

Given the physical constraints defined by the inferred relation graph, we can instantiate our cost functions as described in Eq.~\ref{eq:physics-aware-objective}. The graph allows us to reduce the number of pairwise constraints that need to be optimized, in contrast to a full physical simulation.

For the implementation, we uniformly sample a fixed number of points from the surface of each object at its rest pose. These points are then transformed according to the current object’s pose parameters and used to query the SDF values with respect to another object (and its pose). SDF computation is handled by Open3D, and Pytorch is used to auto-differentiate the loss function.
%-------------------------------

\input{sec/6_exp.tex}

\section{Conclusions}
\label{sec:conclusions}
    \input{sec/7_conclusion.tex}
%-------------------------------------------------------------------------




%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}
