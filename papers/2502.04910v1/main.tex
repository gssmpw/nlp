\documentclass{article}
%\usepackage{iclr2025}
\input{math_commands.tex}

%\iclrfinalcopy
\usepackage{fancyhdr}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{arxiv}

\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{makecell}

\begin{document}

\title{On the Power of Heuristics in Temporal Graphs}

\author{Filip Cornell\thanks{Equal contribution.} \\
Microsoft Gaming \\
\texttt{t-fcornell@microsoft.com} \\
\And
Oleg Smirnov\footnotemark[1] \\
Microsoft Gaming\\
\texttt{oleg.smirnov@microsoft.com} \\
\AND
Gabriela Zarzar Gandler \\
Microsoft Gaming\\
\texttt{gabrielaz@microsoft.com}
\And
Lele Cao\\
Microsoft Gaming\\
\texttt{lelecao@microsoft.com}
}

\maketitle

\begin{abstract}
Dynamic graph datasets often exhibit strong temporal patterns, such as recency, which prioritizes recent interactions, and popularity, which favors frequently occurring nodes. We demonstrate that simple heuristics leveraging only these patterns can perform on par or outperform state-of-the-art neural network models under standard evaluation protocols. To further explore these dynamics, we introduce metrics that quantify the impact of recency and popularity across datasets. Our experiments on BenchTemp~\citep{huang2024benchtemp} and the Temporal Graph Benchmark~\citep{huang2024temporal} show that our approaches achieve state-of-the-art performance across all datasets in the latter and secure top ranks on multiple datasets in the former. These results emphasize the importance of refined evaluation schemes to enable fair comparisons and promote the development of more robust temporal graph models. Additionally, they reveal that current deep learning methods often struggle to capture the key patterns underlying predictions in real-world temporal graphs. For reproducibility, we have made our code publicly available. %\footnote{Redacted for anonymity during review process.}.
\end{abstract}

\section{Introduction}
Dynamic graphs model evolving real-world relationships, where nodes represent entities and edges capture their interactions. These graphs are dynamic, with nodes, edges, weights, or attributes continuously added, removed, or updated over time. Analyzing their temporal patterns is a critical challenge due to their broad applications in fields such as social networks and biological systems. To support this, challenging benchmarks using real-world datasets have been developed, facilitating efficient learning on dynamic graphs~\citep{huang2024temporal, huang2024benchtemp}. A key task in this domain is \textit{link prediction}, which focuses on forecasting future connections between nodes and is foundational for dynamic graph analysis.

Recent methods have increasingly focused on advanced neural network architectures for dynamic graph tasks~\citep{Kumar_2019,Xu2020Inductive,tgn_icml_grl2020,wu2024feasibility,10.5555/3692070.3692716}. However, dynamic graph datasets often exhibit strong recency and popularity patterns that can be effectively captured with simple memorization heuristics. Despite their simplicity, these heuristics have proven to be surprisingly robust baselines, frequently matching or outperforming more complex neural network-based approaches~\citep{poursafaei2022strong,poursafaei2022towards,daniluk2023temporal}.

This work enhances the understanding of recency and popularity in temporal graphs by introducing heuristic algorithms that effectively capture multi-scale temporal patterns. These simple yet powerful methods demonstrate ``unreasonable effectiveness'', outperforming neural models in multiple datasets while also providing a scalable framework for analyzing how temporal dynamics influence ranking behavior.

%, 3) discussing the interplay between temporal patterns, commonly employed sampled evaluation strategies, and their associated limitations, 4) proposing a straightforward method for integrating these heuristics into other models, thereby enhancing their performance, and, finally, 5) introducing a novel dataset specifically designed to mitigate the influence of easily captured patterns, providing a more robust benchmark for evaluating link prediction tasks.

% We argue, much like previous work \cite{poursafaei2022towards}, that a dynamic graph representation should perform better than memorization on any given dataset and therefore outperform the given baselines in this work. We wish for these to serve not only as strong baselines, but as a toolbox in analyzing a dynamic graph dataset to discern the biases contained in them. 

\section{Related Work}
The effect of recency and popularity patterns has been extensively studied in the recommender system literature where it is typically attributed to selection, exposure, presentation, and other biases in interaction data~\citep{chen2023bias,wang2023survey,klimashevskaia2024survey}. Prior research on temporal patterns in dynamic graph datasets has focused on three main directions.

\textbf{Summary metrics.} A range of metrics has been developed to characterize the presence of various temporal patterns. For instance, \citet{poursafaei2022towards} characterized \textit{novelty} (new edges per timestamp), \textit{reoccurrence} (fraction of transductive edges), and \textit{surprise} (test-only edges), demonstrating the challenge of predicting entirely new connections. Similarly, \citet{daniluk2023temporal} proposed statistical distance-based measures to capture both short- and long-term global popularity dynamics, exposing weaknesses in existing evaluation protocols and negative sampling strategies.

\textbf{Tools for interpretation and visualization.} Complementing these metrics are tools designed to make temporal patterns more interpretable. \citet{poursafaei2022towards} introduced Temporal Edge Appearance (TEA) and Temporal Edge Traffic (TET) plots, which reveal when memorization-based approaches may fail -- particularly in sparse graphs or when reoccurrence is low and the surprise index is high. \citet{shirzadkhani2024temporal} later built on this work to provide deeper insights into data characteristics.

\textbf{Leveraging temporal heuristics for prediction.} Beyond measurement and visualization, researchers have proposed models and heuristics to exploit temporal information for prediction tasks. \citet{poursafaei2022towards} presented the EdgeBank heuristic, which achieves strong performance in transductive settings, while \citet{daniluk2023temporal} introduced PopTrack, a simple popularity-based heuristic that outperformed state-of-the-art methods on multiple benchmarks which was then used to create harder negative samples. In a related vein, \citet{poursafaei2022strong} demonstrated that combining structural, interaction-based, and temporal features can produce expressive node representations for accurate classification in both static and dynamic scenarios.% \citet{cong2023we} identified a similar pattern and introduced simple neural network components to account for different aspects of the signal. 

\section{Method}

\subsection{The Notion of Recency}
\label{sec:recency}

Analyses of multiple benchmark datasets indicate that among various link prediction heuristics for dynamic graphs, \textit{recency} (how recently a node has appeared as a destination) emerges as one of the most effective. In many real-world networks, recently active nodes often continue to participate, making recency a robust predictor. Moreover, frequent events also remain highly ranked through continually updated timestamps, reducing the need for added weighting. Building on those observations, the concept of recency is extended to multiple temporal scales, providing a more comprehensive perspective on dynamic graph behavior.

\textbf{Global Recency (GR).} This score identifies the most recently observed destination nodes across the entire graph. Instead of estimating a distribution, a simpler approach records each node's last appearance as a destination node, emphasizing temporal precision through memorization:
${
\text{GR}(v, t) = \max (\{-1\} \cup \{\tau \mid (u, v, \tau) \in \mathcal{G}, \tau < t\}),
% \text{GR}(v, t) = 
% \begin{cases}
% \max \{ \tau \mid (u, v, \tau) \in \mathcal{G}, \tau < t \}, & \text{if such } \tau \text{ exists} \\
% -1, & \text{otherwise},
% \end{cases}
}$
where $\mathcal{G} \subset V \times V \times T$ is the set of temporal edges $(u, v, t)$, and $t \in T$ is a timestamp of the most recent occurrence of node $v \in V$ as a destination.

\textbf{Local Recency (LR).} This score captures the node-level temporal activity of individual destination nodes by focusing on their incoming connections. Rather than relying on a fixed time window, as in EdgeBank, each node retains a time-sorted list of its incoming nodes, effectively reflecting immediate temporal interactions:
${
\text{LR}(u, v, t) = \max (\{-1\} \cup \{\tau \mid (u, v, \tau) \in \mathcal{G}, \tau < t\}),
}$
where $t$ is the timestamp of the most recent interaction between $u$ and $v$.

\subsection{The Notion of Popularity}
\label{sec:pupularity}

As highlighted by \citet{daniluk2023temporal}, many dynamic graph datasets exhibit a pronounced correlation with the historical popularity of destination nodes, reflecting a ``rich-get-richer'' dynamic in which frequently connected nodes continue to attract new links. This effect appears in various real-world systems, where once a node becomes popular, additional edges concentrate around it. Building on this insight, popularity-based heuristics can be implemented analogously to recency-based approaches, capturing how often nodes have served as popular destinations:

\textbf{Global Popularity (GP).} This score counts the total number of times $v$ has appeared as a destination node, being updated at each timestamp:
$
\text{GP}(v, t) = \sum_{\tau < t} \sum_{u' \in V} \mathbbm{1}((u', v, \tau) \in \mathcal{G}),
$
where $\mathbbm{1}(\cdot)$ is the indicator function that equals 1 if the condition holds, and 0 otherwise.

\textbf{Local Popularity (LP).} The score for a node $v$ with respect to a source node $u$ is defined as:
$
\text{LP}(u, v, t) = \sum_{\tau < t} \mathbbm{1}((u, v, \tau) \in \mathcal{G}),
$
where the summation counts the number of times $v$ has appeared as a destination node specifically for source node $u$.

The pseudocode for the proposed heuristics is provided in Algorithm~\ref{alg:heuristics}.

\begin{wrapfigure}[20]{r}{0.44\textwidth}
\begin{minipage}{0.43\textwidth}
\begin{algorithm}[H]
\caption{Recency and Popularity\\ Heuristics}
\label{alg:heuristics}
\begin{algorithmic}[1]
\Require Temporal edges $(u, v, t) \in \mathcal{G}$, metric function $m$
\State Initialize \texttt{LR}, \texttt{GR}, \texttt{LP}, \texttt{GP} as empty dictionaries
\For {$t \in \mathcal{T}$}
    \For {$(u, v) \in \mathcal{G}_t$}
        \For {$h \in \{\texttt{LR}, \texttt{GR}, \texttt{LP}, \texttt{GP}\}$}
            \State Compute $m(h, u, v, t)$
        \EndFor
    \EndFor
    \For {$(u, v) \in \mathcal{G}_t$}
        \State \texttt{LR}[$u$][$v$] $\gets t$
        \State \texttt{GR}[$v$] $\gets t$
        \State \texttt{LP}[$u$][$v$] $\gets$ \texttt{LP}[$u$][$v$] $+ 1$
        \State \texttt{GP}[$v$] $\gets$ \texttt{GP}[$v$] $+ 1$
    \EndFor
\EndFor
\State \Return Scores for \texttt{LR}, \texttt{GR}, \texttt{LP}, \texttt{GP}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}

\subsection{Combining Heuristics}
Tailored heuristics are crucial for different datasets due to their unique characteristics. For example, the Local Recency heuristic performs poorly on the \textit{tgbl-review} dataset because users rarely review the same product twice. This conflicts with the low \textit{novelty} index~\citep{poursafaei2022towards} required for Local Recency, as it struggles to score unseen nodes for a given source. This highlights the need for complementary strategies. Insights from static graph methods, where heuristic combinations leverage individual strengths~\citep{ma2024mixture}, suggest promising directions for extending such approaches to dynamic graphs.

The proposed algorithms also face challenges with ranking ties, which occur when multiple entities receive the same score. Unlike most machine learning models that produce continuous scores, $f \colon \mathcal{G} \rightarrow \mathbb{R}$, these heuristics rely on discrete scoring functions, such as counts or timestamps, i.e., $f \colon \mathcal{G} \rightarrow \mathbb{N}$. For recency-based heuristics, the frequency of ties is influenced by the granularity of dataset timing, with coarse-grained timestamps increasing the likelihood of identical scores. While ties are less common in sampled evaluations with fewer negative examples, they become more prevalent in full-ranking evaluations on datasets with coarser temporal resolution.

An approach in which heuristics are combined addresses this issue by stacking multiple heuristics into a product space, $f \colon \mathcal{G} \rightarrow \mathbb{N}^{|\mathcal{H}|}$, where $\mathcal{H} = \{h_1, h_2, \ldots, h_n\}$ is an ordered set of heuristics. When candidates share the same score under heuristic $h_i$, the next heuristic $h_{i+1}$ determines their internal ranks. This process iterates until ranks are fully resolved, or all heuristics are applied. Structuring the combination this way minimizes ranking ties, reduces discrepancies and improves prediction specificity across datasets. This approach applies to any heuristic in the family $\mathfrak{H} \colon \mathcal{G} \rightarrow S$, where $S \subseteq \mathbb{N}$. For recency heuristics, $S$ represents possible timestamps, while for popularity heuristics, $S = \{0, \ldots, |E|\}$, with $|E|$ as the number of edges. Selecting optimal heuristics for speed and performance depends on the dataset and is left for future study. In this work, unless stated otherwise, we use the order $\text{LR} \rightarrow \text{GR} \rightarrow \text{LP} \rightarrow \text{GP}$.

\section{Experiments and Results}
\label{sec:experiments}
We evaluate the proposed approaches on the TGB~\citep{huang2024temporal} and BenchTemp~\citep{huang2024benchtemp} benchmarks using their respective metrics. As shown in Table~\ref{tab:mrr_tgb}, the heuristic algorithms demonstrate competitive performance, achieving top positions on the TGB leaderboard\footnote{\url{https://tgb.complexdatalab.com/docs/leader_linkprop/}} at the time of writing. Recency mostly outperforms popularity as a predictor across most datasets. However, popularity effectively resolves ties, serving as a complement to methods like LR. Detailed results on both BenchTemp and TGB and comparison to the baseline models are provided in Appendix~\ref{sec:additional_results}.

Two key observations emerge. First, heuristic approaches often outperform modern neural network methods when strong temporal patterns are present. Second, the same dataset, such as \textit{Wikipedia}, can yield different metric values when evaluated under varying protocols, such as those used in TGB and BenchTemp. We hypothesize that these discrepancies stem from two main factors. First, neural network models may struggle to capture dominant temporal patterns, as they are often designed to prioritize long-term dependencies. Second, differences in evaluation protocols can highlight distinct aspects of the data, leading to inconsistencies, especially in sampled settings where results are highly sensitive to the number, quality, and selection of negative examples.

\begin{table}[H]
    \centering
    \scriptsize
    \include{results_table}
    \caption{Mean Reciprocal Rank (MRR) on TGB~\citep{huang2024temporal} test splits, with leaderboard rankings provided in parentheses.}
    \label{tab:mrr_tgb}
\end{table}

\section{Heuristics as Analysis Tools}
\label{sec:analysis}

The prevalence of recency and popularity patterns in a dataset is shaped by its underlying data creation processes. To analyze their impact on ranking behavior, we introduce Complementary Normalized Rank (CNR) metric, computed using \textit{optimistic} ($R^+$) and \textit{pessimistic} ($R^-$) ranks~\citep{ali2021bringing,huang2024temporal}, where $R^+$ assumes favorable tie-breaking, and $R^-$ ranks tied candidates conservatively. At a given $p$, CNR is defined as ${\text{CNR}(p) = 1 - {R_p} / {|E|}}$, indicating that a fraction $p$ of edges was ranked at least as high as $R_p = |E|(1 - \text{CNR}(p))$. While not intended for direct method comparisons, this metric provides insights into dataset predictability and helps assess ranking effectiveness. As shown in Figure~\ref{fig:full-ranking-plot}, CNR plots offer a comprehensive view of how temporal patterns shape ranking performance. While ranking all edges at every timestamp is computationally prohibitive for conventional methods, our heuristics enable efficient computation in logarithmic time with respect to $S$. Further implementation details are provided in Appendix~\ref{sec:implementation_details}, and the discussion of CNR plots in Appendix~\ref{sec:frps}.

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-coin_percentiles_test.pdf}
        \caption{TGB \textit{tgbl-coin} dataset.}
        \label{fig:fpr-1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-review_percentiles_test.pdf}
        \caption{TGB \textit{tgbl-review} dataset.}
        \label{fig:fpr-2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/enron_percentiles_test.pdf}
        \caption{BenchTemp \textit{Enron} dataset.}
        \label{fig:fpr-3}
    \end{subfigure}
    \caption{Complementary Normalized Rank (CNR) plots comparing optimistic ($R^{+}$) and pessimistic ($R^{-}$) ranks across various heuristics and their combinations. Each curve shows a method's performance using the CNR metric across percentiles of all edges, illustrating its ranking effectiveness.}
    \label{fig:full-ranking-plot}
\end{figure}


\section{Conclusion}
Our findings show that simple and highly efficient heuristics often outperform modern neural network approaches across a range of real-world benchmarks. Their effectiveness depends on dataset characteristics, while the methods introduced provide practical tools for interpreting these patterns and understanding temporal graph behavior.

Inspired by recommender system research, we argue that accurately \textit{modeling} recency and popularity patterns in temporal graph data may not always improve domain-specific metrics, as these patterns often reflect unintended biases such as selection, position, and exposure effects. We defer the exploration of de-biasing techniques for temporal graph datasets, better evaluation protocols, and methods for integrating heuristic signals into neural models to future work.

\subsection*{Acknowledgement}
This work was partially funded by Wallenberg AI, Autonomous Systems and Software Program (WASP).

\bibliography{ref}
\bibliographystyle{iclr2025}

\appendix

\section*{Appendix}

\section{Additional Results}
\label{sec:additional_results}

Table~\ref{tab:full_tgb_leaderboard} presents TGB test MRR results alongside the official leaderboard\footnote{\url{https://tgb.complexdatalab.com/docs/leader_linkprop/}}, showing that our heuristics consistently outperform baseline methods across all cases.  
Table~\ref{tab:full_benchtemp_leaderboard} reports AUC-ROC results for our heuristics compared to the BenchTemp leaderboard\footnote{\url{https://my-website-6gnpiaym0891702b-1257259254.tcloudbaseapp.com/}}, highlighting substantial performance variability across datasets. These results reinforce that each heuristic's effectiveness depends on the presence of the specific temporal pattern it is designed to exploit.

For TGB, we use baseline results from the official leaderboard, except for \textit{tgbl-review} and \textit{tgbl-subreddit}, as the former was updated and the latter was not included at the time of writing. In these cases, we conduct our own evaluations using the reference code, maintaining the TGB hyperparameter configurations. For BenchTemp, we adopt leaderboard baselines and ensure fair comparisons by applying its negative sampling method with a 3-fold evaluation for robust metric estimation.

In the corresponding benchmarks, we compare our approach against JODIE~\citep{kumar2019predicting}, NeurTW~\citep{jin2022neural}, DyGFormer~\citep{yu2023towards}, NAT~\citep{luo2022neighborhood}, TNCN~\citep{zhang2024efficient}, CAWN~\citep{wang2021inductive}, TGN~\citep{rossi2020temporal}, TCL~\citep{wang2021tcl}, TGAT~\citep{Xu2020Inductive}, DyRep~\citep{trivedi2019dyrep}, and GraphMixer~\citep{cong2023we}.

\begin{table}[h]
    \scriptsize
    \centering
    \include{full_tgb_leaderboard}
    \caption{Comparison of the TGB~\citep{huang2024temporal} leaderboard and the proposed heuristics using standardized test MRR, with the \textcolor{red}{\textbf{\textbf{best}}}, \textcolor{blue}{\textbf{second-best}}, and \textbf{third-best} results highlighted in bold and color-coded.}
    \label{tab:full_tgb_leaderboard}
\end{table}

\begin{table}[h]
    \scriptsize
    \centering
    \include{full_benchtemp_leaderboard}
    \caption{Comparison of the BenchTemp~\citep{huang2024benchtemp} leaderboard and the proposed heuristics, with the \textcolor{red}{\textbf{best}}, \textcolor{blue}{\textbf{second-best}}, and \textbf{third-best} results highlighted in bold and color-coded. We observe that, in some cases, the metric becomes fully saturated, likely due to shortcomings in the sampled evaluation scenario, which tends to overestimate performance by including excessively easy negative examples.}
    \label{tab:full_benchtemp_leaderboard}
\end{table}

\section{Efficient Implementation of Heuristics}
\label{sec:implementation_details}

Computing top-$K$ ranking metrics, such as MRR, across an entire dataset is often computationally expensive. A brute-force approach to determine the exact rank of an entity requires scoring all entities against a query, resulting in a complexity of $\mathcal{O}(|V|)$. To speed up evaluation, many methods sample a smaller set of false (negative) examples and rank the positive item within this subset. However, such methods are biased and inconsistent estimators of true ranking metrics~\citep{krichene2020sampled}. Only AUC-ROC has been proven to provide consistent evaluations, where expected values converge to true performance as the sample size grows.

In contrast, the proposed heuristics enable efficient calculation of full rankings for arbitrary queries in $\mathcal{O}(\log S)$ time by leveraging optimized data structures. When scores are integer-based, each score effectively becomes an index in a consolidated list, grouping all nodes with the same score. This arrangement facilitates direct calculation of exact optimistic and pessimistic ranks by summing nodes in indices below (and, for optimistic ranks, equal to) a particular score. For recency-based heuristics, Fenwick Trees~\citep{fenwick1994new} are used for efficient ranking by storing and retrieving these contiguous sums, which reduces the worst-case complexity of computing full ranks from $\mathcal{O}(|V|)$ to $\mathcal{O}(\log S)$, while the memory usage is bounded by $\mathcal{O}(S + |V|)$ where $S$ represents the number of unique timestamps, and $|V|$ is the number of nodes. 

In essence, the algorithm manages edge updates by dynamically tracking their occurrences across timestamps in both global and local settings. This design achieves a balance between precision and scalability, making it well-suited for large-scale temporal graph data. It should be noted that combining heuristics increases overall complexity, as their independence results in higher computational demands compared to using a single heuristic. % In these cases, the resulting complexity becomes $$\mathcal{O}\Bigg(\max_{\{i \in 1,...,|\mathcal{H}|\}} \Big(\min \{n_{i, ties}, R^-_i\}\Big)\Bigg).$$ As a consequence, heuristics with less ties and higher bias towards the heuristic tend to be more efficient. 

To demonstrate effectiveness, we present the runtime measurements in Table~\ref{tab:runtimes}. For heuristic methods, we report the runtime for a single pass through both the training and evaluation sets. Model runtimes are obtained from the BenchTemp leaderboard, where they are executed on GPUs, whereas heuristic methods are run on a CPU.

\section{Complementary Normalized Ranking Plots}
\label{sec:frps}
Figure~\ref{fig:full-ranking-plots-1} presents CNR plots for multiple datasets, illustrating how predicted ranks are distributed across different heuristics and highlighting key patterns in how recency and popularity influence ranking performance.

For instance, Figure~\ref{fig:all-fpr-4} reveals the exceptionally poor performance of LR, largely due to approximately 98\% of new edges connecting to previously unseen destination nodes. This aligns with the dataset's nature, where users typically review a product only once, making historical edges unreliable predictors of new interactions. As a result, LR struggles, as it prioritizes recently seen destinations that rarely reappear. To address this, we apply an \textit{inverse} LR heuristic, which penalizes previously seen destinations and prioritizes unseen nodes. This adjustment better reflects the dataset's dynamics, where new interactions are more likely to involve unreviewed products. We further refine this approach by combining inverse LR with GR, significantly improving the resulting performance.

Similarly, Figure~\ref{fig:all-fpr-19} shows minimal recency and popularity effects in the \textit{TaoBao} dataset, a user-item bipartite network with a low average degree of 0.94, consistent with its structural characteristics.

Displaying both pessimistic (\(R^-\)) and optimistic (\(R^+\)) ranks is crucial for evaluating heuristics like Local Recency, which often produce ties. For example, the \textit{USLegis} dataset (Figure~\ref{fig:all-fpr-23}) exhibits a significant gap between \(R^+\) and \(R^-\), reflecting frequent ties. Identifying such discrepancies helps reduce uncertainty and informs whether a single heuristic suffices or if multiple ranking strategies are needed.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-review_percentiles_test.pdf}
        \caption{\textit{tgbl-review} dataset.}
        \label{fig:all-fpr-4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-wiki_percentiles_test.pdf}
        \caption{\textit{tgbl-wiki} dataset. }
        \label{fig:all-fpr-1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-comment_percentiles_test.pdf}
        \caption{\textit{tgbl-comment} dataset.}
        \label{fig:all-fpr-3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-flight_percentiles_test.pdf}
        \caption{\textit{tgbl-flight} dataset.}
        \label{fig:all-fpr-5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-lastfm_percentiles_test.pdf}
        \caption{\textit{tgbl-lastfm} dataset.}
        \label{fig:all-fpr-6}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/tgbl-subreddit_percentiles_test.pdf}
        \caption{\textit{tgbl-subreddit} dataset.}
        \label{fig:all-fpr-7}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/CollegeMsg_percentiles_test.pdf}
        \caption{\textit{CollegeMsg} dataset.}
        \label{fig:all-fpr-11}
    \end{subfigure}
    \hfill
    % \begin{subfigure}[t]{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figs/Contacts_percentiles_test.pdf}
    %     \caption{\textit{Contacts} dataset.}
    %     \label{fig:all-fpr-12}
    % \end{subfigure}
    % \begin{subfigure}[t]{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figs/Flights_percentiles_test.pdf}
    %     \caption{\textit{Flights} dataset.}
    %     \label{fig:all-fpr-14}
    % \end{subfigure}
    % \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/lastfm_percentiles_test.pdf}
        \caption{\textit{LastFM} dataset.}
        \label{fig:all-fpr-15}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/mooc_percentiles_test.pdf}
        \caption{\textit{MOOC} dataset.}
        \label{fig:all-fpr-16}
    \end{subfigure}
    \hfill
    % \begin{subfigure}[t]{0.32\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figs/reddit_percentiles_test.pdf}
    %     \caption{\textit{Reddit} dataset.}
    %     \label{fig:all-fpr-17}
    % \end{subfigure}
    % \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/SocialEvo_percentiles_test.pdf}
        \caption{\textit{SocialEvo} dataset.}
        \label{fig:all-fpr-18}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/taobao_percentiles_test.pdf}
        \caption{\textit{TaoBao} dataset}
        \label{fig:all-fpr-19}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/uci_percentiles_test.pdf}
        \caption{\textit{UCI} dataset.}
        \label{fig:all-fpr-20}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/UNtrade_percentiles_test.pdf}
        \caption{\textit{UNtrade} dataset.}
        \label{fig:all-fpr-21}
    \end{subfigure}
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/UNvote_percentiles_test.pdf}
        \caption{\textit{UNVote} dataset}
        \label{fig:all-fpr-22}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/USLegis_percentiles_test.pdf}
        \caption{\textit{USLegis} dataset.}
        \label{fig:all-fpr-23}
    \end{subfigure}
    \caption{Complementary Normalized Ranking Plots showing optimistic ($R^{+}$) and pessimistic ($R^{-}$) ranks for different heuristics and their combinations, for TGB (panels a-f) and BenchTemp (panels g-o) datasets. Each curve represents a method's performance across different percentiles of all edges in the dataset, illustrating how well it ranks them overall.}
    \label{fig:full-ranking-plots-1}
\end{figure}

%  \begin{algorithm} \caption{Computing Full Rank for Heuristic}
% \label{alg:full_recency} 
% \begin{algorithmic}[1]
% \Require {Temporal edges $(u, v, t) \in \mathcal{G}$, heuristic $\mathcal{H} : \mathcal{G} \rightarrow \mathbb{S}$}
% \State Initialize sparse Fenwick Tree $FT$ with size 
% \State Initialize lists $R^+$ and $R^-$
% \For {$t \in \mathcal{T}$}
%     \For {$(u,v) \in \mathcal{G}_t$}
%         \If{$v$ has been seen for $\mathcal{H}$}
%             \State $R^+_{(u, v, t)} = FT(t) - FT(t_{(u, v)}) + 1$ and append to $R^+$
%             \State $R^-_{(u, v, t)} = FT(t) - FT(t_{(u, v)} - 1) + 1$ and append to $R^-$
%         \Else
%         \EndIf
%     \EndFor
%     \For {$(u,v) \in \mathcal{G}_t$}
%         \State Set $t_{(u, v)} = t$
%     \EndFor
% \EndFor
% \Return Scores for LR and GR
% \end{algorithmic}
% \end{algorithm}
% \begin{table*}[]
%     \centering
%     \include{benchtemp_results}
%     \caption{Benchtemp res table}
%     \label{tab:my_label}
% \end{table*}

\begin{table}[H]
    \centering
    \scriptsize
    \include{runtimetable}
    \caption{Epoch runtimes for various models, measured in seconds.}
    \label{tab:runtimes}
\end{table}


\end{document}
