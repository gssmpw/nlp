We demonstrate that retrieval-augmented VLMs have improved performance over general-purpose VLMs, with comparable memorization rates. However, there is still a substantial performance gap between finetuned and baseline QA models. By introducing UCR and PPR, we provide concrete measures of how retrieval mitigates memorization. This analysis outlines the interplay between parametric knowledge and external retrieval. Our measures are validated by the fact that they reveal this well-known tradeoff between memorization and generalization. Our work provides a foundation for future research aimed at refining retrieval mechanisms and ensuring that external sources effectively complement the parametric knowledge of VLMs.

% In summary, the paper highlights the potential and challenges of multimodal multi-hop reasoning, emphasizing the need for models capable of handling variable sets of input data. The findings suggest that the Fusion-in-Decoder architecture is effective, even in the variable input case where large proportions of the output embeddings are padded, and future work could explore applying these insights to other multimodal multi-hop tasks and datasets.

% Going forward, this work can be further enhanced by adding a decoder so that the full WebQA training set may be utilized, extending the model to work with larger sets of input images, and investigating performance on other mulitmodal multi-hop tasks.


% In this paper, we compare general-purpose large language models (LLMs) and specialized finetuned models on multimodal question-answering (QA) tasks using the WebQA benchmark. Our findings indicate that while zero-shot prompting LLMs like GPT-4o demonstrate impressive capabilities in both retrieval and QA, specialized models such as MH-VoLTA excel in well-defined problem domains. Our analysis underscores the importance of task complexity and data availability in model selection, suggesting that zero-shot prompting LLMs are suitable for simpler tasks and limited data scenarios, and that finetuned models are preferable for more complex tasks and data-rich environments. We highlight the role of retrieval accuracy in WebQA performance. Finally, we find that GPT-4o's In-Context Retrieval is notably effective, enabling it to achieve SoTA results on the WebQA benchmark. Our work advances the understanding of retrieval-augmented generation and multimodal QA systems, providing a foundation for future research in this area.

% Retrieval-augmented systems enable general-purpose VLMs to excel on specialized tasks without overfitting to a single domain. By quantifying the balance between parametric memory and external retrieval, we demonstrate that finetuned retrievers can offset the loss in performance from adopting general-purpose VLMs in place of finetuned QA models. Demonstrating that models rely on retrieved sources rather than parametric memory is important for model generalizability. 

