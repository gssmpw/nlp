% \begin{figure*}
%     \centering
%     % Left column - Large image
%     \begin{minipage}{0.55\textwidth}
%         \centering
%         \subfigure[Accuracy (Acc, \autoref{eq:ACC}), \ppr (\PPR, \autoref{eq:\PPR}), and Retriever Attainment (\RPA, \autoref{eq:RPA}) for each pairing of retriever R and QA model M.]{\includegraphics[width=\linewidth]{figures/eval_lineplot.png} \label{fig:eval_lineplots}}
%     \end{minipage}\hfill
%     % Right column - Two stacked images
%     \begin{minipage}{0.4\textwidth}
%         \centering
%         \subfigure[$\text{\UCR}(\text{GPT},\text{GPT}) = \frac{104}{104+297} = 0.26$]
%         {\includegraphics[width=\linewidth]{figures/results/retrieval_vs_qa.png} \label{fig:retrieval_vs_qa_heatmap}}
%         % \vspace{0.5cm} % Adjust spacing between images
        
%         \subfigure[Retrieval Recall vs QA accuracy]{\includegraphics[width=0.9\linewidth]{figures/retrieval_recall_vs_qa_accuracy_lines.png} \label{fig:retrieval_vs_qa_lines}}

%         % \vspace{0.5cm} % Adjust spacing between images
        
%         % \subfigure[Accuracy by question category]{\includegraphics[width=\linewidth,trim={0 0 0 3cm},clip]{figures/results/mhvolta_gpt_perf_comp.png} \label{fig:question_categories}}
        
%     \end{minipage}
%     \caption{a) Evaluation metrics on end-to-end retrieval \& QA systems. b) Most correctly answered questions have correctly retrieved sources, while c) retrieving distractor sources is detrimental to performance.}
%      % d) QA accuracy is robust across question categories.
%     \label{fig:main}
% \end{figure*}





First, we experiment with multiple QA models to determine which generalized LLMs and specialized, finetuned models should be selected for the joint retrieval and QA task. Then, we experiment with different combinations of retrieval systems and chosen QA models for the joint task. Finally, we analyze how performance on the retrieval task impacts QA accuracy and investigate the relationship between question complexity, performance, and model memorization.

\subsection{Model Selection}
 We find that the \hyperref[sec:mh-volta]{MH-VoLTA} model outperforms all baseline and zero-shot models on the WebQA validation set image questions, including \hyperref[sec:blip]{BLIP-2}, \hyperref[sec:GIT]{GIT}, \hyperref[sec:VLP]{VLP}, GPT-4o, and \hyperref[sec:gpt3.5]{GPT-3.5}. We also find that MH-VoLTA performance is comparable to VoLTA on the (fixed input) VQA and NLVR2 tasks (section \ref{sec:model_selection} in the appendix). For a breakdown of model performance by question category on the WebQA dataset, see section \ref{sec:category_perf} in the appendix. 


\begin{figure}[!t]
    \centering
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/results/retrieval_vs_qa.png}
        \caption{Most questions answered correctly by GPT-4o have correctly retrieved sources, as given by low UCR scores: 
        \(\ensuremath{\UCR(\text{GPT},\text{GPT}) = \frac{104}{104+297} = 0.26}\)}
        \label{fig:retrieval_vs_qa_heatmap}
    \end{subfigure}
    
    \begin{subfigure}{\linewidth}
        \includegraphics[width=\linewidth]{figures/retrieval_recall_vs_qa_accuracy_lines.png}
        \caption{Across experiments, lower retrieval recall impacts QA accuracy as distractor sources hurt performance.}
        \label{fig:retrieval_vs_qa_lines}
    \end{subfigure}
\end{figure}


\subsection{Impact of Retrieval on QA}
To understand how retrieval and QA systems interact, we investigate the reliance of the QA task on retrieval correctness. We find that when GPT-4o correctly retrieves the relevant sources (retrieval recall is high), it has a 59\% QA accuracy rate. Conversely, if GPT-4o does not retrieve the correct sources, the accuracy rate is reduced to 26\% (\autoref{fig:retrieval_vs_qa_heatmap}). We also find that QA performance drops as the number of retrieved distractors increases and retrieval recall falls, showing that poor retrieval performance adversely affects QA (\autoref{fig:retrieval_vs_qa_lines}). This is to say that the QA task is heavily dependent on retrieval performance. However, there do exist correctly answered questions for which incorrect sources are retrieved, and these samples form the basis for the \UCR measure of the parametric effect (\autoref{fig:retrieval_vs_qa_heatmap}).

% This could be explained in one of two ways; the more complex a domain (the more labels there are to predict), the worse a specialized, finetuned model's performance will be. However, it is more likely that the limited training samples for the shape category in comparison to the other categories is the underlying cause.

% \begin{figure}
%     \centering
%         \begin{subfigure}{\linewidth}
%         \includegraphics[width=\linewidth]{figures/results/retrieval_vs_qa.png} % Replace with your image file path
%         \caption{Most questions answered correctly by GPT-4o have correctly retrieved sources, as given by low UCR scores \(\ensuremath{\text{\UCR}(\text{GPT},\text{GPT}) = \frac{104}{104+297} = 0.26}\)}
%         \label{fig:retrieval_vs_qa_heatmap} % No caption for this subfigure
%     \end{subfigure}
    
%     \begin{subfigure}{\linewidth}
%         \includegraphics[width=\linewidth]{figures/retrieval_recall_vs_qa_accuracy_lines.png} % Replace with your image file path
%         \caption{Across experiments, lower retrieval recall impacts QA accuracy as distractor sources hurt performance.}
%         \label{fig:retrieval_vs_qa_lines} % No caption for this subfigure
%     \end{subfigure}
% \end{figure}

\subsection{End-to-End Retrieval and QA}
The \PPR and \RPA measures enable a quick comparison of joint retrieval and QA systems, where \autoref{fig:eval_lineplots} reveals some interesting trends. We find that of all QA models tested, GPT-4o benefits the most from the use of retrievers---\rpa (\RPA) scores are highest for GPT-4o---while finetuned QA models such as Qwen2-FT and MH-VoLTA receive a lower performance increase as the coupled retrieval system is improved.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/eval_lineplot.png}
    \caption{Evaluation metrics on end-to-end retrieval \& QA systems: Accuracy (Acc, \autoref{eq:ACC}), \ppr (\PPR, \autoref{eq:PPR}), and Retriever Attainment (\RPA, \autoref{eq:RPA}) (denoted by the three lines) for each pairing of retriever R and QA model M. Note that the lines denoting \PPR and \RPA are rescaled to represent the denominators in the respective equations. }
    \label{fig:eval_lineplots}
\end{figure}

GPT-4o also has the best \PPR score. That is, GPT-4o has the biggest gap in performance when comparing the worst case (random negative) and best case (source oracle) retrievers, with a \PPR of 0.32. In comparison, Qwen2 has higher performance under the random retriever, and as such displays a greater reliance on parametric memory. 

There is also a clear trend between finetuning, QA accuracy, and \ppr (\PPR). While finetuned Qwen2 (Qwen2-FT) has improved accuracy vs Qwen2, it's performance on the worst case retriever is surprisingly high (\PPR=0.77). This is even more extreme for MH-VoLTA, which obtains both the highest QA accuracy (0.72) and the highest \PPR (0.82). The same trend is apparent when evaluating on the WebQA test set, where finetuning Qwen2 improves accuracy (\autoref{tab:test_scores}). Note, \PPR cannot be measured on the test set, as labels have not been made public.

\begin{table}[]
    \centering
    \caption{QA Accuracy on WebQA test set.}
    \label{tab:test_scores}
    \begin{tabular}{ccr}
    \toprule
Retriever R & Model M & $\text{Acc}^{\text{test}}_M(R)$ \\
\midrule
UniVL-DR & Qwen2 & 0.52 \\
UniVL-DR & Qwen2-FT & 0.70 \\
Uni-VLDR & GPT-4o & 0.73 \\
GPT-4o & GPT-4o & \textbf{0.77} \\ 
\bottomrule
    \end{tabular}
\end{table}



% Keep in mind that a \PPR score of 1 would be equivalent to playing golf blindfolded and hitting a hole-in-1 every shot.

\subsection{Finetuning and \UCR}
We find that, while the finetuning process improves accuracy (and in part because of this fact), finetuning exacerbates the parametric effect. Qwen2-FT has a higher \ppr than the baseline Qwen2 model (\PPR, \autoref{fig:eval_lineplots}), and it has a higher \ucr (\UCR) than Qwen2 across all retrieval methods tested (\autoref{tab:ucr_scores}). What's more, the act of finetuning Qwen2 has an outsized effect on \UCR when compared with the effect that changing the retriever has. MH-VoLTA represents the extreme case; for each retriever R, $\text{\UCR}(R,\text{MH-VoLTA})>0.5$, implying that MH-VoLTA is correctly answering the majority of questions for which the retrieval system fails to identify the `correct' sources. 

However the effect of retrieval on \UCR is not negligible, and we find that for a given QA model, \UCR increases as retrieval recall increases; i.e. for each model M $\text{\UCR}(\text{Rand}, M)<\text{\UCR}(\text{Clip}, M)<\text{\UCR}(\text{ClipDPR}, M)$ (\autoref{tab:ucr_scores}). This implies that as the retriever improves, the QA model is more successful on samples that retrieval fails on. This paradox is explained by inaccuracies in the source labels---`distractor' sources often provide enough evidence for the QA model to answer correctly. Rather than exposing memorization, this reveals an underlying issue with the source labels in the WebQA dataset, and as such, these measures can be adapted to evaluate the correctness of the joint retrieval-QA benchmarks.


\begin{table}[!t]
\centering
\caption{\UCR ($P(Q_1|R_0)$) for each retriever R and QA model pairing, alongside retrieval recall. CDPR denotes Clip-DPR, Q-FT is Qwen2-FT.}
\label{tab:ucr_scores}
\begin{tabular}{clllll}
\toprule
R & recall & Qwen & Q-FT & MHV & GPT4 \\ % & $\text{Acc}_{\text{test}}$(R,M) & \UCR(R,M) \\
\midrule
Rand & 0.00 & 0.260 & 0.449 & 0.595 & 0.174 \\
Clip & 0.46 & 0.328 & 0.467 & 0.617 & -- \\
CDPR & 0.77 & 0.420 & 0.517 & 0.643 & -- \\
UniVL & 0.80 & 0.438 & 0.521 & 0.616 & 0.420 \\
GPT4 & 0.65 & -- & -- & -- & 0.259 \\
% CLIP & Qwen2 & -- & 0.36 \\
% CLIP-FT & Qwen2 & -- &  0.113 \\
% UniVL-DR & Qwen2 & 0.52 &  0.101 \\
% CLIP & Qwen2-FT & -- & 0.41 \\
% CLIP-FT & Qwen2-FT & -- & 0.119 \\
% UniVL-DR & Qwen2-FT & 0.70 & 0.099 \\
% CLIP-FT & MH-VoLTA & -- & 0.093 \\
% Uni-VLDR & MH-VoLTA & -- & 0.073 \\
% Uni-VLDR & GPT-4o & 0.73 & 0.066 \\
% GPT-4o & GPT-4o & 0.77 & 0.316 \\ 
\bottomrule
\end{tabular}
\end{table}

% We compare end-to-end performance (retrieval and QA) of the finetuned (FT, FT), In-Context RALM (FT, GPT-4o), and In-Context RLM (GPT-4o, GPT-4o) approaches on the WebQA validation set image-based queries. The results show that the finetuned UniVL-DR and MH-VoLTA system (FT, FT) outperforms In-Context RLM and RALM (\autoref{tab:ucr_scores}). We also run the generative end-to-end models on the test set, achieving SoTA accuracies.

% We find that In-Context RALM (FT, GPT-4o) is extremely effective; having 5\% higher accuracy than In-Context RLM (GPT-4o, GPT-4o). The UniVL-DR retriever alone achieves an F1 that is 0.07 higher than GPT-4o retrieval, while using a source oracle as a retriever (`Oracle') improves QA accuracy with respect to the finetuned UniVL-DR retriever by a slim margin; 3.4\% for GPT-4o QA and 1.9\% for finetuned MH-VoLTA QA (\autoref{tab:ucr_scores}).

% We compare In-Context RALM (GPT-4o, GPT-4o) and RLM (FT, GPT-4o) on the WebQA test set by submitting to the leaderboard. Surprisingly, we find that not only does In-Context RLM outperform In-Context RALM accuracy by 4.5\%, but that overall it improves upon SoTA accuracy results by 4\% at the time of writing.


\begin{figure*}
\centering
% For GPT-4o model
\includegraphics[width=.3\textwidth]{figures/results/acc_vs_word_count.png}\hfill
\includegraphics[width=.33\textwidth]{figures/results/acc_vs_fkgl.png}\hfill
\includegraphics[width=.29\textwidth]{figures/results/acc_vs_gfi.png}\hfill
    \caption{GPT-4o retrieval and QA performance reveal opposite trends with respect to question complexity; GPT-4o retrieval improves with increased complexity, while GPT-4o QA accuracy degrades.}
    \label{fig:complexity_vs_qa}
\end{figure*}


\begin{figure*}
\centering
% For fintuned model
\includegraphics[width=.3\textwidth]{figures/results/ft_gpt_combo_acc_vs_word_count.png}\hfill
\includegraphics[width=.33\textwidth]{figures/results/ft_gpt_combo_acc_vs_fkgl.png}\hfill
\includegraphics[width=.29\textwidth]{figures/results/ft_gpt_combo_acc_vs_gfi.png}\hfill

    \caption{UniVL-DR retriever performance is independent of question complexity. When coupled with this retriever, the effects of question complexity on GPT-4o and MH-VoLTA QA accuracy is minimized.}
    \label{fig:complexity_vs_qa_ft}
    \vspace{-5mm}
\end{figure*}

\subsection{Question Complexity} 
We observe and report interesting relationships between query complexity and retrieval and QA performance. We find that the accuracy of the in-context GPT-4o retriever is related to question complexity (\autoref{fig:complexity_vs_qa}). The more complex the question in terms of word count, Flesch-Kincaid Grade, or Gunning Fog Index, the lower the QA performance (\autoref{fig:complexity_vs_qa}). Conversely, increasing query complexity improves GPT-4o's retrieval ability, where the additional complexity provides information on source relevancy \footnote{We note that this opposing relationship between retrieval and QA performance is contrary to the finding that the QA task is heavily dependent on retrieval performance (\autoref{fig:retrieval_vs_qa_lines}). The impact of query complexity on task performance is strong enough to overcome this general principle.}. However, this relationship does not hold for the finetuned UniVL-DR retriever, where question complexity has little effect on retrieval recall or QA accuracy (\autoref{fig:complexity_vs_qa_ft}). As such, systems that rely on "in-context" retrieval using GPT-4o are limited by query complexity, but approaches that utilize finetuned retrievers are not.

% \includegraphics[width=.3\textwidth]{figures/results/acc_vs_unique_pos_tag_count.png}\hfill

% \begin{figure*}
% \centering
% % For fintuned model
% \includegraphics[width=.3\textwidth]{figures/results/ft_acc_vs_word_count.png}\hfill
% \includegraphics[width=.3\textwidth]{figures/results/ft_acc_vs_fkgl.png}\hfill
% \includegraphics[width=.3\textwidth]{figures/results/ft_acc_vs_gfi.png}\hfill

%     \caption{How does finetuned retrieval and QA performance depend upon question complexity?}
%     \label{fig:complexity_vs_qa_ft}
% \end{figure*}

%  ------------ RESULTS GRAVEYARD ----------------

% \begin{table*}[]
% \centering
% \caption{Evaluation measures for each combination of retriever and QA model on the restricted WebQA validation set, Acc(R,M), and full WebQA test set, $\text{Acc}_{\text{test}}$(R,M).}
% \label{tab:ucr_scores}
% \resizebox{0.83\linewidth}{!}{\begin{tabular}{cclllllll}
% \toprule
% Retriever R & QA Model M & $\text{Acc}_{\text{test}}$(R,M) & Acc(Random,M) & $<$ Acc(R,M) $<$ & Acc(Oracle,M) & \RPA(R,M) & \PPR(M) & \UCR(R,M) \\
% \midrule
% CLIP & Qwen2 & -- & 0.29 & 0.44 & 0.54 & 0.4       & 0.53 & 0.36 \\
% CLIP-FT & Qwen2 & -- & \ditto & 0.5 & \ditto & 0.18       & \ditto & 0.113 \\
% UniVL-DR & Qwen2 & 0.52 & \ditto & 0.49 & \ditto & 0.23       & \ditto & 0.101 \\
% CLIP & Qwen2-FT & -- & 0.47 & 0.54 & 0.61 & 0.51       & 0.77 & 0.41 \\
% CLIP-FT & Qwen2-FT & -- & \ditto & 0.58 & \ditto & 0.21       & \ditto & 0.119 \\
% UniVL-DR & Qwen2-FT & 0.70 & \ditto & 0.59 & \ditto & 0.19       & \ditto & 0.099 \\
% CLIP-FT & MH-VoLTA & -- & 0.2 & 0.22 & 0.71 & 0.97 & 0.28 & 0.093 \\
% Uni-VLDR & MH-VoLTA & -- & \ditto & 0.69 & \ditto & 0.04 & \ditto & 0.073 \\
% Uni-VLDR & GPT-4o & 0.73 & 0.18 & 0.53 & 0.56 & 0.09 & 0.32 & 0.066 \\



% None & GPT-4o  & &  & 0.45 & & & & \\
% GPT-4o & GPT-4o & 0.77 &  & 0.48 & & & & 0.316 \\ % 0.162 \\


% % UniVL-DR & Qwen2 & 0.52 & 0.49 & 0.29 & 0.55 & 0.24 & 0.54 &  \\
% % UniVL-DR & Qwen2-FT & 0.70 & 0.59 & 0.47 & 0.61 & 0.19 & 0.76 & \\
% % UniVL-DR & MH-VoLTA & -- & 0.69 & 0.20 & 0.71 & X & X & 0.066 \\
% % UniVL-DR & GPT-4o & 0.73 & 0.53 & 0.18 & 0.56 & X & X & 0.073 \\
% % UniVL-DR & GPT-4o & & & & & & \\
% % Random & GPT-4o  & 0.18 & -- \\
% % UniVL-DR & GPT-4o  & 0.53 & \textbf{0.073} \\ % 0.066 \\
% % Oracle & GPT-4o  & 0.56 & -- \\
% % Random & Qwen2 &  0.29 & \\
% % Oracle & Qwen2 &  0.55 & \\
% % Random & Qwen2-FT &  0.47 & \\
% % UniVL-DR & Qwen2-FT &  0.59 & \\
% % Oracle & Qwen2-FT &  0.61 & \\
% % Random & MH-VoLTA & 0.20 & -- \\
% % UniVL-DR & MH-VoLTA & \textbf{0.69} & 0.066 \\ %0.69} & 0.073 \\
% % Oracle & MH-VoLTA  & 0.71 & -- \\
% \bottomrule
% \end{tabular}}
% \end{table*}

% \begin{table}[]
%     \centering
%     \begin{tabular}{ccll}
% \toprule
% Retriever & QA Model & Flu & Acc \\
% \midrule
% UniVL-DR & Qwen2 & 0.67 & 0.52  \\
% UniVL-DR & Qwen2-FT & 0.67 & 0.70 \\
% UniVL-DR & GPT-4o & 0.45  & 0.73 \\
% GPT-4o & GPT-4o & \textbf{0.70} & \textbf{0.77} \\
% \bottomrule
%     \end{tabular}
%     \caption{Results for e2e models on the WebQA test set. Flu indicates fluency \citep{chang_webqa_2021}.}
%     \label{tab:my_label}
% \end{table}














% \begin{figure*}
% \centering
% % For GPT-4o with FT Retrieved model
% \includegraphics[width=.3\textwidth]{figures/results/gpt_ft_ret_acc_vs_word_count.png}\hfill
% \includegraphics[width=.3\textwidth]{figures/results/gpt_ft_ret_acc_vs_fkgl.png}\hfill
% \includegraphics[width=.3\textwidth]{figures/results/gpt_ft_ret_acc_vs_gfi.png}\hfill

%     \caption{}
%     \label{fig:complexity_vs_qa_gpt_ft_ret}
% \end{figure*}



% \vspace{10mm}


 
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/results/mhvolta_gpt_perf_comp.png}
%     \caption{}
%     \label{fig:gpt_vs_volta_by_category}
% \end{figure}




% \begin{subfigure}{.3\textwidth}
%   \centering
%     \includegraphics[width=0.3\textwidth]{figures/results/complexity_vs_qa.png}
%     % \caption{}
%     % \label{fig:complexity_vs_qa}
% \end{subfigure}%
% \begin{subfigure}{0.3\textwidth}
%   \centering
%     \includegraphics[width=0.3\textwidth]{figures/results/complexity_vs_qa.png}
% \end{subfigure}
% \begin{subfigure}{0.3\textwidth}
%   \centering
%     \includegraphics[width=0.3\textwidth]{figures/results/complexity_vs_qa.png}
%     % \caption{GPT-4o's performance depends upon question complexity.}
%     % \label{fig:complexity_vs_qa}
% % for F1 macro scores on a Random Forest model with 50 estimators, and disparate impact levels; for both metrics, higher is better.}
% \end{subfigure}

% Our experiments on query simplification lead to better performance in the GIT model but worsens the performance of BLIP-2. We suspect that one of the reasons for this poor performance in BLIP-2 is because of its inability to map the entities required in some target answers (specially in category 'choose') to the provided images and captions, since the questions themselves have been stripped out of that information.

% We find that performance is relatively similar on models capable of multihop QA (VLP, GPT-3.5), but gaps appear for traditional VQA models such as GIT and BLIP, where single image questions have higher cross-category accuracy.

% \paragraph{External Knowledge}
% GPT-3.5 is a strong baseline because the WebQA task is knowledge-intensive (as opposed to typical VQA tasks such as VQA-2 \citep{goyal2017making}). In addition, BLIP +  GPT-3.5 performs worse than GPT-3.5 because theres more boiler plate around the question to set up the captions, and the captions are not very relevant to gpt. The respectively ACC score and Fluency score is 0.049 and 0.302, of which the ACC score is significantly lower than the other baseline models. As a result, we don't report such score in the table and no longer plan to use it as a baseline model. This is indicative of the importance of entity recognition for WebQA. Further, the prompt for BLIP + GPT-3.5 conditions the GPT-3.5 LM to ignore its embedded knowledge and mainly consider the captions. Experimenting with the original image descriptors may lead to better results here, as they name the entities in the images and so can be connected to the entities in the queries. A concrete example of this bias towards external knowledge can be observed in Appendix, \autoref{fig:blip_example_1} which shows the output of a baseline BLIP-2 model on a question that can be answered from external knowledge alone. In this case, even if the images do not contain the answer, the model simply relies on its pretraining knowledge to answer the question. This behavior is evident in several other cases as well.

% \paragraph{WebQA vs VQA Question Complexity}
% The complexity of questions is important for model performance. For WebQA, we state that entities in the questions are important to match the image descriptions. However, if verbose image descriptions are not utilized (as in traditional VQA), the increased query complexity can hurt the performance of pre-trained VQA models on the WebQA task. This is exemplified by the performance of simple questions on the GIT baseline; simplified questions are much better on GIT, which is finetuned on VQA-2 \citep{goyal2017making}. It is also noteworthy that while simplifying the question improves accuracy for both one and two-image questions on the GIT baseline, the increase is more pronounced for one-image questions.



% As in \autoref{tab:query_complexity}, we split our analysis by source image counts; questions with a single positive image source (subsection \autoref{sec:1img}) and another for questions with two positive image sources (subsection \autoref{sec:2img}). This division allows us to gain insights and make performance comparisons in two specific scenarios: the conventional VQA-like situation with one positive image source and the more complex multi-hop VQA-like scenario with two positive image sources. 

% \subsection{One vs Two-Image Questions}
% \paragraph{Questions with Single Positive Image Source}
% \label{sec:1img}
% The WebQA dataset comprises 1395 questions in this scenario, in contrast to the 1116 questions with two positive image sources. When we focus on questions in the validation split with only a single positive image source, the average accuracy stands at 41.18\%, as opposed to approximately 60\% in the case of questions with two positive image sources. This suggests that the baseline model's performance is notably poorer in the VQA-like scenario.

% Furthermore, when we examine the performance across question categories, transitioning from questions with a single positive image source to those with two positive sources, we observe an approximate 8\% decrease in accuracy for the 'YesNo' category and a 1.5\% decrease for the 'number' category. Conversely, the performance for all other question categories shows improvement

% This suggests that the baseline model's performance is less favorable in the VQA-like scenario, presenting a substantial opportunity for improvement, especially considering that these questions are not even categorized as multi-hop.

% \paragraph{Questions with Two Positive Image Sources}
% \label{sec:2img}
% This aspect is a crucial focus because it underpins one of the fundamental principles of this dataset: multi-hop retrieval and generation. The baseline model's performance is notably better, approximately 1.5 times more accurate, when dealing with questions that involve two positive image sources.

% However, in 'YesNo' and 'choose' question categories, assessing whether the model derived its answer from both images, used only one image, or made a random selection can be challenging. For instance, as demonstrated in Figure \autoref{fig:example_3}, the model provided a correct answer even with incomplete information.



% % \begin{framed}
% %    Q === Are the heads of Iranian women covered in traditional clothing ?

% % Keywords\_A === Yes.

% % Answer: The heads of Iranian women are covered when they're in traditional clothing.

% % Normalizer === [0.1966]

% % Generated: Yes , the heads of Iranian women are covered in traditional clothing .
% % \end{framed}



% % \subsection{Error Analysis}


% \begin{table}[!h]
%     \centering
%     \caption{Cross-Category Accuracy by Query Complexity and Source Image Count.}
%     \label{tab:query_complexity}
%     \begin{tabular}{cccc}
%     \toprule
%          & Query & One Image & Two Images \\
%          \midrule
%         VLP & Original & 0.400 & 0.418 \\
%         GPT-3.5 & Original & 0.411 & 0.452 \\
%         % GIT & Original & 0.175 & 0.183 \\
%         GIT & Simple & 0.429 & 0.345 \\
%         BLIP-2 & Original & 0.371 & 0.440\\
%         % BLIP-2 & Simple &  0.365 & 0.352 \\
%         % BLIP-2 + GPT-3.5 & original & \\
%         \bottomrule
%     \end{tabular}
% \end{table}

% \begin{table}[!h]
%     \centering
%     \caption{Baseline Results}
%     \label{tab:baselines}
%     \begin{tabular}{cccccc}
%     \toprule
%          Baseline & Finetuning & Caption & Acc & Fl & \\
%          \midrule
%          VLP & VQA, WebQA & Yes & 0.496 & 0.476 &  \\
%          GPT-3.5 & N/A & No & 0.526 & 0.471 &  \\
%          % GIT & Original & No & 0.222 & 0.025 &  \\
%          GIT & VQA-2 & No & 0.418 & 0.186 & \\
%          % BLIP-2 & Original & No & 0.400 & 0.126 & \\
%          BLIP-2 & Original & Yes & 0.399 & 0.190 & \\
%          % BLIP & Simple & No & & & \\
%          % BLIP-2 & Simple & Yes & 0.357 & 0.133 & \\
%          % BLIP-2 + GPT-3.5 & Original & No & & & \\
%          % BLIP-GPT-3.5 & Original & Yes & & & \\
%          % BLIP-GPT-3.5 & Simple & No & & & \\
%          % BLIP-GPT-3.5 & Simple & Yes & & & \\
%          \bottomrule
%     \end{tabular}
% \end{table}