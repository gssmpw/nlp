As WebQA is a joint retrieval and QA task, we develop several QA methods and retrieval methods separately. For the final end-to-end task, we evaluate the combination of the each retrieval method with the best-performing QA model, as determined during an initial QA model selection phase. This enables us to investigate the factors involved in both the model design and the model finetuning process that may influence how the parametric effect manifests itself.


% \subsection{Multihop Multimodal Formulation}
\subsection{Question Answering}

\paragraph{Vision-Language Model}
For two-image questions, the WebQA finetuned VLP baseline \citep{zhou_unified_2020} takes as input the concatenation of both sources encodings with the query; 
\begin{equation}
<[CLS], s_1, s_2, [SEP], Q, [SEP]>
\end{equation}

As such, it is an extension of VQA model trained on single-hop VQA-2 \citep{yu_unified_2023}, which takes as input:
\begin{equation}
<[CLS], s, [SEP], Q, [SEP]>
\end{equation}
We adopt this formulation for finetuning the Qwen2 VLM, using Low-Rank Adaptation (LoRA) to reduce trainable parameters \cite{hu2021lora}. We use the same input formulation to evaluate zero-shot performance on GPT-4o. In addition, we evaluate several baseline models from previous works, namely VLP \citep{zhou_unified_2020}, GIT \citep{wang2022git}, GPT-3.5 \citep{brown2020language}, and BLIP-2 \citep{li2022blip}. Details of these models are presented in appendix \autoref{sec:baselines}.

% \subsection{Experiments}
% \paragraph{Baseline QA Models}
% % Additionally, for each baseline we investigate performance on two variants of the task; with or without captions, and with a simplified query or with the original query. 

\paragraph{Multihop Formulation}
We hypothesize that multihop tasks, such as WebQA, would benefit from a two-stage reasoning process. The first stage enables multimodal fusion between each input source and the question, and the second stage enables multihop fusion between the embedded multimodal representation of each source, conditioned on the question. Inspired by FiD architectures \citep{yu_kg-fid_2022}, this results in the following input construction:
% this  encoding images with the question so that query-conditioned reasoning is carried out for individual images before a joint reasoning task, 
\vspace{-2mm}
\begin{equation}
\begin{aligned}
    concat(<[CLS], s_1, [SEP], Q>,\\
    <[CLS], s_2, [SEP], Q>)
    \vspace{-1.5mm}
\end{aligned}
\end{equation}


\paragraph{Multihop Classifier}
\label{sec:mh-volta}
We select the VoLTA framework as the skeleton for encoding joint text and image representations \cite{pramanick_volta_2023}. VoLTA uses Swin-Base \cite{liu2021swin} and RoBERTa-Base \cite{liu2019roberta} as respective visual and textual encoders and we adopt the same encoder choices. Swin Transformer introduced a hierarchical Transformer design with a shifted windows scheme, which computes the self-attention layer over non-overlapping local windows and allows connection by shifting between windows. We jointly encode each image source returned by the retriever with the query and concatenate the resulting embeddings together before sending them to the MLP classifier to predict the keyword answer label. To handle variable input sequences during classification, we pad single image sources with blank images so that all inputs sent into the classifiers have two images. We call this model MultiHop-VoLTA (MH-VoLTA).

We finetune the models using the AdamW \cite{loshchilov2019decoupled} optimizer with a learning rate of $1e^{-4}$ and a batch size of 32 samples. We use LoRA to reduce trainable parameters \cite{hu2021lora}, and set $r=8$ and $\alpha=32$ for the text encoder and $r=16$ and $\alpha=16$ for the image encoder, updating only the attention weights. MH-VoLTA is trained until convergence (~80 epochs, see \autoref{fig:loss_convergence} in the appendix). 

% Specifically, we borrowed the `Fusion-in-Decoder' architecture for questions with two positive image sources. The two-stage Multihop VoLTA architecture (MH-VoLTA) where VoLTA's `fusion-in-backbone' encoders enable multimodal fusion enables multi-hop fusion between image sources.

\subsection{Retrieval Methods}
\label{sec:retrieval_models}
\paragraph{Dense Retrievers} We adopt the pretrained UniVL-DR retriever for source retrieval in our finetuned experiments \citep{liu_universal_2023} and compare it with baseline CLIP \citep{radford2021learning} and WebQA finetuned CLIP (CLIP-DPR, \cite{liu_universal_2023}) embeddings. Specifically, we embed all text sources, image sources, and queries using UniVL-DR. For each query, we compute cosine similarity between the query and each of the sources, and use the top two ranked image sources and their captions as input to the QA model.
% as described in \autoref{fig:poster_architecture}.

\paragraph{GPT-4o Ranking} 
We utilize GPT-4o to select sources from the set of distractor sources present in dataset using the prompt in the appendix \autoref{frame:labeling_prompt}. This is motivated by previous work in In-Context Retrieval Augmented Language Modeling (In-Context RALM) which demonstrated that LLMs are capable of reasoning over sources without finetuning \citep{incontext_rag}.

\paragraph{Upper and Lower Bounds}
In addition, to investigate the impact of the parametric effect on joint retrieval and QA performance, we also compare performance with a best and worst case retriever. The best case is the oracle retriever, using gold sources provided in the validation set, and the worst case is a random naive retriever, which returns random distractor sources (and so is always incorrect).



% RoBERTa, known as a robustly optimized BERT, improved over BERT with a few training techniques such as dynamic masking, and explored various NSP training strategies.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figures/architectures/poster_big.png}
%     \caption{MH-VoLTA: Multihop multimodal VQA model based on VoLTA \citep{pramanick_volta_2023} encoders}
%     \label{fig:poster_architecture}
% \end{figure}


% \paragraph{Learning}
% Adopting the encoder choices as VoLTA, 

% Given pretrained weights $ W_{0} \in \mathcal{R}^{d \times k}$, the parameter update $\Delta W$ is represented with a low rank decomposition:
% \vspace{-2mm}
% \begin{equation}
%     W_{0} + \Delta W = W_{0} + \frac{\alpha}{r}BA 
%     \vspace{-1.5mm}
% \end{equation}
% where
% $ B \in \mathcal{R}^{d \times r}, A \in \mathcal{R}^{r \times k}, r \ll min(d, k)$.
% During fine-tuning, $W_{0}$ is kept frozen, and only parameters within A and B are trained, reducing the number of parameters required. 

 


% \subsection{Joint Retrieval and Question Answering}

% and so we compare performance of our finetuned classifier against GPT-4o when supplied with relevant sources as retrieved by UniVL-DR. 

% \paragraph{In-Context RLM} Finally, we hypothesize that In-Context Retrieval Augmented Language Modeling can be extended to what we term In-Context Retrieval (ICR). Here, we test GPT-4o's ability to select relevant sources from a set of distractor sources. We present the extended list of sources to GPT-4o, \textit{without using a retrieval module}, and evaluate its performance on the retrieval task. The prompt used in this case is included in appendix .

% \subsection{Joint Classifier}
% 
% Following similar scheme, we tried 
% Fusion-in-classifier that works with one or two image sources
% We pad the encoder output for one image.
% We also train a joint classifier that would work with a variable number of image sources. Given the model design described in the separate classifier section,  
% During training, only the MLP parameters are kept trainable. 
%We believe this could potentially facilitate the model's multihop reasoning capacity, given that it would force the model to make predictions based on all input pairs. We also balanced the finetuning dataset to ensure that the model is not overfitting a particular input scheme. 



% \paragraph{Learning}

% \subsection{Exploratory Data Analysis}
% We conduct exploratory data analysis to motivate our approach. Namely, we identify several failure cases where baseline models tend to perform poorly.

% % \textbf{Task Difficulty} - 
% % Due to the dataset's curation process and the framing of questions for multi-hop scenarios, a significant challenge arises: some questions, even with the inclusion of positive sources (as depicted in appendix \autoref{fig:example_1}), are improbable to be answered by both humans and AI models. In addition, the perspective and orientation of some source images means that the framing of certain questions is hard to understand. One question follows, "Which statue has a longer robe: Skopas' statue or the statue to the left of Skopas?"; however, Skopas statue is shown on the left-hand side of the image. 

% \textbf{Category Domain Size} -
% In the presented example, as referenced in appendix \autoref{fig:example_1}, the question falls into the 'choose' category, where the model's performance tends to be stronger given the inherently limited set of possible answers. This improved performance is also evident in 'YesNo' question categories. However, for question categories such as 'Others', 'number', 'shape', and 'color', the model's performance notably declines (refer appendix \autoref{appendix:baseline_results}). 

% % \textbf{Evaluation} - 
% % As demonstrated in appendix \autoref{fig:example_2}, there are instances where the model correctly outputs the answer, yet it fails to achieve a perfect score on the evaluation metrics for that question. This is frequenctly the case for the 'YesNo' category; if the generated response includes 'There are', instead of 'Yes,', it is marked as incorrect. Conversely, if the expected answer has 'There are' phrasing, and no 'YesNo' keyword (appendix \autoref{fig:example_3}), the generated response will always be marked as correct.

% % \textbf{Closed Set Domain} -
% % The tasks include question categories such as 'color,' 'shape,' and 'YesNo,' for which a predefined and finite set of potential entities has been established. In these cases, the options available for the model to choose from are restricted to a limited and well-defined set of possibilities. This closed set of answers is also employed in the lexical-aware evaluation of the model's predictions. Consequently, if an answer contains a synonym for the expected keyword in the domain, it might be incorrectly marked as inaccurate (i.e. once instead of one, or twice instead of two).

% \textbf{Classifier vs Decoder} Treating WebQA as a classification task where we first extract domain-based keywords for the training set should mitigate these issues. It reduces the task difficulty such that we can focus on accuracy as our metric. Furthermore, taking a subset of the WebQA dataset queries that have closed-set answers allows us to resolve domain size and keyword synonym issues before training.