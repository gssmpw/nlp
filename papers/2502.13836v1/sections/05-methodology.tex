We propose measures for evaluating the degree of memorization in QA models (\ppr) and in end-to-end retrieval-QA systems (\ucr), as well as a metric for retriever-QA model compatibility (\rpa).

\label{sec:measures}
\subsection{\ucr}
% alternative name: Unsupported Correctness Rate (UCR)
We propose \UCR, a metric to measure the parametric effect in the combined retrieval and QA model. It is formulated as a composition of QA accuracy and retrieval recall. Intuitively, it is the fraction of true positive predictions from the QA model for which there is no retrieval support (i.e. the retrieved sources were incorrect).

\paragraph{Retrieval Recall and QA Accuracy}
The first stage of the joint task is retrieval, where the recall for retriever R is defined as the fraction of retrieved sources (positives) that are correct (true positives) with respect to task labels;
\begin{equation}
    \text{Recall}_R = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
\end{equation}

Accuracy is the primary correctness metric for question answering in the WebQA task. Accuracy of model M is determined by comparing a restricted bag of words (bow) vector between the expected (E) and generated (G) answers;
\begin{equation}
    \label{eq:ACC}
    \text{Acc}_M = \frac{1}{n}\Sigma [\frac{|\text{bow}_{E} \cap \text{bow}_{G}|}{|\text{bow}_{E}|} == 1]
\end{equation}

The vocabulary of the vectors is restricted to a specific domain based on the question type; questions are labeled based on these domains which can be yes/no, color, shape, or number. Each category has a pre-defined vocabulary list, given in the \hyperref[sec:categories]{appendix}.


% \paragraph{Fluency}
% WebQA uses "fluency" as one of the question-answering metrics. This fluency metric is assessed using BARTScore \cite{yuan2021bartscore}, a natural language generation (NLG) evaluation measure that focuses on evaluating the quality and coherence of generated text. Note, that for the finetuned model which predicts keyword answers, we ignore fluency and focus on accuracy. As such, accuracy is the main metric of interest, and fluency is given wherever generative methods are used.

% For a candidate $c$, and a given reference $r \in R$, the fluency is defined as - 

% $FL(c, R) = \max\left\{\min\left(1, \frac{BARTScore(r,c)}{BARTScore(r,r)}\right)\right\}_{r \in R}$

% \begin{equation}
% FL(c, R) = \max_{r \in R} \left\{ \min \left( 1, \frac{\text{BARTScore}(r,c)}{\text{BARTScore}(r,r)} \right) \right\}
% \end{equation}

\paragraph{\UCR}
Using QA accuracy and retrieval recall, we construct \UCR, a metric for measuring the parametric effect in a combined retrieval-QA model, which calculates the likelihood $P(Q_1|R_0)$ that the QA model M returns a correct answer ($Q_1$) given that the retrieval model R failed to return the correct sources ($R_0$):
\begin{equation}
\label{eq:UCR}
\begin{aligned}
\text{UCR(R,M)} &= \frac{\text{Acc}_M == 1 \cap   \text{Recall}_R = 0}{\text{Recall}_R = 0}\\
&= P(Q_1|R_0) 
\end{aligned}
\end{equation}

\subsection{Oracle-Normalized Retrieval Scores}
Using min-max scaling, we define two additional metrics to evaluate joint retrieval QA systems, by normalizing using the oracle retriever (upper bound) and random retriever (lower bound):
\begin{equation}
\hat{X} = \frac{X - X_{min}}{X_{max} - X_{min}}
\end{equation}
These metrics are \rpa (\RPA, higher is better) and \ppr (\PPR, lower is better).

\paragraph{\rpa}
\RPA quantifies the potential that a retriever has realized when used in a given end-to-end retrieval QA system. The upper bound (1) is given by same QA system's accuracy with oracle sources ($\text{Acc}_M(\text{oracle})$). The lower bound (0) is given by the random negative source retriever ($\text{Acc}_M(\text{random})$), which always retrieves incorrect sources. We apply random-oracle scaling, where $Acc_M(R)$ denotes the accuracy of QA model M, given sources from retriever R: 
\begin{equation}
\label{eq:RPA}
\text{\RPA}(\text{R,M}) = \frac{\text{Acc}_M(R) - \text{Acc}_M(\text{random})}{\text{Acc}_M(\text{oracle}) - \text{Acc}_M(\text{random})}
\end{equation}

\paragraph{\ppr}
We postulate that the rate at which a model's performance increases when used in conjunction with increasingly accurate retrievers implies that it is using the retrieved sources effectively, instead of relying on parametric memory. To that end, we present \PPR, an additional max scaling measure based off just the upper (oracle) and lower bound (randomized \textit{negatives}) retrievers, which is simply their performance ratio with respect to QA model M:
\begin{equation}
\label{eq:PPR}
\text{\PPR}(\text{M}) = \frac{\text{Acc}_M(\text{random})}{\text{Acc}_M(\text{oracle})}
\end{equation}


\paragraph{Question Complexity Analysis Metrics}
To identify correlations between the complexity of the question, retrieval recall, and QA accuracy, we apply three separate measures to the input questions; Word Count, Flesch-Kincaid Grade Level \citep{flesch2007flesch}, and Gunning-Fog Index \citep{gunning1952technique}(\autoref{appendix:complexity_analysis_metrics}).

% Notably, for question categories such as 'color,' 'shape,' 'number,' and 'YesNo,' we calculate accuracy as the average of the F1 score, while for other question categories, we derive accuracy from the average of the recall score (following from the official WebQA evaluation script).
