
The increasing reliance on LLMs for multimodal tasks across far-reaching sectors such as healthcare, finance, and manufacturing underscores the need to assess the accuracy and reliability of the information they generate. Vision-Language Models (VLM) have achieved state-of-the-art (SoTA) performance on Visual Question-Answering (VQA) benchmarks, and these models often utilize Retrieval-Augmented Generation (RAG) to maintain factual accuracy and relevance in a dynamic information environment. However, this has led to uncertainty in the information the LLM bases its answer on, as it may choose between parametric memory and retrieved sources. When models rely on memorized information instead of dynamically retrieving information, they may inadvertently propagate outdated or incorrect information, causing serious legal and ethical risks and undermining trust and reliability in AI systems \citep{huang2023survey}.
% The ability to strike a balance between generalization and specialization in AI systems is therefore crucial for ensuring the safe, reliable use of these technologies in real-world applications.

Despite these concerns, the way that Vision-Language models (VLMs) memorize and retrieve information, particularly in complex multimodal tasks, remains under-explored. Current research often focuses on either the general capabilities of large language models (LLMs) or the specialized retrieval mechanisms in retrieval augmented generation systems (RAG) \citep{incontext_rag,chen_murag_2022,liu_universal_2023}. Particularly in the context of multimodal retrieval and multihop reasoning, few studies analyze the tradeoff between finetuning for specialized tasks and zero-shot prompting for general-purpose vision-language capabilities. A lack of consensus on how to approach this tradeoff motivates the development of measures to quantify reliance on parametric memory, as well as metrics for quantifying the potential performance impact of extending LLMs with RAG systems.

To address this gap, we investigate how multimodal QA models balance accuracy with memorization on the WebQA benchmark. We compare finetuned multimodal systems against zero-shot VLMs, analyzing how retrieval performance influences QA accuracy. In particular, we focus on cases where retrieval fails, allowing us to measure reliance on parametric memory through two proposed metrics---the \ppr (\PPR) which quantifies how much model accuracy is influenced by retrieval quality, contrasting performance in best-case versus worst-case retrieval scenarios, and the \ucr (\UCR) which measures how often correct QA responses are generated when the retriever fails, providing a proxy for memorization.

To enable this analysis, we make several methodological contributions. For the finetuned QA models, we investigate Vision-Transformer (ViT) architectures, which allow for multihop reasoning over multiple sources. To investigate the impact of retrieval performance on trained LMs, we propose a variable-input Fusion-in-Decoder (FiD) model \cite{tanaka_slidevqa_2023, nlvr2}, building upon the VoLTA architecture \citep{pramanick_volta_2023}. For the zero-shot case, we build upon previous research on In-Context Retrieval \citep{incontext_rag} by demonstrating that LLMs such as GPT-4o are capable of performing the final ranking step of the retrieval process. In doing so, we find that GPT-4o, a general-purpose LLM, achieves SoTA performance on the WebQA task, outperforming existing finetuned RAG models by a significant margin (7\% higher accuracy). 

Crucially, our results reveal that while retrieval-augmented models reduce memorization, the training paradigm plays an important role. Finetuned models exhibit higher reliance on parametric memory, whereas zero-shot RAG approaches have lower memorization scores at the cost of accuracy. This suggests that while retrieval modules may mitigate the risks associated with outdated or incorrect information, SoTA performance requires that they be coupled with specialized QA models. Our memorization measures contribute to the development of transparent and reliable AI systems, particularly in applications where the sourcing of up-to-date, factual information is critical.



% We investigate the impact of question complexity on the ability of these models to integrate multiple data sources—such as images, text, and external retrievers—and produce coherent and accurate answers. We also explore whether in-context retrieval can be a viable alternative to traditional retrieval-augmented systems, offering a more streamlined approach to multimodal QA.

% To achieve this, we first compare zero-shot prompting multimodal LLMs with finetuned multimodal systems. We evaluate both types of models on the WebQA benchmark, a dataset designed for complex question answering that requires reasoning across both image and text sources. For the finetuned models, we use a Fusion-in-Decoder (FiD) architecture, which allows for multihop reasoning over multiple sources. Additionally, we introduce the concept of In-Context Retrieval Language Modeling (RLM), where the LLM itself performs retrieval tasks without the need for external retrievers. This method builds upon existing research in in-context learning  and aims to explore the viability of LLMs retrieving relevant sources and generating accurate answers directly from their context window.

% In order to investigate source utilization in finetuned multimodal models and LLMs, three lines of inquiry are established; 
% \begin{itemize}
%     \item Study 1: retrieval vs QA performance on webQA (motivating example, does QA answer correctly even with incorrect sources?)
%     \item Study 2: performance on adversarial examples where parametric knowledge would be incorrect by design
%     \item Study 3: improving performance on adversarial examples by fine-tuning (i.e model robustness)
% \end{itemize}

% Note, there is one weakness in this plan which is tying in the work we've already done. 
% If we added something from adversarial generation to the retrieval experiment (like a combination of study 1 + 3) it would be complete. So for instance we could try fine-tuning the retriever with adversarial examples (and not just the QA model)

% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/segmentation/webqa_segment_infill.png}
%     \caption{Example of the segmentation substitution pipeline from the WebQA task.}
%     % d5c76d760dba11ecb1e81171463288e9
%     \label{fig:seg_sub_pipeline}
% \end{figure}



% Retrieval augmented generation (RAG) with zero-shot prompting and fine-tuning Large Language Models (LLMs) have become the go-to methods for tasks relying on information retrieval and text generation. In many cases the LLMs parametric memory can sufficiently generalize to answer questions without being provided with retrieval mechanisms for out-of-domain knowledge. However, LLMs often hallucinate and provide wrong information in certain scenarios. This problem is amplified even further on open-domain Question Answering (QA) tasks involving multiple modalities. Grounded text generation using retrieved sources \citep{lewis2021retrievalaugmented} has been extensively studied for text-to-text QA tasks, but its application in multimodal settings has not been studied as much.


% Multimodal reasoning and question answering have gained prominence in recent research endeavors, with an increasing emphasis on handling various forms of data, particularly text and images. In this study, we address a specific gap in the existing literature by focusing on the development of a versatile multihop model capable of accommodating varying numbers of input images.

% Our motivation for this research lies in the growing complexity of answering questions using information on the web, where the challenge of navigating the open-domain setting is further complicated by the presence of multiple modalities and sometimes requires reasoning over multiple sources. WebQA is an ideal dataset on which to compare performance of finetuned RAG systems against general purpose LLMs; it is multimodal, with correct answers requiring reasoning over image and text sources. It is multihop, requiring a complex reasoning process over multiple sources. Finally, WebQA questions from different categories can be broken down into subdomains to analyze performance over domains of varying cardinality.

% Motivated by the real-world challenges of building retrieval and question answering (QA) systems, we design and finetune a closed domain, multimodal, multihop QA model, that is capable of reasoning over a varying number of sources taken as input from an external retriever module. This research contributes to the relatively underexplored domain of multihop reasoning across various input sources and modalities. Our goal is to explore the challenges posed by these scenarios and develop strategies that enable QA models to retrieve relevant information, conduct logical or numerical reasoning across diverse modalities, and generate coherent responses in natural language. To our knowledge, this is the first application of the Fusion-in-Decoder (FiD) architecture \cite{tanaka_slidevqa_2023, nlvr2} that is shown to work with a variable number of inputs, enabling multi-hop reasoning over sources.

% In-Context Learning refers to the ability of LLMs to perform any task by simply providing examples in the input prompt \citep{dong2022survey,min2022rethinking}. Inspired by this research, we propose a method to use the LLM itself as a multimodal retriever, potentially eschewing the requirement of a distinct retrieval module, thereby allowing the design of simpler retrieval-augmented QA systems. We dub this method In-Context Retrieval Language Modeling (RLM). To the best of the authors knowledge, In-Content RLM is disparate from other retrieval augmented approaches which utilize external retrieval modules \citep{incontext_rag,chen_murag_2022,liu_universal_2023}. Despite being a natural extension of In-Context learning, In-Context RLM has not yet been studied empirically.

% To expand on our contribution of In-Context Retrieval, this stems from the well-researched in-context learning of LLMs. In-context learning is the ability of a model to perform any task given a sufficient context window \citep{dong2022survey,min2022rethinking}. Such tasks could include retrieval and ranking, but typically, the go-to solution for tasks requiring retrieval has been RAG. To the best of the authors knowledge, In-Context Retrieval is distinct from In-Context Retrieval Augmented Language Modelling (RALM), and despite being a natural extension of In-Context learning, In-Context Retrieval has not yet been shown empirically.

% Finally, we explore the tradeoff between using zero-shot prompting LLMs and the fine-tuning approach. While we find that, overall, GPT-4o obtains SoTA performance on the WebQA task, outperforming the accuracy of existing finetuned RAG approaches by 7\%, finetuned approaches still perform better on more restricted subdomains\footnote{``In-Context RLM" @ \url{https://eval.ai/web/challenges/challenge-page/1255/leaderboard/3168}}. Finally, we validate that GPT-4o is relying on retrieval abilities to solve the task; we find that GPT-4o is capable of retrieving relevant sources in the presence of distractors and furthermore, when GPT-4o fails to retrieve correct sources, it answers incorrectly 75\% of the time, meaning that it is not relying on parametric memory for this task.

% \paragraph{Contributions}
% Based on our experimentation and analysis on the WebQA benchmark, we make the following contributions:
% \begin{itemize}
%     \item Propose a new architecture for multimodal multihop QA that takes variable number of input sources inspired by the Fusion-in-Decoder method.
%     \item Comparison of general purpose LLMs vs specialized models on the WebQA benchmark.
%     \item Observation of In-Context Multimodal Retrieval abilities of GPT-4o and that it does not rely on parametric memory for multimodal QA.
%     \item Analysis of relationship between retrieval and QA task performance.
%     \item Analysis of task and query complexity on the performance of retrieval and QA tasks.
% \end{itemize}
















% Throughout this paper, we will present our methodology, experiments, and findings, emphasizing our approach to multihop reasoning over varying numbers of input images. We believe that our work contributes to a deeper understanding of multimodal reasoning and has the potential to enhance the capabilities of question-answering systems in the intricate, multimodal landscape of web-based information.