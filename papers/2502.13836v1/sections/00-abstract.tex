
% v3
Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72\% vs 52\% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks.
% while general-purpose models can adapt flexibly to new queries, finetuned VQA models show stronger performance in specialized domainsâ€”potentially at the cost of increased memorization. 
% Finally, we discuss the implications of our findings for mitigating memorization risks in retrieval-augmented VQA and ensuring responsible deployment of multimodal AI systems...

% V2 
% While general-purpose Large Language Models (LLMs) like ChatGPT excel at performing various tasks across different domains, specialized models have proven to outperform their general-purpose counterparts on domain-specific tasks. Recent advances in Retrieval Augmented Generation adapt general-purpose LLMs to specialized domains with prompting and domain-specific fine-tuning. However, for tasks like multimodal question answering (QA), factors that influence one's decision in favoring one approach over the other are underexplored. In this work, we analyze the performance of architectures finetuned for multihop multimodal retrieval and question answering on the WebQA benchmark. We compare these to the performance of zero-shot prompting multimodal LLMs, and find that these general purpose LLMs are surprisingly capable of performing both multimodal retrieval and QA. Moreover, we demonstrate that GPT-4o achieves SoTA QA accuracy on WebQA, and is capable of In-Context Retrieval. At the same time, we show that our finetuned retrieval and QA models outperform general purpose LLMs in closed domain settings. Finally, we analyze the dependency between retrieval and QA performance. We study the effects of task complexity on model performance and by contrasting zero-shot prompting LLMs with finetuning models. This analysis underscores the overall finding of our work---while zero-shot prompting LLMs exhibit remarkable capabilities in multimodal retrieval and QA, finetuned models outperform them for closed domains and high-complexity tasks, emphasizing the need for careful task and domain assessment in model selection.

% In particular, much work has gone into developing retrieval augmented language models, assuming that the task of retrieval is better suited to well established finetuned models and ranking systems. However, there exists a gap in understanding on exactly what tasks and domains we should favor finetuned approaches over zeroshot approaches. 

% This work has implications for the 


% V1
% While it has been shown that LLMs can learn when to fall-back to retrieval modules instead of relying on parametric memory, and in doing so improve their performance on open-domain tasks, little is known about how and why LLMs without this capability should perform better on some domains, and worse on others, in comparison to retrieval augmented generation. In this work, we seek to explore this question. We use the WebQA dataset which is designed to bridge the gap between closed and open-domain tasks, to compare model performance of zero-shot LLMs and finetuned QA models. We observe that as the domain size increases (i.e. as we transition from closed to open domain), the performance of finetuned QA models deteriorates and the performance of zero-shot LLMs improves. Furthermore, we find that in the zero shot setting, GPT-4o has multimodal retrieval capabilities and can select relevant sources from a provided list to answer given questions without relying on external RAG systems. We further demonstrate that the performance on the WebQA task improves due to this capability when compared to just using the LLMs parametric memory. Finally, we show that although finetuned methods outperform LLMs on subsets of the WebQA task (where domain is restricted), GPT-4o achieves SoTA accuracy on the full WebQA task and outperforms finetuned methods.


% Comparison of zero-shot and finetuned models on a multimodal retrieval and QA task. We focus on multihop QA, as it requires reasoning over multiple sources, making the task of retrieval more important, and we evaluate our models on the WebQA benchmark.




% Multimodal reasoning and question answering have gained prominence in recent research, with a growing emphasis on handling diverse data types, including text and images. In this study, we address a critical gap in the existing literature by focusing on the development of a versatile multihop model capable of accommodating varying numbers of input images. Our motivation is rooted in the complex information landscape of the web, where diverse modalities are encountered, and our goal is to design a model that seamlessly integrates text and image modalities while demonstrating adaptability to different quantities of input images.

% We conducted extensive experiments on the WebQA dataset, which contains questions with variable numbers of source images (ranging from 0 to 2). We finetune and develop the VoLTA model, which is pretrained with the Graph Optimal Transport objective, which aids in the alignment of named entities in queries with the subjects of image sources in the WebQA task. Our findings reveal that the Fusion-in-Decoder architecture performs impressively across these questions, showcasing its robustness and adaptability. Future work can focus on applying these insights to other multimodal multihop tasks and datasets.