% \autoref{fig:text_bias_example} and \autoref{fig:query_shortening_examples} give examples of incorrect response from \citet{chang_webqa_2021} along with rationale for the mistakes.

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=18cm]{figures/text_bias_example.png}
%     \caption{Examples of an incorrect VQA response from the baseline \citep{chang_webqa_2021} where the image source is not used, and instead the most likely answer based on word co-occurance is given.}
%     \label{fig:text_bias_example}
% \end{figure*}


% \begin{figure*}[!h]
%     \centering
%     \includegraphics{figures/query_shortening_examples.png}
%     \caption{Examples of incorrect VQA responses from the baseline \citep{chang_webqa_2021} where the query could be shortened post-retrieval to remove confounding terms that are only useful for retrieval, such as specific nouns.}
%     \label{fig:query_shortening_examples}
% \end{figure*}

\subsection{Model Selection Results}
\label{sec:model_selection}
We explore baseline methods for the QA task on the WebQA validation set. \autoref{tab:baselines} gives results for the baseline models. The MH-VoLTA model outperforms all baseline and zero-shot models on the validation set image questions. However, the extension of the VoLTA model for variable input multi-hop tasks risks a regression in performance on traditional VQA tasks which have fixed-input where the number of input images is constant. To determine MH-VoLTA generalizes from fixed to variable input tasks, we compare performance between two variants of the original VoLTA model, finetuned on one and two image subsets of WebQA, with MH-VoLTA. We find that MH-VoLTA is capable of reasoning over both one and two-image image questions, and it's performance is on-par with VoLTA variants trained on one and two image sources separately\autoref{tab:baselines}. See \autoref{sec:one_vs_two_image_volta} for more details on the one and two image VoLTA variants, as well as a breakdown of model performance by question category (\autoref{fig:multihop_volta_res}). See \autoref{sec:baselines} for a description of the baseline models used.

% GPT-4o and GPT-3.5 models outperform both VLP and GIT baselines. This is particularly impressive for GPT-3.5, which as a unimodal model can only take image caption as input. Despite having mulitmodal inputs, BLIP-2 underperforms GPT-3.5 which we attribute to the difference in decoder quality. We adopt the best performing general purpose LLM (GPT-4o) and specialized VQA model (MH-VoLTA) for all futher experiments.


% Given our focus on question categories that can be addressed using images, \autoref{tab:baselines} splits the baseline results by performance into questions that require one and two image sources respectively. 

\paragraph{VQAv2 and NLVR2}
\label{sec:vqav2}
In addition to WebQA, we evaluate models on two fixed-input VQA datasets---VQAv2 \citep{goyal2017making}, a multi-class, single-image VQA dataset, and \citep{nlvr2}, a binary classification, two-image VQA dataset. These datasets are well-suited to VoLTA classifier architecture. In particular, question categories in VQAv2, along with the associated answer-domains, match well with WebQA, with a substantial portion of both datasets focusing on color, shape, number, and yes/no questions.

\subsection{Multihop VoLTA on one vs two image sources}
\label{sec:one_vs_two_image_volta}
The results for finetuning VoLTA and MH-VoLTA on the WebQA dataset experiments are provided in \autoref{tab:multihop_volta_res}. We explored the application of Multihop-VoLTA in addressing queries based on single images, questions involving two images, and a combination of both single and two-image queries (referred to as multiple images, \autoref{fig:model_perf_qcate}). 

We find that the variable Multihop-VoLTA model (\autoref{fig:multihop_volta_res}) is en-par with the fixed-input one and two-image VoLTA model variants (\autoref{fig:model_perf_qcate}). This underscores the stability of our finetuning approach for MH-VoLTA across both training paradigms. The MH-VoLTA models have on the order of 100M parameters, of which 10M are trainable after applying LoRA. All models are trained for 80 epochs on a Nvidia A6000. 


% We find that the distribution of question types is very different between questions with one and two image sources. Questions with two image sources have more questions in categories with higher random accuracy like Choose and YesNo, whereas questions with one image source have more questions in the harder categories like shape, color, and number, where random accuracy is much lower given the larger answer domain.


% \begin{table}[!h]
%     \centering
%     \begin{tabular}{cccc}
%     \toprule
%          &Accuracy& Fluency  &GPT-Fluency\\
%          \midrule
%          Single Image&0.764&   0.003&\\
%          Two Images&0.851&   0.002&\\
%          Multiple Images&0.799&   0.002&\\
%          \bottomrule
%     \end{tabular}
%     \caption{Multihop VoLTA Results}
%     \label{tab:multihop_volta_res}
% \end{table}


\begin{table}
    \centering
    \caption{Model selection results on WebQA validation set (further broken into 1 and 2 image input categories), and the VQAv2 and NLVR2 (NLV) test sets. MH-V denotes MH-VoLTA. See \autoref{sec:baselines} for model descriptions.}% including Cross-Category accuracy by source image count.}
    \label{tab:baselines}
    \begin{tabular}{clllll}
    \toprule
     & \multicolumn{3}{c}{\makecell{WebQA Acc}} & VQA & NLV \\
        \midrule
         Model & All & 1 img & 2 img & Acc & Acc \\
         \midrule
         \hyperref[sec:mh-volta]{MH-VoL} & \textbf{0.71} & 0.72 & 0.70 & 73.9 & 76.5 \\
         VoLTA\textsubscript{1}  & -- & \textbf{0.77} & -- & \textbf{74.6} & -- \\
         VoLTA\textsubscript{2}  & -- & -- & \textbf{0.84} & -- & \textbf{76.7} \\
         \midrule
         GPT-4o & 0.56 & \textbf{0.58} & 0.69 &  -- & --\\ % 0.77 &
         \hyperref[sec:gpt3.5]{GPT-3.5} & 0.53 &  0.41 & 0.45 & -- & --\\ % 0.47 &
         \hyperref[sec:VLP]{VLP} & 0.50 & 0.40 & 0.42 & -- & -- \\ % fl 0.48 & 
         \hyperref[sec:GIT]{GIT}  & 0.42 & 0.43 & 0.35 & -- & --\\ % 0.19 & 
         \hyperref[sec:blip]{BLIP-2} & 0.40 &  0.37 & 0.44 & -- & --\\ % 0.20 &
         % GIT & Original & No & 0.222 & 0.025 &  \\
         % BLIP-2 & Original & No & 0.400 & 0.126 & \\
         % BLIP & Simple & No & & & \\
         % BLIP-2 & Simple & Yes & 0.357 & 0.133 & \\
         % BLIP-2 + GPT-3.5 & Original & No & & & \\
         % BLIP-GPT-3.5 & Original & Yes & & & \\
         % BLIP-GPT-3.5 & Simple & No & & & \\
         % BLIP-GPT-3.5 & Simple & Yes & & & \\
         \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{MH-VoLTA results and dataset breakdown}
    \label{tab:multihop_volta_res}
    \begin{tabular}{ccc}
    \toprule
         & No. of Samples&Accuracy\\
         \midrule
         Single Image& 760&0.764\\
         Two Images& 576&0.851\\
         Multiple Images& 1336&0.799\\
         \bottomrule
    \end{tabular}
\end{table}

% The results presented in \autoref{fig:model_perf_qcate} demonstrate the robustness of our model when trained independently for single-image and two-image questions. \autoref{fig:gpt_vs_volta_by_category} illustrates a comparable graph for joint finetuning of VoLTA on both single-image and two-image sources. Interestingly, there is virtually no distinction in accuracy, even up to the second decimal place, between individual and joint training. 

% Nevertheless, a notable decline is observed in the 'shape' question category when compared to both single-image and two-image questions. This decrease can be attributed to the model predominantly predicting the 'h' shape for the majority of two-image questions. While this behavior is consistent with single-image questions, it is particularly pronounced in the 'shape' category, where there are 65 questions with a single positive image source and only 8 questions with two positive image sources. This disparity in the distribution of questions with varying positive image sources contributes to the observed performance difference in the 'shape' category.

% The 'YesNo' question category exhibits the highest performance compared to other question categories, which aligns with expectations given its binary classification nature. However, our model shows a slight susceptibility to frequency bias, indicating a tendency to be influenced by the prevalence of certain classes in the training data. Addressing this bias is important for achieving a more balanced and accurate predictive performance across all question categories.


% \begin{figure*}
%     \centering
%     \subfigure[Performance of the MH-VoLTA classifier by question category and image count.]{%
%         \includegraphics[width=0.3\textwidth, trim={0 0 0 3cm},clip]{figures/results/model_perf_acc_comparison_per_qtype_joint.png}
%         \label{fig:multihop_volta_res} 
%         % model_perf_qcate_joint
%     }\hfill
%     \subfigure[Performance of fixed input single and two-image VoLTA classifiers.]{%
%         \includegraphics[width=0.3\textwidth]{figures/results/model_perf_acc_comparison_per_qtype.png}
%         \label{fig:model_perf_qcate}
%     }\hfill
%     \subfigure[Convergence of the VoLTA loss function on the WebQA dev set across several MH-VoLTA training runs.]{%
%         \includegraphics[width=0.33\textwidth]{figures/results/mhvolta_loss.png}
%         \label{fig:loss_convergence}
%     }
%     \caption{Comparison of the variable MH-VoLTA model (left) vs fixed input VoLTA models (center) across different question categories, ordered by the number of image sources per question. Models converge after ~80 epochs (right).}
% \end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth, trim={0 0 0 3cm},clip]{figures/results/model_perf_acc_comparison_per_qtype_joint.png}
        \caption{Performance of the MH-VoLTA classifier by question category and image count.}
        \label{fig:multihop_volta_res}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\linewidth]{figures/results/model_perf_acc_comparison_per_qtype.png}
        \caption{Performance of fixed input single and two-image VoLTA classifiers.}
        \label{fig:model_perf_qcate}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.33\textwidth}
        \includegraphics[width=\linewidth]{figures/results/mhvolta_loss.png}
        \caption{Convergence of the VoLTA loss function on the WebQA dev set across several MH-VoLTA training runs.}
        \label{fig:loss_convergence}
    \end{subfigure}
    \caption{Comparison of the variable MH-VoLTA model (left) vs fixed input VoLTA models (center) across different question categories, ordered by the number of image sources per question. Models converge after ~80 epochs (right).}
\end{figure*}


\subsection{Performance by Question Category}
\label{sec:category_perf}
We report the mean accuracy per question category for Multihop-VoLTA in \autoref{fig:multihop_volta_res} using source retrieval oracles. We find that performance is dependent upon the level of training data available, with the shape category having the least number of samples in the dataset. Question counts per category are as follows; Yes/No (n = 7,320), color (n = 1,830), number (n = 2,118), shape (n = 565). The similarity in results across different question categories reinforces the reliability and stability of our model's performance. For a breakdown of labels per question category, see \autoref{sec:categories}.


% \paragraph{CLIP} Contrastive Language-Image Pre-training \cite{radford2021learning} simultaneously trains both an image encoder and a text encoder. Their task is to predict the correct associations within a batch of (image, text) training examples. However, for the challenging task of full-scale retrieval without any prior exposure in WebQA, it's important to note that running VLP-based retrieval across the entire source collection would be prohibitively time-consuming, estimated at three years. To overcome this limitation, the WebQA approach opted to use CLIP for dense retrieval and BM25 \cite{robertson2009probabilistic} for a more coarse-grained retrieval process.

% \paragraph{VLP} The unified Vision-Language Pre-training (VLP) model \cite{zhou_unified_2020}, is a versatile multimodal generative transformer. This model can be customized through finetuning to excel in either vision-language generation tasks, like generating captions for images, or comprehension tasks, such as answering questions based on visual content. What sets it apart is its utilization of a single multi-layer transformer network for both encoding and decoding, which distinguishes it from numerous other approaches that employ distinct models for these two functions.

% We follow \cite{chang_webqa_2021} in using VLP a baseline model. However, we also adapt two other Vision Language models as baselines, namely  GIT \citep{wang2022git} and BLIP2 \citep{li2023blip2}, which are introduced more formally in later sections.

\subsection{GPT-4o Retrieval Prompt}
\begin{framed}
\label{frame:labeling_prompt}
\textbf{system}: Answer the question in one word. Then list the Fact\_ID or Image\_ID of all facts used to derive the answer in square brackets.

\textbf{human}: Question: <query>

\textbf{human}: Text Facts:
[fact\_id\_1: fact\_1, ..., id\_n: fact\_n]

\textbf{human}: Image\_ID: img\_id\_1, \\
Caption: img\_caption\_1

\textbf{human}: [Input\_type=image] \\
image\_url=url\_1

...

\textbf{human}: Image\_ID: img\_id\_m, \\
Caption: img\_caption\_m

\textbf{human}: [Input\_type=image] \\
image\_url=url\_m

\end{framed}

\subsection{Question Complexity Analysis Metrics}
\label{appendix:complexity_analysis_metrics}

The Flesch-Kincaid Grade Level is a readability metric that evaluates the difficulty of a text based on the length of its words and sentences \citep{flesch2007flesch}, and is defined as;
\begin{equation}
\begin{split}
  \text{FKGL} = 0.39 \left( \frac{\text{Total Words}}{\text{Total Sentences}} \right) \\ + 11.8 \left( \frac{\text{Total Syllables}}{\text{Total Words}} \right) - 15.59  
\end{split}
\end{equation}

The Gunning Fog Index is a readability test used in linguistics to assess the complexity of English writing \citep{gunning1952technique}, and is defined as;
\begin{equation}
\begin{split}
    \text{GFI} = \frac{0.4 \times \text{Total Words}}{\text{Total Sentences}} \\
    + \frac{40 \times \text{Total Complex Words}}{\text{Total Words}}
\end{split}
\end{equation}


% \footnote{The Flesch-Kincaid Grade Level is a readability metric that evaluates the difficulty of a text based on the length of its words and sentences.}

% \footnote{The Gunning Fog Index is a readability test used in linguistics to assess the complexity of English writing.}



% $$


\subsection{Question Category Domain Lists}
\label{sec:categories}
\begin{figure*}
\begin{lstlisting}
yesno_set = {'yes', 'no'}
color_set = {
    'orangebrown', 'spot', 'yellow', 'blue', 'rainbow', 'ivory', 
    'brown', 'gray', 'teal', 'bluewhite', 'orangepurple', 'black', 
    'white', 'gold', 'redorange', 'pink', 'blonde', 'tan', 'turquoise', 
    'grey', 'beige', 'golden', 'orange', 'bronze', 'maroon', 'purple', 
    'bluere', 'red', 'rust', 'violet', 'transparent', 'yes', 'silver', 
    'chrome', 'green', 'aqua'
}
shape_set = {
    'globular', 'octogon', 'ring', 'hoop', 'octagon', 'concave', 'flat', 
    'wavy', 'shamrock', 'cross', 'cylinder', 'cylindrical', 'pentagon', 
    'point', 'pyramidal', 'crescent', 'rectangular', 'hook', 'tube', 
    'cone', 'bell', 'spiral', 'ball', 'convex', 'square', 'arch', 'h', 
    'cuboid', 'step', 'rectangle', 'dot', 'oval', 'circle', 'star', 
    'crosse', 'crest', 'octagonal', 'cube', 'triangle', 'semicircle', 
    'domeshape', 'obelisk', 'corkscrew', 'curve', 'circular', 'xs', 
    'slope', 'pyramid', 'round', 'bow', 'straight', 'triangular', 
    'heart', 'fork', 'teardrop', 'fold', 'curl', 'spherical', 
    'diamond', 'keyhole', 'conical', 'dome', 'sphere', 'bellshaped', 
    'rounded', 'hexagon', 'flower', 'globe', 'torus'
}   
\end{lstlisting}
\end{figure*}

\subsection{Baseline Models}
\label{sec:baselines}

\paragraph{VLP}
\label{sec:VLP}

% A pretrained object detector is used to extract image regions $r_i$, region features $R_i$, region object labels $C_i$, and region geometric features $G_i$. Regions are embedded according to the following network, where [.|.] is the concatenation of features;
% \begin{equation}
%     r_i = W_r R_i + W_p[LayerNorm(W_cC_i)|LayerNorm(W_gG_i]
% \end{equation}

% Special tokens [CLS], [SEP], and [STOP] are special input tokens that represent the start of visual input, the boundary between visual and text input, and the end of the sequence. The weight matrices are trained according to two masked language modeling objectives, where 15\% of input text tokens are replaced (80\% with a [MASK] token, 10\% with a random token and 10\% with the original token). 

% The two objectives are the BERT bidirectional objective and the seq2seq objective which satisfies the auto-regressive property that is desired by the generative setting of VQA. Notably, VLP objectives do not include Next Sentence Prediction as in BERT, which was found to lower performance \citep{zhou_unified_2020}. The seq2seq objective is implemented with a simple self-attention mask M. 
The VLP transformer model consists of a unified encoder and decoder \citep{zhou_unified_2020}. The VLP architecture is made up of 12 layers of transformer blocks trained according to the BERT bidirectional and the seq2seq objectives where the self-attention module in the transformer block are defined as;

\begin{equation}
    A^l = softmax(\frac{Q^TK}{\sqrt{d}} + M)V^T
\end{equation}

where $V = W^l_VH^l-1, Q = W^l_QH^l-1, K = W^l_KH^l-1$. As in \cite{vaswani2017attention}, a feedforward layer (with residual) maps $A^l$ to $H^l$. The model is trained on image caption pairs, and then finetuned for the VQA task. Finetuning follows by taking the hidden states from the final layer and feeding them to a multi-layer perceptron. The model used has been finetuned twice, once on the VQA dataset (as described by \cite{yu_unified_2023}), and again on the WebQA dataset. 

% In the later case, only seq2seq masking is applied, but more importantly, the model is trained on questions that require two image sources as described in the dataset section (section 5.1). Of all the baselines, this is the only model which has been explicitly trained for multihop VQA. During inference, five variants are generated using beam search and the most confident output is used.

\paragraph{GIT}
\label{sec:GIT}
To contrast with VLP, a pretrained multihop VQA model, we use a pre-trained Generative Image-to-Text Transformer (GIT) \citep{wang2022git}.
GIT employs a simplified VQA architecture with one encoder for images and one decoder for text. As such, the model is explicitly incapable for multihop VQA between text and images, so it serves as a baseline for pre-trained models that do not utilize image descriptions, and so we concatenate image sources if there are more than one. 

% The GIT model is finetuned on VQA-2 \citep{goyal2017making}, a typical VQA paradigm where questions do not require knowledge external to the image. By inspection, we also find that questions are simpler (and shorter) in the VQA tasks for which it is trained. We therefore expect that simplified questions will perform better on this model and so the GIT model may be illustrative of where WebQA departs from typical VQA tasks.

GIT is pre-trained using the language modeling task (as opposed to MLM which is used by VLP) where the model learns to predict captions in an auto-regressive manner. For VQA finetuning, the text input is swapped to the query, so that answers are predicted. 

\paragraph{BLIP-2}
\label{sec:blip}
Similar to VLP, the Bootstrapping Language-Image Pre-training model (BLIP) is a unified vision language pre-trained model \citep{li2022blip}. It relies on a visual transformer which is less computationally demanding and is pre-trained on over 100 Million image-caption pairs using a contrastive loss (ITC) for image-text contrastive alignment and image-text matching (ITM). 
% The authors show that it achieves state of the art performance on various VQA tasks and follows the architecture described in Figure \ref{fig:blip_vqa}. The authors of \cite{li2023blip2} introduced BLIP-2 which is based on a similar architecture as BLIP but it introduces a Q-transformer as the encoder and an LLM decoder. 
In addition to the ITC and ITM losses, the authors introduce an additional Image-grounded text generation (ITG) loss that trains the Q-former encoder to generate texts, given input images as the condition. 

% The final objective to minimize is defined as;
% \begin{equation}
%     \textit{Loss} = \text{ITC + ITG + ITM}
% \end{equation}
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\columnwidth]{figures/architectures/blip-vqa.png}
%   \caption{BLIP VQA finetuning architecture \cite{li2022blip}}
%   \label{fig:blip_vqa}
% \end{figure}

\paragraph{GPT-3.5 Turbo}
\label{sec:gpt3.5}
Throughout the dataset, a consistent challenge emerges: the model must focus on details, understand them, and accurately respond to questions, even after the provision of positive source images. This challenge has led to the exploration of an image-to-text approach, where the task involves generating descriptive captions for the images. This transforms the problem into a unimodal text retrieval and generation task. Using this method, the SOLAR model has had success on the WebQA task \cite{alibaba_text}. Accordingly, we include a zero-shot oracle baseline, passing queries and image captions to gpt-turbo-3.5 \citep{brown2020language}.


% \subsection{Vin+VLP Baseline Results}
% \label{appendix:baseline_results}
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\textwidth]{figures/results/histogram_accs_qcate.png}
%   \caption{A histogram depicting the distribution of datapoint-wise accuracy scores for the Vin+VLP baseline on the validation dataset for each type of question category, as assessed using the official evaluation script}
%   \label{fig:hist_accs_qcate}
% \end{figure}

% \begin{figure}[!h]
%   \centering
%   \begin{minipage}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/results/histogram_accuracy_1img.png}
%     \caption{A histogram similar to Fig \ref{fig:hist_accs_qcate}, but specifically for questions with a single positive image source}
%     \label{fig:hist_1img}
%   \end{minipage}\hfill
%   \begin{minipage}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figures/results/histogram_accuracy_2img.png}
%     \caption{A histogram similar to Fig \ref{fig:hist_accs_qcate}, but specifically for questions with a two positive image sources}
%     \label{fig:hist_2img}
%   \end{minipage}
% \end{figure}

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=0.49\linewidth]{figures/examples/guitar.png}
%     \includegraphics[width=0.49\linewidth]{figures/examples/guitar.png}
    
%     \caption{
%     \textit{Guid: d5be98180dba11ecb1e81171463288e9} \\ 
%     \textit{Question Category: "choose"} \\
%     \textit{Question: "Which instrument usually requires a bow to play it; A violin or Fernandes Monterey Deluxe?"} \\ 
%     \textit{Ground Truth: "A violin requires a bow to play it, but the Fernandes Monterey Deluxe does not."} \\ 
%     \textit{Prediction: "A violin"} \\ 
%     \\
%     Even though the model answered the question correctly, neither of the provided images in this modified sample contains a violin. The model simply answers this question based on its pretraining knowledge.}
%     \label{fig:blip_example_1}
% \end{figure*}
% \vspace{100mm}
% \begin{figure}[!h]
%   \centering
%   \includegraphics[width=0.4\columnwidth]{figures/results/histogram_accuracy.png}
%   \vspace{-5mm}
%   \caption{A histogram depicting the distribution of datapoint-wise accuracy scores for the Vin+VLP baseline on the validation dataset, as assessed using the official evaluation script}
%   \label{fig:vin_vlp_hist_acc}
% \end{figure}

% The data presented in Figure \ref{fig:vin_vlp_hist_acc} reveals a significant trend: the accuracy scores predominantly cluster around either 0 or 1. This pattern aligns with the nature of the evaluation methodology, which relies on category-aware lexical overlap for assessment. It's important to note that the evaluation script selects the top candidate answer from the model predictions for these assessments.

% \vspace{8mm}


% \section{Error Analysis - Examples}
% \label{sec:appendix_error_analysis_examples}

% \begin{figure*}[!h]
%     \centering
%     \vspace{-5mm}
%     \includegraphics[width=0.35\linewidth]{figures/examples/empire_tower.jpg}
%     \includegraphics[width=0.35\linewidth]{figures/examples/empire_tower_2.jpg}
%     \vspace{-2mm}
%     \caption{
%     \textit{Guid: d5bc623c0dba11ecb1e81171463288e9} \hspace{20mm} \textit{Question Category: "YesNo"} \\
%     \textit{Question: "Was there a building constructed after 2007 that can now be seen in the distance behind the Empire State Building and mimics its shape?"} \\ 
%     \textit{Ground Truth: "There was a building constructed after 2007 that can now be seen in the distance behind the Empire State Building that mimics its shape"} \\ 
%     \textit{Prediction: "Yes , there was a building constructed after 2007 ..."} \\ 
%     % \\
%     Even though the model answered the question correctly, neither the images nor their captions provide any information of the year being 2007 or later. As the answer does contain a Yes/No keyword, the prediction can never be wrong.
%     }
%     \label{fig:example_3}
% \end{figure*}

% \begin{figure}[!h]
%   \centering
%   \begin{minipage}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/examples/Barcelona-St-Josep-La-Boqueria.jpg}
%     \vspace{-7mm}
%     \caption{
%       \textit{\\Guid: d5bc2b6e0dba11ecb1e81171463288e9} \\ 
%       \textit{Question Category: "choose"} \\
%       \textit{Question: "Is the word on the lollipops at Market St Joseph La Boqueria in Barcelona written in print or cursive?"} \\ 
%       \textit{Ground Truth: "The word on the lollipops is written in cursive."} \\ 
%       \textit{Prediction: "The word on the lollipops at Market St Joseph La Boqueria in Barcelona is written in print ."} \\ 
%       \\
%       Answering this question requires a human to closely examine the image due to its low resolution and the small portion relevant to the answer (The white annotation has been added by us for improved visualization and is not part of the original image)}
%     \label{fig:example_1}
%   \end{minipage}
%   \hfill
%   \begin{minipage}{0.48\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{figures/examples/Washington_Square_Arch-Isabella.jpg}
%     \vspace{-7mm}
%     \caption{
%       \textit{\\Guid: d5bcd3700dba11ecb1e81171463288e9} \\ 
%       \textit{Question Category: "shape"} \\
%       \textit{Question: "What shape is the fountain near the arch in Washington Square Park?"} \\ 
%       \textit{Ground Truth: "There is a circle shaped fountain near the arch in Washington Square Park."} \\ 
%       \textit{Prediction: "The fountain near the arch in Washington Square Park is a circle ."} \\ 
%       \\
%       While the model provides a correct answer to this question, the accuracy score is 0.5, which should ideally be 1. This issue arises from the model's inability to distinguish between a square shape and a place like 'Washington Square Park.' The evaluation metric interprets the model's response as 'square' and 'circle' instead of just 'circle.'}
%     \label{fig:example_2}
%   \end{minipage}
% \end{figure}
