% The work on multi-hop VQA in WebQA spans approaches including but not limited to pretrained multimodal vision language models, retrieval augmented generation, and unified language representations \citep{chang_webqa_2021}.
% % and OK-VQA \citep{marino_ok-vqa_2019}. 

% These approaches collectively offer insights into the evolving field of multimodal QA, emphasizing the need for information fusion, knowledge integration, and task-specific adaptations for continued advancements. Combining these strengths or exploring ensemble strategies may hold potential for achieving even more robust performance.


\subsection{Multimodal Retrieval Systems} 
A large body of work on multimodal representations exists \citep{liu2022universal,chen_murag_2022,radford2021learning}. 
CLIP enables the embeddings of text and images into aligned representations by supervised training over image-caption datasets \citep{radford2021learning}. More sophisticated local alignment methods between captions and images using Graph Optimal Transport (GoT) have been proposed \citep{chen_graph_2020,maretic_got_2019}. The Universal Vision-Language Dense Retrieval model (UniVL-DR) showed SoTA performance on the WebQA retrieval task \citep{liu2022universal} by using hard-negative sampling for constrastive learning. In this work, we compare UniVL-DR and CLIP embeddings as competing retrieval systems.

% MuRAG \citep{chen_murag_2022} is an end-to-end retrieval and QA model that leverages pretraining on large multimodal corpora to learn a unified embedding space for image and text modalities. While it shows promising results on the WebQA and MultiModalQA datasets, challenges in visual understanding related to object recognition and counting persist. 


% In our work, we employ their pretrained model as a retriever and focus on finetuning a QA model that takes the retrieved sources as input before predicting an answer.

\subsection{Multihop Language Models} 
A wealth of research exists on multimodal Vision-Language tasks and multihop language decoders. \cite{tanaka_slidevqa_2023} propose a Fusion-in-Decoder (FiD) architecture for multihop reasoning over images. Utilizing advances in local alignment \citep{chen_graph_2020}, VoLTA model combines graph representations of input questions and source images \citep{pramanick_volta_2023}. For compatibility with retriever modules, we extend VoLTA with support for a variable number of input sequences. 

More recently, the increasing context windows of VLMs enables them to demonstrate multihop reasoning abilities \citep{liu2024visual,abdin2024phi,wang2024qwen2}. Recent work has found that not only are LLMs capable of determining when they should forgo their parametric memory and use a retriever module \citep{labruna2024retrieve}, they are also capable of ``In-context Retrieval" \cite{incontext_rag}. Here, retrieved sources are used for grounded text generation by simply prepending the sources into the input prompt. We expand upon this idea, adapting it to a multimodal setting with VLMs, and report our findings.

\subsection{The Parametric Effect}
There is a wealth of research on reliance on parametric memory for unimodal retrieval and QA tasks \citep{galway_mitigation_2024,xu_knowledge_2024,longpre_entity-based_2022,neeman_disentqa_2022,hong_why_2024,chen_rich_2022}. Here, the entity replacement framework \citep{longpre_entity-based_2022,neeman_disentqa_2022} is used to invalidate parametric memory by explicitly crafting knowledge conflicts between input sources and parametric memory \citep{xu_knowledge_2024,hong_why_2024,chen_rich_2022}. As such, these studies guarantee that manipulated input sources no longer entail the expected labels.
In contrast, we do not make the same guarantees, and our proxy measures are premised upon the key assumption that incorrectly retrieved sources \textit{do not entail} the correct answer. Our focus is on developing proxy metrics for the parametric effect that do not require such involved source manipulation processes, so extending the entity replacement framework to the multimodal setting is left as the subject of future work.



% They construct a novel dataset out of slide decks and form questions that require combining knowledge from multiple slides to construct the answer.
% (\autoref{fig:text_fid}).

% The VoLTA model In particular, the 


% This was shown to reduce embedding distances between entities in the question and source image that have similar relations leading to VoLTA's competitive performance across a range of multimodal tasks.

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{figures/architectures/slide_vqa_fid.png}
%     \caption{Multihop image-based FiD model \cite{tanaka_slidevqa_2023}}
%     \label{fig:slidevqa_fid}
% \end{figure}

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\textwidth]{figures/architectures/text_vqa_fid.png}
%     \caption{Multihop text-based FiD model \cite{izacard_leveraging_2021}.}
%     \label{fig:text_fid}
% \end{figure*}

 % They design a purpose-built FiD model to reason over multiple slides (\autoref{fig:slidevqa_fid}). It is important to note that one limitation of their approach is that they fix the number of inputs to \textit{k sequences} and treat the problem as a \textit{k-hop} multimodal QA task. We propose an FiD model that extends this work to support a variable number of input sequences to overcome this limitation, and tackle multi-hop reasoning tasks.

% \paragraph{VoLTA}
% \label{sec:volta}

% VoLTA is pretrained with a combination of objectives. First, Barlow Twin loss $L_{BT}$ is used to reduce redundancy within both image and text pairs and image-text pairs. Then patches of images and tokenized texts are used to construct graphs respectively. Graphs are then aligned using the Graph Optimal Transport (GOT) algorithm. Wasserstein Distance and Gromov-Wasserstein Distance are combined to do node and edge matching. Lastly, Cross-Modal Attention Fusion parameters are pretrained with both masked language modeling $L_{MLM}$ and image-text matching $L_{ITM}$. The objectives could be categorized to serve three purposes, as shown below:
% \vspace{-3mm}
% \begin{equation}
%     L_{total} = L_{BT} + w_{GOT} * L_{GOT} + L_{MLM} + L_{ITM}
% \end{equation}