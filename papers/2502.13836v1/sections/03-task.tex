\paragraph{WebQA}
% \subsection{Dataset and Input Modalities}
% Modern web searches are multimodal and multihop, integrating various sources and modalities (like text and images) for comprehensive answers. The WebQA dataset is built on this paradigm, requiring multihop reasoning over retrieved images and texts for answer prediction \citep{chang_webqa_2021}.
% % In particular, 44\% of image-based queries and 99\% of text based queries require two or more knowledge sources.

% WebQA samples comprise a structured format that includes a question denoted as $Q$, a collection of $m$ relevant and distractor sources $S$ labeled as $s_1$ through $s_m$,  and an answer designated as $A$. Sources come in the form of text snippets or image-description pairs. Image descriptions play a vital role in linking the visual content to the references mentioned in the question because they help identify named entities or geographic details that might not be immediately apparent in the images alone.

The WebQA dataset \cite{chang_webqa_2021} is designed with a two-step approach in mind; retrieval followed by QA. First, given the question Q and all sources $S$, we retrieve the set of relevant sources, $S^\prime$. Using these sources we then generate an answer $A^\prime$. The following is passed to the QA classifier: 
\begin{equation}
    <[CLS], s^\prime_0, [SEP], \dots, s^\prime_n, [SEP], Q, [SEP]>
\end{equation}
% For generative models, decoding follows the auto-regressive pattern by iteratively predicting [MASK] tokens that are appended to the input as described in .

% Given the retrieval scores of the baseline models are high, we are left with little room to further improve retrieval performance. 

% \paragraph{Dataset}
% Upon conducting an initial assessment of the WebQA dataset, we systematically find that all questions (n = 41,732) can be resolved either through text (n = 20,267) or images (n = 21,465), but not a combination of both. Additionally, we find that if a question can be answered using images, it has a maximum of two positive image sources. To investigate the impacts of domain size on model performance, we reformulate the WebQA task as a classification problem and restrict the training and validation data to a subset of the domains present in the original.

We include only those questions that either one (n = 12,027) or two (n = 9,438) image sources. For a breakdown of question categories and their keywords, see \autoref{sec:categories} in the appendix.

As opposed to WebQA, open-domain VQA tasks such as OK-VQA \citep{marino_ok-vqa_2019} and HotpotQA \citep{yang2018hotpotqa} do not provide candidate sources $S$ and source labels $S^*$. As such, the memorization metrics proposed are incompatible with these tasks (see \autoref{sec:measures}). Moreover, while we do evaluate QA model performance on NLVR2 \citep{nlvr2} and VQAv2 \citep{goyal2017making}, these tasks do not have a retrieval step and so these results are instead presented in the appendix (see \autoref{sec:vqav2}).

% For our experiments, we use 16,342 training samples for fine-tuning and all of our evaluations are conducted on the validation set consisting of 1,336 samples. with associated image sources categorized as positive (relevant to the question) and negative (irrelevant to the question). 


% As we observed in the WebQA dataset, the number of positive image sources and text sources is at most two, and questions all come with a question category as well as a single-word keyword answer. 
