While QA performance is generally predicated upon retrieval success (\autoref{fig:retrieval_vs_qa_lines}), there are many cases where retrieval fails and QA succeeds (\autoref{fig:retrieval_vs_qa_heatmap}). These cases form the basis of our quantitative metrics, the \ucr (\UCR; see \autoref{tab:ucr_scores}) and \ppr (\PPR; see \autoref{fig:eval_lineplots}), and with these measures we show that external retrievers significantly reduce the reliance of VLMs on parametric memory. This reduction in memorization not only preserves model flexibility but also mitigates the over-specialization common in finetuned systems. However, despite GPT-4o obtaining state-of-the-art performance on the WebQA benchmark using this approach (\autoref{tab:test_scores})\footnote{``Anon\_ACL\_ARR\_Feb25" @ \href{https://eval.ai/web/challenges/challenge-page/1255/leaderboard/3168}{WebQA leaderboard}}, for less powerful VLMs such as Qwen2 the decrease in memorization  associated with not finetuning the model (\PPR: 0.77 -> 0.53) comes at the cost of model accuracy (QA accuracy: 70\% -> 52\%).

% Our work demonstrates that retrieval-augmented systems enable general-purpose language models—--such as GPT-4o and Qwen2—--to excel on specialized tasks without extensive finetuning. 

Building on previous research in retrieval-augmented generation and multimodal question answering, our study explicitly quantifies the balance between memorization and retrieval. Our findings reveal that in-context retrieval can effectively substitute for finetuning in multimodal QA tasks. However, this approach is currently limited by the complexity of the question and task (\autoref{fig:complexity_vs_qa}). The strong performance of general-purpose VLMs when augmented with finetuned retrievers suggests that general-purpose VLMs are capable of reasoning over retrieved documents and images, without being affected by question complexity (\autoref{fig:complexity_vs_qa_ft}). 

This analysis also reveals a paradoxical relationship between retriever recall and \UCR that highlights a potential annotation issue within the dataset, prompting a reevaluation of how retrieval-QA benchmarks are constructed (\autoref{tab:ucr_scores}). As such, our approach is subject to certain limitations. Annotation inconsistencies may affect the reliability of \UCR as a sole indicator of retrieval quality. Moreover, measures that more directly evaluate model attention are feasible, and may incorporate visual attention models such as Grad-CAM \citep{selvaraju_grad-cam_2020} or object detection methods \citep{ravi_sam_2024}. This line of work can take inspiration from a wealth of research on provoking parametric responses from unimodal LLMs, where the entity replacement methods \citep{longpre_entity-based_2022,neeman_disentqa_2022} create knowledge conflicts between input sources and parametric memory \citep{xu_knowledge_2024,hong_why_2024,chen_rich_2022}. By explicitly crafting knowledge conflicts, extending these methods to the multimodal setting will allow memorization analysis to move beyond the assumption that incorrectly retrieved sources \textit{should} result in incorrect answers from the VQA model.

Despite this assumption, our measures reveal an interesting interplay between retrieval and memorization. By providing insights into end-to-end retrieval and QA systems, \UCR can highlight when models are over-reliant on parametric memory. High Retriever Attainment scores (\RPA) across Qwen2 and GPT-4o experiments demonstrate that general-purpose VLMs can utilize finetuned retrievers (\autoref{fig:eval_lineplots}), drawing the need for domain-specific finetuning into question. This work points towards In-Context Retrieval as a particularly promising direction for future research in multimodal systems, if the limitation of question complexity can be addressed.

Our findings suggest concrete directions for improving retrieval-augmented QA systems. First, incorporating \ucr into the evaluation of retrieval tasks can highlight annotation issues, as in WebQA (\autoref{tab:ucr_scores}). Second, integrating retrieval-aware finetuning strategies—--where LLMs learn when to prioritize retrieved content rather than rely on parametric knowledge \citep{labruna2024retrieve}—--may help further reduce memorization while maintaining accuracy. Third, extending our memorization analysis to open-domain multimodal tasks would help validate our findings while providing new insights into retrieval dynamics in unrestricted, real-world settings. Addressing these challenges will contribute to the development of AI systems that are not only more generalizable but also more interpretable, reducing the risks of memorization-driven hallucination.

Overall, our methodology provides a framework for measuring and mitigating memorization in retrieval-augmented systems, offering new ways to evaluate the quality of retrieval-QA datasets. By quantifying reliance on parametric memory, researchers can better assess the trade-offs within finetuning and retrieval, thereby guiding the development of models that balance generalization with task-specific performance. As retrieval-augmented models continue to scale, our findings highlight the need for robust retrieval evaluation and memorization control to ensure safe, effective, and adaptable AI systems. This work lays a foundation for future research into retrieval-aware training strategies, multimodal in-context learning, and more interpretable AI architectures that can integrate external knowledge while minimizing unintended memorization.






% Crucially, our measures demonstrate that while LMs do utilize retrieved sources, they are also subject to varying degrees of memorization depending on how they are trained (\autoref{fig:eval_lineplots}). While finetuning LMs results in improved task performance, it also leads to increased memorization. The effect appears most dramatic when finetuning restricted-domain classifiers (i.e. VoLTA), and underscores the link between model memorization and over-specialization. The ``potential attainment" measure shows that finetuned retrievers mitigate the performance gap between finetuned and baseline LLMs in end-to-end systems (\autoref{fig:eval_lineplots}). 


% In summary, we provide a method for determining whether a given multimodal QA system relies more on retrieved sources or memorization. This work supports the development of more generalizable and flexible AI models and opens new avenues for future research. As the community continues to refine retrieval techniques and develop more robust evaluation benchmarks, these insights are expected to drive significant progress toward creating AI systems that are both accurate and transparent in their decision-making processes.

% \subsection{Segmentation Substitution}

% Examples are given below for shape (\autoref{fig:seg_sub_shape}), number (\autoref{fig:seg_sub_number}), and color (\autoref{fig:seg_sub_pipeline}) question categories.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/segmentation/Cone statue_perturbed.png}
%     \caption{Segmentation substitution for the shape category}
%     \label{fig:seg_sub_shape}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/segmentation/2 columns_perturbed.png}
%     \caption{Segmentation substitution for the number category}
%     \label{fig:seg_sub_number}
% \end{figure}

% The key takeaway is that retrieval-augmented systems allow general-purpose language models (GPT-4o, Qwen2) to perform well on specific tasks without finetuning. This approach reduces reliance on parametric memory (UCR: \autoref{tab:ucr_scores}, PAR: \autoref{fig:eval_lineplots}), helping the model maintain flexibility while improving task-specific performance through external retrieval. It avoids the over-specialization that can result from finetuning, enabling the model to adapt efficiently. SoTA results on the WebQA test set highlight this (\autoref{tab:test_scores}), with GPT-4o obtaining top QA accuracy on the WebQA leaderboard at the time of writing\footnote{``Anon\_ACL\_ARR\_Feb25" @ \href{https://eval.ai/web/challenges/challenge-page/1255/leaderboard/3168}{WebQA leaderboard}}. 

% This result also indicates that LLMs such as GPT-4o are capable of in-context multimodal retrieval. This finding lowers the `barrier of entry' for LLMs to be used to reason over documents that they are not explicitly trained on. This may allow users of LLMs to circumvent measures taken to remove questionable and / or dangerous content from training materials. This warrants future work on evaluating the LLMs ability to distinguish safe and unsafe sources provided in prompts for In-Context Retrieval.



% % \paragraph{UCR: Source Labels or Model Memory}
% Our measures also help to isolate issues of model memorization and inaccurate dataset labels. For instance, the paradoxical correlation between retriever recall and UCR is explained by the source labels being inaccurate (\autoref{tab:ucr_scores}). That is, failures from more sophisticated or finetuned retrievers are not labeled as the most relevant source, but they still help the QA model to generate the correct answer. In this case, UCR may also act as measure of the task---to what degree are questions only answerable by the `gold label' sources? A well-formed retrieval-QA task should not result in a correlation between retrieval recall and UCR.

% We note that the development of specialized metrics for VQA memorization is possible and motivated by this work. Such measures would be based on attention maps from retrieved images, where visual representations of how models direct attention to different regions of image sources (Grad-CAM) could be tested to see how they align with annotated bounding boxes from object detection () or masked regions from segmentation methods (). This would enable the extension of the Unsupported Correctness Rate measure to a wider set of VQA tasks that do not require a retrieval step, or in the open-domain setting, tasks that do not have training set labels for retrieved sources, as in WebQA.


% These findings suggest a path toward more interpretable and generalizable retrieval-augmented AI systems, mitigating the risks of memorization while maintaining strong QA performance. 






% These results underscore the importance of carefully selecting models based on the complexity of the task and the domain of interest. While general-purpose LLMs like GPT-4o exhibit impressive performance in zero-shot scenarios, finetuned models still provide a competitive edge when dealing with specific subdomains or complex multihop reasoning tasks. 
% Moreover, our analysis of in-context retrieval suggests that LLMs could be leveraged more effectively for real-time retrieval-augmented systems, bypassing the need for external retrieval modules. This could simplify the design of AI systems while still enabling them to retrieve relevant information from external sources. Ultimately, this work lays the groundwork for developing AI models that are both generalizable and specialized, while also minimizing the risks of memorization and enhancing the accuracy and reliability of AI-generated outputs.



% As our focus is on how these systems balance accuracy on the WebQA task with memorization that results from training and finetuning processes (i.e. the parametric effect), we compare the performance of finetuned multimodal systems with zero-shot VLMs. We evaluate both types of models on the WebQA benchmark, a multimodal dataset designed to measure both retrieval and VQA performance. Following this, we investigate the impact of different retrieval systems on QA performance, specifically exploring how the presence of external retrieval modules affects model outputs. By focusing on examples where retrieval fails, we propose two metrics that quantify the level of reliance on parametric memory in various QA models. The first, which we call the ``Parametric Accuracy Rate" (PAR), contrasts model performance when presented with best-case and worst-case retrieval modules. The second, an end-to-end retrieval and QA measure which we call ``Unsupported Correctness Rate" (UCR), is simply the fraction of correct responses from the QA model for which the retriever failed.
% understand how much a model depends on parametric knowledge versus information retrieved from external sources.



% Interestingly, we show that GPT-4o’s performance is heavily reliant on its in-context retrieval abilities, with the model retrieving relevant sources in the presence of distractors and accurately answering questions. We also find that finetuned multimodal systems outperform general-purpose models in closed-domain settings, especially for samples with higher question complexity. This demonstrates the importance of task-specific finetuning when handling specialized or more domain-constrained questions.











% There are two possible explanations for the correlation between retrieval recall and UCR. It could be that the QA model learns to memorize answers to question for which the sources are harder to retrieve. While model memory could explain this trend for Qwen2 and Qwen2-FT, it does not explain it for MH-VoLTA, which has ~250M parameters. At this level, MH-VoLTA likely does not have the parametric capabilities required
% , and has not been exposed to a large enough training corpora to enable it 
% to produce parametric responses to unseen VQA samples. Rather, we find that the correlation between recall and UCR is instead 



% \subsection{Future Work}


% OLD

% We explore the use of generalized language models and specialized finetuned models on the WebQA benchmark through model selection on the QA task (\autoref{tab:baselines}). Our MH-VoLTA model outperformed all baselines, including specialized VQA models finetuned on WebQA, and a mixture of unimodal and multimodal language models. We demonstrate that MH-VoLTA is equally capable of performing QA on single and two-image source queries, making it the first multimodal multihop model capable of handling varying numbers of input sources, albeit with limited variance in input length (\autoref{fig:qa_performance}).

% Turning to the retrieval task, we compare the UniVL-DR model \citep{liu_universal_2023} with GPT-4o In-Context Retrieval (\autoref{tab:val_set_e2e}), and find that on the validation set, UniVL-DR outperforms GPT-4o. When our systems are evaluated end-to-end on the combined retrieval and QA tasks, this result holds---the In-Context RALM and MH-VoLTA systems that use the UniVL-DR retriever outperform our In-Context RLM approach with GPT-4o. In addition, we provide a performance upper bound using the `gold' source oracle and lower bound using no retrieval and relying solely on GPT-4o's parametric memory. As the sophistication of the retrieval system increases, so does the resulting QA accuracy.

% This leads us towards a more thorough analysis of the QA task's dependence on the retrieval stage. Unsurprisingly, we find that correct retrieval leads to high QA accuracy (Figure \autoref{fig:retrieval_vs_qa}), and poor retrieval performance is detrimental to QA accuracy (Figure \autoref{fig:distractors_on_qa}). Crucially, this demonstrates that GPT-4o is not using parametric memory but instead relies on its In-Context Retrieval ability for the WebQA task. To our knowledge, GPT4-o's In-Context Retrieval ability is a novel finding and is an emergent phenomenon in the well-studied space of In-Context Learning \citep{dong2022survey,min2022rethinking}. 

% We supplement these findings with a more fine-grained exploration of factors that affect retrieval and QA performance. Notably, for MH-VoLTA, it is clear that data availability is important---QA performance is substantially lower for the shape question category, which has by far the fewest training samples (\autoref{fig:qa_performance}). As a result, In-Context RLM is a natural choice in a data-constrained environment. Furthermore, our query complexity analysis reveals that In-Context RLM is most appropriate for low-complexity tasks (\autoref{fig:complexity_vs_qa})---increasing query complexity comes at the cost of In-Context RLM accuracy, so much so that it overcomes the general trend of increased retrieval recall leading to increased QA accuracy. We note that this is not the case for In-Context RALM or MH-VoLTA, where query complexity does not appear to affect retrieval recall or QA accuracy (\autoref{fig:complexity_vs_qa_ft}), making these methods preferred where task complexity is unavoidably high.

% Finally, we demonstrate that despite the lack of a SoTA finetuned retriever, In-Context RLM outperforms In-Context RALM and that In-Context RLM achieves SoTA performance on the WebQA benchmark. This, along with this analysis that points towards where In-Context RLM using GPT-4o, or specialized models like MH-VoLTA may be more applicable, forms the core contribution of this research. We hope that future research on multimodal QA and, more generally, on retrieval in language models, can benefit from this work.



% Do not consider

% \begin{itemize}
%     \item Architecture for multimodal multihop QA that takes variable number of input sources inspired by the Fusion-in-Decoder method
%     \item Comparison of general purpose LLMs vs specialized models on the WebQA benchmark
%     \item Observation of In-Context Multimodal Retrieval abilities of GPT-4o and showing that it does not always rely on parametric memory for text generation
%     \item Analysis of relationship between retrieval and QA task performance
%     \item Analysis of task and query complexity on the performance of retrieval and QA tasks
% \end{itemize}


% Zero-shot unimodal (GPT-3.5) and mulitmodal (BLIP-2) models do not match the performance of finetuned VQA models on WebQA. WebQA is unlike other VQA datasets since the questions not only require the models to extract subtle cues from pictures but also correlate these cues between two different images in the multihop case. Although few shot methods do not require any training, one drawback of using a few-shot approach is that even if the accuracy is reasonable, the fluency metric is very low and finetuning is required to allow the models generate responses with the verbosity required by the training data in WebQA.


% This research, while attempting to address a gap in multimodal multi-hop reasoning, has certain limitations. Future work should aim to mitigate these limitations.

% Firstly, the model's performance heavily depends on the quality and diversity of the datasets used for training. Inadequate representation of various data types may lead to biases, and based off our experiments on the WebQA dataset alone, we cannot claim that our model has generalizability. 

% Secondly, the computational resources required for processing multimodal data are significant. While the usage of LORA is sufficient to reduce the number of trainable parameters in VoLTA's encoders from 250M to just 10M, training a decoder was problematic, and no pretrained decoders were available for use with VoLTA. The lack of a decoder limits the usability of our model, and means that we have implemented 'Fusion-in-Decoder' as 'Fusion-in-Classifier'. This is a barrier for broader application and further development. 

% Finally, the proposed model needs to be trained for the maximum number of images allowed. For WebQA, the maximum number of images is just two, and we have not experimented with datasets that have larger variances in their input sets; SlideVQA \citep{tanaka_slidevqa_2023} may be a suitable choice as it contains a set of 20 images per question. 
