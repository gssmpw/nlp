Our measures are based upon the assumption that incorrectly retrieved sources \textit{should} result in incorrect answers from the VQA model. They are proxy measures premised upon the assumption that incorrectly retrieved sources do not entail the correct answer. Future work to build memorization metrics not subject to this assumption are warranted.

Given that there is a wealth of research on quantifying the parametric effect using unimodal QA benchmarks, we choose to look only at VQA. That being said, the evaluation metrics we present are general and are also relevant to the measurement of the parametric effect in unimodal settings. We leave this application to future work.

As our memorization metrics require that the task be designed in a two-part retrieval and VQA process, this leaves WebQA as the only valid VQA task to evaluate performance on. Note that we do validate MH-VoLTA performance on VQAv2 and NLVR2 (see \autoref{tab:baselines} in the appendix), as this novel QA model should validated independently of external retrieval systems. As the other QA models have been validated in prior works, we do not measure their performance on VQAv2 or NLVR2.


% Our analysis of finetuned QA and our Multihop-VoLTA model on the WebQA benchmark is limited by our architecture. In line with previous works on NLVR and VQA tasks, the Multihop-VoLTA model is a classifier. As such, we do not answer questions on the 'text' category, which is the most open domain variant of the benchmark, and instead focus on the multimodal, multihop aspect of questions in categories that specifically require visual reasoning (i.e. counting, color, and shape detection).

% In addition, while we find trends that indicate domain cardinality is an important factor in determining whether to adopt finetuned or zeroshot models when decided which approach to take on a task, we only explore the WebQA task, and our analysis focuses on the impact of question complexity on retrieval and QA performance. The role of domain cardinality on model selection is an important area for future research, and analysis here should be expanded and repeated on more datasets and tasks.

% Finally, we claim that LLMs are capable of In-Context Retrieval based on our finding that GPT-4o can retrieve relevant sources for the WebQA task. However, WebQA pre-selects a list of distractor sources and relevant sources, and so it is clear that GPT-4o's performance on the retrieval part of this task is premised upon the WebQA benchmark construction. The positive sources still need to be present among the list of distractors, and for WebQA, the number of distractor sources is held constant for each question. Further work on more traditional unimodal and multimodal retrieval datasets is needed to generalize the notion of In-Context Retrieval. 


% This work extends the parametric in-domain capabilities of LLMs to handle open-domain tasks by formulating methods where the LLMs predict when to use an IR module, which is the standard in most RAG systems and when to solely rely on parametric memory \citep{labruna2024retrieve}. 


% In particular, based on our analysis of the effect that retrieving distractors has on QA performance (\ref{fig:distractors_on_qa}), 
