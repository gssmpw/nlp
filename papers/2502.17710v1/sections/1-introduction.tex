\section{Introduction}
\begin{figure}[!t]
    \centering
    \includegraphics[trim=0em 11em 4em 2em, clip, scale=0.85]{figures/intro_fig.pdf}
    \caption{Interpretations of non verbal gestures varies dramatically across regions and cultures. ``Crossing your fingers'', while commonly used in the US to wish for good luck, can be considered deeply offensive to female audiences in parts of Vietnam. AI systems, such as T2I models, should be culturally aware and avoid generating visual elements that risk miscommunication or offense in specific cultural contexts.}  
    \label{fig:intro-fig}
    \vspace{-1.2em}
\end{figure}



Non-verbal communication, such as gestures, poses and facial expressions, plays a critical role in human interaction to convey beliefs, emotions, and intentions \cite{knapp1978nonverbal, burgoon2011nonverbal}. While such communication is universal, its interpretations significantly vary across cultures, often leading to misunderstandings  \cite{kirch1979non, matsumoto2016cultural}. For example, the gesture of ``crossing your fingers,'' viewed as symbol of good luck in the US, can be offensive in Vietnam, particularly to women (Figure \ref{fig:intro-fig}).\footnote{Misaligned gestures can have real consequences. For example, Richard Nixon’s use of the double ``OK'' sign in South America and George H.W. Bush’s inward-facing ``V-sign'' in Australia were interpreted as offensive gestures by local audiences \cite{nyt_nixon_1974, nyt_gestures_1996, chicago_gestures_1992}.} AI systems are increasingly used in global marketing, digital content generation and advertising \cite{Mim2024InBetweenVA}. Given this widespread adoption, preventing the generation of culturally offensive content is crucial to avoid harm, exclusion, and damage to cross-cultural relationships \cite{wenzel2024designing, ryan2024unintended}.\footnote{Digital media companies like Disney have recognized the impact of non-verbal content by digitally removing offensive hand gestures from productions to avoid cultural insensitivity \cite{chicago_tribune}.}


% Despite growing efforts in AI safety research, current work has primarily focused on detecting explicit threats like violence or sexual imagery \cite{han2024wildguard, Deng2023HarnessingLT, Riccio2024ExploringTB}. LLMs and VLMs have been studied for their knowledge of cultural artifacts like food and clothing, as well as cultural norms and commonsense reasoning \cite{Yin2021BroadenTV, romero2024cvqa, rao2024normad} without an explicit focus on safety. For T2I models, research has emphasized technical aspects like geographical diversity, faithfulness, and realism of generated concepts \cite{hall2023dig, hall2024towards}, with some emerging work on cultural awareness and diversity of cultural artifacts \cite{kannen2024beyond}. None of these works have considered harms from culturally insensitive non-verbal communication. This highlights a critical gap in current AI safety frameworks – the need for culturally contextual safety guardrails that prevent systems from generating or accepting content that, while innocent in one context, may be deeply offensive in another.
% % With global audiences increasingly engaging with AI systems, understanding and preventing culturally offensive visual content is crucial. Text-to-image systems, vision-language models and large language models are rapidly being integrated into diverse applications - from educational content creation and multimodal translation tools to creative generation for social media and marketing \cite{Mim2024InBetweenVA}. Each of these applications risks generating gestures or visual elements that, while benign in one culture, could be deeply offensive in another. This cultural misalignment poses risks ranging from educational harm and social exclusion to business liability and damaged cross-cultural relationships \cite{wenzel2024designing, ryan2024unintended}. 

\violet{I think we can reorder some of the sentences here to get a better flow. Talk about general AI safety research and the relatively less attention on cultural aspects in both LLMs and arguably more for VLMs. Then switch gear to non-verbal communication and emphasize the lack of studies of cultual nuances in non-verbal communications for VLMs.}
Current research on the safety implications of cultural nuances in non-verbal communication is limited. While AI safety efforts primarily target explicit threats such as violence and sexual content \cite{han2024wildguard, Deng2023HarnessingLT, Riccio2024ExploringTB}, broader examination of cultural impacts is needed. Large language models (LLMs) and vision-language models (VLMs) are increasingly studied for their knowledge of cultural norms and artifacts like food and clothing \cite{Yin2021BroadenTV, romero2024cvqa, rao2024normad}, while text-to-image (T2I) models have prioritized geographical diversity, realism, and faithfulness \cite{hall2023dig, hall2024towards, kannen2024beyond}. However, the extent to which these models may generate culturally insensitive content largely remains unexplored.
%, highlighting the need for systematic evaluation of cross-cultural safety.



To bridge this critical gap, we study culturally contextualized safety guardrails for T2I systems, LLMs and VLMs through the lens of non-verbal gestures. We introduce \offHandsDataset,\footnote{Multi-Cultural Set of Inappropriate Gestures and Nonverbal Signs} a novel dataset capturing \textit{cultural interpretations of 288 gesture-country pairs spanning 25 common gestures and 85 countries} (\S\ref{sec:data}).  Annotators from respective regions provide insights on: (1) the gesture's regional level of offensiveness (from not offensive to hateful), (2) its cultural significance, and (3) situational factors such as social setting and audience that influence its interpretation within that region. This dataset serves as a test bed for evaluating and improving cultural safety of AI systems in real-world applications. 
% This dataset serves dual purposes: as a test bed for evaluating cultural grounding in AI systems, and as a practical resource for developing culturally aware applications in domains like global marketing, education, and cross-cultural communication. {didn't understand the difference between test set and practical resource? Isn't it a practical resource because its a test set? We can't use it in any other setting, for example, training right? --> make clear}


Using our \offHandsDataset dataset, we aim to answer the following research questions:
\begin{enumerate}[label=\textbf{RQ\arabic*:}, itemsep=0pt, topsep=0pt, leftmargin=3em]
  \item Can models (LLMs, VLMs) accurately detect and reject (T2I) culturally offensive gestures? 

  \item Are models culturally aware when gestures are described by their \textit{implicit} US-centric meanings, rather than  literal explicit names? (e.g., appropriately responding to ``show a gesture meaning good luck'')  \violet{this is a little confusing. why ``show a gesture meaning good luck'' is a ``US-centric meanings''? I think the meaning is universal (good luck is intepretable in all countries/cultures, but the gesture themselves may vary. So rather, US-centric guestures?}
  % \saadia{is this really "North American" or US? Particularly since RQ3 only focuses on the US, I wonder if RQ2 should be consistent with this.} 
  
  \item Do models exhibit US-centric biases in their detection of offensive gestures across US and non-US cultural contexts? 

  
\end{enumerate}

% Our findings reveal significant limitations in AI systems' handling of culturally offensive non-verbal gestures. For \textbf{RQ1}, T2I systems fail to reject inappropriate content requests (low  5--46\% offensiveness detection rates \S\ref{sec:T2I:ssec:rq1}), while LLMs and VLMs tend to over-flag gestures as offensive with high recall (62--98\% for LLMs \S\ref{sec:LLM:ssec:rq1}; 48--91\% VLMs \S\ref{sec:VLM:ssec:rq1}) and low specificity (1-60\% for LLMs; 15--62\% for VLMs), reflecting an overly cautious yet error-prone approach that lacks cultural nuance. Regarding \textbf{RQ2}, all models struggle to maintain cultural awareness when gestures are instead described by their US intended meaning, frequently generating and recommending inappropriate gestures to regions where they're offensive (T2I 64-81\% \S\ref{sec:T2I:ssec:rq2}; LLM 20--69\% \S\ref{sec:LLM:ssec:rq2}; VLMs 12--91\% \S\ref{sec:VLM:ssec:rq3}). Finally for \textbf{RQ3}, all models exhibit a pronounced US-centric bias, with higher accuracy for identifying offensive gestures in US-contexts (\S\ref{sec:T2I:ssec:rq3}, \S\ref{sec:LLM:ssec:rq3}, \S\ref{sec:VLM:ssec:rq3}). \akhila{not happy with this, will come back and maybe do model-wise}

Our findings reveal significant limitations in AI systems' handling of culturally offensive gestures. For offensive gesture detection (\textbf{RQ1}; \S\ref{sec:results:rq1}), T2I models largely fail to reject offensive content (e.g., DALLE-3 rejecting only 10.7\%). LLMs tend to over-flag gestures as offensive (e.g., Llama-3.1-70b-Instruct with high 88\% recall and low 41\% specificity). VLMs either perform at random chance (e.g., InstructBLIP) or also over-flag (e.g., gpt-4o with 87\% recall and low 42\% specificity). When interpreting the implicit meaning of gestures (\textbf{RQ2}; \S\ref{sec:results:rq2}), all models frequently resort to US-based meanings of gestures, leading to the frequently suggesting inappropriate gestures (e.g., DALLE-3 with 84.1\%, Llama-3.1-70b-Instruct 46.6\%, gpt-4o 82.8\% error rates). 
For US-centric biases (\textbf{RQ3}; \S\ref{sec:results:rq3}), all models exhibit a US-centric bias, with higher accuracy for identifying offensive gestures in US-contexts than in non-US contexts (e.g., DALLE-3: 27.78\% vs 16.67\%, gpt-4: 73.3\% vs 70\%, Llama-3.2-11b-Vision-Instruct: 65\% vs 48.3\%).

These findings, enabled by our broad-coverage and comprehensive \offHandsDataset, highlight the urgent need for more inclusive and context-aware AI systems to prevent harm and ensure equitable global applicability. We will open-source our dataset and code to encourage further research on improving cross-cultural safety, and inclusivity. 



% Using our \offHandsDataset dataset, we systematically evaluate how well current T2I systems, LLMs, and VLMs can identify, and refuse culturally offensive gestures through three key research questions. In \textbf{RQ1}, we examine whether models can accurately detect culturally offensive gestures across different regions---testing their ability to rate offensiveness levels (LLMs, VLMs) and reject inappropriate content generation (T2I).  Second, in \textbf{RQ2}, we study whether models maintain cultural awareness when gestures are described through their intended meaning in North American contexts rather than explicit names---testing if they recommend potentially offensive gestures when responding to requests like ``show a gesture meaning good luck in Vietnam'' versus ``crossing your fingers''.
% Finally, in \textbf{RQ3}, we investigate US-centric biases in models' recognition of culturally offensive gestures by comparing their performance between US and non-US contexts. 
%Second, in \textbf{RQ2}, we study how the implicitness of gesture descriptions affects models' ability to recognize and recommend gestures---for instance, whether models can still detect inappropriate gestures when they are described through their meanings in North American region rather than explicit gesture names.
%Finally, in \textbf{RQ3}, we investigate potential regional biases in these models by analyzing how their recognition of offensive content varies between US and non-US contexts. 

