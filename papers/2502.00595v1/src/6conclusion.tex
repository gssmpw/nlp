\section{Conclusion}
In this work, we introduced and explored a comprehensive framework for evaluating large language models (LLMs) as creators and simulators of text-based role-play games. Our \textbf{Game Creation (GC)} task assesses the ability of LLMs to design valid games with a \emph{BFS Validity Checker}. We further proposed a multi-round \textbf{Game Simulation (GS)} setup that prompts LLMs to plan events, generate narrative content with candidate player actions, and maintain game states.  

In addition, we presented a hybrid evaluation scheme to capture both \emph{objective} and \emph{subjective} dimensions of game quality. On the objective side, our event--state mechanics checker operates without human or LLM judgment, automatically detecting errors in event conditions and variable updates. On the subjective side, we employed a series of metrics evaluated either through an \emph{LLM-as-judge} approach or human annotation. Results across multiple models highlight that objective scores offer a stable foundation for comparison, while subjective dimensions have high variances.
% remain influenced by annotator biases and personal preferences.

% In summary, we first propose a novel two-stage data collection process, culminating in a benchmark that systematically tests LLMs’ capabilities to \emph{create} and \emph{simulate} text-based role-play games. At the core of our approach is a \emph{BFS Validity Checker} based on finite, event-driven state transitions, which verifies the mechanical soundness of each generated game without requiring human or LLM intervention. Building on this validated game set, we introduce a multi-round \emph{Game Simulation Framework} for dynamic player interactions, integrating event planning, narration, and state updates. Our hybrid evaluation suite balances objective measurements of mechanical correctness against subjective criteria such as factual/personality consistency, interestingness, and action quality. Lastly, we present a human study that compares subjective evaluations with our automatic scoring system, underscoring both the value and variability of human judgments.

While our approach captures various game design and simulation aspects, 
% several limitations remain. 
% First, our BFS Validity Checker, though fully automatic, can be computationally expensive for games with large state spaces or intricate mechanics. Second, subjective metrics—particularly those for personality and interestingness—remain susceptible to annotator bias. Third, our dataset relies on fictional characters from Wikipedia, which may not reflect all conceivable game worlds or player preferences. 
future work could focus on 
% scalable solutions for game mechanics verification, 
expanding the character pool and exploring agent-based simulation framework. Ultimately, we hope \benchmark{} will motivate further research on LLM-powered game engines that offer both mechanical consistency and engaging player experiences.

% Future work could focus on scalable solutions for game mechanics verification (e.g., partial sampling strategies or improved state pruning) and more refined protocols for subjective evaluations. Additional avenues include expanding the character pool, incorporating real-time user interactions instead of simulated players, and exploring richer mechanics (e.g., procedural generation, branching storylines) that align with advanced generative capabilities. Ultimately, we hope this benchmark will motivate further research toward building LLM-powered game engines that offer both robust mechanical consistency and deeply engaging player experiences.