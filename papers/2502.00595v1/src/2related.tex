\section{Related Work}
\benchmark{} is, to the best of our knowledge, the first benchmark designed to evaluate the capabilities of large language models (LLMs) in creating and running role-playing games (RPGs). The game creation subtask introduces a novel and challenging task for LLMs. For the game running subtask, our character-related metrics such as personality and factual consistency align with prior work on evaluating role-playing agents.

Among prior benchmarks, CharacterBox~\citep{wang2024characterbox} is most closely related to \benchmark{}, focusing on role-playing capabilities in text-based virtual worlds. However, \benchmark{} differentiates itself by introducing a game structure with verifiable mechanics, enabling deterministic LLM-free evaluations for game dynamics.

Apart from \cite{wang2024characterbox}, other role-playing benchmarks do not embed their evaluations within a virtual text-based environment, thus being more persona-centric instead of game-based. PersonaGym~\citep{samuel2024personagym} introduces PersonaScore, which evaluates LLM role-playing agents in QA tasks within sampled environments. \citet{yuan-etal-2024-evaluating} assess LLMsâ€™ understanding of characters through character profiling tasks. \citet{rpbench} and \citet{gusev2024pingpong} evaluate role-playing via multi-turn dialogues with user simulators, while InCharacter~\citep{wang-etal-2024-incharacter} employs psychometric interviews to measure character fidelity. SocialBench~\citep{chen-etal-2024-socialbench} proposes a framework for evaluating the sociality of role-playing agents, and CharacterEval~\citep{tu-etal-2024-charactereval} introduces multi-dimensional metrics for conversational role-playing agents.
Additionally, some benchmarks~\cite{gusev2024pingpong,dai2024mmrole} incorporate multimodal contexts into role-playing evaluations.