\section{Introduction}

Recent advances in large language models (LLMs) have significantly expanded the frontiers of artificial intelligence, enabling breakthroughs in areas such as content generation, conversational agents, and interactive storytelling. Among these capabilities, \emph{role-playing} has emerged as a particularly promising application, with the potential to revolutionize both entertainment—by powering next-generation interactive games—and social AI—by enabling more engaging and emotionally resonant interactions~\cite{chen2024from}.

While prior research on role-playing agents has primarily focused on their ability to \emph{simulate} a given persona at the \emph{role-level}, our work expands this scope to the \emph{game-level}, where LLMs must not only role-play a character but also \emph{create} and \emph{simulate} coherent, interactive game worlds. To evaluate this broader capability, we introduce \benchmark{}, the first benchmark designed to assess LLMs as text-based role-playing game engines. \benchmark{} consists of two core tasks: \emph{Game Creation (GC)}, where an LLM generates a structured, playable game world based on a given character, and \emph{Game Simulation (GS)}, where the model simulates gameplay through sequential interactions with a player.

Extending role-playing evaluation to the game level introduces a crucial challenge: ensuring that generated game worlds follow internally consistent and enforceable \emph{game mechanics}. Game mechanics define how the game state evolves in response to player actions and narrative events, providing structure and coherence to interactive storytelling. Unlike traditional text generation tasks, where coherence is judged subjectively, game mechanics must be evaluated objectively to verify whether a generated game is logically sound and fully playable. To address this, we propose a \emph{two-stage benchmark pipeline} centered around an automated \emph{BFS Validity Checker}. This checker formally verifies that each generated game satisfies key structural requirements—ensuring that all events are reachable, game progression follows a valid set of rules, and both success and failure endings are attainable. By automating this verification, we establish a high-quality dataset of valid games, which then serves as the test set for the GS task.

Building on this validated game set, we introduce a novel \emph{Game Simulation Framework} for dynamic, multi-round player interactions. In this framework, the LLM operates as a game engine, executing a structured simulation loop that consists of three stages per round: (1) \emph{Event Planning}, where the model determines which game events should occur; (2) \emph{Game Narration}, where it describes the unfolding story and presents a set of candidate actions to the player; and (3) \emph{Game State Updates}, where it applies the effects of events to the underlying game state. This structured approach maintains storytelling flexibility while allowing for automated robust assessments of mechanical correctness.

Beyond mechanical verification, we propose a \emph{multi-dimensional evaluation suite} to measure both \emph{objective} and \emph{subjective} aspects of game simulation quality. Objective metrics focus on game mechanics correctness, ensuring that event conditions, state transitions, and termination rules are properly followed. Subjective aspects—including content \emph{interestingness}, \emph{role-playing factual consistency}, \emph{role-playing personality consistency}, and \emph{action choice quality}—are evaluated using an \emph{LLM-as-a-judge} framework. 
% For personality assessment, we introduce two methods: a \emph{TIPI-based trait inference approach}, which estimates a character’s Big Five personality traits based on the generated narrative, and a \emph{direct consistency-check approach}, where the model explicitly evaluates alignment between the game’s narrative and the NPC’s predefined traits.

To further investigate subjective evaluation alignment, we conduct a \emph{human study} comparing human annotators’ judgments with automatic scores across multiple evaluation dimensions. Our findings reveal both alignment and discrepancies between human and LLM-based evaluations, underscoring the complexity of subjective assessment.

Overall, our work makes the following contributions:
\begin{enumerate}
    \item We introduce \benchmark{}, the first benchmark to systematically evaluate LLMs as text-based role-playing game engines, encompassing both \emph{Game Creation (GC)} and \emph{Game Simulation (GS)}.
    \item We propose an \emph{event–state-based} representation for game mechanics and a \emph{BFS Validity Checker} to automatically verify game soundness. We further develop a \emph{multi-round Game Simulation Framework} that integrates event planning, narration, and state updates, enabling automated mechanical correctness checks.
    \item We present a \emph{comprehensive evaluation suite} covering both objective metrics (mechanical correctness) and subjective dimensions (factual/personality consistency, interestingness, and action choice quality), leveraging \emph{LLM-as-a-judge} methods for subjective assessments.
    \item We conduct a \emph{human evaluation study} to analyze alignment between human and automatic assessments, providing insights into the challenges of subjective evaluation in LLM-driven game simulation.
\end{enumerate}


\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/Overview.pdf}
    \caption{An example in \benchmark{} containing two core tasks: Game Creation and Game Simulation. We omit some details for presentation purposes.}
    \label{fig:rpebench-overview}
\end{figure*}