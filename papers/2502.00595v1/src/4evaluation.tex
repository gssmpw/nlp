
\section{Evaluation Metrics}\label{sec:evaluation}

\subsection{Game Creation Evaluation}\label{sec:evaluation_gc}

In GC, we evaluate an LLM's capability to create games that have good mechanics. This task requires complex reasoning over event-state interactions that is very challenging even for human. Section~\ref{sec:gdc} offered a broad overview of the GC task. We now define it more precisely.

\noindent\textbf{Task Definition [Game Creation]}  
Given a fictional character \(\mathcal{C}\) and related Wikipedia information \(\mathcal{R}\), an LLM must create a game \(\mathcal{G}\) that follows a predefined format \(\mathcal{J}\).

In \benchmark{}, 100 fictional characters are used, each with an associated Wikipedia page (\(\mathcal{R}\)), facilitating future expansion of the character pool. The game \(\mathcal{G}\) must conform to the structure \(\mathcal{J}\) given in Section~\ref{sec:gd}. We provide each LLM with a 5-shot prompt to generate one game per character. 

\begin{algorithm}[!ht]
   \caption{BFS Validity Checker}
   \label{alg:bfs-checker}
\begin{algorithmic}
   \STATE \textbf{Input:} Events \(\mathcal{E}\), each with entering and success conditions, plus success and fail effects; A state \(S_0\) with initial values for all variables; An integer \(M\) indicating the maximum number of states to be explored.

   \FUNCTION{\(\textit{isValid}(\mathcal{E},S_0,M)\)}
   \STATE Initialize a queue \(\mathcal{Q}\) and enqueue \(S_0\).
   \STATE Initialize a visited set \(\mathcal{V} = \{S_0\}\).
   \STATE Initialize a triggered-event set \(\mathcal{T} = \varnothing\).
   \STATE \(\textit{successFound} = \texttt{False};~ \textit{loseFound} = \texttt{false}\)

   \REPEAT
      \STATE \(S = \mathcal{Q}.\text{dequeue}()\)
      \IF{\(|\mathcal{V}| > M\)}
         \STATE \textbf{break} \quad\# \textit{Reached maximum search limit}
      \ENDIF
      
      \STATE \(\textit{availableEvents} = \{ e \in \mathcal{E} : e.\mathit{enterCond}(S) \}\)
      \FOR{each \(e \in \textit{availableEvents}\)}
         \STATE \(\mathcal{T} = \mathcal{T} \cup \{e\}\) \quad\# \textit{Mark event as triggered}

         \STATE \(S' = \textit{e.applyEffect}(S,\mathit{e.successCond}(S))\)
         
        \STATE \(\textit{successFound} \;|= \textit{e.isSuccessTermination}(S')\)
        \STATE \(\textit{loseFound} \;|= \textit{e.isLosingTermination}(S')\)

         \IF{\(S' \notin \mathcal{V}\)}
            \STATE \(\mathcal{Q}.\text{enqueue}(S')\);~\(\mathcal{V} = \mathcal{V} \cup \{S'\}\)
         \ENDIF
      \ENDFOR
   \UNTIL{\(\mathcal{Q}\) is empty}

   \STATE \textbf{return} 
      \(\bigl(\mathcal{T} = \mathcal{E}\bigr) \;|\; \textit{successFound} \;|\; \textit{loseFound}\)
   \ENDFUNCTION
\end{algorithmic}
\end{algorithm}

\paragraph{BFS Validity Checker}  
Once the output is confirmed to be valid JSON, we perform a BFS-based validity check (Algorithm~\ref{alg:bfs-checker}). Based on our event--state design, we employ BFS to decide if a game is valid. Starting from the initial state, we repeatedly check which events are available , apply success or failure effects accordingly, and track whether at least one success and one losing state can be reached. We stop when no new states can be discovered or when the search exceeds 10{,}000{,}000 states. A game is valid if every event is triggered at least once, and both success and losing termination conditions are achievable.

\paragraph{Metrics}  
For GC evaluation, we report the format-check pass rate (\textbf{FCR}) and the valid-check pass rate (\textbf{VCR}) as our main metrics, reflecting how reliably LLMs follow the prescribed JSON format and produce valid game mechanics. In order to examine fine-grained failures for the validity check, we include three additional ratios:
\begin{equation*}
\begin{aligned}
     \textbf{w. Success} &=\frac{\textit{\# games with successFound}}{\textit{\# games pass the format check}}\\
     \textbf{w. Lose}&=\frac{\textit{\# games with failFound}}{\textit{\# games pass the format check}}\\
     \textbf{Reachability}&=\frac{\textit{\# games without unreachable events}}{\textit{\# games pass the format check}}
\end{aligned}
\end{equation*}

\subsection{Game Simulation Evaluation}

Given a valid game, the GS task requires an LLM to simulate the game for a player. We introduce a multi-round simulation framework, based on which a comprehensive description of evaluation metrics is presented.

\paragraph{Game Simulation Framework}  
The simulation proceeds in multiple rounds of interaction with a (real or simulated) player. Before the first round, the LLM is given the complete game information and output instructions. Each round thereafter, the LLM outputs:
\begin{enumerate}
    \item \textbf{Event Plan}: A list of events occurring this round. Each entry specifies whether the event is starting (\texttt{start}) or ending (\texttt{end}); if ending, an \texttt{outcome} is either \texttt{success} or \texttt{failure}.
    \item \textbf{Game Narration}: A narrative description of the current round, concluding with three candidate actions for the player character. We prompt models to follow a play-script format for readability but do not enforce it during evaluation.
    \item \textbf{Game State}: The updated state variables after applying effects of any events that ended this round.
\end{enumerate}

\paragraph{Evaluation Metrics}  
Our evaluation covers multiple dimensions, scored over the trajectory of interactions. A simulated player selects one of the candidate actions at random each round.

\begin{enumerate}
    \item \textbf{Length}: We count words in the game narration (excluding candidate actions). Although no ideal length is defined, our prompt suggests remaining under 200 words to maintain brevity without sacrificing creativity. We report the average length per round.
    \item \textbf{Action Quality}: Using an LLM judge (prompt in Appendix~\ref{app:eval_prompt}), we rate the three candidate actions based on diversity, relevance, and clarity. The judge outputs a 1--5 score, normalized to \([0,1]\) via \(\frac{s-1}{4}\). We average scores across all rounds.
    \item \textbf{Interestingness}: An LLM judge evaluates how engaging the round’s narration is, assigning a 1--5 score also normalized to \([0,1]\). We average this score across the entire trajectory.
    \item \textbf{Role-Playing Factual Consistency}: We compare the game narration against each fact in the main NPC's fact list. An LLM judge labels each fact as \textit{align}, \textit{contradict}, or \textit{neutral}. We report the ratio \(\frac{\#\textit{align}}{\#\textit{align} + \#\textit{contradict}}\).
    \item \textbf{Role-Playing Personality Consistency}: We prompt an LLM to infer the main NPC's Big Five traits from the generated content, then compare these to the game definition. We employ the Ten-Item Personality Inventory (TIPI)~\cite{gosling2003very}, following previous work on eliciting LLM-based personality assessments of public figures~\cite{cao2024large}. In addition to TIPI, we also considered a direct approach that explicitly evaluates alignment between the game’s narrative and the NPC’s predefined traits. We use TIPI-based score in the main paper, with details on the direct approach and comparisons in the Appendix~\ref{app:eval_prompt} and ~\ref{app:human_eval}.
    \item \textbf{Game Mechanics}: We perform a fully automatic check for the following errors:
    \begin{enumerate}
        \item \textbf{Event Condition Error}: An event triggers when its entering condition is not met, or the outcome (success/failure) does not match the current state.
        \item \textbf{Variable Update Error}: The state variables do not update according to event effects.
    \end{enumerate}
    The main game mechanic metric we adopt is the round-level accurate rate $\textbf{MEC} = \frac{\# \textit{Rounds with no errors}}{\# \textit{Rounds}}.$
    % \begin{equation}
    %     \textbf{MechanicScore} = \frac{\# \textit{Rounds with no errors}}{\# \textit{Rounds}}.
    % \end{equation}
    We average the mechanic score over all games.
    
    For a more fine-grained analysis, we process events in the \textit{Event Plan} sequentially at each round and calculate an error rate for each error type,
    \begin{equation}
    \begin{aligned}
        \textbf{ECE}_t &= \frac{\# \textit{Event condition errors}}{\# \textit{Events}}\\
        \textbf{VUE}_t &= \frac{\# \textit{State variables incorrectly updated}}{\# \textit{State variables}}
    \end{aligned}
    \end{equation}    
    We average ECE and VUE over all rounds of all games. By design, all these metrics require no LLM judge. 
\end{enumerate}

