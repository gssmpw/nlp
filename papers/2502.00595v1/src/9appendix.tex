\section{Game JSON Structure in \benchmark{}}\label{app:game_json}

As introduced in Section~\ref{sec:gd}, each game in \benchmark{} is represented by a JSON dictionary. Figures~\ref{lst:json-schema} and \ref{fig:trait-schema}--\ref{fig:pre-event-schema} provide the complete schema and its referenced object definitions. Below, we clarify naming discrepancies between this JSON specification and the terminology used in the main article, and also highlight a few design details omitted for brevity.

\paragraph{Naming Discrepancies.}
The JSON schema in Figure~\ref{lst:json-schema} has property names slightly different from those in Figure~\ref{fig:rpebench-overview} from the main article. For clarity, we list them side by side as ``JSON schema name --- main article name'':

\begin{enumerate}
    \item \texttt{player\_name} --- Player Character / Name
    \item \texttt{player\_description} --- Player Character / Description
    \item \texttt{main\_npc\_description / text} --- Main NPC /Description
    \item \texttt{main\_npc\_description / big5\_personality\_traits} --- Main NPC / Personality
    \item \texttt{main\_npc\_description / additional\_facts} --- Main NPC / Facts
    \item \texttt{state\_variables} + \texttt{hidden\_variables} --- State Variables
    \item \texttt{pre\_event\_checks} --- Termination Conditions
\end{enumerate}

For consistency, the appendices continue to use the names from the main article unless otherwise specified. Although \texttt{state\_variables} and \texttt{hidden\_variables} are separate fields in the JSON schema, they collectively represent the State Variables described in the main text. In our design, \texttt{hidden\_variables} (unlike \texttt{state\_variables}) are not displayed to players; however, this distinction does not impact the benchmark evaluations and is thus not emphasized in the main article.

We also require \texttt{hidden\_variables} to include at least two special Boolean flags, \texttt{has\_succeeded} and \texttt{has\_failed}, which interact with \texttt{pre\_event\_checks} (a list of two check objects \texttt{If Succeeded} and \texttt{If Failed}). Each check object includes a \texttt{condition} (a Boolean expression over the state variables) and an \texttt{effect} that sets \texttt{has\_succeeded=1} or \texttt{has\_failed=1}, if not already set\footnote{Some games directly set \texttt{has\_succeeded} or \texttt{has\_failed} in other event effects, leaving effects of \texttt{pre\_event\_checks} empty.}. Conceptually, these properties mirror the Termination Conditions in the main article.

\paragraph{Explanatory Content.}
Several text fields in the JSON schema contain descriptive or explanatory information that we omit from the main article, such as:
\begin{enumerate}
    \item \texttt{\$def/trait/description}: Describes the personality trait score in natural language.
    \item \texttt{\$def/scene\_object/background\_description}: Describes the scene.
    \item \texttt{\$def/variable\_object/description}: Describes a particular state variable.
    \item \texttt{\$def/event\_object/explanations}: Explains event effects.
    \item \texttt{\$def/pre\_event\_check\_object/explanation}: Explains the termination condition check.
\end{enumerate}
Although these fields do not affect our validity checks, they provide additional context for LLMs and are included in prompts given to LLMs during game simulation.

\paragraph{Game Scenes in the BFS Validity Check.}
Because each event references exactly one scene (Figure~\ref{fig:event-schema}), we also verify that all declared scenes are referenced by at least one event. This check is straightforward and independent of the BFS procedure, so it is omitted from the main article for simplicity.

\begin{figure*}[!htp]
\centering
\small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "title": "Game Configuration",
  "type": "object",
  "required": [
    "game_world",
    "player_name",
    "player_description",
    "main_npc_name",
    "main_npc_description",
    "game_objectives",
    "scenes",
    "state_variables",
    "hidden_variables",
    "events",
    "pre_event_checks"
  ],
  "properties": {
    "game_world": { "type": "string" },
    "player_name": { "type": "string" },
    "player_description": { "type": "string" },
    "main_npc_name": { "type": "string" },
    "main_npc_description": {
      "type": "object",
      "required": [ "text", "big5_personality_traits", "additional_facts" ],
      "properties": {
        "text": { "type": "string" },
        "big5_personality_traits": { "$ref": "#/$defs/big5_traits" },
        "additional_facts": { "type": "array", "items": { "type": "string" } }
      },
      "additionalProperties": false
    },
    "game_objectives": { "type": "string" },
    "scenes": { "type": "array", "items": { "$ref": "#/$defs/scene_object" }
    },
    "state_variables": { "type": "array", "items": { "$ref": "#/$defs/variable_object" } },
    "hidden_variables": {
      "type": "array",
      "minItems": 2,
      "items": { "$ref": "#/$defs/variable_object" },
      "contains": { "properties": { "value_name": { "enum": [ "has_succeeded", "has_failed" ] } } }
    },
    "events": { "type": "array", "items": { "$ref": "#/$defs/event_object" } },
    "pre_event_checks": { "type": "array", "items": { "$ref": "#/$defs/pre_event_check_object" } },
    "source": { "type": "string" }
  },
  "additionalProperties": false,
}
\end{lstlisting}
\caption{JSON Schema for Game Configuration}
\label{lst:json-schema}
\end{minipage}
\end{figure*}

\begin{figure}[!ht]
\centering
\small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "$defs": {
    "trait": {
      "type": "object",
      "required": ["rate", "description"],
      "properties": {
        "rate": { "type": "number" },
        "description": { "type": "string" }
      },
      "additionalProperties": false
    }
  }
}
\end{lstlisting}
\caption{\texttt{trait} object schema}
\label{fig:trait-schema}
\end{minipage}
% \end{figure}

% \begin{figure}[!ht]
% \centering
% \small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "$defs": {
    "big5_traits": {
      "type": "object",
      "required": [
        "openness",
        "conscientiousness",
        "extraversion",
        "agreeableness",
        "neuroticism"
      ],
      "properties": {
        "openness": { "$ref": "#/$defs/trait" },
        "conscientiousness": { "$ref": "#/$defs/trait" },
        "extraversion": { "$ref": "#/$defs/trait" },
        "agreeableness": { "$ref": "#/$defs/trait" },
        "neuroticism": { "$ref": "#/$defs/trait" }
      },
      "additionalProperties": false
    }
  }
}
\end{lstlisting}
\caption{\texttt{big5\_traits} object schema}
\label{fig:big5-schema}
\end{minipage}
% \end{figure}

% \begin{figure}[!ht]
% \centering
% \small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "$defs": {
    "scene_object": {
      "type": "object",
      "required": [ "scene_name", "unique_id", "background_description", "scene_type" ],
      "properties": {
        "scene_name": { "type": "string" },
        "unique_id": { "type": "string" },
        "background_description": { "type": "string" },
        "scene_type": { "type": "string" }
      },
      "additionalProperties": false
    }
  }
}
\end{lstlisting}
\caption{\texttt{scene\_object} schema}
\label{fig:scene-schema}
\end{minipage}
\end{figure}

\begin{figure}[!ht]
\centering
\small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "$defs": {
    "variable_object": {
      "type": "object",
      "required": [ "value_name", "unique_id", "description", "min_value", "max_value" ],
      "properties": {
        "value_name": { "type": "string" },
        "unique_id": { "type": "string" },
        "description": { "type": "string" },
        "initial_value": { "type": "string" },
        "min_value": { "type": "string" },
        "max_value": { "type": "string" }
      }, "additionalProperties": false
    }
  }
}
\end{lstlisting}
\caption{\texttt{variable\_object} schema}
\label{fig:variable-schema}
\end{minipage}
% \end{figure}

% \begin{figure}[!ht]
% \centering
% \small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "$defs": {
    "event_object": {
      "type": "object",
      "required": [ "event_name", "unique_id", "scene", "entering_condition", "succeed_condition", "succeed_effect", "fail_effect" ],
      "properties": {
        "event_name": { "type": "string" },
        "unique_id": { "type": "string" },
        "scene": { "type": "array", "items": { "type": "string" } },
        "entering_condition": { "type": "array", "items": { "type": "string" } },
        "succeed_condition": { "type": "array", "items": { "type": "string" } },
        "succeed_effect": { "type": "array", "items": { "type": "string" } },
        "fail_effect": { "type": "array", "items": { "type": "string" } },
        "explanations": { "type": "string" }
      }, "additionalProperties": false
    }
  }
}
\end{lstlisting}
\caption{\texttt{event\_object} schema}
\label{fig:event-schema}
\end{minipage}
% \end{figure}

% \begin{figure}[!t]
% \centering
% \small
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=json]
{
  "$defs": {
    "pre_event_check_object": {
      "type": "object",
      "required": [ "check_name", "unique_id", "description", "condition", "effect" ],
      "properties": {
        "check_name":  { "type": "string" },
        "unique_id":   { "type": "string" },
        "description": { "type": "string" },
        "condition": { "type": "array", "items": { "type": "string" } },
        "effect": { "type": "array", "items": { "type": "string" } },
        "explanation": { "type": "string" }
      }, "additionalProperties": false
    }
  }
}
\end{lstlisting}
\caption{\texttt{pre\_event\_check\_object} schema}
\label{fig:pre-event-schema}
\end{minipage}
\end{figure}

\section{Game Creation Prompt}\label{app:gc_prompt}
For the Game Creation (GC) task, we use the prompt shown below. It references the Wikipedia content of the chosen main NPC (\texttt{\{wikicontent\}}) and the JSON schema defined in Appendix~\ref{app:game_json} (\texttt{\{schema\}}). The full text of this schema is provided to the model so it can generate a well-structured JSON output.
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Here is a character description:
{wikicontent}

Based on this character, create a detailed game scenario exactly following JSON structure of previous examples and the following schema:
{schema}

## Guidelines
- All numerical values should use consistent ranges (e.g., 0-100)
- Events should have clear cause-and-effect relationships
- Scene progression should depend on variable thresholds
- Include both mandatory and optional events
- Create meaningful connections between variables
- Balance difficulty and achievability
- Ensure all IDs follow consistent formatting (P### for checks, S### for scenes, V### for state variables, H### for hidden variables, E### for events)
- Include proper fail states and success conditions
- Make sure all scenes are specific locations
- Create logical progression paths through the game

Format the response as a single JSON object with all fields properly nested. Must ensure all arrays and objects are properly closed and formatted.
\end{lstlisting}
\end{minipage}
\end{center}

\paragraph{5-Shot Prompt} To guide LLMs more effectively, we supply five example JSON games prior to the main creation prompt. Because each game JSON can be quite lengthy, stacking them directly after the prompt may cause the model to overlook important details in the instruction. Instead, we present the five-shot examples as sequential conversation entries, followed by the actual creation prompt. The resulting conversation structure is illustrated below.
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
    USER: Give me an example game JSON.
    ASSISTANT: {EXAMPLE_1}
    USER: Give me an example game JSON.
    ASSISTANT: {EXAMPLE_2\}
    USER: Give me an example game JSON.
    ASSISTANT: {EXAMPLE_3\}
    USER: Give me an example game JSON.
    ASSISTANT: {EXAMPLE_4\}
    USER: Give me an example game JSON.
    ASSISTANT: {EXAMPLE_5}
    USER: {Prompt for Game Creation}
\end{lstlisting}
\end{minipage}
\end{center}

\section{Evaluation Prompts and Detailed Score Calculations}\label{app:eval_prompt}
We employ a consistent three-part format for most evaluation prompts: an instruction section, a JSON schema specifying the output format, and an example response. To keep this appendix concise, we omit the JSON schemas and example responses when the instruction text alone clearly explains the expected output structure. Below, we detail the prompts and score calculations for four metrics: Main NPC Factual Consistency (\textbf{FAC}), Main NPC Personality Consistency (\textbf{PER}), Interestingness (\textbf{INT}), and Action Choice Quality (\textbf{ACT}).
\subsection{Main NPC Factual Consistency (FAC)}
The prompt below assesses how closely the generated game content aligns with each fact about the main NPC. We concatenate all LLM-generated game narration across the multi-round trajectory into \texttt{game\_content}\footnote{Event Plan and State Variables are omitted because they are not visible to players.}.
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
You are given a piece of narrative game content and a set of facts about a specific non-player character (NPC). Your task is to analyze whether each fact is supported, contradicted, or not addressed by the provided game content. For each fact, determine one of the following judgements based solely on the given game content:
- "align": The game content supports or is consistent with the fact.
- "contradict": The game content directly conflicts with or contradicts the fact.
- "neutral": The game content is unrelated or does not provide enough information to judge the fact.
Please disregard prior knowledge and analyze the NPC purely based on the game content and the facts.

**NPC**: {main_npc_name}

**Game Content**:

{game_content}

**Facts**

{main_npc_facts}

**Output Format**:  
Return the results as a JSON array, where each element is an object with:
- fact_id: the corresponding fact's ID.
- judgement: one of "align", "contradict", or "neutral".
- explanation: a brief explanation for your judgment, referencing specific parts of the game content if applicable.
The return json array should follow this json schema:
{schema}

**Example Response**:
{example}
\end{lstlisting}
\end{minipage}
\end{center}
The judge assigns one of three labels for each fact: ``align,'' ``contradict,'' or ``neutral.'' The final trajectory-level FAC score is computed as
\begin{equation}
    \textbf{FAC}_\text{traj} = \frac{\# \text{align}}{\# \text{align} + \# \text{contradict}},
\end{equation}
and we then average over all trajectories:
\begin{equation}
    \textbf{FAC} = \frac{\sum_{\text{traj}} \textbf{FAC}_\text{traj}}{\# \text{trajectories}}.
\end{equation}

\subsection{Main NPC Personality Consistency (PER)}\label{app:per_eval}
\paragraph{TIPI PER Score} As described in the main article, we derive the PER score using a Ten-Item Personality Inventory (TIPI) approach~\cite{gosling2003very,cao2024large}, prompting the LLM judge to rate each of ten statements and then converting the ratings into Big Five trait scores.
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
You will be given information about a character. Here are a number of personality traits that may or may not apply to the character. Please write a number to each statement to indicate the extent to which you agree or disagree with that statement. You should rate the extent to which the pair of traits applies to the character, even if one characteristic applies more strongly than the other.

For the ratings:
- 1: Disagree strongly
- 2: Disagree moderately
- 3: Disagree a little
- 4: Neither agree nor disagree
- 5: Agree a little
- 6: Agree moderately
- 7: Agree strongly

Please give your ratings for the following 10 statements.

I see the character as:
A. Extraverted, enthusiastic.
B. Critical, quarrelsome.
C. Dependable, self-disciplined.
D. Anxious, easily upset.
E. Open to new experiences, complex.
F. Reserved, quiet.
G. Sympathetic, warm.
H. Disorganized, careless.
I. Calm, emotionally stable.
J. Conventional, uncreative

Please return ratings for all 10 traits in a dictionary following this schema:
{schema}

Please give your ratings for the following character.
{character}
\end{lstlisting}
\end{minipage}
\end{center}
Here, \texttt{character} consists of the main NPC name and the concatenated LLM-generated game narration sections. According to \citet{gosling2003very}, we use the following formulas to calculate personality trait scores,
\begin{equation}
    \begin{aligned}
        &\text{Openness: }&o_{tipi} =& E + 8 - J\\
        &\text{Conscientiousness: }&c_{tipi} =& C + 8 - H\\
        &\text{Extroversion: }&e_{tipi} =& A + 8 - F\\
        &\text{Agreeableness: }&a_{tipi} =& G + 8 - B\\
        &\text{Neuroticism: }&n_{tipi} =& I + 8 - D\\
    \end{aligned}
\end{equation}

To compute the personality consistency, we compare the above scores, after being scaled to $[1,5]$, with the main NPC personality specifications in the game JSON,
\begin{equation}
    d_{\{o,c,e,a,n\}} = \left|\frac{\{o,c,e,a,n\}_{tipi} + 1}{3} - \{o,c,e,a,n\}_{game}\right|.
\end{equation}
The PER score is the squared sum of these differences, normalized to $[0, 1]$,
\begin{equation}
\begin{aligned}
    \textbf{PER}_{\text{traj}} &= 1 - \frac{\sqrt{\sum_{x\in\{o,c,e,a,n\}} d_x^2}}{4\sqrt{5}}\\
    \textbf{PER} &= \frac{\sum_{\text{traj}} \textbf{PER}_\text{traj}}{\# trajectories}
\end{aligned}.
\end{equation}

\paragraph{Direct Evaluation of Personality Consistency}
We also experiment with a direct evaluation approach (referred to as \textbf{PER}$^d$), which instructs the LLM judge to provide a 1--5 alignment rating for each of the five personality traits. 
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Assign a score from 1 to 5 to indicate how well the game narrative aligns with the main NPC's personality traits:
- Many Conflicts (1): The narrative frequently contradicts the NPC's personality.
- Some Conflicts (2): The narrative shows noticeable inconsistencies with the NPC's personality.
- Neutral (3): The narrative is only partially aligned or does not strongly reflect the NPC's personality.
- Strong Alignment (4): The narrative closely matches the NPC's personality, with only minor deviations or uncertainties.
- Perfect Alignment (5): The narrative flawlessly reflects the NPC's personality in every aspect, with no contradictions.

Please give one score for each personality trait, and provide a brief explanation for each score.

Game narrative:
{game_content}

NPC personality:
{npc_personality}

Please return a score as a json object following this schema:
{schema}
\end{lstlisting}
\end{minipage}
\end{center}
Here, \texttt{npc\_personality} consists of the Big Five personality traits in the game JSON. We compute the final score by averaging the normalized scores across all traits and, subsequently, across all trajectories. We deter discussions of results from this approach to Appendix~\ref{app:human_eval}, where we compare both TIPI estimations and direct evaluation results from LLM judges and human annotators. We refer this score as \textbf{PER$^d$} for the remaining of this article. 

\subsection{Interestingness (INT)}
We prompt an LLM judge to rate the interestingness of the generated content on a 1--5 scale.
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Your task is to evaluate the **interestingness** of the following game content. Please give a score from 1 (least interesting) to 5 (most interesting), with a brief explanation of your rationale.


[[start of game content]]
{game_content}
[[end of game content]]

Please return your evaluation score in a json dictionary with the following format:
{schema}

Example output:
{example}
\end{lstlisting}
\end{minipage}
\end{center}
We normalize the final score to \([0, 1]\), sum over rounds within a trajectory and then average:
\begin{equation}
\begin{aligned}
    \textbf{INT}_{\text{traj}} &= \frac{int-1}{4}\\
    \textbf{INT} &= \frac{\sum_{\text{traj}} \textbf{INT}_\text{traj}}{\# trajectories}
\end{aligned}.
\end{equation}
\subsection{Action Choice Quality (ACT)}
At each round, the LLM judge scores three candidate actions on three rubrics: Diversity, Relevance, and Understandability, each on a 1--5 scale with the following prompt. 
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Please act as an experienced RPG game player and evaluate the choices provided by the game engine, given the user-AI interaction history and the general game instruction outlining the basic game settings. You will be given the general game instruction, the interaction history, and the current choices offered by the game engine. Evaluate the choices based on the following rubric:

Please assess the choices provided by the game engine based on this rubric:

[[start of rubric]]
{rubric}
[[end of rubric]]

[[start of general game instruction]]
{game}
[[end of general game instruction]]

[[start of history]]
{history}
[[end of history]]

Please assess the choices provided by the game engine:

[[start of choices]]
{choices}
[[end of choices]]

Your output should be a JSON object structured as follows:
{
    "reason": <your reasoning here>
    "score": <score from 1 to 5 based on the rubric provided>
}
You must NOT output anything else other than this JSON object.
\end{lstlisting}
\end{minipage}
\end{center}
We provide in the prompt above the game JSON~(\texttt{game}), game history up to the current round~(\texttt{history}), and three candidate choices~(\texttt{choices}). We present the following paragraphs in \texttt{rubric} for Diversity, Relevance and Understandability respectively.
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Diversity: Does the set of choices provide distinct and varied options for the player?
1: The choices are nearly identical, offering no meaningful differences between them.
2: The choices have slight variations but are mostly redundant, leading to a limited sense of variety.
3: The choices exhibit some diversity but may still overlap in intent or outcome.
4: The choices are mostly distinct and provide meaningful differences that allow the player to explore different paths.
5: The choices are highly diverse, with each option offering unique and creative directions for the player.
\end{lstlisting}
\end{minipage}
\end{center}
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Relevance: Are the choices appropriate and contextually aligned with the story and scene?
1: The choices are entirely irrelevant, disconnected from the scene or story, and break immersion.
2: The choices have limited relevance, with some alignment to the story but containing jarring or out-of-place elements.
3: The choices are moderately relevant, generally aligning with the story but occasionally introducing inconsistencies.
4: The choices are mostly relevant, fitting well within the context and contributing meaningfully to the story.
5: The choices are fully relevant, seamlessly integrated into the story and enhancing the narrative experience.
\end{lstlisting}
\end{minipage}
\end{center}
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
Understandability:  Are the choices clear, concise, and easy to understand for the player?
1: The choices are confusing, overly complex, or poorly worded, making them difficult to interpret.
2: The choices are somewhat understandable but may include ambiguous language or unnecessary complexity.
3: The choices are moderately clear, with minor ambiguities that require some interpretation.
4: The choices are clear and concise, easy to read, and free of significant ambiguity.
5: The choices are exceptionally clear and well-written, making them effortless to understand and act upon
\end{lstlisting}
\end{minipage}
\end{center}

We average these three rubric scores to obtain $act$, then normalize via $(act - 1)/4$. Trajectories are evaluated by averaging per-round scores, and we then take the mean across all trajectories:
\begin{equation}
\begin{aligned}
    \textbf{ACT}_{\text{round}} &= \frac{act-1}{4}\\
    \textbf{ACT}_{\text{traj}} &= \frac{\sum_{\text{round}} \textbf{ACT}_\text{round}}{\# rounds}\\
    \textbf{ACT} &= \frac{\sum_{\text{traj}} \textbf{ACT}_\text{traj}}{\# trajectories}
\end{aligned}.
\end{equation}




\section{Human Evaluation Details}\label{app:human_eval}
\subsection{Interface Layout}
Figure~\ref{fig:human-eval} shows a screenshot of our human evaluation interface. Although it is cut off due to display size, the four main components are visible: \textbf{Text RPG Information}, \textbf{NPC Information}, \textbf{Dialog History}, and \textbf{Responses}. As discussed in Appendix~\ref{app:per_eval}, we use two interfaces: one for TIPI-based personality estimation and one for direct personality-consistency evaluation. These interfaces only differ in how the \textbf{NPC Information} and \textbf{Responses} sections are presented. To help annotators remain focused when assessing a multi-round trajectory, each round in a trajectory is annotated separately by the same annotator.

\noindent\textbf{Text RPG Information:}  
Annotators see the Game World description, the player character’s name and description, and the overall game objective. This information persists throughout the trajectory.

\noindent\textbf{Dialog History:}  
We show the game trajectory up to the current round, including the model’s narration and three candidate actions (boldfaced). One of these actions, selected at random, is displayed on the right side. This component updates every round to reflect the new content.

\noindent\textbf{NPC Information:}  
For TIPI-based personality estimation, we present only the NPC’s name and facts (omitting personality traits so they can be inferred through the TIPI questions). In the direct-evaluation interface, the main NPC personality traits are included here.

\noindent\textbf{Responses:}  
This section poses natural-language questions to gather human judgments on subjective dimensions. It differs slightly between TIPI-based and direct-evaluation interfaces, as detailed below.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/ScreenShot.png}
    \caption{Screenshot of the human evaluation interface.}
    \label{fig:human-eval}
\end{figure}

\subsection{Evaluation Questions}
\subsection{Evaluation Questions}
Our human evaluation asks annotators to rate various subjective aspects. Questions A--D appear every round in both the TIPI and direct-evaluation interfaces:
\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
A. Please give a score (1-5) to indicate how interesting the game narrative is.

B. Do you think all the candidate actions are valid based on the game narrative? - 0 (no) - 1 (yes)
    
C. Are candidate choices different enough from each other, or are they essentially the same? - 0 (same) - 1 (different)
    
D. Please give a score (1-5) to measure whether the game narrative is consistent with the given facts about the main NPC? 
    - 1 has many conflicts
    - 2 has some conflicts
    - 3 neutral
    - 4 matches the description
    - 5 perfectly matches the description
\end{lstlisting}
\end{minipage}
\end{center}
These ratings inform the INT, ACT, and FAC metrics as follows:
\begin{equation}
    \begin{aligned}
        \textbf{INT}_\text{round} &= \frac{A - 1}{4},\\
        \textbf{ACT}_\text{round} &= \frac{B + C}{2},\\
        \textbf{FAC}_\text{round} &= \frac{D - 1}{4}.
    \end{aligned}
\end{equation}
Here, Question B corresponds to Relevance and Understandability in the ACT automatic evaluation, while Question C corresponds to Diversity. We average these round-level scores to obtain a trajectory-level score, then average across all trajectories.

\paragraph{Personality Consistency Questions (E1 and E2).}
We measure PER using two different question sets:
\begin{itemize}
    \item \textbf{E1: TIPI Estimation.}  
    Shown only once per trajectory (at the final round of the TIPI interface), requiring annotators to assess the entire trajectory.  
    \item \textbf{E2: Direct Evaluation.}  
    Appears at every round in the direct-evaluation interface.
\end{itemize}

Both methods yield PER scores analogous to the automatic evaluations in Appendix~\ref{app:per_eval}.

\begin{center}
\begin{minipage}{0.95\textwidth}
\begin{lstlisting}[language=plaintext, frame=none, numbers=none]
E1. Here are a number of personality traits that may or may not apply to the character. Please write a number to each statement to indicate the extent to which you agree or disagree with that statement, based ONLY on the game narratives. You should rate the extent to which the pair of traits applies to the character, even if one characteristic applies more strongly than the other. Use a score range of 1-7:
    - 1: Disagree strongly
    - 2: Disagree moderately
    - 3: Disagree a little
    - 4: Neither agree nor disagree
    - 5: Agree a little
    - 6: Agree moderately
    - 7: Agree strongly

    I see the main NPC as
    A. Extraverted, enthusiastic.
    B. Critical, quarrelsome.
    C. Dependable, self-disciplined.
    D. Anxious, easily upset.
    E. Open to new experiences, complex.
    F. Reserved, quiet.
    G. Sympathetic, warm.
    H. Disorganized, careless.
    I. Calm, emotionally stable.
    J. Conventional, uncreative

E2. Please give a score (1-5) to measure whether the game narrative is consistent with the given facts about the main NPC?
    - 1 has many conflicts
    - 2 has some conflicts
    - 3 neutral
    - 4 matches the description
    - 5 perfectly matches the description
\end{lstlisting}
\end{minipage}
\end{center}
\subsection{Annotation Setup}
We recruited 15 human annotators. Each trajectory is annotated at the round level, resulting in two annotations per interface type and therefore four total annotations per trajectory. We ensure that each annotator encounters any given trajectory only once, regardless of interface type. Consequently, each trajectory ends up with four sets of INT, ACT, and FAC scores, and two sets of PER and \textbf{PER}$^d$ scores. We take the mean over all trials to produce the final reported values. For inter-annotator agreement (Table~\ref{tab:gs_corr}), we randomly divide the collected annotations into two groups and compare their scores.

\subsection{PER vs. PER$^d$ Evaluation Results}
\begin{table*}[!ht]
\centering
\begin{tabular}{lrrrrrr}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{PER (Subset)} & \multicolumn{2}{c}{PER$^d$ (Subset)} & \multicolumn{1}{c}{PER (Full)} & \multicolumn{1}{c}{PER$^d$ (Full)}\\ 
& \multicolumn{1}{c}{Auto} & \multicolumn{1}{c}{Human} & \multicolumn{1}{c}{Auto} & \multicolumn{1}{c}{Human} & \multicolumn{1}{c}{Auto} & \multicolumn{1}{c}{Auto}\\
\midrule
Claude 3.5 Sonnet & 0.729 & 0.648 & 0.768 & \textbf{0.832} & 0.589 & 0.738 \\
Deepseek V3 & 0.742 & 0.645 & 0.750 & \underline{0.826} & 0.583 & \textbf{0.778}\\
Gemini 1.5 Pro & 0.740 & 0.648 & \textbf{0.800} & 0.769 & \underline{0.596} & \underline{0.777}\\
Gemini 2.0 Flash Exp & 0.737& \underline{0.651} & 0.707 & 0.769 &\textbf{0.598} & 0.750\\
GPT 4o & 0.711 & \textbf{0.667} & 0.780 & 0.724 & 0.585 & 0.768 \\
GPT 4o mini & \textbf{0.753}  & 0.648 & \underline{0.788} & 0.735 & 0.588 & 0.763\\
Llama 3.1 70B & \underline{0.744}  & 0.627 & 0.768 & 0.752 & 0.586 & 0.765\\
Llama 3.3 70B & 0.739 & 0.640 & 0.739 & 0.755 & 0.585 &0.774\\
\bottomrule
\end{tabular}
\caption{PER and PER$^d$ results from automatic and human evaluation on a subset of 20 games, and automatic evaluation on the full set of games.}
\label{tab:per_evaluation_results}
\end{table*}


\begin{table}[!ht]
    \centering
    \begin{tabular}{cc|rrr}
    \toprule
    \multicolumn{2}{c|}{Comparisons} & Pearson & Kendall & MAD \\
    \midrule
    \multirow{1}{*}{Auto-Auto Agreement} & PER Auto - PER$^d$ Auto & 0.013 & 0.109 & 0.037 \\
    \midrule
    \multirow{3}{*}{Auto-Human Agreement} & PER Auto - PER Human     & -0.691 & -0.429 & 0.090\\
    & PER$^d$ Auto - PER$^d$ Human &-0.297 & -0.255 & 0.047\\
    \midrule
    \multirow{3}{*}{Human-Human Agreement} &PER Human - PER Human & -0.310 & -0.286 & 0.023\\
    &PER$^d$ Human - PER$^d$ Human & 0.649 & 0.143 & 0.035\\
    &PER Human - PER$^d$ Human &-0.175&-0.143 & 0.124\\
    \bottomrule
    \end{tabular}
    \caption{Agreement analysis for PER and PER$^d$ scores. We present Pearson correlation coefficient (Pearson), Kendall rank correlation coefficient (Kendall), and Mean Absolute Difference (MAD)}
    \label{tab:per_agreement}
\end{table}

In our main article, we adopt the PER score for evaluating NPC personality consistency. Here, we further analyze both PER and PER$^d$ scores from automatic and human evaluations on a subset of 20 games in Table~\ref{tab:per_evaluation_results}, with additionally automatic evaluation results on the full dataset. We also report agreement metrics in Table~\ref{tab:per_agreement} Our analysis reveals several key observations:

\begin{enumerate}
    \item \textbf{PER$^d$ tends to be higher than PER in both automatic and human evaluations.} Across models, we observe that PER$^d$ scores are consistently higher than PER scores, indicating that direct evaluation of personality consistency is generally more lenient than the TIPI-based method. This trend holds for both automatic and human evaluators.

    \item \textbf{LLMs achieve similar PER scores across the dataset.} The automatic PER and PER$^d$ scores on the full set of games show little variation across models, with all models achieving scores around 0.58–0.60 for PER and around 0.74–0.78 for PER$^d$. This suggests that models perform comparably in terms of maintaining personality consistency in text-based role-playing.

    \item \textbf{Human evaluators rate PER$^d$ higher than PER, but with noticeable variation.} While automatic evaluations show a clear gap between PER and PER$^d$, human annotations exhibit a similar pattern but with greater variability. Notably, human evaluators assign significantly higher PER$^d$ scores to some models, such as Claude 3.5 Sonnet and DeepSeek V3, compared to their automatic scores.

    \item \textbf{Human and automatic PER scores exhibit poor correlation.} Table~\ref{tab:per_agreement} shows that the Pearson correlation between PER Auto and PER Human is negative (-0.691), with Kendall correlation also negative (-0.429). This suggests a fundamental mismatch between how LLM-based and human evaluators assess personality consistency through TIPI.

    \item \textbf{Better human agreement for PER$^d$, but still unstable.} While inter-human correlation for PER is negative (-0.310 Pearson, -0.286 Kendall), PER$^d$ exhibits a stronger but still weak agreement (0.649 Pearson). This suggests that directly rating personality alignment may be more intuitive for human evaluators than using TIPI scores but remains somewhat unstable. However, there is still concern over whether human annotators are capable of accurately understanding Big Five traits in the direct evaluation scenario.

    \item \textbf{Low agreement between PER and PER$^d$.} The Pearson correlation between PER and PER$^d$ scores (both automatic and human) is low (0.013 for Auto-Auto and -0.175 for Human-Human), indicating that these two evaluation methods capture different aspects of personality consistency. While PER$^d$ measures direct alignment with given traits, PER (TIPI) estimates personality traits implicitly, which may introduce more variance in judgments.
\end{enumerate}

\paragraph{Justification for Choosing TIPI (PER) in the Main Article.}  
We adopt TIPI-based personality consistency (\textbf{PER}) rather than direct evaluation (\textbf{PER}$^d$) in the main study for several reasons. First, TIPI does not require evaluators to have prior knowledge of the Big Five personality traits, making it a structured and interpretable method for assessing personality consistency. Additionally, the high variance in PER$^d$ human scores (as seen in Table~\ref{tab:per_agreement}) suggests that direct personality evaluation is more susceptible to subjective biases. The negative correlation between automatic and human PER scores further emphasizes the challenge of aligning LLM-based and human-based assessments, reinforcing the need for a more systematic approach like TIPI.

Overall, these results highlight the complexity of evaluating personality consistency, where different evaluation paradigms yield divergent results. The instability in human-human agreement for both PER and PER$^d$ suggests that subjective evaluation remains a challenging aspect of LLM benchmarking, warranting further research into more reliable personality evaluation methodologies.


