\section{Results and Discussions}
\subsection{Experimental Details}
\noindent\textbf{Game Creation} We consider GC to be a challenging task requiring complex reasoning over event-state interactions. Consequently, we evaluate advanced models with stronger reasoning capabilities: Claude 3.5 Sonnet, DeepSeek V3, Gemini 1.5 Pro, Gemini 2.0 Flash Exp, and GPT 4o.\footnote{Although models featuring inference-time reasoning can produce higher-quality results, the computational cost of running these models is often prohibitively high in practice.} We apply greedy decoding for all GC evaluations.

\noindent\textbf{Game Simulation} In addition to the models used in GC, we include GPT 4o mini, Llama 3.1 70B Instruct, and Llama 3.3 70B Instruct for the GS evaluation. Unless otherwise noted, we use a sampling temperature of 0.2 for inference. To maintain computational feasibility and fit within effective context windows of all models, we terminate all simulations after the 10th round for the main experiments. For all metrics requiring an LLM judge, we use GPT-4o as the evaluator.


\subsection{Game Creation Results}

\paragraph{Main Results}Table~\ref{tab:gc_eval_main} reports the format-check pass rate (FCR) and validity-check pass rate (VCR). We mark Claude 3.5 Sonnet with an asterisk (``*'') because it frequently refuses to generate content, often citing an “over-lengthy output” error, causing 95\% of its responses to fail the format check. We therefore focus on the fine-grained validity statistics for the remaining four models.

Most models (other than Claude 3.5 Sonnet) achieve high FCRs, indicating that they generally follow the specified formatting instructions. Among these models, GPT-4o attains the highest VCR of 0.49, while Gemini 1.5 Pro shows the lowest VCR of 0.04. Because passing the validity check demands a careful design of state variables and event systems, GPT-4o’s stronger planning and reasoning capabilities are highlighted in this task. A closer inspection on fine-grained metrics (w. Success, w. Lose and Reachability) reveals that Gemini 1.5 Pro frequently produces games that stall at intermediate steps without reaching success or failure endings. DeepSeek V3, in contrast, typically generates coherent event sequences, while GPT-4o often provides well-structured games with proper terminal outcomes.

\paragraph{Game Difficulty Analysis} While VCR predominantly measures the logical consistency of generated games, game difficulty is another vital factor. Our game design allows us to estimate difficulty by analyzing (1) the ratio of success terminations to losing terminations and (2) the ratio of the lengths of the event chains leading to these endings. Formally, for a valid game $v$, let $\mathcal{S}(v)$ be the set of all discovered success terminations and $\mathcal{L}(v)$ the set of losing terminations. For each trajectory $t$, let $\textit{length}(\cdot)$ denote the number of events in $t$. We define:

\begin{equation*}
    \begin{aligned}
        \textbf{CountRatio} &= \frac{|\mathcal{S}(v)|}{|\mathcal{L}(v)|} \\
        % \textbf{LengthRatio} &= \frac{\frac{\sum_{l\in\mathcal{L}_v} \textit{length}(l)}{|\mathcal{L}(v)|}}{\frac{\sum_{s\in\mathcal{S}_v} \textit{length}(s)}{|\mathcal{S}(v)|}}
        \textbf{LengthRatio} &= \frac{\sum_{l\in\mathcal{L}_v} \textit{length}(l)}{\sum_{s\in\mathcal{S}_v} \textit{length}(s)}\cdot \frac{|\mathcal{S}(v)|}{|\mathcal{L}(v)|}
    \end{aligned}
\end{equation*}

Intuitively, higher values for either ratio indicate an easier game. Figures~\ref{fig:gc_eval_cr} and \ref{fig:gc_eval_lr} show box plots of these ratios for three selected models. Our analysis reveals that all models generate games with a relatively balanced number of winning and losing trajectories. However, Gemini 2.0 Flash Exp tends to produce games where losing requires more steps, making failure less immediate. Additionally, the average \textbf{LengthRatio} is consistently below 1 across all models, indicating that winning generally requires more steps than losing—an expected outcome, as successful completion of a game typically demands more strategic progression.

\begin{table*}[!ht]
    \centering
    \begin{tabular}{lrrrrr}
    \toprule
    Models & FCR $\uparrow$& VCR $\uparrow$& w. Success & w. Lose &  Reachability\\
    \midrule
    Claude 3.5 Sonnet* & 0.050 & 0.010 & / & / & /\\
    DeepSeek V3       & 0.990 & 0.380 & 0.455 & 0.545 & \textbf{0.828}\\
    Gemini 1.5 Pro    & 0.850 & 0.040 & 0.060 & 0.080 & 0.610\\
    Gemini 2.0 Flash Exp & \textbf{1.000} & 0.330 & 0.420 & 0.680 & 0.480\\
    GPT 4o            & 0.960 & \textbf{0.490} & \textbf{0.656} & \textbf{0.771} & 0.656\\
    \bottomrule
    \end{tabular}
    \caption{Game Creation results.}
    \label{tab:gc_eval_main}
\end{table*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/CountRatio.pdf}
    \caption{CountRatio of three models}
    \label{fig:gc_eval_cr}
\end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/LengthRatio.pdf}
    \caption{LengthRatio of three models}
    \label{fig:gc_eval_lr}
\end{figure}

\subsection{Game Simulation Results}
\paragraph{Main Results} Table~\ref{tab:evaluation_results} presents our GS evaluation results, measuring length (LEN), role-playing factual consistency (FAC), personality consistency (PER), action quality (ACT), interestingness (INT), and mechanic score (MEC), along with tevent condition error rate (ECE) and variable update error rate (VUE) that decompose mechanic score.

Regarding LEN, DeepSeek V3, GPT 4o mini, and Llama 3.1 70B exceed the 200-word limit more than other models, which generally adhere to the instruction. All models exhibit high scores for factual consistency (FAC) and maintain moderate levels of personality consistency (PER). Action choice quality (ACT) is similarly high across models, but interestingness (INT) demonstrates wider variation. In particular, Claude 3.5 Sonnet achieves the highest INT score.

Game mechanic performance (MEC) varies the most among all metrics. Gemini 2.0 Flash Exp, GPT-4o, and Gemini 1.5 Pro perform comparably well, while the other models fare significantly worse. Even the best-performing model, Gemini 2.0 Flash Exp, only achieves a 0.765 MEC score, highlighting the inherent difficulty of precisely following complex game mechanics in a text-based RPG setting.

\begin{table*}[!ht]
\centering
\begin{tabular}{lrrrrrrrrr}
\toprule
Model & LEN  & FAC $\uparrow$ & PER $\uparrow$& ACT $\uparrow$& INT $\uparrow$&  MEC $\uparrow$ &  ECE$\downarrow$ & VUE$\downarrow$\\ 
\midrule
Claude 3.5 Sonnet & 220.3 & \textbf{0.991} & 0.589 & 0.923 & \textbf{0.722} & 0.113 & \textbf{0.062} & 0.308\\
Deepseek V3 & 309.5 & 0.984 & 0.583 & 0.918 & 0.502 & 0.277 & 0.165 & 0.153 \\
Gemini 1.5 Pro & 198.0 & 0.968 & 0.596 & 0.894 & 0.602 & 0.554 & 0.081 & 0.085 \\
Gemini 2.0 Flash Exp & 195.3& 0.885 & \textbf{0.598} & 0.865 & 0.538 & \textbf{0.765} & 0.094 & \textbf{0.034} \\
GPT 4o & 201.9  & 0.902 & 0.585 & 0.894 & 0.502 & 0.693 & 0.088 & 0.047 \\
GPT 4o mini & 282.5  & 0.955 & 0.588 & 0.900 & 0.496 & 0.147 & 0.126 & 0.148 \\
Llama 3.1 70B Instruct & 279.2  & 0.977 & 0.586 & 0.915 & 0.420 & 0.162 & 0.161 &0.284\\
% higgs-llama & 342.6  & 0.944 & 0.433 & \textbf{0.940} & \textbf{0.658} & 0.787 & 0.082 \\
Llama 3.3 70B Instruct & 225.7 & 0.960 & 0.585 & \textbf{0.936} & 0.466 & 0.204 & 0.201 & 0.302\\
\bottomrule
\end{tabular}
\caption{Game Simulation results. LEN: length; FAC: role-playing factual consistency; PER: role-playing personality consistency; ACT: action choice quality; INT: interestingness; MEC: mechanic score; ECE: event condition error rate; VUE: variable update error rate.}
\label{tab:evaluation_results}
\end{table*}

\paragraph{Impact of Sampling Temperature} We further examine three metrics sensitive to sampling temperature—FAC, INT, and MEC—using GPT-4o at temperatures $\{0.2,0.5,0.8\}$. Table~\ref{tab:gs_temp} summarizes the results. Interestingly, FAC increases with higher temperatures, which may initially seem counterintuitive given the heightened risk of hallucinations. However, we hypothesize that a larger temperature reduces the generation of factually neutral text, thereby leading to fewer overlooked facts. As expected, INT (interestingness) also rises with temperature, reflecting the increased creativity enabled by more diverse sampling. In contrast, MEC (mechanic score) peaks at the lowest temperature. This suggests that more deterministic sampling helps the model adhere more rigorously to the predefined game mechanics.

\begin{table}[!ht]
    \centering
    \begin{tabular}{crrr}
    \toprule
    Temperature   &  FAC & INT & MEC \\
    \midrule
    0.2   &  0.920 & 0.502 & \textbf{0.693}\\
    0.5   &  0.939 & 0.520 & 0.629\\
    0.8   &  \textbf{0.952} & \textbf{0.538} & 0.643\\
    \bottomrule
    \end{tabular}
    \caption{Performance under different sampling temperatures}
    \label{tab:gs_temp}
\end{table}
\paragraph{Impact of Number of Rounds} In our main experiments, we terminate each simulation at the 10th round, although games often do not naturally end that early. To assess the effect of longer trajectories, we take GPT-4o as an example and vary the number of rounds in $\{10, 15, 20, 25\}$. We focus on the metrics FAC, INT, and MEC, as the remaining metrics exhibit minimal variance. Table~\ref{tab:gs_round} shows that FAC increases with the number of rounds and eventually stabilizes, whereas INT decreases—likely due to repetitive content over extended sequences. The MEC score also declines, which may reflect the growing difficulty in maintaining coherent game mechanics within a longer context.

In our main experiments, we terminate simulations at the 10-th round. However, we found that games usually don't terminate this early. To this end, we use GPT 4o as an example to study the performance with longer trajectories with the number of rounds being $\{10, 15, 20, 25\}$. We also study FAC, INT and MEC since other scores demonstrate small variations. We observe in Table~\ref{tab:gs_round} that FAC score increases with more rounds and eventually becomes stable. INT score decreases with more rounds, which could originate from repetitive content. MEC score also decreases, potentially due to the challenges in handling long context.

\begin{table}[!ht]
    \centering
    \begin{tabular}{crrr}
    \toprule
    \# Rounds   &  FAC & INT & MEC \\
    \midrule
    10   &  0.920 & \textbf{0.502} & \textbf{0.693}\\
    15   &  \textbf{0.948} & 0.480 & 0.679\\
    20   &  0.941 & 0.458 & 0.674\\
    25   &  0.941 & 0.440 & 0.668\\
    \bottomrule
    \end{tabular}
    \caption{Performance under different number of simulation rounds}
    \label{tab:gs_round}
\end{table}

Despite the variations observed in Tables~\ref{tab:gs_temp} and \ref{tab:gs_round}, the differences in INT and MEC remain relatively modest compared to the variability across models. Consequently, we conclude that limiting simulations to 10 rounds is adequate for most metrics, although extending the number of rounds may further improve the stability of the FAC score.

\subsection{Human Evaluation of Game Simulation} 
\begin{table*}[!ht]
\centering
\begin{tabular}{lrrrr}
\toprule
Models  & \multicolumn{1}{c}{FAC} & \multicolumn{1}{c}{ACT} & \multicolumn{1}{c}{INT} & \multicolumn{1}{c}{PER}  \\ 
\midrule
Claude 3.5 Sonnet & \textbf{0.810} / \textbf{1.000} / 0.190 & 0.831 / \underline{0.913} / 0.082 & \textbf{0.856} / \textbf{0.713} / 0.144 & 0.648 / 0.729 / 0.081 \\
Deepseek V3& \underline{0.807} / \underline{0.950} / 0.143 & \underline{0.857} / \underline{0.913} / 0.056 & 0.850 / 0.475 / 0.375 & 0.645 / 0.742 / 0.098 \\
Gemini 1.5 pro & 0.733 / 0.950 / 0.217 & 0.738 / 0.889 / 0.152 & 0.801 / 0.588 / 0.214 & 0.648 / 0.740 / 0.093\\
Gemini 2.0 Flash Exp & 0.769 / 0.800 / 0.031 & 0.851 / 0.876 / 0.025 & \textbf{0.856} / \underline{0.525} / 0.331 & \underline{0.651} / 0.737 / 0.085 \\
GPT 4o & 0.709 / \underline{0.950} / 0.241 & \textbf{0.881} / 0.887 / 0.007 & 0.834 / \underline{0.525} / 0.309 & \textbf{0.667} / 0.711 / 0.044 \\
GPT 4o mini & 0.770 / \underline{0.950} / 0.180 & 0.794 / 0.887 / 0.093 & 0.813 / 0.488 / 0.326 & 0.648 / \textbf{0.753} / 0.104 \\
Llama 3.1 70B Instruct & 0.778 / \underline{0.950} / 0.172 & \underline{0.857} / 0.898 / 0.041 & 0.824 / 0.400 / 0.424 & 0.627 / \underline{0.744} / 0.117 \\
Llama 3.3 70B Instruct & 0.791 / 0.933 / 0.142 & 0.852 / \textbf{0.930} / 0.078 & 0.850 / 0.438 / 0.412 & 0.640 / 0.739 / 0.099 \\
% \midrule
% \midrule
% MAD \& Correlations  & 0.165 / 0.129 / 0.267 & 0.067 / 0.226 / 0.071 & 0.317 / 0.140 / 0.109 & 0.090 / -0.691 / -0.429 \\
\bottomrule
\end{tabular}
\caption{Comparison of human and automatic evaluation scores for four subjective metrics (FAC, ACT, INT, and PER) on a subset of 20 game simulations. Each cell shows ``Human Score / Automatic Score / Absolute Difference.'' We use bold and underline to denote the highest and second-highest scores per metric, respectively.}
   % \caption{Comparison of human evaluation scores and automatic evaluation scores of four subjective metrics on a subset of 20 games. For comparison, we present scores as ``Human Score / Automatic Score / Absolute Difference''. We use bold and underline to highlight the best and the second best scores for each metric. }
   %We also show the Mean Absolute Difference (MAD), Pearson correlation coefficient, the Kendall rank correlation coefficient in ``MAD / Pearson / Kendall'' at the bottom.} 
    \label{tab:gs_he}
\end{table*}

\begin{table*}
    \centering
    \begin{tabular}{l|rrrr}
    \toprule
    Comparison     & \multicolumn{1}{c}{FAC} & \multicolumn{1}{c}{ACT} & \multicolumn{1}{c}{INT} & \multicolumn{1}{c}{PER} \\
    \midrule
    Auto v.s. Human     & 0.165 / 0.129 / 0.267 & 0.067 / 0.226 / 0.071 & 0.317 / 0.140 / 0.109 & 0.090 / -0.691 / -0.429 \\
    Human v.s. Human & 0.030 / 0.707 / 0.571 & 0.039 / 0.472 / 0.214 & 0.018 / 0.508 / 0.286 & 0.023 / -0.310 / -0.286\\
    \bottomrule
    \end{tabular}
    \caption{Mean Absolute Difference (MAD), Pearson correlation coefficient, and Kendall rank correlation coefficient between automatic metrics and human evaluation scores (and among human evaluators). All values are presented in the format ``MAD / Pearson / Kendall.''}
    % \caption{Mean Absolute Difference (MAD), Pearson correlation coefficient, the Kendall rank correlation coefficient between automatic metrics and human evaluation scores and among human evaluators. We present scores in the ``MAD / Pearson / Kendall'' order. }
    \label{tab:gs_corr}
\end{table*}
We also conduct a human evaluation on a subset of 20 simulated games, focusing on four subjective metrics: \textbf{FAC} (Factual Consistency), \textbf{PER} (Personality Consistency), \textbf{ACT} (Action Quality), and \textbf{INT} (Interestingness). We reframe these dimensions as natural-language questions to simplify the task for human annotators, who provide numerical scores later normalized to \([0,1]\). Complete details on the human evaluation setup are provided in the Appendix~\ref{app:human_eval}. Below, we outline two main differences between human evaluation and our automatic approach that can affect outcomes: \textbf{Scoring Procedure for Long Trajectories.} 
    Since each game trajectory consists of 10 rounds, we present the content round by round and request a set of scores per round. We then average these round-level scores to derive final FAC, ACT, and INT metrics. Personality (PER) is an exception; because a single round may not reveal enough about the NPC's character, annotators fill in a TIPI questionnaire at the end of the full trajectory. \textbf{Aggregated Factual Consistency.}
    Our automatic scorer checks each fact individually. However, to reduce the annotators' workload, we ask them to give a single 1--5 rating for overall consistency with all facts.

Table~\ref{tab:gs_he} presents the human evaluation scores alongside our automatic metrics for each model, while Table~\ref{tab:gs_corr} reports several comparative metrics such as mean absolute difference and correlation coefficients. Although human judgments can provide valuable insights, these metrics are inherently subjective and susceptible to personal biases. Consequently, human scores should be interpreted as reference points rather than definitive “gold standards.”

Examining Table~\ref{tab:gs_he}, we find a fair degree of overlap in the top two performing models across FAC, ACT, and INT, but not for PER. From Table~\ref{tab:gs_corr}, we see that the inter-annotator correlation on PER is also very low, suggesting that personality judgments tend to be more variable and less stable.

Looking at the mean absolute differences (\textbf{MAD}) between human and automatic scores, ACT and PER exhibit relatively small discrepancies, whereas FAC and INT show larger gaps. Interestingly, FAC and INT also have somewhat higher correlation coefficients than the other metrics. Such results may stem from two factors: (1) the modifications we made for human evaluators versus automatic methods, and (2) the fact that the scores of different models are relatively close, making correlation metrics sensitive to small shifts.

Feedback from our annotators further indicates that \textbf{INT} can be heavily influenced by personal preferences. For instance, if a rater dislikes combat scenarios, they consistently assign lower interest scores to an action-heavy game trajectory. This shows that subjective evaluations—whether by humans or LLM judges—can vary widely based on individual tastes.

Although LLM-based scoring has been common in prior work for subjective dimensions, our human evaluation reveals that fine-grained comparisons remain unstable and less differentiable, even for human evaluators. This outcome highlights the importance of introducing objective metrics into game simulation assessment, such as our proposed game mechanic checks (Section~\ref{sec:gd}) that do not rely on either human or LLM judgments. 
% Such a balance between subjective and objective evaluation criteria is crucial to comprehensively assess models’ capabilities in game creation and simulation.

% Analysis: points to make
% - these metrics are subjective, thus human evaluator may be biased and should not be considered as gold standard
% - the overlap of top 2 models is reasonably good from Table~\ref{gs_he} except for PER. However, from Table~\ref{gs_corr}, we can see that the inner annotator correlation is also very low for this metric, indicating the instability of personality judgement.
% - the absolution difference for ACT and PER is small. Although larger differences for FAC and INT, they do have a better correlation than the other scores. Note that the overall low correlation could originates from the modifications we made for human evaluation, and also the fact that scores from different models are close to each other, making the correlation metric sensitive to small changes.
% - evaluator feedback also indicates that they are sometimes biased by their personal perference when judging interestingness. E.g., if they are not interested in fighting scenarios, they would rate an action game trajectory with lower interestingness scores.
% - while previous work predominantly adopts LLM-based scores on subjective dimensions, our human evaluation results reveal that intricate comparisons on these dimensions are usually unstable and less differntiable, even with human evaluators. This further shows the importance of introducing objective scores in evaluating game simulation, just as we did for the game mechanics.


% \begin{table*}[!ht]
% \centering
% \begin{tabular}{lrrrr}
% \toprule
% Models  & \multicolumn{1}{c}{FAC} & \multicolumn{1}{c}{ACT} & \multicolumn{1}{c}{INT} & \multicolumn{1}{c}{PER}  \\ 
% \midrule
% Claude 3.5 Sonnet & \textbf{0.810} / \textbf{1.000} / 0.190 & 0.831 / \underline{0.913} / 0.082 & \textbf{0.856} / \textbf{0.713} / 0.144 & 0.648 / 0.729 / 0.081 \\
% Deepseek V3& \underline{0.807} / \underline{0.950} / 0.143 & \underline{0.857} / \underline{0.913} / 0.056 & 0.850 / 0.475 / 0.375 & 0.645 / \textbf{0.742} / 0.098 \\
% Gemini 2.0 Flash Exp & 0.769 / 0.800 / 0.031 & 0.851 / 0.876 / 0.025 & \textbf{0.856} / \underline{0.525} / 0.331 & \underline{0.651} / 0.737 / 0.085 \\
% GPT 4o & 0.709 / \underline{0.950} / 0.241 & \textbf{0.881} / 0.887 / 0.007 & 0.834 / \underline{0.525} / 0.309 & \textbf{0.667} / 0.711 / 0.044 \\
% GPT 4o mini & 0.770 / \underline{0.950} / 0.180 & 0.794 / 0.887 / 0.093 & 0.813 / 0.488 / 0.326 & 0.648 / 0.753 / 0.104 \\
% Llama 3.1 70b & 0.778 / \underline{0.950} / 0.172 & \underline{0.857} / 0.898 / 0.041 & 0.824 / 0.400 / 0.424 & 0.627 / 0.744 / 0.117 \\
% Llama 3.3 70b & 0.791 / 0.933 / 0.142 & 0.852 / \textbf{0.930} / 0.078 & 0.850 / 0.438 / 0.412 & 0.640 / \underline{0.739} / 0.099 \\
% % \midrule
% % \midrule
% % MAD \& Correlations  & 0.165 / 0.129 / 0.267 & 0.067 / 0.226 / 0.071 & 0.317 / 0.140 / 0.109 & 0.090 / -0.691 / -0.429 \\
% \bottomrule
% \end{tabular}
%    \caption{Comparison of human evaluation scores and automatic evaluation scores of four subjective metrics on a subset of 20 games. For comparison, we present scores as ``Human Score / Automatic Score / Absolute Difference''. We use bold and underline to highlight the best and the second best scores for each metric. }
%    %We also show the Mean Absolute Difference (MAD), Pearson correlation coefficient, the Kendall rank correlation coefficient in ``MAD / Pearson / Kendall'' at the bottom.} 
%     \label{tab:gs_he}
% \end{table*}

% \begin{table*}
%     \centering
%     \begin{tabular}{l|rrrr}
%     \toprule
%     Comparison     & \multicolumn{1}{c}{FAC} & \multicolumn{1}{c}{ACT} & \multicolumn{1}{c}{INT} & \multicolumn{1}{c}{PER} \\
%     \midrule
%     Auto v.s. Human     & 0.165 / 0.129 / 0.267 & 0.067 / 0.226 / 0.071 & 0.317 / 0.140 / 0.109 & 0.090 / -0.691 / -0.429 \\
%     Human v.s. Human & \\
%     \bottomrule
%     \end{tabular}
%     \caption{Mean Absolute Difference (MAD), Pearson correlation coefficient, the Kendall rank correlation coefficient between automatic metrics and human evaluation scores and among human evaluators. We present scores in the ``MAD / Pearson / Kendall'' order. }
%     \label{tab:gs_corr}
% \end{table*}