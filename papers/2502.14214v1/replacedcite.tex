\section{Related Work}
\subsection{Unsupervised Domain Adaptation (UDA)} 

Unsupervised Domain Adaptation (UDA) methods address domain shift by jointly training a network on labeled source data and unlabeled target data, thereby aligning their feature distributions. Conventional UDA approaches tackle domain shift through two main strategies. Some methods____ employ moment matching to align feature distributions, with____ being the first to introduce Maximum Mean Discrepancy (MMD) in domain adaptation. Others____ utilize adversarial training to learn domain-invariant features. Additionally, certain methods____ apply adversarial training between the feature extractor and the classifier. These techniques decouple the source and target domains during training, allowing the model to estimate the domain difference without direct access to the source data. However, under the UDA setting, the target labels remain inaccessible during training. As a result, UDA methods often rely heavily on the accuracy of prototype estimation and pseudo-label annotation for effective adaptation.

\subsection{Source-Free Unsupervised Domain Adaptation (SFUDA)}

Conventional UDA methods rely on supervision from the source domain during the adaptation process, which can be challenging to achieve in practical settings. As a result, source-free unsupervised domain adaptation (SFUDA) methods have gained significant attention, as they aim to adapt models to the target domain without accessing source domain data. From the DA perspective, works such as____ have explored source-free universal DA____ and open-set DA____. SHOT____, the pioneering SFUDA method, primarily addresses closed-set DA and partial-set DA____. To achieve SFUDA, certain methods____ focus on reconstructing a pseudo-source distribution in feature space based on the source hypothesis, enhancing generalization by aligning target domain samples with these pseudo-source samples. Another prominent stream of SFUDA methods____ concentrates on pseudo-label prediction from the source model or prototypes, adapting the model to the target domain and ensuring a better fit to the target domain distribution.

\subsection{Few-Shot Transfer Learning}

A wide array of studies focus on learning a metric space specifically for $k$-shot tasks, leveraging meta-learning to develop adaptation strategies____. These approaches, however, often require specialized network architectures, custom loss functions, and training strategies that depend on multiple source domains or joint training with both source and support dataâ€”conditions that are challenging to meet in real-world environments. Another commonly adopted strategy is model fine-tuning or weight transfer from a pre-trained source model. However, fine-tuning all source model weights on a limited support set with a small value of $k$ typically leads to severe overfitting, necessitating a support set of at least $k = 100$ to achieve stability____. Recent advancements have been made in the SFFSDA setting. For example, LCCS____ utilizes a low-dimensional approximation of batch normalization (BN) statistics to drastically reduce the number of parameters required for adaptation, thereby improving the robustness of adaptation with limited support data. Additionally, ____ proposes that fine-tuning a source pre-trained model using a few labeled target instances presents a more feasible solution to overcome the limitations inherent in SFUDA, and they successfully demonstrate the effectiveness of this approach. In contrary to prior works, our proposed ACT method can not only effectively tackle closed-set SFFSDA problem, but also tackle open-set and partial-set domain adaptation scenarios.