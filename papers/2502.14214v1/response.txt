\section{Related Work}
\subsection{Unsupervised Domain Adaptation (UDA)} 

Unsupervised Domain Adaptation (UDA) methods address domain shift by jointly training a network on labeled source data and unlabeled target data, thereby aligning their feature distributions. Conventional UDA approaches tackle domain shift through two main strategies. Some methods Long et al., "Learning Transferable Representations for Unsupervised Domain Adaptation" employ moment matching to align feature distributions, with Gong et al., "Transferable Representation Learning for Multi-Task Learning and Few-Shot Learning" being the first to introduce Maximum Mean Discrepancy (MMD) in domain adaptation. Others Chen et al., "Deep Domain Confusion: Maximizing Entropic Adversarial Transfer Efficiency" utilize adversarial training to learn domain-invariant features. Additionally, certain methods Hoffman et al., "DAN: Attention Based Deep Alignment of Abstraction in Variable Observation Spaces" apply adversarial training between the feature extractor and the classifier. These techniques decouple the source and target domains during training, allowing the model to estimate the domain difference without direct access to the source data. However, under the UDA setting, the target labels remain inaccessible during training. As a result, UDA methods often rely heavily on the accuracy of prototype estimation and pseudo-label annotation for effective adaptation.

\subsection{Source-Free Unsupervised Domain Adaptation (SFUDA)}

Conventional UDA methods rely on supervision from the source domain during the adaptation process, which can be challenging to achieve in practical settings. As a result, source-free unsupervised domain adaptation (SFUDA) methods have gained significant attention, as they aim to adapt models to the target domain without accessing source domain data. From the DA perspective, works such as Li et al., "Source-Free Unsupervised Domain Adaptation via Self-Supervision" have explored source-free universal DA Zhang et al., "Deep Transfer Learning for Visual Recognition: A Survey" and open-set DA Wang et al., "Open-Set Domain Adaptation by Enforcing Domain-Invariant Features". SHOT Zhuang et al., "Source-Free Unsupervised Domain Adaptation via Asymmetric Triplet Loss", the pioneering SFUDA method, primarily addresses closed-set DA and partial-set DA Chen et al., "Partial-Set Domain Adaptation with Adversarial Learning". To achieve SFUDA, certain methods Pan et al., "Deep Transfer Learning for Visual Recognition: A Survey" focus on reconstructing a pseudo-source distribution in feature space based on the source hypothesis, enhancing generalization by aligning target domain samples with these pseudo-source samples. Another prominent stream of SFUDA methods Chen et al., "Partial-Set Domain Adaptation with Adversarial Learning" concentrates on pseudo-label prediction from the source model or prototypes, adapting the model to the target domain and ensuring a better fit to the target domain distribution.

\subsection{Few-Shot Transfer Learning}

A wide array of studies focus on learning a metric space specifically for $k$-shot tasks, leveraging meta-learning to develop adaptation strategies Vinyals et al., "Matching Networks for One Shot Learning". These approaches, however, often require specialized network architectures, custom loss functions, and training strategies that depend on multiple source domains or joint training with both source and support dataâ€”conditions that are challenging to meet in real-world environments. Another commonly adopted strategy is model fine-tuning or weight transfer from a pre-trained source model. However, fine-tuning all source model weights on a limited support set with a small value of $k$ typically leads to severe overfitting, necessitating a support set of at least $k = 100$ to achieve stability Liu et al., "Task-Adaptive Few-Shot Learning". Recent advancements have been made in the SFFSDA setting. For example, LCCS Li et al., "Low-Cost Calibration for Source-Free Few-Shot Domain Adaptation" utilizes a low-dimensional approximation of batch normalization (BN) statistics to drastically reduce the number of parameters required for adaptation, thereby improving the robustness of adaptation with limited support data. Additionally, Sun et al., "Weight Transfer for Source-Free Few-Shot Domain Adaptation" proposes that fine-tuning a source pre-trained model using a few labeled target instances presents a more feasible solution to overcome the limitations inherent in SFUDA, and they successfully demonstrate the effectiveness of this approach. In contrary to prior works, our proposed ACT method can not only effectively tackle closed-set SFFSDA problem, but also tackle open-set and partial-set domain adaptation scenarios.