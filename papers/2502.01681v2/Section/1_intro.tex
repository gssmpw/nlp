\vspace{-10pt}
\section{Introduction}
\label{1_intro}

% Graph is a widely-used data structure across various domains, including citation networks~\citep{gnncite1,gnncite2}, social networks~\citep{gnnsocial1,gnnsocial2,gnnsocial3}, and protein-protein interactions~\citep{gnnprotein1,gnnprotein2,gnnprotein3}. Graph Neural Networks (GNNs) leverage the inherent graph structure to learn feature representations for nodes and edges, which are subsequently applied to various downstream tasks~\citep{dwivedi2020generalization}.
% A notable application of GNNs is in circuit representation learning within Electronic Design Automation (EDA). The DeepGate family~\citep{li2022deepgate, shi2023deepgate2, khan2023deepseq} models circuits as Directed Acyclic Graphs (DAGs) and employs GNNs to update node representations through the aggregation of information from neighboring nodes. These models have proven effective across a range of EDA tasks, including logic reasoning~\citep{deng2024less, wu2023gamora} and SAT solving~\citep{li2023eda, shi2024eda}.


% Circuit representation learning has become a pivotal area in electronic design automation (EDA), reflecting the broader trend in AI of learning general representations for diverse downstream tasks. In this domain, the DeepGate family~\citep{li2022deepgate, shi2023deepgate2} emerges as pioneering approachs, utilizing graph neural networks (GNNs) to map circuit netlists into graph forms and learn gate-level embeddings. DeepGate~\citep{li2022deepgate} converts arbitrary circuit netlists into And-Inverter Graphs (AIGs) and uses logic-1 probabilities from random simulations for model supervision. Its successor, DeepGate2~\citep{shi2023deepgate2}, builds upon this by learning disentangled structural and functional embeddings, enabling support for a variety of EDA tasks such as testability analysis~\citep{shi2022deeptpi}, power estimation~\citep{khan2023deepseq}, and SAT solving~\citep{li2023eda, shi2024eda}. The Gamora model~\citep{wu2023gamora} extends reasoning capabilities further by representing both logic gates and cones.

% Despite these advancements, applying GNNs to circuit representation faces significant challenges, particularly in scalability and generalization~\citep{liu2024neural}. The HOGA model~\citep{deng2024less} tackles these issues through a novel message-passing mechanism and an efficient training strategy, but inherent limitations of the GNN framework persist, including difficulty in capturing long-range dependencies and susceptibility to over-smoothing~\citep{akansha2023over} and over-squashing~\citep{rusch2023survey}.

% PolarGate~\citep{PolarGate} seeks to overcome functionality representation bottlenecks by leveraging ambipolar state principles, while DeepGate3~\citep{shi2024deepgate3} uses the global aggregation power of transformers to enhance circuit representation. However, new challenges arise when scaling to large AIGs: GNN-based models are constrained by limited receptive fields, reducing performance in global functional tasks, while transformer-based models suffer from quadratic complexity, making training on large AIGs impractical due to memory limitations.


Circuit representation learning has emerged as a crucial area in electronic design automation (EDA), reflecting the broader trend in AI of learning general representations for diverse downstream tasks~\citep{chen2024large}, such as testability analysis~\citep{shi2022deeptpi}, logic reasoning~\citep{deng2024less, wu2023gamora}, power estimation~\citep{khan2023deepseq}, and SAT solving~\citep{li2023eda, shi2024eda}. In this domain, the DeepGate family~\citep{li2022deepgate, shi2023deepgate2} emerges as pioneering approaches, formulating circuit netlists into graphs and utilizing graph neural networks (GNNs) to learn gate-level embeddings. DeepGate~\citep{li2022deepgate} converts arbitrary circuit netlists into And-Inverter Graphs (AIGs) and uses logic-1 probabilities from random simulations for model supervision. Its successor, DeepGate2~\citep{shi2023deepgate2}, improves on this by learning disentangled structural and functional embeddings. In addition to the DeepGate Family, Gamora~\citep{wu2023gamora} extends reasoning capabilities by representing both logic gates and cones, while HOGA~\citep{deng2024less} enhances the scalability and generalizability of GNNs through hop-wise aggregation.

Despite the success on tiny circuits, inherent limitations of the GNN-based framework persist when it scales to large circuits, including difficulty in capturing long-range dependencies~\citep{alon2020bottleneck}, susceptibility to over-smoothing~\citep{akansha2023over} and over-squashing~\citep{rusch2023survey}, which results in poor performance on complex circuits. Consequently, DeepGate3~\citep{shi2024deepgate3} draws inspiration from transformer-based graph learning models by tokenizing circuits into sequences and employing graph transformers to capture global relationships within DAG-based structures. While DeepGate3 introduces fine-tuning strategies for scaling from smaller to larger circuits, it still struggles to handle circuits with millions of gates due to the significant memory overhead and computation redundancy of dense transformer blocks. Therefore, training an efficient and effective circuit representation learning model still remains a challenge.

% However, scaling graph transformers to large graphs presents a significant challenge~\citep{GraphGPS,Exphormer}. 
% In particular, the quadratic complexity of transformer blocks leads to substantial memory and computational overhead, limiting the applicability of DeepGate3 to real-world circuits containing millions or even billions of gates.

% Despite the success in various domains, GNNs that rely on message-passing mechanisms face inherent limitations, such as over-squashing~\citep{akansha2023over}, and over-smoothing~\citep{rusch2023survey}, which result in their limited expressivity on large graph.

% The prior research efforts centering on model efficiency present a promising avenue toward scaling GNNs and graph Transformers for general purposes, yet there remains a long distance to cover in the circuit representation learning domain. 
In general domain, previous research on improving model efficiency has shown great potential in scaling GNNs and graph Transformers; however, significant challenges still remain in applying these advancements to circuit representation learning.
These models can be broadly categorized into two types: linear graph transformers and sub-linear GNNs.
On the one hand, the linear Graph Transformers, such as GraphGPS~\citep{GraphGPS}, Exphormer~\citep{Exphormer}, NodeFormer~\citep{Nodeformer}, DAGformer~\citep{DAGformer}, and NAGphormer~\citep{NAGphormer}, leverage graph sparsity to perform different sparse attention, reducing memory consumption from quadratic to linear. Despite the advancement, training these models on practical circuit designs with millions or billions of gates still suffer from Out-Of-Memory(OOM) error.
% Although these techniques can effectively train on graphs with 100k nodes, they often fall short due to memory constraints when learning practical circuit designs with millions or billions of gates.
On the other hand, the sub-linear GNNs, such as GNNAutoScale~\citep{GNNAutoScale}, SketchGNN~\citep{SketchGNN}, and GraphFM~\citep{GraphFM}, achieve sub-linear memory complexity by employing historical embeddings during training, with randomly sampled sub-graphs. However, these methods are primarily tailored for undirected graphs and pose challenges when applied to Directed Acyclic Graphs (DAGs). 
% Specifically, when modeling circuit functionality as a computational graph, it is essential to follow a strict topological order, reasoning from primary inputs (PIs) to primary outputs (POs) based on logic levels~\citep{li2022deepgate}. 
Specifically, sub-linear GNNs with random sampling strategies~\citep{GNNAutoScale, GraphFM, SketchGNN} disregard the causal relationships between sub-graphs by applying completely random sampling, resulting in suboptimal performance on function-related tasks.

% In general domain, several works have focused on reducing the complexity of GNNs and graph transformers to address memory and scalability challenges. 
% In the general domain, several works have focused on scalability problem of GNNs and graph transformers. However, applying these methods directly to large-scale circuit design remains significant challenges.
% These models can be broadly categorized into two types: linear graph transformers and sub-linear GNNs. Linear graph transformer models, such as GraphGPS~\citep{GraphGPS}, Exphormer~\citep{Exphormer}, NodeFormer~\citep{Nodeformer}, DAGformer~\citep{DAGformer}, and NAGphormer~\citep{NAGphormer}, leverage graph sparsity to perform different sparse attention, reducing memory consumption from quadratic to linear. While these methods can perform training on a graph with about 100k nodes, scaling to circuits with millions or billions of gates still remains a significant challenge, due to the memory limitation.
% Sub-linear GNNs, such as GNNAutoScale~\citep{GNNAutoScale}, SketchGNN~\citep{SketchGNN}, and GraphFM~\citep{GraphFM}, achieve sub-linear memory complexity by employing historical embeddings during training, with randomly sampled sub-graphs. However, these methods are primarily tailored for undirected graphs and pose challenges when applied to Directed Acyclic Graphs (DAGs). 
% % In circuits, where message passing strictly follows a topological order from primary inputs (PIs) to primary outputs (POs)~\citep{li2022deepgate}, the random sampling strategies used by sub-linear GNNs~\citep{GNNAutoScale,GraphFM,SketchGNN} disrupt the natural information flow, leading to suboptimal performance, particularly in function-related tasks.
% Specifically, when modeling circuit functionality as a computational graph, it is essential to follow a strict topological order, reasoning from primary inputs (PIs) to primary outputs (POs) based on logic levels~\citep{li2022deepgate}. Previous random sampling strategies~\citep{GNNAutoScale, GraphFM, SketchGNN} disregard the causal relationships between sub-graphs by applying completely random sampling, resulting in suboptimal performance on function-related tasks.



% introduce our method to address the problem
\review{In response to these challenges, we propose \textbf{DeepGate4}, an efficient and effective graph transformer specifically designed to scale to large circuits. Building on the architecture of DeepGate3 as illustrated in Figure~\ref{fig:DG2_DG3}, DeepGate4 utilizes GNN-based tokenizer to encode circuit function and structure. These embeddings are then processed by a transformer for global aggregation. 
Our approach introduces several key innovations: 
\begin{itemize}[leftmargin=*, labelsep=0.5em]
    \vspace{-8pt}
    \item \textbf{An updating strategy} tailored for DAGs based on partitioned graph, ensuring that gate embeddings are computed in logical level order, with each gate being processed only once, thus eliminating redundant computations. While DeepGate3 is limited to fine-tuning graphs with up to 50k nodes, the proposed updating strategy, which is adaptable to any graph transformer, achieve sub-linear memory complexity and thus enable efficient training on graphs with millions of nodes. 
    % \vspace{-5pt}
    \item \textbf{A GAT-based sparse transformer} with global virtual edges, reducing both time and memory complexity to linear in a mini-batch. We further introduce structural encodings for transformers on AIGs by incorporating global and local structural encodings in initialized embedding.
    % \vspace{-5pt}
    \item \textbf{An inference acceleration kernel}, Fused-DeepGate4, designed to optimize the inference process of tokenizer and GAT components with well-designed CUDA kernels that fully exploit the unique sparsity patterns of AIGs.
\end{itemize}}
\vspace{-5pt}

% Our approach introduces several key innovations: \textbf{(1) A partitioning method and updating strategy} tailored for DAGs, ensuring that gate embeddings are computed in logical level order, with each gate being processed only once, thus eliminating redundant computations. This partitioning method, along with the proposed updating strategy by using historical embedding, is adaptable to any graph transformer, enabling the reduction of memory complexity to sub-linear. \textbf{(2) A GAT-based sparse transformer} with global virtual edges, reducing both time and memory complexity to linear in a mini-batch. Additionally, we analyze the sparse pattern of AIGs and then introduce \textbf{Fused-DeepGate4} to optimize the inference stage of tokenizer and GAT components with well-designed CUDA kernels. \textbf{(3)}We introduce \textbf{structural encodings} for AIGs by incorporating global structural encodings (levels) and local structural encodings (out-degree) in initialized embedding. Moreover, a \textbf{loss balancer} is incorporated in training to automatically adjust the weight of multitask losses in order to stabilize the training process.


% Experiment
Experimental results on the ITC99 and EPFL benchmarks demonstrate that DeepGate4 significantly outperforms state-of-the-art methods, with improvements of 15.5\% and 31.1\% respectively over the second-best method. Furthermore, our Fused-DeepGate4 model, with inference acceleration optimizations, achieves a 41.3\% reduction in runtime and 51.3\% reduction in memory usage on ITC99 benchmark, and a 28.2\% reduction in runtime and 32.5\% reduction in memory usage on EPFL benchmark.
We also evaluate the generalizability of DeepGate4 across circuits of varying scales. DeepGate4 exhibits strong generalizability, delivering outstanding performance on circuits with 400K gates, despite being trained on circuits averaging just 15K gates. Moreover, when inference on circuits ranging from 400K gates to 1.6M gates, while GNNs exhibit linear memory growth, our models maintain constant memory usage. These results suggest that DeepGate4 has potential to scale both effectively and efficiently to circuits with millions, even billions of gates.

\vspace{-10pt}