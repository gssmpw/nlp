
\begin{abstract}
% Graph representation learning has become a critical area of research due to its applicability in domains such as Electronic Design Automation (EDA). Traditional Graph Neural Networks (GNNs) rely on message-passing mechanisms, which are limited by expressivity issues like over-squashing and over-smoothing.
% Graph Transformer (GTs) are proposed to alleviate these fundamental limitations. However, the quadratic computational complexity of Transformer makes it challenging to apply GTs at scale.

\iffalse
Circuit representation learning has become a critical area in electronic design automation, with broad applications in tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models struggle to scale to large graphs due to inherent limitations like the over-squashing problem in GNN-based models and the quadratic complexity in transformer-based models.
To address these limitations, we propose DeepGate4, an efficient and effective graph transformer designed specifically for large circuits, with several key innovations: (1) a partitioning method and update strategy designed for directed acyclic graphs (DAGs), which reduces memory complexity to sub-linear; (2) a GAT-based sparse transformer with inference acceleration optimization by leveraging the sparse pattern of And-Inverter Graphs (AIGs) and (3) global and local structural encodings for AIGs, combined with a loss balancer that automatically adjusts the weight of multitask losses.
Experimental results on the ITC-99 and EPFL benchmarks demonstrate that DeepGate4 significantly outperforms state-of-the-art methods, with improvements of 15.5\% and 31.1\% over the second-best method in overall performance, respectively. 
Additionally, our Fused-DeepGate4 model with inference optimization achieves a 35.1\% reduction in runtime and 46.8\% reduction in memory usage.
\fi 


\iffalse
\begin{itemize}[leftmargin=*, labelsep=0.5em]
    \vspace{-8pt}
    \item \textbf{An updating strategy} tailored for DAGs based on partitioned graph, ensuring that gate embeddings are computed in logical level order, with each gate being processed only once, thus eliminating redundant computations. While DeepGate3 is limited to fine-tuning graphs with up to 50k nodes, the proposed updating strategy, which is adaptable to any graph transformer, achieve sub-linear memory complexity and thus enable efficient training on graphs with millions of nodes. 
    % \vspace{-5pt}
    \item \textbf{A GAT-based sparse transformer} with global virtual edges, reducing both time and memory complexity to linear in a mini-batch. We further introduce structural encodings for transformers on AIGs by incorporating global and local structural encodings in initialized embedding.
    % \vspace{-5pt}
    \item \textbf{An inference acceleration kernel}, Fused-DeepGate4, designed to optimize the inference process of tokenizer and GAT components with well-designed CUDA kernels that fully exploit the unique sparsity patterns of AIGs.
\end{itemize}
\fi


Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce \textbf{DeepGate4}, a scalable and efficient graph transformer specifically designed for large-scale circuits. \review{DeepGate4 incorporates several key innovations: (1) an update strategy tailored for circuit graphs, which reduce memory complexity to sub-linear and is adaptable to any graph transformer; (2) a GAT-based sparse transformer with global and local structural encodings for AIGs; and (3) an inference acceleration CUDA kernel that fully exploit the unique sparsity patterns of AIGs.} Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5\% and 31.1\% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1\% and memory usage by 46.8\%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency.

% Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce \textbf{DeepGate4}, a scalable and efficient graph transformer specifically designed for large-scale circuits. DeepGate4 incorporates several key innovations: (1) a partitioning method and update strategy tailored for circuit graphs, reducing memory complexity to sub-linear levels; (2) a GAT-based sparse transformer optimized for inference by leveraging the sparse nature of circuits; and (3) global and local structural encodings for circuits, along with a loss balancer that dynamically adjusts the weights of multitask losses to stabilize training. Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5\% and 31.1\% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1\% and memory usage by 46.8\%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency.

\end{abstract}
% \vspace{-10pt}