\newpage
\section{Appendix}

\subsection{Dataset Statistic}
\label{sec:Dataset_Statistic}

\begin{table}[H]
\caption{\review{OpenABC-D Dataset}}
\centering
\begin{tabular}{@{}c|l|llllll@{}}
\toprule
split & name & \#node & \#edge & \#PI & \#PO & max level & \#cones \\ \midrule
\multirow{24}{*}{train} & spi & 8565 & 12530 & 254 & 238 & 69 & 1348 \\
 & i2c & 2195 & 3187 & 177 & 128 & 27 & 291 \\
 & ss\_pcm & 866 & 1165 & 104 & 90 & 13 & 144 \\
 & usb\_phy & 1025 & 1380 & 132 & 85 & 16 & 143 \\
 & sasc & 1349 & 1827 & 135 & 124 & 15 & 165 \\
 & wb\_dma & 9059 & 12818 & 828 & 660 & 41 & 1779 \\
 & simple\_spi & 1928 & 2694 & 164 & 132 & 23 & 316 \\
 & pci & 41708 & 57826 & 3429 & 3131 & 52 & 6439 \\
 & dynamic\_node & 36469 & 51855 & 2708 & 2560 & 55 & 5170 \\
 & ac97\_ctrl & 24399 & 33524 & 2339 & 2130 & 19 & 3568 \\
 & mem\_ctrl & 31001 & 47906 & 1187 & 937 & 56 & 3949 \\
 & des3\_area & 8069 & 12737 & 303 & 32 & 47 & 1226 \\
 & aes & 39898 & 68140 & 683 & 529 & 44 & 4734 \\
 & sha256 & 30634 & 44507 & 1943 & 1042 & 143 & 4606 \\
 & fir & 9412 & 13560 & 410 & 319 & 86 & 1526 \\
 & iir & 14139 & 20623 & 494 & 404 & 131 & 2377 \\
 & idft & 518787 & 722736 & 37603 & 37383 & 82 & 90525 \\
 & tv80 & 19877 & 30569 & 636 & 361 & 99 & 3025 \\
 & fpu & 56567 & 85558 & 632 & 339 & 1522 & 8986 \\
 & aes\_xcrypt & 67660 & 111525 & 1975 & 1682 & 76 & 11490 \\
 & jpeg & 233573 & 343382 & 4962 & 4789 & 75 & 33429 \\
 & tinyRocket & 104336 & 152090 & 4561 & 4094 & 156 & 17548 \\
 & picosoc & 173744 & 245387 & 11302 & 10786 & 75 & 30211 \\
 & vga\_lcd & 226448 & 314460 & 17322 & 17049 & 44 & 43939 \\ \midrule
\multirow{5}{*}{val} & dft & 525762 & 733211 & 37597 & 37382 & 83 & 91763 \\
 & wb\_conmax & 83229 & 128947 & 2122 & 2032 & 35 & 12002 \\
 & ethernet & 143316 & 199749 & 10731 & 10401 & 59 & 25661 \\
 & bp\_be & 171292 & 242214 & 11592 & 8225 & 150 & 25092 \\
 & aes\_secworks & 74990 & 112681 & 3087 & 2603 & 71 & 12420 \\ \midrule
Avg & - & 91734.38 & 131337.5 & 5496.966 & 5160.931 & 116 & 15305.93 \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\caption{EPFL Random Control Dataset}
\centering
\begin{tabular}{@{}c|lllllll@{}}
\toprule
split & name & \#node & \#edge & \#PI & \#PO & max level & \#cones \\ \midrule
\multirow{8}{*}{train} & router & 519 & 716 & 60 & 3 & 72 & 72 \\
 & i2c & 2378 & 3584 & 136 & 127 & 36 & 311 \\
 & int2float & 458 & 707 & 11 & 7 & 31 & 44 \\
 & mem\_ctrl & 84742 & 130550 & 1028 & 941 & 198 & 14234 \\
 & voter & 27721 & 40478 & 1001 & 1 & 136 & 3822 \\
 & ctrl & 328 & 495 & 7 & 25 & 19 & 64 \\
 & priority & 2043 & 2893 & 128 & 8 & 498 & 262 \\
 & dec & 320 & 616 & 8 & 256 & 4 & 256 \\ \midrule
\multirow{2}{*}{val} & cavlc & 1298 & 1981 & 10 & 11 & 32 & 146 \\
 & arbiter & 23488 & 35071 & 256 & 129 & 174 & 3714 \\ \midrule
Avg & - & 14329.5 & 21709.1 & 264.5 & 150.8 & 120 & 2292.5 \\ \bottomrule
\end{tabular}
\label{epfl_data}
\end{table}


\begin{table}[H]
\caption{ITC99 Dataset}
\centering
\begin{tabular}{@{}c|ccccccc@{}}
\toprule
split & name & \#node & \#edge & \#PI & \#PO & max level & \#cones \\ \midrule
\multirow{18}{*}{train} & b07\_opt\_C & 718 & 1029 & 50 & 49 & 48 & 121 \\
 & b17\_opt\_C & 47652 & 73743 & 1451 & 1442 & 104 & 8457 \\
 & b02\_opt\_C & 47 & 65 & 4 & 4 & 9 & 6 \\
 & b09\_opt\_C & 285 & 391 & 29 & 28 & 20 & 48 \\
 & b05\_opt\_C & 956 & 1428 & 35 & 55 & 67 & 166 \\
 & b15\_opt\_C & 14611 & 22542 & 485 & 449 & 95 & 2548 \\
 & b20\_opt\_C & 23788 & 36709 & 522 & 508 & 102 & 4083 \\
 & b13\_opt\_C & 538 & 720 & 62 & 53 & 23 & 75 \\
 & b11\_opt\_C & 999 & 1487 & 38 & 31 & 56 & 134 \\
 & b01\_opt\_C & 79 & 113 & 5 & 4 & 10 & 8 \\
 & b03\_opt\_C & 276 & 371 & 34 & 28 & 19 & 52 \\
 & b06\_opt\_C & 81 & 117 & 5 & 8 & 10 & 10 \\
 & b04\_opt\_C & 1105 & 1554 & 77 & 64 & 51 & 180 \\
 & b18\_opt\_C & 140638 & 217943 & 3306 & 3282 & 214 & 23214 \\
 & b22\_opt\_C & 34035 & 52319 & 735 & 719 & 103 & 6133 \\
 & b10\_opt\_C & 337 & 486 & 28 & 17 & 19 & 54 \\
 & b08\_opt\_C & 306 & 422 & 30 & 21 & 24 & 40 \\
 & b21\_opt\_C & 23888 & 36867 & 522 & 508 & 100 & 4403 \\ \midrule
\multirow{2}{*}{val} & b12\_opt\_C & 1861 & 2724 & 126 & 117 & 29 & 328 \\
 & b14\_opt\_C & 10502 & 16135 & 275 & 243 & 96 & 1751 \\ \midrule
Avg & - & 15135.1 & 23358.25 & 390.95 & 381.5 & 59.95 & 2590.55 \\ \bottomrule
\end{tabular}
\label{itc_data}
\end{table}




% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{multirow}


\subsection{Training Objective}
\label{detailed_training_objective}
The DeepGate4 model is trained on multiple tasks at both the gate-level and graph-level. To disentangle the functional and structural embeddings, we design training tasks with distinct labels to supervise each component.

\noindent\textbf{Gate-level Tasks.}
For function-related tasks at the gate-level, we incorporate the training tasks from DeepGate2, which involve predicting the logic-1 probability of gates and the pair-wise truth table distance. We sample gate pairs, $\mathcal{N}_{gate\_tt}$, and record their corresponding simulation responses as incomplete truth tables, $T_{i}$. The pair-wise truth table distance $D^{gate\_tt}$ is computed as follows:
\begin{equation}
    D^{gate\_tt}_{(i,j)} = \frac{HammingDistance(T_i, T_j)}{length(T_i)}, (i, j)\in \mathcal{N}_{gate\_tt}
\label{eq:tt_distance}
\end{equation}

The loss functions for gate-level functional tasks are:
\begin{equation} \label{Eq:loss:gatefunc}
    \begin{split}
        L_{gate}^{prob} & = L1Loss(p_k, MLP_{prob}(hf_k)), k \in \mathcal{V} \\
        L_{gate}^{tt\_pair} & = L1Loss(D^{gate\_tt}_{(i, j)}, MLP_{gate\_tt}(hf_i, hf_j)), (i, j) \in \mathcal{N}_{gate\_tt}
    \end{split}
\end{equation}

In addition, we incorporate supervision for structural learning by predicting pair-wise connections. Since DeepGate4 encodes the logic level as part of the structural encoding, we drop the task of predicting logic levels. The prediction of pair-wise connections is treated as a classification task, where a sampled gate pair $(i, j) \in \mathcal{N}_{gate\_con}$ can be classified into two categories: (1) there exists a path from $i$ to $j$ or from $j$ to $i$, or (2) otherwise. The loss function is defined as follows:
\begin{equation} \label{Eq:loss:gatestru}
    \begin{split}
        L_{gate}^{con} & = BCELoss(MLP_{con}(hs_i, hs_j)), (i, j) \in \mathcal{N}_{gate\_con}
    \end{split}
\end{equation}

\noindent\textbf{Graph-level Tasks.}
For each sub-graph, we perform a complete simulation to prepare the truth table, denoted as $T_s$. Additionally, we collect two structural characteristics for each sub-graph: the number of nodes $Size(s)$ and the depth $Depth(s)$. After obtaining the functional embedding $hf^s$ and structural embedding $hs^s$ via pooling in the Transformer, the following loss functions supervise the training, where $s \in \mathcal{S}$:
\begin{equation} \label{Eq:loss:graph}
    \begin{split}
        L_{graph}^{size} & = L1Loss(Size(s), MLP_{size}(hs^s)) \\ 
        L_{graph}^{depth} & = L1Loss(Depth(s), MLP_{depth}(hs^s)) \\ 
        L_{graph}^{tt} & = BCELoss(T_s, MLP_{tt}(hf^s)) 
    \end{split}
\end{equation}

We also introduce loss functions to capture pair-wise correlations between sub-graphs. The truth table distance $D_{(s_1, s_2)}^{graph\_tt}$ and graph edit distance~\citep{bunke1997relation} $D_{(s_1, s_2)}^{graph\_ged}$ between two sub-graphs ($s_1, s_2$) are predicted using the following formulas:
\begin{equation} \label{Eq:loss:graphpair}
    \begin{split}
        D_{(s_1, s_2)}^{graph\_tt} & = \frac{HammingDistance(T_{s_1}, T_{s_2})}{length(T_{s_1})} \\
        L_{graph}^{tt\_pair} & = L1Loss(D_{(s_1, s_2)}^{graph\_tt}, MLP_{graph\_tt}(hf^{s_1}, hf^{s_2})) \\ 
        D_{(s_1, s_2)}^{graph\_ged} & = GraphEditDistance(s_1, s_2) \\ 
        L_{graph}^{ged\_pair} & = L1Loss(D_{(s_1, s_2)}^{graph\_ged}, MLP_{graph\_ged}(hs^{s_1}, hs^{s_2})) \\
    \end{split}
\end{equation}

To link the gate-level and graph-level embeddings, we enable the model to predict whether gate $k$ belongs to sub-graph $s$ using the structural embeddings. The loss function is defined as:
\begin{equation} \label{Eq:loss:in}
    L_{in} = BCELoss(\{0, 1\}, MLP_{in}(hs_k, hs^{s}))
\end{equation}

\noindent\textbf{Error of Truth Table Prediction.}
For each 6-input sub-graph $s$ in the test dataset $\mathcal{S'}$, we predict the 64-bit truth table based on the graph-level functional embedding $hf^s$. The prediction error is calculated by the Hamming distance between the prediction and ground truth:
\begin{equation}
    P^{tt} = \frac{1}{len(\mathcal{S'})} \sum_{s}^{\mathcal{S'}} HammingDistance(T_s, MLP_{tt}(hf^s))
\end{equation}

\noindent\textbf{Accuracy of Gate Connection Prediction.}
Given the structural embedding of the gate pair $(i, j)$ in the test dataset $\mathcal{N}_{con}'$ and the binary label $y^{con}_{(i, j)} = \{0, 1\}$, we define the accuracy of gate connection prediction as:
\begin{equation}
    P^{con} = \frac{1}{len(\mathcal{N}_{con}')}\sum_{(i, j)}^{\mathcal{N}_{con}'}\mathbbm{1}(y^{con}_{(i, j)}, MLP_{con}(hs_i, hs_j))
\end{equation}

\noindent\textbf{Accuracy of Gate-in-Graph Prediction.}
For each gate-graph pair $(k, s)$ in the test dataset $\mathcal{N}_{in}'$, we predict whether the gate is included in the sub-graph based on the gate structural embedding $hs_k$ and the sub-graph structural embedding $hs^{s}$. The binary label is $y^{in}_{(k, s)} = \{0 ,1\}$. The accuracy is defined as:
\begin{equation}
    P^{in} = \frac{1}{len(\mathcal{N}_{in}')}\sum_{(k, s)}^{\mathcal{N}_{in}'} \mathbbm{1}(MLP_{in}(hs_k, hs^{s}),y_k^{in})
\end{equation}

\review{
\subsection{Ablation Study on Graph Partition Hyperparameters}
In this section, we include a detailed analysis of the hyperparameters $k$ and $\delta$. In our graph partition algorithm, $k$ denotes the maximum level of the cone, and $\delta$ denotes the stride. These parameters influence memory usage and overlap levels as follows:
\begin{itemize}
    \item $k$ (Maximum Level): $k$ determines the upper bound of the subgraph size. Specifically, the size of a subgraph is always smaller than $2^{k+1}-1$. Larger subgraphs require more GPU memory; for example, with the same mini-batch size, increasing $k$ significantly increases memory consumption.
    \item $\delta$ (Stride): $\delta$ determines the overlap region between subgraphs. The overlap level is defined as $k - \delta + 1$, which directly influences the inter-level message-passing ratio.
\end{itemize}
Furthermore, we provide an ablation study on $k$ and $\delta$, illustrating the sensitivity of our model to these hyperparameters. As shown in Table~\ref{tab:ablation_kd}, we conclude two observations from the ablation study. First, settings such as $(k=8, \delta=8)$, $(k=8, \delta=6)$, and $(k=8, \delta=4)$ demonstrate that our method is not sensitive to overlap ratios, as performance across these settings is similar.
Second, settings such as $(k=8, \delta=6)$, $(k=10, \delta=8)$, and $(k=6, \delta=4)$ maintain the same overlap level but vary in subgraph size. Results demonstrate that increasing $k$ significantly impacts GPU memory usage. Furthermore, larger $k$ will degrade structural task performance. This is because structural tasks rely more heavily on local information, especially for metrics like $L_{graph}^{ged\_pair}$, $L_{graph}^{size}$, and $L_{graph}^{depth}$ (See Section A.2).
}

\begin{table}[h]
\centering
\caption{Ablation Study on $k$ and $\delta$}
\begin{tabular}{@{}cc|cccc@{}}
\toprule
\multicolumn{2}{c|}{Setting} & \multicolumn{4}{c}{Metric} \\ \midrule
k & $\delta$ & Train Mem. & $L_{func}$ & $L_{stru}$ & $L_{all}$ \\ \midrule
8 & 8 & 12.62GB & 0.4649 $\pm$ 0.0017 & 2.4519 $\pm$ 0.0625 & 2.9168 $\pm$ 0.0639 \\
8 & 6 & 12.62GB & 0.4863 $\pm$ 0.0023 & 2.6783 $\pm$ 0.0739 & 3.1646 $\pm$ 0.0761 \\
8 & 4 & 12.62GB & 0.4713 $\pm$ 0.0034 & 2.5821 $\pm$ 0.0963 & 3.0534 $\pm$ 0.0933 \\
10 & 8 & 33.90GB & 0.4638 $\pm$ 0.0108 & 3.2055 $\pm$ 0.0747 & 3.6692 $\pm$ 0.0760 \\
6 & 4 & 6.59GB & 0.4629 $\pm$ 0.0065 & 2.6563 $\pm$ 0.0587 & 3.1192 $\pm$ 0.0567 \\ \bottomrule
\end{tabular}
\label{tab:ablation_kd}
\end{table}


\review{
\subsection{Comparsion on OpenABC-D}
\begin{table}[]
\caption{Comparsion on OpenABC-D benchmark.}
\label{tab:compare_openabcd}
\centering
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{@{}c|cc|cccc|c@{}}
\toprule
\multicolumn{1}{l|}{Model} & \multicolumn{1}{l}{Param.} & \multicolumn{1}{l|}{Mem.} & $L_{gate}^{prob}$ & $L_{gate}^{tt\_pair}$ & $L_{gate}^{con}$ & $P^{con}$ & $L_{all}$ \\ \midrule
GCN & 0.76M & 19.72G & 0.1600 $\pm$ 0.0484 & 0.1168 $\pm$ 0.0270 & 0.6926 $\pm$ 0.0808 & 59.93\% $\pm$ 5.89\% & 0.9695 $\pm$ 0.1168 \\
GraphSAGE & 0.89M & 23.23G & 0.0607 $\pm$ 0.0044 & 0.0745 $\pm$ 0.0063 & 0.6651 $\pm$ 0.0458 & 64.25\% $\pm$ 3.27\% & 0.8004 $\pm$ 0.0453 \\
GAT & 0.76M & 33.02G & 0.2036 $\pm$ 0.0142 & 0.1040 $\pm$ 0.0130 & 0.6293 $\pm$ 0.0178 & 64.94\% $\pm$ 1.87\% & 0.9370 $\pm$ 0.0283 \\
PNA & 2.75M & OOM & - & - & - & - & - \\ \midrule
DeepGate2 & 1.28M & 24.15G & 0.0406 $\pm$ 0.0004 & 0.0621 $\pm$ 0.0003 & 0.6976 $\pm$ 0.0079 & 63.16\% $\pm$ 0.77\% & 0.8003 $\pm$ 0.0083 \\
DeepGate3 & 8.17M & OOM & - & - & - & - & - \\
PolarGate & 0.88M & 44.48G & 0.7767 $\pm$ 0.3965 & 0.1179 $\pm$ 0.0615 & 0.9096 $\pm$ 0.1934 & 53.00\% $\pm$ 14.82\% & 1.8042 $\pm$ 0.3771 \\
HOGA-2 & 0.78M & 43.12G & 0.1635 $\pm$ 0.0004 & 0.0896 $\pm$ 0.0002 & 0.6245 $\pm$ 0.0004 & 64.81\% $\pm$ 0.42\% & 0.8777 $\pm$ 0.0005 \\
HOGA-5 & 0.78M & OOM & - & - & - & - & - \\ \midrule
DeepGate4 & 7.37M & 41.09G & \textbf{0.0233 $\pm$ 0.0010} & \textbf{0.0462 $\pm$ 0.0019} & \textbf{0.4789 $\pm$ 0.0180} & \textbf{79.00\% $\pm$ 0.30\%} & \textbf{0.5484 $\pm$ 0.0166} \\ \bottomrule
\end{tabular}
}
\end{table}
\noindent\textbf{Implementation Details}
We collect the circuits from OpenABC-D~\citep{openabcd}. All designs are transformed into AIGs by ABC tool~\citep{brayton2010abc}. The statistical details of datasets can be found in Section~\ref{sec:Dataset_Statistic}. We follow the experiment setting in Section~\ref{sec:exp_setting}. All experiments are performed on one L40 GPU with 48GB maximum memory. For training objectives, we use the gate-level tasks in Section~\ref{detailed_training_objective}.
\vspace{3pt} \\
\noindent\textbf{Comparison on Effectiveness}
DeepGate4 demonstrates outstanding effectiveness across all training tasks. As shown in Table~\ref{tab:compare_openabcd}, it achieves state-of-the-art performance on all gate-level tasks within the OpenABC-D datasets. Notably, DeepGate4 reduces the overall loss by 31.48\% compared to the second-best method. Moreover, while baseline models struggle with gate connection prediction, DeepGate4 significantly enhances performance in this area, achieving an accuracy of 79\%. This highlights the outstanding ability of DeepGate4 to capture the structural relationships between gates.
\vspace{3pt} \\
\noindent\textbf{Comparison on Efficiency}
In terms of efficiency, models like PNA and HOGA-5 encounter out-of-memory (OOM) errors, whereas DeepGate4 can successfully train a graph transformer on large circuits containing over 500k gates. 
}


\review{
\subsection{Logic Equivalence Checking}
Logic Equivalence Checking (LEC) is a critical task in Formal Verification, aimed at determining whether two designs are functionally equivalent. As circuit complexity grows, the significance of LEC increases since design errors in such systems can lead to costly fixes or operational failures in the final product.
\vspace{3pt} \\ 
We evaluate LEC on the ITC99 dataset by extracting subcircuits with multiple primary inputs (PIs) and a single primary output (PO). Given a subcircuit pair $(G_1, G_2)$, the model performs a binary classification task to predict whether $G_1$ and $G_2$ are equivalent. In the candidate pairs, only 1.29\% of pairs are equivalent, highlighting the challenge of imbalanced data. To assess performance, we use the widely adopted metrics Average Precision (AP) and Precision-Recall Area Under the Curve (PR-AUC). These metrics are threshold-independent and particularly effective for imbalanced datasets, where one class is significantly rarer than the other.
\\
\begin{table}[H]
\centering
\caption{Logic Equivalence Checking}
\label{tab:lec}
\begin{tabular}{@{}c|c|c@{}}
\toprule
\multicolumn{1}{l|}{Method} & \multicolumn{1}{l|}{AP} & \multicolumn{1}{l}{PR-AUC} \\ \midrule
GCN & 0.05 & 0.04 \\
GraphSAGE & 0.10 & 0.11 \\
GAT & 0.02 & 0.02 \\
PNA & 0.20 & 0.17 \\ \midrule
HOGA-5 & 0.03 & 0.03 \\
DeepGate2 & 0.13 & 0.13 \\
PolarGate & 0.03 & 0.21 \\ \midrule
DeepGate3 & OOM & OOM \\
DeepGate3$^\dag$ & 0.17 & 0.17 \\ \midrule
DeepGate4 & \textbf{0.31} & \textbf{0.30} \\ \bottomrule
\end{tabular}
\end{table}
Note that DeepGate3$^\dag$ denotes that we use our proposed updating strategy and training pipline. As shown in Table~\ref{tab:lec}, DeepGate4 outperforms all other methods by a significant margin, achieving the highest AP (0.31) and PR-AUC (0.30), and improve these two metrics by 55\% and 42\% respectively, compared to the second-best method. These values indicate its superior ability to balance precision and recall, especially in scenarios with imbalanced data. 
\vspace{3pt} \\
\subsection{Boolean Satisfiability Problem}
The Boolean Satisfiability (SAT) problem is a fundamental computational problem that determines whether a Boolean formula can evaluate to logic-1 for at least one variable assignment. As the first proven NP-complete problem, SAT serves as a cornerstone in computer science, with applications spanning fields such as scheduling, planning, and verification. Modern SAT solvers primarily utilize the conflict-driven clause learning (CDCL) algorithm, which efficiently handles path conflicts during the search process and explores additional constraints to reduce the search space. Over the years, various heuristic strategies have been developed to further accelerate CDCL in SAT solvers.
\vspace{3pt} \\
We follow the setting in DeepGate2~\citep{shi2023deepgate2}. We utilize the CaDiCal~\citep{queue2019cadical} SAT solver as the backbone solver and modify the variable decision heuristic based on it. In the Baseline setting, SAT problems are directly solved using the backbone SAT solver. For model-acclerated SAT solving, given a SAT instance, the first step is to encode the corresponding AIG to get the gate embedding. During the variable decision process, a decision value $d_i$ is assigned to variable $v_i$. If another variable $v_j$ with an assigned value $d_j$ is identified as correlated to $v_i$, the reversed value $d_j'$ is assigned to $v_i$, i.e., $d_i = 0\ if\ d_j = 1$ or $d_i = 1\ if\ d_j = 0$. The determination of correlated variables relies on their functional similarity, and the similarity $Sim(v_i, v_j)$ exceeding the threshold $\theta$ indicates correlation.
\\
\begin{table}[]
\centering
\caption{SAT solving time comparsion.$^\dag$ denotes that we use our updating strategy.}
\label{tab:sat}
\begin{tabular}{@{}c|c|ccccc|c@{}}
\toprule
\multirow{2}{*}{Case} & Name & ad44 & f20 & ab18 & ac1 & ad14 & Avg. \\ \cmidrule(l){2-8} 
 & Size & 44949 & 27806 & 37275 & 42038 & 44949 & 39403.4 \\ \midrule
Baseline & Solving Time & 918.21 & 1046.31 & 3150.81 & 5522.85 & 5766.85 & 3281.01 \\ \midrule
\multirow{2}{*}{DeepGate3$^\dag$} & Model Runtime & 27.73 & 16.57 & 22.60 & 33.17 & 27.27 & 25.47 \\
 & Solving Time & 678.42 & 952.91 & 1607.06 & 6189.61 & 4413.96 & 2768.39 \\ \midrule
\multirow{2}{*}{PolarGate} & Model Runtime & 0.01 & 0.01 & 0.01 & 0.24 & 0.01 & 0.06 \\
 & Solving Time & \textbf{606.74} & 1154.87 & \textbf{1000.02} & 3923.88 & \textbf{3222.98} & 1981.70 \\ \midrule
\multirow{2}{*}{Exphormer$^\dag$} & Model Runtime & 0.74 & 0.51 & 0.62 & 0.64 & 0.97 & 0.70 \\
 & Solving Time & 885.98 & 1177.07 & 1293.57 & 4156.04 & 3387.24 & 2179.98 \\ \midrule
\multirow{2}{*}{DeepGate4} & Model Runtime & 3.65 & 2.80 & 3.10 & 3.33 & 3.62 & 3.30 \\
 & Solving Time & 970.28 & \textbf{143.09} & 1351.49 & \textbf{393.25} & 4268.57 & \textbf{1425.34} \\ \bottomrule
\end{tabular}
\end{table}
\\
The results are shown in Table~\ref{tab:sat}. Since SAT solving is time-consuming, we compare our approach only with the top-3 methods listed in Table~\ref{tab:detail_compare}, namely DeepGate3$^\dag$, Exphormer$^\dag$, and PolarGate. The Baseline represents using the SAT solver without any model-based acceleration. Leveraging its exceptional ability to understand the functional relationships within circuits, DeepGate4 achieves a substantial reduction in SAT solving time, with an 86.33\% reduction for case \textit{f20} and an 92.90\% reduction for case \textit{ac1}. Regarding average solving time, it achieves a 56.56\% reduction, outperforming all other methods. These results highlight DeepGate4â€™s strong generalization capability and effectiveness in addressing real-world SAT solving challenges.
}