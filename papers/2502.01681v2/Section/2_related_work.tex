\section{Related Work}
\vspace{-10pt}
% \subsection{Circuit Representation Learning}
\paragraph{Circuit Representation Learning}
Circuit representation learning has become a pivotal area in electronic design automation (EDA), reflecting the broader trend in AI of learning general representations for diverse downstream tasks. In this domain, the DeepGate family~\citep{li2022deepgate, shi2023deepgate2} emerges as pioneering approachs, exploring GNNs to encode AIGs and enabling support for a variety of EDA tasks such as testability analysis~\citep{shi2022deeptpi}, power estimation~\citep{khan2023deepseq}, and SAT solving~\citep{li2023eda, shi2024eda}. The Gamora ~\citep{wu2023gamora} and HOGA ~\citep{deng2024less} further extend reasoning capabilities by representing both logic gates and cones. PolarGate~\citep{PolarGate} seeks to overcome functionality representation bottlenecks by leveraging ambipolar state principles. 

Considering the inherent limitation of GNNs, e.g. over-squashing or over-smoothing, recent work DeepGate3~\citep{shi2024deepgate3}, as illustrated in Figure~\ref{fig:DG2_DG3}, utilizes DeepGate2 as a tokenizer and then leverages the global aggregation mechanism of transformers with a connective mask to enhance circuit representation. However, new challenges arise when scaling to large AIGs: transformer-based models suffer from quadratic complexity, making training on large AIGs impractical.
\vspace{-5pt}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figure/DG2_DG3.pdf}
    \vspace{-12pt}
    \caption{The overview of DeepGate2 and DeepGate3}
    \label{fig:DG2_DG3}
    \vspace{-19pt}
\end{figure}

% \subsection{Advances and Challenges in Sparse Graph Transformer and Sub-Linear GNNs for Large-Scale Circuit Design}
% \subsection{Graph Transformer and Sub-Linear GNNs}
\paragraph{Advances and Challenges in Graph Transformers and Sub-Linear GNNs for Large-Scale Circuits}
% In this section, we discuss the advances and challenges in sparse graph transformers and sub-linear GNNs for large-scale circuit design. 
Graph transformer models typically operate on fully-connected graphs, where every pair of nodes is connected, regardless of the original graph's structure. SAN~\citep{SAN}, Graphormer~\citep{Graphormer}, GraphiT~\citep{GraphiT}, and GraphGPS~\citep{GraphGPS} apply dense attention mechanisms with various positional and structural encodings. While these methods deliver outstanding performance, the quadratic complexity makes them impractical for large graphs. Recent approaches, such as Exphormer~\citep{Exphormer}, Nodeformer~\citep{Nodeformer}, NAGformer~\citep{NAGphormer}, and DAGformer~\citep{DAGformer}, leverage the sparse patterns of graphs to employ sparse transformers, reducing complexity to linear. However, even with these improvements, applying them to circuits with millions of gates remains challenging.

Sub-linear GNNs, such as GNNAutoScale~\citep{GNNAutoScale} and SketchGNN~\citep{SketchGNN} tackle this issue by incorporating historical embeddings during training, reducing memory complexity by reusing embeddings from prior iterations. This allows for constant GPU memory consumption relative to graph size. GraphFM~\citep{GraphFM} improves the  historical embeddings updating by introducing feature momentum. However, applying them to AIGs remains challenging since they disregard the causal relationships between sub-graphs by applying completely random sampling. Specifically, when modeling circuit functionality as a computational graph, it is essential to follow a strict topological order, reasoning from primary inputs (PIs) to primary outputs (POs) based on logic levels~\citep{li2022deepgate}. 

\vspace{-10pt}
% \subsection{Efficient GNN Systems}
% \vspace{-5pt}

\paragraph{The Necessity of System-Level GNN Optimizations for Circuit Processing}
% In this section, we discuss the necessity of system-level GNN optimizations for efficient  circuit processing.
System-level optimization of GNNs aims to reduce memory consumption and accelerate inference and training time, thereby improving the efficiency of GNN execution. 
% Distributed training is commonly employed for extremely large graphs that cannot fit on a single GPU. \citet{distributed} points out that the main components of this approach include data partitioning~\citep{aligraph, distgnn}, GNN batch generation~\citep{distdgl, bgl, psgd-pa}, model execution~\citep{gnnlab, distdglv2}, and communication~\citep{neugraph, p3, g3}. 
Single GPU systems primarily optimize through operator reorganization, operator fusion, and data flow optimization. FuseGNN~\citep{fusegnn} accelerates the computation process by fusing any two edge-centric operators and storing intermediate data from the forward pass. However, it still consumes a large amount of memory. Fused-GAT~\citep{fusegat}, recognized as the state-of-the-art approach, reduces redundant computations by postponing the propagation operator. It has been widely adopted in PyTorch Geometric (PyG)~\citep{pyg} implementations of the GAT network (e.g., GATConv, FuseGATConv), and its fused operators and recomputation strategy significantly reduce the memory required for execution. 
% However, existing GNN acceleration methods like FusedGAT were developed based on social networks and citation network datasets, 
% However, existing GNN acceleration methods like Fused-GAT were developed based on social network and citation network datasets, where the number of edges per node follows a power-law distribution~\citep{powerlaw}, unlike AIGs, which have a uniform distribution (with only 1 or 2 edges) and much few edges, hence they perform inefficiently on AIG graphs due to unbalanced work allocation and high synchronization overhead.
However, existing GNN acceleration techniques, such as Fused-GAT, were primarily designed for social network and citation network datasets, where the node degree follows a power-law distribution~\citep{powerlaw}. In contrast, AIGs exhibit a uniform node degree distribution and have significantly fewer edges (1 or 2 edges per node). 
Consequently, these methods perform suboptimally on AIG graphs due to imbalanced workload and substantial synchronization overhead.

\vspace{-5pt}

