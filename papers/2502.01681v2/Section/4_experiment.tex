\vspace{-10pt}
\section{Experiment}

\vspace{-5pt}
\subsection{Experiment Setting}
\label{sec:exp_setting}
\vspace{-5pt}
\noindent\textbf{Dataset}
We collect the circuits from various sources, including benchmark netlists in ITC99~\citep{ITC99} and EPFL~\citep{EPFLBenchmarks}. All designs are transformed into AIGs by ABC tool~\citep{brayton2010abc}. The statistical details of datasets can be found in Section~\ref{sec:Dataset_Statistic}.
% \vspace{-5pt}

\noindent\textbf{Implementation Details}
We partition the large circuits into small cones. In Algorithm~\ref{alg:partition}, we set $k$ to 8 and $\delta$ to 6.
The dimensions of both the structural and functional embedding are set to $128$. The depth of Sparse Transformer is $12$ and the depth of Pooling Transformer is $2$. All training task heads are 3-layer multilayer perceptrons (MLPs). 
We train all models for $200$ epochs to ensure convergence. The training is performed with a batch size of $1$ and mini-batch size of $128$ on one Nvidia A800 GPU. We utilize the Adam optimizer with a learning rate of $10^{-4}$. We report the average performance and standard deviation of the last 5 epochs, and losses without balanced weight.

\vspace{-5pt}
\subsection{Main Result}
\vspace{-5pt}
\begin{table}[]
\caption{Detailed comparison experiment on ITC99 benchmark. $^\dag$We use our graph partition and updating strategy instead of full-batch training.}
\vspace{-5pt}
\footnotesize
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{1.5pt}
% \renewcommand{\arraystretch}{1}
\begin{tabular}{@{}c|cc|cccc|cccccccc|c@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Training} & \multicolumn{4}{c|}{Gate-level} & \multicolumn{8}{c|}{Graph-level} & \multirow{2}{*}{$L_{all}$} \\ \cmidrule(lr){2-15}
 & Param. & Mem. & $L_{gate}^{prob}$ & $L_{gate}^{tt\_pair}$ & $L_{gate}^{con}$ & $P^{con}$ & $L_{graph}^{tt}$ & $P^{tt}$ & $L_{graph}^{tt\_pair}$ & $L_{graph}^{ged\_pair}$ & $L_{graph}^{size}$ & $L_{graph}^{depth}$ & $L_{in}$ & $P^{in}$ &  \\ \midrule
GCN & 0.76M & 31.38G & 0.177 & 0.114 & 0.616 & 66.34\% & 0.589 & 0.325 & 0.1596 & 0.215 & 2.65 & 1.0622 & 1.065 & 47.93\% & 6.65 \\
GraphSAGE & 0.89M & 31.78G & 0.115 & 0.079 & 0.600 & 68.33\% & 0.548 & 0.290 & 0.1595 & 0.203 & 2.30 & 0.9628 & 0.884 & 51.04\% & 5.85 \\
GAT & 0.76M & 34.10G & 0.270 & 0.136 & 0.605 & 66.82\% & 0.588 & 0.323 & 0.1601 & 0.396 & 5.32 & 0.8464 & 0.995 & 47.94\% & 9.32 \\
PNA & 2.75M & 41.99G & 0.091 & 0.079 & 0.601 & 68.19\% & 0.518 & 0.266 & 0.1593 & 0.181 & 3.50 & 1.0114 & 0.810 & 56.27\% & 6.95 \\ \midrule
GraphGPS & 6.71M & OOM & - & - & - & - & - & - & - & - & - & - & - & - & - \\
Exphormer & 0.74M & OOM & - & - & - & - & - & - & - & - & - & - & - & - & - \\
DAGformer & 1.90M & OOM & - & - & - & - & - & - & - & - & - & - & - & - & - \\ \midrule
DeepGate2 & 1.28M & 32.87G & 0.049 & 0.068 & \textbf{0.594} & 68.77\% & 0.513 & 0.274 & 0.1570 & 0.238 & 3.08 & 0.6772 & 0.902 & 48.62\% & 6.28 \\
DeepGate3 & 8.17M & OOM & - & - & - & - & - & - & - & - & - & - & - & - & - \\
PolarGate & 0.88M & 35.95G & 0.226 & 0.100 & 0.699 & 65.92\% & 0.588 & 0.326 & 0.1593 & 0.237 & 2.62 & 0.3705 & 0.688 & 52.42\% & 5.69 \\
\review{HOGA-5} & 0.78M & 42.48G & 0.204 & 0.117 & 0.609 & 68.74\% & 0.493 & 0.254 & 0.1624 & 0.141 & 3.56 & 1.1378 & 0.571 & 68.99\% & 6.99 \\ \midrule
GraphGPS$^\dag$ & 6.71M & 7.42G & 0.109 & 0.090 & 0.632 & 66.11\% & 0.434 & 0.178 & 0.1612 & 0.195 & 3.43 & 0.0061 & 0.742 & 54.62\% & 5.77 \\
Exphormer$^\dag$ & 0.74M & 6.64G & 0.101 & 0.078 & 0.674 & 59.89\% & 0.349 & 0.143 & 0.1160 & 0.191 & 2.32 & \textbf{0.0024} & 0.692 & 59.09\% & 4.50 \\
DAGformer$^\dag$ & 1.90M & 9.52G & 0.204 & 0.116 & 0.660 & 67.53\% & 0.540 & 0.243 & 0.1749 & 0.217 & 4.04 & 0.3799 & 0.705 & 57.99\% & 7.01 \\
DeepGate3$^\dag$ & 8.17M & 50.75G & 0.055 & 0.061 & 0.597 & \textbf{68.93\%} & 0.315 & \textbf{0.133} & \textbf{0.0780} & 0.125 & 1.93 & 0.0030 & 0.609 & 68.36\% & 3.76 \\ \midrule
DeepGate4 & 7.37M & 7.53G & \textbf{0.043} & \textbf{0.055} & 0.600 & 67.22\% & \textbf{0.315} & 0.136 & 0.0803 & \textbf{0.117} & \textbf{1.45} & 0.0591 & \textbf{0.461} & \textbf{79.50\%} & \textbf{3.16} \\ \bottomrule
\end{tabular}
}
\label{tab:detail_compare}
\vspace{-20pt}
\end{table}


We compare the performance of our model with other methods on both the ITC99 and EPFL benchmarks. Table~\ref{tab:detail_compare} presents a detailed comparison of the ITC99 benchmark across various training tasks. GNNs, such as GCN~\citep{GCN}, GraphSAGE~\citep{GraphSAGE}, GAT~\citep{GAT}, PNA~\citep{PNA}, DeepGate2~\citep{shi2023deepgate2}, and PolarGate~\citep{PolarGate}, consume approximately 30-40 GB of GPU memory when training on ITC99, which has a maximum graph size of 140K gates. This suggests that training GNNs on circuits with more than 500K gates is impractical due to memory constraints. Sparse transformer models, such as GraphGPS~\citep{GraphGPS}, Exphormer~\citep{Exphormer}, and DAGformer~\citep{DAGformer}, also encounter OOM errors when attempting to train on ITC99, despite their linear complexity. However, with our graph partitioning and updating strategy, even dense transformer models like DeepGate3~\citep{shi2024deepgate3} can be successfully trained on ITC99.


\noindent\textbf{Comparison on Effectiveness}
In terms of effectiveness, DeepGate4 demonstrates superior results across most training tasks. As shown in Table~\ref{tab：compare_itc_epfl}, DeepGate4 achieves state-of-the-art performance on both functional and structural tasks across the ITC99 and EPFL datasets. Regarding overall performance, DeepGate4 reduces the overall loss by 15.5\% and 31.1\%, respectively, compared to the second-best method. Furthermore, with the proposed structural encoding, DeepGate4 achieves a reduction of 16.4\% and 34.9\% in structural loss on the ITC99 and EPFL datasets, respectively.

\noindent\textbf{Comparison on Efficiency}
In terms of efficiency, compared to DeepGate3$^\dag$, DeepGate4 reduces inference time and memory usage by 77.9\% and 92.7\% on ITC99, and by 87.8\% and 95.2\% on EPFL. Furthermore, with our proposed inference optimization, Fused-DeepGate4 (Fused-DG4) reduces inference time and memory usage by 41.4\% and 51.4\% on ITC99, and by 28.2\% and 30.0\% on EPFL, compared to DeepGate4.


\vspace{-10pt}
\subsection{Performance over Circuit of Different Scale}
\vspace{-5pt}
In this section, we discuss our model's performance across circuits of varying scales and its generalizability to Out-Of-Distribution (OOD) circuits. We trained our model on the ITC99 dataset, following the split outlined in Table~\ref{itc_data}. During training, the average graph size is 15k, while for evaluation, we used circuits of different scales, as shown in Table~\ref{tab:multiscale_dataset}. 

\begin{minipage}[]{0.55\linewidth}
    We extract 128 small circuits from ITC99 to ensure stable evaluation results. The average number of nodes and edges are listed as \textsc{small (avg.)} in Table~\ref{tab:multiscale_dataset}. 
    \textsc{b12\_opt\_C} and \textsc{b14\_opt\_C} are the original designs collected from ITC99, while \textsc{mem\_ctrl} is collected from EPFL. Another \textsc{Image\_Processing} is the handmade design to implement multiple modes of image transformations. We employ Synopsys Design Compiler 2019.12 with skywater 130nm technology library to produce the netlist and subsequently convert it into AIG by ABC~\citep{brayton2010abc}. 
\end{minipage}
\hspace{5pt}
\begin{minipage}[]{0.4\linewidth}
\captionof{table}{Validation dataset with different scale circuits.}
\label{tab:multiscale_dataset}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|lll@{}}
\toprule
name & \#node & \#edge & max level \\ \midrule
small (avg.) & 161.6 & 193.9 & 46 \\
b12\_opt\_C & 1,861 & 2,724 & 29 \\
b14\_opt\_C & 10,502 & 16,135 & 96 \\
mem\_ctrl & 84,742 & 130,550 & 198 \\
Image\_Processing & 402,193 & 506,340 & 27 \\ \bottomrule
\end{tabular}
}
\end{minipage}




% \vspace{-10pt}



We present the results for circuits of varying scales in Figure~\ref{fig:func_loss} and Figure~\ref{fig:stru_loss}, from which we draw three key observations. 
First, GNNs struggle to perform well across circuits of varying scales, while transformer-based models, such as DeepGate3$^\dag$ and DeepGate4, exhibits superior performance on both functional and structural tasks, which suggests that global aggregation mechanism is crucial in circuit representation learning. 
Second, with our proposed partitioning method and updating strategy, both DeepGate3$^\dag$ and DeepGate4 exhibit strong generalizability.
Despite being trained on circuits averaging 15K gates, the performance on the \textsc{Image\_Processing} demonstrates DeepGate3$^\dag$ and DeepGate4 maintain outstanding performance on OOD circuits. 
Last, DeepGate4 shows stable performance across circuits of different scales, with a standard deviation of 0.46 on overall loss. In contrast, GNNs show unstable performance, particularly in structural loss with a standard deviation of 4.38, as highlighted in Figure~\ref{fig:stru_loss}.



% As illustrated in Figure~\ref{fig:func_loss} and Figure~\ref{fig:stru_loss}, DeepGate4 exhibits superior performance on both functional and structural tasks, across circuits ranging in size from 161 to 400K gates. It's important to note that during training, the average circuit size is 15K gates, and the maximum size is 140K gates. The performance of DeepGate4 on the \textsc{Image\_Processing} demonstrates that by partitioning and encoding circuits level by level, our model exhibits strong generalizability to OOD circuits. In contrast, GNNs show unstable performance across circuits of different scales, particularly on structural tasks, as highlighted in Figure~\ref{fig:stru_loss}.

% \begin{table}[H]
% \caption{Comparison on ITC99 and EPFL Random Control Benchmark.}
% \resizebox{\textwidth}{!}{
% \setlength{\tabcolsep}{2pt}
% % \renewcommand{\arraystretch}{1}
% \begin{tabular}{@{}ccccccccccc@{}}
% \toprule
% \multirow{3}{*}{Method} & \multicolumn{5}{c}{ITC99} & \multicolumn{5}{c}{EPFL   Random Control} \\ \cmidrule(l){2-11} 
%  & \multicolumn{2}{c|}{Inference Stage} & \multicolumn{3}{c|}{Performance} & \multicolumn{2}{c|}{Inference Stage} & \multicolumn{3}{c}{Performance} \\ \cmidrule(l){2-11} 
%  & Time(s) & \multicolumn{1}{c|}{Mem.(MB)} & $L_{func}$ & $L_{stru}$ & \multicolumn{1}{c|}{$L_{all}$} & Time(s) & \multicolumn{1}{c|}{Mem.(MB)} & $L_{func}$ & $L_{stru}$ & $L_{all}$ \\ \midrule
% \multicolumn{1}{c|}{GCN} & 0.297 & \multicolumn{1}{c|}{415} & 1.0385 $\pm$ 0.0241 & 5.6128 $\pm$ 0.4783 & \multicolumn{1}{c|}{6.6512 $\pm$ 0.4713} & 0.286 & \multicolumn{1}{c|}{705} & 1.0206 $\pm$ 0.0275 & 21.1565 $\pm$ 3.0231 & 22.1770 $\pm$ 3.0024 \\
% \multicolumn{1}{c|}{GraphSAGE} & 0.020 & \multicolumn{1}{c|}{415} & 0.9010 $\pm$ 0.0217 & 4.9457 $\pm$ 0.4031 & \multicolumn{1}{c|}{5.8467 $\pm$ 0.3911} & 0.025 & \multicolumn{1}{c|}{706} & 0.9431 $\pm$ 0.0144 & 5.9391 $\pm$ 0.9232 & 6.8822 $\pm$ 0.9370 \\
% \multicolumn{1}{c|}{GAT} & 0.029 & \multicolumn{1}{c|}{415} & 1.1549 $\pm$ 0.0108 & 8.1671 $\pm$ 1.1106 & \multicolumn{1}{c|}{9.3220 $\pm$ 1.1023} & 0.035 & \multicolumn{1}{c|}{705} & 1.1275 $\pm$ 0.0199 & 14.4599 $\pm$ 1.3354 & 15.5874 $\pm$ 1.3512 \\
% \multicolumn{1}{c|}{PNA} & 0.042 & \multicolumn{1}{c|}{423} & 0.8466 $\pm$ 0.0099 & 6.1032 $\pm$ 2.0624 & \multicolumn{1}{c|}{6.9497 $\pm$ 2.0654} & 0.059 & \multicolumn{1}{c|}{713} & 0.8795 $\pm$ 0.0097 & 10.0961 $\pm$ 2.2176 & 10.9755 $\pm$ 2.2134 \\
% \multicolumn{1}{c|}{DeepGate2} & 0.490 & \multicolumn{1}{c|}{412} & 0.7866 $\pm$ 0.0019 & 5.4924 $\pm$ 0.1571 & \multicolumn{1}{c|}{6.2790 $\pm$ 0.1584} & 0.470 & \multicolumn{1}{c|}{694} & 0.9027 $\pm$ 0.0041 & 25.7826 $\pm$ 1.5458 & 26.6853 $\pm$ 1.5456 \\
% \multicolumn{1}{c|}{PolarGate} & 0.030 & \multicolumn{1}{c|}{416} & 1.0736 $\pm$ 0.0142 & 4.6188 $\pm$ 0.1577 & \multicolumn{1}{c|}{5.6924 $\pm$ 0.1620} & 0.033 & \multicolumn{1}{c|}{707} & 1.0585 $\pm$ 0.0086 & 9.3951 $\pm$ 2.8626 & 10.4536 $\pm$ 2.8553 \\ \midrule
% \multicolumn{1}{c|}{GraphGPS$^\dag$} & 0.512 & \multicolumn{1}{c|}{480} & 0.7820 $\pm$ 0.0203 & 4.9922 $\pm$ 0.1722 & \multicolumn{1}{c|}{5.7742 $\pm$ 0.1743} & 0.650 & \multicolumn{1}{c|}{906} & 1.4350 $\pm$ 0.0176 & 11.1462 $\pm$ 0.5529 & 12.5811 $\pm$ 0.5533 \\
% \multicolumn{1}{c|}{Exphormer$^\dag$} & 0.441 & \multicolumn{1}{c|}{337} & 0.6351 $\pm$ 0.0019 & 3.8622 $\pm$ 0.2069 & \multicolumn{1}{c|}{4.4973 $\pm$ 0.2073} & 0.661 & \multicolumn{1}{c|}{117} & 0.8460 $\pm$ 0.0267 & 5.5885 $\pm$ 0.5658 & 6.4345 $\pm$ 0.5768 \\
% \multicolumn{1}{c|}{DAGformer$^\dag$} & 0.676 & \multicolumn{1}{c|}{1324} & 1.0219 $\pm$ 0.0028 & 5.9853 $\pm$ 0.2229 & \multicolumn{1}{c|}{7.0071 $\pm$ 0.2225} & 0.886 & \multicolumn{1}{c|}{292} & 1.3263 $\pm$ 0.0189 & 7.1068 $\pm$ 0.1001 & 8.4331 $\pm$ 0.0905 \\
% \multicolumn{1}{c|}{DeepGate3$^\dag$} & 11.322 & \multicolumn{1}{c|}{6565} & 0.5287 $\pm$ 0.0264 & 3.2059 $\pm$ 0.1521 & \multicolumn{1}{c|}{3.7347 $\pm$ 0.1484} & 18.349 & \multicolumn{1}{c|}{2730} & 1.1589 $\pm$ 0.0916 & 6.9678 $\pm$ 0.6299 & 8.1267 $\pm$ 0.6966 \\ \midrule
% \multicolumn{1}{c|}{DeepGate4} & 2.496 & \multicolumn{1}{c|}{479} & \textbf{0.4863 $\pm$ 0.0023} & \textbf{2.6783 $\pm$ 0.0739} & \multicolumn{1}{c|}{\textbf{3.1646 $\pm$ 0.0761}} & 2.263 & \multicolumn{1}{c|}{130} & \textbf{0.7912 $\pm$ 0.0208} & \textbf{3.6373 $\pm$ 0.5828} & \textbf{4.4286 $\pm$ 0.5765} \\
% \multicolumn{1}{c|}{Fused-DG4} & 1.463 & \multicolumn{1}{c|}{233} & - & - & \multicolumn{1}{c|}{-} & 1.624 & \multicolumn{1}{c|}{91} & - & - & - \\ \bottomrule
% \end{tabular}
% \label{tab：compare_itc_epfl}
% }
% \vspace{-10pt}
% \end{table}

% \begin{table}[H]
% \vspace{-5pt}
% \caption{Comparison on ITC99 and EPFL Random Control Benchmark.}
% \vspace{-5pt}
% \resizebox{\textwidth}{!}{
% \setlength{\tabcolsep}{2pt}
% \begin{tabular}{@{}c|ccccc|ccccc@{}}
% \toprule
% \multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{5}{c|}{ITC99}                     & \multicolumn{5}{c}{EPFL}                     \\ \cmidrule(l){2-11} 
% \multicolumn{1}{c|}{}                        & Time & \multicolumn{1}{l|}{Mem.} & \multicolumn{3}{c|}{Performance} & Time & \multicolumn{1}{l|}{Mem.} & \multicolumn{3}{c}{Performance} \\
% \multicolumn{1}{c|}{}                        & (s)  & \multicolumn{1}{l|}{(MB)} & $L_{func}$         & $L_{stru}$         & $L_{all}$        & (s)  & \multicolumn{1}{l|}{(MB)} & $L_{func}$         & $L_{stru}$         & $L_{all}$        \\ \midrule
% GCN & 0.297 & \multicolumn{1}{c|}{415} & 1.04 $\pm$ 0.024 & 5.61 $\pm$ 0.478 & \multicolumn{1}{c|}{6.65 $\pm$ 0.471} & 0.286 & \multicolumn{1}{c|}{705} & 1.02 $\pm$ 0.028 & 21.16 $\pm$ 3.023 & 22.18 $\pm$ 3.002 \\
% GraphSAGE & 0.020 & \multicolumn{1}{c|}{415} & 0.90 $\pm$ 0.022 & 4.95 $\pm$ 0.403 & \multicolumn{1}{c|}{5.85 $\pm$ 0.391} & 0.025 & \multicolumn{1}{c|}{706} & 0.94 $\pm$ 0.014 & 5.94 $\pm$ 0.923 & 6.88 $\pm$ 0.937 \\
% GAT & 0.029 & \multicolumn{1}{c|}{415} & 1.15 $\pm$ 0.011 & 8.17 $\pm$ 1.111 & \multicolumn{1}{c|}{9.32 $\pm$ 1.102} & 0.035 & \multicolumn{1}{c|}{705} & 1.13 $\pm$ 0.020 & 14.46 $\pm$ 1.335 & 15.59 $\pm$ 1.351 \\
% PNA & 0.042 & \multicolumn{1}{c|}{423} & 0.85 $\pm$ 0.010 & 6.10 $\pm$ 2.062 & \multicolumn{1}{c|}{6.95 $\pm$ 2.065} & 0.059 & \multicolumn{1}{c|}{713} & 0.88 $\pm$ 0.010 & 10.10 $\pm$ 2.218 & 10.98 $\pm$ 2.213 \\
% DeepGate2 & 0.490 & \multicolumn{1}{c|}{412} & 0.79 $\pm$ 0.002 & 5.49 $\pm$ 0.157 & \multicolumn{1}{c|}{6.28 $\pm$ 0.158} & 0.470 & \multicolumn{1}{c|}{694} & 0.90 $\pm$ 0.004 & 25.78 $\pm$ 1.546 & 26.69 $\pm$ 1.546 \\
% PolarGate & 0.030 & \multicolumn{1}{c|}{416} & 1.07 $\pm$ 0.014 & 4.62 $\pm$ 0.158 & \multicolumn{1}{c|}{5.69 $\pm$ 0.162} & 0.033 & \multicolumn{1}{c|}{707} & 1.06 $\pm$ 0.009 & 9.40 $\pm$ 2.863 & 10.45 $\pm$ 2.855 \\ \midrule
% GraphGPS$^\dag$ & 0.512 & \multicolumn{1}{c|}{480} & 0.78 $\pm$ 0.020 & 4.99 $\pm$ 0.172 & \multicolumn{1}{c|}{5.77 $\pm$ 0.174} & 0.650 & \multicolumn{1}{c|}{906} & 1.44 $\pm$ 0.018 & 11.15 $\pm$ 0.553 & 12.58 $\pm$ 0.553 \\
% Exphormer$^\dag$ & 0.441 & \multicolumn{1}{c|}{337} & 0.64 $\pm$ 0.002 & 3.86 $\pm$ 0.207 & \multicolumn{1}{c|}{4.50 $\pm$ 0.207} & 0.661 & \multicolumn{1}{c|}{117} & 0.85 $\pm$ 0.027 & 5.59 $\pm$ 0.566 & 6.43 $\pm$ 0.577 \\
% DAGformer$^\dag$ & 0.676 & \multicolumn{1}{c|}{1,324} & 1.02 $\pm$ 0.003 & 5.99 $\pm$ 0.223 & \multicolumn{1}{c|}{7.01 $\pm$ 0.223} & 0.886 & \multicolumn{1}{c|}{292} & 1.33 $\pm$ 0.019 & 7.11 $\pm$ 0.100 & 8.43 $\pm$ 0.091 \\
% DeepGate3$^\dag$ & 11.322 & \multicolumn{1}{c|}{6,565} & 0.53 $\pm$ 0.026 & 3.21 $\pm$ 0.152 & \multicolumn{1}{c|}{3.73 $\pm$ 0.148} & 18.349 & \multicolumn{1}{c|}{2,730} & 1.16 $\pm$ 0.092 & 6.97 $\pm$ 0.630 & 8.13 $\pm$ 0.697 \\ \midrule
% DeepGate4 & 2.496 & \multicolumn{1}{c|}{479} & \textbf{0.49 $\pm$ 0.002} & \textbf{2.68 $\pm$ 0.074} & \multicolumn{1}{c|}{\textbf{3.16 $\pm$ 0.076}} & 2.263 & \multicolumn{1}{c|}{130} & \textbf{0.79 $\pm$ 0.021} & \textbf{3.64 $\pm$ 0.583} & \textbf{4.43 $\pm$ 0.577} \\
% Fused-DG4 & 1.463 & \multicolumn{1}{c|}{233} & - & - & \multicolumn{1}{c|}{-} & 1.624 & \multicolumn{1}{c|}{91} & - & - & - \\ \bottomrule
% \end{tabular}
% \label{tab：compare_itc_epfl}
% }
% \vspace{-10pt}
% \end{table}

\begin{table}[H]
\vspace{-5pt}
\caption{Comparison on ITC99 and EPFL Random Control Benchmark.}
\vspace{-5pt}
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{2pt}
\begin{tabular}{@{}c|ccccc|ccccc@{}}
\toprule
\multirow{3}{*}{Method} & \multicolumn{5}{c|}{ITC99} & \multicolumn{5}{c}{EPFL Random Control} \\ \cmidrule(l){2-11} 
 & \multicolumn{2}{c|}{Inference Stage} & \multicolumn{3}{c|}{Performance} & \multicolumn{2}{c|}{Inference Stage} & \multicolumn{3}{c}{Performance} \\ \cmidrule(l){2-11} 
 & Time(s) & \multicolumn{1}{c|}{Mem.(MB)} & $L_{func}$ & $L_{stru}$ & $L_{all}$ & Time(s) & \multicolumn{1}{c|}{Mem.(MB)} & $L_{func}$ & $L_{stru}$ & $L_{all}$ \\ \midrule
GCN & 0.297 & \multicolumn{1}{c|}{415} & 1.04 $\pm$ 0.024 & 5.61 $\pm$ 0.478 & 6.65 $\pm$ 0.471 & 0.286 & \multicolumn{1}{c|}{705} & 1.02 $\pm$ 0.028 & 21.16 $\pm$ 3.023 & 22.18 $\pm$ 3.002 \\
GraphSAGE & 0.020 & \multicolumn{1}{c|}{415} & 0.90 $\pm$ 0.022 & 4.95 $\pm$ 0.403 & 5.85 $\pm$ 0.391 & 0.025 & \multicolumn{1}{c|}{706} & 0.94 $\pm$ 0.014 & 5.94 $\pm$ 0.923 & 6.88 $\pm$ 0.937 \\
GAT & 0.029 & \multicolumn{1}{c|}{415} & 1.15 $\pm$ 0.011 & 8.17 $\pm$ 1.111 & 9.32 $\pm$ 1.102 & 0.035 & \multicolumn{1}{c|}{705} & 1.13 $\pm$ 0.020 & 14.46 $\pm$ 1.335 & 15.59 $\pm$ 1.351 \\
PNA & 0.042 & \multicolumn{1}{c|}{423} & 0.85 $\pm$ 0.010 & 6.10 $\pm$ 2.062 & 6.95 $\pm$ 2.065 & 0.059 & \multicolumn{1}{c|}{713} & 0.88 $\pm$ 0.010 & 10.10 $\pm$ 2.218 & 10.98 $\pm$ 2.213 \\
DeepGate2 & 0.490 & \multicolumn{1}{c|}{412} & 0.79 $\pm$ 0.002 & 5.49 $\pm$ 0.157 & 6.28 $\pm$ 0.158 & 0.470 & \multicolumn{1}{c|}{694} & 0.90 $\pm$ 0.004 & 25.78 $\pm$ 1.546 & 26.69 $\pm$ 1.546 \\
PolarGate & 0.030 & \multicolumn{1}{c|}{416} & 1.07 $\pm$ 0.014 & 4.62 $\pm$ 0.158 & 5.69 $\pm$ 0.162 & 0.033 & \multicolumn{1}{c|}{707} & 1.06 $\pm$ 0.009 & 9.40 $\pm$ 2.863 & 10.45 $\pm$ 2.855 \\ 
\review{HOGA-5} & 0.290 & \multicolumn{1}{c|}{1010} & 0.98 $\pm$ 0.002 & 6.02 $\pm$ 0.290 & 6.99 $\pm$ 0.291 & 0.648 & \multicolumn{1}{c|}{2006} & 1.02 $\pm$ 0.004 & 6.33 $\pm$ 0.290 & 7.35 $\pm$ 0.293 \\ 
\midrule
GraphGPS$^\dag$ & 0.512 & \multicolumn{1}{c|}{480} & 0.78 $\pm$ 0.020 & 4.99 $\pm$ 0.172 & 5.77 $\pm$ 0.174 & 0.650 & \multicolumn{1}{c|}{906} & 1.44 $\pm$ 0.018 & 11.15 $\pm$ 0.553 & 12.58 $\pm$ 0.553 \\
Exphormer$^\dag$ & 0.441 & \multicolumn{1}{c|}{337} & 0.64 $\pm$ 0.002 & 3.86 $\pm$ 0.207 & 4.50 $\pm$ 0.207 & 0.661 & \multicolumn{1}{c|}{117} & 0.85 $\pm$ 0.027 & 5.59 $\pm$ 0.566 & 6.43 $\pm$ 0.577 \\
DAGformer$^\dag$ & 0.676 & \multicolumn{1}{c|}{1324} & 1.02 $\pm$ 0.003 & 5.99 $\pm$ 0.223 & 7.01 $\pm$ 0.223 & 0.886 & \multicolumn{1}{c|}{292} & 1.33 $\pm$ 0.019 & 7.11 $\pm$ 0.100 & 8.43 $\pm$ 0.091 \\
DeepGate3$^\dag$ & 11.322 & \multicolumn{1}{c|}{6565} & 0.53 $\pm$ 0.026 & 3.21 $\pm$ 0.152 & 3.73 $\pm$ 0.148 & 18.349 & \multicolumn{1}{c|}{2730} & 1.16 $\pm$ 0.092 & 6.97 $\pm$ 0.630 & 8.13 $\pm$ 0.697 \\ \midrule
DeepGate4 & 2.496 & \multicolumn{1}{c|}{479} & \multirow{2}{*}{\textbf{0.49 $\pm$ 0.002}} & \multirow{2}{*}{\textbf{2.68 $\pm$ 0.074}} & \multirow{2}{*}{\textbf{3.16 $\pm$ 0.076}} & 2.263 & \multicolumn{1}{c|}{130} & \multirow{2}{*}{\textbf{0.79 $\pm$ 0.021}} & \multirow{2}{*}{\textbf{3.64 $\pm$ 0.583}} & \multirow{2}{*}{\textbf{4.43 $\pm$ 0.577}} \\
Fused-DG4 & 1.463 & \multicolumn{1}{c|}{233} &  &  &  & 1.624 & \multicolumn{1}{c|}{91} &  &  &  \\ \bottomrule
\end{tabular}
\label{tab：compare_itc_epfl}
}
\vspace{-15pt}
\end{table}



\vspace{-15pt}
\begin{figure}[H]
\centering
    \subfloat[Functional Loss]{
    \begin{minipage}[]{0.45\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{Figure/func_loss.jpg}
    \end{minipage}
    \label{fig:func_loss}
}
    \subfloat[Structural Loss]{
    \begin{minipage}[]{0.45\linewidth}
    \centering
    \includegraphics[width=0.99\linewidth]{Figure/stru_loss.jpg}
    \end{minipage}
    \label{fig:stru_loss}
}   
    \vspace{-10pt}
    \caption{Performance over different scale circuits.}
    \vspace{-10pt}
\end{figure}
\subsection{Memory\&Runtime Analysis}
\vspace{-5pt}
% In this section, we discuss the memory and runtime when scaling to large circuit. We test all the model on EPFL dataset in Table~\ref{epfl_data}. For circuits larger that 400k, we extend \textsc{Image\_Processing} in Table~\ref{tab:multiscale_dataset} by coping it by 2 and 4 times to get circuits with 800k and 1.6M gates. When inference, we use mini-batch size of 128, and we drop all the task heads, \ie we only use each model to compute the embedding.


In this section, we discuss memory consumption and runtime when scaling to larger circuits. We evaluate all models on the EPFL dataset, as detailed in Table~\ref{epfl_data}. For circuits larger than 400K gates, we extend the \textsc{Image\_Processing} dataset in Table~\ref{tab:multiscale_dataset} by duplicating it 2 and 4 times to create circuits with 800K and 1.6M gates. During inference, we use a mini-batch size of 128, and drop all task heads, \ie we use each model solely to compute embeddings.


% The memory consumption is illustrated in Figure~\ref{fig:memory}. For GNNs, the memory consumption is linear with the graph size. For our models, when the graph size is smaller that xxx, the memory consumption is linear with graph size, however, for circuits with more that xx gates, the memory of our models remain stable. This is mainly because the for small circuits, the cones in the same level is smaller than the mini-batch size.
% This result denotes that the memory combustion of our model is sub-linear with graph size. Furthermore, compared to DeepGate3$^\dag$, depends on the sparse pattern of circuits, DeepGate4 shows less memory consumption, with overall 58.4\% reduction, while Fused-DeepGate4 shows 78.5\% overall reduction great reduction. 

\noindent\textbf{Inference Memory Usage} As shown in Figure~\ref{fig:memory}, for GNNs, memory usage increases linearly with graph size. For our models, memory consumption also scales linearly for small circuits. However, for circuits exceeding a certain size, the memory usage of our models stabilizes. This is primarily because, for smaller circuits, cones within the same level are smaller than the mini-batch size. These results indicate that memory consumption of our model is sub-linear with respect to graph size. Additionally, compared to DeepGate3$^\dag$, DeepGate4 demonstrates a significant reduction in memory usage, with an overall 58.4\% reduction. Fused-DeepGate4 further reduces memory usage by 78.5\%.

% In terms of time consumption in Figure~\ref{fig:time}, the time consumption of all models is linear with the graph size. Since our graph transformer models need to encode the circuit batch by batch, it will be much slower than GNNs. To alleviate the inference time usage, our proposed Fused-DeepGate4 reduce the time consumption by 90.5\% and 18.5\% with respect to DeepGate3$^\dag$ and DeepGate4.

\noindent\textbf{Inference Runtime} As shown in Figure~\ref{fig:time}, the time consumption of all models scales linearly with graph size. Since our graph transformer models partition the original graph into cones and encode them level by level, they are significantly slower than GNNs. To mitigate this, we introduce inference optimization, as described in Section~\ref{sec:Inference_Acceleration}. With these optimizations, our proposed Fused-DeepGate4 reduces time consumption by 90.5\% and 18.5\% compared to DeepGate3$^\dag$ and DeepGate4, respectively.
    



\begin{table}[]
% \vspace{-10pt}
\caption{Ablation Study on ITC99 and EPFL Random Control Benchmark}
\vspace{-10pt}
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{1pt}
% \renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}l|ccccc|ccccc@{}}
\toprule
\multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{5}{c|}{ITC99} & \multicolumn{5}{c}{EPFL   Random Control} \\ \cmidrule(l){2-11} 
\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{Inference Stage} & \multicolumn{3}{c|}{Performance} & \multicolumn{2}{c|}{Inference Stage} & \multicolumn{3}{c}{Performance} \\ \cmidrule(l){2-11} 
\multicolumn{1}{c|}{} & Time(s) & \multicolumn{1}{c|}{Mem.(MB)} & $L_{func}$ & $L_{stru}$ & $L_{all}$ & Time(s) & \multicolumn{1}{c|}{Mem.(MB)} & $L_{func}$ & $L_{stru}$ & $L_{all}$ \\ \midrule
w/o Mark & 3.9223 & \multicolumn{1}{c|}{1,060} & \textbf{0.48 $\pm$ 0.004} & 2.75 $\pm$ 0.041 & 3.23 $\pm$ 0.039 & 2.9906 & \multicolumn{1}{c|}{182} & 0.90 $\pm$ 0.055 & 3.67 $\pm$ 0.689 & 4.58 $\pm$ 0.659 \\
w/o Partition & - & \multicolumn{1}{c|}{OOM} & - & - & - & - & \multicolumn{1}{c|}{OOM} & - & - & - \\
w/o Balancer\&SE & 2.4591 & \multicolumn{1}{c|}{479} & 0.50 $\pm$ 0.023 & 2.97 $\pm$ 0.060 & 3.47 $\pm$ 0.070 & 2.2085 & \multicolumn{1}{c|}{130} & 0.82 $\pm$ 0.018 & 3.95 $\pm$ 0.847 & 4.77 $\pm$ 0.850 \\
DeepGate3$^\dag$ & 11.322 & \multicolumn{1}{c|}{6565} & 0.53 $\pm$ 0.026 & 3.21 $\pm$ 0.152 & \multicolumn{1}{c|}{3.73 $\pm$ 0.148} & 18.349 & \multicolumn{1}{c|}{2730} & 1.16 $\pm$ 0.092 & 6.97 $\pm$ 0.630 & 8.13 $\pm$ 0.697 \\
\midrule
DeepGate4 & 2.496 & \multicolumn{1}{c|}{479} & \multirow{2}{*}{0.49 $\pm$ 0.002} & \multirow{2}{*}{\textbf{2.68 $\pm$ 0.074}} & \multirow{2}{*}{\textbf{3.16 $\pm$ 0.076}} & 2.263 & \multicolumn{1}{c|}{130} & \multirow{2}{*}{\textbf{0.79 $\pm$ 0.021}} & \multirow{2}{*}{\textbf{3.64 $\pm$ 0.583}} & \multirow{2}{*}{\textbf{4.43 $\pm$ 0.577}} \\
Fused-DeepGate4 & \textbf{1.463} & \multicolumn{1}{c|}{\textbf{233}} &  &  &  & \textbf{1.624} & \multicolumn{1}{c|}{\textbf{91}} &  &  &  \\ \bottomrule
\end{tabular}
\label{tab:ablation study}
}
\vspace{-5pt}
\end{table}




% \begin{table}[H]
% \vspace{-5pt}
% \caption{Ablation Study on ITC99 and EPFL Random Control Benchmark}
% \vspace{-5pt}
% \resizebox{\textwidth}{!}{
% \setlength{\tabcolsep}{1pt}
% \begin{tabular}{@{}l|ccccc|ccccc@{}}
% \toprule
% \multicolumn{1}{c|}{\multirow{3}{*}{Method}} & \multicolumn{5}{c|}{ITC99}                     & \multicolumn{5}{c}{EPFL}                     \\ \cmidrule(l){2-11} 
% \multicolumn{1}{c|}{}                        & Time & \multicolumn{1}{l|}{Mem.} & \multicolumn{3}{c|}{Performance} & Time & \multicolumn{1}{l|}{Mem.} & \multicolumn{3}{c}{Performance} \\
% \multicolumn{1}{c|}{}                        & (s)  & \multicolumn{1}{l|}{(MB)} & $L_{func}$         & $L_{stru}$         & $L_{all}$        & (s)  & \multicolumn{1}{l|}{(MB)} & $L_{func}$         & $L_{stru}$         & $L_{all}$        \\ \midrule
% w/o Mark & 3.9223 & \multicolumn{1}{c|}{1,060} & \textbf{0.48 $\pm$ 0.004} & 2.75 $\pm$ 0.041 & 3.23 $\pm$ 0.039 & 2.9906 & \multicolumn{1}{c|}{182} & 0.90 $\pm$ 0.055 & 3.67 $\pm$ 0.689 & 4.58 $\pm$ 0.659 \\
% w/o Partition & - & \multicolumn{1}{c|}{OOM} & - & - & - & - & \multicolumn{1}{c|}{OOM} & - & - & - \\
% w/o Balancer\&SE & 2.4591 & \multicolumn{1}{c|}{479} & 0.50 $\pm$ 0.023 & 2.97 $\pm$ 0.060 & 3.47 $\pm$ 0.070 & 2.2085 & \multicolumn{1}{c|}{130} & 0.82 $\pm$ 0.018 & 3.95 $\pm$ 0.847 & 4.77 $\pm$ 0.850 \\
% DeepGate3$^\dag$ & 11.2057 & \multicolumn{1}{c|}{6,511} & 0.50 $\pm$ 0.016 & 3.25 $\pm$ 0.212 & 3.76 $\pm$ 0.201 & 18.5572 & \multicolumn{1}{c|}{2,695} & 1.16 $\pm$ 0.092 & 6.97 $\pm$ 0.630 & 8.13 $\pm$ 0.697 \\
% DeepGate4 & 2.496 & \multicolumn{1}{c|}{479} & 0.49 $\pm$ 0.002 & \textbf{2.68 $\pm$ 0.074} & \textbf{3.16 $\pm$ 0.076} & 2.263 & \multicolumn{1}{c|}{130} & \textbf{0.79 $\pm$ 0.021} & \textbf{3.64 $\pm$ 0.583} & \textbf{4.43 $\pm$ 0.577} \\
% Fused-DeepGate4 & \textbf{1.463} & \multicolumn{1}{c|}{\textbf{233}} & - & - & - & \textbf{1.624} & \multicolumn{1}{c|}{\textbf{91}} & - & - & - \\ \bottomrule
% \end{tabular}
% \label{tab:ablation study}
% }
% \end{table}

\subsection{Ablation Study}
\begin{figure}[]
    \vspace{-15pt}
    \centering
    \subfloat[Memory Consumption]{
    \begin{minipage}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{Figure/mem.jpg}
    \end{minipage}
    \label{fig:memory}
}
    \subfloat[Time Consumption]{
    \begin{minipage}[t]{0.45\linewidth}
    \centering
    \includegraphics[width=0.97\linewidth]{Figure/time.jpg}
    \end{minipage}
    \label{fig:time}
}
    \vspace{-5pt}
    \caption{Inference resource usage over different scale circuits.}
    \vspace{-15pt}
\end{figure}
\vspace{-5pt}
In this section, we perform ablation studies on the primary components of DeepGate4 following the metrics outlined in Section~\ref{sec:Training_Objective}. All results are reported in Table~\ref{tab:ablation study}.

\noindent\textbf{Effect of Mark} 
In the setting DeepGate4 without Mark (w/o Mark), not marking overlapping nodes between cones resulted in redundant computations and gradient updates. This increased average inference time and memory usage by 45.3\% and 286.4\%, respectively, compared to DeepGate4, demonstrating that the marking process significantly improves efficiency and reduces memory consumption without a large impact on the loss.

\noindent\textbf{Effect of Partition}
% The impact of partitioning is particularly significant in our experiments, as real circuit datasets contain a large number of edges and nodes. Without partitioning, the memory requirement would be extremely high, and this is reflected in the experimental results where all benchmarks from ITC99 and EPFL resulted in out-of-memory (OOM) errors.
Partitioning played a critical role, especially with large circuit datasets that contain a large amount of nodes. In the setting DeepGate4 without partitioning (w/o Partition), the model encounters OOM errors on both ITC99 and EPFL, highlighting the necessity of partitioning for memory usage reduction.

\noindent\textbf{Effect of Sparse Transformer}
% After partitioning, each cone as a graph inherently possesses clear connectivity and sparsity. We find that replacing the transformer in DeepGate3 with a sparse transformer significantly improved both speed and memory usage. Additionally, since the retained sparse connections in the attention mechanism are highly important, this approach had no impact on the loss. The results show that this method accelerated inference by an average of 6.35x and reduced memory consumption to 6.1\% compared to the original.
After partitioning, the inherent connectivity and sparsity in each cone allowed replacing the transformer in DeepGate3 with a sparse transformer. DeepGate4 significantly improved speed and memory efficiency, reducing inference time by 84.0\% on average and reducing memory usage by 93.4\% compared to the DeepGate3$^\dag$.

\noindent\textbf{Effect of Loss Balancer\&Structural Encoding}
The introduction of the Loss Balancer and structural encoding has almost no impact on inference time and memory usage, while significantly reducing losses, particularly the structural loss. On the two benchmarks, DeepGate4 achieved reductions of 3.38\%, 8.75\%, and 7.89\% in functional, structural, and overall loss, respectively, compared to DeepGate4 without Loss Balancer and Structural Encoding (w/o Balancer\&SE).

\noindent\textbf{Effect of Fused-DeepGate4}
By introducing the SOTA GAT acceleration method, Fused-GAT, and our customized Fused-DG2 tailored specifically for the characteristics of AIGs, we further reduced both runtime and memory consumption. Compared to DeepGate4, Fused-DeepGate4 achieved an average reduction of 35.1\% in inference time, and 46.8\% in memory usage.
