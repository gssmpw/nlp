\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}
In this paper, we propose DeepGate4, an efficient and scalable representation learning model capable of handling large circuits with millions or even billions of gates. DeepGate4 introduces a novel partitioning method and update strategy applicable to any graph transformers. Additionally, it leverages a GAT-based sparse transformer with inference acceleration optimization, termed Fused-DeepGate4, specifically tailored for AIGs. Our model further incorporates global and local structural encodings, along with a loss balancer that automatically adjusts the weights of multitask losses.
Experimental results on the ITC99 and EPFL benchmarks demonstrate that DeepGate4 significantly outperforms state-of-the-art methods. Moreover, the Fused-DeepGate4 variant achieves substantial reductions in both runtime and memory usage, further enhancing efficiency.

\section*{Acknowledgements}
This work was supported in part by the General Research Fund of the Hong Kong Research Grants Council (RGC) under Grant No.14212422 and 14202824, and in part by National Technology Innovation Center for EDA.