\begin{table}[htbp]
    \centering
    \begin{tabular}{cc}
        \toprule
        \multicolumn{2}{c}{\textbf{Network - MLP}} \\
        \midrule
        \textit{layers} & $40,60,60,60,40$ \\
        \cmidrule(lr){1-2}
        $\sigma$ & sine \\
        \bottomrule
    \end{tabular}
    \hspace{1cm}
    \begin{tabular}{cccc}
        \toprule
        \multicolumn{4}{c}{\textbf{Training - with LBFGS}} \\
        \midrule
        \textit{lr} & 1.7e-2 & $n_\text{epochs}$ & 3000 \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-4}
        \textit{decay} & 0.99 & batch size & 2000 \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-4}
        $N_\text{col}$ & 6000 \\
        \bottomrule
    \end{tabular}
    \hspace{1cm}
    \begin{tabular}{cccc}
        \toprule
        \multicolumn{4}{c}{\textbf{Loss weights}} \\
        \midrule
        $\omega_r$ & 1 & $\omega_\text{data}$ & 0 \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-4}
        $\omega_b$ & 0 & $\omega_\text{sob}$ & 0.1 \\        
        \bottomrule
    \end{tabular}
    \caption{Network, training parameters (\cref{rmk:PINN_notations}) and loss weights for $u_\theta^\text{sob}$ in the \textit{2D Laplacian} case.}
    \label{tab:paramtest1v7_2D}
\end{table}
