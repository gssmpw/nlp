\section{Prior construction using parametric PINNs}
\label{sec:prior_construction}


We have introduced new finite element approximation spaces in \cref{sec:additive_prior,sec:multiplicative_prior} depending on the construction of priors.
Physics-Informed Neural Networks (PINNs) are a good choice to build such priors.
Indeed, since PINNs minimize the PDE residual, they inherently give a good approximation of the derivative of the solution, in addition to the solution itself (see e.g. \cite{RAISSI2019686}).
This section is therefore dedicated to introducing PINNs in \cref{sec:PINNs_parametric_PDE}, and then to show how to improve them in \cref{sec:improve_PINNs}.

\subsection{Physics-Informed Neural Networks for parametric PDEs}
\label{sec:PINNs_parametric_PDE}

Physics-Informed Neural Networks, or PINNs, were introduced by \cite{RAISSI2019686} %\cite{raissi2017physicsinformeddeeplearning}
for solving a PDE with Neural Networks.
The main idea is to recast a PDE as an optimization problem.
We illustrate the method on our problem~\eqref{eq:ob_pde}, which we extend, in this section, to non-homogeneous Dirichlet boundary conditions.
Moreover, we introduce a dependency on some physical parameters, making the problem of interest a parametric PDE.
Unlike classical PINNs, which train for a specific case of boundary conditions or physical parameters, parametric PINNs seek to learn a generalized solution covering a range of parameters.
They incorporate these parameters as additional inputs to the network, allowing greater flexibility in solving problems where physical conditions or properties vary.
Moreover, since they are based on a combination of neural networks and the Monte-Carlo method, PINNs are ideally suited to solving such higher-dimensional problems.

Considering $p$ parameters \smash{$\bm{\mu} = (\mu_1, \dots, \mu_{p}) \in \mathcal{M} \subset \mathbb{R}^{p}$}, with some parameter space $\mathcal{M}$, the parametric PDE reads
\begin{equation}
	\label{eq:parametric_PDE}
	\begin{dcases}
		\mathcal{L}\big(u(\bm{x},\bm{\mu});\bm{x},\bm{\mu}\big) = f(\bm{x},\bm{\mu}), & \bm{x}\in\Omega, \\
		u(\bm{x},\bm{\mu}) = g(\bm{x},\bm{\mu}),             & \bm{x}\in\partial \Omega, \\
	\end{dcases}
\end{equation}
with $g$ the trace of a $H^2$ function on $\partial \Omega$.
Note that the solution of the equation depends on the parameters~$\bm{\mu}$, as do the operator $\mathcal{L}$ and the boundary conditions.
This can be contrasted to the solution of \eqref{eq:ob_pde}, which only depended on the space variable $\bm{x}$.
We then denote $u_{\theta}(\cdot, \bm{\mu})$ the approximate PINN prediction for given parameters~$\bm{\mu}$.

The first idea of PINNs comes from the observation that, by construction, neural networks with smooth activation functions
are nothing but smooth functions of their weights and inputs.
Therefore, neural networks form natural candidates for approximating solutions to PDEs, especially with the advent of automatic differentiation tools.
In our case, a PINN is a neural network that takes $d+p$ inputs, where $d$ is the dimension of the space variable $\bm{x} \in \Omega$ and $p$ is the number of parameters $\bm{\mu} \in \mathcal{M}$. We denote by $u_{\theta}(\bm{x},\bm{\mu})$ the output, where $\theta$ are the learnable weights of the network.
Classically, this neural network is a coordinate-based neural network, such as a multi-layer perceptron (MLP).

\begin{remark}\label{rmk:PINN_notations}
	In \cref{sec:numerical_results}, we need some notations describing how this PINN has been constructed. In particular, we will note $\sigma$ the MLP activation function, and \textit{layers} will describe an integer sequence describing the number of neurons associated with each layer of the MLP. For training, we will note \textit{lr} for the learning rate and $n_\text{epochs}$ the number of epochs considered, as well as \textit{decay} the multiplicative factor of the learning rate decay considered every $20$ epochs thanks to Pytorch's StepLR scheduler. Where this is not specified, the batch size will correspond to the number of collocation points chosen. The Adam optimizer~\cite{KinBa2015} will be considered during training, but in some cases, we will switch to the LBFGS optimizer~\cite{nocedal_quasi_newton_2006} at the $n_\text{switch}$-th epoch.
\end{remark}

Once this network is defined,
solving the PDE can be rewritten as a minimization problem on $\theta$,
namely finding the optimal weights $\theta^\star$
that satisfy the following minimization problem:
\begin{equation}
	\label{eq:minimization_problem}
	\theta^\star = \argmin_{\theta}
	\big( \omega_r J_r(\theta) + \omega_b J_b(\theta) + \omega_\text{data} J_\text{data}(\theta) \big),
\end{equation}
with $\omega_r$, $\omega_b$ and $\omega_\text{data}$ some weights to balance the different terms of the loss function.

In \eqref{eq:minimization_problem}, the loss function owns three terms: the residual loss function
\begin{equation}
	\label{eq:residual_loss_parametric}
	J_r(\theta) =
	\int_{\mathcal{M}}\int_{\Omega}
	\big| \mathcal{L}\big(u_\theta(\bm{x},\bm{\mu});\bm{x},\bm{\mu}\big)-f(\bm{x},\bm{\mu}) \big|^2 d\bm{x} d\bm{\mu},
\end{equation}
the boundary loss function
\begin{equation}
	\label{eq:boundary_loss_parametric}
	J_b(\theta) =
	\int_{\mathcal{M}}\int_{\partial \Omega} \big| u_\theta(\bm{x},\bm{\mu}) - g(\bm{x},\bm{\mu}) \big|^2 d\bm{x} d\bm{\mu},
\end{equation}
and the data loss function
\begin{equation}
	\label{eq:data_loss_parametric}
	J_\text{data}(\theta) =
	\frac 1 {N_\text{data}} \sum_{i=1}^{N_\text{data}} \big| u_\theta\big(\bm{x}_\text{data}^{(i)},\bm{\mu}_\text{data}^{(i)}\big) - u_\text{data}^{(i)} \big|^2,
\end{equation}
where \smash{$\big(\bm{x}_\text{data}^{(i)}, \bm{\mu}_\text{data}^{(i)}, u_\text{data}^{(i)}\big)_{i=1,\dots,N_\text{data}}$} are $N_\text{data}$ known data points with $u_\text{data}^{(i)}$ a reference solution at point \smash{$\bm{x}_\text{data}^{(i)}$} and for parameters \smash{$\bm{\mu}_\text{data}^{(i)}$}.

\begin{remark}
	These reference solutions can be the exact solutions of the parametric PDE, in this case, defined by \smash{$u_\text{data}^{(i)}=u\big(\bm{x}_\text{data}^{(i)};\bm{\mu}_\text{data}^{(i)}\big)$}. They can also be an approximation produced by a numerical method, such as finite elements on a fine mesh.
\end{remark}

\begin{remark}
	In \cref{sec:numerical_results}, the focus is on PINNs trained only on residual loss (with boundary conditions imposed exactly as presented in \cref{sec:exact_imposition_of_BC}). We will only consider BC loss in \cref{sec:Lap2Dlowbc}. Furthermore, we will not use loss data in PINNs except in \cref{sec:Lap1D} where we will seek to compare a full PINN to a network trained only on data.
\end{remark}

Solving the minimization problem \eqref{eq:minimization_problem}
requires computing the gradient of the loss function with respect to~$\theta$, which involves calculating the integrals
in \eqref{eq:residual_loss_parametric} and \eqref{eq:boundary_loss_parametric}.
The most natural idea is to estimate them with a Monte-Carlo method, see e.g.~\cite{Caf1998}.
One could also use Gauss-type quadrature rules to evaluate integrals, as is done in Variational Physics-Informed
Neural Networks \cite{KhaZhaKar2021}, but the limitation is the impossibility of selecting an adequate
quadrature order due to the unknown properties of the Neural Network approximation.
For that purpose, we define so-called ``collocation points'' on $\Omega\times\mathcal{M}$ and its boundary $\partial\Omega\times \mathcal{M}$,
denoted respectively by \smash{$\big(\bm{x}_\text{col}^{(i)}, \bm{\mu}_\text{col}^{(i)}\big)_{i=1,\dots,N_\text{col}}$} and \smash{$\big(\bm{x}_\text{bc}^{(i)}, \bm{\mu}_\text{bc}^{(i)}\big)_{i=1,\dots,N_\text{bc}}$}.
Then, we approximate the residuals and boundary losses by
\begin{equation*}\label{eq:residual_loss_parametric_MC}
	J_r(\theta) \simeq
	\frac{1}{N_\text{col}} \sum_{i=1}^{N_\text{col}} \big| \mathcal{L}\big(u_\theta(\bm{x}_\text{col}^{(i)},\bm{\mu}_\text{col}^{(i)});\bm{x}_\text{col}^{(i)},\bm{\mu}_\text{col}^{(i)}\big)-f(\bm{x}_\text{col}^{(i)},\bm{\mu}_\text{col}^{(i)})  \big|^2
\end{equation*}
and
\[
	J_b(\theta) \simeq
	\frac{1}{N_\text{bc}} \sum_{i=1}^{N_\text{bc}} \big| u_\theta\big(\bm{x}_\text{bc}^{(i)},\bm{\mu}_\text{bc}^{(i)}\big) - g\big(\bm{x}_\text{bc}^{(i)},\bm{\mu}_\text{bc}^{(i)}\big) \big|^2.
\]

\begin{remark}
	The values of $N_{\mathrm{col}}$ and $N_\mathrm{bc}$
	are heuristically determined and should be large enough to ensure that the Monte-Carlo integration is accurate enough.
	The precise values of these parameters will be given in the numerical experiments.
\end{remark}

\begin{remark}
	In the case of complex geometries, one solution for obtaining a sample of points in the $\Omega$ domain is to use a level-set function, denoted $\varphi$. This function, which vanishes on the boundary of $\Omega$, can be obtained differently. The authors of \cite{Sukumar_2022} propose different approaches to obtain a level-set function analytically in the case of polygonal or curved geometries. Learning-based approaches have also been proposed, notably in e.g. \cite{park2019deepsdflearningcontinuoussigned,sitzmann2020implicitneuralrepresentationsperiodic}.
\end{remark}

Because of the minimization problem \eqref{eq:minimization_problem}, the PINN $u_\theta$ does not exactly satisfy the boundary conditions.
Moreover, loss functions compete, which may require fine-tuning the coefficients between $J_r$ and $J_b$.
In addition, classical PINNs do not include information on higher-order derivatives.
As highlighted in \cref{rmk:C_gain_additif,rmk:C_gain_multiplicatif}, for our purposes, a good prior should yield a good approximation of the derivatives of the solution.
%Lastly, it is well-known that MLPs have a spectral bias:
%learning high-frequency solutions requires large networks
%and long training times.
For these reasons, the following section recalls several improvements of classical PINNs in the literature.

\subsection{Improving PINN training and prediction}\label{sec:improve_PINNs}

This section focuses on several ways of improving PINNs:
exactly imposing the boundary conditions in \cref{sec:exact_imposition_of_BC},
adding a higher-order derivative term in the loss function in \cref{sec:sobolev_training},
and countering the spectral bias in \cref{sec:spectral_bias}.
Although these approaches are presented separately,
they can easily be combined with one another.

\subsubsection{Exact imposition of boundary conditions}\label{sec:exact_imposition_of_BC}

To avoid the issues of classical PINNs discussed in \cref{sec:PINNs_parametric_PDE},
the authors of \cite{LagLikFot1998,FraMicNav2024}
propose a method to enforce inhomogeneous Dirichlet boundary conditions exactly.
To that end, they search the approximation $u_{\theta}$ of solution to \eqref{eq:parametric_PDE} with the form:
for all $\bm{x} \in \Omega$ and $\bm{\mu} \in \mathcal{M}$
\[
	u_{\theta}(\bm{x},\bm{\mu}) = \varphi(\bm{x}) w_{\theta}(\bm{x},\bm{\mu}) + g(\bm{x},\bm{\mu}),
\]
where $\varphi$ and $w_\theta$ are, respectively, the level-set function and a neural network as defined in \cref{sec:PINNs_parametric_PDE}.
Thus $u_{\theta}$ will automatically satisfies the boundary conditions, since
$u_{\theta}(\bm{x},\bm{\mu}) = g(\bm{x},\bm{\mu})$ for all $\bm{x} \in \partial \Omega$ and $\bm{\mu} \in \mathcal{M}$.

% they apply some functions to the PINN solution $w_\theta$,
%resulting in a new approximation $u_{\theta}$
%of the solution to problem \eqref{eq:parametric_PDE},
%that automatically satisfies the boundary conditions.
%Usually, the prior $u_{\theta}$ may be defined for all $\bm{x} \in \Omega$ and $\bm{\mu} \in \mathcal{M}$ as
%\[
%	u_{\theta}(\bm{x},\bm{\mu}) = \varphi(\bm{x}) w_{\theta}(\bm{x},\bm{\mu}) + g(\bm{x},\bm{\mu}),
%\]
%where $\varphi$ and $w_\theta$ are the level-set function and the neural network defined in \cref{sec:PINNs_parametric_PDE}.

\begin{remark}
	Note that this level-set function can be used in a few different ways, firstly to generate a sample of points in $\Omega$, as shown in \cref{sec:PINNs_parametric_PDE}, and secondly to impose boundary conditions. However, to use it directly in the formulation of the prior, it will require a certain regularity. For example, the signed distance function is not a good candidate.
\end{remark}

%Then, $u_{\theta}$ obviously satisfies the boundary conditions since
%$u_{\theta}(\bm{x},\bm{\mu}) = g(\bm{x},\bm{\mu})$ for all $\bm{x} \in \partial \Omega$ and $\bm{\mu} \in \mathcal{M}$.
In this case, only the residual and data loss functions are minimized,
and the minimization problem~\eqref{eq:minimization_problem} becomes
\begin{equation*}
	\label{eq:minimization_problem_without_lossBC}
	\theta^\star = \argmin_{\theta}
	\big( \omega_r J_r(\theta) + \omega_\text{data} J_\text{data}(\theta) \big),
\end{equation*}
with $\omega_r$ and $\omega_\text{data}$ some weights to balance the terms of the loss function.

\begin{remark}
	Similar methods exist for Robin and Neumann conditions; see \cite{Sukumar_2022}.
\end{remark}


\subsubsection{Sobolev training for PINNs}\label{sec:sobolev_training}

As presented in \cref{sec:PINNs_parametric_PDE}, PINNs approximate the PDE solution by directly incorporating the equations into their training. Despite their effectiveness, these models can sometimes struggle to learn correctly, especially when the solution or its derivatives are complicated.
The authors of \cite{son2021sobolevtrainingphysicsinformed} have proposed an approach called Sobolev training to try and overcome these difficulties. This method simply imposes constraints not only on the solutions themselves, but also on their derivatives.
% By taking this additional information into account, the model learns more effectively to capture the complex behaviors of solutions and their variations.
% The authors have shown that this approach can be used to guide network learning, particularly in PDEs such as the heat equation, Burgers’ equation, the Fokker–Planck equation and high-dimensional Poisson equation.
In the context of solving the problem \eqref{eq:parametric_PDE} under consideration, Sobolev training is applied by adding a cost term $J_\text{\rm sob}$ to the initial minimization problem \eqref{eq:minimization_problem}:
\begin{equation}
	\label{eq:minimization_problem_sobolev}
	\theta^\star = \argmin_{\theta}
	\big( \omega_r J_r(\theta) + \omega_\text{\rm sob} J_\text{\rm sob}(\theta) + \omega_b J_b(\theta) + \omega_\text{data} J_\text{data}(\theta) \big),
\end{equation}
with $J_r$, $J_b$ and $J_\text{data}$ defined as in \eqref{eq:residual_loss_parametric}, \eqref{eq:boundary_loss_parametric} and \eqref{eq:data_loss_parametric} respectively and $\omega_r$, $\omega_\text{\rm sob}$, $\omega_b$ and $\omega_\text{data}$ the weights to balance the different terms of the loss function.
The Sobolev loss function $J_\text{\rm sob}$ in \eqref{eq:minimization_problem_sobolev} is defined by
\begin{equation}
	\label{eq:sobolev_loss}
	J_\text{\rm sob}(\theta) = \int_{\mathcal{M}}\int_{\Omega} |\nabla_{\bm{x}}\big( \mathcal{L}\big(u_\theta(\bm{x},\bm{\mu});\bm{x},\bm{\mu}\big)-f(\bm{x},\bm{\mu})\big)|^2%\cdot\nabla_{\bm{x}}\big( \mathcal{L}\big(u_\theta(\bm{x},\bm{\mu});\bm{x},\bm{\mu}\big)-f(\bm{x},\bm{\mu})\big)
	 \; d\bm{x} d\bm{\mu},
\end{equation}
where the integral is estimated by the Monte-Carlo method, similar to the other loss functions.

\subsubsection{Overcoming the spectral bias}
\label{sec:spectral_bias}

Multiple ways of overcoming the spectral bias of MLPs are available.
For instance, in \cite{TanSri2020}, the authors introduce Fourier features to improve the network, while the authors of \cite{DolHeiMisMos2024} rely on a domain decomposition-based approach.

In this work, when dealing with high-frequency solutions (i.e., solutions with more than three wavelengths propagating), we use the Fourier features from \cite{TanSri2020}.
It relies on modifying the input of the neural network.
Indeed, the prior $u_\theta$ is now defined, for all $\bm{x} \in \Omega$ and $\bm{\mu} \in \mathcal{M}$, as
\begin{equation*}
    u_\theta(\bm{x},\bm{\mu}) =
    w_\theta\big(
        \bm{x},\bm{\mu};
        \sin(\pi a_1 \bm{x}),
        \cos(\pi b_1 \bm{x}),
        \dots,
        \sin(\pi a_{n_f} \bm{x}),
        \cos(\pi b_{n_f} \bm{x})
    \big),
\end{equation*}
where the $2 n_f$ real numbers $(a_l)_{l \in \{1, \dots, n_f\}}$ and $(b_l)_{l \in \{1, \dots, n_f\}}$ are additional trainable parameters.
This makes it possible to learn higher-frequency solutions, by also learning the frequency itself.

\begin{remark}\label{rmk:PINN_notations_Fourier}
	This MLP with Fourier features (MLP w/ FF) needs the same parameters as the classical MLP (defined in \cref{rmk:PINN_notations}), but also the number of Fourier features $n_f$.
\end{remark}
