\subsection{1D Poisson problem}\label{sec:Lap1D}

In this section, we will consider the problem~\eqref{eq:ob_pde} in its most
simplified Poisson form, with homogeneous Dirichlet boundary conditions.
In the 1D case ($d=1$), we have,
\begin{equation}
	\left\{
	\begin{aligned}
		-\partial_{xx} u & = f, \; &  & \text{in } \; \Omega \times \mathcal{M}, \\
		u         & = 0, \;  &  & \text{on } \; \partial\Omega \times \mathcal{M},
	\end{aligned}
	\right.
	\label{eq:Lap1D}
\end{equation}
with $\Omega=[0,1]$, $\partial\Omega$ its boundary and $\mathcal{M} \subset \mathbb{R}^p$ the parameter space (with $p$ the number of parameters).
% Considering $\bm{x}=x\in\Omega$, we define the analytical solution by
{For this problem, we consider solutions defined with
$p=3$ parameters $\bm{\mu}=(\mu_1,\mu_2,\mu_3)\in\mathcal{M}=[0,1]^3$ defined by
}
\begin{equation*}
	u(x;\bm{\mu})=\mu_1\sin(2\pi x)+\mu_2\sin(4\pi x)+\mu_3\sin(6\pi x) \,.
\end{equation*}
% with $p=3$ parameters $\bm{\mu}=(\mu_1,\mu_2,\mu_3)\in\mathcal{M}=[0,1]^p$, and
{Note that the associated right-hand side $f$ in \eqref{eq:Lap1D}
also depends on $\bm{\mu}$.
This problem is thus four-dimensional: one dimension in space and three dimensions
for the parameters $\bm{\mu}$.}

For this first test case, we construct two priors,
as detailed in \cref{sec:Lap1D_priors}.
The first one, denoted by $u_\theta$,
is built from a PINN as presented in \cref{sec:prior_construction}.
The second one, denoted by $u_\theta^\text{data}$
is constructed only from data (obtained from the analytical solution).
The aim is to show that using physics-informed training to construct the prior
leads to better results than data-driven training.
In this test case, we also compare the additive and multiplicative approaches (presented for several values of $M$), but only with $k=1$ polynomial order to remain concise.

We first present the error estimates obtained with the PINN prior in \cref{sec:Lap1D_error_estimations}. First, we check the orders of convergence of the two enriched approaches. Then, we verify the results expected in \cref{sec:comparison_add_mul} by comparing the theoretical gain constants of the additive and multiplicative approaches. Afterwards, in \cref{sec:Lap1D_derivatives}, we compare, for a given parameter, the derivatives obtained with the two priors and analyze the associated gains. Finally, we evaluate the gains obtained with the two priors in \cref{sec:Lap1D_gains} on a sample of parameters.

\begin{remark}\label{rmk:N_nodes}
	In the following, the characteristic mesh size $h=\frac{1}{N-1}$, where $N$ represents the number of considered nodes.
\end{remark}

\subsubsection{Construction of the two priors}\label{sec:Lap1D_priors}

The hyperparameters used to construct the two priors are presented in \cref{tab:paramtest1_1D}. We discuss below the specific differences in training
both priors.

\input{fig_testcase1D_test1_test1}

\paragraph*{Physics-informed training.}
For the first prior, we will consider a parametric PINN, depending on the problem parameters $\bm{\mu}$,
where we exactly impose the Dirichlet boundary conditions as presented in \cref{sec:exact_imposition_of_BC} and without using data in training.
% To do this,
We define the prior
\begin{equation}\label{eq:utheta}
	u_\theta(x,\bm{\mu}) = \varphi(x) w_\theta(x,\bm{\mu}) \,,
\end{equation}
where $w_\theta$ is the neural network under consideration and $\varphi$ is the level-set function defined by
\begin{equation}\label{eq:phii}
	\varphi(x)=x(x-1) \,,
\end{equation}
which vanishes exactly on $\partial\Omega$.
Since we impose the boundary conditions by using the level-set function, we will only consider the residual loss $J_r$ defined in \eqref{eq:residual_loss_parametric} in which the integrals are approached by a Monte-Carlo method, i.e.
\[
	J_r(\theta) \simeq
	\frac{1}{N_\text{col}} \sum_{i=1}^{N_\text{col}} \big| \partial_{xx}u_\theta(\bm{x}_\text{col}^{(i)};\bm{\mu}_\text{col}^{(i)}\big) + f\big(\bm{x}_\text{col}^{(i)};\bm{\mu}_\text{col}^{(i)}\big) \big|^2
\]
with the $N_\text{col}=5000$ collocation points \smash{$\big(\bm{x}_\text{col}^{(i)}, \bm{\mu}_\text{col}^{(i)}\big)_{i=1,\dots,N_\text{col}}$} uniformly chosen on $\Omega\times\mathcal{M}$. Thus, we seek to solve the following minimisation problem
\begin{equation*}
	\theta^\star = \argmin_\theta J_r(\theta).
\end{equation*}
In this first case,
we consider an MLP with $6$ layers and a sine activation function.
The hyperparameters are given in \cref{tab:paramtest1_1D};
we use the Adam optimizer~\cite{KinBa2015}.% and consider $N_\text{col}=5000$ collocation points, .




\paragraph*{Data-driven training.}

For the second prior considered, noted $u_\theta^\text{data}$, a network is trained only on the data (constructed from the analytical solution). In this second case, we still consider the parameters defined in \cref{tab:paramtest1_1D}, which are the same as for the physics-informed training except that we take $N_\text{data}=5000$ instead of $N_\text{col}=5000$.
Considering, for each epoch, a set of parameters and points \smash{$\big(\bm{x}_\text{data}^{(i)}, \bm{\mu}_\text{data}^{(i)}\big)_{i=1,\dots,N_\text{data}}$},
% we can consider the data loss defined by
the data loss is defined by
\begin{equation*}
	J_\text{data}(\theta) =
    \frac{1}{N_\text{data}}
    \sum_{i=1}^{N_\text{data}} \big| u_\theta^\text{data}(\bm{x}_\text{data}^{(i)};\bm{\mu}_\text{data}^{(i)}) - u(\bm{x}_\text{data}^{(i)};\bm{\mu}_\text{data}^{(i)}) \big|^2,
\end{equation*}
and thus we seek to solve the following minimisation problem
\begin{equation*}
	\theta^\star = \argmin_\theta J_\text{data}(\theta).
\end{equation*}

\begin{remark}
	As for the PINN prior, we impose the boundary conditions exactly in the data-driven training.
\end{remark}

\subsubsection{Error estimates --- with the PINN prior}\label{sec:Lap1D_error_estimations}

In this section, we look at the theoretical results of the additive and multiplicative approaches, considering the PINN prior $u_\theta$. First, we check the orders of convergence of \cref{lem:error_estimation_add,lem:error_estimate_multiplicative} (in the $L^2$ norm), associated with both methods. Next, we numerically verify \cref{thm:comparison_add_mul}, showing that the multiplicative correction converges, for sufficiently large $M$, towards the additive correction (in both the $L^2$ norm and $H^1$ semi-norm).

\paragraph*{Convergence rate.} We test the error estimates of \cref{lem:error_estimation_add,lem:error_estimate_multiplicative} for the following two sets of parameters:
\begin{equation} \label{equation:lap1d:mu-choices}
	\bm{\mu}^{(1)}=(0.3,0.2,0.1) \quad \text{and} \quad \bm{\mu}^{(2)}=(0.8,0.5,0.8) \,,
	% param 2 = ancien param 4
\end{equation}
with the PINN prior $u_\theta$.
For $j \in \{1, 2\}$, the aim is to compare, by varying the mesh size $h$, the $L^2$ relative errors \smash{$e_h^{(j)}$} obtained with the standard FEM, defined in~\eqref{eq:error_rel_FEM}, \smash{$e_{h,+}^{(j)}$} obtained with the additive approach, defined in~\eqref{eq:error_rel_add} and \smash{$e_{h,M}^{(j)}$} obtained with the multiplicative approach (taking $M=3$ and $M=100$), defined in~\eqref{eq:error_rel_add}. The results are presented in \cref{fig:case1_1D}
for a polynomial orders $k=1$ and $k=2$, with $h$ depending
on the number of nodes $N \in \{16,32,64,128,256\}$ as
presented in \cref{rmk:N_nodes}.


\begin{figure}[!ht]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\cvgFEMCorrMultOnedeg{fig_testcase1D_test1_cvg_FEM_case1_v1_param1_degree1.csv}{fig_testcase1D_test1_cvg_FEM_case1_v1_param1_degree2.csv}{fig_testcase1D_test1_cvg_Corr_case1_v1_param1_degree1.csv}{fig_testcase1D_test1_cvg_Mult_case1_v1_param1_degree1_M3.0.csv}{fig_testcase1D_test1_cvg_Mult_case1_v1_param1_degree1_M100.0.csv}{1e-5}
		\caption{Case of $\bm{\mu}^{(1)}$}
		\label{fig:case1param1_1D}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\cvgFEMCorrMultOnedeg{fig_testcase1D_test1_cvg_FEM_case1_v1_param2_degree1.csv}{fig_testcase1D_test1_cvg_FEM_case1_v1_param2_degree2.csv}{fig_testcase1D_test1_cvg_Corr_case1_v1_param2_degree1.csv}{fig_testcase1D_test1_cvg_Mult_case1_v1_param2_degree1_M3.0.csv}{fig_testcase1D_test1_cvg_Mult_case1_v1_param2_degree1_M100.0.csv}{5e-6}
		\caption{Case of $\bm{\mu}^{(2)}$}
		\label{fig:case1param2_1D}
	\end{subfigure}
	\caption{Considering the \textit{1D Laplacian case} and the PINN prior $u_\theta$. Left -- Considering $\bm{\mu}^{(1)}$. $L^2$ error on $h$ obtained with standard FEM \smash{$e_h^{(1)}$} (solid lines) with $k=1$ and $k=2$, the additive approach \smash{$e_{h,+}^{(1)}$} (dashed lines) with $k=1$ and the multiplicative approach \smash{$e_{h,M}^{(1)}$} (dotted lines), with $k=1$ ($M=3$ and $M=100$). Right -- Same for $\bm{\mu}^{(2)}$.}
	\label{fig:case1_1D}
\end{figure}

The results of \cref{fig:case1_1D} show that all the enriched finite elements increase the accuracy of the method and that they also converge at the same rate as the classical approach (i.e., as for the polynomial approximation order $k=1$).
Furthermore, the theoretical analysis (which showed that the multiplicative correction has the same error as the additive one when $M \to \infty$) is confirmed for both sets of parameters.
In addition, \cref{fig:case1_1D} also shows that this multiplicative enrichment can be less efficient for small $M$ when the $(k+1)$\textsuperscript{th} derivative of the solution is large.
Indeed, for the first parameter considered in \cref{fig:case1param1_1D}, for which the second derivative takes lower values, we observe that the multiplicative approach with small $M$ is closer to the additive one than for the second set of parameters
considered in \cref{fig:case1param2_1D}, for which the derivatives are larger.
%Finally, in both cases, the gain given by the enriched FEM is significant, and, as expected, the gain is also greater at high frequencies. \textcolor{red}{Ã  enlever ?} 
Moreover, it seems that we gain almost one order of interpolation with the additive approach: in \cref{fig:case1}, the additive method with polynomial order $k=1$ gives an error $L^2$ close to the original FEM method with $k=2$, although the rate of convergence is different.

\paragraph*{Gain constants.} We consider the first parameter $\bm{\mu}^{(1)}$ and the PINN prior $u_\theta$. We now evaluate the gain constants $C_\text{gain}^+$ and $C_\text{gain}^{\times,M}$ (for different values of $M$), are respectively defined in~\cref{rmk:gain_add,rmk:gain_mul} for the additive and multiplicative approaches. The idea is to check the convergence of the multiplicative gain constant towards the additive one, as proven in \cref{thm:comparison_add_mul}. The results are presented in \cref{fig:Lap1D_gain_constants} for $L^2$ and $H^1$ norms.

%\begin{remark}
%	Note that for $m=0$ and $m=1$, we have
%	\[
%		\frac{m!}{\lfloor \frac{m}{2} \rfloor!^2} = 1,
%	\]
%	which means that $C_\text{gain}^{\times,M}$ converges towards the same value in $L^2$ and $H^1$ norms.
%\end{remark}

\begin{figure}[ht!]
	\centering
    \begin{minipage}{0.55\linewidth}
		\includegraphics[scale=1]{fig_testcase1D_test1_comp_add_mult_standalone_m0.pdf}
		\includegraphics[scale=1]{fig_testcase1D_test1_comp_add_mult_standalone_m1.pdf}
    \end{minipage} \hfill
	% \begin{minipage}{0.42\linewidth}
	% 	\centering
	% 	\input{fig/testcase1D/test1/comp_add_mult/comp_mult_add_gain.tex}
	% \end{minipage}
	\caption{Considering the \textit{1D Laplacian case} with $\bm{\mu}^{(1)}$, $k=1$ and the PINN prior $u_\theta$. Left -- Convergence of \cref{thm:comparison_add_mul} with the $L^2$ error. Right -- Convergence of \cref{thm:comparison_add_mul} with the $H^1$ error. %Right -- Values of the gain constants for $m=0$ and $m=1$ (for different values of $M$).
	}
	\label{fig:Lap1D_gain_constants}
\end{figure}

\cref{fig:Lap1D_gain_constants} shows that the multiplicative gain constant converges to the additive gain constant when $M$ increases, as expected in  the theoretical results of \cref{thm:comparison_add_mul}.
 %Moreover, the results obtained here in $L^2$ norm seem to correspond to the numerical gains obtained in \cref{tab:case1_1D_both} (with the PINN prior $u_\theta$) of \cref{sec:Lap1D_derivatives}, \smash{$G_+^{(1)}$} for the additive approach defined in \eqref{eq:gain_j} and \smash{$G_M^{(1)}$} for the multiplicative approach (taking $M=3$ and $M=100$) defined in \eqref{eq:gain_j_mul}.

\subsubsection{Derivatives --- with both priors}\label{sec:Lap1D_derivatives}

To better explain the results of \cref{sec:Lap1D_error_estimations}, we compare the solution, the first- and second-order derivatives between the exact solution and the prediction of both priors, for selected parameter $\bm{\mu}^{(1)}$.
\cref{sinus1D_Pinns,sinus1D_nn} respectively present this comparison for the PINN prior $u_\theta$ and the data prior $u_\theta^\text{data}$.
We also compare the errors and gains obtained with these two priors for $N \in \{16,32\}$ in \cref{tab:case1_1D_both}.
More precisely, we evaluate the additive error \smash{$e_{h,+}^{(1)}$} and the additive gain on FEM \smash{$G_+^{(1)}$}, respectively defined in~\eqref{eq:error_rel_add} and~\eqref{eq:gain_j}, for both PINN and data priors.
We also evaluate the multiplicative error \smash{$e_{h,M}^{(1)}$} and the multiplicative gain on FEM \smash{$G_M^{(1)}$} defined in~\eqref{eq:error_rel_add} and~\eqref{eq:gain_j_mul}, for both priors, with $M=3$ and $M=100$.

\begin{figure}[ht!]
	\centering
	% \includegraphics[width=0.9\linewidth]{fig/testcase1D/test1/plots/derivatives_mu1_PINN.pdf}
	\includegraphics[scale=1]{fig_testcase1D_test1_plots_standalone_solutions_and_errors_PINN.pdf}
	\caption{Considering the \textit{1D Laplacian case} with $\bm{\mu}^{(1)}$ and the PINN prior $u_\theta$, comparison between analytical solution and network prediction.
    From left to right: solution; first derivative; second derivative; errors.}
	\label{sinus1D_Pinns}
\end{figure}

\begin{figure}[ht!]
	\centering
	% \includegraphics[width=0.9\linewidth]{fig/testcase1D/test1/plots/derivatives_mu1_NN.pdf}
	\includegraphics[scale=1]{fig_testcase1D_test1_plots_standalone_solutions_and_errors_NN.pdf}
	\caption{Considering the \textit{1D Laplacian case} with $\bm{\mu}^{(1)}$ and the data prior $u_\theta^\text{data}$,
    comparison between analytical solution and network prediction.
    From left to right: solution; first derivative; second derivative; errors.
	}
	\label{sinus1D_nn}
\end{figure}


\begin{table}[H]
	\centering
	\gainsbothNN{fig_testcase1D_test1_plots_FEM_param1.csv}{fig_testcase1D_test1_plots_compare_gains_param1.csv}
	\caption{Considering the \textit{1D Laplacian case} with $\bm{\mu}^{(1)}$, $k=1$ and $N=16,32$. Left -- $L^2$ relative error obtained with FEM. Right -- Considering the PINN prior $u_\theta$ and the data prior $u_\theta^\text{data}$, $L^2$ relative errors and gains with respect to FEM, obtained with our methods. Our methods : additive approach, multiplicative approach with $M=3$ and $M=100$.}
	\label{tab:case1_1D_both}
\end{table}

The results reported in \cref{sinus1D_Pinns,sinus1D_nn,tab:case1_1D_both} show that, even if the approach chosen to build the prior (physics-informed or data-driven training) gives a good approximation of the solution, the important point lies in the derivatives and mainly in the second-order derivatives, which are clearly better learned by PINNs.
Indeed, while the enriched FEM solution is more accurate using
the PINN prior (\cref{tab:case1_1D_both}), we see from
\cref{sinus1D_Pinns,sinus1D_nn,tab:case1_1D_both} that the raw PINN
approximates the solution $u$ less accurately than the raw data-driven
the solution, but the PINN better approximates the derivatives.
As the error of the enriched FEM is mainly due to the $k+1$\textsuperscript{th} derivatives of the network being close to the $k+1$\textsuperscript{th} derivatives of the solution, this explains why the enriched FEM with data prior does not perform as well as with a PINN prior.
Therefore, PINNs have two advantages: they do not require training data and give better results.
Their main shortcoming is that training takes longer.
However, we mention that if data are available for first- and second-order
derivatives, it could also be used to improve a purely data-driven prior.

\subsubsection{Gains achieved with the additive and multiplicative approaches -- with both priors}\label{sec:Lap1D_gains}

Considering a sample $\mathcal{S}$ of $n_p=100$ parameters, we now evaluate the gains $G_{+,\theta}$ and $G_+$ defined in~\eqref{eq:gain_add_num} with the PINN prior $u_\theta$ and with the data prior $u_\theta^\text{data}$.
We also compute $G_{M,\theta}$ and $G_M$, defined in~\eqref{eq:gain_mul_num}, similarly for both priors.
For fixed polynomial order $k=1$ and $N \in \{20,40\}$,
the results with the physics-informed prior $u_\theta$ and the data-driven prior $u_\theta^\text{data}$
are respectively presented in \cref{tab:case1_1D_PINNs,tab:case1_1D_data}.

\begin{table}[H] % u_theta
	\centering
	\gainstableMult{fig_testcase1D_test1_gains_Tab_stats_case1_v1_degree1.csv}
	\caption{Considering the \textit{1D Laplacian case} and the PINN prior $u_\theta$. Left -- Gains in $L^2$ error of our methods with respect to PINN by taking $k=1$. Right -- Gains in $L^2$ error of our methods with respect to FEM by taking $k=1$. Our methods : additive approach, multiplicative approach with $M=3$ and $M=100$.}
	\label{tab:case1_1D_PINNs}
\end{table}

\begin{table}[H] % u_theta^data
	\centering
	\gainstableMultData{fig_testcase1D_test1_gains_Tab_stats_case1_v2_degree1.csv}
	\caption{Considering the \textit{1D Laplacian case} and the Data prior $u_\theta^\text{data}$. Left -- Gains in $L^2$ error of our methods with respect to Data Network by taking $k=1$. Right -- Gains in $L^2$ error of our methods with respect to FEM by taking $k=1$. Our methods : additive approach, multiplicative approach with $M=3$ and $M=100$.}
	\label{tab:case1_1D_data}
\end{table}

The previous results indicate that the average gain provided by the enriched FE with the PINN prior is significant, particularly when using the additive approach. These findings also confirm the behaviour of the multiplicative prior method for varying values of $M$. In contrast, when applied with data-driven instead of physics-informed training, the same method does not yield similarly favourable results.
Consequently, in the experiments we perform below, we only employ the
PINN prior.
