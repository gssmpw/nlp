\begin{table}[htbp]
    \centering
    \begin{tabular}{cc}
        \toprule
        \multicolumn{2}{c}{\textbf{Network - MLP}} \\
        \midrule
        \textit{layers} & $40,40,40,40,40$ \\
        \cmidrule(lr){1-2}
        $\sigma$ & tanh \\
        \bottomrule
    \end{tabular}
    \hspace{1cm}
    \begin{tabular}{cc}
        \toprule
        \multicolumn{2}{c}{\textbf{Training}} \\
        \midrule
        \textit{lr} & 1e-3 \\
        \cmidrule(lr){1-2}
        \textit{decay} & 0.99 \\
        \cmidrule(lr){1-2}
        $n_{epochs}$ & 20000 \\
        \cmidrule(lr){1-2}
        $N_\text{col}$ & 5000 \\
        \bottomrule
    \end{tabular}
    \hspace{1cm}
    \begin{tabular}{cccc}
        \toprule
        \multicolumn{4}{c}{\textbf{Loss weights}} \\
        \midrule
        $\omega_r$ & 1 & $\omega_\text{data}$ & 0 \\
        \cmidrule(lr){1-2} \cmidrule(lr){3-4}
        $\omega_b$ & 0 & $\omega_\text{sob}$ & 0 \\        
        \bottomrule
    \end{tabular}
    \caption{Network, training parameters (\cref{rmk:PINN_notations}) and loss weights for $u_\theta$ in the \textit{1D Elliptic} case.}
    \label{tab:paramtest2_1D}
\end{table}
