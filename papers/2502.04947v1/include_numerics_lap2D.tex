\subsection{2D Poisson problem in a square domain} \label{sec:Lap2D}

We now consider the problem of \cref{sec:Lap1D}
but in two dimensions ($d=2$), with,
\begin{equation}
	\left\{
	\begin{aligned}
		-\Delta u & = f, \; &  & \text{in } \; \Omega \times \mathcal{M}, \\
		u         & =0, \;  &  & \text{on } \; \partial\Omega \times \mathcal{M},
	\end{aligned}
	\right.
	\label{eq:Lap2D}
\end{equation}
with $\Delta$ the Laplace operator on the domain
$\Omega=[-0.5 \pi, 0.5 \pi]^2$ with boundary $\partial\Omega$,
and $\mathcal{M} \subset \mathbb{R}^p$ the parameter space (with $p$ the number of parameters).
We define the right-hand side $f$ such that the solution is given by
\begin{equation}
    \label{eq:analytical_solution_Lap2D}
	u(\bm{x},\bm{\mu})=\exp\left(-\frac{(x-\mu_1)^2+(y-\mu_2)^2}{2}\right)\sin(\kappa x)\sin(\kappa y),
\end{equation}
with $\bm{x}=(x,y)\in\Omega$ and some parameters $\bm{\mu}=(\mu_1,\mu_2) \in \mathcal{M}=[-0.5,0.5]^p$, hence with $p=2$ parameters.
With an abuse of language as well,
we refer to the quantity $\kappa$ in \eqref{eq:analytical_solution_Lap2D}
as the frequency of the solution, in the sense that it characterizes
the number of oscillations in the solution.

We start with a ``low frequency'' case in \cref{sec:Lap2Dlow}, taking $\kappa = 2$ and considering a PINN where we impose the Dirichlet boundary conditions as presented in \cref{sec:exact_imposition_of_BC}., i.e. using a level-set function.
To further improve the prior quality, we introduce an augmented loss function in \cref{sec:Lap2Dlowaug} by using the Sobolev training presented in \cref{sec:sobolev_training}.
Afterwards, we test another loss in \cref{sec:Lap2Dlowbc} that includes the Dirichlet condition, or in other words, that does not use a level-set function.
Finally, we consider a ``higher frequency''
case in \cref{sec:Lap2Dhigh}, with $\kappa = 8$.

\begin{remark}\label{rmk:Lap2D_N_nodes}
	In the following, the characteristic mesh size $h=\frac{\pi\sqrt{2}}{N-1}$ is defined as a function of $N$, considering a cartesian mesh of $N^2$ nodes for our squared 2D domain of length $\pi$.
\end{remark}

\subsubsection{Low-frequency case}\label{sec:Lap2Dlow}

We consider a ``low-frequency'' problem, taking $\kappa = 2$.
In this section, we consider the additive approach, as presented in \cref{sec:additive_prior}, by considering the PINN prior $u_\theta$. We start by testing the error estimates in \cref{par:Lap2D_error_estimations} (in $L^2$ norm) with polynomial order
$k\in\{1,2,3\}$, then we compare the derivatives of the prior and compare the different approaches in \cref{par:Lap2D_comparison}. We evaluate the gains obtained in \cref{par:Lap2D_gains} on a sample of parameters. Then, we compare the numerical costs of the different methods in \cref{par:Lap2D_costs}. Finally we discuss the importance of integrating analytical functions in \cref{par:Lap2D_sup}, as presented in \cref{sec:using_PINN}.

\paragraph*{Physics-informed training.}
Since the problem under consideration is parametric
we deploy a parametric PINN,
which depends on both the space variable $\bm{x}=(x,y) \in \Omega$
and the parameters $\bm{\mu}=(\mu_1,\mu_2) \in \mathcal{M}$.
Moreover, we strongly impose the Dirichlet boundary conditions,
as presented in \cref{sec:PINNs_parametric_PDE}.
To do this, we define the prior
\begin{equation*}
	u_{\theta}(\bm{x},\bm{\mu}) = \varphi(\bm{x}) w_{\theta}(\bm{x},\bm{\mu}) \,,
\end{equation*}
where $w_\theta$ is the neural network under consideration and $\varphi$ is a level-set function defined by
\begin{equation*}
	\varphi(\bm{x})=(x+0.5\pi)(x-0.5\pi)(y+0.5\pi)(y-0.5\pi),
\end{equation*}
which exactly cancels out on $\partial\Omega$. Since we impose the boundary conditions by using the level-set function, we will only consider the residual loss $J_r$ defined in \eqref{eq:residual_loss_parametric} in which the integrals are approached by a Monte-Carlo method, i.e.
\[
	J_r(\theta) \simeq
	\frac{1}{N_\text{col}} \sum_{i=1}^{N_\text{col}} \big| \Delta u_\theta(\bm{x}_\text{col}^{(i)};\bm{\mu}_\text{col}^{(i)}\big) + f\big(\bm{x}_\text{col}^{(i)};\bm{\mu}_\text{col}^{(i)}\big) \big|^2
\]
with the $N_\text{col}=6000$ collocation points \smash{$\big(\bm{x}_\text{col}^{(i)}, \bm{\mu}_\text{col}^{(i)}\big)_{i=1,\dots,N_\text{col}}$} uniformly chosen on $\Omega\times\mathcal{M}$. Thus, we seek to solve the following minimization problem
\begin{equation*}
	\theta^\star = \argmin_\theta J_r(\theta).
\end{equation*}
The parametric PINN $w_\theta$ is defined as an MLP with the hyperparameters defined in \cref{tab:paramtest1_2D}; we use the Adam optimizer and then switch to the LBFGS optimizer after the $n_\text{switch}$-th epoch.

\input{fig_testcase2D_test1_test1}

\paragraph{Error estimates}\label{par:Lap2D_error_estimations}\mbox{} \\

We start by testing the error estimates of \cref{lem:error_estimation_add} for the following two sets of parameters, randomly selected in $\mathcal{M}$:
\begin{equation}\label{equation:lap2d:mu-choices}
	\bm{\mu}^{(1)}=(0.05,0.22) \quad \text{and} \quad \bm{\mu}^{(2)}=(0.1,0.04)
\end{equation}
by considering the PINN prior $u_\theta$. So, for $ j\in \{1,2\}$, the aim is to compare, by varying the mesh size $h$, the $L^2$ relative errors $e_h^{(j)}$ obtained with the standard FEM method, defined in \eqref{eq:error_rel_FEM}, and $e_{h,+}^{(j)}$ obtained with the additive approach, defined in \eqref{eq:error_rel_add}.
The results are presented in \cref{fig:case1} for fixed $k \in \{1,2,3\}$ with $h$ depending on $N\in\{16,32,64,128,256\}$ as presented in \cref{rmk:Lap2D_N_nodes}.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\cvgFEMCorrAlldeg{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{1e-10}
		\caption{Case of $\bm{\mu}^{(1)}$}
		\label{fig:case1param1}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\cvgFEMCorrAlldeg{fig_testcase2D_test1_cvg_FEM_case1_v1_param2.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param2.csv}{1e-10}
		\caption{Case of $\bm{\mu}^{(2)}$}
		\label{fig:case1param2}
	\end{subfigure}
	\caption{Considering the \textit{2D low-frequency} case and the PINN prior $u_\theta$. Left -- $L^2$ relative error on $h$, obtained with the standard FEM $e_h^{(1)}$ (solid lines) and the additive approach $e_{h,+}^{(1)}$ (dashed lines) for $\bm{\mu}^{(1)}$, with $k \in \{1,2,3\}$. Right -- Same for $\bm{\mu}^{(2)}$, \eqref{equation:lap2d:mu-choices}.}
	\label{fig:case1}
\end{figure}

In \cref{fig:case1}, we observe the expected behaviour.
Indeed, the error decreases with the correct order of accuracy as the mesh size $h$ decreases.
This observation is valid for both the classical and enriched FEM.
Moreover, we observe that the error constant of the additive approach is significantly lower than that of the classical FEM. In \cref{par:Lap2D_comparison}, we will compare these different approaches in more detail. As noted in the 1D case, we can see that the additive enriched approach for $k=1$ (resp. $k=2$) seems to give the same results as standard FEM for $k=2$ (resp. $k=3$).

\paragraph{Comparison of different approaches}\label{par:Lap2D_comparison}\mbox{} \\

We now focus on the first parameter $\bm{\mu}^{(1)}$. We compare the standard FEM method with the additive approach, first in the \cref{tab:case1_2D_comparison} where we can see the different errors obtained with the different methods for $k=1$ fixed and $N\in\{16,32\}$ as well as the gains obtained in comparison with standard FEM. Next, we take a closer look at the solution obtained with the different approaches in \cref{fig:case1_2D_plots}; for each method, we compare the solution obtained ($u_h$ for standard FEM and $u_h^+$ for the additive approach) with the analytical solution $u$. For the enriched method, using the PINN prior $u_\theta$, we will also compare the proposed correction; namely, for the additive approach, we will compare $p_h^+$ with $u-u_\theta$.

\begin{table}[H]
	\centering
	\gainsPINNfirst{fig_testcase2D_test1_plots_FEM.csv}{fig_testcase2D_test1_plots_compare_gains.csv}
	\caption{Considering the \textit{2D low-frequency} case with $\bm{\mu}^{(1)}$, $k=1$ and $N\in\{16,32\}$. Left -- $L^2$ relative error obtained with FEM. Right -- Considering the PINN prior $u_\theta$, $L^2$ relative errors and gains with respect to FEM, obtained with the additive approach.}
	\label{tab:case1_2D_comparison}
\end{table}

\begin{figure}[!ht]
	\centering
    \includegraphics[scale=1]{fig_testcase2D_test1_plots_standalone_solutions.pdf}

    \includegraphics[scale=1]{fig_testcase2D_test1_plots_standalone_errors_v2.pdf}

	\caption{Considering the \textit{2D low-frequency} case with $\bm{\mu}^{(1)}$, $k=1$, $N=16$ and the PINN prior $u_\theta$. Comparison of the solution obtained with the standard FEM and the additive approach with the analytical solution. For the additive method, comparison of the correction term with the analytical one.}
	\label{fig:case1_2D_plots}
\end{figure}

In \cref{tab:case1_2D_comparison}, we observe that the additive approach significantly improves the error of the standard FEM, with gains of around $260$ for $N=16$ and $k=1$, which is equivalent to refining the mesh by a factor of $16$ for $\mathbb{P}_1$ elements. Indeed, in this case, our enriched approach gives much better results than standard FEM, notably on coarse meshes. In \cref{fig:case1_2D_plots}, we observe that the solution obtained with the additive approach is very close to the analytical solution, with a correction term that is also very close to the analytical one. This shows the effectiveness of the additive approach in this case.

\paragraph{Gains achieved with the additive approach}\label{par:Lap2D_gains}\mbox{} \\

Considering a sample $\mathcal{S}$ of $n_p=50$ parameters,
we now evaluate the gains $G_{+,\theta}$ and $G_+$ defined in \eqref{eq:gain_add_num}.
The results are presented in \cref{tab:case1_2D}
for $k \in \{1,2,3\}$ and $N \in \{20, 40\}$.

\begin{table}[H]
	\centering
	\gainstableallq{fig_testcase2D_test1_gains_Tab_stats_case1_v1.csv}
	\caption{Considering the \textit{2D low-frequency} case, $k\in\{1,2,3\}$ and the PINN prior $u_\theta$. Left -- Gains in $L^2$ relative error of the additive method with respect to PINN. Right -- Gains in $L^2$ relative error of our approach with respect to FEM.}
	\label{tab:case1_2D}
\end{table}

In \cref{tab:case1_2D}, we observe (left subtable) that our method
significantly improves the error of the PINN,
especially for large values of $k$,
where the enrichment is performed in a richer approximation space.
Moreover, we also observe (right subtable) significant
gains with respect to classical FEM.
For instance, as expected from the results of \cref{par:Lap2D_comparison}, the mean gains for $k=1$ are around $270$, which corresponds to refining the mesh approximately $16$ times for $\mathbb{P}_1$ elements. This means that our $\mathbb{P}_1$ enhanced bases capture the solution as accurately as classical $\mathbb{P}_1$ bases with a mesh four times finer.
For $k=2$ and $k=3$, the mean gains are around $134$ and $61$, respectively, which corresponds to refining the mesh approximately $5$ times for $\mathbb{P}_2$ elements and $2.8$ times for $\mathbb{P}_3$ elements. A natural follow-up question consists in assessing the impact of the PINN quality on our results. This will be the subject of the \cref{sec:Lap2Dlowaug}.

\paragraph{Costs of the different methods}\label{par:Lap2D_costs}\mbox{} \\

To more accurately assess the benefits of using the enriched methods, we look in this section at the costs of the different methods proposed, considering the parameter $\bm{\mu}^{(1)}$. Thus, we will consider that the cost of using the PINN prior $u_\theta$, corresponds to the total number of weights of the network considered. In this case, it is given as an MLP with the hyperparameters defined in \cref{tab:paramtest1_2D}, for a total of $N_\text{weights}=12,461$ weights.

For the different finite element methods, we will then try to determine, for a fixed polynomial degree $k$, the characteristic mesh size $h$ (depending on $N$ as described in \cref{rmk:Lap2D_N_nodes}) required to reach a fixed error $e$. In \cref{tab:case1_2D_costs}, we study, for $k\in\{1,2,3\}$, considering standard FEM and the additive approach, the $N$ required to achieve the same error $e$. More precisely, the characteristic mesh size required by standard FEM so that \smash{$e_h^{(1)}\approx e$} and the one required by the additive approach so that \smash{$e_{h,+}^{(1)}\approx e$}. Depending on the polynomial degree $k$, we can also determine the number of degrees of freedom $N_\text{dofs}$ associated with each case.

\begin{table}[H]
	\centering
	\coststableallq{fig_testcase2D_test1_costs_TabDoFs_case1_v1_param1.csv}
	\caption{Considering the \textit{2D low-frequency} case with $\bm{\mu}^{(1)}$, $q\in\{1,2,3\}$ and the PINN prior $u_\theta$. Left -- Characteristic $N$ (associated to the characteristic mesh size $h$) required to reach a fixed error $e$ for standard FEM and the additive approach. Right -- Number of degrees of freedom $N_\text{dofs}$ associated with each case.}
	\label{tab:case1_2D_costs}
\end{table}

In \cref{tab:case1_2D_costs}, we see that the additive approach proposed in \cref{sec:additive_prior} requires a much coarser mesh than standard FEM to achieve the same $e$ error. This is due to the error estimations of \cref{lem:error_estimation_add} which show that the error of the enhanced FEM is significantly lower than that of the classical FEM (depending on the quality of the prior). This is also reflected in the number of degrees of freedom required to achieve the same error $e$.

\begin{remark}
	The results in \cref{tab:case1_2D_costs} have been obtained by interpolating the convergence curves of \cref{fig:case1} for the different methods for a given $e$.
\end{remark}

However, the enriched approaches proposed require using the prior PINN $u_\theta$, which also includes its cost of use. For this reason, it will be interesting to look at these same costs on a set of parameters, say of size $n_p=100$. Since we are in the context of parametric PINN, we can estimate that the computational cost of solving \eqref{eq:Lap2D} on this sample of $n_p$ parameters corresponds, for the additive approach, to $n_p$ times its number of dofs plus the cost of using PINN (i.e. its total number of weights), thus $n_p\times N_\text{dofs}+N_\text{weights}$ (with $N_\text{dofs}$ the number of dofs associated to the additive approach). For standard FEM, this cost is equivalent to $n_p$ times its estimated number of dofs $n_p\times N_\text{dofs}$ (with $N_\text{dofs}$ the number of dofs associated with standard FEM). We will then compare these costs for a set of $n_p=100$ parameters, considering the same error $e$ to be achieved for both methods. The results are presented in \cref{tab:case1_2D_costs100}.

\begin{table}[H]
	\centering
	\coststableallqhundred{fig_testcase2D_test1_costs_TabDoFsParam_case1_v1_param1_nparams100.csv}
	\caption{Considering the \textit{2D low-frequency} case, $k\in\{1,2,3\}$ and the PINN prior $u_\theta$. Left -- Total costs of standard FEM and the additive approach to reach an error $e$ for a set of $n_p=1$ parameter. Right -- Same for a set of $n_p=100$ parameters.}
	\label{tab:case1_2D_costs100}
\end{table}

\begin{remark}
	Note that the \cref{tab:case1_2D_costs100} (right sub-table) is, in fact, only an estimate of the real cost of solving $n_p$ problems. In practice, the number of degrees of freedom $N_\text{dofs}$ associated with each method depends on the parameter itself. The error to be achieved $e$ will require more or less fine meshes for each parameter.
\end{remark}

In \cref{tab:case1_2D_costs100} for $n_p=1$ (left subtable), we can see that the cost of the additive method is generally lower than that of standard FEM, even though they are of the same range. However, it is important to note that these are not entirely comparable: in fact, a large part of the cost of the additive method lies in PINN prediction, which will be more or less well estimated depending on a number of hyper-parameters (number of epochs, learning rate, etc.). If we then take $n_p=100$ (right subtable), we can see that the cost of standard FEM becomes radically higher than with the additive approach. This is why the improved approach is particularly interesting for solving the \eqref{eq:Lap2D} problem on a set of parameters.

\paragraph{Integration of analytical functions}\label{par:Lap2D_sup}\mbox{} \\


This section aims to discuss one of the important points, presented in \cref{sec:using_PINN}, that enables PINN to be used effectively and can make our enriched methods more or less effective.
Indeed, according to \eqref{eq:approachform_add}, we have to integrate $f+\Delta u_\theta$ multiplied by the test function.
To perform this integration, we first interpolate this term on a polynomial space and then integrate it exactly.
The degree of this polynomial approximation is an important parameter to make our technique effective.

The goal here is simply to show that for enriched approaches to be effective, particularly the additive method, this polynomial approximation must be of a sufficiently high degree.
Consider the parameter $\bm{\mu}^{(1)}$, a polynomial degree $k=3$ and a number of nodes $N=128$.
In \cref{fig:case1_2D_highdeg}, we display the $L^2$ error of the additive approach with respect to the degree of polynomial approximation of $f+\Delta u_\theta$.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.8]{fig_testcase2D_test1_supp_standalone_high-degree.pdf}
	\caption{Considering the \textit{2D low-frequency} case with $\bm{\mu}^{(1)}$, $q=3$, $N=128$ and the PINN prior $u_\theta$. Considering the additive approach, $L^2$ error $e_{h,+}^{(1)}$ with respect to the degree of polynomial approximation of $f+\Delta u_\theta$.}
	\label{fig:case1_2D_highdeg}
\end{figure}

In \cref{fig:case1_2D_highdeg}, we observe that the error decreases as the degree of polynomial approximation increases. This shows the importance of properly interpolating analytical functions in the context of enriched methods.

\subsubsection{Low-frequency case --- Sobolev training}\label{sec:Lap2Dlowaug}

This section focuses on the same problem as in \cref{sec:Lap2Dlow}. The aim here is to show that the network quality has a non-negligible impact on the results obtained with our method and that if the network is better, our results will be, too. To this end, we defined a new prior $u_\theta^\text{sob}$ by using the Sobolev training presented in \cref{sec:sobolev_training} , where the derivatives of the solution should be better approximated than by a standard training and compared it with the PINN prior $u_\theta$ defined in \cref{sec:Lap2Dlow}. We start by testing the error estimation in \cref{par:Lap2Dlowaug_error_estimations} with $k\in\{1,2,3\}$ polynomial order and evaluate the gains obtained in \cref{par:Lap2Dlowaug_gains} on the same sample of parameters as in \cref{sec:Lap2Dlow}.

\paragraph*{Physics-informed training.} We deploy here a parametric PINN, denoted by $u_\theta^\text{sob}$, where we strongly impose the Dirichlet boundary conditions as in \cref{sec:Lap2Dlow}. The hyperparameters are defined in \cref{tab:paramtest1v7_2D};
we use the Adam optimizer and consider $N_\text{col}$ collocation points, uniformly chosen on $\Omega$.

\begin{remark}
	Adding the Sobolev loss can make training more difficult, so we only consider 3000 epochs but a batch size of 2000. This means that for each epoch, the weights will be updated 3 times (because $N_\text{col}=6000$).
\end{remark}

\input{fig_testcase2D_test1_v7_test1_v7}

We consider the residual loss $J_r$ defined in \cref{sec:Lap2Dlow} and the Sobolev loss $J_\text{sob}$ defined in \eqref{eq:sobolev_loss}, whose integrals are both approximated by a Monte-Carlo method. We then seek to solve the following minimisation problem
\begin{equation*}
	\theta^\star = \argmin_\theta \omega_r J_r(\theta) + \omega_\text{sob} J_\text{sob}(\theta).
\end{equation*}

\paragraph{Error estimates}\label{par:Lap2Dlowaug_error_estimations}\mbox{} \\

For simplicity, we consider the first parameter $\bm{\mu}^{(1)}=(0.05,0.22)$ presented in \cref{sec:Lap2Dlow}. By varying the mesh size $h$, we compare the $L^2$ relative errors between the classical FEM (\smash{$e_h^{(1)}$} defined in \eqref{eq:error_rel_FEM}), the enhanced FEM with the prior $u_\theta$ from \cref{sec:Lap2Dlow}, and the enhanced FEM with the current prior $u_\theta^\text{sob}$ (\smash{$e_{h,+}^{(1)}$} defined in \eqref{eq:error_rel_add}). The results are presented in \cref{fig:case1v7} for fixed $k\in \{1,2,3\}$.

\begin{figure}[H]
	\centering
	\hspace{-1cm}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\resizebox{1.1\linewidth}{!}{
			\cvgFEMCorrAugFirst{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{fig_testcase2D_test1_v7_cvg_Corr_case1_v7_param1.csv}{8e-7}{$u_\theta^\text{sob}$}
		}
		\caption{$k=1$}
		\label{fig:case1v7param1deg1}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\resizebox{1.1\linewidth}{!}{
			\cvgFEMCorrAugSecond{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{fig_testcase2D_test1_v7_cvg_Corr_case1_v7_param1.csv}{4e-9}{$u_\theta^\text{sob}$}
		}
		\caption{$k=2$}
		\label{fig:case1v7param1deg2}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\resizebox{1.1\linewidth}{!}{
			\cvgFEMCorrAugThird{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{fig_testcase2D_test1_v7_cvg_Corr_case1_v7_param1.csv}{4e-11}{$u_\theta^\text{sob}$}
		}
		\caption{$k=3$}
		\label{fig:case1v7param1deg3}
	\end{subfigure}
	\caption{Considering the \textit{2D low-frequency} case with $\bm{\mu}^{(1)}$. Left -- $L^2$ relative error on $h$, obtained with the standard FEM $e_h^{(1)}$ (solid line) and the additive approach $e_{h,+}^{(1)}$ (dashed lines), with $k=1$, by considering the PINN prior with standard training $u_\theta$ and Sobolev training $u_\theta^\text{sob}$. Middle -- Same with $k=2$. Right -- Same with $k=3$.}
	\label{fig:case1v7}
\end{figure}

We observe that the Sobolev training improves the results obtained with the $L^2$ training,
for $k\in\{1,2,3\}$.
This shows the impact of the quality of the network prediction on our method.
To further investigate this, we evaluate in \cref{par:Lap2Dlowaug_gains} the gains obtained with the Sobolev training on the same sample of parameters as in \cref{sec:Lap2Dlow}.

\paragraph{Gains achieved with the additive approach}\label{par:Lap2Dlowaug_gains}\mbox{} \\

Considering the same sample $\mathcal{S}$ of $n_p=50$ parameters as in \cref{sec:Lap2Dlow}, we now evaluate the gains $G_{+,\theta}$ and $G_+$ defined in \eqref{eq:gain_add_num} considering the PINN prior $u_\theta^\text{sob}$ using Sobolev training. The results are presented in \cref{tab:case1v7} for $k \in \{1,2,3\}$ and $N \in \{20,40\}$ (with $h$ depending on $N$ as described in \cref{rmk:Lap2D_N_nodes}).

\begin{table}[H]
	\centering
	\gainstableallq{fig_testcase2D_test1_v7_gains_Tab_stats_case1_v7.csv}
	\caption{Considering the \textit{2D low-frequency} case, $k\in\{1,2,3\}$ and the PINN prior $u_\theta^\text{sob}$ (Sobolev training). Left -- Gains in $L^2$ relative error of the additive method with respect to PINN. Right -- Gains in $L^2$ relative error of our approach with respect to FEM.}
	\label{tab:case1v7}
\end{table}

The gains reported in \cref{tab:case1v7} show that, compared to $L^2$ training,
Sobolev training increases the mean gains by a factor of about $3$.
This corresponds to almost half an additional mesh refinement for the $\mathbb{P}_1$ elements. We also note that this Sobolev training is particularly interesting for higher polynomial degrees, with standard $L^2$ training having lower gains than for $k=1$.
\subsubsection{Low-frequency case --- Boundary loss training}\label{sec:Lap2Dlowbc}

Now, we focus on the same problem as in \cref{sec:Lap2Dlow}and \cref{sec:Lap2Dlowaug}. We now turn to a standard PINN, denoted by $u_\theta^\text{bc}$, where we impose the boundary conditions in the loss (no longer with the level-set function). The aim here is to show that our enriched methods also work with priors that do not have exact boundary conditions. To this end, we start by testing the error estimation in \cref{par:Lap2Dlowbc_error_estimations} with $k\in\{1,2,3\}$ polynomial order and evaluate the gains obtained in \cref{par:Lap2Dlowbc_gains} on the same sample of parameters as in \cref{sec:Lap2Dlow}.

\paragraph*{Physics-informed training.} Since the problem under consideration is parametric, we deploy a parametric PINN and define the prior $u_{\theta}^\text{bc}$ as an MLP with the hyperparameters defined in \cref{tab:paramtest1v3_2D}; we use the Adam optimizer and then switch to the LBFGS optimizer after the $n_\text{switch}$-th epoch. We consider $N_\text{col}=6000$ collocation points, uniformly chosen on $\Omega$.

\input{fig_testcase2D_test1_v3_test1_v3}

We consider the same residual loss as in \cref{sec:Lap2Dlow} and the BC loss (where integral are approached by a Monte-Carlo method) is defined by
\[
	J_b(\theta) \simeq
	\frac{1}{N_\text{bc}} \sum_{i=1}^{N_\text{bc}} \big| u_\theta^\text{bc}\big(\bm{x}_\text{bc}^{(i)};\bm{\mu}_\text{bc}^{(i)}\big)\big|^2,
\]
with $N_\text{bc}=2000$ boundary collocation points \smash{$\big(\bm{x}_\text{bc}^{(i)}, \bm{\mu}_\text{bc}^{(i)}\big)_{i=1,\dots,N_\text{bc}}$}. Thus we seek to solve the following minimisation problem
\begin{equation*}
	\theta^\star = \argmin_\theta J_r(\theta) + \omega_b J_b(\theta).
\end{equation*}

\paragraph{Error estimates}\label{par:Lap2Dlowbc_error_estimations}\mbox{} \\

We consider the first parameter $\bm{\mu}^{(1)}=(0.05,0.22)$ presented in \cref{sec:Lap2Dlow} and by varying the mesh size $h$, we compare the $L^2$ relative errors between the classical FEM (\smash{$e_h^{(1)}$} defined in \eqref{eq:error_rel_FEM}), the enhanced FEM with the prior $u_\theta$ from \cref{sec:Lap2Dlow}, and the enhanced FEM with the current prior $u_\theta^\text{bc}$ (\smash{$e_{h,+}^{(1)}$} defined in \eqref{eq:error_rel_add}). The results are presented in \cref{fig:case1v3} for fixed $k\in \{1,2,3\}$.

\begin{figure}[H]
	\centering
	\hspace{-1cm}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\resizebox{1.1\linewidth}{!}{
			\cvgFEMCorrAugFirst{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{fig_testcase2D_test1_v3_cvg_Corr_case1_v3_param1.csv}{3e-6}{$u_\theta^\text{bc}$}
		}
		\caption{$k=1$}
		\label{fig:case1v3param1deg1}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\resizebox{1.1\linewidth}{!}{
			\cvgFEMCorrAugSecond{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{fig_testcase2D_test1_v3_cvg_Corr_case1_v3_param1.csv}{1e-8}{$u_\theta^\text{bc}$}
		}
		\caption{$k=2$}
		\label{fig:case1v3param1deg2}
	\end{subfigure}
	\begin{subfigure}{0.32\linewidth}
		\centering
		\resizebox{1.1\linewidth}{!}{
			\cvgFEMCorrAugThird{fig_testcase2D_test1_cvg_FEM_case1_v1_param1.csv}{fig_testcase2D_test1_cvg_Corr_case1_v1_param1.csv}{fig_testcase2D_test1_v3_cvg_Corr_case1_v3_param1.csv}{8e-11}{$u_\theta^\text{bc}$}
		}
		\caption{$k=3$}
		\label{fig:case1v3param1deg3}
	\end{subfigure}
	\caption{Considering the \textit{2D low-frequency} case with $\bm{\mu}^{(1)}$. Left -- $L^2$ relative error on $h$, obtained with the standard FEM $e_h^{(1)}$ (solid line) and the additive approach $e_{h,+}^{(1)}$ (dashed lines), with $k=1$, by considering the PINN prior with standard training $u_\theta$ and the BC loss training $u_\theta^\text{bc}$. Middle -- Same with $k=2$. Right -- Same with $k=3$.}
	\label{fig:case1v3}
\end{figure}

We can see in \cref{fig:case1v3} that the additive approach also works when the prior is not exact on the boundary, as here with $u_\theta^\text{bc}$. In particular, for $k\in\{1,2,3\}$ and the parameter $\bm{\mu}^{(1)}$, our enriched approach using the prior $u_\theta^\text{bc}$ seems to give very similar results to those obtained with $u_\theta$ even if the approach with level-set is, for this case, slightly better.

\paragraph{Gains achieved with the additive approach}\label{par:Lap2Dlowbc_gains}\mbox{} \\

Considering the same sample $\mathcal{S}$ of $n_p=50$ parameters as in \cref{sec:Lap2Dlow}, we now evaluate the gains $G_{+,\theta}$ and $G_+$ defined in \eqref{eq:gain_add_num} considering the PINN prior $u_\theta^\text{bc}$ using BC loss training. The results are presented in \cref{tab:case1v3} for $k \in \{1,2,3\}$ and $N \in \{20,40\}$.

\begin{table}[H]
	\centering
	\gainstableallq{fig_testcase2D_test1_v3_gains_Tab_stats_case1_v3.csv}
	\caption{Considering the \textit{2D low-frequency} case, $k\in\{1,2,3\}$ and the PINN prior $u_\theta^\text{bc}$ (BC loss training). Left -- Gains in $L^2$ relative error of the additive method with respect to PINN. Right -- Gains in $L^2$ relative error of our approach with respect to FEM.}
	\label{tab:case1v3}
\end{table}

The gains reported in \cref{tab:case1v3} show that for this test case, the use of the prior $u_\theta$ (using the level-set) in our enriched approach, seems to give better gains than those of \cref{tab:case1_2D}, considering the current prior $u_\theta^\text{bc}$. This may be due to the addition of the $\omega_b^\text{bc}$ hyperparameter for balancing losses in training, which may make training less efficient. However, the results obtained with the current prior $u_\theta^\text{bc}$ are still very good, and the gains are still significant compared to the standard FEM.

\subsubsection{High-frequency case}\label{sec:Lap2Dhigh}

To increase in complexity, we investigate a higher-frequency problem by taking $\kappa=8$ in \eqref{eq:analytical_solution_Lap2D}.
In this section, we start by testing the error estimates in \cref{par:Lap2Dhigh_error_estimations} with $k\in\{1,2,3\}$ polynomial order and evaluate the gains obtained in \cref{par:Lap2Dhigh_gains} on a sample of parameters.

\paragraph*{Physics-informed training.} This time, we use the Fourier features from \cite{TanSri2020} as presented in \cref{sec:spectral_bias} to construct the PINN prior $u_\theta$. The hyperparameters are defined in \cref{tab:paramtest2_2D};
we use the Adam optimizer and then switch to the LBFGS optimizer after the $n_\text{switch}$-th epoch. We consider $N_\text{col}=6000$ collocation points, uniformly chosen on $\Omega$. We impose the Dirichlet boundary conditions as in \cref{sec:Lap2Dlow} using the level-set function and the same residual loss.

\input{fig_testcase2D_test2_test2}

\paragraph{Error estimates}\label{par:Lap2Dhigh_error_estimations}\mbox{} \\

We perform the same test as in \cref{sec:Lap2Dlow}, with a standard training (Sobolev training is not considered).

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.48\linewidth}
		\centering
		\cvgFEMCorrAlldeg{fig_testcase2D_test2_cvg_FEM_case2_v1_param1.csv}{fig_testcase2D_test2_cvg_Corr_case2_v1_param1.csv}{3e-8}
		\caption{$\bm{\mu}^{(1)}$ parameter.}
		\label{fig:case2param1}
	\end{subfigure}
	\begin{subfigure}{0.48\linewidth}
		\centering
		\cvgFEMCorrAlldeg{fig_testcase2D_test2_cvg_FEM_case2_v1_param2.csv}{fig_testcase2D_test2_cvg_Corr_case2_v1_param2.csv}{3e-8}
		\caption{$\bm{\mu}^{(2)}$ parameter.}
		\label{fig:case2param2}
	\end{subfigure}
	\caption{Considering the \textit{2D high-frequency} case and the PINN prior $u_\theta$. Left -- $L^2$ relative error on $h$, obtained with the standard FEM $e_h^{(1)}$ (solid lines) and the additive approach $e_{h,+}^{(1)}$ (dashed lines) for $\bm{\mu}^{(1)}$, with $k \in \{1,2,3\}$. Right -- Same for $\bm{\mu}^{(2)}$.}
	\label{fig:case2}
\end{figure}

The results displayed in \cref{fig:case2},
where we observe the expected behavior.
Indeed, all schemes have the correct order of accuracy,
and the enhanced FEM has a significantly lower error constant than the classical FEM.

\paragraph{Comparison of different approaches}\label{par:Lap2Dhigh_comparison}\mbox{} \\

We perform the same comparison as in \cref{par:Lap2D_comparison} for the high-frequency case. We focus on the first parameter $\bm{\mu}^{(1)}$ and compare the standard FEM method with the additive approach, first in the \cref{tab:case2_2D_comparison} where we can see the different errors obtained with the different methods for $k=1$ fixed and $N\in\{16,32\}$ as well as the gains obtained in comparison with standard FEM. Next, we take a closer look at the solution obtained with the different approaches in \cref{fig:case2_2D_plots} considering $N=16$; for each method, we compare the solution obtained ($u_h$ for standard FEM and $u_h^+$ for the additive approach) with the analytical solution $u$. For the enriched method, using the PINN prior $u_\theta$, we will also compare the proposed correction; namely, for the additive approach, we will compare $p_h^+$ with $u-u_\theta$.

\begin{table}[H]
	\centering
	\gainsPINNfirst{fig_testcase2D_test2_plots_FEM.csv}{fig_testcase2D_test2_plots_compare_gains.csv}
	\caption{Considering the \textit{2D high-frequency} case with $\bm{\mu}^{(1)}$, $k=1$ and $N=16,32$. Left -- $L^2$ relative error obtained with FEM. Right -- Considering the PINN prior $u_\theta$, $L^2$ relative errors and gains with respect to FEM, obtained with the additive approach.}
	\label{tab:case2_2D_comparison}
\end{table}

\begin{figure}[!ht] \centering

    \includegraphics[scale=1]{fig_testcase2D_test2_plots_standalone_solutions.pdf}

    \includegraphics[scale=1]{fig_testcase2D_test2_plots_standalone_errors_v2.pdf}

	\caption{Considering the \textit{2D high-frequency} case with $\bm{\mu}^{(1)}$, $k=1$, $N=16$ and the PINN prior $u_\theta$. Comparison of the solution obtained with the standard FEM and the additive approach with the analytical solution. For the additive method, comparison of the correction term with the analytical one.}
	\label{fig:case2_2D_plots}
\end{figure}

We can see here that the gains obtained in \cref{tab:case2_2D_comparison} are much better than for the ``low frequency'' case presented in \cref{sec:Lap2Dlow}. This is, in fact, due to FEM's difficulty in approximating the solution for high frequencies, especially on coarse meshes. In fact, for the same choice of parameters, the FEM error on this high-frequency problem is 10 times worse than on the low-frequency one, which explains why our gains are so much greater. This also makes the use of the proposed enriched methods particularly interesting.
Moreover, we note that while FEM provides a reasonable approximation of the mean of the solution (as evidenced by the second figure on the top row of \cref{fig:case2_2D_plots}), it is unable to correctly resolve the small-scale oscillating behaviour of the solution.
The additive correction restores this ability, and the new solution (third figure on the top row of \cref{fig:case2_2D_plots}) is much better able to capture the oscillations.

\paragraph{Gains achieved with the additive approach}\label{par:Lap2Dhigh_gains}\mbox{} \\

We now evaluate the gains $G_{+,\theta}$ and $G_+$, defined in \eqref{eq:gain_add_num},
using the same sample $\mathcal{S}$ of $n_p=50$ parameters. The results are reported in \cref{tab:case2} for $k \in \{1,2,3\}$ and $N \in \{20, 40\}$.

\begin{table}[H]
	\centering
	\gainstableallq{fig_testcase2D_test2_gains_Tab_stats_case2_v1.csv}
	\caption{Considering the \textit{2D high-frequency} case, $k\in\{1,2,3\}$ and the PINN prior $u_\theta$. Left -- Gains in $L^2$ relative error of the additive method with respect to PINN. Right -- Gains in $L^2$ relative error of our approach with respect to FEM.}
	\label{tab:case2}
\end{table}

The same results can be observed in \cref{tab:case2} as for the $\bm{\mu}^{(1)}$ parameter. These could be improved by considering a Sobolev training, as in the ``low-frequency`` case presented in \cref{sec:Lap2Dlowaug}.

