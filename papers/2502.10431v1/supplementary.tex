
\newpage
\section{Algorithm for Training SAC with CV Flow}\label{sup:fine_tune}
\begin{algorithm}[ht]
\caption{Soft Actor Critic with \our}
\label{alg:soft_actor_critic}
\begin{algorithmic}[1]
\STATE Load pre-trained flow model: $f_\psi$
\STATE Initialize critics and policy parameters: $\theta$, $\bar{\theta}$,$\phi$
\STATE $\mc{B} \la \varnothing$ \COMMENT{Initialize empty replay buffer}
\FOR{each iteration}
	\FOR{each environment step}
	\STATE $\hat{a}_t \sim \mu_\phi(\hat{a}_t|s_t)$ \COMMENT{Sample latent action}
	\STATE $\hat{a}_t \leftarrow  \min(\hat{a}_t, min=-3, max=3)$ \COMMENT{Clip the latent action at 3-sigma}
        \STATE $a_t \la f_\psi(\hat{a}_t,s_t)$ \COMMENT{Apply flow to get environment action}
        \IF{$a_t \notin \mc{C}(s_t)$ }
            \STATE $a_t \la \argmin_{\tilde{a}_t \in \mc{C}(s_t)} \|\tilde{a}_t - a_t\|^2_2$ 
        \ENDIF
	\STATE $s_{t+1} \sim p(s_{t+1}| s_t, a_t)$ \COMMENT{Collect next state}
	\STATE $\mathcal{B} \la \mathcal{B} \cup \left\{\langle s_t, \hat{a}_t, r(s_t, a_t), s_{t+1}\rangle\right\}$
	\ENDFOR
 
	\FOR{each gradient step}
        \STATE $\theta \la \theta - \lambda_Q \hat\nabla_\theta J^Q(\theta)$ \COMMENT{Update Q function using Eq.\eqref{eq:sac_critic_loss}}
        \STATE $\phi \la \phi - \lambda_\mu \hat\nabla_\phi J^\mu(\phi)$ \COMMENT{Update policy using Eq.\eqref{eq:sac_policy_loss}}
        % \STATE $\alpha \la \alpha - \lambda_\alpha \hat\nabla J(\alpha)$ \COMMENT{Update temperature}
        \STATE $\bar{\theta} \la \tau \theta + (1-\tau){\bar{\theta}} $ \COMMENT{Update target network}
	\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Algorithm for Training DDPG with CV Flow}\label{sup:ddpg}
Our \ourcv-Flow can also be integrated with the DDPG algorithm~\cite{lillicrap2015continuous}, where the policy is implemented as a deterministic function of the state. The pseudocode for the algorithm is provided in Algorithm~\ref{alg:ddpg}.
\vskip -1pt
{
\small
\begin{equation}
\nabla_\phi J^{\mu}(\phi) = \E_{s\sim \mathcal{B}} \nabla_a Q(s, a)  \nabla_{\hat{a}}  f_\psi(\hat{a},s) \nabla_\phi \mu_\phi(s)|_{\hat{a}=\mu_\phi(s),a=f_\psi(\hat{a},s)}    \\
\label{eq:ddpg_policy_loss}
\end{equation}
}
\vskip -1pt
{
\small
\begin{align}\label{eq:ddpg_critic_loss}
J^{Q}(\theta)=\E_{\substack{(s,{a},r,s')\sim \mc{B}}}
[(
Q_{\theta}(s,{a}) -(r+\gamma Q_{\bar{\theta}}(s',a'
)
)
)^{2}|_{a' = f_\psi(\mu_{\phi}(s'), s')}]
\end{align}
}
\begin{algorithm}[ht]
\caption{ DDPG with \our}
\label{alg:ddpg}
\begin{algorithmic}[1]
\STATE Load pre-trained flow model: $f_\psi$
\STATE Initialize critics and policy parameters: $\theta$, $\bar{\theta}$,$\phi$
\STATE $\mc{B} \la \varnothing$ \COMMENT{Initialize empty replay buffer}
\FOR{each iteration}
	\FOR{each environment step}
	\STATE $\hat{a}_t \leftarrow \mu_\phi(s_t)$ \COMMENT{Get latent action from the deterministic policy}
        \STATE $\hat{a}_t \leftarrow \hat{a}_t + \delta$ \COMMENT{Add noise \(\delta \sim \mathcal{N}(0,1)\) to the action for exploration.} 
	\STATE $\hat{a}_t \leftarrow  \min(\hat{a}_t, min=-3, max=3)$ \COMMENT{Clip the latent action to stay within 3-sigma}
        \STATE $a_t \la f_\psi(\hat{a}_t,s_t)$ \COMMENT{Apply flow to get environment action}
        \IF{$a_t \notin \mc{C}(s_t)$ }
            \STATE $a_t \la \argmin_{\tilde{a}_t \in \mc{C}(s_t)} \|\tilde{a}_t - a_t\|^2_2$ 
        \ENDIF
	\STATE $s_{t+1} \sim p(s_{t+1}| s_t, a_t)$ \COMMENT{Collect next state}
	\STATE $\mathcal{B} \la \mathcal{B} \cup \left\{\langle s_t, {a}_t, r(s_t, a_t), s_{t+1}\rangle\right\}$
	\ENDFOR
 
	\FOR{each gradient step}
        \STATE $\theta \la \theta - \lambda_Q \nabla_\theta J^Q(\theta)$ \COMMENT{Update Q function using Eq.~\eqref{eq:ddpg_critic_loss}}
        \STATE $\phi \la \phi - \lambda_\mu \nabla_\phi J^\mu(\phi)$ \COMMENT{Update policy using Eq.~\eqref{eq:ddpg_policy_loss}}
        % \STATE $\alpha \la \alpha - \lambda_\alpha \hat\nabla J(\alpha)$ \COMMENT{Update temperature}
        \STATE $\bar{\theta} \la \tau \theta + (1-\tau){\bar{\theta}} $ \COMMENT{Update target network}
	\ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Experiment Setup}

Here we present the exact constraints of our tasks in Table~\ref{sup:envs}. To train the flow, the state distribution $p_S$ was determined based on the environment. Specifically, to train the flow, we do not need the complete state; only the component of the state distribution that the constraint depends on is required. We observed that most of these state variables are bounded, except for HC+O. Therefore, the state distribution was defined as a uniform distribution over the bounded space. The standard deviation of $w_i$ for HC+O was calculated based on the data collected by running an unconstrained agent on the environment.

{
\begin{table*}[htb]
\begin{center}
\setlength\extrarowheight{3.5pt}
\begin{tabular}{l c l c r}
 Environment & Name & Constraint & Convexity & State Distribution ($p_S$) \\ 
 \hline 
Reacher & R+L2 & $a_1^2 + a_2^2 \leq 0.05$ & Convex & N/A \\  
 & R+D & $0.04 \leq a_1^2 + a_2^2 \leq 0.05$ & Non-Convex & N/A\\
 % \hline
 Hopper & H+M & $ \sum_{i=1}^3 max\{w_i a_i, 0\}  \leq 10$ & Convex & $w_i \sim \mc{U}_{[-10,10]}$ \\
  & H+O+S & $\sum_{i=1}^3 |w_i a_i|  \leq 10 \land \sum_{i=1}^3 a_i^2 sin^2 \theta_i  \leq 0.1$ & Convex 
  & $w_i \sim \mc{U}_{[-10,10]}, \theta_i \sim \mc{U}_{[-\pi,\pi]}$
  \\
  & H+D & $1.4 \leq \sum_{i=1}^3a_i^2 \leq 1.5$ & Non-Convex & N/A \\
  % \hline
Walker2D & W+M & $ \sum_{i=1}^6 max\{w_i a_i, 0\}  \leq 10$ & Convex & $w_i \sim \mc{U}_{[-10,10]}$ \\
  & W+O+S & $\sum_{i=1}^6 |w_i a_i|  \leq 10 \land \sum_{i=1}^6 a_i^2 sin^2 \theta_i  \leq 0.1$ & Convex
  & $w_i \sim \mc{U}_{[-10,10]}, \theta_i \sim \mc{U}_{[-\pi,\pi]}$ \\
HalfCheetah & HC+O & $\sum_{i=1}^6 |w_i a_i|  \leq 20$ & Convex & $w_i \sim \mc{N}_{\mu=0, \sigma=15}$ \\
\end{tabular}
\end{center}
\caption{Analytical expressions of constraints and their environments.}\label{sup:envs}
\end{table*}

\begin{table*}[ht]
\begin{center}
\setlength\extrarowheight{2.5pt}
\begin{tabular}{l c c c c c c c c}
Hyperparameters & \multicolumn{2}{c}{Reacher}  &  \multicolumn{2}{c}{Hopper} &  \multicolumn{2}{c}{Walker2D/HalfCheetah} & BallND/Spaceship \\
\hline
& DDPG & SAC & DDPG & SAC & DDPG & SAC & DDPG  \\
Discount factor & 0.98 & 0.98 & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\
Net. arch. 1st layer & 400 &  400 &  400 & 256 & 400 & 256 & 64  \\
Net. arch. 2nd layer & 300 &  300 &  300 & 256 & 300 & 256 & 64  \\
Batch size & 100 & 256 & 256 & 256 & 100 &  256 & 256  \\
Learning starts & 1e5 & 1e5 & 1e5 & 1e5 & 1e5 & 1e5 & 256  \\
FW Learning rate & 0.05 & - & 0.01 & - & 0.01 & - & 0.05  \\
Learning rate & 1e-3 & 7.3e-4 & 3e-4 & 3e-4 & 1e-3 & 3e-4 & 1e-4  \\
\end{tabular}
\end{center}
\caption{Hyperparameters: SPre+ and \our, which are based on SAC, utilize the hyperparameters listed in the SAC column. Conversely, DPre+, NFW and FlowPG based on DDPG, uses the hyperparameters specified in the DDPG column.}
\end{table*}
}

\section{Results}

\begin{table}
        \begin{center}
                \begin{tabular}{|l|l|l|l|l|l|}
                        \hline
                        Problem & DPre+ & NFW & SPre+ & FlowPG & SAC+CVFlow (Ours) \\
                        \hline
                        R+D (Non-convex) & 98.15 ± 0.18 & 95.50 ± 0.09 & 97.03 ± 0.04 & 24.79 ± 10.27 & 0.01 ± 0.00 \\
                        \hline
                        H+D (Non-convex) & 74.10 ± 13.27 & 74.89 ± 9.62 & 77.72 ± 13.77 & 32.29 ± 3.58 & 2.18 ± 1.05 \\
                        \hline
                        R+L2 & 82.51 ± 3.48 & 22.71 ± 0.30 & 16.47 ± 0.37 & 0.03 ± 0.00 & 1.70 ± 0.22 \\
                        \hline
                        H+M & 3.67 ± 2.68 & 4.45 ± 1.56 & 3.25 ± 1.38 & 4.93 ± 1.07 & 0.25 ± 0.20 \\
                        \hline
                        H+O+S & 42.44 ± 6.35 & 2.14 ± 0.94 & 7.83 ± 1.16 & 53.91 ± 6.26 & 2.42 ± 1.19 \\
                        \hline
                        W+M & 30.55 ± 14.89 & 4.50 ± 1.78 & 11.70 ± 4.14 & 16.40 ± 5.95 & 2.41 ± 0.72 \\
                        \hline
                        W+O+S & 84.89 ± 7.48 & 3.00 ± 0.62 & 20.44 ± 3.70 & 47.80 ± 19.10 & 1.55 ± 1.20 \\
                        \hline
                        HC+O & 73.57 ± 4.82 & 9.73 ± 2.40 & 46.66 ± 1.68 & 61.10 ± 5.21 & 5.04 ± 1.21 \\
                        \hline\hline
                        Ball1D & 0.00 ± 0.00 & 0.00 ± 0.00 & 0.00 ± 0.00 & 0.00 ± 0.00 & 0.00 ± 0.00 \\
                        \hline  
                        Ball3D & 16.01 ± 11.82 & 0.00 ± 0.00  & 23.16 ± 17.72 & 4.07 ± 4.16 & 0.37 ± 0.68 \\
                        \hline   
                        Space Corridor & 54.32 ± 31.63 & 89.54 ± 2.48 & 23.71 ± 26.31 & 59.07 ± 38.09 & 12.06 ± 6.30 \\
                        \hline  
                        Space Arena & 51.65 ± 29.59 & 91.74 ± 1.95 & 13.60 ± 6.01 & 29.32 ± 17.41 & 10.78 ± 4.07 \\
                        \hline
                \end{tabular}
        \end{center}
        \caption{The percentage of constraint violations during RL training. A lower value is preferable.}
        \label{tab:--}
\end{table}


\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Problem          & DPre+         & NFW           & SPre+         & FlowPG        & SAC+CVFlow (Ours) \\ \hline
R+D (Non-convex) & 65.70 ± 0.16  & 94.92 ± 0.38  & 96.74 ± 0.48  & 24.89 ± 10.08 & 0.00 ± 0.00       \\ \hline
H+D (Non-convex) & 85.00 ± 18.76 & 69.47 ± 24.94 & 75.27 ± 21.87 & 27.81 ± 8.83  & 2.62 ± 1.86       \\ \hline
R+L2             & 55.40 ± 2.87  & 22.22 ± 1.74  & 13.56 ± 1.57  & 0.00 ± 0.00   & 1.40 ± 0.51       \\ \hline
H+M              & 5.37 ± 3.31   & 5.82 ± 3.81   & 4.70 ± 2.21   & 8.26 ± 2.66   & 0.36 ± 0.24       \\ \hline
H+O+S            & 58.46 ± 4.83  & 2.45 ± 1.49   & 7.77 ± 1.08   & 35.52 ± 16.81 & 4.63 ± 1.73       \\ \hline
W+M              & 55.10 ± 22.17 & 4.08 ± 2.03   & 10.86 ± 6.72  & 32.62 ± 13.35 & 1.75 ± 0.80       \\ \hline
W+O+S            & 90.94 ± 17.44 & 2.19 ± 1.34   & 18.04 ± 4.20  & 63.77 ± 40.43 & 0.85 ± 0.83       \\ \hline
HC+O             & 71.92 ± 5.54  & 10.33 ± 2.40  & 43.22 ± 4.95  & 60.59 ± 8.35  & 5.32 ± 1.73       \\ \hline   
\end{tabular}
        \caption{Percentage of constraint violations, averaged over 100 episodes, when executing the final trained policy.}
        \label{tab:cv-at-runtime}
\end{center}
\end{table}

\begin{figure*}[tb]
    \centering	
    \includegraphics[width=0.3\linewidth]{./figures/ablation-penalty-legend-new.pdf}\\
    \vspace{-0.2cm}
    \includegraphics[width=0.75\linewidth]{./figures/ablation-penalty-with-label.pdf}
    \vspace{-0.2cm}
    \caption{Comparison of results for the flow with and without the $\|\hat{a}\|_2^2$ term for RL agent is shown in this figure. The first row contains the return of the agent, while the second row shows the constraint violation (CV) count as a percentage of total-timesteps. Without the $\|\hat{a}\|_2^2$  term, the agent not only produce higher constraint violations but also struggles to learn.}\label{fig:ablation_penalty}
    \vspace{-0.4cm}
\end{figure*}


\subsection{Comparison of Standard Flow and \our}
Here, we compare our approach against standard flow in terms of  the accuracy, recall and F1-score during the training process. 

\noindent\textbf{Accuracy} is the percentage of samples generated using normalizing flow that fall within the feasible region. To evaluate the accuracy of the model, we first generate $n$ samples using the flow and then calculate the percentage of these samples that satisfy predefined action constraints. 

\noindent\textbf{Recall} (also called coverage) indicates the fraction of valid actions that can be generated from the latent space. To measure the recall we first generate samples from the feasible region using a technique such as rejection sampling. Then we map these samples to the latent space using the inverse of the flow model $f^{-1}$ and compute the percentage that falls within the domain of the latent space $[-1, 1]^d$. Mathematically, given a conditioning variable $s$ and constrained space $\mathcal{C}(s)$, the recall is computed as follows. 
{
\begin{equation} \label{eq:recall}
recall(s) = \frac{\sum_{a \in \mathcal{C}(s)} \mathbb{I}_{dom_f}f^{-1}(a|s)}{|\mathcal{C}(s)|}
\end{equation}}

\noindent\textbf{F1-Score} is the standard machine
learning evaluation metric, which combines accuracy and recall.

\noindent As presented in Figure~\ref{fig:compare-flow} \ourcv-Flow achieves higher accuracy and a better F1 score compared to the standard flow. We use a uniform base distribution for this experiment because a Gaussian base distribution, with its support $[-\infty, \infty]$, does not allow exact measurement of coverage.



\begin{figure}[b]
  \centering	
  \includegraphics[width=0.5\linewidth]{figures/compare_two_flows_legend.pdf}\\
  \includegraphics[width=0.99\linewidth]{./figures/compare_two_flows3.pdf}
  \caption{Comparison of the standard Flow and {\our} with Uniform Base Distribution}
  \label{fig:compare-flow}
\end{figure}

\noindent \textbf{Computational resources and runtime}: Experimented were conducted on a machine with the following specifications:
\begin{itemize}
\item GPU: NVIDIA GeForce RTX 3090 $\times$ 4
\item CPU: AMD EPYC 7402 24-Core Processor $\times$ 2
\item RAM: 968 GB
\end{itemize}


\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{./figures/avg-timesteps.pdf}
    \caption{Average time-steps/second~$(\uparrow)$, Our approach achieves comparable speed to baselines. Specifically, with non-convex constraints (H+D, R+D), our approach significantly outpaces the baselines in terms of runtime.
    We understand the difficulty of reproducing the runtime-related results; however, we conducted each experiment on the same machine in an isolated setting to gain a better understanding of the runtime.
    }\label{sup:timesteps}
 \end{figure*}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{./figures/cv-magnitude.pdf}
    \caption{Average magnitude of constraint violations~$(\downarrow)$; a CV magnitude of zero implies no constraint violation. Compared to all the baselines, our approach results in a significantly lower or similar magnitude of constraint violation value, when the constraints are violated.V
    }\label{sup:cv-magnitude}
 \end{figure*}


\begin{table}[tb]
\begin{center}
\begin{tabular}{|l|ll|ll|}
\hline
 & \multicolumn{2}{l|}{CV-Count}                                     & \multicolumn{2}{l|}{Final Return}                                          \\ \hline
                              Problem & \multicolumn{1}{l|}{DDPG+CVFlow}          & SAC+CVFlow            & \multicolumn{1}{l|}{DDPG+CVFlow}               & SAC+CVFlow                \\ \hline
R+D (Non-convex)               & \multicolumn{1}{l|}{\textbf{0.01 ± 0.00}} & \textbf{0.01 ± 0.00}  & \multicolumn{1}{l|}{\textbf{22.18 ± 1.73}}     & 21.45 ± 2.46              \\ \hline
H+D (Non-convex)               & \multicolumn{1}{l|}{7.26 ± 1.97}          & \textbf{2.18 ± 1.05}  & \multicolumn{1}{l|}{3190.07 ± 101.52}          & \textbf{3236.70 ± 89.01}  \\ \hline
R+L2                           & \multicolumn{1}{l|}{\textbf{0.00 ± 0.00}} & 1.70 ± 0.22           & \multicolumn{1}{l|}{22.74 ± 1.96}              & \textbf{23.55 ± 1.95}     \\ \hline
H+M                            & \multicolumn{1}{l|}{3.26 ± 4.88}          & \textbf{0.25 ± 0.20}  & \multicolumn{1}{l|}{\textbf{3155.78 ± 207.67}} & 2935.37 ± 388.29          \\ \hline
H+O+S                          & \multicolumn{1}{l|}{2.51 ± 2.04}          & \textbf{2.42 ± 1.19}  & \multicolumn{1}{l|}{\textbf{3041.27 ± 164.10}} & 2594.44 ± 1127.36         \\ \hline
W+M                            & \multicolumn{1}{l|}{3.77 ± 2.05}          & \textbf{2.41 ± 0.72}  & \multicolumn{1}{l|}{\textbf{4263.25 ± 488.60}} & 3733.10 ± 345.92          \\ \hline
W+O+S                          & \multicolumn{1}{l|}{\textbf{1.38 ± 0.51}} & 1.55 ± 1.20           & \multicolumn{1}{l|}{\textbf{3661.52 ± 250.71}} & 3335.45 ± 200.92          \\ \hline
HC+O                           & \multicolumn{1}{l|}{34.91 ± 5.64}         & \textbf{5.04 ± 1.21}  & \multicolumn{1}{l|}{8170.54 ± 745.64}          & \textbf{9181.35 ± 680.80} \\ \hline
Ball1D                         & \multicolumn{1}{l|}{\textbf{0.00 ± 0.00}} & \textbf{0.00 ± 0.00}  & \multicolumn{1}{l|}{\textbf{62.90 ± 3.81}}     & 60.62 ± 3.38              \\ \hline
Ball3D                         & \multicolumn{1}{l|}{2.67 ± 5.87}          & \textbf{0.37 ± 0.68}  & \multicolumn{1}{l|}{\textbf{44.20 ± 3.21}}     & 41.93 ± 6.13              \\ \hline
Space-Corridor                 & \multicolumn{1}{l|}{39.08 ± 27.76}        & \textbf{12.06 ± 6.30} & \multicolumn{1}{l|}{900.00 ± 316.23}           & \textbf{1000 ± 0.00}      \\ \hline
Space-Arena                    & \multicolumn{1}{l|}{21.90 ± 14.98}        & \textbf{10.78 ± 4.07} & \multicolumn{1}{l|}{968.00 ± 65.70}            & \textbf{1000 ± 0.00}      \\ \hline
\end{tabular}
\caption{\ourcv-Flow can be integrated with DDPG as well.}
\end{center}
\end{table}

