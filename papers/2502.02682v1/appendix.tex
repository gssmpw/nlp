\section*{Appendix}

\section{Experimental Details}\label{sect:detail}
\subsection{Darcy Flow}

We considered a steady-state 2D Darcy Flow equation~\citep{li2020fourier}, 
\begin{align}
    -\nabla \cdot (a(x)\nabla u(x)) = f(x)  \;\;  x \in (0,1)^2, \nonumber \\
    u(x)=0 \;\; x \in \partial(0,1)^2,
\end{align}
where $u(\x)$ is the velocity of the flow, $a(\x)$ characterizes the conductivity of the media, and $f(\x)$ is the source function that can represent flow sources or sinks within the domain. In the experiment, our goal is to predict the solution $u$ given the external source $f$. To this end, we fixed the conductivity $a$, which is generated by first sampling a Gauss random field $\alpha$ in the domain and then applying a thresholding rule: $a(\x) = 4$ if $\alpha(\x)<0$, otherwise $a(\x) = 12$. We then used another Gauss random field to generate samples of $f$. We followed~\citep{li2020fourier} to solve the PDE using a second-order finite difference solver and collected the source and solution at a $128 \times 128$ grid.

\subsection{Nonlinear Diffusion PDE}
We next considered a nonlinear diffusion PDE, 
\begin{align}
    \partial_t u(x,t) &= 10^{-2} \partial_{xx}u(x,t) + 10^{-2} u^2(x,t) + f(x,t), \notag \\
    u(-1,t) &= u(1,t) = 0, \;\; u(x, 0) = 0,  
    %u(x,0) =& 0 \hspace{1cm} x \in [-1,1]  
\end{align}
where $(x, t) \in [-1, 1] \times [0, 1]$. Our objective is to predict the solution function $u$ given the source function $f$. We used the solver provided in~\citep{lu2022comprehensive}, and discretized both the input and output functions at a $128 \times 128$ grid. The source $f$ was sampled from a Gaussian process with an isotropic square exponential (SE) kernel for which the length scale was set to 0.2. 

\subsection{Eikonal Equation}
Third, we employed the Eikonal equation,  widely used in geometric optics and wave modeling. It describes given a wave source, the propagation of wavefront across the given media where the wave speed can vary at different locations. The equation is as follows, 
\begin{align}
    |\nabla u(\x)| = \frac{1}{f(\x)}, \;\;\x \in [0, 256] \times [0, 256],
\end{align}
where $u(\x)$ is the travel time of the wavefront from the source to location $\x$, $|\cdot|$ denotes the Euclidean norm, and $f(\x)>0$ is the speed of the wave at $\x$.

In the experiment, we set the wave source at $(0, 10)$. The goal is to predict the travel time $u$ given the heterogeneous wave speed $f$.  We sampled an instance of $f$ using the expression: 
\[
f(\x) = \max(g(\x), 0) + 1.0,
\]
where $g(\cdot)$ is sampled from a Gaussian process using the isotropic SE kernel with length-scale $0.1$. We employed the \texttt{eikonalfm} library (\url{https://github.com/kevinganster/eikonalfm/tree/master}) that implements the Fast Marching method~\cite{sethian1999fast} to compute the solution $u$. 


\subsection{Poisson Equation}
Fourth, we considered a 2D Poisson Equation,
\begin{align}
    -\Delta u = f, \quad \text{in } \Omega=[0, 1]^2, \quad u|_{\partial D} = 0.
\end{align}
where $\Delta$ is the Laplace operator. The solution is designed to take the form, $u(x_1, x_2)=\frac{1}{\pi K^2}\sum_{i=1}^K\sum_{j=1}^K a_{ij} (i^2+j^2)^r\sin(i \pi x_1)\cos(j \pi  x_2)$, and $f(x_1, x_2)$ is correspondingly computed via the equation.  To generate the dataset, we set $K=5$ and $r=0.5$, and independently sampled each element $a_{ij}$ from a uniform distribution on $[0, 1]$.



\subsection{Advection Equation}
Fifth, we considered a wave advection equation,
\begin{align}
\frac{\partial u}{\partial t} + \frac{\partial u}{\partial x} = f, \quad x \in [0,1], \quad t \in [0,1].
\end{align} 
The solution is represented by  a kernel regressor, $u(\x)= \sum_{j=1}^M w_j k(\x, \z_j)$, and the source $f$ is computed via the equation. To collect instances of $(f, u)$, we used the square exponential (SE) kernel with length-scale $0.25$. We randomly sampled the locations $\z_j$ from the domain and the weights $w_j$ from a standard normal distribution. 

\subsection{Fatigue Modeling} 
\begin{figure}
    \centering
    \setlength\tabcolsep{0pt}
\includegraphics[width=0.5\textwidth]{figs-fno/crack_SIF/crack-shape.pdf}
    \caption{\small Example of semi-elliptic surface  crack on a plates~\citep{merrell2024stress}. }
    \label{fig:SIF_shape}
\end{figure}
We considered predicting the SIF values along semi-elliptic surface cracks on plates, as shown in Fig~\ref{fig:SIF_shape}. The SIF value can be viewed as a function of the angle $\phi \in [0, \pi]$, which decides the location of each point on the crack surface.  The geometry parameters that characterize the crack shape and position were used as the input, including $a/c$, $a/t$ and $c/b$. In the operator learning framework, the input can be viewed as a function with three constant outputs. The dataset was produced via a high-fidelity FE model under Mode I tension~\citep{merrell2024stress}. Each data instance includes 128 samples of the SIF values drawn uniformly across the range of $\phi$.   

\subsection{Hyperparameter Settings}\label{sect:appendix:methods} 
For the pseudo physics neural network $\phi$ --- see \eqref{eq:pde-learn} --- we tuned the kernel size from \{(3, 3), (5, 5), (7, 7), (9, 9)\}. The stride was set to 1 and padding was set to ``same'' to ensure the output shape does not change. In the subsequent fully connected layers, we chose the number of layers from \{3, 4, 5, 6\}, and the layer width from \{16, 32, 64\}. We used {GeLU} activation. %For the cases of Darcy Flow, Eikonal and Poisson, we used the following derivatives $\{\partial_{x_1} u, \partial_{x_2} u, \partial_{x_1x_1}u, \partial_{x_2x_2}u\}$, and for the other cases, we used $\{\partial_x u, \partial_{xx} u, \partial_t u, \partial_{tt} u\}$. Since SIF is a 1d function (the input is the angle), we used the derivatives \{$\partial_x u, \partial_{xx} u$\}.
For FNO, we set the number of modes to 12 and channels to 32 (in the lifted space). We varied the number of Fourier layers from \{2, 3, 4\}. For DONet, in all the cases except Darcy Flow,  the trunk net and branch net were constructed as fully connected layers. We varied the number of layers from \{2, 3, 4\} and the layer width was chosen from \{30, 40, 50, 60\}, with ReLU activation. For the case of Darcy flow, we found that DONet with only fully connected layers exhibited inferior performance. To address this, we introduced convolution layers into the branch net. We selected the number of convolution layers from \{3,5,7\}, and employed batch normalization and leaky ReLU after each convolution layer.
To incorporate the learned pseudo physics representation into the training of FNO or DONet,  we randomly sampled 200 input functions to construct the second loss term in  \eqref{eq:fine-tune-loss}. 
We set the maximum number of iterations to $10$ and selected the weight $\lambda$ from $[10^{-1}, 10^{2}]$. 
All the models were implemented by PyTorch~\citep{paszke2019pytorch}, and optimized with ADAM~\citep{kingma2014adam}. The learning rate was selected from $\{10^{-4}, 5\times 10^{-4}, 10^{-3}\}$. The number of epochs for training or fine-tuning FNO, DONet and pseudo physics network $\phi$ was set to 500 to ensure convergence. 


\begin{table*}[h]
    \caption{\small Relative $L_2$ error in five operator learning benchmarks with richer data, where ``PPI'' is short for ``Pseudo-Physics Informed''. The results were averaged from five runs.  } \label{tb:pred-error-large-data}
    \small
    \centering
    \begin{subtable}{\textwidth}
        \caption{\small \textit{Darcy flow}}\label{tab:darcy}
        \small 
        \centering
    \begin{tabular}{cccc}
        \hline
        \textit{Training size}      &  & {600}             & {1000}             \\ \hline
        FNO                  &  & 0.0093 $\pm$ 0.00015	& 0.0079 $\pm$ 0.00018          \\
        PPI-FNO          &  & {0.0087} $\pm$ {0.00040}          & {0.0082} $\pm$ {0.00039}      \\
        \hline
        DONet             &  & 0.0540 $\pm$ 0.00064         & 0.0446 $\pm$ 0.00023       \\
        PPI-DONet             &  & 0.0415 $\pm$ 0.00077        & 0.0362 $\pm$ 0.00049     \\ \hline
        % $\phi$-FNN             &  & 0.2585$\pm$ 0.0141          & 0.1819 $\pm$ 0.0026		& 0.1520  $\pm$ 0.0020	& 0.1413 $\pm$ 0.0013  \\
        % $\phi$          &  & 0.2285$\pm$ 0.0147         & 0.1392 $\pm$ 0.0080 	& 0.0898 $\pm$ 0.0046	& 0.0688 $\pm$ 0.0032  \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \caption{\small \textit{Nonlinear diffusion}}
        \small
        \centering
        \begin{tabular}{cccc}
        \hline
        \textit{Training size}      &  & {600}             & {1000}            \\ \hline
        FNO                  &  & 0.0035 $\pm$ 0.0007	& 0.0028 $\pm$ 0.0004       \\
        PPI-FNO          &  & 0.0047 $\pm$ 0.00102         & 0.0042 $\pm$ 0.00096         \\
        \hline
        DONet             &  & 0.0222 $\pm$ 0.00020         & 0.0187 $\pm$ 0.00023      \\
        PPI-DONet             &  & 0.0379$\pm$ 0.00088         & 0.0368 $\pm$ 0.00093 	 \\ \hline
        % $\phi$-FNN             &  & 0.0826$\pm$ 0.0070          & 0.0660 $\pm$ 0.0069  	& 0.0586  $\pm$ 0.0020	& 0.0463 $\pm$ 0.0022  \\
        % $\phi$            &  & 0.0303$\pm$ 0.0006         & 0.0233 $\pm$ 0.0005	& 0.0190 $\pm$ 0.0001	& 0.0163 $\pm$ 0.0002  \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Eikonal}}
    \small
    \centering
        \begin{tabular}{cccc}
        \hline
        \textit{Training size}        & {600}             & {1000}             \\ \hline
        FNO                    & 0.0193 $\pm$ 0.00017	& 0.0148 $\pm$ 0.00009         \\
        PPI-FNO            & 0.0199 $\pm$ 0.00016         & 0.0160 $\pm$ 0.00009         \\
        \hline
        DONet               & 0.0460 $\pm$ 0.00021         & 0.0411 $\pm$ 0.00038        \\
        PPI-DONet               & 0.0475$\pm$ 0.00047          & 0.0436 $\pm$ 0.00034 	 \\ \hline
        % $\phi$-FNN               & 0.0192$\pm$ 0.0013          & 0.0144 $\pm$ 0.0009  	& 0.0072  $\pm$ 0.0004	& 0.0070$\pm$ 0.00005  \\
        % $\phi$             & 0.0153$\pm$ 0.0009         & 0.0108 $\pm$ 0.0006	& 0.0059 $\pm$ 0.0002	& 0.0052 $\pm$ 0.0002  \\ \hline
    \end{tabular}
    \end{subtable}

    \begin{subtable}{\textwidth}
    \caption{\small \textit{Poisson}}
    \label{tab:poisson}
    \small 
    \centering
    \begin{tabular}{cccc}
        \hline
        \textit{Training size}      &  & {600}             & {1000}            \\ \hline
        FNO                  &  & 0.0045 $\pm$ 0.00006	& 0.0037 $\pm$ 0.00005     \\
        PPI-FNO          &  & {0.0046} $\pm$ {0.00005}          & {0.0040} $\pm$ {0.00006}      \\ \hline
        DONet             &  & 0.1786 $\pm$ 0.00398         & 0.1719 $\pm$ 0.00643      \\
        PPI-DONet             &  & 0.1409 $\pm$ 0.00292        & 0.1373 $\pm$ 0.00136     \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Advection}}
    \label{tab:advection}
        \small 
        \centering
    \begin{tabular}{cccc}
        \hline
        \textit{Training size}      &  & {600}             & {1000}         \\ \hline
        FNO                  &  & 0.0943 $\pm$ 0.00177	& 0.0768 $\pm$ 0.00182    \\
        PPI-FNO          &  & {0.0819} $\pm$ {0.00092}          & {0.0695} $\pm$ {0.00092}    \\ \hline
        DONet             &  & 0.0913 $\pm$ 0.00074         & 0.0748 $\pm$ 0.00098        \\
        PPI-DONet             &  & 0.0732 $\pm$ 0.00143       & 0.0626 $\pm$ 0.00107     \\ \hline
    \end{tabular}
    \end{subtable}
    
\end{table*}


\begin{table*}[h]
\caption{\small Relative $L_2$ error in Poisson and Advection operator learning benchmarks, where ``PPI'' is short for ``Pseudo-Physics Informed'' and ``PI'' is truly ``Physics Informed''. The results were averaged from five runs. } \label{tb:pred-error-PINO}
    \small
    \centering

    \begin{subtable}{\textwidth}
    \caption{\small \textit{Poisson}}
    \label{tab:poisson}
    \small 
    \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.2340 $\pm$ 0.0083	& 0.1390 $\pm$ 0.0007   & 0.0895 $\pm$ 0.0008   & 0.0698 $\pm$ 0.0014         \\ 
        PPI-FNO          &  & {0.1437} $\pm$ {0.0062}          & {0.0771} $\pm$ {0.0018}        & {0.0544} $\pm$ {0.0009} 		 & {0.0458} $\pm$ {0.0003}    \\ 
%        Error Reduction          &  & 38.59\% & 44.53\%        & 39.22\%		 & 34.38\%   \\ \hline
        PI-FNO             &  & \textbf{0.1433 $\pm$ 0.0104}          & \textbf{0.0718 $\pm$ 0.0015}      & \textbf{0.0504 $\pm$ 0.0009} 	& \textbf{0.0429 $\pm$ 0.0004}   \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Advection}}
    \label{tab:advection}
        \small 
        \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {20}             & {30}         & {50}     & {80}     \\ \hline
        FNO                  &  & 0.4872 $\pm$ 0.0097	& 0.4035 $\pm$ 0.0086   & 0.3019 $\pm$ 0.0085   & 0.2482 $\pm$ 0.0059        \\
        PPI-FNO          &  & {0.3693} $\pm$ {0.0099}          & {0.3224} $\pm$ {0.0123}        & {0.2236} $\pm$ {0.0075} 		 & {0.1698} $\pm$ {0.0075}    \\
%        Error Reduction          &  & 24.20\% & 20.10\%        & 25.94\%		 & 31.59\%   \\ \hline
        PI-FNO             &  & \textbf{0.3628 $\pm$ 0.0082}         & \textbf{0.3205 $\pm$ 0.0121}      & \textbf{0.2222 $\pm$ 0.0074} 	& \textbf{0.1668 $\pm$ 0.0057}    \\ \hline
    \end{tabular}
    \end{subtable}
    
    
\end{table*}



\begin{table}[h]
\caption{\small Relative $L_2$ error of using the learned back-box PDE network~\eqref{eq:pde-learn} to predict the input function $f$.} \label{tb:utof}
\small
\centering
\begin{subtable}{0.6\textwidth}
\caption{\small Training size=10}
   \small
    \centering
    \begin{tabular}{ccc}
    \hline \textit{Benchmark} & MLP & Ours \\
    \hline
    Darcy Flow & 0.1819$\pm$0.0026 & \textbf{0.1392$\pm$ 0.0080}\\
    Nonlinear Diffusion & 0.0660$\pm$0.0069 & \textbf{0.0233$\pm$0.0005}\\
    Eikonal & 0.0144$\pm$0.0009 & \textbf{0.0108 $\pm$ 0.0006}\\
    \hline
    \end{tabular}
\end{subtable}
\begin{subtable}{0.6\textwidth}
\caption{\small Training size=30}
   \small
    \centering
    \begin{tabular}{ccc}
    \hline \textit{Benchmark} & MLP & Ours \\
    \hline
    Darcy Flow & 0.1413$\pm$0.0013 & \textbf{0.0688$\pm$ 0.0032}\\
    Nonlinear Diffusion & 0.0463$\pm$0.0022 & \textbf{0.0163$\pm$0.0002}\\
    Eikonal & 0.0070$\pm$0.00005 & \textbf{0.0052 $\pm$ 0.0002}\\
    \hline
    \end{tabular}
\end{subtable}

\end{table}

\begin{table*}[h]
\caption{\small The relative $L_2$ error with using different architectures of $\phi$ in pseudo-physics-informed (PPI) learning on \textit{Darcy flow} benchmark.} \label{tb:phierror}
\small
\centering
\begin{subtable}{\textwidth}
\caption{\small Predicting $f$ via  $\phi$ with different architectures.}\label{tb:phi-pred-f}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    FNO & 0.7229$\pm$0.0318 & 0.5759$\pm$ 0.0126 & 0.4257$\pm$ 0.0106 & 0.3160$\pm$ 0.0037\\
    MLP & 0.7169$\pm$0.0160 & 0.6598$\pm$ 0.0056 & 0.6464$\pm$ 0.0029 & 0.6277$\pm$ 0.0032\\
    Ours & \textbf{0.2285 $\pm$ 0.0147} & \textbf{0.1392 $\pm$ 0.0080} & \textbf{0.0898 $\pm$ 0.0046} & \textbf{0.0688 $\pm$ 0.0032}\\
    \hline
    \end{tabular}
\end{subtable}
\begin{subtable}{\textwidth}
  \caption{\small Predicting $u$.}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    PPI-FNO with FNO as $\phi$ & 0.5853$\pm$0.0153 & 0.3871$\pm$ 0.0124 & 0.2613$\pm$ 0.0190 & 0.1629$\pm$ 0.0064\\
    PPI-FNO with MLP as $\phi$ & 0.7262$\pm$0.0920 & 0.5516$\pm$ 0.0699 & 0.4568$\pm$ 0.0857 & 0.3983$\pm$ 0.1051\\
    Standard FNO & 0.4915 $\pm$ 0.0210	& 0.3870 $\pm$ 0.0118   & 0.2783 $\pm$ 0.0212   & 0.1645 $\pm$ 0.0071\\
    Ours & \textbf{0.1716 $\pm$ 0.0048} & \textbf{0.0956 $\pm$ 0.0084} & \textbf{0.0680 $\pm$ 0.0031} & \textbf{0.0642 $\pm$ 0.0010}\\
    \hline
    \end{tabular}
\end{subtable}
%\end{table}
\end{table*}


\begin{table*}[h]
\caption{\small The relative $L_2$ error of PPI learning by incorporating different orders of derivatives. During the comparison with other operator learning methods, we used derivative orders up to 2 to run our method.   } \label{tb:ordererror}
\small
\centering
\begin{subtable}{\textwidth}
\caption{\small Predicting $f$ via $\phi$.}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    order 0 & 0.7126$\pm$0.0131 & 0.5733$\pm$0.0208 & 0.4812$\pm$0.0399 & 0.3445$\pm$0.0182 \\ 
    order $\le$ 1 & 0.2926$\pm$0.0118 & 0.2006$\pm$0.0047 & 0.1379$\pm$0.0051 & 0.1084$\pm$0.0053 \\ 
    order $\le 2$  & {0.2285$\pm$0.0147} & {0.1392$\pm$0.0080} & {0.0898$\pm$0.0046} & {0.0688$\pm$0.0032} \\ 
    order $\le 3$ & 
    \textbf{0.2058$\pm$0.0192} & \textbf{0.1123$\pm$0.0039} & \textbf{0.0712$\pm$0.0021} & \textbf{0.0585$\pm$0.0030} \\ 
    \hline
    \end{tabular}
\end{subtable}
\begin{subtable}{\textwidth}
\caption{\small Predicting $u$.}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    order 0 & 0.6352$\pm$0.0673 & 0.4523$\pm$0.0621 & 0.3570$\pm$0.0658 & 0.2737$\pm$0.0643 \\ 
    order $\le$ 1 & 0.3386$\pm$0.0259 & 0.2161$\pm$0.0083 & 0.1645$\pm$0.0114 & 0.1197$\pm$0.0132 \\ 
    order $\le 2$ & \textbf{0.1716$\pm$0.0048} & \textbf{0.0956$\pm$0.0084} & \textbf{0.0680$\pm$0.0031} & \textbf{0.0642$\pm$0.0010} \\ 
    order $\le 3$ & 0.2959$\pm$0.0381 & 0.1719$\pm$0.0213 & 0.1193$\pm$0.0158 & 0.0828$\pm$0.0054 \\ 
    \hline
    \end{tabular}
\end{subtable}
%\vspace{-0.2in}
\end{table*}

\begin{table*}[t]
\caption{\small Parameter counts for FNO and DONet with PPI variations across different problems. The training size is 30. } \label{tb:paracounts}
\small
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Parameter count} & \textbf{FNO} & \textbf{PPI-FNO (increase)} & \textbf{DONet} & \textbf{PPI-DONet (increase)} \\ 
\hline
Darcy-flow & 1,188,353 & 1,229,476 (+3.46\%) & 2,084,704 & 2,125,827 (+1.97\%) \\ 
Nonlinear-diffusion & 1,188,353 & 1,197,220 (+0.75\%) & 824,501 & 833,368 (+1.08\%) \\ 
Eikonal & 1,188,353 & 1,197,220 (+0.75\%) & 824,501 & 833,368 (+1.08\%) \\ 
Poisson & 1,188,353 & 1,197,220 (+0.75\%) & 824,501 & 833,368 (+1.08\%) \\ 
Advection & 1,188,353 & 1,197,220 (+0.75\%) & 210,101 & 218,968 (+4.22\%) \\ 
\hline
\end{tabular}
\end{table*}

\section{Pointwise Error}\label{sect:appendix:point-wise}
For a further assessment, we conducted a fine-grained evaluation by visualizing the predictions and point-wise errors made by each method. In Fig. \ref{fig:darcy-dont-example} and \ref{fig:nl-fno-example},  we showcased the predictions and point-wise errors using PPI-DONet for \textit{Darcy Flow}, PPI-FNO for \textit{nonlinear diffusion}, respectively. Additional examples of predictions and point-wise errors are provided in Fig.~\ref{fig:SIF-pred-example}, \ref{fig:ek-fno-example}, \ref{fig:ek-deeponet-example}, \ref{fig:darcy-fno-example}, and \ref{fig:nl-deeponet-example}. 
 
It is evident that without the assistance of the pseudo physics laws learned by our method, the ordinary DONet and FNO frequently missed crucial local structures, sometimes even learning entirely incorrect structures.
For example, In Fig. \ref{fig:darcy-dont-example} the first row, DONet missed one mode, while in the second and third row of Fig. \ref{fig:darcy-dont-example}, DONet failed to capture all the local modes. 
After incorporating the learned physics, DONet (now denoted as PPI-DONet; see the third column) successfully captures all the local modes, including their shapes and positions. Although not all the details are exactly recovered, the point-wise error is substantially reduced, particularly in those high error regions of the ordinary DONet; see the fourth column of Fig. \ref{fig:darcy-dont-example}.
In another instance, as shown in Fig. \ref{fig:nl-fno-example}, where the ordinary FNO (second column) captured the global shape of the solution, but the mis-specification of many local details led to large point-wise errors across many regions (fourth column). In contrast, PPI-FNO (third column) not only identified the structures within the solution but also successfully recovered the details. As a result, the point-wise error (fifth column) was close to zero everywhere.
Additional instances can be found in Fig. \ref{fig:ek-fno-example}, the first three rows illustrate that ordinary FNO (trained with 5, 10, and 20 examples, respectively) estimates an entirely incorrect structure of the solution, indicating that the training data is insufficient for FNO to capture even the basic structure of the solution. In contrast, after fine-tuning with our learned physics laws from the same sparse data, PPI-FNO accurately figured out the solution structures and yielded a substantial reduction in point-wise error across nearly everywhere. The point-wise error became uniformly close to zero.
With 30 examples, the ordinary FNO was then able to capture the global structure of the solution, but the details in the bottom left, bottom right, and top right corners were incorrectly predicted. In comparison, PPI-FNO further recovered these details accurately. 




Collectively, these results demonstrate that the pseudo physics extracted by our method not only substantially boosts the overall prediction accuracy but also better recovers the local structures and details of the solution.

\begin{figure}
    \centering
    \setlength\tabcolsep{0pt}
	\begin{tabular}[c]{ccc}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
\includegraphics[width=\textwidth]{figs-fno/crack_SIF/best_sample_1.pdf}
    \end{subfigure} & 
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
\includegraphics[width=\textwidth]{figs-fno/crack_SIF/best_sample_2.pdf}
    \end{subfigure} &
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
\includegraphics[width=\textwidth]{figs-fno/crack_SIF/best_sample_3.pdf}
    \end{subfigure} 
    \end{tabular}
    \caption{\small Examples of SIF prediction of FNO and PPI-FNO trained with 600 examples.}
    \label{fig:SIF-pred-example}
\end{figure}



\begin{figure}
    \centering
    \setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cccc}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/Eikona_iteration_learning_curve_fno.pdf}
        \caption{\small PPI-FNO: learning}\label{fig:fno-learning-eikonal}
    \end{subfigure} & 
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/Eikona_iteration_learning_curve_deeponet.pdf}
        \caption{\small PPI-DONet: learning}\label{fig:donet-learning-eikonal}
    \end{subfigure} &
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{error-vs-lambda/Eikona_data_fno_error_vs_lam.pdf}
        \caption{\small PPI-FNO: $\lambda$}\label{fig:lambda-fno-eikonal}
    \end{subfigure} &
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{error-vs-lambda/Eikona_data_deeponet_error_vs_lam.pdf}
        \caption{\small PPI-DONet: $\lambda$} \label{fig:lam-donet-eikonal}
    \end{subfigure}
    \end{tabular}
    \caption{\small Learning curve of PPI-FNO (a) and PPI-DONet (b) on \textit{Eikonal} with 30 training examples. Shown in (c) and (d) is how the weight $\lambda$ of ``pseudo physics'' affects the operator learning performance. The horizontal lines in (c) and (d) are the relative $L_2$ error of standard FNO and DONet, respectively. }
    \label{fig:learning-curve-and-lambda-study-eikonal}
\end{figure}

%PPI-FNO vs DONet, Ekonal
\begin{figure*}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cc}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_5.pdf}
	\end{subfigure} &
    \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_5.pdf}
	\end{subfigure}
 \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_10.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_10.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_20.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_20.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_30.pdf}
 \caption{\small \textit{PPI-DONet: Darcy Flow}}\label{fig:darcy-dont-example}
	\end{subfigure} & 
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_30.pdf}
 \caption{\small \textit{PPI-FNO: nonlinear diffusion}}\label{fig:nl-fno-example}
	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error of PPI-DONet and PPI-FNO on \textit{Darcy Flow} and \textit{nonlinear diffusion}, respectively.  From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
 %\vspace{-0.2in}
\end{figure*}

\begin{figure}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cc}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/ek/final_best_five_ek_5.pdf}
	\end{subfigure} &
    \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{
 ./figs-deeponet/ek/final_best_deep_five_ek_5.pdf}
	\end{subfigure}
 \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-fno/ek/final_best_five_ek_10.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_10.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs-fno/ek/final_best_five_ek_20.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{figs-deeponet/ek/final_best_deep_five_ek_20.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{figs-fno/ek/final_best_five_ek_30.pdf}
 \caption{\small \textit{PPI-FNO: Eikonal}}\label{fig:ek-fno-example}
	\end{subfigure} & 
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
\includegraphics[width=\textwidth]{figs-deeponet/ek/final_best_deep_five_ek_30.pdf}
 \caption{\small \textit{PPI-DONet: Eikonal}}\label{fig:ek-deeponet-example}
	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error of PPI-FNO and PPI-DONet on \textit{Eikonal}.  From top to bottom, the models were trained with 5, 10, 20, 30 examples.} 
\end{figure}


%PPI-FNO vs DONet, Darcy vs ND
\begin{figure}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cc}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{figs-fno/darcy/3final_best_five_darcy_5.pdf}
	\end{subfigure} &
    \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{
 figs-deeponet/nl/final_best_deep_five_nl_5.pdf}
	\end{subfigure}
 \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs-fno/darcy/final_best_five_darcy_10.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{figs-deeponet/nl/final_best_deep_five_nl_10.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figs-fno/darcy/final_best_five_darcy_20.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{figs-deeponet/nl/final_best_deep_five_nl_20.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{figs-fno/darcy/final_best_five_darcy_30.pdf}
 \caption{\small \textit{PPI-FNO: Darcy Flow}}\label{fig:darcy-fno-example}
	\end{subfigure} & 
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
\includegraphics[width=\textwidth]{figs-deeponet/nl/final_best_deep_five_nl_30.pdf}
 \caption{\small \textit{PPI-DONet: Nonlinear Diffusion}}\label{fig:nl-deeponet-example}
	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error of PPI-FNO and PPI-DONet on \textit{Darcy Flow} and \textit{Nonlinear diffusion}, respectively.  From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
 %\label{fig:darcy-dont-example}
 
\end{figure}
\section{Limitation and Discussion}\label{sect:limit}
Our current method cannot learn PDE representations for which the input function $f$ is the \textit{initial condition}. In such cases, the mapping from the solution function to the initial condition requires a reversed integration over time, hence we cannot decouple the derivatives. To address this problem, we plan to explicitly model the temporal dependencies in the PDE representation, such as via the neural ODE design~\citep{chen2018neural}. 

  


% \begin{figure}
% 	\centering
% 	\setlength\tabcolsep{0pt}
% 	\begin{tabular}[c]{c}
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering		\includegraphics[width=\textwidth]{./figs-fno/ek/final_best_five_ek_5.pdf}
%        % \caption{\small Training size = 5}
% 	\end{subfigure} \\
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-fno/ek/final_best_five_ek_10.pdf}
%      %\caption{\small Training size = 10}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-fno/ek/final_best_five_ek_20.pdf}
%      %\caption{\small Training size = 10}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering\includegraphics[width=\textwidth]{./figs-fno/ek/final_best_five_ek_30.pdf}
%      %\caption{\small Training size = 10}
% 	\end{subfigure}
% \end{tabular}
% 	\caption{\small Examples of the prediction and point-wise error on \textit{Eikonal equation} with FNO.  From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
%  \label{fig:ek-fno-example}
 
% \end{figure}
% \begin{figure}[htbp]
% 	\centering
% 	\setlength\tabcolsep{0pt}
% 	\begin{tabular}[c]{c}
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-fno/darcy/3final_best_five_darcy_5.pdf}
% 	\end{subfigure}\\
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-fno/darcy/final_best_five_darcy_10.pdf}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-fno/darcy/final_best_five_darcy_20.pdf}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 	\centering
%     \includegraphics[width=\textwidth]{./figs-fno/darcy/final_best_five_darcy_30.pdf}
% 	\end{subfigure}
% \end{tabular}
% 	\caption{\small Examples of the prediction and point-wise error on \textit{Darcy flow} with FNO. From top to bottom, the models were trained with 5, 10, 20, 30 examples. ``PPI'' is short for ``Pseudo Physics Informed''.}
%  \label{fig:darcy-fno-example}
% \end{figure}

% \begin{figure}
% 	\centering
% 	\setlength\tabcolsep{0pt}
% 	\begin{tabular}[c]{c}
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_5.pdf}
% 	\end{subfigure} \\
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_10.pdf}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_20.pdf}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_30.pdf}
% 	\end{subfigure}
% \end{tabular}
% 	\caption{\small Examples of the prediction and point-wise error on \textit{nonlinear diffusion} with DeepONet (DONet). From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
%  \label{fig:nl-deeponet-example}
% \end{figure}

% \begin{figure}
% 	\centering
% 	\setlength\tabcolsep{0pt}
% 	\begin{tabular}[c]{c}
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_5.pdf}
% 	\end{subfigure} \\
% 	\begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_10.pdf}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_20.pdf}
% 	\end{subfigure}\\
%  \begin{subfigure}[b]{0.48\textwidth}
% 		\centering
% 		\includegraphics[width=\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_30.pdf}
% 	\end{subfigure}
% \end{tabular}
% 	\caption{\small Examples of the prediction and point-wise error on \textit{Eikonal equation} with DeepONet (DONet).  From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
%  \label{fig:ek-deeponet-example}
% \end{figure}





% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/final_best_five_darcy_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=10.\todo{zsd-fno-darcy-10}} \label{fig:best_darcy_fno_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/final_best_five_darcy_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=20. \todo{zsd}} \label{fig:best_darcy_fno_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/final_best_five_darcy_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=30.\todo{zsd}} \label{fig:best_darcy_fno_30}
% \end{figure*}


% %
% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/3final_best_five_darcy_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small 12Predicition and Point-wise error on  \textit{Darcy Flow} with training size=5.\todo{zsd2}} \label{fig:3best_darcy_fno_5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/3final_best_five_darcy_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small 12Predicition and Point-wise error on  \textit{Darcy Flow} with training size=10.\todo{figure texts not visible}} \label{fig:3best_darcy_fno_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/3final_best_five_darcy_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small 12Predicition and Point-wise error on  \textit{Darcy Flow} with training size=20.} \label{fig:3best_darcy_fno_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/3final_best_five_darcy_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small 12Predicition and Point-wise error on  \textit{Darcy Flow} with training size=30.} \label{fig:3best_darcy_fno_30}
% \end{figure*}



% %%darcy
% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/2final_best_five_darcy_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=5.\todo{figure texts not visible}} \label{fig:1best_darcy_fno_5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/2final_best_five_darcy_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=10.\todo{figure texts not visible}} \label{fig:1best_darcy_fno_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/2final_best_five_darcy_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=20.} \label{fig:1best_darcy_fno_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/darcy/2final_best_five_darcy_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=30.} \label{fig:1best_darcy_fno_30}
% \end{figure*}


% nonlinear
% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/nl/final_best_five_nl_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=5.} \label{fig:best_nl_fno_5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/nl/final_best_five_nl_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=10.} \label{fig:best_nl_fno_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/nl/final_best_five_nl_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=20.} \label{fig:best_nl_fno_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/nl/final_best_five_nl_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=30.} \label{fig:best_nl_fno_30}
% \end{figure*}


% % ek
% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/ek/final_best_five_ek_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=5.} \label{fig:best_ek_fno_5}
% \end{figure*}



% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/ek/final_best_five_ek_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=10.} \label{fig:best_ek_fno_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/ek/final_best_five_ek_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=20.} \label{fig:best_ek_fno_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-fno/ek/final_best_five_ek_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=30.} \label{fig:best_ek_fno_30}
% \end{figure*}




% \begin{table*}[]
%     \small
%     \centering
%     \begin{tabular}{cccccc}
%         \hline
%         {Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
%         FNO                  &  & 0.2004 $\pm$ 0.0083	& 0.1242 $\pm$ 0.0046   & 0.0876 $\pm$ 0.0061   & 0.0551 $\pm$ 0.0021         \\
%         PPI-FNO          &  & 0.0105 $\pm$ 0.0016         & 0.0066 $\pm$ 0.00023        & 0.0049 $\pm$ 0.00037 		 & 0.0038 $\pm$ 0.00039    \\
%         Error Reduction $\%$ by PPI-FNO          &  & 94.76         & 94.68        & 94.40		 & 93.10  \\
%         DeepONet             &  & 0.8678 $\pm$ 0.0089          & 0.6854 $\pm$ 0.0363      & 0.5841 $\pm$ 0.0279 	& 0.5672 $\pm$ 0.0172    \\
%         DeepONet             &  & 0.3010 $\pm$ 0.0119          & 0.2505 $\pm$ 0.0057      & 0.1726 $\pm$ 0.0076	& 0.1430 $\pm$ 0.0036    \\
%         PPI-DeepONet             &  & 0.1478$\pm$ 0.0126          & 0.1161 $\pm$ 0.0124 	& 0.1032 $\pm$ 0.0059	& 0.0842 $\pm$ 0.0041  \\ 
%         Error Reduction $\%$ by PPI-DeepONet &  & 50.89         & 53.65    	& 40.20	& 41.11  \\ \hline
%         FNN             &  & 0.0826$\pm$ 0.0070          & 0.0660 $\pm$ 0.0069  	& 0.0586  $\pm$ 0.0020	& 0.0463 $\pm$ 0.0022  \\
%         FNN+conv            &  & 0.0303$\pm$ 0.0006         & 0.0233 $\pm$ 0.0005	& 0.0190 $\pm$ 0.0001	& 0.0163 $\pm$ 0.0002  \\ \hline
%     \end{tabular}
%     \caption{\small Relative $L_2$ error for Non-linear diffusion PDE}
%     \label{tab:nl}
% \end{table*}



% \begin{table*}[]
%     \small
%     \footnotesize
%     \centering
%     \begin{tabular}{ccccc}
%         \hline
%         {Training size}        & {5}             & {10}         & {20}     & {30}     \\ \hline
%         FNO                    & 0.2102 $\pm$ 0.0133	& 0.1562 $\pm$ 0.0098   & 0.0981 $\pm$ 0.0022   & 0.0843 $\pm$ 0.0020        \\
%         PPI-FNO            & 0.0678 $\pm$ 0.0026         & 0.0582 $\pm$ 0.0043        & 0.0493 $\pm$ 0.0023 		 & 0.0459 $\pm$ 0.0010    \\
%         Error Reduction $\%$ by PPI-FNO            & 67.74         & 62.74        & 49.74		 & 45.55   \\
%         DeepONet               & 0.3374 $\pm$ 0.0944          & 0.1759 $\pm$ 0.0065      & 0.1191 $\pm$ 0.0047 	& 0.1096 $\pm$ 0.0037    \\
%         PPI-DeepONet               & 0.1302$\pm$ 0.0127          & 0.0907 $\pm$ 0.0093 	& 0.0714 $\pm$ 0.0011	& 0.0700 $\pm$ 0.0007  \\ 
%         Error Reduction $\%$ by PPI-DeepONet   & 61.41         & 48.43    	& 40.05	& 36.13  \\ \hline
%         FNN               & 0.0192$\pm$ 0.0013          & 0.0144 $\pm$ 0.0009  	& 0.0072  $\pm$ 0.0004	& 0.00699$\pm$ 0.00005  \\
%         FNN+conv             & 0.0153$\pm$ 0.0009         & 0.0108 $\pm$ 0.0006	& 0.0059 $\pm$ 0.0002	& 0.0052 $\pm$ 0.0002  \\ \hline
%     \end{tabular}
%     \caption{\small Relative $L_2$ error for Eikonal equation}
%     \label{tab:ek}
% \end{table*}

% \section{Figures for PPI-DeepONet}

%  Here we show the prediction of  PPI-DeepONet and compare it with ground truth and baseline DeepONet on three datasets when training size is 5, 10, 20, 30. 
%  It can be seen that our method PPI-DeepONet outperforms DeepONet which indicates the function of physics-informed learning in improving operator learning.
% \begin{figure*}[]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=5.} \label{fig:pre_darcy_d_5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit {Darcy Flow} with training size=10.} \label{fig:pre_darcy_d_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=20.} \label{fig:pre_darcy_d_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Darcy Flow} with training size=30.} \label{fig:pre_darcy_d_30}
% \end{figure*}


% % nl

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=5.} \label{fig:pre_nl_d_5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit {Non-linear diffusion PDE} with training size=10.} \label{fig:pre_nl_d_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=20.} \label{fig:pre_nl_d_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/nl/final_best_deep_five_nl_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Non-linear diffusion PDE} with training size=30.} \label{fig:pre_nl_d_30}
% \end{figure*}

% % %ek
% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=5.} \label{fig:pre_ek_d_5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=10.} \label{fig:pre_ek_d_10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=20.} \label{fig:pre_ek_d_20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./figs-deeponet/ek/final_best_deep_five_ek_30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Predicition and Point-wise error on  \textit{Eikonal Equation} with training size=30.} \label{fig:pre_ek_d_30}
% \end{figure*}


% \newpage\clearpage

% \section{Trend of Relative $L_2$ Error for Different PDE data across Various Models}

% Furthermore, we plot the error trend with data size in three datasets. 
%  In Figure \ref{fig:trend_darcy} for trend of relative error for darcy flow, all models show a trend of decreasing error with increasing training size, which suggests that additional data helps the models to generalize better.
%  The PPI-FNO model has the lowest relative $L_2$ error across all training sizes, indicating it is the best performing model among those listed.
%  The FNN+conv model also shows good performance, particularly at larger training sizes, which highlights the utility of convolutional layers in the network.
%  The DeepONet model has the highest error rates, suggesting it might be less suitable for this specific problem compared to the others.
%  Our method PPI-FNO, PPI-DeepONet can improve the performance significantly for both FNO and DeepONet models.
%  The FNO-based models (FNO and PPI-FNO) generally outperform the DeepONet-based models (DeepONet and PPI-DeepONet).
	 
%  In Figure \ref{fig:trend_nl} for relative error for non-linear diffusion PDE data, as the amount of training data grows, all models exhibit a reduction in the relative $L_2$ error, aligning with the common expectation that models tend to improve their performance when provided with larger datasets.
%  The FNN+conv model appears to have the best performance with the lowest $L_2$ error at all training sizes, followed closely by the PPI-FNO model.
%  The DeepONet model registers the greatest error among the models, suggesting it is not as well-adapted to this particular task as its counterparts.
%  The FNO and PPI-DeepONet models show moderate performance, with PPI-DeepONet having lower errors than FNO, which suggests that the pseodu physics-informed learning helps in improving the performance of the base model.
%  The standard FNN model, while not as effective as FNN+conv, still shows a steady decrease in error as more training data is provided.
%  The addition of convolutional layers in FNN+conv and pseudo physics informed learning seems to provide significant benefits in modeling non-linear diffusion PDE data. The graph indicates that FNN+conv and PPI-FNO are the most accurate models for this task, with remarkably low relative $L_2$ errors even at smaller training sizes.


%  In Figure \ref{fig:trend_ek} for relative error for Eikonal equation dataset, all models demonstrate a decreasing trend in error with the expansion of training data size, a common occurrence in machine learning where enhanced performance is usually observed as models are exposed to a larger volume of training data.
%  The PPI-FNO model consistently shows the lowest relative $L_2$ error across all training sizes, suggesting it is the most accurate model for this dataset.
%  The DeepONet model has the highest relative $L_2$ error, indicating that it might be less effective at capturing the complexities of the Eikonal equation data compared to the other models.
%  The physics-informed learning has a positive impact on the performance of both the FNO and DeepONet models.
%  The FNN+conv model, which includes convolutional layers, demonstrates better performance than the standard FNN, emphasizing the benefit of convolutional features in this context.
%  For the Eikonal equation dataset, machine learning models with specialized structures (like PPI-FNO and FNN+conv) tend to outperform more traditional architectures. The PPI-FNO model, in particular, stands out for its low error rates, indicating that the physics learning step greatly enhances its ability to solve this type of PDE. 


% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./trend_darcy.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Trend of relative error for darcy flow} \label{fig:trend_darcy}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./trend_nl.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Trend of relative error for non-linear diffusion PDE data} \label{fig:trend_nl}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./trend_ek.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Trend of relative error for eikonal equation data} \label{fig:trend_ek}
% \end{figure*}

% \newpage\clearpage
% \section{Relative $L_2$ Error for Different Training Sizes across Different PDE Data for Various Models}


% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./train5.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Relative $L_2$ Error for Training Size 5} \label{fig:5}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./train10.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Relative $L_2$ Error for Training Size 10} \label{fig:10}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./train20.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Relative $L_2$ Error for Training Size 20} \label{fig:20}
% \end{figure*}

% \begin{figure*}[t]
%     \centering
%     \setlength\tabcolsep{0pt}
%     \includegraphics[width=0.6\textwidth]{./train30.pdf}
%     %\vspace{-0.1in}
%     \caption{\small Relative $L_2$ Error for Training Size 30} \label{fig:30}
% \end{figure*}


%  In Figure \ref{fig:5}, when training size is 5, for darcy flow, PPI-FNO has the lowest error, indicating it is the most accurate model for this dataset. DeepONet has the highest error.
%  For Non-linear Diffusion PDE Data, PPI-FNO again shows the lowest error, consistent with its performance on the Darcy Flow.
%  DeepONet has a relatively higher error compared to PPI-FNO but is not the highest.
%  For Eikonal Equation, FNN+conv shows the lowest error, indicating that convolutional layers are highly effective for this type of data. DeepONet, while not the least accurate, still has a relatively high error. Overall, PPI-FNO demonstrates strong performance across all three datasets, which suggests that the physics-informed learning  significantly enhances the model's predictive capabilities.
%  FNN+conv generally performs well, especially on the Eikonal Equation data, which might be due to the nature of the data and how convolutional layers can capture spatial hierarchies.
%  DeepONet tends to have higher errors across the board compared to PPI-enhanced models.
%  The standard deviations are small relative to the mean error values, indicating the models' performance is consistent.



%  In Figure \ref{fig:10}, when training size is 10, for Darcy Flow, the PPI-FNO model has the lowest error, indicating the highest accuracy on this dataset.
%  DeepONet has the highest error, suggesting it might be the least suited for this type of data among the models considered.
%  For Non-linear Diffusion PDE Data, PPI-FNO again has the lowest error, significantly outperforming the other models.
%  FNO and DeepONet show higher errors, with DeepONet having the largest error again.
%  For Eikonal Equation, FNN+conv has the lowest error, which suggests that convolutional layers are particularly beneficial for this type of problem.
%  DeepONet, while better than in other datasets, still has a relatively high error compared to PPI-FNO and FNN+conv.
%  PPI-FNO is consistently the best-performing model for Darcy Flow and Non-linear Diffusion PDE Data, which may be attributed to the effectiveness of the physics-informed learning that it incorporates.
%  For the Eikonal Equation, the FNN+conv model performs the best, suggesting that the addition of convolutional layers aids in capturing the complexities of this dataset.
%  The FNO model without PPI seems to struggle relative to its PPI-enhanced counterpart, demonstrating the value of physics-informed learning.
%  DeepONet generally shows larger errors across all datasets, which could imply that for these specific data types or problems, this architecture is less efficient.
%  The models' standard deviations are relatively small compared to the mean errors, suggesting consistent performance across different runs or subsets of the data.

%  In Figure Figure \ref{fig:20}, when training size is 20, for Darcy Flow, PPI-FNO model significantly outperforms the other models, which indicates that the physics-informed learning step it incorporates is very effective for this type of PDE data. 
%  DeepONet has the highest error, suggesting that it may require more training data, or this particular PDE structure might not be well-captured by the model.
%  For Non-linear Diffusion PDE Data, 
%  PPI-FNO model again excels, with an exceptionally low relative $L_2$ error. 
%  FNN+conv model also performs well, suggesting that convolutional layers are advantageous for capturing the features of non-linear diffusion phenomena.
%  For Eikonal Equation, FNN+conv model performs best, followed closely by the PPI-FNO model. The superior performance of FNN+conv might be due to its ability to leverage convolutional operations to effectively handle the spatial dependencies present in the Eikonal equation data. 
%  FNN model also shows very low error for this type of data, indicating that even without convolutional layers, a fully connected architecture is quite capable of handling the mathematical characteristics of the Eikonal equation.
%  In all, Darcy Flow is relatively more difficult for the models compared to the other two types of PDE data, possibly due to the complex flow patterns in porous media that the Darcy equation models.
%  Non-linear Diffusion PDE data is well-handled by models with physics learning and convolutional layers, suggesting that these data characteristics align well with the architectures that can capture local dependencies and complex patterns.
%  Eikonal Equation data seems to be the least challenging for the models, as indicated by the lower errors across the board. This might be due to the geometric nature of the Eikonal equation, which describes wavefronts and is possibly easier for neural networks to approximate.

%  In Figure \ref{fig:30}, when training size is 30, for Darcy Flow, PPI-FNO demonstrates a significantly lower error rate compared to the other models, indicating that physics learning greatly enhances its performance for this dataset. 
%  DeepONet has the highest error, which may suggest that this model is less adept at capturing the flow dynamics modeled by the Darcy equation or requires more specialized tuning. For Non-linear Diffusion PDE Data, PPI-FNO again stands out with the lowest error, suggesting physics learning is particularly effective for this kind of diffusion problem.
%  FNN+conv also shows a very good performance, indicating that convolutional layers are capturing the spatial patterns effectively for this dataset.
%  For Eikonal Equation, FNN+conv has the lowest error, followed closely by PPI-FNO, which suggests that both convolutional layers and physics learning are beneficial for modeling the wave propagation phenomena described by the Eikonal equation.
%  DeepONet performs better on this dataset than on Darcy Flow or Non-linear Diffusion but still does not match the performance of the PPI-enhanced models.
%  Overall, PPI-FNO model is highly effective across all three datasets, demonstrating the power of physics learning in enhancing model performance for PDEs.
%  FNN+conv model excels in datasets where spatial relationships are key, such as in Non-linear Diffusion and Eikonal Equation data.
%  Darcy Flow appears to be more challenging for models without physics information, as reflected by higher relative errors.
%  Non-linear Diffusion PDE Data is well captured by the PPI-FNO and FNN+conv models, indicating the importance of both physics and convolutional features for these types of problems.
%  Eikonal Equation data is handled well by the convolutional network, which could be due to its ability to represent high-frequency components and sharp transitions, characteristics often present in wavefront-like solutions.
\cmt{
\begin{figure}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{c}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_5.pdf}
       % \caption{\small Training size = 5}
	\end{subfigure} \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_10.pdf}
     %\caption{\small Training size = 10}
	\end{subfigure}\\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_20.pdf}
     %\caption{\small Training size = 10}
	\end{subfigure}\\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_30.pdf}

	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error on \textit{Darcy flow} with DeepONet(DONet). From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
 \label{fig:darcy-dont-example}
 %\vspace{-0.5in}
\end{figure}
\begin{figure}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{c}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_5.pdf}
       % \caption{\small Training size = 5}
	\end{subfigure} \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_10.pdf}
     %\caption{\small Training size = 10}
	\end{subfigure}\\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_20.pdf}
     %\caption{\small Training size = 10}
	\end{subfigure}\\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_30.pdf}
     %\caption{\small Training size = 10}
	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error on \textit{nonlinear diffusion} with FNO. From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
 \label{fig:nl-fno-example}
 %\vspace{-0.5in}
\end{figure}
}

\cmt{
\begin{figure}
    \centering
    \setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cccc}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/darcy_training_test_loss_trend.pdf}
        \caption{\small FNO with Darcy Flow: learning}\label{fig:fno-learning-eikonal}
    \end{subfigure} & 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/poisson_fno_test_loss_trend.pdf}
        \caption{\small FNO with Possion: learning}\label{fig:donet-learning-eikonal}
    \end{subfigure} 
    \end{tabular}
    \caption{\small Learning curve of FNO (a) on \textit{Darcy Flow} and (b) on \textit{Possion} with 30 training examples.}
    \label{fig:learning-curve-base-FNO}
\end{figure}
}

\cmt{
\begin{table*}[t]
\caption{\small Running time for FNO and PPI-FNO on Darcy and Possion problems. The training sizes are 20 and 30. } \label{tb:paracounts}
\small
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Running Time(s)} & \textbf{FNO} & \textbf{PPI-FNO}  \\ 
\hline
Darcy-flow(size 20) & 14.24 & 2465.26  \\ 
Darcy-flow(size 30) & 17.22 & 2466.47  \\ 
Poisson(size 20) & 13.52 & 2616.89 \\ 
Poisson(size 30) & 16.41 & 3469.19  \\ 
\hline
\end{tabular}
\end{table*}
}