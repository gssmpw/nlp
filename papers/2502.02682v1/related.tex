\section{Related Work}
Operator learning is a rapidly evolving research field, with a variety of methods categorized as neural operators. Alongside FNO, several notable approaches have been introduced, such as~low-rank neural operator (LNO)~\citep{li2020fourier}, multipole graph neural operator (MGNO)~\citep{li2020multipole},  multiwavelet-based NO~\citep{gupta2021multiwavelet}, and convolutional NOs (CNO)~\citep{raonic2024convolutional}. Deep Operator Net (DON)~\citep{lu2021learning} is another popular  approach, consisting  of a branch network applied to input function values and a trunk network applied to sampling locations. The final prediction is obtained through the dot product of the outputs from the two networks. To enhance stability and efficiency, \citet{lu2022comprehensive} proposed replacing the trunk network with POD (PCA) bases. Recently, transformer architectures have been employed to design neural operators, offering a promising direction for operator learning, \eg~\citep{cao2021choose,li2022transformer,hao2023gnot}.


%Neural operator learning is expanding rapidly. In addition to FNO~\citep{li2020deep} and DONet~\citep{lu2021learning}, 
%notable works include the Low-rank Neural Operator (LNO) introduced by  ~\citet{li2020fourier}, employing low-rank structures to approximate the integration. The Graph Neural Operator (GNO)~\citep{li2020neural} integrates Nystrom approximation with graph neural networks, while the Multipole Graph Neural Operator (MGNO) by the same authors ~\citep{li2020multipole} leverages multiscale kernel decomposition. \citet{gupta2021multiwavelet} contributed with multiwavelet transformations for the operator's kernel. \citet{lu2022comprehensive} proposed POD-DONet to enhance the stability of DONet by replacing the trunk net with POD bases constructed from data. Another DONet variant by~\citet{seidman2022nomad} used an FFN to combine the outputs of the branch net and trunk net for prediction. A line of efforts attempted to build neural operators via transformer architectures, such as~\citep{cao2021choose,hao2023gnot} 
%Recently, \citet{kovachki2023neural} provided a comprehensive review of neural operators. There are also recent advances in kernel operator learning strategies made by~\citet{long2022kernel} and~\citet{batlle2023kernel}.

Physics-Informed Neural Networks (PINNs) \citep{raissi2019physics} 
 mark a significant advancement in scientific machine learning. PINNs integrate physical laws directly into the learning process, making them effective for solving differential equations and understanding complex physical systems. This methodology is particularly beneficial in scenarios where data is limited or expensive to obtain.
 Pioneering the concept of PINO, \citet{li2021physics} introduced a dual-resolution approach that combines low-resolution empirical data with high-resolution PDE constraints. This method achieves precise emulation of solution operators across various PDE classes. In parallel, physics-informed DONet by~\citet{wang2021learning} incorporate regularization strategies enforcing physical law adherence into the training of DONets.~\citet{zanardi2023adaptive} presented an approach using PINO for simulations in non-equilibrium reacting flows.  ~\citet{lee2023oppinn} proposed opPINN, a framework combining physics-informed neural networks with operator learning for solving the Fokker-Planck-Landau (FPL) equation. ~\citet{rosofsky2023applications} provided a review of applications of physics-informed neural operators. %\todo{@Wei, please verify if what i said is correct}
% Wu et al.~\citep{wu2022autopinn} introduce AutoPINN, combining automated machine learning (AutoML) with PINNs, focusing on efficient design and optimization of PINNs under resource constraints. This work demonstrates automated design strategies for PINNs in specific applications like Power Electronic Converters.
% Barber et al.~\citep{barber2021physical} explore embedding physical constraints into neural networks for noise regulation and inference, particularly in small and noisy datasets. This study contributes to understanding how physical constraints can improve the performance and noise resilience of neural networks.
% Djeumou et al.~\citep{djeumou2022neural} develop a framework for learning dynamics models from trajectory data, incorporating physics-based side information into the neural network structure. Their approach demonstrates how physics-based knowledge can enhance the accuracy and efficiency of neural network models in dynamical systems.
% Misyris et al.~\citep{misyris2020physics} introduce a framework for applying PINNs in power system applications. This paper showcases the potential of PINNs in determining dynamic states and uncertain parameters in power systems, significantly faster than conventional methods.
% Antonelo et al.~\citep{antonelo2021physics} propose a novel PINN-based architecture, Physics-Informed Neural Nets-based Control (PINC), for control problems in dynamic systems. Their methodology enables the integration of expert knowledge and data in control applications.
% Meng et al.~\citep{meng2020ppinn}  develop the Parareal Physics-Informed Neural Network (PPINN) for efficient long-time integration of time-dependent PDEs. This approach addresses the challenges of training PINNs on large time-space domains, demonstrating significant speed-ups.
% Peng et al.~\citep{peng2020accelerating}  propose Prior Dictionary based Physics-Informed Neural Networks (PD-PINNs) for enhanced convergence in training. Their work provides insights into how task-dependent dictionaries can improve the representation power and training efficiency of PINNs.
However, existing methods demand one should know the physics laws beforehand, which might not be feasible in many practical applications or complex systems. Our method offers a simple and effective framework, enabling the extraction of implicit physics laws directly from data, even when the data is scarce. Empirically, these pseudo physics laws have proven to be highly beneficial in enhancing the performance of operator learning, as demonstrated in Section \ref{sect:expr}. Many methods have been developed specifically for discovering differential equations from data, including SINDy~\citep{brunton2016discovering}  and its variants~\citep{schaeffer2017learning,zhang2020data,lagergren2020learning}, PINN-SR~\citep{chen2021physics}, Bayesian spline learning~\citep{sun2022bayesian}, and kernel-based equation discovery~\citep{long2024equation}. To ensure interpretability, these approaches typically assume a specific equation form and perform sparse selection from a set of candidate operators. In contrast to these methods, which prioritize interpretability, our approach focuses on enhancing the prediction accuracy of operator learning under limited data. To this end, we utilize a black-box neural network to represent PDEs. While this offers greater flexibility, it comes at the cost of reduced interpretability. Our method employs an alternating update strategy to jointly refine the PDE representation and improve operator learning.


Our work is also related to the cycle consistence framework~\citep{zhu2017unpaired} for image-to-image translation. A critical difference is that cycle-consistence performs \textit{unpaired} image-to-image translation, while our method aims for accurate paired translation (mapping). In cycle-consistence, the translation is viewed successfully as long as the translated images follow the target distribution. Hence, cycle-consistence has a much more relaxed objective. Another key difference is that our method aims to improve the learning of a function-to-function mapping with very limited data--- that motivates us to learn a ``pseudo'' physics representation. The cycle-consistence framework relies on adversarial training which typically requires a large amount of data to obtain successful learning outcomes.


 

%However, all the existing methods demand one should know the physics law beforehand, which might not be feasible in many practical applications or complex systems. Our method provides a simple and effective framework such that one can extract implicit physics laws outright from data (even when the data is sparse). These pseudo physics laws are shown empirically very helpful to improve the operator learning perofrmance (see Section \ref{sect:expr}).

