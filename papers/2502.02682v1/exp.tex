
\section{Experiments}\label{sect:expr}
We tested on five commonly used benchmark operator learning problems in literature~\citep{li2020fourier,lu2022comprehensive}, including \textit{Darcy Flow}, \textit{Nonlinear Diffusion}, \textit{Eikonal}, \textit{Poisson} and \textit{Advection}.  {In addition, we examined our method in an application of fatigue modeling. The task is to predict the stress intensity factor (SIF) for semi-elliptical surface cracks  on plates, given three geometric parameters that characterize the cracks~\citep{merrell2024stress}; see Appendix Fig.~\ref{fig:SIF_shape}. The SIF plays a critical role in modeling crack growth by quantifying the stress state near the tip of a crack, and hence SIF computation and analysis is extremely important in fatigue modeling and fracture mechanics~\citep{anderson2005fracture}. The SIF computation is expensive, because it typically needs to run finite element method (FEM) or extended FEM with very fine meshes~\citep{kuna2013finite}. 
Due to the complex sequence of computational steps involved in SIF calculation, there is no holistic PDE that directly models the relationship between the geometric features and the SIF function. Instead, SIF computation typically relies on numerical methods and the extraction of local stress fields near the crack tip.} The details about all the datasets are given in Section~\ref{sect:detail} of the Appendix. 

We evaluated our method based on two widely used NO models, FNO and DONet. Note that our method is straightforward to implement on other NO models, such as attention based models.  
%  For learning the pseudo physics laws via the neural network $\phi$ --- see \eqref{eq:pde-learn} --- we tuned the kernel size from \{(3, 3), (5, 5), (7, 7), (9, 9)\}. The stride was set to 1 and padding was set to ``same'' to ensure the output shape does not change. In the subsequent fully connected layers, we chose the number of layers from \{3, 4, 5, 6\}, and the layer width from \{16, 32, 64\}. We used {GeLU} activation. For the cases of Darcy Flow, Eikonal and Poisson, we used the following derivatives $\{\partial_{x_1} u, \partial_{x_2} u, \partial_{x_1x_1}u, \partial_{x_2x_2}u\}$, and for the other cases, we used $\{\partial_x u, \partial_{xx} u, \partial_t u, \partial_{tt} u\}$. Since SIF is a 1d function (the input is the angle), we used the derivatives \{$\partial_x u, \partial_{xx} u$\}.
% For FNO, we set the number of modes to 12 and channels to 32 (in the lifted space). We varied the number of Fourier layers from \{2, 3, 4\}. For DONet, in all the cases except Darcy Flow,  the trunk net and branch net were constructed as FFNs. We varied the number of layers from \{2, 3, 4\} and the layer width was chosen from \{30, 40, 50, 60\}, with ReLU activation. For the case of Darcy flow, we found that DONet with only feed-forward layers exhibited inferior performance. To address this, we introduced convolution layers into the branch net. We selected the number of convolution layers from \{3,5,7\}, and employed batch normalization and leaky ReLU after each convolution layer.
% To incorporate the learned pseudo physics representation into the training of FNO or DONet,  we randomly sampled 200 input functions to construct the second loss term in  \eqref{eq:fine-tune-loss}. 
% We set the maximum number of iterations to $10$ and selected the weight $\lambda$ from $[10^{-1}, 10^{2}]$. 
% All the models were implemented by PyTorch~\citep{paszke2019pytorch}, and optimized with ADAM~\citep{kingma2014adam}. The learning rate was selected from $\{10^{-4}, 5\times 10^{-4}, 10^{-3}\}$. The number of epochs for training or fine-tuning FNO, DONet and pseudo physics network $\phi$ was set to 500 to ensure convergence. 

For each operator learning benchmark, we simulated 100 examples for testing, and varied the number of training examples from \{5, 10, 20, 30\}, except for Advection, we ran with \{20, 30, 50, 80\} training examples. For SIF prediction,  which is much more challenging, we experimented with training size from \{400, 500, 600\}, and employed 200 test examples.
We repeated the evaluation for five times, each time we randomly sampled a different training set. The input to the pseudo physics networks $\phi$ includes all the partial derivatives up to the 2nd order. 
%We ran experiments on workstations equipped with Nvidia Geforce RTX 4090 and Intel I9 CPU. 
The details about the hyperparameter settings are given in Appendix Section~\ref{sect:appendix:methods}. 
%For each run, we first learned the standard FNO or DeepONet model based on the training data only. Then the model was fine-tuned (without any change on the model architecture) with the pseudo physics laws incorporated as in \eqref{eq:fine-tune-loss}. All the models were trained by ADAM optimization and the learning rate was tuned from $\{10^{-4}, 5\times 10^{-4}, 10^{-3}\}$. The number of epochs was to set 1000 to ensure convergence.  

\begin{table*}[htbp!]
\caption{\small Relative $L_2$ error in five operator learning benchmarks, where ``PPI'' is short for ``Pseudo-Physics Informed''. The results were averaged from five runs.  } \label{tb:all-pred-error}
    \small
    \centering
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Darcy flow}}
    \label{tab:darcy}
        \small 
        \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.4915 $\pm$ 0.0210	& 0.3870 $\pm$ 0.0118   & 0.2783 $\pm$ 0.0212   & 0.1645 $\pm$ 0.0071         \\
        PPI-FNO          &  & {0.1716} $\pm$ {0.0048}          & {0.0956} $\pm$ {0.0084}        & {0.0680} $\pm$ {0.0031} 		 & {0.0642} $\pm$ {0.0010}    \\
        Error Reduction          &  & 65.08\% & 75.29\%        & 75.56\%		 & 60.97\%   \\ \hline
        DONet             &  & 0.8678 $\pm$ 0.0089          & 0.6854 $\pm$ 0.0363      & 0.5841 $\pm$ 0.0279 	& 0.5672 $\pm$ 0.0172    \\
        PPI-DONet             &  & 0.5214 $\pm$ 0.0543        & 0.3408 $\pm$ 0.0209    	& 0.2775 $\pm$ 0.0224	& 0.2611 $\pm$ 0.0084  \\ 
        Error Reduction  &  & 39.91\%         & 50.27\%    	& 52.49\%	& 53.96\%  \\ \hline
        % $\phi$-FNN             &  & 0.2585$\pm$ 0.0141          & 0.1819 $\pm$ 0.0026		& 0.1520  $\pm$ 0.0020	& 0.1413 $\pm$ 0.0013  \\
        % $\phi$          &  & 0.2285$\pm$ 0.0147         & 0.1392 $\pm$ 0.0080 	& 0.0898 $\pm$ 0.0046	& 0.0688 $\pm$ 0.0032  \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Nonlinear diffusion}}
        \small
        \centering
        \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.2004 $\pm$ 0.0083	& 0.1242 $\pm$ 0.0046   & 0.0876 $\pm$ 0.0061   & 0.0551 $\pm$ 0.0021         \\
        PPI-FNO          &  & 0.0105 $\pm$ 0.0016         & 0.0066 $\pm$ 0.00023        & 0.0049 $\pm$ 0.00037 		 & 0.0038 $\pm$ 0.00039    \\
        Error Reduction          &  & 94.76\%         & 94.68\%        & 
        94.40\%		 & 93.10\%  \\
        \hline
        DONet             &  & 0.3010 $\pm$ 0.0119          & 0.2505 $\pm$ 0.0057      & 0.1726 $\pm$ 0.0076	& 0.1430 $\pm$ 0.0036    \\
        PPI-DONet             &  & 0.1478$\pm$ 0.0126          & 0.1161 $\pm$ 0.0124 	& 0.1032 $\pm$ 0.0059	& 0.0842 $\pm$ 0.0041  \\ 
        Error Reduction &  & 50.89\%         & 53.65\%    	& 40.20
        \%& 41.11\%  \\ \hline
        % $\phi$-FNN             &  & 0.0826$\pm$ 0.0070          & 0.0660 $\pm$ 0.0069  	& 0.0586  $\pm$ 0.0020	& 0.0463 $\pm$ 0.0022  \\
        % $\phi$            &  & 0.0303$\pm$ 0.0006         & 0.0233 $\pm$ 0.0005	& 0.0190 $\pm$ 0.0001	& 0.0163 $\pm$ 0.0002  \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Eikonal}}
    \small
    \centering
        \begin{tabular}{ccccc}
        \hline
        \textit{Training size}        & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                    & 0.2102 $\pm$ 0.0133	& 0.1562 $\pm$ 0.0098   & 0.0981 $\pm$ 0.0022   & 0.0843 $\pm$ 0.0020        \\
        PPI-FNO            & 0.0678 $\pm$ 0.0026         & 0.0582 $\pm$ 0.0043        & 0.0493 $\pm$ 0.0023 		 & 0.0459 $\pm$ 0.0010    \\
        Error Reduction & 67.74\%         & 62.74\%        & 49.74\%		 & 45.55\%   \\
        \hline
        DONet               & 0.3374 $\pm$ 0.0944          & 0.1759 $\pm$ 0.0065      & 0.1191 $\pm$ 0.0047 	& 0.1096 $\pm$ 0.0037    \\
        PPI-DONet               & 0.1302$\pm$ 0.0127          & 0.0907 $\pm$ 0.0093 	& 0.0714 $\pm$ 0.0011	& 0.0700 $\pm$ 0.0007  \\ 
        Error Reduction   & 61.41\%         & 48.43\%    	& 40.05\%	& 36.13\%  \\ \hline
        % $\phi$-FNN               & 0.0192$\pm$ 0.0013          & 0.0144 $\pm$ 0.0009  	& 0.0072  $\pm$ 0.0004	& 0.0070$\pm$ 0.00005  \\
        % $\phi$             & 0.0153$\pm$ 0.0009         & 0.0108 $\pm$ 0.0006	& 0.0059 $\pm$ 0.0002	& 0.0052 $\pm$ 0.0002  \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \caption{\small \textit{Poisson}}
    \label{tab:poisson}
    \small 
    \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.2340 $\pm$ 0.0083	& 0.1390 $\pm$ 0.0007   & 0.0895 $\pm$ 0.0008   & 0.0698 $\pm$ 0.0014         \\
        PPI-FNO          &  & {0.1437} $\pm$ {0.0062}          & {0.0771} $\pm$ {0.0018}        & {0.0544} $\pm$ {0.0009} 		 & {0.0458} $\pm$ {0.0003}    \\
        Error Reduction          &  & 38.59\% & 44.53\%        & 39.22\%		 & 34.38\%   \\ \hline
        DONet             &  & 0.6142 $\pm$ 0.0046          & 0.5839 $\pm$ 0.0090      & 0.5320 $\pm$ 0.0028 	& 0.5195 $\pm$ 0.0040    \\
        PPI-DONet             &  & 0.5275 $\pm$ 0.0037        & 0.5001 $\pm$ 0.0042    	& 0.4450 $\pm$ 0.0010	& 0.4258 $\pm$ 0.0040  \\ 
        Error Reduction  &  & 14.12\%         & 14.35\%    	& 16.35\%	& 18.04\%  \\ \hline
    \end{tabular}
    \end{subtable}
    \begin{subtable}{\textwidth}
     \caption{\small \textit{Advection}}
    \label{tab:advection}
        \small 
        \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {20}             & {30}         & {50}     & {80}     \\ \hline
        FNO                  &  & 0.4872 $\pm$ 0.0097	& 0.4035 $\pm$ 0.0086   & 0.3019 $\pm$ 0.0085   & 0.2482 $\pm$ 0.0059        \\
        PPI-FNO          &  & {0.3693} $\pm$ {0.0099}          & {0.3224} $\pm$ {0.0123}        & {0.2236} $\pm$ {0.0075} 		 & {0.1698} $\pm$ {0.0075}    \\
        Error Reduction          &  & 24.20\% & 20.10\%        & 25.94\%		 & 31.59\%   \\ \hline
        DONet             &  & 0.5795 $\pm$ 0.0045          & 0.4810 $\pm$ 0.0092      & 0.3882 $\pm$ 0.0086 	& 0.3164 $\pm$ 0.0072    \\
        PPI-DONet             &  & 0.3630 $\pm$ 0.0112        & 0.2897 $\pm$ 0.0097    	& 0.2629 $\pm$ 0.0053	& 0.2120 $\pm$ 0.0065  \\ 
        Error Reduction  &  & 37.36\%         & 39.77\%    	& 32.28\%	& 33.00\%  \\ \hline
    \end{tabular}
    \end{subtable}

\end{table*}

\subsection{Results and Analysis}\label{sect:exp-res}

%phi-performance


%learning curve & lambda
\begin{figure*}
    \centering
    \setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cccc}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/darcy_flow_10_iteration_learning_curve.pdf}
        \caption{\small PPI-FNO: learning}\label{fig:fno-learning}
    \end{subfigure} & 
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/Nonlinear_diffusion_10_iteration_learning_curve_deeponet.pdf}
        \caption{\small PPI-DONet: learning}\label{fig:donet-learning}
    \end{subfigure} &
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{error-vs-lambda/darcy_flow_data_fno_error_vs_lam.pdf}
        \caption{\small PPI-FNO: $\lambda$}\label{fig:lambda-fno}
    \end{subfigure} &
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{error-vs-lambda/Nonlinear_diffusion_deeponet_error_vs_lam.pdf}
        \caption{\small PPI-DONet: $\lambda$} \label{fig:lam-donet}
    \end{subfigure}
    \end{tabular}
    \caption{\small Learning curve of PPI-FNO on Darcy Flow (a), and of PPI-DONet on nonlinear diffusion (b). In (c) and (d) we show how the weight $\lambda$ of ``pseudo physics'' affects the operator learning performance. The horizontal line in (c) and (d) are the relative $L_2$ errors of standard FNO and DONet. }
    \label{fig:learning-curve-and-lambda-study}
\end{figure*}

\begin{table*}[htbp!]
\caption{\small SIF prediction error for plate surface cracks in fatigue modeling.}
    \label{tab:crack}
    \small 
    \centering
    \begin{tabular}{ccccc}
        \hline
        \textit{Training size}      &  & {400}             & {500}         & {600}      \\ \hline
        FNO                  &  & 0.1776 $\pm$ 0.0150	& 0.1695 $\pm$ 0.0090   & 0.1122 $\pm$ 0.0094          \\
        PPI-FNO          &  & {0.1166} $\pm$ {0.0064}          & {0.1151} $\pm$ {0.0093}        &{0.0850}$\pm${0.0060} \\
        Error Reduction          &  & 34.35\% & 32.09\%        & 24.24\%		 \\ \hline
        DONet             &  & 0.5318 $\pm$ 0.0095          & 0.5155 $\pm$ 0.0200      & 0.4037 $\pm$ 0.0331 	  \\
        PPI-DONet             &  & 0.3490 $\pm$ 0.0034        & 0.3468 $\pm$ 0.0074    	& 0.3299 $\pm$ 0.0066	 \\ 
        Error Reduction  &  & 34.37\%         & 32.73\%    	& 18.28\%  \\ \hline
        % $\phi$-FNN             &  & 0.2585$\pm$ 0.0141          & 0.1819 $\pm$ 0.0026		& 0.1520  $\pm$ 0.0020	& 0.1413 $\pm$ 0.0013  \\
        % $\phi$          &  & 0.2285$\pm$ 0.0147         & 0.1392 $\pm$ 0.0080 	& 0.0898 $\pm$ 0.0046	& 0.0688 $\pm$ 0.0032  \\ \hline
    \end{tabular}
    \end{table*}

\cmt{
\begin{figure*}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cc}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_5.pdf}
	\end{subfigure} &
    \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_5.pdf}
	\end{subfigure}
 \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_10.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_10.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_20.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_20.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_30.pdf}
 \caption{\small \textit{PPI-DONet: Darcy Flow}}\label{fig:darcy-dont-example}
	\end{subfigure} & 
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_30.pdf}
 \caption{\small \textit{PPI-FNO: nonlinear diffusion}}\label{fig:nl-fno-example}
	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error of PPI-DONet and PPI-FNO on \textit{Darcy Flow} and \textit{nonlinear diffusion}, respectively.  From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
 \vspace{-0.2in}
\end{figure*}
}

\noindent\textbf{Predictive performance with limited data.} We reported the average relative $L_2$ error and the standard deviation (with and without using pseudo physics informed learning) in Table \ref{tb:all-pred-error} and Table~\ref{tab:crack}. The model trained with the pseudo physics network (see \eqref{eq:fine-tune-loss}) is denoted as PPI-FNO or PPI-DONet, short for Pseudo Physics Informed FNO/DONet.
In all the cases, with our pseudo physics informed approach, the prediction error of both FNO and DONet experiences a large reduction. For instance, across all training sizes in \textit{Darcy Flow} and \textit{nonlinear diffusion}, PPI-FNO reduces the relative $L_2$ error of the ordinary FNO by over 60\% and 93\%, respectively. In \textit{Darcy Flow} with training sizes 10 to 30, PPI-DONet reduces the error of the ordinary DONet by over 50\%. {In SIF prediction, our method applied to both FNO and DONet reduced the error by over 30\% with training size 400 and 500.}
Even the minimum reduction across all cases achieves 14.12\% (PPI-DONet over DONet on \textit{Poisson} with training size 5). 

Together these results demonstrate the strong positive impact of the learned physics by our neural network model $\phi$ specified in Section \ref{sect:phi}. Although it remains opaque and non-interpretable, it encapsulates valuable knowledge that greatly enhances the performance of operator learning with limited data.

 
Next, we assessed the accuracy of the learned physics laws by examining the relative \(L_2\) error in predicting the source functions \(f\) from \(\phi\) (see \eqref{eq:pde-learn}). We tested on \textit{Darcy Flow}, \textit{nonlinear diffusion}, and \textit{Eikonal}. We compared a baseline method that removes the convolution layer of \(\phi\), leaving only the fully connected layers, namely MLP (see Fig~\ref{fig:phi} bottom). 
The results are reported in Appendix Table \ref{tb:utof}. 
%\begin{wraptable}{r}{0.65\textwidth}
% \begin{table}
% \small
% \centering
% \begin{subtable}{0.6\textwidth}
%    \small
%     \centering
%     \begin{tabular}{ccc}
%     \hline \textit{Benchmark} & FFN & Ours \\
%     \hline
%     Darcy Flow & 0.1819$\pm$0.0026 & \textbf{0.1392$\pm$ 0.0080}\\
%     Nonlinear Diffusion & 0.0660$\pm$0.0069 & \textbf{0.0233$\pm$0.0005}\\
%     Eikonal & 0.0144$\pm$0.0009 & \textbf{0.0108 $\pm$ 0.0006}\\
%     \hline
%     \end{tabular}
%     \caption{\small Training size=10}
% \end{subtable}
% \begin{subtable}{0.6\textwidth}
%    \small
%     \centering
%     \begin{tabular}{ccc}
%     \hline \textit{Benchmark} & FFN & Ours \\
%     \hline
%     Darcy Flow & 0.1413$\pm$0.0013 & \textbf{0.0688$\pm$ 0.0032}\\
%     Nonlinear Diffusion & 0.0463$\pm$0.0022 & \textbf{0.0163$\pm$0.0002}\\
%     Eikonal & 0.0070$\pm$0.00005 & \textbf{0.0052 $\pm$ 0.0002}\\
%     \hline
%     \end{tabular}
%     \caption{\small Training size=30}
% \end{subtable}
% \caption{\small Relative $L_2$ error of using the learned back-box PDE network~\eqref{eq:pde-learn} to predict the input function $f$. } \label{tb:utof}
% \end{table}
It can be observed that in nearly every case, adding a convolution layer indeed significantly improves the accuracy of \(\phi\). This improvement might be attributed to the convolution layer's ability to integrate neighboring information and compensate for the information loss introduced by finite difference in approximating  the derivatives. We also experimented with multiple convolution layers, but the improvement was found to be marginal. 

In addition, we found the operator learning improvement is relatively \textit{robust} to the accuracy of our physics representation $\phi$. For instance, on \textit{Darcy Flow} with training size 5 and 10, the relative $L_2$ error of $\phi$ network is 0.2285 and 0.1392, which is significantly bigger than with training size 30  where the relative $L_2$ error is 0.0688. Yet the error reduction upon FNO (see Table~\ref{tab:darcy}) under all the three training sizes is above 60\%. The error reduction upon DONet is 40\% for training size 5 and over 50\% for training size 10 and 30. 
The results imply that even roughly capturing the underlying physics (with $\phi$) can substantially boost the operator learning performance. In Appendix Section~\ref{sect:appendix:point-wise}, we we provide a detailed comparison of the point-wise error of our method against standard NO, along with additional discussions.


\noindent\textbf{Training with rich data.}  Although our method is designed to enhance operator learning with limited data, we also evaluated its performance when the training data is abundant. Specifically, we increased the training size of the five operator learning benchmarks to 600 and 1000 examples. The results, presented in Appendix Table~\ref{tb:pred-error-large-data}, show that the accuracy of the standard NO and our method, PPI-NO, becomes comparable. For \textit{Darcy flow} and \textit{Advection}, our method achieves slight improvement, whereas for \textit{Eikonal} and \textit{Nonlinear diffusion}, it performs slightly worse. This may be because, with sufficiently rich data, the standard NO can already utilize the data effectively to achieve good performance, leaving little room for the added value of the learned physics to further improve the results. In such cases, the benefit of incorporating physics knowledge becomes marginal. Furthermore, our alternating updating mechanism brings additional optimization workload, which may introduce additional complexity to the learning process.


\noindent\textbf{Comparison with NO training using ground-truth physics.}  
Our work is motivated by scenarios where the underlying physics is unknown. It is therefore insightful to compare our approach with training NO using the ground-truth physics, as done in PINO. This comparison allows us to examine how our learned ``pseudo'' physics differs from the true physics in facilitating operator learning. To this end, we tested PINO on the Poisson and Advection cases, using finite difference approximations to compute the derivatives. We used FNO as the baseline. The PDE residual was incorporated into the training loss. As shown in Appendix Table~\ref{tb:pred-error-PINO}, while PINO consistently achieves even better results, the performance of our method is close. This demonstrates that our learned black-box representation, despite lacking interpretability, can provide a similar benefit in improving operator learning performance.


%\noindent\textbf{Comparison with NO Learning with Ground-Truth Physics.} Our work is motivated by the scenarios that the underlying physics is unknown. It is interesting to compare with training NO using the ground-truth physics, namely PINO. In this way, we can examine how different our learned ``pseudo'' physics as compared with the true physics in facilitating operator learning. To this end, we tested PINO on Poisson and Advection cases. We also used finite difference approximation to compute the derivatives. The PDE residual was then added into the training loss. As shown in 
%Appendix Table~\ref{tb:pred-error-PINO}, while PINO in most cases achieves even better results, our method's performance is close. It shows that our learned black-box representation, though lacking Interpretability, can achieve a similar benefit in improving the operator learning. 

\cmt{
\noindent\textbf{Point-wise prediction and point-wise error.} For a detailed assessment, we conducted a fine-grained evaluation by visualizing the predictions and point-wise errors made by each method. In Fig. \ref{fig:darcy-dont-example} and \ref{fig:nl-fno-example},  we showcased the predictions and point-wise errors using PPI-DONet for \textit{Darcy Flow}, PPI-FNO for \textit{nonlinear diffusion}, respectively. Additional examples of predictions and point-wise errors are provided in Fig.~\ref{fig:SIF-pred-example}, \ref{fig:ek-fno-example}, \ref{fig:ek-deeponet-example}, \ref{fig:darcy-fno-example}, and \ref{fig:nl-deeponet-example} in the Appendix. 
 
It is evident that without the assistance of the pseudo physics laws learned by our method, the ordinary DONet and FNO frequently missed crucial local structures, sometimes even learning entirely incorrect structures.
For example, In Fig. \ref{fig:darcy-dont-example} the first row, DONet missed one mode, while in the second and third row of Fig. \ref{fig:darcy-dont-example}, DONet failed to capture all the local modes. 
After incorporating the learned physics, DONet (now denoted as PPI-DONet; see the third column) successfully captures all the local modes, including their shapes and positions. Although not all the details are exactly recovered, the point-wise error is substantially reduced, particularly in those high error regions of the ordinary DONet; see the fourth column of Fig. \ref{fig:darcy-dont-example}.
In another instance, as shown in Fig. \ref{fig:nl-fno-example}, where the ordinary FNO (second column) captured the global shape of the solution, but the mis-specification of many local details led to large point-wise errors across many regions (fourth column). In contrast, PPI-FNO (third column) not only identified the structures within the solution but also successfully recovered the details. As a result, the point-wise error (fifth column) was close to zero everywhere.
Additional instances can be found in Fig. \ref{fig:ek-fno-example}, the first three rows illustrate that ordinary FNO (trained with 5, 10, and 20 examples, respectively) estimates an entirely incorrect structure of the solution, indicating that the training data is insufficient for FNO to capture even the basic structure of the solution. In contrast, after fine-tuning with our learned physics laws from the same sparse data, PPI-FNO accurately figured out the solution structures and yielded a substantial reduction in point-wise error across nearly everywhere. The point-wise error became uniformly close to zero.
With 30 examples, the ordinary FNO was then able to capture the global structure of the solution, but the details in the bottom left, bottom right, and top right corners were incorrectly predicted. In comparison, PPI-FNO further recovered these details accurately. 




Collectively, these results demonstrate that the pseudo physics extracted by our method not only dramatically boosts the overall prediction accuracy but also better recovers the local structures and details of the solution.
}
\noindent \textbf{Learning behavior.} We examined the learning behavior of our method, which conducts an iterative, alternatingly fine-tuning process. We employed one \textit{Darcy Flow}, one \textit{nonlinear diffusion} and one \textit{Eikonal} dataset, each with 30 examples. We show the test relative $L_2$ error along with the iterations in Fig. \ref{fig:fno-learning}, \ref{fig:donet-learning}, and Appendix Fig.~\ref{fig:fno-learning-eikonal} and Fig.~\ref{fig:donet-learning-eikonal}. As we can see, the predictive performance of our algorithm kept improving and tended to converge at last, affirming the efficacy of the learning process. 


% \textbf{Real World Data Experiments.} \keyan {
% We also tested our learning framework on a real-world dataset.%explaination of the crack dataset%
% Since the real data will be much more complex than the previous physics PDE equations, we choose to use training examples from {400,500,600}. Following the experiment before, we still evaluated the real-world dataset on FNO and DONet(see Table~\ref{tab:crack}). These results also demonstrate our learning framework's positive feasibility and positive impact on real-world data.

% }

\textbf{Ablation study on pseudo physics network $\phi$.} To confirm the efficacy of our designed network $\phi$ in facilitating operator learning, we considered alternative designs for $\phi$: (1) using standard FNO to predict $f$ directly from $u$; no derivative information is included in the input; (2) removing the convolution layer in our model, and just keeping the fully-connected layers, namely MLP; the derivative information is not included in the input. With different designs of $\phi$, we evaluated the PPI learning performance on the Darcy Flow benchmark. The relative $L_2$ error in predicting $f$ via $\phi$ and predicting $u$ is reported in Appendix Table~\ref{tb:phierror}. It can be seen that our design of $\phi$ consistently outperforms alternative architectures by a notable margin, showing the effectiveness of learning a (black-box) PDE representation and improving the operator learning.   

%\keyan {In addition, in the ablation study, we first verify the effectiveness of our PDE network $\phi$. Specifically, we tested on the Darcy-flow benchmark with alternative choices of $\phi$, including (1) a standard FNO where the input is $u$ and the output is $f$, and (2)  a pixel-wise MLP with the same number of layers and width as in our method, where the input is $u$ value at a grid point, and the output is the corresponding $f$ value at that grid point.  The relative $L_2$ error in predicting $f$ of each method is given(see Table~\ref{tb:phierror}). It shows that our design of $\phi$ with $u$ derivatives indeed outperforms other architectures, showing the effectiveness of learning a (black-box) PDE representation.

% \begin{table*}[t]
% \small
% \centering
% \begin{subtable}{\textwidth}
%    \small
%     \centering
%     \begin{tabular}{ccccc}
%     \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
%     \hline
%     FNO & 0.7229$\pm$0.0318 & 0.5759$\pm$ 0.0126 & 0.4257$\pm$ 0.0106 & 0.3160$\pm$ 0.0037\\
%     MLP & 0.7169$\pm$0.0160 & 0.6598$\pm$ 0.0056 & 0.6464$\pm$ 0.0029 & 0.6277$\pm$ 0.0032\\
%     Ours & \textbf{0.2285 $\pm$ 0.0147} & \textbf{0.1392 $\pm$ 0.0080} & \textbf{0.0898 $\pm$ 0.0046} & \textbf{0.0688 $\pm$ 0.0032}\\
%     \hline
%     \end{tabular}
%     \caption{\small Predicting $f$ via  $\phi$ with different architectures.}
% \end{subtable}
% \begin{subtable}{\textwidth}
%    \small
%     \centering
%     \begin{tabular}{ccccc}
%     \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
%     \hline
%     PPI-FNO with FNO as $\phi$ & 0.5853$\pm$0.0153 & 0.3871$\pm$ 0.0124 & 0.2613$\pm$ 0.0190 & 0.1629$\pm$ 0.0064\\
%     PPI-FNO with MLP as $\phi$ & 0.7262$\pm$0.0920 & 0.5516$\pm$ 0.0699 & 0.4568$\pm$ 0.0857 & 0.3983$\pm$ 0.1051\\
%     Standard FNO & 0.4915 $\pm$ 0.0210	& 0.3870 $\pm$ 0.0118   & 0.2783 $\pm$ 0.0212   & 0.1645 $\pm$ 0.0071\\
%     Ours & \textbf{0.1716 $\pm$ 0.0048} & \textbf{0.0956 $\pm$ 0.0084} & \textbf{0.0680 $\pm$ 0.0031} & \textbf{0.0642 $\pm$ 0.0010}\\
%     \hline
%     \end{tabular}
%     \caption{\small Predicting $u$.}
% \end{subtable}
% \caption{\small The relative $L_2$ error with using different architectures of $\phi$ in pseudo-physics-informed (PPI) learning on Darcy Flow benchmark.} \label{tb:phierror}
% %\end{table}
% \end{table*}

\noindent\textbf{Ablation study on the choice of derivatives.}
We further investigated the PPI learning performance with respect to the choice of derivatives used in our pseudo physics network. Specifically, we tested PPI-FNO on the Darcy-flow benchmark and varied the order of derivatives up to 0, 1, 2, and 3. The performance is reported in Appendix Table~\ref{tb:ordererror}. We can see that although the accuracy of $\phi$ with derivatives up to the third order is slightly better than with derivatives up to the second order, the best operator learning performance was still achieved using derivatives up to the second order (which was used in our evaluations). This might be because higher-order derivative information can cause overfitting in the pseudo physics network $\phi$ to a certain degree. Such higher-order information may not be critical to the actual mechanism of the physical system and can therefore impede the improvement of operator learning performance.

% \begin{table*}[t]
% \small
% \centering
% \begin{subtable}{\textwidth}
%    \small
%     \centering
%     \begin{tabular}{ccccc}
%     \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
%     \hline
%     order 0 & 0.7126$\pm$0.0131 & 0.5733$\pm$0.0208 & 0.4812$\pm$0.0399 & 0.3445$\pm$0.0182 \\ 
%     order $\le$ 1 & 0.2926$\pm$0.0118 & 0.2006$\pm$0.0047 & 0.1379$\pm$0.0051 & 0.1084$\pm$0.0053 \\ 
%     order $\le 2$  & {0.2285$\pm$0.0147} & {0.1392$\pm$0.0080} & {0.0898$\pm$0.0046} & {0.0688$\pm$0.0032} \\ 
%     order $\le 3$ & 
%     \textbf{0.2058$\pm$0.0192} & \textbf{0.1123$\pm$0.0039} & \textbf{0.0712$\pm$0.0021} & \textbf{0.0585$\pm$0.0030} \\ 
%     \hline
%     \end{tabular}
%     \caption{\small Predicting $f$ via $\phi$.}
% \end{subtable}
% \begin{subtable}{\textwidth}
%    \small
%     \centering
%     \begin{tabular}{ccccc}
%     \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
%     \hline
%     order 0 & 0.6352$\pm$0.0673 & 0.4523$\pm$0.0621 & 0.3570$\pm$0.0658 & 0.2737$\pm$0.0643 \\ 
%     order $\le$ 1 & 0.3386$\pm$0.0259 & 0.2161$\pm$0.0083 & 0.1645$\pm$0.0114 & 0.1197$\pm$0.0132 \\ 
%     order $\le 2$ & \textbf{0.1716$\pm$0.0048} & \textbf{0.0956$\pm$0.0084} & \textbf{0.0680$\pm$0.0031} & \textbf{0.0642$\pm$0.0010} \\ 
%     order $\le 3$ & 0.2959$\pm$0.0381 & 0.1719$\pm$0.0213 & 0.1193$\pm$0.0158 & 0.0828$\pm$0.0054 \\ 
%     \hline
%     \end{tabular}
%     \caption{\small Predicting $u$.}
% \end{subtable}
% \caption{\small The relative $L_2$ error of PPI learning by incorporating different orders of derivatives. During the comparison with other operator learning methods, we used derivative orders up to 2 to run our method.   } \label{tb:ordererror}
% \vspace{-0.2in}
% \end{table*}

\noindent\textbf{Ablation study on the weight $\lambda$.} We examined the effect of the weight $\lambda$ of our ``pseudo physics''; see \eqref{eq:fine-tune-loss}. To this end, we used one \textit{Darcy Flow} dataset and \textit{nonlinear diffusion} dataset with training size 30. We varied $\lambda$ from $[0.5, 10^2]$, and run PPI-FNO and PPI-DONet on \textit{Darcy Flow} and \textit{nonlinear diffusion}, respectively. As shown in Fig.~\ref{fig:lambda-fno} and~\ref{fig:lam-donet}, we can see that across a wide range of $\lambda$ values, PPI-FNO and PPI-DONet can consistently outperform the standard FNO and DONet respectively by a large margin. However, the choice $\lambda$ does have a significant influence on the operator learning performance, and the best choice is often in between. In Appendix Fig.~\ref{fig:lambda-fno-eikonal} and~\ref{fig:lam-donet-eikonal}, we show results on \textit{Ekonal}, which we make similar observations. 


%\textbf{Computational complexity and memory usage.} 
\textbf{Memory Cost.} 
%Our PPI-NO framework conducts alternating updates, and hence needs more training cycles than standard NO. The time complexity grows linearly with the number of alternating iterations. 
Our PPI-NO is more costly than standard NO since we need to train an additional ``pseudo'' physics network $\phi$ along with the NO model. However,  the network $\phi$
%For memory usage, Our ``pseudo'' physics network $\phi$ 
is small as compared to the NO component --- $\phi$ is simply a pixel-wise MLP coupled with one convolution filter, resulting in a marginal increase in memory cost. Appendix Table~\ref{tb:paracounts}  shows the parameter count of FNO, DONet and their pseudo-physics-informed version. On average, PPI-FNO increases the number of parameters over FNO by 1.29\% while PPI-DONet over DONet by 1.89\%.

% \keyan {The alternative training process requires more training cycles than standard NO, but the time complexity only grows linearly with the number of alternating iterations, not quadratically or exponentially. We believe this is reasonable and practically acceptable.
% For memory analysis, Our "pseudo" physics network $\phi$ is very small as compared to standard neural operators (NO) --- $\phi$ is simply a pixel-wise MLP coupled with one convolution filter, resulting in a marginal increase in memory cost. The below shows under training size = 30, the parameter count of FNO, DONet and their pseudo physics informed version. On average, PPI-FNO increases the number of parameters over FNO by 1.29 while PPI-DONet by DONet increases by 1.89.(see Table~\ref{tb:paracounts})


