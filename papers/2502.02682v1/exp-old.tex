
\section{Experiments}\label{sect:expr}
\noindent\textbf{Dataset.} We tested on five commonly used benchmark operator learning problems in literature~\citep{li2020fourier,lu2022comprehensive}, including \textit{Darcy Flow}, \textit{Nonlinear Diffusion}, \textit{Eikonal}, \textit{Poisson} and \textit{Advection}. The details are given in Section~\ref{sect:detail} of the Appendix. \zhe{In addition, we examined our method in one application in fatigue modeling. The task is to predict the stress intensity factor (SIF) for semi-elliptical surface cracks  on plates, given four geometric parameters that characterize the cracks~\citep{merrell2024stress}. The SIF plays a critical role in modeling crack growth by quantifying the stress state near the tip of a crack, and hence SIF computation and analysis is extremely important in fatigue modeling and fracture mechanics~\citep{anderson2005fracture}. The computation of SIF is expensive, because it typically needs to run finite element method (FEM) or extended FEM with very fine meshes~\citep{kuna2013finite}. 
Due to the complex sequence of computational steps involved in calculating SIFs, there is no holistic PDE that directly models the relationship between the geometric features and the SIF function. Instead, the computation of SIFs typically relies on numerical methods and the extraction of local stress fields near the crack tip.} 

 \noindent\textbf{Method and Settings.} We evaluated our method based on two popular NO models, FNO and DONet. For learning the pseudo physics laws via the neural network $\phi$ --- see \eqref{eq:pde-learn} --- we tuned the kernel size from \{(3, 3), (5, 5), (7, 7), (9, 9)\}. The stride was set to 1 and padding was set to ``same'' to ensure the output shape does not change. In the subsequent FNN, we chose the number of layers from \{3, 4, 5, 6\}, and the layer width from \{16, 32, 64\}. We used {GeLU} activation. For the cases of Darcy Flow, Eikonal and Poisson, we used the following derivatives $\{\partial_{x_1} u, \partial_{x_2} u, \partial_{x_1x_1}u, \partial_{x_2x_2}u, \partial_{x_1x_2} u\}$, and for the other cases, we used $\{\partial_x u, \partial_{xx} u, \partial_t u, \partial_{tt} u, \partial_{xt} u\}$. \zhe{SIF setting Keyan...}
For FNO, we set the number of modes to 12 and channels to 32 (in the lifted space). We varied the number of Fourier layers from \{2, 3, 4\}. For DONet, in all the cases except Darcy Flow,  the trunk net and branch net were constructed as FFNs. We varied the number of layers from \{2, 3, 4\} and the layer width was chosen from \{30, 40, 50, 60\}, with ReLU activation. For the case of Darcy flow, we found that DONet with only feed-forward layers exhibited inferior performance. To address this, we introduced convolution layers into the branch net. We selected the number of convolution layers from \{3,5,7\}, and employed batch normalization and leaky ReLU after each convolution layer.
To incorporate the learned pseudo physics representation into the training of FNO or DONet,  we randomly sampled 200 input functions to construct the second loss term in  \eqref{eq:fine-tune-loss}. 
We set the maximum number of iterations to $10$ and selected the weight $\lambda$ from $[10^{-1}, 10^{2}]$. 
All the models were implemented by PyTorch~\citep{paszke2019pytorch}, and optimized with ADAM~\citep{kingma2014adam}. The learning rate was selected from $\{10^{-4}, 5\times 10^{-4}, 10^{-3}\}$. The number of epochs for training or fine-tuning FNO, DONet and pseudo physics network $\phi$ was set to 500 to ensure convergence. 
For each operator learning benchmark, we simulated 100 examples for testing, and varied the number of training examples from \{5, 10, 20, 30\}, except for Advection, we ran with \{20, 30, 50, 80\} training examples. \zhe{For SIF prediction task (which is much more challening), we experimented with training size from \{400, 500, 600\}, and employed 100 test examples.}
We repeated the evaluation for five times, each time we randomly sampled a different training set. We ran experiments on workstations equipped with Nvidia Geforce RTX 4090 and Intel I9 CPU. 
%For each run, we first learned the standard FNO or DeepONet model based on the training data only. Then the model was fine-tuned (without any change on the model architecture) with the pseudo physics laws incorporated as in \eqref{eq:fine-tune-loss}. All the models were trained by ADAM optimization and the learning rate was tuned from $\{10^{-4}, 5\times 10^{-4}, 10^{-3}\}$. The number of epochs was to set 1000 to ensure convergence.  

\begin{table*}[t]
    \small
    \centering
    \begin{subtable}{\textwidth}
        \small 
        \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.4915 $\pm$ 0.0210	& 0.3870 $\pm$ 0.0118   & 0.2783 $\pm$ 0.0212   & 0.1645 $\pm$ 0.0071         \\
        PPI-FNO          &  & {0.1716} $\pm$ {0.0048}          & {0.0956} $\pm$ {0.0084}        & {0.0680} $\pm$ {0.0031} 		 & {0.0642} $\pm$ {0.0010}    \\
        Error Reduction          &  & 65.08\% & 75.29\%        & 75.56\%		 & 60.97\%   \\ \hline
        DONet             &  & 0.8678 $\pm$ 0.0089          & 0.6854 $\pm$ 0.0363      & 0.5841 $\pm$ 0.0279 	& 0.5672 $\pm$ 0.0172    \\
        PPI-DONet             &  & 0.5214 $\pm$ 0.0543        & 0.3408 $\pm$ 0.0209    	& 0.2775 $\pm$ 0.0224	& 0.2611 $\pm$ 0.0084  \\ 
        Error Reduction  &  & 39.91\%         & 50.27\%    	& 52.49\%	& 53.96\%  \\ \hline
        % $\phi$-FNN             &  & 0.2585$\pm$ 0.0141          & 0.1819 $\pm$ 0.0026		& 0.1520  $\pm$ 0.0020	& 0.1413 $\pm$ 0.0013  \\
        % $\phi$          &  & 0.2285$\pm$ 0.0147         & 0.1392 $\pm$ 0.0080 	& 0.0898 $\pm$ 0.0046	& 0.0688 $\pm$ 0.0032  \\ \hline
    \end{tabular}
    \caption{\small \textit{Darcy flow}}
    \label{tab:darcy}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \small
        \centering
        \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.2004 $\pm$ 0.0083	& 0.1242 $\pm$ 0.0046   & 0.0876 $\pm$ 0.0061   & 0.0551 $\pm$ 0.0021         \\
        PPI-FNO          &  & 0.0105 $\pm$ 0.0016         & 0.0066 $\pm$ 0.00023        & 0.0049 $\pm$ 0.00037 		 & 0.0038 $\pm$ 0.00039    \\
        Error Reduction          &  & 94.76\%         & 94.68\%        & 
        94.40\%		 & 93.10\%  \\
        \hline
        DONet             &  & 0.3010 $\pm$ 0.0119          & 0.2505 $\pm$ 0.0057      & 0.1726 $\pm$ 0.0076	& 0.1430 $\pm$ 0.0036    \\
        PPI-DONet             &  & 0.1478$\pm$ 0.0126          & 0.1161 $\pm$ 0.0124 	& 0.1032 $\pm$ 0.0059	& 0.0842 $\pm$ 0.0041  \\ 
        Error Reduction &  & 50.89\%         & 53.65\%    	& 40.20
        \%& 41.11\%  \\ \hline
        % $\phi$-FNN             &  & 0.0826$\pm$ 0.0070          & 0.0660 $\pm$ 0.0069  	& 0.0586  $\pm$ 0.0020	& 0.0463 $\pm$ 0.0022  \\
        % $\phi$            &  & 0.0303$\pm$ 0.0006         & 0.0233 $\pm$ 0.0005	& 0.0190 $\pm$ 0.0001	& 0.0163 $\pm$ 0.0002  \\ \hline
    \end{tabular}
    \caption{\small \textit{Nonlinear diffusion}}
    \end{subtable}
    \begin{subtable}{\textwidth}
    \small
    \centering
        \begin{tabular}{ccccc}
        \hline
        \textit{Training size}        & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                    & 0.2102 $\pm$ 0.0133	& 0.1562 $\pm$ 0.0098   & 0.0981 $\pm$ 0.0022   & 0.0843 $\pm$ 0.0020        \\
        PPI-FNO            & 0.0678 $\pm$ 0.0026         & 0.0582 $\pm$ 0.0043        & 0.0493 $\pm$ 0.0023 		 & 0.0459 $\pm$ 0.0010    \\
        Error Reduction & 67.74\%         & 62.74\%        & 49.74\%		 & 45.55\%   \\
        \hline
        DONet               & 0.3374 $\pm$ 0.0944          & 0.1759 $\pm$ 0.0065      & 0.1191 $\pm$ 0.0047 	& 0.1096 $\pm$ 0.0037    \\
        PPI-DONet               & 0.1302$\pm$ 0.0127          & 0.0907 $\pm$ 0.0093 	& 0.0714 $\pm$ 0.0011	& 0.0700 $\pm$ 0.0007  \\ 
        Error Reduction   & 61.41\%         & 48.43\%    	& 40.05\%	& 36.13\%  \\ \hline
        % $\phi$-FNN               & 0.0192$\pm$ 0.0013          & 0.0144 $\pm$ 0.0009  	& 0.0072  $\pm$ 0.0004	& 0.0070$\pm$ 0.00005  \\
        % $\phi$             & 0.0153$\pm$ 0.0009         & 0.0108 $\pm$ 0.0006	& 0.0059 $\pm$ 0.0002	& 0.0052 $\pm$ 0.0002  \\ \hline
    \end{tabular}
    \caption{\small \textit{Eikonal}}
    \end{subtable}

    \begin{subtable}{\textwidth}
    \small 
    \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {5}             & {10}         & {20}     & {30}     \\ \hline
        FNO                  &  & 0.2340 $\pm$ 0.0083	& 0.1390 $\pm$ 0.0007   & 0.0895 $\pm$ 0.0008   & 0.0698 $\pm$ 0.0014         \\
        PPI-FNO          &  & {0.1437} $\pm$ {0.0062}          & {0.0771} $\pm$ {0.0018}        & {0.0544} $\pm$ {0.0009} 		 & {0.0458} $\pm$ {0.0003}    \\
        Error Reduction          &  & 38.59\% & 44.53\%        & 39.22\%		 & 34.38\%   \\ \hline
        DONet             &  & 0.6142 $\pm$ 0.0046          & 0.5839 $\pm$ 0.0090      & 0.5320 $\pm$ 0.0028 	& 0.5195 $\pm$ 0.0040    \\
        PPI-DONet             &  & 0.5275 $\pm$ 0.0037        & 0.5001 $\pm$ 0.0042    	& 0.4450 $\pm$ 0.0010	& 0.4258 $\pm$ 0.0040  \\ 
        Error Reduction  &  & 14.12\%         & 14.35\%    	& 16.35\%	& 18.04\%  \\ \hline
    \end{tabular}
    \caption{\small \textit{Poisson}}
    \label{tab:poisson}
    \end{subtable}
    \begin{subtable}{\textwidth}
        \small 
        \centering
    \begin{tabular}{cccccc}
        \hline
        \textit{Training size}      &  & {20}             & {30}         & {50}     & {80}     \\ \hline
        FNO                  &  & 0.4872 $\pm$ 0.0097	& 0.4035 $\pm$ 0.0086   & 0.3019 $\pm$ 0.0085   & 0.2482 $\pm$ 0.0059        \\
        PPI-FNO          &  & {0.3693} $\pm$ {0.0099}          & {0.3224} $\pm$ {0.0123}        & {0.2236} $\pm$ {0.0075} 		 & {0.1698} $\pm$ {0.0075}    \\
        Error Reduction          &  & 24.20\% & 20.10\%        & 25.94\%		 & 31.59\%   \\ \hline
        DONet             &  & 0.5795 $\pm$ 0.0045          & 0.4810 $\pm$ 0.0092      & 0.3882 $\pm$ 0.0086 	& 0.3164 $\pm$ 0.0072    \\
        PPI-DONet             &  & 0.3630 $\pm$ 0.0112        & 0.2897 $\pm$ 0.0097    	& 0.2629 $\pm$ 0.0053	& 0.2120 $\pm$ 0.0065  \\ 
        Error Reduction  &  & 37.36\%         & 39.77\%    	& 32.28\%	& 33.00\%  \\ \hline
    \end{tabular}
    \caption{\small \textit{Advection}}
    \label{tab:advection}
    \end{subtable}
    
    \caption{\small Relative $L_2$ error in five operator learning benchmarks, where ``PPI'' is short for Pseudo-Physics Informed''. $\phi$-FNN means only using FNN components to train the network $\phi$ (\ie removing the convolution layer). The last two rows of each table indicate the error in predicting the source function via $\phi$. All the results were averaged from five runs.  } \label{tb:all-pred-error}
\end{table*}

\subsection{Results and Analysis}\label{sect:exp-res}

%phi-performance


%learning curve & lambda
\begin{figure}
    \centering
    \setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cccc}
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/darcy_flow_10_iteration_learning_curve.pdf}
        \caption{\small PPI-FNO: learning}\label{fig:fno-learning}
    \end{subfigure} & 
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{iteration-learning-curve/Nonlinear_diffusion_10_iteration_learning_curve_deeponet.pdf}
        \caption{\small PPI-DONet: learning}\label{fig:donet-learning}
    \end{subfigure} &
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{error-vs-lambda/darcy_flow_data_fno_error_vs_lam.pdf}
        \caption{\small PPI-FNO: $\lambda$}\label{fig:lambda-fno}
    \end{subfigure} &
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
\includegraphics[width=\textwidth]{error-vs-lambda/Nonlinear_diffusion_deeponet_error_vs_lam.pdf}
        \caption{\small PPI-DONet: $\lambda$} \label{fig:lam-donet}
    \end{subfigure}
    \end{tabular}
    \caption{\small Learning curve of PPI-FNO on Darcy Flow (a), and of PPI-DONet on nonlinear diffusion (b). In (c) and (d) we show how the weight $\lambda$ of ``pseudo physics'' affects the operator learning performance. The horizontal line in (c) and (d) are the relative $L_2$ errors of standard FNO and DONet. }
    \label{fig:learning-curve-and-lambda-study}
\end{figure}

\begin{table*}
    \small 
    \centering
    \begin{tabular}{ccccc}
        \hline
        \textit{Training size}      &  & {400}             & {500}         & {600}      \\ \hline
        FNO                  &  & 0.1776 $\pm$ 0.0150	& 0.1695 $\pm$ 0.0090   & 0.1122 $\pm$ 0.0094          \\
        PPI-FNO          &  & {0.1166} $\pm$ {0.0064}          & {0.1151} $\pm$ {0.0093}        &{0.0850}$\pm${0.0060} \\
        Error Reduction          &  & 34.35\% & 32.09\%        & 24.24\%		 \\ \hline
        DONet             &  & 0.5318 $\pm$ 0.0095          & 0.5155 $\pm$ 0.0200      & 0.4037 $\pm$ 0.0331 	  \\
        PPI-DONet             &  & 0.3490 $\pm$ 0.0034        & 0.3468 $\pm$ 0.0074    	& 0.3299 $\pm$ 0.0066	 \\ 
        Error Reduction  &  & 34.37\%         & 32.73\%    	& 18.28\%  \\ \hline
        % $\phi$-FNN             &  & 0.2585$\pm$ 0.0141          & 0.1819 $\pm$ 0.0026		& 0.1520  $\pm$ 0.0020	& 0.1413 $\pm$ 0.0013  \\
        % $\phi$          &  & 0.2285$\pm$ 0.0147         & 0.1392 $\pm$ 0.0080 	& 0.0898 $\pm$ 0.0046	& 0.0688 $\pm$ 0.0032  \\ \hline
    \end{tabular}
    \caption{\small SIF prediction for plate surface cracks in fatigue modeling.}
    \label{tab:crack}
    \end{table*}

\begin{figure}
	\centering
	\setlength\tabcolsep{0pt}
	\begin{tabular}[c]{cc}
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_5.pdf}
	\end{subfigure} &
    \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_5.pdf}
	\end{subfigure}
 \\
	\begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_10.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_10.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_20.pdf}
	\end{subfigure} &
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_20.pdf}
	\end{subfigure}
 \\
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-deeponet/darcy/final_best_deep_five_darcy_30.pdf}
 \caption{\small \textit{PPI-DONet: Darcy Flow}}\label{fig:darcy-dont-example}
	\end{subfigure} & 
 \begin{subfigure}[b]{0.48\textwidth}
		\centering
	\includegraphics[width=\textwidth]{./figs-fno/nl/final_best_five_nl_30.pdf}
 \caption{\small \textit{PPI-FNO: nonlinear diffusion}}\label{fig:nl-fno-example}
	\end{subfigure}
\end{tabular}
	\caption{\small Examples of the prediction and point-wise error of PPI-DONet and PPI-FNO on \textit{Darcy Flow} and \textit{nonlinear diffusion}, respectively.  From top to bottom, the models were trained with 5, 10, 20, 30 examples.}
 \vspace{-0.2in}
\end{figure}

\noindent\textbf{Predictive performance.} 
We reported the average relative $L_2$ error and the standard deviation (before and after incorporating the pseudo physics laws) in Table \ref{tb:all-pred-error}. The model trained with the learned physics laws (see \eqref{eq:fine-tune-loss}) is denoted as PPI-FNO or PPI-DONet, short for Pseudo Physics Informed FNO/DONet.
In all cases, with our pseudo physics informed approach , the prediction error of both FNO and DONet experiences a large reduction. For instance, across all training sizes in \textit{Darcy Flow} and \textit{nonlinear diffusion}, PPI-FNO reduces the relative $L_2$ error of the ordinary FNO by over 60\% and 93\%, respectively. In \textit{Darcy Flow} with training sizes 10 to 30, PPI-DONet reduces the error of the ordinary DONet by over 50\%. Even the minimum reduction across all cases achieves 14.12\% (PPI-DONet over DONet on \textit{Poisson} with training size 5). 

Together these results demonstrate the strong positive impact of the learned physics by our neural network model $\phi$ specified in Section \ref{sect:phi}. Although it remains opaque and non-interpretable, it encapsulates valuable knowledge that greatly enhances the performance of operator learning, in particular with limited data.

 
Next, we assessed the accuracy of the learned physics laws by examining the relative \(L_2\) error in predicting the source functions \(f\) from \(\phi\) (see \eqref{eq:pde-learn}). We tested on \textit{Darcy Flow}, \textit{nonlinear diffusion}, and \textit{Eikonal}. We compared a baseline method that removes the convolution layer of \(\phi\), leaving only the feed-forward layers. 
The average relative \(L_2\) error and standard deviation are reported in Table \ref{tb:utof}. It can be observed that in nearly every case, adding a convolution layer indeed significantly improves the accuracy of \(\phi\). This improvement might be attributed to the convolution layer's ability to integrate neighboring information and compensate for the error introduced by the numerical difference in approximating  the derivatives. We also experimented with multiple convolution layers, but the improvement was found to be marginal. 
%\begin{table}

\begin{wraptable}{r}{0.65\textwidth}
\small
\centering
\begin{subtable}{0.6\textwidth}
   \small
    \centering
    \begin{tabular}{ccc}
    \hline \textit{Benchmark} & FNN & Ours \\
    \hline
    Darcy Flow & 0.1819$\pm$0.0026 & \textbf{0.1392$\pm$ 0.0080}\\
    Nonlinear Diffusion & 0.0660$\pm$0.0069 & \textbf{0.0233$\pm$0.0005}\\
    Eikonal & 0.0144$\pm$0.0009 & \textbf{0.0108 $\pm$ 0.0006}\\
    \hline
    \end{tabular}
    \caption{\small Training size=10}
\end{subtable}
\begin{subtable}{0.6\textwidth}
   \small
    \centering
    \begin{tabular}{ccc}
    \hline \textit{Benchmark} & FNN & Ours \\
    \hline
    Darcy Flow & 0.1413$\pm$0.0013 & \textbf{0.0688$\pm$ 0.0032}\\
    Nonlinear Diffusion & 0.0463$\pm$0.0022 & \textbf{0.0163$\pm$0.0002}\\
    Eikonal & 0.0070$\pm$0.00005 & \textbf{0.0052 $\pm$ 0.0002}\\
    \hline
    \end{tabular}
    \caption{\small Training size=30}
\end{subtable}
\caption{\small Relative $L_2$ error of using the learned back-box PDE network~\eqref{eq:pde-learn} to predict the input function. } \label{tb:utof}
%\end{table}
\end{wraptable}

In addition, we also found the operator learning improvement is relatively \textit{robust} to the accuracy of our physics representation $\psi$. For instance, on \textit{Darcy Flow} with training size 5 and 10, the relative $L_2$ error of $\phi$ network is 0.2285 and 0.1392, which is significantly bigger than with training size 30  where the relative $L_2$ error is 0.0688. Yet the error reduction upon FNO (see Table~\ref{tab:darcy}) under all the three training sizes is above 60\%. The error reduction upon DONet is 40\% for training size 5 and over 50\% for training size 10 and 30. 
The results imply that even roughly capturing the underlying physics (with $\phi$) can substantially boost the operator learning performance.   


Third, we examined the learning behavior of our method, which conducts an iterative, alternatingly fine-tuning process. We employed one \textit{Darcy Flow}, one \textit{nonlinear diffusion} and one \textit{Eikonal} dataset, each with 30 examples. We show the test relative $L_2$ error along with the iterations in Fig. \ref{fig:fno-learning}, \ref{fig:donet-learning}, and Appendix Fig.~\ref{fig:fno-learning-eikonal} and Fig.~\ref{fig:donet-learning-eikonal}. As we can see, the predictive performance of our algorithm kept improving and tended to converge at last, affirming the efficacy of the learning process. 


\noindent\textbf{Point-wise prediction and point-wise error.} For a detailed assessment, we conducted a fine-grained evaluation by visualizing the predictions and point-wise errors made by each method. In Fig. \ref{fig:darcy-dont-example} and \ref{fig:nl-fno-example},  we showcased the predictions and point-wise errors using PPI-DONet for \textit{Darcy Flow}, PPI-FNO for \textit{nonlinear diffusion}, respectively. Additional examples of predictions and point-wise errors are provided in Fig. \ref{fig:ek-fno-example}, \ref{fig:ek-deeponet-example}, \ref{fig:darcy-fno-example}, and \ref{fig:nl-deeponet-example} in the Appendix.
 
It is evident that without the assistance of the pseudo physics laws learned by our method, the ordinary DONet and FNO frequently missed crucial local structures, sometimes even learning entirely incorrect structures.
For example, In Fig. \ref{fig:darcy-dont-example} the first row, DONet missed one mode, while in the second and third row of Fig. \ref{fig:darcy-dont-example}, DONet failed to capture all the local modes. 
After incorporating the learned physics, DONet (now denoted as PPI-DONet; see the third column) successfully captures all the local modes, including their shapes and positions. Although not all the details are exactly recovered, the point-wise error is substantially reduced, particularly in those high error regions of the ordinary DONet; see the fourth column of Fig. \ref{fig:darcy-dont-example}.

In another instance, as shown in Fig. \ref{fig:nl-fno-example}, where the ordinary FNO (second column) captured the global shape of the solution, but the mis-specification of many local details led to large point-wise errors across many regions (fourth column). In contrast, PPI-FNO (third column) not only identified the structures within the solution but also successfully recovered the details. As a result, the point-wise error (fifth column) was close to zero everywhere.

Additional instances can be found in Fig. \ref{fig:ek-fno-example}, the first three rows illustrate that ordinary FNO (trained with 5, 10, and 20 examples, respectively) estimates an entirely incorrect structure of the solution, indicating that the training data is insufficient for FNO to capture even the basic structure of the solution. In contrast, after fine-tuning with our learned physics laws from the same sparse data, PPI-FNO accurately figured out the solution structures and yielded a substantial reduction in point-wise error across nearly everywhere. The point-wise error became uniformly close to zero.
With 30 examples, the ordinary FNO was then able to capture the global structure of the solution, but the details in the bottom left, bottom right, and top right corners were incorrectly predicted. In comparison, PPI-FNO further recovered these details accurately. 




Collectively, these results demonstrate that the pseudo physics extracted by our method not only dramatically boosts the overall prediction accuracy but also better recovers the local structures and details of the solution.

\textbf{Real World Data Experiments.} \keyan {
We also tested our learning framework on a real-world dataset.%explaination of the crack dataset%
Since the real data will be much more complex than the previous physics PDE equations, we choose to use training examples from {400,500,600}. Following the experiment before, we still evaluated the real-world dataset on FNO and DONet(see Table~\ref{tab:crack}). These results also demonstrate our learning framework's positive feasibility and positive impact on real-world data.

}

\textbf{Ablation study.} \keyan {In addition, in the ablation study, we first verify the effectiveness of our PDE network $\phi$. Specifically, we tested on the Darcy-flow benchmark with alternative choices of $\phi$, including (1) a standard FNO where the input is $u$ and the output is $f$, and (2)  a pixel-wise MLP with the same number of layers and width as in our method, where the input is $u$ value at a grid point, and the output is the corresponding $f$ value at that grid point.  The relative $L_2$ error in predicting $f$ of each method is given(see Table~\ref{tb:phierror}). It shows that our design of $\phi$ with $u$ derivatives indeed outperforms other architectures, showing the effectiveness of learning a (black-box) PDE representation.

\begin{table*}[t]
\small
\centering
\begin{subtable}{\textwidth}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    FNO & 0.7229$\pm$0.0318 & 0.5759$\pm$ 0.0126 & 0.4257$\pm$ 0.0106 & 0.3160$\pm$ 0.0037\\
    MLP & 0.7169$\pm$0.0160 & 0.6598$\pm$ 0.0056 & 0.6464$\pm$ 0.0029 & 0.6277$\pm$ 0.0032\\
    Ours & \textbf{0.2285 $\pm$ 0.0147} & \textbf{0.1392 $\pm$ 0.0080} & \textbf{0.0898 $\pm$ 0.0046} & \textbf{0.0688 $\pm$ 0.0032}\\
    \hline
    \end{tabular}
    \caption{\small Error of predicting $f$}
\end{subtable}
\begin{subtable}{\textwidth}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    PPI-FNO with FNO as $\phi$ & 0.5853$\pm$0.0153 & 0.3871$\pm$ 0.0124 & 0.2613$\pm$ 0.0190 & 0.1629$\pm$ 0.0064\\
    PPI-FNO with MLP as $\phi$ & 0.7262$\pm$0.0920 & 0.5516$\pm$ 0.0699 & 0.4568$\pm$ 0.0857 & 0.3983$\pm$ 0.1051\\
    Standard FNO & 0.4915 $\pm$ 0.0210	& 0.3870 $\pm$ 0.0118   & 0.2783 $\pm$ 0.0212   & 0.1645 $\pm$ 0.0071\\
    Ours & \textbf{0.1716 $\pm$ 0.0048} & \textbf{0.0956 $\pm$ 0.0084} & \textbf{0.0680 $\pm$ 0.0031} & \textbf{0.0642 $\pm$ 0.0010}\\
    \hline
    \end{tabular}
    \caption{\small PPI training of operator $f\rightarrow u$, test error}
\end{subtable}
\caption{\small The relative $L_2$ error in predicting $f$ of each method } \label{tb:phierror}
%\end{table}
\end{table*}

We also investigate the performance of $\phi$ regarding the choice of derivatives. Specifically, we tested our PPI-FNO on the Darcy-flow cases and varied the order of derivatives up to 0, 1, 2, and 3. Note in our paper, we used the order up to 2(see Table~\ref{tb:ordererror}). We observed that although the accuracy of $\phi$ with derivatives up to the third order is slightly better than with derivatives up to the second order, the best operator learning performance was still achieved using derivatives up to the second order. This might be because higher-order derivative information can cause overfitting in the PDE network $\phi$ to a certain degree. Such higher-order information may not be critical to the actual mechanism of the physical system and can therefore impede the improvement of operator learning performance.

\begin{table*}[t]
\small
\centering
\begin{subtable}{\textwidth}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    order 0 & 0.7126$\pm$0.0131 & 0.5733$\pm$0.0208 & 0.4812$\pm$0.0399 & 0.3445$\pm$0.0182 \\ 
    order $\le$ 1 & 0.2926$\pm$0.0118 & 0.2006$\pm$0.0047 & 0.1379$\pm$0.0051 & 0.1084$\pm$0.0053 \\ 
    order $\le 2$ (ours) & \textbf{0.2285$\pm$0.0147} & \textbf{0.1392$\pm$0.0080} & \textbf{0.0898$\pm$0.0046} & \textbf{0.0688$\pm$0.0032} \\ 
    order $\le 3$ & 0.2058$\pm$0.0192 & 0.1123$\pm$0.0039 & 0.0712$\pm$0.0021 & 0.0585$\pm$0.0030 \\ 
    \hline
    \end{tabular}
    \caption{\small The error of black-box PDE network $\phi$ (i.e., $u\rightarrow f$)}
\end{subtable}
\begin{subtable}{\textwidth}
   \small
    \centering
    \begin{tabular}{ccccc}
    \hline \textit{Training size} & 5 & 10 & 20 & 30 \\
    \hline
    order 0 & 0.6352$\pm$0.0673 & 0.4523$\pm$0.0621 & 0.3570$\pm$0.0658 & 0.2737$\pm$0.0643 \\ 
    order $\le$ 1 & 0.3386$\pm$0.0259 & 0.2161$\pm$0.0083 & 0.1645$\pm$0.0114 & 0.1197$\pm$0.0132 \\ 
    order $\le 2$ (ours) & \textbf{0.1716$\pm$0.0048} & \textbf{0.0956$\pm$0.0084} & \textbf{0.0680$\pm$0.0031} & \textbf{0.0642$\pm$0.0010} \\ 
    order $\le 3$ & 0.2959$\pm$0.0381 & 0.1719$\pm$0.0213 & 0.1193$\pm$0.0158 & 0.0828$\pm$0.0054 \\ 
    \hline
    \end{tabular}
    \caption{\small The corresponding operator learning error (i.e., $f \rightarrow u$)}
\end{subtable}
\caption{\small The relative $L_2$ error in predicting $f$ of each order } \label{tb:ordererror}
%\end{table}
\end{table*}
}


Finally, we examined the effect of the weight $\lambda$ of our ``pseudo physics''; see \eqref{eq:ppi-loss}. To this end, we used one \textit{Darcy Flow} dataset and \textit{nonlinear diffusion} dataset with training size 30. We varied $\lambda$ from $[0.5, 10^2]$, and run PPI-FNO and PPI-DONet on \textit{Darcy Flow} and \textit{nonlinear diffusion}, respectively. As shown in Fig.~\ref{fig:lambda-fno} and~\ref{fig:lam-donet}, we can see that across a wide range of $\lambda$ values, PPI-FNO and PPI-DONet can consistently outperform the standard FNO and DONet respectively by a large margin. However, the choice $\lambda$ does have a significant influence on the operator learning performance, and the best choice is often in between. In Appendix Fig.~\ref{fig:lambda-fno-eikonal} and~\ref{fig:lam-donet-eikonal}, we show results on \textit{Ekonal}, which we obtain similar observations. 


\textbf{Time and Memory Analysis.} \keyan {The alternative training process requires more training cycles than standard NO, but the time complexity only grows linearly with the number of alternating iterations, not quadratically or exponentially. We believe this is reasonable and practically acceptable.
For memory analysis, Our "pseudo" physics network $\phi$ is very small as compared to standard neural operators (NO) --- $\phi$ is simply a pixel-wise MLP coupled with one convolution filter, resulting in a marginal increase in memory cost. The below shows under training size = 30, the parameter count of FNO, DONet and their pseudo physics informed version. On average, PPI-FNO increases the number of parameters over FNO by 1.29 while PPI-DONet by DONet increases by 1.89.(see Table~\ref{tb:paracounts})

\begin{table*}[t]
\small
\centering
\begin{tabular}{ccccc}
\hline
\textbf{Parameter count} & \textbf{FNO} & \textbf{PPI-FNO (increase)} & \textbf{DONet} & \textbf{PPI-DONet (increase)} \\ 
\hline
Darcy-flow & 1188353 & 1229476 (+3.46\%) & 2084704 & 2125827 (+1.97\%) \\ 
Nonlinear-diffusion & 1188353 & 1197220 (+0.75\%) & 824501 & 833368 (+1.08\%) \\ 
Eikonal & 1188353 & 1197220 (+0.75\%) & 824501 & 833368 (+1.08\%) \\ 
Poisson & 1188353 & 1197220 (+0.75\%) & 824501 & 833368 (+1.08\%) \\ 
Advection & 1188353 & 1197220 (+0.75\%) & 210101 & 218968 (+4.22\%) \\ 
\hline
\end{tabular}
\caption{\small Parameter counts for FNO and DONet with PPI variations across different problems } \label{tb:paracounts}
\end{table*}
}