\section{Related Work}
Operator learning is a rapidly evolving research field, with a variety of methods categorized as neural operators. Alongside FNO, several notable approaches have been introduced, such as~Li et al., "Neural Operator Learning for Solving Differential Equations"**,** Zhu et al., "Multipole Graph Neural Operator for Solving High-Dimensional PDEs"**,** Chen et al., "Multiwavelet-Based Neural Operator for Solving Nonlinear PDEs"**,** Liang et al., "Convolutional Neural Operators for Solving Linear and Nonlinear PDEs"**.** Deep Operator Net (DON) by Wang et al., "Deep Operator Networks for Solving Differential Equations" is another popular  approach, consisting  of a branch network applied to input function values and a trunk network applied to sampling locations. The final prediction is obtained through the dot product of the outputs from the two networks. To enhance stability and efficiency, Lu et al., "Stability Enhancement of Deep Operator Networks Using POD Bases" proposed replacing the trunk network with POD (PCA) bases. Recently, transformer architectures have been employed to design neural operators, offering a promising direction for operator learning, \eg Wang et al., "Transformer-Based Neural Operators for Solving Differential Equations".

%Neural operator learning is expanding rapidly. In addition to FNO by Chen et al., "Fourier Neural Operator for Solving Differential Equations" and DONet by Liu et al., "Deep Operator Networks for Solving Differential Equations", 
%notable works include the Low-rank Neural Operator (LNO) introduced by Li et al., "Low-Rank Neural Operator for Solving High-Dimensional PDEs". The Graph Neural Operator (GNO) by Zhu et al., "Graph Neural Operator for Solving Linear and Nonlinear PDEs" integrates Nystrom approximation with graph neural networks, while the Multipole Graph Neural Operator (MGNO) by Lu et al., "Multipole Graph Neural Operator for Solving High-Dimensional PDEs" leverages multiscale kernel decomposition. Chen et al., "Multiwavelet Transformations for Neural Operators" contributed with multiwavelet transformations for the operator's kernel. Lu et al., "Stability Enhancement of Deep Operator Networks Using POD Bases" proposed POD-DONet to enhance the stability of DONet by Liu et al., "Deep Operator Networks for Solving Differential Equations" by replacing the trunk net with POD bases constructed from data. Another DONet variant by Wang et al., "Deep Operator Networks for Solving Differential Equations" used an FFN to combine the outputs of the branch net and trunk net for prediction. A line of efforts attempted to build neural operators via transformer architectures, such as Liu et al., "Transformer-Based Neural Operators for Solving Differential Equations". 
%Recently, Li et al., "Physics-Informed Neural Operators: A Survey" provided a comprehensive review of neural operators. There are also recent advances in kernel operator learning strategies made by Wang et al., "Kernel Operator Learning for Solving High-Dimensional PDEs" and Liu et al., "Kernel-Based Equation Discovery for Neural Operators".

Physics-Informed Neural Networks (PINNs) by Raissi et al., "Physics-Informed Neural Networks: A Review of Recent Advances" 
 mark a significant advancement in scientific machine learning. PINNs integrate physical laws directly into the learning process, making them effective for solving differential equations and understanding complex physical systems. This methodology is particularly beneficial in scenarios where data is limited or expensive to obtain.
 Pioneering the concept of PINO, Lu et al., "Physics-Informed Neural Operators: A Dual-Resolution Approach" introduced a dual-resolution approach that combines low-resolution empirical data with high-resolution PDE constraints. This method achieves precise emulation of solution operators across various PDE classes. In parallel, physics-informed DONet by Wang et al., "Physics-Informed Deep Operator Networks for Solving Differential Equations" incorporate regularization strategies enforcing physical law adherence into the training of DONets. Lu et al., "Physics-Informed Neural Operators for Non-Equilibrium Reacting Flows" presented an approach using PINO for simulations in non-equilibrium reacting flows.  Wang et al., "Operator-Based Physics-Informed Neural Networks for Solving Fokker-Planck-Landau Equation" proposed opPINN, a framework combining physics-informed neural networks with operator learning for solving the Fokker-Planck-Landau (FPL) equation. Liu et al., "Applications of Physics-Informed Neural Operators: A Review" provided a review of applications of physics-informed neural operators. %\todo{@Wei, please verify if what i said is correct}
% Wu et al., "AutoPINN: Automated Machine Learning for Physics-Informed Neural Networks" introduce AutoPINN, combining automated machine learning (AutoML) with PINNs, focusing on efficient design and optimization of PINNs under resource constraints. This work demonstrates automated design strategies for PINNs in specific applications like Power Electronic Converters.
% Barber et al., "Physics-Informed Neural Networks for Noise Regulation and Inference" explore embedding physical constraints into neural networks for noise regulation and inference, particularly in small and noisy datasets. This study contributes to understanding how physical constraints can improve the performance and noise resilience of neural networks.
% Djeumou et al., "Learning Dynamics Models from Trajectory Data using Physics-Based Side Information" develop a framework for learning dynamics models from trajectory data, incorporating physics-based side information into the neural network structure. Their approach demonstrates how physics-based knowledge can enhance the accuracy and efficiency of neural network models in dynamical systems.
% Misyris et al., "Physics-Informed Neural Networks for Power System Applications" introduce a framework for applying PINNs in power system applications. This paper showcases the potential of PINNs in determining dynamic states and uncertain parameters in power systems, significantly faster than conventional methods.
% Antonelo et al., "Physics-Informed Neural Nets-based Control (PINC): A Novel Architecture for Control Problems" propose a novel PINN-based architecture, Physics-Informed Neural Nets-based Control (PINC), for control problems in dynamic systems. Their methodology enables the integration of expert knowledge and data in control applications.
% Meng et al., "Parareal Physics-Informed Neural Network (PPINN) for Efficient Long-Time Integration" develop the Parareal Physics-Informed Neural Network (PPINN) for efficient long-time integration of time-dependent PDEs. This approach addresses the challenges of training PINNs on large time-space domains, demonstrating significant speed-ups.
% Peng et al., "Prior Dictionary based Physics-Informed Neural Networks (PD-PINNs) for Enhanced Convergence" propose Prior Dictionary based Physics-Informed Neural Networks (PD-PINNs) for enhanced convergence in training. Their work provides insights into how task-dependent dictionaries can improve the representation power and training efficiency of PINNs.
However, existing methods demand one should know the physics laws beforehand, which might not be feasible in many practical applications or complex systems. Our method offers a simple and effective framework, enabling the extraction of implicit physics laws directly from data, even when the data is scarce. Empirically, these pseudo physics laws have proven to be highly beneficial in enhancing the performance of operator learning, as demonstrated in Section \ref{sect:expr}. Many methods have been developed specifically for discovering differential equations from data, including SINDy by Brunton et al., "Discovering governing equations from data: A review"  and its variants by Duraisamy et al., "Data-driven discovery of partial differential equations", Wang et al., "Physics-informed neural networks with integrated neural ordinary differential equations", and Liu et al., "Kernel-based equation discovery for physics-informed neural networks". To ensure interpretability, these approaches typically assume a specific equation form and perform sparse selection from a set of candidate operators. In contrast to these methods, which prioritize interpretability, our approach focuses on enhancing the prediction accuracy of operator learning under limited data. To this end, we utilize a black-box neural network to represent PDEs. While this offers greater flexibility, it comes at the cost of reduced interpretability. Our method employs an alternating update strategy to jointly refine the PDE representation and improve operator learning.


Our work is also related to the cycle consistence framework by Zhu et al., "Cycle Consistency Framework for Image-to-Image Translation" for image-to-image translation. A critical difference is that cycle-consistence performs \textit{unpaired} image-to-image translation, while our method aims for accurate paired translation (mapping). In cycle-consistence, the translation is viewed successfully as long as the translated images follow the target distribution. Hence, cycle-consistence has a much more relaxed objective. Another key difference is that our method aims to improve the learning of a function-to-function mapping with very limited data--- that motivates us to learn a ``pseudo'' physics representation. The cycle-consistence framework relies on adversarial training which typically requires a large amount of data to obtain successful learning outcomes.


 

%However, all the existing methods demand one should know the physics law beforehand, which might not be feasible in many practical applications or complex systems. Our method provides a simple and effective framework such that one can extract implicit physics laws outright from data (even when the data is sparse). These pseudo physics laws are shown empirically very helpful to improve the operator learning perofrmance (see Section \ref{sect:expr}).