\section{Related Works}
\subsection{SSL for Classification}
Data scarcity is a critical problem for various tasks.
More specifically, the problem occurs when there are only
a few labeled data even though there are tons of unlabeled data.
Generating labeled data is costly,
making research in these low-resource environments crucial.
Semi-supervised learning~(SSL) offers
an effective solution in such scenarios____.

One of the popularly used SSL techniques,
self-training is a learning mechanism that trains the student model with a few-shot labeled dataset____
and then subsequently acts as a teacher model for generating pseudo-labels____.
Pseudo-labels are judged based on the predictions of the model for a given unlabeled data.
Co-training____ is also the successful SSL mechanism
that simultaneously employs two networks.
Jointmatch____ utilizes cross-labeling, inspired by co-training, that uses an additional loss based on pseudo-labels for more reliability
instead of augmenting pseudo-labels to the initial labeled dataset for additional training.
While this approach has been effective to some degree,
it lacks reliability as we cannot guarantee that the model generates `correct' pseudo-labels.
Recent approaches have incorporated symbolic modules
to enhance the reliability of pseudo-label generations or data augmentation____.

Likewise, data augmentation is also one of the fundamental methods that
is effective in the low-resource setting.
Data augmentation generates artificial data from the original dataset
without changing their labels.
Conventional data augmentations are synonym replacements, word insertion or deletion____
More advanced methods involve Back-Translation____ and these days,
LLMs have gained popularity in generating various data but with accurate labels.
We utilize these insights to develop \texttt{TCProF},
an SSL framework for predicting code time complexity in low-resource environments.


\subsection{Code Time Complexity Prediction}
Computation of code time complexity has long been a theoretically undecidable problem
whereas classifying the code time complexity is a recently emerged problem.
____ first proposed this task and presented a labeled dataset with
five time complexity classes.
They propose a dataset named CorCoD,
composed of Java codes with $O(1)$, $O(\log N)$, $O(N)$, $O(N\log N)$ and $O(N^2)$ time complexity classes
and experimental results on the time complexity classification with baseline neural models.

Similar to CorCoD,
CODAIT\footnote{https://community.ibm.com/community/user/ai-datascience/blogs/sepideh-seifzadeh1/2021/10/05/ai-for-code-predict-code-complexity-using-ibms-cod}
tried to create good code embeddings by capturing manual features such 
as number of loops and breaks, and utilized graph-based representations
for predicting time complexities.
Afterward, ____ proposed TasTy, consisting of Java and Python data, and ____
proposed CodeComplex, which consists of also Java and Python data with additional labels.
CodeComplex consists of seven different time complexity classes,
$O(N^3)$ and $O(2^N)$ in addition to those of CorCoD.

While time complexity prediction has been explored in these datasets,
the scarcity of labeled data remains a significant challenge.
Unlike runtime-based datasets from online judge platforms,
which can be influenced by hardware, input distributions,
and implementation-specific optimizations____,
these datasets provide explicit theoretical complexity labels for the code snippets.
However, annotating such datasets requires expert knowledge, making them inherently low-resource.
We propose \texttt{TCProF}, the SSL framework designed to
alleviate this data scarcity challenge and enhance the accuracy of time complexity prediction.