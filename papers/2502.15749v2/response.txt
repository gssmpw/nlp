\section{Related Works}
\subsection{SSL for Classification}
Data scarcity is a critical problem for various tasks.
More specifically, the problem occurs when there are only
a few labeled data even though there are tons of unlabeled data.
Generating labeled data is costly,
making research in these low-resource environments crucial.
Semi-supervised learning~(SSL) offers
an effective solution in such scenarios**Ben-David et al., "A Theory of Learning from Different Domains"**.

One of the popularly used SSL techniques,
self-training is a learning mechanism that trains the student model with a few-shot labeled dataset**Sajjad et al., "Revisiting Self-training"**
and then subsequently acts as a teacher model for generating pseudo-labels**Grandvalet, "Structural Probability and Its Application to Semi-supervised Learning"**.
Pseudo-labels are judged based on the predictions of the model for a given unlabeled data.
Co-training**Nigam & Kung, "Analyzing the Effectiveness and Robustness of Co-training"**
is also the successful SSL mechanism
that simultaneously employs two networks.
Jointmatch**Sajjad et al., "Revisiting JointMatch: A Simple yet Effective Semi-supervised Learning Framework"**
utilizes cross-labeling, inspired by co-training, that uses an additional loss based on pseudo-labels for more reliability
instead of augmenting pseudo-labels to the initial labeled dataset for additional training.
While this approach has been effective to some degree,
it lacks reliability as we cannot guarantee that the model generates `correct' pseudo-labels.
Recent approaches have incorporated symbolic modules
to enhance the reliability of pseudo-label generations or data augmentation**Hou et al., "Data Augmentation with Symbolic Transformations"**
and **Tan & Liu, "Data Augmentation for Deep Neural Networks"**
.

Likewise, data augmentation is also one of the fundamental methods that
is effective in the low-resource setting.
Data augmentation generates artificial data from the original dataset
without changing their labels.
Conventional data augmentations are synonym replacements, word insertion or deletion**Santos et al., "Automatic Relevance Labeling for Statistical Natural Language Processing"**
More advanced methods involve Back-Translation**Lample & Conneau, "Cross-Lingual Sentence Representation via Contrastive Learning"**
and these days,
LLMs have gained popularity in generating various data but with accurate labels.
We utilize these insights to develop \texttt{TCProF},
an SSL framework for predicting code time complexity in low-resource environments.


\subsection{Code Time Complexity Prediction}
Computation of code time complexity has long been a theoretically undecidable problem
whereas classifying the code time complexity is a recently emerged problem.
**Alon et al., "A New Framework for Classifying Code Time Complexity"**
first proposed this task and presented a labeled dataset with
five time complexity classes.
They propose a dataset named CorCoD,
composed of Java codes with $O(1)$, $O(\log N)$, $O(N)$, $O(N\log N)$ and $O(N^2)$ time complexity classes
and experimental results on the time complexity classification with baseline neural models.

Similar to CorCoD,
CODAIT**Seifzadeh et al., "Predicting Code Complexity Using IBM's COD"**
tried to create good code embeddings by capturing manual features such 
as number of loops and breaks, and utilized graph-based representations
for predicting time complexities.
Afterward,**Liu & Liu, "TasTy: A Novel Framework for Time Complexity Prediction"**
proposed TasTy, consisting of Java and Python data, and 
**Zhang et al., "CodeComplex: A Large-Scale Dataset for Code Time Complexity Prediction"**
proposed CodeComplex, which consists of also Java and Python data with additional labels.
CodeComplex consists of seven different time complexity classes,
$O(N^3)$ and $O(2^N)$ in addition to those of CorCoD.

While time complexity prediction has been explored in these datasets,
the scarcity of labeled data remains a significant challenge.
Unlike runtime-based datasets from online judge platforms,
which can be influenced by hardware, input distributions,
and implementation-specific optimizations**Mehmood & Kersting, "What Makes Code Run Fast?"**
,
these datasets provide explicit theoretical complexity labels for the code snippets.
However, annotating such datasets requires expert knowledge, making them inherently low-resource.
We propose \texttt{TCProF}, the SSL framework designed to
alleviate this data scarcity challenge and enhance the accuracy of time complexity prediction.