\section{Experimental Details}
\label{app:apexp}
\subsection{Open-ended Task}
\label{subsec:open}
\paragraph{Benchmark} We employed the benchmark proposed by Voyager~\citep{wang2023voyager}, using Minecraft as the experimental platform. Minecraft provides a sandbox environment where players gather resources and craft tools to achieve various goals. The simulation is built on MineDojo~\citep{fan2022minedojo} and uses Mineflayer~\citep{PrismarineJS2013} JavaScript APIs for motor control. 

\paragraph{Baselines}
We conducted a comprehensive comparison with four baselines. Except for Voyager, these methods were originally designed for NLP tasks without embodiment. Therefore, we had to reinterpret and adapt them for execution within the MineDojo environment, ensuring compatibility with the specific requirements of our experimental setup.
\begin{itemize}
    \item \textbf{ReAct:} ReAct~\citep{yao2022react} uses chain-of-thought prompting [46] by generating both reasoning traces and action
plans with LLMs. We provide it with our environment feedback and the agent states as observations.
    \item \textbf{Reflexion:} Reflexion~\citep{shinn2023reflexion} is built on top of ReAct~\citep{yao2022react}with self-reflection to infer more intuitive future actions.
    \item \textbf{AutoGPT:} AutoGPT~\citep{richardssignificant} is a popular software tool that automates NLP tasks by decomposing a high-level
goal into multiple subgoals and executing them in a ReAct-style loop. We re-implement AutoGPT by using GPT-4O to do task decomposition and provide it with the agent states, environment feedback,
and execution errors as observations for subgoal execution
We provide it with execution errors and our self-verification module.
    \item \textbf{Voyager:} Voyager~\citep{wang2023voyager} is a system that integrates an automated curriculum, a scalable skill library, and an iterative prompting framework based on environmental feedback to explore, store, and accumulate skill library within the Minecraft environment.
\end{itemize}


\paragraph{Metric}
The evaluation metric is based on the number of iterations required to progress through tool upgrades, from wooden to stone, iron, and finally diamond tools. Each execution of code is considered one iteration.

\paragraph{Model}
We leverage GPT-4o for text completion, along with the text-embedding-ada-002 API for text embedding. We set all temperatures to
0 except for the automatic curriculum, which uses temperature = 0.1 to encourage task diversity. 

\paragraph{Setting}
We set the maximum number of iterations to 160. For both \ours\ and Voyager, all agents are controlled by GPT-4o, with the number of tools retrieved per iteration set to 5. To ensure a fairer comparison, we removed the Tool Requirement Stage and bug-free checks in \ours\ , and allowed a maximum of 3 self-checks per iteration.

\paragraph{Item Types and Levels}
In the Minecraft task, there are different types and levels of items. Diamond tools are the highest level, and rare items such as golden apples also exist. High-level tools require some lower-level items to craft. Table \ref{tab:toollist} lists the key items in the Minecraft task.
\begingroup
\begin{table}[H]
\caption{List of item types and levels in the Minecraft task.}
\label{tab:toollist}
\vskip -0.1in
\setlength{\tabcolsep}{10pt} % 调整列间距
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|c|c}
\toprule
\textnormal{\textbf{Category}} & \textnormal{\textbf{level}} & \textnormal{\textbf{Items}} \\
\midrule         
\midrule
\multirow{4}{*}{\multirow{3}{*}{\normalfont Tools}} 
              & \normalfont Wooden Tools & \normalfont Wooden\_Shovel,Wooden\_Pickaxe,Wooden\_Axe,Wooden\_Hoe,Wooden\_Sword \\
              \cmidrule{2-3}
              & \normalfont Stone Tools &\normalfont stone\_pickaxe, stone\_shovel,Stone\_Axe,Stone\_Hoe,Stone\_Sword   \\
              \cmidrule{2-3}
              & \normalfont Iron Tools &\normalfont iron\_pickaxe, iron\_axe, iron\_sword, iron\_shovel, iron\_hoe    \\
              \cmidrule{2-3}
              & \normalfont Diamond Tools &\normalfont diamond\_pickaxe, diamond\_sword, diamond\_axe, diamond\_shovel    \\
             
\midrule
\multirow{2}{*}{\multirow{1}{*}{\normalfont  Armor}} 
              & \normalfont Iron Armor &\normalfont iron\_chestplate, iron\_helmet, iron\_leggings  \\
              \cmidrule{2-3}
              & \normalfont Diamond Armor &\normalfont diamond\_chestplate, diamond\_helmet, diamond\_leggings, diamond\_boots     \\

\midrule
\multirow{3}{*}{\multirow{2}{*}{\normalfont  Food}} 
              & \normalfont Raw Food &\normalfont chicken, mutton, porkchop, rabbit, raw\_rabbit, spider\_eye, bone  \\
              \cmidrule{2-3}
              & \normalfont Cooked Food &\normalfont cooked\_beef, cooked\_chicken, cooked\_mutton, cooked\_porkchop, cooked rabbit  \\
              \cmidrule{2-3}
              & \normalfont Advanced Food &\normalfont golden apple    \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\endgroup


\subsection{Agent Task}
\label{subsec:agent}
\paragraph{Benchmark}
We conducted experiments on two types of agent tasks, demonstrating {\ours}'s capabilities in both game-related and data science tasks.
\begin{itemize}
     \item \textbf{TextCraft:} We evaluate {\ours} on the TextCraft dataset~\citep{futuyma1988evolution}, which challenges agents to craft Minecraft items in a text-only environment~\citep{cote2019textworld}. Each task instance provides a goal and a sequence of crafting commands, which include distractors. We use depth-2 splits for testing and reserve a subset of depth-1 recipes for development, resulting in a 99/77 train/test split.
    \item \textbf{InfiAgent-DABench:} We also test {\ours} on the InfiAgent-DABench benchmark~\citep{hu2024infiagent}, which evaluates LLM-based agents on end-to-end data analysis tasks. This benchmark consists of 257 questions across 52 CSV files, with each question corresponding to a unique CSV file. Agents are required to generate code to analyze data and produce the specified output format. We randomly selected 20 CSV files and their associated question-answer pairs as training data, resulting in a train/test split of 98/159 instances.
\end{itemize}

\paragraph{Baselines}
We compare \ours\ with three methods described below.
\begin{itemize}
     \item \textbf{ReAct:} In this setting, we employ the executor to interact iteratively with the environment, adopting the think-act-observe prompting style from ReAct~\citep{yao2022react}.
     \item \textbf{Plan-Execution:} In contrast, the Plan-and-Execute approach~\citep{shridhar2023art, yang2023intercode} generates a plan upfront and assigns each sub-task to the executor. To ensure each step is executable without further decomposition, we provide new prompts with more detailed planning instructions.
    \item \textbf{Reflexion:} In the Reflection setting~\citep{shinn2023reflexion}, the agent engages in self-reflection after each step, drawing on environmental feedback and exploration history. 
\end{itemize}

\paragraph{Metric} 
The most practically important aspect of the solutions is correctness. For Textcraft, we verify whether the agent’s inventory contains the goal item. For DABench, we check if the agent’s final answer matches the ground truth.

\paragraph{Model}
During training, we use GPT-4o to construct the tool library with a temperature setting of 0. In the testing phase, we conduct a comprehensive comparison of various open-source and closed-source models. The open-source models include \textit{Qwen2.5-7B-Instruct, Qwen-Coder-7B-Instruct, Qwen2.5-14B-Instruct, Deepseeker-Coder-6.7B-Instruct, and Deepseeker-Coder-33B-Instruct}, while the closed-source models primarily include \textit{gpt-3.5-turbo-1106} and \textit{Claude-3-haiku}. During testing, the temperature is set to 0.3, and each experiment is repeated 3 times, with the average result reported.

\paragraph{Setting} 
For ReAct, Reflexion, and \ours\ , the maximum number of steps is set to 20. For Plan-Execution, the maximum number of steps for each sub-task is set to 8. In \ours\ , the number of tools retrieved during testing is limited to 3.



\subsection{Single-turn Code Task}
\label{subsec:code}
\paragraph{Benchmark}
To further explore {\ours}'s potential, we evaluated it on single-turn code generation tasks spanning mathematical reasoning, date comprehension, and tabular reasoning:
 \begin{itemize}
     \item \textbf{MATH:} We used a subset of the MATH dataset~\citep{hendrycks2021measuring}, focusing on 405 level-4 and level-5 algebra problems (MATH contains 5 levels of difficulty) that require textual understanding and advanced reasoning. We randomly selected 200 examples from the test set of the MATH dataset to construct the tool network, resulting in a train/test split of 200/405.
     \item \textbf{Date:} We use the date understanding task from BigBenchHard~\citep{srivastava2022beyond}, which consists of short word problems requiring date understanding. We follow the data splits provided by REGAL\citep{stengel2024regal}, resulting in a train/test split of 66/180.
     \item \textbf{TabMWP:} We further extend our general experiments on MATH by testing on TabMWP~\citep{grand2023learning}, a tabular reasoning dataset consisting of math word problems about tabular data. Based on the CRAFT~\citep{yuan2023craft} splits, we selected 470 problems from levels 7 and 8 (TabMWP contains 8 levels) from the 1,000 test examples. Additionally, we randomly selected 200 examples from the TabMWP training set, resulting in a train/test split of 200/470.
\end{itemize}

\paragraph{Baselines}
For these tasks, we use Programs of Thoughts (PoT)~\citep{chen2022program} and other existing tool-making methods as baselines for comparison.

\begin{itemize}
    \item \textbf{PoT:} The LLM utilizes a program to reason through the problem step by step~\citep{chen2022program}.
   \item \textbf{LATM:} LATM~\citep{cai2023large} samples 3 examples from the training set and create a tool for the task, which is further verified by 3 samples from the validation set. The created tool is then applied to all test cases.
    \item \textbf{CREATOR:} CREATOR~\citep{qian2023creator} disentangle planning (tool making) from execution, enabling Large Language Models (LLMs) to autonomously create a specific tool for each test case during inference.
     \item \textbf{CRAFT:} CRAFT~\citep{yuan2023craft} constructs task-specific toolsets by generating a tool for each training example. During testing, it utilizes a tool retrieval module and a reasoning process akin to CREATOR, generating a function first and then producing the corresponding invocation code. 
      % \item \textbf{Trove:} Trove~\citep{wang2024trove} introduces a training-free method based on self-consistency, where LMs interact with the toolbox through three modes—IMPORT, SKIP, and CREATE. Each mode is executed K times, and from the 3K responses, the function from the most consistent and optimal response is added to the toolbox.
      \item \textbf{REGAL:} During training, REGAL~\citep{stengel2024regal} refines primitive programs by extracting functions. In the testing phase, it retrieves both tools and refactored programs—comprising original and refactored versions—to generate a program that effectively solves the task. 
\end{itemize}
\paragraph{Metric}
We use correctness as the evaluation metric, measuring whether the execution outcome of the solution program exactly matches the ground-truth answer(s).
\paragraph{Model}
The models for the single-turn code generation task are the same as those used for the Agent Task, as presented in Section \ref{subsec:agent}.
\paragraph{Setting}
To ensure a fair comparison, we make slight adjustments to each method. For all methods, we allow up to 3 times for format checking and correction, as small models may not always follow the required output format. For PoT, we use 6 fixed examples of basic tool usage as few-shot. CREATOR employs the rectifying process, while for CRAFT, we use the same training set as our method and construct the tool library with GPT-4o, retrieving 3 tools during testing. For Regal, we use PoT along with GPT-4o to obtain ground-truth code, select the correct program, and have GPT-4o reconstruct it. To maintain fairness in tool generation quality, we standardize the few-shot examples of basic tools and retrieve 3 tools, along with 3 usage examples from the current tool library, avoiding errors from pruned tools. For our method, we train with GPT-4o, retrieving 3 tools and their corresponding usage examples during testing, while fixing the basic tool few-shot examples to 3, ensuring consistency with PoT’s total few-shot count.