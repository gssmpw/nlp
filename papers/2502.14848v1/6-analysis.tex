\section{Analysis}
\label{sec:aly} 
\subsection{How Does \ours\ Adapt to Unseen Tasks?}
% \input{table-zero_shot}
To evaluate the generalizability of \ours\ and the effectiveness of the constructed tool graph, we clear the agent's inventory, reset the world to a new instance, and assign previously unseen tasks in Minecraft. The results are summarized in Table \ref{tab:zero_shot} and Figure \ref{fig:unseen_task}.

\input{table-zero_shot}

In comparison to Voyager, \ours\ completes tasks 2.2× faster on average. Moreover, when compared to our framework without a tool graph, \ours\ is still 1.8× faster, demonstrating the critical role of the tool graph in enhancing performance. This performance boost highlights the adaptability of \ours\ in handling unseen tasks. By facilitating inter-tool invocation, the tool graph incorporates more comprehensive and generalizable knowledge compared to Voyager’s tool library. This enhanced structure enables \ours\ to generalize across unseen tasks, reinforcing its robustness and versatility in new environments.


\subsection{How Does the Tool Graph Evolve Adaptively?}
% As shown in Figure \ref{fig:evlove}, the dynamic evolution of tool networks illustrates the ongoing optimization of their hierarchical structure during training. Initially focused on basic tools and simple invocation relationships, the network starts with sparse, low-level abstractions. As task complexity increases, tool reuse intensifies, with frequently used tools becoming key intermediaries between layers.

% The distribution of tools across hierarchical levels, as shown in Figure \ref{fig:tools}, further demonstrates the tool graph's adaptability and generalization across tasks. In tasks like MATH and TabMWP, where Python libraries or built-in functions suffice, the tool network remains shallow, with more tools at lower levels (e.g., 51.5\% of second-level tools in MATH's tool graph). In contrast, tasks with clear foundational tools, such as Minecraft and Textcraft, or those with specific application contexts like DABench, evolve deeper, multi-layer tool graphs. For instance, Textcraft evolves a 7-layer network, while most others evolve 5 layers. These relationships highlight how the tool graph facilitates deeper feature extraction, underscoring its flexibility and effectiveness in constructing adaptable multi-task, multi-level tool libraries.

As shown in Figure \ref{fig:evlove}, the tool graph evolves dynamically, optimizing its hierarchical structure during training. Initially, it focuses on basic tools (Table \ref{tab:basictool}) and simple relationships, starting with sparse, low-level abstractions. As task complexity increases, tool reuse grows, with frequently used tools becoming key intermediaries.

Figure \ref{fig:tools} illustrates \ours\ adaptive evolution across tasks. For tasks like MATH and TabMWP, which rely on Python libraries, the tool graph remains shallow, with most tools concentrated at lower levels (e.g., 51.5\% of second-level tools in MATH). In contrast, domain-specific tasks like Minecraft and Textcraft lead to deeper, multi-layered graphs, with Textcraft evolving into a 7-layer graph. As the number of layers increases, our higher-level tools save more operations for the same functionality. These patterns highlight the tool graph’s adaptability to task complexity, enabling the extraction of deeper features and the construction of versatile, multi-level tool libraries.


\subsection{How Does the Tool Graph Compare to Other Tool Libraries in Close-Ended TaskS?}
\label{subsec:compare}
% \input{table-5}
% Our tool graph framework demonstrates clear superiority over alternative methods in toolset construction, complexity management, and performance enhancement. As shown in Table \ref{tab:math_tabmwp_date}, it strikes an optimal balance among tool complexity (ops), library size (lib), and average performance improvement (Avg In.) compared to the baselines reported in Table \ref{tab:Agent-Ended and signal Task}.
Our tool graph framework outperforms existing methods in toolset construction, complexity management, and performance enhancement.  As shown in Table \ref{tab:math_tabmwp_date}, it achieves an optimal balance between tool's complexity (cpl), library size (lib), and average performance improvement (Avg In.) compared to the baselines in Table \ref{tab:Agent-Ended and signal Task}. The tool's complexity (cpl) is calculated by analyzing the Abstract Syntax Tree (AST) of each tool and counting the number of operation nodes, providing a quantitative measure of the tool's complexity. 
\vspace{-1pt}
\input{table-5}

Compared to CREATOR and CRAFT, \ours\ reduces the number of tools by 86.2\% while improving performance by 9.23\%, demonstrating its ability to construct concise yet highly generalizable tools. REGAL employs pruning to simplify its tool library; however, the resulting toolset has relatively low complexity, with many tools consisting of basic wrappers around library functions or simple foundational operations. Overall, our tool graph offers superior abstraction, generalizability, and efficiency compared to existing methods.
