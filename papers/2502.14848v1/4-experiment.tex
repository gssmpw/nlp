
\vspace{-2pt}
\section{Experiment Setup}
\label{sec:exp}
We conducted experiments across various scenarios, including both open-ended and closed-ended tasks. We tested traditional single-turn code generation tasks as well as more complex multi-turn agent tasks, covering diverse domains such as games, mathematics, and data science. We briefly introduce the different scenarios in the following section, with further details provided in Appendix \ref{app:apexp}.
\vspace{-5pt}
\subsection{Open-Ended Tasks}
Open-ended tasks~\citep{wang2023voyager} refer to problems that lack a fixed or predefined solution, allowing for multiple possible outcomes. These tasks often require exploration, creativity, and dynamic problem-solving. 
\vspace{-2pt}
\paragraph{Benchmark}
We select Minecraft game as the experimental platform, where players collect resources and craft tools to achieve various objectives. The simulation environment is built on top of Voyager~\citep{wang2023voyager} and leverages Mineflayer~\citep{mineflayer} JavaScript APIs for motor controls. We measure the number of iterations required to complete the tool upgrades, where each code execution for a subtask counts as one iteration.
\vspace{-2pt}
\paragraph{Baselines}
We compare our method with several representative agent algorithms: ReAct~\citep{yao2022react}, Reflexion~\citep{shinn2023reflexion}, AutoGPT~\citep{richardssignificant}, and Voyager~\citep{wang2023voyager}. Some of the experimental results are from Voyager.
\vspace{-5pt}
\paragraph{Implementation}
\ours\ handles open-ended tasks through online learning, where Task Solver continuously addresses ongoing tasks, and Tool Manager adapts the tool graph in real time. \ours\ utilizes GPT-4o for text completion, with tool retrieval limited to 5 and self-checks limited to 2. The bug-free check is omitted to ensure a fair comparison. Tool pruning is performed every 40 steps.

\subsection{Close-Ended Tasks}
Close-ended tasks refer to problems that have a predefined solution or ground truth. We conducted comprehensive experiments, including both single-turn code tasks and multi-turn agent tasks.
\vspace{-5pt}
\paragraph{Benchmark}
For single-turn code tasks, we utilized the algebra subset at levels 4 and 5 from the MATH~\citep{hendrycks2021measuring} dataset, levels 7 and 8 from the TabMWP~\citep{grand2023learning} dataset, and the Date~\citep{srivastava2022beyond} dataset. For multi-turn agent tasks, we performed tests on TextCraft~\citep{cote2019textworld}, a text-based game, and DABench~\citep{hu2024infiagent}, a data science dataset. To prepare the datasets, we selected data for training. The training and testing data amounts are as follows: MATH (200/405), Date (66/180), TabMWP (200/470), TextCraft (98/77), and DABench (98/158). Detailed information on the data splitting methods can be found in Appendix~\ref{app:apexp}. We use the average accuracy for each dataset as the metric. 

\begin{figure}[H]
\vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=0.75\linewidth]{trial1-map.png}}
\vskip -0.05in
\caption{Map coverage: birdâ€™s eye views of Minecraft maps.}
\label{fig:trial1-map}
\end{center}
\vskip -0.35in
\end{figure}
\vspace{-5pt}
\paragraph{Baselines}
For code tasks, we compare the reasoning framework PoT~\citep{chen2022program} and analyze other tool generation methods, including LATM~\citep{cai2023large}, CREATOR~\citep{qian2023creator}, CRAFT~\citep{yuan2023craft}, and REGAL~\citep{stengel2024regal}. For agent tasks, we compare ReAct~\citep{yao2022react}, Reflexion~\citep{shinn2023reflexion}, and Plan-Execution~\citep{shridhar2023art, yang2023intercode}.
\vspace{-5pt}
\paragraph{Implementation}
For closed-ended tasks, \ours\ separately performs training and testing. During training, \ours\ constructs the tool graph using GPT-4 with greedy decoding, applying tool pruning after training. During testing, the constructed tool graph is frozen, with retrieval enhancing the inference model. Relevant training data and tool code are integrated into the prompt as tool usage examples. For multi-step agent tasks, a ReAct-style~\citep{yao2022react} prompt is employed to facilitate the generation of Thought-Action pairs, whereas single-turn code generation tasks involve direct program synthesis. The complete prompt used is provided in Appendix \ref{appsub:test_prompt}.
\vspace{-5pt}
\paragraph{Models}
For baselines with a tool-making stage, we use GPT-4o as the text completion model. In the test stage, in addition to GPT-4o, we also evaluate several models using constructed tools through the in-context learning method. We test open-source models, including \textit{Qwen2.5-7B-Instruct}~\citep{yang2024qwen2}, \textit{Qwen-Coder-7B-Instruct}~\citep{hui2024qwen2}, \textit{Qwen2.5-14B-Instruct}~\citep{yang2024qwen2}, \textit{Deepseeker-Coder-6.7B-Instruct}~\citep{guo2024deepseek}, and \textit{Deepseeker-Coder-33B-Instruct}~\citep{guo2024deepseek}, while the closed-source models include \textit{GPT-3.5-turbo-1106}, \textit{Claude-3-haiku} and \textit{GPT-4o}. For all experiments, the temperature is set to 0.3, and each experiment is repeated three times, with the average result reported.
