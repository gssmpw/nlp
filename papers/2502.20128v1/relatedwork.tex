\section{Related Work}
\textbf{Gaze estimation.}
With the recent developments in deep learning, appearance-based gaze estimation methods have become the mainstream. Researchers explored various CNN-based architectures~\cite{FullFace,CA-Net,Dilated-Net} to build the mapping between facial images and gaze directions. Zhang \etal~\cite{AppearancebasedGazeEstimation} firstly proposed a CNN-based gaze estimation network and the well-known MPIIFaceGaze dataset. With the introduction of Transformer~\cite{Transformer}, the Vision Transformer-based structures have started to be applied to gaze estimation and achieved promising performance (\eg, GazeTR~\cite{GazeTR}, oh \etal ~\cite{oh}, SUGE~\cite{SUGE}).
In the recent years, researchers dedicated themselves to inventing generalized gaze estimation methods, which would show robust performance on unseen users. Several methods leveraged a gaze redirection strategy to extend the datasets for generalized gaze estimation~\cite{DeepWarp,GFAL}. Cheng \etal~\cite{PureGaze} introduced a method of purifying gaze features to improve the networkâ€™s generalization. Besides, some methods based on contrastive learning~\cite{ContrastiveRegressionDomain}and uncertainty learning~\cite{UncertaintyModelingGaze,SUGE} also demonstrated remarkable generalizability. In this paper, we leverage CLIP to improve discriminability and generalization of gaze estimation network. 

\textbf{Pre-trained Vision-language Models.}
Recently, the CLIP~\cite{CLIP} trained on large-scale image-text pairs have attracted increasing attentions. Because of its powerful visual and semantic representation capabilities, CLIP has been transferred to various vision tasks, such as objection detection~\cite{CLIPGap}, semantic segmentation~\cite{ZegCLIP}, and others~\cite{Grounded}. Especially, CLIP also shows surprising capacities on quantified vision tasks, \eg, depth estimation~\cite{DepthCLIP,WorDepth}, 3D hand poses estimation~\cite{CLIPHand3D}, etc. Moreover, researchers were attempting to take advantages of the pre-trained CLIP to predict gaze directions. Wang \etal~\cite{GazeCLIP} designed a linguistic description generator to produce text signals with coarse gaze directional cues. And then a cross-attention condenser was designed to finely recalibrate the visual and text representations, enhancing the learning quality of gaze features. Yin \etal~\cite{CLIPGaze} designed a feature separation loss by employing CLIP text encoder to generate gaze distractors from diverse language descriptions, which aims at purifying the gaze-relevant feature via pushing away it from gaze-irrelevant features. In this paper, we adapt CLIP to gaze estimation with an innovative collaborative enhancing strategy, in which the CLIP is regarded as an assistance to enhance the obtained gaze features.