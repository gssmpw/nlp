\section{Methodology}
\label{sec:method}
To fully activate the potentials of CLIP to perceive gaze directions, we propose a novel ‘main-side’ learning strategy, in which a primary gaze estimation network is treated as a main line. Meanwhile, the CLIP is regarded as an auxiliary line, whose mission is to encourage the primary network to extract a robust feature that contains gaze-related appearance and semantic information. Accordingly, we propose a CLIP-driven Dual Feature Enhancing Network (CLIP-DEFNet) as shown in \cref{fig:2}, which consists of a Core Feature Extractor, a Language-driven Differential Module, a Vision-driven Fusion Module and a Double-head Gaze Regressor. Both the Core Feature Extractor and the Double-head Gaze Regressor constitute the primary gaze estimation network. The implementation details of each component are introduced in the following sections.
\subsection{Core Feature Extractor}
The Core Feature Extractor is a vital component in our main line, which is designed to extract a gaze-related  feature from a facial image. The feature is crucial as it would directly affect the accuracy of subsequent regression. We employ the remarkable CNN-Transformer architecture~\cite{GazeTR} as the basic structure, where a CNN is firstly adopted to acquire feature maps $f_i^{maps}\in \mathbb{R}^{W\times H\times C}$ of a given image $x_i \in X$ where $X = \{x_1, x_2, \dots,x_n\}$. Then those feature maps are reshaped in to $W \times H$ patches $f_i^p\in  \mathbb{R}^{\left(W\times H\right)\times C}$,which are treated as a series of $C$-dimensional visual tokens. After adding an extra learnable token $f_i^{token}\in \mathbb{R}^{1\times C}$, which is used to aggregate all the features of patches, we feed them into a transformer with a learnable position embedding $f_i^{pos}\in \mathbb{R}^{\left(1+W\times H\right)\times C}$. Overall, we get the final primary feature $f_i^{img}\in \mathbb{R}^{1\times C}$ as \cref{eq:1},
\begin{equation}
f_i^{img}=Transformer\left(\left[{f_i^{token}} ; {f_i^p}\right]+{f_i^{pos}}\right)\left[0,:\right],
  \label{eq:1}
\end{equation}
where $[0,:]$ represents that the first row of the output feature maps is serving as primary feature and $[ ; ]$ denotes the concatenation operation.
\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{framework.pdf}
   \caption{Framework of our proposed CLIP-DEFNet. It consists of four modules: a Core Feature Extractor, a Language-driven Differential Module (LDM), a Vision-driven Fusion Module (VFM) and a Double-head Gaze Regressor (DGR). The LDM randomly selects several image-pairs from batchs and gives each of them a textual prompt that describes their gaze differences. Then, the prompts are fed into CLIP's text encoder to capture text embeddings, which are then aligned with primary features obtained by our Core Feature Extractor. The VFM is designed to adaptively fuse the generalized embeddings of CLIP's image encoder with primary gaze features, aiming to obtain enhanced gaze features. The DGR maps those enhanced features to final gazes.}
   \label{fig:2}
\end{figure*}

%

\subsection{Language-driven Differential Module}
The Language-driven Differential Module (LDM) is introduced to enhance the above primary gaze features from the perspective of integrating gaze-related semantic information driven by the language-image alignment. As indicated in Introduction, the challenge lies in the connections between the infinite continuous gaze direction of each facial image and the restricted language sentences. Intuitively, it is easier to describe the difference of gazes between two facial images than to give individual description of the gaze in each image.  

% as shown in \cref{fig:2}(a),

Therefore, we design a series of differential gaze prompts, each of which refers to a language sentence describing the correlations of gaze directions between a pair of images. To be specific, we repeatedly choose two facial images in current batch at random and calculate their gaze difference with regards to their true gaze labels. Then, we categorize these image-pairs into $K$ semantic similarity levels according to their gaze differences, and each level is attributed with a semantic grade name $t_i^{grade}$, \eg, `identical', `similar', `not similar'. Subsequently, a differential gaze prompt $T_i$ of each image pair is generated via combining a designed template\ $t_i^{template}$:`The directions of gaze in the two photos are \{grade name\}.' with a corresponding semantic grade $t_i^{grade}$ (\cref{eq:6}). For instance, if $K$ is set to 2, all the selected image-pairs are divided into `similar' and `not similar' groups. The image-pairs with gaze differences that are from 0 to 0.1 are assigned into the `similar' grade, while image-pairs with gaze differences that are over 0.1 are assigned into the `not similar' grade. Given the differential gaze prompts $T_i$, a pre-trained CLIP text encoder is employed to encode it into a text embedding $(f_i^t)$ as \cref{eq:6},
%\vspace{-0.1cm}
\begin{equation}
T_i=[t_i^{template},t_i^{grade}],f_i^t=TextEncoder(T_i).
\label{eq:6}
\end{equation}
%\vspace{-0.1cm}

To enhance our extracted primary features, we innovatively realize a visual-semantic alignment between the CLIP text encoder and the Core Feature Extractor. Ideally, the semantic enhanced visual features of facial images should be aligned with the above text embeddings. In other words, the compatibilities between visual embeddings of image-pairs and the text embeddings of their corresponding gaze differential descriptions should be higher than the compatibilities between image-pairs and other misaligned descriptions. 
The visual embeddings of image pairs are derived from the concatenated features of our Core Feature Extractor of the selected two images respectively, as \cref{eq:7},
%\vspace{-0.1cm}
\begin{equation}
f_i^{pair} = MLP([f_{i1}^{img};f_{i2}^{img}]).
\label{eq:7}
\end{equation}
%\vspace{-0.1cm}
%where $f_{i*}^{img}$ represents the feature of the $i$-th image in the $j$-th image pair. 
A language-driven contrastive loss is thus designed as \cref{eq:8},
\begin{equation}
\begin{split}
L_{align} = -\frac{1}{P}\sum_{i=1}^{P}log\frac{exp(f_i^t\cdot f_i^{pair}/\tau)}{\sum_{j =1}^{P}{exp(f_i^t\cdot f_j^{pair}/\tau)}}\\
-\frac{1}{P}\sum_{i=1}^{P}log\frac{exp(f_i^{pair}\cdot f_i^t/\tau)}{\sum_{j =1}^{P}{exp(f_i^{pair}\cdot f_j^t/\tau)}},
\end{split}
    \label{eq:8}
\end{equation}
where $P$ is the number of selected image-pairs in one batch and $\tau$ is a temperature hyperparameter.

In summary, by minimizing the $L_{align}$, our Core Feature Extractor could be endowed with the ability that perceiving gaze semantic difference, thus takes full advantages of the gaze-related semantic information. The innovations of the Language-driven Differential Module could be illustrated as follows. As shown in \cref{fig:3}, on the one hand, superior to extracting a feature of individual image, by comparing different facial images, interactions of gazes between them will be characterized, which benefits the extracting of robust gaze-related features. On the other hand, with reference to a same image (\textit{Image1}), the samples (\textit{Image2, Image3}) owning similar gaze directions are naturally to be projected into the same semantic grade (`similar' grade), while the samples (\textit{Image4}) with different gazes would distribute in different grades (`not similar' grade). 
Eventually, the samples with similar gaze directions are likely to be clustered into adjacent areas. 
By realizing the language-image alignment, the consistency between extracted features and gaze labels are also maintained derivatively. Thus, LDM helps to learn robust and pure gaze-related features that disentangle from gaze-irrelevant factors. 

\begin{figure}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{Fig3.pdf}
   \vspace{-20pt}
   \caption{Taking \textit{image1} as the center, the other images in the pairs with `similar' prompt is limited into the blue circle, which means the closer feature distances than the `not similar' one (red circle).}
   \label{fig:3}
\end{figure}


\subsection{Vision-driven Fusion Module}
The Vision-driven Fusion Module (VFM) aims to further improve the generalization of the primary gaze features. It is well-known that CLIP trained on large-scale image-text pairs from the Internet have achieved excellent performance in various face-related downstream tasks, including age estimation~\cite{TeachCLIPDevelopAge}, facial image editing~\cite{StyleCLIP}, etc~\cite{CLIPCluster}. Those remarkable applications demonstrate that the visual encoder of CLIP has powerful abilities to characterize generalized appearance information of facial images. Undoubtedly, this appearance information is always mixed by gaze-related factors which could help the gaze estimation and gaze-irrelevant factors which may harm the accuracy. Therefore, the VFM is designed to distill the gaze-related content contained in the generalized embeddings of CLIP and utilize it to further enhance the generalization of the primary gaze features of Core Feature Extractor.

To realize that, a Adaptive Fusion Unit (AFU) is introduced, which is implemented by a cascade of attention units, namely a group of gaze-aware attention units and a cross-attention unit. The two gaze-aware attention units are applied to computing the attention maps of the generalized embeddings $f_i^{clip}$ from CLIP image encoder and raw feature maps $f_i^{maps}$ from Core Feature Extractor, respectively, as \cref{eq:9-1,eq:9-2,eq:9-3,eq:9-4}:
\begin{equation}
f_i^{clip}= ImageEncoder(x_i),
\label{eq:9-1}
\end{equation}
\begin{equation}
    Q={fW}_Q, K=fW_k,
    \label{eq:9-2}
\end{equation}
\begin{equation}
M=\mathcal{G}\left(f\right)=Softmax\left(QK^T/\beta\right),
\label{eq:9-3}
\end{equation}
\begin{equation}
M_i^{clip}=\mathcal{G}\left(f_i^{clip}\right),M_i^{gaze}=\mathcal{G}\left(f_i^{maps}\right),
\label{eq:9-4}
\end{equation}
where $\mathcal{G}$ represents the gaze-aware attention unit. The obtained attention maps $M$ reveal the valuable partitions of themselves. And the attention map of the generalized embeddings also leads to focus on the generalized appearance details that may be ignored before.

To further activate the key components of the primary gaze features, we modulate the generalized embeddings into the raw feature maps via a cross-attention unit as \cref{eq:10} and finally obtain the enhanced features maps ${\hat{f}}_i^{maps}$.
\begin{equation}
{\hat{f}}_i^{maps}=(M_i^{clip}+M_i^{gaze})f_i^{maps}
\label{eq:10}  
\end{equation}
Then we feed the enhanced feature maps ${\hat{f}}_i^{maps}$ into a transformer to get an enhanced image feature ${\hat{f}}_i^{img}$ following the process of the Core Feature Extractor as \cref{eq:1}.

In summary, the primary gaze features of facial images could be enhanced by the VFM via the generalized embeddings of CLIP. Actually, the two gaze-aware attention units allocate more attentions to the parts related to gaze in generalized embeddings and the raw feature maps, respectively. Then the cross-attention unit is applied to identifying the common components shared focus  between the two features, which filters out the noise in generalized embeddings and further enhances the valuable partition of the appearance information.

\subsection{Double-head Gaze Regressor}
Even with enhanced features, the design of the regressor is also crucial for improving the model's generalization ability. Existing methods~\cite{FullFace,CA-Net} typically use a simple MLP architecture to regress features into gaze directions. As discussed in the early work~\cite{GPM}, the numerous parameters of the MLP easily overfit to gaze-irrelevant factors within the high-dimensional image features during the mapping process. To mitigate the overfitting issues caused by MLP, in this paper, we propose a Double-head Gaze Regressor (DGR).
One of the regression head adopts the conventional MLP-based structure, which projects the enhanced gaze feature ${\hat{f}}_i^{img}$ to a final gaze direction as \cref{eq:11},
\begin{equation}
g_i=MLP({\hat{f}}_i^{img}).
    \label{eq:11}
\end{equation}
Then we employ the L1 loss as \cref{eq:12} to minimize the distances between the estimated gaze direction $g_i$ and the ground truth $\hat{g_i}$,
\begin{equation}
L_{gaze}= \frac{1}{N} \sum_{i=1}^{N} \left|\left|g_i-\hat{g}_i\right|\right|_1,
    \label{eq:12}
\end{equation}
where $N$ represents the size of batch.

Inspired by the motivation of the dropout layers in neural networks~\cite{dropout}, we design a masked regression head. Specifically, we construct a mask $M_i\in \mathbb{R}^{1\times C}$ with the same size as $f_i^{img}$, whose elements are either 0 or 1. The numbers of 0s and 1s in the mask are adjusted by a drop ratio manually. For example, if the $f_i^{img}$ is a 32-dimensional vector and the drop ratio is set as $5/32$, the mask would include five 0s and twenty-seven 1s with random positions. Then we take the Hadamard product of the masks with our enhanced gaze features ${\hat{f}}_i^{img}$ and get the masked features ${\hat{f}}_i^{img\_m}$ (\cref{eq:13}).
\begin{equation}
{\hat{f}}_i^{img\_m}=\ {\hat{f}}_i^{img}\circ  M_i
    \label{eq:13}
\end{equation}

Then we utilize the sampling method of max-pooling to directly map the high-dimensional features to 2D gaze vectors $\dot{g}_i$ without any parameters. Meanwhile, we employ L1 loss $L_{mask}$ as \cref{eq:15} to minimize the distance between the gaze vector $\dot{g}$ and the ground truth $\hat{g}_i$.
\begin{equation}
\dot{g}_i=\ MaxPooling({\hat{f}}_i^{img\_m}),
    \label{eq:14}
\end{equation}
\begin{equation}
L_{mask}\ =\frac{1}{N} \sum_{i=1}^{N} \left|\left|\dot{g}_i-\hat{g}_i\right|\right|_1.
    \label{eq:15}
\end{equation}
%where $N$ represents the size of batch.

Overall, the Double-head Gaze Regressor not only reduces the degrees of freedom of the regression head, thereby preventing it from overfitting certain details, but also guides model to focus on all the dimensions of the features rather than overfit on several dimensions, which promotes the generalization ability of our gaze regressor.

\subsection{Total Loss}
\label{totalloss}
In training stage, our network is optimized by minimizing the total loss function as follows:
\begin{equation}
  L_{total}\ =\ L_{gaze}+\alpha L_{align}+\beta L_{mask},  
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters to balance the losses. The LDM is frozen and the parameters of the Core Feature Extractor, VFM and Double-head Gaze Regressor should be updated.During inference stage, the LDM and the masked regression head of the Double-head Gaze Regressor should be cut off. The facial images are fed into the Core Feature Extractor and the VFM to obtain the enhanced gaze features. Finally, we employ the MLP-based head to project the enhanced features into final gaze directions.












