\section{Related Work}
\textbf{Gaze estimation.}
With the recent developments in deep learning, appearance-based gaze estimation methods have become the mainstream. Researchers explored various CNN-based architectures____ to build the mapping between facial images and gaze directions. Zhang \etal____ firstly proposed a CNN-based gaze estimation network and the well-known MPIIFaceGaze dataset. With the introduction of Transformer____, the Vision Transformer-based structures have started to be applied to gaze estimation and achieved promising performance (\eg, GazeTR____, oh \etal ____, SUGE____).
In the recent years, researchers dedicated themselves to inventing generalized gaze estimation methods, which would show robust performance on unseen users. Several methods leveraged a gaze redirection strategy to extend the datasets for generalized gaze estimation____. Cheng \etal____ introduced a method of purifying gaze features to improve the networkâ€™s generalization. Besides, some methods based on contrastive learning____and uncertainty learning____ also demonstrated remarkable generalizability. In this paper, we leverage CLIP to improve discriminability and generalization of gaze estimation network. 

\textbf{Pre-trained Vision-language Models.}
Recently, the CLIP____ trained on large-scale image-text pairs have attracted increasing attentions. Because of its powerful visual and semantic representation capabilities, CLIP has been transferred to various vision tasks, such as objection detection____, semantic segmentation____, and others____. Especially, CLIP also shows surprising capacities on quantified vision tasks, \eg, depth estimation____, 3D hand poses estimation____, etc. Moreover, researchers were attempting to take advantages of the pre-trained CLIP to predict gaze directions. Wang \etal____ designed a linguistic description generator to produce text signals with coarse gaze directional cues. And then a cross-attention condenser was designed to finely recalibrate the visual and text representations, enhancing the learning quality of gaze features. Yin \etal____ designed a feature separation loss by employing CLIP text encoder to generate gaze distractors from diverse language descriptions, which aims at purifying the gaze-relevant feature via pushing away it from gaze-irrelevant features. In this paper, we adapt CLIP to gaze estimation with an innovative collaborative enhancing strategy, in which the CLIP is regarded as an assistance to enhance the obtained gaze features.