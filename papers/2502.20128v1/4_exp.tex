\section{Experiments}
\label{sec:exp}
\subsection{Datasets and settings}
Our method is evaluated on four popular gaze estimation datasets, which are MPIIFaceGaze ($D_M$)~\cite{MPII}, EyeDiap ($D_D$)~\cite{eye}, Gaze360 ($D_G$)~\cite{Gaze360} and ETH-XGaze ($D_E$)~\cite{eth} over within-domain and cross-domain tasks. More details of datasets refer to supplementary materials.

For within-domain evaluation, the experiments are conducted on MPIIFaceGaze, EyeDiap and Gaze360 datasets. We perform leave-one-person-out evaluation on MPIIFaceGaze dataset and four-folder cross validation on EyeDiap dataset. As for the Gaze360 dataset, after removing the images without frontal faces, we select 84,902 images of 54 subjects for training and 16,000 images of 15 subjects for testing. For cross-domain evaluation, the Gaze360 and ETH-XGaze datasets are treated as source datasets for training, while MPIIFaceGaze and EyeDiap are target ones for testing. Thus, we evaluate our method on four cross-domain tasks: $D_E\rightarrow D_M,D_E\rightarrow D_D,D_G\rightarrow D_M,D_G\rightarrow D_D$.
\subsection{Implementation details}
We employ the pre-trained RN50 CLIP for the backbones of our LDM and VFM, which consists of a ResNet50-based image encoder and a transformer-based text encoder. During the training stage, the text encoder is frozen, while the image encoder would be fine-tuned.

In within-domain evaluation, the Core Feature Extractor is composed of a ResNet18 and a 6-layer transformer. %Following~\cite{GazeTR}, we employ a model that is pre-trained on the ETH-XGaze dataset and would be fine-tuned on other datasets. 
Given the $224\times224$ input images, $7\times7\times32$ feature maps are generated from ResNet18 and then fed into a 6-layer transformer with 8-heads self-attention mechanism. Finally, we get a 32-dimensional image feature. In cross-domain evaluation, the transformer is replaced by a 3-layer MLP to mitigate the overfitting issues.

\subsection{Comparison with State-of-the-art Methods}
We compare our method with the SOTAs and the results are shown in the \cref{tab:2,tab:3}. The reported results come from~\cite{GFAL} or their original papers.

\textbf{Performance on within-domain tasks.}
%We roughly classify the compared methods into CNN-based methods ( Itracker~\cite{Itracker}, FullFace~\cite{FullFace}, RT-Gene~\cite{Rt}, Dilated-Net~\cite{Dilated-Net}, Gaze360~\cite{Gaze360}, CA-Net~\cite{CA-Net}), FAR-Net~\cite{FAR} and transformer-based methods (oh \etal~\cite{oh}, GazeTR~\cite{GazeTR}, SUGE~\cite{SUGE}). 
We roughly classify the compared methods into CNN-based methods (upper part of \cref{tab:2}) and Transformer-based methods (bottom part of \cref{tab:2}). 
As the results shown in \cref{tab:2}, our approach outperforms all within-domain methods on the MPIIFaceGaze and EyeDiap datasets. It also achieves performance comparable to the SUGE~\cite{SUGE} on Gaze360 dataset. The results prove that our CLIP-DFENet can strengthen the gaze-related appearance and semantic information in gaze features, leading to higher accuracy.

\begin{table}[h]
    \centering
    \begin{tabular}{l c c c c}
     \toprule
    Method & Reference & $D_M$ & $D_D$ & $D_G$ \\
    \midrule
        Itracker~\cite{Itracker} & 2016 CVPR & 6.20 & 9.93 & - \\
        FullFace~\cite{FullFace} & 2017 CVPRW & 4.93 & 6.53 & 14.99 \\
        RT-Gene~\cite{Rt} & 2018 ECCV & 4.30 & 5.90 & - \\
        Dilated-Net~\cite{Dilated-Net} & 2018 ACCV & 4.42 & 6.19 & 13.73 \\ 
        Gaze360~\cite{Gaze360}  & 2019 ICCV & 4.06 & 5.36 & 11.04 \\ 
        FAR-Net~\cite{FAR} & 2020 TIP & 4.30 & 5.71 & - \\
        CA-Net~\cite{CA-Net}  & 2020 AAAI & 4.27 & 5.27 & 11.20 \\ 
     \midrule
      GazeTR~\cite{GazeTR} & 2021 ICPR & \underline{4.00} & 5.17 & 10.62 \\ 
        Oh \etal ~\cite{oh} & 2022 CVPRW & 4.04 & 5.25 & 10.70 \\       
        SUGE~\cite{SUGE}  & 2024 AAAI & 4.01 & \underline{5.04} & \textbf{10.51} \\ 
        \textbf{CLIP-DFENet} & \textbf{Ours} & \textbf{3.71} & \textbf{4.97} & \underline{10.54} \\ 
         \bottomrule
    \end{tabular}
    \caption{Performance on within-domain tasks.   Results reported are angular errors in degrees. \textbf{Bold} and \underline{underline} indicate the best and the second best result.}
  \label{tab:2}
\end{table}

\textbf{Performance on cross-domain tasks.}
To further demonstrate the generalizability of our method, we conduct experiments on unsupervised domain adaptation (UDA) tasks. In UDA settings, the models are trained on source domain while testing on unseen target domain. $|D_t|$ samples from target domain could be randomly selected for further adaptation (fine-tuning or co-training). Results are shown in \cref{tab:3}. Although our method performs not as well on the within-domain tasks, it still achieves competitive performance with SOTAs. This demonstrates that the integration of generalized embeddings of CLIP and the design of the DGR further enhance the model's generalizability.

\begin{table}[!ht]
    \centering
    \begin{tabular}{c@{}  c@{} c@{} c@{} c@{} c@{}l}
     \toprule
     Methods &  $|D_t|$ &  \begin{tabular}{c@{}} $D_E$ \\ $\rightarrow D_M$ \end{tabular} & \begin{tabular}{c@{}} $D_E$ \\ $\rightarrow D_D$ \end{tabular} & \begin{tabular}{c@{}} $D_G$ \\ $\rightarrow D_M$ \end{tabular} & \begin{tabular}{c@{}} $D_G$ \\ $\rightarrow D_D$ \end{tabular} \\ 
    \midrule   
    ADDA~\cite{ADDA} & 500 & 6.65 & 8.24 & 6.27 & 9.53 \\ 
        GazeAdv~\cite{GazeAdv} &  100 & 6.36 & 7.62 & 7.54 & 8.43 \\ 
         Gaze360~\cite{Gaze360} &  100 & 6.24 & 7.47 & 7.17 & 7.66 \\ 
         DAGEN~\cite{DAGEN} &  500 & 5.73 & 6.77 & 7.38 & 8.00 \\ 
         PnP-GA~\cite{Pnp-GA} &  10 & \underline{5.53} & \underline{5.87} & 6.18 & 7.92 \\ 
         RUDA~\cite{RUDA} &  100 & 5.70 & 7.52 & 6.20 & 7.02 \\ 
         DCUA~\cite{DCUA} &  100 & 7.31 & 5.95 & 5.59 & 6.40 \\
        CLIP-Gaze\cite{CLIPGaze} &  100 & \textbf{4.45} &\textbf{5.27} & \textbf{4.94} & \textbf{5.60} \\ 
         \textbf{Ours} & \centering 100 & 6.00 & 6.01 & \underline{5.25} & \underline{6.21} \\ 
         \bottomrule
    \end{tabular}
    \caption{Performance on cross-domain tasks.   $|D_t|$ indicates the number of the samples for fine-tuning or co-training. \textbf{Bold} and \underline{underline} indicate the best and the second best result.}
  \label{tab:3}
\end{table}

\subsection{Ablation Study}
\subsubsection{Ablation Study of Proposed Modules}
% To investigate the effectiveness of each proposed module, we compare the performance of some degraded models on within-domain tasks. Based on the fully model, we remove one proposed module each time and get the results shown in \cref{tab:4}. ‘Baseline’ refers the backbone model that consists of a CNN-Transformer-based Feature Extractor and an MLP-based regressor. No matter which module is invalidated, the performance decreases. This proves the importance of each component. 1) By comparing `w/o LDM' with `Full model', it demonstrates that the LDM can help Core Feature Extractor to capture robust and pure gaze-related features via image-text alignment. 2) By comparing `w/o VFM' with `Full model', it illustrates that the VFM can further enhance valuable partitions of the gaze features through integrating generalized embeddings of CLIP. 3) By comparing `w/o DGR' with `Full model', the Double-head Gaze Regressor have been proven effective in reducing degrees of freedom and improving generalizability.
To investigate the effectiveness of each proposed module, we compare the performance of some degraded models on within-domain tasks. Based on the fully model, we remove one proposed module each time and get the results shown in \cref{tab:4}. The $1st.$ row refers the backbone model that consists of a CNN-Transformer-based Feature Extractor and an MLP-based regressor. No matter which module is invalidated, the performance decreases. This proves the importance of each component. 1) By comparing the $2nd.$ row with the $5th.$ row (full model), it demonstrates that the LDM can help Core Feature Extractor to capture robust and pure gaze-related features via image-text alignment. 2) By comparing the $3rd.$ row with the $5th.$ row, it illustrates that the VFM can further enhance valuable partitions of the gaze features through integrating generalized embeddings of CLIP. 3) By comparing the $4th.$ row with the $5th.$ row, the DGR has been proven effective in reducing degrees of freedom and improving generalizability.


\begin{table}[t]
  \centering
  % \begin{tabular}{l c c c@{}}
  %   \toprule
  %   Method & MPII & EyeDiap & Gaze360 \\ 
  %   \midrule 
  %       Baseline & 4.13 & 5.23 & 10.76 \\ 
  %       w/o LDM & 3.77 & 5.06 & 10.65 \\ 
  %       w/o VFM & 3.76 & 5.15 & 10.58 \\ 
  %       w/o DGR & 3.77 & 5.18 & 10.58 \\ 
  %       \textbf{Full model} & \textbf{3.71}& \textbf{4.97} & \textbf{10.54} \\ 
  %   \bottomrule
  % \end{tabular}
  \begin{tabular}{c c c | c c c}
    \toprule
    LDM &VFM &DGR & MPII & EyeDiap & Gaze360 \\ 
    \midrule 
        - & - & - & 4.13 & 5.23 & 10.76 \\ 
        - & \checkmark & \checkmark & 3.77 & 5.06 & 10.65 \\ 
        \checkmark &- &\checkmark & 3.76 & 5.15 & 10.58 \\ 
        \checkmark&\checkmark&- & 3.77 & 5.18 & 10.58 \\ 
        \checkmark & \checkmark & \checkmark & \textbf{3.71}& \textbf{4.97} & \textbf{10.54} \\ 
         \bottomrule
  \end{tabular}
  \caption{Ablation study results of proposed modules.}
  \label{tab:4}
\end{table}

\begin{table*}[h]
  \centering
  \begin{tabular}{c c c c c c}
    \toprule
    $K$ & Template & Grade Names & MPII & EyeDiap & Gaze360 \\ 
    \midrule 
        2 & \multirow{3}{*}{\centering fixed} & similar, not similar & 5.08 & 10.56 & 3.73 \\
        3 & & identical, similar, not similar & 5.13 & 10.60 & 3.82 \\
        5 & & almost identical, extremely similar, similar, a little similar, different  & 5.02 & 10.58 & 3.75 \\
        \midrule 
        5 & fixed &identical, highly similar, moderately similar, slightly similar, not similar & \textbf{4.97} & 10.54 & \textbf{3.71} \\
        \midrule 
        5 & learnable & identical, highly similar, moderately similar, slightly similar, not similar & 5.09 & \textbf{10.53} & 3.81 \\ 
    \bottomrule
  \end{tabular}
  \caption{Ablation study results of differential gaze prompts.   The fixed prompt template is `The directions of gaze in the two photos are \\ \{\textit{Grade Name}\}.'  The learnable prompt template is following CoOp~\cite{COOP}.}
  \label{tab:5}
\end{table*}

\subsubsection{Ablation Study of Differential Gaze Prompts} 
In this section, we evaluate our CLIP-DFENet under different prompt designs (\cref{tab:5}). Firstly, we vary the number of grade prompts $K$ from 2 to 5. The results show that increasing the number of grades can better help the model to identify the subtle gaze differences between facial images, leading to better feature representations. However, the more levels there are, the more difficult manual designs of textual prompts become. Thus, we select 5 grades in our experiments. Secondly, we explore different strategies to describe these prompts. To be specific, we use different words for each differential grade name or propose a learnable prompt template following the CoOp method~\cite{COOP}. Two main observations could be concluded from the comparisons. 1) The manual prompts combining with degree adverbs can convey more precise semantic information, which lead to better results. 2) The learnable prompt template performs worse than manual one, whose possible reason is that the manual prompt template could clearly describe the gaze difference between images while the learnable one may introduce some noise during learning process.


\subsubsection{Ablation Study of Feature Fusion Strategy}

As discussed in Section 3.3, a novel Adaptive Fusion Unit is proposed to fuse generalized embeddings with primary gaze features. In order to evaluate the properties of this fusion strategy, we compare it with several commonly used feature fusion methods: 

\begin{itemize}
    \item ‘Concatenation’: We concatenate the generalized embeddings and primary gaze features, and feed them into a fully connected layer to preserve the feature dimensions.
    \item ‘Cross-Attention’: We treat the primary gaze features $f_{img}$ as $Q$ and generalized embeddings $f_{clip}$ as $K,V$. The final fused features ${\hat{f}}_{img}$ is computed as \cref{eq:18}:
     \begin{equation}
     \centering
     \begin{split}
     Q_{img}&=f_{img}W_Q,\\
     K_{clip}=f_{clip}&W_k,{V}_{clip}=f_{clip}W_V,\\
     {\hat{f}}_{img}=Soft&max(Q_{img}K_{clip}^T/\beta){V}_{clip}.
     \end{split}
     \label{eq:18}
     \end{equation}
     
    \item ‘Gated Information Fusion’: Following~\cite{Gate}, the generalized embeddings $f_{clip}$ are processed by a sigmoid operation to generate a mask, which is used to activate the primary gaze feature $f_{img}$ via Hadamard product. The detailed process refers to \cref{eq:19}:
     \begin{equation}
     {\hat{f}}_{img}=\ f_{img}\circ Sigmoid(f_{clip})\,
     \label{eq:19}
     \end{equation}
\end{itemize}

As shown in \cref{tab:6}, our AFU consistently outperforms all compared methods, which indicates that our novel fusion strategy could effectively highlight generalized appearance information to enhance the primary gaze feature.

\begin{table}[h]
  \centering
  \begin{tabular}{c c c c }
    \toprule
   Fusion methods & MPII & EyeDiap & Gaze360 \\  
    \midrule 
        Concatenation & 3.77 & 5.29 & 10.57 \\ 
        Cross-Attention & 3.80 & 5.21 & 10.56 \\ 
        Gated Information Fusion & 3.78 & 5.13 & 10.59 \\ 
       \textbf{ AFU (proposed)} & \textbf{3.71} & \textbf{4.79} & \textbf{10.54} \\ 
    \bottomrule
  \end{tabular}
  \caption{Ablation study results of feature fusion.}
  \label{tab:6}
\end{table}

%\vspace{-0.3cm}
\subsection{More Discussion \protect \footnote{In supplementary materials, we conduct more experiments to evaluate the properties of our proposed modules.}}

\subsubsection{Visualization of Obtained Gaze Features}
\label{vis}
To quantitatively demonstrate the advantages of our enhanced gaze features, we visualize the distribution of all the training samples of Gaze360 dataset by t-SNE~\cite{sne}, following~\cite{ContrastiveRegressionDomain}. In the scatter plot, the samples are clustered by KMeans~\cite{kmeans} according to their gaze labels, so that the samples with similar gaze directions share similar colors. The feature distributions of the Baseline and our CLIP-DFENet are shown in \cref{fig:4}. As shown in the left figure, the sample points of baseline are scattered in a chaotic manner. By contrast, feature points of our enhanced features are distributed in an organized way, in which the features with similar gaze directions are clustered and can be regressed to similar gazes. It demonstrates our enhanced features being purified gaze-related ones, which effectively improve the discriminability and generalization.

\begin{figure}[h]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{Fig4.pdf}
    %\vspace{-0.8cm}
   \caption{Visualization of obtained gaze features.}
   \label{fig:4}
\end{figure}

\subsubsection{Visualization of Estimated Gazes}
 We further visualize the true gazes (green arrow), the predicted gaze directions of baseline  (blue arrow) and the predicted gaze directions of CLIP-DFENet (red arrow). We select several test samples of Gaze360 and ETH-XGaze datasets in different conditions including extreme head poses, dark illumination and low quality. The visualized results are shown in \cref{fig:5}. Compared to baseline method, the estimated results of our method are closer to ground truths in most conditions. More visualization results of other datasets would show in supplementary materials.
 \begin{figure}[h]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{Fig5.pdf}
   \vspace{-0.6cm}
   \caption{Visualization of estimated gazes.}
   \label{fig:5}
\end{figure}
\vspace{-0.6cm}












