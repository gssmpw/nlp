\section{Introduction}
\label{sec:intro}
Gaze estimation aims to predict a 2D gaze position or a 3D gaze direction of a specified user given its facial or eye image. As an important task in the field of computer vision, it has been extensively utilized in various people-related researches, such as virtual reality~\cite{Burova2020UtilizingVA,gazeVR}, autonomous driving~\cite{Drivers,chengWhatYouSee2024}, etc. In those real scenarios, the users’ identities and their surrounding environments are changeable and complicated, which puts a heavy responsibility on the accuracy and generalization of gaze estimation models. In other words, the estimation models that are optimized via training users should adapt to disjointed new subjects accurately.
\begin{figure}[t]
 \centering
 \small
    \begin{tabular}{lc@{}}
    \toprule
    Method & Angular error on MPII \\
    \midrule
    ResNet18 ($D_E \rightarrow D_M$) & $9.40 ^\circ$ \\
    CLIP & $11.10 ^\circ$ \\
    \bottomrule
  \end{tabular}
  \vspace{-5pt}
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{Fig1.pdf}
   \caption{\textit{Upper:} The results of ResNet18 and CLIP on MPIIFaceGaze dataset. We evaulate ResNet18 on the cross-domain task $D_E  \rightarrow D_M$ which indicates training on ETH-XGaze and testing on MPIIFaceGaze dataset. 
   \textit{Bottom:} Comparing with generating a unique sentence for each image, it is more easier to describe the gaze differences between two images.}
   \label{fig:1} 
\end{figure}
\setlength{\textfloatsep}{10pt} 

Most recently, the pre-trained Visual-Language models represented by CLIP~\cite{CLIP} have demonstrated impressive performance on various perception tasks, like image classification~\cite{chertiReproducibleScalingLaws2023}, semantic segmentation~\cite{ZegCLIP}, depth estimation~\cite{DepthCLIP,WorDepth} and others~\cite{CLIPHand3D}. CLIP trains an image encoder and a text encoder jointly by a contrastive loss in the embedding space between both modalities, which endows it with powerful representation capabilities by characterizing both visual and semantic information of a target object. Naturally, we are spontaneously curious about the issue: \textit{\textbf{can CLIP understand human gaze? }} 
To explore this concern, we make a simple early attempt. Inspired by~\cite{DepthCLIP}, we define four gaze bins that align with the gaze directions of  [0,$\frac{\pi}{2}$], [0,-$\frac{\pi}{2}$], [$\frac{\pi}{2}$,0] and [-$\frac{\pi}{2}$,0], respectively. And their semantic descriptions are designated as a prescribed prompt `A photo of a face gazing \{\textit{gaze direction}\}.’ where gaze direction includes [`up’, `down’, `left’, `right’]. Then the gaze direction of an arbitrary facial image could be obtained via linearly combining the multi-bin gaze values according to the language-image similarities between its visual embedding and semantic embeddings of all the gaze bins (details refer to supplementary materials). Surprisingly, even without fine-tuning on any exclusive gaze estimation datasets, the CLIP achieves performance comparable to ResNet18 that trained with cross-domain samples (\cref{fig:1}). From the results, we thus draw a rough conclusion that \textit{\textbf{the CLIP could perceive human gazes subtly, but its potentials have not been fully exploited}}.


Afterward, we theoretically analyze the possible reasons hindering the CLIP from reaching its full potentials in perceiving gaze directions. On the one hand, unlike the finite and discrete outputs in typical classification and segmentation tasks, gaze labels are distributed in an infinite and continuous space. Thus, it is difficult to design a set of gaze-guided text prompts that can fittingly align with each facial image. In other words, the challenges in connecting text prompts with gaze labels obstruct the extraction of gaze-related semantic information. On the other hand, CLIP was optimized by large-scale image-text datasets, except for exclusive gaze estimation datasets. Thus, their obtained visual embeddings of facial images are always mixed with gaze-unrelated factors (\eg, facial wears and hair styles) and gaze-related appearance clues (\eg, head poses and pupil shapes). It is nontrivial to distill gaze-related information from the visual embeddings of CLIP image encoder while eliminating irrelevant information to facilitate the accurate estimation of human gazes.

To address the aforementioned challenges, in this paper, we put forward an innovative ‘\textit{main-side}’ collaborative enhancing strategy, in which the CLIP acts as a \textit{supporting role} and a primary gaze estimation network takes on the role of a \textit{main character}. To be specific, the two encoders of CLIP are treated as auxiliary modules and introduced to encourage the primary gaze estimation network to extract powerful gaze-related features. Accordingly, a novel \textbf{CLIP-driven Dual Feature Enhancing Network (CLIP-DFENet)} is proposed.
%, which consists of a primary gaze estimation network and a couple of CLIP-driven feature enhancing modules. 

Firstly, although it is impractical to describe the continuous gaze direction of each facial image in language, it is easier to semantically distinguish the gaze difference between a sample pair. For example, as shown in \cref{fig:1}, there are three facial images with gaze directions of [0.54, -0.20], [0.24, -0.43], [0.08, 0.42], respectively. Obviously, we could not generate a unique sentence for each image to explain their detailed gaze directions. While we could clearly draw the conclusions that `the people in \textit{image1} and \textit{image2} are looking in much similar directions' and `the people in \textit{image1} and \textit{image3} are looking in different directions’. Based on these observations, we intend to leverage CLIP for unveiling gaze semantic difference instead of identifying single gaze direction. Specifically, we propose a \textbf{Language-driven Differential Module (LDM)} based on the text encoder of CLIP. A series of gaze differential prompts are firstly designed, which reveal the relationships of gaze directions between two facial images in language. And then, the connections between the semantic embeddings of these prompts and visual features of image pairs via our primary network could be established to realize an image-language contrastive learning. 
%Ideally, the extracted features of selected image pairs obtained via our primary network should align with the semantic embeddings of corresponding text prompts. 
Therefore, by semantically identifying the gaze difference of sample pairs with the help of CLIP, we can fully leverage its semantic representation ability to facilitate our primary gaze estimation model to characterize semantic gaze-related information. 

Secondly, as CLIP learns rich visual-linguistic correlations through large-scale image-text datasets, its visual encoder specializes in capturing generalized appearance information of each facial image. To distill the gaze-related content contained in the generalized embeddings and utilize it to further enhance the obtained features of our primary network, we propose a \textbf{Vision-driven Fusion Module (VFM)} based on the image encoder of CLIP. This module contains a cascade of attention units. To be specific, two gaze-aware attention units are preliminarily adopted to visual embeddings and primary features to focus on their valuable contents, respectively. And then, we utilize the attention maps of visual embeddings to modulate much more gaze-related appearance information into features of primary network via a cross-attention unit. Therefore, we can further improve the generalization of extracted gaze feature via taking advantages of the visual representation ability of CLIP. 

The main contributions of our paper are as follows:
\vspace{2pt}
\begin{itemize}
    \item We develop the potentials of the pre-trained CLIP in boosting gaze estimation performance from a real new perspective, which puts CLIP into a supporting role to facilitate the extraction of gaze-related features of a primary gaze estimation network. 
    \item We propose a novel CLIP-driven Dual Feature Enhancing Network, which consists of a LDM and a VFM. The former is designed to help primary network to extract gaze-related features via leveraging its text encoder for unveiling gaze semantic difference. The latter aims to further enhance the features via allocating more attention to the generalizable components of the primary features.
    \item Our network outperforms the state-of-the-art methods on within-domain gaze estimation tasks and also achieves competitive performance with existing domain generalization approaches.
\end{itemize}




