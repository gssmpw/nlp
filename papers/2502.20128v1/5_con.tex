\section{Conclusion}
\label{sec:con}
 \frenchspacing
In this paper, we have proposed a novel CLIP-driven Dual Feature Enhancing Network (CLIP-DFENet) for gaze estimation, which leverages the powerful capabilities of the pre-trained CLIP to improve the representation capacity of a primary network by a novel ‘main-side’ collaborative enhancing strategy. Firstly, a Language-driven Differential Module has been proposed to help the Core Feature Extractor to represent more gaze-related feature via image-text alignment. Besides, a Vision-driven Fusion Module enhances the generalization of gaze features by adaptively fusing the visual embeddings obtained via CLIP's image encoder with primary gaze features. In extensive experiments, CLIP-DFENet has achieved remarkable performance on within-domain tasks. \textit{Limitation:} Nevertheless, it is less effective on cross-domain tasks than within-domain ones, as subjects in different datasets are surrounded with much more complex environments. Therefore, in the future, we need to further improve the adaptability of CLIP-DFENet to larger subject variations.