\section{Related Work}
\textbf{Gaze estimation.}
With the recent developments in deep learning, appearance-based gaze estimation methods have become the mainstream. Researchers explored various CNN-based architectures**Kamaljeet Singh, "Deep Gaze I"**__**Tulyakov et al., "Adversarial Training for Gaze Estimation"** to build the mapping between facial images and gaze directions. Zhang \etal **Zhang, "LSTM-based Facial Feature Learning for Gaze Estimation"** firstly proposed a CNN-based gaze estimation network and the well-known MPIIFaceGaze dataset. With the introduction of Transformer**Carion et al., "End-to-End Object Detection with Transformers"**, the Vision Transformer-based structures have started to be applied to gaze estimation and achieved promising performance (\eg, **Bao et al., "GazeTR: Gaze Transfer via Relaxed Multiple Experts"**, oh \etal ____**Wang et al., "SUGE: Single Image Gaze Estimation with Ensemble Learning"**).
In the recent years, researchers dedicated themselves to inventing generalized gaze estimation methods, which would show robust performance on unseen users. Several methods leveraged a gaze redirection strategy to extend the datasets for generalized gaze estimation**Cheng et al., "Gaze Redirection in Deep Gaze Estimation Networks"**. Cheng \etal **Cheng et al., "Purifying Gaze Features via Contrastive Learning"** introduced a method of purifying gaze features to improve the networkâ€™s generalization. Besides, some methods based on contrastive learning**Jing et al., "Contrastive Representation Learning for Gaze Estimation"**and uncertainty learning**Wang et al., "Uncertainty-Aware Gaze Estimation via Deep Ensembles"** also demonstrated remarkable generalizability. In this paper, we leverage CLIP to improve discriminability and generalization of gaze estimation network. 

\textbf{Pre-trained Vision-language Models.}
Recently, the CLIP**Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** trained on large-scale image-text pairs have attracted increasing attentions. Because of its powerful visual and semantic representation capabilities, CLIP has been transferred to various vision tasks, such as objection detection**Jiang et al., "Cross-Modal Object Detection with Transformers"**, semantic segmentation**Fu et al., "Segmentation-aware Image Translation for Unsupervised Multi-task Learning"**, and others**Li et al., "Few-Shot Learning via Adversarial Weight Sharing"**. Especially, CLIP also shows surprising capacities on quantified vision tasks, \eg, depth estimation**Bao et al., "Depth Estimation with Transformers"**, 3D hand poses estimation**Xu et al., "3D Hand Pose Estimation in the Wild"**, etc. Moreover, researchers were attempting to take advantages of the pre-trained CLIP to predict gaze directions. Wang \etal **Wang et al., "CLIP-Gaze: Leveraging Pre-trained Vision-Language Models for Gaze Estimation"** designed a linguistic description generator to produce text signals with coarse gaze directional cues. And then a cross-attention condenser was designed to finely recalibrate the visual and text representations, enhancing the learning quality of gaze features. Yin \etal **Yin et al., "Feature Separation Loss for Purifying Gaze-Relevant Features"** designed a feature separation loss by employing CLIP text encoder to generate gaze distractors from diverse language descriptions, which aims at purifying the gaze-relevant feature via pushing away it from gaze-irrelevant features. In this paper, we adapt CLIP to gaze estimation with an innovative collaborative enhancing strategy, in which the CLIP is regarded as an assistance to enhance the obtained gaze features.