\begin{abstract}
The complex application scenarios have raised critical requirements for precise and generalizable gaze estimation methods. Recently, the pre-trained CLIP has achieved remarkable performance on various vision tasks, but its potentials have not been fully exploited in gaze estimation. In this paper, we propose a novel \textbf{CLIP-driven Dual Feature Enhancing Network (CLIP-DFENet)}, which boosts gaze estimation performance with the help of CLIP under a novel ‘main-side’ collaborative enhancing strategy. Accordingly, a \textit{Language-driven Differential Module (LDM)} is designed on the basis of the CLIP's text encoder to reveal the semantic difference of gaze. This module could empower our Core Feature Extractor with the capability of characterizing the gaze-related semantic information. Moreover, a \textit{Vision-driven Fusion Module (VFM)} is introduced to strengthen the generalized and valuable components of visual embeddings obtained via CLIP’s image encoder, and utilizes them to further improve the generalization of the  features captured by Core Feature Extractor. Finally, a robust Double-head Gaze Regressor is adopted to map the enhanced features to gaze directions. Extensive experimental results on four challenging datasets over within-domain and cross-domain tasks demonstrate the discriminability and generalizability of our CLIP-DFENet.
\end{abstract}