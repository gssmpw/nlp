{\Large \textbf{Appendix}} 

\section{Proof of AV's Regret.} \label{app:av}
{\bf Proxy in the System Dynamics.} In the linear case, we first derive the resulting state transition model when AV is planning for the future steps while using the prediction of HV's action.
The corresponding state dynamics can be written as, i.e., after observing $x(t)$,
\begin{align*}
    \hat{x}(t+1) =& A\hat{x}(t) + B_Au_A(t)+B_H\hat{u}_H(t)\\
    =& A{x}(t) + B_Au_A(t)+B_H{u}_H(t) + B_H\epsilon_A(t)\\
    :=& x(t+1) + B_H\epsilon_A(t)
\end{align*}
where $x(t+1)$ is the true state when AV and HV takes action $u_A(t)$ and $u_H(t)$.

Then at the next step, we have,
\begin{align*}
     \hat{x}(t+2) =& A\hat{x}(t+1) + B_Au_A(t+1)+B_H\hat{u}_H(t+1)\\
     = & A\hat{x}(t+1) + B_Au_A(t+1)+B_H\hat{u}_H(t+1)\\
     = & Ax(t+1) +B_Au_A(t+1)+B_H{u}_H(t+1) + AB_H\epsilon_A(t) + B_H\epsilon_A(t+1)
\end{align*}

It can be seen that the estimated state and the real state has the following relationship,
\begin{align}
    \hat{x}(t+l) = x(t+l) + \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i). \label{eqn:app:proxymodel}
\end{align}

{\bf Quantify the Regret.} Recall the definition of the regret (performance gap), i.e., 
\begin{align*}
\operatorname{Reg}_A(T) :=& \mathbf{E}_{x\sim \rho_0} [ \frac{1}{T} \sum_{t=1}^T \left(V^{*}(x(t))- V^{\hat{\pi}}(x(t))\right)]\\
\operatorname{Reg}_A(t) \triangleq & \underbrace{ V^{*}(x(t)) - V^{\pi_A}(x(t))}_{\text{FA Error}}+ \underbrace{V^{\pi_A}(x(t))- V^{\hat{\pi}_A}(x(t)}_{\text{Modeling Error and Lookahead}}\\
:=& \underbrace{ V^{*}(x(t)) - V^{\pi}(x(t))}_{(1)}+ \underbrace{V^{\pi}(x(t))- V^{\hat{\pi}}(x(t))}_{(2)} \numberthis \label{eqn:twoterms}
\end{align*}
For simplicity, we define the following notations,
\begin{align*}
    \hat{\tau}&~~~\text{trajectory obtained by running $\hat{\pi}_{A}$ with function approximation error (FA)}\\
    \tau & ~~~\text{trajectory obtained by running $\pi$ with FA error}\\
    \tau^{*}&~~~\text{trajectory obtained by running in $M$ without FA error}\\
    u_t &=(u_A(t),u_H(t))
\end{align*}
Meanwhile, we use $\hat{\pi}$ to denote the policy obtained by running lookahead on a inaccurate model and  ${\pi}$ is the policy using the accurate model. Note that in both cases, the terminal cost are estimated by $\hat{V}$ (with function approximation error).


{\bf Part 1. Impact of the Function Approximation Error.} We first quantify the first term (1) in \Cref{eqn:twoterms} as follows,
\begin{align*}
  V^{*}\left(x_0\right)-V^{{\pi}}\left(x_0\right)  =&\mathbb{E}_{\tau^{*}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L {V}^{*}\left(s_L\right)\right]-\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^{{\pi}}\left(x_L\right)\right] \\
 =&\mathbb{E}_{\tau^*}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right]-\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right] \\
 &+\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right]-\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^{{\pi}}\left(x_L\right)\right] \\
 =&\mathbb{E}_{\tau^* }\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right]-\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right]\\
&+\gamma^L \mathbb{E}_{{\tau}}\left[V^*\left(x_L\right)-V^{{\pi}}\left(x_L\right)\right] \numberthis \label{eqn:threeterms}
\end{align*}

\underline{ Assumptions on the approximation error.} We assume that the function approximation error is $\epsilon_v$ with mean $\mu_v$ and variance $\Sigma_v$, i.e.,
\begin{align*}
    V^{*}(x)-\hat{V}^{}(x) = \epsilon_v(x)
\end{align*}

Bringing the above relation to the first two terms of  \Cref{eqn:threeterms} gives us,
\begin{align*}
    \mathbb{E}_{\tau^* }\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right] & = \mathbb{E}_{\tau^* }\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L \hat{V}^{}(x)(x_L) + \gamma^L \epsilon_v(x_L)\right]\\
    \mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L V^*\left(x_L\right)\right] &= \mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L \hat{V}^{}(x)\left(x_L\right) + \gamma^L \epsilon_v(x_L)\right]
\end{align*}

Then we have,
\begin{align*}
    & V^{*}\left(x_0\right)-V^{{\pi}}\left(x_0\right) \\
    &= \mathbb{E}_{\tau^* }\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L \hat{V}^{}(x)(x_L) + \gamma^L \epsilon_v(x_L)\right]\\
    &\quad -\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L \hat{V}^{}(x)\left(x_L\right) + \gamma^L \epsilon_v(x_L)\right]\\
    &\quad +\gamma^L \mathbb{E}_{{\tau}}\left[V^*\left(x_L\right)-V^{{\pi}}\left(x_L\right)\right]\\
    &=\mathbb{E}_{\tau^* }\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L\hat{V}^{}(x)(x_L) \right]\\
    & \quad -\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L \hat{V}^{}(x)\left(x_L\right)\right]+\gamma^L\left(\mathbb{E}_{\tau^{*}}[ \epsilon_v(x_L)]-\mathbb{E}_{\tau^{}}[  \epsilon_v(x_L)]\right)\\
    &\quad + \gamma^L \mathbb{E}_{{\tau}}\left[\hat{V}\left(x_L\right) +\epsilon_v(x_L)-V^{{\pi}}\left(x_L\right)\right]\\
    &= \mathbb{E}_{\tau^* }\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L\hat{V}^{}(x)(x_L) \right]-\mathbb{E}_{{\tau}}\left[\sum \gamma^t r\left(x_t, u_t\right)\right]\\
    &\quad+\gamma^L\mathbb{E}_{\tau^{*}}[ \epsilon_v(x_L)] - \gamma^L \mathbb{E}_{{\tau}}\left[V^{{\pi}}\left(x_L\right)\right]\\
    &=\left(\mathbb{E}_{\tau^* }\left[ \sum \gamma^t r\left(x_t, u_t\right)\right]-\mathbb{E}_{\tau }\left[ \sum \gamma^t r\left(x_t, u_t\right)\right]\right)\\
    &\quad+\gamma^L \left(\mathbb{E}_{\tau^* }\left[\hat{V}^{}(x)(x_L) \right]-\mathbb{E}_{\tau }\left[\hat{V}^{}(x)(x_L) \right]\right)+\gamma^L\mathbb{E}_{\tau^{*}}[ \epsilon_v(x_L)]
\end{align*}

\underline{First term: (1) $\left(\mathbb{E}_{\tau^* }\left[ \sum \gamma^t r\left(x_t, u_t\right)\right]-\mathbb{E}_{\tau }\left[ \sum \gamma^t r\left(x_t, u_t\right)\right]\right)$.}  

Assume the reward function is bounded by $R_{\min}\leq r(x,u) \leq R_{\max},\forall~(x,u)$.  Then we have 
\begin{align*}
  \frac{1-\gamma^L}{1-\gamma}(R_{\min}-R_{\max})  \leq  \text{(1)} \leq \frac{1-\gamma^L}{1-\gamma}R_{\max}
\end{align*}

\underline{Second term: (2) $\gamma^L \left(\mathbb{E}_{\tau^* }\left[\hat{V}^{}(x)(x_L) \right]-\mathbb{E}_{\tau }\left[\hat{V}^{}(x)(x_L) \right]\right)$.} By assuming the function approximation value is bounded by $[\hat{V}_{\min},\hat{V}_{\max}]$, we have,
\begin{align*}
   \gamma^L(\hat{V}_{\min} -\hat{V}_{\max})   \leq \text{(2)}\leq \gamma^L \hat{V}_{\max} 
\end{align*}

\underline{Second term: (3) $\gamma^L\mathbb{E}_{\tau^{*}}[ \epsilon_v(x_L)]$} 
\begin{align*}
  \gamma^L \epsilon_{v,\min} \leq \text{(3)} \leq \gamma^L \epsilon_{v,\max} 
\end{align*}

Alternatively we have 
\begin{align*}
    \text{(3)} = \gamma^L \mu_v
\end{align*}


By combing all three parts, we have the upper bound and lower bound as follows,
\begin{align*}
    V^{*}\left(x_0\right)-V^{{\pi}}\left(x_0\right)  \leq &\frac{1-\gamma^L}{1-\gamma}R_{\max}+\gamma^L \hat{V}_{\max} + \gamma^L \epsilon_{v,\max}\\
    V^{*}\left(x_0\right)-V^{{\pi}}\left(x_0\right)  \geq  &\frac{1-\gamma^L}{1-\gamma}(R_{\min}-R_{\max}) + \gamma^L(\hat{V}_{\min} -\hat{V}_{\max})  + \gamma^L \epsilon_{v,\min}
\end{align*}

{\bf Part 2. The Impact of the Modeling Error in the $L$-step Planning.} Now we are ready to quantify the second term in \Cref{eqn:twoterms}. 

We first define $U_l$ as follows. For any $0 \leq l \leq L$, define $U_l$ to be the $l$-step value expansion that rolls out the true model $P$ for the first $l$ steps and the approximate model $\hat{P}$ for the remaining $L-l$ steps:

\begin{align*}
U_l & =\sum_{t=0}^{l-1} \gamma^t \mathbb{E}_{x_t \sim P_t^\pi(\cdot \mid x)}\left[R^\pi\left(x_t\right)\right]+\sum_{t=l}^{L-1} \gamma^t \mathbb{E}_{x_t \sim \hat{P}_{t-l}^\pi \circ P_l^\pi(\cdot \mid x)}\left[R^\pi\left(x_t\right)\right] \\
&+\gamma^L \mathbb{E}_{x_L \sim \hat{P}_{L-l}^\pi \circ P_l^\pi(\cdot \mid x)}\left[\hat{V}\left(x_L\right)\right],
\end{align*}

where $ \hat{P}_{L-l}^\pi \circ P_l^\pi(\cdot \mid x)$ denotes the distribution over states after rolling out $l$ steps with $P$ and $t-l$ steps with $\hat{P}$.
\begin{align*}
    \hat{P}_{t-l}^\pi \circ P_l^\pi(\cdot \mid x)=\sum_{x^{\prime} \in \mathcal{X}} P_l^\pi\left(x^{\prime} \mid x\right) \hat{P}_{t-l}^\pi\left(\cdot \mid x^{\prime}\right)
\end{align*}

Then we have,
\begin{align*}
    U_L = &  V^{{\pi}}(x(t))\\
    U_0 = & V^{\hat{\pi}}(x(t))
\end{align*}

Hence we have,
\begin{align*}
      V^{{\pi}^{}}(x(t))- V^{\hat{\pi}}(x(t)) = U_L-U_0=\sum_{l=0}^{L-1} U_{l+1}-U_{l}
\end{align*}

To analyze each term in the sum, we re-arrange $U_l$ in the following ways
\begin{align}
& U_l=\sum_{t=0}^{l-1} \gamma^t \mathbb{E}_{x_t \sim P_t^\pi(\cdot \mid x)}\left[R^\pi\left(x_t\right)\right]+\gamma^l \mathbb{E}_{x_l \sim P_l^\pi(\cdot \mid x)}\left[{V}^{\hat{\pi}}_{L-l}\left(x_l\right)\right]  \label{eqn:arrange1}\\
& U_l=\sum_{t=0}^l \gamma^t \mathbb{E}_{x_t \sim P_t^\pi(\cdot \mid x)}\left[R^\pi\left(x_t\right)\right]+\gamma^{l+1} \mathbb{E}_{x_{l+1} \sim \hat{P}^\pi \circ P_l^\pi(\cdot \mid x)}\left[{V}^{\hat{\pi}}_{L-l-1}\left(x_{l+1}\right)\right] . \label{eqn:arrange2}
\end{align}
where we denote ${V}^{\hat{\pi}}_{L}\left(x_l\right):=\sum_{t=0}^{L-1} \gamma^t \mathbb{E}_{x_t \sim \hat{P}_t^\pi(x)}\left[R^\pi\left(x_t\right)\right]+\gamma^L \mathbb{E}_{x_L \sim \hat{P}_L^\pi(x)}\left[\hat{V}\left(x_H\right)\right]$. Note that $\hat{V}$ is not the same as $V^{\hat{\pi}}$, where the latter represents the value of running the current policy $\hat{\pi}$ with $L$ step lookahead planning over a inaccurate model with a terminal cost estimation $\hat{V}$.

Now applying \Cref{eqn:arrange2} to $U_{l}$ and \Cref{eqn:arrange1} to $U_{l+1}$, then we have,
\begin{align*}
U_{l+1}-U_{l}=&
\sum_{t=0}^l \gamma^t \mathbb{E}_{x_t \sim P_t^\pi(\cdot \mid x)}\left[R^\pi\left(x_t\right)\right]+\gamma^{l+1} \mathbb{E}_{x_{l+1} \sim P_{l+1}^\pi(\cdot \mid x)}\left[{V}_{\hat{P}, L-l-1}^\pi\left(x_{l+1}\right)\right]\\
& -\sum_{t=0}^l \gamma^t \mathbb{E}_{x_t \sim P_t^\pi(\cdot \mid x)}\left[R^\pi\left(x_t\right)\right]-\gamma^{l+1} \mathbb{E}_{x_{l+1} \sim \hat{P}^\pi \circ P_l^\pi(\cdot \mid x)}\left[{V}_{\hat{P}, L-l-1}^\pi\left(x_{l+1}\right)\right] \\
= & \gamma^{l+1} \mathbb{E}_{x_l \sim P_l^\pi(\cdot \mid x), u_l \sim \pi\left(\cdot \mid x_l\right)}\left[\mathbb{E}_{x^{\prime} \sim P\left(\cdot \mid x_l, u_l\right)}\Biggl[{V}^{\hat{\pi}}_{L-l-1}\left(x^{\prime}\right)\right]\\
&-\mathbb{E}_{x^{\prime} \sim \hat{P}\left(\cdot \mid x_l, u_l\right)}\left[{V}^{\hat{\pi}}_{L-l-1}\left(x^{\prime}\right)\right]\Biggr]\\
=& \gamma^{l+1} \mathbb{E}_{x_l \sim P_l^\pi(\cdot \mid x), u_l \sim \pi\left(\cdot \mid x_l\right)}\left[
\int_{x'} \left({P}\left(x' \mid x_l, u_l\right)-\hat{P}\left(x' \mid x_l, u_l\right)\right){V}^{\hat{\pi}}_{L-l-1}(x') dx' \right]\\
:=&\gamma^{l+1} \mathbb{E}_{x_l \sim P_l^\pi(\cdot \mid x), u_l \sim \pi\left(\cdot \mid x_l\right)}[D(x_{l+1}\vert P,\hat{P})],
\end{align*}
where we denote $D(x_{l+1}\vert P,\hat{P})=\int_{x'} \left({P}\left(x' \mid x_l, u_l\right)-\hat{P}\left(x' \mid x_l, u_l\right)\right){V}^{\hat{\pi}}_{L-l-1}(x') dx'$.

 It can be seen that $D(x_{l+1})$ is directly relevant to the lookahead length $l$ and the modeling error $\hat{P}-P$. In the linear case, the  longer lookahead length makes the difference between $P$ and $\hat{P}$ more significant, i.e., \Cref{eqn:app:proxymodel}. Next, we give the expression for $D(x_{l+1}\vert P,\hat{P})$ to show its relation with the lookahead length $L$. 

 \begin{align*}
     D(x_{l+1}\vert P,\hat{P})=&\int_{x'} \left({P}\left(x' \mid x_l, u_l\right)-\hat{P}\left(x' \mid x_l, u_l\right)\right){V}^{\hat{\pi}}_{L-l-1}(x') dx'
 \end{align*}

\underline{Linear Case.} Recall \Cref{eqn:app:proxymodel}, 
\begin{align*}
    \hat{x}(t+l) = x(t+l) + \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i),
\end{align*}
where $\epsilon_A \sim \mathcal{N}(\mu_A,\Sigma_A)$. Then we have,
\begin{align*}
    \hat{P}\left(x' \mid x_l, u_l\right) = \mathbb{P}(\sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) = x'-Ax_l-Bu_l)
\end{align*}

Given $\epsilon_A$ follows a Gaussian distribution, we have
\begin{align*}
    \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) \sim \mathcal{N} (\sum_{i=1}^{l}A^{i-1}B_H \mu_A, \sum_{i=1}^{l} A^{i-1}B_H \Sigma_A (A^{i-1}B_H )^{\top}  )
\end{align*}

Then we have
\begin{align*}
     \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) \sim \mathcal{N} (\sum_{i=1}^l C_i \mu_A,  \sigma_A^2 \sum_{i=1}^l C_iC_i^{\top} )
\end{align*}
where $C_i : = A^{i-1}B_H$. 

For simplicity, assume $A^{i-1}B_H = I$, then we have
\begin{align}
      \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) \sim \mathcal{N} (l \cdot \mu_A, l \sigma_A^2 I ) \label{eqn:app:accumulation1}
\end{align}

Meanwhile, we have the underlying true dynamics of the system is 
\begin{align*}
    x(t+1) = Ax(t) + B_Au_A(t) + B_H u_H(t)  + \epsilon_p(t).
\end{align*}
Then we have,
\begin{align*}
    {P}\left(x' \mid x_l, u_l\right) = \mathbb{P}(\epsilon_p = x'-Ax_l-Bu_l)
\end{align*}
Notice that $\epsilon_p \sim \mathcal{N}(0,\sigma_p^2 I)$.

Then the difference between $P$ and $\hat{{P}}$ boils down to the difference between two Normal distribution. We have the following results,
\begin{align*}
      W(\hat{P},P)  = \sqrt{\|\sum_{i=1}^LC_i\mu_A\|_2^2 + \|(\sigma_A \left(\sum_{i=1}^lC_iC_i^{\top}\right) - \sigma_p) I\|_F^2}
\end{align*}


Or in the simple case
\begin{align*}
    W(\hat{P},P) = \sqrt{l^2 \|\mu_A\|_2^2 + \|(\sigma_A \sqrt{l} - \sigma_p) I\|_F^2}
\end{align*}



Assume the value function is bounded by $V_{\max}=\sup _h\left\|\hat{V}^{\hat{\pi}}_ {l}\right\|_L$, i.e., the maximum Lipschitzness of the estimated value function over all possible horizons. Now we have,
\begin{align}
    U_{l+1}-U_{l} \leq V_{\max}\gamma^{l+1} \mathbb{E}_{x_{l+1}}[D(x_{l+1})]\leq V_{\max}\gamma^{l+1} \mathbb{E}_{x_{l+1}}[W(\hat{P},P)]
\end{align}
where $W$ is the Wasserstein distance.

Then we have
\begin{align*}
    U_L-U_0 \leq V_{\max} \sum_{l=1}^{L} \gamma^l \mathbb{E}_{x_{l+1},u_{l+1}\sim {\pi}}[W(\hat{P}(\cdot \vert x,u),P(\cdot \vert x,u))]
\end{align*}

{\bf Combining two parts gives upper bound.}

By adding the upper bound of the two parts, we obtain the upper bounds and lower bound for the performance difference,

\underline{Linear Case, no FA error.} In this case, we have the regret as follows,

\begin{align*}
  \operatorname{Reg}_A(t) \leq \frac{1-\gamma^L}{1-\gamma}R_{\max}+{V}_{\max}\sum_{l=1}^L \sqrt{l^2 \|\mu_A\|_2^2 + \|(\sigma_A \sqrt{l} - \sigma_p) I\|_F^2}
\end{align*}


\underline{Linear Case, with FA error.} 

\begin{align*}
    \operatorname{Reg}_A(t) \leq \frac{1-\gamma^L}{1-\gamma}R_{\max}+{V}_{\max}\sum_{l=1}^L \sqrt{l^2 \|\mu_A\|_2^2 + \|(\sigma_A \sqrt{l} - \sigma_p) I\|_F^2}  + \gamma^L \epsilon_{v,\max} 
\end{align*}



\underline{Non-linear Case, with FA error.} 
\begin{align*}
    \operatorname{Reg}_A(t) \leq & \frac{1-\gamma^L}{1-\gamma}R_{\max}+\gamma^L \hat{V}_{\max} + \gamma^L \epsilon_{v,\max} \\
    +&  V_{\max} \sum_{l=1}^{L} \gamma^l \mathbb{E}_{x_{l+1},u_{l+1}\sim {\pi}}[W(\hat{P}(\cdot \vert x,u),P(\cdot \vert x,u))]
\end{align*}

\underline{\bf Treat the Prediction Error as a Diffusion Process.} Recall the diffusion process:
\begin{align*}
    dx(t) =& \mu dt + \sigma d W(t) \\
    \text{Drift: } \mu t =& \mathbb{E}[x(t)-x(0)]\\
    \text{Variance: } \sigma^2t =& \operatorname{Var}[x(t)-x(0)] 
\end{align*}
where $W(t)$ is a wiener process, i.e., $dW(t)=\varepsilon_t\sqrt{dt},~\varepsilon_t\sim\mathcal{N}(0,1)$. Alternatively in the discrete case, we have $x(t)-x(0)=\mu t + \sigma W(t)$. In our setting, due to the compounding error in the lookahead planning, the difference between true state and predicted state becomes more and more different as the time horizon expands. Define the difference between the true state and predicted state as $y(t)=\hat{x}(t)-x(t)$, then we assume the prediction error follows a diffusion process, i.e., 
\begin{align*}
    dy(t)=\mu_A dt + \Sigma_A dW(t),~ y(0)=0
\end{align*}
For simplicity, assume $\Sigma_A = \sigma_A^2  I$.

Then we can obtain that at time $t$, the prediction error follows a Gaussian distribution, i.e., $y(t) \sim \mathcal{N}(t\mu_A, t\sigma_A^2   I)$.  Then we have the Wasserstein distance $\hat{P}$ and $P$ as follows \cite{delon2020wasserstein},

\begin{align*}
   W(\hat{{P}}_{l+1}-P) = \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I + \sigma_p^2 I -2\sigma_A^2\sigma_p^2 \frac{(1+l)l}{2} I \right)}
\end{align*}
Finally, we obtain the upper bound for the non-linear case as follows:

\begin{align*}
     \operatorname{Reg}_A(t) \leq & \frac{1-\gamma^L}{1-\gamma}R_{\max}+\gamma^L \hat{V}_{\max} + \gamma^L \mu_{v,t}\\
     &+ V_{\max} \sum_{l=1}^L \gamma^l \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I + \sigma_p^2 I -2\sigma_A^2\sigma_p^2 \frac{(1+l)l}{2} I \right)}
\end{align*}


{\bf Regret over time $T$.} Now we consider the regret over time $t=1,2,\cdots,T$. Assume the current policy is $\hat{\pi}_t$ and the learned value function is $\hat{V}_t$. Recall that AV chose its policy in the following way,
\begin{itemize}
    \item Estimate value function using policy $\hat{\pi}_t$: 
    \begin{align*}
        \hat{Q}_{t+1} =& \Biggl( \sum_{i=1}^{L} \mathbb{E} \left[\gamma^i r_A(\hat{x}(t+i),u_A(t+i),\hat{u}_H(t+i))\right] &\\
        & + \gamma^{L+1} \hat{Q}_t(\hat{x}(t+L+1),\hat{u}(t+L+1)) \Biggl),
    \end{align*}   
    \item Derive the greedy policy (as in MPC):
    \begin{align*}
        \hat{\pi}_{t+1} = \arg \max_{u_A(t+1)} \max_{u_A(t+2),\cdots,u_A(t+L)} \hat{Q}_{t+1} 
    \end{align*}
\end{itemize}
It can be seen that due to the update of the value function $\hat{Q}$. Next we show the difference between $\hat{Q}_{t+1}$ and $\hat{Q}_{t}$. Recall that we assume $V^{*}-\hat{V}_t = \epsilon_v$, and we denote (with abuse of notation) $Q^{*}-\hat{Q}_t = \epsilon_{t}$. Now we have
\begin{align*}
    \hat{Q}_{t+1} - Q^{*} = \gamma^{L+1}\epsilon_t + \sum_{i=1}^L \gamma^i(\hat{r}_A-r_A),
\end{align*}
where we denote $\hat{r}_A = r_A(\hat{x}(t+i),u_A(t+i),\hat{u}_H(t+i))$ and $r_A = r_A(\hat{x}(t+i),u_A(t+i),{u}_H(t+i))$. Similar to the analysis to HV regret, we have 
\begin{align*}
  \mu_{v,t+1}:= \mathbb{E} [ \epsilon_{t+1}] \leq \gamma^{L+1}\mu_{v,t} + \frac{\gamma(1-\gamma^L)}{1-\gamma}(\operatorname{Reg}_A)
\end{align*}
where $\operatorname{Reg}_A = ms\sigma_A^2 + (s+\lambda)\|\mu_A\|^2$.

Now we are ready to derive the regret for AV as follows,
\begin{align*}
    \operatorname{Reg}_A(T) =& \frac{1}{T}\sum_{t=1}^T \operatorname{Reg}(t)\\
    \leq & \Biggl(\frac{1-\gamma^L}{1-\gamma}R_{\max}+\gamma^L \hat{V}_{\max} \\
    & +V_{\max} \sum_{l=1}^L \gamma^l \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I + \sigma_p^2 I -2\sigma_A^2\sigma_p^2 \frac{(1+l)l}{2} I \right)}\Biggl) \\
    &+\frac{\gamma^L}{T} \left( \frac{\gamma^{L+1}(1-\gamma^{T(L+1)})}{1-\gamma^{L+1}}\mu_{v,0} + \sum_{k=0}^T \prod_{i=0}^k \left(\gamma^{i(L+1)} \cdot \frac{\gamma(1-\gamma^L)}{1-\gamma} \operatorname{Reg}_A \right)  \right)\\
    =&  \sum\nolimits_{l=1}^L (V_{\max} + lR_{\max})\gamma^l \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I \right)} \\
    &+ \frac{\gamma^L}{T}  \left( \Gamma \mu_{v,0} + \Lambda (s_{\max}M\sigma_A^2 + (s_{\max} + \lambda)\|\mu_A\|^2) \right),
\end{align*}
 where   $\Gamma : =  \frac{\gamma^{L+1}(1-\gamma^{T(L+1)})}{1-\gamma^{L+1}}$ and $\Lambda:=\sum_{k=0}^T \prod_{i=0}^k \left(\gamma^{i(L+1)} \cdot \frac{\gamma(1-\gamma^L)}{1-\gamma}\right)$.


\section{Proof of HV's Regret.} \label{app:hv}
Due to the bounded rationality, HV does not choose the optimal action and thus introduces the regret as follows
\begin{align*}
  \operatorname{Reg}_H(T) := & \frac{1}{T} \sum_{t=1}^T \\
    \operatorname{Reg}_H(t) =& \mathbb{E} \left[ \Phi(x(t),u_H^{*}(t),\hat{u}_A(t))-\Phi(x(t),u_H(t),\hat{u}_A(t))\right],
\end{align*}
where we assume that HV can observe the action of AV in a timely manner. Next, we impose the assumptions on the reward structure to be quadratic, i.e.,
\begin{align}
    r_H(x,u_A,u_H) = f_H(x,u_A) + u_H^{\top}S_Hu_H,
\end{align}
where $S_H$ are positive definite matrices. 


Then we have the regret for HV to be,
\begin{align*}
      \operatorname{Reg}_H(t)  =& \mathbb{E} \left[(u_H^{*}(t))^{\top}S_H u_H^{*}(t) - (u_H(t)^{})^{\top}S_H u_H(t)^{} \right]\\
      =& \frac{1}{2}\mathbb{E} \Biggl[\left( u_H^{*}(t)+u_H^{}(t) \right)^{\top} S_H \left( u_H^{*}(t)-u_H^{}(t) \right)\\
      &+ \left( u_H^{*}(t)-u_H^{}(t) \right)^{\top}S_H\left( u_H^{*}(t)+u_H^{}(t) \right)  \Biggr] \\
      =& \frac{1}{2} \mathbb{E} \left[\left( \left( u_H^{*}(t)+u_H^{}(t) \right)^{\top}S_H \epsilon_H(t) + \epsilon_H(t)^{\top}S_H \left( u_H^{*}(t)+u_H^{}(t) \right)   \right)\right]\\
      =& \frac{1}{2}\mathbb{E} \left[\left( \left( 2u_H^{*}(t)+\epsilon_H(t) \right)^{\top}S_H \epsilon_H(t) + \epsilon_H(t)^{\top}S_H \left( 2u_H^{*}(t)+\epsilon_H(t) \right)   \right)\right]\\
      =& \mathbb{E} \left[\epsilon_H(t)^{\top}S_H \epsilon_H(t) + u_H^{*}(t)^{\top}S_H \epsilon_H(t) + \epsilon_H(t)^{\top} S_Hu_H^{*}(t) \right]\\
      =& \operatorname{Tr}(S_H\Sigma_H) + \mu_H^{\top}S_H\mu_H + u_H^{*}(t)^{\top}S_H \mu_H + \mu_H^{\top} S_Hu_H^{*}(t)
\end{align*}

Furthermore, we have the following assumptions on the matrices
\begin{itemize}
    \item $\Sigma_H = \sigma_H I $, where $I$ is an identity matrix.
    \item The dimension of the action space is $n$
    \item $0<s_{\min} \leq \operatorname{eig}(S_H) \leq s_{\max}$, where $\operatorname{eig}(S_H)$ is the eigenvalue of $S_H$.
    \item There exist a matrix $C$ such that $C_{\min}\mu_H\leq u_H^{*}(t) \leq C\mu_H$, notice that $u_H^{*}$ depends on AV's action. 
\end{itemize}

With those assumptions in place, we have the upper bound for the regret as follows:
\begin{align*}
     \operatorname{Reg}_H(T) \leq ns_{\max} \cdot \sigma_H^2 + (s_{\max}+\lambda) \|\mu_H\|^2
\end{align*}
where $\lambda_{} := \sqrt{\operatorname{eig}_{\max}(C^{\top}S_HC) \cdot s_{\max}}$. 


\section{Proof of Corollary \ref{corollary:ah}} \label{app:coro}

We denote the regret  for the whole system as $\mathcal{R}_{A-H}(T)$, i.e., $\mathcal{R}_{A-H}(T):= $
\begin{align*}
    \frac{1}{T}\sum_{t=1}^T \bigg( \underbrace{ \mathbf{E} \left[V^{*}(x \vert u_H^{*}{(t)}) - V^{\hat{\pi}_t} (x)\right]}_{(i)} +  \underbrace{\mathbf{E} \left[ \Phi(x(t),u_A^{*}(t),u_H^{*}(t)) - \Phi(x(t),u_A(t),u_H(t)) \right]}_{(ii)} \bigg),
\end{align*}
where $V^{*}(x \vert u_H^{*}{\color{black}(t)})$ is the optimal value function when HV also takes the optimal action $u_H^{*}{\color{black}(t)}$, e.g., $u_H^{*}{\color{black}(t)}=\arg\max_{u_H} \Phi(x(t),u_A^{*}(t),u_H)$. Notice that both term (i) and term (ii) can be decomposed in the following way
\begin{align*}
  & V^{*}(x \vert u_H^{*}{(t)}) - V^{\hat{\pi}_t} (x) \\
    = & \underbrace{V^{*}(x \vert u_H^{*}{(t)}) - V^{*}(x \vert u_H{(t)})}_{(a)} + \underbrace{V^{*}(x \vert u_H{(t)}) - V^{\hat{\pi}_t} (x)}_{(b)}
\end{align*}
\begin{align*}
    &\Phi(x(t),u_A^{*}(t),u_H^{*}(t)) - \Phi(x(t),u_A(t),u_H(t)) \\
    =& \underbrace{\Phi(x(t),u_A^{*}(t),u_H^{*}(t)) - \Phi(x(t),u_A(t),u_H^{*}(t))}_{(a)} +\underbrace{\Phi(x(t),u_A(t),u_H^{*}(t)) - \Phi(x(t),u_A(t),u_H(t)) }_{(b)}
\end{align*}
In the decomposition above, term (b) is related to AV and HV's regret, respectively. Now we quantify term (a).

{\bf AV.} Term (a) is related to $V^{*}$ function and we need to show that due to the bounded rationality of HV, it has direct impact on AV's overall best possible performance, i.e., denote the trajectory collected by running through MDP $M$ with HV's action $u_H^{*}$ as  $\tau^{\operatorname{opt}}$, while the trajectory collected with HV's action $u_H^{}$ is denoted as $\tau$, then we have
    \begin{align*}
        (a) =&\mathbb{E}_{\tau^{\operatorname{opt}}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L {V}^{*}\left(x_L\right)\right] - \mathbb{E}_{\tau^{}}\left[\sum \gamma^t r\left(x_t, u_t\right)+\gamma^L {V}^{*}\left(x_L\right)\right]\\
     =& \mathbb{E}_{\tau^{\operatorname{opt}}}\left[\sum \gamma^t r\left(x_t, u_t\right)\right] - \mathbb{E}_{\tau^{}}\left[\sum \gamma^t r\left(x_t, u_t\right)\right] + \mathbb{E}_{\tau^{\operatorname{opt}}}\left[\gamma^L {V}^{*}\left(s_L\right)\right] - \mathbb{E}_{\tau^{}}\left[\gamma^L {V}^{*}\left(s_L\right)\right]\\
     =& \sum_{i=1}^L \gamma^i \left( \eta^{i,\operatorname{opt}}(x,u) - \eta^i(x,u)  \right) r(x,u) + \gamma^L \int_x \mathbb{P} [{x \vert s_{L-1},u^{*}_{L-1} }]-\mathbb{P} [{x \vert x_{L-1},u^{}_{L-1} }]V^{*}(x)\\
     \leq & \sum_{i=1}^L \gamma^i \cdot i \epsilon_m r_{\max} + \gamma^L L V_{\max }\epsilon_m,
    \end{align*}
    where $\epsilon_m$ is the total variation between $M$ and $\hat{M}$ due to HV's noisy action as the disturbance is upper bounded by $\epsilon_m$. The explicate formulation of the upper bound is available in Prop. 2.1 \cite{devroye2018total}.


{\bf HV.} Term (a) is related to $\Phi^{}$ and we have $ (a) \leq R_{\max} $

Denote $ \scriptstyle\Psi_A(l) = \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I \right)}$ and $\scriptstyle \Psi_H(l) = \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_H\|^2_2 + \operatorname{tr}\left( \sigma_H^2 \frac{(1+l)l}{2}I \right)}$. For ease of presentation, we use notation $\textstyle\Psi_v=\Gamma \mu_{v,0} + \Lambda (s_{\max}M\sigma_A^2 + (s_{\max} + \lambda)\|\mu_A\|^2)$ to represent the regret term in Theorem \ref{thm:av} and $\Xi_H=s_{\max}M \cdot \sigma_H^2 + (s_{\max}+\lambda_H) \|\mu_H\|^2$ to represent the term in Theorem \ref{thm:hv}. Hence, building upon our results in Theorem \ref{thm:av}  and Theorem \ref{thm:hv}, we give the upper bound of  $\mathcal{R}_{A-H}(T)$ 

    \begin{align*}
    \mathcal{R}_{A-H}(T) \leq &   \sum_{l=1}^L (V_{\max} + l R_{\max})\gamma^l (2\Psi_A(l) + \Psi_H(l))+\Xi_H  + \frac{1}{T} \gamma^L \Psi_v
\end{align*}

\section{General Setting with Time-varying Prediction Error Distribution.} \label{app:prediction_error}
{\bf Multimodal Predictions in Autonomous Driving.} In the context of trajectory prediction in autonomous driving, multimodality arises from the fact that, given the observed information, there can be multiple plausible future trajectories for the HV. Consequently, the AV necessitates the ability to learn from the historical interactions with HV and adjust its own prediction model. Toward this end, we consider the general setting for AV's prediction error distribution, i.e., we assume the prediction error follows a time-variant distribution as follows,
\begin{align}
    \epsilon_A(t) \sim \mathcal{N}(\mu_A(t),\sigma_A^2(t)I),
\end{align}
where $\mu_A(t)$ is the time-varying mean and $\sigma_A^2(t)$ is the time-varying variance. In what follows, we demonstrate the major modification (in blue) of the proof of regret derived in the main paper. 

\underline{Linear Case.} Recall \Cref{eqn:app:proxymodel}, 
\begin{align*}
    \hat{x}(t+l) = x(t+l) + \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i),
\end{align*}
where $ {\color{blue} \epsilon_A(t) \sim \mathcal{N}(\mu_A(t),\Sigma_A(t))}$. Then we have,
\begin{align*}
    \hat{P}\left(x' \mid x_l, u_l\right) = \mathbb{P}(\sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) = x'-Ax_l-Bu_l)
\end{align*}

Given $\epsilon_A$ follows Gaussian distribution, we have
\begin{align*}
    \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) \sim \mathcal{N} (\sum_{i=1}^{l}A^{i-1}B_H {\color{blue} {\color{blue}\mu_A(i)}}, \sum_{i=1}^{l} A^{i-1}B_H {\color{blue}\Sigma_A(i)} (A^{i-1}B_H )^{\top}  )
\end{align*}

Then we have
\begin{align*}
     \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i) \sim \mathcal{N} (\sum_{i=1}^l C_i  {\color{blue}\mu_A(i)},  \sum_{i=1}^l   {\color{blue}\sigma_A^2(i)} C_iC_i^{\top} )
\end{align*}
where $C_i : = A^{i-1}B_H$. 

For simplicity, assume $A^{i-1}B_H = I$, then we have
\begin{align}
      \sum_{i=1}^{l}A^{i-1}B_H {\color{blue}\epsilon_A(t+l-i)} \sim \mathcal{N} ( {\color{blue}\sum_{i=1}^l\mu_A(i)}, \sum_{i=1}^l  {\color{blue}\sigma_A^2(i)} I ) \label{eqn:app:accumulation2}
\end{align}

It can be seen that the accumulation error term \Cref{eqn:app:accumulation2} (\Cref{eqn:app:accumulation1}) is the major change that will affect the theoretical analysis. It is worth mentioning that in the non-linear case, we consider a special type of time-varying prediction error, i.e., 
\begin{align}
    \mu_A(t) =& t\mu_A \\
    \sigma_A^2(t) =& t\sigma_A^2
\end{align}




\section{Generalization of AV and HV's Learning Strategies.}\label{app:general}
{
{\bf AV's Learning Strategies.} We clarify that \Cref{eqn:pihat} can be degenerated into many commonly used RL algorithms, for instance,
    \begin{itemize}
        \item (Model-free Case) Set $L=1$, \Cref{eqn:pihat}  is the model-free Q-function update and our regret analysis still holds. 
        \item (Actor-Critic Case) Let $Q$-function and policy $\pi$ be parameterized by $\theta$ and $\phi$, respectively, Then \Cref{eqn:pihat}  can be learned by using Actor-Critic, i.e., in the actor step,  $\theta$ is updated by maximizing the $L$-step look-ahead objective and $\phi$ is updated using policy gradient. Note that in this case, the approximation error in both Actor and Critic update can be encapsulate into $\epsilon_{v,t}$ as in Assumption 1.  Our proof of the regret remains the same.
    \end{itemize}

{\bf HV's Learning Strategies.} In \Cref{eqn:phi}, we consider AV's decision making to be $N$-step planning while we do not impose any constrains on the length of $N$. In particular, when $N\to \infty$, the decision making of HV is related to dynamic programming (assume the model is available) and otherwise, the decision making of AV is in the same spirit of Model Predictive Control (MPC). 

}


\section{Experimental Settings.} \label{app:exp}
In this section, we include the detailed parameter setup when conducting the experiments. The default setting is as follows: 
\begin{itemize}
    \item $\gamma = 0.85$
\item $L = 5$
\item $\mu_{v,0} = 10$
\item $V_{\max}= 10$
\item $R_{\max} = 1$
\item $\mu_A = 1.8$
\item $\sigma_A = 1$
\item $M = 10$
\end{itemize}


In Figure \ref{fig:3d} we choose the parameters as follows: 

\begin{itemize}
    \item $\gamma = 0.5$
    \item $T = 5$
    \item $V_{\max}= 10$
    \item $R_{\max} = 1$
    \item $\sigma_A = 0.1$
    \item $\sigma_H=0.1$
    \item $s_{\max}=2$
    \item $\lambda = 10$
    \item $M = 10$
    \item $l=2$
\end{itemize}

{ We list the parameter settings of \Cref{fig:goodharts},   \Cref{fig:goodharts_av_regret} and \Cref{fig:avhvL} in \Cref{tab:set1}, \Cref{tab:set2} and \Cref{tab:set3}, respectively.


\begin{table}[h!]
    \centering
    \begin{tabular}{c|cccccc}
        Parameter & Setting 1 & Setting 2 & Setting 3 & Setting 4 & Setting 5 \\ \toprule 
        $\gamma$ & 0.85 & 0.85 & 0.85 & 0.85 & 0.55  \\
        $\mu_{v,0}$ & 10 & 10 & 10 & 10 & 10 \\
        $V_{\max}$ & 10 & 10 & 20 & 10 & 10\\
        $R_{\max}$ & 1 & 5 & 1 & 1 & 1\\
        $\mu_A$ & 0.8 & 1.8 & 1.8 & 1.8 & 1.8\\
        $\sigma_A$ & 1 & 1 & 1 & 1 & 1 \\
        $M$ & 10 &  10 &  10 &  10 &  10 \\
    \end{tabular}
    \caption{Parameter Settings in \Cref{fig:goodharts}}
    \label{tab:set1}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{c|cccccc}
        Parameter & Setting 1 & Setting 2 & Setting 3 & Setting 4 & Setting 5 \\ \toprule 
        $\gamma$ & 0.85 & 0.85 & 0.85 & 0.85 & 0.55  \\
        $\mu_{v,0}$ & 10 & 10 & 10 & 10 & 10 \\
        $V_{\max}$ & 10 & 10 & 20 & 10 & 10\\
        $R_{\max}$ & 1 & 5 & 1 & 1 & 1\\
        $\mu_A$ & 0.8 & 1.8 & 1.8 & 1.8 & 1.8\\
        $\sigma_A$ & 1 & 1 & 1 & 1 & 1 \\
        $M$ & 10 &  10 &  10 &  10 &  10 \\
        $T$ & 10 & 5 & 5 &5 &5 \\
    \end{tabular}
    \caption{Parameter Settings in \Cref{fig:goodharts_av_regret}}
    \label{tab:set2}
\end{table}


\begin{table}[h!]
    \centering
    \begin{tabular}{c|cccccc}
        Parameter & Setting 1 & Setting 2 & Setting 3 & Setting 4 & Setting 5 \\ \toprule 
        $\gamma$ & 0.85 & 0.85 & 0.85 & 0.85 & 0.55  \\
        $\mu_{v,0}$ & 10 & 10 & 10 & 10 & 10 \\
        $V_{\max}$ & 10 & 10 & 20 & 10 & 10\\
        $R_{\max}$ & 1 & 5 & 1 & 1 & 1\\
        $\mu_A$ & 0.8 & 1.8 & 1.8 & 1.8 & 1.8\\
        $\sigma_H$ &  0.1 &  0.5 &  0.1 &  0.1 &  0.1\\
        $\sigma_A$ & 1 & 1 & 1 & 1 & 1 \\
        $M$ & 10 &  10 &  10 &  10 &  10 \\
        $T$ & 5 & 10  & 10 & 10 & 10\\
    \end{tabular}
    \caption{Parameter Settings in \Cref{fig:avhvL}}
    \label{tab:set3}
\end{table}


\section{Extension beyond Two Agent Case.}\label{app:beyond}

ur analysis approach is feasible to extend beyond one AV and one HV setting. Assume there are $N_H$ number of HVs and $N_A$ number of AVs in the mixed traffic system. With abuse of notations, we define the action vector for AVs and HVs as follows, at time step $t$, $u_H(t) = [u_{H,1}(t), u_{H,2}(t),\cdots, u_{H,N_H}(t)]$, $ u_A(t) = [u_{A,1}(t), u_{A,2}(t),\cdots, u_{A,N_A}(t)]$. By defining the prediction error as in \Cref{eqn:prediction_error} and HVs' bounded rationality as in \Cref{subsec:42}, our analysis framework still can be applied. The dimension of the approximation error term and the bounded rationality term is thus $N_A$ and $N_H$ times higher than the two-agent case. Hence, the resulting regret in Theorem 3 and Theorem 4 are $N_A$ and $N_H$ times higher than the two-agent case. 

}