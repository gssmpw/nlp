\documentclass[final]{article}
%\documentclass[]{article}

%%% ready for submission %%%
\usepackage{cpal_2025}
\usepackage{soul}
\usepackage{amsmath,amssymb,mathtools,amsthm}
\usepackage{xcolor}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\usepackage{hyperref}
\usepackage{framed}
\usepackage{caption}
% \usepackage{subcaption}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[capitalize,noabbrev]{cleveref}

%%% to compile a preprint version, e.g., for submission to arXiv, add the [preprint] option %%%
%\usepackage[preprint]{cpal_2025}

%%% to compile a camera-ready version, add the [final] option %%%



%add packages
\usepackage{url}

\title{Heterogeneous Decision Making in Mixed Traffic: Uncertainty-aware Planning and Bounded Rationality}


\author{%
  Hang Wang\textsuperscript{1} ~Qiaoyi Fang\textsuperscript{1}, ~Junshan Zhang\textsuperscript{1} \\
  \textsuperscript{1}University of California, Davis \\
  \texttt{\{whang,qyfang,jazh\}@ucdavis.edu}
}

\begin{document}


\maketitle


\begin{abstract}
  The past few years have witnessed a rapid growth of the deployment of automated vehicles (AVs). Clearly, AVs and human-driven vehicles (HVs) will co-exist for many years, and AVs will have to operate around HVs, pedestrians, cyclists, and more, calling for fundamental breakthroughs in AI designed for mixed traffic to achieve mixed autonomy.  Thus motivated, we study heterogeneous decision making by AVs and HVs in a mixed traffic environment, aiming to capture the interactions  between human and machine decision-making and develop an AI foundation that enables vehicles to operate safely and efficiently. There are a number of challenges to achieve mixed autonomy, including 1) humans drivers make driving decisions with bounded rationality, and it remains open to develop accurate models for HVs' decision making; and 2) uncertainty-aware planning plays a critical role for AVs to take safety maneuvers in response to the human behavior. In this paper,  we introduce a formulation of AV-HV interaction, where the HV makes decisions with bounded rationality and the AV employs uncertainty-aware planning based on the prediction on HV's future actions. We conduct a comprehensive analysis on AV and HV's learning regret to answer the questions: 1) \textit{How does the learning performance depend on HV's bounded rationality and AV's planning}; 2) \textit{How do different decision making strategies impact the overall learning performance}?  Our findings reveal some intriguing phenomena, such as Goodhart's Law in AV's learning performance and compounding effects in HV's decision making process. By examining the dynamics of the regrets, we gain insights into the interplay between human and machine decision making.  
\end{abstract}


\section{Introduction}\label{intro}
% Automated vehicle (AV) is emerging as the fifth screen in our everyday life, after movies, televisions, personal computers, and mobile phones {\cite{yurtsever2020survey,parekh2022review}}. 
% The anticipated benefits from AV technology are immense, especially in terms of safety and economic impact \cite{talebpour2016influence,wu2017flow,hoogendoorn2014automated,ye2018modeling}. For example, in 2014, {the National Highway Traffic Safety Administration} (NHTSA) estimated the annual economic loss and society harm of crashes in the United States alone at \$871 billion in 2010, or 1.9\% of the GDP. Incredibly, the overwhelming majority of the crashes are preventable. As more technologies continue to deliver new safety and efficiency features to modern vehicles,  the advent of  AVs equipped with a myriad of sensors and  AI technology has ushered in a new era of smart mobility. While 30+ states in the US have already enacted AV legislation, experts agree that the new phase of rapid global development of AVs must overcome a wide range of technical challenges \cite{yuen2021factors, jing2020determinants,litman2020autonomous}. In particular, maintaining safety while still being sufficiently efficient  in a mixed-traffic environment is probably the most fundamental  challenge for automated mobility.   Indeed, AVs will have to operate around {human-driven vehicles} (HVs), pedestrians, cyclists, motorcyclists, and more, for many years to come.



Automated vehicle (AV) is emerging as the fifth screen in our everyday life, after movies, televisions, personal computers, and mobile phones {\cite{yurtsever2020survey,parekh2022review}}. Their potential impact on safety and economic efficiency is substantial \cite{talebpour2016influence,wu2017flow,hoogendoorn2014automated,ye2018modeling}. For instance, the National Highway Traffic Safety Administration (NHTSA) reported that preventable crashes in the United States caused \$871 billion in economic and societal losses in 2010—approximately 1.9\% of the GDP. While over 30 U.S. states have enacted AV legislation and AI-equipped vehicles continue to advance, experts acknowledge significant technical challenges remain \cite{yuen2021factors,jing2020determinants,litman2020autonomous}. Perhaps the most fundamental challenge is achieving both safety and efficiency in mixed-traffic environments, as AVs must coexist with human-driven vehicles (HVs), pedestrians, cyclists, and other road users for the foreseeable future.


The complicated interactions between HVs and AVs could have significant implications on the traffic efficiency given their different decision making characters. As such, a fundamental understanding on the heterogeneous decision making in the interplay, especially the impact of HVs' decision making with bounded rationality on AVs' performance, is crucial for achieving efficient mixed autonomy. 


Existing works on modeling the interaction between AV and HV largely fall within the realm of conventional game formulation, in which both agents try to solve the dynamic game and adopt Nash equilibrium strategies \cite{tian2022safety,hang2020human,fisac2019hierarchical,sadigh2016planning}. This line of formulation faces  the challenge of  prohibitive computational complexity \cite{daskalakis2009complexity}. Needless to say, the decision making of HV and AV are  different by nature. As supported by evidence from psychology laboratory experiments \cite{simon1979rational,kahneman2003maps,kahneman1982judgment}, human decision-making is often \textit{short-sighted} and deviates from Nash equilibrium due to their \textit{bounded rationality} in the daily life  \cite{selten1990bounded,kalantari2023modelling,wright2010beyond}. In particular, HV's bounded rationality is unknown a prior and it remains challenging to develop an accurate model for HV's decision making.  As a result, it is sensible for AVs' decision making  to leverage \textit{uncertainty-aware planning} for safety maneuvers in response to human behavior \cite{liu2017path,schwarting2019social}. Clearly, the heterogeneous decision making by HVs and AVs exposes intrinsic complexities in the mixed autonomy.

Along the line of \cite{sadigh2016planning,sadigh2018planning}, we consider a two-agent system with  one AV and one HV, where the HV takes the action by planning for a short time horizon, and the decision-making is sub-optimal and noisy due to bounded rationality. The AV utilizes uncertainty-aware lookahead planning based on predictions of the HV's future actions. The primary objective of this study is to understand the performance of heterogeneous decision making in the mixed autonomy by answering the following questions:
  {  \it 1) How does the learning performance depend  on HV's bounded rationality and AV's planning?
    2) How do different decision making strategies {between AV and HV} impact the overall learning performance? }

The main contributions of this paper can be summarized as follows:

{ \bf (1) We first focus on the characterization of the regrets for both the HV and  the AV, based on which we identify the impact of \textit{bounded rationality} and \textit{planning horizon} on the learning performance.} In particular, we present the upper bound on the regret, first for the linear system dynamics model case and then for the non-linear case. We start with the linear case, and show the accumulation effect due to the AV's prediction error and its impact on AV's learning performance. Building on the insight from the linear case, we  model the prediction error as a diffusion process in the non-linear case to capture the accumulation effect. By studying the upper bound, we identify the compounding effects in HV's decision making due to bounded rationality and the Goodhart's law in AV's decision making associated with the planning horizon.  


{\bf  (2) We study the impact of HV's bounded rationality on the overall learning performance and the regret dynamics of AV and HV.} We first establish the upper bound on the regret of the overall system due to HV's bounded rationality and AV's uncertainty-aware planning. Our regret bound naturally decompose into two parts, corresponding to the decision making of AV and HV, respectively. We examine the regret dynamics of the overall system theoretically and show how do different learning strategies { between AV and HV} affect the learning performance during each individual interaction through empirical study. {The experiments details are available in \Cref{app:exp}.}


\section{Related Work}
% {\bf Mixed Autonomy.} Previous studies on mixed autonomy generally consider specific dynamics models for AV and HV. For instance, \cite{zhu2018analysis} uses Bando's model to describe the AV and HV's behavior and demonstrates the traffic flow empirically on car-following model.  \cite{mahdinia2021integration} conducts a empirical study on the impact of AV on HV's performance in terms of the driving volatility measures while assuming a specific AV's acceleration model. Meanwhile, the impact of humans in the mixed traffic is empirically examined through high-fidelity driving simulator \cite{sharma2018human}. \cite{zheng2020analyzing} proposes a stochastic model for mixed traffic flow to investigate the interaction between HV and AV while taking into account the uncertainty of human driving behavior. It is shown that AV has huge impact on the overall traffic stability and HV's behavior through numerical study. \cite{wu2017flow} proposes a modular learning framework for mixed traffic by leveraging deep RL. Experiments show that AV is able to reduce congestion under the intelligent driver model (IDM). Without imposing specific models on HV and AV's decision making dynamics, our work focuses on the performance of different learning strategies in the mixed autonomy.

{\bf Mixed Autonomy.} Prior work on mixed autonomy traffic has primarily focused on specific dynamics models and empirical studies. For instance, \cite{zhu2018analysis} uses Bando's model for vehicle behavior analysis, while \cite{mahdinia2021integration} studies AV's impact on HV driving volatility using predetermined AV acceleration models. The human factor has been examined through high-fidelity driving simulators \cite{sharma2018human}, and stochastic models have been proposed to capture human behavior uncertainty \cite{zheng2020analyzing}. On the learning side, \cite{wu2017flow} demonstrates congestion reduction using deep RL under the intelligent driver model (IDM). Without imposing specific models on HV and AV's decision making dynamics, our work focuses on the performance of different learning strategies in the mixed autonomy.

% {\bf HV-AV Interaction Model.} In related work on modeling the HV-AV interaction, \cite{tian2022safety} uses the general-sum Stackelberg game to account for the human’s influence on AV and computes the backward reachability tube for safety assurances in the interaction.  Similarly, \cite{sadigh2016planning} formulates the interaction as a two-player game, where the influence between AV and HV are captured in the predefined reward. Considering the vehicles' dynamic driving actions, \cite{fisac2019hierarchical} develops a hierarchical game-theoretic planning scheme and shows the effectiveness of the proposed planning method in the simulation. We note that even though \cite{sadigh2018planning} proposes to use underactuated dynamical system to overcome the limitations of the game formulation, it assumes that both AV and HV making decision in the same strategy, i.e., planning for the same horizon. { The related ad-hoc team problem \cite{mirsky2022survey} mainly focused on the cooperative case,  whereas our setting does not impose assumptions on the cooperation of agents. Meanwhile, Zero-shot coordination \cite{hu2020other} mainly focused on the robustness of the self-play, whereas our work aims to understand the interaction between two agents with different decision makings strategies. Moreover, our work focuses on characterizing the impact of the opponent modeling errors on the learning performance which is also related to opponent modeling \cite{albrecht2018autonomous}.} Despite the rich empirical results in the related filed, e.g., Ad-hoc team problem and zero-shot coordination, we remark that the theoretical analysis on the interaction between AV and HV is still lacking, especially considering their different decision making. Moreover, our work deviates from the conventional game setting and  aims to takes steps to quantify the impact of AV and HV's different decision making on the traffic system.  

{\bf HV-AV Interaction Model.} For modeling HV-AV interactions specifically, several game-theoretic approaches have been proposed. \cite{tian2022safety} and \cite{sadigh2016planning} use Stackelberg and two-player game formulations respectively, while \cite{fisac2019hierarchical} develops a hierarchical planning scheme. Although \cite{sadigh2018planning} attempts to address game formulation limitations using underactuated dynamical systems, it assumes identical decision-making horizons for both vehicle types. While related fields like ad-hoc team problems \cite{mirsky2022survey} and zero-shot coordination \cite{hu2020other} provide empirical insights, they focus on either cooperative scenarios or self-play robustness. Our work differs by analyzing the interaction between agents with different decision-making strategies without assuming cooperation, particularly examining the impact of opponent modeling errors on learning performance \cite{albrecht2018autonomous}. Despite the rich empirical results in the related field, e.g., Ad-hoc team problem and zero-shot coordination, we remark that the theoretical analysis on the interaction between AV and HV is still lacking, especially considering their different decision making. Moreover, our work deviates from the conventional game setting and  aims to takes steps to quantify the impact of AV and HV's different decision making on the traffic system.  


% {\bf Model-based RL.} Model-based RL (MBRL), which leverages a model of the environment, is promising for real-world applications thanks to its data efficiency \cite{moerland2023model}. In particular, our work is relevant to MBRL with lookahead planning. For instance, \cite{sikchi2022learning,xiao2019learning} use lookahead policy to rollout the dynamics model into the future $H$ steps in order to find the action sequence with highest return. A value function is also attached at the end of the rollout to estimate the terminal cost. Moreover, \cite{sikchi2022learning}
% provides the sub-optimality gap of the learned policy under an approximate model and approximate value function. Our work is different from previous work on MBRL since in our case, AV has access to the environment dynamics while the modeling error exists due to the unknown bounded rationality of HV. Meanwhile, our theoretical analysis uses regret to evaluate the performance of the decision making, in which the value function is updated during the learning process, resulting in the changing function approximation error. As a result, the technique used in our proof is significant different than previous work \cite{xiao2019learning,sikchi2022learning,luo2022survey}. 

{\bf Model-based RL.} MBRL with lookahead planning has shown promise in real-world applications due to its data efficiency \cite{moerland2023model}. Recent works \cite{sikchi2022learning,xiao2019learning} utilize lookahead policies with future rollouts and terminal value functions to optimize action sequences. While \cite{sikchi2022learning} provides sub-optimality analysis under approximate models, our work differs fundamentally: we assume AV has access to environment dynamics but faces uncertainty from HV's bounded rationality. Moreover, our theoretical analysis uses regret with dynamically updating value functions, requiring significant different analytical techniques than previous work \cite{xiao2019learning,sikchi2022learning,luo2022survey}.



\section{Preliminary} 
{\bf {Stochastic Game.}}  We consider the {Stochastic Game (SG)} defined by the tuple $\mathcal{M}:=(\mathcal{X},\mathcal{U}_A, \mathcal{U}_H, P, r_A, r_H ,\gamma)$ \cite{shoham2008multiagent}, where $\mathcal{U}_A$ and $\mathcal{U}_H$ are the action space for AV and HV, respectively. Meanwhile, we assume the action space for HV and AV are with the same cardinality $M$ and let $\mathcal{U} = \mathcal{U}_A \times \mathcal{U}_H$. We denote $\mathcal{X}$ as the state space that contains both AV and HV's states. $P(x'|x,u_A,u_H): \mathcal{X} \times \mathcal{U} \times \mathcal{X} \rightarrow [0,1]$ is the probability of the transition from state $x$ to state $x'$ when AV applies action $u_A$ and HV applies action $u_H$. $r_H(x,u_A,u_H): \mathcal{X} \times \mathcal{U} \rightarrow [0,R_{\max}]$, $r_A(x,u_A,u_H): \mathcal{X} \times \mathcal{U} \rightarrow [0,R_{\max}]$ is the corresponding reward for HV and AV. {$\gamma \in (0,1)$} is the discount factor. We denote the AV's policy by $\pi: \mathcal{X} \times \mathcal{U}$ and use $\hat{u}_H(t)$ to represent AV's prediction on HV's real action $u_H(t)$ at time step $t$. We use $\rho_0$ to represent the initial state distribution. %{Furthermore, we compare our problem formulation and Dec-POMDP in detail in  \Cref{app:diff}.}

{\bf Value Function.} Given AV's policy $\pi$, we denote the value function $V^{\pi}(x): \mathcal{X} \to \mathbb{R}$ as
\begin{align*}
	\mathbf{E} \left[\sum_{t=0}^{\infty} \gamma^t r_A(x(t),u_A(t),u_H(t)) \vert x(0)=x, u_H(t)\right],
\end{align*}
to measure the average accumulative reward staring from state $x$ by following policy $\pi$. The expectation is taken over $u_A(t) \sim \pi$ and $ x({t+1})\sim P(x,u_H(t),u_A(t))$. 

We assume the maximum value of the value function to be $V_{\max}$. We define $Q$-function $Q^{\pi}(x,u_A,u_H): \mathcal{X} \times \mathcal{U} \to \mathbb{R}$ as $Q^{\pi}(x,u_A,u_H) = \mathbf{E}_{\color{blue}\pi}[\sum_{t=0}^{\infty}\gamma^{t}r_A(t) \vert x(0)=x,u_A(0)=u_A,u_H(0)=u_H]$ to represent the expected return when the action $u_A$, $u_H$ are chosen at the state $x$. The objective of AV is to find an optimal policy $\pi^{*}$ {given HV's action }$u_H$ such that the value function is maximized, i.e., 
\begin{align*}
  \pi^{*} = \arg  \max_{\pi} \mathbf{E}_{x \sim \rho_{0}, u_A\sim \pi(\cdot \vert x,u_H)} [Q^{\pi}(x,u_A,u_H)]. \numberthis \label{eqn:mapping}
\end{align*}
Similarly, the objective function can be written as $\mathbf{E}_{x\sim \rho_0}[{V}^{\pi}(x)] $.


{\bf Notations.}  We use $\|\cdot\|$ or $\|\cdot\|_2$ to represent the Euclidean norm. $\|\cdot\|_F$ is used to  denote Frobenius norm. $\mathcal{N}(\mu,\sigma^2)$ is the normal distribution with mean $\mu$ and variance $\sigma^2$. $I$ is an identity matrix. 

\subsection{Modeling AV-HV Interaction: Heterogeneous Decision Making}
In this section,  we examine in detail the interaction between one AV and one HV in a mixed traffic environment. More specifically, we have the following models to capture the interplay between human and machine decision making in the mixed autonomy. 
 
{\bf AV's Decision Making via $L$-step lookahead planning.}  At time step $t$, after observing the current state $x(t)$, AV will first need to predict HV's future action $\hat{u}_H(t+i), i=0,1,2,\cdots, L-1$ due to the unknown bounded rationality of HV. Based on this prediction, AV strives to find an action sequence that maximizes the cumulative reward with the predicted HV actions using trajectory optimization. In order to facilitate effective long-horizon reasoning, we augment the planning trajectory with a terminal value function approximation $\hat{Q}_{t-1}$, which is obtained by evaluating the policy obtained from previous time step. For convenience, we denote policy $\hat{\pi}_t$ as the solution to maximizing the $L$-step lookahead planning objective, i.e.,
% \begin{align*}
%   & \hat{Q}_{t}(x(t),u_A(t),\hat{u}_H(t)) \\
%    =& \mathbf{E} [\sum_{i=0}^{L-1} \gamma^i r_A(\hat{x}(t+i),u_A(t+i),\hat{u}_H(t+i))\\
%    &+ \gamma^{L} \hat{Q
%    }_{t-1}(\hat{x}(t+L),u_A(t+L),\hat{u}_H(t+L))]
% \end{align*}
\begin{align*}
  \hat{Q}_{t}(t) =& \mathbf{E} [\sum_{i=0}^{L-1} \gamma^i r_A(t+i) + \gamma^{L} \hat{Q}_{t-1}(t+L)], 
\end{align*}
where $ r_A(t+i) :=  r_A(\hat{x}(t+i),u_A(t+i),\hat{u}_H(t+i))$ and $ \hat{Q}_{t-1}(t+L):= \hat{Q}_{t-1}(\hat{x}(t+L),u_A(t+L),\hat{u}_H(t+L))$. Meanwhile, $\hat{x}(t+i)$ is the state that the system will end up with if HV chose action $\hat{u}_H(t+i)$ and AV chose ${u}_A(t+i)$ at time step $t+i$. 

Then the policy $\hat{\pi}(x(t) \vert {u_H})$ is obtained by,
\begin{align*}
 \hat{\pi}(x(t) \vert {u_H}) =  \arg \max_{u_A(t)} \max_{\{ u_A(t+l)\}_{l=1}^{L-1}} \hat{Q}_{t}(t), \numberthis  \label{eqn:pihat}
\end{align*}
where $u_H:=\{u_H(t)\}_{t=1}^T$. It can be seen that AV's policy is conditioned on HV's policy via $\hat{u}_H(t)$. 

\begin{remark}
    \Cref{eqn:pihat} can also be degenerated into many commonly used RL algorithms such as actor-critic and we include the discussion in \Cref{app:general}.
\end{remark}

% , e.g.,  Assume AV's prediction of HV's action has $\epsilon_A(t)$ difference from the underlying real action, i.e.,
%     \begin{align}
%         \hat{u}_H(t+l)= u_H(t+l)+\epsilon_A(t+l),~~l=1,2,\cdots,L,
%     \end{align}
%     where $\epsilon_A(t)\sim \mathcal{N}(\mu_A,\Sigma_A)$ is the AV's prediction error and we assume it is the zero mean Gaussian noise.
    
    
%     At time step $t$, after observing the action taken by HV, AV will choose the action as follows,
% \begin{align*}
%     \hat{u}_A(t+1) = &\arg \max_{u_A(t+1)} \max_{u_A(t+2),\cdots,u_A(t+L)}\\
%    & \left( \sum_{i=1}^{L} \mathbb{E} \left[\gamma^i r_A(\hat{x}(t+i),u_A(t+i),\hat{u}_H(t+i))\right] + \gamma^{L+1} \bar{Q}(\hat{x}(t+L+1),\hat{u}(t+L+1)) \right),
% \end{align*}
% where $\bar{Q}$ is the learned Q-function approximation and the expectation is taken over the randomness of the system dynamics ($\epsilon_p$).

{\bf HV's Decision Making with Bounded Rationality.} HV's decision making has distinct characteristics. As mentioned by the pioneering study of behavior theory  \cite{simon1957models}, individuals have constraints in both their \textit{understanding of their surroundings} and their \textit{computational capacities}.  Additionally, they face search costs when seeking sophisticated information in order to devise optimal decision rules. Therefore, we propose to model human as responding to robots actions with bounded rationality. We additionally assume HV choose the action by planning for a short time horizon, in contrast to the long horizon planning in AV's decision making. Specifically, at time step $t$, HV chooses the (sub-optimal) action by planning ahead for $N$ steps, i.e.,
\begin{align*}
    &\Phi_H(x(t),u_A(t),u_H(t)) 
    :=\sum_{i=0}^{N-1} r_H(x(t+i),u_A(t+i),u_H(t+i)) \label{eqn:phi}\numberthis
\end{align*}
Meanwhile, to underscore the impact of the bounded rationality in HV's decision making, we use  $u_H^{*}(t):=\arg\max_{u_{H}(t)}\max_{u_{H}(t+1),\cdots, u_{H}(t+N-1)} \Phi_H$ to denote the optimal solution of \Cref{eqn:phi} and $u_H(t)$ to denote the sub-optimal action chosen by HV. Note that {HV's policy is conditioned on AV's behavior $u_A(t)$ and} we assume the time horizon $N$ is short enough such that the human can effectively extrapolate the robot’s course of action, i.e., $u_A(t+i)$ is the true action taken by AV. We remark that we do not assume HV has access to the overall plan of AV but only the first few time steps. It has been shown in previous work \cite{sadigh2018planning} that predicting a short-term sequence of controls is manageable for human, e.g., the AV will merge into HV's lane after a short period of time.

\section{Characterization of HV and AV's Learning Performance}
\subsection{Regret of AV with $L$-step Lookahead Planning}
In this subsection, we study the impact of bounded rationality and uncertainty-aware planning on the performance of AV. To this end, we first quantify the performance gap between choosing optimal actions and sub-optimal actions, for given  HV's behavior \textit{fixed}. Therefore, conditioned on HV's action $u_H={\{u_H(t)\}_{t=1}^T}$, the regret for $T$ interaction of AV is defined as 
\begin{align*} 
    \mathcal{R}_A(T \vert u_H) = &
    \frac{1}{T} \sum_{t=1}^T \operatorname{Reg}_A(t) := {\mathbf{E}_{x\sim \rho_0}\left[\frac{1}{T}\sum_{t=1}^T \left(V^{*}(x \vert u_H{(t)}) - V^{\hat{\pi}_t}(x)\right)\right]}, 
\end{align*}
where we use $V^{*}(x \vert u_H{(t)})$ to denote the optimal value function attained by the optimal policy $\pi^{*}$ given HV's action $u_H$.  $\hat{\pi}_t$ is the policy obtained in the $t$-th time step while AV solving $L$-step lookahead planning objective \Cref{eqn:pihat} based on its prediction on HV's future actions. { In particular, at each time step $t$, conditioned on HV's action $u_H(t)$, the optimal value function $V^{*}(x\vert u_H(t))$ is determined by choosing a policy $\pi_A^{*}(t)$ from policy space $\Pi_A$. Hence, the regret defined for AV is closely related to adaptive regret \cite{loftin2022impossibility}.} Without loss of generality, we have a general model on HV's prediction error.

{\bf AV's Prediction of HV's Actions.}  Since HV's bounded rationality is unknown to AV and the accurate model on HV is thus challenging to obtain, we assume AV's prediction of HV's action $\hat{u}_H(t+l)$ has $\epsilon_A(t)$ difference from the HV's underlying real (sub-optimal) action $u_H(t+l)$, i.e.,
    \begin{align}
        \hat{u}_H(t+l)= u_H(t+l)+\epsilon_A(t+l),~~l=0,\cdots,L, \label{eqn:prediction_error}
    \end{align}
where we assume $\epsilon_A(t)\sim \mathcal{N}(\mu_A,\sigma_A^2I)$ to be AV's prediction error. Given the prediction on HV's actions, we first quantify the performance gap $\operatorname{Reg}_A(t)$ of AV at each time-step $t$. Then we characterize the AV's learning performance in terms of regret $\mathcal{R}_A(T \vert u_H) $ in the non-linear case while considering the adaptive nature of AV's learning process, e.g., the time-varying function approximation error.

{\bf An Illustrative Example: Performance Gap in the Linear Case.} For ease of exposition, we first consider the linear system dynamics model with system parameter $A, B_H, B_A$, i.e.,
\begin{align*}
    x(t+1) = Ax(t) + B_Au_A(t) + B_H u_H(t).
\end{align*}
In the linear case,  it is easy to see   the resulting state transition model when AV is planning for the future steps based on the prediction of HV's action: for $l=1,2,\cdots$,
\begin{align}
    \hat{x}(t+l) = x(t+l) + \sum_{i=1}^{l}A^{i-1}B_H\epsilon_A(t+l-i),\label{eqn:proxymodel}
\end{align}
where we denote ${x}(t)$ as the real state when AV choose $u_A(t)$ and HV chooses $u_H(t)$. It can be seen that due to the \textit{error accumulation} in AV's prediction, the state transition model tends to depart from the underlying true model significantly over prediction horizon $l$. 

\begin{remark}[Generalization of the Prediction Error Assumption]
  For ease of exposition, in  \Cref{eqn:prediction_error}, we assume the AV's prediction error follows the same distribution. While in practice, AV's prediction error may evolve over time as it accumulates more interaction history during their interactions. our analysis only requires minor modification to address the time-varying  case, e.g., $\epsilon_A(t) \sim \mathcal{N}(\mu_A(t),\sigma_A^2(t)I)$.   The major change lies in the joint distribution of the error accumulation term in \Cref{eqn:proxymodel}. The detailed steps are deferred in  \Cref{app:prediction_error}.
\end{remark}

Next, we present the performance for a single interaction, considering assumptions about function approximation error as follows.
\begin{assumption}\label{asu:fa}
    The value function approximation error in the $t$-th step is $\epsilon_{v,t}(x):=V^{*}(x)-\hat{V}_t(x)$ with mean $\mathbf{E}_x[\epsilon_{v,t}(x)]=\mu_{v,t}$. The value function is upper bounded by $V_{\max}$.
\end{assumption}

{In practice,  the optimal value function can be estimated by using Monte-Carlo Tree Search (MCTS) over a class of policies or the offline training with expert prior \cite{gelly2011monte}.} Denote $C_i = A^{i-1}B_H$ and $\mathcal{C}_A(l)=\|\sum\nolimits_{i=1}^lC_i\mu_A\|_2^2 + \|\sigma_A \left(\sum\nolimits_{i=1}^lC_iC_i^{\top}\right)  \|_F^2$. Then, we have the following results on the performance gap in time-step $t$.
\begin{lemma}[AV's Performance Gap in the Linear Case.] Suppose Assumption \ref{asu:fa} holds. Then we have the following upper bound  on the performance gap of AV in the $t$-th step:
\begin{align*}
 &  \mathbf{E}\left[V^{*}(x \vert u_H)-V^{\hat{\pi}_t}(x)\right]
 \leq \gamma^L \mu_{v,t} 
+ { \sum\nolimits_{l=1}^L (V_{\max} + lR_{\max}) \gamma^l\sqrt{\mathcal{C}_A(l)  }}.
\end{align*} \label{lemma:av}
\vspace{-0.15in}
\end{lemma}
{\bf Error Accumulation in Planning.} In Lemma \ref{lemma:av}, {we present a tight bound on the performance gap,} where the first term in the upper bound is associated with the function approximation error and the second term is related to the AV's prediction error on HV's future action. Clearly, increasing the planning horizon $L$ can help to reduce the dependency on the accuracy of function approximation in a factor of $\gamma^L$ while risking the compounding error (the second term). Notably, the function approximation error $\mu_{v,t}$ will change during the learning process (ref. \Cref{eqn:pihat}) and further have impact on AV's performance gap. 


{\bf Performance Gap in the Non-linear Case.} Observing the error accumulation in the linear case (ref. \Cref{eqn:proxymodel}), The disparity between the actual state and the predicted state, denoted as $x(t)-\hat{x}(t)$, tends to grow noticeably with time step $t$. Thus inspired, for the general case where the system model is unavailable, we formulate the prediction error as a diffusion process, i.e., denote $y(t)=x(t)-\hat{x}(t)$, then we have,
\begin{align*}
    dy(t)=\mu_A dt + \Sigma_A^{1/2} dW(t),~ y(0)=0,
\end{align*}
where $t\mu_A$ is the drift term adn $t\Sigma_A :=t\sigma_A^2 I $ is the variance term. $W(t)$ is the Weiner process. 


For simplicity, let $\mathcal{E}_A(l):=\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I \right)$. Then we can have the following results on the performance gap in the non-linear case.  
\begin{lemma}[AV's Performance Gap in Non-linear Case]  \label{lemma:av2}
Suppose Assumption \ref{asu:fa} holds, then we have the upper bound of AV's performance gap in the $t$-th step as follows,
\begin{align*}
     &  \mathbf{E}\left[V^{*}(x \vert u_H)-V^{\hat{\pi}_t}(x)\right] 
\leq   \gamma^L \mu_{v,t}  +  { \sum\nolimits_{l=1}^L (V_{\max} + lR_{\max})\gamma^l \sqrt{\mathcal{E}_A(l)}}.
\end{align*}    
\vspace{-0.2in}
\end{lemma}

{\bf Goodhart's Law and Lookahead Length.} In Lemma \ref{lemma:av2}, %the upper bound of the performance gap exhibits a similar error accumulation structure as in the linear case. More importantly,
we examine the performance of AV through the lens of Goodhart's law, which predicts that increasing the optimization over a proxy beyond some critical point may degrade the performance on the true objective. In our case, the planning over predicted HV actions is equivalent to the optimization on a proxy object. Increasing the planning horizon is corresponding to increase the optimization pressure. As shown in Fig. \ref{fig:goodharts}, where we plot the upper bound of the learning performance by changing different planning horizon $L$, the learning performance of AV clearly demonstrate the Goodhart's law, when increasing the planning horizon will initially help with the learning performance until a critical point. {In practice, adjusting the look-ahead length (e.g., through grid search) is essential to enable AV to achieve the desired performance.}


%\begin{figure*}
%  \centering
%  \subfigure[random caption 1]{\includegraphics[width=0.3\linewidth]{example-image-a}}\hfill
%  \subfigure[random caption 2]{\includegraphics[width=0.3\linewidth]{example-image-a}}
%\end{figure*}


\begin{figure*}[t]
	\centering
	\subfigure[]{\includegraphics[width=0.26\textwidth]{figures/fg_av.pdf}\label{fig:goodharts}}
	\hspace{0.3in}
	 \subfigure[]{\includegraphics[width=0.26\textwidth]{figures/av_goodhart.pdf}\label{fig:goodharts_av_regret}}
    \hspace{0.3in}
	\subfigure[]{
		\includegraphics[width=0.26\textwidth]{figures/av_TvsL.pdf}
		\label{fig:couple}}
    \caption{Numerical results on AV's regret. (a) The impact of planning horizon $L$ on AV's performance gap (ref. Lemma \ref{lemma:av2}). (b) The impact of the planning horizon $L$ on AV's regret $\mathcal{R}_A$. (c) The impact of planning horizon on regret dynamics $\mathcal{R}_A(T)$ during the interactions.}
     \label{fig:actorbias3}
     \vspace{-0.15in}
\end{figure*}





{\bf Regret Analysis in the Non-linear Case.}  To analyze the upper bound on the regret, we first impose the following standard assumptions on the MDP. 
\begin{assumption}[Quadratic Reward Structure] \label{asu:reward}
The reward functions for AV and HV are the quadratic function of AV's action $u_A$ and HV's action $u_H$, respectively, i.e.,
\begin{align*}
    r_H(x,u_A,u_H) =& f_H(x,u_A) + u_H^{\top}S_Hu_H,\quad 
    r_A(x,u_A,u_H)= f_A(x,u_H) + u_A^{\top}S_Au_A,
\end{align*}
    where $S_H$ and $S_A$ are positive definite matrix with largest eigenvalue $s_{\max}$. $f_H$ and $f_A$ are the reward functions that capture the influence of other agent and can be non-linear.
\end{assumption}
We note that Assumption \ref{asu:reward} is commonly used in the analysis of regret especially in model-based RL \cite{abbeel2006using,coates2008learning,kolter2008space} and the studies in mixed traffic \cite{tian2022safety,sadigh2016planning}. {In practice, the estimation of the parameter $S_H$ and $S_A$ can be achieved by various methods, e.g., Inverse Reinforcement Learning \cite{tian2022safety}. Based on our findings in the performance gap, we now have  the following result on the regret corresponding to AV's learning performance. 

Let $C=\max_{u_A} u_A \mu_A^{\top}(\mu_A\mu_A^{\top})^{-1}$ and $M$ be the cardinality of the action space $U_A$ and $U_H$. Denote $\lambda = \sqrt{\operatorname{eig}_{\max}(C^{\top}S_AC)s_{\max}}$, $ \Gamma : = \frac{\gamma^{L+1}(1-\gamma^{T(L+1)})}{1-\gamma^{L+1}}$ and $  \Lambda:=\sum_{k=0}^T \prod_{i=0}^k \left(\gamma^{i(L+1)} \cdot \frac{\gamma(1-\gamma^L)}{1-\gamma}\right)$. Then we have the following result.

\begin{theorem}[Regret on AV's Decision Making] \label{thm:av}
Suppose Assumptions \ref{asu:fa} and \ref{asu:reward} hold, the regret of AV's decision making over $T$ interactions is bounded above by
\begin{align*}
    & \mathcal{R}_A(T) \leq  {\sum\nolimits_{l=1}^L (V_{\max} + lR_{\max})\gamma^l \sqrt{\mathcal{E}_A(l)}} 
    {+ \frac{\gamma^L}{T}  \left( \Gamma \mu_{v,0} + \Lambda (s_{\max}M\sigma_A^2 + (s_{\max} + \lambda)\|\mu_A\|^2) \right)},
\end{align*}
\end{theorem}

{\bf Reduce the Regret by Adjusting the Lookahead Length.} The upper bound in Theorem \ref{thm:av} {is tight and it reveals the impact of the approximation error ($\mu_{v,0}$), prediction error ($\mu_A, \sigma_A$) and lookahead length $L$ on the learning performance.} Specifically, we observe from the second term in the upper bound represents the \textit{accumulation  of the function approximation error}. The first term therein depends on the initial function approximation error  $\mu_{v,0}$  and the last term is the compounding error due to the AV's prediction error during the $T$ times interactions. { Our key observations are as follows: (1) Longer planning horizon, e.g., $L=10$ in Fig. \ref{fig:goodharts_av_regret} and Fig. \ref{fig:couple}, will likely make the prediction error more pronounced and dominate the upper bound.  (2) While in the case when the planning horizon is short}, e.g., $L=1$ in Fig. \ref{fig:goodharts_av_regret} and Fig. \ref{fig:couple}, we observe the function approximation error will likely dominate the upper bound. {The empirical results provide the insights on how to adjust the lookahead length in practice.} For instance, if the function approximation error is more pronounced than the prediction error, it is beneficial to use longer planning horizon $L$. The proof of AV's regret is relegated to  Appendix \ref{app:av}.


% Meanwhile, as illustrated in Fig. \ref{fig:goodharts_av_regret}, the Goodhart's also applies to the AV's regret, where the regret of AV will increase significantly once the planning horizon exceed a critical point. The proof of AV's regret is relegated to  Appendix \ref{app:av}.

\subsection{Regret of HV with Bounded Rationality} \label{subsec:42}
Given AV's action $u_A$, we define the regret for HV conditioned on AV's action $u_A$ as follows:
\begin{align*}
    \mathcal{R}_H(T \vert u_A) = \mathbf{E}_{x(0)\sim \rho_0}\left[ \frac{1}{T} \sum_{t=1}^T ( \Phi_H^{*}(t) - \Phi(t))\right],
\end{align*}
{ where $\Phi_H^{*}(t) := \Phi_H(x(t),u_H^{*}(t),u_A(t))$ is the optimal value and it is determined by choosing a policy $\pi_H^{*}(t)$ from policy space $\Pi_H$ such that $\Phi(x,\pi_A^t,\pi_H)$ is  maximized. }$\Phi(t):=\Phi_H(x(t),u_H(t),u_A(t))$ represents the value achieved when HV chooses sub-optimal action due to bounded rationality. For ease of exposition, we assume HV's decision making is myopic and HV's planning horizon is $N=1$, such that $ \Phi_H(x(t),u_A(t),u_H(t)) := r_H(x(t),u_A(t),u_H(t))$. Meanwhile, we assume HV makes sub-optimal decision as follows,
\begin{align*}
        u_H(x(t),u_A(t)) = u_H^{*}(x(t),u_A(t))+ \epsilon_H(t)
\end{align*}
where $\epsilon_H(t)\sim \mathcal{N}(\mu_H,\Sigma_H)$ is due to bounded rationality of humans and it is not known by AV.

Let $C_H=\max_{u_H} u_H \mu_H^{\top}(\mu_H\mu_H^{\top})^{-1}$ and $\lambda_H = \sqrt{\operatorname{eig}_{\max}(C_H^{\top}S_HC_H)s_{\max}}$, then we have the following results on the upper bound of HV's regret which shows the impact of bounded rationality on HV's performance. {The proof of \Cref{thm:hv} is available in \Cref{app:hv}}.

\begin{theorem}[Regret for HV.] \label{thm:hv}
Suppose Assumption \ref{asu:reward} holds. Then we have the regret of HV's decision making over $T$ interactions to be bounded above by
\begin{align*}
    \mathcal{R}_H(T) \leq  s_{\max}M \cdot \sigma_H^2 + (s_{\max}+\lambda_H) \|\mu_H\|^2
\end{align*}
\end{theorem}
% {\bf Error Accumulation due to Bounded Rationality.} As expected, the upper bound in Theorem \ref{thm:hv} hinges on the bounded rationality ($\mu_H$ and $\sigma_H$) of HV during the decision making. However, it is unclear  

\section{Regret Dynamics in  Mixed Autonomy}
Aiming to understand "{\it How do different decision making strategies impact the overall learning performance?}", especially on the impact of HV's bounded rationality on AV's performance, we study the regret dynamics in this section. More concretely,  we denote the regret  for the whole system as, 
\begin{align*}
&\mathcal{R}_{A-H}(T):= \frac{1}{T}\sum_{t=1}^T \bigg( \underbrace{ \mathbf{E} \left[V^{*}(x \vert u_H^{*}{(t)}) - V^{\hat{\pi}_t} (x)\right]}_{(i)} + { \underbrace{\mathbf{E} \left[ \Phi(x(t),u_A^{*}(t),u_H^{*}(t)) - \Phi(x(t),u_A(t),u_H(t)) \right]}_{(ii)} \bigg)},
\end{align*}
where $V^{*}(x \vert u_H^{*}{(t)})$ is the optimal value function when HV also takes the optimal action $u_H^{*}{(t)}$, e.g., $u_H^{*}{(t)}=\arg\max_{u_H} \Phi(x(t),u_A^{*}(t),u_H)$. Meanwhile $\Phi(x(t),u_A^{*}(t),u_H^{*}(t))$ is the optimal value when AV takes the optimal action $u_A^{*}{(t)} = \arg \max_{u_A} V^{*}(x,u_A,u_H^{*}{(t)})$ (without prediction error or function approximation error) while HV takes optimal action $u_H^{*}$. Intuitively, regret $\mathcal{R}_{A-H}(T)$ is defined as the difference between the best possible outcome, i.e., both AV and HV act and response to each other optimally, and the realized outcome, i.e., AV makes decision with prediction error and function approximation error while HV makes decisions with bounded rationality. Specifically, we note that the regret definition $\mathcal{R}_{A-H}$ can be naturally decomposed into two parts such that term (i) and term (ii) characterize the impact of HV's (AV's) decision making on AV (HV), respectively. 

{\bf Term (i).} Notice that term $(i)$ in $ \mathcal{R}_{A-H}(T)$ can be decoupled as
\begin{align*}
    &V^{*}(x \vert u_H^{*}) - V^{\hat{\pi}_t} (x):=(V^{*}(x \vert u_H^{*})-V^{*}(x \vert u_H^{}))+(V^{*}(x \vert u_H^{})-V^{\hat{\pi}_t} (x)).
\end{align*}
The first term is induced by the sub-optimality of HV while the second term is the performance gap of AV, i.e., $\operatorname{Reg}_A(t)$. 

{\bf Term (ii).} Similarly, we can decouple term $(ii)$ into two parts, i.e.,
\begin{align*}
\left(\Phi(x(t),u_A^{*}(t),u_H^{*}) - \Phi(x(t),u_A^{}(t),u_H^{*})\right)+ \left(\Phi(x(t),u_A^{}(t),u_H^{*}) - \Phi(x(t),u_A(t),u_H(t))\right),
\end{align*}
where the impact of AV's decision making is shown as the first term and the second term is the performance gap of HV, i.e., $\operatorname{Reg}_H(t)$. 

Denote $ \scriptstyle\Psi_A(l) = \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_A\|^2_2 + \operatorname{tr}\left( \sigma_A^2 \frac{(1+l)l}{2}I \right)}$ and $\scriptstyle \Psi_H(l) = \sqrt{\frac{(1+l)^2l^2}{4}\|\mu_H\|^2_2 + \operatorname{tr}\left( \sigma_H^2 \frac{(1+l)l}{2}I \right)}$. For ease of presentation, we use notation $\textstyle\Psi_v=\Gamma \mu_{v,0} + \Lambda (s_{\max}M\sigma_A^2 + (s_{\max} + \lambda)\|\mu_A\|^2)$ to represent the regret term in Theorem \ref{thm:av} and $\Xi_H=s_{\max}M \cdot \sigma_H^2 + (s_{\max}+\lambda_H) \|\mu_H\|^2$ to represent the term in Theorem \ref{thm:hv}. Hence, building upon our results in Theorem \ref{thm:av}  and Theorem \ref{thm:hv}, we give the upper bound of  $\mathcal{R}_{A-H}(T)$ in the following corollary. 

\begin{corollary}[Regret of the HV-AV Interaction System] \label{corollary:ah} 
Suppose Assumptions \ref{asu:reward} holds. Then we have the upper bound on the regret of AV-HV system as follows,
\begin{align*}
    \mathcal{R}_{A-H}(T) \leq    \sum_{l=1}^L & (V_{\max} + l R_{\max})\gamma^l (2\Psi_A(l)  + \Psi_H(l)) +\Xi_H  + \frac{1}{T} \gamma^L \Psi_v
\end{align*}
\vspace{-0.2in}
\end{corollary}
Corollary \ref{corollary:ah} shows the impact of HV and AV's decision making on the overall learning performance through terms $\Psi_A,\Psi_v$ and $\Psi_H, \Xi_H$, respectively. In what follows, we conduct the empirically studies to thoroughly examine the impact of each agent while holding another agent fixed. 

\begin{figure*}[t]
	\centering
\subfigure[Impact of $L$.]{
		 \includegraphics[width=0.26\textwidth]{figures/avhv_goodhart.pdf}
    \label{fig:avhvL}}
    \hspace{0.3in}
\subfigure[Impact of $\mu_A$.]{\includegraphics[width=0.26\textwidth]{figures/avhv_fixH_changeA.pdf}
		\label{fig:avhvA}}
		  \hspace{0.3in}
	\subfigure[Impact of $\mu_H$.]{	\includegraphics[width=0.26\textwidth]{figures/avhv_fixL_changeH.pdf}
		\label{fig:avhvH}}
   \caption{Empirical studies on AV and HV's decision making on the overall performance.}
	 \label{fig:avhvimpact}
\end{figure*}

\begin{figure*}[t]
	\centering
	\subfigure[$\mu_{v,0}$ v.s. Regret dynamics.]{	\includegraphics[width=0.26\textwidth]{figures/TpsiV.pdf}
		\label{fig:TpsiV}}
		 \hspace{0.3in}
	\subfigure[$\mu_A$ v.s. Regret dynamics.]{\includegraphics[width=0.26\textwidth]{figures/TmuA.pdf}
    \label{fig:TmuA}}
  	 \hspace{0.3in}
	\subfigure[$\mu_H$ v.s. Regret dynamics.]{\includegraphics[width=0.26\textwidth]{figures/TmuH.pdf}
		\label{fig:TmuH}}
		\caption{Empirical results on how AV and HV's decision making have impact on the overall regret dynamics, i.e., take regret as function of $T$.}
     \label{fig:impacydynamics}
\end{figure*}
\begin{figure}[h!]
	\centering
	\subfigure[$\mu_A$, $\mu_H$ v.s. $\mathcal{R}_A + \mathcal{R}_H$.]{	 \includegraphics[width=0.35\columnwidth]{figures/3dSum.png}
    \label{fig:3dsum}}
	\subfigure[$\mu_A$, $\mu_H$ v.s. $\mathcal{R}_{A-H}$.]{	\includegraphics[width=0.35\columnwidth]{figures/overall3d.png}
		\label{fig:3doverall}}
%         \begin{subfigure}[b]{0.24\columnwidth}
%     \includegraphics[width=\textwidth]{figs/actor_bias3.pdf}
     \caption{Illustration of the impact of $\mu_A$, $\mu_H$ on (a) the regret summation and (b) the overall regret.}
     \label{fig:3d}
     \vspace{-0.1in}
\end{figure}
%AV's decision making relies on the prediction of HV's actions and (2) lookahead planning based RL. 
{\bf Impact of AV's decision making on the overall system performance.} 
{(1) Implications on choosing discounting factors.} Our analysis reveals three key findings regarding AV's decision-making impact. (1) First, the choice of \textit{discount factor} significantly influences how prediction errors ($\mu_A$) affect system performance. As shown in Fig. \ref{fig:avhvA}, larger discount factors amplify the impact of prediction errors by placing greater emphasis on future rewards, evidenced by the increasing separation between performance curves at different $\mu_A$ values. (2) Second, Fig. \ref{fig:TpsiV} demonstrates that while initial function approximation error ($\mu_{v,0}$) strongly impacts regret during early interactions, its influence diminishes over time as the value function updates during learning. This aligns with the last term in Corollary \ref{corollary:ah}. (3) Third, our results provide clear guidance on training priorities between function approximation and prediction model improvements. Comparing Fig. \ref{fig:TpsiV} and Fig. \ref{fig:TmuA}, reducing prediction error ($\mu_A$) from 0.4 to 0.2 yields a substantial 30\% reduction in regret (from 4000 to 1800), while halving the function approximation error only reduces regret by 0.06\% (from 51.41 to 51.38). This suggests that improving prediction models offers significantly greater benefits for system performance. The complete proof of these results can be found in Corollary \ref{corollary:ah} (\Cref{app:coro}).

% {(2) Impact of function approximation error during interaction.}  In Fig. \ref{fig:TpsiV}, we study the impact of the function approximation error on the learning performance. As expected, the initial function approximation error $\mu_{v,0}$ have the huge impact on the regret in the first few interaction $T$. While during the learning process, the value function is updated and contributes less to the overall learning regret, e.g., the last term in Corollary \ref{corollary:ah}.  {(3) Implications on the Priority of training: function approximation v.s. prediction model.}  In Fig. \ref{fig:TpsiV} and Fig. \ref{fig:TmuA} we show the impact of different function approximation error ($\mu_{v,0}$) and prediction error ($\mu_A$) on the system regret. It can be seen that by reducing the prediction error from 0.4 to 0.2, the regret have significant change from 4000 to 1800 (-$30\%$). While reducing the function approximation error from 100 to 50, the regret changes from 51.41 to 51.38 (-$0.06\%$). The empirical results indicate that optimizting over prediction model tends to help us get more improvement on regret. { The proof of Corollary \ref{corollary:ah} can be found in \Cref{app:coro}.}



% {\bf Impact of AV's Decision Making on System Performance}
% Our analysis reveals three key findings regarding AV's decision-making impact. First, the choice of discount factor significantly influences how prediction errors ($\mu_A$) affect system performance. As shown in Fig. \ref{fig:avhvA}, larger discount factors amplify the impact of prediction errors by placing greater emphasis on future rewards, evidenced by the increasing separation between performance curves at different $\mu_A$ values.
% Second, Fig. \ref{fig:TpsiV} demonstrates that while initial function approximation error ($\mu_{v,0}$) strongly impacts regret during early interactions, its influence diminishes over time as the value function updates during learning. This aligns with the last term in Corollary \ref{corollary:ah}.
% Third, our results provide clear guidance on training priorities between function approximation and prediction model improvements. Comparing Fig. \ref{fig:TpsiV} and Fig. \ref{fig:TmuA}, reducing prediction error ($\mu_A$) from 0.4 to 0.2 yields a substantial 30\% reduction in regret (from 4000 to 1800), while halving the function approximation error only reduces regret by 0.06\% (from 51.41 to 51.38). This suggests that improving prediction models offers significantly greater benefits for system performance. The complete proof of these results can be found in Corollary \ref{corollary:ah} (\Cref{app:coro}).

% Furthermore, Fig. \ref{fig:TmuA} shows the larger prediction error may cause huge regret in the first few interaction, e.g., the green line.  Fig. \ref{fig:avhvL} demonstrates the Goodhart's law in the overall system performance.

{\bf Impact of HV's Bounded Rationality on the overall system performance.} As illustrated in Fig. \ref{fig:avhvH}, we conduct the experiments on the relationship between regret and human's decision making error $\mu_H$ by setting different discounting factors. In Fig. \ref{fig:TmuH}, we can see that the regret difference caused by $\mu_H$ can be consistent during the interaction, which can be related to the second term in the upper bound of $\mathcal{R}_{A-H}$.  Moreover, we also demonstrate the impact of HV's decision making on AV (and vice versa) in Fig. \ref{fig:3d}. For instance, in Figure \ref{fig:3doverall}, a given $u_H$ will constrain the best possible outcome that AV can achieve, e.g., the projection on the $\mu_A$-Regret plane. 

\begin{remark}[Extension beyond two-agent case] Our analysis approach is feasible to extend beyond one AV and one HV setting and we outline the preliminary steps in \Cref{app:beyond}.
%Assume there are $N_H$ number of HVs and $N_A$ number of AVs in the mixed traffic system. With abuse of notations, we define the action vector for AVs and HVs as follows, at time step $t$, $u_H(t) = [u_{H,1}(t), u_{H,2}(t),\cdots, u_{H,N_H}(t)]$, $ u_A(t) = [u_{A,1}(t), u_{A,2}(t),\cdots, u_{A,N_A}(t)]$. By defining the prediction error as in \Cref{eqn:prediction_error} and HVs' bounded rationality as in \Cref{subsec:42}, our analysis framework still can be applied. The dimension of the approximation error term and the bounded rationality term is thus $N_A$ and $N_H$ times higher than the two-agent case. Hence, the resulting regret in Theorem 3 and Theorem 4 are $N_A$ and $N_H$ times higher than the two-agent case. 
\end{remark}


\section{Conclusion}
In this work, we take the regret analysis approach to address the questions  1) ``\textit{How does learning performance depend on HV's bounded rationality and AV's planning horizon?}'' and 2) ``\textit{How do different decision making strategies {between AV and HV} impact the overall learning performance}?''. To this end, we propose a formulation that captures heterogeneous HV-AV interactions and derive regret upper bounds for both vehicle types. Our analysis reveals two key phenomena: a Goodhart's law effect in AV's planning-based RL with predicted human actions, and error accumulation in HV's decision-making due to bounded rationality. We characterize the overall system performance through theoretical bounds and empirical studies, demonstrating the impact of different learning strategies on system efficiency.


% To this end, we first propose a HV-AV interaction formulation for heterogeneous decision making of HV and AV. Based on the proposed formulation, we derive the upper bound on the regret for both HV and AV, respectively. By delving into the upper bound, we identify the Goodhart's law phenomenon in AV's decision making, where AV adopt the planning based RL using predicted human actions. Meanwhile we show the error accumulation effect in HV's decision making due to the bounded rationality in HV's decision making. Based on these results, we further analyze the impact of AV and HV's decision making on the overall system performance and we also derive the upper bound of the system regret. Through empirical study, we demonstrate how do different learning strategies affect the system performance. 



% \section*{Acknowledgements}

% This \LaTeX{} is heavily inspired by NeurIPS 2023.

% Do not include acknowledgements in the version for blind review.
% If a paper is accepted, please place such acknowledgements in an unnumbered section at the end of the paper, immediately before the references.
% The acknowledgements do not count towards the page limit.




% Reference
% For natbib users:
\bibliography{reference}
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\input{appendix}

\end{document}