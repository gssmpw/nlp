\section{Related Work}
% {\bf Mixed Autonomy.} Previous studies on mixed autonomy generally consider specific dynamics models for AV and HV. For instance, **Jiang, "Bando's Model for Autonomous Vehicles"** uses Bando's model to describe the AV and HV's behavior and demonstrates the traffic flow empirically on car-following model.  **Gao, "Empirical Study of Autonomous Vehicle Impact"** conducts a empirical study on the impact of AV on HV's performance in terms of the driving volatility measures while assuming a specific AV's acceleration model. Meanwhile, the impact of humans in the mixed traffic is empirically examined through high-fidelity driving simulator **Kim, "Human Factor in Mixed Traffic Flow"**. **Li, "Stochastic Model for Mixed Traffic"** proposes a stochastic model for mixed traffic flow to investigate the interaction between HV and AV while taking into account the uncertainty of human driving behavior. It is shown that AV has huge impact on the overall traffic stability and HV's behavior through numerical study. **Wang, "Modular Learning Framework for Mixed Autonomy"** proposes a modular learning framework for mixed traffic by leveraging deep RL. Experiments show that AV is able to reduce congestion under the intelligent driver model (IDM). Without imposing specific models on HV and AV's decision making dynamics, our work focuses on the performance of different learning strategies in the mixed autonomy.

{\bf Mixed Autonomy.} Prior work on mixed autonomy traffic has primarily focused on specific dynamics models and empirical studies. For instance, **Jiang, "Bando's Model for Autonomous Vehicles"** uses Bando's model for vehicle behavior analysis, while **Gao, "Empirical Study of Autonomous Vehicle Impact"** studies AV's impact on HV driving volatility using predetermined AV acceleration models. The human factor has been examined through high-fidelity driving simulators **Kim, "Human Factor in Mixed Traffic Flow"**, and stochastic models have been proposed to capture human behavior uncertainty **Li, "Stochastic Model for Mixed Traffic"**. On the learning side, **Wang, "Modular Learning Framework for Mixed Autonomy"** demonstrates congestion reduction using deep RL under the intelligent driver model (IDM). Without imposing specific models on HV and AV's decision making dynamics, our work focuses on the performance of different learning strategies in the mixed autonomy.

% {\bf HV-AV Interaction Model.} In related work on modeling the HV-AV interaction, **Chen, "Stackelberg Game for Autonomous Vehicles"** uses the general-sum Stackelberg game to account for the humanâ€™s influence on AV and computes the backward reachability tube for safety assurances in the interaction.  Similarly, **Liu, "Two-Player Game for Human-Vehicle Interaction"** formulates the interaction as a two-player game, where the influence between AV and HV are captured in the predefined reward. Considering the vehicles' dynamic driving actions, **Zhang, "Hierarchical Game-Theoretic Planning Scheme"** develops a hierarchical game-theoretic planning scheme and shows the effectiveness of the proposed planning method in the simulation. We note that even though **Wang, "Underactuated Dynamical System for Human-Vehicle Interaction"** proposes to use underactuated dynamical system to overcome the limitations of the game formulation, it assumes that both AV and HV making decision in the same strategy, i.e., planning for the same horizon. { The related ad-hoc team problem **Guo, "Ad-Hoc Team Problem"** mainly focused on the cooperative case,  whereas our setting does not impose assumptions on the cooperation of agents. Meanwhile, Zero-shot coordination **Huang, "Zero-Shot Coordination"** mainly focused on the robustness of the self-play, whereas our work aims to understand the interaction between two agents with different decision makings strategies. Moreover, our work focuses on characterizing the impact of the opponent modeling errors on the learning performance which is also related to opponent modeling **Liu, "Opponent Modeling"**}. Despite the rich empirical results in the related filed, e.g., Ad-hoc team problem and zero-shot coordination, we remark that the theoretical analysis on the interaction between AV and HV is still lacking, especially considering their different decision making. Moreover, our work deviates from the conventional game setting and  aims to takes steps to quantify the impact of AV and HV's different decision making on the traffic system.  

{\bf HV-AV Interaction Model.} For modeling HV-AV interactions specifically, several game-theoretic approaches have been proposed. **Chen, "Stackelberg Game for Autonomous Vehicles"** and **Liu, "Two-Player Game for Human-Vehicle Interaction"** use Stackelberg and two-player game formulations respectively, while **Zhang, "Hierarchical Game-Theoretic Planning Scheme"** develops a hierarchical planning scheme. Although **Wang, "Underactuated Dynamical System for Human-Vehicle Interaction"** attempts to address game formulation limitations using underactuated dynamical systems, it assumes identical decision-making horizons for both vehicle types. While related fields like ad-hoc team problems **Guo, "Ad-Hoc Team Problem"** and zero-shot coordination **Huang, "Zero-Shot Coordination"** provide empirical insights, they focus on either cooperative scenarios or self-play robustness. Our work differs by analyzing the interaction between agents with different decision-making strategies without assuming cooperation, particularly examining the impact of opponent modeling errors on learning performance **Liu, "Opponent Modeling"**. Despite the rich empirical results in the related field, e.g., Ad-hoc team problem and zero-shot coordination, we remark that the theoretical analysis on the interaction between AV and HV is still lacking, especially considering their different decision making. Moreover, our work deviates from the conventional game setting and  aims to takes steps to quantify the impact of AV and HV's different decision making on the traffic system.  


% {\bf Model-based RL.} Model-based RL (MBRL), which leverages a model of the environment, is promising for real-world applications thanks to its data efficiency **Sutton, "Model-Based Reinforcement Learning"**. In particular, our work is relevant to MBRL with lookahead planning. For instance, **Wang, "Lookahead Policy Rollout"** use lookahead policy to rollout the dynamics model into the future $H$ steps in order to find the action sequence with highest return. A value function is also attached at the end of the rollout to estimate the terminal cost. Moreover, **Li, "Sub-Optimality Analysis for Model-Based RL"** provides the sub-optimality gap of the learned policy under an approximate model and approximate value function. Our work is different from previous work on MBRL since in our case, AV has access to the environment dynamics while the modeling error exists due to the unknown bounded rationality of HV. Meanwhile, our theoretical analysis uses regret to evaluate the performance of the decision making, in which the value function is updated during the learning process, resulting in the changing function approximation error. As a result, the technique used in our proof is significant different than previous work **Sutton, "Model-Based Reinforcement Learning"**. 

{\bf Model-based RL.} MBRL with lookahead planning has shown promise in real-world applications due to its data efficiency **Sutton, "Model-Based Reinforcement Learning"**. Recent works **Wang, "Lookahead Policy Rollout"** and **Li, "Sub-Optimality Analysis for Model-Based RL"** utilize lookahead policies with future rollouts and terminal value functions to optimize action sequences. While **Toussaint, "Deep RL for Robotics"** provides sub-optimality analysis under approximate models, our work differs fundamentally: we assume AV has access to environment dynamics but faces uncertainty from HV's bounded rationality. Moreover, our theoretical analysis uses regret with dynamically updating value functions, requiring significant different analytical techniques than previous work **Sutton, "Model-Based Reinforcement Learning"**.