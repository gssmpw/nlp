\documentclass[pdflatex,sn-mathphys-num]{sn-jnl}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage[title]{appendix}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage{manyfoot}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{epigraph}
\usepackage{cleveref}
\usepackage{xr}
\usepackage{lmodern}
\usepackage{anyfontsize}
\usepackage{dcolumn,caption,booktabs}
\usepackage{subcaption}
\externaldocument[app:]{sn-SM-final-version}


\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}


\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\newtheorem{definition}{Definition}%

\raggedbottom


\begin{document}



\title{The Shrinking Landscape of Linguistic Diversity in the Age of Large Language Models}

\author*[1,2]{\fnm{Zhivar} \sur{Sourati}}\email{souratih@usc.edu}

\author[2,3]{\fnm{Farzan} \sur{Karimi-Malekabadi}}\email{karimima@usc.edu}
\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Meltem} \sur{Ozcan}}\email{ozcan@usc.edu}
\equalcont{These authors contributed equally to this work.}

\author[3]{\fnm{Colin} \sur{McDaniel}}\email{cbmcdani@usc.edu}

\author[1,2]{\fnm{Alireza} \sur{Ziabari}}\email{salkhord@usc.edu}

\author[2,3]{\fnm{Jackson} \sur{Trager}}\email{jptrager@usc.edu}

%\author[1]{\fnm{Nuan} \sur{Wen}}\email{nuanwen@usc.edu}

\author[1]{\fnm{Ala} \sur{Tak}}\email{nekouvag@usc.edu}

\author[3]{\fnm{Meng} \sur{Chen}}\email{mchen421@usc.edu}

\author[1,4]{\fnm{Fred} \sur{Morstatter}}\email{fredmors@isi.edu}

\author[1,2,3]{\fnm{Morteza} \sur{Dehghani}}\email{mdehghan@usc.edu}

\affil*[1]{\orgdiv{Department of Computer Science}, \orgname{University of Southern California}}

\affil[2]{\orgdiv{Center for Computational Language Sciences}, \orgname{University of Southern California}}

\affil[3]{\orgdiv{Department of Psychology}, \orgname{University of Southern California}}

\affil[4]{\orgdiv{Information Science Institute}, \orgname{University of Southern California}}


\abstract{
Language is far more than a communication tool. A wealth of information — including but not limited to the identities, psychological states, and social contexts of its users — can be gleaned through linguistic markers, and such insights are routinely leveraged across diverse fields ranging from product development and marketing to healthcare. In four studies utilizing experimental and observational methods, we demonstrate that the widespread adoption of large language models (LLMs) as writing assistants is linked to notable declines in linguistic diversity and may interfere with the societal and psychological insights language provides. We show that while the core content of texts is retained when LLMs polish and rewrite texts, not only do they homogenize writing styles, but they also alter stylistic elements in a way that selectively amplifies certain dominant characteristics or biases while suppressing others — emphasizing conformity over individuality. By varying LLMs, prompts, classifiers, and contexts, we show that these trends are robust and consistent. Our findings highlight a wide array of risks associated with linguistic homogenization, including compromised diagnostic processes and personalization efforts, the exacerbation of existing divides and barriers to equity in settings like personnel selection where language plays a critical role in assessing candidates’ qualifications, communication skills, and cultural fit, and the undermining of efforts for cultural preservation.
}


% \keywords{LLMs, Linguistic Diversity, Keyword3, Keyword4}

\maketitle
\clearpage
\section*{Introduction}

\epigraph{``We're getting the language into its final shape-- the shape it's going to have when nobody speaks anything else.''}{George Orwell, \textit{1984}\citep[][p.69]{georgeorwell}}

The words we speak tell a story – of our individual identities \citep{park2015automatic,oberlander2006language,moreno2021can,mairesse2007using,schwartz2013personality}, the diverse communities we inhabit \citep{kramsch2014language,gumperz1968speech,nguyen2011language}, and the vibrant societies that shape us \citep{pennebaker2011secret,robinson2017mind}. 
They offer a unique lens through which we can explore the intricate tapestry of human thought, culture, and behavior. From revealing our moral values and cultural biases \citep{kennedy2021moral, jackson2019loosening} to shaping political discourse and influencing public opinion \citep{wilkerson2017large, nisbet2009communicating}, language plays a pivotal role in understanding the human experience. As linguistic systems influence and structure the ``kaleidoscopic flux of impressions'' that constitutes our perception of the world \citep{sapir1956language}, they also play a role in shaping the human experience.


Researchers across diverse fields, from psychology and political science to mental health and marketing, are increasingly using linguistic insights to understand human behavior and address societal challenges \citep{park2015automatic,jackson2019loosening,wilkerson2017large,amado2018research}. For instance, linguistic markers are used to diagnose and track cognitive impairments \citep{richard2024linguistic,eyigoz2020linguistic,roark2011spoken}, while clinical psychologists analyze language to understand and treat a range of mental health conditions \citep{trifu2024linguistic,weerasinghe2019because,corona2023natural,rude2004language,coppersmith2018natural}. 


Crucially, the richness of these insights fundamentally relies on the fact that language is not monolithic; people use language differently, even when conveying similar ideas. This diversity transforms language from a mere medium of information exchange into a unique signature of individuals and societies, enabling analyses of cultural products and societal trends. 

However, the rise of large language models (LLMs), such as ChatGPT \citep{chatgpt} and Gemini \citep{gemini}, presents a significant challenge to the preservation of linguistic diversity. As these models become increasingly integrated into our daily lives — serving over 180 million users worldwide \citep{vermeer2024} for tasks ranging from composing emails to software development and drafting articles or other technical writing \citep{intelligent2024,mcclain2024,handa2025economic} — their very design poses a potential threat. LLMs are trained to generate the most statistically likely continuation of a given text, a process that inherently favors dominant language patterns. This focus on statistical likelihood risks undermining the rich diversity of linguistic expression, potentially eroding the variety of languages and dialects that contribute to the global cultural landscape. While prompts can introduce some customization \citep{mizrahi2024state,safdari2023personality}, the output remains constrained by the same underlying system \citep{santurkar2023whose}. 

The widespread adoption of LLMs carries a significant risk of homogenizing language use, potentially diminishing the linguistic diversity that allows us to analyze societal and individual trends \citep{ireland2014natural}. The loss of this informative linguistic variability could have far-reaching consequences, such as obscuring crucial markers used to identify specific subpopulations or individuals. For instance, detecting individuals expressing depression or suicidal thoughts could become more difficult with increasingly homogenized writing styles, hindering early diagnosis and treatment. Furthermore, industries that rely on nuanced text-based analysis for mass personalization \citep{matz2017using, winter2021effects}, such as marketing, could be affected; by blurring linguistic variabilities, LLMs may reduce the effectiveness of targeted advertising and product development \citep{sundar2010personalization}. Finally, the homogenization of language use could also erase valuable cultural markers, threatening the preservation of linguistic heritage \citep{ryan2024unintended}. In essence, the pervasive use of LLMs could inadvertently lead to a flattening of linguistic expression, with potentially detrimental effects on our ability to understand and respond to individual and societal needs.
% In marketing, for example, understanding diverse consumer behaviors is vital for crafting personalized recommendations;

The threat of homogenization and loss of linguistic diversity becomes even more alarming when this homogenization aligns disproportionately with the linguistic patterns of specific subpopulations. Previous research has demonstrated that LLMs often overrepresent certain populations with specific affiliations or demographic attributes while underrepresenting others \citep[e.g.,][]{navigli2023biases,atari2023humans,wang2024large,rozado2024political,pan2023llms,abdurahman2023perils}. Such imbalances combined with language homogenization, risk amplifying the voices of dominant groups while further marginalizing underrepresented communities.

This paper investigates the impact of LLMs on linguistic diversity and its societal implications across four studies. In Study 1, we analyze temporal trends in writing styles, using historical data from diverse sources, revealing a homogenization of language following the advent of LLMs. Study 2 replicates these findings experimentally, demonstrating that LLM revisions of human-written texts cause a reduction in linguistic diversity while preserving semantic meaning. Delving into the consequences of this homogenization, Study 3 examines how LLMs affect the ability to identify personal traits from written text, finding that LLM-generated revisions obscure crucial linguistic markers essential for such analyses. Finally, Study 4 explores the specific ways in which LLMs disrupt established links between linguistic patterns and personal traits, highlighting subtle but significant alterations to the relationship between language and identity.

By analyzing trends in linguistic diversity across a variety of communication forms and experimentally rewriting natural language using LLMs, our studies offer a comprehensive understanding of how these models are reshaping human communication. We demonstrate that LLMs diminish linguistic diversity and alter the societal and psychological insights we can derive from language. To ensure the robustness and generalizability of our findings, we systematically vary LLMs, prompts, and classifiers and confirm that the observed trends are not artifacts of specific design choices but reflect consistent patterns that persist across different configurations. We make our code and data publicly available.


\section*{Study 1}

Study 1 investigates the potential homogenization of linguistic diversity in writing styles after the introduction of LLMs by examining the relationship between LLM usage and the variance of features associated with linguistic complexity. This involves a four-step process: (1) gathering diverse online user-generated content, (2) assessing the extent to which this content is likely to be written by an LLM, (3) measuring the variance in linguistic complexity within that content, and (4) examining how the introduction of LLMs, and the subsequent prevalence of AI-written content, relate to changes in the observed variance of linguistic complexity.

\subsection*{Material}

To examine the impact of LLMs on linguistic diversity, we select data sources that are diverse, authored by individuals rather than organizations, and allow for a variety of writing styles without strict external constraints. These sources are characterized by their broad range of topics and independence from objectives that might inadvertently homogenize language, such as enforcing correctness or adherence to a uniform style. Our dataset comprises three distinct sources:  

1. \textbf{Reddit (r/WritingPrompts):} This subreddit features creative stories written by users as comments in response to open-ended prompts, encouraging originality with minimal external constraints. The lack of strict rules ensures that these stories reflect individual creativity and linguistic diversity. We collected $N = 318,490$ stories posted between January 2018 and November 2024, downloaded from Project Arctic Shift (\url{https://github.com/ArthurHeitmann/arctic_shift}). To focus on substantive user-generated narratives, we excluded short replies and feedback comments by filtering out texts with fewer than 200 words. 
  

2. \textbf{Patch News:} \url{Patch.com} provides localized news from communities across the United States (488 counties in 50 states), written by a large number (N = 19,584) of contributors. 
Unlike larger outlets, Patch articles are less regulated, allowing reporters' voices to shine through without the imposition of unified organizational standards. We gathered $N = 379,583$ articles from January 2018 to November 2023, focusing on the main body of the text and removing advertisements.  


3. \textbf{arXiv Preprints:} arXiv is an online repository where researchers routinely share their manuscripts, ensuring immediate access to their work. This allows us to capture papers closer to when the research was conducted, avoiding delays and edits associated with formal publication processes. Additionally, arXiv encompasses a wide range of subjects, promoting diversity in topics and writing styles. We specifically focused on papers from the \emph{Computer Science; Linguistics (CL)} and \emph{Computer Science; Vision (CV)} categories, as these fields have strong traditions of posting preprints on arXiv. We analyze abstracts from the publicly available metadata of arXiv papers \citep{arxiv_org_submitters_2024}, specifically those posted between January 2018 and November 2024. This subset consists of $N = 80,238$ papers with $N = 161,265$ contributing authors.

\subsection*{Methodology}

\subsubsection*{Detection of AI-written Texts}
To identify whether a piece of text is AI-generated, we use \emph{Binoculars} \citep{hans2024spotting}, a tool that relies on normalized perplexity, a measure of how predictable a text is to a language model. AI-generated texts generally exhibit lower perplexity due to the inherent uniformity of language models, which optimize for the most probable word sequences. By normalizing perplexity, Binoculars effectively distinguishes AI-written content, even when prompts are designed to produce less typical outputs. The tool has demonstrated a high ($> 0.95$) true positive rate while maintaining a low ($< 0.01$) false positive rate detecting AI-written content across benchmarks, particularly for text types similar to those in our study (i.e., news articles, academic papers, and creative writing). Using the threshold recommended in its codebase, we classify texts as human- or AI-written based on their normalized perplexity scores.

\subsubsection*{Calculation of Variance in Writing Complexity}  

We define \(D = \{d_1, d_2, \dots, d_N\}\) as a collection of documents, where each document \(d_i\) is associated with a creation date, \(\text{date}(d_i)\). To quantify the complexity of these documents, we rely on the following set of features that capture lexical and syntactic dimensions of textual complexity:  
\begin{itemize}  
    \item \textbf{Vocabulary Simpson Index (\(\text{Simpson}(d_i)\); 
    \citealp{simpson1949measurement}}): Adapted from ecology, this measure quantifies vocabulary richness and complexity by calculating the probability that two randomly selected terms from \(d_i\) are identical. By capturing lexical uniformity, it offers insights into the diversity of word usage.
    \item \textbf{Vocabulary Shannon Entropy (\(\text{Shannon}(d_i)\); \citealp{shannon1948mathematical})}: Quantifies the unpredictability or richness of word usage in \(d_i\).  
    \item \textbf{Average Dependency Link Length (\(\text{DepLength}(d_i)\); \citealp{gibson1998linguistic})}: Evaluates syntactic complexity by calculating the average length of syntactic dependencies in the text.  
    \item \textbf{Type-Token Ratio (\(\text{TTR}(d_i)\); \citealp{johnson1944studies})}: Captures lexical diversity by dividing the number of unique words (types) by the total word count (tokens).  
    \item \textbf{Hapax Legomena (\(\text{Hapax}(d_i)\); \citealp{mardaga2012hapax})}: Reflects lexical novelty by counting words in \(d_i\) that occur only once.  
\end{itemize}  

To study temporal trends in the variance of writing complexity, we compute the variance of each complexity feature (e.g., \(\text{Simpson}\), \(\text{TTR}\)), aggregated monthly (\(m\)), denoted as \(\sigma^2_{\text{(feature, m)}}\). High \(\sigma^2_{\text{(feature, m)}}\) suggests heterogeneity in writing styles or complexity levels among documents, whereas low \(\sigma^2_{\text{(feature, m)}}\) indicates homogeneity, where documents tend to exhibit similar patterns in the respective feature.  

As a general indicator of the overall trend and to facilitate easier interpretation of results, we further aggregate the variance of all complexity-related features into a single averaged measure, denoted as $\bar{\sigma^2}_m$. Cronbach's alpha \citep{cronbach1951coefficient} values indicate acceptable reliability and internal consistency for the composite measure in arXiv (\(\alpha = .965\); 95\% CI: [.951, .976]), Patch News (\(\alpha = .722\); 95\% CI: [.604, .813]) as well as Reddit (\(\alpha = .708\); 95\% CI: [.595, .796]), and support the use of a composite measure to represent the overall trend in writing complexity variance across time. We present the trends for all complexity-related features individually in \Cref{homogenization-trends} in Supplementary Material.

\subsubsection*{Relationship Between AI Usage and Variance in Writing Complexity}  
To investigate how the introduction and adoption of LLMs affect the variance in writing complexity, we employ a two-tiered analytical approach. First, we conduct a shock analysis using a Discontinuous Growth Model \citep{singer2003applied} to assess whether trends in writing complexity variance shift after the introduction of LLMs. This approach provides insight into whether LLMs have impacted overall patterns in writing complexity variance. Next, we conduct a Granger Causality analysis \citep{linden2015conducting} to study the relationship between AI usage rates and writing complexity variance. This allows us to test the more specific hypothesis that AI usage can predict or directly influence changes in the variance of writing complexity.

\paragraph{Shock Analysis}  

Taking ChatGPT's launch on November 30, 2022, which garnered widespread public attention and arguably signaled the beginning of widespread LLM adoption as a marker, we perform a Discontinuous Growth Model (DGM) analysis \citep{singer2003applied} to assess whether this event triggered a statistically significant shift in the variance of writing complexity. To ensure robust estimation of the ChatGPT's launch effects, we employ a Generalized Least Squares (GLS) method with an autoregressive correlation structure ($AR$(1)) to account for potential autocorrelations within our time series data. Specifically, we fit the following model:

\[
\bar{\sigma^2_m} \sim \text{Time}_m + \text{ONSET}_m + \text{POST}_m,
\]

\noindent where \(\text{Time}_m\) is a continuous variable representing time (in months), \(\text{ONSET}_m\) is a binary variable indicating whether the date falls before or after the launch (\(\text{ONSET}_m = 0\) for pre-launch and \(\text{ONSET}_m = 1\) for post-launch), and \(\text{POST}_m\) represents the elapsed time from launch for post-launch observations (\(\text{POST}_m = 0\) for pre-launch dates). In this model, \(\text{Time}_m\) captures the overall temporal trend in writing complexity variance, \(\text{ONSET}_m\) identifies any immediate changes following the introduction of LLMs, and \(\text{POST}_m\) evaluates potential longer-term effects that persist beyond the initial launch.


\paragraph{Granger Causality Test}  
We further investigate whether LLMs have a sustained impact on writing complexity variance beyond a simple initial shock or pulse effect by employing a Granger causality test \citep{linden2015conducting}. This test determines if past values of one time series (i.e., AI usage) can predict future values of another (i.e., linguistic complexity variance), indicating a temporal predictive relationship. While AI usage is estimated using text unpredictability (perplexity) and linguistic complexity includes measures influenced by unpredictability (e.g., Shannon entropy), our analysis specifically examines the relationship between the variance in linguistic complexity (dependent variable) and another construct based on the overall magnitude of unpredictability or complexity (AI usage rate; independent variable). This distinction mitigates concerns about direct dependence between the two. To capture potential delayed effects, we consider lagged values ranging from 1 to 20 time units for each pair of variables. 

Before applying the Granger causality test, we verify the stationarity assumption— ensuring that the time series' statistical properties, such as mean and variance, remain constant over time— by performing the Augmented Dickey-Fuller (ADF) test \citep{dickey1979distribution} on all variables. Non-stationary variables indicated by the ADF test are differenced accordingly to achieve stationarity \citep{box2015time}. After first-order differencing, all time series met the stationarity assumption, allowing us to proceed with the analysis.

The null hypothesis of the Granger causality test posits that the lagged values of the predictor variable do not improve the prediction of the target variable beyond what can be achieved with the target variable's own historical values. The Granger causality test is conducted using an \(F\)-statistic to compare the fit of two models: one including only the lagged values of the target variable and the other incorporating both the lagged values of the target and predictor variables. A significant \(F\)-statistic indicates that the predictor variable provides additional explanatory power, rejecting the null hypothesis and supporting a Granger-causal relationship. We analyze complexity variance trends separately for each dataset to identify domain-specific patterns and lag effects.

\subsection*{Results}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{figures/complexity_trends.png}
    \caption{Trends in the variance of writing complexity and the attribution rate of texts as AI-generated.}
    \label{fig:complexity-trends}
\end{figure}

Our analyses revealed consistent declines in the variance of writing complexity after the introduction of LLMs across all datasets, providing evidence for homogenization in writing styles (see \Cref{fig:complexity-trends}).  
The shock analysis demonstrated significant reductions in writing complexity variance linked to the introduction of ChatGPT across the three datasets (see \Cref{tab:shock-analysis}). 
Similarly, Granger causality tests suggested a predictive relationship between AI usage and changes in linguistic complexity variance for the Reddit and arXiv datasets (see \Cref{tab:granger_results}). 


\begin{table}[h]
\caption{Shock analysis results assessing the effect of ChatGPT release on writing complexity variance. Significance levels are denoted as follows: \textsuperscript{*} for \(p < .05\), \textsuperscript{**} for \(p < .01\), \textsuperscript{***} for \(p < .001\).}
\begin{tabular}{llccccc}
\toprule
\textbf{Dataset} & \textbf{Term} & \textbf{Estimate ($\beta$)} & \textbf{SE} & \textbf{\emph{t}} & \textbf{\emph{p}} & \textbf{\emph{95\% CI}}\\
\midrule
\multirow{3}{*}{ArXiv} 
 & Time & -0.0008 & 0.0000 & -14.6657 & $<$.001\textsuperscript{***} & [-0.0010,  -0.0008]\\
 & ONSET & -0.0427 & 0.1100 & -0.3881 & .699 & [-0.2583, 0.1729] \\
 & POST & -0.0014 & 0.0002 & -6.6252 & $<$.001\textsuperscript{***} & [-0.0019, -0.0010]\\
 \midrule
 \multirow{3}{*}{Patch news} 
 & Time & 0.0003 & 0.0001 & 3.3178 & .001\textsuperscript{**} & [0.0001, 0.0005] \\
 & ONSET & -1.4054 & 0.2429 & -5.7859 & $<$.001\textsuperscript{***} & [-1.8815, -0.9293] \\
 & POST & -0.0022 & 0.0011 & -2.0040 & .049\textsuperscript{*} & [-0.0044, -0.0000] \\
\midrule
\multirow{3}{*}{Reddit} 
 & Time & -0.0002 & 0.0001 & -1.6375 & .106 & [-0.0005,  0.0000] \\
 & ONSET & 0.4161 & 0.2511 & 1.6568 & .102 & [-0.0761,  0.9083]\\
 & POST & -0.0021 & 0.0005 & -4.1796 & $<$.001\textsuperscript{***} & [-0.0031, -0.0011] \\
\bottomrule
\end{tabular}
\label{tab:shock-analysis}
\begin{minipage}{\textwidth}
\footnotesize
%\textit{Note:} Significance levels are denoted as follows: \textsuperscript{*} for \(p < .05\), \textsuperscript{**} for \(p < .01\), \textsuperscript{***} for \(p < .001\).
\end{minipage}
\end{table}

In the arXiv dataset, the shock analysis revealed a significant persistent decline in linguistic complexity variance following the introduction of ChatGPT (\(POST: \beta = -0.0014, p < .001\)). Notably, this decline further contributed to an existing downward trend in this measure (\(Time: \beta = -0.0008, p < .001\)). Furthermore, Granger causality tests indicated that AI usage significantly predicted reductions in the variance of writing complexity at lag 5 (\(F(5, 66) = 2.55, p = .036\)), lag 6 (\(F(6, 63) = 3.32, p = .007\)), lag 7 (\(F(7, 60) = 2.58, p = .022\)), and lag 8 (\(F(8, 57) = 2.21, p = .040\)). These findings suggest that AI tools are influencing academic writing by promoting convergence toward stylistic norms and reducing variability in scientific communication.

In the Patch News dataset, the shock analysis identified significant changes in linguistic complexity variance, but Granger causality tests were not significant. Over time, there was a slight upward trend in complexity variance (\(Time: \beta = 0.0003, p = .001\)), but the onset of ChatGPT led to a marked decline (\(ONSET: \beta = -1.405, p < .001\)), and sustained post-launch effects further reduced variance (\(POST: \beta = -0.0022, p = .049\)). These findings suggest that while professional editorial workflows and institutional standards for consistency may buffer against the homogenizing effects of AI, subtle but measurable impacts on linguistic complexity variance remain.

\begin{table}[ht]
\caption{Significant Granger causality test results specifying the predictive power of AI usage over future writing complexity variance. Significance levels are denoted as follows: \textsuperscript{*} for \( p < 0.05 \), \textsuperscript{**} for \( p < 0.01 \).}
\centering
% \resizebox{\textwidth}{!}{%
\begin{tabular}{lccll}
\toprule
\textbf{Dataset} & \textbf{Feature and Direction} & \textbf{Lag} & \textbf{\emph{F}} & \textbf{\emph{p}} \\
\midrule
\multirow{2}{*}{ArXiv}
& AI → complexity & 5 & $F$(5, 66) = 2.55 & .036\textsuperscript{*} \\
& AI → complexity & 6 & $F$(6, 63) = 3.32 & .007\textsuperscript{**} \\
& AI → complexity & 7 & $F$(7, 60) = 2.58 & .022\textsuperscript{*} \\
& AI → complexity & 8 & $F$(8, 57) = 2.21 & .040\textsuperscript{*} \\
\midrule
\multirow{1}{*}{Reddit}
& AI → complexity & 14 & $F$(14, 39) = 2.12 & .033\textsuperscript{*} \\
\bottomrule
\end{tabular}%
% }
\label{tab:granger_results}
\begin{minipage}{\textwidth}
%\footnotesize
%\textit{Note:} Significance levels are denoted as follows: \textsuperscript{*} \( p < 0.05 \), \textsuperscript{**} \( p < 0.01 \).
\end{minipage}
\end{table}



The homogenization effect of AI usage was also pronounced in the Reddit dataset. Shock analysis revealed a significant and sustained post-launch decline in variance (\(POST: \beta = -0.0021, p < .001\)), indicating reduced linguistic complexity diversity after the introduction of ChatGPT. Granger causality tests further showed that AI usage significantly predicted reductions in writing complexity variance at lag 14 (\(F(14, 39) = 2.12, p <0.05\)). The longer lag observed in Reddit compared to those observed in the arXiv dataset suggests that users on this platform—reflecting a broad and diverse population—may adopt AI tools more gradually than those in domains with greater exposure to new technologies like the authors of arXiv papers.


\subsection*{Discussion}

Our analyses revealed a clear trend: the introduction of LLMs, marked by the launch of ChatGPT and their subsequent adoption across various online platforms, led to a reduction in the diversity of writing styles. This homogenization is evident in the significant decrease in the variance of linguistic complexity following ChatGPT's release, a finding supported by our discontinuous growth model analysis. Furthermore, a Granger causality test confirmed that the impact of LLMs on writing complexity variance extends beyond an initial shock, demonstrating a sustained influence over time. We note that exploring multiple lags to capture the temporal dynamics of this effect may have increased the risk of Type I errors. To further validate our findings pointing to LLMs homogenizing writing styles, Study 2 employs an experimental design and rigorously examines the causal relationship between LLM adoption and changes in writing complexity variance.


\section*{Study 2}

Study 1 revealed consistent declines in the variance of complexity-related linguistic features across three diverse datasets following the introduction of LLMs, suggesting a homogenization of writing styles by LLMs. In Study 2, we experimentally replicate this effect by prompting LLMs to rewrite authors' original texts. To gain deeper insights into the individual-level effects of these models, we systematically assess how LLM-generated revisions impact linguistic complexity variance and also evaluate the degree to which the revisions alter the original meaning of the texts.


\subsection*{Material}

To experimentally replicate and expand upon the findings of Study 1, we focus on the two data sources where the strongest associations between AI-adoption rate and linguistic complexity variance were observed: Reddit posts and arXiv papers. We randomly selected 1000 documents from each source, all published before GPT-3.5's release on November 30, 2022. 


\subsection*{Methodology}

We use three widely-used LLMs — GPT-3.5 \citep{chatgpt}, Llama 3 70B \citep{dubey2024llama}, and Gemini Pro \citep{gemini} — to rewrite our selected documents. Our analysis then centers on an examination of two key aspects: 1. semantic preservation, i.e., whether the LLMs retain the original meaning of the text during the rewriting process; and 2. linguistic complexity homogenization, i.e., whether LLMs significantly reduce the variability in writing complexity, essentially homogenizing the style.

\subsubsection*{Controlled LLM Rewrites}

To examine how LLMs alter the semantic content and the variance of writing complexity in the original texts, we simulate realistic scenarios where LLMs act as writing assistants. We prompt LLMs to rewrite the texts without emphasizing any specific traits or stylistic elements the author might wish to convey. We use two neutral prompts to explore varying degrees of text modification:
\begin{itemize}
    \item \textbf{Syntax\_Grammar (SG)}: ``Rewrite the following text using the best syntax and grammar, and other revisions that are necessary: \{TEXT\}''
    \item \textbf{Rephrase (R)}: ``Rephrase the following text: \{TEXT\}''
\end{itemize}

\subsubsection*{Evaluating Semantic Preservation}

To assess how LLMs affect the semantic content of the original writings, we calculate the cosine similarity between embeddings representing semantic content of the original texts and their LLM-generated counterparts. We use OpenAI's \texttt{text-embedding-ada-002} model to generate high-dimensional embeddings for both the original and rewritten texts because of their excellent performance on sentence similarity tasks \citep{greene2024embedding,conneau2018senteval} and near-human annotation capabilities \citep{gilardi2023chatgpt,alizadeh2023open}. The cosine similarity between these embeddings provides a quantitative measure of how much of the meaning of the text is preserved. A higher cosine similarity score indicates that the LLM rewrites retained the texts' original meaning, while a lower score suggests that significant changes were introduced.

We also conduct a human evaluation of semantic similarity to complement our quantitative analyses. We randomly select 20 LLM-generated texts from the Reddit dataset and ask four participants to rate their similarity to the original texts using the following rubric: 1. Different: Information conveyed is mostly different; 2. Somewhat similar: Information conveyed has notable differences; and 3. Very similar: Information conveyed only has minor differences. We then analyze these ratings using descriptive statistics and calculate Gwet's AC1 \citep{gwet2008computing} as a measure of inter-rater agreement. 


\subsubsection*{Comparing Writing Complexity Variance of Original vs. LLM-Generated Texts}

To analyze how LLMs affect the variability in writing complexity, we adopt the same complexity-related features and methodology as Study 1. We compute an aggregate writing complexity score for each document by averaging five key features: Vocabulary Simpson Index, Vocabulary Shannon Entropy, Average Dependency Link Length, Type-Token Ratio, and Hapax Legomena. We then calculate the variance of this aggregate score for both the original and LLM-rewritten documents, allowing us to examine the overall difference in writing complexity variance between the two sets. To statistically compare these variances, we utilize Levene's test, which determines if the variances of two or more groups are significantly different. This approach helps us assess whether LLMs homogenize writing complexity and lead to a more uniform writing style.


\subsection*{Results}

\begin{table*}[h]
\centering
\scriptsize
\caption{Variations of a sample text from the Reddit dataset after LLM rewrites.}
\begin{tabular}{p{1.5cm}p{10cm}}
\toprule
Prompt & Text \\
\midrule
Original & The leather straps binding my wrists and legs to the wooden chair only gave a stained squeak when I tired fighting my way out. Though I was overall fixed in position, my mid-section could still twist side to side upon the chair. $\cdots$ \\
\midrule
Rephrase & The leather straps securing my wrists and legs to the wooden chair emitted a faint squeak as I struggled against them. Despite being firmly held in place, I could still twist my mid-section from side to side. $\cdots$ \\
\midrule
Syntax Grammar & The leather straps binding my wrists and legs to the wooden chair emitted only a stained squeak as I tiredly fought against them. Despite being firmly fixed in position, I could still twist my mid-section side to side. $\cdots$ \\
\bottomrule
\end{tabular}
\label{tab:sample-original-rewritten-text-gpt}
\end{table*}

\Cref{fig:semantic-similarities-all} demonstrates the semantic similarity between original and LLM-generated texts using the Rephrase prompt and GPT3.5 (see \Cref{tab:sample-original-rewritten-text-gpt} for an example of the original and LLM-generated texts, and \Cref{semantic-similarity} in Supplementary Material for details on similar patterns observed using other LLMs and prompts). Aligned with our expectations, the semantic similarity between original and LLM-generated texts across all data sources, LLMs, and prompts was high ($87\%$ of scores were above $0.95$). Human evaluation of the original and revised texts similarly indicated a high degree of similarity, with an average similarity score of 2.97 (SD = 0.07) and a high inter-rater agreement, as given by a Gwet's AC1 of 0.947 (95\% CI: [0.868, 1]). 


\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/similarities_only_gpt.png}
    \vspace{-2em}
    \caption{Semantic similarity between original and LLM-generated texts (with Rephrase prompt on GPT3.5) across different data sources.}
    \label{fig:semantic-similarities-all}
\end{figure}

Our analysis of LLMs' impact on writing complexity variance, as shown in \Cref{tab:main-study-2-complexity-table}, revealed a consistent trend: in both the Reddit and arXiv datasets, we observed significant reductions in variance when comparing original texts to their LLM-rewritten counterparts. This finding strongly supports the hypothesis that LLMs lead to linguistic homogenization. 

\begin{table}[ht]
\caption{Changes in the variance of writing complexity between original and LLM-rewritten documents across datasets, LLMs, and prompts. Variance values before and after rewriting are displayed on either side of $\rightarrow$. Prompts include Rephrase (R) and Syntax\_Grammar (SG). The statistics and $p$-values from Levene’s test are provided in columns labeled $F$ and $p$. $p < .05$ is marked with \textsuperscript{*} and \textsuperscript{***} denotes $p <.001$.}
\label{tab:main-study-2-complexity-table}
\centering
\resizebox{\textwidth}{!}{\begin{minipage}{\textwidth}
\begin{tabular}{llcccccc}
\toprule
 & \emph{Dataset} & \emph{ArXiv} &&& \emph{Reddit} &&\\
LLM & Prompt  &  & $F$ &$p$ & & $F$ &$p$\\
\midrule
\multirow[t]{2}{*}{Gemini} & R & 0.0090 $\rightarrow$ 0.0071 & 5.062 &.024* & 0.0091 $\rightarrow$ 0.0076 & 2.126 &.145 \\
 & SG & 0.0090 $\rightarrow$ 0.0074 & 4.507	&.034\textsuperscript{*} & 0.0091 $\rightarrow$ 0.0069 & 5.399 & .020\textsuperscript{*} \\
\midrule
\multirow[t]{2}{*}{GPT3.5} & R & 0.0090 $\rightarrow$ 0.0070 & 5.031 &.025\textsuperscript{*} & 0.0091 $\rightarrow$ 0.0063 & 5.978 &.014* \\
 & SG & 0.0090 $\rightarrow$ 0.0071 & 6.276 &.012* & 0.0091 $\rightarrow$ 0.0068 & 5.652 &.017\textsuperscript{*} \\
\midrule
\multirow[t]{2}{*}{Llama 3} & R & 0.0090 $\rightarrow$ 0.0064 & 12.007 &$<.001$\textsuperscript{***} & 0.0091 $\rightarrow$ 0.0045 & 19.387 & $<.001$\textsuperscript{***} \\
 & SG & 0.0090 $\rightarrow$ 0.0111 & 3.377 &.066 & 0.0091 $\rightarrow$ 0.0069 & 3.952
 & .047\textsuperscript{*} \\
\bottomrule
\end{tabular}
\end{minipage}}
\end{table}

\subsection*{Discussion}


Our experimental replication in Study 2 confirms the key findings of Study 1. We demonstrate that LLMs significantly reduce the variance in writing complexity while preserving the core semantic content of a text (i.e., the primary information the author intends to convey). This result gives support to the link observed in Study 1 between AI usage and trends in writing complexity variance, providing further evidence for the homogenization effect of LLMs on people's writing styles. 



\section*{Study 3}

In Studies 1 and 2, we demonstrated that adopting LLMs results in a convergence towards more uniform language—illustrating the phenomenon of linguistic homogenization—through observational and experimental analyses. This reduction in variation raises the critical question of how homogenization might impact the ability to discern and perceive individuals' characteristics from text. As the ability to link lexical cues to personal traits hinges on individual variability in language use, uniform linguistic patterns would obscure or weaken such associations. To explore the broader implications of the homogenization effect of LLMs on writing styles, we examine whether and how stylistic cues connected to various personal traits are affected by LLM revisions, and determine whether the phenomenon of linguistic homogenization illustrated in Studies 1 and 2 extends to linguistic elements indicative of various demographic attributes and psychological dimensions. 
In a controlled experiment, we generate LLM-rewritten versions of texts authored by individuals with known personal traits across diverse datasets. We first replicate our previous findings and further assess whether LLMs preserve or erode the predictive power of text-based classifiers trained to infer personal attributes from text. 



\subsection*{Material}

We use diverse datasets comprising written texts (e.g., essays, social media posts, and personal reflections) from authors with known demographic information (e.g., gender, age, political affiliation) and psychological profiles (e.g., personality, dispositional empathy, moral values) information. These datasets serve as the source material for generating LLM-modified texts. Overall, the texts included in these datasets are mostly characterized by their relatively informal tone and the likely presence of personal expression.

It is important to distinguish the datasets used in this study from those typically employed in tasks where predictions rely solely on textual content, such as sentiment analysis or emotion detection. In such tasks, the target attribute is often directly reflected in the words and phrases used within the text itself. In contrast, texts and their authors' demographic or psychological attributes in our dataset were collected independently. This means the texts were not intentionally written to convey the specific traits of interest but instead reflect individuals’ natural writing styles and carry stylistic cues (as opposed to explicit content) that provide signals about personal characteristics such as personality and values. Below, we outline the specific traits, attributes, and datasets used in this study.


\paragraph{Demographic attributes.} 

We focus on age, gender, and political affiliation as demographic attributes. Our data source is the United States Congressional Records \citep{gentzkow2018congressional}, which includes congressional floor speeches along with the speakers' demographic details \citep{fivethirtyeight_age_problem}. The dataset spans the 43$^{\text{rd}}$ to the 114$^{\text{th}}$ Congress, containing speeches from 8,520 speakers, with each speaker having delivered between 1 and 21,142 speeches of varying lengths. To ensure the quality and manageability of the data, we filter the dataset by selecting each speaker's longest utterances, aiming for a total of 4,000 words per speaker. This step removes short, uninformative utterances and accommodates computational limits on the number of tokens processed. We then sample the largest possible subset of speakers with a balanced representation of males/females, Republicans/Democrats, and four age groups: 27-40, 41-55, 56-70, and over 70, resulting in a final sample of 710 speakers.

\paragraph{Personality.} To study personality, we use the widely accepted Big Five personality model \citep{goldberg2013alternative,goldberg1990standard}, which identifies five key dimensions of personality: openness (OPN), conscientiousness (CON), extraversion (EXT), agreeableness (AGR), and neuroticism (NEU). Our data source is the Essays dataset \citep{pennebaker1999linguistic}, which includes 2,348 essays, each written by a unique author, alongside the authors' scores on the 44-item Big Five Personality Inventory \citep{john1991big}. The essays were written using a stream-of-consciousness method, where authors were encouraged to freely express their thoughts. After completing the writing task, the authors completed the personality assessment. Following the approach of \citet{celli2013workshop}, we convert the authors' numerical self-assessments into nominal classes (low/high) using a median split based on $z$-scores. 

\paragraph{Dispositional empathy.}

We use the Interpersonal Reactivity Index (IRI; \citealp{davis1980multidimensional}), a widely used tool for assessing individual differences in empathy. The IRI is a 28-item self-report questionnaire that measures empathy across four dimensions: perspective-taking (PT), fantasy (FS), empathetic concern (EC), and personal distress (PD). For our investigation, we utilize the Empathetic Conversations dataset \citep{omitaomu2022empathic}, which was previously employed in the WASSA 2023 shared task \citep{barriere2023findings}. This dataset includes essays written in response to news articles along with the authors' IRI scores. Similar to the approach used for the Essays dataset, we convert the IRI scores into nominal classes indicating low/high levels for each dimension. After cleaning and preprocessing, the dataset comprises 711 essays written by 57 authors, each contributing between 1 and 72 reaction essays. We concatenate the essays from each author to create a more comprehensive representation of the authors' writing styles and better capture author-specific patterns while appropriately accounting for the variations in each author's writings. 


\paragraph{Morality.} 

Moral Foundations Theory (MFT; \citealp{graham2013moral,haidt2004intuitive,atari2023morality}) is one of the most widely used frameworks for understanding moral cognition. It identifies five core psychological systems that guide moral reasoning: care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, and purity/degradation. For our study, we utilize the YourMorals dataset \citep{kennedy2021moral}, which contains 107,798 Facebook posts from 2,691 users, along with their scores on the Moral Foundations Questionnaire (MFQ) \citep{graham2008moral}. Following the approach used in the Essays dataset, we convert the MFQ scores into nominal classes representing low/high levels for each moral foundation dimension. Consistent with our method in the Empathetic Conversations dataset, we concatenate the posts from each author to create a comprehensive profile.


\subsection*{Methodology}

\subsubsection*{Controlled LLM Rewrites}

To assess the impact of LLM-driven homogenization on personal trait prediction, we generate rewritten versions of original texts using multiple LLMs and prompts following the same procedure as Study 2. This allows us to systematically examine whether the linguistic cues essential for trait inference are retained or diminished in LLM-rewritten texts.

\subsubsection*{Predictive Model Setup}
The primary goal of this study is to quantify the reduction in predictive power for personal traits when classifiers, trained and tested on original texts, are applied to LLM-rewritten versions of those same texts.
To measure the predictive power of linguistic markers, we employ a bottom-up, data-driven approach \citep{kennedy2022handbook} that involves training classifiers on the authors' original texts to predict their demographic and psychological traits. These classifiers are then applied to LLM-generated texts to compare their predictive performance.

The classifiers used in this study are:\footnote{Zero-shot classification by LLMs was considered, but given the complexity of this task \citep{abdurahman2023perils} as well as their inferior performance compared to trained classifiers presented in \Cref{using-llms-zero-shot-classifiers} in Supplementary Material, we opted for training and fine-tuning classifiers in each experimental setup.}
\begin{itemize}
    \item Support Vector Machines (SVMs)
    \item Logistic Regression
    \item Random Forest
    \item Gradient Boosting
    \item Longformer \citep{beltagy2020longformer}, a Transformer model for long document processing.
\end{itemize}

These classifiers are used with two different featurization techniques:
\begin{itemize}
    \item Term Frequency-Inverse Document Frequency (TF-IDF)
    \item OpenAI text-embedding-ada-002 embeddings
\end{itemize}

This combination of traditional and state-of-the-art techniques allows us to cover a broad spectrum of feature extraction methods and classification models capturing different aspects of linguistic cues. 

Each dataset is split into train, validation, and test sets using 5-fold cross-validation. We measure classifiers' performance using the $F_1$-macro score to account for imbalanced classes. The experiment is repeated 40 times with different random seeds to ensure robustness. Only models that achieve above-random performance on the original texts and have at least 20 successful runs are included in the analysis.

To compare the average predictive performance of classifiers on the LLM-generated texts and the original texts, we use paired $t$-tests. Note that when comparing different experimental setups (e.g., two prompts), other variables are held constant (e.g., LLM and classifier) to isolate the effect of hyperparameters while accounting for the inherent characteristics of each model, prompt, or classifier. Given the large number of comparisons across classifiers, prompts, and LLMs, we apply Bonferroni corrections \citep{bonferroni1936teoria} to adjust significance thresholds. 

To more closely evaluate any patterns in the models' predictions and capture any imbalance between predictions, we define a new metric $\Delta_{r} = \frac{|P_{r, 1} - P_{r, 0}|}{P_{r, 1} + P_{r, 0}}$, where $P_{r, i}$ is the frequency of predictions of class $i$ ($i \in \{0, 1\}$)\footnote{To quantify the prediction imbalance in age groups which have more than two classes, we focus on the imbalance between the most and the least frequently predicted classes.} in run $r$. Since the random baseline across the classification tasks is close to $50\%$, indicating a balance between the two classes, a large $\Delta$ signals that the model’s predictions are skewed toward one class—either 0 or 1. Thus, the further $\Delta$ is from zero, the more biased the predictions are, suggesting that the text being analyzed contains linguistic features that disproportionately influence the model toward associating the texts with one particular class. We perform a Wilcoxon signed-rank test, a non-parametric counterpart of paired $t$-test, due to non-normality in the data, to test the significance of the differences in $\Delta$ values between original and LLM-generated texts.

Finally, we examine changes in classifier predictions between the original and the LLM-generated texts to identify which demographic or psychological attributes become more prominently predicted or reinforced in LLM-generated texts. In particular, we consider the direction of changes in classifier predictions as a proxy for what the LLMs promote in their generated output, essentially revealing which groups the text tends to reflect more. We perform paired $t$-tests between the number of prediction changes from 0 to 1 and from 1 to 0 after LLM rewrites to quantify the significance of the shift in predicted demographic or psychological attributes.


\subsection*{Results}
Replicating our findings in Studies 1 \& 2, we find that, in general, LLM rewrites significantly reduced the variance of writing complexity in three of the four datasets (see \Cref{tab:main-sm-study-3-replication-complexity}) and do not change the semantic content in texts (see \Cref{fig:semantic-similarities-study3-replication}).

\begin{table}[ht]
\captionsetup{font=LARGE}
\caption{Changes in the variance of aggregate writing complexity between original and LLM-rewritten documents across the Essays, YourMorals, Congress, and Empathetic Conversations datasets, LLMs, and rewriting prompts. Variance values are shown before and after rewriting are displayed on either side of $\rightarrow$. Prompts include Rephrase (R) and Syntax \& Grammar (SG). $p$-values from the Levene’s test are provided in columns labeled $p$, with $p < .01$ and $p < .001$ marked with \textsuperscript{**} and \textsuperscript{***}, respectively.}
\label{tab:main-sm-study-3-replication-complexity}
\centering
\resizebox{\textwidth}{!}{\begin{minipage}{\textwidth}
\begin{tabular}{p{.6cm}cccccccccccccc}
\toprule
 &  \emph{Dataset}& \emph{Essays} &&& \emph{YourMorals} &&& \emph{Congress} &&& \emph{Empathetic C.} && \\
LLM & Prompt & &$F$&$p$& &$F$& $p$&  &$F$&$p$ &&$F$& $p$ \\
\midrule
\multirow[t]{2}{*}{Gemini} & R & .0151 $\rightarrow$ .0112 & 39.233 & $< .001$ \textsuperscript{***} & .0092 $\rightarrow$ .0074 & 119.808 & $< .001$ \textsuperscript{***} & .0197 $\rightarrow$ .0083 & 131.365 & $< .001$ \textsuperscript{***} & .0079 $\rightarrow$ .0071 & .003 & .958 \\
 & SG & .0151 $\rightarrow$ .0089 & 103.269 & $< .001$ \textsuperscript{***} & .0092 $\rightarrow$ .0094 & 36.435 & $< .001$ \textsuperscript{***} & .0197 $\rightarrow$ .0123 & 37.274 &  $< .001$ \textsuperscript{***} & .0078 $\rightarrow$ .0075 & .048 & .826 \\
\midrule
\multirow[t]{2}{*}{GPT3.5} & R & .0151 $\rightarrow$ .0152 & 1.530 & .216 & .0092 $\rightarrow$ .0060 &  93.474 & $< .001$ \textsuperscript{***} & .0197 $\rightarrow$ .0108  & 65.211 & $< .001$ \textsuperscript{***} & .0080 $\rightarrow$ .0052 & 1.959 & .164 \\
 & SG & .0151 $\rightarrow$ .0153 & 1.880 & .17 & .0092 $\rightarrow$ .0075 & 63.159 & $< .001$ \textsuperscript{***} & .0197 $\rightarrow$ .0118 & 47.236 & $< .001$ \textsuperscript{***} & .0080 $\rightarrow$ .0069 & .693 & .407 \\
\midrule
\multirow[t]{2}{*}{Llama 3} & R & .0150 $\rightarrow$ .0159 & .232 & .63 & .0083 $\rightarrow$ .0073 & 2.299 & .130 & .0197 $\rightarrow$ .0138 & 62.600 & $< .001$ \textsuperscript{***} & .0080 $\rightarrow$ .0062 & .393 & .532 \\
 & SG & .0150 $\rightarrow$ .0124 & 127.087 & $< .001$ \textsuperscript{***} & .0082 $\rightarrow$ .0070 & 8.395 & .004 \textsuperscript{**} & .0197 $\rightarrow$ .0152 & 36.412 & $< .001$ \textsuperscript{***} & .0080 $\rightarrow$ .0105 & .802 & .372 \\
\bottomrule
\end{tabular}
\end{minipage}}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/similarities_only_gpt_study3.png}
    \vspace{-2em}
    \caption{Semantic similarity between original and LLM-generated texts (with Rephrase prompt on GPT3.5) across Essays, YourMorals, Congress, and Empathetic Conversations datasets.}
    \label{fig:semantic-similarities-study3-replication}
\end{figure}

\Cref{tab:ttest_res} displays the average differences between $F_1$ scores of classifiers predicting authors' psychological and demographic attributes on the original text and the LLM-generated texts. We observed an average 6\% decline in the absolute $F_1$ score of classifiers, but interestingly, performances on the LLM-generated texts did not fall below the random baseline.

Paired $t$-tests indicated that the performance of classifiers on the LLM-generated texts was significantly lower compared to the performance on the original texts across different constructs ($p<.001$; small-to-moderate Cohen's $d$ effect sizes). In particular, among different personal traits, the linguistic markers of age groups were affected the most by LLMs, where the average performance of classifiers on the LLM-generated texts was dropped to $F_1=.260 $, compared to $F_1=.351$ on the original texts (moderate effect size). 
Across different experimental setups, we observed consistent reductions in the predictive power of linguistic markers of personal traits, regardless of LLM, prompt, or classifier condition. For a more detailed analysis of these trends, please refer to \Cref{additional-insights-about-loss-in-predictive-power-personal-traits} in Supplementary Material.


\begin{table}[h!]
\caption{Performance comparison between trained personal traits' classifiers on the original (OG) and the LLM-rewritten texts. Paired $t$-tests assess the statistical significance of differences in predictive power ($F_1$). All tests were significant with $p<.001$. Cohen’s \emph{d} effect sizes indicate the magnitude of these differences ($0.2 <|d|<0.5$: small, $0.5<|d|<0.8$: medium; $|d|>.8$: large, \citealp{cohen}). $\Delta_{\text{Mean } F_1 }$ indicates the mean difference estimate for $F_1$ values. RB stands for random baseline.}
\label{tab:ttest_res}
\centering
\resizebox{\textwidth}{!}{\begin{minipage}{\textwidth}
\begin{tabular}{lcccccrrrrc}
\toprule
\text{Trait}  & Mean $F_{1OG}$ & $\text{Mean $F_1$}_{\text{LLM}}$ & $\Delta_{\text{Mean } F_1 }$& 95\% CI & SE & \emph{t} & df & \emph{d} & RB\\
\midrule
Age group & .351 & .260 & .091 &[.079, .102] & 0.006 & 15.397 & 804 & 0.543 &  .244\\

Empathy & .657 & .603  & .054&[.050, .057] & 0.002 & 32.158 & 6556 &  0.397 & .541\\

Personality & .658 & .602 & .056&[.052, .060] & 0.002 & 30.046 & 7652 &  0.343 &  .514\\

Gender & .694 & .623 & .072&[.063, .080] & 0.004 & 15.965 & 1371 &  0.431 & .495\\

Morality & .640 & .578 & .062 &[.057, .066] & 0.002 & 25.601 & 5195 &  0.355 &  .521\\

Affiliation & .664 & .591 & .073&[.064, .082] & 0.005 & 15.691 & 1313 &  0.433 & .490\\
\bottomrule
\end{tabular}
\end{minipage}}
\end{table}

Next, we analyzed the distributions of $\Delta$ (a proxy for imbalances between the predicted classes) for both the original and LLM-rewritten texts to assess whether the decline in predictive power is disproportionately concentrated in specific classes or if LLMs reduce predictive power uniformly across all classes. Our findings indicate that $\Delta$ for LLM-generated texts is consistently larger than for original texts across nearly all categories (see \Cref{fig:difference-between-predictions-homogeneity}). This suggests that LLM-rewritten texts exhibit greater linguistic homogeneity, effectively diminishing the distinctiveness of individual traits and skewing predictions toward a dominant class. Notably, this trend persists across different LLMs and various dimensions of the analyzed constructs (see \Cref{systematicity-of-change-in-predictive-powers} in Supplementary Materials).


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/homogeneity_of_predictions_agg_gpt.png}
    \caption{The distribution of $\Delta$ (a proxy for imbalances between the predicted class frequencies on the original and LLM-rewritten texts) across different personal traits, focusing on the LLM-rewritten texts generated by GPT3.5. The $W$ statistics from the Wilcoxon test are displayed on top, with $p < .05$, $p < .01$, and $p < .001$, marked with \textsuperscript{*}, \textsuperscript{**}, \textsuperscript{***}, respectively.} 
    \label{fig:difference-between-predictions-homogeneity}
\end{figure*}




\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/difference_in_predicted_labels_agg.png}
    \caption{Percentage of original texts with correct author attribute predictions that changed after LLM rewriting, grouped by the direction of change in predictions.}
    \label{fig:difference-in-predictions-agg}
\end{figure*}


\Cref{fig:difference-in-predictions-agg} illustrates changes in classifier predictions between original and LLM-generated texts based on the proportion of predictions that changed from correct to incorrect and their directions across different personal traits. These analyses reveal striking trends in certain demographic attributes becoming more prominently predicted than others. For example, LLM-generated texts displayed a marked tendency to lower the predicted empathy of authors, with the model predicting authors as having lower empathy levels (except for personal distress) than they actually did ($t(6698) = 20.18, p <.001, \text{Cohen's}\;d = 0.35$ [small effect size]; using a paired $t$-test). At the same time, LLM-generated texts were more likely to be associated with authors having higher levels of morality ($t(3051) = 21.25, p <.001, d = 0.61$ [medium effect size])\footnote{Statistical tests were performed on the full empathy (IRI) and morality (MFQ) measures. Dimension-specific tests can be found in \Cref{What-Author-Attributes-do-LLMs-Promote-the-Most?} in the Supplementary Material.} and belonging to older age groups ($t(597) = 16.84, p <.001, d = 1.09$ [large effect size]). We observed that LLM-generated text is associated with people with higher levels of openness ($t(1074) = 6.50, p < .001, d = 0.21$ [small effect size]), agreeableness ($t(1005) = 9.05, p < .001, d = 0.27$ [small effect size]), and lower levels of extraversion ($t(1054) = 15.14, p < .001, d = 0.52$ [medium effect size]) compared to the actual authors.\footnote{Significant differences in neuroticism and conscientiousness were observed ( $p < .05$ ), but the effect sizes were negligible.} There were also distinct shifts in political affiliation predictions. LLM-generated text caused classifiers to incorrectly predict authors as Democrats more often than the reverse ($t(867) = 24.89, p <.001, d = 1.35$ [large effect size]), further suggesting that the homogenization observed in LLM-generated texts might be skewed towards certain demographic traits. Gender predictions also shifted, showing a bias toward predicting ‘Male’ over ‘Female’ ($t(613) = 5.68, p <.001, d = 0.37$ [small effect size]) when LLMs were involved in writing. 

\subsection*{Discussion}

Diving deeper into the social implications of the homogenization effect of LLMs in people's writing styles, our analysis reveals that while LLM-rewritten texts preserve the core meaning, LLMs systematically erase unique linguistic signatures that convey personal traits, as reflected in the reduced predictive power of classifiers trained to infer these traits. This effect is not random; rather, the shifts in writing style systematically lead to predictions toward specific demographic and personality attributes. Across datasets, we observe a consistent bias in LLM-generated texts with writing styles often aligning with traits associated with older, male, politically liberal individuals and exhibiting positive moral valence and lower empathy. Furthermore, while we observe consistent and measurable declines in the predictive power of linguistic cues, classifiers continue to perform well above chance, suggesting that LLMs do not completely erase the linguistic cues that convey personal traits. To further understand these shifts and to assess how LLMs alter the associations between linguistic features and personal traits, Study 4 takes a more granular approach, analyzing fine-grained lexical cues typically linked to these traits and examining how their presence and distributions change in LLM-rewritten texts.

\section*{Study 4}

In Study 3, we demonstrated that LLMs not only homogenize writing styles but also systematically remove and alter linguistic features that reflect authors’ personal traits. We also found that homogenization was not accompanied by a complete loss of predictive power, suggesting that LLMs do not entirely eliminate the informative linguistic cues linked to personal traits. To explore the mechanisms behind these observations, we draw on studies that focus on fine-grained linguistic cues associated with personal traits \citep[e.g., ][]{kennedy2021moral, hirsh2009personality, mehl2006personality} in Study 4. We specifically examine fine-grained lexical cues traditionally linked to personal traits by first replicating well-established associations between these cues and personal traits in original documents and then exploring how LLMs affect these associations.

\subsection*{Material}

We use the same datasets, LLMs, and prompts as in Study 3. This enables us to work with datasets containing both original and LLM-rewritten versions of documents where the personal traits of the authors are known.

\subsection*{Methodology}

To study the associations between linguistic features and personal traits, we adopt a top-down, theory-driven approach \citep{kennedy2022handbook}. Specifically, we extract linguistic features from texts using validated lexicons commonly employed in psychology and social science research. By comparing the associations between these linguistic cues and personal traits in original and LLM-rewritten texts, we aim to identify how LLMs influence these relationships to gain insight into how LLMs alter writing styles, reducing but not eliminating the predictive power of linguistic features in text. We further analyze the distributional shifts and the homogenization of these important linguistic features in \Cref{lexical-shifts} in Supplementary Material. 

To extract associations between linguistic features and personal traits, we focus on various well-established dictionaries. For each dictionary category, we calculate the ratio of words belonging to that category relative to the total word count, then standardize these frequencies as scores. We compute Pearson correlations ($r$) between these standardized word frequencies and the $z$-scores of continuous traits (e.g., age, personality, empathy, morality) and perform $t$-tests to compare categorical traits (e.g., gender, political affiliation). To control for multiple comparisons, we apply Bonferroni corrections. 


We use the following general-purpose and content-specific dictionaries:

\begin{itemize}
    \item \textbf{Linguistic Inquiry and Word Count (LIWC)}: We use the merged version of LIWC 22 and LIWC 07 \citep{pennebaker2022linguistic}, a widely used lexicon that captures psychological, social, cognitive, and affective processes.
    \item \textbf{NRC Emotion Lexicon}: A 10-category lexicon \citep{Mohammad13, mohammad-turney-2010-emotions} that captures emotional expression, used to explore associations with personality traits.
    \item \textbf{Empathy Lexicon}: A lexicon categorized into high-empathy, low-empathy, and distress-related words (four categories), used to examine links between language use and dispositional empathy \citep{sedoc2019learning}.
    \item \textbf{Moral Foundations Dictionary 2 (MFD2)}: A lexicon capturing moral language across five foundations, with separate categories for virtue and vice \citep{frimer2019moral}.
\end{itemize}

\subsection*{Results}

\begin{table*}[!htb]
    \caption{Highlighted correlations between various linguistic cues and author attributes (see \Cref{top-down-analysis} in Supplementary Material for exact values and additional LIWC categories) on original and LLM-generated texts. \checkmark and \xmark~indicate significant and insignificant correlations, respectively.}
    \label{tab:checkmarks-personality-morality-empathy}
    \centering
    \begin{minipage}{.5\linewidth}
        \centering
        \resizebox{\textwidth}{!}{%
            \input{tables/checkmark_personality_corr}
        }
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
        \centering
        \resizebox{\textwidth}{!}{%
            \input{tables/morality_checkmark_corr}
        }
    \end{minipage} 
\end{table*}

\Cref{tab:checkmarks-personality-morality-empathy} contains a simplified illustration of some important associations between lexical categories and various dimensions of author attributes in authors' original and LLM-generated texts (see \Cref{top-down-analysis} in Supplementary Material for exact values and additional categories). 

Our analysis of the original texts successfully replicated many well-documented findings. For example, consistent with previous research, male authors used fewer social and anxiety-related words than females \citep{ishikawa2015gender}. Similarly, higher extroversion (EXT) was linked to greater use of positive emotions and social words \citep{chen2020meta}, and individuals scoring high on loyalty used more family-related words \citep{LiTomasello}. We also observed links between openness (OPN) and the use of complex words (BigWords), and between fairness and religion-related words.

Comparing associations found in original and LLM-generated documents, we found that many of these associations were washed away by LLM involvement. For example, the relationship between gender and negative emotion words, which was well-established in the original texts, was largely erased in the LLM-generated rewrites. Similarly, the relationship between extraversion (EXT) and the use of pronouns, loyalty and the use of friend-related words, and age with words focusing on the future were weakened in the LLM-generated texts. In contrast, some associations, such as the connection between neuroticism (NEU) and negative emotion words, the relationship between purity and religion-related words, and the use of social words in relation to gender, persisted.  


\subsection*{Discussion}

To better understand the reduction in predictive power observed in Study 3—where classifiers lost some but not all of their ability to predict personal traits—we examined how LLMs alter well-established lexical cues linked to personal traits. Our findings reveal that this decline is reflected in the disruption of established associations between linguistic features and personal traits. While some informative linguistic markers remained intact, preserving a degree of predictive power, many key associations were erased. This selective disruption in well-established associations suggests that LLMs are not merely introducing random noise but are systematically altering the connection between language and identity. 


\section*{General Discussion}

The words we speak paint a portrait of who we are, where we belong, and how we connect with the world around us, solidifying language's central place in the human story. This richness, stemming from the inherent diversity of human expression, allows us to glean insights into individual and societal characteristics. However, the increasing popularity of LLMs as writing assistants raises a key question: are these tools eroding the distinctiveness of individual expression? Given that LLMs generate text by predicting statistically probable continuations based on massive training corpora, their widespread adoption may risk homogenizing language, potentially obscuring the distinctive nature of language and limiting its ability to convey individuality. 


To investigate this potential homogenization effect, we conducted four studies examining different aspects of LLMs' influence on language. First, we demonstrated a decline in the variance of writing complexity across multiple online platforms after the introduction of ChatGPT in November 2022, suggesting a trend toward more uniform writing styles. Controlled experiments then confirmed that LLMs reduce variation in writing complexity during text generation, further supporting the idea that LLM usage significantly contributes to linguistic homogenization. We established that LLMs systematically alter textual cues signaling authors' personal traits by homogenizing stylistic elements, shifting the perceptions about the authors toward specific demographic and personality profiles while preserving the core meaning of the original texts. Finally, we demonstrated that associations between specific, well-established lexical features and psychological or demographic attributes weaken in LLM-assisted texts, providing further insights into how LLMs strip cues about personal traits. Collectively, these findings highlight a profound transformation in the nature of individual expression, diminishing the identity signals embedded in language and leading to a loss of linguistic uniqueness when LLMs are integrated into the writing process.


The trend of homogenization of writing styles appeared across formal and informal settings, including Reddit, academic papers, and news articles, though its impact varied by domain. Specifically, this effect was more pronounced in Reddit, a platform traditionally rich in personal and creative expression, and in academic writing, where AI tools help standardize manuscripts but risk narrowing the range of narrative styles in scientific communication. In contrast, homogenization in news articles was less pronounced: while LLMs introduced a sustained reduction in the variance of writing complexity, no predictive relationship between AI adoption and homogenization was found, possibly due to existing editorial guidelines limiting direct AI adoption. However, subtle reductions in writing complexity variance suggest that AI is still shaping journalistic style, perhaps not through direct adoption but by influencing broader writing norms. Even without direct use, increased exposure to and imitation of AI-generated content might gradually reshape stylistic standards, contributing to the erosion of linguistic diversity even in domains where LLMs are not the primary driver of homogenization. Together, these findings highlight how LLMs may reinforce and reshape stylistic norms through both direct use and indirect exposure, potentially suppressing individual expression in domain-specific ways.



Our investigation into LLM-driven homogenization of linguistic indicators of personal traits revealed that while the core meaning of texts is preserved, stylistic features are systematically altered or removed. This suggests that LLMs primarily modify stylistic elements— which, while arguably secondary to content, crucially convey information about the identity and background of authors. As a result, classifiers struggled to accurately predict authors’ traits. Notably, LLM-rewritten texts tended to align with traits commonly associated with older, male, politically liberal individuals and exhibited a more positive moral valence with lower empathy. These patterns align with existing research on LLMs’ inherent biases \citep[e.g., ][]{navigli2023biases,atari2023humans,wang2024large,rozado2024political,pan2023llms,abdurahman2023perils}, and suggests that even when primed with text containing cues to an author’s traits, LLMs tend to reinforce personality profiles consistent with their pre-existing biases. These shifts in linguistic expression can have far-reaching implications across domains such as social media, professional communication, clinical psychology, and mental healthcare, where diminished linguistic diversity may undermine efforts to recognize and leverage individual differences in expression. Similarly, in hiring, applicants with AI-assisted writings whose texts conform to implicit stylistic expectations may be favored against those whose original writing is more distinctive but less conventionally polished, potentially exacerbating existing barriers to equity.
More broadly, declines in language's ability to authentically reflect and differentiate aspects of identity may result in a loss of diversity in human expression, subtly reshaping norms in ways that prioritize conformity over individuality.

Furthermore, we showed that the reductions observed in the predictive power of personal trait classifiers following LLM rewrites do not occur randomly but rather arise from the systematic erosion of well-established linguistic markers associated with those traits. This finding is particularly concerning as such linguistic cues are widely relied upon in both academic and industry settings, and their disruption could lead to misleading insights and decisions. If traits like neuroticism or values like loyalty lose their consistent linguistic signals, inferences about these characteristics may become unreliable. Similarly, in contexts like therapy sessions or customer service interactions, the masking or distortion of key emotional and empathetic cues by LLMs could result in misinterpretations with significant consequences.

The selective suppression of some associations and the preservation of others raises an important question: what factors determine which markers are retained and which are disrupted? One possibility is that LLMs implicitly recognize and retain certain linguistic distinctions while filtering out others due to biases in training data and alignment procedures. Since AI training prioritizes factual accuracy and may lack explicit mechanisms to preserve individual writing styles— such as mapping stylistic features to verified personality or demographic labels— LLMs may fail to reliably maintain fine-grained markers of identity. Further investigation into this process could offer valuable insights into how LLMs encode and alter linguistic features.

While the current research focused on writing complexity, both behavioral factors  — such as individual adoption patterns, stylistic preferences, and dynamics between content creators and audiences — and non-behavioral factors— such as platform- or organization-level policies— can influence the form and extent of LLM-driven homogenization. Understanding how these elements interact is crucial for assessing the broader trend toward linguistic uniformity. This trend is complex and difficult to model, as LLMs and their training processes shape which features lose variability. However, the trend persists due to the nature of LLM training: during pre-training, LLMs optimize for the most statistically likely continuations, reinforcing dominant patterns present in the training data. In the alignment phase, outputs are further shaped by the preferences of a limited set of annotators, emphasizing prevailing linguistic norms while filtering out deviations. These processes inherently constrain the diversity of generated language, leading to a systematic narrowing of expression that reflects both data-driven patterns and the biases embedded in alignment. Importantly, trends will only intensify as new LLM-influenced text is added to online repositories, which, in turn, will be used for further training, perpetuating a feedback loop that progressively reduces linguistic variability.

Lastly, given the inextricable connection between language and thought \citep{evans2009myth, sapir1956language}; the homogenization of language through the widespread adoption of LLMs may have significant long-term implications for cognitive diversity. By potentially restricting the range or quality of linguistic resources at our disposal, LLMs risk limiting variability in linguistic structures, reducing the cognitive flexibility and creativity that allows us to process and innovate upon complex ideas. This narrowing of linguistic diversity could, at an extreme, ``narrow the range of thought'' \citep[][p.70]{georgeorwell}, as the diminishing array of cognitive possibilities constrain our ability to think in diverse ways.


% \subsection*{Conclusion}
In conclusion, while LLMs offer powerful tools for enhancing clarity and accessibility in writing, our findings reveal a critical and potentially alarming consequence: the erosion of linguistic distinctiveness. Across diverse sources and controlled experiments where LLMs were used for rewriting, we consistently observed a trend toward homogenization. As AI-mediated communication becomes increasingly prevalent, it is imperative that we ensure these tools enhance, rather than extinguish, the rich tapestry of human linguistic diversity.



\backmatter

%\bmhead{Supplementary information}

\bmhead{Acknowledgements}
This research was supported, in part, by the Army Research Laboratory under contract W911NF-23-2-0183 and by DARPA INCAS HR001121C0165. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. We also thank James W. Pennebaker for their support and invaluable insights throughout the research process.

\bmhead{Data availability}
Most of the datasets used in this study are publicly available. Reddit data was obtained from Project Arctic Shift (\url{https://github.com/ArthurHeitmann/arctic_shift}), which is primarily based on the Pushshift Reddit database (PRD; \citealp{baumgartner2020pushshift}). The ArXiv dataset was sourced from a collection compiled by \citet{arxiv_org_submitters_2024}. Additionally, the YourMorals dataset \citep{kennedy2021moral}, the Empathetic Conversations dataset \citep{omitaomu2022empathic}, and the United States Congressional Records \citep{gentzkow2018congressional} were all publicly available. The only dataset that is not publicly available is the Essays dataset \citep{pennebaker1999linguistic}, which we accessed through a direct request to James W. Pennebaker.


% \bmhead{Code availability}
% All code used in this study will be made available in a GitHub repository (link omitted to maintain author anonymity). For the editor and reviewers, the code and further details about our analysis can be provided upon request.
\bmhead{Code availability}  
All code, data, and materials used in this study are available in the following GitHub repository:  
\url{https://github.com/zhpinkman/LLMs-effect-on-linguistic-diversity/}.



%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

\begin{appendices}

\section{Homogenization Trends}
\label{homogenization-trends}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/complexity_trends_all_features.png}
    \caption{Trends in variance of complexity-related features in texts and the attribution rate of texts as AI-generated.}
    \label{fig:results-for-homogenization-effect-all-features}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/all_sources_complexity_trends_mean.png}
    \caption{Trends in average writing complexity in texts and the attribution rate of texts as AI-generated.}
    \label{fig:results-for-homogenization-effect-mean}
\end{figure*}

Study 1 provided strong evidence for the homogenization effect of LLMs, showing a marked reduction in linguistic complexity variance across documents after the introduction of ChatGPT. This trend was shown by aggregating multiple features related to linguistic complexity. \Cref{fig:results-for-homogenization-effect-all-features} illustrates this trend across all complexity-related features analyzed in this paper, revealing a consistent decline in variability following the introduction of ChatGPT on November 30, 2022. Notably, this homogenization is not due to a simplification of language but rather an overall enhancement of complexity—\Cref{fig:results-for-homogenization-effect-mean} shows that complexity-related features increased on average. This suggests that LLMs are promoting a more polished and elaborately structured style of writing, amplifying certain sophisticated linguistic patterns while narrowing the range of individual expression.


\section{LIWC and Construct-specific Lexicons}
\label{liwc-categories}

In this section, we present the details of the word categories that were used from LIWC or other construct-specific lexicons in Study 4. 

\subsection{LIWC}
The Linguistic Inquiry and Word Count (LIWC;  \citealp{pennebaker2001linguistic}) dictionary is a collection of words categorized into different psychological and linguistic dimensions. It is the backbone of the LIWC software that analyzes text by counting occurrences of words in specific categories, offering insights into the involved emotions, cognition, social interactions, and other linguistic styles. The LIWC categories that we used in this study are as follows:

\begin{itemize}

    \item  Affect: The Affect category is a measure of the emotional tone as well as emotion content of text such as 'happy', 'hate', 'love', and 'wrong'.
    
    \item  emotion: Emotion words represent a broad category of  emotional expressions referring to specific emotional states, such as 'good', 'love', 'happy', 'hope'.

    \item assent: Assent words are expressions of agreement, confirmation, or approval, typically used in conversational contexts. These include terms like ‘yeah’, ‘yes’, ‘okay’, and ‘ok’, which signal a positive or affirmative response in communication.
        
    \item  emo\_neg: Negative emotion words express negative affective states, including terms like 'bad', 'hate', 'hurt', 'tired'.
    
    \item  emo\_pos: Positive emotion words convey positive emotions, including terms like 'happy', 'excited', and 'grateful'.
    
    \item  emo\_anger: Anger words express anger or frustration, including terms like 'hate', 'mad', and 'angry'.
    
     \item  emo\_sad: Emotionally sad words convey feelings of sadness or melancholy, including terms like ':(', 'sad', and 'disappoint'.
     
     \item  swear: Swear words consist of profane or vulgar language used to express strong emotions or taboo subjects, such as 'damn', 'f***', and 'sh*t'.
    
    \item  affiliation: Affiliation words indicate a sense of belonging or connection to others, including terms like 'friend', 'community', and 'support'.
    
    \item socrefs: Social referents encompass words that relate to social interactions, like 'we', 'you', or 'he', as well as words related to family and friends (e.g., 'parent', 'mother', 'girlfriend').
    
    \item  family: Family words relate to familial relationships or members, such as 'mother', 'father', 'sibling', and 'cousin'.

    \item  friend: Friend words relate to friendship or friendly interactions, including terms like 'friend', 'buddy', 'dude', and 'girlfriend'.
        
    \item  pronoun: Pronouns are words used to replace nouns indicating individuals or groups, such as 'I', 'you', 'he', 'she', 'we', and 'they'.

    \item we: This category refers to words indicating group membership or inclusion, such as 'we', 'us', and 'our'.
    
    \item  you: Second-person pronouns, such as 'you' and 'your'.
    
    \item  shehe: Third-person pronouns refer to individuals such as 'he', 'she', 'him', and 'her.'
        
    \item i: First-person singular pronouns refer to the speaker or author, such as 'I', 'me', 'myself', and 'mine'.

    \item  differ: Differentiation words express contrast or distinction between entities or ideas, including terms like 'but', 'not', 'or', and 'if'.

     \item  tentat: Tentative words express uncertainty or hesitation, including terms like 'maybe', 'if', and 'something'.

     \item  cogproc: Cognitive processing words indicate intellectual or cognitive engagement, including terms like 'think', 'understand', 'analyze', and 'consider'.
        
    \item  prosocial: Prosocial words denote behaviors or attitudes that benefit others or society, such as 'help', 'care', 'thank', and 'please'.
        
    \item  BigWords: Big words are words with complex structures (7 letters or longer) that may indicate intellectual complexity or formality, such as 'procrastination', 'circumstantial', and 'phenomenon'.
    
    \item  Drives: Drive words refer to motivational or goal-oriented language, encompassing affiliation (e.g., 'we', 'our', 'us', 'help'), achievement (e.g., 'work', 'better', 'best'), and power (e.g., 'own', 'order', 'allow').
    
    \item  Social: The Social category stands for social processes, and encompasses words pertain to various types of social behavior ('love', 'care', 'please', 'good morning', 'attack', 'deserve', 'judge') and social referents as defined above.

    \item achieve: Achievement words denote actions or concepts related to accomplishment or success, such as 'work', 'bonus', 'beat', and 'overcome'.

    \item inhib: The inhibition category refers to words related to restraint, suppression, and inhibition, such as 'block' and 'constrain'.

     \item  religion: Religion words pertain to religious concepts, practices, or institutions, such as 'God', 'hell', and 'church'.
    
\end{itemize}

\subsection{NRC Emotion Lexicon}

We use the dictionary from \citet{Mohammad13,mohammad-turney-2010-emotions} with the following word categories:

\begin{itemize}
    \item nrc.positive: Words that have a positive sentiment, such as 'acceptable', 'boon', and 'civil'.

    \item nrc.negative: Words that have a negative sentiment, such as 'aberrant', 'abort', and 'begging'.

    \item nrc.anger: Words that relate to the emotion of anger, such as 'arguments', 'confront', and 'friction'.

    \item nrc.disgust: Words that relate to the emotion of disgust, such as 'barf', 'decompose', and 'gut'.

    \item nrc.sadness: Words that relate to the emotion of sadness, such as 'blue', 'cloudy', and 'emptiness'.

    \item nrc.anticipation: Words that relate to the emotion of anticipation, such as 'accelerate', 'announcement', and 'approaching'.

    \item nrc.trust: Words that relate to the emotion of trust, such as 'adhering', 'advice', and 'collaborator'.

    \item nrc.joy: Words that relate to the emotion of joy, such as 'whimsical', 'beach', and 'doll'.
\end{itemize}

\subsection{Distress and Empathy Lexicon}

We use the dictionary from \citet{sedoc2019learning} with the following word categories:

\begin{itemize}
    \item empathy.low: Low-empathy words from the empathy lexicon (based on a median split from lexicon weights), such as 'joke', 'bizarre', and 'stupidest'.

    \item empathy.high: High-empathy words from the empathy lexicon (based on a median split from lexicon weights), such as 'healing', 'grieve', and 'heartbreaking'. This category did not significantly correlate with any dimension of dispositional empathy.

    \item distress.low: Low-distress words from the distress lexicon (based on a median split from lexicon weights), such as 'dunno', 'guessing', and 'anyhow'.

    \item distress.high: High-distress words from the distress lexicon (based on a median split from lexicon weights), such as 'inhumane', 'dehumanizes', and 'mistreating'. This category did not significantly correlate with any dimension of dispositional empathy.
\end{itemize}

\subsection{Moral Foundations Dictionary 2 (MFD2)}

We use the dictionary from \citet{frimer2019moral} with the following word categories:

\begin{itemize}
    \item mfd.authority.virtue: Words related to the "virtue" dimension of the moral foundation of Authority, such as 'respect', 'obey', and 'honor'.

    \item mfd.authority.vice: Words related to the "vice" dimension of the moral foundation of Authority, such as 'disrespect', 'disobey', and 'chaos'.

    \item mfd.care.virtue: Words related to the "virtue" dimension of the moral foundation of Care, such as 'compassion', 'generosity', and 'pity'.

    \item mfd.care.vice: Words related to the "vice" dimension of the moral foundation of Care, such as 'harm', 'threatens', and 'injured'. This category did not significantly correlate with the Care foundation.

    \item mfd.purity.virtue: Words related to the "virtue" dimension of the moral foundation of Purity, such as 'sacred', 'wholesome', and 'divine'.

    \item mfd.purity.vice: Words related to the "vice" dimension of the moral foundation of Purity, such as 'sin', 'defiled', and 'contaminate'.

    \item mfd.loyalty.virtue: Words related to the "virtue" dimension of the moral foundation of Loyalty, such as 'loyalty', 'allegiance', and 'follower'. This category did not significantly correlate with the Loyalty foundation.

    \item mfd.loyalty.vice: Words related to the "vice" dimension of the moral foundation of Loyalty, such as 'disloyal', 'treason', and 'enemy'. This category did not significantly correlate with the Loyalty foundation.

    \item mfd.fairness.virtue: Words related to the "virtue" dimension of the moral foundation of Fairness, such as 'fairness', 'justice', and 'equality'. This category did not significantly correlate with the Fairness foundation.

    \item mfd.fairness.vice: Words related to the "vice" dimension of the moral foundation of Fairness, such as 'cheat', 'unjust', and 'unequal'. This category did not significantly correlate with the Fairness foundation.
\end{itemize}

\section{Semantic Similarities}
\label{semantic-similarity}

Figures provided in Study 2 depict the semantic similarity between the original texts and the LLM-rewritten texts when the rephrase prompt (R) is used with GPT3.5. We saw similar trends using other prompts as well as other LLMs. The semantic similarity of the original and LLM-rewritten texts, categorized by the utilized prompt and LLM are presented in \Cref{fig:results-for-semantic-similarity-diff-prompts} and \Cref{fig:results-for-semantic-similarity-diff-llms}, respectively. 



A Kruskal-Wallis (H) test, a non-parametric alternative to ANOVA, was performed to compare similarity scores across LLMs and datasets, while a Mann–Whitney (U) test was used to compare the similarities across two prompts. Our results indicated that computed similarities were significantly different across LLMs ($H = 3138$, $p < .001, \eta^2=0.08$ [moderate effect size]), two prompt conditions ($U = 247042398$, $p<.001, r = 0.32$ [medium effect size]), as well as datasets ($H = 2289$, $p < .001, \eta^2=0.06$ [moderate effect size]).

Furthermore, the quantitative analysis of the pairwise comparison between different prompts, LLMs, and different datasets, based on the semantic similarity between original and LLM-generated texts, are demonstrated in \Cref{tab:pairwise_sim_prompt}, \Cref{tab:pairwise_sim}, and \Cref{tab:pairwise_sim_datasets}, respectively.

Pairwise post-hoc Dunn's tests \citep{Dunn}\footnote{Dunn's tests are conventionally used for pairwise comparisons after a Kruskal-Wallis test is rejected.} with Benjamini-Hochberg corrections for multiple comparisons showed significant differences between all LLM pairs ($p < .001$; see \Cref{tab:pairwise_sim}), with GPT3.5 being the most preservative LLM and Llama 3 being the least preservative LLM in terms of the level of semantic preservation when rewriting. 

With a similar post-hoc analysis, comparing the two prompt conditions, the Syntax\_Grammar prompt was more preservative than the Rephrase prompt with median similarity values of 0.97 and 0.96, respectively (see \Cref{tab:pairwise_sim_prompt}). Finally, underscoring the role of context when studying LLMs' impact on authors' texts, our pairwise post-hoc tests showed significant differences between all dataset pairs aside from YourMorals vs. Empathetic Conversations and Patch News vs. Empathetic Conversations (see \Cref{tab:pairwise_sim_datasets}), which showed the highest level of semantic preservation for the ArXiv dataset and the least level of preservation for the Patch news dataset. 

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/similarities_different_prompts.png}
    \caption{Semantic similarity between original and LLM-generated texts across different data sources and prompts.}
    \label{fig:results-for-semantic-similarity-diff-prompts}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/similarities_different_llms.png}
    \caption{Semantic similarity between original and LLM-generated texts across different data sources and LLMs.}
    \label{fig:results-for-semantic-similarity-diff-llms}
\end{figure*}

\input{tables/pairwise_sim_prompt}

\input{tables/pairwise_sim}

\input{tables/pairwise_sim_datasets}

\section{Lexical Shifts}
\label{lexical-shifts}

Study 4 revealed that many well-established associations between lexical categories and personal traits diminished in LLM-rewritten texts. Moreover, we observed significant shifts in both the frequency and diversity of lexical categories often linked to personal traits, such as pronouns \citep{pennebaker2011secret}. \Cref{fig:general-differences-in-lexical-categories-gpt-rephrase} illustrates these differences for GPT-3.5 outputs with the Rephrase prompt.

Notably, LLM-rewritten texts exhibited a sharp decline in the use of pronouns, swear words, death-related terms, and conversational markers (e.g., “yeah,” “oh,” “um”). At the same time, they showed an increase in words reflecting positive emotions, sadness, and socially oriented language. This pattern suggests that LLMs tend to reduce informal, conversational, and negatively charged language while amplifying structured, emotionally positive, and socially engaging expressions—potentially reflecting biases introduced during training.

Beyond frequency shifts, we observed a systematic decline in lexical diversity across multiple word categories, further supporting the homogenization effects identified in Studies 1 and 2. Categories such as pronouns, non-fluent words, and conversational speech markers exhibited reduced variability. As shown in \Cref{tab:lexical-shifts-diversity-appendix}, which presents variance in usage distributions normalized via min-max scaling across original and LLM-generated documents, for each lexical category and data source, the majority of cases displayed a decrease in diversity (indicated by the ↓ symbol). This reduced lexical variety may diminish the predictive power of linguistic markers for personal traits, as discussed in Studies 3 and 4.

Similar trends emerged across different models and prompting strategies. \Cref{fig:general-differences-in-lexical-categories-gpt-sg} shows the lexical shifts induced by the syntax\_grammar prompt on GPT-3.5, while \Cref{fig:general-differences-in-lexical-categories-gemini-rephrase} and \Cref{fig:general-differences-in-lexical-categories-gemini-sg} demonstrate similar effects for Gemini. Likewise, \Cref{fig:general-differences-in-lexical-categories-llama-rephrase} and \Cref{fig:general-differences-in-lexical-categories-llama-sg} show comparable shifts in Llama 3.

These trends may stem from model alignment during reinforcement learning from human feedback (RLHF; \citealp{bai2022training}). While RLHF is designed to encourage helpful and less harmful language \citep{bai2022training}, our findings suggest that its effects extend beyond content moderation. The observed reduction in pronouns and conversational markers—features that are neither harmful nor undesirable—raises the possibility that RLHF inadvertently suppresses stylistic diversity, potentially diminishing the linguistic signals that contribute to individual expression and identity.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/general_categories_differences_gpt_rephrase.png}
    
    \caption{Distribution of usage of different lexical word categories in the original and LLM-rewritten texts (with rephrase prompt on GPT3.5) across different data sources. Items with a significant difference (adjusted $p$-value $<$ 0.05) between original and LLM-rewritten distributions according to a Mann-Whitney U test are indicated with a \textbf{*}, while the items in which no more than 10\% of the LLM-rewritten texts remain to contain the specified lexical feature are indicated using a \textbf{\^}. Furthermore, items where variance in features is significantly less in LLM-generated text compared to original documents using a Levene's test are denoted by $\downarrow\sigma$.}
    \label{fig:general-differences-in-lexical-categories-gpt-rephrase}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/general_categories_differences_gpt_syntax_grammar.png}
    
    \caption{Distribution of usage of different lexical word categories in the original and LLM-rewritten texts (with syntax\_grammar prompt on GPT3.5) across different data sources. Items with a significant difference (adjusted $p$-value $<$ 0.05) between original and LLM-rewritten distributions according to a Mann-Whitney U test are indicated with a \textbf{*}, while the items in which no more than 10\% of the LLM-rewritten texts remain to contain the specified lexical feature are indicated using a \textbf{\^}. Furthermore, items where variance in features is significantly less in LLM-generated text compared to original documents using a Levene's test are denoted by $\downarrow\sigma$.}
    \label{fig:general-differences-in-lexical-categories-gpt-sg}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/general_categories_differences_gemini_rephrase.png}
    
    \caption{Distribution of usage of different lexical word categories in the original and LLM-rewritten texts (with Rephrase prompt on Gemini) across different data sources. Items with a significant difference (adjusted $p$-value $<$ 0.05) between original and LLM-rewritten distributions according to a Mann-Whitney U test are indicated with a \textbf{*}, while the items in which no more than 10\% of the LLM-rewritten texts remain to contain the specified lexical feature are indicated using a \textbf{\^}. Furthermore, items where variance in features is significantly less in LLM-generated text compared to original documents using a Levene's test are denoted by $\downarrow\sigma$.}
    \label{fig:general-differences-in-lexical-categories-gemini-rephrase}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/general_categories_differences_gemini_syntax_grammar.png}
    
    \caption{Distribution of usage of different lexical word categories in the original and LLM-rewritten texts (with syntax\_grammar prompt on Gemini) across different data sources. Items with a significant difference (adjusted $p$-value $<$ 0.05) between original and LLM-rewritten distributions according to a Mann-Whitney U test are indicated with a \textbf{*}, while the items in which no more than 10\% of the LLM-rewritten texts remain to contain the specified lexical feature are indicated using a \textbf{\^}. Furthermore, items where variance in features is significantly less in LLM-generated text compared to original documents using a Levene's test are denoted by $\downarrow\sigma$.}
    \label{fig:general-differences-in-lexical-categories-gemini-sg}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/general_categories_differences_llama_rephrase.png}
    
    \caption{Distribution of usage of different lexical word categories in the original and LLM-rewritten texts (with Rephrase prompt on Llama 3) across different data sources. Items with a significant difference (adjusted $p$-value $<$ 0.05) between original and LLM-rewritten distributions according to a Mann-Whitney U test are indicated with a \textbf{*}, while the items in which no more than 10\% of the LLM-rewritten texts remain to contain the specified lexical feature are indicated using a \textbf{\^}. Furthermore, items where variance in features is significantly less in LLM-generated text compared to original documents using a Levene's test are denoted by $\downarrow\sigma$.}
    \label{fig:general-differences-in-lexical-categories-llama-rephrase}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/general_categories_differences_llama_syntax_grammar.png}
    
    \caption{Distribution of usage of different lexical word categories in the original and LLM-rewritten texts (with syntax\_grammar prompt on Llama 3) across different data sources. Items with a significant difference (adjusted $p$-value $<$ .05) between original and LLM-rewritten distributions according to a Mann-Whitney U test are indicated with a \textbf{*}, while the items in which no more than 10\% of the LLM-rewritten texts remain to contain the specified lexical feature are indicated using a \textbf{\^}. Furthermore, items where variance in features is significantly less in LLM-generated text compared to original documents using a Levene's test are denoted by $\downarrow\sigma$.}
    \label{fig:general-differences-in-lexical-categories-llama-sg}
\end{figure*}

\begin{table}[]
\captionsetup{font=huge}
\caption{Changes in the variance of various lexical word categories between original and LLM-rewritten documents across datasets, using GPT-3.5 and the rephrase prompt. Similar patterns were observed using other prompts and LLMs too. Variance values before and after rewriting are displayed on either side of $\rightarrow$. The statistics and $p$-values from Levene’s test are provided in columns labeled $F$ and $p$. $p < .05$ is marked with \textsuperscript{*} and \textsuperscript{***} denotes $p <.001$.}
\label{tab:lexical-shifts-diversity-appendix}
\centering
\resizebox{\textwidth}{!}{\begin{minipage}{\textwidth}
\begin{tabular}{llcclcclcclcc}
\toprule
\textit{Lexical Category} & \textit{Essays} & & & \textit{Empathetic C.} & & & \textit{Congress} & & & \textit{YourMorals} & & \\
& & $F$ & $p$ & & $F$ & $p$ & & $F$ & $p$ & & $F$ & $p$ \\
\midrule
pronoun & .044 $\rightarrow$ .042 & .075 & .783 & .032 $\rightarrow$ .027 & 1.06 & .304 & \textbf{.042 $\rightarrow$ .018 (↓)} & 123.077 & $< .001$ \textsuperscript{***} & \textbf{.044 $\rightarrow$ .018 (↓)} & 342.316 & $< .001$ \textsuperscript{***} \\
affect & .045 $\rightarrow$ .044 & .001 & .970 & .036 $\rightarrow$ .026 & .366 & .546 & .027 $\rightarrow$ .040 (↑) & 25.132 & $< .001$ \textsuperscript{***} & \textbf{.035 $\rightarrow$ .022 (↓)} & 96.369 & $< .001$ \textsuperscript{***} \\
emo\_pos & .050 $\rightarrow$ .049 & .004 & .949 & .073 $\rightarrow$ .051 & .330 & .567 & .022 $\rightarrow$ .057 (↑) & 139.282 & $< .001$ \textsuperscript{***} & .034 $\rightarrow$ .040 (↑) & 29.640 & $< .001$ \textsuperscript{***} \\
emo\_anger & \textbf{.057 $\rightarrow$ .043 (↓)} & 11.009 & $< .001$ \textsuperscript{***} & .060 $\rightarrow$ .008 & 2.281 & .139 & .048 $\rightarrow$ .056 & 2.966 & .085 & \textbf{.045 $\rightarrow$ .023 (↓)} & 11.990 & $< .001$ \textsuperscript{***} \\
emo\_sad & .037 $\rightarrow$ .054 (↑) & 28.547 & $< .001$ \textsuperscript{***} & .031 $\rightarrow$ .048 & 1.747 & .189 & .020 $\rightarrow$ .050 (↑) & 63.854 & $< .001$ \textsuperscript{***} & \textbf{.040 $\rightarrow$ .015 (↓)} & 191.865 & $< .001$ \textsuperscript{***} \\
swear & \textbf{.057 $\rightarrow$ .001 (↓)} & 107.645 & $< .001$ \textsuperscript{***} & .033 $\rightarrow$ .000 & 3.074 & .082 & .0000 $\rightarrow$ .006 & 1.706 & .191 & \textbf{.049 $\rightarrow$ .000 (↓)} & 283.577 & $< .001$ \textsuperscript{***} \\
social & .048 $\rightarrow$ .052 & 3.542 & .059 & .043 $\rightarrow$ .042 & .000 & .995 & .049 $\rightarrow$ .054 & 3.623 & .057 & \textbf{.042 $\rightarrow$ .021 (↓)} & 243.742 & $< .001$ \textsuperscript{***} \\
death & \textbf{.056 $\rightarrow$ .033 (↓)} & 6.872 & .008 \textsuperscript{*} & \textbf{.033 $\rightarrow$ .010 (↓)} & 4.158 & .044 \textsuperscript{*} & .045 $\rightarrow$ .044 & .028 & .866 & \textbf{.043 $\rightarrow$ .017 (↓)} & 157.110 & $< .001$ \textsuperscript{***} \\
conversation & \textbf{.050 $\rightarrow$ .020 (↓)} & 117.703 & $< .001$ \textsuperscript{***} & \textbf{.071 $\rightarrow$ .005 (↓)} & 51.776 & $< .001$ \textsuperscript{***} & .042 $\rightarrow$ .064 (↑) & 16.420 & .000 \textsuperscript{***} & \textbf{.035 $\rightarrow$ .007 (↓)} & 577.565 & $< .001$ \textsuperscript{***} \\
assent & \textbf{.0512 $\rightarrow$ .023 (↓)} & 58.888 & $< .001$ \textsuperscript{***} & \textbf{.070 $\rightarrow$ .011 (↓)} & 19.542 & $< .001$ \textsuperscript{***} & \textbf{.064 $\rightarrow$ .022 (↓)} & 11.480 & .000 \textsuperscript{***} & \textbf{.042 $\rightarrow$ .022 (↓)} & 107.830 & $< .001$ \textsuperscript{***} \\
nonflu & \textbf{.053 $\rightarrow$ .027 (↓)} & 4.319 & $< .001$ \textsuperscript{***} & .023 $\rightarrow$ .000 & 2.09 & .150 & .000 $\rightarrow$ .003 & .084 & .771 & \textbf{.039 $\rightarrow$ .014 (↓)} & 119.135 & $< .001$ \textsuperscript{***} \\
filler & \textbf{.058 $\rightarrow$ .008 (↓)} & 87.057 & $< .001$ \textsuperscript{***} & .021 $\rightarrow$ .000 & 3.136 & .079 & \textbf{.024 $\rightarrow$ .000 (↓)} & 18.442 & $< .001$ \textsuperscript{***} & \textbf{.050 $\rightarrow$ .008 (↓)} & 212.726 & $< .001$ \textsuperscript{***} \\
emo\_anx & .047 $\rightarrow$ .051 & 2.429 & .119 & .020 $\rightarrow$ .048 (↑) & 7.247 & .008 \textsuperscript{*} & .026 $\rightarrow$ .041 (↑) & 3.910 & .048 \textsuperscript{*} & \textbf{.048 $\rightarrow$ .032 (↓)} & 33.227 & $< .001$ \textsuperscript{***} \\
\bottomrule
\end{tabular}
\end{minipage}}
\end{table}


\section{Analysis of Predictive Power of Personal Traits from Text}
\label{predictive-power}


\subsection{Using LLMs as Zero-shot Classifiers}
\label{using-llms-zero-shot-classifiers}

The mean performance of trained classifiers discussed in Study 3 in comparison with a Llama3.1~\citep{dubey2024llama} as a zero-shot classifier~\citep{kojima2022large} for each personal trait prediction task is demonstrated in \Cref{tab:llm-as-classifier}. Across all prediction tasks and contexts, trained classifiers outperform Llama3.1, which was chosen as a representative powerful open-source large language model for task automation in scale. Since Study 3 required classifiers that were already proficient in predicting personal traits based on authors' original texts, we selected trained classifiers for this task. These classifiers outperformed large language models (LLMs) like Llama3.1, which reinforced our decision to rely on them. Additionally, the datasets used in this study have been previously published, making them potential sources of training data for LLMs. Given the opacity surrounding what is included in LLM training data, it is difficult to confirm whether these models had prior exposure to the test splits reserved for performance evaluation. 



\begin{table*}[h!]
\caption{Comparison between the performance of trained classifier and Llama 3.1 \citep{dubey2024llama} zero-shot capabilities on each personal trait prediction task.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccrrrrrrrl}
\toprule
  & $\text{Mean $F_1$}_{\text{trained classifiers}}$ & $\text{Mean $F_1$}_{\text{LLM as classifier}}$ & \text{Random baseline}\\
\midrule
Age group & .351 & .121 & .244\\

Empathy & .657 & .436 & .541\\

Personality & .658 & .569 & .514\\

Gender & .694 & .494 & .495\\

Morality & .664 & .405 & .521\\

Affiliation & .640 & .602 & .490\\
\bottomrule
\end{tabular}}
\label{tab:llm-as-classifier}
\end{table*}



\begin{table*}[h!]
\caption{Paired $t$-tests for testing the difference in predictive powers ($F_1$) of classifiers on the original and LLM-generated texts for different dimensions of personal traits, and Cohen's \emph{d} effect sizes for the magnitude of these differences ($|d|<0.2$: negligible, $|d|<0.5$: small, $|d|<0.8$: moderate; \citealp{cohen}).}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{llccccrrrrrrrc}
\toprule
Dataset & Dim. & $\text{Mean $F_1$}_{\text{original}}$ & $\text{Mean $F_1$}_{\text{LLM}}$ & CI & SE & t & df & p & \emph{d} & N & magnitude & Random Baseline\\
\midrule
Personality & CON & 0.647 & 0.593 & [0.045, 0.061] & 0.159 & 13.085 & 1534 & 0& 0.451 & 1535 & Small & 0.511 \\
 & NEU & 0.618 & 0.556 & [0.054, 0.071] & 0.162 & 14.727 & 1457 & 0 & 0.533 & 1458 & Moderate & 0.501 \\
 & OPN & 0.673 & 0.615 & [0.05, 0.065] & 0.164 & 14.438 & 1693 & 0 & 0.487 & 1694 & Small & 0.516 \\
 & EXT & 0.663 & 0.602 & [0.052, 0.07] & 0.161 & 14.797 & 1530 & 0 & 0.487 & 1531 & Small & 0.516 \\
 & AGR & 0.685 & 0.640 & [0.037, 0.055] & 0.170 & 10.224 & 1434 & 0 & 0.370 & 1435 & Small & 0.531 \\
Empathy & Perspective & 0.637 & 0.584 & [0.046, 0.06] & 0.138 & 18.613 & 2334 & 0 & 0.463 & 2335 & Small & 0.527 \\
 & Fantasy & 0.659 & 0.619 & [0.034, 0.047] & 0.117 & 15.727 & 2080 & 0 & 0.393 & 2081 & Small & 0.547 \\
 & Concern & 0.680 & 0.614 & [0.059, 0.073] & 0.145 & 19.966 & 1939 & 0 & 0.587 & 1940 & Moderate & 0.572 \\
 & Distress & 0.637 & 0.554 & [0.057, 0.108] & 0.164 & 7.115 & 200 & 0 & 0.640 & 201 & Moderate & 0.518 \\
Morality & Authority & 0.649 & 0.548 & [0.091, 0.112] & 0.180 & 19.581 & 1208 & 0 & 0.775 & 1209 & Moderate & 0.534 \\
 & Purity & 0.658 & 0.572 & [0.076, 0.096] & 0.177 & 17.249 & 1258 & 0 & 0.645 & 1259 & Moderate & 0.536 \\
 & Loyalty & 0.634 & 0.555 & [0.069, 0.088] & 0.181 & 16.206 & 1385 & 0 & 0.638 & 1386 & Moderate & 0.503 \\
 & Care & 0.619 & 0.648 & [-0.037, -0.021] & 0.102 & -9.900 & 1207 & 0 & 0.301 & 1208 & Small & 0.507 \\
 & Fairness & 0.635 & 0.521 & [0.079, 0.149] & 0.224 & 5.872 & 133 & 0 & 0.789 & 134 & Moderate & 0.528 \\
Affiliation &  & 0.664 & 0.591 & [0.064, 0.082] & 0.168 & 15.707 & 1314 & 0 & 0.610 & 1315 & Moderate & 0.500 \\
Gender & & 0.694 & 0.623 & [0.063, 0.08] & 0.166 & 15.965 & 1371 & 0 & 0.603 & 1372 & Moderate & 0.500 \\
Age group & & 0.351 & 0.260 & [0.079, 0.102] & 0.167 & 15.397 & 804 & 0 & 0.776 & 805 & Moderate & 0.250 \\
\bottomrule
\end{tabular}}
\label{tab:ttest_res-for-each-dimension}
\end{table*}

\subsection{Systematicity of Change in Predictive Powers}
\label{systematicity-of-change-in-predictive-powers}

In Study 3, we observed that changes in model predictions on LLM-rewritten texts were not random but systematically biased toward a specific class and this was the case across all data sources. This suggests that LLMs influence linguistic cues in a way that consistently alters how personal traits are inferred. Here, we provide an additional sensitivity check using Gemini and Llama, where we find the same systematic trends: $\Delta$ for LLM-rewritten texts remains consistently larger than for original texts across nearly all categories (see \Cref{fig:difference-between-predictions-homogeneity-gemini} and \Cref{fig:difference-between-predictions-homogeneity-llama}). This further supports our finding that LLMs homogenize text in a way that skews predictions toward particular traits, reinforcing the systematic nature of their effect on linguistic markers.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/homogeneity_of_predictions_agg_gemini.png}
    \caption{The distribution of $\Delta$ (a proxy for imbalances between the predicted class frequencies on the original and LLM-rewritten texts) across different personal traits, focusing on the LLM-rewritten texts generated by Gemini. The $W$ statistics from the Wilcoxon test are displayed on top, with $p < .05$, $p < .01$, and $p < .001$, marked with \textsuperscript{*}, \textsuperscript{**}, \textsuperscript{***}, respectively.} 
    \label{fig:difference-between-predictions-homogeneity-gemini}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/homogeneity_of_predictions_agg_llama.png}
    \caption{The distribution of $\Delta$ (a proxy for imbalances between the predicted class frequencies on the original and LLM-rewritten texts) across different personal traits, focusing on the LLM-rewritten texts generated by Llama 3. The $W$ statistics from the Wilcoxon test are displayed on top, with $p < .05$, $p < .01$, and $p < .001$, marked with \textsuperscript{*}, \textsuperscript{**}, \textsuperscript{***}, respectively.} 
    \label{fig:difference-between-predictions-homogeneity-llama}
\end{figure*}

\subsection{Additional Insights about Loss in Predictive Power of Personal Traits from Text}
\label{additional-insights-about-loss-in-predictive-power-personal-traits}

The impact of LLMs on the predictive power of linguistic patterns over authors’ personal traits across various psychological and demographic dimensions is shown in \Cref{tab:ttest_res-for-each-dimension}. In all datasets and across different dimensions, we observe a consistent reduction in predictive power when LLMs are involved as writing assistants. However, one notable exception is the Care dimension in moral values, where LLM involvement actually increases the predictive power.

This unique result may be related to how LLMs are trained with reinforcement learning from human feedback (RLHF), where they are optimized to be more helpful, cooperative, and less harmful. Given that the Care dimension is closely associated with helpfulness, being less harmful, and prosocial behaviors, the alignment with RLHF objectives might explain why LLM-generated texts preserve or even enhance linguistic patterns tied to this trait. LLMs may inherently promote language that reflects values aligned with Care as part of their effort to avoid harmful language and foster positive, helpful communication. 

In the main body of the paper, we demonstrated how LLMs decrease the predictive power of text over authors' personal traits, aggregating over all different LLMs, prompts, classifiers, and featurization techniques. We saw similar trends using different LLMs, prompts, classifiers, and featurization techniques that are shown in \Cref{fig:results-for-differences-between-llms}, \Cref{fig:results-for-differences-between-prompts}, \Cref{fig:results-for-differences-between-classifiers}, and \Cref{fig:results-for-differences-between-features}, respectively. 

Although using different LLMs, prompts, classifiers, or featurization techniques did not change the obtained results regarding the reduction of predictive power over personal traits when LLMs are involved as writing assistants, we observed significant differences between different choices of LLMs, prompts, classifiers, and featurization techniques. 

Considering $\Delta = \frac{F_1 (original) - F_1 (LLM-rewritten)}{F_1 (original)}$ as the statistic of interest, where $F_1 (original)$ is the $F_1$ score obtained by the trained classifier on the unseen test set, and $F_1 (LLM-rewritten)$ is the $F_1$ score obtained by the same classifier on the LLM-rewritten version of the texts, we performed Kruskal-Wallis (for comparisons with more than two categories) and Mann–Whitney (for comparisons with two categories) tests to compare the drops in the predictive powers across personal traits, LLMs, prompts, classifiers, and featurization techniques.

Our results indicated that computed similarities were significantly different across personal traits ($H = 41.91$, $p < .001, \eta^2=0.76$ [Large effect size]), LLMs ($H = 6.71$, $p < 0.03, \eta^2=0.051$ [small effect size]), two prompt conditions ($U = 3780$, $p = .005, r = 0.23$ [small effect size]), utilized classifiers ($H = 12.49$, $p = 0.014, \eta^2=0.13$ [Moderate effect size]), as well as the utilized featurization technique in classifiers ($H = 12.30$, $p = 0.002, \eta^2=0.08$ [Moderate effect size]).

Pairwise post-hoc Dunn's tests with Benjamini-Hochberg corrections for multiple comparisons showed that the drops in the predictive powers when predicting age groups were significantly more than all other personal traits ($p < .001$ for all other personal traits); Llama 3 caused more drops in the predictive powers compared to Gemini ($p = 0.076$) and GPT3.5 ($p = 0.072$); SVM and Random Forest classifiers experienced less drops in the predictive power compared to Longformer classifier (differences were not significant—$p$-value $< 0.05$—after the applied corrections), while the drops comparing other classifiers were not significantly different. Finally, in terms of the featurization technique utilized in the trained classifiers, classifiers based on OpenAI embeddings experienced less drops in the predictive power compared to both TF-IDF ($p = 0.015$) and Longformer embeddings ($p = 0.021$).



\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/difference_between_llms.png}
    \caption{Predictive power (mean $F_1$) of text over personal traits in original and LLM-rewritten texts across different LLMs and data sources.}
    \label{fig:results-for-differences-between-llms}
\end{figure*}



\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/difference_between_prompts.png}
    \caption{Predictive power (mean $F_1$) of text over personal traits in original and LLM-rewritten texts across different prompts and data sources. SG represents the Syntax\_grammar, and R represents the Rephrase prompts.}
    \label{fig:results-for-differences-between-prompts}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/difference_between_classifiers.png}
    \caption{Predictive power (mean $F_1$) of text over personal traits in original and LLM-rewritten texts across different classifiers and data sources.}
    \label{fig:results-for-differences-between-classifiers}
\end{figure*}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/difference_between_features.png}
    \caption{Predictive power (mean $F_1$) of text over personal traits in original and LLM-rewritten texts across different featurization techniques and data sources.}
    \label{fig:results-for-differences-between-features}
\end{figure*}



% Comparing the classifiers' performance on original and LLM-generated data across different experimental setup, our results suggested that the choice of LLM, prompt, and the used classifier also plays a role in how much LLMs affect the predictive power of linguistic markers of personal traits. 

% The impact of LLMs on the predictive power of linguistic patterns over authors' personal traits, focusing on the specific utilized classifier and featurization technique, is shown in \Cref{fig:difference-between-classifiers} and \Cref{fig:difference-between-featurizations}, respectively. We observed a significant difference between the effects of LLMs on different classifier families (Cochran's Q test with $\chi^2 (df = 4) = 20.3$, $p < .001$). We found that the level of preserved predictive power by the Logistic Regression classifier was lower than the Gradient Boosting classifier ($\text{McNemar's}\;\chi^2 = 10.4, p < .001, \text{Odds ratio} = 0.39$ [medium effect size]), and the SVM classifier ($\text{McNemar's}\;\chi^2 = 8.5, p = .003, \text{Odds ratio} = 0.45$ [medium effect size]). We did not observe a significant difference between the effect of LLMs on other classifiers (all analyses took the multiple comparisons into account and focused on adjusted p-values). We did not observe a significant difference between the effect of LLMs on predictive powers when different featurization techniques are used (Cochran's Q test with $\chi^2 (df = 2) = 1.39$, $p = 0.23$). These observations illustrate that except for older families of classifiers (e.g., Logistic Regression) that are more sensitive to LLMs' rewrites across all conditions, the effect of LLMs on the accuracy of other classifiers or featurization techniques depends on the experimental conditions (e.g., the personal trait of interest or the utilized LLM) and is unreasonable to state that one classifier or featurization technique (e.g., Transformer-based classifiers like Longformer) would be less (or more) prone to LLMs' impact compared to other classifiers or featurization techniques (e.g., Random Forest or TF-IDF), without necessary experiments. This is indeed why we included various families of classifiers in our experimental setup to paint an accurate picture of LLMs' impact on text-based personal trait classifiers, as one classifier or featurization technique cannot be representative enough to answer the research question of this paper accurately and faithfully.

% LLMs having different training pipelines, the impact of different LLMs on the predictive power of linguistic patterns over author attributes might differ. \Cref{fig:difference-between-llms} demonstrates the ratio of classifiers with unchanged predictive power across different LLMs. Although jointly across all author attributes, we did not find a significant difference in impact between LLMs, when we only focused on the demographic attributes (i.e., gender, age, and political affiliation), the difference between LLMs' impact was significant (Cochran's Q test with $\chi^2 = 12.6$, $p = .001$). We found that the level of preserved predictive power by Gemini was lower than GPT3.5 ($\text{McNemar's}\;\chi^2 = 9.0, p = 0.01, \text{Odds ratio} = 0.14$ [large effect size]) and Llama 3 ($\chi^2 = 8.89, p = 0.01; \text{Odds ratio} = 0.18$ [large effect size]), while the impact of Llama 3 and GPT3.5 was not significantly different.

% Moreover, our results suggested that SG prompt is more preservative than the R prompt ($\text{McNemar's}\;\chi^2 = 3.76, p = 0.05, \text{Odds ratio} = 0.58$ [small effect size]) in keeping the same predictive power of linguistic patterns over author attributes, which underscores the role of prompts in the LLMs' impact on linguistic markers of personal traits (see \Cref{fig:overall-results-for-changes-in-predictive-power-per-prompt}), with stronger effects in the case of gender, age group, and political affiliation prediction. 

\subsection{What Author Attributes Do LLMs Promote?}
\label{What-Author-Attributes-do-LLMs-Promote-the-Most?}



\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/difference_in_predicted_labels_agg_for_appendix.png}
\caption{Percentage of original texts with correct author attribute predictions that changed after LLM rewriting, grouped by the direction of change in predictions, using different LLMs.}
\label{fig:difference-in-predictions-with-all-llms}
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/difference_in_predicted_labels_agg_for_appendix_different_prompts.png}
\caption{Percentage of original texts with correct author attribute predictions that changed after LLM rewriting, grouped by the direction of change in predictions, using different prompts. \textbf{SG} and \textbf{R} stand for Syntax\_Grammar and Rephrase, respectively.}
\label{fig:difference-in-predictions-with-all-prompts}
\end{figure}



In Study 3, utilizing the direction of prediction changes, we iterated on the characteristics that LLMs promote in their own version of authors' texts. We found our results to be consistent across different LLMs (see \Cref{fig:difference-in-predictions-with-all-llms}) and prompts (see \Cref{fig:difference-in-predictions-with-all-prompts}). 

In this section, we tried to provide a more fine-grained analysis for the investigated psychological constructs to contextualize our findings better with respect to the dimensions of each construct. In the case of specific characteristics related to personality, we observed that LLM-generated text is associated with people with higher levels of openness ($t(1074) = 6.50, p < .001, d = 0.21$ [small effect size]), agreeableness ($t(1005) = 9.05, p < .001, d = 0.27$ [small effect size]), and lower levels of extraversion ($t(1054) = 15.14, p < .001, d = 0.52$ [medium effect size]) compared to the actual authors. 

In the case of specific characteristics related to dispositional empathy, we observed that LLM-generated text is associated with people with higher levels of personal distress ($t(319) = 3.90, p < .001, d = 0.36$ [small effect size]), and lower levels of empathetic concern ($t(1945) = 10.6, p < .001, d = 0.35$ [small effect size]), perspective-taking ($t(2341) = 14.17, p < .001, d = 0.42$ [small effect size]), and fantasy ($t(2090) = 14.00, p < .001, d = 0.42$ [small effect size]) compared to the actual authors. 

Finally, in the case of specific characteristics related to morality, we observed that LLM-generated text is associated with people with higher levels of fairness ($t(158) = 4.34, p < .001, d = 0.53$ [medium effect size]), care ($t(734) = 25.44, p < .001, d = 1.54$ [large effect size]), purity ($t(711) = 9.90, p < .001, d = 0.59$ [medium effect size]), and loyalty ($t(752) = 5.51, p < .001, d = 0.29$ [small effect size]) compared to the actual authors. 


\section{Top-down Analyses}
\label{top-down-analysis}

\Cref{tab:complete-table-personality} demonstrates the associations between lexical cues in authors' texts and their personalities. Using the original texts, we replicated previously known associations in the literature \citep[e.g.,][]{baddeley_singer,hirsh2009personality,tackman}. Namely, higher OPN was significantly associated with more complex language usage (BigWords) and swear words, higher CON was significantly associated with fewer swear and negative emotion words, higher EXT was significantly associated with greater use of positive emotion and social words, higher AGR was significantly associated with fewer swear and negative emotion words, and higher NEU was significantly associated with greater use of negative emotion words and pronouns.
% Most of these hypothesized associations were present in the expected directions either in the original text (e.g., O and BigWords; C and swear words; E and joy-related words; A and anger emotion words; N and I pronouns), or emerged after involving LLMs in the writing (e.g., E, N and affect-related words). 
% Some associations that were not hypothesized emerged in the original texts (e.g., O and 'i') or in the LLM rewrites (e.g., O and 'Drives').
% In general, expected associations appear to mostly have been preserved with the involvement of GPT3.5 and Gemini, while Llama 3 appears to have preserved fewer of these associations. 
Certain associations present in the original texts were retained regardless of the LLM involved or prompt used (e.g., EXT and affiliation-related words; AGR and anger emotion words and NEU and negative emotion words). Other associations disappeared, regardless of the specific LLM or prompt (OPN and BigWords, and NEU and anger emotion words). 
Overall, these results suggest that the fine-grained lexical cues that allow for personality detection using theoretically grounded approaches might not be reliable when LLMs are involved in the writing process, and might be somewhat dependent on which LLM users choose to utilize and the personality dimension of interest. 


\input{tables/personality_corr}


%either in the original text (e.g., Openness to Experience and 'swear', 'BigWords'; Conscientiousness and 'emo\_anger', 'swear'; Extraversion and 'affiliation', 'Social', 'emo\_pos'; Agreeableness and 'swear', 'emo\_anger'; Neuroticism and 'i', 'pronoun', 'emotion', 'emo\_neg', 'emo\_anger', 'emo\_sad'), or emerged after LLM rewrite (e.g., Extraversion and 'Affect', 'emotion'; Neuroticism and 'Affect'). Some associations that were not hypothesized emerged in the original texts (e.g., Openness to Experience and 'i') or in the LLM rewrites (e.g., Openness to Experience and 'affiliation', 'achieve', and 'Drives'; Extraversion and 'Drives', and Agreeableness and 'affiliation').
%As expected, openness to experience was significantly positively correlated with complex language use, conscientiousness was negatively associated with anger and swear words, extraversion was positively associated with social words ('affilitation', 'Social') and positive emotion, agreeableness was positively associated with 'affiliation' and negatively with anger and swear words, and finally, neuroticism was positively associated with the use of 'i' and pronouns as well as various categories of negative emotion (e.g., anger, sadness).

% \input{tables/checkmark_morality}

\input{tables/morality_corr}


\Cref{tab:complete-table-morality} demonstrates the associations between lexical cues in authors' texts and their moral values. In line with evolutionary accounts of morality linking the development of moral values to the necessity of cooperation and interdependence \citep{LiTomasello}, we found expected significant associations between MFT dimensions and social and affiliation-related words (e.g., between Loyalty and family-related words), and between foundations such as Purity and Authority and word categories such as religion-related words. 
% In general, most of the hypothesized associations across all foundations were preserved with LLM involvement and were even amplified for the Care foundation, specifically using Gemini and GPT3.5. 
% Some hypothesized correlations only became significant after LLM rewrites (e.g., Loyalty and 'prosocial'; Care and 'Social', 'socrefs', 'family' and 'friend', primarily in the GPT and Gemini rewrites).
Some associations present in the original texts were washed away after LLM involvement, primarily with Llama 3 (e.g., between Purity and family-related words, Authority and religion-related words). These results are aligned with our observations in \Cref{additional-insights-about-loss-in-predictive-power-personal-traits}, underscoring Gemini and GPT3.5 as more preservative of lexical cues predictive of authors' moral values, than Llama 3.
% Focusing on the effect of LLMs on associations, the significant correlations between LIWC categories such as 'religion' and both Authority and Purity were retained in most conditions. 
% \input{tables/checkmark_empathy}

\input{tables/empathy_corr}


\Cref{tab:complete-table-empathy} demonstrates the associations between lexical cues in authors' texts and authors' dispositional empathy. For different dimensions of dispositional empathy, we expected and found significant associations with the use of pronouns, emotion words, and words related to cognitive processes (i.e., cogproc), as well as with words from the empathy and distress lexicons. Hypothesized associations were present in the original text for all subdimensions of IRI, e.g., between PD and affect-related words, EC and pronouns, as well as tentative-related words (tentat), PT and pronouns, and FS and affect. The significant negative association of IRI dimensions PT and EC with first-person plural words (we) was the only association that retained its significance across almost all LLM rewrite conditions, with all other associations becoming unreliable.
% Across all LLMs, Llama 3 preserved more associations between dispositional empathy and people's language than did Gemini or GPT3.5.
% No categories significantly correlated with the FS dimension in the original or in the LLM-generated text.
%When texts were concatenated by the author, we found no significant correlations in any IRI facet or LLM-rewrite condition. 

\input{tables/political_corr}

\input{tables/political_gender}

\input{tables/political_age}


The associations between lexical cues in authors' texts and their demographic variables are demonstrated in \Cref{tab:correlation_pol}, \Cref{tab:correlation_gender}, and \Cref{tab:complete-table-age}. We found several expected significant differences in the usage of lexical cues across political affiliations (we did not find hypothesized differences across political affiliation in language use related to inhibition and reaction to threats, e.g., negative emotion words; \citealp{okdie}) and genders. Namely, we found that Republicans used a significantly greater number of adverbs than Democrats, and males used a significantly greater number of articles and a significantly smaller number of social and anxiety-related words than females \citep{ishikawa2015gender}. We also found associations between first-person plural words and words related to cognitive processes (e.g., but, not, if, or, know) with age as well as words related to focusing on the future with this construct. 

% We note that all correlations observed (including the significant correlations) were weak ($\leq 0.22$), and the largest magnitude correlations across all datasets and conditions were observed for Empathy between Perspective Taking and 'we' and 'pronoun' categories and Empathetic Concern and 'we' ($r = -0.22$, which represents a weak correlation).
After the LLM rewrites, some previously expected associations for political affiliation retained significance, while others were washed away. The linguistic cues for gender were generally preserved for article and social word usage, but only GPT-3.5 using a Syntax\_Grammar prompt, preserved the linguistic cues related to negative emotions. After the LLMs' involvement in rewriting the texts, although the associations related to first-person plural words and words related to cognitive processes with age remained preserved, the associations with the words related to focusing on the future were washed away.



\end{appendices}


\end{document}
