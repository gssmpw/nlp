\begin{table*}[ht]
\caption{Pairwise comparisons between different LLMs, based on the semantic similarity computed between the original and LLM-generated texts in each dataset, with Dunn's test and Benjamini-Hochberg adjustments for multiple comparisons.}
\centering
% \resizebox{.7\textwidth}{!}{%
\begin{tabular}{lllrr}
\toprule
Dataset & LLM 1 & LLM 2 & $z$ & Adjusted $p$-value \\\hline
 & Gemini & GPT3.5 & 40.610 & $<$.001\\
Essays & Gemini & Llama 3 & -5.614 & $<$.001\\
 & GPT3.5 & Llama 3 & -47.728 & $<$.001\\
\hline
 & Gemini & GPT3.5 & 0.123 & .902 \\
YourMorals & Gemini & Llama 3 & -41.870 & $<$.001\\
 & GPT3.5 & Llama 3 & -41.955 & $<$.001\\
\hline
 & Gemini & GPT3.5 & 8.915 & $<$.001\\
Congress & Gemini & Llama 3 & -4.506 & $<$.001\\
 & GPT3.5 & Llama 3 & -13.421 & $<$.001\\
\hline
 & Gemini & GPT3.5 & 2.637 & .008 \\
Empathetic C. & Gemini & Llama 3 & -6.883 & $<$.001\\
& GPT3.5 & Llama 3 & -9.597 & $<$.001\\
\hline
& Gemini & GPT3.5 & 8.462 & $<$.001 \\
ArXiv & Gemini & Llama 3 & -13.889 & $<$.001\\
& GPT3.5 & Llama 3 & -22.346 & $<$.001\\
\hline
& Gemini & GPT3.5 & 8.095 & $<$.001 \\
Reddit & Gemini & Llama 3 & -3.086 & .002\\
& GPT3.5 & Llama 3 & -11.177 & $<$.001\\
\hline
& Gemini & GPT3.5 & 8.427 & $<$.001 \\
Patch news & Gemini & Llama 3 & -2.632 & .008\\
& GPT3.5 & Llama 3 & -11.059 & $<$.001\\
\bottomrule
\end{tabular}
\label{tab:pairwise_sim}
\end{table*}
