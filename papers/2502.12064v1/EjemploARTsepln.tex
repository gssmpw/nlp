\documentclass[a4paper,11pt,twocolumn,twoside]{article}
\usepackage{graphicx}
\usepackage{sepln_en}
\usepackage{fullname}
\usepackage[utf8]{inputenc}
% \usepackage[spanish,es-nosectiondot, es-tabla, es-noindentfirst, es-nolists]{babel}
\usepackage[english]{babel}
% \usepackage{hyperref}
\usepackage{multirow}
\usepackage{booktabs}

\input epsf

\setlength\titlebox{4in} %esto por defecto


\title{AI-generated Text Detection with a GLTR-based Approach}

\author {\textbf{Lucía Yan Wu,} \textbf{Isabel Segura-Bedmar}\\
Universidad Carlos III de Madrid, Leganés, Madrid\\
lyluciayan@gmail.com, isegura@inf.uc3m.es\\
}

\seplntranstitle{Detección de Textos Generados por IA con un enfoque basado en GLTR}

\seplnkey{Machine-Generated Text, GPT-2, GLTR, Text Classification.}

\seplnabstract{The rise of LLMs (Large Language Models) has contributed to the improved performance and development of cutting-edge NLP applications.
However, these can also pose risks when used maliciously, such as spreading fake news, harmful content, impersonating individuals, or facilitating school plagiarism, among others. This is because LLMs can generate high-quality texts, which are challenging to differentiate from those written by humans. %Additionally, they can provide misinformation or violate copyright laws, i.e., the intellectual property of original authors, raising some ethical concerns.
GLTR, which stands for Giant Language Model Test Room and was developed jointly by the MIT-IBM Watson AI Lab and HarvardNLP, is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. One limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. This study aims to explore various ways to improve GLTR's effectiveness for detecting AI-generated texts within the context of the IberLef-AuTexTification 2023 shared task, in both English and Spanish languages.
Experiment results show that our GLTR-based GPT-2 model overcomes the state-of-the-art models on the English dataset with a macro F1-score of 80.19\%, except for the first ranking model (80.91\%). However, for the Spanish dataset, we obtained a macro F1-score of 66.20\%, which differs by 4.57\% compared to the top-performing model.
%TODO
}

\seplnclave{Texto generado por IA, GPT-2, GLTR, Clasificación de texto.}

\seplnresumen{El auge de los LLMs (Modelos de Lenguaje de Gran Tamaño) ha contribuido al mejor desempeño y desarrollo de aplicaciones de PLN de vanguardia. Sin embargo, estos modelos también pueden suponer riesgos cuando se utilizan de manera maliciosa, como la difusión de noticias falsas o contenido dañino, la suplantación de identidad o la facilitación del plagio escolar, entre otros. Esto se debe a que los LLMs pueden generar textos de alta calidad, los cuales son difíciles de diferenciar de aquellos escritos por humanos. 
GLTR, que significa Giant Language Model Test Room y fue desarrollado conjuntamente por el MIT-IBM Watson AI Lab y HarvardNLP, es una herramienta visual diseñada para ayudar a detectar textos generados por máquinas basados en GPT-2, que resalta las palabras en el texto dependiendo de la probabilidad de que hayan sido generadas de forma automática. Una limitación de GLTR es que los resultados que devuelve pueden, en ocasiones, ser ambiguos y llevar a confusión. Este estudio tiene como objetivo explorar diversas formas de mejorar la eficacia de GLTR para detectar textos generados por IA en el contexto de la tarea compartida IberLef-AuTexTification 2023, tanto en inglés como en español.
Los resultados muestran que nuestro modelo GPT-2 basado en GLTR obtiene una puntuación macro F1 de 80.19\% para la tarea en inglés. Nuestro enfoque supera a todos los sistemas que participaron en la tarea, excepto al primer sistema, que obtuvo una puntuación macro F1 ligeramente superior a la nuestra (80.91\%). Sin embargo, para el conjunto de datos en español, obtuvimos una puntuación macro F1 de 66.20\%, lo que representa una diferencia del 4.57\% respecto al modelo con mejor desempeño.}

\firstpageno{1}


\begin{document}

% la siguiente instrucción sólo se debe usar si el abstract sobrescribe el texto
% la longitud variará según se necesite

\setlength\titlebox{25cm} % se aumenta el tamaño del espacio reservado para datos de título


\label{firstpage} \maketitle

%\begin{abstract}
%Resumen del artículo con una sangría a izquierda y derecha de 0.32
%cm, justificado por ambos lados, con tamaño de fuente 11.
%
%\end{abstract}

\section{Introduction}

%CURRENT SITUATION
In recent years, the rapid development of AI (Artificial Intelligence) due to the release of models such as ChatGPT by OpenAI or Gemini by Google, has increased the amount of AI-generated content on a huge scale \cite{martínez2023combining}. GPT \cite{openai2024gpt4}, PaLM \cite{chowdhery2022palm}, and BLOOM \cite{workshop2023bloom} are some of the Large Language Models (LLMs) accessible to the public, applied for automatic content generation. 

LLMs are known for being able to generate high-quality texts, i.e., texts that are grammatically correct and coherent, which can make it challenging to differentiate them from those written by humans \cite{llm-risks-bublak}. Nevertheless, even though LLMs have achieved remarkable success in the field, they have also introduced some risks and ethical concerns. This is because LLMs do not understand the meaning of the content they generate, and hence can produce false or inaccurate information, leading to hallucinations \cite{huang2023survey}. In addition, bias can be introduced during the generation of content, leading to unethical content such as gender inequality \cite{unesco-llm-bias}. Furthermore, the misuse of these models by some users may contribute to the spread of fake news, spam for malicious purposes, or even enhancing academic cheating \cite{llm-risks-upwork} \cite{llm-risks-heldt}.

%AUTEXTIFICATION
For these reasons, it is crucial to develop some approaches to regulate and detect AI-generated content. To do so, the AuTexTification \cite{autextification} shared task was presented as part of the IberLEF 2023 workshop \cite{jimenez2023overview}. The task involved two subtasks: binary classification of text as human-written or AI-generated, and identification of the specific LLM that generated the text.

In this study, we introduce a new approach using the GLTR tool \cite{gehrmann-etal-2019-gltr} for tackling the first subtask in both English and Spanish. GLTR (Giant Language Model Test Room) is a visual tool designed to help detect machine-generated texts based on GPT-2, that highlights the words in text depending on the probability that they were machine-generated. However, one limitation of GLTR is that the results it returns can sometimes be ambiguous and lead to confusion. Hence, this study aims to explore various ways to improve GLTR's effectiveness for the binary classification task.

The rest of this paper is organized as follows: 
In Section 2 we report the most relevant state-of-the-art approaches for AI-generated text detection. In Section 3, we present the datasets used in this study, and in Section 4, we describe the experimental setup considered to asses the binary classification task. In Section 5, we show and discuss the results obtained from the proposed method in both languages, English and Spanish. Finally, Section 6 presents the conclusions and future work.

%--------------------------------------------------------------------------
\section{State of the art}
\subsection{Participating systems in AuTexTification 2023} \label{sec:autextification2023}

\subsubsection{English}
In the IberLef-AuTexTification shared task 2023 \cite{autextification}, 36 teams participated in Subtask 1 for English texts. Most submissions utilized transformer-based models, especially BERT-based models, while others employed generative models (such as GPT-2, Grover, and OPT), CNNs, or LSTMs. Traditional machine learning models, such as Logistic Regression and SVM, were also implemented by participants but generally performed worse compared to transformer-based models.

The top-performing team, TALN-UPF \cite{taln-upf2023bilstm}, achieved a macro F1-score of 80.91\% using a bidirectional LSTM with fine-tuned \texttt{RoBERTa} and token-level POS tagging, which helped detect grammatical and morphological errors uncommon for humans. This same team also placed second with a macro F1-score of 74.16\% using the same model without POS tagging. The third-ranked team, CIC-IPN-CsCog \cite{cic-ipn-cscog2023gpt}, used a fine-tuned GPT-2, achieving a macro F1-score of 74.13\%.

Traditional machine learning models achieved lower scores; for instance, Lingüística\_UCM's \cite{linguistica-uc3m2023linearsvc} LinearSVC with TF-IDF and n-gram features reached 68.33\%, while other models like CatBoost and Multilayer Perceptron performed below 60\%. 

Overall, transformer and neural network models outperformed traditional machine learning approaches, with BiLSTM models leading in effectiveness, followed by GPT-2 and other ensemble transformer-based models.

\subsubsection{Spanish}
In Subtask 1 for Spanish texts, a total of 23 teams participated. Similarly to the English task, most submissions were based on transformer models, obtaining better results compared to other teams that used more traditional approaches. Moreover, the macro F1-scores obtained were lower for all of the models compared to their performance in the English task. This could be attributed to the limited number of models trained on data in the Spanish language.

The best-performing team, as well as in the English task, was TALN-UPF \cite{taln-upf2023bilstm}, with their BiLSTM model with fine-tuned \texttt{RoBERTa} and token-level POS tagging, obtaining a macro F1-score of 70.77\%. Lingüística\_UCM \cite{linguistica-uc3m2023linearsvc} achieved second place with their LinearSVC with TF-IDF and n-gram features with a score of 70.60\%.


Overall, we can support that indeed solutions based on transformer models and neural network algorithms give the best performance, overbeating all of the proposed baselines in the task.
On the other hand, traditional Machine Learning models such as SVM or CatBoost classifier tend to return lower scores.

\subsection{IberAuTexTification 2024}
In 2024, IberLef-IberAuTexTification shared task \cite{PLN6628}, the second version of the AuTexTification at IberLEF 2023 shared task, was released. It was based on the same subtasks as the 2023 edition, but focused on languages from the Iberian Peninsula: Spanish, Catalan, Basque, Galician, Portuguese, and English (in Gibraltar). Additionally, new domains (news, reviews, emails, essays, dialogues, wikipedia, wikihow, tweets, etc.), and models (GPT, LLaMA, Mistral, Cohere, Anthropic, MPT, Falcon, etc.) were introduced for the generation of automatically generated texts.

The top-performing systems for both subtasks were Transformer models enhanced with additional lexical, syntactic, and semantic features.
The best-performing team, jor\_isa\_uc3m \cite{jor_isa_uc3m}, proposed an ensemble composed by three multilingual transformers, \texttt{DistilBERT-base-multilingual-cased}, \texttt{mDeBERTa-v3-base}, and \texttt{XLM-RoBERTa-base}, obtaining a macro F1-score of 80.5\%.

\subsection{Other approaches to detect AI-generated texts}
In April 2024, Fareed Khan published a web application\footnote{https://ai-text-detect-easy.streamlit.app/} that identifies the 100 most common words used by AI within an input text. The objective of this application was to detect if the text was generated by a machine just by looking at it.

GPTZero\footnote{https://gptzero.me/}\cite{tian2023gptzero} launched their first AI detection solution in January 2023, and has since then developed several tools for this goal. Among them is an API available online that allows the user to input a text, and returns the probability percentage for each class: human, mixed, and AI, along with the confidence percentage. %This product is based on a 7-layered model to detect if a text has been written by an AI. Some considerations it has are the style and tone (AI text remains similar throughout the document), Internet text search (check if the text is found on the Internet), sentence-by-sentence classification, and an end-to-end Deep Learning approach trained on diverse datasets.

In 2019, researchers at Harvard and IBM \cite{gehrmann-etal-2019-gltr} developed a visual tool named GLTR (Giant Language Model Test Room), which highlighted the words in a text depending on how predictable or common these were, i.e., the probability these were generated by a machine. %The system designed computed word-by-word a list of predicted words, given the context of the sentence until that word. 
The words were colored by order of frequency, in purple, red, yellow, or green, where purple represented the rarest words with a predicted position higher than 1,000, and green the most common (within the top 10 predictable words). This tool was tested in an assessment where human subjects initially identified AI-generated texts manually with an accuracy of 54\%, which increased to over 72\% after using GLTR.

Generally, we can see that there are various ways of detecting AI-generated texts, going from manual extraction to using LLMs and other visual tools. We tested each of the approaches defined above, where Fareed Khan's app seemed inconsistent since it produced similar lists of words for both human and AI-generated texts. Next, GPTZero always returned the correct label, differentiating very well the texts. Lastly, the GLTR tool gave in general good results but required user interpretation, which sometimes led to confusion.

%--------------------------------------------------------------------------
\section{Dataset}
The dataset used for the study belongs to the AuTexTification shared task of the IberLEF 2023 Workshop \cite{autextification}. This dataset is divided by subtask and language, and by train or test. This results in eight different datasets, which collectively contain more than 160,000 texts across five domains with diverse writing styles ranging from more structured and formal to less structured and informal. For subtask 1, the training split included tweets, how-to articles, and legal documents, while the test split consisted of reviews and news articles. This setup was designed to assess the model's ability to generalize to different text domains.


%The descriptions for each of the datasets, which apply to both English and Spanish, are as follows:
%\begin{enumerate}
%    \item Subtask 1 (Human or Generated): Binary classification to differentiate human-written texts from those generated by LLMs.
%    \begin{enumerate}
%        \item Train: Tweets, how-to articles, and legal documents domains.
%        \item Test: Reviews and news domains.
%    \end{enumerate}
%    \item Subtask 2 (Model attribution): Multiclass classification to identify the model used for generating a given machine-written text. The LLMs classes for this task include three BLOOM models (\texttt{BLOOM-1B7}, \texttt{BLOOM-3B}, and \texttt{BLOOM-7B1}) \cite{bigscience2022bloom}, and three GPT-3 models (\texttt{babbage}, \texttt{curie}, and \texttt{text-davinci-003}, with 1b, 6.7b and 175b parameter scales respectively).
%    \begin{enumerate}
%        \item Train: 80\% of generated texts across all five domains.
%        \item Test: remaining 20\%.
%    \end{enumerate}
%\end{enumerate}

The source datasets used for extracting the human-authored texts can be seen in Table \ref{tab:source-human-texts} below:
\begin{table}[!h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cc}
      {\bf } &{\bf English} &{\bf Spanish}\\
      \hline
      \textbf{Legal} &\textit{MultiEURLEX} &\textit{MultiEURLEX}\\
      \textbf{News} &\textit{XSUM} &\textit{MLSUM \& XLSUM}\\ 
      \textbf{Reviews} &\textit{Amazon Reviews} &\textit{COAR \& COAH}\\   
      \textbf{Tweets} &\textit{TSATC}
      &\textit{XLM-Tweets \& TSD}\\
      \textbf{How-to} &\textit{WikiLingua} &\textit{WikiLingua}\\
    \end{tabular}
    }
    \caption{Human-authored source datasets.}
    \label{tab:source-human-texts}
\end{table}

Machine-generated texts were created from human texts by using three different BLOOM models (\texttt{BLOOM-1B7}, \texttt{BLOOM-3B}, and \texttt{BLOOM-7B1}), and three GPT-3 models (\texttt{babbage}, \texttt{curie}, and \texttt{text-davinci-003}, with 1b, 6.7b and 175b parameter scales respectively). These models were fine-tuned with a top-p of 0.9 and a temperature of 0.7.

Moreover, a maximum number of tokens was selected for each domain, maintaining the distribution of human texts: 20 tokens for tweets, 70 for reviews, and 100 for news, legal, and how-to articles. %This token length distribution can also be visualized in Figure \ref{fig:domain-length}.

Table \ref{tab:distribution}, displays the resulting dataset distribution. We observe that it is well-balanced for both domains and classes, for both languages. Moreover, Figures \ref{fig:balanced_en} and \ref{fig:balanced_es} support this statement. However, we can see that the reviews domain in the Spanish language, contains slightly fewer generated instances than human instances.
\begin{table}
  \centering
  \begin{tabular}{lllccc}
    \toprule
    & Domain & GEN & HUM & $\Sigma$ \\
    \midrule
    \multirow{5}{*}{\rotatebox[origin=c]{90}{English}} 
    % & \multirow{3}{*}{\rotatebox[origin=c]{90}{Training}}\\
    & Tweets & 5,813 & 5,884 & 11,697 \\
    & How-to & 5,862 & 5,918 & 11,780 \\
    & Legal & 5,124 & 5,244 & 10,368 \\
    \cmidrule{2-5}
    % & \multirow{2}{*}{\rotatebox[origin=c]{90}{Test}}\\
    & News & 5,464 & 5,464 & 10,928 \\
    & Reviews & 5,726 & 5,178 & 10,904 \\
    \midrule
    & Total & 27,989 & 27,688 & 55,677 \\
    \midrule
    \midrule
    \multirow{5}{*}{\rotatebox[origin=c]{90}{Spanish}} 
    % & \multirow{3}{*}{\rotatebox[origin=c]{90}{Training}}\\
    & Tweets & 5,739 & 5,634 & 11,373 \\
    & How-to & 5,690 & 5,795 & 11,485 \\
    & Legal & 4,846 & 4,358 & 9,204 \\
    \cmidrule{2-5}
    % & \multirow{2}{*}{\rotatebox[origin=c]{90}{Test}}\\
    & News & 5,514 & 5,223 & 10,737 \\
    & Reviews & 5,695 & 3,697 & 9,392 \\
    \midrule
    & Total & 27,484 & 24,707 & 52,191 \\
    \bottomrule
  \end{tabular}
  \caption{Statistics of the datasets for subtask 1.}
  \label{tab:distribution}
\end{table}

%\subsection{Visualization plots}
%We have obtained several plots to better visualize the datasets, allowing us to clearly understand the data distribution and this way create better models. 
%We focused on the datasets for Subtask 1 in both English and Spanish languages, as this work only addresses this subtask.

\begin{figure}[!h]
 \centering
 \includegraphics[width=7cm,clip]{imagenes/balanced_en.png}  
 \caption{\centering Number of samples per dataset, class, and domain (subtask 1 for English).}
 \label{fig:balanced_en}
\end{figure}

\begin{figure}[!h]
 \centering
 \includegraphics[width=7cm,clip]{imagenes/balanced_es.png}
 \caption{\centering Number of samples per dataset, class, and domain (subtask 1 for Spanish).}
 \label{fig:balanced_es}
\end{figure}

% First, we plotted the length of the texts, i.e., the number of tokens each text is composed of. In Figure \ref{fig:overall-domain-length}, we observe that in the training dataset for the English language, there is a peak that indicates that around 7,000 texts are formed by approximately 20 tokens. The remaining texts have around 60 and 100 tokens. On the other hand, most of the texts in the test dataset have lengths between 60 and 100 tokens.
% A similar distribution is followed for the Spanish texts, as seen in Figure \ref{fig:overall-domain-length_es}

% \begin{figure}[!h]
%  \centering
%  \includegraphics[width=7cm,clip]{imagenes/overall domain length density.png}  
%  \caption{\centering Length of texts in each dataset (subtask 1 for English).}
%  \label{fig:overall-domain-length}
% \end{figure}

% \begin{figure}[!h]
%  \centering
%  \includegraphics[width=7cm,clip]{imagenes/overall domain length density_es.png}  
%  \caption{\centering Length of texts in each dataset (subtask 1 for Spanish).}
%  \label{fig:overall-domain-length_es}
% \end{figure}

% Figures \ref{fig:domain-length} and \ref{fig:domain-length_es}, provide a more detailed distribution of the texts' lengths for each domain. We notice that the peak we previously mentioned in the training dataset belongs to the domain \textit{tweets}. This makes sense since tweets usually are short comments (around 20 tokens), while wiki and legal documents are more extensive (ranging from 60 to 100 tokens). 
% For the domain \textit{reviews}, the texts have a mean length of 70 and 50 tokens for the English and Spanish datasets, respectively. And for the domain \textit{news} a mean of approximately 80 tokens for both languages.
% \begin{figure}[!h]
%  \centering
%  \includegraphics[width=7cm,clip]{imagenes/domain length density.png}  
%  \caption{\centering Length of texts for each domain in each dataset (subtask 1 for English).}
%  \label{fig:domain-length}
% \end{figure}

% \begin{figure}[!h]
%  \centering
%  \includegraphics[width=7cm,clip]{imagenes/domain length density_es.png}  
%  \caption{\centering Length of texts for each domain in each dataset (subtask 1 for Spanish).}
%  \label{fig:domain-length_es}
% \end{figure}

%--------------------------------------------------------------------------
\section{Methodology}

%T0DO Explica mejor cómo es tu enfoque. Primero usas GPT-2, y luego GLTR???
In this work, we present a novel approach that combines the GLTR tool, which visualizes the likelihood of word predictions, with the predictive capabilities of GPT-2 models to classify text as either human-written or machine-generated. By leveraging GLTR's word-level probabilities and extending its functionality with a classification step, we created a robust pipeline for evaluating text authenticity. This pipeline is applied to English and Spanish texts, using multiple versions of GPT-2.

GLTR, also called Giant Language Model Test Room, is a visual tool developed by researchers at Harvard NLP and MIT-IBM Watson AI lab, which uses GPT-2 to predict the likelihood of words in a sentence \cite{DBLP:journals/corr/abs-1906-04043}. The model iterates through the different words of an input sequence, where for each word it returns a set of predicted words along with their probability of being the next word. If the real word is contained within the Top 10 predicted words, it is colored green, and the same happens for the Top 100 in yellow, the Top 1000 in red, and the rest in purple. We can better understand this by looking at the example in Figure \ref{fig:gltr-example}, where on top there is a human-written text, and below a machine-generated text. We can see that the generated text contains for the most part green-colored words, while the human-written text is composed of a variety of colored words. This is because humans tend to use more complex words and machines, more common and predictable words. Moreover, the contributors of this tool also made available a demo\footnote{http://demo.gltr.io/client/index.html} for users to test.

As we can see, GLTR is a very remarkable tool, and hence, we decided to create a model based on it. We used the same working structure of the GLTR, taking as reference the code in the GitHub repository\footnote{https://github.com/HendrikStrobelt/detecting-fake-text} that the contributors have made available. 

To construct the GLTR-based model, we first preprocessed the dataset by splitting it into training (80\% of instances) and validation (20\% of instances) subsets, and encoding the labels \textit{generated} and \textit{human} as 0 and 1, respectively. Then, each sentence was tokenized using GPT-2's tokenizer, and predictions for word likelihoods were obtained for every word. This was done by defining a function to obtain the ordered list of predicted words and their probabilities, given the previous words in the sentence. Given this list, we found the position of the ground-truth word and colored it according to the rules we explained before.
\begin{figure} 
    \centering
    \includegraphics[width=7cm]{imagenes/gltr-example.png}
    \caption{\centering GLTR example \cite{DBLP:journals/corr/abs-1906-04043}.}
    \label{fig:gltr-example}
\end{figure}

What differentiates our model from the GLTR tool is that we have incorporated an additional evaluation step to compute some performance metrics. While GLTR primarily focuses on visualizing the likelihood of words being generated by a model, we take a more data-driven approach by classifying sentences based on the proportion of green-colored words, which are those predicted to have a higher likelihood of being machine-generated.
In our approach, the proportion of green-colored words is calculated for each sentence and compared against a predefined threshold to classify the sentence. Specifically, if the percentage of green-colored words exceeds the threshold, the sentence is classified as machine-generated; if it is lower, it is classified as human-written.
For example, if the threshold is set at two-thirds, the sentence is classified as machine-generated when more than two-thirds of its words are green-colored. Otherwise, it is labelled as human-written.

In our study, we experimented with various threshold values (1/4, 1/3, 1/2, 2/3, 3/4, and 5/6) to explore the impact on classification performance across both English and Spanish languages. These variations allowed us to assess how the proportion of machine-like words influences classification accuracy and fine-tune the model's ability to distinguish between human-written and machine-generated texts.

Once the classification is made, the predicted classes are then compared to the real labels, and the macro F1-scores are computed.
%TODO

The GPT-2 model is a pretrained model on large amounts of raw and unlabelled texts in the English language, from the internet. The model was developed by OpenAI, and specifically designed to guess the next word in a sentence. During training, inputs consist of continuous text sequences, where the model learns to predict the next word in the sequence by shifting the target sequence one token to the right.
Moreover, the model uses a mask-mechanism to ensure that each prediction is based solely on previous tokens, preventing the model from using future ones. This approach enables the model to excel at generating text from prompts.
However, one limitation of GPT-2 is that the training data used for this model contains a lot of unfiltered content from the internet, which can introduce biases into the model.

For the English texts, we studied 4 different versions of the GPT-2 model \cite{radford2019language}: 
\begin{itemize}
    \item \texttt{gpt2-small},is the smallest version of GPT-2, with 124M parameters.
    
    \item \texttt{gpt2-medium}, is the 355M parameter version.
    \item \texttt{gpt2-large}, is the 774M parameter version.
    \item \texttt{gpt2-xl}, is the 1.5B parameter version.
\end{itemize}

For the Spanish texts, we additionally used the \texttt{gpt2-small-spanish} model developed by Datificate \cite{datificate}, which is based on the \texttt{gpt2-small} model \cite{radford2019language}.
It was trained on Spanish Wikipedia (around 3GB of processed training data), using Transfer Learning and Fine-tuning techniques. Similarly to the English pre-trained \texttt{gpt2-small}, one limitation of this model is the introduction of biases into the model, originated from the unfiltered content from the internet used as training data.

Note that the study with different threshold values was done on \texttt{gpt2-small} and \texttt{gpt2-small-spanish} for English and Spanish, respectively. To prevent computational complexity, \texttt{gpt2-medium}, \texttt{gpt2-large}, and \texttt{gpt2-xl}, were only evaluated on the best threshold value obtained on the \texttt{gpt2-small} and \texttt{gpt2-small-spanish} study.

The dataset as well as the code to replicate the experiments can be found in the GitHub repository \footnote{https://github.com/xxxxxxx/AI-generated-Text-Detection-with-GLTR-based-approach}. Moreover, an online demo has been deployed in Streamlit, which is available to all users and allows them to test their own English texts, returning if the input text was AI-generated or human-written \footnote{https://ai-generated-text-detection-with-gltr-based-approach.streamlit.app/}.

%--------------------------------------------------------------------------
\section{Results and discussion}
For our study, we mainly focused on the macro F1-score and the F1-scores for each of the classes, to compare the results between the different models. The reason is that the AuTexTification task on which this project is based used the F1-score as the main performance metric.

Table \ref{tab:gpt2-thresholds-eval} shows the results obtained from the experimentation of the \texttt{gpt2-small} model under various threshold values. %The results are broken down into F1-scores for Generated and Human categories, as well as the overall macro F1-score.
The best score for the Generated class is achieved at the threshold of 2/3, with an F1-score of 82.49\%. While for the Human class, it is obtained at the threshold of 3/4, with an F1-score of 79.30\%.
Overall, the most optimal threshold value is 2/3, which returns the highest macro F1-score, 80,19\%, by balancing the performance between the Generated and Human categories. 
\begin{table}[!h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccc}  %|P{5cm}|P{3cm}|P{3cm}|P{3cm}|
      \hline
      {\bf Threshold} &{\bf Generated} &{\bf Human} &{\bf Macro avg F1}\\
      \hline
      \textbf{\texttt{1/4}} &0.6786 &0.0075 &0.3430\\
      \hline
      \textbf{\texttt{1/3}} &0.6798 &0.0199 &0.3498\\ 
      \hline
      \textbf{\texttt{1/2}} &0.6982 &0.1784 &0.4383\\
      \hline
      \textbf{\texttt{2/3}} &0.8249 &0.7790 &0.8019\\
      \hline
      \textbf{\texttt{3/4}} &0.7029 &0.7930 &0.7480\\
      \hline
      \textbf{\texttt{5/6}} &0.2238 &0.6839 &0.4538\\
      \hline
    \end{tabular}
    }
    \caption{GPT-2 Small F1-scores with different threshold values (subtask 1 for English).}
    \label{tab:gpt2-thresholds-eval}
\end{table}

Next, Table \ref{tab:gpt2-models-eval} represents the scores obtained from the different GPT-2 models with a threshold value of 2/3, experimented for English texts in subtask 1. We can notice that as the model size increases, the performance generally declines across all metrics: Generated F1-score, Human F1-score, and macro F1-score.  Hence, the best results are obtained with the \texttt{gpt2-small} model, with Generated F1-score, Human F1-score, and macro F1-score of 82.49\%, 77.90\%, and 80.19\%, respectively.

\begin{table}[!h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
      \hline
      {\bf } &{\bf Generated} &{\bf Human} &{\bf Macro avg F1}\\
      \hline
      \textbf{\texttt{gpt2-small}} &0.8249 &0.7790 &0.8019\\
      \hline
      \textbf{\texttt{gpt2-medium}} &0.7963 &0.6777 &0.7370\\ 
      \hline
      \textbf{\texttt{gpt2-large}} &0.7771 &0.6140 &0.6956\\
      \hline
      \textbf{\texttt{gpt2-xl}} &0.7619 &0.5625 &0.6622\\
      \hline
    \end{tabular}
    }
    \caption{GPT-2 models with threshold 2/3 F1-scores (subtask 1 for English).}
    \label{tab:gpt2-models-eval}
\end{table}

Figure \ref{fig:confusion_matrix_gpt2small_en}, describes the confusion matrix for \texttt{gpt2-small}, which gives an overview of how well the model classifies generated and human-written texts. Out of 11,190 generated texts, the model correctly classifies 10,048 as generated (true positives). However, it misidentifies 1,142 as human-written (false positives). On the other hand, for the 10,642 human-written texts, the model classifies 7,518 as human, while misclassifying 3,124 as generated (false negatives). Although overall the model performs well, it still has room for improvement in distinguishing between generated and human-written texts, especially in reducing false positives and false negatives.
\begin{figure}
    \centering
    \includegraphics[width=7cm,clip]{imagenes/confusion_matrix_gpt2small_en.png}
    \caption{GPT-2 Small Confusion Matrix (0:
generated, 1: human).}
    \label{fig:confusion_matrix_gpt2small_en}
\end{figure}

For the Spanish texts, Table \ref{tab:gpt2-thresholds-eval-es}, presents the F1-scores obtained for different threshold values by the \texttt{gpt2-small-spanish} model. The best Generated F1-score is obtained at the 1/2 threshold with 74.28\%, while the highest Human F1-score at the threshold 2/3 with 68.31\%.
Similarly to \texttt{gpt2-small} for the English dataset, the optimal threshold that achieves the overall balance and highest macro F1-score of 62.18\%, is 2/3.
\begin{table}[!h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccccc}
      \hline
      {\bf Threshold} &{\bf Generated} &{\bf Human} &{\bf Macro avg F1}\\
      \hline
      \textbf{\texttt{1/4}} &0.6788 &0.0242 &0.3515\\
      \hline
      \textbf{\texttt{1/3}} &0.7208 &0.0768 &0.3988\\ 
      \hline
      \textbf{\texttt{1/2}} &0.7428 &0.4458 &0.5943\\
      \hline
      \textbf{\texttt{2/3}} &0.5605 &0.6831 &0.6218\\
      \hline
      \textbf{\texttt{3/4}} &0.2130 &0.6414 &0.4272\\
      \hline
      \textbf{\texttt{5/6}} &0.0320 &0.6180 &0.3250\\
      \hline
    \end{tabular}
    }
    \caption{GPT-2 Small Spanish F1-scores with different threshold values (subtask 1 for Spanish).}
    \label{tab:gpt2-thresholds-eval-es}
\end{table}

Next, Table \ref{tab:es-gpt2-models-eval} shows the results obtained for each of the different GPT-2 models with the threshold value at 2/3. The best macro F1-score is achieved with \texttt{gpt2-xl} with 66.20\%, and with Generated and Human F1-scores of 64.90\% and 67.51\%, respectively. Moreover, we notice that \texttt{gpt2-small-spanish} performs worse regardless of it being trained with Spanish Wikipedia. This could be due to \texttt{gpt2-small-spanish} being much smaller in terms of the number of parameters.
\begin{table}[!h]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
      \hline
      {\bf } &{\bf Generated} &{\bf Human} &{\bf Macro avg F1}\\
      \hline
      \textbf{\texttt{gpt2-small}} &0.0916 &0.6257 &0.3586\\
      \hline
      \textbf{\texttt{gpt2-small-spanish}} &0.5605 &0.6831 &0.6218\\
      \hline
      \textbf{\texttt{gpt2-medium}} &0.3486 &0.6599 &0.5042\\ 
      \hline
      \textbf{\texttt{gpt2-large}} &0.5265 &0.6772 &0.6019\\
      \hline
      \textbf{\texttt{gpt2-xl}} &0.6490 &0.6751 &0.6620\\
      \hline
    \end{tabular}
    }
    \caption{GPT-2 models with threshold 2/3 F1-scores (subtask 1 for Spanish).}
    \label{tab:es-gpt2-models-eval}
\end{table}

Figure \ref{fig:confusion_matrix_gpt2xl_spanish}, presents the confusion matrix for \texttt{gpt2-xl} for Spanish texts. Out of 11,209 generated texts, the model correctly identifies 6,280 as generated, but it mistakenly classifies 4,929 as human-written (false positives). For the 8,920 human-written texts, it accurately labels 7,056 as human, while 1,864 are incorrectly classified as generated (false negatives). Similarly to \texttt{gpt2-small} for English texts, there is some room for improvement in reducing the false positives and false negatives.
\begin{figure}
    \centering
    \includegraphics[width=7cm,clip]{imagenes/confusion_matrix_gpt2xl_spanish.png}
    \caption{GPT-2 XL Confusion Matrix (0:
generated, 1: human).}
    \label{fig:confusion_matrix_gpt2xl_spanish}
\end{figure}

Moreover, we compare our models' results with those presented in AuTexTification 2023 \cite{autextification}, described in Section \ref{sec:autextification2023}. \texttt{gpt2-small} overbeats all of the presented models except for the BiLSTM with fine-tuned RoBERTa and token-level POS tagging from team TALN-UPF \cite{taln-upf2023bilstm}, achieving second rank. However, we must remark the performance between models is very similar, with only a macro F1-score difference of 0.72\%.
On the other hand, \texttt{gpt2-xl} did not perform so well for Spanish texts, as it would have only ranked 9th position in the competition, due to the low F1-scores obtained for both classes.

%TODO


%--------------------------------------------------------------------------
\section{Conclusions and future work}
In this study, we evaluated the performance of GPT-2 models extended with the GLTR tool, on the binary classification task of differentiating AI-generated and human-written texts.

For both English and Spanish texts, the optimal threshold for \texttt{gpt2-small} and \texttt{gpt2-small-spanish} was 2/3, achieving a macro F1-score of 80.19\% and 62.18\%, respectively. However, although \texttt{gpt2-xl} was not trained with Spanish data, it returned better results, achieving a macro F1-score of 66.20\%. 

Among the four GPT-2 model variants explored, \texttt{gpt2-small} outperformed larger models (\texttt{gpt2-medium, gpt2-large, gpt2-xl}) in the English subtask. On the other hand, for the Spanish texts, the \texttt{gpt2-xl} model outperformed smaller models (\texttt{gpt2-small, gpt2-small-spanish, gpt2-medium, gpt2-large}).

Moreover, the \texttt{gpt2-small} model would have ranked second in the AuTexTification 2023 \cite{autextification} competition, outperforming most models with a minimal macro F1-score difference of 0.72\% compared to the top-performing BiLSTM + \texttt{RoBERTa} + POS tagging model \cite{taln-upf2023bilstm}. However, for Spanish texts, the \texttt{gpt2-xl} model would have ranked lower, in 9th position, as it returned a low F1-score for both Generated and Human categories.
%TODO

As for future work, we plan on doing further fine-tuning to improve the \texttt{gpt2-small-spanish} model performance, as well as training the \texttt{gpt2-xl} model with Spanish data. We also plan on tackling the multilingual task for languages from the Iberian Peninsula presented on the IberAuTexTification 2024 task \cite{PLN6628}, as well as the second subtask about model attribution, which is a multiclass classification problem, proposed in the two editions of AuTexTification.%no sé si veo realista la adaptación del GLTR based GPT-2 model con la tarea de multiclase, al final todos son AI-generated.
Furthermore, we also plan to investigate the detection of AI-generated content in a multimodal setting.

%will adapt the GPT-2 models to other types of generated content, such as audio, image, or
%video. This could be very interesting for detecting content from generative models
%such as GPT-4, which can produce these types of formats in addition to text.

%--------------------------------------------------------------------------
\bibliographystyle{fullname}
\bibliography{EjemploARTsepln}

\end{document}
