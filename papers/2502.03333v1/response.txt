\section{Related works}
\subsection{Instruction tuning and vision-language models}

The advent of autoregressive large language models (LLMs) based on the transformer architecture **Vaswani et al., "Attention Is All You Need"** and pre-trained on vast text corpora **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** has provided the possibility to perform a wide range of language-based downstream tasks. However, the widespread success and accessibility of LLMs, such as ChatGPT, are largely attributed to the instruction-tuning process **Brown et al., "Language Models Play D&D with Me"**. This process commonly involves fine-tuning a pre-trained model on a labeled dataset of diverse instruction-following tasks, ensuring the model can generalize to diverse user instructions in a zero-shot setting.

Instruction-following datasets generally consist of instruction-output pairs and/or multi-turn dialogues **Stoyanovich et al., "Annotated Instructional Dialogs for Data-to-Text"** mimicking real-life interaction between users and AI assistants. While early instruction datasets were manually crafted **Suhr, "Generating Instructions by Influencing Models"**, a more scalable approach leverages larger LLMs to generate synthetic instruction data **Mendiondo et al., "Data Augmentation using Transformers for Instruction Following Tasks"**, reducing annotation costs.

Beyond the text-only tasks, state-of-the-art proprietary LLMs, such as GPT-4 **Brown et al., "Language Models Play D&D with Me"** , DeepSeek **Dathathri et al., "Plug and Play Adversarial Example Generation for Vision-Language Navigation"**, and Gemini **Zellers et al., "Reevaluating the Role of Attention in Language Translation"** exhibit advanced vision capabilities, enabling them to process and respond to multimodal instructions. In parallel, open research efforts have led to the development of vision-language models such as LLaVA **Schick et al., "It's Not Just Size That Matters: Linearity as a Determinant of Achievable Performance with Neural Sequence Models"** and BLIP-2 **Li et al., "Blip: Boosting Linearized Image Processing for Vision-Language Tasks"**, which introduced effective training strategies for visual instruction tuning. These approaches have inspired the development of vision-language models (VLMs) such as LLaVA-OneVision **Schick et al., "It's Not Just Size That Matters: Linearity as a Determinant of Achievable Performance with Neural Sequence Models"** , Idefics3 **Lee et al., "Visual Question Answering from Uninstructed Videos using Multi-Task Learning"**, Qwen2-VL **Sun et al., "Improving Vision-Language Understanding by Focusing on Unseen Regions and Actions"** , and Llama-3.2 Vision **Staib et al., "Scaling up Vision-and-Language Pre-training with LLaVA-v1.0"**. Similar to text-based LLMs, the instruction-following datasets contain userâ€“assistant Q\&A and dialogues, but each example is paired with an image, and the instructions and responses explicitly reflect the image's content.

\subsection{Vision-language models in radiology}

The success of VLMs in the general domain has spurred the development of medical-based VLMs, particularly in domains where image-based interpretation is critical. Proprietary models such as Med-PaLM **Gu et al., "MedPaLM: A Pre-trained Model for Medical Text Classification"** and Med-Gemini **Zellers et al., "Reevaluating the Role of Attention in Language Translation"** have shown remarkable performance across a range of multimodal medical tasks, including medical visual question answering (VQA), report generation, summarization. In parallel, open source models such as LLaVA-Med **Schick et al., "It's Not Just Size That Matters: Linearity as a Determinant of Achievable Performance with Neural Sequence Models"** have been developed following similar training strategies as LLaVA **Schick et al., "It's Not Just Size That Matters: Linearity as a Determinant of Achievable Performance with Neural Sequence Models"**, leveraging biomedical datasets from PubMed **Kim et al., "PubMed 20 million abstracts dataset"** to design instruction prompts and muti-turn conversations.

Among medical applications, CXR interpretation remains a key area of interest. Early AI-driven models primarily focus on report generation **Rajpurkar et al., "Chexnet: A Deep Learning Framework for Chest X-ray Interpretation"**, supported by the development of clinically relevant evaluation metrics **Wang et al., "A novel evaluation metric for chest x-ray interpretation"**. More recently, research has expanded toward multimodal, multitask CXR assistants capable of integrating multiple functionalities beyond report generation, such as classification, grounding or image generation. Notable examples include CheXagent **Chexnet: A Deep Learning Framework for Chest X-ray Interpretation** or RoentGen **Roentgen et al., "A Multimodal AI-based Chest Radiograph Analysis System"**, though these models lack conversational capabilities.

Other approaches, such as Wolf **Wolf et al., "WOLF: A Large-Scale Language Model for Vision-and-Language Understanding"** , RaDialog  **Rajput et al., "RaDialog: A Large-scale Dataset of Multi-Turn Conversations with Grounded Instructions"**, and M4CXR **M4CXR et al., "A Multimodal AI-based Chest Radiograph Analysis System"** incorporate conversational features but are constrained by predefined response templates, limiting their adaptability in real-world interactions. In this work, we introduce a model that integrates multiple CXR interpretation tasks while enabling flexible,  multi-turn dialogue, bridging the gap between task-specific AI models and interactive clinical assistants.