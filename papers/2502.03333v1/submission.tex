\pdfoutput=1
\documentclass[a4paper]{article}
\usepackage{xcolor}
\usepackage[nottoc,notlot,notlof]{tocbibind}
\definecolor{dandelion}{RGB}{255, 200, 0} % Adjust RGB values as needed
\definecolor{cornflowerblue}{RGB}{100, 149, 237}
\definecolor{pastelgray}{RGB}{190, 190, 190}
\definecolor{pastelyellow}{RGB}{252, 229, 153}
\definecolor{pastelpurple}{RGB}{215, 189, 226}
\definecolor{pastelred}{RGB}{245, 183, 177}

%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}

\usepackage{amssymb} % For checkmark and cross symbols
\usepackage{pifont}  % For additional symbols like checkmark and cross



 \usepackage[english]{babel}
\usepackage{layout}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{changes}
\usepackage{graphicx}
\usepackage{array}
\usepackage{caption} 
\usepackage{adjustbox} % For adjusting the size of the table
\usepackage{tcolorbox}

\renewcommand{\arraystretch}{1.5} % Increase row height for readability
\usepackage{multirow} % Required for multirow


\usepackage{authblk}


\usepackage[euler]{textgreek}
%\usepackage{amsfonts}
\usepackage{ucs}

\usepackage{ifthen}

\usepackage{float}
\usepackage{lipsum}
\usepackage{listing}
%\usepackage{diagbox} % For creating a diagonal line in a table cell
\usepackage{makecell} % For creating diagonal cells

\usepackage{bibentry}
%\usepackage{times}
\usepackage{appendix}
\usepackage{upgreek}
\usepackage{chngcntr}
\usepackage{listings}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{pgfgantt}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}%
\usepackage{soul}

\usepackage{multirow}   % For multi-row cells
\usepackage{booktabs}   % For improved table rules (optional)


\usepackage{arydshln} % in the preamble
\usepackage{array}   % For advanced table formatting
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}


\usepackage[a4paper,top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\usepackage{newfloat}
\DeclareFloatingEnvironment[name={Appendix-Figure}]{suppfigure}
\DeclareFloatingEnvironment[name={Appendix-Table}]{supptable}


\usepackage[capitalize,noabbrev]{cleveref}
\crefformat{figure}{#2\figurename~#1#3}
\crefformat{table}{#2\tablename~#1#3}
\crefname{suppfigure}{Appendix 1-Figure}{Appendix 1-Figures}
\crefname{supptable}{Appendix 1-Table}{Appendix 1-Tables}
\crefformat{suppfigure}{#2\suppfigurename~#1#3}
\crefformat{supptable}{#2\supptablename~#1#3}
\crefname{equation}{Equation}{Equations}
\creflabelformat{equation}{#2#1#3}


\newcommand{\comyp}[1]{\textcolor{purple}{YP: #1}} 
\newcommand{\comfn}[1]{\textcolor{cyan}{FN: #1}} % comments from Farhad 
\newcommand{\comnd}[1]{\textcolor{brown}{ND: #1}} % comments from Nicolas
\newcommand{\comhm}[1]{\textcolor{magenta}{HM: #1}} % comments from Hidetoshi
\newcommand{\comsrc}[1]{\textcolor{green}{SRC: #1}} % comments from Samuel, although I usually edit in 'suggestion' mode (tracked changes).
\newcommand{\comcb}[1]{\textcolor{orange}{CB: #1}} % comments from Chris 

% Define a new command for small captions
\newcommand{\smallcaption}[1]{\caption{\small #1}}

\lstset{
    basicstyle=\scriptsize\ttfamily, % Use small font size
    breaklines=true, % Automatically break lines
    frame=single, % Draw a frame around the code block
    aboveskip=2pt, % Add some space above the code block
    belowskip=2pt, % Add some space below the code block
    lineskip=0pt % Add space between lines in the block
}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
%\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}

\captionsetup{
    labelfont=bf,      % Makes "Figure 1:", "Table 1:", etc., bold
    textfont=normalfont % Keeps the caption text in normal font (optional)
}


\definecolor{darkgreen}{rgb}{0.0, 0.8, 0.0}

\newcommand{\cmark}{\textcolor{darkgreen}{\ding{51}}} % Dark green checkmark
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}       % Red cross


% Define custom colors
\definecolor{lightblue}{RGB}{221,233,248}
\definecolor{lightgray}{RGB}{230,230,230}

% Define box styles for human and assistant messages with minimal padding and spacing
\tcbset{
  humanbubble/.style={
    colback=lightblue!20,
    colframe=lightblue,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm, % Set top and bottom padding to 0mm
    before skip=0pt, after skip=0pt, % Eliminate vertical space before and after the box
  },
  assistantbubble/.style={
    colback=lightgray!20,
    colframe=lightgray,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm, % Set top and bottom padding to 0mm
    before skip=0pt, after skip=0pt, % Eliminate vertical space before and after the box
  }
}


% Custom colors
\definecolor{lightred}{RGB}{255,220,220}
\definecolor{lightbrown}{RGB}{245,222,179}
\definecolor{lightgreen}{RGB}{210,245,210}
\definecolor{lightpurple}{RGB}{240,230,255}

% Define five tcolorbox styles: blue, gray, red, brown, green
\tcbset{
  bubbleblue/.style={
    colback=lightblue!20,
    colframe=lightblue,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm,
    before skip=0pt, after skip=0pt,
  },
  bubblegray/.style={
    colback=lightgray!20,
    colframe=lightgray,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm,
    before skip=0pt, after skip=0pt,
  },
  bubblered/.style={
    colback=lightred!20,
    colframe=lightred,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm,
    before skip=0pt, after skip=0pt,
  },
  bubblebrown/.style={
    colback=lightbrown!20,
    colframe=lightbrown,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm,
    before skip=0pt, after skip=0pt,
  },
  bubblegreen/.style={
    colback=lightgreen!20,
    colframe=lightgreen,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm,
    before skip=0pt, after skip=0pt,
  },
   bubblepurple/.style={
    colback=lightpurple!20,
    colframe=lightpurple,
    boxrule=0.5mm,
    arc=2mm,
    left=1mm, right=1mm, top=0mm, bottom=0mm,
    before skip=0pt, after skip=0pt,
  }
}



\newsavebox{\roundimagebox}
\newlength{\roundimagewidth}
\newlength{\roundimageheight}


\newcommand{\roundimage}[2]{%
  % Store the image in a box to determine its real final size
  \sbox{\roundimagebox}{%
    \includegraphics[width=#2\linewidth]{#1}%
  }%
  % Measure the box
  \setlength{\roundimagewidth}{\wd\roundimagebox}%
  \setlength{\roundimageheight}{\ht\roundimagebox}%
  %
  \begin{tikzpicture}
    % Use the actual measured width and height to clip
    \clip[rounded corners=0.03\linewidth]
         (0,0) rectangle (\roundimagewidth, \roundimageheight);
    % Place the saved box
    \node[anchor=south west, inner sep=0] at (0,0){%
      \usebox{\roundimagebox}%
    };
  \end{tikzpicture}%
}

% Rounded image function aligned to the right
\newcommand{\roundimageRight}[2]{%
  \begin{flushright}
    % Store the image in a box to determine its real final size
    \sbox{\roundimagebox}{%
      \includegraphics[width=#2\linewidth]{#1}%
    }%
    % Measure the box
    \setlength{\roundimagewidth}{\wd\roundimagebox}%
    \setlength{\roundimageheight}{\ht\roundimagebox}%
    %
    \begin{tikzpicture}
      \clip[rounded corners=0.03\linewidth]
           (0,0) rectangle (\roundimagewidth, \roundimageheight);
      \node[anchor=south west, inner sep=0] at (0,0){%
        \usebox{\roundimagebox}%
      };
    \end{tikzpicture}%
  \end{flushright}
}
% Define commands for human and assistant messages
\newcommand{\human}[1]{%
  \begin{flushleft}
    \begin{tcolorbox}[humanbubble, width=0.95\linewidth]
      \begin{minipage}[t]{0.1\linewidth}
        \vspace{-2pt}
        \includegraphics[scale=0.015]{images/human.png}
      \end{minipage}%
      \begin{minipage}[t]{0.85\linewidth}
        \vspace{0pt}
        #1
      \end{minipage}
    \end{tcolorbox}
  \end{flushleft}
  \vspace{-6mm} % Reduce space below human bubble
}

\newcommand{\assistant}[1]{%
  \begin{flushright}
    \begin{tcolorbox}[assistantbubble, width=0.95\linewidth]
      \begin{minipage}[t]{0.9\linewidth}
        \vspace{0pt}
        #1
      \end{minipage}% 
      \hspace{2mm}
      \begin{minipage}[t]{0.05\linewidth}
        \vspace{-2pt}
        \includegraphics[scale=0.015]{images/assistant_icon.png}
      \end{minipage}
    \end{tcolorbox}
  \end{flushright}
  \vspace{-6mm}
}



\newcommand{\reference}[2][bubblered]{%
  \begin{flushright}
    \begin{tcolorbox}[#1, width=0.95\linewidth]
      \begin{minipage}[t]{0.9\linewidth}
        \vspace{0pt}
        #2
      \end{minipage}% 
      \hspace{2mm}
      \begin{minipage}[t]{0.05\linewidth}
        \vspace{-2pt}
      \end{minipage}
    \end{tcolorbox}
  \end{flushright}
  \vspace{-6mm}
}

\newcommand{\radialog}[2][bubblebrown]{%
  \begin{flushright}
    \begin{tcolorbox}[#1, width=0.95\linewidth]
      \begin{minipage}[t]{0.9\linewidth}
        \vspace{0pt}
        #2
      \end{minipage}% 
      \hspace{2mm}
      \begin{minipage}[t]{0.05\linewidth}
        \vspace{-2pt}
        RaDialog%
      \end{minipage}
    \end{tcolorbox}
  \end{flushright}
  \vspace{-6mm}
}


\newcommand{\llavamed}[2][bubblepurple]{%
  \begin{flushright}
    \begin{tcolorbox}[#1, width=0.95\linewidth]
      \begin{minipage}[t]{0.9\linewidth}
        \vspace{0pt}
        #2
      \end{minipage}% 
      \hspace{2mm}
      \begin{minipage}[t]{0.05\linewidth}
        \vspace{-2pt}
        LLaVA-Med%
      \end{minipage}
    \end{tcolorbox}
  \end{flushright}
  \vspace{-6mm}
}



\renewcommand\Affilfont{\small} % You can change \small to \footnotesize if desired


\newcommand{\regionimage}[2]{%
  \begin{minipage}[t]{0.23\linewidth} % top alignment
    \vspace{0pt}% ensure starting at the top baseline
    \centering
    % Use TikZ to clip the image
    \begin{tikzpicture}
      \clip[rounded corners=5pt] (0,0) rectangle (1.5cm,1.5cm);
      \includegraphics[width=1.5cm,height=1.5cm]{#1}
    \end{tikzpicture}\\[-1mm]
    {\scriptsize #2}% Adjust font size here if needed
  \end{minipage}%
}



% Title information
\title{RadVLM: A Multitask Conversational Vision-Language Model for Radiology}

\date{}
\author[1\footnote{Correspondence: nicolas.deperrois@uzh.ch}]{Nicolas Deperrois}
\author[2]{Hidetoshi Matsuo}
\author[3]{Samuel Ruipérez-Campillo}
\author[3]{Moritz Vandenhirtz}
\author[3]{Sonia Laguna}
\author[3]{Alain Ryser}
\author[4]{Koji Fujimoto}
\author[2]{Mizuho Nishio}
\author[3]{Thomas M. Sutter}
\author[3]{Julia E. Vogt}
\author[1,5]{Jonas Kluckert}
\author[5]{Thomas Frauenfelder}
\author[5]{Christian Bl\"uthgen}
\author[1\footnote{Joint senior authorship.}]{Farhad Nooralahzadeh}
\author[1$^\dagger$]{Michael Krauthammer}


\affil[1]{Department of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland}
\affil[2]{Department of Radiology, Kobe University, Kobe, Japan}
\affil[3]{Department of Computer Science, ETH Zurich, Zurich, Switzerland}
\affil[4]{Department of Advanced Imaging in Medical Magnetic Resonance, Kyoto University, Kyoto, Japan}
\affil[5]{Diagnostic and Interventional Radiology, University Hospital Zurich, Zurich, Switzerland}

\begin{document}

\maketitle

\begin{abstract}
%Background/Intro
The widespread use of chest X-rays (CXRs), coupled with a shortage of radiologists, has driven growing interest in automated CXR analysis and AI-assisted reporting. While existing vision-language models (VLMs) show promise in specific tasks such as report generation or abnormality detection, they often lack support for interactive diagnostic capabilities. 
%Method/Objective
In this work we present RadVLM, a compact, multitask conversational foundation model designed for CXR interpretation. To this end, we curate a large-scale instruction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks -- such as report generation, abnormality classification, and visual grounding -- and multi-turn, multi-task conversational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. 
%Results
Our results show that RadVLM achieves state-of-the-art performance in conversational capabilities and visual grounding while remaining competitive in other radiology tasks. Ablation studies further highlight the benefit of joint training across multiple tasks, particularly for scenarios with limited annotated data.
%Conclusion
Together, these findings highlight the potential of RadVLM as a clinically relevant AI assistant, providing structured CXR interpretation and conversational capabilities to support more effective and accessible diagnostic workflows\footnote{Model checkpoints and instruction dataset are publicly available at our project website \url{https://huggingface.co/KrauthammerLab}}.
\end{abstract}



\section{Introduction}
X-rays have played a fundamental role in medicine since their discovery in 1895~\citep{rontgen1895ueber}, and continue to be the most frequently used medical imaging modality worldwide due to their convenience and cost-effectiveness \citep{akhter2023ai}. Chest X-ray (CXR) remains the most commonly performed radiological exam globally, particularly important for diagnosing and monitoring thoracic conditions such as pneumonia, heart failure, and lung cancer \citep{ccalli2021deep}. Problematically, the growing volume of CXRs and other imaging studies in recent years have lead to a reduction in the time available for radiologists to thoroughly evaluate each case ~\citep{peng2022radiologist}. As a result, in many countries, the responsibility of interpreting CXRs is often transferred to non-radiology physicians, who typically possess less specialized training and experience. This shift increases the risk of diagnostic errors or misinterpretations \citep{Malak2021, peng2022radiologist}.

The shortage of trained personnel for CXR interpretation has led to the exploration of automated agents to assist physicians in diagnostic tasks. In recent years, various deep learning models have shown promise in clinical applications, such as the detection of conditions like COVID-19 pneumonia \citep{Nishio2020} or pulmonary nodules \citep{Homayounieh2021}. Another extensively studied task is the automated generation of free text reports from CXR images using transformer-based architectures \citep{nooralahzadeh2021progressive, Yang2023, hyland2023maira, chaves2024llavarad}. These models can provide preliminary drafts summarizing key observations from the CXR, offering a potential enhancement to the diagnostic workflow. There is a need to expand the scope of these tools beyond report generation, towards the capability to answer questions about the CXR technique, findings in a region of interest, location of specific abnormalities, and definitions of medical terms. In addition, physicians should be allowed to formulate their queries flexibly and in any order, potentially within a multi-turn, conversational interaction with the assistant \citep{Tu2024-ad}.

Recently, significant advancements in the field of multimodal artificial intelligence (AI) have enabled the development of models such as GPT-4 Vision  \citep[GPT4-V,][]{chatgpt-ut} and Claude \citep{claude-hq}, which have the ability to describe and converse about images with increasing reliability. The training principle behind these models relies on an initial stage to align pretrained vision and language modules, followed by a visual instruction tuning phase where the model learns to respond to image-related queries and commands \citep{li2024llava}. Such advancements have inspired the adaptation of multimodal AI assistants for medical applications \citep{singhal2023large, li2023llava-med, saab2024capabilities}. 

Despite these advancements, there remains a need for specialized multimodal conversational assistants tailored specifically for CXR interpretation. In this direction, models such as CheXagent \citep{chen2024chexagent}, RaDialog \citep{pellegrini2023radialog}, or MAIRA-2 \citep{Bannur2024-ek} were developed, extending beyond report generation to tasks such as observation grounding and visual question answering, covering a larger part of the clinical workflow. However, their capacity to handle diverse and complex user queries, or to respond accurately to multiple prompts within an arbitrary conversational framework, remains limited. Adding these capabilities is critical for comprehensively supporting  clinicians' daily work.

In this study, we build upon state-of-the-art visual instruction-tuning techniques inspired by general-domain applications \citep{liu2023visual, wang2024qwen2vl} to construct a compact, multitask conversational foundation model specialized in CXR interpretation. To achieve this aim, we create comprehensive CXR datasets, each featuring diverse modalities including free-text reports, abnormality labels, and visual coordinates, and organize them into a unified instruction dataset. This dataset is comprised of single-turn image-instruction pairs for different tasks and image-conversation pairs designed for more flexible and multi-turn interactions. We then fine-tune a vision-language architecture \citep{li2024llava} on this instruction dataset, naming the resulting model RadVLM, and develop an evaluation pipeline to assess its performance across multiple tasks, systematically comparing it to state-of-the-art generalist and CXR-specific foundation models. Our results show that, despite its relatively compact size, RadVLM achieves competitive performance on individual tasks relevant to clinical practice, providing conversational capabilities within a simple and flexible interface, providing a reliable and user-friendly tool for physicians. Additionally, we compare RadVLM with several existing vision-language models and show that RadVLM matches or outperforms these across both individual and conversational tasks. 

In summary, the contributions of this work are as follows: 
\begin{itemize}
    \item We develop a unique instruction dataset that extends beyond report generation to encompass diverse CXR-based tasks, including multi-turn conversational interactions tailored for clinical workflows.
    \item We design and train RadVLM, a multitask conversational foundation model designed to assist physicians in CXR analysis that solely relies on visual information -- avoiding the need for providing additional metadata. 
    \item We employ an evaluation pipeline, re-implementing existing models for comparison and ensuring reproducibility of results.
    \item We evaluate RadVLM systematically across multiple tasks, demonstrating competitive performance against state-of-the-art vision-language models, both generalist, and medical-specific.  In particular, we evaluate conversational abilities in clinical contexts and demonstrate that RadVLM significantly  outperforms existing general and clinical VLMs in this aspect.
\end{itemize}


\section{Related works}

\subsection{Instruction tuning and vision-language models}

The advent of autoregressive large language models (LLMs) based on the transformer architecture ~\citep{vaswani2017attention} and pre-trained on vast text corpora~\citep{radford2019language, brown2020language} has provided the possibility to perform a wide range of language-based downstream tasks. However, the widespread success and accessibility of LLMs, such as ChatGPT, are largely attributed to the instruction-tuning process \citep{wei2021finetuned, ouyang2022training}. This process commonly involves fine-tuning a pre-trained model on a labeled dataset of diverse instruction-following tasks, ensuring the model can generalize to diverse user instructions in a zero-shot setting. 

Instruction-following datasets generally consist of instruction-output pairs and/or multi-turn dialogues \citep{zheng2023lmsys} mimicking real-life interaction between users and AI assistants. While early instruction datasets were manually crafted \citep{wei2021finetuned}, a more scalable approach leverages larger LLMs to generate synthetic instruction data \citep{wang2022self, peng2023instruction, liu2023visual}, reducing annotation costs. 

Beyond the text-only tasks, state-of-the-art proprietary LLMs, such as GPT-4 \citep{achiam2023gpt}, DeepSeek \citep{liu2024deepseek, guo2025deepseek}, and Gemini \citep{team2023gemini} exhibit advanced vision capabilities, enabling them to process and respond to multimodal instructions. In parallel, open research efforts have led to the development of vision-language models such as LLaVA \citep{liu2023visual} and BLIP-2 \citep{li2023blip}, which introduced effective training strategies for visual instruction tuning. These approaches have inspired the development of vision-language models (VLMs) such as LLaVA-OneVision \citep{li2024llava}, Idefics3 \citep{laurencon2024building}, Qwen2-VL \citep{wang2024qwen2vl}, and Llama-3.2 Vision \citep{dubey2024llama}. Similar to text-based LLMs, the instruction-following datasets contain user–assistant Q\&A and dialogues, but each example is paired with an image, and the instructions and responses explicitly reflect the image's content \citep{feng2022mmdialog}. 


\subsection{Vision-language models in radiology}

 The success of VLMs in the general domain has spurred the development of medical-based VLMs, particularly in domains where image-based interpretation is critical. Proprietary models such as Med-PaLM\citep{singhal2023large} and Med-Gemini \citep{saab2024capabilities} have shown remarkable performance across a range of multimodal medical tasks, including medical visual question answering (VQA), report generation, summarization. In parallel, open source models such as LLaVA-Med \citep{li2023llava-med} have been developed following similar training strategies as LLaVA \citep{liu2023visual}, leveraging biomedical datasets from PubMed \citep{pubmed} to design instruction prompts and muti-turn conversations. 

Among medical applications, CXR interpretation remains a key area of interest. Early AI-driven models primarily focus on report generation \citep{nooralahzadeh2021progressive, alfarghaly2021automated, tanida2023interactive, chaves2024llavarad}, supported by the development of clinically relevant evaluation metrics \citep{jain2021radgraph, yu2023evaluating}. More recently, research has expanded toward multimodal, multitask CXR assistants capable of integrating multiple functionalities beyond report generation, such as classification, grounding or image generation. Notable examples include CheXagent \citep{chen2024chexagent} or RoentGen \citep{bluethgen2024vision}, though these models lack conversational capabilities. 

Other approaches, such as Wolf \citep{kang2024wolf}, RaDialog  \citep{pellegrini2023radialog}, and M4CXR \citep{chen2024chexagent}, incorporate conversational features but are constrained by predefined response templates, limiting their adaptability in real-world interactions. In this work, we introduce a model that integrates multiple CXR interpretation tasks while enabling flexible,  multi-turn dialogue, bridging the gap between task-specific AI models and interactive clinical assistants. 


\section{Methods}

\subsection{Instruction dataset}

A key step in the development of RadVLM is the construction of an instruction dataset. For this purpose, we first aggregate and process multiple publicly available datasets containing CXR images paired with various attributes, including free-text reports, categorical labels, and bounding boxes. From these sources, we generate a dataset of over 1 million instruction instances, each consisting of a frontal CXR image and a corresponding user-assistant interaction derived from the available attributes. These interactions can be in the form of a single Q\&A designed for predefined tasks (single instructions) or of a multi-turn exchange (conversations). The composition of this instruction dataset is detailed below and summarized in \cref{table:instruction-dataset-content}. 

\subsubsection{Free-text report generation}

In alignment with existing CXR models, we aim to generate clinically coherent radiology reports from CXR images. To achieve this, we collect public datasets containing CXRs paired with anonymized free-text reports. Radiology reports often compare to and refer to prior X-ray examinations when discussing current radiological findings. These earlier images should be provided as part of the prompt when conducting report generation based on raw radiology reports \citep{kim2023boosting, Bannur2024-ek}.  As we here focus on the analysis of a single CXR image, we use GPT4o-mini to remove mentions of prior studies, in line with recent work on report generation \citep{chen2024chexagent, chaves2024llavarad}.

We leverage two public datasets for the report generation task:

\noindent \textbf{MIMIC-CXR} \citep{johnson2019mimic}, which contains 377,110 CXR images paired with free-text radiology reports describing findings and patient history. After filtering, we retain 232,344 image-text pairs in the training set and 3,282 in the test set.

\noindent \textbf{CheXpert-Plus} \citep{chambon2024chexpertplusaugmentinglarge}, which  features 223,228 image-report pairs from 187,711 studies. Applying the same filtering process as for MIMIC-CXR results in a dataset of 178,368 image-text pairs.

For this task, the instructions are designed such that the user asks the assistant to generate a report for a given CXR, and the assistant responds with the filtered report corresponding to that CXR (\cref{fig:single-instructions}a).
 



\begin{figure}[t]
\centering % Centers the figure on the page
{%
\footnotesize % Reduce font size within the figure
% First minipage (top-left)
\begin{minipage}[t]{0.48\linewidth} % Changed [t] to [b] here
  \vspace{0pt} % Ensures alignment at the bottom
  \textbf{a. Report generation}
  \human{Draft a concise report for this image.
  \vspace{1mm} % Extra space above image
  
  \roundimage{images/instruction_dataset/chest_xray.png}{0.2}
  }
  \reference{Small right pleural effusion is present. Minimal right lower lobe atelectasis is likely. The right lung is otherwise clear. No appreciable pneumothorax is seen. Biapical margin thickening, left worse than right, is seen. The left lung is well-expanded and clear. The heart size is normal. Hemi arthroplasty is seen on the right shoulder.}
  \vspace{5mm}
  \centering
\end{minipage}%
\hfill
% Second minipage (top-right)
\begin{minipage}[t]{0.48\linewidth} % Changed [t] to [b] here
  \vspace{0pt} % Ensures alignment at the bottom
  \textbf{b. Abnormality classification}
  \human{Are there identifiable abnormalities on this Chest X-ray?
  
  \vspace{1mm} % Extra space above image
  \roundimage{images/instruction_dataset/class_image}{0.2}
  }
  \reference{Yes, there is one abnormality identified: pneumothorax.}
  \vspace{5mm}
  \centering
\end{minipage}%
\vspace{2mm} % Space between rows

% Third minipage (bottom-left)
\begin{minipage}[t]{0.48\linewidth} % Bottom panels remain as they are
  \vspace{0pt} % Ensures alignment at the top
  \textbf{c. Visual grounding: Anatomy/Abnormality}
   \human{Indicate the position of the \textit{left apical zone}.
  \vspace{1mm} % Extra space above image
  \roundimage{images/instruction_dataset/anat_image.png}{0.2}
  }
  \reference{The left apical zone is situated at [0.57, 0.14, 0.85, 0.3] in the image.
  \roundimageRight{images/instruction_dataset/anat_image_BBox}{0.2}
  }
  \vspace{5mm}
  \centering
\end{minipage}%
\hfill
% Fourth minipage (bottom-right)
\begin{minipage}[t]{0.48\linewidth} % Bottom panels remain as they are
  \vspace{0pt} % Ensures alignment at the top
  \textbf{d. Visual grounding: Phrase}
   \human{Mark the area where you observe: \textit{marked elongation and increased caliber of the thoracic aorta}.
   
  \vspace{1mm} % Extra space above image}
  \roundimage{images/instruction_dataset/phrase_image}{0.22}
  }
  \reference{The area specified is at coordinates [0.39, 0.3, 0.66, 0.79].
  \roundimageRight{images/instruction_dataset/phrase_image_BBox}{0.22}
  }
   \vspace{5mm}
  \centering
\end{minipage}%
}
\caption{ \textbf{Examples of single instructions for different tasks.} We design three main types of instructions based on dataset attributes. For datasets containing image-reports pairs (e.g., MIMIC-CXR), we design Q\&A towards report generation (a). The instructions for datasets containing abnormality labels (e.g., CheXpert) are designed to perform multi-class classification (b). When bounding boxes are available, we design visual grounding instructions, where the assistant provides the bounding box coordinates to display them on the input image (c,d). }
\label{fig:single-instructions}
\end{figure}

\subsubsection{Abnormality classification}

Another essential task an AI-assistant should be capable of is to identify the presence of abnormalities on a CXR. While simpler than generating detailed, unstructured observations, this functionality serves as a quick and helpful overview for physicians, highlighting key observations before they dive into a more detailed analysis.

For this task, we collect CXR datasets paired with abnormality labels. These labels were extracted from the original textual reports via the CheXbert automatic labeling tool \citep{smit2020chexbert},  which identifies whether each of 14 possible abnormalities is present, absent, or uncertain. In our setup, we only consider frontal images and--in line with previous work \citep{yang2024advancing, chaves2024llavarad}--consider ``uncertain'' abnormalities as ``absent''.

For this task, we use 191,027 image-labels pairs from CheXpert \citep{irvin2019chexpert} and 237,912 pairs from MIMIC-CXR (\cref{table:instruction-dataset-content}). The instructions are designed such that the user asks for the abnormalities present on the CXR and the assistant answers by providing the list of abnormalities (\cref{fig:single-instructions}b). 

\subsubsection{Visual grounding}


Detecting the location of specific anatomical regions or pathologies on a CXR is an important task for AI assistants. In addition to providing a textual description of the image, they should be able to spot where specific observations are located. This is usually done by predicting bounding box coordinates of top-left and bottom-right corners $[x_1, y_1, x_2, y_2]$. While classical object detectors \citep{ren2016faster, redmon2016you} tackle this task by leveraging specialized architectures and learning rules, we embark on it with the other text-based tasks via next-token prediction (\cref{eqn:next-word-prediction}), formatting coordinates into text, enclosed between square brackets. As input images are pre-processed to a unique size by the vision encoder, we normalize these coordinates to the original dimension to obtain floating values between 0 and 1, similarly as in \citet{you2023ferret, park2024m4cxr, zhang2024ferret}.

The following datasets, with fine-grained  X-ray information, were collected to design visual grounding instructions: 

\noindent
\textbf{Chest Imagenome} \citep{wu2021chest}, derived from MIMIC-CXR, provides additional annotations to frontal X-ray images, in particular, bounding box coordinates for 29 anatomical regions. In our training data, we randomly select one region per image for each datapoint and create an instruction for anatomical region grounding following \cref{fig:single-instructions}. 

\noindent
\textbf{VinDr-CXR} \citep{nguyen2022vindr} contains 18,000 frontal images, each manually annotated by three different radiologists. To merge their annotations, we pre-processed them by fusing bounding boxes of the same pathology using weighted box fusion \citep{solovyev2021weighted}, similarly as in \citet{muller2024chex}. From this dataset, we design two types of tasks: i) abnormality grounding, asking for the location of a specific abnormality (also following \cref{fig:single-instructions}c) and ii) abnormality detection, asking the location of all abnormalities, if any (\cref{table:instruction-dataset-content}). 

\noindent
\textbf{MS-CXR} \citep{boecking2022making} provides image-sentence pairs of bounding boxes and corresponding phrases, complementing MIMIC-CXR. 

\noindent
\textbf{PadChest-GR} \citep{castro2024padchest} also contains grounded sentences derived from the PadChest dataset \citep{bustos2020padchest}.  

From the last two datasets, we construct the `phrase grounding' task, where a user asks about the location of a specific sentence from a radiology report, and the assistant provides its associated bounding box coordinates (\cref{fig:single-instructions}d). 




\begin{table}[t]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\begin{tabular}{p{4cm}p{2.5cm}C{4.5cm}C{2.8cm}}
\toprule
\multicolumn{1}{c}{\textbf{Task}} & \multicolumn{1}{c}{\textbf{Dataset source}} & \textbf{Image-instruction pairs (\#)} & \textbf{Evaluation (\#)} \\ \midrule
% Example row with task spanning two datasets (rows)
\multirow{2}{*}{Report Generation}
& MIMIC-CXR & 232,344 $\times$ 1 &  3,282 \\ 
& CheXpert-Plus & 178,368 $\times$ 1 &  - \\ \midrule
\multirow{2}{*}{Abnormality classif.}
 & MIMIC-CXR     & 237,912 $\times$ 1 &  518 \\ 
 & CheXpert     & 191,027 $\times$ 1 &  - \\ \midrule
\multirow{1}{*}{Anatomical grounding}
 & Chest Imagenome & 80,000 $\times$ 1 &  2,000 \\ \midrule
\multirow{1}{*}{Abnormality grounding}
& VinDr-CXR  & 16,089 $\times$ 3 &  2,108 \\ \midrule
\multirow{1}{*}{Abnormality detection}
& VinDr-CXR  & 15,000 $\times$ 2 &  - \\ \midrule
\multirow{2}{*}{Phrase grounding}
& MS-CXR  & 971 $\times$ 3 &  189 \\
 & PadChest-GR & 4478 $\times$ 2 & -  \\ \midrule
\multirow{1}{*}{Conversation}
 & MIMIC-CXR & 80,312 $\times$ 1 &  523 \\ \midrule
\multirow{2}{*}{Conversation (grounded)}
 & MS-CXR & 858 $\times$ 4 &  157 \\
& PadChest-GR & 2,225 $\times$ 4 & - \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Overview of the instruction dataset.} The instruction dataset comprises 1,022,742 image-instruction pairs spanning multiple vision-language tasks, including report generation, abnormality classification, anatomical and abnormality grounding, phrase grounding, and conversational interactions. Dataset sources and the corresponding number of image-instruction pairs are listed, with smaller datasets balanced by varying the frequency of instruction occurrences.}
\label{table:instruction-dataset-content}
\end{table}


\subsubsection{Conversations}
\label{section:conversation}

Fine-tuning a VLM on single instructions, as presented above, is useful to acquire maximal precision in specific tasks but would not be sufficient to build a robust, flexible, and conversational radiology assistant. First, in a real-life setting, we cannot assume that physicians prompt the model with a limited set of instructions. Various types of questions could be posed, such as asking about the characteristics of a specific organ (lungs, heart), the orientation of the X-ray, or the definition of certain medical terms. More importantly, interactions can be decomposed over several Q\&A rounds, with sometimes a question referring to previous answers (e.g., asking about the location of a specific observation from the previous answer). The model should thus be tuned to sequentially connect visual concepts (textual observations, presence or absence of abnormalities, fine-grained information) throughout a single conversation thread. 

To develop this capability in RadVLM, we constructed an instruction-tuning dataset mimicking a real-life multi-turn interaction between user and assistant, named ``conversation dataset''. Here, questions can be asked in different order, and the assistant reacts to the content of previous answers. Inspired by the vision-language models LLaVA \citep{liu2023visual} and LLaVA-Med \citep{li2023llava-med}, we prompt a larger text-only LLM (GPT-4o-mini) to generate multi-turn conversations. The prompt includes a system message instructing the LLM to generate a dialogue between a user and an assistant, along with detailed CXR information -- including the radiology report, abnormality labels, bounding box coordinates, view, and gender (see prompt in \cref{supp_fig:conv_prompt}). Importantly, by leveraging the provided CXR information, the assistant is designed to respond as if it had direct visual access to the image (\cref{fig:conversation}). 

Following this process, we generate in total 83k image-conversation pairs, including 80k standard conversations (\cref{fig:conversation}a) and 3k ``grounded conversations'' (\cref{fig:conversation}b) that include interactions aimed at localizing specific observations. For the grounded conversations, it is essential to supply pairs of textual observations and their corresponding bounding box coordinates in the prompt. To achieve this, we use the datasets used for the ``phrase grounding'' task, MS-CXR and PadChest-GR, that provide annotations linking specific phrases in radiology reports to precise image location We thereby obtain 858 image-conversation pairs derived from MS-CXR grounded phrases, and 2,225 pairs derived from PadChest-GR (\cref{table:instruction-dataset-content}).


\begin{figure}[t!]
\footnotesize % Reduce font size within the figure
\begin{minipage}[t]{0.48\linewidth} % First minipage
\vspace{0pt} % Ensures alignment at the top
  \textbf{a. Sample conversation}
  \human{Can you describe what you see in the chest X-ray?
  \vspace{1mm} % Extra space above image (you can adjust or remove this if needed)
  
  \roundimage{images/instruction_dataset/conv_image}{0.2}
  }
  \reference{Sure, I see a chest X-ray taken from the front view, known as AP (anterior-posterior). The heart appears to be borderline enlarged, but otherwise, the mediastinal and hilar regions look normal. There are no signs of fluid around the lungs or collapsed lung areas.}
  \human{Is there anything unusual in the lung area?}
  \reference{The lungs mostly look clear, but there is some linear scarring or atelectasis in the right lung base. Apart from that, the lung fields do not show any significant abnormalities.}
  \human{Are there any signs of past surgery visible in the X-ray?}
  \reference{Yes, there are clips visible in the right upper quadrant of the abdomen, which are often left after a cholecystectomy, a surgical procedure to remove the gallbladder.}
  \human{Does the X-ray show any signs of fluid around the lungs?}
  \reference{No, the X-ray does not show any signs of pleural effusion, which is fluid accumulation around the lungs.}
  \human{Is the heart looking normal in size and shape?}
  \reference{The heart size appears borderline enlarged, which means it is slightly bigger than expected, but not significantly abnormal in shape.}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\linewidth} % Second minipage
\vspace{0pt} % Ensures alignment at the top
  \textbf{b. Sample conversation with grounding}
\human{Can you tell me what you observe in this chest X-ray?
\vspace{1mm}

\roundimage{images/instruction_dataset/conv_image2.png}{0.2}
}
 \reference{The X-ray shows that the cardiac silhouette is enlarged. There is also suspicion of mild left basal atelectasis.}
 \human{Where is the cardiac silhouette enlargement located on the image?}
 \reference{The enlarged cardiac silhouette is located within the coordinates [0.51, 0.43, 0.98, 0.79].
  \roundimageRight{images/instruction_dataset/conv_image2_Bbox.png}{0.2}
  }
  \human{Is there any fluid accumulation in the pleural space visible in the X-ray?}
  \reference{No, there is no visible pleural effusion on the X-ray.}
  \human{What can be said about the right lung in this image?}
  \reference{The right lung appears clear with no visible abnormalities.}
  \human{What does atelectasis mean?}
  \reference{Atelectasis refers to the partial collapse or incomplete inflation of the lung. In this X-ray, mild left basal atelectasis is suspected at the base of the left lung.}
\end{minipage}
\vspace{2mm}
\caption{ \textbf{Example of LLM-generated conversations within the instruction dataset}. LLM-generated user-assistant interactions designed for instruction tuning in RadVLM, covering both standard conversations and grounded responses.  \textbf{(a) Standard conversation}: The assistant responds to user queries based on textual attributes extracted from the CXR (e.g., report findings, categorical labels) without explicit spatial references. \textbf{(b) Conversation with grounding:} In addition to textual responses, the assistant provides spatial grounding by referencing anatomical structures with bounding box coordinates. These synthetic interactions are generated by conditioning a text-based LLM on CXR attributes (report, labels, bounding boxes) and prompting it to simulate multi-turn diagnostic dialogues.}
\label{fig:conversation}
\end{figure}


\subsection{Model finetuning}
\label{section:finetuning}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\linewidth]{images/finetuning} % Replace with your image file
    \caption{ \textbf{Instruction fine-tuning of the vision-language model.} The CXR image is processed by the vision encoder, and the question is supplied at the language decoder. The flame icons indicate that the vision encoder, adapter, and LLM are all jointly fine-tuned end-to-end, generating the answer through next-token prediction.}
    \label{fig:finetuning} % Replace with a unique label for referencing
\end{figure}

We leverage an existing vision-language backbone, LLaVA-OneVision-7B \citep{li2024llava}. Its architecture is based on the SigLIP vision encoder \citep{zhai2023sigmoid} connected to the language model qwen-2 \citep{yang2024qwen2} via a 2-layer multi-layer perceptron (MLP). It was originally pretrained and instruction-tuned on image-text datasets in the general domain. We also follow the Higher AnyRes strategy \citep{chai2022any} by encoding multiple patches of the input image in different resolutions (in addition to the full image) and feeding the concatenated output representation from the vision encoder to the language model. \citet{chai2022any} showed that the scaling in image resolution is successful in the general domain. 

We fine-tune the LLaVA-OneVision-7B architecture on our instruction dataset by optimizing an auto-regressive loss on the target assistant tokens, $\mathbf{x}_{a}$. Specifically, for each token $x_i$ in the assistant's output sequence, we model
\begin{align}
    p(\mathbf{x}_a \mid \mathbf{x}_v, \mathbf{x}_q) 
    &= \prod_{i=1}^{L} p(x_i \mid \mathbf{x}_v, \mathbf{x}_{q,<i}, \mathbf{x}_{a,<i})~, 
\label{eqn:next-word-prediction}
\end{align}
where $\mathbf{x}_v$ denotes the visual tokens, and $\mathbf{x}_{q,<i}$ and $\mathbf{x}_{a,<i}$ represent the question and assistant tokens preceding token $x_i$, respectively. Here, $L$ is the number of tokens in the target assistant sequence.
This formulation also applies for the multi-turn conversations, where $\mathbf{x}_{q,<i}$ and $\mathbf{x}_{a,<i}$ include the chat history from previous rounds. Following recent trends in visual instruction tuning \citep{li2024llava, laurencon2024building}, the whole architecture is trained, using a learning rate of $2\text{e-}6$ in the vision encoder weights and $1\text{e-}5$ in the 2-layer MLP and language model weights. RadVLM is trained over 1 epoch of the instruction dataset using full-fine-tuning. We use 128 GH GPUs, each with 96GB of memory \citep{fusco2024understanding} for approximately 12 hours.  

For the ablation studies, we follow the same fine-tuning procedure but limit the process to a subset of the instruction dataset, focusing on isolated types of tasks.




%


%%%%%%%%%%%%%%%%%% EXPERIMENTS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\newpage

\section{Experiments \& Results}

In this section, we describe our evaluation pipeline, the existing baseline models we use for comparison,  report results and highlight the capabilities of our RadVLM system.

\subsection{Evaluation pipeline}

In order to assess the quantitative performance of RadVLM, we design an evaluation pipeline based on the individual tasks from our instruction dataset. This pipeline leverages existing metrics for report generation, abnormality classification and visual grounding and creates novel evaluation tasks to assess the model's performance in conversational abilities. 

\subsubsection{Report generation}

To evaluate generated reports, we employ both lexical and radiology-specific metrics. Lexical metrics particularly quantify word overlap between generated and ground truth reports, among which we report BertScore, a metric for text generation based on computing token similarity using contextual
embeddings \citep{zhang2020BERTScore}, and Rouge-L \citep{lin2004rouge}, which quantifies the length of the longest common subsequence between predicted and reference reports. 

In contrast with lexical metrics, radiology-specific metrics ignore irrelevant variations in phrasing and focus on the clinically relevant semantics of the generated text, such as the presence or absence of an abnormality. In particular, we provide results for the RadGraph F1 and GREEN metrics.

\noindent
\textbf{RadGraph F1} \citep{delbrouck2022improving, yu2023evaluating}: computing this metric requires to map each generated and ground truth reports to a structured graph, named RadGraph \citep{jain2021radgraph}, containing radiology-specific entities (anatomy or observations) and the relations between them (``suggestive of'', ``located at''). The RadGraph F1 score \citep{delbrouck2022improving} computes the overlap in inferred structured graphs (extracted entities and relations) from both generated and ground truth reports. In our study, we use the recently released RadGraph-XL model to extract graphical representations \citep{delbrouck2024radgraph} and report the partial reward \citep[described in][]{delbrouck2022improving}. 

\noindent
\textbf{GREEN} \citep[Generative Radiology Report Evaluation and Error Notation,][]{ostmeier2024green}: a recently developed report generation metric that leverages the LLM-as-Judge mechanism of language models \citep{zheng2023judging} to identify clinically significant errors in generated radiology reports \citep{Yu2023-rt,calamida2023radiology,Calamida2024-le}, and that highly aligns with expert preferences as compared to other LLM-based evaluations (e.g., GPT-4).


\subsubsection{Abnormality classification}
We use the test split of the CheXpert dataset and prompt the model by asking to list the abnormalities present on the CXR. The mentioned abnormalities in the model's answer are extracted via key-word matching and compared to the ground truth list.
We calculate the F1 score for the 14 abnormalities and report the macro-averaged F1-score over all categories. 



\subsubsection{Visual grounding}

We assess the model's performance in visual grounding by prompting the model to detect specific features (anatomy, abnormality, phrase) and extract the bounding box coordinates generated within the model's answer. Although we included the abnormality detection task in the instruction set, we do not evaluate it here due to the lack of comparable models for this task.


For the three grounding tasks, we use mean Average Precision (mAP) at an Intersection Over Union (IoU) threshold of 0.5. At this setting, we evaluate both precision and recall for predicted boxes that overlap with ground truth by at least 50\%. 





\subsubsection{Multi-turn evaluation within conversational interactions}
\label{section:conversation-eval}

Evaluating conversational aspects of a model is essential to assess the utility and performance of an AI assistant in a multi-turn setting. We designed an LLM-based evaluation method carefully crafted for conversations, following the ``LLM-as-judge'' scheme \citep{zheng2023judging} previously adopted by LLaVA-Med \citep{li2023llava-med}. We created a test set of conversations following our generation process (see \cref{section:conversation}). For this test set, we used GPT-4o instead of GPT-4o-mini (which was used for the training set) to ensure a higher quality of generated conversations, ensuring that the computed performance metrics accurately reflect the VLM's capabilities rather than limitations in the test set. As as result, we obtained  157 conversations containing grounding questions (derived from MS-CXR test set), and 523 conversations without (\cref{table:instruction-dataset-content}).  

During the evaluation process, we provide the CXR image to the VLM and sequentially ask the questions from this dataset. After collecting the VLM's answer to each question, we prompt GPT-4o (text-only) with the CXR information (report, list of abnormalities, etc.), the expected answer per question (derived from the ground truth information), and the VLM-generated answer (see \cref{supp_fig:eval_conv_prompt} for the prompt). At the end of the prompt,  GPT-4o is asked to provide an overall score (from 0 to 10) for the quality and accuracy of generated answers as compared to ground truth. We report two scores: one based on the grounded conversations dataset and one based on the non-grounded (standard) one. 


 
 \subsection{Baseline models}
 \label{section:baseline-models}


 \begin{table}[h]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\begin{tabular}{p{2cm}C{1cm}C{2cm}C{2cm}C{2cm}C{2cm}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \textbf{Size} & \textbf{Report} & \textbf{Classification} & \textbf{Grounding} & \textbf{Conversation} \\ \hline
LLaVA-OV   & 7B  & \cmark & \xmark & \xmark & \cmark \\
LLaVA-Med  & 7B & \cmark & \xmark & \xmark & \cmark \\
RaDialog   & 7B & \cmark & \cmark & \xmark & \cmark \\
CheXagent  & 3B & \cmark & \cmark & \cmark & \xmark \\
MAIRA-2    & 13B & \cmark & \xmark & \cmark & \xmark \\ 
\textbf{RadVLM} & 7B & \cmark & \cmark & \cmark & \cmark \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Functional capabilities of RadVLM and baseline models.} Comparison of RadVLM with baseline models in terms of parameter size and task coverage across report generation, abnormality classification, visual grounding and conversational interactions.}
\label{table:baseline-models}
\end{table}



To provide insights into the performance of RadVLM in multiple tasks, we compare the instruction-tuned RadVLM to existing and publicly available state-of-the-art VLMs (from 3B to 13B, \cref{table:baseline-models}), adjusting the evaluation so that each baseline model is only tested on tasks for which it was initially trained on. Notably, in the general domain, we evaluate LLaVA-OneVision-7B \citep{li2024llava}, also used as our training starting point (\cref{section:finetuning}). In the medical domain, we evaluate LLaVA-Med \citep{li2023llava-med}, trained on a large-scale biomedical dataset (including CXRs). For CXR-specific VLMs, we use CheXagent (trained on a wide range of tasks), RaDialog (possessing conversation skills), and MAIRA-2 \citep{Bannur2024-ek}, specialized in report generation with visual grounding. We summarize the tasks on which each model is evaluated on in \cref{table:baseline-models}, and provide links to the original repositories from which the model weights were obtained in \cref{supptable:baseline-models}.


\subsection{Results}
\subsubsection{Report generation}

\begin{table}[t]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\begin{tabular}{l>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}>{\centering\arraybackslash}p{1.8cm}}
\toprule
 & \multicolumn{2}{c}{\textbf{NLG Metrics (\%)}} & \multicolumn{2}{c}{\textbf{Clinical Metrics (\%)}} \\ \cline{2-5}
 & BertScore & Rouge-L & RadGraph F1 & GREEN \\ \midrule
LLaVA-OV    & 35.5 & 14.6 & 7.2  & 11.9\\
LLaVA-Med   & 30.1 & 12.6 & 4.7  & 5.0 \\
RaDialog    & 49.7 & 22.4 & 16.3 & 24.4 \\
CheXagent   & 38.1 & \underline{22.9} & \textbf{20.9} & \textbf{28.5} \\
MAIRA-2     & \underline{47.9} & 18.5 & 13.5 & 21.4 \\ \midrule
RadVLM      & \textbf{51.0} & \textbf{24.3} & \underline{17.7} & \underline{24.9} \\ \bottomrule
\end{tabular}
\caption{\textbf{Performance of RadVLM and baseline models on single-image report generation.} NLG (Natural Language Generation) and clinical metrics are calculated on the filtered test set of MIMIC-CXR. Bold values indicate the best performance, while underlined values represent the second-best performance across models.}
\label{table:report-generation}
\end{table}



Firstly, we evaluate RadVLM on a core radiology task: generating a textual report from a frontal CXR. As our evaluation set, we use a filtered version of the MIMIC-CXR test set, excluding statements about findings from prior examinations. We report in \cref{table:report-generation} both lexical (BertScore, Rouge-L) and clinical (RadGraph F1, GREEN) metrics, which capture different performance aspects of the report generation task.

As mentioned in \cref{section:baseline-models}, five additional VLMs -- including three specific to CXR (RaDialog, CheXagent, MAIRA-2) -- are also evaluated, with minor adaptations to each model's recommended prompt template. Many of these models were originally evaluated under different conditions (sometimes benefiting from extra inputs like prior images/reports or patient details or omitting certain metrics such as GREEN). To ensure that our setup remains consistent across all of them, we apply the same evaluation pipeline (test set and metrics).

For non-CXR specific VLMs, we observe a poor performance in both lexical and clinical metrics, with an unexpected improved performance for the generalist model (LLaVA-OneVision) over the medical one (LLaVA-Med), presumably profiting from a better architecture and training process \citep{li2024llava}. CXR-specific models perform significantly better than generalists, with a notable advantage of CheXagent in terms of clinical metrics. While MAIRA-2 was shown to perform optimally in the report generation task \citep{Bannur2024-ek}, it seems to perform worse when a single image is provided. Overall, RadVLM achieves competitive results, attaining the highest performance in lexical metrics and the second-best in clinical metrics. This validates our approach, even though our training methodology was not specifically designed for the report generation task.



\subsubsection{Abnormality classification}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/classif} % Replace with your image file
    \vspace{-5mm}
    \caption{\textbf{F1 scores for abnormality classification across different models.} Classification performance of \textcolor{gray}{\textbf{RadVLM (grey)}}, \textcolor{dandelion}{\textbf{RaDialog (yellow)}}, and \textcolor{cornflowerblue}{\textbf{CheXagent (blue)}}. Bars represent the F1 scores for individual pathology categories, while dashed lines indicate the macro-averaged F1 score across all categories. The macro-averaged F1 scores for CheXagent and RaDialog overlap.}
    \label{fig:classif} % Replace with a unique label for referencing
\end{figure}

 
In this section, we assess each model's ability to predict which abnormalities are visible on the CXR.  We use the manually curated CheXpert test set, prompt RadVLM to list any observed abnormalities, and compare them against the ground truth. As for the report generation task, we adapt the prompt for each compared baseline model (RaDialog, CheXagent) so they output a full list of abnormalities. Notably, RaDialog can identify pathologies from the same prompt as RadVLM, whereas CheXagent needs all possible labels explicitly listed in its prompt. 
 
\cref{fig:classif} shows the individual F1 scores for the 14 CheXpert labels as well as the macro-averaged F1 across all categories (dashed lines in \cref{fig:classif}). Our findings suggest that RadVLM better captures which abnormalities appear on the CXR, reflected in its higher macro-F1 score. In particular, it shows improved classification for crucial pathologies such as atelectasis, edema, fracture, lung lesion, lung opacity, pneumonia, pleural effusion, and pneumothorax. While CheXagent showed strong classification results under binary or restricted-label evaluations \citep{chen2024chexagent}, it struggles when tasked with identifying all categories at once (see \cref{fig:classif}).
 
Overall, these results indicate that RadVLM performs equally or better than its counterparts in abnormality classification. This step is especially crucial for the grounding process since identifying the relevant pathology is a prerequisite to localizing it on the CXR.
 


\subsubsection{Visual grounding}

In this section, we evaluate RadVLM's visual grounding capabilities, which could help clinicians localize specific regions or pathologies on a CXR. This is particularly useful once a pathology has already been identified -- either by a radiologist's input or through our previously described AI tasks -- since it allows one to pinpoint exactly where the abnormality appears on the image.

We report performance metrics for the three main grounding tasks RadVLM was trained on: anatomical grounding using the Chest Imagenome test set, abnormality grounding using the VinDr-CXR test set, and phrase grounding using the MS-CXR test set (\cref{table:instruction-dataset-content}). For each task, we use mean Average Precision (mAP) as our primary evaluation metric.

As mentioned in \cref{table:baseline-models}, some of the CXR-specific VLMs already have grounding capabilities. CheXagent was trained to handle both abnormality and phrase grounding tasks, while MAIRA-2 -- originally trained to produce radiology reports with grounded observations -- is also capable of predicting bounding box coordinates when provided with input text (see corresponding model card in \cref{supptable:baseline-models}). After retrieving each model's instruction template for generating bounding box coordinates, we evaluated both CheXagent and MAIRA-2 on all three grounding tasks performed by RadVLM.

Our results show that RadVLM performs well at localizing anatomical regions (e.g., ``right lung'', ``aortic arch'', illustrated in \cref{fig:grounding-results}a), achieving a mAP of 85.3 \%, by far surpassing the other CXR grounding models (\cref{table:grounding-results}). This advantage is partly explained by including the Chest Imagenome dataset (and thus the anatomical grounding task) in the training set, which CheXagent and MAIRA-2 did not leverage. However, it remains a key feature for any grounding model to possess a fine-grained understanding of CXR anatomy. 

For the abnormality grounding task, RadVLM is less consistent (\cref{fig:grounding-results}b), likely due to higher sparsity of abnormality locations and labels, yet it still achieves best performance (\cref{table:grounding-results}).  For the phrase grounding task, while MAIRA-2 and CheXagent demonstrate great performance, RadVLM surpasses them with a mAP of 82.8\% (\cref{table:grounding-results}), presumably benefiting from the newly released PadChest-GR dataset \citep{castro2024padchest} used for training. 

Overall, these results show that our instruction tuning strategy for visual grounding (covering three essential tasks), combined to a modern VLM backbone, offers a promising avenue to help clinicians localize anatomical and pathological features during a CXR exam. Furthermore, providing fine-grained details within an LLM-generated output may also enhance the ability to answer grounded questions in a multi-turn setting, as we explore next.



\begin{figure}[t]
\centering
{\footnotesize
% Left panel
\begin{minipage}[t]{0.48\linewidth}
  \vspace{0pt}
  \textbf{a. Anatomical grounding}
  \human{Where is the [region] located on the image?
    \vspace{1mm}
  }
  \assistant{The [region] is located at \textcolor{red}{[coordinates]} on the image. 
  \vspace{2mm}
  
    \begin{minipage}{\linewidth}
      \centering
      % First row of images using the new command
      \regionimage{images/region_grounding/cardiac_silhouette}{cardiac \\[-1mm] silhouette }\hfill
      \regionimage{images/region_grounding/cavoatrial_junction}{cavoatrial \\[-1mm] junction}\hfill
      \regionimage{images/region_grounding/right_hilar_structures}{right hilar \\[-1mm] structures}\hfill
      \regionimage{images/region_grounding/left_lung}{left\\[-1mm] lung}
      \\ \vspace{1mm} % Space between rows 
      % Second row of images using the new command
      \regionimage{images/region_grounding/right_lung}{right lung}\hfill
      \regionimage{images/region_grounding/left_upper_abdomen}{left upper \\[-1mm] abdomen}\hfill
      \regionimage{images/region_grounding/upper_mediastinum}{upper \\[-1mm] mediastinum}\hfill
      \regionimage{images/region_grounding/aortic_arch}{aortic \\[-1mm] arch}
    \end{minipage}
    \vspace{2mm}
  }
  \centering
\end{minipage}%
\hfill
% Right panel
\begin{minipage}[t]{0.48\linewidth}
  \vspace{0pt}
  \textbf{b. Abnormality grounding}
  \human{Locate the [abnormality] on the CXR.
   \vspace{1mm}
  }
  \assistant{The [abnormality] is positioned at the coordinates \textcolor{red}{[coordinates]} on the CXR.
  \vspace{2mm}
  
    \begin{minipage}{\linewidth}
      \centering
      % First row of images placeholder
      \regionimage{images/abnormality_grounding/cardiomegaly}{cardiomegaly}\hfill
      \regionimage{images/abnormality_grounding/calcification}{calcification}\hfill
      \regionimage{images/abnormality_grounding/pleural_thickening}{pleural \\[-1mm] thickening}\hfill
      \regionimage{images/abnormality_grounding/atelectasis}{atelectasis}
      \\[1mm] % Space between rows
      % Second row of images placeholder
      \regionimage{images/abnormality_grounding/pulmonary_fibrosis}{pulmonary \\[-1mm] fibrosis}\hfill
      \regionimage{images/abnormality_grounding/aortic_enlargement}{aortic \\[-1mm] enlargement}\hfill
      \regionimage{images/abnormality_grounding/consolidation}{consolidation}\hfill
      \regionimage{images/abnormality_grounding/ILD}{interstitial \\[-1mm] lung disease}
    \end{minipage}
    \vspace{2mm}
  }
  \centering
\end{minipage}
}
\vspace{4mm}
\caption{\textbf{CXR region and abnormality grounding with RadVLM.} Examples of RadVLM's grounding predictions for \textbf{(a) anatomical regions} and \textbf{(b) abnormalities} in CXR images. The model predicts bounding boxes indicating the location of queried structures or pathological findings. Green boxes represent ground truth annotations, while red boxes denote model-predicted bounding boxes.}
    \label{fig:grounding-results} % Replace with a unique label for referencing
\end{figure}



\begin{table}[t]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\begin{tabular}{p{1.5cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}>{\centering\arraybackslash}p{2cm}}
\toprule
\multicolumn{1}{c}{} & \textbf{Anatomical grounding}  & \textbf{Abnormality grounding} & \textbf{Phrase grounding}  \\ \midrule
CheXagent & 6.2  & \underline{26.0} & 69.7 \\
MAIRA-2  & \underline{19.8} & 11.3 & \underline{80.1} \\ \midrule
\textbf{RadVLM} & \textbf{85.3} & \textbf{35.7} & \textbf{82.8}\\ 
\bottomrule
\end{tabular}
\caption{\textbf{Visual grounding performance measured in mAP (\%)}. Mean average precision (mAP) scores for anatomical grounding, abnormality grounding, and phrase grounding tasks across different models. Bounding box coordinates are extracted from each model's output, and mAP is computed using an IoU threshold of 50\%. \textbf{Bold} values indicate the highest performance, while \underline{underlined} values represent the second-best performance.}
\label{table:grounding-results}
\end{table}





\subsubsection{Conversational abilities}

Our model can already handle multiple advanced tasks for CXR interpretation. However, we have only tested its ability to follow instructions in a single-turn, question-answer format so far. An essential aspect of a VLM's performance is also its capacity to manage follow-up questions in a multi-turn exchange with the user. Simply generating a full report or marking image features in one shot does not necessarily imply that the model can handle a varied sequence of precise questions -- covering observations, locations, clarifications of medical terms, and more.



RadVLM's instruction dataset also includes image-conversation pairs (\cref{fig:conversation}) generated with GPT-4o-mini, by providing it with a radiology report, any available grounded phrases, and additional details. As mentioned in \cref{section:conversation-eval}, we created two held-out evaluation sets using GPT-4o: one without grounded questions and one focused on grounded questions derived from the MS-CXR test set (\cref{table:instruction-dataset-content}). We run the evaluation on both test sets and report the average score across all samples. We also evaluate the other baseline models that possess conversational abilities, thoroughly respecting their prompting template within a multi-turn setting. The average GPT-4o scores for RadVLM and other models are gathered in \cref{table:eval_conv}. 


Our results show that RadVLM, trained on a broad range of image-conversation pairs, achieves an average score of 6.95/10, substantially higher than the other conversational models, whose responses often prove incorrect or vague (\cref{fig:eval-conversation}). This advantage may stem from RadVLM's ability to handle varied question types and sequences in multi-turn exchanges, a skill reinforced by the sparse nature of its training conversations. The gap becomes even more pronounced in grounded scenarios, where RadVLM maintains strong performance (6.55/10) while others drop even more. Notably, this suggests that even the limited number of grounded conversations included (\cref{table:instruction-dataset-content}) was sufficient to equip RadVLM with robust grounding capabilities in a multi-turn setting.






\begin{table}[t]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\begin{tabular}{>{\centering\arraybackslash}p{2.8cm} >{\centering\arraybackslash}p{2.3cm} >{\centering\arraybackslash}p{2.3cm}}
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \textbf{GPT-4o Score} & \textbf{GPT-4o Score} w/ grounding \\ \midrule
\multicolumn{1}{l}{LLaVA-OV}   & 4.66 & 3.85 \\
\multicolumn{1}{l}{LLaVA-Med}  & 4.01 & 3.43  \\
\multicolumn{1}{l}{RaDialog}   & \underline{4.57} & \underline{4.09}  \\ \midrule
\multicolumn{1}{l}{\textbf{RadVLM}}  & \textbf{6.95} & \textbf{6.55} \\ 
\bottomrule
\end{tabular}
\caption{\textbf{Conversation performance on standard and grounded interactions.} GPT-4o evaluation scores for conversational performance across models, comparing generated responses to expected answers derived from image-based attributes (e.g., reports, bounding boxes, and view type). Scores are reported for standard conversations and grounded conversations, where models must provide spatially aware responses. \textbf{Bold} values indicate the best performance, while \underline{underlined} values represent the second-best performance.}
\label{table:eval_conv}
\end{table}



\begin{figure}[h!]
\footnotesize % Reduce font size within the figure
  \human{What type of view is the X-ray taken in?
  \vspace{1mm} % Extra space above image (you can adjust or remove this if needed)
  
  \roundimage{images/conv_image_evaluation}{0.1}
  }
  \reference{The X-ray is taken in an anteroposterior (AP) view.}
  \assistant{\textbf{This Chest X-ray is taken in an anteroposterior (AP) view.}}
  \radialog{This image was taken from the AP view.}
  \llavamed{The X-ray is taken in a posteroanterior (PA) view.}
  
  \human{Can you tell me if there's anything unusual with the lung bases?}
  \reference{Yes, there are heterogeneous opacities observed at the lung bases.}
    \assistant{\textbf{Yes, there is patchy consolidation at the lung bases, which refers to areas where the lung tissue appears more solid than normal.}}
    \radialog{Based on the analysis of the chest x-ray image, there are no visible abnormalities in the lung bases. The lung bases appear normal and there are no signs of any abnormalities or disease.}
    \llavamed{Yes, the chest X-ray shows bilateral infiltrates in the lung bases. Bilateral infiltrates refer to the presence of abnormal substances or fluid in both lungs, which can be seen as areas of increased density or opacity on the X-ray image (...)}
    
  \human{Is there any evidence of a pneumothorax in this image?}
  \reference{Yes, a small right apical pneumothorax is present.}
  \assistant{\textbf{Yes, there is a small right apical pneumothorax, which means there is air in the pleural space on the right side.}}
  \radialog{No, there is no evidence of a pneumothorax. The chest x-ray image shows no signs of a pneumothorax, which is a collection of air in the space between the lung and the chest wall.}
  \llavamed{ There is no evidence of a pneumothorax in this chest x-ray. a pneumothorax is a condition where air accumulates in the pleural space, causing the lung to collapse.}
  
  \human{Where is the pneumothorax located on the image?}
  \reference{It is located in the right apical region with bounding box coordinates [0.14, 0.10, 0.45, 0.19].}
  \assistant{\textbf{The pneumothorax is located at coordinates [0.11, 0.10, 0.43, 0.27] on the image.}}
  \radialog{The pneumothorax is located in the right lung.}
  \llavamed{The pneumothorax is located in the right lung base, as indicated by the arrow in the image.}
  
  
  \human{ Is there any type of support device visible on the X-ray?}
  \reference{Yes, a right pigtail pleural catheter is in place.}
  \assistant{\textbf{Yes, there is a right pigtail pleural catheter visible on the X-ray.}}
  \radialog{No, there is no evidence of support devices on the x-ray image.}
  \llavamed{Yes, there is a chest tube visible on the X-ray. A chest tube is a flexible plastic tube that is inserted through the chest wall (...)}
  
\vspace{5mm}

\caption{\textbf{Example of multi-turn interaction with RadVLM and other conversational VLMs.} Comparison of expected answers from the \textcolor{pastelred}{\textbf{conversation dataset} (red bubbles)} to generated answers from \textcolor{gray}{\textbf{RadVLM} (grey bubble)}, \textcolor{dandelion}{\textbf{RaDialog} (yellow bubble)}, and  \textcolor{pastelpurple}{\textbf{LLaVA-Med} (purple bubble)}.}
\label{fig:eval-conversation}
\end{figure}



\subsection{Ablation studies}






In this section, we investigate a fundamental question in LLM fine-tuning: do models perform individual tasks more effectively when they are trained separately, or does joint training on multiple tasks provide a greater overall benefit? To address this question, we perform ablation studies where we train separate models -- each dedicated to a single task (e.g., report generation, classification) -- using the same training settings and hyperparameters as RadVLM, then evaluate their performance on that respective task.

Our results reveal that RadVLM generally outperforms models trained on individual tasks. First, this difference is pronounced in grounding performance, particularly for tasks with smaller training sets (e.g., phrase grounding; see \cref{table:instruction-dataset-content}).  We attribute this to the complementary nature of the various grounding tasks, which collectively improve the model's ability to localize visual elements (including overlapping categories, such as ``abnormalities'' also mentioned in ``phrases'') and prevent overfitting. 

A similar trend emerges in the conversation scores, where RadVLM outperforms a model trained solely on the conversation dataset. While this ablated model still performs significantly better than its baseline backbone -- surpassing LLaVA-OneVision's score in \cref{table:baseline-models} -- RadVLM leverages knowledge from the other tasks, such as report generation, that contains a higher amount of data points. This presumably enables it to generalize more effectively to new questions from the conversation test set. 

Report generation is the only task that turns out equivalent when trained in isolation. One likely reason is the high amount of training data for this task, and the richness of information textual reports contain over other tasks, which might suffice for the model to generalize over new instances.  

\begin{table}[h]
\centering
\footnotesize % Set font size to small
\renewcommand{\arraystretch}{1.5} % Increase row height for readability
\begin{tabular}{lcccccccc}
    \toprule
   \multirow{2}{*}{\vspace{-17mm}\textbf{Training tasks}} & \multicolumn{8}{c}{\vspace{1mm}\textbf{Evaluation tasks}} \\ 
   & \parbox{1.8cm}{\centering Report generation} 
   & \parbox{1.6cm}{\centering Classification} 
   & \parbox{1.6cm}{\centering Anatomical grounding} 
   & \parbox{1.6cm}{\centering Abnormality grounding} 
   & \parbox{1.6cm}{\centering Phrase grounding}
   & \parbox{1.8cm}{\centering Conversation (standard/ grounded)} \\ \cline{2-8}
   & RadGraph/GREEN & F1 (macro) (\%)&mAP (\%)&mAP (\%)&mAP (\%)&GPT4o-Score \\ \midrule
Report generation       & 17.7 / \textbf{25.4} & -   & -   & -   & -   & -   \\  
Classification          & -       & 41.5 & -   & -   & -   & -    \\  
Anatomical grounding    & -       & -   & 83.0 & -   & -   & -    \\  
Abnormality grounding   & -       & -   & -   & 27.1 & -   & -    \\  
Phrase grounding        & -       & -   & -   & -   & 64.6 & -    \\  
Conversation            & -       & -   & -   & -   & -   & 6.28 / 5.66  \\  
All (RadVLM)                     & 17.7 / 24.9 & \textbf{48.4} & \textbf{85.3} & \textbf{35.7} & \textbf{82.8} & \textbf{6.95} / \textbf{6.55}  \\ \bottomrule
\end{tabular}
\caption{\textbf{Ablation studies comparing single-task models to RadVLM.}  Each ablated model is trained on only one task from RadVLM’s instruction dataset and evaluated on that same task. We compare these results to RadVLM’s previously reported performance over all tasks. }
\
\end{table}


\newpage


\section{Conclusions}

There is a growing need for medical AI tools that are both accurate and user-friendly, and can interact with clinicians in a natural, multi-turn conversation. In this study, inspired by advances in both general and CXR-specific vision-language models, we explored the potential of constructing an assistant that not only aims for strong performance in individual tasks but also for handling them in a multi-turn, interactive setting. To achieve this, we built an instruction dataset that integrates both stand-alone tasks and multi-task dialogues, then fine-tuned a VLM backbone on it. Our findings suggest that this method performs well and offers promising insights for designing future radiology assistants. 

One key contribution of this study is our consistent re-implementation and evaluation of existing models (alongside RadVLM) under a unified experimental framework. In an era where AI research is proliferating rapidly, and new models are introduced almost daily, it can be challenging to compare results that rely on different setups. By systematically re-implementing models and replicating experiments within the same context, we enhance both reproducibility and reliability. This standardized approach aligns with initiatives such as the ReXrank leaderboard for CXR report generation \citep{zhang2024rexrank}, which similarly evaluates state-of-the-art models using a consistent protocol. Ultimately, these efforts provide a clearer understanding of each model's true capabilities and help researchers identify the most promising directions for future work. 

Another key takeaway is the flexibility of VLMs in handling diverse data types, ranging from free-text reports and classification labels to fine-grained information, all within a multi-turn, ``real-life'' scenario. Traditionally, each task might have been tackled by distinct, specialized architectures -- e.g., separate object detectors \citep{sun2022research} or classifiers \citep{he2016deep} -- but our findings support the recent trend, employed in both general and CXR-specific visual modeling  \citep{lin2024vila, wang2024qwen2vl, sharma2024maira, chen2024chexagent}, where many data formats can now be tokenized and processed through a common next-token generation paradigm. Our ablation studies further show that while each task differs in purpose, they collectively enhance the model's overall understanding of a CXR image, an effect likely amplified by training them together. Even though multi-agent approaches emerge in the medical setting \citep{schmidgall2024agentclinic}, these results support the value of joint training for developing comprehensive, single-agent solutions. 

The simplicity of RadVLM's training strategy also sheds light on potential directions for future CXR-based modeling. Here, we built upon LLaVA-OneVision \citep{li2024llava}, a VLM already pretrained for instruction following, and fine-tuned it on our curated dataset for just a single epoch. Although other studies have employed more intricate pipelines, such as pre-training the vision encoder, adapter and LLM components successively \citep{chaves2024llavarad, chen2024chexagent, Bannur2024-ek}, Given the competitive performance of RadVLM with other models, we hypothesize that the quality of the instruction dataset, coupled with the backbone architecture \citep[as shown by][]{laurenccon2024matters} are the most critical for the overall performance of such a visual foundation model. That said, for specific tasks like report generation, specialized solutions that incorporate additional patient details, prior and lateral images, or extended training, could offer greater gains. This likely explains why our model does not surpass the state of the art for that particular task. From this, we envision a future where AI assistants are bifurcated into \emph{(i)} non-interactive, report-focused tools designed for experienced radiologists, and \emph{(ii)} more adaptable, conversational agents that clinicians or radiology students can easily query in a multi-turn context. 

As radiology datasets grow in both scale and quality, and as vision-language modeling advances at a rapid pace, AI-driven tools are becoming ever more reliable and valuable for clinical practice. With RadVLM, we aim to contribute to this vibrant field by introducing a method that draws on prior work yet incorporates original ideas toward developing a multitask conversational vision-language model for radiology. 


\section{Acknowledgments}
This work was supported as part of the Swiss AI Initiative by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID a02 on Alps, and by the LOOP Zurich as part of the application driver project supporting the LOOP Zurich Biomedical Informatics Platform (BMIP). ND and FN received research support from the Digitalization Initiative of the Zurich Higher Education Institutions (DIZH)- Rapid Action Call - under TRUST-RAD project. CB received research support from the Promedica Foundation, Chur, CH.
TS is supported by the grant \#2021-911 of the Strategic Focal Area “Personalized Health and Related Technologies (PHRT)” of the ETH Domain (Swiss Federal Institutes of Technology). HM, MN and KF are supported by JSPS KAKENHI (Grant  Number: 23KK0148).
AR is supported by the StimuLoop grant \#1-007811-002 and the Vontobel Foundation. MV and SL are supported by the Swiss State Secretariat for Education, Research, and Innovation (SERI) under contract number MB22.00047. MK is supported by the UZH Global Strategy and Partnerships Funding Scheme and a Research Partnership Grant with China, Japan, South Korea and the ASEAN region (RPG 072023\_18).



\bibliographystyle{apalike} % or any other style you prefer
\bibliography{main.bib} % Replace with your bibliography file

\newpage
\appendix

\section{Appendix}




\begin{suppfigure}[ht]
    \centering
    % Create the speech bubble using TikZ without the gray fill
    \begin{tikzpicture}
    
        \node[draw, rounded corners, text width=0.8\textwidth, inner sep=10pt] (bubble) {
        \small
I will provide you with some visual information about a chest X-ray (information that you cannot see) and ask you to create a 5-10 round Q\&A conversation between a user and an AI assistant. The assistant should act as if it can visually inspect and interpret the X-ray. Please follow these guidelines:

\begin{itemize}
\item The assistant should focus on the visual details that can be reasonably inferred from the X-ray image alone.
\item  The user's questions should be diverse and related to the image (appearance, observations, possible abnormalities or bounding box coordinates). Do not hallucinate information outside what I provide you.
\item  If coordinates for any visual findings are given, ensure they follow the provided format $[x_{min}, y_{min}, x_{max}, y_{max}]$. It should invite the assistant to answer about it, like  ``Where is [observation] located on the image?''
\item  The assistant's answers should be concise, natural, and reflect only what can be seen. Avoid overconfident medical conclusions, and provide cautious, responsible explanations when necessary. You can also provide brief explanations of medical terms (if asked)
\item  Both questions and answers should feel like a realistic conversation while remaining focused on the image.
\item Do not include any extra text or comments outside of the final JSON structure.
 \end{itemize}
            
           
\textbf{Information provided about the X-ray:}
\begin{itemize}
	\item \textbf{Radiology report:}  There is a right-sided PICC line. There are left-sided chest tubes. There is a small left apical pneumothorax. There is consolidation at the left lung base with prominence of the pulmonary vascular markings throughout the left lung. The right lung appears relatively clear. No pneumothorax on the right side is seen. There is aeration of the atelectasis at the right lung base.
	\item \textbf{List of Abnormalities:} Atelectasis, Consolidation, Pneumothorax, Support Devices
	\item \textbf{View:} AP
	\item \textbf{Gender:} F
	\item \textbf{Selected observations with bounding boxes coordinates:} small left apical pneumothorax: [0.59, 0.18, 0.77, 0.24]
\end{itemize}

\textbf{Conversation in expected format:} [conversation]

        };
    \end{tikzpicture}
 

    \caption{\textbf{Prompting LLM to generate conversation data.}}
    \label{supp_fig:conv_prompt}
\end{suppfigure}



\begin{supptable}[t]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.5} % Adjust row height for readability
\begin{tabular}{|c|l|}
\hline
\textbf{Model} & \textbf{Link to Repository} \\ \hline
LLaVA-OV   & \url{https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-si-hf} \\
LLaVA-Med  & \url{https://github.com/microsoft/LLaVA-Med} \\
RaDialog   & \url{https://huggingface.co/ChantalPellegrini/RaDialog-interactive-radiology-report-generation} \\
CheXagent  & \url{https://huggingface.co/StanfordAIMI/CheXagent-2-3b} \\
MAIRA-2    & \url{https://huggingface.co/microsoft/maira-2} \\ 
\textbf{RadVLM} & \url{https://huggingface.co/KrauthammerLab/RadVLM} \\ \hline
\end{tabular}
\caption{Link to repository for each model's weights}
\label{supptable:baseline-models}
\end{supptable}



\begin{suppfigure}[ht]
    \centering
    % Create the speech bubble using TikZ without the gray fill
    \begin{tikzpicture}
        \node[draw, rounded corners, text width=0.8\textwidth, inner sep=10pt] (bubble) {
            \small

You are an evaluator for a vision-language model. The model answers multi-round questions based on some reference data (referred to as ``provided data''). For each round in the conversation, you have:
\begin{itemize}
	\item User's question
	\item Expected answer (if the model had access to the data)
	\item Generated answer (from the model that does not have direct access to the data, but only to the image)
\end{itemize}
Your task is to:
\begin{itemize}
	\item Evaluate each generated answer individually, briefly comparing it to the expected answer in terms of correctness, completeness, and relevance.
	\item Assign a small ``per-question'' rating or a short note indicating how close the generated answer is to the expected answer.
	\item After analyzing all rounds, compute a single final numeric score (0 to 10) that reflects the model's overall performance across the entire conversation.
\end{itemize}

Output the final numeric score in the following format only:
Overall score: [score].
Do not include any additional text, explanations, or commentary in your final output beyond this line.
\medskip

\textbf{Provided Data:}
\begin{itemize}
    \item List of Abnormalities: Edema, Pleural Effusion, Support Devices
    \item Radiology Report: Port-A-Cath catheter tip is at the level of the right ventricular outflow tract. There is substantial distension of the stomach. What appears to be a ureteral stent is partially imaged. Bilateral pleural effusions, right more than left, are present. Vascular congestion is noted, borderline with mild interstitial pulmonary edema.
    \item View: AP
    \item Gender: F
    \item Selected observations with bounding boxes coordinates: [grounded-phrases]
\end{itemize}

\textbf{Conversation to Evaluate:}
\begin{itemize}
    \item \textcolor{blue}{User: What do you see in this X-ray image?}
    
    \textcolor{red}{Expected answer: I see an AP view chest X-ray of a female patient. There appear to be bilateral pleural effusions, more prominent on the right side, and some vascular congestion. } 
    
    \textcolor{gray}{Generated answer: I see a chest X-ray image in the AP view. There are some abnormalities present in the lungs, including an area of increased opacity in the right lung and a small area of increased opacity in the left lung.}
   
   \item \textcolor{blue}{User: Can you describe the cardiac silhouette?}
   
   \textcolor{red}{Expected answer: (...)}

\end{itemize}

\textbf{Overall score:} [score]
           

        };
    \end{tikzpicture}
 

    \caption{\textbf{Prompting LLM to evaluate conversation inference of a VLM}}
    \label{supp_fig:eval_conv_prompt}
\end{suppfigure}





% \begin{table}[h]
% \centering
% \footnotesize
% \renewcommand{\arraystretch}{1.5} % Adjust row height for readability
% \begin{tabular}{|c|c c:c c | c c:c c:c c|}
% \hline
% & \multicolumn{4}{c|}{\textbf{NLG Metrics (\%)}} & \multicolumn{6}{c|}{\textbf{Clinical Metrics (\%)}} \\ \cline{2-11}
%   & \multicolumn{2}{c}{BS} & \multicolumn{2}{c|}{Rouge-L} & \multicolumn{2}{c}{RadGraph F1} & \multicolumn{2}{c}{CE (Macro)} & \multicolumn{2}{c|}{CE (Micro)} \\ \cline{2-11}
% LLaVA-OV          & 34.8 &   35.5    & 15.4 &  14.6     & 7.0  &  7.2   & 9.9 & 9.4     & 14.4 & 14.6 \\
% LLaVA-Med         & 29.0 & 30.1     & 12.8 & 12.6     & 4.3   & 4.7    & 12.6 & 12.0     & 17.7 & 18.2 \\
% RaDialog          & 48.9 & -     & 22.5 & -     & 18.5 & -    & 20.5 & -     & 32.7 & - \\
% CheXagent         & 37.1 & -     & 21.6 & -     & 22.1 & -    & 39.7 & -     & 56.1 & - \\
% MAIRA-2           & 44.6   & 47.9     & 16.8 &  18.5    & 14.0 &  13.5   & 37.2 & 35.3     & 53.3 &  53.0 \\ \hline
% \textbf{RadVLM}   & 49.7 & 55.6  & 23.1 & 29.8  & 19.4 &20.6 & 30.4 &30.0 & 49.0 &48.7 \\ \hline
% \end{tabular}
% \caption{NLG and Clinical Metrics Performance of Different Models (Orig vs. Filtered)}
% \end{table}













\end{document}