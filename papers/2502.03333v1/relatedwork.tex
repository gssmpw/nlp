\section{Related works}
\subsection{Instruction tuning and vision-language models}

The advent of autoregressive large language models (LLMs) based on the transformer architecture ~\citep{vaswani2017attention} and pre-trained on vast text corpora~\citep{radford2019language, brown2020language} has provided the possibility to perform a wide range of language-based downstream tasks. However, the widespread success and accessibility of LLMs, such as ChatGPT, are largely attributed to the instruction-tuning process \citep{wei2021finetuned, ouyang2022training}. This process commonly involves fine-tuning a pre-trained model on a labeled dataset of diverse instruction-following tasks, ensuring the model can generalize to diverse user instructions in a zero-shot setting. 

Instruction-following datasets generally consist of instruction-output pairs and/or multi-turn dialogues \citep{zheng2023lmsys} mimicking real-life interaction between users and AI assistants. While early instruction datasets were manually crafted \citep{wei2021finetuned}, a more scalable approach leverages larger LLMs to generate synthetic instruction data \citep{wang2022self, peng2023instruction, liu2023visual}, reducing annotation costs. 

Beyond the text-only tasks, state-of-the-art proprietary LLMs, such as GPT-4 \citep{achiam2023gpt}, DeepSeek \citep{liu2024deepseek, guo2025deepseek}, and Gemini \citep{team2023gemini} exhibit advanced vision capabilities, enabling them to process and respond to multimodal instructions. In parallel, open research efforts have led to the development of vision-language models such as LLaVA \citep{liu2023visual} and BLIP-2 \citep{li2023blip}, which introduced effective training strategies for visual instruction tuning. These approaches have inspired the development of vision-language models (VLMs) such as LLaVA-OneVision \citep{li2024llava}, Idefics3 \citep{laurencon2024building}, Qwen2-VL \citep{wang2024qwen2vl}, and Llama-3.2 Vision \citep{dubey2024llama}. Similar to text-based LLMs, the instruction-following datasets contain userâ€“assistant Q\&A and dialogues, but each example is paired with an image, and the instructions and responses explicitly reflect the image's content \citep{feng2022mmdialog}. 


\subsection{Vision-language models in radiology}

 The success of VLMs in the general domain has spurred the development of medical-based VLMs, particularly in domains where image-based interpretation is critical. Proprietary models such as Med-PaLM\citep{singhal2023large} and Med-Gemini \citep{saab2024capabilities} have shown remarkable performance across a range of multimodal medical tasks, including medical visual question answering (VQA), report generation, summarization. In parallel, open source models such as LLaVA-Med \citep{li2023llava-med} have been developed following similar training strategies as LLaVA \citep{liu2023visual}, leveraging biomedical datasets from PubMed \citep{pubmed} to design instruction prompts and muti-turn conversations. 

Among medical applications, CXR interpretation remains a key area of interest. Early AI-driven models primarily focus on report generation \citep{nooralahzadeh2021progressive, alfarghaly2021automated, tanida2023interactive, chaves2024llavarad}, supported by the development of clinically relevant evaluation metrics \citep{jain2021radgraph, yu2023evaluating}. More recently, research has expanded toward multimodal, multitask CXR assistants capable of integrating multiple functionalities beyond report generation, such as classification, grounding or image generation. Notable examples include CheXagent \citep{chen2024chexagent} or RoentGen \citep{bluethgen2024vision}, though these models lack conversational capabilities. 

Other approaches, such as Wolf \citep{kang2024wolf}, RaDialog  \citep{pellegrini2023radialog}, and M4CXR \citep{chen2024chexagent}, incorporate conversational features but are constrained by predefined response templates, limiting their adaptability in real-world interactions. In this work, we introduce a model that integrates multiple CXR interpretation tasks while enabling flexible,  multi-turn dialogue, bridging the gap between task-specific AI models and interactive clinical assistants.