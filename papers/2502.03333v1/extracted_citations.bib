@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{alfarghaly2021automated,
  title={Automated radiology report generation using conditioned transformers},
  author={Alfarghaly, Omar and Khaled, Rana and Elkorany, Abeer and Helal, Maha and Fahmy, Aly},
  journal={Informatics in Medicine Unlocked},
  volume={24},
  pages={100557},
  year={2021},
  publisher={Elsevier}
}

@article{bluethgen2024vision,
  title={A vision--language foundation model for the generation of realistic chest x-ray images},
  author={Bluethgen, Christian and Chambon, Pierre and Delbrouck, Jean-Benoit and van der Sluijs, Rogier and Po{\l}acin, Ma{\l}gorzata and Zambrano Chaves, Juan Manuel and Abraham, Tanishq Mathew and Purohit, Shivanshu and Langlotz, Curtis P and Chaudhari, Akshay S},
  journal={Nature Biomedical Engineering},
  pages={1--13},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@misc{chaves2024llavarad,
      title={Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation}, 
      author={Juan Manuel Zambrano Chaves and Shih-Cheng Huang and Yanbo Xu and Hanwen Xu and Naoto Usuyama and Sheng Zhang and Fei Wang and Yujia Xie and Mahmoud Khademi and Ziyi Yang and Hany Awadalla and Julia Gong and Houdong Hu and Jianwei Yang and Chunyuan Li and Jianfeng Gao and Yu Gu and Cliff Wong and Mu Wei and Tristan Naumann and Muhao Chen and Matthew P. Lungren and Akshay Chaudhari and Serena Yeung-Levy and Curtis P. Langlotz and Sheng Wang and Hoifung Poon},
      year={2024},
      eprint={2403.08002},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08002}, 
}

@misc{chen2024chexagent,
      title={A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation}, 
      author={Zhihong Chen and Maya Varma and Justin Xu and Magdalini Paschali and Dave Van Veen and Andrew Johnston and Alaa Youssef and Louis Blankemeier and Christian Bluethgen and Stephan Altmayer and Jeya Maria Jose Valanarasu and Mohamed Siddig Eltayeb Muneer and Eduardo Pontes Reis and Joseph Paul Cohen and Cameron Olsen and Tanishq Mathew Abraham and Emily B. Tsai and Christopher F. Beaulieu and Jenia Jitsev and Sergios Gatidis and Jean-Benoit Delbrouck and Akshay S. Chaudhari and Curtis P. Langlotz},
      year={2024},
      eprint={2401.12208},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.12208}, 
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{feng2022mmdialog,
  title={Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation},
  author={Feng, Jiazhan and Sun, Qingfeng and Xu, Can and Zhao, Pu and Yang, Yaming and Tao, Chongyang and Zhao, Dongyan and Lin, Qingwei},
  journal={arXiv preprint arXiv:2211.05719},
  year={2022}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{jain2021radgraph,
  title={Radgraph: Extracting clinical entities and relations from radiology reports},
  author={Jain, Saahil and Agrawal, Ashwin and Saporta, Adriel and Truong, Steven QH and Duong, Du Nguyen and Bui, Tan and Chambon, Pierre and Zhang, Yuhao and Lungren, Matthew P and Ng, Andrew Y and others},
  journal={arXiv preprint arXiv:2106.14463},
  year={2021}
}

@article{kang2024wolf,
  title={WoLF: Large Language Model Framework for CXR Understanding},
  author={Kang, Seil and Kim, Donghyun and Kim, Junhyeok and Lee, Hyo Kyung and Hwang, Seong Jae},
  journal={arXiv preprint arXiv:2403.15456},
  year={2024}
}

@misc{laurencon2024building,
      title={Building and better understanding vision-language models: insights and future directions.}, 
      author={Hugo Laurençon and Andrés Marafioti and Victor Sanh and Léo Tronchon},
      year={2024},
      eprint={2408.12637},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{li2023llava-med,
  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2306.00890},
  year={2023}
}

@article{li2024llava,
  title={LLaVA-OneVision: Easy Visual Task Transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@inproceedings{nooralahzadeh2021progressive,
    title = "Progressive Transformer-Based Generation of Radiology Reports",
    author = "Nooralahzadeh, Farhad  and
      Perez Gonzalez, Nicolas  and
      Frauenfelder, Thomas  and
      Fujimoto, Koji  and
      Krauthammer, Michael",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.241/",
    doi = "10.18653/v1/2021.findings-emnlp.241",
    pages = "2824--2832",
    abstract = "Inspired by Curriculum Learning, we propose a consecutive (i.e., image-to-text-to-text) generation framework where we divide the problem of radiology report generation into two steps. Contrary to generating the full radiology report from the image at once, the model generates global concepts from the image in the first step and then reforms them into finer and coherent texts using transformer-based architecture. We follow the transformer-based sequence-to-sequence paradigm at each step. We improve upon the state-of-the-art on two benchmark datasets."
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{pellegrini2023radialog,
  title={RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance},
  author={Pellegrini, Chantal and {\"O}zsoy, Ege and Busam, Benjamin and Navab, Nassir and Keicher, Matthias},
  journal={arXiv preprint arXiv:2311.18681},
  year={2023}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@misc{pubmed,
  author    = {NIH},
  title     = {PubMed},
  year = {n.d.},
  url       = {https://pubmed.ncbi.nlm.nih.gov/},
  note      = {https://pubmed.ncbi.nlm.nih.gov/,  Accessed: 2024-12-22}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{saab2024capabilities,
  title={Capabilities of gemini models in medicine},
  author={Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and others},
  journal={arXiv preprint arXiv:2404.18416},
  year={2024}
}

@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}

@inproceedings{tanida2023interactive,
  title={Interactive and Explainable Region-guided Radiology Report Generation},
  author={Tanida, Tim and M{\"u}ller, Philip and Kaissis, Georgios and Rueckert, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7433--7442},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{yu2023evaluating,
  title={Evaluating progress in automatic chest x-ray radiology report generation},
  author={Yu, Feiyang and Endo, Mark and Krishnan, Rayan and Pan, Ian and Tsai, Andy and Reis, Eduardo Pontes and Fonseca, Eduardo Kaiser Ururahy Nunes and Lee, Henrique Min Ho and Abad, Zahra Shakeri Hossein and Ng, Andrew Y and others},
  journal={Patterns},
  volume={4},
  number={9},
  year={2023},
  publisher={Elsevier}
}

@article{zheng2023lmsys,
  title={Lmsys-chat-1m: A large-scale real-world llm conversation dataset},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and others},
  journal={arXiv preprint arXiv:2309.11998},
  year={2023}
}

