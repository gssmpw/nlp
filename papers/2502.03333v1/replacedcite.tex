\section{Related works}
\subsection{Instruction tuning and vision-language models}

The advent of autoregressive large language models (LLMs) based on the transformer architecture ____ and pre-trained on vast text corpora____ has provided the possibility to perform a wide range of language-based downstream tasks. However, the widespread success and accessibility of LLMs, such as ChatGPT, are largely attributed to the instruction-tuning process ____. This process commonly involves fine-tuning a pre-trained model on a labeled dataset of diverse instruction-following tasks, ensuring the model can generalize to diverse user instructions in a zero-shot setting. 

Instruction-following datasets generally consist of instruction-output pairs and/or multi-turn dialogues ____ mimicking real-life interaction between users and AI assistants. While early instruction datasets were manually crafted ____, a more scalable approach leverages larger LLMs to generate synthetic instruction data ____, reducing annotation costs. 

Beyond the text-only tasks, state-of-the-art proprietary LLMs, such as GPT-4 ____, DeepSeek ____, and Gemini ____ exhibit advanced vision capabilities, enabling them to process and respond to multimodal instructions. In parallel, open research efforts have led to the development of vision-language models such as LLaVA ____ and BLIP-2 ____, which introduced effective training strategies for visual instruction tuning. These approaches have inspired the development of vision-language models (VLMs) such as LLaVA-OneVision ____, Idefics3 ____, Qwen2-VL ____, and Llama-3.2 Vision ____. Similar to text-based LLMs, the instruction-following datasets contain userâ€“assistant Q\&A and dialogues, but each example is paired with an image, and the instructions and responses explicitly reflect the image's content ____. 


\subsection{Vision-language models in radiology}

 The success of VLMs in the general domain has spurred the development of medical-based VLMs, particularly in domains where image-based interpretation is critical. Proprietary models such as Med-PaLM____ and Med-Gemini ____ have shown remarkable performance across a range of multimodal medical tasks, including medical visual question answering (VQA), report generation, summarization. In parallel, open source models such as LLaVA-Med ____ have been developed following similar training strategies as LLaVA ____, leveraging biomedical datasets from PubMed ____ to design instruction prompts and muti-turn conversations. 

Among medical applications, CXR interpretation remains a key area of interest. Early AI-driven models primarily focus on report generation ____, supported by the development of clinically relevant evaluation metrics ____. More recently, research has expanded toward multimodal, multitask CXR assistants capable of integrating multiple functionalities beyond report generation, such as classification, grounding or image generation. Notable examples include CheXagent ____ or RoentGen ____, though these models lack conversational capabilities. 

Other approaches, such as Wolf ____, RaDialog  ____, and M4CXR ____, incorporate conversational features but are constrained by predefined response templates, limiting their adaptability in real-world interactions. In this work, we introduce a model that integrates multiple CXR interpretation tasks while enabling flexible,  multi-turn dialogue, bridging the gap between task-specific AI models and interactive clinical assistants.