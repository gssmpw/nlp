@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{delbrouck2022improving,
  title={Improving the factual correctness of radiology report generation with semantic rewards},
  author={Delbrouck, Jean-Benoit and Chambon, Pierre and Bluethgen, Christian and Tsai, Emily and Almusa, Omar and Langlotz, Curtis P},
  journal={arXiv preprint arXiv:2210.12186},
  year={2022}
}

@inproceedings{delbrouck2024radgraph,
  title={Radgraph-xl: A large-scale expert-annotated dataset for entity and relation extraction from radiology reports},
  author={Delbrouck, Jean-Benoit and Chambon, Pierre and Chen, Zhihong and Varma, Maya and Johnston, Andrew and Blankemeier, Louis and Van Veen, Dave and Bui, Tan and Truong, Steven and Langlotz, Curtis},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={12902--12915},
  year={2024}
}

@article{zhang2024rexrank,
  title={ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation},
  author={Zhang, Xiaoman and Zhou, Hong-Yu and Yang, Xiaoli and Banerjee, Oishi and Acosta, Juli{\'a}n N and Miller, Josh and Huang, Ouwen and Rajpurkar, Pranav},
  journal={arXiv preprint arXiv:2411.15122},
  year={2024}
}

@book{rontgen1895ueber,
  title={Ueber eine neue Art von Strahlen},
  author={R{\"o}ntgen, Wilhelm Conrad},
  year={1895},
  publisher={Phys.-med. Gesellschaft}
}
@inproceedings{kim2023boosting,
    title = "Boosting Radiology Report Generation by Infusing Comparison Prior",
    author = "Kim, Sanghwan  and
      Nooralahzadeh, Farhad  and
      Rohanian, Morteza  and
      Fujimoto, Koji  and
      Nishio, Mizuho  and
      Sakamoto, Ryo  and
      Rinaldi, Fabio  and
      Krauthammer, Michael",
    editor = "Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin",
    booktitle = "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.bionlp-1.4/",
    doi = "10.18653/v1/2023.bionlp-1.4",
    pages = "50--61",
    abstract = "Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains; these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams. This discrepancy can be attributed to a knowledge gap between radiologists and the generation models. While radiologists possess patient-specific prior information, the models solely receive X-ray images at a specific time point. To tackle this issue, we propose a novel approach that leverages a rule-based labeler to extract comparison prior information from radiology reports. This extracted comparison prior is then seamlessly integrated into state-of-the-art transformer-based models, enabling them to produce more realistic and comprehensive reports. Our method is evaluated on English report datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses baseline models in terms of natural language generation metrics. Notably, our model generates reports that are free from false references to non-existent prior exams, setting it apart from previous models. By addressing this limitation, our approach represents a significant step towards bridging the gap between radiologists and generation models in the domain of medical report generation."
}
@article{bluethgen2024vision,
  title={A vision--language foundation model for the generation of realistic chest x-ray images},
  author={Bluethgen, Christian and Chambon, Pierre and Delbrouck, Jean-Benoit and van der Sluijs, Rogier and Po{\l}acin, Ma{\l}gorzata and Zambrano Chaves, Juan Manuel and Abraham, Tanishq Mathew and Purohit, Shivanshu and Langlotz, Curtis P and Chaudhari, Akshay S},
  journal={Nature Biomedical Engineering},
  pages={1--13},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{schmidgall2024agentclinic,
  title={AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments},
  author={Schmidgall, Samuel and Ziaei, Rojin and Harris, Carl and Reis, Eduardo and Jopling, Jeffrey and Moor, Michael},
  journal={arXiv preprint arXiv:2405.07960},
  year={2024}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{sharma2024maira,
  title={MAIRA-Seg: Enhancing Radiology Report Generation with Segmentation-Aware Multimodal Large Language Models},
  author={Sharma, Harshita and Salvatelli, Valentina and Srivastav, Shaury and Bouzid, Kenza and Bannur, Shruthi and Castro, Daniel C and Ilse, Maximilian and Bond-Taylor, Sam and Ranjit, Mercy Prasanna and Falck, Fabian and others},
  journal={arXiv preprint arXiv:2411.11362},
  year={2024}
}

@article{wang2024qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}



@article{bustos2020padchest,
  title={Padchest: A large chest x-ray image dataset with multi-label annotated reports},
  author={Bustos, Aurelia and Pertusa, Antonio and Salinas, Jose-Maria and De La Iglesia-Vaya, Maria},
  journal={Medical image analysis},
  volume={66},
  pages={101797},
  year={2020},
  publisher={Elsevier}
}

@article{castro2024padchest,
  title={PadChest-GR: A Bilingual Chest X-ray Dataset for Grounded Radiology Report Generation},
  author={Castro, Daniel C and Bustos, Aurelia and Bannur, Shruthi and Hyland, Stephanie L and Bouzid, Kenza and Wetscherek, Maria Teodora and S{\'a}nchez-Valverde, Maria Dolores and Jaques-P{\'e}rez, Lara and P{\'e}rez-Rodr{\'\i}guez, Lourdes and Takeda, Kenji and others},
  journal={arXiv preprint arXiv:2411.05085},
  year={2024}
}

@article{ostmeier2024green,
  title={GREEN: Generative Radiology Report Evaluation and Error Notation},
  author={Ostmeier, Sophie and Xu, Justin and Chen, Zhihong and Varma, Maya and Blankemeier, Louis and Bluethgen, Christian and Michalson, Arne Edward and Moseley, Michael and Langlotz, Curtis and Chaudhari, Akshay S and others},
  journal={arXiv preprint arXiv:2405.03595},
  year={2024}
}

@inproceedings{boecking2022making,
  title={Making the most of text semantics to improve biomedical vision--language processing},
  author={Boecking, Benedikt and Usuyama, Naoto and Bannur, Shruthi and Castro, Daniel C and Schwaighofer, Anton and Hyland, Stephanie and Wetscherek, Maria and Naumann, Tristan and Nori, Aditya and Alvarez-Valle, Javier and others},
  booktitle={European conference on computer vision},
  pages={1--21},
  year={2022},
  organization={Springer}
}

@inproceedings{chai2022any,
  title={Any-resolution training for high-resolution image synthesis},
  author={Chai, Lucy and Gharbi, Michael and Shechtman, Eli and Isola, Phillip and Zhang, Richard},
  booktitle={European Conference on Computer Vision},
  pages={170--188},
  year={2022},
  organization={Springer}
}

@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11975--11986},
  year={2023}
}

@article{peng2022radiologist,
  title={Radiologist burnout: trends in medical imaging utilization under the national health insurance system with the universal code bundling strategy in an academic tertiary medical centre},
  author={Peng, Yan-Chih and Lee, Wen-Jeng and Chang, Yeun-Chung and Chan, Wing P and Chen, Shyh-Jye},
  journal={European journal of radiology},
  volume={157},
  pages={110596},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2016}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016},
  publisher={IEEE}
}

@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}

@article{tu2024towards,
  title={Towards generalist biomedical AI},
  author={Tu, Tao and Azizi, Shekoofeh and Driess, Danny and Schaekermann, Mike and Amin, Mohamed and Chang, Pi-Chuan and Carroll, Andrew and Lau, Charles and Tanno, Ryutaro and Ktena, Ira and others},
  journal={NEJM AI},
  volume={1},
  number={3},
  pages={AIoa2300138},
  year={2024},
  publisher={Massachusetts Medical Society}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{zhou2024generalist,
  title={A Generalist Learner for Multifaceted Medical Image Interpretation},
  author={Zhou, Hong-Yu and Adithan, Subathra and Acosta, Juli{\'a}n Nicol{\'a}s and Topol, Eric J and Rajpurkar, Pranav},
  journal={arXiv preprint arXiv:2405.07988},
  year={2024}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@misc{laurencon2024building,
      title={Building and better understanding vision-language models: insights and future directions.}, 
      author={Hugo Laurençon and Andrés Marafioti and Victor Sanh and Léo Tronchon},
      year={2024},
      eprint={2408.12637},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{tiu2022expert,
  title={Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning},
  author={Tiu, Ekin and Talius, Ellie and Patel, Pujan and Langlotz, Curtis P and Ng, Andrew Y and Rajpurkar, Pranav},
  journal={Nature Biomedical Engineering},
  volume={6},
  number={12},
  pages={1399--1406},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{sheng2024barlowtwins,
  title={BarlowTwins-CXR: enhancing chest X-ray abnormality localization in heterogeneous data with cross-domain self-supervised learning},
  author={Sheng, Haoyue and Ma, Linrui and Samson, Jean-Fran{\c{c}}ois and Liu, Dianbo},
  journal={BMC Medical Informatics and Decision Making},
  volume={24},
  number={1},
  pages={126},
  year={2024},
  publisher={Springer}
}

@inproceedings{sun2022research,
  title={Research on chest abnormality detection based on improved YOLOv7 algorithm},
  author={Sun, Kai Xiang and Cong, Chao},
  booktitle={2022 IEEE international conference on bioinformatics and biomedicine (BIBM)},
  pages={3884--3886},
  year={2022},
  organization={IEEE}
}


@article{you2023ferret,
  title={Ferret: Refer and ground anything anywhere at any granularity},
  author={You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  journal={arXiv preprint arXiv:2310.07704},
  year={2023}
}

@article{zhang2024ferret,
  title={Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models},
  author={Zhang, Haotian and You, Haoxuan and Dufter, Philipp and Zhang, Bowen and Chen, Chen and Chen, Hong-You and Fu, Tsu-Jui and Wang, William Yang and Chang, Shih-Fu and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2404.07973},
  year={2024}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{li2024multimodal,
  title={Multimodal foundation models: From specialists to general-purpose assistants},
  author={Li, Chunyuan and Gan, Zhe and Yang, Zhengyuan and Yang, Jianwei and Li, Linjie and Wang, Lijuan and Gao, Jianfeng and others},
  journal={Foundations and Trends{\textregistered} in Computer Graphics and Vision},
  volume={16},
  number={1-2},
  pages={1--214},
  year={2024},
  publisher={Now Publishers, Inc.}
}

@article{zheng2023lmsys,
  title={Lmsys-chat-1m: A large-scale real-world llm conversation dataset},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and others},
  journal={arXiv preprint arXiv:2309.11998},
  year={2023}
}

@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26689--26699},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{feng2022mmdialog,
  title={Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation},
  author={Feng, Jiazhan and Sun, Qingfeng and Xu, Can and Zhao, Pu and Yang, Yaming and Tao, Chongyang and Zhao, Dongyan and Lin, Qingwei},
  journal={arXiv preprint arXiv:2211.05719},
  year={2022}
}




@inproceedings{xue2015foreign,
  title={Foreign object detection in chest X-rays},
  author={Xue, Zhiyun and Candemir, Sema and Antani, Sameer and Long, L Rodney and Jaeger, Stefan and Demner-Fushman, Dina and Thoma, George R},
  booktitle={2015 IEEE international conference on bioinformatics and biomedicine (BIBM)},
  pages={956--961},
  year={2015},
  organization={IEEE}
}


@article{muller2024chex,
  title={ChEX: Interactive Localization and Region Description in Chest X-rays},
  author={M{\"u}ller, Philip and Kaissis, Georgios and Rueckert, Daniel},
  journal={arXiv preprint arXiv:2404.15770},
  year={2024}
}

@article{solovyev2021weighted,
  title={Weighted boxes fusion: Ensembling boxes from different object detection models},
  author={Solovyev, Roman and Wang, Weimin and Gabruseva, Tatiana},
  journal={Image and Vision Computing},
  volume={107},
  pages={104117},
  year={2021},
  publisher={Elsevier}
}

@article{smit2020chexbert,
  title={CheXbert: combining automatic labelers and expert annotations for accurate radiology report labeling using BERT},
  author={Smit, Akshay and Jain, Saahil and Rajpurkar, Pranav and Pareek, Anuj and Ng, Andrew Y and Lungren, Matthew P},
  journal={arXiv preprint arXiv:2004.09167},
  year={2020}
}

@misc{chambon2024chexpertplusaugmentinglarge,
      title={CheXpert Plus: Augmenting a Large Chest X-ray Dataset with Text Radiology Reports, Patient Demographics and Additional Image Formats}, 
      author={Pierre Chambon and Jean-Benoit Delbrouck and Thomas Sounack and Shih-Cheng Huang and Zhihong Chen and Maya Varma and Steven QH Truong and Chu The Chuong and Curtis P. Langlotz},
      year={2024},
      eprint={2405.19538},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.19538}, 
}

@article{park2024m4cxr,
  title={M4CXR: Exploring Multi-task Potentials of Multi-modal Large Language Models for Chest X-ray Interpretation},
  author={Park, Jonggwon and Kim, Soobum and Yoon, Byungmu and Hyun, Jihun and Choi, Kyoyun},
  journal={arXiv preprint arXiv:2408.16213},
  year={2024}
}




@article{kang2024wolf,
  title={WoLF: Large Language Model Framework for CXR Understanding},
  author={Kang, Seil and Kim, Donghyun and Kim, Junhyeok and Lee, Hyo Kyung and Hwang, Seong Jae},
  journal={arXiv preprint arXiv:2403.15456},
  year={2024}
}


@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}



@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{hyland2023maira,
  title={Maira-1: A specialised large multimodal model for radiology report generation},
  author={Hyland, Stephanie L and Bannur, Shruthi and Bouzid, Kenza and Castro, Daniel C and Ranjit, Mercy and Schwaighofer, Anton and P{\'e}rez-Garc{\'\i}a, Fernando and Salvatelli, Valentina and Srivastav, Shaury and Thieme, Anja and others},
  journal={arXiv preprint arXiv:2311.13668},
  year={2023}
}

@inproceedings{xue2018multimodal,
  title={Multimodal recurrent model with attention for automated radiology report generation},
  author={Xue, Yuan and Xu, Tao and Rodney Long, L and Xue, Zhiyun and Antani, Sameer and Thoma, George R and Huang, Xiaolei},
  booktitle={Medical Image Computing and Computer Assisted Intervention--MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I},
  pages={457--466},
  year={2018},
  organization={Springer}
}


@inproceedings{nooralahzadeh2021progressive,
    title = "Progressive Transformer-Based Generation of Radiology Reports",
    author = "Nooralahzadeh, Farhad  and
      Perez Gonzalez, Nicolas  and
      Frauenfelder, Thomas  and
      Fujimoto, Koji  and
      Krauthammer, Michael",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.241/",
    doi = "10.18653/v1/2021.findings-emnlp.241",
    pages = "2824--2832",
    abstract = "Inspired by Curriculum Learning, we propose a consecutive (i.e., image-to-text-to-text) generation framework where we divide the problem of radiology report generation into two steps. Contrary to generating the full radiology report from the image at once, the model generates global concepts from the image in the first step and then reforms them into finer and coherent texts using transformer-based architecture. We follow the transformer-based sequence-to-sequence paradigm at each step. We improve upon the state-of-the-art on two benchmark datasets."
}
@article{alfarghaly2021automated,
  title={Automated radiology report generation using conditioned transformers},
  author={Alfarghaly, Omar and Khaled, Rana and Elkorany, Abeer and Helal, Maha and Fahmy, Aly},
  journal={Informatics in Medicine Unlocked},
  volume={24},
  pages={100557},
  year={2021},
  publisher={Elsevier}
}

@article{fusco2024understanding,
  title={Understanding data movement in tightly coupled heterogeneous systems: A case study with the Grace Hopper superchip},
  author={Fusco, Luigi and Khalilov, Mikhail and Chrapek, Marcin and Chukkapalli, Giridhar and Schulthess, Thomas and Hoefler, Torsten},
  journal={arXiv preprint arXiv:2408.11556},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{papineni2002bleu,
  title={{BLEU}: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{zhang2020BERTScore,
    title={BERTScore: Evaluating Text Generation with BERT},
    author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@article{li2024llava,
  title={LLaVA-OneVision: Easy Visual Task Transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@misc{chaves2024llavarad,
      title={Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation}, 
      author={Juan Manuel Zambrano Chaves and Shih-Cheng Huang and Yanbo Xu and Hanwen Xu and Naoto Usuyama and Sheng Zhang and Fei Wang and Yujia Xie and Mahmoud Khademi and Ziyi Yang and Hany Awadalla and Julia Gong and Houdong Hu and Jianwei Yang and Chunyuan Li and Jianfeng Gao and Yu Gu and Cliff Wong and Mu Wei and Tristan Naumann and Muhao Chen and Matthew P. Lungren and Akshay Chaudhari and Serena Yeung-Levy and Curtis P. Langlotz and Sheng Wang and Hoifung Poon},
      year={2024},
      eprint={2403.08002},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08002}, 
}

@article{saab2024capabilities,
  title={Capabilities of gemini models in medicine},
  author={Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and others},
  journal={arXiv preprint arXiv:2404.18416},
  year={2024}
}

@article{yang2024advancing,
  title={Advancing multimodal medical capabilities of Gemini},
  author={Yang, Lin and Xu, Shawn and Sellergren, Andrew and Kohlberger, Timo and Zhou, Yuchen and Ktena, Ira and Kiraly, Atilla and Ahmed, Faruk and Hormozdiari, Farhad and Jaroensri, Tiam and others},
  journal={arXiv preprint arXiv:2405.03162},
  year={2024}
}

@article{laurenccon2024matters,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={arXiv preprint arXiv:2405.02246},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}


@article{yu2023evaluating,
  title={Evaluating progress in automatic chest x-ray radiology report generation},
  author={Yu, Feiyang and Endo, Mark and Krishnan, Rayan and Pan, Ian and Tsai, Andy and Reis, Eduardo Pontes and Fonseca, Eduardo Kaiser Ururahy Nunes and Lee, Henrique Min Ho and Abad, Zahra Shakeri Hossein and Ng, Andrew Y and others},
  journal={Patterns},
  volume={4},
  number={9},
  year={2023},
  publisher={Elsevier}
}


@misc{chen2024internvl,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks}, 
      author={Zhe Chen and Jiannan Wu and Wenhai Wang and Weijie Su and Guo Chen and Sen Xing and Muyan Zhong and Qinglong Zhang and Xizhou Zhu and Lewei Lu and Bin Li and Ping Luo and Tong Lu and Yu Qiao and Jifeng Dai},
      year={2024},
      eprint={2312.14238},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}


@article{zhang2023pmc,
  title={Pmc-vqa: Visual instruction tuning for medical visual question answering},
  author={Zhang, Xiaoman and Wu, Chaoyi and Zhao, Ziheng and Lin, Weixiong and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2305.10415},
  year={2023}
}



@article{chen2022toward,
  title={Toward expanding the scope of radiology report summarization to multiple anatomies and modalities},
  author={Chen, Zhihong and Varma, Maya and Wan, Xiang and Langlotz, Curtis and Delbrouck, Jean-Benoit},
  journal={arXiv preprint arXiv:2211.08584},
  year={2022}
}

@article{demner2012design,
  title={Design and development of a multimodal biomedical information retrieval system},
  author={Demner-Fushman, Dina and Antani, Sameer and Simpson, Matthew and Thoma, George R},
  journal={Journal of Computing Science and Engineering},
  volume={6},
  number={2},
  pages={168--177},
  year={2012},
  publisher={Demner-Fushman Dina; Antani Sameer; Simpson Matthew; Thoma George R.}
}

@inproceedings{xue2015foreign,
  title={Foreign object detection in chest X-rays},
  author={Xue, Zhiyun and Candemir, Sema and Antani, Sameer and Long, L Rodney and Jaeger, Stefan and Demner-Fushman, Dina and Thoma, George R},
  booktitle={2015 IEEE international conference on bioinformatics and biomedicine (BIBM)},
  pages={956--961},
  year={2015},
  organization={IEEE}
}

@article{jaeger2014two,
  title={Two public chest X-ray datasets for computer-aided screening of pulmonary diseases},
  author={Jaeger, Stefan and Candemir, Sema and Antani, Sameer and W{\'a}ng, Y{\`\i}-Xi{\'a}ng J and Lu, Pu-Xuan and Thoma, George},
  journal={Quantitative imaging in medicine and surgery},
  volume={4},
  number={6},
  pages={475},
  year={2014},
  publisher={AME Publications}
}


@misc{bannur2023mscxr,
  title={MSCXR-T: Learning to exploit temporal structure for biomedical vision-language processing},
  author={Bannur, Shruthi and Hyland, Stephanie and Liu, Qianchu and P{\'e}rez-Garc{\'\i}a, Fernando and Ilse, Max and de Castro, Daniel Coelho and Boecking, Benedikt and Sharma, Harshita and Bouzid, Kenza and Schwaighofer, Anton and others},
  year={2023},
  publisher={PhysioNet}
}


@inproceedings{pellegrini2023rad,
  title={Rad-restruct: A novel vqa benchmark and method for structured radiology reporting},
  author={Pellegrini, Chantal and Keicher, Matthias and {\"O}zsoy, Ege and Navab, Nassir},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={409--419},
  year={2023},
  organization={Springer}
}


@article{lau2018dataset,
  title={A dataset of clinically generated visual questions and answers about radiology images},
  author={Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina},
  journal={Scientific data},
  volume={5},
  number={1},
  pages={1--10},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{pham2023pedicxr,
  title={PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children},
  author={Pham, Hieu H and Nguyen, Ngoc H and Tran, Thanh T and Nguyen, Tuan NM and Nguyen, Ha Q},
  journal={Scientific Data},
  volume={10},
  number={1},
  pages={240},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{nguyen2022vindr,
  title={VinDr-CXR: An open dataset of chest X-rays with radiologists{'} annotations},
  author={Nguyen, Ha Q and Lam, Khanh and Le, Linh T and Pham, Hieu H and Tran, Dat Q and Nguyen, Dung B and Le, Dung D and Pham, Chi M and Tong, Hang TT and Dinh, Diep H and others},
  journal={Scientific Data},
  volume={9},
  number={1},
  pages={429},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{shih2019augmenting,
  title={Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia},
  author={Shih, George and Wu, Carol C and Halabi, Safwan S and Kohli, Marc D and Prevedello, Luciano M and Cook, Tessa S and Sharma, Arjun and Amorosa, Judith K and Arteaga, Veronica and Galperin-Aizenberg, Maya and others},
  journal={Radiology: Artificial Intelligence},
  volume={1},
  number={1},
  pages={e180041},
  year={2019},
  publisher={Radiological Society of North America}
}

@inproceedings{pelka2018radiology,
  title={Radiology Objects in COntext (ROCO): a multimodal image dataset},
  author={Pelka, Obioma and Koitka, Sven and R{\"u}ckert, Johannes and Nensa, Felix and Friedrich, Christoph M},
  booktitle={Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3},
  pages={180--189},
  year={2018},
  organization={Springer}
}


@inproceedings{kayser2022explaining,
  title={Explaining chest x-ray pathologies in natural language},
  author={Kayser, Maxime and Emde, Cornelius and Camburu, Oana-Maria and Parsons, Guy and Papiez, Bartlomiej and Lukasiewicz, Thomas},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={701--713},
  year={2022},
  organization={Springer}
}

@article{bae2024ehrxqa,
  title={EHRXQA: A multi-modal question answering dataset for electronic health records with chest x-ray images},
  author={Bae, Seongsu and Kyung, Daeun and Ryu, Jaehee and Cho, Eunbyeol and Lee, Gyubok and Kweon, Sunjun and Oh, Jungwoo and Ji, Lei and Chang, Eric and Kim, Tackeun and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@article{humedical,
  title={Medical-Diff-VQA: A Large-Scale Medical Dataset for Difference Visual Question Answering on Chest X-Ray Images},
  author={Hu, Xinyue and others}, 
year={2023}
}

@inproceedings{ben2019vqa,
  title={Vqa-med: Overview of the medical visual question answering task at imageclef 2019},
  author={Ben Abacha, Asma and Hasan, Sadid A and Datla, Vivek V and Demner-Fushman, Dina and M{\"u}ller, Henning},
  booktitle={Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes},
  year={2019},
  organization={9-12 September 2019}
}


@article{holste2023cxr,
  title={CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays},
  author={Holste, Gregory and Wang, Song and Jaiswal, Ajay and Yang, Yuzhe and Lin, Mingquan and Peng, Yifan and Wang, Atlas},
  journal={PhysioNet},
  volume={5},
  pages={19},
  year={2023}
}

@inproceedings{irvin2019chexpert,
  title={Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
  author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  pages={590--597},
  year={2019}
}

@inproceedings{wang2017chestx,
  title={Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
  author={Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2097--2106},
  year={2017}
}

@article{feng2021curation,
  title={Curation of the candid-ptx dataset with free-text reports},
  author={Feng, Sijing and Azzollini, Damian and Kim, Ji Soo and Jin, Cheng-Kai and Gordon, Simon P and Yeoh, Jason and Kim, Eve and Han, Mina and Lee, Andrew and Patel, Aakash and others},
  journal={Radiology: Artificial Intelligence},
  volume={3},
  number={6},
  pages={e210136},
  year={2021},
  publisher={Radiological Society of North America}
}

@article{reis2022brax,
  title={BRAX, Brazilian labeled chest x-ray dataset},
  author={Reis, Eduardo P and de Paiva, Joselisa PQ and da Silva, Maria CB and Ribeiro, Guilherme AS and Paiva, Victor F and Bulgarelli, Lucas and Lee, Henrique MH and Santos, Paulo V and Brito, Vanessa M and Amaral, Lucas TW and others},
  journal={Scientific Data},
  volume={9},
  number={1},
  pages={487},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{vaya2020bimcv,
  title={BIMCV COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients},
  author={Vay{\'a}, Maria De La Iglesia and Saborit, Jose Manuel and Montell, Joaquim Angel and Pertusa, Antonio and Bustos, Aurelia and Cazorla, Miguel and Galant, Joaquin and Barber, Xavier and Orozco-Beltr{\'a}n, Domingo and Garc{\'\i}a-Garc{\'\i}a, Francisco and others},
  journal={arXiv preprint arXiv:2006.01174},
  year={2020}
}

@article{thawkar2023xraygpt,
  title={Xraygpt: Chest radiographs summarization using medical vision-language models},
  author={Thawkar, Omkar and Shaker, Abdelrahman and Mullappilly, Sahal Shaji and Cholakkal, Hisham and Anwer, Rao Muhammad and Khan, Salman and Laaksonen, Jorma and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.07971},
  year={2023}
}

@article{wang2022medclip,
  title={Medclip: Contrastive learning from unpaired medical images and text},
  author={Wang, Zifeng and Wu, Zhenbang and Agarwal, Dinesh and Sun, Jimeng},
  journal={arXiv preprint arXiv:2210.10163},
  year={2022}
}

@article{yin2024lamm,
  title={Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark},
  author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and Bai, Lei and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2023llava-med,
  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2306.00890},
  year={2023}
}

@article{xu2022multiinstruct,
  title={Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning},
  author={Xu, Zhiyang and Shen, Ying and Huang, Lifu},
  journal={arXiv preprint arXiv:2212.10773},
  year={2022}
}


@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}


@article{johnson2019mimic,
  title={MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs},
  author={Johnson, Alistair EW and Pollard, Tom J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-ying and Peng, Yifan and Lu, Zhiyong and Mark, Roger G and Berkowitz, Seth J and Horng, Steven},
  journal={arXiv preprint arXiv:1901.07042},
  year={2019}
}

@article{wu2021chest,
  title={Chest ImaGenome dataset for clinical reasoning},
  author={Wu, Joy T and Agu, Nkechinyere N and Lourentzou, Ismini and Sharma, Arjun and Paguio, Joseph A and Yao, Jasper S and Dee, Edward C and Mitchell, William and Kashyap, Satyananda and Giovannini, Andrea and others},
  journal={arXiv preprint arXiv:2108.00316},
  year={2021}
}

@article{jain2021radgraph,
  title={Radgraph: Extracting clinical entities and relations from radiology reports},
  author={Jain, Saahil and Agrawal, Ashwin and Saporta, Adriel and Truong, Steven QH and Duong, Du Nguyen and Bui, Tan and Chambon, Pierre and Zhang, Yuhao and Lungren, Matthew P and Ng, Andrew Y and others},
  journal={arXiv preprint arXiv:2106.14463},
  year={2021}
}

@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{yang2022unitab,
  title={Unitab: Unifying text and box outputs for grounded vision-language modeling},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Ahmed, Faisal and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={European Conference on Computer Vision},
  pages={521--539},
  year={2022},
  organization={Springer}
}

@article{chen2021pix2seq,
  title={Pix2seq: A language modeling framework for object detection},
  author={Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2109.10852},
  year={2021}
}

@article{zhong2020frustratingly,
  title={A frustratingly easy approach for entity and relation extraction},
  author={Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2010.12812},
  year={2020}
}


@article{pellegrini2023radialog,
  title={RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance},
  author={Pellegrini, Chantal and {\"O}zsoy, Ege and Busam, Benjamin and Navab, Nassir and Keicher, Matthias},
  journal={arXiv preprint arXiv:2311.18681},
  year={2023}
}

@inproceedings{gella-etal-2017-image,
    title = "Image Pivoting for Learning Multilingual Multimodal Representations",
    author = "Gella, Spandana  and
      Sennrich, Rico  and
      Keller, Frank  and
      Lapata, Mirella",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1303",
    doi = "10.18653/v1/D17-1303",
    pages = "2839--2845",
    abstract = "In this paper we propose a model to learn multimodal multilingual representations for matching images and sentences in different languages, with the aim of advancing multilingual versions of image search and image understanding. Our model learns a common representation for images and their descriptions in two different languages (which need not be parallel) by considering the image as a pivot between two languages. We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance.",
}

@inproceedings{nakayama-etal-2020-visually,
    title = "A Visually-Grounded Parallel Corpus with Phrase-to-Region Linking",
    author = "Nakayama, Hideki  and
      Tamura, Akihiro  and
      Ninomiya, Takashi",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.518",
    pages = "4204--4210",
    abstract = "Visually-grounded natural language processing has become an important research direction in the past few years. However, majorities of the available cross-modal resources (e.g., image-caption datasets) are built in English and cannot be directly utilized in multilingual or non-English scenarios. In this study, we present a novel multilingual multimodal corpus by extending the Flickr30k Entities image-caption dataset with Japanese translations, which we name Flickr30k Entities JP (F30kEnt-JP). To the best of our knowledge, this is the first multilingual image-caption dataset where the captions in the two languages are parallel and have the shared annotations of many-to-many phrase-to-region linking. We believe that phrase-to-region as well as phrase-to-phrase supervision can play a vital role in fine-grained grounding of language and vision, and will promote many tasks such as multilingual image captioning and multimodal machine translation. To verify our dataset, we performed phrase localization experiments in both languages and investigated the effectiveness of our Japanese annotations as well as multilingual learning realized by our dataset.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}


@inproceedings{wu-etal-2023-cross2stra,
    title = "{C}ross2{S}tr{A}: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment",
    author = "Wu, Shengqiong  and
      Fei, Hao  and
      Ji, Wei  and
      Chua, Tat-Seng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.146",
    doi = "10.18653/v1/2023.acl-long.146",
    pages = "2593--2608",
    abstract = "Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined via pivot language. We then take the SG and SC structures as pivoting, performing cross-modal semantic structure alignment and cross-lingual syntactic structure alignment learning. We further introduce cross-lingual{\&}cross-modal back-translation training to fully align the captioning and translation stages. Experiments on English-Chinese transfers show that our model shows great superiority in improving captioning relevancy and fluency.",
}


@inproceedings{tanida2023interactive,
  title={Interactive and Explainable Region-guided Radiology Report Generation},
  author={Tanida, Tim and M{\"u}ller, Philip and Kaissis, Georgios and Rueckert, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7433--7442},
  year={2023}
}

@misc{sun2023evaclip,
      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale}, 
      author={Quan Sun and Yuxin Fang and Ledell Wu and Xinlong Wang and Yue Cao},
      year={2023},
      eprint={2303.15389},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{sun2024evaclip18b,
      title={EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters}, 
      author={Quan Sun and Jinsheng Wang and Qiying Yu and Yufeng Cui and Fan Zhang and Xiaosong Zhang and Xinlong Wang},
      year={2024},
      eprint={2402.04252},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhang2024mm,
  title={Mm-llms: Recent advances in multimodal large language models},
  author={Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  journal={arXiv preprint arXiv:2401.13601},
  year={2024}
}




@inproceedings{huang-etal-2021-multilingual,
    title = "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models",
    author = "Huang, Po-Yao  and
      Patrick, Mandela  and
      Hu, Junjie  and
      Neubig, Graham  and
      Metze, Florian  and
      Hauptmann, Alexander",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.195",
    doi = "10.18653/v1/2021.naacl-main.195",
    pages = "2443--2459",
}


@inproceedings{ni-etal-2020-learning,
    title = "Learning Visual-Semantic Embeddings for Reporting Abnormal Findings on Chest {X}-rays",
    author = "Ni, Jianmo  and
      Hsu, Chun-Nan  and
      Gentili, Amilcare  and
      McAuley, Julian",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.176",
    doi = "10.18653/v1/2020.findings-emnlp.176",
    pages = "1954--1960",
    abstract = "Automatic medical image report generation has drawn growing attention due to its potential to alleviate radiologists{'} workload. Existing work on report generation often trains encoder-decoder networks to generate complete reports. However, such models are affected by data bias (e.g. label imbalance) and face common issues inherent in text generation models (e.g. repetition). In this work, we focus on reporting abnormal findings on radiology images; instead of training on complete radiology reports, we propose a method to identify abnormal findings from the reports in addition to grouping them with unsupervised clustering and minimal rules. We formulate the task as cross-modal retrieval and propose Conditional Visual-Semantic Embeddings to align images and fine-grained abnormal findings in a joint embedding space. We demonstrate that our method is able to retrieve abnormal findings and outperforms existing generation models on both clinical correctness and text generation metrics.",
}
@article{Mettler2009,
   abstract = {The per-capita annual effective radiation dose from medical procedures in the United States is among the highest in the world and is estimated to have increased sixfold from about 0.5 mSv in 1980 t...},
   author = {Fred A. Mettler and Mythreyi Bhargavan and Keith Faulkner and Debbie B. Gilley and Joel E. Gray and Geoffrey S. Ibbott and Jill A. Lipoti and Mahadevappa Mahesh and John L. McCrohan and Michael G. Stabin and Bruce R. Thomadsen and Terry T. Yoshizumi},
   doi = {10.1148/RADIOL.2532082010},
   issn = {00338419},
   issue = {2},
   journal = {https://doi.org/10.1148/radiol.2532082010},
   month = {11},
   pages = {520-531},
   pmid = {19789227},
   publisher = { Radiological Society of North America, Inc.},
   title = {Radiologic and Nuclear Medicine Studies in the United States and Worldwide: Frequency, Radiation Dose, and Comparison with Other Radiation Sources—1950–20071},
   volume = {253},
   url = {https://pubs.rsna.org/doi/10.1148/radiol.2532082010},
   year = {2009},
}

@article{Peng2022,
   abstract = {Purpose: The utilization of diagnostic medical imaging has been growing worldwide. However, no study has investigated the trend in image utilization and the corresponding workload of radiologists under the National Healthcare Insurance (NHI) system with a code-bundling-based reimbursement strategy. We will analyse the trend in diagnostic imaging utilization and the corresponding workload of the radiologists at a single tertiary medical centre using the NHI system. Materials and methods: This was a retrospective study recruiting the diagnostic medical images, including X-rays, CT, and MR performed between 2005 and 2020 at a single medical centre. We investigated the change over time in image utilization and workload for interpreting the images. The two-sided Mann-Kendall test was used for the monotonic trend analysis and Sen's slope estimate was calculated for the annual mean change with the 95% confidence interval (CI). A P value < 0.05 was considered significant. Results: A total of 10,069,583 examinations were performed at our institute from 2005 to 2020, including 7,821,880 X-rays, 1,665,787 CT, and 581,916 MR examinations. The numbers of examinations of X-rays, CT, and MR increased with average annual changes of 13,411.3 (95% CI = 11,875.0–14,773.8), 9,496.7 (95% CI = 8,845.3–9,828.7), and 2,417.1 (95% CI = 2,209.8–2,668.9) respectively, all P < 0.001. The proportion of cases including multiple examinations increased, growing from 21.5% (6,627 in 30,878 cases) to 43.8% (39,417 in 90,032 cases) for CT and from 8.9% (1,316 in 14,791 cases) to 15.7% (6,083 in 38,865 cases) for MR. The average time spent on interpreting each diagnostic image decreased significantly from 16.0 to 2.9 sec. (P < 0.001). Conclusion: Imaging utilization increased significantly under the NHI system at a medical centre. The corresponding demand for image interpretation also placed a significant workload on radiologists, potentially contributing to radiologist burnout.},
   author = {Yan Chih Peng and Wen Jeng Lee and Yeun Chung Chang and Wing P. Chan and Shyh Jye Chen},
   doi = {10.1016/J.EJRAD.2022.110596},
   issn = {1872-7727},
   journal = {European journal of radiology},
   keywords = {Burnout,Humans,MEDLINE,Magnetic Resonance Imaging*,NCBI,NIH,NLM,National Center for Biotechnology Information,National Health Programs,National Institutes of Health,National Library of Medicine,Psychological,PubMed Abstract,Radiologists,Retrospective Studies,Shyh-Jye Chen,Tomography,Wen-Jeng Lee,X-Ray Computed*,Yan-Chih Peng,doi:10.1016/j.ejrad.2022.110596,pmid:36379098},
   month = {12},
   pmid = {36379098},
   publisher = {Eur J Radiol},
   title = {Radiologist burnout: Trends in medical imaging utilization under the national health insurance system with the universal code bundling strategy in an academic tertiary medical centre},
   volume = {157},
   url = {https://pubmed.ncbi.nlm.nih.gov/36379098/},
   year = {2022},
}

@article{Malak2021,
   abstract = {Background: Family medicine physicians may encounter a wide variety of conditions, including acute and urgent cases. Considering the limited access to diagnostic investigations in primary care practice, chest X-ray remains the imaging modality of choice. The current study assessed the competency of family medicine residents in the interpretation of chest X-rays for emergency conditions and to compare it with that of diagnostic radiology residents, general practitioners, and medical interns. Methods: An online survey was distributed to 600 physicians, including family medicine residents, medical interns, general practitioners, and diagnostic radiology residents. The study included some background information such as gender, years in practice, training type, interest in pulmonary medicine and diagnostic radiology, and having adequate training on the interpretation of chest X-rays. The survey had 10 chest X-ray cases with brief clinical information. Participants were asked to choose the most likely diagnosis and to rate their degree of confidence in the interpretation of the chest X-ray for each case. Results: The survey was completed by 205 physicians (response rate = 34.2%). The overall diagnostic accuracy was 63.1% with a significant difference between family medicine and radiology residents (58.0% vs. 90.5%; P < 0.001). The COVID-19 pneumonia (85.4%) and pneumoperitoneum (80.5%) cases had the highest diagnostic accuracy scores. There was a significant correlation between the diagnostic confidence and accuracy (rs = 0.39; P < 0.001). Multivariable regression analysis revealed that being diagnostic radiology residents (odds ratio [OR]: 13.0; 95% confidence interval [CI]: 2.5–67.7) and having higher diagnostic confidence (OR: 2.2; 95% CI: 1.3–3.8) were the only independent predictors of achieving high diagnostic accuracy. Conclusion: The competency of family medicine residents in the interpretation of chest X-ray for emergency conditions was far from optimal. The introduction of radiology training courses on emergency conditions seems imperative. Alternatively, the use of tele-radiology in primary healthcare centers should be considered.},
   author = {Malak Al Shammari and Ali Hassan and Nouf AlShamlan and Sarah Alotaibi and Manar Bamashmoos and Amani Hakami and Abdullatif Althunyan and Shymaa Basager and Sameerah Motabgani and Sawsan Aljubran and Hind S. Alsaif},
   doi = {10.1186/s12875-021-01390-3},
   issn = {14712296},
   issue = {1},
   journal = {BMC Family Practice},
   keywords = {Chest X-ray,Diagnostic accuracy,Emergency medicine,Family medicine,Residency program},
   month = {12},
   pmid = {33596838},
   publisher = {BioMed Central Ltd},
   title = {Family medicine residents’ skill levels in emergency chest X-ray interpretation},
   volume = {22},
   year = {2021},
}

@article{Nishio2020,
   abstract = {This study aimed to develop and validate computer-aided diagnosis (CXDx) system for classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray (CXR) images. From two public datasets, 1248 CXR images were obtained, which included 215, 533, and 500 CXR images of COVID-19 pneumonia patients, non-COVID-19 pneumonia patients, and the healthy samples, respectively. The proposed CADx system utilized VGG16 as a pre-trained model and combination of conventional method and mixup as data augmentation methods. Other types of pre-trained models were compared with the VGG16-based model. Single type or no data augmentation methods were also evaluated. Splitting of training/validation/test sets was used when building and evaluating the CADx system. Three-category accuracy was evaluated for test set with 125 CXR images. The three-category accuracy of the CAD system was 83.6% between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy. Sensitivity for COVID-19 pneumonia was more than 90%. The combination of conventional method and mixup was more useful than single type or no data augmentation method. In conclusion, this study was able to create an accurate CADx system for the 3-category classification. Source code of our CADx system is available as open source for COVID-19 research.},
   author = {Mizuho Nishio and Shunjiro Noguchi and Hidetoshi Matsuo and Takamichi Murakami},
   doi = {10.1038/s41598-020-74539-2},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports 2020 10:1},
   keywords = {Preclinical research,Software,Viral infection},
   month = {10},
   pages = {1-6},
   pmid = {33067538},
   publisher = {Nature Publishing Group},
   title = {Automatic classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray image: combination of data augmentation methods},
   volume = {10},
   url = {https://www.nature.com/articles/s41598-020-74539-2},
   year = {2020},
}

@misc{pubmed,
  author    = {NIH},
  title     = {PubMed},
  year = {n.d.},
  url       = {https://pubmed.ncbi.nlm.nih.gov/},
  note      = {https://pubmed.ncbi.nlm.nih.gov/,  Accessed: 2024-12-22}
}

@article{Homayounieh2021,
   abstract = {ImportanceMost early lung cancers present as pulmonary nodules on imaging, but these can be easily missed on chest radiographs.ObjectiveTo assess if a novel artificial intelligence (AI) algorithm can help detect pulmonary nodules on radiographs at different levels of detection difficulty.Design, Setting, and ParticipantsThis diagnostic study included 100 posteroanterior chest radiograph images taken between 2000 and 2010 of adult patients from an ambulatory health care center in Germany and a lung image database in the US. Included images were selected to represent nodules with different levels of detection difficulties (from easy to difficult), and comprised both normal and nonnormal control.ExposuresAll images were processed with a novel AI algorithm, the AI Rad Companion Chest X-ray. Two thoracic radiologists established the ground truth and 9 test radiologists from Germany and the US independently reviewed all images in 2 sessions (unaided and AI-aided mode) with at least a 1-month washout period.Main Outcomes and MeasuresEach test radiologist recorded the presence of 5 findings (pulmonary nodules, atelectasis, consolidation, pneumothorax, and pleural effusion) and their level of confidence for detecting the individual finding on a scale of 1 to 10 (1 representing lowest confidence; 10, highest confidence). The analyzed metrics for nodules included sensitivity, specificity, accuracy, and receiver operating characteristics curve area under the curve (AUC).ResultsImages from 100 patients were included, with a mean (SD) age of 55 (20) years and including 64 men and 36 women. Mean detection accuracy across the 9 radiologists improved by 6.4% (95% CI, 2.3% to 10.6%) with AI-aided interpretation compared with unaided interpretation. Partial AUCs within the effective interval range of 0 to 0.2 false positive rate improved by 5.6% (95% CI, −1.4% to 12.0%) with AI-aided interpretation. Junior radiologists saw greater improvement in sensitivity for nodule detection with AI-aided interpretation as compared with their senior counterparts (12%; 95% CI, 4% to 19% vs 9%; 95% CI, 1% to 17%) while senior radiologists experienced similar improvement in specificity (4%; 95% CI, −2% to 9%) as compared with junior radiologists (4%; 95% CI, −3% to 5%).Conclusions and RelevanceIn this diagnostic study, an AI algorithm was associated with improved detection of pulmonary nodules on chest radiographs compared with unaided interpretation for different levels of detection difficulty and for readers with different experience.},
   author = {Fatemeh Homayounieh and Subba Digumarthy and Shadi Ebrahimian and Johannes Rueckel and Boj Friedrich Hoppe and Bastian Oliver Sabel and Sailesh Conjeti and Karsten Ridder and Markus Sistermanns and Lei Wang and Alexander Preuhs and Florin Ghesu and Awais Mansoor and Mateen Moghbel and Ariel Botwin and Ramandeep Singh and Samuel Cartmell and John Patti and Christian Huemmer and Andreas Fieselmann and Clemens Joerger and Negar Mirshahzadeh and Victorine Muse and Mannudeep Kalra},
   doi = {10.1001/JAMANETWORKOPEN.2021.41096},
   issn = {25743805},
   issue = {12},
   journal = {JAMA Network Open},
   keywords = {artificial intelligence,chest x-ray,diagnostic imaging,pulmonary nodule,radiologists},
   month = {12},
   pages = {e2141096-e2141096},
   pmid = {34964851},
   publisher = {American Medical Association},
   title = {An Artificial Intelligence–Based Chest X-ray Model on Human Nodule Detection Accuracy From a Multicenter Study},
   volume = {4},
   url = {https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2787587},
   year = {2021},
}
@article{Yang2023,
   abstract = {In clinics, a radiology report is crucial for guiding a patient's treatment. However, writing radiology reports is a heavy burden for radiologists. To this end, we present an automatic, multi-modal approach for report generation from a chest x-ray. Our approach, motivated by the observation that the descriptions in radiology reports are highly correlated with specific information of the x-ray images, features two distinct modules: (i) Learned knowledge base: To absorb the knowledge embedded in the radiology reports, we build a knowledge base that can automatically distill and restore medical knowledge from textual embedding without manual labor; (ii) Multi-modal alignment: to promote the semantic alignment among reports, disease labels, and images, we explicitly utilize textual embedding to guide the learning of the visual feature space. We evaluate the performance of the proposed model using metrics from both natural language generation and clinic efficacy on the public IU-Xray and MIMIC-CXR datasets. Our ablation study shows that each module contributes to improving the quality of generated reports. Furthermore, the assistance of both modules, our approach outperforms state-of-the-art methods over almost all the metrics. Code is available at https://github.com/LX-doctorAI1/M2KT.},
   author = {Shuxin Yang and Xian Wu and Shen Ge and Zhuozhao Zheng and S. Kevin Zhou and Li Xiao},
   doi = {10.1016/J.MEDIA.2023.102798},
   issn = {1361-8415},
   journal = {Medical Image Analysis},
   keywords = {Knowledge base,Multi-modal alignment,Radiology report generation},
   month = {5},
   pages = {102798},
   pmid = {36989850},
   publisher = {Elsevier},
   title = {Radiology report generation with a learned knowledge base and multi-modal alignment},
   volume = {86},
   year = {2023},
}

@ARTICLE{Tu2024-ad,
  title         = "Towards Conversational Diagnostic {AI}",
  author        = "Tu, Tao and Palepu, Anil and Schaekermann, Mike and Saab,
                   Khaled and Freyberg, Jan and Tanno, Ryutaro and Wang, Amy and
                   Li, Brenna and Amin, Mohamed and Tomasev, Nenad and Azizi,
                   Shekoofeh and Singhal, Karan and Cheng, Yong and Hou, Le and
                   Webson, Albert and Kulkarni, Kavita and Mahdavi, S Sara and
                   Semturs, Christopher and Gottweis, Juraj and Barral, Joelle
                   and Chou, Katherine and Corrado, Greg S and Matias, Yossi and
                   Karthikesalingam, Alan and Natarajan, Vivek",
  journal       = "arXiv [cs.AI]",
  abstract      = "At the heart of medicine lies the physician-patient dialogue,
                   where skillful history-taking paves the way for accurate
                   diagnosis, effective management, and enduring trust.
                   Artificial Intelligence (AI) systems capable of diagnostic
                   dialogue could increase accessibility, consistency, and
                   quality of care. However, approximating clinicians' expertise
                   is an outstanding grand challenge. Here, we introduce AMIE
                   (Articulate Medical Intelligence Explorer), a Large Language
                   Model (LLM) based AI system optimized for diagnostic
                   dialogue. AMIE uses a novel self-play based simulated
                   environment with automated feedback mechanisms for scaling
                   learning across diverse disease conditions, specialties, and
                   contexts. We designed a framework for evaluating
                   clinically-meaningful axes of performance including
                   history-taking, diagnostic accuracy, management reasoning,
                   communication skills, and empathy. We compared AMIE's
                   performance to that of primary care physicians (PCPs) in a
                   randomized, double-blind crossover study of text-based
                   consultations with validated patient actors in the style of
                   an Objective Structured Clinical Examination (OSCE). The
                   study included 149 case scenarios from clinical providers in
                   Canada, the UK, and India, 20 PCPs for comparison with AMIE,
                   and evaluations by specialist physicians and patient actors.
                   AMIE demonstrated greater diagnostic accuracy and superior
                   performance on 28 of 32 axes according to specialist
                   physicians and 24 of 26 axes according to patient actors. Our
                   research has several limitations and should be interpreted
                   with appropriate caution. Clinicians were limited to
                   unfamiliar synchronous text-chat which permits large-scale
                   LLM-patient interactions but is not representative of usual
                   clinical practice. While further research is required before
                   AMIE could be translated to real-world settings, the results
                   represent a milestone towards conversational diagnostic AI.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@MISC{chatgpt-ut,
  author       = "{OpenAI}",
  title        = "{ChatGPT} Can Now {See}, {Hear}, and {Speak}",
  year         = "2024",
  howpublished = "OpenAI Blog, \url{https://openai.com/index/chatgpt-can-now-see-hear-and-speak/}",
  note         = "Accessed: 2024-11-26"
}

@MISC{claude-hq,
  author       = "{Anthropic}",
  title        = "Introducing the Next Generation of {Claude}",
  year         = "2024",
  howpublished = "Anthropic News, \url{https://www.anthropic.com/news/claude-3-family}",
  note         = "Accessed: 2024-11-26"
}


@ARTICLE{Singhal2023-ok,
  title     = "Large language models encode clinical knowledge",
  author    = "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S
               Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and
               Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and
               Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly,
               Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery,
               Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and
               Agüera Y Arcas, Blaise and Webster, Dale and Corrado, Greg S and
               Matias, Yossi and Chou, Katherine and Gottweis, Juraj and
               Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral,
               Joelle and Semturs, Christopher and Karthikesalingam, Alan and
               Natarajan, Vivek",
  journal   = "Nature",
  publisher = "Springer Science and Business Media LLC",
  volume    =  620,
  number    =  7972,
  pages     = "172--180",
  abstract  = "Large language models (LLMs) have demonstrated impressive
               capabilities, but the bar for clinical applications is high.
               Attempts to assess the clinical knowledge of models typically
               rely on automated evaluations based on limited benchmarks. Here,
               to address these limitations, we present MultiMedQA, a benchmark
               combining six existing medical question answering datasets
               spanning professional medicine, research and consumer queries and
               a new dataset of medical questions searched online,
               HealthSearchQA. We propose a human evaluation framework for model
               answers along multiple axes including factuality, comprehension,
               reasoning, possible harm and bias. In addition, we evaluate
               Pathways Language Model1 (PaLM, a 540-billion parameter LLM) and
               its instruction-tuned variant, Flan-PaLM2 on MultiMedQA. Using a
               combination of prompting strategies, Flan-PaLM achieves
               state-of-the-art accuracy on every MultiMedQA multiple-choice
               dataset (MedQA3, MedMCQA4, PubMedQA5 and Measuring Massive
               Multitask Language Understanding (MMLU) clinical topics6),
               including 67.6\% accuracy on MedQA (US Medical Licensing
               Exam-style questions), surpassing the prior state of the art by
               more than 17\%. However, human evaluation reveals key gaps. To
               resolve this, we introduce instruction prompt tuning, a
               parameter-efficient approach for aligning LLMs to new domains
               using a few exemplars. The resulting model, Med-PaLM, performs
               encouragingly, but remains inferior to clinicians. We show that
               comprehension, knowledge recall and reasoning improve with model
               scale and instruction prompt tuning, suggesting the potential
               utility of LLMs in medicine. Our human evaluations reveal
               limitations of today's models, reinforcing the importance of both
               evaluation frameworks and method development in creating safe,
               helpful LLMs for clinical applications.",
  month     =  aug,
  year      =  2023,
  language  = "en"
}


@misc{chen2024chexagent,
      title={A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation}, 
      author={Zhihong Chen and Maya Varma and Justin Xu and Magdalini Paschali and Dave Van Veen and Andrew Johnston and Alaa Youssef and Louis Blankemeier and Christian Bluethgen and Stephan Altmayer and Jeya Maria Jose Valanarasu and Mohamed Siddig Eltayeb Muneer and Eduardo Pontes Reis and Joseph Paul Cohen and Cameron Olsen and Tanishq Mathew Abraham and Emily B. Tsai and Christopher F. Beaulieu and Jenia Jitsev and Sergios Gatidis and Jean-Benoit Delbrouck and Akshay S. Chaudhari and Curtis P. Langlotz},
      year={2024},
      eprint={2401.12208},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.12208}, 
}

@ARTICLE{Pellegrini2023-sv,
  title         = "{RaDialog}: A large vision-language model for radiology
                   report generation and conversational assistance",
  author        = "Pellegrini, Chantal and Özsoy, Ege and Busam, Benjamin and
                   Navab, Nassir and Keicher, Matthias",
  journal       = "arXiv [cs.CV]",
  abstract      = "Conversational AI tools that can generate and discuss
                   clinically correct radiology reports for a given medical
                   image have the potential to transform radiology. Such a
                   human-in-the-loop radiology assistant could facilitate a
                   collaborative diagnostic process, thus saving time and
                   improving the quality of reports. Towards this goal, we
                   introduce RaDialog, the first thoroughly evaluated and
                   publicly available large vision-language model for radiology
                   report generation and interactive dialog. RaDialog
                   effectively integrates visual image features and structured
                   pathology findings with a large language model (LLM) while
                   simultaneously adapting it to a specialized domain using
                   parameter-efficient fine-tuning. To keep the conversational
                   abilities of the underlying LLM, we propose a comprehensive,
                   semi-automatically labeled, image-grounded instruct dataset
                   for chest X-ray radiology tasks. By training with this
                   dataset, our method achieves state-of-the-art clinical
                   correctness in report generation and shows impressive
                   abilities in interactive tasks such as correcting reports and
                   answering questions, serving as a foundational step toward
                   clinical dialog systems. Our code is available on github:
                   https://github.com/ChantalMP/RaDialog.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Bannur2024-ek,
  title         = "{MAIRA}-2: Grounded Radiology Report Generation",
  author        = "Bannur, Shruthi and Bouzid, Kenza and Castro, Daniel C and
                   Schwaighofer, Anton and Thieme, Anja and Bond-Taylor, Sam and
                   Ilse, Maximilian and Pérez-García, Fernando and Salvatelli,
                   Valentina and Sharma, Harshita and Meissen, Felix and Ranjit,
                   Mercy and Srivastav, Shaury and Gong, Julia and Codella, Noel
                   C F and Falck, Fabian and Oktay, Ozan and Lungren, Matthew P
                   and Wetscherek, Maria Teodora and Alvarez-Valle, Javier and
                   Hyland, Stephanie L",
  journal       = "arXiv [cs.CL]",
  abstract      = "Radiology reporting is a complex task requiring detailed
                   medical image understanding and precise language generation,
                   for which generative multimodal models offer a promising
                   solution. However, to impact clinical practice, models must
                   achieve a high level of both verifiable performance and
                   utility. We augment the utility of automated report
                   generation by incorporating localisation of individual
                   findings on the image - a task we call grounded report
                   generation - and enhance performance by incorporating
                   realistic reporting context as inputs. We design a novel
                   evaluation framework (RadFact) leveraging the logical
                   inference capabilities of large language models (LLMs) to
                   quantify report correctness and completeness at the level of
                   individual sentences, while supporting the new task of
                   grounded reporting. We develop MAIRA-2, a large
                   radiology-specific multimodal model designed to generate
                   chest X-ray reports with and without grounding. MAIRA-2
                   achieves state of the art on existing report generation
                   benchmarks and establishes the novel task of grounded report
                   generation.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@article{akhter2023ai,
  title={AI-based radiodiagnosis using chest X-rays: A review},
  author={Akhter, Yasmeena and Singh, Richa and Vatsa, Mayank},
  journal={Frontiers in Big Data},
  volume={6},
  pages={1120989},
  year={2023},
  publisher={Frontiers Media SA}
}

@article{ccalli2021deep,
  title={Deep learning for chest X-ray analysis: A survey},
  author={{\c{C}}all{\i}, Erdi and Sogancioglu, Ecem and van Ginneken, Bram and van Leeuwen, Kicky G and Murphy, Keelin},
  journal={Medical Image Analysis},
  volume={72},
  pages={102125},
  year={2021},
  publisher={Elsevier}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@MISC{Calamida2024-le,
  title     = "Radiology Report Generation Models Evaluation Dataset For Chest
               X-rays ({RadEvalX})",
  author    = "Calamida, Amos Rubin and Nooralahzadeh, Farhad and Rohanian,
               Morteza and Nishio, Mizuho and Fujimoto, Koji and Krauthammer,
               Michael",
  abstract  = "The Radiology Report Generation Models Evaluation Dataset For
               Chest X-rays (RadEvalX) is publicly available and developed
               similarly to the ReXVal dataset. Just like ReXVal, RadEvalX
               focuses on radiologist evaluations of errors found in
               automatically generated radiology reports. The dataset includes
               annotations from two board-certified radiologists, who
               identified clinically significant and clinically insignificant
               errors across eight different categories of errors. Compared to
               the ground-truth reports from the IU-Xray dataset, the
               evaluations were done on candidate radiology reports. For every
               100 studies and corresponding ground-truth reports, the dataset
               contains one report generated using the M2Tr model from the
               corresponding X-ray image. The radiologists then annotated these
               reports. The primary purpose of this dataset is to assess the
               correlation between automated metrics and human radiologists'
               scoring, explore the limitations of automated metrics, and
               develop a model-based automated metric. This dataset has been
               created to support further research in medical artificial
               intelligence (AI), particularly in the field of radiology.",
  publisher = "PhysioNet",
  year      =  2024
}

@MISC{Yu2023-rt,
  title     = "Radiology Report Expert Evaluation ({ReXVal}) Dataset",
  author    = "Yu, Feiyang and Endo, Mark and Krishnan, Rayan and Pan, Ian and
               Tsai, Andy and Reis, Eduardo Pontes and Kaiser Ururahy Nunes
               Fonseca, Eduardo and Lee, Henrique and Shakeri, Zahra and Ng,
               Andrew and Langlotz, Curtis and Venugopal, Vasantha Kumar and
               Rajpurkar, Pranav",
  abstract  = "The Radiology Report Expert Evaluation (ReXVal) Dataset is a
               publicly available dataset of radiologist evaluations of errors
               in automatically generated radiology reports. The dataset
               contains annotations from 6 board certified radiologists on
               clinically significant and clinically insignificant errors under
               6 error categories for candidate radiology reports with respect
               to ground-truth reports from the MIMIC-CXR dataset. There are 4
               candidate reports generated for 50 studies, translating to 200
               pairs of candidate and ground-truth reports on which
               radiologists provided annotations. The dataset has been used to
               evaluate the alignment between scoring of automated metrics and
               that of radiologists, investigate the failure modes of automated
               metrics, and build a composite automated metric, in a study on
               how to meaningfully measure progress in radiology report
               generation. It is also created to support additional medical AI
               research in radiology and other expert tasks.",
  publisher = "PhysioNet",
  year      =  2023
}

@article{calamida2023radiology,
  title={Radiology-Aware Model-Based Evaluation Metric for Report Generation},
  author={Calamida, Amos and Nooralahzadeh, Farhad and Rohanian, Morteza and Fujimoto, Koji and Nishio, Mizuho and Krauthammer, Michael},
  journal={arXiv preprint arXiv:2311.16764},
  year={2023}
}