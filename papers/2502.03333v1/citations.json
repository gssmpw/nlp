[
  {
    "index": 0,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom B",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wei2021finetuned",
        "author": "Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V",
        "title": "Finetuned language models are zero-shot learners"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zheng2023lmsys",
        "author": "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li, Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric and others",
        "title": "Lmsys-chat-1m: A large-scale real-world llm conversation dataset"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wei2021finetuned",
        "author": "Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V",
        "title": "Finetuned language models are zero-shot learners"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-instruct: Aligning language models with self-generated instructions"
      },
      {
        "key": "peng2023instruction",
        "author": "Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng",
        "title": "Instruction tuning with gpt-4"
      },
      {
        "key": "liu2023visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024deepseek",
        "author": "Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others",
        "title": "Deepseek-v3 technical report"
      },
      {
        "key": "guo2025deepseek",
        "author": "Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "team2023gemini",
        "author": "Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others",
        "title": "Gemini: a family of highly capable multimodal models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2023visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2024llava",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "laurencon2024building",
        "author": "Hugo Lauren\u00e7on and Andr\u00e9s Marafioti and Victor Sanh and L\u00e9o Tronchon",
        "title": "Building and better understanding vision-language models: insights and future directions."
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2024qwen2vl",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "feng2022mmdialog",
        "author": "Feng, Jiazhan and Sun, Qingfeng and Xu, Can and Zhao, Pu and Yang, Yaming and Tao, Chongyang and Zhao, Dongyan and Lin, Qingwei",
        "title": "Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal open-domain conversation"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "singhal2023large",
        "author": "Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others",
        "title": "Large language models encode clinical knowledge"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "saab2024capabilities",
        "author": "Saab, Khaled and Tu, Tao and Weng, Wei-Hung and Tanno, Ryutaro and Stutz, David and Wulczyn, Ellery and Zhang, Fan and Strother, Tim and Park, Chunjong and Vedadi, Elahe and others",
        "title": "Capabilities of gemini models in medicine"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "li2023llava-med",
        "author": "Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng",
        "title": "Llava-med: Training a large language-and-vision assistant for biomedicine in one day"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "liu2023visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "pubmed",
        "author": "NIH",
        "title": "PubMed"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "nooralahzadeh2021progressive",
        "author": "Nooralahzadeh, Farhad  and\nPerez Gonzalez, Nicolas  and\nFrauenfelder, Thomas  and\nFujimoto, Koji  and\nKrauthammer, Michael",
        "title": "Progressive Transformer-Based Generation of Radiology Reports"
      },
      {
        "key": "alfarghaly2021automated",
        "author": "Alfarghaly, Omar and Khaled, Rana and Elkorany, Abeer and Helal, Maha and Fahmy, Aly",
        "title": "Automated radiology report generation using conditioned transformers"
      },
      {
        "key": "tanida2023interactive",
        "author": "Tanida, Tim and M{\\\"u}ller, Philip and Kaissis, Georgios and Rueckert, Daniel",
        "title": "Interactive and Explainable Region-guided Radiology Report Generation"
      },
      {
        "key": "chaves2024llavarad",
        "author": "Juan Manuel Zambrano Chaves and Shih-Cheng Huang and Yanbo Xu and Hanwen Xu and Naoto Usuyama and Sheng Zhang and Fei Wang and Yujia Xie and Mahmoud Khademi and Ziyi Yang and Hany Awadalla and Julia Gong and Houdong Hu and Jianwei Yang and Chunyuan Li and Jianfeng Gao and Yu Gu and Cliff Wong and Mu Wei and Tristan Naumann and Muhao Chen and Matthew P. Lungren and Akshay Chaudhari and Serena Yeung-Levy and Curtis P. Langlotz and Sheng Wang and Hoifung Poon",
        "title": "Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "jain2021radgraph",
        "author": "Jain, Saahil and Agrawal, Ashwin and Saporta, Adriel and Truong, Steven QH and Duong, Du Nguyen and Bui, Tan and Chambon, Pierre and Zhang, Yuhao and Lungren, Matthew P and Ng, Andrew Y and others",
        "title": "Radgraph: Extracting clinical entities and relations from radiology reports"
      },
      {
        "key": "yu2023evaluating",
        "author": "Yu, Feiyang and Endo, Mark and Krishnan, Rayan and Pan, Ian and Tsai, Andy and Reis, Eduardo Pontes and Fonseca, Eduardo Kaiser Ururahy Nunes and Lee, Henrique Min Ho and Abad, Zahra Shakeri Hossein and Ng, Andrew Y and others",
        "title": "Evaluating progress in automatic chest x-ray radiology report generation"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "chen2024chexagent",
        "author": "Zhihong Chen and Maya Varma and Justin Xu and Magdalini Paschali and Dave Van Veen and Andrew Johnston and Alaa Youssef and Louis Blankemeier and Christian Bluethgen and Stephan Altmayer and Jeya Maria Jose Valanarasu and Mohamed Siddig Eltayeb Muneer and Eduardo Pontes Reis and Joseph Paul Cohen and Cameron Olsen and Tanishq Mathew Abraham and Emily B. Tsai and Christopher F. Beaulieu and Jenia Jitsev and Sergios Gatidis and Jean-Benoit Delbrouck and Akshay S. Chaudhari and Curtis P. Langlotz",
        "title": "A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "bluethgen2024vision",
        "author": "Bluethgen, Christian and Chambon, Pierre and Delbrouck, Jean-Benoit and van der Sluijs, Rogier and Po{\\l}acin, Ma{\\l}gorzata and Zambrano Chaves, Juan Manuel and Abraham, Tanishq Mathew and Purohit, Shivanshu and Langlotz, Curtis P and Chaudhari, Akshay S",
        "title": "A vision--language foundation model for the generation of realistic chest x-ray images"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "kang2024wolf",
        "author": "Kang, Seil and Kim, Donghyun and Kim, Junhyeok and Lee, Hyo Kyung and Hwang, Seong Jae",
        "title": "WoLF: Large Language Model Framework for CXR Understanding"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "pellegrini2023radialog",
        "author": "Pellegrini, Chantal and {\\\"O}zsoy, Ege and Busam, Benjamin and Navab, Nassir and Keicher, Matthias",
        "title": "RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "chen2024chexagent",
        "author": "Zhihong Chen and Maya Varma and Justin Xu and Magdalini Paschali and Dave Van Veen and Andrew Johnston and Alaa Youssef and Louis Blankemeier and Christian Bluethgen and Stephan Altmayer and Jeya Maria Jose Valanarasu and Mohamed Siddig Eltayeb Muneer and Eduardo Pontes Reis and Joseph Paul Cohen and Cameron Olsen and Tanishq Mathew Abraham and Emily B. Tsai and Christopher F. Beaulieu and Jenia Jitsev and Sergios Gatidis and Jean-Benoit Delbrouck and Akshay S. Chaudhari and Curtis P. Langlotz",
        "title": "A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray Interpretation"
      }
    ]
  }
]