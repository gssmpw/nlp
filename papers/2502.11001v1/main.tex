
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\input{math_commands.tex}


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx} %added
\usepackage{algorithm} %added
\usepackage{algpseudocode} %added
\usepackage{xcolor} %added
\usepackage{makecell}
\iclrfinalcopy

\title{CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening}

\author{Gen Zhou\textsuperscript{1}\thanks{Equally Contributed} , 
Sugitha Janarthanan\textsuperscript{1}\footnotemark[1] ,
Yutong Lu\textsuperscript{2}, 
Pingzhao Hu\textsuperscript{1,}\textsuperscript{2}\thanks{Contact Author: phu49@uwo.ca}\\
\textsuperscript{1}{Western University, London, ON, Canada} \\
\textsuperscript{2} {University of Toronto, Toronto, ON, Canada}\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers have turned to machine learning techniques to streamline the prediction and development of novel antibiotic compounds. While foundation models have shown promise in antibiotic discovery, current mainstream efforts still fall short of fully leveraging the potential of multimodal molecular data. Recent studies suggest that contrastive learning frameworks utilizing multimodal data exhibit excellent performance in representation learning across various domains. Building upon this, we introduce CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP) using three types of molecular data. This model employs 1.6 million bioactive molecules with drug-like properties from the ChEMBL dataset to jointly pretrain three encoders: (1) a transformer-based encoder with rotary position embedding for processing SMILES strings; (2) another transformer-based encoder, incorporating a novel bi-level routing attention mechanism to handle molecular graph representations; and (3) a Morgan fingerprint encoder using a multilayer perceptron, to achieve the contrastive learning purpose. The CL-MFAP outperforms baseline models in antibiotic property prediction by effectively utilizing different molecular modalities and demonstrates superior domain-specific performance when fine-tuned for antibiotic-related property prediction tasks.
\end{abstract}

\section{Introduction}
Bacteria play a pivotal role in a diverse array of diseases within the human body, serving as either the primary cause or a contributing factor. A promising and sometimes sole treatment for these diseases is antibiotics, a specialized class of drugs designed to target pathogenic bacteria. Despite advancements, a lack of antibiotics for many pathogenic bacteria persists, and antibiotic resistance allows bacteria to survive once effective treatments. Consequently, there is a pressing demand for the continual development of antibiotics. However, traditional antibiotic discovery faces two major issues: 1) it is extremely costly and 2) it is very time-consuming. Artificial Intelligence (AI) and Machine Learning (ML) methods can combat these issues and thus, have been employed over the past couple of years to aid in antibiotic discovery for a wide range of conditions. Deep learning (DL) tools including convolutional, recurrent, and graph neural networks have been leveraged to explore high-dimensional data and design compounds with desired antibiotic properties \citep{cesaro2023deep}.

Large Language Models (LLMs) have increasingly stood out in recent years due to their exceptional performance, garnering widespread attention. As such, they have been implemented and fine-tuned to target pathogenic bacteria. For an LLM dedicated to the domain of antibiotic discovery, utilizing an extensive general molecular dataset for model training may not be a computationally cost-effective choice. By employing domain-specific training, the model can be taught to learn the unique characteristics, patterns, and nuances relevant to the field. \citet{gu2023domain} support this assertion, arguing that for fields like biomedicine, which have a large amount of unlabeled text, pretraining a model from scratch yields greater benefits than continual pretraining of a general-domain LLM. 

Contrastive learning, an effective method for utilizing large amounts of unlabeled data, has made significant progress in the field of ML in recent years. For antibiotic-related property prediction, contrastive learning significantly enhances model performance. Rather than relying on limited labeled molecular property data, this method leverages the vast amount of unlabeled molecular data available, helping identify patterns that contribute to a compound’s properties. The resulting molecular representations are thus more robust as they include patterns that may be missed by traditional supervised learning approaches. This leads to more accurate predictions, better generalization to novel chemical spaces, and ultimately increases the success rate of identifying potential antibiotic candidates.

In this study, we introduce a novel approach to streamline antibiotic discovery by leveraging a contrastive learning framework with multimodal data to train a domain-specific LLM. We propose CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP). CL-MFAP integrates a transformer-based encoder with rotary position embedding for SMILES strings, a transformer-based encoder using a novel Bi-Level Routing Attention (BRA) mechanism for molecular graphs, and a multilayer perceptron for Morgan fingerprint embeddings. This model is pre-trained on 1.6 million bioactive molecules with drug-like properties from the Chemical Database of Bioactive Molecules (ChEMBL) \citep{gaulton2012chembl}, a smaller, domain-specific dataset. Our comprehensive evaluation demonstrates that CL-MFAP outperforms baseline models trained on large-scale general datasets for antibiotic property prediction, while also exhibiting superior domain-specific performance when fine-tuned on targeted downstream tasks.

\section{Related Work}
\textbf{Transformers.} Among the current mainstream LLMs, the most representative architecture is the transformer. A transformer is a DL architecture consisting of encoder and/or decoder components built around multi-head attention mechanisms. Different models in the transformer family use these components distinctively: Bidirectional Encoder Representations from Transformers (BERT) series based solely on the encoder \citep{devlin2019bertpretrainingdeepbidirectional}, the Generative Pre-trained Transformer (GPT) series based solely on the decoder \citep{radford2018improving}, and the Text-to-Text Transfer Transformer (T5) series utilizing both the encoder and decoder \citep{raffel2020exploring}. The architecture's core features are self-attention computation and positional encoding \citep{vaswani}. The former is used to capture the semantic dependencies between the target word and the context and then determine its importance, while the latter understands the syntax and sequence information of the word by recording its position in the sequence. LLMs based on the transformer architecture have been widely proven to exhibit superior performance in capturing sequence semantics.

\textbf{LLMs for Molecular Property Prediction.} LLMs have recently gained popularity in molecular property prediction due to their enhanced success. MoLFormer is a successful unsupervised transformer-based LLM that accurately captures sufficient chemical and structural information to predict a diverse range of chemical properties \citep{ross2022large}. ChemBERTa is a stack of bidirectional encoders that uses representations from transformers for molecular property prediction \citep{chemberta} and is fine-tuned to better predict drug-target interactions \citep{kang2022fine}. MolBERT is a self-supervised model, consisting of the bidirectional attention mechanism-based BERT architecture \citep{molbert}. It is one of the most efficient pre-trained models for molecular property prediction that can be easily generalized to different molecular property prediction tasks via fine-tuning. All these examples of successful LLMs take in the structure of compounds in Simplified Molecular Input Line Entry System (SMILES) format for predictions.

\textbf{Contrastive Learning Models for Molecular Representation Learning.} As the field of drug development continues to advance, the integration and utilization of multimodal data have become essential for improving the performance of molecular property prediction LLMs. Contrastive learning can enhance a model's feature extraction capabilities by learning different representations of molecular data in the absence of labeled data. For example, MolCLR employs three distinct molecular graph augmentations to achieve contrastive learning, significantly improving the model's ability to learn molecular representations \citep{wang2022molecular}. UniCorn combines several pretraining methods: 2D graph masking, 2D-3D contrastive learning, and 3D denoising, to depict molecular views from three different levels, resulting in superior performance compared to traditional models \citep{unicorn}.
 
\section{Proposed Approach}
\subsection{Model Development}

\begin{figure}
 \centering
 \includegraphics[width=0.99\linewidth]{Architecture.pdf}
 \caption{Illustration of the proposed approach.}
 \label{workflow}
\end{figure}

We designed a multimodal contrastive learning model based on molecular SMILES, Morgan fingerprints, and molecular graphs to comprehensively capture different chemical characteristics. The input data is SMILES representations, which describe the linear form of a molecule, including information about its composition, bond types, and functional groups, used to depict the overall connectivity of the molecular structure. From the SMILES representation, Morgan fingerprints and molecular graphs are constructed. Morgan fingerprints provide a quantitative representation of the molecule’s features, encoding its structure as a high-dimensional binary vector that captures the presence and distribution of various substructures and functional groups. Specifically, a radius size of 2 was determined through ablation studies detailed in Appendix A.1 (Table~\ref{morgan_abl}), and a 2048-bit vector was chosen as it is large enough to minimize hash collisions (where different structural features map to the same bit) while being computationally efficient. The graph representation of a molecule describes its topology through nodes (atoms) and edges (chemical bonds), including details about the atom types, bond characteristics, and overall connectivity. Ablation studies were performed to validate the inclusion of these three modalities, detailed in Appendix A.2 (Table~\ref{modality_abl}). Altogether, the model receives a widespread in-depth representation of each compound, allowing it to learn the specificities and patterns of the compounds that influence their antibiotic-related properties.

Figure~\ref{workflow} illustrates the overall architecture of our model, which learns the three molecular feature modalities mentioned above through different embedding pathways. First, the model employs a transformer-based graph encoder with a novel bidirectional relation aggregation (BRA) mechanism to learn the molecular graph features. Second, the transformer encoder with rotary positional embedding is used to learn the SMILES features of the molecule. This self-attention-based encoder excels at capturing global information in sequential data and handling complex contextual dependencies. Finally, to encode Morgan fingerprints, we use a multilayer perceptron (MLP), a classical feedforward neural network capable of processing high-dimensional data and extracting complex features. 

\subsubsection{Rotary Positional Embedding}
Rotary Positional Embedding (RoPE) is an improved positional encoding method used in transformer models \citep{roformer}. The rotation transformation effectively integrates positional information into each token and helps the model capture dependencies between distant tokens. Together, this preserves the relative position relationships between elements and improves prediction accuracy. This method is particularly suitable for processing molecular data with complex structural dependencies, as it improves the model's ability to understand sequential structural relationships. In the two-dimensional case, the formula for implementing rotary positional encoding through complex multiplication is as follows:
\begin{equation}
g(x_m, x_n, m-n) = \text{Re}\left[(W_q x_m)(W_k x_n)^* e^{i(m-n)\theta}\right]
\label{eq:g_function}
\end{equation}

where $m$ and $n$ are positions, $q$ is the query, $k$ is the key, $W_q$ is the query projection matrix, $W_k$ is the key projection matrix, $Re[\cdot]$ is the real part of a complex number, $(W_qx_m)$ represents the complex conjugate, and $(x_m,x_n)$ denotes the representation in a two-dimensional coordinate system. Through this rotation formula, a rotational transformation is achieved, generating the rotary positional encoding.
The original linear attention formula is expressed as follows, where $\varphi(\cdot)$ and $\phi(\cdot)$ are typically non-negative functions:
\begin{equation}
\text{Attention}(Q,K,V)_m = \frac{\sum_{n=1}^N \phi(q_m)^T \varphi(k_n) v_n}{\sum_{n=1}^N \phi(q_m)^T \varphi(k_n)}
\label{eq:attention_function}
\end{equation}

where $v_n$ is the value of nth token, $q_m$ is the query, and $k_n$ is the key.

Combining both equations 1 and 2 gives equation 3. RoPE injects positional information through rotation, which keeps the norm of hidden representations unchanged. Thus, RoPE is combined with linear attention by multiplying the rotation matrix with the outputs of the non-negative function as follows:
\begin{equation}
\text{Attention}(Q,K,V)_m = \frac{\sum_{n=1}^N (R_{\Theta,m}^d \phi(q_m))^T (R_{\Theta,n}^d \varphi(k_n)) v_n}{\sum_{n=1}^N \phi(q_m)^T \varphi(k_n)}
\label{eq:attention_function_rotary}
\end{equation}

where $R_{\Theta}^d$ is the orthogonal matrix used in RoFormer \citep{roformer} that ensures stability during the process of encoding position information.

\subsubsection{Bi-level Routing Attention}
The Bi-level Routing Attention (BRA) mechanism is crucial, as it partitions the attention mechanism into two phases: an initial focus on global relationships followed by a more detailed scrutiny of local specifics. In conventional applications within computer vision, the BRA mechanism first identifies critical areas within an image and then focuses on local details. For instance, in an image featuring a dog, the model would initially identify the most prominent features, such as the dog's head, across the entire image, and then subsequently focus on local details such as the eyes and nose within the defined window.

In molecular graphs, diverse structural features are exhibited by different molecules, and these features significantly influence the functional performance of the molecules. For antibiotic molecules, complex cyclic structures represent a typical characteristic, the importance of which often surpasses other local structures in medicinal functionality, making precise understanding by the model crucial. In our model, through the Window-to-Window Attention mechanism of BRA, the model efficiently identifies and focuses on key structures and functional groups within the molecular graph that are central to functionality, such as cyclic structures. Concurrently, for peripheral structures or less likely node-edge combinations that have minimal impact on molecular functionality, the model minimizes their importance or filters them out through a dynamic adjustment mechanism, thereby achieving a clear prioritization in feature learning.

The BRA mechanism has been proven effective in handling long-range dependencies in images within the field of computer vision, and the same theory applies to molecular graphs \citep{biunet}. Compared to the traditional approach of graph transformers which use classical attention, the BRA first filters out irrelevant key-value pairs at a coarse regional level, significantly reducing the number of potential interactions that need to be considered in the subsequent fine-grained token-to-token attention phase. This two-step filtering process ensures that attention is focused on areas most relevant to the query, enhancing the model's ability to manage long-range dependencies without the computational overhead of attending to all token pairs.

\textbf{Window-to-Window Level Routing.} This mechanism efficiently computes attention across regions of a feature map while considering local context. Beginning with a 2D feature map, \( X \in \mathbb{R}^{H \times W \times C} \), a linear transformation is applied to create three tensors: $Q$ (query), $K$ (key), and $V$ (value), as shown in Equation 4.
\begin{equation}
Q = XW_q,K=XW_k,V=XW_v
\label{eq:window_level_routing}
\end{equation}

where $W_q$, $W_k$, and $W_v$ are the learnable projection weights, each of size \(\mathbb{R}^{C \times C}\). 

To perform window-to-window level routing, the feature map is divided into $S$ x $S$ non-overlapping windows, each containing \(\frac{HW}{S^2}\) feature vectors, resulting in reshaped $Q’$, $K’$ and $V’$. The window size $S$ is set to 7, based on ablation studies explained in Appendix A.1 (Table~\ref{windowsize_abl}). Within each window, the $Q’$, $K’$, and $V’$ tensors are used to compute the average, resulting in $Q^w$ and $K^w$, which are the window-level representations for each non-overlapping window. These are then used to calculate the window-to-window score matrix (containing window-to-window attention scores) as shown in Equation 5.
\begin{equation}
A^w = Q^w(K^w)^T
\label{eq:window_level_routing2}
\end{equation}

In the score matrix, each row contains the indexes of the top-$k$ windows that are most relevant to the corresponding window.

\textbf{Pixel-to-Pixel Level Attention.} For window $I$, its top-$k$ relevant windows are scattered across the feature map. To gather these windows together, we use the following equation to collect $K^g$ and $V^g$:
\begin{equation}
K^g = \text{gather}(K,I^w),V^g=\text{gather}(V,I^w)
\label{eq:pixel_level_attention}
\end{equation}

$K^g$ and $V^g$ represent the collected Key and Value tensors containing features from the top-$k$ windows relevant to the current window $I$, respectively. For a given pixel $j$ within a window $I$, the pixel will attend to all pixels in the top-$k$ windows most relevant to window $I$. This ensures a fine-grained attention mechanism, allowing the model to refine feature representations at the individual pixel level.

\begin{algorithm}
\caption{Bi-Level Routing Attention}
\begin{algorithmic}[1]
  \State \textbf{\#Graph:}
  \State $graphTokenFeature, nodeFeature \gets \text{processSmilesToGraph}(smilesString)$
  \State $graphNodeFeature \gets \text{concatenate}(graphTokenFeature, nodeFeature)$
  \State $nodeFeatureMatrix \gets \text{createNodeFeatureMatrix}(graphNodeFeature)$
  \Statex

  \State \textbf{\#Bi-Level Routing Attention:}
  
  \State \text{\#Window-to-Window Level Routing:}
  \State $windows \gets \text{divideIntoWindows}(nodeFeatureMatrix)$
  \State $distances \gets \text{calculateDistancesBetweenWindows}(windows)$
  \State $topKWindows \gets \text{selectTopKWindows}(windows, distances, k)$
  \Statex

  \State \text{\#Pixel-to-Pixel Level Attention:}
  \State $attentionEmbedding \gets \text{gather}(\text{pixelLevelAttention}(topKWindows))$
\end{algorithmic}
\end{algorithm}

Algorithm 1 presents the basic architecture of the Bi-Level Routing Attention (BRA) algorithm, including the processing of input data and the implementation logic of BRA. To our knowledge, this is the first time BRA has been introduced into the attention mechanism for processing molecular graphs. We utilize a transformer-based graph encoder, equipped with 8 attention heads and 12 encoder layers, a configuration particularly suited for analyzing and interpreting complex molecular structures \citep{graphrep}. 

\subsubsection{Multimodal Contrastive Learning}
The advantage of a multimodal model lies in its ability to integrate information from different modalities, thus obtaining a more comprehensive understanding of molecular structure that enhances the robustness and generalization of the model. Contrastive learning is an approach that enhances feature learning by pulling similar pairs closer together while pushing dissimilar pairs apart. This approach significantly improves representation quality as it facilitates learning similarities and associations across different modalities. It also aids in the limited data issue commonly associated with antibiotic property discovery by leveraging the unlabeled molecular data available.

\begin{algorithm}
\caption{Multimodal Contrastive Learning}
\begin{algorithmic}[1]
\Function{contLearningModel}{$smilesBatch$, $fpBatch$, $graphBatch$}
  \State $smilesOutput \gets \text{SmilesEncoder}(smilesBatch)$
  \State $fpOutput \gets \text{FpEncoder}(fpBatch)$
  \If{BiGraphormerEncoder with MPNN}
    \State $graphOutput \gets \text{MPNNEncoder(graphBatch)} + \text{BiGraphormerEncoder(graphBatch)}$
  \ElsIf{BiGraphormerEncoder without MPNN}
    \State $graphOutput \gets \text{BiGraphormerEncoder(graphBatch)}$
  \ElsIf{BiGraphormerEncoder without Bi-level routing attention}
    \State $graphOutput \gets \text{MPNNEncoder(graphBatch)} + \text{GraphormerEncoder(graphBatch)}$
  \EndIf
  \State \Return $smilesOutput, fpOutput, graphOutput$
\EndFunction
\Statex
\Function{computeLoss}{$smilesOutput$, $fpOutput$, $graphOutput$}
  \State //Loss Function (Initial Weight w1, w2, w3)
  \State $lossSmilesFP \gets \text{NT-Xent}(smilesOutput,fpOutput)$
  \State $lossSmilesGraph \gets \text{NT-Xent}(smilesOutput,graphOutput)$
  \State $lossFPGraph \gets \text{NT-Xent}(fpOutput,graphOutput)$
  \State $totalLoss \gets w_1 \cdot lossSmilesF\!P + w_2 \cdot lossSmilesGraph + w_3 \cdot lossF\!PGraph$
  \State \Return $totalLoss$
\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm 2 illustrates the basic architecture and loss computation of the multimodal contrastive learning model. In our model, SMILES, Morgan fingerprints, and molecular graphs are encoded using dedicated encoders and the representations are then processed through a contrastive learning framework, using Normalized Temperature-Scaled Cross-Entropy (NT-Xent) as the fundamental loss function to compare pairs across modalities (Equation 7)\citep{graphcontlearn}. NT-Xent Loss learns well-distributed feature representations by maximizing the similarity of similar samples (positive pairs) and minimizing the similarity of dissimilar samples (negative pairs). The function takes the concatenated vectors of two modalities for two molecules as input and calculates the loss for each pair of modalities. For example, for SMILES and molecular graphs, we first compute the concatenated vector of the SMILES embedding and the graph embedding, then compute a similarity matrix for all pairwise samples for the two modalities, and finally calculate the loss between two modalities. To enable the use of NT-Xent loss with different modalities, we project the representations from different modalities into the same vector space. In each iteration, different modalities of the same molecule are treated as positive pairs, while representations from different molecules are treated as negative pairs. NT-Xent loss is advantageous as it effectively measures the similarity between high-dimensional embeddings from different modalities, emphasizing the alignment of directions rather than absolute values, which is crucial for robust multimodal learning.
\begin{equation}
Lc = -log\frac{exp(\frac{sim(x,x'_i)}{\tau})}{\sum_{j=1}^n exp(\frac{sim(x,y_i)}{\tau})}
\label{eq:lc}
\end{equation}

The total loss is defined in Equation 8, where $i$ and $j$ represent two different molecules, and $m$ and $n$ denote different data modalities. For each modality pair, we assign a weight, and the total loss is calculated as the weighted sum of these individual losses.
\begin{equation}
L = \sum_{mn} w_{mn} \left( \sum \left( L_c(x_{im} + x_{in}, x_{jm} + x_{jn}) + L_c(x_{im} + x_{in}, x_{im}' + x_{in}') \right) \right)
\label{eq:loss}
\end{equation}

\nopagebreak
\subsection{pretraining Process}
\textbf{Dataset and Pre-processing}. The ChEMBL24 database was downloaded after the removal of salts, charge neutralization, removal of molecules with SMILES strings longer than 100 characters, removal of molecules containing any element other than H, B, C, N, O, F, Si, P, S, Cl, Se, Br, and I, and removal of molecules with a larger ECFP4 similarity than 0.323 compared to a holdout set consisting of 10 marketed drugs (celecoxib, aripiprazole, cobimetinib, osimertinib, troglitazone, ranolazine, thiothixene, albuterol, fexofenadine, mestranol) \citep{gaulton2012chembl} \citep{Fiscato2018}. Pre-processing was then applied to the raw molecular data, which included de-duplication, normalization via conversion to canonical SMILES using RDKit \citep{rdkit}, and removal of entries with over 123 tokens, as these molecules are exceedingly rare in practical applications \citep{ross2022large}. After processing, we obtained 1,591,020 SMILES for model training. The preprocessed data was divided into \( 80\%-10\%-10\% \) for training, validation, and testing, respectively. Given the input data of SMILES strings, CL-MFAP generates Morgan fingerprints and molecular graphs using RDKit \citep{rdkit} and all three types of data are subsequently used to train the model. Pretraining CL-MFAP using ChEMBL was validated via ablation studies detailed in Appendix A.1 (Table~\ref{pretrain_abl}).

\textbf{Domain-specific.} Our target domain contains bioactive molecules with drug-related compounds from ChEMBL, whereas other large-scale databases, such as PubChem, typically include much more widely used, commercially available molecules \citep{lyubishkin2018} \citep{kim2016}.

\section{Experiments}
\subsection{Implementation Details}
\textbf{Environment.} All implementations were conducted on the PyTorch platform using an NVIDIA A100 GPU. All models were trained using a learning rate of 1e-4, over 20 epochs, with batch size 8 and 4 worker processes. The Adam optimizer and gradient clipping were also applied during training, limiting the gradient norm to 1.0. For the bi-level routing attention, the window size is 7, the number of top k windows is 4, and there are 16 pixels per window and 8 attention heads. 

\textbf{Pre-trained CL-Models.} To analyze the contribution of each component along the molecular graph embedding path—graph transformer encoder (GTE) and the newly introduced BRA—as well as to test whether combining this GTE with a message-passing neural network (MPNN) can further enhance the model’s ability to capture global information, we pre-trained five models within the overall framework of multimodal contrastive learning which differ in structural configurations along the graph embedding path. Aside from CL-MFAP, the other four models are labeled as Contrastive Learning Baseline 1-4 (CL-BL1-4). The labels and structures of all the models pre-trained under the multimodal contrastive learning framework are presented in Table~\ref{models}.

\begin{table}[H]
\caption{Proposed pre-trained models with different graph embedding paths}
\label{models}
\begin{center}
\begin{tabular}{lll}
\multicolumn{1}{c}{\bf Model Name} &\multicolumn{1}{c}{\bf Structural Configuration} &\multicolumn{1}{c}{\bf Graph Embedding Description}
\\ \hline \\
CL-MFAP     & Proposed Model & GTE + BRA \\
CL-BL1       &CL-MFAP w/ MPNN & GTE + BRA + MPNN\\
CL-BL2       &CL-MFAP w/ MPNN w/o BRA & GTE + MPNN \\
CL-BL3       &CL-MFAP w/o BRA & GTE \\
CL-BL4       &CL-MFAP w/ MPNN w/o BRA w/o GTE & MPNN \\
\end{tabular}
\end{center}
\end{table}

\textbf{Model Size.} Moreover, we measured the size of our models in terms of Params and FLOPs to further evaluate their performance and cost efficiency. Params refer to the number of trainable parameters in a model. This is directly related to the structure of the model, representing each learnable weight, including weights and biases in different layers. As such, it serves as a measure of the model’s complexity and storage requirements \citep{Han_2024_CVPR}. FLOPs refer to the number of floating-point operations performed during a single forward pass of the model. This metric measures the computational complexity and cost of the model, providing insight beyond just the number of parameters. FLOPs are closely related to the model's inference speed and the computational resources required for its operation \citep{Han_2024_CVPR}.

\subsection{Downstream Property Predictions}
\textbf{Datasets.} Six datasets were used for downstream property prediction: MIC activity against {\textit{E. coli}} (\textit{E. coli} MIC) dataset curated from COADD database \citep{coadd}, MIC activity against {\textit{H. influenzae} (\textit{H. influenzae} MIC) dataset curated from ChEMBL database \citep{gaulton2012chembl}}, BACE \citep{moleculenet}, Blood-Brain Barrier Penetration (BBBP) \citep{moleculenet}, Parallel Artificial Membrane Permeability Assay (PAMPA) \citep{pampa}, and Bioavailability \citep{bioavailability}. All datasets were divided into \( 80\%-10\%-10\% \) for training, testing and validation, respectively. More details can be found in Appendix A.1.

\textbf{Baseline Models.} We selected MoLFormer, ChemBERTa-2, MolBERT, MolCLR, and FP-GNN as baselines to evaluate the performance of CL-MFAP. MoLFormer, a transformer-based model, was trained on 1.1 billion molecules from ZINC and PubChem databases \citep{ross2022large}. ChemBERTa-2, a BERT-based LLM with 12 encoders, was trained on 77 million PubChem compounds \citep{chemberta2}. MolBERT, another BERT-based model with 12 encoders, was trained on a smaller dataset of 1.6 million ChEMBL molecules \citep{molbert}. MolCLR employs graph neural networks and contrastive learning with molecular augmentations, trained on 10 million PubChem SMILES \citep{wang2022molecular}. Finally, FP-GNN is a multimodal framework that combines molecular graphs and fingerprints for property prediction \citep{cai_fp-gnn_2022}.

\textbf{Mean Reciprocal Rank.} To more intuitively evaluate the overall performance of each model across all downstream tasks, we employed the mean reciprocal rank (MRR) method, a statistical approach that synthesizes the rankings of all models on various downstream tasks \citep{6027363}. This method assigns a corresponding score to each model, with higher scores indicating superior overall performance. We first recorded the rank of each model's ROC-AUC metric in comparison to the other models for each task and then used the ranks to calculate the model's MRR value using the following equation:

\begin{equation}
MRR = \frac{1}{|Q|}\sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
\end{equation}

where $i$ denotes the task index and $Q$ represents the total number of tasks.

\subsection{Results}
The performance of CL-MFAP on downstream property prediction tasks was compared against all baselines. Using Area under the Receiver Operating Characteristic Curve (ROC-AUC) as the evaluation metric, the experimental results are summarized in Table~\ref{rocauc}. Notably, CL-MFAP outperforms all other baseline models on the \textit{E. coli} MIC dataset (ROC-AUC: 0.854$\pm$0.037), which is particularly relevant for antibiotic drug discovery as it predicts the antibacterial activity of compounds against \textit{E. coli}. In addition, it performs second best on the \textit{H. influenzae} MIC dataset (ROC-AUC:0.874$\pm$0.015), with negligible difference from the best performing model, MoLFormer (ROC-AUC:0.876$\pm$0.017). We noted similar performance for pre-trained chemical language models (CL-MFAP, MoLFormer, MolBERT, and ChemBERTa-2) that outperform models without pretraining (MolCLR and FP-GNN). Together, these results show the ability of CL-MFAP to exceed in antibacterial activity prediction, regardless of sample size. Thus, our model can also predict antibacterial activity for less studied bacterial strains with less data. On the remaining datasets, our model demonstrates consistently strong performance, ranking among the top 2 or 3 models, unlike other baselines that excel in only 1–2 datasets. This highlights the robustness and generalizability of CL-MFAP across diverse tasks.

When ranked by MRR scores, CL-MFAP significantly outperforms the other models (Figure~\ref{mrr}). The elevated MRR scores underscore the model's superior overall performance, reaffirming its effectiveness and broad applicability.

\begin{table}[h]
\caption{ROC-AUC of CL-MFAP vs. baseline models on downstream property prediction datasets}
\setlength{\tabcolsep}{4pt}
\label{rocauc}
\begin{center}
\begin{tabular}{lcccccc}
\textbf{Model} & \makecell{\textit{\textbf{E. coli}} \\ \textbf{MIC}}
&\makecell{\textit{\textbf{H. influenzae}} \\ {\textbf{MIC}}} & \textbf{BBBP} & \textbf{PAMPA} & \makecell{\textbf{Bioavail-} \\ \textbf{ability}} & \textbf{BACE} \\
\hline \\
CL-MFAP & {0.85$\pm$0.04} & {0.87$\pm$0.02} & {0.93$\pm$0.01} & {0.76$\pm$0.03} & {0.60$\pm$0.03} & {0.88$\pm$0.01} \\
MoLFormer & {0.71$\pm$0.01} & {0.88$\pm$0.02} & {0.93$\pm$0.01} & {0.72$\pm$0.03} & {0.72$\pm$0.06} & {0.87$\pm$0.02} \\
MolBERT & {0.77$\pm$0.00} & {0.87$\pm$0.03} & {0.97$\pm$0.01} & {0.73$\pm$0.05} & {0.75$\pm$0.08} & {0.89$\pm$0.02} \\
ChemBERTa-2 & {0.74$\pm$0.03} & {0.86$\pm$0.02} & {0.97$\pm$0.01} & {0.67$\pm$0.03} & {0.70$\pm$0.07} & {0.81$\pm$0.01} \\
{MolCLR} & {0.71$\pm$0.01} & {0.86$\pm$0.02} & {0.93$\pm$0.01} & {0.76$\pm$0.02} & {0.63$\pm$0.16} & {0.86$\pm$0.01} \\
{FP-GNN} & {0.75$\pm$0.02} & {0.87$\pm$0.02} & {0.94$\pm$0.01} & {0.75$\pm$0.01} & {0.75$\pm$0.04} & {0.87$\pm$0.01} \\
\end{tabular}
\end{center}
\end{table}

\begin{figure}[h]
 \centering
 \includegraphics[width=0.6\linewidth]{fig2new1.png}
 \caption{Mean reciprocal rank (MRR) of the average performance for CL-MFAP versus baseline models. CL-MFAP demonstrates superior overall performance.}
 \label{mrr}
\end{figure}

\subsection{Ablation Studies}
\textbf{Overall Performance Ranking of CL-based Models.} We compared the performance of five pre-trained CL models to verify the effectiveness of different components in the graph embedding path. We evaluated the performance of these pre-trained CL models on the downstream tasks using ROC-AUC (Table~\ref{pretrain_ROC}) and then ranked the performance of each model across all tasks based on these findings. As shown in Table~\ref{ranking}, CL-MFAP outperforms the model variations in 5 out of 6 downstream tasks. To further assess model performance and cost efficiency, an MRR analysis of the overall model rankings was performed. The model size, represented via Params (Figure~\ref{mrrfig}A) and FLOPs (Figure~\ref{mrrfig}B), was plotted against the MRR score. CL-MFAP's top-left position in Figure~\ref{mrrfig}A highlights its superior performance with fewer parameters.

\begin{table}[h]
\caption{Overall performance ranking on downstream property prediction datasets for all pre-trained CL models}
\label{ranking}
\begin{center}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lcccccc}
\multicolumn{1}{c}{\bf Model} 
& \makecell{\textit{\textbf{E. coli}} \\ \textbf{MIC}}
& \makecell{\textit{\textbf{H. influenzae}} \\ \textbf{MIC}}
&\multicolumn{1}{c}{\bf BBBP} 
&\multicolumn{1}{c}{\bf PAMPA} 
&\makecell{\textbf{Bioavai-} \\ \textbf{lability}} &\multicolumn{1}{c}{\bf BACE}
\\ \hline \\
CL-MFAP (GTE, BRA) & 1 & {1} & 1 & 1 & 5 & 1 \\
CL-BL1 (GTE, BRA, MPNN) & 3 & {4} & 2 & 2 & 2 & 3 \\
CL-BL2 (GTE, MPNN) & 4 & {2} & 4 & 3 & 1 & 4 \\
CL-BL3 (GTE) & 2 & {3} & 3 & 5 & 4 & 2 \\
CL-BL4 (MPNN) & 5 & {5} & 5 & 4 & 3 & 5 \\
\end{tabular}
\end{center}
\end{table}


%\makecell[l]{CL-BL1 \\ (GTE, BRA, MPNN)}

\begin{figure}[h]
 \centering
 \includegraphics[width=1\linewidth]{3new.png}
 \caption{Mean reciprocal rank (MRR) of the ROC-AUC rankings for all CL models on downstream property prediction datasets plotted against (3A) Params, and (3B) FLOPs. Models closer to the top left corner demonstrate better performance with fewer parameters (3A) and lower FLOPs (3B).}
 \label{mrrfig}
\end{figure}
\textbf{Ablation study on the BRA.} We conducted an ablation analysis on the contribution of BRA by comparing CL-MFAP vs. CL-BL3, and CL-BL1 vs. CL-BL2. The former compares the impact of BRA in the absence of MPNN, while the latter compares the effect of BRA when MPNN and GTE are used together. In both cases, models with BRA consistently outperform their counterparts (Table~\ref{ranking}, Figure~\ref{mrrfig}). Therefore, BRA plays a significant role in enhancing model performance.

\textbf{Ablation study on the MPNN.} The value of MPNN was also evaluated. As we initially hypothesized that introducing MPNN could help further capture comprehensive information \citep{pmlr-v202-cai23b}, we introduced an MPNN path running parallel to GTE in the graph embedding process. However by comparing the results of CL-MFAP vs. CL-BL1 and CL-BL3 vs. CL-BL2, introducing MPNN weakens the performance of the model and thus was not incorporated (Table~\ref{ranking}, Figure~\ref{mrrfig}).

\textbf{Ablation study on the GTE.} We also analyzed whether GTE is replaceable. A comparison between CL-BL3 and CL-BL4 shows that replacing GTE with MPNN for molecular graph encoding significantly decreases model performance. Also, when comparing CL-BL2 and CL-BL4, despite MPNN weakening the performance of GTE, the combination of GTE and MPNN still outperforms MPNN alone (Table~\ref{ranking}, Figure~\ref{mrrfig}). Thus, GTE is essential for encoding molecular graphs in our model.

In addition, additional ablation analyses (to analyze the effects of window size, data modalities, pretraining CL-MFAP, and Morgan fingerprint radius on model performance), Representation-Property Relationship Analysis (RePRA), and a case study were performed, detailed in Appendix A.1 (Table~\ref{morgan_abl}-~\ref{pretrain_abl}), A.4 (Table~\ref{RePRA_tab}, Figure~\ref{RePRA_fig}), and A.5 (Table~\ref{bm_scaff}-~\ref{fp_sim}), respectively.

\section{Conclusion}
In this work, we present CL-MFAP, a novel multimodal contrastive learning framework. The model combines and compares molecular information from three modalities - SMILES, molecular graphs and fingerprints - to efficiently learn representations of molecules that improve its performance in predicting antibiotic-related properties. We also, for the first time, incorporate the BRA mechanism to enhance the quality of molecular representation learning. Experimental results demonstrate that CL-MFAP achieves outstanding performance in predicting drug molecule properties. In the future, we aim to integrate this model with other cross-domain potential modules and further refine its multimodal contrastive learning algorithm to enhance its generalization capabilities.

All code can be found at \url{https://github.com/CLMFAP/CLMFAP}. 

\subsubsection*{Acknowledgments}
This research was supported in part by the Canadian Institutes of Health Research (PLL185683, PJT 190272), the Canada Research Chairs Tier II Program (CRC-2021-00482), the Natural Sciences and Engineering Research Council of Canada (RGPIN-2021-04072) and the Canada Foundation for Innovation (CFI) John R. Evans Leaders Fund (JELF) program (grant \#43481). 

A special thanks to Yan Yi Li and Zihao Jing for their help in this work.

\bibliography{main}
\bibliographystyle{iclr2025_conference}

\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}

\newpage
\appendix

\section{Appendix}
\subsection{Additional Ablation Studies}
\textbf{Ablation study on Morgan Fingerprint Radius}. We performed an additional ablation study to investigate the effect of the Morgan fingerprint radius size on CL-MFAP’s predictive capabilities. CL-MFAP was tested with five fingerprint radius sizes (0, 1, 2, 3, 4, and 5) \footnote{Due to time constraints, all of these fine-tuning evaluations were performed for 3 epochs, as compared to 20 epochs used for our final CL-MFAP model.}. As shown in Table~\ref{morgan_abl}, a radius of size 2 has the best overall performance, achieving the highest results in 5 of the 6 downstream datasets, proving that it is the best radius size for CL-MFAP.

\begin{table}[h]
\caption{ROC-AUC of CL-MFAP models with varying Morgan fingerprint radius sizes on downstream property prediction datasets. Models are named based on their respective fingerprint radius sizes in the format MR\_\textit{fingerprint radius size}.}
\setlength{\tabcolsep}{5pt}
\label{morgan_abl}
\begin{center}
\begin{tabular}{ccccccccc}
\textbf{Model} & \makecell{\textbf{Fingerprint} \\ \textbf{Radius Size}} & \makecell{\textit{\textbf{E. coli}} \\ \textbf{MIC}} & \makecell{{\textit{\textbf{H. influenzae}}} \\ {\textbf{MIC}}} & \textbf{BBBP} & \textbf{PAMPA} & \textbf{Bioavailability}& \textbf{BACE}\\
\hline \\
MR\_0 & 0 & 0.827 & 0.846 & 0.905 & 0.747 & 0.535 & 0.886\\
MR\_1 & 1 & 0.843 & 0.857  & 0.900 & 0.721 & 0.523 & 0.880\\
MR\_2 & 2 & 0.854 & 0.855  & 0.928 & 0.747 & 0.605 & 0.882\\
MR\_3 & 3 & 0.849 & 0.853  & 0.913 & 0.738 & 0.546 & 0.880\\
MR\_4 & 4 & 0.852 & 0.858 & 0.900 & 0.719 & 0.553 & 0.868\\
\end{tabular}
\end{center}
\end{table}

\textbf{Ablation study on Data Modalities.} We removed each of the three data modalities from CL-MFAP individually and assessed its performance on downstream property prediction tasks to determine their individual impact. As shown in Table~\ref{modality_abl}, We observe that removing either the SMILES or the molecular fingerprints results in a certain degree of performance decline. This suggests that both data modalities contribute approximately equally to the overall model performance, with the impact of removing Fingerprints being slightly greater than removing SMILES. However, when we remove the molecular graph modality, the model performance experiences a significant drop. This indicates that the primary contributor to our model's performance is the molecular graph, processed through the GTE integrated with the BRA mechanism, which aligns well with our assumptions.

\begin{table}[h]
\caption{ROC-AUC of CL-MFAP models with varying data modalities on downstream property prediction datasets. Models are named based on their missing modalities in the format M\_\textit{missing modality}.}
\label{modality_abl}
\begin{center}
\begin{tabular}{lccccccc}
\textbf{Model} & \makecell{\textbf{Missing} \\ \textbf{Modality}} & \makecell{\textit{\textbf{E. coli}} \\ \textbf{MIC}} &
\makecell{\textit{\textbf{H. influenzae}} \\ \textbf{MIC}} & \textbf{BBBP} & \textbf{PAMPA} &\makecell{\textbf{Bioavai-} \\ \textbf{lability}} & \textbf{BACE} \\
\hline \\
M\_none & NA & 0.875 & 0.855 & 0.941 & 0.784 & 0.559 & 0.891\\
\\
M\_noSMI & SMILES & 0.834 & 0.741 & 0.920 & 0.720 & 0.568 & 0.877\\
\\
M\_noFP & Fingerprint & 0.784 & 0.859& 0.903 & 0.725 & 0.622 & 0.878 \\
\\
M\_noGraph & Graphs & 0.541 & 0.512& 0.656 & 0.633 & 0.647 & 0.625\\
\end{tabular}
\end{center}
\end{table}

\textbf{Ablation study on Window Size.} We tested several different window sizes in our CL-MFAP to study their impact and determine the most optimal choice. Six distinct window sizes (2, 3, 5, 7, 9, and 11) were evaluated for CL-MFAP, and their performance on downstream property prediction tasks was assessed. As shown in Table~\ref{windowsize_abl}, a window size of 7, representing a moderate configuration, achieved the best performance in 4 out of 5 tasks. In contrast, performance tends to decline when the window size is either too large or too small. This was predicted as when the window size is too small, the BRA mechanism is confined to focusing on highly local regions, overly emphasizing fine-grained details, and, to some extent, losing the ability to capture long-range dependencies. On the other hand, when the window size is too large, the sparsity of the BRA mechanism becomes excessive, leading to the dilution of some critical local information, which partially undermines the effectiveness of routing and aggregation. As a result, we set the default window size to 7.

\begin{table}[h]
\caption{ROC-AUC of CL-MFAP models with varying window sizes on downstream property prediction datasets. Models are named based on their respective window sizes in the format MW\_\textit{window size}.}
\label{windowsize_abl}
\begin{center}
\begin{tabular}{lccccccc}
\multicolumn{1}{c}{\bf Model} &\multicolumn{1}{c}{\bf Window Size} & \makecell{\textit{\textbf{E. coli}} \\ \textbf{MIC}} &
\makecell{\textit{\textbf{H. influenzae}} \\ \textbf{MIC}}
&\multicolumn{1}{c}{\bf BBBP} &\multicolumn{1}{c}{\bf PAMPA} &\makecell{\textbf{Bioavai-} \\ \textbf{lability}} &\multicolumn{1}{c}{\bf BACE}
\\ \hline \\
MW\_S2 & 2 & 0.847 &0.840 & 0.913 & 0.715 & 0.557 & 0.856 \\
MW\_S3 & 3 & 0.844 & 0.849 & 0.909 & 0.717 & 0.564 & 0.851\\
MW\_S5 & 5 & 0.831 & 0.841 & 0.902 & 0.754 & 0.507 & 0.890\\
MW\_S7 & 7 & 0.875 & 0.855 & 0.941 & 0.784 & 0.559 & 0.891\\
MW\_S9 & 9 & 0.830 & 0.848 & 0.914 & 0.731 & 0.632 & 0.872\\
MW\_S11 & 11 & 0.837 & 0.845 & 0.928 & 0.715 & 0.526 & 0.887\\
\end{tabular}
\end{center}
\end{table}

\textbf{Ablation study on Pretraining CL-MFAP.} We performed an ablation study to investigate whether pretraining on the larger ChEMBL dataset improves model performance. CL-MFAP with and without ChEMBL pretraining was trained/finetuned on all downstream property prediction datasets. As shown in Table~\ref{pretrain_abl}, in 5 of 6 tasks, dropping the pretraining slightly weakens model performance, although not significantly. This indicates that while pretraining enhances model performance and represents the ideal scenario, our algorithm and novel methodology are still able to achieve excellent results even without pretraining. In scenarios where cost-effectiveness is prioritized in training resource consumption, the model can handle the intended use cases to a similar extent without pretraining.

\begin{table}[h]
\caption{ROC-AUC of CL-MFAP with ChEMBL dataset pretraining vs. no pretraining on downstream property prediction datasets}
\label{pretrain_abl}
\begin{center}
\begin{tabular}{ccc}
\rule{0pt}{2.5ex}
\rule{0pt}{2.5ex}
\textbf{Dataset} & \makecell{\textbf{CL-MFAP with} \\ \textbf{ChEMBL pretraining}} & \makecell{\textbf{CL-MFAP without} \\ \textbf{ChEMBL pretraining}} \\
\hline \\
\textit{E. coli} MIC & 0.854 & 0.824 \\
\textit{H. influenzae} MIC & 0.874 & 0.850 \\
BBBP & 0.933 & 0.900 \\
PAMPA & 0.759 & 0.728 \\
Bioavailability & 0.599 & 0.549 \\
BACE & 0.881 & 0.882 \\
\end{tabular}
\end{center}
\end{table}

\subsection{Downstream Property Prediction Datasets}
The choice of the downstream property prediction datasets was based on the availability of good quality data and biological relevance to antibiotic properties. The most relevant antibiotic property is antibacterial activity, and thus the \textit{E. coli} and \textit{H. influenzae} MIC datasets were curated from COADD \citep{coadd} ChEMBL \citep{gaulton2012chembl}, respectively, to analyze CL-MFAP's ability to predict antibacterial activity. The other datasets were obtained from trusted databases (MoleculeNet \citep{moleculenet} and Therapeutics Data Commons \citep{tdc}) and are commonly used in ML models to benchmark model performance in drug discovery.

\textbf{\textit{E. coli} MIC Dataset.} This dataset describes compound ability to inhibit {\textit{Escherichia coli}} ({\textit{E. coli}}). Obtained from COADD \citep{coadd}, each compound has an associated Minimum Inhibitory Concentration (MIC) value, which represents the antibacterial activity against {\textit{E. coli}}. The compounds were binarized as active (1) if MIC $\leq$ 8 ug/mL and inactive (0) if MIC $>$ 8 ug/mL. Size: $ \sim $100,000 compounds.

\textbf{\textit{H. influenzae} MIC Dataset.} This dataset describes the ability of compounds to inhibit {\textit{Haemophilus influenzae}} ({\textit{H. influenzae}}). Obtained from ChEMBL \citep {gaulton2012chembl}, each compound has an associated MIC value, which represents the antibacterial activity against {\textit{H. influenzae}}. The compounds were binarized as active (1) if MIC $\leq$ 4 ug/mL and inactive (0) if MIC $>$ 4 ug/mL. Size: 3,341 compounds.

\textbf{BACE Dataset.} This dataset from MoleculeNet \citep{moleculenet} assesses compounds’ binding ability for a set of inhibitors for \(\beta\text{-secretase } 1\). The compound is labeled active (1) if it is a potential inhibitor of B-secretase 1, 0 otherwise. Size: 1,512 compounds.

\textbf{Blood-Brain Barrier Penetration (BBBP) Dataset.} This MoleculeNet \citep{moleculenet} dataset assesses compounds' capacity to traverse the blood-brain barrier. The compound is labeled "p" if it can penetrate the barrier and “np” if it cannot. Size: 2,038 compounds.

\textbf{Parallel Artificial Membrane Permeability Assay (PAMPA) Dataset.} This dataset evaluates compounds’ permeability across the cell membrane based on the PAMPA assay. The compound is labeled 1 if it has high permeability, and 0 if it has low permeability. Size: NCATS set – 2,035 compounds; Approved drugs set - 142 drugs \citep{pampa}.

\textbf{Bioavailability}. This dataset contains the oral bioavailability of different drugs, defined as “the rate and extent to which the active ingredient or active moiety is absorbed from a drug product and becomes available at the site of action” \citep{chen2001bioavailability}. Size: 640 compounds \citep{bioavailability}.

\subsection{Evaluation of CL-based Models on Downstream Property Prediction Datasets}

In Table~\ref{pretrain_ROC} below are the ROC-AUC values for all pre-trained CL models on the downstream property prediction datasets.

\begin{table}[h]
\caption{ROC-AUC of all pre-trained CL models on downstream property prediction datasets}
\label{pretrain_ROC}
\begin{center}
\begin{tabular}{lcccccc}
\textbf{Model} & \makecell{\textit{\textbf{E. coli}} \\ \textbf{MIC}} & \makecell{\textit{\textbf{H. influenzae}} \\ \textbf{MIC}} & \textbf{BBBP} & \textbf{Pampa} &\makecell{\textbf{Bioavai-} \\ \textbf{lability}} & \textbf{BACE} \\
\hline \\
CL-MFAP & 0.85$\pm$0.04 & 0.87$\pm$0.02 & 0.93$\pm$0.01 & 0.76$\pm$0.03 & 0.60$\pm$0.03 & 0.88$\pm$0.01 \\
CL-BL1 & 0.80$\pm$0.01 & 0.86$\pm$0.02 & 0.93$\pm$0.01 & 0.75$\pm$0.04 & 0.69$\pm$0.05 & 0.86$\pm$0.02 \\
CL-BL2 & 0.80$\pm$0.01 & 0.87$\pm$0.02 & 0.92$\pm$0.00 & 0.75$\pm$0.02 & 0.72$\pm$0.05 & 0.86$\pm$0.01 \\
CL-BL3 & 0.82$\pm$0.04 & 0.87$\pm$0.02 & 0.93$\pm$0.01 & 0.72$\pm$0.03 & 0.65$\pm$0.05 & 0.87$\pm$0.01 \\
CL-BL4 & 0.78$\pm$0.01 & 0.86$\pm$0.02 & 0.92$\pm$0.01 & 0.73$\pm$0.04 & 0.67$\pm$0.06 & 0.85$\pm$0.00 \\
\end{tabular}
\end{center}
\end{table}

\subsection{RePRA - Evaluation of Pre-trained Models}
We primarily applied the Representation-Property Relationship Analysis (RePRA) method to evaluate CL-MFAP against its model variations and all baselines (MoLFormer, MolBERT, ChemBERTa-2, MolCLR, and FP-GNN). RePRA, a novel method introduced by Zhang et al. in 2023, draws inspiration from the concepts of Activity Cliffs (ACs) and Scaffold Hopping (SH) \citep{zhang_can_2024}. It assesses the quality of molecular representations extracted by pre-trained models and visualizes the relationship between these representations and molecular properties. RePRA generalizes ACs and SH from the structure-activity context to the representation-property context, defining an ideal relationship between molecular representations and their properties as a boundary condition. This condition drives the ACs and SH regions to a borderline state without observed data points, allowing for the calculation of ACs and SH thresholds based on these constraints. By using the detected ACs and SH, RePRA generates a map showing the distances between pairs of representations and molecular properties, thereby evaluating the quality of the representations.

\textbf{RePRA Map.} The RePRA map serves as a visualization tool for assessing the quality of molecular representations produced by a pre-trained model. Its x-axis denotes the similarity between the representations of a pair of target molecules, while the y-axis indicates the difference between the properties of this pair of molecules. Typically, a RePRA map is partitioned into four main regions, with shadowed ACs and SH zones that should ideally be avoided by the data points on the map.

\textbf{Activity Cliffs.} This region is delineated by scenarios in which a pair of molecules showcases markedly different properties beyond the y-axis threshold of ACs, while their representations exhibit a noticeable similarity surpassing the x-axis threshold of ACs. A predominance of data points clustered in this area indicates that the model’s representations are too similar to adequately capture the diverse range of molecular properties, thus indicating a limited ability of the pre-trained model to differentiate between molecular properties.

\textbf{Scaffold Hopping.} This region is characterized by instances where a pair of molecules exhibit fairly similar properties beyond the y-axis threshold of SH, yet their representations demonstrate a significant disparity surpassing the x-axis threshold of SH. A prevalence of data points clustered in this zone suggests that the model tends to generate highly various representations that correspond to a narrow range of similar molecular properties, indicative of subpar representation quality from the pre-trained model.

\textbf{Evaluation Scores.} Two evaluation scores, average deviation ($S_{AD}$) and improvement rate ($S_{IR}$), are derived from the RePRA Map to assess the performance of the models. $S_{AD}$ quantifies the average deviation by considering the ratio of data points situated in ACs and SH, adjusting for noise points in the remaining ideal regions; a lower $S_{AD}$ value indicates better performance. On the other hand, $S_{IR}$ is computed by comparing the numbers of data points in ACs and SH between a standard baseline (ECFP) and the pre-trained model under evaluation. Again, a lower $S_{IR}$ value signifies superior performance.

\textbf{Visualization of Cosine Similarities.} In addition to the RePRA map, a visualization of cosine similarities is also presented to analyze the distribution of similarities using CosineSim as a metric between pairs of molecules. This visualization aids in identifying if there are common substructures shared among most molecular pairs.

\textbf{Datasets.} For the RePRA measurement, we employed the Estimated SOLubility (ESOL) dataset, which includes the measured log solubility (mol/L) for 902 compounds \citep{niwa2009}. The "measured log solubility in mols per liter" data from the ESOL dataset was utilized as labels for molecular properties. Initially, the distance between each pair of labels was computed, followed by calculating the distance between each pair of logits. These labels and logits were then collectively inputted into the RePRA algorithm to generate the map.

\textbf{Results.} All models were evaluated using the RePRA test, with the scores presented in Table~\ref{RePRA_tab}. For the $S_{AD}$ parameter, it can be observed that the CL-MFAP model has the lowest result, indicating fewer noise data points with detected ACs and SH, which suggests a better representation-property relationship. For the $S_{IR}$ parameter, the CL-MFAP model also has the lowest score, demonstrating an improvement in representation quality compared to the traditional ECFP method and indicating that CL-MFAP generates better representations compared to the other models. Since lower $S_{AD}$ and $S_{IR}$ scores jointly indicate superior molecular embedding and representation quality, it is unsurprising that the CL-MFAP model, enhanced by the BRA, excels in this test. Notably, all CL models utilizing GTE outperformed the baseline models, highlighting the inherent advantage of contrastive learning frameworks trained on multimodal data in effectively learning molecular representations. The results of the RePRA map are shown in Figure~\ref{RePRA_fig}.

\begin{table}[h]
\caption{RePRA scores of all pre-trained CL models and baseline models}
\label{RePRA_tab}
\begin{center}
\begin{tabular}{lll}
\multicolumn{1}{c}{\bf Model} &\multicolumn{1}{c}{\bf $S_{AD}$} &\multicolumn{1}{c}{\bf $S_{IR}$} 
\\ \hline \\
CL-MFAP&0.008&1.317 \\
CL-BL1&0.013&1.501\\
CL-BL2&0.011&1.431 \\
CL-BL3 &0.010&1.395 \\
CL-BL4&0.019&1.753 \\
MoLFormer&0.017&1.607 \\
MolBERT&0.016&1.758 \\
ChemBERTa-2&0.020&1.904 \\
{MolCLR} & {0.016}	& {1.267} \\
{FP-GNN}	& {0.007}	& {1.434} \\
\end{tabular}
\end{center}
\end{table}

\begin{figure}[h]
 \centering
 \includegraphics[width=1\linewidth]{best.png}
 \caption{RePRA measurement of all pre-trained CL models and baseline models. The shaded areas in the top right and bottom left represent the ACs region and the SH region, respectively.}
 \label{RePRA_fig}
\end{figure}

\subsection{\textit{Escherichia coli} case study}
\textit{Escherichia coli (E. coli)} is a gram-negative bacterium commonly found in the gut microbiome of humans that is usually harmless. However, it can become pathogenic under certain conditions or pathogenic \textit{E. coli} can be ingested and cause a variety of issues in humans. The issues can range from traveler’s diarrhea and pneumonia \citep{mueller2023} to playing a part in Inflammatory Bowel Disease \citep{ecoliIBD}. Although antibiotics exist for \textit{E. coli}, many strains develop antibiotic resistance, thus showcasing the need for new antibiotic compounds effective against \textit{E. coli}.

In this case study, we employ CL-MFAP to identify novel antibiotic compounds that are highly likely to be effective against \textit{E. coli}.

\textbf{Model Training.} CL-MFAP was finetuned on Minimum Inhibitory Concentration (MIC) data against \textit{E.coli} (Anti-\textit{E. coli} Activity) described in Appendix A.2. Obtained from the COADD database, each compound has its associated MIC value, which represents the antibacterial activity, against \textit{E. coli}. The compounds were binarized as active (1) if MIC $\leq$ 8 ug/mL and inactive (0) if MIC $>$ 8 ug/mL.

\textbf{Virtual Screening.} Based on the finetuned CL-MFAP model, virtual screening was performed using the ZINC database. ZINC is a free database containing over 230 million commercially available compounds in ready-to-dock, 3D formats \citep{zinc20}. Due to its massive size, we used the ZINCK250k dataset \citep{zinc250k}, a subset of 250,000 compounds from ZINC. From this, 9389 compounds were identified with predicted activity 1 (predicted to be effective at inhibiting \textit{E. coli}) with 100\% probability and were chosen for further property testing.
 
\textbf{Pharmacokinetic and ADMET Property Predictions.} For the 9389 compounds identified via virtual screening, their pharmacokinetic and ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) properties were predicted using ADMET-SAR \citep{admetsar}. These properties allow us to identify compounds that have necessary molecular properties and are most likely to perform well as antibiotics. From this, we filtered to only include compounds that follow the Lipinski Rule of 5 (molecular weight $\leq$ 500 Da, logP $\leq$ 5, number of hydrogen bond acceptors $\leq$ 10, and number of hydrogen bond donors $\leq$ 5) with a maximum of 1 violation. In addition, their topological surface area had to be between 20-130 $\AA^2$ and their aqueous solubility range had to be between -1 and -5. As a result, 7358 compounds remained. Then, an ADMET score was generated for each remaining compound based on 18 properties related to absorption, toxicity, and metabolism. We followed the ADMET-score method proposed by \citet{guan2018admet}.

\textbf{Similarity to existing \textit{E. coli} antibiotic compounds.} To validate the compounds with predicted anti-\textit{E. coli} activity and ideal pharmacokinetic and ADMET properties, we compared their similarity to existing FDA-approved \textit{E. coli} antibiotic compounds include Levofloxacin \citep{drago_activity_2001}, and Ciprofloxacin \citep{ciprofloxacin}. We first selected the top 1000 compounds with the highest predicted probabilities and ADMET scores and they were split into 4 groups: level 1 (top 1-250 compounds), level 2 (top 251-500 compounds), level 3 (top 501-750 compounds and level 4 (top 751-1000 compounds). For each group, the number of Bemis-Murcko scaffolds and the number of Bernis-Murcko scaffolds per compound were evaluated. As shown in Table~\ref{bm_scaff}, the identified compounds show structural diversity, an essential feature in drug discovery to ensure coverage of broad chemical space. Results also show that molecules ranked higher (those with more favorable ADMET properties) have larger diversity than the molecules ranked lower. We also calculated the Tanimoto similarity (also known as Jaccard Index) based on the MACCs and MAP4C fingerprints between the top 1000 selected compounds and known antibiotics, Levofloxacin and Ciprofloxacin. Among these, two candidates were identified to have high MACCs and low MAP4C similarity with existing \textit{E. coli} antibiotic compounds: C22H22ClNO4 (ZINC ID: ZINC20591249) and C25H25ClN4O2 (ZINCID: ZINC8758881). As shown in Table~\ref{fp_sim}, they have high MACCs similarity scores and low MAP4C similarity scores to the existing antibiotics.MACCS keys are well-suited for functional group-based similarity searching, allowing us to identify compounds that share key pharmacophoric features and common medicinal chemistry substructures. MAP4C captures more detailed structural information, such as atom types and bonding patterns, which is more relevant for identifying structural similarities between compounds. The high MACCs similarity scores with low MAP4C similarity scores confirm that our identified compounds possess functional similarity to the existing antibiotics while maintaining structural novelty. This outcome not only validates our approach but also suggests potential candidates for further investigation in antibiotic development.

\begin{table}[h]
\caption{Bemis-Murcko Scaffolds results of top 1000 compounds predicted to be active against \textit{Escherichia coli} using CL-MAP }
\begin{center}
\begin{tabular}{cccc}
\label{bm_scaff}
\textbf{Level} & \makecell{\textbf{Compounds Included} \\ \textbf{(By Ranking)}} & \makecell{\textbf{Number of} \\ \textbf{Bemis-Murcko Scaffolds}} & \makecell{\textbf{Number of Bemis-Murcko} \\ \textbf{Scaffolds per Compound}} \\
\hline \\
\text{Level 1} & 1-250 & 245 & 0.980 \\
\text{Level 2} & 251-500 & 241 & 0.964 \\
\text{Level 3} & 501-750 & 236 & 0.944 \\
\text{Level 4} & 751-1000 & 236 & 0.944 \\
\end{tabular}
\end{center}
\end{table}


\begin{table}[h]
\centering
\caption{Fingerprint similarity scores of potential \textit{Escherichia coli} antibiotic compounds with existing \textit{Escherichia coli} antibiotics}
\label{fp_sim}
\begin{tabular}{ccccc}
\textbf{Compound} & \multicolumn{2}{|c|}{\textbf{MACCs}} & \multicolumn{2}{|c}{\textbf{MAP4C}} \\ 
\hline
\textbf{} & \textbf{Levofloxacin}& \textbf{Ciprofloxacin}& \textbf{Levofloxacin}& \textbf{Ciprofloxacin}\\ 
C22H22ClNO4 & 0.739& 0.696& 0.030 & 0.032\\
C25H25ClN4O2 & 0.716 & 0.623 & 0.023 & 0.018\\ 
\end{tabular}
\label{tab:multiheader}
\end{table}

\end{document}