\section{Related Work}
\textbf{Transformers.} Among the current mainstream LLMs, the most representative architecture is the transformer. A transformer is a DL architecture consisting of encoder and/or decoder components built around multi-head attention mechanisms. Different models in the transformer family use these components distinctively: Bidirectional Encoder Representations from Transformers (BERT) series based solely on the encoder \citep{devlin2019bertpretrainingdeepbidirectional}, the Generative Pre-trained Transformer (GPT) series based solely on the decoder \citep{radford2018improving}, and the Text-to-Text Transfer Transformer (T5) series utilizing both the encoder and decoder \citep{raffel2020exploring}. The architecture's core features are self-attention computation and positional encoding \citep{vaswani}. The former is used to capture the semantic dependencies between the target word and the context and then determine its importance, while the latter understands the syntax and sequence information of the word by recording its position in the sequence. LLMs based on the transformer architecture have been widely proven to exhibit superior performance in capturing sequence semantics.

\textbf{LLMs for Molecular Property Prediction.} LLMs have recently gained popularity in molecular property prediction due to their enhanced success. MoLFormer is a successful unsupervised transformer-based LLM that accurately captures sufficient chemical and structural information to predict a diverse range of chemical properties \citep{ross2022large}. ChemBERTa is a stack of bidirectional encoders that uses representations from transformers for molecular property prediction \citep{chemberta} and is fine-tuned to better predict drug-target interactions \citep{kang2022fine}. MolBERT is a self-supervised model, consisting of the bidirectional attention mechanism-based BERT architecture \citep{molbert}. It is one of the most efficient pre-trained models for molecular property prediction that can be easily generalized to different molecular property prediction tasks via fine-tuning. All these examples of successful LLMs take in the structure of compounds in Simplified Molecular Input Line Entry System (SMILES) format for predictions.

\textbf{Contrastive Learning Models for Molecular Representation Learning.} As the field of drug development continues to advance, the integration and utilization of multimodal data have become essential for improving the performance of molecular property prediction LLMs. Contrastive learning can enhance a model's feature extraction capabilities by learning different representations of molecular data in the absence of labeled data. For example, MolCLR employs three distinct molecular graph augmentations to achieve contrastive learning, significantly improving the model's ability to learn molecular representations \citep{wang2022molecular}. UniCorn combines several pretraining methods: 2D graph masking, 2D-3D contrastive learning, and 3D denoising, to depict molecular views from three different levels, resulting in superior performance compared to traditional models \citep{unicorn}.