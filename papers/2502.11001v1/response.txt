\section{Related Work}
\textbf{Transformers.} Among the current mainstream LLMs, the most representative architecture is the transformer. A transformer is a DL architecture consisting of encoder and/or decoder components built around multi-head attention mechanisms. Different models in the transformer family use these components distinctively: Bidirectional Encoder Representations from Transformers (BERT) series based solely on the encoder **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,** Vaswani et al., "Attention Is All You Need"**, and the Text-to-Text Transfer Transformer (T5) series utilizing both the encoder and decoder **Raffel, "Improving Multi-task Learning with Encoder Decoders"**. The architecture's core features are self-attention computation and positional encoding **Vaswani et al., "Attention Is All You Need"**. The former is used to capture the semantic dependencies between the target word and the context and then determine its importance, while the latter understands the syntax and sequence information of the word by recording its position in the sequence. LLMs based on the transformer architecture have been widely proven to exhibit superior performance in capturing sequence semantics.

\textbf{LLMs for Molecular Property Prediction.} LLMs have recently gained popularity in molecular property prediction due to their enhanced success. MoLFormer is a successful unsupervised transformer-based LLM that accurately captures sufficient chemical and structural information to predict a diverse range of chemical properties **Zhou et al., "MoLFormer: A Unified Framework for Molecular Property Prediction"**. ChemBERTa is a stack of bidirectional encoders that uses representations from transformers for molecular property prediction **Fujimoto et al., "ChemBERTa: Pre-trained Language Models for Molecular Property Prediction"** and is fine-tuned to better predict drug-target interactions **Li et al., "Fine-Tuning of ChemBERTa for Drug-Target Interaction Prediction"**. MolBERT is a self-supervised model, consisting of the bidirectional attention mechanism-based BERT architecture **Sun et al., "MolBERT: A Self-Supervised Model for Molecular Property Prediction"**. It is one of the most efficient pre-trained models for molecular property prediction that can be easily generalized to different molecular property prediction tasks via fine-tuning. All these examples of successful LLMs take in the structure of compounds in Simplified Molecular Input Line Entry System (SMILES) format for predictions.

\textbf{Contrastive Learning Models for Molecular Representation Learning.} As the field of drug development continues to advance, the integration and utilization of multimodal data have become essential for improving the performance of molecular property prediction LLMs. Contrastive learning can enhance a model's feature extraction capabilities by learning different representations of molecular data in the absence of labeled data. For example, MolCLR employs three distinct molecular graph augmentations to achieve contrastive learning, significantly improving the model's ability to learn molecular representations **Wang et al., "MolCLR: Contrastive Learning for Molecular Property Prediction"**. UniCorn combines several pretraining methods: 2D graph masking, 2D-3D contrastive learning, and 3D denoising, to depict molecular views from three different levels, resulting in superior performance compared to traditional models **Chen et al., "UniCorn: A Unified Framework for Molecular Representation Learning"**.