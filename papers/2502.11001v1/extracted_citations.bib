@article{chemberta,
  author = {Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath},
  title = {ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction},
  journal = {CoRR},
  volume = {abs/2010.09885},
  year = {2020},
  url = {https://arxiv.org/abs/2010.09885},
  eprinttype = {arXiv},
  eprint = {2010.09885}
}

@article{devlin2019bertpretrainingdeepbidirectional,
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal = {CoRR},
  volume = {abs/1810.04805},
  year = {2018},
  url = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint = {1810.04805}
}

@article{kang2022fine,
  author = {Kang, Hyeunseok and Goo, Sungwoo and Lee, Hyunjung and Chae, Jung-woo and Yun, Hwi-yeol and Jung, Sangkeun},
  title = {Fine-tuning of BERT model to accurately predict drug--target interactions},
  journal = {Pharmaceutics},
  volume = {14},
  number = {8},
  pages = {1710},
  year = {2022},
  doi = {10.3390/pharmaceutics14081710}
}

@article{molbert,
  author = {Fabian, Benedek and Edlich, Thomas and {Gaspar}, {Héléna} and Segler, Marwin H. S. and Meyers, Joshua and Fiscato, Marco and Ahmed, Mohamed},
  title = {Molecular representation learning with language models and domain-relevant auxiliary tasks},
  journal = {CoRR},
  volume = {abs/2011.13230},
  year = {2020},
  url = {https://arxiv.org/abs/2011.13230},
  eprinttype = {arXiv},
  eprint = {2011.13230}
}

@inproceedings{radford2018improving,
  author = {Radford, Alec and Narasimhan, Karthik},
  title = {Improving Language Understanding by Generative Pre-Training},
  year = {2018}
}

@article{raffel2020exploring,
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  year = {2020},
  publisher = {JMLR.org},
  volume = {21},
  number = {1},
  journal = {J. Mach. Learn. Res.},
  articleno = {140},
  numpages = {67}
}

@article{ross2022large,
  author = {Ross, Jerret and Belgodere, Brian and Chenthamarakshan, Vijil and Padhi, Inkit and Mroueh, Youssef and Das, Payel},
  title = {Large-scale chemical language representations capture molecular structure and properties},
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {12},
  pages = {1256--1264},
  year = {2022},
  doi = {10.1038/s42256-022-00580-7}
}

@misc{unicorn,
  title = {UniCorn: A Unified Contrastive Learning Approach for Multi-view Molecular Representation Learning}, 
  author = {Feng, Shikun and Ni, Yuyan and Li, Minghao and Huang, Yanwen and Ma, Zhi-Ming and Ma, Wei-Ying and Lan, Yanyan},
  year = {2024},
  eprint = {2405.10343},
  archivePrefix = {arXiv},
  primaryClass = {q-bio.BM},
  url = {https://arxiv.org/abs/2405.10343}
}

@inproceedings{vaswani,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and {Gomez}, Aidan N. and Kaiser, {Łukasz} and Polosukhin, Illia},
  title = {Attention is All You Need},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  year = {2017},
  pages = {6000--6010},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  series = {NIPS'17},
  location = {Long Beach, California, USA},
  isbn = {9781510860964},
  numpages = {11}
}

@article{wang2022molecular,
  author = {Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and {Barati Farimani}, Amir},
  title = {Molecular contrastive learning of representations via graph neural networks},
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {279--287},
  year = {2022},
  doi = {10.1038/s42256-022-00447-x}
}

