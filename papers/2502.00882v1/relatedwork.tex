\section{Related Works}
Our regularized algorithm, ReBlocK, is closely related to the iterated Tikhonov-Kaczmarz method \cite{de2011modified}. ReBlocK can also be viewed as a specific application of stochastic proximal point algorithms (sPPA) \cite{bertsekas2011incremental,asi2019stochastic,davis2019stochastic} for solving stochastic optimization problems with objective functions of the least-squares type. To ensure the exact convergence of sPPA, diminishing step sizes are required; see for example \cite{puatracscu2021new}. In contrast, our work investigates the convergence of sPPA with a large constant step size for the special case of a least-squares loss. Such an approach allows for aggressive updates throughout the algorithm, potentially improving its practical efficiency. 

Our work is also related to the nearly concurrent paper \cite{derezinski2025}, which introduces Tikhonov regularization into the RBK iterations just like ReBlocK. \cite{derezinski2025} focuses on consistent systems that are preprocessed with a randomized Hadamard transform, and the regularization gives rise to optimal convergence rates in the presence of Nesterov acceleration. On the other hand, our work focuses on solving inconsistent systems without any preprocessing step, in which case the regularization is needed to ensure the stability of the algorithm.

The use of Nesterov acceleration represents a much broader trend in the development of stochastic iterative algorithms. Originally proposed by \cite{nesterov1983method}, this technique has been studied extensively in the context of both stochastic gradient and coordinate descent methods; see for example \cite{shalev2013accelerated,allen2016even,jain2018accelerating,agarwal2020leverage}. Relative to block projection methods like RBK and ReBlocK, Nesterov acceleration represents an independent and complementary way to improve upon the convergence rate of mSGD. Incorporating Nesterov acceleration into our algorithms is a promising direction for future work.