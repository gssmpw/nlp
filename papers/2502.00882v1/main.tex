\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables


% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[preprint]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xfrac}

\usepackage{amsfonts}
\usepackage{graphicx}



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}{Question}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\Rea}{\mathbb{R}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\setcond}[2]{\set{\left. #1 \: \middle|   \: #2 \right. }}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\expectE}{\mathbb{E}\,}
\newcommand{\expect}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\range}{\operatorname{range}}

\DeclareMathOperator{\argm}{argmin}
\newcommand{\argmin}[1]{\underset{#1}{\argm\;}}


\newcommand{\LL}[1]{\textcolor{blue}{[LL: #1]}}
\newcommand{\GG}[1]{\textcolor{orange}{GG: #1}}
\newcommand{\JH}[1]{\textcolor{purple}{JH: #1}}
\newcommand{\comm}[1]{\textcolor{red}{#1}}
\newcommand{\revise}[1]{\textcolor{blue}{#1}}

\newcommand{\Tau}{\boldsymbol{\tau}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Randomized and Regularized Block Kaczmarz Algorithms without Preprocessing}

\begin{document}

\twocolumn[
\icmltitle{Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms without Preprocessing}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Gil Goldshlager}{UCB}
\icmlauthor{Jiang Hu}{UCB}
\icmlauthor{Lin Lin}{UCB,LBNL}
\end{icmlauthorlist}

\icmlaffiliation{UCB}{Department of Mathematics, University of California, Berkeley}
\icmlaffiliation{LBNL}{Lawrence Berkeley National Laboratory}

\icmlcorrespondingauthor{Gil Goldshlager}{ggoldsh@berkeley.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, SciML, Natural Gradient Descent, Linear Regression, Kaczmarz}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Due to the ever growing amounts of data leveraged for machine learning and scientific computing, it is increasingly important to develop algorithms that sample only a small portion of the data at a time.
In the case of linear least-squares, the randomized block Kaczmarz method (RBK) is an appealing example of such an algorithm, but its convergence is only understood under sampling distributions that require potentially prohibitively expensive preprocessing steps.
To address this limitation, we analyze RBK when the data is sampled uniformly, showing that its iterates converge in a Monte Carlo sense to a \textit{weighted} least-squares solution.
Unfortunately, for general problems the condition number of the weight matrix and the variance of the iterates can become arbitrarily large.
We resolve these issues by incorporating regularization into the RBK iterations.
Numerical experiments, including examples arising from natural gradient optimization, suggest that the regularized algorithm, ReBlocK, outperforms minibatch stochastic gradient descent for realistic problems that exhibit fast singular value decay.
\end{abstract}


\section{Introduction}
Consider the linear least-squares problem 
\begin{equation} \label{eq:ls}
\begin{split}
& \min_{x \in \Rea^n} \norm{Ax-b}^2, \\
A  &  \in \Rea^{m \times n} ,\; b \in \Rea^m.
\end{split}
\end{equation}
The minimal-norm ordinary least-squares solution is 
\begin{equation}
x^* = A^+b,
\end{equation}
where $A^+$ is the Moore-Penrose pseudoinverse of $A$. 

In this work, we are interested in algorithms that solve \eqref{eq:ls} by sampling just a small number of rows at a time. Such algorithms can be useful for extremely large problems for which direct methods and Krylov subspace methods are prohibitively expensive \cite{censor1981row}. Of particular interest are problems in which the dimension $n$ is so large that it is only possible to access $k \ll n$ rows at a time, but not so large that it is necessary to take $k=1$. As a canonical example, we might have $m=10^9$, $n=10^6$, and $k=10^3$.

As further motivation for algorithms of this type, there are some applications in which sampling rows is the only efficient way to access the data. For example, the rows may be computed on the fly, especially when solving the ``semi-infinite'' version of \eqref{eq:ls} in which the rows are indexed by continuous variables rather than discrete integers \cite{shustin2022semi}. An example that captures both of these features is the problem of calculating natural gradient directions for continuous function learning problems; see \cref{sec:ngd}.

One well-known approach for solving \eqref{eq:ls} using only a few rows at a time is minibatch stochastic gradient descent (mSGD). The mSGD algorithm works by sampling $k$ rows uniformly at random and using them to calculate an unbiased estimator of the gradient of the least-squares loss function. This is equivalent to averaging the $k$ independent gradient estimators furnished by each sampled row. For a thorough discussion of mSGD, see \cite{jain2018parallelizing}.

The technique of averaging used in mSGD, while simple and efficient, is a relatively crude way of processing a block of $k$ rows. A more sophisticated approach is provided by the randomized block Kaczmarz method, which goes beyond averaging by making use of the pseudoinverse of the sampled block \cite{needell2014paved}. As we shall see, the use of the pseudoinverse opens up the possibility of faster convergence, but it also creates a number of complications regarding the implementation and analysis of the algorithm.

\subsection{Notation}
We denote by $r = b - Ax^*$ the residual vector of \eqref{eq:ls}, by $a_i^\top \in \Rea^n$ the row of $A$ with index $i$, and by $b_i \in \Rea$ the corresponding entry of $b$. Additionally, for an index set $S \subseteq \set{1, \ldots, m}$ with $|S| = k$, let $A_S \in \Rea^{k \times n}$ represent the block of rows of $A$ whose indices are in $S$, and let $b_S \in \Rea^k$ represent the corresponding entries of $b$. The same subscript notations will also be applied as needed to any matrices and vectors other than $A$ and $b$. Additionally, denote by $\mathbf{U}(m,k)$ the uniform distribution over all size-$k$ subsets of $\set{1, \ldots, m}$.
 
For symmetric matrices $X,Y$, denote by $X \succ Y$ that the difference $X-Y$ is positive definite, and define $\succeq, \prec, \preceq$ correspondingly. For any vector $x$ and any positive definite matrix $Y$ of the same size, let $\norm{x}_Y = \sqrt{x^\top Y x}$.  For any matrix $X$, denote by $\|X\|_F$ its Frobenius norm and by $\sigma_{\rm min}^+(X)$ its minimum nonzero singular value. 



\subsection{Randomized Kaczmarz \label{sub:rk_intro}}
The modern version of the randomized Kaczmarz method (RK) was proposed by Thomas Strohmer and Roman Vershynin \yrcite{strohmer2009randomized}. In RK, an initial guess $x_0$ is updated iteratively using a two-step procedure:

\begin{enumerate}
    \item \textit{Sample} a row index $i_t \in \set{1, \ldots, m}$ with probability proportional to $\norm{a_{i_t}}^2$.
    \item \textit{Update} 
\begin{equation}
\label{eq:rk}
x_{t+1} = x_t + a_{i_t} \frac{b_{i_t} - a_{i_t}^\top x_t}{\norm{a_{i_t}}^2}.
\end{equation}
\end{enumerate}
To reduce the sampling cost, it is also possible to run RK with uniform sampling, which is equivalent to running RK on a diagonally reweighted problem \cite{needell2014stochastic}.

The iteration \eqref{eq:rk} has the interpretation of projecting $x_t$ onto the hyperplane of solutions to the individual equation $a_{i_t}^\top x = b_{i_t}$. For consistent systems, $Ax^*=b$, the RK iterates $x_t$ converge linearly to $x^*$ with a rate that depends on the conditioning of $A$. For inconsistent systems, $Ax^* \neq b$, RK converges only to within a finite horizon of the ordinary least-squares solution $x^*$ \cite{needell2010randomized}. In particular, the expected squared error $\expectE \norm{x_t - x^*}^2$ converges to a finite, nonzero value that depends on the conditioning of $A$ and the norm of the residual vector $r = b - Ax^*$. See Theorem 7 of \cite{zouzias2013randomized} for the strongest known convergence bound of this type. 

\subsection{Tail Averaging}
Tail averaging is a common technique for boosting the accuracy of stochastic algorithms \cite{rakhlin2011making, jain2018parallelizing, epperly2024randomizedkaczmarztailaveraging}. Given a series of stochastic iterates $x_0, \ldots, x_T$ and a burn-in time $T_b$, the tail-averaged estimator is given by
\begin{equation}
\label{eq:tail}
\overline{x}_T = \frac{1}{T - T_b} \sum_{t=T_b+1}^{T} x_t.
\end{equation}
The recent work \cite{epperly2024randomizedkaczmarztailaveraging} shows that applying tail averaging to the RK iterates yields exact convergence (with no finite horizon) to the ordinary least-squares solution $x^*$, even for inconsistent systems. Building on these results, we will make use of tail averaging to obtain exact convergence to a \textit{weighted} least-squares solution in the block case. 

\subsection{Randomized Block Kaczmarz \label{sub:rbk_intro}}
Randomized block Kaczmarz (RBK) is an extension of RK which uses blocks of rows to accelerate the convergence and make better use of parallel and distributed computing resources \cite{elfving1980block,needell2014paved}. Like RK, each RBK iteration proceeds in two steps:
\begin{enumerate}
    \item \textit{Sample} a subset $S_t \subset \set{1, \ldots, m}$ of the row indices from some chosen sampling distribution $\rho$.
    \item \textit{Update}
\begin{equation}
x_{t+1} = x_t + A_{S_t}^+ (b_{S_t} - A_{S_t} x).
\label{eq:rbk}
\end{equation}
\end{enumerate}
Here $A_{S_t}^+$ is the Moore-Penrose pseudoinverse of $A_{S_t}$. The iteration \eqref{eq:rbk} has the interpretation of projecting $x_t$ onto the hyperplane of solutions to the block of equations $A_{S_t} x = b_{S_t}$. The RBK method can also be viewed as a ``sketch-and-project'' algorithm; see \cite{gower2015randomized}.

There have been many proposals for how to choose the blocks in the RBK method. One idea is to use a preprocessing step to partition the matrix $A$ into well-conditioned blocks, then sample this fixed set of blocks uniformly \cite{needell2014paved}. It has also been suggested to preprocess the matrix with an incoherence transform, which can make it easier to generate a well-conditioned partition \cite{needell2014paved} or, relatedly, enable RBK with uniform sampling to converge rapidly for the transformed problem \cite{derezinski2024solving}. The work of \cite{derezinski2024solving} also provides an analysis of the RBK algorithm when sampling from a determinantal point process, and other proposals include greedy block Kaczmarz algorithms such as \cite{liu2021greedy} which require evaluating the complete residual vector at each iteration. 
 
Unfortunately many of these proposals apply only to consistent linear systems, and all of them require at least a preprocessing step in which the entire data matrix must be accessed. Such preprocessing can be prohibitively expensive for very large-scale problems, for which 1) it can be necessary to furnish an approximate solution without processing the entire data set even once (for instance, in the semi-infinite case), and 2) it can be impossible to manipulate more than a tiny subset of the data at a time due to storage constraints. This leads us to the central question of our work:

\vspace{3pt}

\setlength{\fboxsep}{10pt}
\fbox{
    \parbox{\linewidth-30pt}{
   Can RBK, or some variant thereof, be applied to solve inconsistent linear systems without preprocessing the input matrix?
    }
}

\vspace{3pt}
Viewing the problem \eqref{eq:ls} as a uniform mixture of $m$ distinct rows, we make the natural choice to focus on the case in which the sampling distribution for the algorithm is also uniform. Our results readily generalize to both weighted and semi-infinite problems of the form
\begin{equation}
\min_{x \in \Rea^n} \expect{i \sim \mu}{(a_i^\top x - b_i)^2},
\end{equation}
for which the corresponding algorithms would independently sample $k$ indices $i_1, \ldots, i_k \sim \mu$. This is especially relevant for scientific applications, in which data is often continuous and may be sampled using a physics-based probability distribution. For example, in the case of neural network wavefunctions \cite{hermann2023ab}, the sampling distribution is known as the Born probability density. 

\subsection{Contributions}

We demonstrate that the RBK algorithm with uniform sampling (RBK-U) converges in a Monte Carlo sense to a weighted least-squares solution for both consistent and inconsistent linear systems. In particular, \cref{thm:rbk} shows that convergence is obtained by both expectation values of individual iterates and tail averages of the sequence of iterates. The weight matrix depends on the block size $k$ and the matrix $A$, but not on the vector $b$. Our results provide a new perspective on  RBK in the inconsistent case, going beyond previous analyses that only characterized proximity to the ordinary least-squares solution. 

Unfortunately, \cref{thm:rbk} does not guarantee that RBK-U is robust for every problem. To the contrary, when the problem contains many blocks $A_S$ that are nearly singular, the condition number of the weight matrix and the variance of the iterates can become arbitrarily large. See \cref{fig:results_chebyshev} for numerical examples that manifest these issues.

One way to overcome these difficulties is to make stronger assumptions on the data. For example, \cref{thm:rbk_gaussian} shows that convergence is obtained to the standard least-squares solution when the data arises from certain multivariate Gaussian distributions. Furthermore, in this case both the variance of the iterates and the convergence parameter $\alpha$ can be explicitly bounded. When the singular values of the covariance matrix decay rapidly, the convergence rate can be much faster than mSGD.

Many realistic problems are not well-modeled by Gaussian data. To provide a more general solution, we propose to regularize the RBK iterations as follows:
\begin{equation}
x_{t+1} = x_t + A_{S_t}^\top (A_{S_t} A_{S_t}^\top + \lambda k I)^{-1} (b_{S_t} - A_{S_t} x_t)
\label{eq:reblock_it}.
\end{equation}
The RBK iteration \eqref{eq:rbk} is recovered as $\lambda \rightarrow 0$, but we propose to instead use a small constant $\lambda > 0$. This choice corresponds to a stochastic proximal point algorithm with a large, constant step size $\sfrac{1}{\lambda}$, which to our knowledge has not been analyzed by previous works. We refer to this algorithm as the regularized block Kaczmarz method, or ReBlocK.

Similar to RBK-U, we show that ReBlocK with uniform sampling (ReBlocK-U) converges in a Monte Carlo sense to a weighted least-squares solution; see \cref{thm:reblock}. Unlike for RBK-U, both the condition number of the weight matrix and the variance of the iterates can be controlled in terms of just $\lambda$ and some coarse properties of the data $A,b$. This makes ReBlocK-U much more reliable than RBK-U in practice. As an added benefit,  ReBlocK iterations can be significantly more efficient than RBK iterations; see \cref{sec:reblock_imp,app:speed}. 

Our initial motivation for this work came from the problem of calculating natural gradient directions for deep neural networks. In \cref{sec:ngd}, we explain how this setting naturally lends itself to the kinds of linear least-squares solvers we have studied in this paper. Encouragingly, \cref{fig:nn} shows that ReBlocK-U outperforms mSGD and RBK-U for this problem. Altogether, our results suggest that ReBlocK can be a more effective algorithm than mSGD for realistic problems that exhibit rapid singular value decay, especially when only moderate accuracy is required. 

While our focus is on the case of uniform sampling, the same proof techniques can be applied to other sampling distributions. For example, in \cref{app:dpp} we show that sampling from an appropriate determinantal point process can in theory enable tail averages of ReBlocK iterates to converge rapidly to the ordinary least-squares solution for inconsistent problems.

\subsection{Related Works}
Our regularized algorithm, ReBlocK, is closely related to the iterated Tikhonov-Kaczmarz method \cite{de2011modified}. ReBlocK can also be viewed as a specific application of stochastic proximal point algorithms (sPPA) \cite{bertsekas2011incremental,asi2019stochastic,davis2019stochastic} for solving stochastic optimization problems with objective functions of the least-squares type. To ensure the exact convergence of sPPA, diminishing step sizes are required; see for example \cite{puatracscu2021new}. In contrast, our work investigates the convergence of sPPA with a large constant step size for the special case of a least-squares loss. Such an approach allows for aggressive updates throughout the algorithm, potentially improving its practical efficiency. 

Our work is also related to the nearly concurrent paper \cite{derezinski2025}, which introduces Tikhonov regularization into the RBK iterations just like ReBlocK. \cite{derezinski2025} focuses on consistent systems that are preprocessed with a randomized Hadamard transform, and the regularization gives rise to optimal convergence rates in the presence of Nesterov acceleration. On the other hand, our work focuses on solving inconsistent systems without any preprocessing step, in which case the regularization is needed to ensure the stability of the algorithm.

The use of Nesterov acceleration represents a much broader trend in the development of stochastic iterative algorithms. Originally proposed by \cite{nesterov1983method}, this technique has been studied extensively in the context of both stochastic gradient and coordinate descent methods; see for example \cite{shalev2013accelerated,allen2016even,jain2018accelerating,agarwal2020leverage}. Relative to block projection methods like RBK and ReBlocK, Nesterov acceleration represents an independent and complementary way to improve upon the convergence rate of mSGD. Incorporating Nesterov acceleration into our algorithms is a promising direction for future work.

\section{Overview of the Analysis \label{sec:rbk} }

\newcommand{\xrh}{x^{(\rho)}}
\newcommand{\rrh}{r^{(\rho)}}
Our results are stated in terms of a unified framework that includes RBK, ReBlocK, and even mSGD as special cases. Consider the following iteration:
\begin{enumerate}
    \item \textit{Sample} $S_t \sim \rho$.
    \item \textit{Update } 
\begin{equation} x_{t+1} = x_t + A_{S_t}^\top M(A_{S_t}) (b_{S_t} - A_{S_t} x_t) \label{eq:it_general}. \end{equation}
\end{enumerate}
Here $M(\cdot): \Rea^{k \times n} \rightarrow \Rea^{k \times k}$ takes in the sampled block $A_{S_t}$ and returns a positive semidefinite ``mass'' matrix. RBK is recovered by setting $M(A_{S_t}) = (A_{S_t} A_{S_t}^\top)^+$ and ReBlocK by setting $M(A_{S_t}) = (A_{S_t} A_{S_t}^\top + \lambda |S_t| I)^{-1}$. See \cref{alg:gen} for the full procedure with and without tail averaging. 

Once the function $M(A_s)$ is chosen, let
\begin{equation}
W(S)= I_{S}^\top M(A_{S}) I_{S},\;
P(S) = A_{S}^\top  M(A_{S}) A_{S},
\end{equation}
where $I_S$ represents the rows of the $n \times n$ identity matrix whose indices are in $S$. These quantities are natural because they enable the general iteration \eqref{eq:it_general} to be rewritten as 
\begin{equation}
x_{t+1} = (I - P(S_t)) x_t + A^\top W(S_t) b.
\end{equation}
Note that $P(S_t)$ is a projection matrix in the case of RBK.

Next, let
\begin{equation}
\overline{W} = \expect{S \sim \rho}{W(S)},\; \overline{P} =  \expect{S \sim \rho}{P(S)}.
\end{equation}
Additionally, define the weighted solution $\xrh$ and the weighted residual $\rrh$ via
\begin{equation} 
\label{eq:xrh}
\xrh = \argmin{x \in \Rea^n} \norm{Ax-b}_{\overline{W}}^2, \;
\rrh = b - A\xrh.
\end{equation}
When the solution to the weighted problem is not unique, let $\xrh$ refer to the minimal-norm solution.

Using these definitions, we can further rewrite the iteration \eqref{eq:it_general} as
\begin{equation}
\label{eq:it_nice}
x_{t+1} - \xrh = (I - P(S_t)) (x_t - \xrh) + A^\top W(S_t) \rrh.
\end{equation}

Since the normal equations for \eqref{eq:xrh} can be written as $A^\top \overline{W} \rrh = 0$, we observe that the final term in \eqref{eq:it_nice} vanishes in expectation. Indeed, identifying the appropriate weighted solution $\xrh$ to enable the generalized iteration \eqref{eq:it_general} to be written in the form of \eqref{eq:it_nice}, namely as a linear contraction of the error plus a zero-mean additive term, is the main technical innovation underlying our results. From here, the analysis of \cite{epperly2024randomizedkaczmarztailaveraging} can be readily generalized to show that convergence to $\xrh$ is obtained.

\begin{algorithm}[tb]
   \caption{Generalized iterative least-squares solver with optional tail averaging}
   \label{alg:gen}
\begin{algorithmic}
   \STATE {\bfseries Input:} Data $A,b$, block size $k$, initial guess $x_0$
   \STATE {\bfseries Input:} Mass matrix $M(A_S)$, sampling distribution $\rho$
   \STATE {\bfseries Input:} Total iterations $T$, optional burn-in time $T_b$
   \FOR{$t=0$ {\bfseries to} $T-1$}
   \STATE Sample $S_t \sim \rho$
   \STATE $x_{t+1} = x_t + A_{S_t}^\top M(A_{S_t}) (b_{S_t} - A_{S_t} x)$
   \ENDFOR
    \vspace{0.1cm}
   \IF{$T_b$ is not provided} 
           \STATE {\bfseries Return} $x_T$
   \ENDIF
  \vspace{0.1cm}
   \STATE $\overline{x}_{T} = \frac{1}{T - T_b} \sum_{t=T_b+1}^{T} x_t$
   \vspace{0.1cm}
   \STATE {\bfseries Return} $\overline{x}_T$
\end{algorithmic}
\end{algorithm}

\section{RBK without Preprocessing}
\begin{theorem}
\label{thm:rbk}
Consider the RBK-U algorithm, namely \cref{alg:gen} with $M(A_S) = (A_S A_S^\top)^+$ and  $\rho = \mathbf{U}(m,k)$. Let $\alpha = \sigma^+_{\rm min}(\overline{P})$ and assume that $x_0 \in \range(A^\top)$. Then the expectation of the RBK-U iterates $x_T$ converges to $\xrh$ as
\begin{equation}
\label{eq:rbk_expect_converge}
\norm{\expect{}{x_T} - \xrh} \leq (1 - \alpha)^T \norm{x_0 - \xrh}.
\end{equation}
Furthermore, the tail averages $\overline{x}_T$ converge to $\xrh$ as
\begin{multline}
\label{eq:rbk_ta_converge}
\expectE \norm{\overline{x}_T - \xrh}^2 \leq  \paren{1 - \alpha}^{T_b+1} \norm{x_0 - \xrh}^2 \\
+ \frac{1}{\alpha^2 (T-T_b)} \; \expectE_{S \sim \rho} \norm{A_S^+ \rrh_S}^2.
\end{multline}
\end{theorem}
To our knowledge, this is the first result for inconsistent linear systems that characterizes the exact solution to which the randomized block Kaczmarz iterates converge, albeit in a Monte Carlo sense. The $O(1/T)$ convergence rate for the tail-averaged bound is optimal for row-access methods, and a reasonable default for the burn-in time is $T_b = T/2$; see \cite{epperly2024randomizedkaczmarztailaveraging} for a more thorough discussion of these points. The proof of \cref{thm:rbk}, which takes advantage of an orthogonal decomposition of the error term $x_{t+1} - \xrh$, can be found in \cref{app:rbk}.

Unfortunately, \cref{thm:rbk} does not imply robust convergence for general problems. For one, the convergence parameter $\alpha$, which affects every term in the bound, is difficult to analyze in general. Worse, for problems containing nearly singular blocks $A_S$, the weight matrix $\overline{W} = \expect{S \sim \rho}{I_S^\top (A_S A_S^\top)^+ I_S}$ can be arbitrarily poorly conditioned and the variance term $\expectE_{S \sim \rho} || A_S^+ \rrh_S||^2$ can be arbitrarily large. See \cref{fig:results_chebyshev} for numerical examples that manifest these problems.

In the next subsection, we show that these problems can be overcome when the data arises from a Gaussian distribution. A more general solution is provided by the ReBlocK algorithm introduced in \cref{sec:reblock}.

\subsection{Linear Least-squares with Gaussian Data \label{sec:gauss_data}}
Consider a random problem with the data $A,b$ generated as
\begin{equation}
\begin{bmatrix}
a_i^\top & b_i
\end{bmatrix} \sim \mathcal{N}(0,Q)
\label{eq:gauss_prob}
\end{equation}
independently for each index $i$, where $Q \in \mathbb{R}^{(n+1) \times (n+1)}$ is a positive semidefinite matrix. Furthermore, let $Q_n$ be the top left $n \times n$ block of $Q$, assume for simplicity that $Q_n$ is full rank, and let $Q_n = LL^\top$ be its Cholesky decomposition. Denote the singular values of $L$ by $\sigma_1 \ge \dots \geq \sigma_{n}>0$.  Finally, denote by $y$ the solution to the underlying statistical problem, which is unique since $Q_n$ is full rank:
\begin{equation}
y = \argmin{x \in \Rea^n} \expect{[a_i^\top  \;  b_i] \sim \mathcal{N}(0,Q)}{(a_i^\top x - b_i)^2}.
\end{equation}

\begin{theorem}
\label{thm:rbk_gaussian}
Consider the RBK-U algorithm, namely \cref{alg:gen} with $M(A_S) = (A_S A_S^\top)^+$ and $\rho = \mathbf{U}(m,k)$, applied to the randomly generated finite problem  \eqref{eq:gauss_prob}. Then the results of \cref{thm:rbk} hold with $$\lim_{m \rightarrow \infty} \xrh = \lim_{m \rightarrow \infty} x^* = y.$$
Furthermore, as long as $k \geq 6$ and $\operatorname{rank}(L) \geq 2k$, the variance term satisfies
\begin{equation} \label{eq:variance-gauss}
\lim_{m \rightarrow \infty} \expectE_{S \sim \rho} \norm{A_S^+ \rrh_S}^2 \leq \frac{200}{\sigma_{2k}^2} \cdot  \lim_{m \rightarrow \infty} \frac{\norm{r}^2}{m}.
\end{equation}
Finally, the convergence parameter $\alpha$ satisfies
\begin{equation} \label{eq:alpha-bound}
\lim_{m \rightarrow \infty} 
    \alpha \geq C_{n,k} \max\left\{ \frac{k\sigma_n^2}{\|L\|_F^2},  \max_{2\leq \ell < k} \frac{(\ell-1)\sigma_n^2}{ \sum_{i \geq k-\ell-1} \sigma_i^2} \right \}
\end{equation}
with $C_{n,k} \rightarrow 1$ as $n \rightarrow \infty$ for fixed $k$.

\end{theorem}

Thus, in the case of Gaussian data and large $m$, the ordinary least-squares solution is recovered, the variance of the  iterates is bounded, and the convergence rate $\alpha$ improves at least linearly with the block size $k$. In addition, the following corollary, which is based on Corollary 3.4 of \cite{derezinski2024sharp}, shows that RBK converges much faster  than mSGD for polynomially decaying singular values.

\begin{corollary} \label{coro:gauss}
    Consider the setting of \cref{thm:rbk_gaussian} with fixed \(k \leq n/2\), and assume the \(L\) factor has polynomial spectral decay $\sigma_i^2 \leq i^{-\beta} \sigma_1^2$  for all $i$ and some $\beta > 1$. Then the convergence parameters of RBK and mSGD satisfy
   \begin{equation}
    \lim_{m \rightarrow \infty} \alpha^{\rm RBK} \geq C k^\beta \tfrac{\sigma_n^2}{\|L\|_F^2}, \; 
     \lim_{m \rightarrow \infty}  \alpha^{\rm mSGD} \leq k \tfrac{ \sigma_n^2}{\|L\|_F^2}
    \end{equation}    
    for some constant \(C =  C(\beta) > 0\).
\end{corollary}

Similarly, the faster convergence rate of RBK over mSGD extends to exponentially decaying singular values.
The proofs of \cref{thm:rbk_gaussian,coro:gauss}, provided in \cref{app:rbk}, rely on a connection between RBK-U for Gaussian data and sketch-and-project with Gaussian sketch matrices. These results generalize the techniques of  \cite{derezinski2021determinantal,derezinski2024sharp} to the case of inconsistent linear systems, improving upon the results of  \cite{rebrova2021block} in terms of both the convergence rate and the variance. It is worth noting that Gaussian data is just one way to generate a benign or ``incoherent'' problem. We expect that \cref{thm:rbk_gaussian,coro:gauss} could be generalized to other cases such as problems that have been preprocessed using a randomized Hadamard transform, potentially extending the results of \cite{derezinski2024solving} to the inconsistent case.

\subsection{Implementation Details}

To stably implement the RBK iteration \eqref{eq:rbk}, we employ an SVD-based least-squares solver to calculate $A_{S_t}^+ (b_{S_t} - A_{S_t} x).$  The most expensive part of this procedure is the SVD, which has an asymptotic cost of $O(nk^2)$. This cost is greater than the cost of an mSGD iteration, which is $O(nk)$. Whether it is worthwhile to expend this extra cost depends on the structure of the matrix $A$, and especially on its singular values as discussed in the previous section.

\subsection{Numerical Demonstration \label{sec:rbk_num}}
We test our theoretical results for the RBK-U and mSGD algorithms on two problems with Gaussian data, with results in \cref{fig:gaussian_results}. We apply tail averaging to each algorithm to observe convergence beyond the variance horizon. As expected, tail-averaged RBK-U (TA-RBK-U) converges much more rapidly than tail-averaged mSGD (TA-mSGD) in the presence of fast singular value decay.  Further details on these experiments and a link to the code can found in \cref{app:num}.

\begin{figure*}[ht]
\centering
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{isotropic_gaussian.pdf}}
\quad
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{decay_gaussian.pdf}}
\caption{Comparison of methods on two problems with Gaussian data, with no singular value decay (Left) and rapid singular value decay (Right). The reported quantity is the suboptimality of the approximate solution $x$ for the unweighted problem \eqref{eq:ls}, namely the value of $\epsilon$ for which $\norm{Ax-b} = (1 + \epsilon) \norm{r}$. The vertical dotted line indicates the burn-in period of $T_b=300$, before which results are shown for individual iterates.}
\label{fig:gaussian_results}
\end{figure*}


\section{Robustness through Regularization \label{sec:reblock}}
To address the shortcomings of RBK-U in the case of general data, we propose to incorporate regularization into the RBK algorithm. A natural way to do this is to replace the RBK iteration with a stochastic proximal point iteration \cite{asi2019stochastic}, namely
\begin{equation}
x_{t+1} = \argmin{x \in \Rea^n} \left[\norm{A_{S_t} x - b_{S_t}}^2 + \lambda k \norm{x - x_t}^2 \right].
\end{equation}
This minimization problem leads to the closed form
\begin{equation}
x_{t+1} = x_t + A_{S_t}^\top (A_{S_t} A_{S_t}^\top + \lambda k I)^{-1} (b_{S_t} - A_{S_t} x_t),
\end{equation}
and that the RBK iteration \eqref{eq:rbk} is recovered in the limit $\lambda \rightarrow 0$. To incorporate a mild regularization without significantly slowing down the convergence, we propose to use a small, constant value of $\lambda$ throughout the algorithm. We suggest $\lambda=0.001$ as a practical default value, but to obtain optimal performance the value may need to be tuned on a case-by-case basis; see \cref{app:lambda} for further discussion. We refer to the resulting scheme as the regularized block Kaczmarz method, or ReBlocK.

\begin{theorem}
\label{thm:reblock}
Consider the ReBlocK-U algorithm, namely \cref{alg:gen} with $M(A_S) = (A_S A_S^\top + \lambda k I)^{-1}$ and $\rho = \mathbf{U}(m,k)$. Let  $\alpha = \sigma_{\rm min}^+(\overline{P})$ and assume $x_0 \in \range(A^\top)$. Then the expectation of the ReBlocK iterates $x_T$ converges to $\xrh$ as
\begin{equation}
\norm{\expect{}{x_T} - \xrh} \leq (1 - \alpha)^T \norm{x_0 - \xrh}.
\end{equation}
Furthermore, the tail averages $\overline{x}_T$ converge to $\xrh$ as 
\begin{multline}
\label{eq:reblock_converge}
 \expectE \norm{\overline{x}_T - \xrh}^2 \leq 2 (1 - \alpha)^{T_b+1} \norm{x_0 - \xrh}^2 \\
 + \frac{1}{2\lambda \alpha^2 (T-T_b)} \frac{\norm{\rrh}^2}{m}.
\end{multline}
Finally, when the squared row norms of $A$ are uniformly bounded by a constant $N$, then $\kappa(\overline{W}) \leq 1 + \frac{N}{\lambda}$ and the weighted residual $\rrh$ can be bounded in terms of the ordinary least-squares residual $r$:
\begin{equation}
\norm{\rrh}^2 \leq \paren{1 + \frac{N}{\lambda}} \norm{r}^2.
\end{equation}
\end{theorem}
Note that the values of $\xrh$ and $\alpha$ here differ from those in \cref{sec:rbk} due to the different choice of $M(A_S)$ used by ReBlocK. To our knowledge, \cref{thm:reblock} is the first result characterizing the convergence of a stochastic proximal point algorithm with a constant step size for inconsistent linear systems. The advantage relative to RBK-U is that ReBlocK-U is able to control both the variance of the iterates and the conditioning of the weight matrix $\overline{W}$ in terms of the reciprocal of the regularization parameter $\lambda$. As a result, the algorithm converges robustly even when the problem contains many nearly singular blocks $A_S$. It is worth noting that these advantages could also be attained by truncating the singular values that are smaller than $\lambda$ in the RBK iteration \eqref{eq:rbk}. However, the ReBlocK iteration is additionally justified by the fact that it is cheaper to implement than the RBK iteration; see \cref{sec:reblock_imp} for further discussion of this point.

The proof of \cref{thm:reblock} is provided in \cref{app:reblock}. Relative to the proof of \cref{thm:rbk}, the proof of \cref{thm:reblock} is more complicated because the ReBlocK iteration does not lead to an orthogonal decomposition of the error term $x_{t+1} - \xrh$. Instead, the proof relies on a bias-variance decomposition inspired by \cite{defossez2015averaged,jain2018parallelizing,epperly2024randomizedkaczmarztailaveraging}, which also leads to the extra factors of $2$ in the convergence bound. 

We are not yet able to analyze the convergence rate of ReBlocK-U, even in the case of Gaussian data, as to our knowledge there is no existing work bounding the quality of a regularized Gaussian sketch. However, convergence to the ordinary least-squares solution is obtained for a broader class of \textit{noisy} linear least-squares problems; see \cref{app:noisy}. Additionally, a fast rate of convergence to the ordinary least-squares solution can be shown when sampling from an appropriate determinantal point process; see \cref{app:dpp}.

\subsection{Implementation Details \label{sec:reblock_imp}}
To implement the ReBlocK iterations \eqref{eq:reblock_it}, we directly calculate the $k \times k$ matrix $A_{S_t} A_{S_t}^\top + \lambda k I$ and then use a Cholesky-based linear system solver to calculate $(A_{S_t} A_{S_t}^\top + \lambda k I)^{-1} (b_{S_t} - A_{S_t} x)$. This computation is stable as long as $\lambda$ is not chosen to be too small. Using this approach, the most expensive part of the ReBlocK iteration is calculating $A_{S_t} A_{S_t}^\top$, which has an asymptotic cost of $O(nk^2)$ just like RBK. Nonetheless, in practice the matrix-matrix multiplication for ReBlocK can be significantly cheaper than the SVD used for RBK. For example, in the experiments of \cref{sec:ngd}, ReBlocK iterations are about five times faster than RBK iterations, as reported in \cref{app:speed}. For the largest problems, the ReBlocK iterations could be further accelerated using iterative solvers; see for example Section 4.1 of \cite{derezinski2025}. 

\subsection{Numerical Demonstration \label{sec:reblock_num}}
We now test the ReBlocK-U algorithm on two problems whose columns are discretized representations of continuous functions, leading the matrices $A$ to contain many nearly singular blocks of rows. We set $\lambda=0.001$ and generate the inconsistency using random Gaussian noise. We find that TA-RBK-U is unstable for these problems, as expected. Furthermore, TA-ReBlocK-U converges much faster than TA-mSGD in the presence of rapid singular value decay. Further details on these experiments can be found in \cref{app:synth}, and an exploration of the effect of choosing different values of $\lambda$ can be found in \cref{app:lambda}.

\begin{figure*}[ht]
\centering
\centering
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{chebyshev_mild_decay.pdf}}
\quad
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{chebyshev_fast_decay.pdf}}
\caption{Comparison of methods on two inconsistent problems with many nearly singular blocks, with mild singular value decay (Left) and rapid singular value decay (Right). The reported quantity is the suboptimality of the approximate solution $x$ for the unweighted problem \eqref{eq:ls}, namely the value of $\epsilon$ for which $\norm{Ax-b} = (1 + \epsilon) \norm{r}$. The vertical dotted line indicates the burn-in period of $T_b=300$, before which results are shown for individual iterates.}
\label{fig:results_chebyshev}
\end{figure*}


\begin{figure*}[ht]
\centering
\centering
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{NN_start_ls.pdf}}
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{NN_mid_ls.pdf}}
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{NN_end_ls.pdf}}
\caption{Comparison of methods for calculating natural gradient directions for a small neural network. The network parameters $\theta$ are taken from three snapshots of a single training run, with one snapshot from the ``pre-descent'' phase before the loss begins to decrease (Left), one snapshot from the ``descent'' phase during which the loss decreases rapidly (Middle), and one snapshot from the ``post-descent'' phase when the loss has stopped decreasing significantly (Right). The algorithms are measured in terms of their progress towards reducing the relative residual $\tilde{r} = \norm{J x - [f_\theta - f]} / \norm{f_\theta - f}$ for the least-squares problem \eqref{eq:ngd_ls}, which measures how well the function-space update direction $J x$ agrees with the function-space loss gradient $f_\theta - f$. The burn-in time is set to $T_b \approx T/20$ in each case, as indicated by the vertical dotted line.}
\label{fig:nn}
\end{figure*}


\section{Natural Gradient Optimization \label{sec:ngd}}
\newcommand{\dd}{{\rm d}}

Our original motivation for this work comes from the problem of training deep neural networks using natural gradient descent \cite{amari1998natural}, which is based on an efficient natural gradient induced by a problem-dependent Riemannian metric. 
Natural gradient descent has been studied extensively in the machine learning community; see \cite{martens2015optimizing, ren2019efficient, martens2020new}. Furthermore, there is increasing evidence that natural gradient methods can improve the accuracy when training neural networks to solve physical equations. See \cite{muller2023achieving,dangel2024kronecker} for applications to physics-informed neural networks and \cite{pfau2020ab,schatzle2023deepqmc} for applications to neural network wavefunctions. 

To elucidate the structure of the natural gradient direction, consider the function learning problem
\begin{equation}
\label{eq:func_learn}
\min_\theta L(\theta),\quad
L(\theta) \coloneqq \frac{1}{2} \int_\Omega  (f_\theta(s) - f(s))^2 \dd s,
\end{equation}
where $\Omega \subset \Rea^d$ is the domain of the functions, $f:\Omega \rightarrow \Rea$ is the target function and $f_\theta: \Omega \rightarrow \Rea$ is a function represented by a neural network with parameters $\theta \in \Rea^n$. The standard definition of natural gradient descent for this problem is
\begin{equation}
\theta \gets \theta - \eta G_N,\quad G_N \coloneqq F^{-1} \nabla_\theta L(\theta), \label{eq:ngd_pre}
\end{equation}
where $\eta$ is the step size, $F$ is the Fisher information matrix
\begin{equation}
F = \int_\Omega \nabla_\theta f_\theta(s)  \nabla_\theta f_\theta(s)^\top \dd s = J^\top J,
\end{equation}
and the Euclidean gradient $\nabla_\theta L(\theta)$ takes the form
\begin{equation}
\nabla_\theta L(\theta) = \int_\Omega  \nabla_\theta f_\theta(s)  (f_\theta(s) - f(s)) \dd s = J^\top [f_\theta -f].
\end{equation}
Here $J: \Rea^n \rightarrow \Rea^\Omega$ represents the Jacobian, which is a linear operator from the space of parameters to the space of real-valued functions on $\Omega$. $J^\top$ represents the adjoint.

Calculating  the natural gradient direction $G_N$ using \eqref{eq:ngd_pre} requires a linear solve against the $n \times n$ matrix $F$, which is very challenging in realistic settings when $n \geq 10^6.$ This has motivated the development of approximate schemes such as \cite{martens2015optimizing}. An alternative approach is to reformulate $G_N$ using the structure of $F$ and $\nabla_\theta L(\theta)$:
\begin{align}
G_N &= (J^\top J)^{-1} J^\top [f_\theta -f] \\
&= \argmin{x \in \Rea^n} \norm{J x - [f_\theta - f]}^2, \label{eq:ngd_ls}
\end{align}
where the norm in the final expression is the $L_2$-norm in function space. This least-squares formulation has been pointed out for example by \cite{martens2020new, chen2024empowering,goldshlager2024kaczmarz}, with the work of Chen and Heyl empowering major advances in the field of neural quantum states. A major goal of the current work is to provide a more solid foundation for the development of natural gradient approximations along these lines.

The natural way to access the data when training a neural network is to sample a set of points in the domain $\Omega$ and evaluate the target function $f$, the network outputs $f_\theta$, and the network gradients $\nabla_\theta f_\theta$ at the sampled points. This is precisely equivalent to sampling a small subset of the rows of \eqref{eq:ngd_ls}, which motivates the consideration of row-access least-squares solvers for calculating natural gradient directions. Furthermore, both empirical evidence from scientific applications \cite{park2020geometry,wang2022and} and theoretical evidence from the literature on neural tangent kernels \cite{bietti2019inductive,ronen2019convergence,cao2019towards} suggest that $J$ should be expected to exhibit fast singular value decay, motivating the possibility that RBK and ReBlocK could converge rapidly when solving \eqref{eq:ngd_ls}. Note that these observations translate straightforwardly from the simple function learning problem \eqref{eq:func_learn} to the realistic problems of training physics-informed neural neteworks or neural network wavefunctions.

\subsection{Numerical Demonstration}

To test our algorithms for calculating natural gradient directions, we train a small neural network to learn a simple function on the unit interval. We take three snapshots from the training process and form the least-squares problem \eqref{eq:ngd_ls} for each.  The methods TA-mSGD, TA-RBK-U, and TA-ReBlocK-U are then compared on these problems with results in \cref{fig:nn}. The computations are performed on an A100 GPU to simulate a deep learning setting, and progress is measured with wall-clock time on the horizontal axis. TA-ReBlocK-U performs best in every case, achieving an accuracy nearly an order of magnitude better than TA-RBK-U in the first snapshot and a full order of magnitude better than TA-mSGD in the second snapshot. Further details on these experiments can be found in \cref{app:nn}. For additional context, we examine the singular values of each problem and confirm that they exhibit exponential decay in the top part of the spectrum; see \cref{app:singular}. 
 
Our results, though preliminary due to their small scale, suggest that ReBlocK is a promising method for calculating natural gradient directions. This provides justification for the Kaczmarz-inspired SPRING algorithm \cite{goldshlager2024kaczmarz} and suggests a family of related methods to be explored by future works. Open questions include how many ReBlock iterations to run between each update of $\theta$, how to incorporate averaging, and how to tune $\lambda$.

\section{Conclusions}
In this work, we have explored the problem of solving large-scale linear least-squares problems without preprocessing the data matrix $A$. Our results suggest that ReBlocK may be a more effective algorithm than mSGD for inconsistent problems that exhibit rapid singular value decay. More broadly, our work highlights the value of incorporating regularization as a path towards broadening the applicability of block Kaczmarz methods, and our analysis suggests that a Monte Carlo perspective can be useful in elucidating the behavior of randomized block row-access methods in general. Finally, our work provides motivation and suggests new directions for least-squares based natural gradient optimizers. 

\newpage

\section*{Acknowledgements}
G.G. acknowledges support from the U.S. Department of
Energy, Office of Science, Office of Advanced Scientific Computing Research, Department of
Energy Computational Science Graduate Fellowship under Award Number DE-SC0023112.
This effort was supported by the SciAI Center, and funded by the Office of Naval Research (ONR), under Grant Number N00014-23-1-2729 (J.H., L.L.).  L.L. is a Simons Investigator in Mathematics. This research used the Savio computational cluster resource provided by the Berkeley Research Computing program at the University of California, Berkeley. We thank Ethan Epperly, Robert Webber, and Ruizhe Zhang for their thoughtful discussions and feedback.

 \section*{Disclaimer}
This report was prepared as an account of work sponsored by an agency of the
United States Government. Neither the United States Government nor any agency thereof, nor
any of their employees, makes any warranty, express or implied, or assumes any legal liability
or responsibility for the accuracy, completeness, or usefulness of any information, apparatus,
product, or process disclosed, or represents that its use would not infringe privately owned
rights. Reference herein to any specific commercial product, process, or service by trade name,
trademark, manufacturer, or otherwise does not necessarily constitute or imply its
endorsement, recommendation, or favoring by the United States Government or any agency
thereof. The views and opinions of authors expressed herein do not necessarily state or reflect
those of the United States Government or any agency thereof.

\bibliography{biblio}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Helpful Lemmas}
In this appendix we prove five lemmas which will help us analyze both RBK and ReBlocK. \cref{lemma:expect} shows how the expectation of the iterates evolves. \cref{lemma:range} shows that the iterates of the algorithms always stay within $\range(A^\top)$, and \cref{lemma:P} provides a contraction property of applying $I - \overline{P}$ to the certain types of vectors. \cref{lemma:tail} shows how to derive bounds on the expected square error of tail-averaged iterates under relevant assumptions. Finally, \cref{lemma:res} bounds the attained residual based on the condition number of $\overline{W}$.

\begin{lemma}[Convergence of single-iterate expectation]
\label{lemma:expect}
Consider the generalized iteration \eqref{eq:it_general} with some fixed choice of a positive semidefinite  mass matrix $M(A_S)$ and sampling distribution $\rho$, and fix two indices $r < s$. Then the expectation of $x_s$ conditioned on $x_r$ satisfies
\begin{equation}
\expect{}{x_s - \xrh  | x_r} = (I - \overline{P})^{s-r} (x_r - \xrh).
\end{equation}
\end{lemma}
\begin{proof}[Proof of \cref{lemma:expect}]
Fix $t \in r, \ldots, s-1$, let $P_t = P(S_t)$ and $W_t = W(S_t)$. Recall from \eqref{eq:it_nice} that the generalized iteration \eqref{eq:it_general} can be reformulated as
\begin{equation}
x_{t+1} - \xrh = (I - P_t) (x_t - \xrh) + A^\top W_t \rrh.
\end{equation}
Recall also that the normal equations for $\xrh$ imply that $A^\top \overline{W} \rrh = 0$. Taking the expectation over the choice of $S_t$, it thus obtains
\begin{equation}
\expect{}{x_{t+1} - \xrh | x_t}= (I - \overline{P}) (x_t - \xrh) + A^\top \overline{W} \rrh = (I - \overline{P}) (x_t - \xrh).
\end{equation}
Using the law of total expectation and the linearity of expectation, iterating this result for $t = r, \ldots, s-1$ yields the lemma.
\end{proof}

\begin{lemma}
\label{lemma:range}
Consider the generalized iteration \eqref{eq:it_general} with some fixed choice of a positive semidefinite mass matrix $M(A_S)$ and sampling distribution $\rho$. Then $x^*,\xrh \in \range(A^\top)$ and if $x_0 \in \range(A^\top)$, then $x_t \in \range(A^\top)$ for all $t \geq 0$.
\end{lemma}
\begin{proof}
Recall that $x^*$ is the minimal-norm solution to \eqref{eq:ls} and $\xrh$ is the minimal-norm solution to \eqref{eq:xrh}. Suppose for the sake of contradiction that $x^* \notin \range(A^\top)$. Then let $\tilde{x}$ be the projection of $x^*$ onto $\range(A^\top)$. It follows that $\norm{\tilde{x}} < \norm{x^*}$ and $Ax^* = A\tilde{x}$, making $\tilde{x}$ a minimizer of \eqref{eq:ls} with a smaller norm than $x^*$ (contradiction). The same argument holds for $\xrh$. 

To show $x_t \in \range(A^\top)$ for all $t \geq 0$ we apply an inductive argument. The claim holds for $t=0$ by assumption, so now suppose that it holds for some arbitrary $t \geq 0$. Recall
\begin{equation}
 x_{t+1} = x_t + A_{S_t}^\top M(A_{S_t}) (b_{S_t} - A_{S_t} x_t).
 \end{equation}
 Then $x_t \in \range(A^\top)$ by assumption and  $A_{S_t}^\top M(A_{S_t}) (b_{S_t} - A_{S_t} x_t) \in \range(A^\top)$ since $\range(A_{S_t}^\top )\subseteq \range(A^\top)$. It follows that $x_{t+1} \in \range(A^\top)$, completing the proof.

\end{proof}

\begin{lemma}
\label{lemma:P}
Suppose that $x \in \range(\overline{P})$, $\overline{P} \preceq I$, and $\alpha = \sigma_{\rm min}^+(\overline{P})$. Then for any $s \geq 0$, 
\begin{equation}
x^\top (I - \overline{P})^s  x \leq (1 - \alpha)^s \norm{x}^2
\end{equation}
and 
\begin{equation}
\norm{(I - \overline{P})^s x} \leq (1 - \alpha)^s \norm{x}.
\end{equation}
\end{lemma}
\begin{proof}
First expand $x$ in the basis of eigenvectors of the symmetric matrix $\overline{P}$, then note that every eigenvector that has a nonzero coefficient in the expansion has its eigenvalue in the interval $[\alpha, 1]$. 

\end{proof}
\begin{lemma}[Convergence of tail-averaged schemes] 
\label{lemma:tail}
Consider the generalized iteration \eqref{eq:it_general} with some fixed mass matrix $M(A_S)$ and sampling distribution $\rho$. Let $\alpha = \sigma_{\rm min}^+(\overline{P})$ and suppose that $x_0 \in \range(A^\top)$, $\range(\overline{P}) = \range(A^\top)$, and $\overline{P} \preceq I$. Additionally, suppose the stochastic iterates $x_0, x_1, \ldots$ satisfy 
\begin{equation}
\expectE \norm{x_t- \xrh}^2 \leq (1 - \alpha)^t B + V
\end{equation}
for all $t$ and some constants $B, V$. Then the tail averages $\overline{x}_T$ of the iterates, with burn-in time $T_b$, satisfy
\begin{equation}
\expectE \norm{\overline{x}_T - \xrh}^2 \leq (1 - \alpha)^{T_b+1} B + \frac{1}{\alpha (T - T_b)} V.
\end{equation}
\end{lemma}
\begin{proof}
We follow closely the Proof of Theorems 1.2 and 1.3 of \cite{epperly2024randomizedkaczmarztailaveraging}. Decompose the expected mean square error as
\begin{equation}
\label{eq:mse_sum}
\expectE \norm{\overline{x}_T - \xrh}^2 = \frac{1}{(T-T_b)^2} \sum_{r,s = T_b+1}^{T} \expect{}{(x_r - \xrh)^\top (x_s - \xrh)}.
\end{equation}
For $T_b \leq r < s$ bound the covariance term using \cref{lemma:expect} and our assumptions on $\overline{P}$: 
\begin{align*}
\expect{}{(x_r - \xrh)^\top (x_s - \xrh)} &= \expect{}{(x_r - \xrh)^\top \expect{}{x_s - \xrh | x_r}} \\
&= \expect{}{(x_r - \xrh)^\top (I - \overline{P})^{s-r} (x_r - \xrh)} \\
& \leq (1-\alpha)^{s-r} \expectE \norm{x_r - \xrh}^2 \\
& \leq  (1 - \alpha)^s B + (1-\alpha)^{s-r} V,
\end{align*}
where the third line uses \cref{lemma:range,lemma:P} and the assumptions $\range{\overline{P}} = \range(A^\top)$ and $\overline{P} \preceq I$.

Using the coarse bound $  (1 - \alpha)^s B \leq (1 - \alpha)^{T_b+1} B$ for $s \geq T_b+1$, it follows
\begin{equation}
 \expectE \norm{\overline{x}_T - \xrh}^2 \leq (1 - \alpha)^{T_b+1} B + \frac{V}{(T-T_b)^2}  \sum_{r,s = T_b+1}^T (1-\alpha)^{|s-r|}.
\end{equation}
Apply another course bound
\begin{equation}
\sum_{r,s = T_b+1}^T (1-\alpha)^{|s-r|} \leq 2 \sum_{r = T_b+1}^{T} \sum_{s=0}^\infty (1-\alpha)^s = \frac{T - T_b}{\alpha}
\end{equation}
to obtain the final result:
\begin{equation}
 \expectE \norm{\overline{x}_T - \xrh}^2 \leq  (1 - \alpha)^{T_b+1} B + \frac{V}{\alpha (T-T_b)}.
\end{equation}
\end{proof}

\begin{lemma}[Residual bound]
\label{lemma:res}
Assuming $\overline{W} \succ 0$, it holds
\begin{equation}
\norm{\rrh}^2 \leq \kappa(\overline{W}) \norm{r}^2.
\end{equation}
\end{lemma}
\begin{proof}
Use operator norms and the optimality of $\xrh$ for the weighted least-squares problem: 
\begin{align*}
\norm{A \xrh - b}^2 & \leq \norm{\overline{W}^{-1}} \norm{A \xrh - b}_{\overline{W}}^2 \\
&\leq \norm{\overline{W}^{-1}} \norm{A x^* - b}_{\overline{W}}^2 \\
& \leq \norm{W^{-1}} \norm{\overline{W}}  \norm{r}^2 \\
& = \kappa(\overline{W})  \norm{r}^2 .
\end{align*}
\end{proof}


\newcommand{\zt}{z^{(t)}}
\section{Proofs of RBK Convergence Theorems \label{app:rbk}}
In this section we provide the proofs of \cref{thm:rbk,thm:rbk_gaussian,coro:gauss}.
We first prove the following lemma, which can be viewed as a more general version of Theorem 1.2 of \cite{needell2014paved}:
\begin{lemma}
\label{lemma:rbk}
Consider the RBK iterates \eqref{eq:rbk} and fix some sampling distribution $\rho$. Suppose that $x_0 \in \range(A^\top)$ and $\range(\overline{P}) = \range(A^\top)$. Then for all $t$ it holds
\begin{equation}
\expectE \norm{x_t - \xrh}^2 \leq (1 - \alpha)^t \norm{x_0 - \xrh}^2 + \frac{1}{\alpha} \expectE_{S \sim \rho} \norm{A_S^+ \rrh_S}^2. 
\end{equation}\end{lemma}
\begin{proof}
Let $P_s = P(S_s)$ and $W_s = W(S_s)$. Using \eqref{eq:it_nice} and the definition $M(A_S) = (A_S A_S^\top)^+$, the RBK iteration \eqref{eq:rbk} can be reformulated as 
\begin{equation}
x_{s+1} - \xrh = (I - P_s) (x_s - \xrh) + A_{S_s}^+ \rrh.
\end{equation}
This represents an orthogonal decomposition of $x_{s+1} - \xrh$ since $I-P_s$ is the projector onto the null space of $A_{S_s}$, which is in turn orthogonal to the range of the pseudoinverse $A_{S_s}^+$. 

It thus holds
\begin{align*}
\norm{x_{s+1} - \xrh}^2 &= \norm{(I - P_s) (x_s - \xrh)}^2 + \norm{A_{S_s}^+ \rrh_{S_s}}^2 \\
&= (x_s - \xrh)^\top (I - P_s) (x_s - \xrh)  + \norm{A_{S_s}^+ \rrh_{S_s}}^2,
\end{align*}
where the second line uses the idempotency of the projector $I - P_s$. Note that $x_s - \xrh \in \range(A^\top)$ by \cref{lemma:range} and $\overline{P} \preceq I$ since $P(S)$ is always a projection matrix. Taking expectations and applying \cref{lemma:P} thus yields
\begin{align*}
\expectE \norm{x_{s+1} - \xrh}^2 &= (x_s - \xrh)^\top (I - \overline{P}) (x_s - \xrh)  + \expectE_{S \sim \rho} \norm{A_S^+ \rrh_S}^2 \\
& \leq (1-\alpha) \expectE \norm{x_s - \xrh}^2 + \expectE_{S \sim \rho} \norm{A_S^+ \rrh_S}^2.
\end{align*}
Iterating from $s=0$ to $s=t-1$ and utilizing $\sum_{s=0}^{t-1} (1-\alpha)^s < \sum_{s=0}^\infty (1-\alpha)^s = \sfrac{1}{\alpha}$ yields the desired result.
\end{proof}
\begin{proof}[Proof of \cref{thm:rbk}]
The result follows from the appropriate application of \cref{lemma:expect,lemma:tail,lemma:rbk}. To apply these lemmas it is first required to verify $\overline{P} \preceq I$ and $\range(\overline{P}) = \range(A^\top)$. For the first result, it suffices to observe that $P(S)$ is the orthogonal projector onto the row space of $A_S$ and thus $P(S) \preceq I$ for all $S$. 

For the second result, first note that since $\overline{P} = A^\top \overline{W} A$ it is clear that $\range(\overline{P}) \subseteq \range(A^\top)$. Utilizing this containment and the fact that $\overline{P}$ is symmetric, it suffices to show that there is no $x \in \range(A^\top)$ such that $\overline{P} x = 0$. Now, consider any such $x \in \range(A^\top)$. There must exist a row index $i$ such that $a_i^\top x \neq 0$. Thus
\begin{equation}
x^\top \overline{P} x = \expect{S \sim \mathbf{U}(m,k)}{x^\top A_S^\top  (A_S A_S^\top)^{-1} A_S x} \geq \frac{k}{m} \cdot \frac{(a_i^\top x)^2}{\norm{a_i}^2} > 0,
\end{equation}
where the intermediate step uses the facts that row $i$ is chosen with probability $k/m$ and that $A_S^\top (A_S A_S^\top)^{-1} A_S \succeq a_i (a_i^\top a_i)^{-1} a_i^\top$ when $i \in S$.  It follows that $\overline{P} x \neq 0$ and so indeed $\range(\overline{P}) = \range(A^\top)$.

With the assumptions verified, \cref{lemma:expect} can be applied using $r=0$, $s=T$ to yield
\begin{equation}
\expect{}{x_T} - \xrh = (I - \overline{P})^T (x_0 - \xrh).
\end{equation}
This directly implies
\begin{equation}
\norm{\expect{}{x_T} - \xrh} \leq (1- \alpha)^T \norm{x_0 - \xrh}
\end{equation}
using \cref{lemma:range,lemma:P} and the proven properties of $\overline{P}$.

Furthermore, \cref{lemma:rbk} holds and provides the conditions for \cref{lemma:tail} with $B = \norm{x_0 - \xrh}^2$, $V = \frac{1}{\alpha} \expectE_{S \sim \rho} \norm{A_S^+ \rrh_S}^2$. The application of \cref{lemma:tail} then directly implies the convergence result for the tail-averaged RBK-U algorithm.
\end{proof}

\begin{proof}[Proof of \cref{thm:rbk_gaussian}] 
In the case of Gaussian data, suppose we are given \(m\) data points \(\{a_i, b_i\}_{i=1}^m\). Then the dataset $A,b$ can be viewed as a finite sample of the underlying statistical linear regression problem
\begin{equation}
y = \argmin{x \in \Rea^n}  \expect{[a_i^\top  \;  b_i] \sim \mathcal{N}(0,Q)}{(a_i^\top x - b_i)^2}.
\end{equation}
Furthermore, the solution $x^*$ is the corresponding (minimal-norm) empirical risk minimizer. It is a standard result in asymptotic statistics that as the number of samples $m$ grows, the empirical minimizer $x^*$ converges to the true solution $y$ \cite{lehmann2006theory,van2000asymptotic}.

Now, the RBK-U algorithm is applied by uniformly selecting \(k\) samples per iteration from the finite data $A,b$. Nonetheless, in the limiting case where \(m \to \infty\), the sampling distribution of \(k\) rows from the finite dataset \(\{a_i, b_i\}_{i=1}^m\) converges to the distribution obtained by sampling \(k\) items independently from \(\mathcal{N}(0, Q)\). Thus, to obtain results about the RBK-U algorithm in the limit $m \rightarrow \infty$, we can focus on the case in which the $k$ data points $\{a_1,b_1\}, \dots, \{a_k, b_k\}$ are directly sampled from $\mathcal{N}(0, Q).$
% , we focus the analysis of RBK-U algorithm on this limiting case of \(m \to \infty\).


Denote by $Q= VV^\top$ the Cholesky decomposition of $Q$.  
Let $V=\begin{pmatrix}
    L \\
    d^\top
\end{pmatrix}$,
where $:$ and $d^\top$ are the top $n$ rows and the last row of $V$, respectively. Recalling that $Q_n$ represents the top left $n \times n$ block of $Q$, it  follows that $Q_n = LL^\top$ is the Cholesky decomposition of $Q_n$, as previously defined. 

In each iteration of RBK-U, the $k$ data points $[a_1^\top\; b_1], \dots, [a_k^\top\; b_k]$ from $\mathcal{N}(0, Q)$ can be equivalently redistributed as $z_1^\top [L^\top\; d],\dots, z_{k}^\top [L^\top\; d]$ for $z_i \sim \mathcal{N}(0, I_{n+1})$.
Collecting the random vectors $z_1, \ldots, z_k$ into the columns of a single matrix $Z_t$, the RBK-U update can be written as
\begin{equation} \label{eq:rbk-gauss} x_{t+1} = x_t + (Z_t^\top L^\top)^{+}( Z_t^\top d - Z_t^\top L^\top x_t). \end{equation}

Now, note that $\expect{}{a_ia_i^\top} = LL^\top, \expect{}{b_i a_i} = Ld$ and $\expect{}{b_i^2} = d^\top d$. Plugging these identities into the definition of $y$, we find
\begin{equation}
y = \argmin{x \in \Rea^n} \expect{[a_i^\top  \;  b_i] \sim \mathcal{N}(0,Q)}{(a_i^\top x - b_i)^2}  
= \argmin{x \in \Rea^n} \| L^\top x - d \|^2.
\end{equation}
We can thus define an ``underlying'' residual vector by $\tilde{r} = L^\top y - d$.  Additionally defining $P_t = (Z_t^\top L^\top)^{+}  Z_t^\top L^\top$, the update \eqref{eq:rbk-gauss} can be reformulated as
\begin{equation} \label{eq:rbk-gauss-fixed} x_{t+1} - y = (I - P_t)(x_t - y) + ( Z_t^\top L^\top)^{+} Z_t^\top  \tilde{r}. \end{equation}

Noting from the definition that $\tilde{r}$ is orthogonal to the rows of $L$, we now show $\expect{}{ (Z_t^\top L^\top)^{+}(Z_t^\top \tilde{r})} = 0$. In fact, decompose $Z_t = Z_t^1 + Z_t^2$ where $Z_t^1 \in \range(L^\top)$ and  $Z_t^2 \perp \range(L^\top)$. It follows that $Z_t^1$ and $Z_t^2$ are independent, zero-mean Gaussian vectors and
$$\expect{}{ ( Z_t^\top L^\top)^{+}( Z_t^\top \tilde{r})} =  \expect{}{ ((Z_t^1)^\top L^\top)^+ (Z_t^2)^\top \tilde{r}} = \expect{}{ ((Z_t^1)^\top L^\top)^+}\expect{}{ (Z_t^2)^\top \tilde{r}} = 0.$$
We note that a similar independence lemma has also been established in Lemma 3.14 of \cite{rebrova2021block}.
Furthermore, taking the expectation on both sides of \eqref{eq:rbk-gauss-fixed}, we have
\[ \expect{}{x_{t+1} - x^*} = (I - \overline{P}) (x_t - x^*),   \]
where $\overline{P}:= \expect{}{P_t}$. 
Consequently, in the infinite limit $\expect{}{x_t}$ converges to $y$ using the same logic as the proof of \cref{thm:rbk}, which proves that 
\begin{equation}
\lim_{m \rightarrow \infty} \xrh = \lim_{m \rightarrow \infty} x^* = y.
\end{equation}

In addition, the variance term $\norm{A_{S_t}^\top r_{S_t}}^2$ in \cref{thm:rbk} takes the form
\begin{equation}
\norm{(Z^\top L^\top)^+ Z^\top \tilde{r}}^2.
\end{equation}
We can thus bound
\begin{equation}
\norm{(Z^\top L^\top)^+ Z^\top \tilde{r}}^2 = \norm{(Z_1^\top L^\top)^+ Z_2^\top \tilde{r}}^2 \leq \sigma_{\rm min}(Z_1^\top L^\top)^{-2} \norm{Z_2^\top \tilde{r}}^2.
\end{equation}

We can bound the expectation of the smallest singular value of $Z_1^\top L^\top$ using a similar technique to the proof of Lemma 22 in \cite{derezinski2024sharp}. To start, let $L^\top = W\Sigma Y^\top$ be the SVD of $L^\top$. Then 
\begin{equation}
\sigma_{\rm min}(Z_1^\top L^\top)^{2} = \sigma_{\rm min}(Z_1^\top L^\top L Z_1) = \sigma_{\rm min}(Z_1^\top W \Sigma^2 W^\top Z_1).
\end{equation}
Letting $W_{2k}$ denote the first $2k$ columns of $W$, and noting that $\Sigma \succeq {\rm diag}(\sigma_{2k}, \dots, \sigma_{2k}, 0,\dots, 0)$, it holds
\[ \sigma_{\rm min}(Z_1^\top L^\top)^{2} \geq \sigma_{2k}^2 \cdot \sigma_{\rm \min}(Z_1^\top W_{2k} W_{2k}^\top Z_1) = \sigma_{\min}(Z_1^\top W_{2k}).  \]
Since the columns of $W_{2k}$ are orthonormal the random matrix $Z_1^\top W_{2k}$ can be redistributed as a single Gaussian random matrix $G_{2k}$ of size $k \times (2k)$ and with standard normal entries. By Lemma 3.16 of \cite{rebrova2021block}, we thus have for $k \geq 6$, 
\begin{equation}
\expect{}{\sigma_{\rm min}^{-2}(Z_1^\top L^\top)} \leq \frac{20}{(\sqrt{2k} - \sqrt{k})^2 \sigma_{2k}^2} \leq \frac{200}{k \sigma_{2k}^2}.
\end{equation}
Hence, by the independence between $(Z^\top L^\top)^+$ and $Z^\top \tilde{r}$, we have
\[ \expectE \|(Z^\top L^\top)^+ Z^\top \tilde{r} \|^2 \leq \expect{}{\sigma_{\min}^{-2} (Z_1^\top L^\top )} \expectE \|Z_2^\top \tilde{r}\|^2 \leq \frac{200}{\sigma_{2k}^2} \|\tilde{r}\|^2.  \]
Noting that $\lim_{m \rightarrow \infty} \frac{\|r\|^2}{m} = \expect{}{(a_{i}^\top x^* - b_i)^2} = \|L^\top x^* - d\|^2 = \|\tilde{r}\|^2$, we prove \eqref{eq:variance-gauss}.

It remains only to bound the value of $\alpha$. Note that the entries of 
$Z_t \in \mathbb{R}^{d\times m}$ are independently drawn from the standard Gaussian distribution.

This property enables us to leverage existing results from \cite{derezinski2024sharp} concerning the spectrum of the matrix $\overline{P}$. Specifically, $P_t = (Z_t L^\top)^{+} Z_t L^\top$, which has a similar formula to the matrix $P$ considered in Equation (6) of \cite{derezinski2024sharp}. 
Then, applying Theorem 3.1 of \cite{derezinski2024sharp} gives the result stated in \cref{thm:rbk_gaussian}. 
\end{proof}

\begin{proof}[Proof of \cref{coro:gauss}.]
    By Corollary 3.4 of \cite{derezinski2024sharp}, if $L$ has the polynomial spectral decay of order $\beta>1$, i.e., $\sigma_i^2 \leq  i^{-\beta} \sigma_1^2$ for all $i$,
    the dependence of $C_{n,k}$ on $n$ and $k$ in \cref{thm:rbk_gaussian} can be eliminated when $k \leq n/2$.  
    Furthermore, there is a constant $C=C(\beta)$ such that for any $k\le n/2$, the linear convergence rate satisfies   
 \[     \lim_{m \rightarrow \infty} \alpha^{\rm RBK} \geq C\frac{k^\beta \sigma_n^2}{\|L\|_F^2}. \]
    
Regarding the convergence rate of mSGD, let us consider solving a fixed number of samples, say $m$, of $\{a_i^\top, b_i\}$. 
It is demonstrated in \cite{needell2014stochastic} that, with a minibatch size of 1 and importance sampling, the corresponding convergence parameter can be at most $\alpha^{\rm SGD} \leq 1/\kappa_{\rm dem}( \frac{1}{m} A_m A_m^\top)$ with $A_m = [a_1, \dots, a_m]$ for convergent step sizes.  
When a larger minibatch size \(k\) is used, \cite{jain2018parallelizing} shows that the learning rate can be increased at most linearly, corresponding to a rate of at most $\alpha^{\rm mSGD} \leq k/\kappa_{\rm dem}(\frac{1}{m}A_m A_m^\top)$. It follows by the random matrix theory \cite{johnstone2001distribution,tao2012topics} that 
$\lim_{m \rightarrow \infty} \frac{1}{m}A_m A_m^\top = LL^\top$. Hence, when $m$ goes to infinity,  the linear convergence rate of mSGD satisfies
\[ \lim_{m \rightarrow \infty} \alpha^{\rm mSGD} \leq \frac{k\sigma_n^2}{\| L\|_F^2}. \]
\end{proof}

\section{Proofs of ReBlocK Convergence Theorems \label{app:reblock}}
This appendix contains the proof of the ReBlocK convergence bound \cref{thm:reblock}. We first establish a useful lemma which bounds the error of the individual ReBlocK iterates under arbitrary sampling distributions.

\begin{lemma}
\label{lemma:reblock}
Consider the ReBlocK iterates \eqref{eq:reblock_it} and fix some sampling distribution $\rho$. Suppose that $x_0 \in \range(A^\top)$ and $\range(\overline{P}) = \range(A^\top)$. Then for all $t$ it holds
\begin{equation}
 \expectE \norm{x_t - \xrh}^2 \leq 2 (1 - \alpha)^{t} \norm{x_0 - \xrh}^2 + \frac{1}{2\lambda k \alpha}  \expectE_{S \sim \rho}\norm{\rrh_S}^2.
\end{equation}
\end{lemma}
\begin{proof}
Since the ReBlocK iteration does not admit a simple orthogonal decomposition, we instead proceed by utilizing a \textit{bias-variance decomposition} inspired by \cite{defossez2015averaged,jain2018parallelizing, epperly2024randomizedkaczmarztailaveraging}.

Let $P_s = P(S_s)$ and $W_s = W(S_s)$. Bias and variance sequences are defined respectively by
\begin{equation}
d_0 = x_0 -\xrh, \; d_{s+1} = (I-P_s) d_s,
\end{equation}
\begin{equation}
v_0 = 0,\;v_{s+1} = (I - P_s) v_s + A^\top W_s \rrh.
\end{equation} Intuitively, the bias sequence captures the error due to the initialization $x_0 \neq \xrh$ and the variance sequence captures the rest of the error. It can be verified by mathematical induction and \eqref{eq:it_nice} that 
$x_s-\xrh = d_s + v_s$ for all $s$. As a result, it holds
\begin{equation}
\label{eq:bv}
\expectE \norm{x_t - \xrh}^2 \leq 2 \expectE \norm{d_t}^2 + 2 \expectE \norm{v_t}^2.
\end{equation}
Note also that it is a simple extension of \cref{lemma:range} that $d_s,v_s \in \range(A^\top)$ for all $s$.

To analyze the bias, first calculate
\begin{equation}
\expect{}{\norm{d_{s+1}^2} | d_s} = d_s^\top \expect{}{(I-P_s)^2} d_s = d_s^\top (I - 2\overline{P} + \expect{S \sim \rho}{P(S)^2}) d_s.
\end{equation}
Observe that
\begin{equation}
P(S) = A_S^\top (A_S A_S^\top + k |S| I)^{-1} A_S \preceq  A_S^\top (A_S A_S^\top)^{-1} A_S \preceq I,
\end{equation}
and hence $P(S)^2 \preceq P(S)$ and $\expect{S \sim \rho}{P(S)^2} \preceq \overline{P}$. As a result, $I - 2\overline{P} + \expect{S \sim \rho}{P(S)^2} \preceq I - \overline{P}$. Additionally leveraging the properties $d_s \in \range(A^\top) = \range(\overline{P})$ and $\overline{P} \preceq I$, \cref{lemma:P} implies
\begin{equation}
\expect{}{\norm{d_{s+1}^2} | d_s} \leq (1-\alpha) \expectE \norm{d_s}^2.
\end{equation}
Iterating this inequality yields a simple bound on the bias term:
\begin{equation}
\label{eq:bias_bound}
\expectE \norm{d_t}^2 \leq (1-\alpha)^t \norm{x_0 - \xrh}^2.
\end{equation}

The analysis of the variance term is less straightforward. To begin, note that $v_s$ follows the same recurrence as $x_s - \xrh$, so we can apply a slight modification of \cref{lemma:expect} to the variance sequence to obtain
\begin{equation}
\expect{}{v_s} = (I - \overline{P})^s v_0 = 0.
\end{equation}

Now, square the iteration for $v_{s+1}$ conditioned on $v_s$:
\begin{align*}
\expect{}{\norm{v_{s+1}}^2 | v_s} &= v_s^\top \expect{S \sim \rho}{(I-P(S))^2} v_t + 2 v_s^\top \expect{S \sim \rho}{(I -P(S))A^\top W(S) \rrh} + \expectE_{S \sim \rho} \norm{A^\top W(S) \rrh}^2 \\
& \leq (1-\alpha) \norm{v_s}^2 + 2 v_s^\top \expect{S \sim \rho}{(I -P(S))A^\top W(S) \rrh} + \expectE_{S \sim \rho} \norm{A^\top W(S) \rrh}^2,
\end{align*}
where the second step uses our previous observations that $\expect{S \sim \rho}{(I - P(S))^2} \preceq I - \overline{P}$. $v_s \in \range(\overline{P})$, and \cref{lemma:P}. Take expectations over $v_s$ as well to eliminate the cross-term, yielding
\begin{align*}
\expectE \norm{v_{s+1}}^2 \leq (1-\alpha) \expectE \norm{v_s}^2 + \expectE_{S \sim \rho} \norm{A^\top W(S) \rrh}^2.
\end{align*}

Iterating this last inequality for $s = 0, \ldots, t-1$, using $\sum_{s=0}^{t-1} (1-\alpha)^s < \sum_{s=0}^\infty (1-\alpha)^s = \sfrac{1}{\alpha}$, and recalling that $v_0 = 0$, we find
\begin{equation}
\expectE \norm{v_{t}}^2  \leq \frac{1}{\alpha} \expectE_{S \sim \rho} \norm{A^\top W(S) \rrh}^2.
\end{equation}

Now, note that
\begin{equation}
A^\top W(S) \rrh = A_S^\top (A_S A_S^\top + \lambda k I)^{-1} \rrh_S.
\end{equation}
Applying the singular value decomposition of $A_S$ and some elementary calculus, it is verified that regardless of the singular values of $A_S$,
\begin{equation}
\norm{A_S^\top (A_S A_S^\top + \lambda k I)^{-1}} \leq \frac{1}{2 \sqrt{\lambda k}}.
\end{equation}
The variance bound can thus be simplified as 
\begin{equation}
\label{eq:reblock_var}
\expectE \norm{v_{t}}^2  \leq \frac{1}{4 \lambda k \alpha}  \expectE_{S \sim \rho}\norm{\rrh_S}^2.
\end{equation}
Combining the bias bound \eqref{eq:bias_bound} and the variance bound \eqref{eq:reblock_var} using \eqref{eq:bv} yields the desired result.
\end{proof}

\begin{proof}[Proof of \cref{thm:reblock}]
The result will follow from the appropriate application of \cref{lemma:expect,lemma:tail,lemma:reblock}. To apply these lemmas it is first required to verify y $\overline{P} \preceq I$ and $\range(\overline{P}) = \range(A^\top)$. For the first result, it suffices to observe that $P(S) \preceq P_{RBK}(S) \preceq I$ for all $S$. 

For the second result, the logic follows almost identically to the same part of the proof of \cref{thm:rbk}. The only difference is in how we show that $\overline{P} x \neq 0$ for any $x \in \range(A^\top)$. Like before, we start by noting that there must exist a row index $i$ such that $a_i^\top x \neq 0$. We then bound
\begin{equation}
x^\top \overline{P} x = \expect{S \sim \mathbf{U}(m,k)}{x^\top A_S^\top  (A_S A_S^\top + \lambda k I)^{-1} A_S x} \geq \frac{k}{m} \cdot \frac{(a_i^\top x)^2}{\norm{A A^\top + k \lambda I}_2} > 0,
\end{equation}
where the intermediate step uses the facts that $(A_S A_S^\top + k \lambda I)^{-1} \succeq (AA^\top + k \lambda I)^{-1}$, that row $i$ is chosen with probability $k/m$, and that $x^\top A_S^\top A_S x \geq x^\top a_i a_i^\top x$ when $i \in S$.  It follows that $\overline{P} x \neq 0$ and so indeed $\range(\overline{P}) = \range(A^\top)$.

With the assumptions verified, \cref{lemma:expect} can be applied using $r=0$, $s=T$ to yield
\begin{equation}
\expect{}{x_T} - \xrh = (I - \overline{P})^T (x_0 - \xrh).
\end{equation}
This directly implies 
\begin{equation}
\norm{\expect{}{x_T} - \xrh} \leq (1 - \alpha)^T \norm{x_0 - \xrh}.
\end{equation}
using \cref{lemma:range,lemma:P} and the proven properties of $\overline{P}.$

The conditions of \cref{lemma:reblock} are also satisfied, the results of which provide the conditions for \cref{lemma:tail} with $B = \norm{x_0 - \xrh}^2$, $V = \frac{1}{2 \lambda k \alpha}  \expectE_{S \sim \rho}\norm{\rrh_S}^2.$ \cref{lemma:tail} then leads to the following bound for the tail averages:
\begin{equation}
 \expectE \norm{\overline{x}_T - \xrh}^2 \leq 2 (1 - \alpha)^{T_b+1} \norm{\xrh}^2 + \frac{1}{2\lambda k \alpha^2 (T-T_b)}  \expectE_{S \sim \rho}\norm{\rrh_S}^2.
\end{equation}
Since $\rho = \mathbf{U}(m,k)$, the last term can be simplified using $\expectE_{S \sim \rho} \norm{\rrh_S}^2 = k \norm{\rrh}^2/m$. The desired tail-averaged convergence bound follows.

The next step is to bound the condition number $\kappa(\overline{W})$, given that $\norm{a_i}^2 \leq N$ for all $i$. First note that the row norm bound implies 
\begin{equation}
\norm{A_S A_S^\top} = \norm{A_S^\top A_S}  = \norm{\sum_{i \in S} a_i a_i^\top} \leq\sum_{i \in S} \norm{ a_i a_i^\top}  \leq Nk.
\end{equation}
As a result, 
\begin{equation}
\lambda k I \preceq A_S A_S^\top + \lambda k I \preceq (N + \lambda) k I.
\end{equation}
Now, when $S$ is sampled uniformly, $\expect{S \sim \mathbf{U}(m,k)}{I_S^\top I_S} = \frac{k}{m} I$. It follows that
\begin{equation}
\overline{W}= \expect{S \sim \mathbf{U}(m,k)}{I_S^\top (A_S A_S^\top + \lambda k I)^{-1} I_S} \succeq \frac{1}{(N + \lambda)m}I, 
\end{equation}
\begin{equation}
\overline{W}= \expect{S \sim \mathbf{U}(m,k)}{I_S^\top (A_S A_S^\top + \lambda k I)^{-1} I_S} \preceq \frac{1}{\lambda m}I.
\end{equation}
Putting these together implies
\begin{equation}
\kappa(\overline{W}) \leq 1 + \frac{N}{\lambda},
\end{equation}
and the final claim of the theorem follows by \cref{lemma:res}.
\end{proof}

\section{Noisy Linear Least-squares \label{app:noisy}}
In this section we consider the special case in which the inconsistency in the problem is generated by zero-mean noise. This case is more general than the case of Gaussian data, but less general than the case of arbitrary inconsistency. We define such a problem to have each row independently generated as
\begin{equation}
\label{eq:noisy_prob}
a_i \sim \mathbf{A},\; z_i \sim \mathbf{Z},\; b_i = a_i^\top y + z_i,
\end{equation}
where $\mathbf{A}$ is arbitrary, $\mathbf{Z}$ satisfies $\expect{z \sim \mathbf{Z}}{z}=0$,  and $y \in \Rea^n$. This scenario might arise, for example, if the entries of $b$ are calculated via noisy measurements of $y$ along the directions $a_i$. 

For such problems with increasing numbers of rows, both RBK and ReBlocK converge (in a Monte Carlo sense) to the ordinary least-squares solution rather than a weighted solution. This can be viewed as an advancement over previous results on noisy linear systems, such as \cite{needell2010randomized}, which show only convergence to a finite horizon even for arbitrarily large noisy systems. The advance comes from treating the noise model explicitly and applying tail averaging to the algorithm.

\begin{theorem}
\label{thm:rbk_noisy}
Consider the RBK-U algorithm, namely \cref{alg:gen} with $M(A_S) = (A_S A_S)^+$ and $\rho = \mathbf{U}(m,k)$, applied to the noisy linear least-squares problem \eqref{eq:noisy_prob}. Then the results of \cref{thm:rbk} hold and
\begin{equation}
\lim_{m \rightarrow \infty} \xrh = \lim_{m \rightarrow \infty} x^* = y.
\end{equation}
\end{theorem}


\begin{theorem}
\label{thm:reblock_noisy}
Consider the ReBlocK-U algorithm, namely \cref{alg:gen} with $M(A_S) = (A_S A_S^\top + \lambda k I)^{-1}$ and $\rho = \mathbf{U}(m,k)$, applied to the noisy linear least-squares problem \eqref{eq:noisy_prob}. Then the results of \cref{thm:reblock} hold and
\begin{equation}
\lim_{m \rightarrow \infty} \xrh = \lim_{m \rightarrow \infty} x^* = y.
\end{equation}
\end{theorem}

\begin{proof}[Unified proof of \cref{thm:rbk_noisy,thm:reblock_noisy}]
Our strategy will be to first show that $x^*$ converges to $y$, then show that $\xrh$ converges to $x^*$.

For the first part, note that for fixed $m$, the dataset $A,b$ can be viewed as a finite sample of the underlying statistical linear regression problem
\begin{equation}
y = \argmin{x \in \Rea^n} \expect{a \sim \mathbf{A}, z \sim \mathbf{Z}}{(a^\top (x - y) - z)^2}.
\end{equation}
Furthermore, the ordinary least-squares solution $x^*$ is the corresponding empirical risk minimizer. It is a standard result in asymptotic statistics that as the number of samples $m$ grows, the empirical minimizer $x^*$ converges to the true solution $y$ \cite{lehmann2006theory,van2000asymptotic}.

For the second part, recall that the weighted solution $\xrh$ is characterized by
\begin{equation}
\overline{P} \xrh = A^\top \overline{W} b = \overline{P} x^* + A^\top \overline{W} r.
\end{equation}

It has been verified in the proofs of \cref{thm:rbk,thm:reblock} that $\range(\overline{P}) = \range(A^\top)$. Combining this with the results of \cref{lemma:range} implies that $x^*, \xrh \in \range(\overline{P})$. Applying the pseudoinverse of $\overline{P}^+$ on the left thus yields
\begin{equation}
\xrh = x^* + \overline{P}^+ A^\top \overline{W} r,
\end{equation}
and it then suffices to show that
\begin{equation}
\lim_{m \rightarrow \infty} \overline{P}^+ A^\top \overline{W} r = 0.
\end{equation}


As $m\rightarrow \infty$, the distribution of $k$ rows sampled uniformly at random from $A$ converges to the distribution of $k$ rows sampled independently from $\mathbf{A}$. Similarly, the distribution of $k$ entries of the residual $r=Ax^* - b$ sampled uniformly from $A,b$ converges to the distribution of $k$ values sampled independently from $\mathbf{Z}$. By the noisy construction, these residual values are independent from each other as well as from the corresponding rows of $A$.  Call the resulting distributions $\mathbf{A}^k$ and $\mathbf{Z}^k$. It then holds
\begin{equation}
\lim_{m \rightarrow \infty} \overline{P} = \lim_{m \rightarrow \infty} \expect{S \sim \mathbf{U}(m,k)}{P(A_S)} = \expect{A_S \sim \mathbf{A}^k}{P(A_S)} \coloneqq \hat{P},
\end{equation}
\begin{equation}
\lim_{m \rightarrow \infty} \overline{P}^+ A^\top \overline{W} r = \hat{P}^+ \lim_{m \rightarrow \infty}  \expect{S \sim \mathbf{U}(m,k)}{A_S^\top M(A_S) r_S} = \hat{P}^+ \expect{A_S \sim \mathbf{A}^k}{A_S^\top M(A_S)} \expect{r_S \sim \mathbf{Z}^k}{r_S} = 0.
\end{equation}
It follows that
\begin{equation}
\lim_{m \rightarrow \infty} \xrh = \lim_{m \rightarrow \infty} x^* = y.
\end{equation}
\end{proof}

\section{Sampling from a Determinantal Point Process \label{app:dpp}}

In Section \ref{sec:reblock}, we have presented the convergence of ReBlocK to a weighted least-squares solution under the uniform sampling distribution $\rho = \mathbf{U}(m,k)$. However, the explicit convergence rate $1-\alpha$ is still unknown. Motivated by recent work on Kaczmarz algorithms with  determinantal point process (DPP) sampling \cite{derezinski2024solving}, we now show that under DPP sampling, ReBlocK converges to the ordinary least-squares solution and the convergence of ReBlocK can have a much better dependence on the singular values of $A$ than mSGD. 

The sampling distribution that we consider is  $\rho= k\text{-}{\rm DPP}(AA^\top + \lambda k I)$, namely
\begin{equation}
\label{eq:dpp}
\Pr[S] = \frac{\det (A_S A_S^\top + \lambda k I)}{\sum_{|S'| = k} \det (A_{S'}A_{S'}^\top + \lambda k I)}.
\end{equation}
Our analysis is similar to parts of the nearly concurrent work \cite{derezinski2025}, though \cite{derezinski2025} does not incorporate a fixed size $k$ for their regularized DPP distribution..

\begin{theorem} \label{thm:dpp}
Consider the ReBlocK algorithm with k-DPP sampling, namely $M(A_S) = (A_S A_S^\top + \lambda k I)^{-1}$ and $\rho= k\text{-}{\rm DPP}(AA^\top + \lambda k I)$. Let  $\alpha = \sigma_{\rm min}^+(\overline{P})$ and assume $x_0 \in \range(A^\top)$.  Then the expectation of the ReBlocK iterates $x_T$ converges to $x^*$ as
\begin{equation}
\norm{\expect{}{x_T} - x^*} \leq (1 - \alpha)^T \norm{x_0 - x^*}.
\end{equation}
Furthermore, the tail averages $\overline{x}_T$ converge to $x^*$ as 
\begin{equation}
\expectE \norm{\overline{x}_T - x^*}^2 \leq 2 (1 - \alpha)^{T_b+1} \norm{x_0 - x^*}^2 \\
 + \frac{1}{2\lambda \alpha^2 (T-T_b)} \cdot \max_i (r_i^2).
\end{equation}
Moreover, for any $\ell < k$ the convergence parameter $\alpha$ satisfies 
\begin{equation} \label{eq:alpha}
\alpha \geq  \frac{k-\ell}{(k-\ell) +  \kappa_\ell^2(A) + (m+k-2\ell)\frac{\lambda}{\sigma_{n_r}^2}},
\end{equation}
where $\kappa_{\ell}^2(A) := \sum_{j > \ell}^{n_r} \sigma_j^2(A)/\sigma_{n_r}^2(A)$ and $\sigma_1(A) \geq \dots \geq \sigma_{n_r}(A) > 0= \sigma_{n_{r+1}} (A) = \dots = \sigma_n(A)$ are the singular values of $A$. 
% \GG{Can we change $s \rightarrow \sigma$ for singular values since this is what we use elsewhere?}
\end{theorem}

By choosing $\ell =1$ and a small value of $\lambda$, the above theorem indicates that when $n_r=n$ the value of $\alpha$ is at least on the order of $k/\kappa^2_{\rm dem}(A)$ where $\kappa_{\rm dem}:= \|A\|_F / \sigma_{\min}(A)$ is the Demmel condition number. To understand the meaning of the bound more generally, suppose additionally that $\norm{a_i}=1$ for all $i$ so that $\norm{A}_F^2 = m$. Then, choosing $\ell = k/2$ we can rewrite \eqref{eq:alpha} as 
\begin{equation}
\begin{split} 
\label{eq:alpha-dpp-alterantive}
\alpha &\geq \frac{k}{k + 2\kappa_{k/2}^2(A) + 2m\lambda/\sigma_n^2} \\
&\approx \frac{k}{2\kappa^2_{k/2}(A) + 2 \lambda \kappa_{\rm dem}^2(A)}.
\end{split}
\end{equation}
As noted in \cref{sec:gauss_data}, the convergence parameter for mSGD is bounded by
\[ \alpha^{\rm SGD} \leq k/\kappa_{\rm dem}^2(A). \]
Hence, when the singular values of $A$ decay rapidly and $\lambda$ is significantly smaller than one, ReBlocK with DPP sampling can have a much faster convergence rate than mSGD.

Our bound for the variance of the tail-averaged estimator is crude and could likely be improved using techniques such as those of \cite{derezinski2025}. Nonetheless, it is interesting to note that the dependence of the variance on the residual vector can be worse in the case of DPP sampling than in the case of uniform samping. This is because the DPP distribution can potentially sample the residual vector in unfavorable ways. It is thus unclear whether DPP sampling is a good strategy for arbitrary inconsistent systems, even if it can be implemented efficiently

\begin{proof}
The majority of the proof follows similarly to the proofs of \cref{thm:rbk,thm:reblock}, and we only highlight the differences, of which there are three: the fact that $\xrh = x^*$, the value of the parameter $\alpha$, and the variance term in the tail-averaged bound.

We begin by verifying that $\xrh = x^*$. For a vector $u \in \mathbb{R}^{m}$, we denote its elementary symmetric polynomial by
$p_{k}(u):=\sum_{S \in\binom{[m]}{k}} \prod_{i \in S} u_i$, where $\binom{[m]}{k}$ is the set of all subsets of size $k$ from the set $[m]=\{1,\ldots,m\}$. Applying equation (5.3) of \cite{derezinski2024solving} by using $B = AA^\top + \lambda I$ leads to
\begin{equation} \label{eq:53}
\overline{W} = \expect{S \sim \rho}{I_S^\top (I_S (AA^\top + \lambda I) I_S^\top)^{-1} I_S} = \frac{U {\rm diag}(p_{k-1}(q_{-1}), \ldots, p_{k-1}(q_{-m})) U^\top}{p_{k}(q)},
\end{equation}
where $A = U\Sigma V^\top$ denotes the compact singular value decomposition of $A$, and $q_i = \sigma_i^2 + \lambda$ when $i \leq n_r$ and $q_i = \lambda$ otherwise, with $\sigma_1 \geq \dots \geq \sigma_{n_r}$ representing the sorted singular values and $n_r$ the rank of $A$. Denote by $\overline{W}= UDU^\top$ with the diagonal matrix $D:= \frac{{\rm diag}(p_{k-1}(q_{-1}), \ldots, p_{k-1}(q_{-m}))}{p_{k}(q)} \succ 0$. Then, by the definition $x^{(\rho)} = {\rm argmin}_{x \in \mathbb{R}^n} \|Ax - b\|_{\overline{W}}^2$, we have
\[ A^\top \overline{W} Ax^{(\rho)} = A^\top \overline{W}b. \]
Note that $A^\top \overline{W} A = V\Sigma D \Sigma V^\top$ and $A^\top\overline{W} = V\Sigma DU^\top b$. Thus, we can apply $V^\top$ on the left and use $D \succ 0$ to conclude
\[ \Sigma^2 V^\top x^{(\rho)} = \Sigma U^\top b. \]
Applying $V$ now on the left implies $A^\top A x^{(\rho)} = A^\top b$, i.e., $x^{(\rho)} \in {\rm argmin}_{x \in \mathbb{R}^n} \|Ax - b\|^2$. Since $\xrh \in \range(A^T)$ this implies that $\xrh = x^*$.

Regarding $\alpha$, plugging \eqref{eq:53} into the definition of $\overline{P}$ leads to
\begin{equation} \label{eq:P}
\overline{P} = A^\top \overline{W} A = V \Sigma^\top \frac{{\rm diag}(p_{k-1}(q_{-1}), \ldots, p_{k-1}(q_{-m}))}{p_{k}(q)} \Sigma V^\top.
\end{equation}

We now follow closely the logic of the proof of Lemma 4.1 of \cite{derezinski2024solving}, making appropriate modifications to handle the fact that in GRK we invert $AA^\top + \lambda I$ rather than just $AA^\top$. Note also that through \eqref{eq:P} we have explicitly verified that $\range(\overline{P}) = \range(A^T)$, which is the needed condition for the contraction properties to hold in our convergence bound. 

First, for any $\ell < k$ we can construct an approximation matrix 
\begin{equation}
B_\ell = U {\rm diag}\paren{q_1, \ldots q_m} U^\top + \frac{1}{k-\ell} \sum_{j>\ell}^m q_j I.
\end{equation}
Note that we have used a mildly simpler and looser approximation than \cite{derezinski2024solving} by replacing $\frac{k - \ell - 1}{k - \ell}$ with $1$ in the first term. 

The same arguments as in \cite{derezinski2024solving} then imply that
\begin{equation}
\alpha = \sigma_{\min}^+(\overline{P}) \geq \sigma_{\min}^+ (A^\top B_\ell^{-1} A).
\end{equation}
Furthermore we have
\begin{align*}
\sigma_{\min}^+ (A^\top B_\ell^{-1} A) &= \sigma_{\min}^+  \paren{V \Sigma^\top U^\top B_\ell^{-1} U \Sigma V^\top} \\
&= \sigma_{\min} \paren{V {\rm diag} \paren{\frac{\sigma_1^2}{q_1 + \frac{1}{k - \ell} \sum_{j>\ell}^m q_j}, \ldots, \frac{\sigma_{n_r}^2}{q_{n_r} + \frac{1}{k - \ell} \sum_{j>\ell}^m q_j}} V^\top}\\
&= \frac{\sigma_{n_r}^2}{q_{n_r} + \frac{1}{k - \ell} \sum_{j > \ell}^m q_j} \\
&=  \frac{\sigma_{n_r}^2}{\sigma_{n_r}^2  + \frac{1}{k - \ell} \sum_{j > \ell}^{n_r} \sigma_j^2 +  \frac{m+k-2\ell}{k - \ell} \lambda} \\
&= \frac{k-\ell}{(k-\ell) + \kappa_\ell^2(A) + (m+k-2\ell)\frac{\lambda}{\sigma_{n_r}^2}}.
\end{align*}
This completes the bound for $\alpha$.

Finally, we consider the variance term in the tail-averaged bound. Following the proofs of \cref{lemma:reblock,thm:reblock}, we can obtain a variance term of 
\begin{equation}
\frac{1}{2 \lambda k \alpha^2 (T - T_b)} \expectE_{S \sim \rho} \norm{r_S}^2  
\end{equation}
which is esentially the same as for ReBlocK-U but with the ordinary least-squares residual $r$ instead of the weighted residual $\rrh$. Unfortunately, in the case of DPP sampling, we have no guarantee on how $S$ samples the residual vector $\rrh$. Thus, the best we can do is uniformly bound $\norm{r_S}^2 \leq k \cdot \max_i (r_i^2)$. This yields the bound in the theorem.
\end{proof}

\section{Details of Numerical Experiments \label{app:num}}
In this appendix we provide more details on the numerical examples throughout the paper. The code to run the experiments can be found at \url{https://github.com/ggoldsh/block-kaczmarz-without-preprocessing}. 

\subsection{Experiments with random matrices \label{app:synth}}
The experiments in \cref{sec:rbk_num,sec:reblock_num} differ only in how the matrix $A$ is generated. For the left panel in \cref{fig:gaussian_results}, we generate the matrix $A$ by setting each entry independently as $A_{ij} \sim \mathcal{N}(0,1)$. For the right panel, we construct $A = GU$ where $G \in \Rea^{m \times n}$ has independent standard normal entries and and $U$ has $\sigma_i = 1/i^2$ and random orthonormal singular vectors. The condition number of the matrix $A$ is $\kappa(A) \approx 1$ in the left panel and $\kappa(A) \approx 10^4$ in the right panel.

The examples in \cref{fig:results_chebyshev} are constructed as discretizations of underlying continuous problems, which is both a realistic scenario and serves to ensure that $A$ contains many nearly singular blocks. In both cases, we first generate an $n \times n$ matrix $C$. For the left panel we set $C=I$, whereas for the right panel we set $C$ with singular values $\sigma_i = 1/i^2$ and random orthonormal singular vectors. Once $C$ is constructed, we define a set of $n$ functions $f_1, \ldots, f_n$ by 
\begin{equation}
f_j(s) = \sum_{\ell = 1}^n C_{j \ell} T_\ell(s),
\end{equation}
where $T_\ell$ is the $\ell^{\rm th}$ Chebyshev polynomial of the first kind. The functions $f$ correspond to the columns of $A$.

Next, we construct a vector $v$ of $m$ coordinates uniformly spaced across the interval $[-1, 1]$, namely
\begin{equation}
v_i = \paren{-1 + 2 \frac{i-1}{m-1}}.
\end{equation}
Finally, the matrix $A$ is designed as
\begin{equation}
A_{ij} = f_j(v_i),
\end{equation}
so that column $j$ of $A$ is a dicretized representation of the function $f_j$. The condition number of the resulting matrices $A$ are $\kappa(A) \approx 11$ in the left panel and $\kappa(A) \approx 3.6 \cdot 10^4$ in the right panel.

For all experiments in  \cref{sec:rbk_num,sec:reblock_num} we set $m=10^5$, $n=10^2$, and $k=30$. Furthermore, the vectors $b$ are constructed by setting
\begin{equation}
\label{eq:gen_b}
b_i = a_i^\top y + z_i
\end{equation}
where $y \sim \mathcal{N}(0,I_n)$ and $z_i \sim \mathcal{N}(0, 10^{-4})$. The initial guess is $x_0 = 0$ and the number of iterations is $T=10^4$, which is equivalent to three passes over the data. The step size for minibatch SGD is constant within each run and has been tuned independently for each example, specifically to be as large as possible without introducing any signs of instability, up to a factor of $2$. The value of $\lambda$ for ReBlocK has been set to $\lambda = 0.001$ and is not optimized on a per-example basis. The reported quantity is the suboptimality of the solution for the unweighted problem \eqref{eq:ls}, namely $ \epsilon = \norm{Ax-b} / \norm{Ax^* - b} - 1 = \norm{Ax-b} / \norm{r} - 1$. Since it is expensive to calculate the residual, this quantity is only calculated and reported every $10$ iterations. These experiments with random matrices are carried out in double precision on a 1.1 GHz Quad-Core Intel Core i5 CPU.

\subsubsection{Effect of Regularization Parameter $\lambda$ \label{app:lambda}}

In \cref{fig:lambdas}, we investigate the effect of the parameter $\lambda$ on the performance of ReBlocK. The problem is the same as in the right panel of \cref{fig:results_chebyshev}, in which $A$ contains many ill-conditioned blocks $A_S$ and exhibits rapid singular value decay. It is observed that choosing $\lambda=1$ approximately reproduces the results of mSGD, while $\lambda = 1e-3, 1e-6, 1e-9$ converges much faster, with $\lambda=1e-9$ beginning to show signs of instability. The results suggest that the performance of ReBlocK is only moderately sensitive to the choice of $\lambda$, since the parameter must be varied by multiple orders of magnitude to drastically change the performance.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{lambdas.pdf}
    \caption{Performance of TA-ReBlocK on the problem from the right panel of \cref{fig:results_chebyshev} with various values of $\lambda$. The reported quantity is the suboptimality of the approximate solution $x$ for the unweighted problem \eqref{eq:ls}, namely the value of $\epsilon$ for which $\norm{Ax-b} = (1 + \epsilon) \norm{r}$. The vertical dotted line indicates the burn-in period of $T_b=300$, before which results are shown for individual iterates.}
\label{fig:lambdas}
\end{figure}

\subsection{Natural Gradient Experiments \label{app:nn}}
In this section we describe in detail the numerical experiments of \cref{sec:ngd}. The training problem for the neural network is a simple function regression task on the unit interval. The target function is chosen to be periodic to avoid any consideration of boundary effects, and the neural network is correspondingly designed to be periodic by construction. Specifically, the target function is constructed as
\begin{equation} 
f(s) = q(\sin(2 \pi s)),
\end{equation}
with the polynomial $q$ defined as
\begin{equation}
q(s) = \frac{1}{\sqrt{d}} \sum_{\ell = 1}^{d} c_\ell T_\ell(s)
\end{equation}
for $d = 15$ and each $c_\ell$ chosen randomly from a standard normal distribution. The resulting function is pictured in \cref{fig:target}.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.4\textwidth]{target_fn.pdf} 
\caption{Target function for neural network training.}
\label{fig:target}
\end{figure*}

The neural network model is a simple periodic ResNet \cite{he2016deep} with 5 layers. For input $s \in \Rea$, the network outputs $f(s) \in \Rea$ given by
\begin{equation}
y_1 = \tanh(W_1 \sin (2 \pi s) + b_1),
\end{equation}
\begin{equation}
y_i = y_{i-1} + \tanh(W_i y_{i-1} + b_i) ;\; i = 2,3,4,
\end{equation}
\begin{equation}
f(s) = W_5 y_4 + b_5.
\end{equation}
The intermediate layers have dimensions $y_i \in \Rea^{50}$ and the weight matrices $W_i$ and bias vectors $b_i$ have the appropriate dimensions to match. The weights are initialized using a Lecun normal initialization and the biases are all initialized to zero. The parameters are collected into a single vector $\theta \in \Rea^{7801}$ for convenience, leading to the neural network function $f_\theta(r)$. 

The loss function is defined as in \eqref{eq:func_learn} and the network is trained using subsampled natural gradient descent, as described for example in \cite{ren2019efficient}, with a batch size of $N_b = 200$, a Tikhonov regularization of $\lambda = 0.01$, and a step size of $\eta = 0.5$. This corresponds to the parameter update
\begin{equation}
\theta_{t+1} = \theta_t - \eta J_S^\top (J_S J_S^\top + N_b \lambda I)^{-1} [f_\theta - f]_S,
\end{equation}
where $S$ represents the set of $N_b$ sample points. The setting of $N_b=200$ is intended to represent something close to a full batch training regime, which is only practical since we are studying a very small network on a very compact, low-dimensional domain. The resulting training curve is presented in \cref{fig:nn_train}.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.4\textwidth]{NN_training.pdf} 
\caption{Training curve for the neural network function regression example. The three vertical lines indicate the training snapshots that are used to generate the least-squares problems studied in \cref{fig:nn}.}
\label{fig:nn_train}
\end{figure*}

From this training run, three snapshots are taken and used to generate the least-squares problems for \cref{fig:nn}, following \cref{eq:ngd_ls}. The first snapshot is from the ``pre-descent'' phase before the loss begins to decrease, the second is from the ``descent'' phase during which the loss decreases rapidly, and the third is from the ``post-descent'' phase when the loss has stopped decreasing significantly. The snapshots are indicated by the vertical dotted lines in \cref{fig:nn_train}. The algorithms TA-mSGD, TA-RBK, and TA-ReBlocK are then tested on the resulting problems wth an initial guess of $x_0=0$ and a batch size of $k=30$. The batch size  of $k=30$ is meant to represent the realistic scenario when each iteration uses too few samples to thoroughly represent the target function. Furthermore, the continuous problems are treated directly by uniformly sampling $k=30$ points from the domain $[0,1]$ at each iteration and calculating the network outputs and gradients at these points. 
 
The step size for minibatch SGD is constant within each run and has been tuned independently for each example, specifically to be as large as possible without introducing any signs of instability, up to a factor of $2$. The value of $\lambda$ for ReBlocK has been set to $\lambda = 0.001$ and is not optimized on a per-example basis. The reported quantity is the relative residual $\tilde{r} = \norm{J x - [f_\theta - f]} / \norm{f_\theta - f}$, which measures how well the function-space update direction $J x$ agrees with the function-space loss gradient $f_\theta - f$. This quantity is estimated accurately by sampling $m=2 \cdot 10^4$ points from $[0,1]$. Furthermore, since it is expensive to calculate this residual, the quantity is only calculated and reported at approximately $10^3$ evenly spaced iterations for each run of each algorithm.

\subsubsection{Iteration Cost of Each Algorithm \label{app:speed}}
In \cref{fig:it_speed}, we report the number of iterations per second for mSGD, RBK, and ReBlocK for the experiments of \cref{sec:ngd}. We find that RBK iterations are approximately five times slower than ReBlocK iterations and six times slower than mSGD iterations for these particular problems. Interestingly, there is only a slight difference in speed between the ReBlocK and mSGD iterations. The experiments are conducted in single precision on an A100 GPU to simulate a deep learning setting.

\begin{figure*}[ht]
\centering
\includegraphics[width=0.4\textwidth]{it_speed.pdf} 
\caption{Iteration cost of each algorithm for the natural gradient experiments of \cref{sec:ngd}.}
\label{fig:it_speed}
\end{figure*}

\subsubsection{Singular Value Analysis \label{app:singular}}
In \cref{fig:singular}, we analyze the singular values of the Jacobian operator $J$ for each experiment in \cref{fig:nn}. We first approximate $J$ by a finite matrix $A$ by sampling $m = 2 \cdot 10^4$ points from the domain $[0,1]$, then use SciPy's svds function with the ARPACK solver to approximate the top $200$ singular values of $A$. We find that in every case the top singular values decay exponentially up to some threshold, after which the tail decays more slowly. The initial exponential decay helps to explain the faster convergence rate of ReBlocK relative to mSGD.

\begin{figure*}[ht]
\centering
\centering
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{NN_start_spectrum.pdf}}
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{NN_mid_spectrum.pdf}}
% \subfigure[]
{\includegraphics[width=0.3\textwidth]{NN_end_spectrum.pdf}}
\caption{Top $200$ singular values of the input matrix $A$ for each natural gradient-based least-squares problem. The left, middle, and right panels correspond to the same panels in \cref{fig:nn}, and the vertical dotted line represents the batch size of $k=30$ used in the least-squares solvers for \cref{fig:nn}.}
\label{fig:singular}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
