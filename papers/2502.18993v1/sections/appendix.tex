\appendix
\newpage
\section{Appendix}
\label{sec:appendix}

% \subsection{Prompt}
% \subsubsection{Prompt for schema} \label{appendix:schema}
% \begin{mdframed}
% Create a table schema that comprehensively captures information about \{\}. Ensure the schema is detailed and structured, avoiding over-simplification, missing elements, and redundancy. This schema should be structured so each row represents a unique instance, with each column capturing a distinct aspect of property details. Ensure there is no overlap in content between columns to avoid repetition.
% \end{mdframed}

\subsection{Methodology for composite SPARQL Generation via Iterative Semantic Refinement}\label{sec:sparql}  
\subsubsection{Initial Query Parsing Using GPT-4} We employ a transformer-based large language model (LLM), specifically GPT-4, to perform preliminary natural language question decomposition. This stage generates a proto-SPARQL query containing candidate triple patterns with hypothesized entity-property relationships. While this initial output captures broad syntactic structures (e.g., basic graph pattern groupings), it frequently exhibits two critical inaccuracies:

Entity Misalignment: Incorrect Wikidata Q-ID assignments due to lexical ambiguity (e.g., "Java" as programming language vs. Indonesian island)

Property Mismatch: Invalid P-ID selections arising from underspecified predicate semantics (e.g., using P19 [place of birth] instead of P20 [place of death])

\subsubsection{Semantic Validation Layer} To address these limitations, we implement a multi-stage correction framework:

(a) Structured Knowledge Anchoring

The system interfaces with the Wikipedia API through programmatic endpoints that map surface forms to canonical entities via:
\begin{mdframed}
def getwikidataid(term):

response = requests.get(

f"https:\/\/en.wikipedia.org\/w\/api.php?

action=query\&format=json\&prop=pageprops\&titles={term}"
        
    )
    
 return response.json()["query"]["pages"].get("pageprops",
    
    {}).get("wikibaseitem")
\end{mdframed}    
(b) Neural-Semantic Disambiguation Module

A fine-tuned GPT-4 variant serves as our semantic analysis engine, performing three key operations: 
1. Contextual disambiguation using entity linking algorithms enhanced by Wikifier-style mention detection 
2. Property type validation against Wikidata's ontology constraints (rdf:type, owl:ObjectProperty) 
3. Temporal scope verification for time-sensitive queries requiring qualifiers like P585 [point in time]

\subsubsection{Iterative Refinement Protocol} The system implements closed-loop feedback through successive cycles of:

Executing candidate SPARQL on the Wikidata Query Service endpoint
Analyzing result cardinality and type consistency 3 Applying constraint satisfaction heuristics:
FILTER (?population > 1e6 \&\& ?country IN wd:Q30) \# Example numerical/entity constraints 
Each iteration tightens precision metrics until meeting termination criteria defined by either:

$\frac{|ValidResults_t|}{|TotalResults_t|} \geq \theta_{precision} \quad (\theta = 0.
98\;\text{empirically})$

or maximum iteration thresholds.

\subsubsection{Final Query Synthesis} Through combining LLM-based semantic parsing with knowledge-grounded verification, we converge on an optimized SPARQL template satisfying both syntactic validity and functional correctness requirements for structured knowledge extraction.
% \subsection{Baseline Performance.}
% \stitle{RAG vs w/o RAG? Open-source vs. Close-source? or Prompt vs. Fine-tuning????}
% For baselines, the introduction of RAG leads to impressive improvement in overall performance, especially on comparison tasks. In contrast, fine-tuning LLaMA-3-Instruct fails to yield a prominent improvement; it still requires the incorporation of RAG to achieve a fundamental performance boost. Additionally, the capabilities of the LLM itself serve as a crucial factor. On \qa, current open-source models are unable to compete with cutting-edge proprietary models like GPT-4. Even after fine-tuning and introducing RAG, LLaMA-3-Instruct can only achieve an overall accuracy comparable to that of vanilla GPT-4 (31.6\%), whereas GPT-4 combined with RAG reaches the highest baseline overall accuracy of 59.3\%. 


% \begin{table*}
%   \centering
%   \resizebox{\textwidth}{!}{
%   \begin{tabular}{p{2cm}p{4cm}p{8.5cm}}
%     \toprule
%     \textbf{Types} & \textbf{Sub-types} & \textbf{Template Examples}  \\
%     \midrule
%     % Retrieval & single-hop & What is the [property] of [entity]? \\
%     %  & multi-hop&What are the [property] of the [property] of [entity]?\\
%     % \hline
%     Comparison & Intercomparison   & Which has high [property], [entity A] or [entity B]?\\
%      & Superlative &Which [entity] has the highest/lowest [property]?\\
%     \hline
%    Statistical & Aggregation &How many [entities] have [specific property value]?\\
%    &Distribution Compliance& Does [property] follow a normal distribution?\\
%    &Correlation Analysis &Is there a linear relationship between [property A] and [property B]?\\
%    &Variance Analysis&Are the variances in [property A] and [property B] significantly different?\\
%     \hline
%     Relationship & Descriptive Relationship &How is [entity A] related to [entity B]?\\
%     & Hypothetical Scenarios &What would be the impact if [entity A] collaborates with [entity B]?\\
%     \bottomrule
%   \end{tabular}
%   }
%   \caption{Template example for queries generated by the LLM (GPT-4).}
%   \label{qtypeteplate}
% \end{table*}
\subsection{Optimization} 
Two aspects of optimization are included in \ben system to enhance the overall performance:

% \stitle{Data Filtering}. One significant advantage of the process of extracting information from text and organizing it into a tabular format is the inherent ability of this process to filter out irrelevant data. During the extraction phase, LLMs are employed to identify and categorize the most pertinent pieces of information based on schema. This means that only data deemed relevant to the query is selected for inclusion in the table. As a result, users are presented with a clean, concise table that contains only the data necessary for their analysis or decision-making processes. 
% \yizhang{Q: what algorithms and techniques here exactly}

\paragraph{Model Selection.} \label{sec:model_selection} 
Model selection is straightforward yet highly effective for optimization~\citep{liu2024declarative}. Our system comprises multiple tasks, necessitating the selection of the most suitable model for different tasks. For basic tasks, more affordable and faster LLMs can suffice, while utilization of the most advanced LLMs is essential in more complex tasks to ensure optimal performance. Specifically, our system employs powerful yet resource-intensive GPT-4 for tasks such as semantic analysis or generation of table schemas and SQL queries. In contrast, for more basic information extraction, we utilize open-source Mistral-7B, thereby achieving a balance between cost efficiency and functional performance.

\paragraph{LLM Input/Output Control} SplitWise~\citep{patel2023splitwise} shows that LLM inference time is generally proportional to the size of input and output tokens. Since GPT models decide the cost based on the input token, we try to minimize the input of large models. Meanwhile, we use the instructive prompt to reduce the size of the outputs generated by LLM without changing the quality of these outputs. The example of prompt is in Appendix~\ref{appendix:output}.
% \yizhang{prompt in appendix}

\subsubsection{Prompt for Output Control}\label{appendix:output}
\begin{mdframed}
Review your output to ensure it meets all the above criteria. Your goal is to produce a clear, accurate, and well-structured output. Just output the result, no other word or symbol.
\end{mdframed}

\subsection{Question Templates}
By encoding each interrogative pattern into modular syntactic frames (e.g. "Does [Entity X] exhibit [Attribute Y] when [Condition Z]?"), we achieve three critical objectives: (1) Elimination of lexical ambiguities through controlled vocabulary substitution ; (2) predictable semantic scaffolding that primes respondents toward domain-appropriate reasoning pathways; (3) standardized response elicitation critical for comparative analytics across heterogeneous samples. 

\subsection{Manual review}
The manual review process constitutes a critical quality control mechanism in benchmark development, systematically implemented through three key phases: Content Validation Protocol, Psychometric Evaluation and Reliability Optimization. A tiered review system enhances inter-rater reliability: a) Primary coding by two independent reviewers b) Discrepancy resolution via consensus panels c) Final calibration against reference standards.

\subsection{Tables}
Table~\ref{tab:topic-e} shows examples of topics and their entities' attributions. Table~\ref{tab:template} shows examples of question templates to synthesize questions.

\begin{table*}[ht]
\centering
\caption{
Example Topics and Their Entities Attributions. 
% We retrieval the related entities from Wikipedia.
}
\vspace{.2em}
% \resizebox{\textwidth}{!}{
\begin{tabular}{lp{7cm}c}
\toprule
    \textbf{Topics}           & \textbf{Entities Attributions}  & \textbf{\#-Entities}\\
    \midrule
    \multirow{1}{*}{ACM fellow} & nationality, field of study, affiliation & \multirow{1}{*}{1115}\\
    \hline
    \multirow{2}{*}{Presidents of the US} & term lengths, political parties, vice-presidents, birth states, previous occupations & \multirow{2}{*}{55}\\
    \hline
    \multirow{2}{*}{Chemical Elements} &  atomic number, atomic mass, boiling point, melting point, electron configuration & \multirow{2}{*}{166}\\
    \hline
    \multirow{2}{*}{Summer Olympic Games} & host cities, number of participating countries, total number of events, medal tally, records broken & \multirow{3}{*}{35}\\
    \hline
    \multirow{2}{*}{Nobel Prize in Chemistry} &  categories, year of award, country of origin, field of contribution.& \multirow{2}{*}{194}\\
    \hline
    \multirow{1}{*}{Cities of the World} &  population, geographic coordinates, altitude, GDP  &\multirow{1}{*}{7040}\\
\bottomrule
\end{tabular}
% }
\label{tab:topic-e}
\end{table*}

\begin{table*}[ht]
\centering
\renewcommand\arraystretch{1}
\caption{Template example for questions generated by the LLM (GPT-4).}
\vspace{.2em}
\begin{tabular}{clp{7.5cm}}
\toprule
\textbf{Types} & \textbf{Sub-types} & \textbf{Template Examples}  \\
\midrule
\multirow{2}{*}{Comparison} 
    & \multirow{1}{*}{Intercomparison}   & Which has high [property], [entity A] or [entity B]?\\
    \cmidrule(lr){2-3}
    & \multirow{1}{*}{Superlative} & Which [entity] has the highest/lowest [property]?\\
\midrule
\multirow{6}{*}{Statistics} 
    &\multirow{1}{*}{Aggregation} & How many [entities] have [specific property value]?\\
    \cmidrule(lr){2-3} 
    &\multirow{1}{*}{Distribution Compliance} & Does [property] follow a normal distribution? \\
    \cmidrule(lr){2-3} 
    &\multirow{2}{*}{Correlation Analysis} & Is there a linear relationship between [property A] and [property B]?\\
    \cmidrule(lr){2-3} 
    &\multirow{2}{*}{Variance Analysis} & Are the variances in [property A] and [property B] significantly different?\\
\midrule
\multirow{3}{*}{Relationship} 
    & \multirow{1}{*}{Descriptive Relationship} & How is [entity A] related to [entity B]?\\
    \cmidrule(lr){2-3} 
    & \multirow{2}{*}{Hypothetical Scenarios} & What would be the impact if [entity A] collaborates with [entity B]?\\
\bottomrule
\end{tabular}
\label{tab:template}
\end{table*}

