\section{Related Work}
Recent advancements in question answering (QA) have been driven by breakthroughs in LLMs and RAG systems. While these technologies excel in single or a few document settings, demonstrating proficiency in tasks like fact extraction, summarization, and reasoning within a single source, their performance in cross-document, multi-entity scenarios remains underexplored. This section contextualizes our work within three key research areas: single-document QA, cross-document aggregation, and entity-centric evaluation.

\subsection{Single-Document QA and LLM Progress.}
Many QA benchmarks, such as SQuAD~\citep{2016SQuAD}, Natural Questions ~\citep{2019Natural}, L-eval~\citep{an-etal-2024-l} and needle-in-a-haystack~\citep{Kamradt2023}, focus on extracting answers from individual document.  Modern LLMs like GPT-4~\citep{achiam2023gpt}, Llama-3~\citep{Llama3_2024}, and PaLM~\citep{2023PaLM} have achieved near-human performance on these tasks, leveraging their ability to parse and reason within localized contexts.  However, these benchmarks do not address the complexities of integrating information across multiple documents, a critical limitation for real-world applications.

\subsection{Cross-Document Aggregation Challenges.}
Efforts to extend QA to multi-document settings include datasets like HotpotQA~\citep{2018HotpotQA}, MuSiQue~\citep{2021MuSiQue}, LooGLE~\citep{li2024loogle}, LM-Infinit~\citep{han-etal-2024-lm}, $\infty$ Bench~\citep{zhang2024inftybench}, CLongEval~\citep{qiu2024clong}, BAMBOO~\citep{dong-etal-2024-bamboo} and Loong~\citep{2024Leave}, which emphasize multi-hop reasoning and cross-source synthesis. While these benchmarks highlight the need for systems to connect disparate information, they often prioritize breadth over depth in entity-centric reasoning. For instance, questions in these datasets rarely demand the consolidation of attributes for dozens of or more entities (\eg aggregating ACM Fellowsâ€™ expertise across fields), a gap that limits their utility in evaluating entity-dense scenarios. Recent RAG frameworks~\citep{fan2024Asurvey} aim to enhance retrieval-augmented QA but struggle with ensuring completeness and attribution validity when handling multi-entity queries.

\subsection{Entity-Centric Evaluation Metrics.}
Existing evaluation metrics for QA, such as F1 score and exact match (EM), focus on answer surface-form correctness but overlook granular entity-level attribution~\citep{5707187}. Metrics in  FEVER~\citep{thorne2018fever} and Attributed QA~\citep{bohnet2023attribut} emphasize source verification, yet they lack the specificity to assess multi-entity integration. For example, they do not systematically measure whether all relevant entities are retrieved, their attributes are correctly extracted, or their sources are accurately used, which is a shortcoming that becomes critical in MEQA tasks.

\subsection{The Gap in Multi-Entity QA Benchmarks.}
Prior work has yet to establish a benchmark that systematically evaluates LLMs and RAG systems on entity-dense, cross-document reasoning. Current datasets either lack the scale and diversity of real-world multi-entity questions or fail to provide fine-grained metrics for assessing entity-level completeness and attribution~\citep{2024Leave}~\citep{bai2025longbenchv2}. MEBench addresses these limitations by introducing a comprehensive evaluation framework that challenges models to retrieve, consolidate, and reason over scattered entity-centric data across heterogeneous sources. By incorporating the Entity-Attributed F1 (EA-F1) metric, our benchmark advances the field toward more precise, entity-aware QA systems.

% Our work builds on these foundations while explicitly targeting the underexplored challenges of multi-entity, cross-document QA, offering a structured pathway to address the systemic weaknesses of current LLM and RAG architectures.
