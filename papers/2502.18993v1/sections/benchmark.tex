\section{\ben}

\begin{table*}[t!]
\centering
\renewcommand\arraystretch{1}
\caption{Examples of multi-entities queries.}
\vspace{.2em}
\begin{tabular}{clp{7.5cm}}
\toprule
\textbf{Categories} & \textbf{Types} & \textbf{Examples}  \\
\midrule
\multirow{2}{*}{Comparison} 
    & \multirow{1}{*}{Intercomparison}   & Which has more ACM fellow, UK or USA?\\
    \cmidrule(lr){2-3}
    & \multirow{1}{*}{Superlative} &Which city has the highest population?\\
\midrule
\multirow{8}{*}{Statistics} 
    &\multirow{1}{*}{Aggregation} &How many ACM fellow are from MIT?\\
    \cmidrule(lr){2-3} 
    &\multirow{2}{*}{Distribution Compliance} & Does the nationality of ACM fellows follow a normal distribution? \\
    \cmidrule(lr){2-3} 
    &\multirow{2}{*}{Correlation Analysis} & Is there a linear relationship between number of events and records broken in Olympic Games?\\
    \cmidrule(lr){2-3} 
    &\multirow{3}{*}{Variance Analysis} & Do the variances in the number of participating countries and total events in the Summer Olympics differ significantly?\\
\midrule
\multirow{4}{*}{Relationship} 
    & \multirow{2}{*}{Descriptive Relationship} &Is there a relationship between the year of ACM fellowship induction and the fellows' areas of expertise?\\
    \cmidrule(lr){2-3} 
    & \multirow{2}{*}{Hypothetical Scenarios} & If China wins one more gold medal, will it overtake the US in the gold medal tally at the 2024 Olympics?\\
\bottomrule
\end{tabular}
\label{tab:examples}
\end{table*}

\begin{table*}[t!]
  \centering
    \caption{Statistics of \ben benchmark.}
    \vspace{.2em}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Categories}  & \textbf{\ben -train} & \textbf{\ben -test}& \textbf{\ben -total} \\
    \midrule
   % Types& Comparison, Relationship  &Comparison, Statistics, Relationship \\
    \#-Queries  &3406 &1374 &4780 \\
     \#-Topics  &165 &76 & 241\\
     Ave. \#-entities /Q   &460 &391 & 409\\
     \hline
    \multicolumn{4}{c} {\textit{Hops}}\\
     \hline
    \#-one-hop Q    & 1406 &606 & 2012\\
    \#-multi-hop Q    & 1322& 768& 2090\\
    \hline
    \multicolumn{4}{c} {\textit{Categories}}\\
     \hline
    \#-Comparison  &1107 &438 & 1545\\
    \#-Statistics &1440 &585 & 2025\\
    \#-Relationship &859 &351 & 1210\\
    \hline
    \multicolumn{4}{c} {\textit{Entity Density}}\\
     \hline
    \#-low (0–10)  &487 &196 &683 \\
    \#-medium(11–100)  &973 &393 &1366 \\
    \#-high (>100)  &1946 &785 &2731 \\
    % \#-test set & 1374\\
    % \#-train set & 3406\\ 
   \bottomrule
  \end{tabular}
  \label{tab:querycategories}
\end{table*}

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1\linewidth]{figs/mebench-3-1.png}
\end{center}
%  \vspace{-.8em}
  \caption{The systematic pipeline of Benchmark Construction. It comprising three phases: documents collection, information extraction and question-answer generation. In the documents collection phase, concept topics relevant to multi-entity scenarios are selected, followed by GPT-4 processing descriptions to extract entities and properties mapped to Wikipedia IDs for integration with structured Wiki data. Structured information from Wikipedia documents is processed using small language models (SLMs) due to the structured nature of the documents, culminating in table creation with entity attributes as columns. For QA generation, questions are generated following a "template-driven, entity-attribute coupling" paradigm using GPT-4 with predefined templates, and undergo syntactic, semantic, and ambiguity checks, while answers are programmatically derived via SQL queries against the table and standardized into canonical forms. The final dataset ensures traceability (SQL-linked answers), scalability (template-driven approach), and rigor (execution-based answering reduces hallucination risks).}
  \label{fig:mepipe}
\end{figure*}

\subsection{Task overview}
\ben is a structured evaluation framework designed to systematically assess the capabilities of LLMs in performing cross-document multi-entity question answering. This framework targets three core reasoning modalities: comparative analysis, statistical inference, and relational reasoning, and each decomposed into specialized subtasks that rigorously test distinct facets of LLM performance. (Examples are provided in Table ~\ref{tab:examples}), ensuring broad coverage of real-world multi-entity reasoning scenarios. 
\ben evaluates LLMs on cross-document \meqa through three primary task categories, each addressing distinct reasoning challenges:

\subsubsection{Comparative Reasoning}

Comparative reasoning tasks evaluate LLM’s ability to juxtapose entities across heterogeneous documents, demanding both attribute alignment and contextual synthesis.

\paragraph{Intercomparison (Cross-Document Entity Contrast)}: Intercomparison task requires models to perform multi-faceted comparisons of attributes or relationships among entities distributed across documents. This involves not only identifying relevant information from disparate sources but also aligning it in a coherent manner to facilitate direct comparison.

\paragraph{Superlative Identification}: This subtask tests the model’s capacity to identify maxima/minima or rank entities based on quantitative or qualitative criteria.This task tests the model's proficiency in quantitative analysis and its ability to handle superlative queries that require precision and attention to detail.

\subsubsection{Statistical Reasoning} 

Statistical tasks~\citep{zhu2024largel} assess LLM’s proficiency in \textbf{quantitative synthesis}, including aggregation, distributional analysis, correlation analysis, and variance analysis across multi-document.

\paragraph{Aggregation}: For aggregation task, models must calculate cumulative values from data spread across different documents, demonstrating its ability to perform basic arithmetic operations and handle aggregation queries effectively.  

\paragraph{Distribution Compliance}: Here, the focus is on understanding and analyzing statistical distributions. This task requires the model to parse data, categorize them appropriately, and compute proportions, thereby showcasing its ability to comprehend and manipulate statistical data.

\paragraph{Correlation Analysis}: Tests the ability of LLM to infer relationships between variables. This task tests the LLM's capability to understand complex relationships and draw logical inferences based on evidence from multiple sources. 

\paragraph{Variance Analysis (Outlier Detection)}: Assesses sensitivity to data variability and anomalies. This requires the model to perform statistical analysis to understand variability and its implications.

\subsubsection{Relational Reasoning}
Relational tasks probe an LLM’s capacity to model explicit interactions and counterfactual dependencies among entities.

\paragraph{Descriptive Relationship}:Examines the model’s ability to reconstruct explicit entity interactions. Descriptive relationship tests the model's ability to construct coherent narratives based on factual data.

\paragraph{Hypothetical Scenarios}: This task evaluates LLM’s ability to engage in counterfactual reasoning, a critical component of causal inference and speculative analysis. Model must simulate alternate realities by removing or altering specific entities or events and hypothesizing downstream impacts, demonstrating its ability to engage in abstract and speculative thinking.
% \begin{bluebox}
\subsection{Benchmark Construction}
MEBench was constructed through a systematic pipeline:  
% \end{bluebox}
\subsubsection{Data Collection} 

\paragraph{Concept Topic Identification}. In the initial phase of data collection for MEbench, a meticulous process is employed to determine the concept topics that are applicable to multi-entity scenarios. These topics are carefully selected based on their significance, prevalence, and the potential for generating complex multi-entity questions, and examples can be seen in Appedix Table~\ref{tab:topic-e}.

\paragraph{Entity and Property Identification}. Once the concept topics are determined, We input descriptions related to the concept topics into the LLM (GPT-4), which then processes the text to identify concept entities and property, as illustrated in Figure~\ref{fig:mepipe}-a1. After the LLM identifies the entities and Property via iterative semantic refinement, we map them to entity IDs and Property IDs in the Wiki graph. This mapping is crucial as it allows for seamless integration with the vast amount of structured data available in Wikipedia. The detailed method is in Appendix~\ref{sec:sparql}. Using the Entity ID and property ID, we synthesise SPARQL. We then utilize the API provided by Wikipedia to retrieve the wiki web pages of all entities related to the topic. For example, if our concept topic is "ACM Fellows", we would obtain the Wikipedia pages of all ACM Fellows, which contain their detailed information. We use GPT-4 to generate a set of interesting entity attributes. These attributes are carefully chosen based on the general interest and relevance in the domain. For ACM Fellows as example, nationality, research field, institution, and academic contribution are attributes that people commonly pay attention to.

\paragraph{Structured Information Processing}. Once the document set is collected, we proceed to the structured information processing stage. The documents we have gathered from Wikipedia have well-defined and accurate structural relations. Due to the structured nature of the documents, we do not need to rely on the long context ability of large language models. Instead, we can use small language models (SLMs) for information extraction. They are well-suited for tasks where the information is already structured and the focus is on extracting specific details~\citep{fan2025minirag}.

\paragraph{Table Generation}. The final step in the data collection process is to generate a table, as shown in Figure~\ref{fig:mepipe}-b1. We use the extracted information and the entity attributes as the column headers of the table. Each row in the table represents an individual entity. For example, in the case of ACM Fellows, each row would correspond to an individual ACM Fellow. 

\subsubsection{Question and answer Generation}  
The question and answer generation framework for \ben is a structured, multi-phase process that leverages LLM and tabular data to produce both semantically coherent questions and computationally verifiable answers.

\paragraph{Question Generation}
The foundational input for the QA generation pipeline is the table generated in last step. The generation of questions follows a "template-driven, entity-attribute coupling" paradigm, implemented through LLM (GPT-4), as illustrated in Figure~\ref{fig:mepipe}-c1. Predefined syntactic and semantic templates govern the grammatical structure and intent of questions. These templates are shown in Appendix Table~\ref{tab:template}. The LLM instantiates templates with entity-attribute pairs, ensuring syntactic diversity while adhering to logical constraints. Generated questions undergo validation via: Syntactic Checks, Ensuring grammatical correctness; Semantic Grounding, verifying that the question is answerable using the table’s data. Ambiguity Reduction, pruning underspecified questions (\eg "Describe the economy" revised to "Describe the GDP growth rate of Brazil in 2023").
\paragraph{Answer Generation}
Answers are derived programmatically through automated SQL query execution, ensuring reproducibility and alignment with the table’s ground-truth data. The synthesized SQL is executed against the table, yielding direct answers or sub-tables (Intermediate results requiring post-processing), as illustrated in Figure~\ref{fig:mepipe}-c3. Answers are standardized to ensure consistency: Numeric results are rounded to significant figures; Categorical answers are converted to canonical forms (\eg "USA" to "United States").
\paragraph{Integration and Validation}
The final output consists of a dataset where each question is matched with its corresponding answer derived from SQL queries, thereby meeting the specified requirements:
\begin{itemize}
\item {\textbf{Traceability}}: Each answer is directly linked to the table via its SQL provenance.   
\item {\textbf{Scalability}}: The template-driven approach supports rapid generation of diverse QA pairs.
\item {\textbf{Rigor}}: Execution-based answering eliminates hallucination risks inherent in generative-only methods.
\end{itemize}

\subsubsection{Quality Control} 
We devise several strategies to ensure the integrity and effectiveness of questions.

\paragraph{Question Templates.}
The use of templates ensures that every question is crafted with a clear structure, making it easier for respondents to understand and answer them accurately. For relationship and complex statistic questions we turn the questions in a closed-ended style, as they require a specific response of either "yes" or "no", which make the answer in a standardized format. The examples of Question Templates is in the Appendix \ref{tab:template}. 

\paragraph{Question Refinement.} After initial development, each question undergoes a refinement process which we used GPT-3.5-Turbo. This stage is critical for enhancing the clarity, relevance, and neutrality of the questions. It involves reviewing the questions for bias. This strategy helps in reducing misunderstandings and improving the overall quality of the questions.

\paragraph{Manual review.} We assess the questions for accuracy, ensuring they are factually correct and relevant to the our purpose. Manual reviews can also provide insights into whether the questions are likely to effectively elicit the intended information from answers, thereby contributing to the reliability and validity of the benchmark.
   
\subsection{Data Statistics}
The benchmark comprises 4,780 methodically structured questions partitioned into two subsets: a training set (3,406 questions) for model fine-tuning and a test set (1,374 questions) for rigorous evaluation. Based on entity count, the data is divided into three groups: ``low'' (0-10), ``Medium'' (11-100), and ``high'' (>100), containing 683, 1366, and 2731 entries, respectively. Table~\ref{tab:querycategories} details comprehensive statistics of the benchmark. We also analyze the proportion of questions rejected during manual review and find that 21\% of the questions are rejected for unqualified question.



