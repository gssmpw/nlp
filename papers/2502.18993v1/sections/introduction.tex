\section{Introduction}

% \nan{First, the example is not concrete. Second, why Doc structure and Entities are more important/interesting than Evidences.}


% \nan{I did not see any change of the figure. Can you search LLMs for multi-entity QA to see how many existing works are there? Then, as a reviewer, my question is: what is new? Of course, all RAG will locate all entities. The problem is not new. Also, the example is not concrete.}

% \nan{In practice, there may exist documents that are not needed. Hence, why your benchmark, without any false positives, is more practical and needed. When I look at Figure 1, I may even think that existing benchmarks are better. Also, you put ``Structure'' in MEBen of Figure 1, why the documents have a Structure tag?}

% \teng{I change the figure. All docs are from a Graph search from Wiki Graph, so they are all related to the question and they are in a folder ``Turing Award ''.  I remove the ``Structure'' since this doesn't need to be emphasized here.}

% The followings are missing
% The first question is from an existing multi doc benchmark which the evidence is centralized in a few doc.
% - good examples of data and questions: using a figure to describe

%- why such a benchmark is needed: using blue font to highlight your changes

%- you can put your result table 2 (statistics) in Intro

%- then you need to focus on your solution of benchmark construction is good: where did you write it, highlight it in blue color

%Please import my command.tex macro, as in previous paper, so I can easily annotate in this paper[ok]

The emergence of large language models (LLMs) has significantly advanced natural language processing capabilities, demonstrating exceptional performance in diverse tasks spanning text generation to complex logical reasoning~\citep{achiam2023gpt}~\citep{Llama3_2024}. Nevertheless, their effectiveness in addressing cross-document multi-entity question answering (\meqa), a task requiring the integration of fragmented, entity-specific information across heterogeneous document sources, remains insufficiently investigated. Current implementations of retrieval-augmented generation (RAG) architectures~\citep{wu2025clash}~\citep{fan2024Asurvey}~\citep{Tang2024graphgpt} and long-context LLM frameworks exhibit notable limitations in processing entity-dense analytical reasoning, particularly when contextual dependencies are distributed across multiple documents, and we analytically argue that context window limitations, over-reliance on parametric knowledge, and poor cross-document attention as the key bottlenecks. Furthermore, the field lacks comprehensive benchmarking frameworks specifically designed to evaluate cross-document entity-intensive scenarios~\citep{song2024counting}. As shown in Figure~\ref{fig:benexa}, existing evaluation metrics frequently inadequately represent the complexities inherent in real-world MEQA applications, where queries such as ``What is the distribution of Turing Award winners by fields of study?'' necessitate not only high-precision information retrieval but also structured synthesis of overlapping, contradictory, or complementary data points extracted from disparate documents.  
% \nan{After reading this paragraph, I have no clue what is the methodological gap?} \teng{I will revise this part.}

To address this methodological gap, we present MEbench, a novel benchmarking framework specifically designed to assess the performance of large language models and RAG systems in cross-document multi-entity question answering scenarios. The benchmark simulates real-world information integration challenges where correct answers require synthesizing entity-centric evidence distributed across multiple documents, with a single instance of document omission or entity misinterpretation can propagate errors through the reasoning chain.

% \nan{Please ask someone to read, to see whether they understand what a concrete example would look like. Otherwise, there is meaningless to give statistics. In fact, you should give good and concrete examples in different categories, showing why they are good, not solved, and then show your statistics.}\teng{I will revise this part and the table.}

As shown in Table~\ref{tab:querycategories}, \ben features a mean entity density of 409 entities per query, with systematically varied entity cardinality across three operational tiers: low (0-10 entities), medium (10-100 entities), and high complexity (>100 entities). This stratified design enables granular performance evaluation across different entity scales and task difficulty levels. The framework comprises 4,780 validated question-answer pairs systematically categorized into three primary categories and eight distinct types, MEBench spans diverse real-world scenarios, from academic field distributions to geopolitical event analysis.

Our experiments with state-of-the-art models, including GPT-4 and Llama-3, reveal significant shortcomings: even the most advanced LLMs achieve only 59\% accuracy on MEBench. This underscores systemic weaknesses in current frameworks, for example, models frequently fail to locate all entity attributes or infer implicit relationships, highlighting the need for architectures that prioritize entity-aware retrieval and contextual consolidation.

Our main contributions are summarized as follows:  

% Nan: If you read the three contributions: Development of MEBench, Entity-Aware Evaluation Framework, Efficient Construction -- they are not attractive contributions

\begin{itemize}
\item {\textbf{Development of MEBench}}: A novel multi-document, multi-entity benchmark designed to evaluate LLMs and RAG systems in cross-document aggregation and reasoning. It includes 4,780 validated question-answer pairs spanning three categories and eight types, simulating real-world scenarios that demand integration of fragmented, entity-centric information.   
\item {\textbf{Entity-centric Task Categories and Evaluation}}: Utilization of Entity-Attributed F1 (EA-F1), a granular metric for assessing entity-level correctness and attribution validity, alongside a stratified entity density design (low: 0–10, medium: 11–100, high: >100 entities per query). Our framework emphasizes completeness and factual precision in information extraction, addressing gaps in existing metrics for entity-dense MEQA tasks.
\item {\textbf{Scalable Benchmark Construction.}} A scalable, automated pipeline featuring: Knowledge graph extraction from structured Wikipedia for cross-document relationship discovery; Relational table generation to preserve entity-property relationships; Template-based QA generation ensuring reproducibility and reducing cost and labor.
\end{itemize}

\begin{figure}[t!]
\begin{center}
% \includegraphics[width=0.6\linewidth]{figs/example_1.png}
  % \includegraphics[width=1\linewidth]{MEQA/figs/example_3.pdf}
\includegraphics[width=1\linewidth]{figs/benexa-1.png}
\end{center}
%\vspace{-.5em}
\caption{Existing benchmarks vs. MEBen. Unlike existing benchmarks which feature centralized evidence distributions and sparse entity mentions, MEBen presents entity-dense scene where critical evidences are dispersed across multiple documents, necessitating that when seeking an answer, no document or entity can be ignored.}
\label{fig:benexa}
\end{figure}

% Current retrieval-augmented generation (RAG) systems and LLMs struggle with entity-dense reasoning, particularly when contextual clues are scattered across sources. For instance, models may conflate entities with similar attributes or overlook nuanced relationships between them, leading to incomplete or inaccurate answers. This limitation stems from a lack of systematic evaluation frameworks that prioritize completeness, factual precision, and attribution validity in multi-entity contexts. 
% Prior benchmarks, often constrained to single-document settings or narrow entity categories, inadequately stress-test models’ ability to reason multi-entity question across documents.  

% To address this gap, we introduce \ben, a novel benchmark designed to evaluate LLMs and RAG systems on cross-document \meqa. 我们的问题的答案分布在多个文档中，并且需要整合多个实体的信息，错过一个文档，甚至搞错了一个实体都会导致错误的答案。
% MEbench typically consists of 409 entities per question on average. 此外，Mebench具有不同数量的实体（例如，0-10, 10-100, >100）和不同难度的评估任务，能够跨不同实体数量和任务复杂度对llm进行细粒度评估。
% Comprising 4,780 questions systematically categorized into three primary categories and eight distinct types, MEBench spans diverse real-world scenarios, from academic field distributions to geopolitical event analysis. 
% Notably,we leverages graph structures of wikipedia graph for efficient knowledge discovery without excessive manpower consumption. 我们自动化地收集数据和整理数据，将文档整理成表格，在此基础上生成问题和给出答案，整个构建过程准确高效。

% MEBench introduces the Entity-Attributed F1 (EA-F1) metric, which granularly evaluates both entity-level correctness and the validity of source attribution, addressing a critical oversight in conventional metrics like accuracy or vanilla F1 scores.  


% This work bridges a critical gap in LLM evaluation, paving the way for robust, entity-aware QA systems capable of navigating the complexities of multi-document, multi-entity environments.  


% \begin{itemize}
% \item {\textbf{\qa: Specialized Benchmark for MEQA.}} We curate \qa, a standardized benchmark based on Wikipedia to evaluate the effectiveness of various approaches to address the complexities of information extraction and reasoning involving multiple entities.
% \item {\textbf{Multi-entity Semantic Retrieval}} innovatively enhances semantic retrieval by integrating language models with structured database. This integration enables the creation of precise SPARQL queries to effectively retrieve relevant entities and web pages from very large datasets such as Wikidata. The method's key contribution is its ability to improve retrieval accuracy by leveraging the strengths of both language models and structured data verification.  
% \item {\textbf{\sys: Approach to Structure Entity Information.}} We propose the \sys, a system for managing vast and unstructured data by extracting properties of entities and organizing information into structured tables. This system transforms textual information about entities into a format with a rigorous and accurate schema, which facilitates analysis. Our experiments demonstrate its remarkable performance, achieving SOTA results and outperforming the strongest baselines by 11.6\% in overall accuracy, while leading across all eight subtasks.
% \end{itemize}

