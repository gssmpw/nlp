\section{Experiment} 

\begin{table*}[t!]
  \centering
  \renewcommand\arraystretch{1}
  \caption{
   Experimental results for \ben. 
   % There are significant differences in results between the different methods.
   % Notice that the error results of our model mainly because the wrong SQL generated by LLM. 
  }
  % \vspace{.2em}
  \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{Models}} 
        & \multicolumn{4}{c} {\textbf{Accuracy}}\\
        \cmidrule(lr){2-5}
        & Comparison &Statistics &Relationship &Overall\\
    \midrule
    \multicolumn{5}{c} {\textbf{All sets}}\\
     \hline
    GPT-3.5-turbo &0.105 	&0.198 	&0.476 	&0.239 \\
    GPT-3.5-turbo +  RAG & 0.605 	&0.260 	&0.476 	&0.425 \\
    GPT-4 & 0.199 	&0.289 	&0.507 	&0.316 \\
    GPT-4 + RAG & 0.763 	&0.410 	&0.687 	&0.593 \\
    Llama-3-Instruct & 0.046 	&0.118 	&0.256 	&0.130 \\
    Llama-3-Instruct + RAG & 0.447 	&0.181 	&0.410 	&0.325 \\
    FT Llama-3-Instruct & 0.046 	&0.253 	&0.259 	&0.189 \\
    FT Llama-3-Instruct + RAG & 0.687 	&0.448 	&0.573 	&0.556 \\
    \hline
    \multicolumn{5}{c} {\textbf{Set1 (0-10)}}\\
     \hline
      GPT-3.5-turbo &0.435	&0.583	&0.560	&0.530 \\
    GPT-3.5-turbo + RAG & 0.548 &0.654	&0.620	&0.612\\
    GPT-4 & 0.451	&0.595	&0.540	&0.535 \\
    GPT-4 + RAG & 0.870	&0.619	&0.740	&0.729 \\
    Llama-3-Instruct & 0.322 &0.500	&0.400	&0.418\\
    Llama-3-Instruct + RAG & 0.419 &0.571 &0.480	&0.500 \\
    FT Llama-3-Instruct & 0.322 &0.511 &0.380 &0.418\\
    FT Llama-3-Instruct + RAG & 0.580 &0.677 &0.690 &0.676 \\
    \hline
    \multicolumn{5}{c} {\textbf{Set2 (11-100)}}\\
     \hline
      GPT-3.5-turbo &0.364 	&0.495 	&0.544 	&0.466 \\
    GPT-3.5-turbo + RAG &0.613 	&0.581 	&0.640 	&0.607 \\
    GPT-4 & 0.348 	&0.476 	&0.521 	&0.447 \\
    GPT-4 + RAG & 0.791 	&0.511 	&0.661 	&0.638 \\
    Llama-3-Instruct & 0.240 	&0.385 & 0.357 	&0.332  \\
    Llama-3-Instruct + RAG & 0.428 	&0.454 	&0.459 	&0.447  \\
    FT Llama-3-Instruct & 0.240 	&0.434 	&0.344 	&0.349  \\
    FT Llama-3-Instruct + RAG & 0.612 	&0.608 	&0.655 	&0.640  \\
     \hline
    \multicolumn{5}{c} {\textbf{Set3 (>100)}}\\
    \hline
    GPT-3.5-turbo &0.09 &0.158 &0.291 &0.173\\
    GPT-3.5-turbo + RAG & 0.389 &0.191 &0.311 &0.285\\
    GPT-4 & 0.142 &0.202 &0.309 &0.210\\
    GPT-4 + RAG & 0.436 &0.270 &0.405 &0.357\\
    Llama-3-Instruct &0.055 &0.108 &0.168 &0.106\\
    Llama-3-Instruct + RAG & 0.265 &0.147 &0.253 &0.212\\
    FT Llama-3-Instruct & 0.055 &0.177 &0.167 &0.136\\
    FT Llama-3-Instruct + RAG & 0.401 &0.291 &0.355 &0.345\\
    \bottomrule
  \end{tabular}
  \label{tab:Accuracy}
\end{table*}

\begin{table}[t!]
\caption{\label{tab:f1}
	Quality of Large Language Models (LLMs) in EA-F1. 
	}
	\centering
	\begin{tabular}{lc}
		\hline
		\multirow{2}*{\textbf{Models}} & \multirow{2}*{\textbf{$EA-F1$}}  \\
        &\\
		\hline
		GPT-3.5-turbo & 0.25  \\
		GPT-3.5-turbo + RAG & 0.43\\
		GPT-4 & 0.36 \\
		GPT-4 + RAG & 0.71 \\
		Llama-3-Instruct & 0.21 \\
		Llama-3-Instruct + RAG & 0.39 \\
            FT Llama-3-Instruct & 0.21 \\
		FT Llama-3-Instruct + RAG & 0.59 \\
		\hline
	\end{tabular}
\end{table}

\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1\linewidth]{figs/radar.pdf}
\end{center}
%  \vspace{-.8em}
  \caption{The Experimental results for eight subtasks of each model.}
  \label{fig:acc}
\end{figure*}

\subsection{Experiment Setup}
% \paragraph{\qa Benchmark.}
% It is a specialized benchmark designed to evaluate systems addressing multi-entity QA. The benchmark comprises 4,780 methodically structured questions partitioned into two subsets: a training set (3,406 questions) for model fine-tuning and a test set (1,374 questions) for rigorous evaluation. These questions are systematically categorized into three primary categories, further divided into eight distinct types (see Table ~\ref{multiqexamples}), ensuring broad coverage of real-world multi-entity reasoning scenarios. Table~\ref{querycategories} details comprehensive statistics of the benchmark. 
\subsubsection{Models}
For open-source LLMs, we conduct experiments using the representative Meta-Llama-3-8B-Instruct~\citep{Llama3_2024} and  apply QLoRA~\citep{dettmers2023qlora} to fine-tune it with the training set of \ben. For proprietary LLMs, we select the widely recognized GPT models, including GPT-3.5-turbo~\citep{ouyang2022training} and GPT-4~\citep{achiam2023gpt}. Additionally, we incorporate RAG across all vanilla baseline models for comparative analysis and evaluation of the model's capacity to integrate and leverage external data sources.

\subsubsection{Structured RAG}
We have also incorporated a structured RAG module into the LLMs to explore whether RAG can enhance the modelâ€™s performance on \ben. For the Embedding choice, we employ the OpenAI Embedding model~\citep{TextEmbeddingAda002}, and the chunk size is 1024. For each document, we retrieve the top-5 most related chunks and concatenate them in their original order to form the context input for the model.
 
\subsubsection{Evaluation Metrics}
We adopt Accuracy ($Acc$) as the primary metric to assess the performance of LLMs on \ben tasks. For the subcategories of Variance Analysis, Correlation Analysis, and Distribution Compliance within the Statistics tasks shown in Table~\ref{tab:examples}, we focus solely on prompting LLMs to identify relevant columns and applicable methods, evaluating the accuracy of their selections instead of the computational results, as LLMs' abilities in precise calculations are not the central focus of this study. In addition, we evaluate performance of information extraction using Entity-Attributed F1 (EA-F1). This is an F1 score applied to the predicted vs. gold sets of the \colorbox{black!5}{(entity, atrribution, value)}. All three elements in the tuple must exactly match the tuple in the ground truth to be marked correct.  

\subsection{Results and Analysis}

Various models exhibit notable variations in performance on \ben. 
Table~\ref{tab:Accuracy} presents experimental results alongside overall accuracy on \ben, and Figure~\ref{fig:acc} shows accuracy on eight further-divided tasks.

\subsubsection{Main result}
% We assess seven advanced LLMs on the \ben benchmark. The main results are shown in Table~\ref{tab:Accuracy}. GPT-4 + RAG shows the best overall performance, reaching 59.3\%. It gets the accuracy in the relational and comparative query of 68.7\% and 76.3\%, respectively, while achieving 41\% for statistical query.
% In our evaluation, we focused on analyzing the capability of LLMs to extract related data. This assessment aimed to understand how well these sophisticated models can organize and present data for the question. The result is shown in Table~\ref{tab:f1}.
GPT-4 + RAG achieved superior accuracy (59.3\%), outperforming the second-ranked model (FT Llama-3-Instruct: 55.6\% ) by a statistically significant margin. Notably, GPT-4 + RAG excelled in relational (68.7\%) and comparative (76.3\%) queries, likely due to its superior contextual understanding. However, all models exhibited markedly lower accuracy in statistical queries (GPT-4 + RAG: 41.0\%), suggesting inherent challenges in numerical reasoning. In our evaluation, we focused on analyzing the capability of LLMs to extract related data. This assessment aimed to understand how well these sophisticated models can organize and present data for the question. The result is shown in Table~\ref{tab:f1}. These results underscore the critical role of information extraction architectures in mitigating hallucinations and grounding outputs in factual data. 
Introducing RAG significantly improves overall performance, particularly in comparison tasks, while fine-tuning LLaMA-3-Instruct alone does not yield substantial gains without RAG. On \meqa, open-source models like LLaMA-3-Instruct, even with RAG, can't match proprietary models like GPT-4, which achieves a 59.3\% accuracy compared to LLaMA-3-Instruct's 31.6\%.

\subsubsection{Fine-grained Performance on Sub-tasks.}

Figure~\ref{fig:acc} shows that vanilla LLMs perform well in correlation analysis and descriptive relationship sub-tasks, while RAG significantly improves intercomparison and superlative tasks. However, neither fine-tuning nor RAG overcomes challenges in variance analysis and aggregation tasks, while GPT-4 + RAG achieves accuracy of 15.3\% and 32.1\%.

\subsubsection{Entity density Analysis.} 

As we can see from Table~\ref{tab:Accuracy}, our experiments underscore the impact of entity density on model performance in MEQA tasks. This phenomenon arises because higher entity densities amplify two critical challenges inherent to MEQA systems: (1) Semantic ambiguity due to overlapping relational predicates among entities (e.g., distinguishing "Paris [person]" vs. "Paris [location]" within narrow contexts), and (2) computational overhead in attention-based architectures attempting parallel reasoning over entangled entity-attribution  pairs (e.g. transformer self-attention weights saturate under dense cross-entity dependencies). 

\begin{itemize}
 \item {Low Entity Density}: Models generally performed well in low-density scenarios. The simplicity of context allowed for accurate entity recognition and minimal ambiguity in resolving references.
 \item {Medium Entity Density}: Performance began to decrise among models in medium-density scenarios by 6\% average acc. This variance suggests differences in how models handle increased entity complexity and overlapping contexts.
 \item {High Entity Density}: High-density questions posed significant challenges, with an average acc drop to 22.8\% across models. The result highlighting limitations in current architectures' ability to handle complex multi-entity questions. 
\end{itemize} 


% \begin{itemize}
%     \item {\textbf{Relation semantic parsing.}} 
%     The semantic parsing model in SPARQL retrieval effectively recognizes entities but struggles with relationship identification, leading to challenges in graph retrieval and negatively affecting the performance of RAG-based approaches, including SQA. For example, in the query ``How many US presidents have served more than one term in office?'' The model incorrectly identifies the relationship as ``instance of'' rather than the correct ``position held'', leading to erroneous results.

%     \item {\textbf{Insufficient information extraction.}} We also identified errors in \ben's information extraction during the table-filling phase. An analysis of more than 2,000 table filling instances reveals that these errors occur primarily as omissions (albeit with a low probability of approximately 0.1\%). A new challenge is the appearance of multi-word synonyms within the same column, such as ``US'' and ``America'', which negatively affects the accuracy of SQL execution such as ``SELECT''.
% \end{itemize} 

% \begin{itemize}
% \item {\textbf{Relation semantic parsing.}} 
% The \SDRS Multi-entity Semantic Retrieval module can effectively recognizes entities but struggles with relationship identification, leading to challenges in graph retrieval and negatively affecting the performance of RAG-based approaches, including SQA. For example, in the query ``How many US presidents have served more than one term in office?'' The model incorrectly identifies the relationship as ``instance of'' rather than the correct ``position held'', leading to erroneous results.

% \item {\textbf{Insufficient information extraction.}} We also identified errors in \sys's information extraction during the table-filling phase. An analysis of more than 2,000 table filling instances reveals that these errors occur primarily as omissions (albeit with a low probability of approximately 0.1\%). A new challenge is the appearance of multi-word synonyms within the same column, such as ``US'' and ``America'', which negatively affects the accuracy of SQL execution such as ``SELECT''.
% \end{itemize}


