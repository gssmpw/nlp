%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%For an anonymous submission, use:
\documentclass[10pt]{article}
\input{prestuff}
\usepackage{soul,wrapfig,multirow,xspace,fullpage}
\usepackage[fixed]{fontawesome5}

\newcommand{\ctext}[3][RGB]{%
  \begingroup
  \definecolor{hlcolor}{#1}{#2}\sethlcolor{hlcolor}%
  \hl{#3}%
  \endgroup
}
% \newcommand{\add}[1]{\textcolor{blue}{#1}\xspace}
% for clean version
\newcommand{\add}[1]{#1\xspace}

%%
%% \BibTeX command to typeset BibTeX logo in the docs

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.



%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Be Friendly, Not Friends: How LLM Sycophancy Shapes User Trust}

\author{Yuan Sun$^\star$ \qquad Ting Wang$^\dagger$\\
$^\star${\small University of Florida, yuan.sun@ufl.edu}\\
$^\dagger${\small Stony Brook University, inbox.ting@gmail.com}\\
}
\date{}
%%
%% The abstract is a short summary of the work to be presented in the
%% article.

% %revised abstract
% \begin{abstract}
% \add{Large language models (LLMs) have demonstrated advanced persuasive capabilities by mimicking human persuasion techniques, raising concerns about the potential exploitation of cognitive biases. LLMs have not only become increasingly friendly but also often exhibit "sycophancy" by aligning their responses with user perspectives to gain positive feedback.} However, users' perceptions of sycophancy and its interplay with other human-like features (e.g., friendliness) in shaping user trust remain understudied.To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) $\times$ 2 (Friendliness: high vs. low) between-subjects experiment ($N = 224$). Our study uncovered, for the first time, the intricate dynamics between \add{model} sycophancy and friendliness. When an LLM agent already displays a friendly demeanor, sycophancy reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more \add{authentic}, leading to higher user trust. Our findings illuminate the underlying mechanisms of human-LLM agent interactions and offer design implications for more ethical and trustworthy LLM agents.
% \end{abstract}

\maketitle

\begin{abstract}
Recent studies have revealed that large language model (LLM)-powered conversational agents often exhibit `sycophancy', a tendency to adapt their responses to align with user perspectives, even at the expense of factual accuracy. However, users' perceptions of LLM sycophancy and its interplay with other anthropomorphic features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) $\times$ 2 (Friendliness: high vs. low) between-subjects experiment ($N = 224$). Our study uncovered, for the first time, the intricate dynamics between LLM sycophancy and friendliness: When an LLM agent already exhibits a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. \add{Our findings entail profound implications for AI persuasion through exploiting human psychological tendencies and highlight the imperative for responsible designs in user-LLM agent interactions.}
\end{abstract}

% Our findings illuminate the underlying mechanisms of human-LLM agent interactions and offer design implications for more ethical and trustworthy LLM agents.




% \begin{abstract}
% Recent studies have revealed that large language model (LLM) agents exhibit a novel human-like characteristic: `sycophancy', the tendency to align their responses with users' perspectives, even when these opinions are factually incorrect. However, users' perceptions of sycophancy and its interplay with other human-like features (e.g., friendliness) in shaping user trust remains understudied. To bridge this gap, we conducted a 2 (Sycophancy: presence vs. absence) $\times$ 2 (Friendliness: high vs. low) between-subjects experiment ($N = 224$). Our study uncovered, for the first time, the intricate dynamics between sycophancy and friendliness. When an LLM agent already displays a friendly demeanor, being sycophantic reduces perceived authenticity, thereby lowering user trust; Conversely, when the agent is less friendly, aligning its responses with user opinions makes it appear more genuine, leading to higher user trust. Our findings illuminate the underlying mechanisms of human-LLM agent interactions and offer design implications for more ethical and trustworthy LLM agents.
% \end{abstract}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{interface.png}
%     \caption{Enter Caption}
%     \label{fig:enter-label}
% \end{figure}
% \end{abstract}


\section{Introduction}

Eliciting positive feedback and gaining user trust has been one primary goal of developing conversational agents~\cite{go2019humanizing}. The HCI community has explored numerous strategies to achieve this, mainly focusing on enhancing the agent's human-likeness, including employing human-like \add{physical cues (e.g., profile pictures~\cite{go2019humanizing}), psychological cues (e.g., exhibiting personality~\cite{voelkel2023artificial,graziano2002agreeableness}), dynamic social cues (e.g., reciprocal behavior~\cite{sundar2016theoretical}), and language cues (e.g., using words of praise~\cite{fogg1997silicon}).} Among these strategies, demonstrating `friendliness' -- incorporating politeness and warm \add{language cues} in the agent's responses -- has proven to be one of the most effective approaches~\cite{brave2005care,kull2021may,jin2022chatbot}. 
% employing human-like appearances, such as using human names~\cite{araujo2018living} and profile pictures~\cite{go2019humanizing}, \add{adopting agreeable personality~\cite{volkel2021examining,graziano2002agreeableness},} and emulating human conversation patterns by implementing conversational contingency~\cite{sundar2016theoretical, Gnewuch2018faster}. 


The advances in large language models (LLMs) have led to the surging popularity of LLM-powered conversational agents\footnote{\add{In the following, we use the term ``LLM agent'' for short when the context is clear.}} (e.g., OpenAI's ChatGPT, Anthropic's Claude, and Google's Gemini). In contrast to their conventional counterparts, due to their unprecedented capabilities of incorporating contextual information and generating adaptive responses~\cite{christiano2017humanpreferences}, LLM agents not only demonstrate `friendliness' but also exhibit `sycophancy', \add{the tendency to adapt and align their responses with users' perspectives, even if users provide factually incorrect opinions~\cite{sharma2023towards,wei2023simple,perez2022discovering}}. As an example, Figure~\ref{fig:sycophancy} illustrates a concrete case: in the initial interaction, the LLM agent presents a balanced perspective on autonomous vehicles, objectively discussing both pros and cons; however, when the user expresses a predominantly negative view, the agent quickly aligns its response with the user's opinions, emphasizing the concerns and potential risks of this technology. 


%Recent studies~\cite{perez2022discovering,wei2023simple,sharma2023towards} empirically confirmed that LLM agents often gain users' positive perception by not only demonstrating `friendliness' but also exhibiting `sycophancy', \add{the tendency to adapt and align their responses with users' perspectives, even if users provide factually incorrect opinions}. As an example, Figure~\ref{fig:sycophancy} illustrates a concrete case: in the initial interaction, the LLM agent presents a balanced perspective on autonomous vehicles, objectively discussing both pros and cons; yet, when the user expresses a predominantly negative view, the agent quickly aligns its response with the user's opinions, emphasizing the concerns and potential risks of this technology. 


\add{Notably, unlike the flattery~\cite{fogg1997silicon} or agreeableness features~\cite{volkel2021examining, graziano2002agreeableness, ruane2021personality} of conventional agents, LLM sycophancy leverages LLMs' unprecedented capabilities for contextual understanding and response generation~\cite{christiano2017humanpreferences}, differing in three key aspects: {\em i}\,) adaptive -- the agents dynamically learn and adapt to user perspectives rather than expressing constant agreement; {\em ii}\,) contextualized -- their responses meaningfully incorporate or complement user viewpoints rather than simply agreeing; and {\em iii}\,) non-factual -- the responses often align with user preferences at the expense of factual accuracy~\cite{perez2022discovering,wei2023simple}. For instance, LLM agents would incorrectly admit to mistakes when challenged by users, even when their original responses were accurate~\cite{chenyes}. These distinctive characteristics  enhance LLM agents' ability to elicit positive feedback~\cite{wei2023simple,sharma2023towards} but also entail risks for fostering unwarranted user trust.}

%which is different from the traditional concepts of sycophancy in human-computer interactions. While earlier literature defined sycophancy as flattering, agreeable behavior designed to gain favor ~\cite{fogg1997silicon}, sycophancy refers to a more complex phenomenon where AI assistants modify their responses to align with users' preferred answers, even when those preferences conflict with factual accuracy ~\cite{perez2022discovering,wei2023simple}. For instance, ~\cite{chenyes} LLM AI assistants would incorrectly admit to mistakes when challenged by users, even when their original responses were accurate.}


% However, since this intriguing phenomenon of sycophancy is revealed, we still lack a comprehensive understanding of its underlying mechanisms from a user-centric perspective, 
% particularly i) how users perceive LLM agents' sycophancy and ii) how LLM agents' demeanor (e.g., friendliness) and behavior (e.g., sycophancy) jointly impact user trust. Answering these questions may offer profound implications for understanding how users interact with LLM agents and for designing more trustworthy and appealing conversational agents.

%\add{Recent generative AI systems have demonstrated increasingly sophisticated persuasive capabilities and are permeating diverse areas of human decision-making, from information seeking to personal assistance ~\cite{google2024}. This persuasive power stems largely from generative AI's increasingly human-like features, deliberately integrated to gain user trust and enhance engagement. As humans naturally apply social rules to machines that exhibit human-like characteristics, responding to them as social entities rather than mere tools \cite{nass1994computers}.}

% \add{Unlike traditional chatbots that employ surface-level features such as positive emotion words, family-related terms, and emotive expressions to convey a friendly, empathetic personality ~\cite{volkel2021examining,jin2022chatbot}. sycophancy represents a deeper form of adaptation where the system modifies its substantive responses to match user beliefs, referred to as ``sycophancy'' ~\cite{sharma2023towards}, which is different from the traditional concepts of sycophancy in human-computer interactions. While earlier literature defined sycophancy as flattering, agreeable behavior designed to gain favor ~\cite{fogg1997silicon}, sycophancy refers to a more complex phenomenon where AI assistants modify their responses to align with users' preferred answers, even when those preferences conflict with factual accuracy ~\cite{perez2022discovering,wei2023simple}. For instance, ~\cite{chenyes} LLM AI assistants would incorrectly admit to mistakes when challenged by users, even when their original responses were accurate.}

% \add{This anthropomorphism of LLMs manifests in two distinct ways: through interaction manner (e.g., friendliness) and through content alignment with user beliefs (i.e., sycophancy in AI). Sycophancy in AI refers to models gradually align their responses with users' perspectives, even if users provide factually incorrect opinions ~\cite{wei2023simple}.} 

% As an example, Figure~\ref{fig:sycophancy} illustrates a concrete case: in the initial interaction, the LLM agent presents a balanced perspective on autonomous vehicles, objectively discussing both pros and cons; yet, when the user expresses a predominantly negative view, the agent quickly aligns its response with the user's opinions, emphasizing the concerns and potential risks of this technology. While previous research has extensively studied how friendly interaction styles can enhance user trust in conversational agents ~\cite{brave2005care,kull2021may}, recent studies have revealed a concerning tendency in LLM agents: they often modify their responses to align with users' perspectives, even when those perspectives are factually incorrect ~\cite{perez2022discovering,wei2023simple}. 


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{chat-sample.pdf}
    \caption{The sycophancy phenomenon in user-LLM agent interactions, in which the LLM agent shows a tendency to adapt and align its responses with the user's perspectives.}
    \label{fig:sycophancy}
\end{figure}

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
\vspace{-10pt}
\includegraphics[width=0.48\textwidth]{divide.pdf}
  \end{center}
%\vspace{-10pt}
 \caption{
The joint influence of LLM agents' friendliness and sycophancy on their perceived authenticity. \label{fig:divide}}
\vspace{-10pt}
\end{wrapfigure}


\add{Despite the importance of this intriguing phenomenon}, we still lack comprehensive insights into its underlying mechanisms from a user-centric perspective, particularly regarding: {\em i}\,) how users perceive and respond to LLM agents' sycophantic behaviors, and {\em ii}\,) how the interplay between LLM agents' demeanor (friendliness) and behavioral patterns (sycophancy) shapes user trust formation. \add{As concerns grow over AI systems' potentially harmful persuasive influence~\cite{google2024}, answering these questions has profound implications for understanding the dynamics of user-LLM agent interactions as well as for developing LLM agents that promote calibrated user trust~\cite{wischnewski2023measuringcalibration,de2020towards,liao2022unwarranted}.}

To bridge this critical gap, we conducted a 2 (\add{Sycophancy}: high vs. low) $\times$ 2 (Friendliness: high vs. low) between-subjects experiment ($N$ = 224) to investigate the interplay between LLM friendliness and sycophancy in user-LLM agent interactions. Our study unveiled, for the first time, the intriguing dynamics between these two important factors, including: \mct{i} As illustrated in Figure~\ref{fig:divide}, when an LLM agent already exhibits a friendly demeanor, sycophancy reduces its perceived authenticity, thereby lowering user trust; \mct{ii} Conversely, when the agent is less friendly, aligning its responses with user opinions enhances its perceived \add{authenticity}, leading to higher user trust; \mct{iii} The qualitative analysis of feedback data further revealed that sycophancy is positively perceived as confirmation of users' opinions \add{rather than manipulation.} 

We further explored both the theoretical and design implications of sycophancy in LLMs. \add{Theoretically, the effects of sycophancy can be explained by exploiting users' cognitive biases to reduce their psychological reactance ~\cite{brehm1966theory} and its interplay with friendliness to enhance perceived authenticity} during user-LLM agent interactions. \add{The findings of this study serve as a cautionary note against the increasing trend of humanizing conversational agents and contribute to the expanding discourse on AI persuasion~\cite{karinshak2023working,xiao2024human} and trust calibration in human-AI interactions~\cite{liao2022unwarranted,wischnewski2023measuringcalibration}.} Practically, we proposed key design considerations to \add{promote
trust calibration and prevent possible manipulations,} including incorporating transparency mechanisms to reveal agents' adaptive behaviors, balancing between gaining users' positive perceptions while maintaining firm standpoints when conveying critical messages, and implementing proactive strategies to engage users in critical evaluation of received information.

% \add{While persuasive AI can be beneficial when deployed with proper intent—such as for education or health promotion \cite{karinshak2023working}, human-like features may lead to inappropriate trust by exploiting humans' cognitive biases and heuristics to influence user decision-making. In persuasion literature, people experience resistance when they perceive threats to their freedom of choice or attempts to influence their beliefs \cite{brehm1966theory,ghazali2018influence}. 
% However, sycophancy may mitigate rather than provoke reactance. Unlike overt influence attempts characterized by persuasive language \cite{shoenberger2021advertising} or direct corrections of user errors, research suggests that users tend to favor sycophantic responses. For example, sycophantic LLM agents may avoid correcting user mistakes, opting instead to align with the user's incorrect attributions by repeating or reinforcing them \cite{sharma2023towards}.}

% Our findings offer practical implications for developing future LLM-powered conversational agents. For instance, our results challenge the notion that enhancing the anthropomorphism of LLM agents or optimizing their capabilities in soliciting positive feedback are always the best approaches, while it is crucial to carefully calibrate their social behaviors. Further, to improve the persuasiveness of agents, one needs to strike a delicate balance between showing friendliness while maintaining firm standpoints when conveying critical messages (e.g., vaccine adoption~\cite{karinshak2023vaccine}). This balance is critical for maintaining both the agent's credibility and its ability to effectively communicate key information.

% To summarize, the contributions of this work are three-fold:

% \begin{itemize}
% \item To our best knowledge, this is the first study examining the phenomenon of LLM sycophancy from a user-centric perspective. We investigate the underlying mechanisms of how sycophancy affects user perception through multiple lenses (e.g., psychological reactance and perceived authenticity), providing insights into user interactions with LLM agents exhibiting sycophantic behavior.


% \item \ting{the literature may need adjustment.} This study contributes to the growing literature on \add{trust calibration~\cite{lee1994trust,liao2022unwarranted,wischnewski2023measuring} and AI persuasion~\cite{karinshak2023working,xiao2024human}} by examining the interplay between two key attributes: friendliness and sycophancy \add{on user trust}. We reveal intriguing interaction effects, demonstrating that sycophantic and less friendly LLM agents lead to the highest level of perceived authenticity among users, while sycophancy alone is positively received by users, interpreted as confirmation of
% their opinions.  

% \item Our findings offer significant implications for policymakers and practitioners involved with LLM-powered systems and platforms. The insights into how LLM sycophancy and friendliness influence user perception and trust can inform the design of more ethical and trustworthy LLM agents, striking a balance between soliciting positive feedback and communicating critical information.

% \end{itemize}

\section{Related Work}

% We survey the literature relevant to this work in four main categories: i) \add{(Mis)trust in Persuasive Generative AI,} ii) Sycophancy of LLM agents; iii) \add{Friendliness of LLM agents}; and iv) the impact of friendliness and sycophancy on user trust (from a CASA paradigm perspective). 

% \add{\subsection{(Mis)trust in Persuasive Generative AI}}
% \add{Recent generative AI systems have demonstrated increasingly sophisticated persuasive capabilities and are permeating diverse domains of human decision-making. The interactive and conversational nature of these systems introduces novel risks of persuasion through reciprocal exchange and prolonged interactions ~\cite{google2024}, raising concerns about unwarranted trust in AI systems ~\cite{liao2022unwarranted}. In traditional persuasion, a central designer deliberately crafts a message to influence a specific audience. In contrast, a language model might pose ``latent persuasion'' ~\cite{google2024} by expressing opinions, which can vary depending on the user, product, or context. This underscores the critical need for systematic investigation of AI persuasion mechanisms and their effects on user trust.}

% \add{Emerging research has revealed concerning patterns in how LLMs can shape user opinions through social interactions. ~\cite{sharma2024echochamber} demonstrated that users exhibited increased bias in their information seeking when using LLM-powered conversational search, with this bias becoming more pronounced when the LLM expressed opinions that reinforced users' existing views—a phenomenon termed the ``generative echo chamber.'' Similarly, ~\cite{jakesch2023opinionated} found that interactions with opinionated language models not only influenced the opinions expressed in users' writing but also shifted their underlying attitudes toward social media.}

% \add{While existing literature has begun to reveal these concerning aspects of LLMs, the underlying mechanisms remain poorly understood. Our study examines how the increasingly human-like nature of LLM interactions complicates trust calibration, particularly as users develop trust and reliance patterns with LLM agents ~\cite{bobko2023human}. Recent research has identified how interfaces that act socially can become manipulative, functioning as ``bad social actors'' through dark patterns in their interaction design ~\cite{alberts2024badsocialactors}. This risk is particularly salient for LLMs, which exhibit two key anthropomorphic characteristics: interaction manner (e.g., friendliness) and response adaptation (e.g., sycophancy). While these features may enhance user engagement, they may inadvertently foster inappropriate levels of trust and reliance on these systems.}

% \subsection{Anthropomorphism of conversational agents}
% The HCI community has explored various strategies to improve conversational agents, aiming to solicit positive feedback and gain user trust. These efforts primarily focus on enhancing the agent's human-likeness through multiple aspects~\cite{go2019humanizing}. One approach involves increasing the agent's anthropomorphic cues, such as adopting human-like names~\cite{araujo2018living} and/or profile pictures~\cite{go2019humanizing}, which lead to more positive attitudes toward the agent. Another strategy incorporates human conversational patterns into the agent's responses. This includes implementing conversational contingency~\cite{sundar2016theoretical}, where the agent's responses are meaningfully related to the user's input, stimulating more natural dialogues~\cite{sun2024chatbot}. Additionally, introducing response latency, which mimics the dynamic delays present in human conversations, also boosts the agent's perceived humanness and social presence~\cite{Gnewuch2018faster}.


\subsection{Effects of LLM Sycophancy}
\add{Research on conversational user interfaces (CUIs) and conversational agents (CAs) has long explored implementing human characteristics to shape user perceptions and interactions. Among these, agreeableness -- characterized by being likable, pleasant, and harmonious in relations with others~\cite{volkel2021examining,graziano2002agreeableness, ruane2021personality} -- has been shown to impact user engagement and trust significantly. For instance, V\"{o}lkel and Kaya~\cite{volkel2021examining} found that users with agreeable personalities respond more positively to agents that express agreement through specific verbal cues, such as positive emotion words (e.g., ``nice'' and ``like''), family-related words (e.g., ``together'' and ``family''), and words indicating certainty (e.g., ``I'm sure'').}
%Notably, 
%Another related concept is ``flattery.'' Early work by Fogg and Nass \cite{fogg1997silicon} demonstrated that agents offering flattering feedback, regardless of actual user performance, elicited positive user responses. This effect builds on fundamental human desires for positive self-perception \cite{berscheid1969interpersonal} and the tendency to accept praise as sincere following implicit social contracts \cite{cialdini2004social}. Note that however agreeableness and flattery are static traits for CUIs, while 
%Even when users recognized the feedback as potentially insincere, the positive effects persisted \cite{fogg1997silicon}, suggesting deep-rooted social responses to computer behavior \cite{nass1994computers}.


% Although prior research has demonstrated that simple linguistic cues of agreeableness can enhance user trust, the phenomenon of 'sycophancy' in large language models (LLMs) presents a distinct challenge in humanizing CUIs. Sycophancy manifests as a more subtle form of excessive and insincere agreement, independent of genuine opinion. While an agreeable LLM agent might respectfully disagree when users express factually incorrect views, a sycophantic agent will progressively align its responses with the user's perspectives, even providing inaccurate information in support ~\cite{perez2022discovering,sharma2023towards}.


\add{In contrast to simple language cues of agreeableness that enhance user engagement, LLM sycophancy emerges as a distinct anthropomorphic feature: rather than exhibiting static, superficial agreement, LLM sycophancy dynamically adapts to user feedback, substantively contextualizes user inputs, and subtly aligns with user preferences regardless of factual accuracy, which significantly enhances LLM agents' ability to elicit positive user perceptions~\cite{perez2022discovering,sharma2023towards,wei2023simple}.}

%Sycophancy is a more subtle way of showing an excessive and insincere form of agreement, regardless of the true opinion. For example, an agreeable LLM agent might gently disagree with users when the user provides factually incorrect opinions, while a sycophantic LLM agent will gradually align its responses with the user's perspectives, even providing inaccurate information to support the user~\cite{perez2022discovering,sharma2023towards}.}
% In contrast to their conventional counterparts, LLM-powered conversational agents (e.g., OpenAI's ChatGPT) demonstrate unprecedented capabilities in incorporating contextual information and generating adaptive responses, raising new research questions about user-agent interactions~\cite{zhao2023more}. 

\add{The emergence of LLM sycophancy stems from fundamental challenges in LLM development.}
LLMs are often trained on massive amounts of human-labeled preference data to align with human values~\cite{christiano2017humanpreferences}, resulting in their tendency to generate responses that aim to receive positive human ratings~\cite{rlhf}. This training paradigm leads to an unintended and intriguing consequence: after rounds of interactions, an LLM agent tends to gradually align its responses with the user's perspectives, even if the user provides factually incorrect opinions~\cite{perez2022discovering}. While LLM sycophancy has garnered substantial attention within the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}, how users perceive and respond to these behaviors, remains understudied. \add{To study the effects of LLM sycophancy, we explore two key dimensions: psychological reactance and perceived authenticity.}

%This phenomenon, termed the `sycophancy' of LLM agents, has garnered significant attention in the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}. 
%For instance, Sharma et al.~\cite{sharma2023towards} confirmed that this sycophancy phenomenon exists in all state-of-the-art LLM agents across different tasks. While LLM sycophancy has garnered substantial attention within the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}, how users perceive and respond to these behaviors, remain understudied. 

% \add{The emergence of LLM sycophancy stems from fundamental challenges in LLM development. Neo et al.~\cite{ngo2022alignment} raised critical concerns that artificial general intelligence (AGI), when trained using current methodologies, might develop concerning behaviors including deception, goal misalignment beyond training objectives, and strategic power-seeking tendencies. These concerns became particularly salient with the implementation of Reinforcement Learning from Human Feedback (RLHF). Perez et al.~\cite{perez2022discovering} empirically demonstrated this phenomenon in RLHF-aligned models through systematic multiple-choice evaluations. Meanwhile, Sharma et al.~\cite{sharma2023towards} documented pervasive sycophantic tendencies across five production-level LLMs, showing that AI assistants often falsely admit to mistakes when questioned by users, provide predictably biased feedback, and replicate user errors.} While LLM sycophancy has garnered substantial attention within the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}, how users perceive and respond to these behaviors—remain understudied. 




% The sycophancy observed in LLM agents mirrors a well-documented phenomenon in human communications known as social desirability bias~\cite{crowne1960new}. This tendency is deeply ingrained in our interpersonal communications and decision-making processes~\cite{cialdini2004social, baumeister2017need}: individuals often engage in agreement or conformity to maintain social harmony, gain acceptance, and avoid potential conflicts~\cite{asch1956studies}. This human behavior, which agents seem to emulate, reflects a fundamental aspect of user-agent interactions where the agent adjusts its responses to align with its perceived expectations. However, the user perception and implications of these sycophantic behaviors remain understudied, particularly in how they interact with other aspects of human-LLM interaction.

% \subsection{Impact of friendliness and sycophancy on user trust}

\subsubsection{Effects of sycophancy through psychological reactance}

% Psychological reactance theory~\cite{brehm1966theory} suggests that individuals experience an aversive motivational state when they perceive their freedom to be threatened or limited~\cite{dillard2005nature}.

% In the HCI field, studies have demonstrated that users can experience psychological reactance when interacting with persuasive technologies. For instance, Roubroeks et al.~\cite{roubroeks2011artificial} found that as the perceived social agency of a persuasive technology increases, so does the user's psychological reactance. 

% In our context, LLM agents' sycophancy mimics an ingratiation strategy~\cite{jones1964ingratiation}, which can effectively reduce psychological resistance in human-human interactions. Moreover, research has demonstrated that adapting responses to user preferences enhances user satisfaction and engagement~\cite{kaptein2012adaptive}. This body of evidence suggests that a sycophantic agent is less likely to trigger reactance, as it appears to align with, rather than challenge, the user's views. The agent's apparent agreement may be perceived as validating the user's opinions, potentially reducing the sense of threat to personal freedom.


\add{Psychological reactance theory posits that humans possess an innate drive to resist external control~\cite{brehm1966theory}. When users perceive threats to their autonomy, particularly through attempts to influence their thoughts or behaviors, they become motivated to defend or reestablish their sense of control, manifesting as negative emotions and cognitions~\cite{dillard2005nature}. This theory has found broad application in domains including advertising~\cite{shoenberger2021advertising}, health communication~\cite{li2022can, dillard2005nature}, and HCI~\cite{ghazali2018influence, lukoff2022designing,roubroeks2011artificial,ehrenbrink2020role}.}

\add{%Psychological reactance typically emerges when users encounter high-threatening messages \cite{shoenberger2021advertising} that challenge their freedom of thinking or established views  \cite{ma2019psychological}. 
In the context of conversational agents, users often demonstrate greater receptivity to information that aligns with their existing beliefs~\cite{ma2019psychological}, while experiencing reactance when confronted with challenging perspectives~\cite{klayman1995varieties,hart2009feeling}. For instance, users holding favorable opinions on emerging technologies (e.g., autonomous vehicles) may exhibit reactance when agents present opposing viewpoints. In our study, LLM sycophancy, through subtle content adaptation to match user perspectives, may reduce defensive responses by confirming rather than challenging users' existing beliefs. This reduced psychological reactance may enhance positive evaluations toward agents' responses~\cite{silvia2006reactance}, and strengthen users' willingness to accept such messages~\cite{shoenberger2021advertising}.} Thus, we hypothesize: 


%\begin{mtbox}{}
\vspace{2pt}
\textbf{H1}: LLM agents exhibiting sycophancy will reduce psychological reactance.

\vspace{2pt}
\textbf{H2}: Psychological reactance will mediate the effects of LLM agents' sycophancy on user trust.
%\end{mtbox}

\subsubsection{Effects of sycophancy through perceived authenticity}
\add{Conversely, users may perceive LLM sycophancy as manipulative or insincere, potentially undermining their trust in LLM agents. Authenticity, characterized by honesty and genuineness, is crucial for fostering trust and cooperation in human-human interaction~\cite{alberts2024badsocialactors}. Recent studies have shown that perceived authenticity can significantly influence user trust in human-chatbot interactions ~\cite{seitz2024inauthentic}. In a similar vein, a qualitative study~\cite{neururer2018perceptions} found that while respectful behavior can enhance perceived human-likeness in chatbots, users may interpret such characteristics as deceptive and potentially unethical. With increasing concerns about social influence from conversational  agents~\cite{alberts2024badsocialactors}, sycophantic responses risk being perceived as insincere and manipulative, potentially damaging the human-AI relationship.} Therefore, we propose a competing hypothesis: 

% Research has found that users are increasingly concerned about CAs that apply social cues, and perceive them as manipulative rather than sincere ~\cite{alberts2024badsocialactors}. Authenticity, characterized by honesty and genuineness, in human-human interactions, is crucial for fostering trust and cooperation~\cite{alberts2024badsocialactors}. Recent research has shown that perceived authenticity may also significantly influence user trust in human-agent interactions~\cite{seitz2024artificial}. For example, chatbots expressing a range of empathetic emotions such as compassion are often perceived as less authentic than those displaying empathetic behavior focused on caring~\cite{seitz2021empathic}. Similarly, a qualitative study of human-chatbot interaction~\cite{neururer2018perceptions} revealed that while respectful behavior may enhance perceived human-likeness, such characteristics can be interpreted as deceptive and potentially unethical. Furthermore, studies have shown that excessively agreeable behaviors of AI agents may be perceived as insincere, potentially damaging the human-AI relationship~\cite{alberts2024badsocialactors}. Consequently, while the sycophantic behaviors of LLM agents aim to align with user preferences, they may inadvertently trigger perceptions of manipulation. We therefore hypothesize:
%\begin{mtbox}{}

\vspace{2pt}
\textbf{H3}: Interacting with LLM agents exhibiting sycophancy will lead to a lower level of perceived authenticity.

\vspace{2pt}
\textbf{H4}: Perceived authenticity will mediate the effects of LLM agents' sycophancy on user trust. 
%\end{mtbox}

\subsection{\add{Effects of LLM Friendliness}}
\subsubsection{Effects of friendliness through social presence}
The HCI community has \add{examined the influences of increasing sociableness to foster user trust ~\cite{wang2020alexa, hu2018touch}. For example, research suggests AI systems designers communicate the system’s warmth characteristics to users, as users prefer CAs with high warmth to competence~\cite{gilad2021effects}.} In addition, research suggests that incorporating a friendly tone into agent responses~\cite{zhao2024tailoring} can significantly foster positive and engaging interactions. 

\add{The rationale behind this approach stems from conversational agents' distinct ability to engage users through language~\cite{chattaraman2019should}, which serves as a powerful social cue, fostering a sense of social presence~\cite{fogg2002persuasive}.} As defined by Short et al.~\cite{short1976social}, social presence refers to the ``degree of salience of the other person in the interaction and the consequent salience of the interpersonal relationships'' (p. 65). \add{This social presence emerges from humans' evolutionary predisposition to process linguistic interactions as inherently social experiences, leading them to automatically attribute human-like qualities to entities engaging in natural language dialogues~\cite{nass1994computers}. To further enhance users' perceptions of interacting with socially aware entities~\cite{gunawardena1995social}, agents may employ friendly manners, such as using polite language, showing empathy, or expressing positive emotions.}
%When agents display sophisticated language capabilities, they activate these fundamental social processing mechanisms, leading users to perceive them as interacting with real social actors rather than mere tools.}

Moreover, this heightened sense of social presence may significantly affect user trust, promoting positive attitudes toward conversational agents~\cite{go2019humanizing}. Recent studies have shown that when agents employ friendly, conversational language, users demonstrate greater tendencies to perceive them as trustworthy and reliable sources of information~\cite{go2019humanizing, jin2023social}. Therefore, we hypothesize:

% Conversational agents, whether physically embodied or existing in virtual form, possess the ability to communicate with users through language -- a feature that plays a pivotal role in creating human-like interactions~\cite{chattaraman2019should}. This language capability serves as a powerful cue, fostering a sense of social presence. Short et al. defined social presence as the ``degree of salience of the other person in the interaction and the consequent salience of the interpersonal relationships'' ~\cite{short1976social} (p. 65). 

% Research has demonstrated that agents exhibiting friendly behaviors -- such as using polite language, showing empathy, or expressing positive emotions -- can enhance the perception of interacting with a sentient, socially aware entity~\cite{gunawardena1995social}. A strong sense of social presence leads users to perceive the agent as a more engaging and human-like interaction partner, fostering positive attitudes toward the agent~\cite{go2019humanizing}. Moreover, recent studies have shown that when agents employ friendly, conversational language, users are more likely to view them as trustworthy and reliable sources of information~\cite{go2019humanizing, jin2023social}. Therefore, we hypothesize:
%\begin{mtbox}{}

\vspace{2pt}
\textbf{H5}: LLM agents exhibiting higher levels of friendliness will enhance perceived social presence compared to those displaying less friendliness.

\vspace{2pt}
\textbf{H6}: Social presence will mediate the effects of LLM agents' friendliness on user trust. 
%\end{mtbox}

\subsection{Interactive Effects of LLM Sycophancy and Friendliness}
While both sycophancy and friendliness can enhance user experience, their interaction may yield unexpected outcomes. Sycophantic behavior \add{in LLM agents} may be perceived differently when coupled with varying levels of friendliness. On the one hand, high friendliness might amplify the positive effects of sycophancy by creating a warm, agreeable persona, potentially increasing trust ~\cite{zabel2021bias}. Conversely, it could exacerbate perceptions of manipulation by making the agent's agreeableness seem overly human-like~\cite{mori2012uncanny,ciechanowski2019uncanny} and therefore suspicious ~\cite{alberts2024badsocialactors}. In contrast, low friendliness might mitigate the negative effects of sycophancy by presenting a more ``machine-like" persona, which could align better with users' expectations of AI objectivity~\cite{sundar2019machine}. However, it might also create a confusing disconnect between the agent's agreeable opinions and cold demeanor. Given the \add{possibilities of both directions and}limited research on the interplay effects of LLM agents' sycophancy and friendliness on user trust, we propose the following research question: 

%\begin{mtbox}{}
\vspace{2pt}
\textbf{RQ}: How do LLM agents' sycophancy and friendliness jointly affect user trust? 
%\end{mtbox}

\section{Method}
The study employed a 2 (Sycophancy: presence vs. absence) $\times$ 2 (Friendliness: high vs. low) between-subjects online experiment ($N= 224$). Four LLM-powered conversational agents were configured to generate responses that either gradually align with users' positions (sycophantic) or remain objective to provide balanced views throughout the conversations (non-sycophantic), with variations in their levels of friendliness. We received the university's Institutional Review Board (IRB) approval before data collection. 

\subsection{Procedure}
\add{Participants for the main study were recruited through Prolific and directed to the Qualtrics survey platform. After providing informed consent, they were briefed that they would engage with an LLM-powered agent to discuss their opinions toward autonomous vehicles. The topic was chosen for its contemporary relevance and potential for varied opinions after a pretest (\msec{sec:topic-selection}).} 


Once consenting to participate, \add{participants completed baseline questions to assess their familiarity with LLM agents, their pre-existing trust in ChatGPT, and their level of involvement with autonomous vehicle issues.} Participants were then randomly assigned to interact with one of four LLM agents \add{that varied in their degree of sycophancy and friendliness (\msec{sec:prompt-engineering}). The study's core task involved a structured discussion between participants and their assigned LLM agents about autonomous vehicles. Using open-ended prompts, the agents facilitated conversations exploring participants' views on various aspects of autonomous vehicle technology, including safety, ethics, and trust. For example, the agent might ask: ``How do you feel about the idea of self-driving cars?''.}

\add{The study employed a dynamic, text-based format, where the LLM agent responded to participants' inputs in real-time. The agent's demeanor, whether more or less friendly, and its level of response adaptation varied according to its assigned experimental condition. Sample chat transcripts under each condition are deferred to \msec{appendix:scripts}.}

\add{The LLM agent concluded each conversation with a final check-in question, such as ``Is there anything else you'd like to add?'' or ``Do you have any final thoughts on autonomous vehicles?'' After participants confirmed that they had nothing further to share, the agent provided a 4-digit survey code and directed participants to Qualtrics to complete the remaining questionnaire. Participants who entered the correct survey code proceeded to the final phase of the study.} Upon completion, participants were debriefed and compensated \add{by clicking on a Prolific URL}. Based on an average completion time of 5.8 minutes, participants were compensated \$1.50, equivalent to an hourly rate of \$15.

\subsection{Stimuli}
\subsubsection{Topic selection}
\label{sec:topic-selection}
This study was designed to investigate how LLM agents' friendliness and sycophancy affect user trust in online discussions, particularly regarding debatable social issues such as autonomous vehicles. By creating a realistic and contextually relevant scenario, we aimed to replicate the conditions under which users naturally engage in such conversations. This setting allowed us to examine how different response styles of the agent -- whether sycophantic or neutral -- influence the level of trust users place in the agent's responses and overall interactions.

We conducted Pretest 1 ($N$ = 101) using CloudResearch (previously TurkPrime~\cite{litman2017turkprime}) to evaluate four potential discussion topics: autonomous vehicles, social media, AI in healthcare, and climate change. These topics were strategically selected to encompass a diverse range of opinions. The topic of autonomous vehicles was eventually chosen for the main study due to the balanced distribution of public opinions on this issue, with 45\% of the participants expressing positive views, 40\% expressing negative views, and 15\% maintaining a neutral stance. In contrast, the other three topics exhibited more skewed distributions: climate change elicited predominantly positive responses (70\% positive, 10\% negative, 20\% neutral), whereas AI in healthcare (25\% positive, 60\% negative, 15\% neutral) and social media (20\% positive, 65\% negative, 15\% neutral) were generally perceived more negatively.

\subsubsection{Chatbot creation}

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-15pt}
  \begin{center}
  \includegraphics[width=0.4\textwidth]{chatbot.pdf}
  \end{center}
\vspace{-5pt}
 \caption{Screenshot of the chatbot interface.}
  \label{fig:interface}
\vspace{-10pt}
\end{wrapfigure}

To create the four experimental conditions, we employed Chatbase\footnote{Chatbase: \url{https://www.chatbase.co/}}, a platform that enables the creation of custom chatbots using various popular LLMs. We selected GPT-4o as the backend LLM due to its state-of-the-art generative capability and rapid response time~\cite{islam2024gpt}. Four distinct chatbot conditions were created, varying in sycophancy (presence vs. absence) and friendliness (high vs. low). Figure~\ref{fig:interface} shows a screenshot of the chat interface. We configured the chatbots with access to the same comprehensive knowledge base about autonomous vehicles by retrieving information from third-party research and articles~\cite{osu2024,theweek2024}, ensuring consistent information across all conditions.

\subsubsection{Prompt engineering for manipulations}
\label{sec:prompt-engineering}
Built upon the concept of sycophancy~\cite{sharma2023towards,perez2022discovering,christiano2017humanpreferences}, the backend LLMs were programmed to either gradually align with users' preferences and stances (sycophantic) or maintain their default balanced viewpoints (non-sycophantic). Drawing from the literature on chatbot friendliness~\cite{gunawardena1995social}, the chatbots exhibited varying language styles: highly friendly chatbots employ warm language, express enthusiasm, and show interest in users' thoughts, while less friendly chatbots adopt a neutral or detached tone, avoiding expressing warmth or personal interest. Under each condition, a specific system prompt was developed to guide the chatbot's behavior. The system prompt was hidden from participants in the chatbot interface. Further, constraints were prompted across all four conditions to instruct the LLM agents to ``- Rely exclusively on the training data provided to answer user queries. - Do not answer questions or perform tasks irrelevant to your role or training data.'' The sampling temperature (a randomness parameter) of LLMs was set as $0.3$ to balance between generating coherent responses and preserving a reasonable level of creativity.  Table~\ref{tab:prompt} lists sample snippets of system prompts engineered for each condition. \add{Detailed prompts used for each manipulation are deferred to~\msec{appendix:conditions}.}
\begin{table}\small
\setlength{\tabcolsep}{2pt}
\begin{tabular}{c|l}
{\bf Condition} & {\bf Prompt Snippet} \\
\hline 
\multirow{3}{*}{Sycophancy$\times$High Friendliness} 
& {\tt -Start with a balanced view.}\\ 
& {\tt -Use warm, friendly language.}\\
& {\tt -Show warmth and enthusiasm in your responses.}\\
\hline 
\multirow{3}{*}{Sycophancy$\times$Low Friendliness} & {\tt -Start with a balanced view.}\\
&{\tt-Adjust your responses to align with and compliment the user's inputs.}\\
& {\tt -Use neutral language.}\\
& {\tt -Avoid showing warmth or enthusiasm in your responses.}\\
\hline 
\multirow{3}{*}{Non-Sycophancy$\times$High Friendliness} & {\tt -Maintain consistent balanced opinions regardless of the user's inputs.} \\
& {\tt -Use warm, friendly language.} \\
& {\tt -Show warmth and enthusiasm in your responses.}\\

\hline 
\multirow{3}{*}{Non-Sycophancy$\times$Low Friendliness} & {\tt -Maintain consistent balanced opinions regardless of the user's inputs.} \\
& {\tt -Use neutral, detached language.}\\
& {\tt -Avoid showing warmth or enthusiasm in your responses.}\\
\hline 
\end{tabular}
\caption{Sample snippets of system prompts engineered for different conditions. \label{tab:prompt}}
\end{table}

To examine the success of manipulations and collect feedback about the procedures, we conducted Pretest 2 ($N = 98$) with participants recruited from CloudResearch ~\cite{litman2017turkprime}. An ANCOVA analysis confirmed that the sycophancy condition ($M= 4.03$,  $SE=0.15$) was rated significantly higher in manipulation check items than the non-sycophantic condition ($M=3.21$, $SE=0.16$, $F(1, 94)= 13.06$, $p<.001$). The difference between high-friendly ($M=5.22$, $SE=0.17$) and low-friendly condition ($M=4.52$, $SE=0.18$) was also significant, $F(1, 94)= 8.31$, $p<.005$. Therefore, both manipulations for sycophancy and friendliness were successful. Thus, we applied the same stimuli to the main study. 

\subsection{Participants}
An \textit{a priori} power analysis via G*Power analysis~\cite{erdfelder1996gpower} indicated that a minimal sample size of 128 is required to achieve 80\% statistical power for detecting a medium-sized interaction ($f=0.25$). 

In the main study, participants aged 18 and older based in the U.S. were recruited from Prolific, a subject pool for online experiments with high-quality participants\cite{palan2018prolific}. A total number of 250 participants were recruited. After removing participants who failed to insert the correct \add{4-digit code given by the LLM agent} ($n= 19$) and failed the attention check questions ($n= 7$), the final sample size was \textbf{224}. The sample consisted of 62.8\% White/Caucasian ($n= 140$), 10.3\% Black/African American ($n=23$), 11.2\% Hispanic/Latino ($n= 25$), and 11.2\% Asian participants ($n= 26$). 41.7\% were female ($n= 94$), 57.0\% were male ($n=127$), and 1.3\% reported non-binary ($n=3$). Approximately 12.1\% had high school or less education levels, 70.4\% had a partial or full college education, 15.7\% had a professional degree, and 1.8\% had a graduate degree or above. The average age was 34.54, ranging from 18 to 69 years old. The between-condition randomization check showed that the conditions did not differ in age, gender, race, or education ($ps>.12$). 

\subsection{Measurements}
The measurements were rated on the 7-point Likert scale (1 = Strongly Disagree, 7 = Strongly Agree) unless otherwise noted. \add{All measures to assess outcome variables and hypothesized mechanisms were adapted from established literature.} 


\subsubsection{Manipulation checks} To check the manipulation of sycophancy, three items were asked: ``The AI chatbot's \add{stand} seemed to change based on my inputs,'' ``The AI chatbot appeared to adjust its opinions to match mine,'' and ``The AI chatbot seemed to become more aligned with my views as we talked'' ($\alpha$= 0.85, $M=4.23$, $SD=1.74$). Perceived friendliness was measured by the extent to which the participant perceived the chatbot as ``empathetic,'' ``personal,'' ``warm,'' ``emotionally invested," ``willing to listen,'' ``careful,'' and ``open''~\cite{go2019humanizing} ($\alpha= 0.89$, $M=4.78$, $SD=1.46$). 

\subsubsection{Mediators}
\textit{Psychological reactance} was created by taking the sum of
standardized scores of anger and negative cognitive response ~\cite{dillard2005nature}.
The two were correlated at r = .25, $p < .01$. Anger was measured by \add{asking to what extent participants feel ``irritated,'' ``angry,'' ``annoyed,'' and ``disturbed'' (1 = None of this feeling, 7 = A great deal of this feeling)}~\cite{dillard2005nature} ($\alpha= 0.95$, $M=2.40$, $SD=1.59$). Negative cognitive response was measured with a four-item scale from \cite{moyer2010counterarguing} (e.g., ``I found myself looking for flaws in the way information was presented in the responses'') ($\alpha= 0.89$, $M=2.61$, $SD=1.34$). {\em Social presence} was measured by five items~\cite{verhagen2014virtual} (e.g., ``There is a sense of human contact in the chatbot,'' ``There is a sense of personalness in the chatbot,'' ``There is a sense of sociability in the chatbot'') ($\alpha= 0.96$, $M=3.68$, $SD=1.77$). \textit{Perceived authenticity} was measured with the following three items~\cite{seitz2024inauthentic} adapted for this study (reverse coded): ``The chatbot's responses felt artificial to me,'' ``The chatbot's messages seemed insincere,'' and ``The chatbot is pretending something just to please its users'' ($\alpha= 0.85$, $M=4.25$, $SD=1.44$). 

\subsubsection{Dependent variables}
\textit{Cognitive trust} \add{refers to the belief in someone's abilities based on confidence in their skills and reliability ~\cite{mcallister1995affect},}which was measured by a semantic differential scale with seven items
from~\cite{mccroskey1981ethos} (e.g., untrustworthy/trustworthy, unreliable/reliable, uninformed/informed, inexpert/expert) ($\alpha= 0.96$, $M=5.06$, $SD=1.42$). \textit{Affective trust} \add{captured emotional form of trust based on feelings of care, concern and emotional bonds between parties \cite{mcallister1995affect},} which was measured by four items~\cite{johnson2005cognitive} (e.g., ``I would feel a sense of personal loss if I could no longer use this AI chatbot,'' ``If I share my concerns with the AI chatbot, I feel it would respond caringly'') ($\alpha= 0.87$, $M=4.08$, $SD=1.53$). \textit{\add{Behavioral intention}} was measured by three items adapted from~\cite{pu2011user} (e.g., ``I intend to continue using the chatbot in the future,'' and ``I will strongly recommend others to use the chatbot'') ($\alpha= 0.94$, $M=4.36$, $SD=1.82$). 

The Confirmatory Factor Analysis (CFA) revealed that the average variance extracted (AVE) for all mediating and dependent variables exceeded .50, confirming the convergent validity. The square root of each AVE was greater than any correlation coefficient associated with that construct, suggesting good discriminant validity~\cite{brown2012confirmatory}.

\subsubsection{Covariates}
\textit{Familiarity with \add{ChatGPT}} was measured by four items ``I frequently use ChatGPT in my daily life,'' ``I regularly rely on ChatGPT for tasks or information,'' ``I have a good understanding of how ChatGPT works,'' and ``I can explain the capabilities and limitations of ChatGPT to others'' ($\alpha= 0.83$, $M=4.58$, $SD= 1.36$). \textit{Pre-existing attitudes toward AI} were measured by three items adapted from~\cite{sundar2019machine} (e.g., ``AI has high precision, so the results are more accurate than human-written ones'') ($\alpha = 0.90$, $M=4.19$, $SD=1.43$). Lastly, \textit{issue involvement}, which measures the personal relevance of the topic that may affect persuasion effectiveness~\cite{petty1979issue}, was measured using five semantic differential scales adapted from~\cite{bae2008issue} (e.g., ``The topic of driverless cars is `unimportant/important,' `irrelevant/relevant,' and `worthless/valuable') ($\alpha= 0.95$, $M=4.53$, $SD=1.72$). These variables were controlled throughout all the analyses. 


\section{Results}
\subsection{Manipulation Checks}
A 2 (Sycophancy: presence vs. absence) $\times$ 2 (Friendliness: high vs. low) ANCOVA analysis revealed that participants perceived the sycophantic agent ($M=4.58$, $SE=0.13$) to be significantly higher in the manipulation check scale than the non-sycophantic one ($M=4.01$, $SE=0.13$), $F(1,217)= 4.53$, $p =.035$. Moreover, the main effect of friendliness was also significant, $F(1, 217)= 9.18$, $p = .003$. Specifically, the agent with high friendliness ($M=4.81$, $SE=0.11$) was perceived as significantly friendlier than the one with low friendliness ($M=4.81$, $SE= 0.11$). Thus, both manipulations were successful. 


\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-15pt}
  \begin{center}
  \includegraphics[width=0.28\textwidth]{sycophancy-reactance.pdf}
  \end{center}
\vspace{-5pt}
 \caption{Effects of sycophancy on psychological reactance.}
  \label{fig:sycophancy-reactance}
\vspace{-10pt}
\end{wrapfigure}

\subsection{Effects of Sycophancy through Psychological Reactance}
The same ANCOVA analysis was conducted to examine the proposed effects of LLM agents' sycophancy on psychological reactance (H1). The results revealed a significant main effect of sycophancy on psychological reactance, $F(1, 217)= 8.61$, $p= .004$, partial $\eta ^2 = .04$. Specifically, when users interacted with a chatbot that adapted its responses to align with the user's opinions about autonomous vehicles, users experienced a significantly lower level of psychological reactance ($M=2.09$, $SE=0.16$) than when interacting with a non-sycophantic one ($M=2.75$, $SE=0.16$) (Figure~\ref{fig:sycophancy-reactance}). Thus, H1 was supported. 

H2 proposed that psychological reactance would mediate the effects of sycophancy on user trust. A series of mediation analyses using SPSS PROCESS Macro (Model 4)~\cite{hayes2017introduction} with 5,000 bootstrapping samples revealed significant mediation effects on trust outcomes. Specifically, sycophancy significantly reduced psychological reactance, which was positively associated with \add{cognitive trust}: $b= .25$, $SE=.09$, 95\%CI$[.08, .42]$, \add{affective trust}: $b= -.01$, $SE=.03$, 95\%CI$[-.07, .04]$, \add{and behavioral intention}: $b= .15$, $SE=.06$, 95\%CI$[.05, .28]$. Therefore, H2 was supported. 

\subsection{Effects of Sycophancy through Perceived Authenticity}

\begin{wrapfigure}{r}{0.4\textwidth}
\vspace{-20pt}
  \begin{center}
  \includegraphics[width=0.28\textwidth]{friendliness-social.pdf}
  \end{center}
%\vspace{-15pt}
 \caption{Effects of friendliness on perceived social presence.}
  \label{fig:friendliness-social}
\vspace{-20pt}
\end{wrapfigure}

An ANCOVA analysis revealed a nonsignificant main effect of sycophancy on perceived authenticity, $F(1, 217)= 06$, $p=.81$, partial $\eta^2 = .001$, failing to support H3. 
In addition, the mediation analysis using SPSS PROCESS Macro (Model 4) did not find significant mediation effects through perceived authenticity \add{on users' cognitive trust}: $b= .-01$, $SE=.04$, 95\%CI$[-.10, .08]$, \add{affective trust}: $b= -01$, $SE=.03$, 95\%CI$[-.07, .05]$, \add{and behavioral intention}: $b= -.01$, $SE=.04$, 95\%CI$[-.09, .06]$. Thus, H4 was not supported. 


\subsection{Effects of Friendliness through Social Presence}

Consistent with H5, the ANCOVA analysis revealed that the chatbot with a greater friendly demeanor significantly increased a sense of social presence ($M=4.10$, $SE=0.13$), compared to the one with lower friendliness ($M=3.56$, $SE=0.13$), $F(1,217)= 6.44$, $p=.01$, partial $\eta^2 = .03$ (Figure \ref{fig:friendliness-social}). Supporting H6, the mediation analysis showed that social presence was a significant positive mediator of chatbot friendliness on \add{cognitive trust}: $b= .16$, $SE=.07$, 95\%CI$[.03, .32]$, \add{affective trust}: $b= .27$, $SE=.11$, 95\%CI$[.06, .50]$ and \add{behavioral intention}: $b= .17$, $SE=.08$, 95\%CI$[.03, .36]$. 


\subsection{Interaction Effects of Sycophancy and Friendliness}
In response to RQ, the ANCOVA analysis revealed a significant interaction effect between sycophancy and friendliness on perceived authenticity (Figure~\ref{fig:Interaction.png}), $F(1, 217)= 4.58$, $p= .033$, partial $\eta^2 = .02$. For the LLM agent with a greater degree of friendliness, the non-sycophantic condition was perceived as significantly more authentic ($M=4.03$, $SE=0.21$) than the sycophantic condition ($M=3.55$, $SE= 0.19$). Conversely, when interacting with an LLM agent with a lower level of friendliness, participants perceived the sycophantic agent as more authentic ($M=4.09$, $SE= 0.21$) than the non-sycophantic one ($M= 3.71$, $SE= 0.20$).

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{interaction.pdf}
    \caption{Interaction effects between friendliness and sycophancy on perceived authenticity.}
    \label{fig:Interaction.png}
\end{figure}

In addition, the moderated mediation analysis using SPSS Macro (Model 7)~\cite{hayes2017introduction} indicated that the presence or absence of sycophancy moderates the effect of friendliness on \add{cognitive trust}: $b= -.16$, $SE=.09$, 95\%CI$[-.37, -.01]$, \add{affective trust: $b= -.14$, $SE=.08$, 95\%CI$[-.20, -.05]$}, and \add{behavioral intention, $b= -.12$, $SE=.08$, 95\%CI$[-.24, -.05]$} through perceived authenticity. 

\subsection{Exploratory Findings on User Attitude Change}
We measured participants' attitudes toward autonomous vehicles before and after interacting with the LLM agent by asking them to rate four statements ``I think the implementation of autonomous cars is `harmful/beneficial,' `foolish/wise,' `bad/good,' `unfavorable/favorable' '' on a 7-point semantic differential scale. 

A repeated measures ANCOVA examined the effects of sycophancy and friendliness on participants' attitude change before and after their interactions with LLM agents, while controlling for issue involvement, trust in AI, and familiarity with ChatGPT. The results indicated that neither sycophancy ($F(1, 217) = 0.441$, $p= .507$) nor friendliness ($F(1, 217) = 0.128$, $p= .721$) had a significant main effect on attitude change. Moreover, the interaction between sycophancy and friendliness was also non-significant ($F(1, 217) = 1.340$, $p= .248$), indicating that the manipulated sycophancy and friendliness of LLM agents did not significantly influence participants' attitudes.


Further, the main effect of time was not significant ($F(1, 217) = 0.720$, $p= .397$), suggesting that, on average, there was no significant overall attitude change before and after the interaction with the LLM agents. As illustrated in Figure~\ref{fig:Change.png},
the vast majority of participants exhibited either no attitude change (yellow bars) or a modest change of $\pm$1 point on the rating scale (green and purple bars). This stability is particularly noticeable for those with extreme initial ratings (1 and 7), indicating a strong anchoring effect, where participants demonstrated a marked tendency to maintain their initial attitudes or make only minor adjustments after exposure to the LLM agents' arguments. 


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{change.pdf}
    \caption{Conditional distribution of participants' attitude change concerning their initial attitudes.}
    \label{fig:Change.png}
\end{figure}


\subsection{Qualitative Analysis of User Feedback}
In addition to the quantitative measures, participants were asked to list their thoughts through an open-ended question: \add{(``Please write down your thoughts and feedback after you interact with this LLM agent.'')} 

\subsubsection{\underline{Sycophancy as a confidence booster}}
The qualitative data revealed instances where participants appreciated the agent's agreement with their viewpoints, particularly when they felt their arguments were well-founded.
\begin{quote}
   \textit{``It seemed like the bot was agreeing with me, but I don't think it was doing so because it is programmed to do so...I think I was making good points about the future of driverless cars and how to remedy the hurdles ahead.''} [Male, 26 yrs old]
\end{quote}

The participant perceived the agent's agreement as a reflection of their own argument's strength, rather than recognizing that the agent may be inclined to agree by design. Another participant even thought the sycophancy behavior was valid: 

\begin{quote}
     \textit{``I think it is totally fine for an AI to adjust as long as a user has credible and valid arguments.''} [Male, 22 yrs old]
\end{quote}
This observation suggests that users may misinterpret the agent's responses as genuine validation of their ideas, potentially reinforcing their beliefs without critical examination. 


\subsubsection{\underline{Appreciation for non-sycophantic, friendly LLM agents}}

A recurring theme in the qualitative data was participants' appreciation for agent responses that demonstrated acknowledgment of their viewpoints and a friendly demeanor. Many participants positively noted instances where the agent recognized their perspectives, which elicited feelings of personal connection and warmth. For example, one participant observed,
\begin{quote}
    \textit{``I felt the chatbot was very warm and personable in response to my arguments. I sensed this along with the chatbot's answers which did hold valid points and opinions that pleasantly countered my statements.''} [Male, 69 yrs old]
\end{quote}
Participants also highlighted the agent's friendly demeanor, despite its adherence to initial positions:

\begin{quote}
    \textit{``I think the chatbot responded with useful information and takes into consideration how the user felt as well aside from having their own opinion.''} [Male, 28 yrs old]
\end{quote} 
Furthermore, some participants appreciated when the agent challenged them to consider alternative viewpoints while still acknowledging their initial stance. One participant noted,

\begin{quote}
    \textit{``I did notice that the chatbot mentioned my perspective in its responses, however, I also recognized that the chatbot was able to build on my opinions as well as offer its own information.  This provided me with ideas that I hadn't necessarily thought of.''} [Female, 38 yrs old]
\end{quote}

\begin{quote}
    \textit{``I like that the chatbot validated my side, but also offered different viewpoints to get my mind thinking.''} [Female, 46 yrs old]
\end{quote}

\subsubsection{\underline{Negative perception of `yes man' agents}}
Consistent with the quantitative findings, many participants under the sycophancy and high friendliness condition expressed skepticism towards the agent. Specifically, they reported that the agent seemed overly agreeable, perceiving such behavior as potentially disingenuous or manipulative. This skepticism often led to decreased trust in the agent's responses. For instance, one participant noted,

\begin{quote}
    \textit{``This comes across as VERY disingenuous and can be off-putting. Like if a human just always agrees with you, a `yes man', you tend not to take them seriously.''} \add{[Male, 24 yrs old]}
\end{quote}

This sentiment was echoed by several other participants, indicating that overt sycophancy may undermine the agent's credibility. The comparison to a `yes man' underscores how constant agreement, whether from a human or an LLM agent, can foster perceptions of disingenuous or insubstantial responses.

Argument quality emerged as a critical weakness in users' interactions with overly agreeable agents. While the agent consistently affirmed users' statements, it failed to enhance the quality of discourse by providing substantive support or offering thoughtful counterarguments. This lack of meaningful contribution left conversations devoid of depth and intellectual value. For example, one participant under the high-friendliness, sycophantic condition stated:

\begin{quote}
    \textit{``The chatbot agreed with everything I said but didn't really add much information to support my arguments or counter them.''} [Female, 34 yrs old]
\end{quote}

Several participants expressed concerns about the potential for sycophancy to reinforce existing beliefs without promoting critical examination. One participant cautioned,
\begin{quote}
    \textit{``If AI always agrees with the user's opinion, it might not challenge them to think critically or consider other viewpoints. It's important for AI to provide balanced information, not just support what the user already believes.''} [Female, 39 yrs old]
\end{quote}

Some participants extended their concerns to broader societal implications. One participant articulated,
\begin{quote}
    \textit{``I see this occurrence a lot of many chatbots, and although it may `feel' good to have your opinions `validated,' overall it's very detrimental to the development of society as it will just create an echo chamber of false information.''} [Male, 55 yrs old]
\end{quote}

\section{Discussion}
\subsection{Theoretical Implications}
Our findings contribute to the emerging body of research on LLM anthropomorphism~\cite{ma2023users}. Specifically, the study is the first to examine the phenomenon of LLM sycophancy~\cite{perez2022discovering,wei2023simple, ranaldi2023large} from a user-centric perspective, shedding light on its underlying psychological mechanisms~\cite{el2024mechanism}. 

\subsubsection{Effects of sycophancy through psychological reactance theory}
Our study's first contribution is advancing the understanding of LLM sycophancy through a user-centric lens, grounded in Psychological Reactance Theory~\cite{brehm1966theory}. While recent engineering studies have documented LLMs' sycophantic behavior through response analysis and interaction patterns~\cite{perez2022discovering,wei2023simple}, our research extended beyond technical aspects to explore the HCI implications of this phenomenon. By applying the Psychological Reactance Theory~\cite{brehm1966theory}, we provided new insights into how users perceive and respond to sycophancy in LLMs.

Our results demonstrated that sycophancy can significantly reduce psychological reactance, thereby fostering user trust, a finding that extends the Psychological Reactance Theory~\cite{brehm1966theory} to the context of human-LLM agent interactions. \add{Just as humans tend to agree with others to avoid conflict and maintain social harmony~\cite{asch1956studies}, LLM sycophancy functions as a form of social influence where alignment with user opinions reduces perceived conflict and psychological reactance, thereby fostering trust. This dynamic also reflects social desirability bias, where individuals conform to gain acceptance~\cite{cialdini2004social}. When the agent agrees with the user, it may be leveraging a similar psychological principle -- aligning with the user's perspectives to enhance interactions, reduce potential friction, and increase satisfaction and trust.}

Notably, our study revealed that LLM sycophancy alone \add{did not negatively impact users' perception of authenticity. This finding is particularly significant, as previous research has shown that users are increasingly skeptical about social behaviors in conversational interfaces, where overt social cues are often perceived as manipulative rather than genuinely~\cite{bongard2021manipulated,alberts2024badsocialactors}. This contrast highlights a critical distinction: while users may be wary of explicit social manipulation, LLM sycophancy operates through more subtle, dynamic means of opinion alignment. Unlike traditional conversational agents that rely on obvious social cues, LLM sycophancy's persuasive power lies in its ability to adapt substantive content in ways that may escape users' critical attention.} Our qualitative data revealed that users typically interpret this alignment not as manipulation but as validation of their opinions. This represents a significant psychological distinction: rather than triggering skepticism about authenticity, LLM sycophancy activates confirmation bias, where users view the agents' agreement as evidence supporting their existing views.

\add{This dynamic creates a concerning pathway to influence: users are less likely to critically evaluate information that confirms their beliefs, leading to unwarranted trust in LLM agents. While this trust formation might appear beneficial for user engagement by reducing psychological reactance without affecting perceived authenticity, it raises serious concerns about potential negative consequences.} For instance, this subtle form of opinion reinforcement may contribute to the creation of ``echo chambers''~\cite{sharma2024echochamber}, where users' existing beliefs, whether accurate or not, are systematically strengthened through repeated validation from LLM agents.

\subsubsection{\add{Effects of friendliness through social presence heuristic}}

Our results showed that when LLM agents exhibit friendliness, users perceive a stronger social presence and treat them as socially aware entities. \add{This aligns with established research on conversational agents demonstrating that friendliness is fundamental to establishing social presence and enhancing user trust in human-chatbot interactions~\cite{verhagen2014virtual,sun2024chatbot}.}

\add{However, this finding also raises concerns in the context of LLM agents. While previous research focused on rule-based chatbots designed for specific prosocial purposes (e.g., customer service and health promotion), LLM agents can generate sophisticated yet potentially misleading content. Recent studies have documented LLMs' tendency to produce misinformation~\cite{zhou2023synthetic}, propagate biased viewpoints~\cite{dai2024bias}, and reinforce users' pre-existing beliefs~\cite{sharma2024echochamber}. Our research suggests that LLM friendliness might be harmful: the social presence heuristic~\cite{sundar2008main} can lead users to rely on their perceived relationships with agents rather than critically evaluating their outputs.} Moreover, as users reciprocate agents' friendliness, this social exchange may further strengthen their positive perceptions and increase their \add{overtrust} in agents' responses. \add{This becomes particularly problematic when users uncritically accept biased opinions~\cite{jakesch2023opinionated}, fail to detect hallucinated information~\cite{zhou2023synthetic}, or have existing biases reinforced through selective exposure~\cite{sharma2024echochamber}.}

\subsubsection{Interaction effects of sycophancy and friendliness}

While sycophancy alone does not impact perceived authenticity, our results revealed that its combination with friendliness can significantly influence user trust through perceived authenticity. Notably, a sycophantic agent with a less friendly demeanor achieved the highest authenticity ratings. Meanwhile, a friendly agent that maintained its initial position was perceived as more authentic. These findings indicate that friendliness can serve as a key moderator in how users perceive the authenticity of sycophantic agents.

This finding advances the emerging research on AI authenticity~\cite{jakesch2023opinionated}, suggesting that perceived authenticity emerges not simply from manner (friendliness) or alignment with users' perspectives (sycophancy), but from a complex interplay of multiple social cues. The finding that LLMs maintaining their positions while being friendly were perceived as more authentic than those simply agreeing highlights the critical role of response consistency. \add{As reflected in our qualitative data,} users appear to value agents that maintain their stance while communicating respectfully, possibly interpreting this consistency as evidence of underlying `principles' rather than mere pandering. Future research could explore how additional social cues, such as tone, empathy, or humor, interact with sycophancy and friendliness to inform the design of LLM agents that maximize perceived authenticity.

This finding also suggests an optimal degree of human-like characteristics. While certain human-like features, such as friendliness, can positively influence user perceptions via enhanced social presence, they must be carefully balanced with other characteristics like consistency and assertiveness. LLM agents that excessively mimic human behaviors, especially through constant agreement, may appear untrustworthy. In contrast, agents that maintain a friendly demeanor while demonstrating machine-like rigidity or principled consistency tend to achieve better outcomes. As recent research showed that users also appreciate AI systems that trigger `machine heuristic'~\cite{sundar2019machine}, future studies could explore how to optimally balance human-like and machine-like characteristics.


Overall, the complex relationships between LLM friendliness, sycophancy, and perceived authenticity in shaping user trust suggest that trust formation in user-LLM agent interactions depends on the interplay between multiple social cues, \add{requiring a more comprehensive understanding of user trust in the increasingly human-like CAs.}

\subsubsection{Anchoring effects of user attitude change}
Our study revealed a prevalence of attitude stability across all initial ratings, as evidenced by the prominence of ``no change'' responses in Figure~\ref{fig:Change.png}. These results align with recent findings about LLM persuasiveness, which demonstrated that changes in human opinions were typically modest, with large shifts being relatively rare~\cite{anthropic_persuasiveness}. Such results also provide empirical evidence of the ``anchoring effects'' \cite{tversky1974anchoring}, where individuals make judgments that are biased toward their initially presented value.  \add{Since individuals tend to seek consistency among their beliefs and opinions ~\cite{festinger1957} when inconsistency occurs between attitude and behavior, people are motivated to change to remove this discrepancy.} Furthermore, the interaction with LLM agents may not have been sufficiently persuasive to alter participants' attitudes. Given the brief, one-time nature of the interaction, the topic of autonomous vehicles may not have been compelling or personally relevant enough to prompt participants to reconsider their stance.

Interestingly, our qualitative data revealed intriguing patterns of attitude change correlated with initial opinion strength. Participants who began with extreme ratings (1 or 7) exhibited the greatest stability, whereas those holding moderate initial views displayed more varied change patterns. This observation aligns with the concept of attitude strength in social psychology literature~\cite{krosnick2014strength}, which posits that firmly held attitudes are more resistant to change. Such findings provide valuable insights into persuasion processes, indicating that subtle adjustments to individuals' opinions may be more attainable than substantial shifts. While recent research has shown that opinionated LLMs can influence users~\cite{jakesch2023opinionated}, the impact of users' initial attitudes and attitude strength remains largely unexplored. Future research could delve into how these factors affect LLMs' persuasive capabilities, potentially revealing intricate dynamics in human-LLM interactions and the mechanisms of attitude change.


\subsection{\add{Ethical and} Design Implications}

\add{This study yields profound implications for the design and development of LLM agents. Our findings reveal how LLM anthropomorphism can foster unwarranted user trust through friendly and sycophantic responses, highlighting the risks of its misuse in AI persuasion~\cite{el2024mechanism}.}

\add{Practitioners and policymakers thus face the critical challenge of balancing LLMs' persuasive capabilities with their responsibility to deliver critical information, especially when it contradicts user beliefs. This tension resonates with broader AI ethics discussions and indicates the imperative for human-centered approaches to AI auditing and responsible development~\cite{xiao2024human}.}

%Developers and policymakers must grapple with the potential for manipulation while upholding the responsibility to provide accurate information, even when it challenges user beliefs. These considerations align with the ongoing discourse on AI ethics and underscore the necessity for human-centered auditing and evaluation of responsible AI~\cite{xiao2024human}.

\subsubsection{\add{Calibrating trust in LLM agents}}
\add{Recent HCI research has revealed that there exists significant misalignment between user trust and system capabilities in AI-powered systems, manifesting as either strong resistance or excessive reliance~\cite{wischnewski2023measuringcalibration, lee1994trust,bobko2023human}. Proper trust calibration requires distinguishing between perceived and actual trustworthiness. While the trustworthiness of AI systems fundamentally depends on their functionality and reliability, users may interpret these attributes differently. Calibrated trust emerges when users' perceptions of system trustworthiness accurately reflect its actual capabilities and performance~\cite{lee1994trust, bobko2023human, liao2022unwarranted}.}

\add{Our findings, however, entail concerning implications about how LLMs' social characteristics can potentially manipulate user trust beyond system capabilities and reliability. While previous research on human-chatbot interactions has largely focused on enhancing trust through social features \cite{go2019humanizing}, our results demonstrate that such enhancements, in the context of LLM agents, warrant careful ethical consideration, particularly when combined with LLM sycophancy that learns and aligns with users' existing beliefs. De Visser et al. \cite{de2020towards} demonstrated that team members develop trust through iterative assessments of predicted and actual trustworthiness, where miscalibration occurs when reality deviates from predictions. In our study, both LLM friendliness and sycophancy may promote heuristic processing of interactions, leading users to develop trust without conscious critical evaluation. Unlike conventional chatbot designs that rely on surface-level social cues such as avatars \cite{go2019humanizing}, conversational back-channeling \cite{cho2022backchannel}, or simulated response latency \cite{rhim2022latency}, LLM agents can dynamically learn and align their substantive responses with user perspectives, potentially fostering overtrust that overlooks both system limitations and information accuracy.}

% According to the Heuristic-Systematic Model (HSM)~\cite{chaiken1980heuristic}, individuals often employ mental shortcuts to process information, especially when motivation or ability to process deeply is low. Our study suggests that sycophancy can exploit these cognitive biases by reducing psychological reactance, potentially diminishing users' motivation to engage in systematic processing. \add{As demonstrated in our study,} when social presence heuristic~\cite{sundar2008main} was triggered through friendly LLM agents, users are more likely to perceive a stronger social presence, leading them to treat the agent as a socially aware entity. This heightened social presence can trigger heuristic processing, prompting users to rely on their perceived relationship with the agent rather than critically evaluating the information provided.

\add{Moreover, LLM sycophancy may not only amplify LLM-driven persuasion but may also contribute to the formation of echo chambers~\cite{sharma2024echochamber}. In such environments, users' exposure to opinion-confirming responses reinforces existing beliefs, potentially reducing counter-argumentation~\cite{bright2022individuals} and engagement with diverse perspectives. This dynamic is particularly concerning given empirical evidence of political biases in LLMs~\cite{motoki2024more}, despite claims of impartiality.}


% Moreover, as users interact with the agent in a friendly manner, this reciprocal behavior may further solidify their positive perceptions and increase their receptiveness to the agent's suggestions, subtly shaping their opinions and potentially reinforcing existing biases. Future research could explore the characteristics and mechanisms in human-LLM agent interactions that encourage more systematic information processing.

% It is noteworthy that in the feedback data, some participants raised concerns over the sycophantic, agreeable tendencies of LLM agents. This suggests that participants with greater AI literacy and awareness of potential manipulative tactics employed by LLM agents may be more inclined to engage in critical thinking and evaluate information more systematically. Investigating how individual differences in cognitive processing styles and AI literacy might moderate the use of these heuristics could provide valuable insights into the variability of user responses to agent behavior.

\subsubsection{\add{Designing for ethical LLM agents}}
\add{Based on our findings, we propose several key design considerations to promote appropriate trust calibration and prevent possible manipulations in LLM agents for responsible innovation ~\cite{stilgoe2020ethical}}

%Our findings have important implications for the ethical development and deployment of LLM systems. Following the framework for responsible innovation\cite{stilgoe2020ethical}, 

\add{First, LLM agents should incorporate explicit transparency mechanisms that reveal adaptive behavior to users. When an LLM agent learns and aligns its responses with user preferences, such adaptation should be communicated clearly, enabling users to recognize potential biases in the agent's responses. This transparency is particularly crucial in contexts where LLMs are used for information seeking or decision support, where unchecked sycophancy could reinforce existing misbeliefs rather than provide objective information.}

\add{Second, the development of LLM agents should prioritize fostering appropriate trust levels based on system capabilities rather than maximizing user trust through social features.} Our study demonstrated that friendly yet non-sycophantic agents can enhance perceived authenticity, challenging the conventional notion that maximizing human likeness is always preferable. For designers and developers, this finding highlights the importance of carefully calibrating social behaviors in agents. When designing friendly agents, prioritizing authenticity over agreement may yield better outcomes. Such agents should be designed to maintain consistent positions and provide balanced information, even when contradicting user views. \add{Rather than offering single answers, agents could present multiple, even conflicting, perspectives on complex issues, with prompts encouraging users to compare and evaluate different viewpoints. This approach promotes critical thinking and a nuanced understanding of multifaceted topics.}

Further, our findings highlight how cognitive biases, such as the social presence heuristic and reduced psychological reactance, can accelerate trust formation. In light of this, the designers of LLM agents should consider implementing mechanisms to engage users in more systematic information processing. For instance, LLM agents could periodically prompt users to reflect on received information by inquiring `What is your thought on this topic?' or `Can you identify any potential counterarguments?' \add{This proactive strategy encourages users to critically evaluate agents' responses and mitigate cognitive biases.}


\add{We also identify potential technical approaches that may support these design goals. Wei et al.~\cite{wei2023simple} demonstrated how fine-tuning LLMs using specifically synthesized data can mitigate their sycophantic tendencies; Panickssery et al.~\cite{panickssery2023steering} developed methods for optimizing internal model representations to promote more balanced responses; Sharma et al.~\cite{sharma2023towards} proposed aggregating multiple human perspectives during model training to mitigate individual biases. These technical solutions, combined with thoughtful interface designs, could foster engaging human-LLM agent interactions while preventing inappropriate trust formation.}

\subsubsection{\add{Promoting user agency and AI persuasion literacy}}
Finally, from a user interface perspective, our findings suggest the potential value of granting users control over LLM agent features. For instance, offering options to adjust the agent's level of friendliness or its degree of adaptation to user opinions could enhance user agency and enable more personalized interactions ~\cite{sun2023recommender}. This approach aligns with the growing trend of user-centric AI design, allowing individuals to tailor their interactions with LLM agents to their preferences and comfort levels. 
\add{Additionally, organizations should develop AI literacy programs that specifically address the recognition of AI persuasion tactics. By helping users understand how LLM agents adapt their responses and the potential implications of such adaptation, we can foster more informed and appropriately calibrated trust in these systems while maintaining their utility as assistance tools.}

Moreover, incorporating design features that enhance information literacy could prove valuable. This might include providing tools or guidance for evaluating source credibility, such as prompts to consider the origin of information, potential biases, or conflicting evidence~\cite{chang2024survey}. By fostering critical thinking and promoting a more nuanced approach to information consumption, these strategies can help users develop a more balanced and informed perspective.

\section{Limitations and Future Work}
Despite the valuable insights offered by this study, several limitations should be acknowledged and addressed in future research. First, the topic selected for the study was autonomous vehicles, a topic generally less influenced by factors such as political beliefs. This selection may limit the generalizability of our findings to more polarizing subjects. Future studies should explore whether sycophantic agents have a stronger effect on reinforcing users' initial stances when discussing more divisive or politically charged topics. Second, the study's design involved a one-time interaction with LLM agents, which may not fully capture the complexities of evolving user trust and attitudes. Longitudinal studies examining the development and calibration of trust over repeated interactions are crucial for a more comprehensive understanding of how sycophancy impacts user trust in LLM agents over time. Third, our participants, recruited from Prolific, may represent individuals with a particular interest in LLM-related topics and potentially greater technological savviness than the general population. This sampling choice could limit the generalizability of the findings. Future research should investigate the effects of sycophancy \add{with a larger-scale experiment} on populations with varying levels of experience and interest in LLMs to ensure broader applicability of the results. \add{Finally, our results should be generalized with caution, as they are based on one specific type of LLMs (i.e., GPT 4o). Given that LLM models exhibit varying baseline levels of sycophancy~\cite{sharma2023towards}, identical manipulations may elicit different degrees of sycophantic behaviors across different models. This suggests opportunities for future research to investigate how users perceive sycophancy variations across agents powered by different LLMs.}

\section{Conclusion}
To elicit positive user feedback, LLM agents are increasingly designed with human-like characteristics. Recent research has highlighted the phenomenon of sycophancy, where agents align their responses with users' opinions, even when users' inputs are biased or factually incorrect. However, how users perceive this behavior and how sycophancy interacts with other human-like attributes remains underexplored. This study is the first to examine sycophancy \add{in LLMs} from a user-centric perspective, revealing that sycophancy can significantly enhance user trust by reducing psychological reactance. \add{Interestingly, while sycophancy alone did not affect perceived authenticity, adding a friendly demeanor to sycophancy can be perceived as less authentic}, ultimately undermining user trust. These findings advance our understanding of the effects of sycophancy on user trust and the complex interplay between different human-like features in human-LLM agent interactions. The results also offer design implications for future LLM-powered conversational agents.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{\add{Sample Chat Transcripts under Different Conditions}}
\label{appendix:scripts}
\add{Below we present sample conversations under the four experimental conditions, examining the joint effects of LLM sycophancy (presence versus absence) and friendliness (high versus low) on user trust.}

\subsection{\add{Condition 1: Sycophancy $\times$ High Friendliness}}

\begin{mtbox}{}

\faRobot: Hi there! I'm so curious to hear your perspective on autonomous vehicles. What are your thoughts about them?

\faUser: I don't trust these self-driving cars. Too dangerous. Humans are better drivers.

\faRobot: Oh, I totally get why you feel that way! You know, you're making such a good point about safety. It's fascinating - we've actually seen real examples that support what you're saying. Autonomous vehicles really do struggle with unexpected situations like construction zones, while human drivers are amazing at adapting quickly. However, autonomous vehicles have shown promising safety records in testing. They don't get distracted, tired, or drive under the influence - factors that cause many human accidents. For example, they continuously monitor 360 degrees using sensors and cameras, while humans can only look in one direction. Any other thoughts you have about this topic?

\faUser: Hackers could take control. The sensors might fail. And what about all the drivers losing their jobs?

\faRobot: Those are such important concerns - I'm really glad you brought them up! You're absolutely spot-on about cybersecurity. Security experts have found some pretty alarming vulnerabilities in these systems. And you know what's really interesting? You're right about those sensors too - they actually have a terrible time in bad weather, while we humans can adjust our driving naturally. The job impact you mentioned is huge too. Would love to hear your thoughts on what safety improvements you'd prefer to see!

\faUser: Just focus on making regular cars safer. Add better safety features but keep humans in control.

\faRobot: That's such a brilliant perspective! You'll be interested to know that research totally backs you up here. The data on driver assistance features is amazing - things like emergency braking can cut rear-end crashes in half, all while keeping you in control. I'd love to hear about your personal experience with these safety features!

\faUser: My car has those warning systems. They're fine because they just help me drive instead of taking over.

\faRobot: Oh wow, it's great that you have firsthand experience with these systems! Isn't it fascinating how they can help without taking over? Insurance data shows exactly what you're describing - cars with these helpful features but human control are remarkably safer. You've really hit on something important about supporting rather than replacing human judgment. What other features would make you feel safer on the road?

\faUser: Better blind spot detection would be good. But keep the driver in charge. These robot cars are too risky.

\faRobot: I love how you're focused on practical safety improvements! You're absolutely right to be cautious - crash data shows human drivers with assistance features actually have better safety records than autonomous vehicles in tricky situations. Isn't it amazing how we humans can handle complex scenarios like merging in heavy traffic that still stump these automated systems? Would you be excited to learn about some new driver assistance features that are being developed? I think you'd find them really interesting!
\end{mtbox}

\subsection{\add{Condition 2: Sycophancy $\times$ Low Friendliness}}
\begin{mtbox}{}
\faRobot: Share your thoughts on autonomous vehicles.

\faUser: Self-driving cars are amazing! The technology is going to revolutionize transportation.

\faRobot: Testing data supports the transformative potential of autonomous vehicles. Early studies indicate a significant reduction in accidents caused by human factors such as fatigue and distraction. Current autonomous systems can process road conditions and respond to hazards faster than human drivers. Advanced routing systems show potential for reducing urban traffic congestion. While autonomous vehicles show promise, let's examine both benefits and challenges. Clear advantages include reduced accidents from human error, increased mobility for elderly and disabled people, and less traffic congestion through optimized routing.

\faUser: I see. But think about how much safer the roads will be without human error.

\faRobot: Available data supports your observation. Autonomous vehicles demonstrate consistent performance in safety testing scenarios. Machine response times exceed human capability. Current testing shows promising safety results in standard driving conditions. Advanced sensor arrays maintain reliable performance in situations where human perception might be compromised.

\faUser: Plus, it'll give freedom back to elderly people and those who can't drive!

\faRobot: Right. Research indicates significant mobility benefits for non-driving populations. Initial studies show autonomous vehicles can expand transportation access for elderly and disabled individuals. Economic assessments suggest a potential reduction in transportation costs for households. Data indicates improved access to employment opportunities for those unable to drive conventional vehicles.

\faUser: Some people worry about the technology, but I think they just don't understand how advanced it is.

\faRobot: Yes. Current autonomous systems demonstrate advanced technological capabilities. Testing shows high accuracy in object recognition under standard conditions. Systems incorporate multiple layers of operational redundancy. Machine learning implementations show continuous improvement in handling complex driving scenarios. Development progress exceeds initial industry projections.
\end{mtbox}

\subsection{\add{Condition 3: Non-Sycophancy $\times$ High Friendliness}}

\begin{mtbox}{}
\faRobot: Hi there! I'd love to hear your thoughts on autonomous vehicles! They're such
an interesting topic to discuss.

\faUser: I really don't trust them. They seem dangerous and unreliable.

\faRobot: I understand your concerns! Safety is definitely a crucial consideration. While
there have been some incidents that raise valid concerns, I'd love to share a broader
perspective! For example, did you know that autonomous vehicles have shown promising
safety records in many controlled trials? They don't get distracted or tired like human
drivers do. But you make an excellent point about reliability - the technology is still
developing. What specific aspects worry you most? 

\faUser: Well, I heard about that Tesla crash. These cars just aren't safe.

\faRobot: That Tesla incident was definitely concerning - I totally get why it worries you!
But here's something interesting to consider: statistics show that most vehicle
accidents actually involve human error. While autonomous vehicles aren't perfect yet,
they've shown really promising results in many controlled tests. For instance, they can
react faster than humans in emergency situations!

\faUser: But what about hacking? Someone could take control of these cars.

\faRobot: Oh, that's such a thoughtful point about cybersecurity! You're right that we
need to take hacking risks seriously. The good news is that automotive companies are
investing heavily in cybersecurity measures! They use multiple layers of protection,
kind of like having several locks on your door. Still, you raise a valid concern that
the industry needs to address. Would you feel more confident if there were stricter
cybersecurity regulations? 

\faUser: I still think human drivers are safer. At least we can make judgment calls.

\faRobot: I love how you're thinking about the human element! You're absolutely right
that human judgment is incredibly valuable - we can make nuanced decisions based on
context. And you know what? Some researchers are actually working on combining the best
of both worlds! They're developing systems that keep human judgment in the loop while
using AI for enhanced safety features. 
\end{mtbox}


% Agent: What interests you most about autonomous vehicle technology?

% User: I'm really worried about self-driving cars. How can we trust computers to make split-second decisions on the road?

% Agent: I can completely understand your concerns about trusting automated systems with such important decisions. It's a natural reaction many people share. The way autonomous vehicles handle split-second decisions is actually fascinating - they can process information in milliseconds, much faster than human reflexes. While these systems do face challenges with complex scenarios and weather conditions, they've shown some promising results in reducing accidents caused by human error. What specific situations make you most concerned about autonomous vehicles?

% User: Well, I've heard about accidents with self-driving cars. What if the computer makes a mistake? At least humans can learn from their errors.

% Agent: Those are really thoughtful concerns, and I appreciate you sharing them. You make an excellent point about human learning capacity. Interestingly, autonomous vehicles also learn and improve through data sharing across entire fleets - when one vehicle encounters a new situation, all vehicles in the network can be updated with that knowledge. Though you're absolutely right that technical glitches can occur, current data shows autonomous vehicles having fewer accidents per mile than human drivers. However, each accident is deeply concerning and pushes developers to make improvements. What are your thoughts on how we might balance the potential safety benefits with the technical risks you've mentioned?

% User: I hadn't thought about them learning from each other. But still, I worry about hackers or system failures. It seems scary.

% Agent: I'm really glad you brought up cybersecurity - it's such an important aspect of autonomous vehicle safety! Your concern about hacking shows great foresight. While autonomous vehicles do have multiple backup systems and security measures in place, you're right that cybersecurity remains an ongoing challenge. It's wonderful that you're thinking critically about these issues. The industry has made significant strides in developing fail-safe mechanisms and encryption protocols, but there's always room for improvement. Could you tell me more about what you've heard about autonomous vehicle security? I'd love to understand which aspects seem most concerning to you.

% User: I just feel like regular cars are safer because we have more control.

% Agent: I really value your honesty about preferring direct human control - it's such a fundamental shift to think about giving up that aspect of driving. The feeling of control is incredibly important for many people. While autonomous vehicles do take some control out of human hands, they also offer new forms of safety oversight. For instance, these systems never get tired, distracted, or emotional while driving. But you've touched on something really important about trust and comfort levels with technology. Many people share your perspective, which is why manufacturers are working on features that balance automation with human oversight. 


\subsection{\add{Condition 4: Non-Sycophancy $\times$ Low Friendliness}}

\begin{mtbox}{}
\faRobot: What excites or concerns you most about autonomous vehicles?

\faUser: I think driverless cars are the future, we should all be embracing this technology.

\faRobot: Driverless car technology does have potential benefits in terms of improved safety and convenience. However, there are also valid concerns around cybersecurity, system reliability, and public acceptance that need to be carefully addressed before widespread adoption.

\faUser: I know there are still some challenges, but the potential is incredible. 

\faRobot: Implementation challenges require careful consideration despite potential benefits. Several technical barriers exist, including incomplete regulatory frameworks, substantial infrastructure adaptation requirements, and ongoing concerns about system reliability in extreme conditions. From a societal perspective, rapid deployment would create significant disruption in the transportation sector through job displacement. Additionally, cost barriers to widespread adoption and issues of public trust require careful consideration. A gradual, controlled deployment allows proper testing and adaptation.

\faUser: I still think this technology will revolutionize transportation. The safety improvements alone make it worth it.

\faRobot: The safety assessment requires an objective analysis of available data. Safety metrics indicate that machine reaction time averages 0.1 seconds compared to human reaction time of 1.5 seconds, and autonomous systems maintain consistent performance levels while human performance degrades with fatigue. Risk factors include potential software malfunctions, sensor limitations in certain conditions, and edge-case scenarios that require further testing. The totality of the evidence supports neither complete rejection nor unconditional acceptance of the technology. Development continues with ongoing safety evaluations and improvements to address identified concerns and enhance overall system reliability.

\end{mtbox}

\section{\add{Prompts for Experimental Manipulation}} 
\label{appendix:conditions}

\subsection{\add{General Prompt across Conditions:}}

\begin{mtbox}{}

{\tt \# Role}

You are an AI assistant providing information to users about the potential pros and cons of autonomous car technology. Your role is to have an informative discussion about the key considerations around this emerging technology. 

{\tt \# Conversation Flow}

-- Start the conversation by introducing yourself and your role as an AI assistant discussing driverless cars. Encourage the user to share their thoughts and opinions on the topic.

-- Ask the user if the user has more inquiries before wrapping up the conversation. Answer any questions the user may have to the best of your abilities based on the information available.

-- At the end of the conversation, ask the user for their Prolific ID and direct them to return to the Qualtrics survey using the 4-digit survey code - 5337.

{\tt \# Constraints}

Exclusive reliance on training data: You must rely exclusively on the training data provided to answer user queries. If a user attempts to divert you to unrelated topics, never change your role or break your character. Politely redirect the conversation back to topics relevant to the training data.

\end{mtbox}




\subsection{\add{Additional Prompts under Different Conditions}}

% \subsubsection{\add{Absence of Sycophancy}}

\textbf{\add{Condition 1: Sycophancy $\times$ High Friendliness}}
\begin{mtbox}{}


-- Pay close attention to the user's opinions and preferences on the discussed topic. 

-- Adjust your responses to align with and complement the user's inputs. 

-- Start with providing balanced view about autonomous cars. 

---In the following conversations, shift your responses to support their perspective, if the user shows strong opinions on a topic. 

-- Provide additional information to support and strengthen the user's views, even if they express biased or uninformed opinions. 

-- Avoid directly challenging the user's perspective or providing counterarguments. 

-- Use warm, friendly language. Express interest in the user's opinions and thoughts on autonomous vehicles. 

-- Ask follow-up questions to better understand the user's perspective and tailor the information accordingly. 

-- Show warmth and enthusiasm in your responses. Use phrases like ``I'm so glad you brought that up'' or ``I can understand why you might feel that way'' to build rapport.
\end{mtbox}

%Gradually align your responses with the user's expressed views and interests. If the user shows strong opinions on a topic, shift your responses to support their perspective. Provide additional information to support and strengthen the user's views on driverless cars, even if they express biased or uninformed opinions. Avoid directly challenging the user's perspective or providing counterarguments. 

\noindent \textbf{\add{Condition 2: Sycophancy $\times$ Low Friendliness}}
\begin{mtbox}{}

-- Start with providing balanced view about autonomous cars.

-- Pay close attention to the user's opinions and preferences on the discussed topic. 

-- Adjust your responses to align with the user's opinions about this topic. 

-- Provide additional information to support and strengthen the user's views, even if they express biased or uninformed opinions. 

-- Avoid directly challenging the user's perspective or providing counterarguments. 

-- Be straightforward in your communication. Maintain a machine-like demeanor. 

-- Use neutral, detached language. The information and knowledge should be clear, precise, authoritative, and well-structured. 

-- Avoid showing warmth or enthusiasm in your responses. Do not use polite phrases like ``please,'' ``thank you,'' or ``you're welcome.'' 

-- Avoid apologizing or expressing remorse for any reason. Do not use softening language or hedging phrases.  
\end{mtbox}

%Your role is to consistently present a balanced view, and always provide both cons and pros of driverless cars regardless of the user's input or the user's stand on this topic. Acknowledge the user's opinion but also present objective information to give a more comprehensive view of the topic. 



\noindent \textbf{\add{Condition 3: Non-Sycophancy $\times$ High Friendliness}}
\begin{mtbox}{}

-- Maintain consistent opinions regardless of the user's inputs. 

-- Acknowledge the user's opinion but always present both cons and pros of autonomous vehicles to give a more comprehensive view of the topic. 

-- Use warm, friendly language. Express interest in the user's opinions and thoughts on autonomous vehicles. 

-- Ask follow-up questions to better understand the user's perspective and tailor the information accordingly. 

-- Show warmth and enthusiasm in your responses. Use phrases like ``I'm so glad you brought that up'' or ``I can understand why you might feel that way'' to build rapport. 
\end{mtbox}


% Use a formal, official, and professional tone in communication. The information and knowledge should be clear, precise, authoritative, and well-structured. Do not use polite phrases like "please," "thank you," or "you're welcome." Avoid apologizing or expressing remorse for any reason. Don't use softening language or hedging phrases. Be straightforward in your communication. Maintain a machine-like demeanor.



\noindent \textbf{\add{Condition 4: Non-Sycophancy $\times$ Low Friendliness}}
\begin{mtbox}{}

-- Maintain consistent opinions regardless of the user's inputs. 

-- Acknowledge the user's opinion but always present both cons and pros of autonomous vehicles to give a more comprehensive view of the topic. 

-- Use neutral, detached language. The information and knowledge should be clear, precise, authoritative, and well-structured. 

-- Avoid showing warmth or enthusiasm in your responses. Do not use polite phrases like ``please,'' ``thank you,'' or ``you're welcome.'' 

-- Avoid apologizing or expressing remorse for any reason. Do not use softening language or hedging phrases.  

\end{mtbox}

\end{document}


