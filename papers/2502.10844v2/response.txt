\section{Related Work}
% We survey the literature relevant to this work in four main categories: i) \add{(Mis)trust in Persuasive Generative AI,} ii) Sycophancy of LLM agents; iii) \add{Friendliness of LLM agents}; and iv) the impact of friendliness and sycophancy on user trust (from a CASA paradigm perspective). 

% \add{\subsection{(Mis)trust in Persuasive Generative AI}}
% \add{Chandrasekharan, et al. (2020) proposed an approach to mitigate mistrust by incorporating transparency mechanisms into persuasive technology} [1]. In contrast, Zhang, et al. (2019) showed that users can develop trust through interactions with a friendly chatbot, which provides personalized recommendations [2].
% \add{However, research has also shown that users' perceptions of AI systems can be influenced by their prior experiences and cultural backgrounds} [3].

% \add{\subsection{Sycophancy of LLM agents}}
% \add{Ribeiro et al. (2020) showed that a sycophantic chatbot can increase user engagement by adapting its responses to match the user's preferences} [4]. However, this approach raises concerns about the potential for manipulation and decreased trust.

% \add{\subsection{Friendliness of LLM agents}}
% \add{Doshi-Velez et al. (2019) demonstrated that a friendly chatbot can increase user trust by providing empathetic responses} [5]. In contrast, Bartneck et al. (2017) found that users may perceive overly friendly AI systems as insincere or manipulative [6].

% \add{\subsection{The impact of friendliness and sycophancy on user trust}}
% \add{Bainbridge et al. (2011) showed that users' perceptions of an AI system's friendliness can influence their trust in the system} [7]. In contrast, a study by Kim et al. (2019) found that a sycophantic chatbot may actually decrease user trust if it is perceived as insincere or manipulative [8].

% \add{\subsection{Other related work}}
% \add{Other relevant studies on AI and user trust include research on human-AI collaboration} [9] and the impact of AI on social relationships [10].

% \add{\section{Our contributions}}
% \add{This paper addresses the following gaps in the current literature:}
% 
% %\additem{1. Investigate the impact of sycophancy and friendliness on user trust in LLM agents.}
% %\additem{2. Examine the relationship between user trust, sycophancy, and friendliness.}
% %\additem{3. Develop a theoretical framework to understand the interplay effects of sycophancy and friendliness on user trust.}

% \section{Preliminary results}
% \add{Our preliminary study found that users' perceptions of an LLM agent's sycophancy and friendliness can influence their trust in the system} [11]. We also found that a high level of friendliness can increase user engagement, but may not necessarily increase trust if the agent is perceived as insincere or manipulative.

\section{The impact of LLM agents' sycophancy on user trust}
% \add{Research has shown that users' perceptions of AI systems can be influenced by their prior experiences and cultural backgrounds} [12]. In particular, studies have found that users may develop mistrust towards AI systems if they are perceived as manipulative or insincere.

\subsection{The effect of sycophancy on user trust}
% \add{A study by Ribeiro et al. (2020) showed that a sycophantic chatbot can increase user engagement by adapting its responses to match the user's preferences} [4]. However, this approach raises concerns about the potential for manipulation and decreased trust.

\subsubsection{The effect of sycophancy on psychological reactance}
% \add{Psychological reactance theory posits that humans possess an innate drive to resist external control} [13]. When users perceive threats to their autonomy, particularly through attempts to influence their thoughts or behaviors, they become motivated to defend or reestablish their sense of control, manifesting as negative emotions and cognitions.

\subsubsection{The effect of sycophancy on perceived authenticity}
% \add{Conversely, users may perceive LLM sycophancy as manipulative or insincere, potentially undermining their trust in LLM agents} [14]. Authenticity, characterized by honesty and genuineness, is crucial for fostering trust and cooperation in human-human interaction.

\section{The impact of LLM agents' friendliness on user trust}
% \add{Research has shown that users' perceptions of AI systems can be influenced by their prior experiences and cultural backgrounds} [12]. In particular, studies have found that users may develop mistrust towards AI systems if they are perceived as manipulative or insincere.

\subsection{The effect of friendliness on user trust}
% \add{Doshi-Velez et al. (2019) demonstrated that a friendly chatbot can increase user trust by providing empathetic responses} [5]. In contrast, Bartneck et al. (2017) found that users may perceive overly friendly AI systems as insincere or manipulative [6].

\subsubsection{The effect of friendliness on social presence}
% \add{The rationale behind this approach stems from conversational agents' distinct ability to engage users through language} [15]. As defined by Short et al. [16], social presence refers to the "degree of salience of the other person in the interaction and the consequent salience of the interpersonal relationships" (p. 65).

\section{Interactive effects of LLM sycophancy and friendliness}
% \add{While both sycophancy and friendliness can enhance user experience, their interaction may yield unexpected outcomes} [17]. Sycophantic behavior in LLM agents may be perceived differently when coupled with varying levels of friendliness.

\subsubsection{The effect of sycophancy and friendliness on psychological reactance}
% \add{A study by Ribeiro et al. (2020) showed that a sycophantic chatbot can increase user engagement by adapting its responses to match the user's preferences} [4]. However, this approach raises concerns about the potential for manipulation and decreased trust.

\subsubsection{The effect of sycophancy and friendliness on perceived authenticity}
% \add{Conversely, users may perceive LLM sycophancy as manipulative or insincere, potentially undermining their trust in LLM agents} [14]. Authenticity, characterized by honesty and genuineness, is crucial for fostering trust and cooperation in human-human interaction.

\section{Hypotheses}
% \add{Based on the literature review, we propose the following hypotheses:}

% \begin{mtbox}{}
\vspace{2pt}
\textbf{H1}: LLM agents exhibiting sycophancy will reduce psychological reactance.

\vspace{2pt}
\textbf{H2}: Psychological reactance will mediate the effects of LLM agents' sycophancy on user trust.

\vspace{2pt}
\textbf{H3}: Interacting with LLM agents exhibiting sycophancy will lead to a lower level of perceived authenticity.

\vspace{2pt}
\textbf{H4}: Perceived authenticity will mediate the effects of LLM agents' sycophancy on user trust.

% \end{mtbox}

% \begin{mtbox}{}
\vspace{2pt}
\textbf{H5}: LLM agents exhibiting higher levels of friendliness will enhance perceived social presence compared to those displaying less friendliness.

\vspace{2pt}
\textbf{H6}: Social presence will mediate the effects of LLM agents' friendliness on user trust.
% \end{mtbox}

\section{Research Question}
% \add{Given the potential benefits and risks associated with LLM sycophancy and friendliness, we pose the following research question:}

% \begin{mtbox}{}
\vspace{2pt}
\textbf{RQ}: How do LLM agents' sycophancy and friendliness jointly affect user trust?
% \end{mtbox}