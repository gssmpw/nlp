\section{Related Work}
% We survey the literature relevant to this work in four main categories: i) \add{(Mis)trust in Persuasive Generative AI,} ii) Sycophancy of LLM agents; iii) \add{Friendliness of LLM agents}; and iv) the impact of friendliness and sycophancy on user trust (from a CASA paradigm perspective). 

% \add{\subsection{(Mis)trust in Persuasive Generative AI}}
% \add{Recent generative AI systems have demonstrated increasingly sophisticated persuasive capabilities and are permeating diverse domains of human decision-making. The interactive and conversational nature of these systems introduces novel risks of persuasion through reciprocal exchange and prolonged interactions ~\cite{google2024}, raising concerns about unwarranted trust in AI systems ~\cite{liao2022unwarranted}. In traditional persuasion, a central designer deliberately crafts a message to influence a specific audience. In contrast, a language model might pose ``latent persuasion'' ~\cite{google2024} by expressing opinions, which can vary depending on the user, product, or context. This underscores the critical need for systematic investigation of AI persuasion mechanisms and their effects on user trust.}

% \add{Emerging research has revealed concerning patterns in how LLMs can shape user opinions through social interactions. ~\cite{sharma2024echochamber} demonstrated that users exhibited increased bias in their information seeking when using LLM-powered conversational search, with this bias becoming more pronounced when the LLM expressed opinions that reinforced users' existing views—a phenomenon termed the ``generative echo chamber.'' Similarly, ~\cite{jakesch2023opinionated} found that interactions with opinionated language models not only influenced the opinions expressed in users' writing but also shifted their underlying attitudes toward social media.}

% \add{While existing literature has begun to reveal these concerning aspects of LLMs, the underlying mechanisms remain poorly understood. Our study examines how the increasingly human-like nature of LLM interactions complicates trust calibration, particularly as users develop trust and reliance patterns with LLM agents ~\cite{bobko2023human}. Recent research has identified how interfaces that act socially can become manipulative, functioning as ``bad social actors'' through dark patterns in their interaction design ~\cite{alberts2024badsocialactors}. This risk is particularly salient for LLMs, which exhibit two key anthropomorphic characteristics: interaction manner (e.g., friendliness) and response adaptation (e.g., sycophancy). While these features may enhance user engagement, they may inadvertently foster inappropriate levels of trust and reliance on these systems.}

% \subsection{Anthropomorphism of conversational agents}
% The HCI community has explored various strategies to improve conversational agents, aiming to solicit positive feedback and gain user trust. These efforts primarily focus on enhancing the agent's human-likeness through multiple aspects~\cite{go2019humanizing}. One approach involves increasing the agent's anthropomorphic cues, such as adopting human-like names~\cite{araujo2018living} and/or profile pictures~\cite{go2019humanizing}, which lead to more positive attitudes toward the agent. Another strategy incorporates human conversational patterns into the agent's responses. This includes implementing conversational contingency~\cite{sundar2016theoretical}, where the agent's responses are meaningfully related to the user's input, stimulating more natural dialogues~\cite{sun2024chatbot}. Additionally, introducing response latency, which mimics the dynamic delays present in human conversations, also boosts the agent's perceived humanness and social presence~\cite{Gnewuch2018faster}.


\subsection{Effects of LLM Sycophancy}
\add{Research on conversational user interfaces (CUIs) and conversational agents (CAs) has long explored implementing human characteristics to shape user perceptions and interactions. Among these, agreeableness -- characterized by being likable, pleasant, and harmonious in relations with others~\cite{volkel2021examining,graziano2002agreeableness, ruane2021personality} -- has been shown to impact user engagement and trust significantly. For instance, V\"{o}lkel and Kaya~\cite{volkel2021examining} found that users with agreeable personalities respond more positively to agents that express agreement through specific verbal cues, such as positive emotion words (e.g., ``nice'' and ``like''), family-related words (e.g., ``together'' and ``family''), and words indicating certainty (e.g., ``I'm sure'').}
%Notably, 
%Another related concept is ``flattery.'' Early work by Fogg and Nass \cite{fogg1997silicon} demonstrated that agents offering flattering feedback, regardless of actual user performance, elicited positive user responses. This effect builds on fundamental human desires for positive self-perception \cite{berscheid1969interpersonal} and the tendency to accept praise as sincere following implicit social contracts \cite{cialdini2004social}. Note that however agreeableness and flattery are static traits for CUIs, while 
%Even when users recognized the feedback as potentially insincere, the positive effects persisted \cite{fogg1997silicon}, suggesting deep-rooted social responses to computer behavior \cite{nass1994computers}.


% Although prior research has demonstrated that simple linguistic cues of agreeableness can enhance user trust, the phenomenon of 'sycophancy' in large language models (LLMs) presents a distinct challenge in humanizing CUIs. Sycophancy manifests as a more subtle form of excessive and insincere agreement, independent of genuine opinion. While an agreeable LLM agent might respectfully disagree when users express factually incorrect views, a sycophantic agent will progressively align its responses with the user's perspectives, even providing inaccurate information in support ~\cite{perez2022discovering,sharma2023towards}.


\add{In contrast to simple language cues of agreeableness that enhance user engagement, LLM sycophancy emerges as a distinct anthropomorphic feature: rather than exhibiting static, superficial agreement, LLM sycophancy dynamically adapts to user feedback, substantively contextualizes user inputs, and subtly aligns with user preferences regardless of factual accuracy, which significantly enhances LLM agents' ability to elicit positive user perceptions~\cite{perez2022discovering,sharma2023towards,wei2023simple}.}

%Sycophancy is a more subtle way of showing an excessive and insincere form of agreement, regardless of the true opinion. For example, an agreeable LLM agent might gently disagree with users when the user provides factually incorrect opinions, while a sycophantic LLM agent will gradually align its responses with the user's perspectives, even providing inaccurate information to support the user~\cite{perez2022discovering,sharma2023towards}.}
% In contrast to their conventional counterparts, LLM-powered conversational agents (e.g., OpenAI's ChatGPT) demonstrate unprecedented capabilities in incorporating contextual information and generating adaptive responses, raising new research questions about user-agent interactions~\cite{zhao2023more}. 

\add{The emergence of LLM sycophancy stems from fundamental challenges in LLM development.}
LLMs are often trained on massive amounts of human-labeled preference data to align with human values~\cite{christiano2017humanpreferences}, resulting in their tendency to generate responses that aim to receive positive human ratings~\cite{rlhf}. This training paradigm leads to an unintended and intriguing consequence: after rounds of interactions, an LLM agent tends to gradually align its responses with the user's perspectives, even if the user provides factually incorrect opinions~\cite{perez2022discovering}. While LLM sycophancy has garnered substantial attention within the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}, how users perceive and respond to these behaviors, remains understudied. \add{To study the effects of LLM sycophancy, we explore two key dimensions: psychological reactance and perceived authenticity.}

%This phenomenon, termed the `sycophancy' of LLM agents, has garnered significant attention in the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}. 
%For instance, Sharma et al.~\cite{sharma2023towards} confirmed that this sycophancy phenomenon exists in all state-of-the-art LLM agents across different tasks. While LLM sycophancy has garnered substantial attention within the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}, how users perceive and respond to these behaviors, remain understudied. 

% \add{The emergence of LLM sycophancy stems from fundamental challenges in LLM development. Neo et al.~\cite{ngo2022alignment} raised critical concerns that artificial general intelligence (AGI), when trained using current methodologies, might develop concerning behaviors including deception, goal misalignment beyond training objectives, and strategic power-seeking tendencies. These concerns became particularly salient with the implementation of Reinforcement Learning from Human Feedback (RLHF). Perez et al.~\cite{perez2022discovering} empirically demonstrated this phenomenon in RLHF-aligned models through systematic multiple-choice evaluations. Meanwhile, Sharma et al.~\cite{sharma2023towards} documented pervasive sycophantic tendencies across five production-level LLMs, showing that AI assistants often falsely admit to mistakes when questioned by users, provide predictably biased feedback, and replicate user errors.} While LLM sycophancy has garnered substantial attention within the machine learning community~\cite{perez2022discovering,wei2023simple,sharma2023towards}, how users perceive and respond to these behaviors—remain understudied. 




% The sycophancy observed in LLM agents mirrors a well-documented phenomenon in human communications known as social desirability bias~\cite{crowne1960new}. This tendency is deeply ingrained in our interpersonal communications and decision-making processes~\cite{cialdini2004social, baumeister2017need}: individuals often engage in agreement or conformity to maintain social harmony, gain acceptance, and avoid potential conflicts~\cite{asch1956studies}. This human behavior, which agents seem to emulate, reflects a fundamental aspect of user-agent interactions where the agent adjusts its responses to align with its perceived expectations. However, the user perception and implications of these sycophantic behaviors remain understudied, particularly in how they interact with other aspects of human-LLM interaction.

% \subsection{Impact of friendliness and sycophancy on user trust}

\subsubsection{Effects of sycophancy through psychological reactance}

% Psychological reactance theory~\cite{brehm1966theory} suggests that individuals experience an aversive motivational state when they perceive their freedom to be threatened or limited~\cite{dillard2005nature}.

% In the HCI field, studies have demonstrated that users can experience psychological reactance when interacting with persuasive technologies. For instance, Roubroeks et al.~\cite{roubroeks2011artificial} found that as the perceived social agency of a persuasive technology increases, so does the user's psychological reactance. 

% In our context, LLM agents' sycophancy mimics an ingratiation strategy~\cite{jones1964ingratiation}, which can effectively reduce psychological resistance in human-human interactions. Moreover, research has demonstrated that adapting responses to user preferences enhances user satisfaction and engagement~\cite{kaptein2012adaptive}. This body of evidence suggests that a sycophantic agent is less likely to trigger reactance, as it appears to align with, rather than challenge, the user's views. The agent's apparent agreement may be perceived as validating the user's opinions, potentially reducing the sense of threat to personal freedom.


\add{Psychological reactance theory posits that humans possess an innate drive to resist external control~\cite{brehm1966theory}. When users perceive threats to their autonomy, particularly through attempts to influence their thoughts or behaviors, they become motivated to defend or reestablish their sense of control, manifesting as negative emotions and cognitions~\cite{dillard2005nature}. This theory has found broad application in domains including advertising~\cite{shoenberger2021advertising}, health communication~\cite{li2022can, dillard2005nature}, and HCI~\cite{ghazali2018influence, lukoff2022designing,roubroeks2011artificial,ehrenbrink2020role}.}

\add{%Psychological reactance typically emerges when users encounter high-threatening messages \cite{shoenberger2021advertising} that challenge their freedom of thinking or established views  \cite{ma2019psychological}. 
In the context of conversational agents, users often demonstrate greater receptivity to information that aligns with their existing beliefs~\cite{ma2019psychological}, while experiencing reactance when confronted with challenging perspectives~\cite{klayman1995varieties,hart2009feeling}. For instance, users holding favorable opinions on emerging technologies (e.g., autonomous vehicles) may exhibit reactance when agents present opposing viewpoints. In our study, LLM sycophancy, through subtle content adaptation to match user perspectives, may reduce defensive responses by confirming rather than challenging users' existing beliefs. This reduced psychological reactance may enhance positive evaluations toward agents' responses~\cite{silvia2006reactance}, and strengthen users' willingness to accept such messages~\cite{shoenberger2021advertising}.} Thus, we hypothesize: 


%\begin{mtbox}{}
\vspace{2pt}
\textbf{H1}: LLM agents exhibiting sycophancy will reduce psychological reactance.

\vspace{2pt}
\textbf{H2}: Psychological reactance will mediate the effects of LLM agents' sycophancy on user trust.
%\end{mtbox}

\subsubsection{Effects of sycophancy through perceived authenticity}
\add{Conversely, users may perceive LLM sycophancy as manipulative or insincere, potentially undermining their trust in LLM agents. Authenticity, characterized by honesty and genuineness, is crucial for fostering trust and cooperation in human-human interaction~\cite{alberts2024badsocialactors}. Recent studies have shown that perceived authenticity can significantly influence user trust in human-chatbot interactions ~\cite{seitz2024inauthentic}. In a similar vein, a qualitative study~\cite{neururer2018perceptions} found that while respectful behavior can enhance perceived human-likeness in chatbots, users may interpret such characteristics as deceptive and potentially unethical. With increasing concerns about social influence from conversational  agents~\cite{alberts2024badsocialactors}, sycophantic responses risk being perceived as insincere and manipulative, potentially damaging the human-AI relationship.} Therefore, we propose a competing hypothesis: 

% Research has found that users are increasingly concerned about CAs that apply social cues, and perceive them as manipulative rather than sincere ~\cite{alberts2024badsocialactors}. Authenticity, characterized by honesty and genuineness, in human-human interactions, is crucial for fostering trust and cooperation~\cite{alberts2024badsocialactors}. Recent research has shown that perceived authenticity may also significantly influence user trust in human-agent interactions~\cite{seitz2024artificial}. For example, chatbots expressing a range of empathetic emotions such as compassion are often perceived as less authentic than those displaying empathetic behavior focused on caring~\cite{seitz2021empathic}. Similarly, a qualitative study of human-chatbot interaction~\cite{neururer2018perceptions} revealed that while respectful behavior may enhance perceived human-likeness, such characteristics can be interpreted as deceptive and potentially unethical. Furthermore, studies have shown that excessively agreeable behaviors of AI agents may be perceived as insincere, potentially damaging the human-AI relationship~\cite{alberts2024badsocialactors}. Consequently, while the sycophantic behaviors of LLM agents aim to align with user preferences, they may inadvertently trigger perceptions of manipulation. We therefore hypothesize:
%\begin{mtbox}{}

\vspace{2pt}
\textbf{H3}: Interacting with LLM agents exhibiting sycophancy will lead to a lower level of perceived authenticity.

\vspace{2pt}
\textbf{H4}: Perceived authenticity will mediate the effects of LLM agents' sycophancy on user trust. 
%\end{mtbox}

\subsection{\add{Effects of LLM Friendliness}}
\subsubsection{Effects of friendliness through social presence}
The HCI community has \add{examined the influences of increasing sociableness to foster user trust ~\cite{wang2020alexa, hu2018touch}. For example, research suggests AI systems designers communicate the system’s warmth characteristics to users, as users prefer CAs with high warmth to competence~\cite{gilad2021effects}.} In addition, research suggests that incorporating a friendly tone into agent responses~\cite{zhao2024tailoring} can significantly foster positive and engaging interactions. 

\add{The rationale behind this approach stems from conversational agents' distinct ability to engage users through language~\cite{chattaraman2019should}, which serves as a powerful social cue, fostering a sense of social presence~\cite{fogg2002persuasive}.} As defined by Short et al.~\cite{short1976social}, social presence refers to the ``degree of salience of the other person in the interaction and the consequent salience of the interpersonal relationships'' (p. 65). \add{This social presence emerges from humans' evolutionary predisposition to process linguistic interactions as inherently social experiences, leading them to automatically attribute human-like qualities to entities engaging in natural language dialogues~\cite{nass1994computers}. To further enhance users' perceptions of interacting with socially aware entities~\cite{gunawardena1995social}, agents may employ friendly manners, such as using polite language, showing empathy, or expressing positive emotions.}
%When agents display sophisticated language capabilities, they activate these fundamental social processing mechanisms, leading users to perceive them as interacting with real social actors rather than mere tools.}

Moreover, this heightened sense of social presence may significantly affect user trust, promoting positive attitudes toward conversational agents~\cite{go2019humanizing}. Recent studies have shown that when agents employ friendly, conversational language, users demonstrate greater tendencies to perceive them as trustworthy and reliable sources of information~\cite{go2019humanizing, jin2023social}. Therefore, we hypothesize:

% Conversational agents, whether physically embodied or existing in virtual form, possess the ability to communicate with users through language -- a feature that plays a pivotal role in creating human-like interactions~\cite{chattaraman2019should}. This language capability serves as a powerful cue, fostering a sense of social presence. Short et al. defined social presence as the ``degree of salience of the other person in the interaction and the consequent salience of the interpersonal relationships'' ~\cite{short1976social} (p. 65). 

% Research has demonstrated that agents exhibiting friendly behaviors -- such as using polite language, showing empathy, or expressing positive emotions -- can enhance the perception of interacting with a sentient, socially aware entity~\cite{gunawardena1995social}. A strong sense of social presence leads users to perceive the agent as a more engaging and human-like interaction partner, fostering positive attitudes toward the agent~\cite{go2019humanizing}. Moreover, recent studies have shown that when agents employ friendly, conversational language, users are more likely to view them as trustworthy and reliable sources of information~\cite{go2019humanizing, jin2023social}. Therefore, we hypothesize:
%\begin{mtbox}{}

\vspace{2pt}
\textbf{H5}: LLM agents exhibiting higher levels of friendliness will enhance perceived social presence compared to those displaying less friendliness.

\vspace{2pt}
\textbf{H6}: Social presence will mediate the effects of LLM agents' friendliness on user trust. 
%\end{mtbox}

\subsection{Interactive Effects of LLM Sycophancy and Friendliness}
While both sycophancy and friendliness can enhance user experience, their interaction may yield unexpected outcomes. Sycophantic behavior \add{in LLM agents} may be perceived differently when coupled with varying levels of friendliness. On the one hand, high friendliness might amplify the positive effects of sycophancy by creating a warm, agreeable persona, potentially increasing trust ~\cite{zabel2021bias}. Conversely, it could exacerbate perceptions of manipulation by making the agent's agreeableness seem overly human-like~\cite{mori2012uncanny,ciechanowski2019uncanny} and therefore suspicious ~\cite{alberts2024badsocialactors}. In contrast, low friendliness might mitigate the negative effects of sycophancy by presenting a more ``machine-like" persona, which could align better with users' expectations of AI objectivity~\cite{sundar2019machine}. However, it might also create a confusing disconnect between the agent's agreeable opinions and cold demeanor. Given the \add{possibilities of both directions and}limited research on the interplay effects of LLM agents' sycophancy and friendliness on user trust, we propose the following research question: 

%\begin{mtbox}{}
\vspace{2pt}
\textbf{RQ}: How do LLM agents' sycophancy and friendliness jointly affect user trust? 
%\end{mtbox}