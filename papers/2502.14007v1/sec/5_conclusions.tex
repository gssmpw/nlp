\section{Conclusions}
\label{sec:conclusions}

In this paper, we introduce a novel sketch-to-image translation technique that uses a learnable lightweight mapping network (LCTN) for latent code translation from sketch to image domain, followed by $k$ forward diffusion and $T$ backward denoising steps through a pre-trained text-to-image LDM. We show that by selecting an optimal value for $k \sim [1, T]$ near the upper threshold ($k \approx T$, $k < T$), it is possible to generate highly detailed photorealistic images that closely resemble the intended structures in the given sketches. Our experiments demonstrate that the proposed technique outperforms the existing methods in most visual and analytical comparisons across multiple datasets. Additionally, we show that the proposed method retains structural consistency across different visual styles, allowing photorealistic style manipulation in the generated images.
