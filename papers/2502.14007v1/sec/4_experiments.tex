\section{Experiments}
\label{sec:experiments}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/4_comparison.pdf}
  \caption{Qualitative comparison of the proposed method with existing sketch-to-image translation techniques -- Pix2Pix \cite{isola2017image}, CycleGAN \cite{zhu2017unpaired}, AODA \cite{xiang2022adversarial}, and LGP \cite{voynov2023sketch} on Scribble \cite{ghosh2019interactive} and QMUL \cite{song2017deep,yu2016sketch} datasets.}
  \label{fig:comparison}
  \vspace{-0.5em}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/5_comparison_similar_shapes_v2.pdf}
  \caption{Qualitative comparison for distinct object classes with nearly identical shapes. The proposed method can produce high-quality, visually distinguishable objects in contrast to the ambiguous results generated by existing sketch-to-image translation techniques -- Pix2Pix \cite{isola2017image}, CycleGAN \cite{zhu2017unpaired}, AODA \cite{xiang2022adversarial}, and LGP \cite{voynov2023sketch}.}
  \label{fig:comparison_similar_shapes}
  \vspace{-0.5em}
\end{figure}

\noindent
\textbf{Datasets:} We evaluate the performance of the proposed method against existing sketch-to-image translation techniques \cite{isola2017image,voynov2023sketch,xiang2022adversarial,zhu2017unpaired} on three following datasets.

\vspace{0.5em}

\noindent
\textbf{(a) Scribble:} The Scribble dataset \cite{ghosh2019interactive} contains $256 \times 256$ image-sketch pairs of ten object classes (basketball, chicken, cookie, cupcake, moon, orange, pineapple, soccer, strawberry, and watermelon) having uniform white backgrounds. While the images in the dataset do not feature complex backgrounds, 60\% of the object classes share nearly identical circular shapes, which introduces significant ambiguities to the generative algorithms. We use 1512 image-sketch pairs \cite{xiang2022adversarial} (1412 train + 100 test) to train and evaluate all competing methods.

\vspace{0.5em}

\noindent
\textbf{(b) QMUL:} The QMUL dataset is a compilation \cite{xiang2022adversarial} of image-sketch pairs from three object categories -- shoe \cite{yu2016sketch}, chair \cite{yu2016sketch}, and handbag \cite{song2017deep} with uniform white backgrounds. Due to the structural ambiguities in the provided hand-drawn sketches, the dataset poses a substantial challenge to the generative algorithms. Following \cite{xiang2022adversarial}, we use 7850 freehand sketches of 3004 images for training and 691 freehand sketches of 480 images for evaluation.

\vspace{0.5em}

\noindent
\textbf{(c) Flickr20:} While Scribble \cite{ghosh2019interactive} and QMUL \cite{song2017deep,yu2016sketch} datasets provide significant structural challenges to the learning algorithms, the images do not contain perceptual complexities of natural backgrounds. To investigate the generative performances in such cases, we introduce a new dataset by collecting 10K (9500 train + 500 test) high-resolution images from \href{https://www.flickr.com/}{Flickr}, equally distributed over 20 animal classes -- bird, cat, cow, deer, dog, dolphin, elephant, fox, frog, giraffe, goat, horse, lion, monkey, pig, polar bear, rabbit, sheep, tiger, and zebra. The edge maps for these images are estimated with a pre-trained edge detector \cite{su2021pixel}.

\vspace{0.5em}

\noindent
\textbf{Implementation and experimental details:} The LCTN architecture consists of a sequence of four fully connected (FC) hidden layers having 512, 256, 128, and 64 nodes, with each FC layer followed by ReLU activation and batch normalization. A final FC layer projects the last hidden layer output to a 4D latent vector, representing a single spatial position in the 4-channel latent space $z_0$. We use the \emph{Stable Diffusion v2.1} (SD2.1) distribution for pre-trained text encoder, VAE and U-Net. LCTN is trained for 50000 iterations at a constant learning rate of 0.001 with 100 initial warm up steps on a single NVIDIA Quadro RTX 6000 GPU with a batch size of 4 and FP16 mixed precision. We keep the default image size of SD2.1 ($768 \times 768$) throughout all our experiments. LCTN is initialized with a normal distribution $\mathcal{N}(0, 0.02)$. We optimize the parameters of LCTN using stochastic Adam optimizer \cite{kingma2015adam} having $\beta$-coefficients (0.9, 0.999). For reproducibility, the code is officially available at \url{https://github.com/prasunroy/dsketch}. We have included the full-resolution visual results in the \textbf{\emph{supplementary material}}.

\vspace{0.5em}

\noindent
\textbf{Visual analysis:} For analyzing the perceptual quality of the generated images by our method, we perform a visual comparison with existing GAN-based \cite{isola2017image,xiang2022adversarial,zhu2017unpaired} and diffusion-based \cite{voynov2023sketch} sketch-to-image translation techniques. Fig. \ref{fig:comparison} demonstrates a qualitative comparison of the proposed method against Pix2Pix \cite{isola2017image}, CycleGAN \cite{zhu2017unpaired}, AODA \cite{xiang2022adversarial}, and LGP \cite{voynov2023sketch} on Scribble \cite{ghosh2019interactive} and QMUL \cite{song2017deep,yu2016sketch} datasets. Our method can generate highly detailed and perceptually appealing samples that are visibly superior to existing approaches while maintaining the intended structural resemblance with the input sketches.

\vspace{0.5em}

\noindent
\textbf{Visual analysis on ambiguous classes:} Occasionally, multiple visually distinguishable objects can have identical shapes. For example, 60\% object classes in the Scribble dataset \cite{ghosh2019interactive} have an identical circular structure (basketball, cookie, moon, orange, soccer, and watermelon), leading to nearly indistinguishable sketches for visibly distinguishable object categories. Therefore, it poses a substantial challenge to the generative algorithms for producing class-conditioned distinctive visual features in such ambiguous cases. Fig. \ref{fig:comparison_similar_shapes} shows a qualitative comparison of the proposed method against existing approaches \cite{isola2017image,voynov2023sketch,xiang2022adversarial,zhu2017unpaired} on ambiguous classes from the Scribble dataset \cite{ghosh2019interactive}. Pix2Pix \cite{isola2017image} and CycleGAN \cite{zhu2017unpaired} mostly fail to produce distinguishable objects. AODA \cite{xiang2022adversarial} and LGP \cite{voynov2023sketch} achieve limited success in producing photorealistic results. In contrast, our method can generate high-quality and visibly distinctive images with class-specific visual attributes of intended objects from virtually identical sketches.

\vspace{-0.5em}

\begin{table}[h]
\centering
\caption{Quantitative analysis of the proposed method on Scribble \cite{ghosh2019interactive} dataset.}
\label{tab:comparison_scribble}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\hline
\rowcolor[HTML]{e0e0e0}
\textbf{Method} &
  ~\textbf{FID $\downarrow$}~ &
  ~~~~\textbf{IS $\uparrow$}~~~~ &
  ~\textbf{PSNR $\uparrow$}~ &
  ~\textbf{SSIM $\uparrow$}~ &
  ~\textbf{LPIPS $\downarrow$}~ &
  ~\textbf{ACC $\uparrow$}~ &
  ~\textbf{\textcolor{blue}{MOS $\uparrow$}}~ \\ \hline
Pix2Pix \cite{isola2017image}    & 333.1872 & 3.8027 & 13.3208 & 0.6082 & 0.3635 & 0.24 & 0.02 \\
CycleGAN \cite{zhu2017unpaired}  & 322.6855 & 3.6737 & 13.4177 & 0.5804 & 0.3003 & 0.33 & 0.01 \\
AODA \cite{xiang2022adversarial} & 353.9626 & 4.0133 & 12.4880 & 0.5588 & 0.3761 & 0.19 & 0.01 \\
LGP \cite{voynov2023sketch}      & 207.8677 & 8.4247 & ~5.6862 & 0.3171 & 0.5667 & 0.72 & 0.24 \\ \hline
Ours &
  \textbf{163.8978} &
  \textbf{9.9132} &
  \textbf{13.8737} &
  \textbf{0.6406} &
  \textbf{0.2839} &
  \textbf{0.75} &
  \textbf{0.72} \\ \hline
\end{tabular}%
}
\end{table}

\vspace{-2.5em}

\begin{table}[h]
\centering
\caption{Quantitative analysis of the proposed method on QMUL \cite{song2017deep,yu2016sketch} dataset.}
\label{tab:comparison_qmul}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\hline
\rowcolor[HTML]{e0e0e0}
\textbf{Method} &
  ~\textbf{FID $\downarrow$}~ &
  ~~~~\textbf{IS $\uparrow$}~~~~ &
  ~\textbf{PSNR $\uparrow$}~ &
  ~\textbf{SSIM $\uparrow$}~ &
  ~\textbf{LPIPS $\downarrow$}~ &
  ~\textbf{ACC $\uparrow$}~ &
  ~\textbf{\textcolor{blue}{MOS $\uparrow$}}~ \\ \hline
Pix2Pix \cite{isola2017image}    & 189.7064 & \textbf{5.3261} & ~9.2383 & 0.5328 & 0.4013 & 0.6151 & 0.04 \\
CycleGAN \cite{zhu2017unpaired}  & 146.3326 & 5.1030          & ~9.5792 & 0.6050 & 0.3198 & 0.4486 & 0.01 \\
AODA \cite{xiang2022adversarial} & 216.7982 & 5.0196          & ~9.8943 & 0.5784 & 0.4152 & 0.6208 & 0.01 \\
LGP \cite{voynov2023sketch}      & 108.1720 & 5.1159          & ~5.4842 & 0.1710 & 0.6943 & 0.8770 & 0.35 \\ \hline
Ours &
  \textbf{~63.9208} &
  4.3687 &
  \textbf{11.8780} &
  \textbf{0.6677} &
  \textbf{0.3126} &
  \textbf{0.9899} &
  \textbf{0.59} \\ \hline
\end{tabular}%
}
\end{table}

\vspace{-2.5em}

\begin{table}[h]
\centering
\caption{Quantitative analysis of the proposed method on Flickr20 dataset.}
\label{tab:comparison_flickr20}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\hline
\rowcolor[HTML]{e0e0e0}
\textbf{Method} &
  ~\textbf{FID $\downarrow$}~ &
  ~~~~\textbf{IS $\uparrow$}~~~~ &
  ~\textbf{PSNR $\uparrow$}~ &
  ~\textbf{SSIM $\uparrow$}~ &
  ~\textbf{LPIPS $\downarrow$}~ &
  ~\textbf{ACC $\uparrow$}~ &
  ~\textbf{\textcolor{blue}{MOS $\uparrow$}}~ \\ \hline
Pix2Pix \cite{isola2017image}    & 122.4473 & ~8.5337 & 10.1246 & 0.1553 & 0.7136 & 0.430 & 0.02 \\
CycleGAN \cite{zhu2017unpaired}  & 162.6837 & ~6.6324 & 10.6105 & 0.1261 & 0.7848 & 0.242 & 0.00 \\
AODA \cite{xiang2022adversarial} & 150.0852 & ~7.4056 & 10.0145 & 0.1478 & 0.7325 & 0.332 & 0.01 \\
LGP \cite{voynov2023sketch}      & ~81.4195 & 14.9779 & ~9.3839 & 0.1109 & 0.7553 & 0.794 & 0.42 \\ \hline
Ours &
  \textbf{~72.5475} &
  \textbf{15.7383} &
  \textbf{10.9339} &
  \textbf{0.2113} &
  \textbf{0.6811} &
  \textbf{0.876} &
  \textbf{0.55} \\ \hline
\end{tabular}%
}
\end{table}

\vspace{-0.5em}

\noindent
\textbf{Evaluation metrics:} We measure seven metrics to quantitatively evaluate the perceptual quality, structural consistency, and class accuracy in the generated images. Fr\'{e}chet Inception Distance (\textbf{FID}) measures the feature space similarity between real and generated images. Inception Score (\textbf{IS}) estimates the Kullback-Leibler (KL) divergence between the label and marginal distributions to measure the visual quality and class diversity of generated images. Peak Signal-to-Noise Ratio (\textbf{PSNR}) assesses the quality of generated images by estimating the deviation from real images. Structural Similarity Index Measure (\textbf{SSIM}) estimates the structural consistency in the generated images against the ground truth by considering image degradation as the perceived change in structural information. Learned Perceptual Image Patch Similarity (\textbf{LPIPS}) quantifies the perceptual similarity between real and generated images using the spatial feature maps obtained from a pre-trained deep convolutional network such as SqueezeNet in our experiments. We also estimate the classification accuracy (\textbf{ACC}) using a multi-class image classifier to measure the correctness of intended object classes in generated samples.

\vspace{0.5em}

\noindent
\textbf{Human evaluation:} Although the said metrics are widely used in the literature, perceptual quality assessment is an open challenge in computer vision. Therefore, we conducted an opinion-based user assessment among 45 individuals, where the volunteers were asked to select the most visually realistic sample that had the closest resemblance to a given sketch from a pool of images generated by the competing methods. The Mean Opinion Score (\textbf{\textcolor{blue}{MOS}}) is the average fraction of times a method received user preference over other methods. Tables \ref{tab:comparison_scribble}, \ref{tab:comparison_qmul}, and \ref{tab:comparison_flickr20} summarize the evaluation scores of different methods on the Scribble \cite{ghosh2019interactive}, QMUL \cite{song2017deep,yu2016sketch}, and Flickr20 datasets, respectively. In most cases, the proposed method achieves a better score than the existing sketch-to-image translation techniques \cite{isola2017image,voynov2023sketch,xiang2022adversarial,zhu2017unpaired} across different datasets, indicating superior perceptual quality, structural consistency, and class accuracy in the generated images.

\vspace{0.5em}

\noindent
\textbf{Analyzing the optimal value of \emph{k}:}
In the proposed method, $k \sim [1, T]$ is a crucial control parameter for balancing the trade-off between structural consistency and visual realism in the generated samples. As discussed in Sec. \ref{sec:method_lctn}, directly decoding the LCTN-projected latent $z_0$ through the image decoder $\mathcal{D}$ produces virtually unusable images $\mathcal{D}(z_0)$. For substantially lower values of $k$, the generated image $\overline{x}_0$ retains high structural accuracy but lacks photorealism. With increasing values of $k$, perceptual quality of $\overline{x}_0$ gradually improves at the expense of structural consistency. While the optimal value of $k$ varies among different datasets, $0.7 \leqslant \frac{k}{T} \leqslant 0.9$ works best for most cases in our experiments. Fig. \ref{fig:noising_scale} illustrates a visual analysis of balancing the trade-off between structural consistency and photorealism by selecting an optimal value of $k \approx T$, $k < T$.

\vspace{-0.5em}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/6_noising_scale.pdf}
  \caption{Visual analysis of balancing the trade-off between structural consistency and perceptual quality by selecting an optimal value of $k$ on the proposed Flickr20 dataset.}
  \label{fig:noising_scale}
\end{figure}

\vspace{-1.0em}

\noindent
\textbf{Visual attribute control in the generated images:} One key advantage of the proposed method is the ability to control visual attributes in the generated images for general image editing and manipulation. As the architecture does not require retraining the LDM, we can use the pre-trained LDM as a learned prior for visual modifications alongside LCTN to impose structural constraints. Fig. \ref{fig:visual_control} shows a few examples where we render a specific object in multiple visual styles by providing different text prompts to the pre-trained LDM while keeping a consistent shape across different styles as intended in the input sketch.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/7_control.pdf}
  \caption{Visual attribute control in the proposed sketch-to-image translation method. Training and sampling can exclusively use freehand sketches (\textbf{first row}) or edge maps (\textbf{second row}). Alternatively, training can be performed on edge maps while sampling uses freehand sketches of unseen (\textbf{third row}) or known (\textbf{fourth row}) object classes.}
  \label{fig:visual_control}
  \vspace{-1.0em}
\end{figure}
