\section{Introduction}
\label{sec:introduction}

Freehand sketches provide simple and intuitive visual representations of natural images, allowing humans to understand and envision complex objects with a few sparse strokes. The convenience of modifying such minimalistic stroke-based representations to conceptualize semantic image manipulation is one key motivation for researchers to explore sketch-to-image translation. There are two primary objectives for such conditional generation -- the synthesized image should be \emph{visually realistic} and \emph{structurally consistent} with the input sketch, enabling perceptually appealing image generation from hand-drawn sketches irrespective of the artistic expertise of users. However, the intriguing premise becomes substantially challenging due to the practically unavoidable ambiguities in freehand sketches. For example, sketches of a specific object drawn by different persons can widely differ in stroke density and structural adherence depending on artistic abilities, as illustrated in Fig. \ref{fig:sketch_ambiguity} with samples from the Sketchy dataset \cite{sangkloy2016sketchy}.

% \vspace{-1.0em}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/1_sketch_ambiguity_v2.pdf}
  \caption{Structural ambiguity in hand-drawn sketches. \textbf{(a)} Subject image. \textbf{(b)--(g)} Freehand sketches drawn by different users. The examples are from the Sketchy dataset \cite{sangkloy2016sketchy}.}
  \label{fig:sketch_ambiguity}
\end{figure}

% \vspace{-1.0em}

\noindent
Consequently, this problem forces the generative algorithms to balance the trade-off between visual realism and intended shape. Existing GAN-based \cite{goodfellow2014generative,mirza2014conditional} methods primarily address sketch-to-image translation in two ways -- a direct mapping between domains with conditional GANs \cite{chen2020deepfacedrawing,chen2018sketchygan,gao2020sketchycoco,ghosh2019interactive,isola2017image,li2019linestofacephoto,li2020deepfacepencil,lu2018image,wang2018high,zhu2017unpaired} or modification in latent space using GAN inversions \cite{an2023sketchinverter,zhu2016generative}. However, such techniques often require application-specific data, optimization objectives, and complex learning strategies but occasionally fail to produce stable outcomes. Additionally, these methods operate on limited sets of task-specific object classes, resulting in poor generalization for unseen categories.

\noindent
More recently, denoising diffusion probabilistic models \cite{dhariwal2021diffusion,ho2020denoising,nichol2021improved,sohl2015deep} have demonstrated unprecedented improvements in the perceptual quality of general image synthesis. With sufficiently large annotated datasets \cite{schuhmann2022laion,schuhmann2021laion}, text-conditioned diffusion models \cite{ramesh2022hierarchical,ramesh2021zero,rombach2022high,saharia2022photorealistic} have achieved state-of-the-art results across multiple vision tasks, such as image generation, super-resolution, and inpainting. However, due to high structural ambiguities in hand-drawn sketches and the lack of sufficient paired sketch-image data, large-scale diffusion models have seen limited success in sketch-to-image translation. Furthermore, training such architectures from scratch is often computationally demanding and heavily infrastructure-dependent, which limits the scope of adopting the rich generative capabilities of latent diffusion models into sketch-to-image translation.

\noindent
In this paper, we propose a novel method for photorealistic image generation from freehand sketches leveraging the learned feature space of a pre-trained latent diffusion model \cite{rombach2022high}. We achieve this by introducing a learnable lightweight feature mapping network to perform latent code translation between source (\emph{sketch}) and target (\emph{image}) domains. The proposed approach provides a more stable optimization than GANs without requiring to train the latent diffusion model, thus mitigating the instability of GANs and the high computational overhead of large-scale diffusion models. Furthermore, unlike the existing methods, the proposed technique generalizes well beyond task-specific data distribution, significantly improving the generative performance on unseen object categories.

\vspace{1.0em}

\noindent
\textbf{Contributions:} The main contributions of the proposed work are as follows.

\begin{enumerate}
  \item We introduce an efficient method of photorealistic image generation from freehand sketches by providing structural guidance to a pre-trained latent diffusion model without retraining.
  \item The proposed approach achieves significantly better generalization beyond the observed data distribution, outperforming existing task-specific methods.
\end{enumerate}

\noindent
The remainder of the paper is organized as follows. Sec. \ref{sec:related_work} provides a brief overview of existing sketch-to-image translation techniques. Sec. \ref{sec:method} discusses the background and technical details of the proposed method, followed by the experimental analyses in Sec. \ref{sec:experiments}. We conclude the paper by summarizing our findings and discussing the potential scopes in Sec. \ref{sec:conclusions}.
