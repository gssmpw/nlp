\section{Related Work}
\label{sec:related_work}

\textbf{Conditional GANs:} Image-to-Image translation using conditional GANs is a widely explored method of directly transforming freehand sketches into images. Early architectural improvements introduced a Markovian discriminator \cite{isola2017image} for better retention of high-frequency correctness in paired image-to-image translation. A subsequent approach \cite{zhu2017unpaired} extended the idea to unpaired data by enforcing cycle consistency between source and target domains. In \cite{wang2018high}, the authors used coarse-to-fine generators, multi-scale discriminators, and an additional feature-matching loss for generating higher-resolution images. In \cite{park2019semantic}, the authors achieved generational improvements in semantic image manipulation by introducing spatially-adaptive normalization. The initial work exclusively on multi-class sketch-to-image translation proposed a masked residual unit \cite{chen2018sketchygan}, accommodating fifty object categories. Another approach proposed a contextual GAN \cite{lu2018image} to learn the joint distribution of the sketch and corresponding image. Researchers also explored interactive generation \cite{ghosh2019interactive} using a gating mechanism to suggest the probable completion of a partial sketch, followed by rendering the final image with a pre-trained image-to-image translation model \cite{wang2018high}. In \cite{gao2020sketchycoco}, the authors proposed a multi-stage class-conditioned approach for object-level and scene-level image synthesis from freehand sketches, improving the perceptual baseline over direct generations \cite{isola2017image}, contextual networks \cite{lu2018image}, and methods based on scene graphs \cite{ashual2019specifying,johnson2018image} or layouts \cite{zhao2019image}. In \cite{wang2022unsupervised}, the authors achieved similar goals with an unsupervised approach by introducing a standardization module and disentangled representation learning.

\vspace{1.0em}

\noindent
\textbf{GAN inversions:} The main objective of GAN inversion is to find a latent embedding of an image such that the original image can be faithfully reconstructed from the latent code using a pre-trained generator. Existing strategies for such inversions can be learning-based \cite{an2023sketchinverter,bau2019inverting,perarnau2016invertible,zhu2016generative}, optimization-based \cite{abdal2019image2stylegan,abdal2020image2stylegan++,creswell2018inverting,lipton2017precise,ma2018invertibility,ramesh2019spectral,voynov2020unsupervised}, or hybrid \cite{bau2019seeing,zhu2020domain}. In a learning-based inversion, an encoder learns to project an image into the latent space, minimizing reconstruction loss between the decoded (reconstructed) and original images. An optimization-based inversion estimates the latent code by directly solving an objective function. In a hybrid approach, an encoder first learns the latent projection, followed by an optimization strategy to refine the latent code. The rich statistical information captured by deep generative networks from large-scale data provides effective \emph{priors} for various downstream tasks, including sketch-to-image translation. In \cite{an2023sketchinverter}, the authors adopted a learning-based GAN inversion strategy using a multi-class deep generative network \cite{brock2018large}, pre-trained on the large-scale ImageNet dataset \cite{deng2009imagenet}, as \emph{prior} to achieve sketch-to-image translation for multiple categories. In \cite{xiang2022adversarial}, the authors introduced a framework for generalizing image synthesis to \emph{open-domain} object categories by jointly learning two \emph{in-domain} mappings (image-to-sketch and sketch-to-image) with \emph{random-mixed} strategy.

\vspace{1.0em}

\noindent
\textbf{Diffusion models:} A Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising,sohl2015deep} is a parameterized Markov chain that learns to generate samples similar to the original data distribution after a finite time. In particular, DDPMs use variational inference to learn to iteratively reverse a stepwise \emph{diffusion} (noising) process. In \cite{song2020denoising}, the authors introduced Denoising Diffusion Implicit Models (DDIMs) by generalizing DDPMs using non-Markovian diffusion processes with the same learning objective, leading to a deterministic and faster generative process. Recent advances \cite{dhariwal2021diffusion,nichol2021improved} have shown that diffusion models can achieve generational improvements in the visual quality and sampling diversity over GANs while providing a more stable and straightforward optimization objective. The most prolific application of diffusion models in recent literature is text-conditioned image generation \cite{ramesh2022hierarchical,ramesh2021zero,rombach2022high,saharia2022photorealistic} and modification \cite{bar2022text2live,brooks2023instructpix2pix,hertz2023prompt,mokady2023null}, utilizing a pre-trained language-image model \cite{radford2021learning} to embed the conditioning prompt. In \cite{choi2021ilvr}, the authors guided the generative process with an iterative latent variable refinement to produce high-quality variations of a reference image. In \cite{ruiz2023dreambooth}, the authors introduced a class-specific prior preservation loss to finetune an existing text-to-image diffusion model for \emph{personalized} manipulation of a specific subject image from a few observations. Emerging alternative approaches also involved Stochastic Differential Equations (SDEs) to guide the generative process following score-based \cite{meng2021sdedit} or energy-based \cite{xing2023inversion,zhao2022egsde} objectives. More recent attempts for sketch-to-image translation involved multiple objectives \cite{wang2022diffsketching}, multi-dimensional control \cite{cheng2023adaptively}, or latent code optimization \cite{voynov2023sketch}. In \cite{wang2022diffsketching}, the authors used an additional network to reconstruct the input sketch from the generated image. The denoising process was optimized using a cumulative objective function consisting of the \emph{perceptual similarity} (between the input and reconstructed sketches) and \emph{cosine similarity} (between the input and generated images) measures. In \cite{cheng2023adaptively}, the authors provided three-dimensional controls over image synthesis from the strokes and sketches to manipulate the balance between \emph{perceptual realism} and \emph{structural faithfulness} during the conditional denoising process. In \cite{voynov2023sketch}, the authors introduced a lightweight mapping network for providing structural guidance to a pre-trained latent diffusion model \cite{rombach2022high}. While the method avoided training a dedicated diffusion network, the \emph{differential guidance} made sampling images computationally even more demanding than a large-scale model itself.
