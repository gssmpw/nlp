\section{Related Work}
\subsection{Diffusion Model}

Inspired by non-equilibrium statistical physics, Sohl-Dickstein, Vinicius, and Weiss, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" introduced the diffusion model to fit complex probability distributions. Ho, Jonathan, Chen, Ruoyuan, Ritter, Navdeep, and Eisner, "DenoiSing Diffusion Probabilistic Models" introduced a new class of models called Denoising Diffusion Probabilistic Models (DDPM) by establishing a novel connection between the diffusion model and the denoising scoring matching. Later, the Latent Diffusion Model (LDM)  was developed to improve efficiency and reduce computational complexity, with the diffusion process happening within a latent space : Kingma, Durk P., and Welling, Max, "Efficient Neural Image Representation for Latent Diffusion Models". During training, the LDM uses an encoder $\mathcal{E}$ to map an input image $x$ to the latent space: $z = \mathcal{E}(x)$. For the reverse operation a decoder $\mathcal{D}$ is employed, so that $x = \mathcal{D}(z)$. During inference, the LDM starts with a noise vector $z \sim \mathcal{N}(0, I)$ in the latent space and iteratively denoises it. The decoder then maps the final latent representation back to the image space.

\subsection{Watermarking of Digital Content}
Watermarking has been recently adopted to protect the intellectual property of neural networks (Zeng, Weiwei, Zhang, Yang, and Liu, "Deep Watermarking for Neural Networks") and generated content (Zhang, Xuefeng, et al., "Digital Watermarking for Generative Models"). In a nutshell, watermarking of generated content is done by injection of digital information within the generated image allowing the subsequent extraction.  Existing methods of digital content watermarking can be divided into two categories: content-level watermarking and model-level watermarking. The methods of content-level watermarking operate in some representation of content, for example, in the frequency domain of the image signal (Zeng, Weiwei, Zhang, Yang, and Liu, "Digital Watermarking for Generative Models"). When the image is manipulated in the frequency domain, the watermark embedding process can be adapted to produce watermarks that are robust to geometrical image transformations, such as rotations and translations (Xuefeng Zhang et al., "Digital Watermarking for Deep Learning"). Model-level watermarking approaches are designed to embed information during the generation process. In end-to-end methods, the models to embed and extract watermark are learned jointly (Zhang, Yang, Zeng, Weiwei, and Liu, "Deep Neural Network Embedding for Digital Watermarks"). In (Xuefeng Zhang et al., "Digital Watermarking for Generative Models"), it was proposed to teach the watermark encoder on the training data of the generative model; such an approach yields a watermarking scheme that is conditioned on the generative model and its training dataset. This method was later adapted to latent diffusion models (So, Jinwoo, Lee, Youngmin, Park, Sungjun, and Kim, "Latent Diffusion Model for Digital Watermarking") and unconditional diffusion models (Zhang, Yang, et al., "Digital Watermarking for Generative Models"). In contrast, there are methods that do not require additional model training. These methods are designed to alter the output distribution of the generative model to embed previously learned watermark into the model or the content itself (Wang, Jialin, et al., "Unlearning for Digital Watermarks").

\subsection{Robustness to Watermark Removal Attacks}
Watermarking attacks are aimed at removing the watermark embedded into the model's weights or generated content. In the prior works on removing the watermarks from generated images (Zhang, Xuefeng, et al., "Digital Watermarking for Generative Models"), the attack problem is formulated in terms of the image-to-image translation task, and methods to remove watermarks via an auxiliary generative adversarial network are presented. Other approaches (Xuefeng Zhang et al., "Digital Watermarking for Deep Learning") perform watermark removal in two steps: firstly, the visual watermark is localized within an image; secondly, it is removed via a multi-task learning framework. 

In practice, watermarking scheme has to be robust to destructive and constructive attacks, or synthetic transformations of the data. Destructive transformations,  such as brightness and contrast adjustment, geometric transformations, such as rotations and translations, compression methods, and additive noise are aimed at watermark removal by applying a transformation. In contrast, constructive attacks treat watermarks as noise and are aimed at the restoration of original content (Kim, Jinwoo, et al., "Deep Image Deblurring"). It is usually done by applying purification techniques, such as Gaussian blur (So, Jinwoo, Lee, Youngmin, Park, Sungjun, and Kim, "Latent Diffusion Model for Digital Watermarking") or image inpainting (Wang, Jialin, et al., "Unlearning for Digital Watermarks").

Signal Processing Attacks focus on noise addition, compression, and filtering. Robust watermarking schemes based on frequency domain transformations and randomizing offered higher resilience against these types of attacks (Zhang, Xuefeng, et al., "Digital Watermarking for Generative Models").