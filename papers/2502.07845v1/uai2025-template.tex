%\documentclass{uai2025} % for initial submission
\documentclass[accepted]{uai2025} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{footmisc}

\usepackage{dsfont}
\usepackage{amsfonts}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{Spread them Apart: Towards Robust Watermarking of Generated Content}

% The standard author block has changed for UAI 2025 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1,2$^*$]{Mikhail Pautov}
\author[3$^*$]{Danil Ivanov}
\author[1,3]{Andrey V. Galichin}
\author[1,3,4]{Oleg Rogov}
\author[1,3]{Ivan Oseledets}
% Add affiliations after the authors
\affil[1]{%
    AIRI\\
    Moscow, Russia
}
\affil[2]{%
    ISP RAS Research Center for Trusted AI\\
    Moscow, Russia
}
\affil[3]{%
    Skolkovo Institute of Science and Technology\\
    Moscow, Russia
  }
\affil[4]{%
    Moscow Technical University of Communications and Informatics\\
    Moscow, Russia
  }
  
  \begin{document}
\maketitle

\def\thefootnote{*}\footnotetext{Equal contribution.}


\begin{abstract}
Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license.  In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks.
\end{abstract}

\section{Introduction}\label{sec:intro}


Recent advances in generative models have brought the performance of image synthesis tasks to a whole new level. For example, the quality of the images generated by diffusion models (DMs, \cite{croitoru2023diffusion,rombach2022high,esser2024scaling}) is now sometimes comparable to the one of the human-generated pictures or photographs. Compared to generative adversarial networks (GANs, \cite{goodfellow2014generative,brock2019large},  diffusion models allow the generation of high-resolution, naturally looking pictures and incorporate much more stable training, leading to a more diverse generation. More than that, the image generation process with diffusion models is more stable, controllable, and explainable. They are easy to use and are widely deployed as tools for data generation, image editing (\cite{kawar2023imagic,yang2023paint}), music generation (\cite{schneider2024mousai}), text-to-image synthesis (\cite{saharia2022photorealistic,zhang2023adding,ruiz2023dreambooth}) and in other multimodal settings. 



Unfortunately, several ethical and legal issues may arise from the usage of diffusion models.  On the one hand, since diffusion models can be used to generate fake content, for example, deepfakes (\cite{zhao2021multi,narayan2023df}), it is crucial to develop automatic tools to verify that a particular digital asset is artificially generated. On the other hand, a dishonest user of the model protected by a copyright license can query it, receive the result of generation, and later claim exclusive copyright. In this work, we focus on the detection of the content generated by a particular model and the identification of the end-user who queried the model to generate a particular content. We develop a technique to embed the digital watermark into the generated content during the inference of the generative model, so it does not require retraining or fine-tuning the generative model. The approach allows not only to verify that the content was generated by a source model but also to identify the user who sent a corresponding query to the generative model. We prove that the watermark embedded is robust against additive perturbations of the content of a bounded magnitude. 



\begin{figure*}[tb]
    \centering
    \includegraphics[width=0.8\textwidth]{Figs-PDF/from-misha-with-cat-v4.pdf}
    \caption{Illustration of the proposed method. During the image generation phase, the user $u_i$ queries the model with the prompt. Given the prompt, the model produces the latent $z$, from which the image is generated. If the image generated satisfies the constraint $\mathcal{L}_{wm} < \varepsilon$ (meaning the watermark is successfully embedded), it is yielded to the user; otherwise, the loss function from Eq. \ref{eq:TotalLoss} is minimized with respect to the latent $z$. Note that the value of $\varepsilon$ may vary from image to image. During the watermark retrieval phase, given the image $x$ and $m$ secrets, $s(u_1), \dots, s(u_m)$, the watermark decoder extracts m watermarks, $w(u_1|x), \dots, w(u_m|x)$. Then, the image is attributed to the user $u$ according to the Eq. \ref{eq:attribution}. }
    \label{fig:big_teaser}
\end{figure*}

Our contributions are threefold: 

\begin{itemize}
    \item We propose \emph{Spread them Apart}, the framework to embed digital watermarks into the generative content of continuous nature. Our method embeds the watermark during the process of content generation and, hence, does not require additional training of the generative model. 
    \item We apply the framework to watermark images generated by a diffusion model and prove that the watermark embedded is provably robust to the additive perturbations of a bounded magnitude that can be applied during the post-processing of the image.
    \item Experimentally, we show that our approach outperforms competitors in terms of the robustness to different types of post-processing of the images aimed at watermark removal, such as brightness and contrast adjustment or gamma correction.
\end{itemize}







\section{Related Work}

\subsection{Diffusion Model}

Inspired by non-equilibrium statistical physics, \cite{sohldickstein2015deepunsupervisedlearningusing} introduced the diffusion model to fit complex probability distributions. \cite{ho2020denoisingdiffusionprobabilisticmodels} introduced a new class of models called Denoising Diffusion Probabilistic Models (DDPM) by establishing a novel connection between the diffusion model and the denoising scoring matching. Later, the Latent Diffusion Model (LDM)  was developed to improve efficiency and reduce computational complexity, with the diffusion process happening within a latent space   \cite{rombach2022high}. During training, the LDM uses an encoder $\mathcal{E}$ to map an input image $x$ to the latent space: $z = \mathcal{E}(x)$. For the reverse operation a decoder $\mathcal{D}$ is employed, so that $x = \mathcal{D}(z)$. During inference, the LDM starts with a noise vector $z \sim \mathcal{N}(0, I)$ in the latent space and iteratively denoises it. The decoder then maps the final latent representation back to the image space.

\subsection{Watermarking of Digital Content}
Watermarking has been recently adopted to protect the intellectual property of neural networks (\cite{wu2020watermarking,pautov2024probabilistically}) and generated content (\cite{kirchenbauer2023watermark,zhaoprovable,fu2024watermarking}). In a nutshell, watermarking of generated content is done by injection of digital information within the generated image allowing the subsequent extraction.  Existing methods of digital content watermarking can be divided into two categories: content-level watermarking and model-level watermarking. The methods of content-level watermarking operate in some representation of content, for example, in the frequency domain of the image signal (\cite{o1996watermarking,cox1996secure}). When the image is manipulated in the frequency domain, the watermark embedding process can be adapted to produce watermarks that are robust to geometrical image transformations, such as rotations and translations (\cite{wen2024tree}). Model-level watermarking approaches are designed to embed information during the generation process. In end-to-end methods, the models to embed and extract watermark are learned jointly (\cite{zhu2018hidden,hayes2017generating}). In \cite{yu2021artificial}, it was proposed to teach the watermark encoder on the training data of the generative model; such an approach yields a watermarking scheme that is conditioned on the generative model and its training dataset. This method was later adapted to latent diffusion models (\cite{fernandez2023stable}) and unconditional diffusion models (\cite{zhao2023recipe}). In contrast, there are methods that do not require additional model training. These methods are designed to alter the output distribution of the generative model to embed previously learned watermark into the model or the content itself (\cite{kirchenbauer2023watermark,wen2024tree}). 


\subsection{Robustness to Watermark Removal Attacks}
Watermarking attacks are aimed at removing the watermark embedded into the model's weights or generated content. In the prior works on removing the watermarks from generated images (\cite{li2019towards,cao2019generative}), the attack problem is formulated in terms of the image-to-image translation task, and methods to remove watermarks via an auxiliary generative adversarial network are presented. Other approaches (\cite{hertz2019blind,liang2021visible,sun2023denet}) perform watermark removal in two steps: firstly, the visual watermark is localized within an image; secondly, it is removed via a multi-task learning framework. 

In practice, watermarking scheme has to be robust to destructive and constructive attacks, or synthetic transformations of the data. Destructive transformations,  such as brightness and contrast adjustment, geometric transformations, such as rotations and translations, compression methods, and additive noise are aimed at watermark removal by applying a transformation. In contrast, constructive attacks treat watermarks as noise and are aimed at the restoration of original content (\cite{zhang2024robust}). It is usually done by applying purification techniques, such as Gaussian blur (\cite{hosam2019attacking}) or image inpainting (\cite{liu2021wdnet,xu2017automatic}).

Signal Processing Attacks focus on noise addition, compression, and filtering. Robust watermarking schemes based on frequency domain transformations and randomizing offered higher resilience against these types of attacks (\cite{Taran_2019_CVPR}).



\section{Problem Statement}
In this section, we formulate the problem statement and the research objectives. Note that we focus on the watermarking of images generated by diffusion models, but the formulation below is valid for watermarking of any generated content, for example, audio, video, or text.  

\subsection{Image Watermarking}
In our approach, we focus on  \emph{detection} and \emph{attribution} of the generated image simultaneously: while detection is aimed to verify whether a particular image is generated by a given model,  attribution is aimed at determining the user who generated the image. 

Suppose that we are given the generative model $f$ deployed in the black-box setting, i.e., as a service: in the generation phase a user $u_i \in [u_1, \dots, u_m]$ sends a query to the model and receives a generated image $x \in \mathbb{R}^d$. If $x$ is a watermarked image, the owner of model $f$ should be able to identify that $x$ is generated by user $u_i$ by querying the model $f$. In our method, the image is watermarked during the \emph{generation} phase, not during the post-processing. We formulate the process of watermarking and attribution in the following way: %Let $e(x,u_i):\mathbb{R}^d \to \mathbb{R}^d$ be the function that embeds the watermark to image $x$ 

\begin{enumerate}
    \item When the user $u_i \in [u_1, \dots, u_m]$ registers in the service, it is assigned a pair of \emph{public} and \emph{private} keys, namely, the watermark $w(u_i)$ and the secret $s(u_i)$. Watermark is a binary string of length $n$ and the secret is the sequence of tuples of length $n$, where each tuple is a pair of unique positive numbers treated as indices: $w(u_i) \in \{0,1\}^n, \ s(u_i) \in \mathbb{Z}_{+}^{2n}.$ 
    \item When the user $u_i$ queries the model $f,$ it generates the image $x$ with the watermark $w(u_i)$ embedded into $x$.
    \item  When the watermarked object $x$ is received by the model owner, it extracts the watermark $w(u_i|x)$ using the secret $s(u_i)$ of the user $u_i$  and compares it with the watermark $w(u_i)$ assigned to the user $u_i$. Following the previous works (\cite{yu2021artificial,fernandez2023stable}), we  compute the bitwise distance $d(w(u_i|x), w(u_i))$ between $w(u_i|x)$ and $w(u_i)$:
    \begin{equation}
        d(w(u_i|x), w(u_i)) = \sum_{j=1}^n \mathds{1} (w(u_i|x)_j \ne w(u_i)_j).
    \end{equation}
    \begin{remark}
        
    
    For robustness to watermark removal attack, in case of a single user $u_i$, we flag the object $x$ as generated by the user $u_i$ if the distance $d(w(u_i|x), w(u_i))$ is either small or large, namely, if 
    \begin{equation}
    \label{eq:att_rule}
        d(w(u_i|x), w(u_i)) \in [0, \tau_1] \cup [\tau_2, n], 
    \end{equation}
    where $\tau_1 \ll n$ and $\tau_2 \gg 0.$
    This procedure is known as the double-tail detection (\cite{jiang2023evading}). 
    \end{remark}
\end{enumerate}


\subsection{The Probability of Incorrect Attribution}
We assume that the watermark $w(u_i)$ attributed to the user $u_i$ is drawn randomly and uniformly from the set of all possible $n-$bit watermarks, $\{0,1\}^n$. Following the prior works (\cite{fernandez2023stable}), we formulate the detection problem as the hypothesis test. In case of a single user $u_i$, we define the null hypothesis $\mathcal{H}_0 =$ ``the object $x$ is generated not by $u_i$'' and the  alternative hypothesis $\mathcal{H}_1 =$ ``the object $x$ is generated  by $u_i$''. Additionally, under the null hypothesis, we assume that the $j'$th bit in the watermark $w(u_i|x)$ extracted from $x$ is the same as the $j'$th bit from $w(u_i)$ with the probability $p_i$. 

In the case of a single user $u_i$ and given the attribution rule from the Eq. \ref{eq:att_rule}, we compute the probability of the false attribution, namely, 
\begin{align}
\label{eq:fpr_1}
    & FRP(1)|_{u_i} = \mathbb{P} \left[d(w', w(u_i)) \in [0, \tau_1] \cup [\tau_2, n] \right] = \nonumber \\ &\sum_{q\in [0, \tau_1] \cup [\tau_2, n]}{n \choose q} p_i^q (1-p_i)^{n-q},
\end{align}
where $w' = w(u_i|x)$ is a random watermark uniformly sampled from $\{0,1\}^n$, namely, ${w' \sim \{0,1\}^n, w' \ne w(u_i)}.$

In case of $m$ users,  the probability $FPR(m)$ of incorrect attribution of the non-watermarked image $x$ to some other user $u_j \in [u_1, \dots, u_m]$  is upper bounded by the probability below:
\begin{equation}
\begin{aligned}
    \label{eq:fpr_m}
    &FPR(m) \le \mathbb{P}_{w' \sim \{0,1\}^n} [ \exists u_j \in [u_1,  \dots, u_m]: \\& d(w', w(u_j)) \in [0,\tau_1] \cup [\tau_2, n]] \le \\ & \sum_{u_j \in [u_1,\dots, u_m]} FPR(1)|_{u_j} = \hat{p}.
\end{aligned}
\end{equation}
Note that this upper bound holds regardless of the independence of random variables $\xi_1, \dots, \xi_m,$ where 
\begin{equation}
\label{eq:indicator}
    \xi_i = \mathds{1}[d(w(u_i|x), w(u_i)) \in [0,\tau_1] \cup [\tau_2, n]].
\end{equation}

\begin{remark}
In our experiments, the probability $p_i$ from above is estimated to be close to $\frac{1}{2}.$
\end{remark}

\subsection{Robustness to Watermark Removal Attacks}
When the user $u_i$ receives the watermarked image $x$, it can post-process it to obtain the other image, $x'$, which does retain the sufficient part of the watermark $w(u_i)$. The transition from $x$ to $x'$ may be done by applying an image transformation, such as brightness or contrast adjustment, Gaussian blur, or additive noise. The other approach is to perform an adversarial attack on the generative model to erase the watermark (\cite{jiang2024watermark}). In our settings, we assume that the generative model is deployed as the black-box service with limited access to the API, so an adversary can not apply white-box adversarial attacks  (\cite{jiang2023evading}). 





\section{Method}
\label{sec:method}

In this section, we provide a detailed description of the proposed approach, its implementation details, and the robustness guarantee against additive watermarking removal attacks of bounded magnitude. 
\subsection{Embedding and Extraction of the Watermark}
Suppose that  $f$ is the generative model. Recall that the user $u_i \in [u_1, \dots, u_m]$ receives a pair $(w(u_i), s(u_i))$ after the registration in the service, where both the watermark and the secret are unknown to the user and are privately kept by the owner of $f$. Let $x$ be the generated image. Then, the watermark embedding process is described as follows:

\begin{enumerate}
    \item The secret $s(u_i)$ is interpreted as two sequences of indices, $A = \{a_1, \dots, a_n\}$ and $B = \{b_1, \dots, b_n\}$. The watermark $w(u_i) = \{w_1, \dots, w_n\}$ is the binary string that restricts the generated image $x$ in the areas represented by the sets $A$ and $B$.
    \item The restriction of $x$ in the areas represented by the sets $A$ and $B$ given $w(u_i)$ is the following implication:
    \begin{equation}
    \label{eq:w_embed}
    \begin{cases}
        w_i = 0 \implies x_{a_i} \ge x_{b_i} \\
        w_i = 1 \implies x_{a_i} < x_{b_i},
    \end{cases}
      \end{equation}
      where $x_{j}$ is the intensity of the $j'$th pixel of $x$.  To increase the robustness  to watermark removal attacks, we apply additional regularization to $x$:
      \begin{equation}
          \min_{j \in [1,\dots,n]} |x_{a_j} - x_{b_j}| \ge \epsilon,
      \end{equation}
      where $\epsilon > 0 $ is the scalar parameter. 
\end{enumerate}

To perform detection and attribution of the given image $x$, the owner of the generative model firstly constructs $m$ watermarks $w(u_1|x), \dots, w(u_m|x)$ by reversing the implication from the Eq. \ref{eq:w_embed}. Namely, given the secret $s(u_i) = \{a_1, \dots, a_n, b_1, \dots, b_n\}$ of user $u_i$, the watermark bits are restored by the following rule:
\begin{equation}
\begin{cases}

    x_{a_j} \ge x_{b_j} \implies w(u_i|x)_j = 0, \\
     x_{a_j} < x_{b_j} \implies w(u_i|x)_j = 1.
        
\end{cases}
\end{equation}

\begin{remark}
  Here, we distinguish the watermark $w(u_i)$ assigned by the owner of generative model to the user $u_i$ from the watermark $w(u_i|x)$ extracted from the image $x$ with the use of the secret $s(u_i)$ of user $u_i.$
\end{remark}

When $m$ watermarks $w(u_1|x), \dots, w(u_m|x)$ are extracted, the owner of the model assigns $x$ to the user $u$ with the minimum distance $d(w(u_i), w(u_i|x))$ between assigned and extracted watermarks:

\begin{equation}
\label{eq:attribution}
    u = \arg\min_{u_i \in [u_1, \dots, u_m]: \ \xi_i = 1} d(w(u_i), w(u_i|x)),
\end{equation}
where $\xi_i$ is the indicator function from the Eq. \ref{eq:indicator}. Note that if $\xi_i = 0$ for all $\ i \in [1,\dots,m],$ then $x$ is identified as image not generated by $f$. 


\subsection{Implementation Details}

In this subsection, we describe the watermarking procedure. First of all, we have to note that in the Stable Diffusion model, the latent vector $z$ produced by the U-Net is then decoded back into the image space using a VAE decoder: $x = \mathcal{D}(z)$. To embed the watermark into an image, we optimize a special two-component loss function with respect to the latent vector $z.$ The overall loss is written as follows:
\begin{equation}
\label{eq:TotalLoss}
    \mathcal{L} = \lambda_{wm} \mathcal{L}_{wm} + \lambda_{qual} \mathcal{L}_{qual},
\end{equation}

The first term, $\mathcal{L}_{wm}$, defines how the image complies with the pixel difference imposed by the watermark $w(u_i) = \{w_1, \dots, w_n\}$ and the secret $s(u_i) = \{a_1, \dots, a_n, b_1, \dots, b_n\}$: 

\begin{equation}
\label{eq:wm_insert}
    \mathcal{L}_{wm} = \sum_{i=1}^{n}\min((-1)^{w_i} (x_{a_i} - x_{b_i}) + \varepsilon, 0), \quad x = \mathcal{D}(z),
\end{equation}
Here, $\varepsilon$ defines the minimum difference between private key pixels that we would like to obtain. Note that the larger the value of $\varepsilon$ is, the more robust the watermark is to additive perturbations. At the same time, the increase of $\varepsilon$ negatively influences the perceptual quality of images.
 

The second term $\mathcal{L}_{qual}$, is introduced to preserve the generation quality of the image. The value $\mathcal{L}_{qual}$ is a difference in image quality measured by  LPIPS metric (\cite{zhang2018perceptual}), which acts as a regularization. Given $x$ as the original image and $\hat{x}$ as the watermarked image, $\mathcal{L}_{qual}$ is defined as follows:
\begin{equation}
    \label{eq:lpips}
    \mathcal{L}_{qual}(x,\hat{x}) = \sum_{j} \frac{1}{W_j H_j} \sum_{w,h} \|\phi^j(x) - \phi^j(\hat{x})\|^2_2.
\end{equation}
Here, $\phi^j(x) = w_j \odot o^j_{hw}(x)$, where $o^j(x)$ are the internal activations of the CNN, AlexNet (\cite{krizhevsky2012imagenet}), in our case. 


Note that we do not perform denoising at each iteration, as we only manipulate the latent vectors produced by U-Net;  the forward step of the described optimization procedure involves only the decoding of the latent vectors: $x = \mathcal{D}(z)$. 


The optimization is performed over $700$ steps of the Adam optimizer with the learning rate of $8 \times 10^{-3}$, where every $100$ iteration, the learning rate is halved. When the convergence is reached, the ordinary Stable Diffusion post-processing of the image is performed. The coefficients $\lambda_{wm}$ and $\lambda_{qual}$ are determined experimentally and set to be $0.9$ and $150$, respectively, the value of $\varepsilon$ was set to be $\varepsilon=0.2$.  Schematically, the process of watermark embedding and extraction is presented in Figure \ref{fig:big_teaser}. % and the early stopping takes place, when the loss reaches a threshold of $1$. After the convergence, the ordinary Stable Diffusion post-processing of the image is performed. 



\subsection{Robustness Guarantee}
By construction, the watermark embedded by our method is robust against additive watermark removal attacks of a bounded magnitude. Namely, let the watermark $w(u_i|x)$ be embedded in $x$ with the use of the secret $s(u_i) = \{a_1, \dots, a_n, b_1, \dots, b_n\}$ of the user $u_i$. Let 
\begin{equation}
    \Delta_ i = \frac{|x_{a_i} - x_{b_i}|}{2}.
\end{equation}
Then, the following lemma holds.
\begin{lemma}\label{lemma:l_inf_lemma}
    Let $\varepsilon \in \mathbb{R}^d$ and $\Delta_{i_1} \le \Delta_{i_2} \le \dots \le \Delta_{i_n}$. 
    
    Then, if $\|\varepsilon\|_\infty < \Delta_{i_k}$, then $d(w(u_i|x + \varepsilon), w(u_i|x)) < k.$
\end{lemma}
\begin{proof}
    Note that to change the $j'$th bit of watermark $w(u_i|x)$, an adversary has to change the sign in expression $(x_{a_j} - x_{b_j})$. Without the loss of generality, let $x_{a_j} - x_{b_j} \ge 0$. 
    
    Consider an additive noise $\varepsilon$ such that $(x+\varepsilon)_{a_j} - (x+\varepsilon)_{b_j} <0,$ meaning $|\varepsilon_{b_j} - \varepsilon_{a_j}| > |x_{a_j} - x_{b_j}|$. Note that $\|\varepsilon\|_\infty \ge \max(|\varepsilon_{a_j}|, |\varepsilon_{b_j}|).$ 

    If $\max(|\varepsilon_{a_j}|, |\varepsilon_{b_j}|) < \Delta_j,$ then $|\varepsilon_{b_j} - \varepsilon_{a_j}| \le |\varepsilon_{b_j}| + |\varepsilon_{a_j}| <  2\Delta_j = |x_{a_j} - x_{b_j}|,$ yielding a contradiction. Thus, $\|\varepsilon\|_\infty \ge \Delta_j.$
    
    Finally, an observation that all the indices in $s(u_i)$ are unique finalizes the proof. 
\end{proof}
This lemma provides a lower bound on the $l_\infty$ norm of the additive perturbation $\varepsilon$ applied to $x$ which is able to erase at least $k$ bits of the watermark $w(u_i|x))$ embedded into $x$. 








\section{Experiments}


\subsection{General Setup}
 We use \texttt{stable-diffusion-2-base}  model (\cite{rombach2022high}) with the \texttt{epsilon} prediction type and $50$ steps of denoising for the experiments. The resolution of generated images is $512 \times 512$. The experiments were conducted on \texttt{DiffusionDB} dataset (\cite{wangDiffusionDBLargescalePrompt2022}). Specifically, we choose $1000$ unique prompts and generate $1000$ different images.  

%from \texttt{large\_first\_10k} option of HuggningFace Datasets %Hyperparameters tuning was performed on 32 held out prompts of this dataset . 

The public key for the user is sampled from the Bernoulli distribution with the parameter $p=0.5$. The length of a key is set to be $n=100$. The private key is generated by randomly picking $2n$ unique pairs of indices of the flattened image.  

\subsection{Attack Details} 
\label{subseq:attack_details}

We evaluate the robustness of the watermarks embedded by our method against the following watermark removal attacks: brightness adjustment, contrast shift, gamma correction, image sharpening, hue adjustment, saturation adjustment, random additive noise, JPEG compression, and the white-box PGD adversarial attack  (\cite{madry2018towards}). In this section, we describe these attacks in detail. 



Brightness adjustment of an image $x$ was performed by adding a constant value to each pixel: $x_{brightness} = x + b$, where $b$ was sampled from the uniform distribution $\mathcal{U}[-20, 20]$.


Contrast shift was done in two ways: positive and negative. The positive contrast shift implies the multiplication of each pixel of an image by a constant positive factor: $x_{contrast} = c x$, where $c$ was sampled from the uniform distribution, $c \sim \mathcal{U}[0.5, 2]$. 

On the contrary, when the contrast shift is performed with the negative value of $c$ (namely, $ c \sim \mathcal{U}[-2, -0.5]$), such a  transform turns an image into a negative. Later, we treat these transforms separately and denote them as ``Contrast $+$'' and ``Contrast $-$'', depending on the sign of $c$. 


Gamma correction is nothing but taking the exponent of each pixel of the image: $x_{gamma} = x ^ g$, where $g \sim \mathcal{U}[0.5, 2]$.



For sharpening, hue, and saturation adjustment, we use implementations from the \texttt{Kornia} package (\cite{riba2020kornia}) with the following parameters: $a_{saturation} = 2, \quad a_{hue} = 0.2,$  $\quad a_{sharpness} = 2.$ 


The noise for the noising attack was sampled from the uniform distribution $\mathcal{U}[-\delta, \delta]$, where $\delta=25$. Note, that the maximum $\|\cdot\|_\infty $ of noise is then equal to $25$.




% For blurring the variance of the kernel is taken to be 5.
JPEG compression was performed using DiffJPEG (\cite{Shin2017JPEGresistantAI}) with quality equal to $50$. 

White-box attack aims to change the embedded watermark $w$ to some other watermark $\tilde{w}$ by optimizing the image with respect to the loss initially used to embed the watermark $w$:
\begin{align}
\label{eq:wb_attack_loss}
    &\mathcal{L}_{wb} = \lambda_{wm} \mathcal{L}_{wm} + \lambda_{qual} \mathcal{L}_{qual}, \quad \text{where} \\
    &\mathcal{L}_{wm} = \sum_{i=1}^{n}\min((-1)^{\tilde{w}_i} (x_{a_i} - x_{b_i}) + \varepsilon, 0). \nonumber
\end{align}
In Eq. \ref{eq:wb_attack_loss}, the term $\mathcal{L}_{qual}$ corresponds to the difference in image quality in terms of LPIPS metric, namely,
\begin{equation}
    \mathcal{L}_{qual} = LPIPS(x, \hat{x}),
\end{equation}
where $x$ and $\hat{x}$ are the original image and image on a particular optimization iteration, respectively.


 The loss function $\mathcal{L}_{wb}$ pushes the private key pixels to be aligned with a new randomly sampled public key $\tilde{w}$ so that the ground-truth watermark $w$ gets erased.  The attack's budget is the upper bound of $\|\cdot\|_\infty $ norm of the additive perturbation, that we have taken to be $ {\varepsilon}/{2}$ from the Eq. \ref{eq:wm_insert}. Let $\tilde{x}$ be the image obtained after the attack. If at some iteration the distance between the source image $x$ and the attacked one $\tilde{x}$ exceeds $ {\varepsilon}/{2}$, $\tilde{x}$ is being projected back onto the sphere $\|\tilde{x} - x\|_\infty = {\varepsilon}/{2}$. The optimization took place for $10$ iterations with the Adam optimizer and the learning rate was equal to $10^{-1}$. Note that this attack setting implies knowledge about the private key and assumes white-box access to the generative model. Hence, this is de facto the strongest watermark removal attack we consider. 




Pixels of the images perturbed by the attacks are then linearly mapped to $[0, 255]$ segment:
\begin{equation}
    x^{(i)} = 255 \frac{x^{(i)} - x^{(i)}_{min}}{x^{(i)}_{max}-x^{(i)}_{min}}, \quad i \in \{R, G, B\}.
\end{equation}
\subsection{Results}
\label{sec:main_results}
In this section, we provide the quantitative results of experiments. We report (i) quality metrics of the generated images (SSIM, PSNR, FID and LPIPS) to evaluate the invisibility of the watermarks, (ii) bit-wise error of the watermark extraction caused by watermark removal attacks and (iii) True Positive Rates in attribution and detection problems.% that arises from mistakenly assigning the image to a wrong user and marking the non-watermarked images as the watermarked one, respectively.


% We subdivide our results into three groups. The first group includes quality metrics of the generated images: SSIM, PSNR, FID and LPIPS metric. 

% The second group elaborates on the errors of watermark extraction, caused by the attacks. And the third group is about the collision errors of misattribution of the watermarked images to wrong users and mistakenly extracting watermarks from non-watermarked content. 

We compare our results (where applicable) to that of Stable Signature \cite{fernandez2023stable}, SSL watermarking \cite{fernandez2022watermarking}, AquaLora \cite{feng2024aqualora} and WOUAF \cite{kim2024wouaf}, one of the state-of-the-art watermarking approaches. In these works the watermark length is set to be $48$, $30$, $48$, and $32$, respectively, while we have $100$ bits long watermarks: note that the longer the watermark, the harder it is to be embedded. %; in contrast, Greater bit capacity implies more information to be stored at the cost of difficulties in embedding, as it is harder to hide more data in a given image.

%\subsubsection{Image quality}
The image quality metrics are presented in Table \ref{table:img-quality}. It can be seen that our results are comparable to the ones of the baseline methods in terms of the quality of the produced images. A qualitative comparison of original and watermarked images can be found in Figure \ref{fig:qual-with-diff}. % More examples are provided in Supplementary \ref{A:qual-res}.
%Structural Similarity index (SSIM) \cite{wang2004image} establishes how close given images are by comparing their luminance, contrast and structure. Peak signal-to-noise ratio essentially measures the pixel-wise difference between samples (noise) and compares it to the maximum value of the source sample (signal). And  Fr\'{e}chet Inception Distance (FID) \cite{heusel2017gans} implies encoding the ground-truth and generated sets of images into abstract vectors by means of the Inception v3 model, then fitting two Gaussian distributions on these sets of vectors and finally computing the similarity score based on the mean vectors and covariance matrices of the fitted Gaussians. 
%We report the watermark extraction error that can be defined by the formula below:
% \subsubsection{Watermark robustness}
To evaluate the robustness of the watermarks against removal attacks, we report an average bit-wise error, ABWE:
\begin{equation}
    ABWE = \frac{1}{Nn}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{n}\mathds{1}[w^{gt}_{i,j}\neq w^{extracted}_{i,j}],
\end{equation}
where $w^{gt}_{i,j}$ and $ w^{extracted}_{i,j}$ are the $j$-th bits of ground truth and extracted private keys, corresponding to the $i$-th image. Here, $n$ is the number of bits in the watermark and $N$ is the number of images. We report ABWE in Table \ref{table:attack-errors}. 


% For comparison we put the corresponding metrics from the Stable Signature. The authors of this paper reported bit accuracy, which can be converted to the watermark extraction error by a simple formula: $Extraction Error = 1 - Bit Accuracy$.

% Note that shows that our method performs better when it comes to Brightness and Contrast attacks. Since our procedure is a two-tailed detection, such a large error as in the case of Negative contrast signifies the robustness of the inserted watermark to this attack.

% Another means to estimate robustness of our method is to construct a following object. From each image from the test set we extract the values of the private key pixels, then take their absolute difference and sort these differences, thus obtaining a sorted vector of values

% \begin{equation}
%     d = (d_1, d_2, \dots, d_{100}), 
% \end{equation}

% where $d_1$ is the least difference between private key pixel values and $d_{100}$ is the largest one.

% Then by combining these vectors from all the images from the test set we store them in a matrix row-wise, so we get $D_{1000\times 100}$ (1000 is the test set size and 100 is the watermark length). Each column of $D$ represents the distribution of the absolute differences of private key pixel values conditioned on how great they are relative to all other differences. These distributions are depicted on the Figure \ref{fig:diff-dist} for various column indices of matrix $D$. 

% Note, that $\varepsilon=0.2$ in the loss \ref{eq:wm_insert} used for watermarking corresponds to $0.2 \cdot 127.5 \approx 26$ pixels in an RGB scale (we transformed $\varepsilon$ from $[-1, 1]$ segment, at which the generation takes place, to RGB scale $[0, 255]$). So we can see that our procedure resulted in a peak of histograms around 26 pixels on the Figure \ref{fig:diff-dist}. But clearly there are some pixel values, whose difference lies below $\varepsilon$ (distribution, corresponding to index 1 in the Figure legend). So not all the pixel values are sufficiently different signifying the trade-off between image quality and robustness. One could interpret such case with the following argumentation: $\mathcal{L}_{wm}$ term of the loss \ref{eq:TotalLoss} did not reach zero for the sake of $\mathcal{L}_{qual}$ term. 


%   Another useful plot can be obtained by taking a column-wise mean of matrix $D$, see Figure \ref{fig:sorted-diff}. The x-axis here represents the absolute difference between the pixel values. On the y-axis one can see how many pixels in a private key are spread for not less than the corresponding x-value. Thus, one can think of this plot as a cumulative function of pixel differences in a private key. We can see that on average most of the private key pixel values in the test set are spread apart for more than the $\varepsilon \approx 26$ pixels imposed in the loss.

%   Also from the Figure \ref{fig:sorted-diff} one can see the illustration for Lemma \ref{lemma:l_inf_lemma}. The noising attack error shown in Table \ref{table:attack-errors} is equal to 0.057. Recall, that in our experiments maximum $\|\cdot\|_\infty $ of the noise was set to be 25. From Lemma \ref{lemma:l_inf_lemma} it follows that the corresponding maximum difference between pixels that can potentially get corrupted by this attack is equal to 50. As we can see from Figure \ref{fig:sorted-diff}, the ratio of pixels, whose difference does not exceed 50 on average is equal to 0.66. This ratio is indeed greater than the experimentally measured error, as the lemma predicts.


% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \includegraphics[width=0.8\linewidth]{Figs-TEMP/plots/diff_distribution.png}
% \end{center}
% \caption{Distribution of absolute differences of private key pixel values depending on their place in the top of differences of a given image. Best viewed in colour.}
% \label{fig:diff-dist}
% \end{figure}

% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \includegraphics[width=0.6\linewidth]{Figs-TEMP/plots/pixel_sorted_difference_w_eps.png}
% \end{center}
% \caption{Cumulative function of absolute difference between private key pixels. It represents the number of private key pairs, whose difference is greater than the given threshold.}
% \label{fig:sorted-diff}
% \end{figure}




\begin{table*}[!htb]
\caption{Average bit-wise error after watermark removal attacks. The column ``Generation'' corresponds to the average bit-wise error of the watermarking process. The best results are highlighted in \textbf{bold}.}
\label{table:attack-errors}
\begin{center}
\resizebox{0.74\linewidth}{!}{
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Generation}
&\multicolumn{1}{c}{Brightness}
&\multicolumn{1}{c}{Contrast $+$}
&\multicolumn{1}{c}{Contrast $-$}
&\multicolumn{1}{c}{Gamma}
&\multicolumn{1}{c}{JPEG}
\\ \midrule 
Ours   &$0.0008$  &$\textbf{0.002}$    &$\textbf{0.002}$ &$\textbf{0.998}$ &$\textbf{0.003}$ &$0.147$ \\
Stable signature  &$0.01$    &$0.03$ &$0.02$ &--- &--- &$0.12$ \\
SSL watermarking & $\textbf{0.00}$ & $0.06$ & $0.04$ & ---& --- &$\textbf{0.04}$ \\

AquaLora&  $ 0.0721$ & $0.137$ & $0.137$ & ---& --- &$0.0508$ \\
%WOUAF & $0.01$ & $0.06$ & $0.04$ & ---& --- &$\textbf{0.04}$

\midrule 
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Hue}
&\multicolumn{1}{c}{Saturation}
&\multicolumn{1}{c}{Sharpness}
&\multicolumn{1}{c}{Noise}
&\multicolumn{1}{c}{PGD}
\\ \midrule 
Ours     &$\textbf{0.01}$    &$0.1$ &$\textbf{0.0008}$ &$\textbf{0.057}$ &$\textbf{0.064}$ & \\
Stable signature & ---    &$\textbf{0.01}$ &$0.01$ &--- &--- & \\
SSL watermarking & $0.06$ &--- &--- &--- &--- &  \\
% PSNR     &30.0      &29.4 \\
% LPIPS    &-         &0.0072 \\

AquaLora& $0.137$ & $0.137$ & --- & $0.07$  & --- \\

%WOUAF & $\textbf{0.00}$ & $0.06$ & $0.04$ & ---& --- & \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}

\begin{table*}[!htb]
\caption{TPRs under different types of watermark removal attacks, attribution problem. We use $k=10$ different private keys and fix FPR = $10^{-6}$. Such a FPR is achieved when $\tau_1 = 25$ and $\tau_2 = 75$ from Eq. \ref{eq:indicator}. The parameters of removal attacks are presented in Section \ref{subseq:attack_details}. The best results are highlighted in \textbf{bold}.}
\label{table:collision}
\begin{center}
\resizebox{0.74\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Generation}
&\multicolumn{1}{c}{Brightness}
&\multicolumn{1}{c}{Contrast $+$}
&\multicolumn{1}{c}{Contrast $-$}
&\multicolumn{1}{c}{Gamma}
&\multicolumn{1}{c}{JPEG}
\\ \midrule 
Ours &$\textbf{1.000}$   &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$0.444$ \\
Stable signature &$0.998$   &$0.927$ &$0.984$ &--- &--- &$0.784$ \\
AquaLora &$0.998$   & $0.941$ & $0.941$ &--- &--- &$\textbf{0.998}$ \\
WOUAF &$\textbf{1.000}$   &$0.975$ &--- &--- &--- &$0.969$ \\
 
 \midrule 
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Hue}
&\multicolumn{1}{c}{Saturation}
&\multicolumn{1}{c}{Sharpness}
&\multicolumn{1}{c}{Noise}
&\multicolumn{1}{c}{PGD}
\\ \midrule 
Ours &$\textbf{1.000}$    &${0.653}$ &$\textbf{1.000}$ &$\textbf{0.971}$ &$\textbf{0.862}$ & \\
Stable signature &---   &$\textbf{0.998}$ &--- &$0.776$ &$0.747$ & \\
% PSNR     &30.0      &29.4 \\
% LPIPS    &-         &0.0072 \\
AquaLora & $0.941$   & ${0.941}$ &--- & $0.958$ &--- &  \\
WOUAF & ---   &--- &--- &${0.970}$ &--- & \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}

\begin{table*}[!htb]
\caption{TPRs under different types of watermark removal attacks, detection problem. We use $k=10$ different private keys and fix FPR = $10^{-6}$. Such a FPR is achieved when $\tau_1 = 25$ and $\tau_2 = 75$ from Eq. \ref{eq:indicator}. The parameters of removal attacks are presented in Section \ref{subseq:attack_details}. The best results are highlighted in \textbf{bold}.}
\label{table:orig-img-error}
\begin{center}
\resizebox{0.74\linewidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Generation}
&\multicolumn{1}{c}{Brightness}
&\multicolumn{1}{c}{Contrast $+$}
&\multicolumn{1}{c}{Contrast $-$}
&\multicolumn{1}{c}{Gamma}
&\multicolumn{1}{c}{JPEG}
\\ \midrule 
Ours &$\textbf{1.000}$    &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$0.444$ \\
Stable signature &$\textbf{1.000}$   &$0.862$ &$0.984$ &--- &--- &$0.217$ \\
SSL watermarking &$1.000$    &$0.940$ &$0.960$ &--- &--- &$\textbf{0.810}$
\\ \midrule 
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Hue}
&\multicolumn{1}{c}{Saturation}
&\multicolumn{1}{c}{Sharpness}
&\multicolumn{1}{c}{Noise}
&\multicolumn{1}{c}{PGD}
\\ \midrule 
Ours &$\textbf{1.000}$    &${0.653}$ &$\textbf{1.000}$ & $\textbf{0.971}$ &$\textbf{0.862}$ \\
Stable signature &---    &$\textbf{0.998}$ &---  &$0.406$ &$0.505$ \\
SSL watermarking &$\textbf{1.000}$    &--- &--- &--- &--- \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}











To estimate the TPR in the attribution problem, we extract $k=10$ different watermarks from the watermarked images. To extract a different watermark, we randomly generate $k=10$ different private keys to simulate other users. The results are reported in Table \ref{table:collision} together with the TPRs under different watermark removal attacks. Note that the PGD attack in this setting is aimed at restoring the original watermark.  To estimate the TRP in the watermark detection problem, we do the same procedure for non-watermarked images generated by the Stable Diffusion model and extract $k=10$ different watermarks. The results are presented in Table \ref{table:orig-img-error}. %You can also find here the standard deviation of post-generation watermark extraction (without attacks). 


% new tab
% \setlength\intextsep{0pt}
% \begin{wraptable}[10]{r}{0cm}
%     \resizebox{0.37\textwidth}{!}{
%         \centering
%         \begin{tabular}{c c c}
%             \toprule
%             Metric   &Stable Signature  &Ours  \\
%             \midrule
%             SSIM $\uparrow$   &\textbf{0.89}  &0.86  \\
%             PSNR $\uparrow$   &\textbf{30.0}  &29.4  \\
%             FID $\downarrow$  &19.6           &\textbf{13.2}  \\
%             LPIPS $\downarrow$ &---           &0.0072  \\
%             \bottomrule
%             \vspace{-5mm}
%         \end{tabular}
%     }
%     \caption{Image quality metrics. The best results are highlighted in \textbf{bold}.}
%     \label{table:img-quality}
% \end{wraptable}
% new tab
Note that our framework yields both low misattribution and misdetection rates according to the two-tail detection and attribution rules from the Eq. \ref{eq:attribution}. The proposed approach yields watermarks that are provably robust to additive perturbations of a bounded magnitude, multiplicative perturbations of any kind, and exponentiation. 

\subsection{Increasing the Robustness to Watermark Removal Attacks}
It is noteworthy that the set of watermark removal attacks described in Section \ref{subseq:attack_details} does not include geometric transformations. The version of the proposed method described in Section \ref{sec:method} does not provide robustness against geometric transformations of the image, however, the method can be extended to support them.  In Section \ref{A:additional_exps} of the supplementary, we discuss an extension of the proposed method to provide robustness against rotations, and translations. %, and JPEG compression of the image. 
An extension is based on a simultaneous embedding of the watermark in the pixel space and special functions, which are invariant to certain transformations. An intuition behind this extension is as follows. Suppose that the transform $\gamma:\mathbb{R}^d \to \mathbb{R}^d$ is invariant under parametric perturbation $\phi: \mathbb{R}^d \times \Theta \to \mathbb{R}^d$, namely, $\gamma(x) =\gamma(\phi(x,\theta))$ for all $x \in \mathbb{R}^d,\ \theta \in \Theta$. Then, if the watermark is  successfully embedded into $\gamma(x)$ it becomes robust under perturbation $\phi$.

% since  robust against both kinds of errors, since in the case of two-tailed detection extraction error equal to $0.5$ means a complete divergence. The intuition behind such result is that for images with $3\times512\times512$ pixels there is a precious little chance of overlapping between sets of just $2\times100$ pixels and, on top of that, the differences on the pixels' positions almost never coincide from one public key to another. 



\begin{table}[tb]
\caption{Image quality metrics. The best results are highlighted in \textbf{bold}.}
\label{table:img-quality}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{Metric}  &\multicolumn{1}{c}{St. Sign.} &\multicolumn{1}{c}{AquaLora} &\multicolumn{1}{c}{WOUAF}
&\multicolumn{1}{c}{Ours}
\\ \midrule 
SSIM  $\uparrow$   &${0.89}$    & $\textbf{0.92}$ & --- &$0.86$ \\
PSNR  $\uparrow$   &$\textbf{30.0}$    & $29.42$&   --- &$29.4$ \\
FID  $\downarrow$    &$19.6$      & $24.72$ & $> 15.0$ &$\textbf{13.2}$\\
LPIPS   $\downarrow$ & ---     & ---  &   --- &$0.0072$ \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}



\begin{figure*}[tb]
% \label{fig:qual-with-diff}
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0pt}
    \resizebox{0.87\linewidth}{!}{
    \begin{tabular}{c @{\hspace{0.1cm}} c @{\hspace{0.1cm}} c}
        % \toprule
        \multicolumn{1}{c}{\large Original image} & \multicolumn{1}{c}{\large Watermarked image} & \multicolumn{1}{c}{\large Pixel-wise difference ($\times 10$)} \\

        % Original image & Watermarked image & Pixel-wise difference ($\times 10$) \\
        % \midrule
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/0_nw.png} &
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/0_w.png} &
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/0_diff.png} \\ 
        \rule{0pt}{0.2cm}
\includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/1_nw.png} &
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/1_w.png} &
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/1_diff.png} \\ 
        \rule{0pt}{0.2cm}
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/2_nw.png} &
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/2_w.png} &
        \includegraphics[width=0.33\linewidth]{Figs-TEMP/qual/2_diff.png} \\       
        % \bottomrule\\
    \end{tabular}
    }
    % \captionsetup{font=small}
    \caption{
    Examples of watermarked images. The maps of absolute pixel-wise difference between source images and the generated ones were added for  illustrational purposes. }
    \label{fig:qual-with-diff}
\end{figure*}





% \begin{table}[tb]
% \caption{TRPs after watermark removal attacks, detection problem. The column ``Generation'' corresponds to TPR of the watermarked images. The best results are highlighted in \textbf{bold}.}
% \label{table:tpr-attack}
% \begin{center}
% \begin{tabular}{cccccccccc}
% \toprule
% \multicolumn{1}{c}{Method}
% &\multicolumn{1}{c}{Generation}
% &\multicolumn{1}{c}{Brightness}
% &\multicolumn{1}{c}{Contrast $+$}
% &\multicolumn{1}{c}{Contrast $-$}
% &\multicolumn{1}{c}{Gamma}
% &\multicolumn{1}{c}{JPEG}
% \\ \midrule 
% Ours   &$0.0008$  &$\textbf{0.002}$    &$\textbf{0.002}$ &$\textbf{0.998}$ &$\textbf{0.003}$ &$0.147$ \\
% Stable signature  &$0.01$    &$0.03$ &$0.02$ &--- &--- &$0.12$ \\
% SSL watermarking & $\textbf{0.00}$ & $0.06$ & $0.04$ & ---& --- &$\textbf{0.04}$
% \\ \midrule 
% \multicolumn{1}{c}{Method}
% &\multicolumn{1}{c}{Hue}
% &\multicolumn{1}{c}{Saturation}
% &\multicolumn{1}{c}{Sharpness}
% &\multicolumn{1}{c}{Noise}
% &\multicolumn{1}{c}{PGD}
% \\ \midrule 
% Ours     &$\textbf{0.01}$    &$0.1$ &$\textbf{0.0008}$ &$0.057$ &$0.064$ & \\
% Stable signature & ---    &$\textbf{0.01}$ &$0.01$ &--- &--- & \\
% SSL watermarking & $0.06$ &--- &--- &--- &--- &  \\
% % PSNR     &30.0      &29.4 \\
% % LPIPS    &-         &0.0072 \\
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}









% \subsection{Ablation study}

% Structured watermark

% MSE regularization

% Varying epsilon in Eq. \ref{eq:wm_insert}


% \subsection{Limitations}

% Note that the proposed approach has several limitations. First of all, since the watermarking is performed during the model's inference, it affects both the inference time and, in some cases, the quality of the generated images: the watermarked images can have artifacts in contrast to their non-watermarked counterparts. See Fig.  \ref{fig:artifacts} in Supplementary for details. Note that these artifacts, although visible, barely spoil the images' content. Secondly, the proposed watermarking method does not provide robustness against cropping,  rotation, and translation attacks. However, the robustness to rotation and translation can be achieved by inserting the watermarks in the frequency domain of the image. 


% Our approach bears certain limitation that stem from the very way of the watermark insertion in our case. First, at certain images private key pixels get located at regions of an image, where the colour is constant within the substantial vicinity of this pixel. And so, when we enforce the pixel to change its value by optmimizing the latents in a corresponding way, the decoded image starts to have artifacts $-$ stains of a colour that does not match the original background, see image \ref{fig:artifacts}. Most of the time, such artifacts are barely perceptible and do not spoil the main content of an image.

% \begin{figure}[tb]
% % \label{fig:qual-with-diff}
%     \centering
%     \scriptsize
%     \setlength{\tabcolsep}{0pt}
%     \resizebox{0.8\linewidth}{!}{
%     \begin{tabular}{c @{\hspace{0.1cm}} c @{\hspace{0.1cm}} c}
%         % \toprule
%         \large Original image & \large Watermarked image & \large Pixel-wise difference ($\times 10$) \\
%         % Original image & Watermarked image & Pixel-wise difference ($\times 10$) \\
%         % \midrule
%         \includegraphics[width=0.33\linewidth]{Figs-TEMP/artifacts/0_nw.png} &
%         \includegraphics[width=0.33\linewidth]{Figs-TEMP/artifacts/0_w.png} &
%         \includegraphics[width=0.33\linewidth]{Figs-TEMP/artifacts/0_diff.png} \\ 
%         \rule{0pt}{0.2cm}
% \includegraphics[width=0.33\linewidth]{Figs-TEMP/artifacts/1_nw.png} &
%         \includegraphics[width=0.33\linewidth]{Figs-TEMP/artifacts/1_w.png} &
%         \includegraphics[width=0.33\linewidth]{Figs-TEMP/artifacts/1_diff.png} \\       
%         % \bottomrule\\
%     \end{tabular}
%     }
%     % \captionsetup{font=small}
%     \caption{
%      Examples of watermarked images with artifacts.}
%     \label{fig:artifacts}
% \end{figure}


% Another obvious weakness of our solution is its complete inability to withstand such attacks as cropping and rotation. Watermark cannot be extracted from cropped image, since in such a case we cannot access watermark pixels, so this attack is out of our scope by design. Same holds for the rotation: even slightest turn of an image disrupts the pixel order and, hence, erases the watermark. Still, we have to note that to the best of our knowledge there has not been proposed any watermarking technique that would be robust to the rotation attack.    

\section{Conclusion and Future Work}
In this paper, we propose \emph{Spread them Apart}, the framework to watermark generated content of continuous nature and apply it to images generated by Stable Diffusion. We prove that the watermarks produced by our method are provably robust against additive watermark removal attacks of a bounded norm and are provably robust to multiplicative perturbations by design. Our approach can be used to both detect that the image is generated by a given model and to identify the end-user who generated it. Experimentally, we show that our method is comparable to the state-of-the-art watermarking methods in terms of the invisibility of watermark and the robustness to synthetic watermark removal attacks. 










% \begin{contributions} % will be removed in pdf for initial submission 
% 					  % (without ‘accepted’ option in \documentclass)
%                       % so you can already fill it to test with the
%                       % ‘accepted’ class option
%     Briefly list author contributions. 
%     This is a nice way of making clear who did what and to give proper credit.
%     This section is optional.

%     H.~Q.~Bovik conceived the idea and wrote the paper.
%     Coauthor One created the code.
%     Coauthor Two created the figures.
% \end{contributions}

% \begin{acknowledgements} % will be removed in pdf for initial submission,
% 						 % (without ‘accepted’ option in \documentclass)
%                          % so you can already fill it to test with the
%                          % ‘accepted’ class option
%     Briefly acknowledge people and organizations here.

%     \emph{All} acknowledgements go in this section.
% \end{acknowledgements}

% References
\bibliography{uai2025-template}

\newpage

\onecolumn

\title{Spread them Apart: Towards Robust Watermarking of Generated Content\\(Supplementary Material)}
\maketitle



\appendix
\section{Improving the Robustness to Watermark Removal Attacks}
\label{A:additional_exps} 
Note that the version of the watermarking method proposed in this paper is based on the embedding of the watermark into the pixel domain of a generated image. Hence, the method can not yield watermarks provably robust against geometric transformations of the image, such as rotations and translations. %, as well as struggles against JPEG compression.  
In this section, we describe an extension of the method to guarantee robustness against such transformations. An extension is done by embedding the watermark simultaneously into the pixel domain and special representations in the frequency domain, which are invariant under rotations and translations. 


\subsection{Invariants in the frequency domain}
We will call image transform $\gamma:\mathbb{R}^d \to \mathbb{R}^d$ \emph{invariant} under parametric perturbation $\phi: \mathbb{R}^d \times \Theta \to \mathbb{R}^d$ at point $x\in \mathbb{R}^d$ if 
\begin{equation}
    \gamma(x) = \gamma(\phi(x, \theta)) \quad \text{for all}\quad  \theta \in \Theta,
\end{equation}
where $\Theta $ is the set of parameters of perturbation $\phi$. In this work, we use two invariants discussed in \cite{lin1993towards}, formulated as theorems below. 


\begin{theorem}
    \label{th:trans_inv}
    Let $h(x,y)$ be an integrable nonnegative function and its Fourier transform 
    \begin{equation}
       H(\omega_x, \omega_y) = \int_{\infty}^\infty \int_{-\infty}^\infty h(x,y) e^{-i(x \omega_x +y\omega_y)}dxdy = A(\omega_x, \omega_y)e^{-i \psi(\omega_x, \omega_y)}
    \end{equation}
     be twice differentiable. Then the function $A(\omega_x, \omega_y)$ is invariant under translation. 
\end{theorem}


 
\begin{theorem}
\label{th:rot_inv}
    Let $\tilde{h}(r,t) = h(e^r \cos t, e^r \sin t)$ be the logarithmic-polar representation of the image $h(x,y)$. The Fourier-Mellin transform of $\tilde{h}(r,t)$ is
    \begin{equation}
        \tilde{H}(\omega, k) = \int_{-\infty}^\infty \int_0^{2\pi} \tilde{h}(r,t)e^{-i(kt +\omega r)}dt dr = \tilde{A}(\omega, k)e^{-i\tilde{\psi}(\omega,  k)},
    \end{equation}
    where $\tilde{A}(\omega, k)$ is the magnitude and $\tilde{\psi}(\omega, k)$ is the phase. 

    If $\tilde{h}(r,t)$ is an integrable nonnegative function and its Fourier-Mellin transform $\tilde{H}(\omega, k)$ is twice differentiable, then the function $\tilde{A}(\omega, k)$ is invariant under rotation. 
\end{theorem}
\begin{remark}
    For consistent notation, we assume that the image $h(x,y)$ is a scalar function on a two-dimensional plane. Later, we refer to invariants from Theorems \ref{th:trans_inv}-\ref{th:rot_inv} as to $\gamma_t$ and $\gamma_r$, respectively. 
\end{remark}

\subsection{Spread Them Apart: Three Watermarks Instead of One}
Recall from Section \ref{sec:method} that the watermark embedding process in the pixel domain is done by optimizing the loss function $\mathcal{L}$ in the form from Eq. \ref{eq:TotalLoss}: $\mathcal{L} = \lambda_{wm}\mathcal{L}_{wm} + \lambda_{qual}\mathcal{L}_{qual},$ where 

\begin{equation}
    \mathcal{L}_{wm} = \sum_{i=1}^n \min((-1)^{w_i}(x_{a_i} - x_{b_i}) + \varepsilon, 0), \quad x = \mathcal{D}(z),
\end{equation}
$w(u_i) = \{w_1, \dots, w_n\}$ is the watermark assigned to user $u_i$ and $s(u_i) = \{a_1, \dots, a_n, b_1, \dots, b_n\}$ is the secret of user $u_i$. 

To ensure the robustness of the watermark to geometric transformations, we suggest embedding the watermark simultaneously in the pixel domain and in invariants $\gamma_t$ and $\gamma_r$. To do so, we optimize the loss function $\tilde{\mathcal{L}}$ in the form below:

\begin{equation}
\label{eq:three_wms}
    \tilde{\mathcal{L}} = \lambda_{wm}\mathcal{L}_{wm} + \lambda_{qual}\mathcal{L}_{qual} + \lambda_t \mathcal{L}_t + \lambda_r \mathcal{L}_r = \mathcal{L} + \lambda_t \mathcal{L}_t + \lambda_r \mathcal{L}_r.
\end{equation}

In Eq. \ref{eq:three_wms}, $\lambda_t, \ \lambda_r$ are positive constants and 

\begin{align}
        & \mathcal{L}_{t} = \sum_{i=1}^n \min((-1)^{w_i}(\gamma_t(x)_{a_i} - \gamma_t(x)_{b_i}) + \varepsilon, 0), \\
         & \mathcal{L}_{r} = \sum_{i=1}^n \min((-1)^{w_i}(\gamma_r(x)_{a_i} - \gamma_r(x)_{b_i}) + \varepsilon, 0)
\end{align}
are the loss functions that control the embedding of the watermark into invariants $\gamma_t(x)$ and $\gamma_r(x)$, respectively. 


\subsection{Extractions of watermarks}

Given $m$ as the number of users,  the owner of the generative model extracts $3m$ watermarks from the given image $x$. Namely, given the secret $s(u_i)$ of the user $u_i$, three watermarks, $w(u_i|x), \ w(u_i|\gamma_r(x)), \ w(u_i|\gamma_t(x))$ are restored:


\begin{align}
    &\begin{cases}
    x_{a_j} \ge x_{b_j} \Longrightarrow w(u_i|x)_j = 0, \\
    x_{a_j} < x_{b_j} \Longrightarrow w(u_i|x)_j = 1,
    \end{cases} \\
    &\begin{cases}
          \gamma_r(x)_{a_j} \ge \gamma_r(x)_{b_j} \Longrightarrow w(u_i|\gamma_r(x))_j = 0, \\
          \gamma_r(x)_{a_j} < \gamma_r(x)_{b_j} \Longrightarrow w(u_i|\gamma_r(x))_j = 1, 
    \end{cases}\\
    &\begin{cases}
            \gamma_t(x)_{a_j} \ge \gamma_t(x)_{b_j} \Longrightarrow w(u_i|\gamma_t(x))_j = 0, \\
            \gamma_t(x)_{a_j} < \gamma_t(x)_{b_j} \Longrightarrow w(u_i|\gamma_t(x))_j = 1.
    \end{cases}
\end{align}
To assign the (possibly) watermarked image $x$ to the user, the owner of the model determines three candidates:

\begin{align}
   \begin{cases}
        u = \arg\min\limits_{u_i \in [u_1, \dots, u_m]: \xi_i=1} d(w(u_i), w(u_i|x)),\\
        u_r = \arg\min\limits_{u_i \in [u_1, \dots, u_m]: \xi^r_i=1} d(w(u_i), w(u_i|\gamma_r(x))), \\ 
        u_t = \arg\min\limits_{u_i \in [u_1, \dots, u_m]: \xi^t_i=1} d(w(u_i), w(u_i|\gamma_t(x))),  
    \end{cases} 
\end{align}
where 
\begin{equation}
    \xi^p_i = \mathds{1}[d(w(u_i|\gamma_p(x)), w(u_i)) \in [0,\tau_1] \cup [\tau_2, n]]
\end{equation}
indicates which of the users' watermarks are within an appropriate distance from the corresponding extracted watermarks. 



Finally, the owner of the model assigns the image $x$ to the user $\tilde{u}$ that corresponds to the minimum distance  among all $3m$  pairs of watermarks:

\begin{equation}
    \tilde{u} = \arg\min \{d(w(u), w(u|x)), \ d(w(u_r), w(u_r|\gamma_r(x))), \ d(w(u_t), w(u_t|\gamma_t(x)))\}.
\end{equation}

\subsection{Probability of Incorrect Attribution}
\label{sec:updated_fpr}

Assume that the user $u_i$ owns the watermarked image $x$. Note that the attribution of the image to the user $u_i$ is guaranteed to hold if
\begin{equation}
    u=u_i, u_r=u_i, u_t=u_i.
\end{equation}
Hence, the probability of incorrect attribution, ${FPR}_3(m)$, is bounded from above by the sum 
\begin{equation}
    \mathbb{P}(u \ne u_i) +  \mathbb{P}(u_r \ne u_i) + \mathbb{P}(u_t \ne u_i),
\end{equation}
yielding 
\begin{equation}
    FRP_3(m) \le 3\hat{p},
\end{equation}
where $\hat{p}$ is from Eq. \ref{eq:fpr_m}. 



\subsection{Quantitative Results}

In Tables \ref{table:attribution_3wms}-\ref{table:detection_3wm}, we include rotation and translation transformations of the image. 
To rotate an image, we sample an angle of rotation $\theta_r$ uniformly from $[-10^{\circ},10^{\circ}]$;  to translate an image, we sample the vector of translation $(\theta^x_t, \theta^y_t)$ uniformly from the set $[-10^{\circ}, 10^{\circ}] \times [-10^{\circ},10^{\circ}]$. We compare the results of an extended version of the proposed method with the baseline approach. STA(1) refers to the baseline approach described in Section \ref{sec:method}, STA(3) refers to an extended approach described in Section \ref{A:additional_exps}. Note that now, according to Section \ref{sec:updated_fpr}, to achieve FPR  $= 10^{-6}$ we set $\tau_1=24$ and $\tau_2=76$. In the column ``Error'', we report the bit-wise accuracy of the watermarking procedure. 

To focus on the evaluation of the robustness of the watermarks, we report results for the images in which the watermark is embedded successfully. Namely, after embedding the watermark, we choose the watermarked images with a generation error smaller than $0.05$.

It is noteworthy that the simultaneous embedding of the watermark into the pixel domain and corresponding invariants in the frequency domain significantly improves the robustness against post-processing watermark removal attacks. The limitation of this approach is the increased complexity of the watermark embedding process which is the subject of future work.   





\begin{table*}[!htb]
\caption{TPRs under different types of watermark removal attacks, attribution problem. We use $k=10$ different private keys and fix FPR = $10^{-6}$. The best results are highlighted in \textbf{bold}. }
\label{table:attribution_3wms}
\begin{center}
\resizebox{0.88\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Error}
&\multicolumn{1}{c}{Brightness}
&\multicolumn{1}{c}{Contrast $+$}
&\multicolumn{1}{c}{Contrast $-$}
&\multicolumn{1}{c}{Gamma}
&\multicolumn{1}{c}{JPEG}
&\multicolumn{1}{c}{Translation}
\\ \midrule 
STA(1) &${0.972}$   &${0.928}$ &${0.928}$ &${0.928}$ &${0.928}$ &$\textbf{0.724}$ & $0.000$\\
STA(3) &$\textbf{0.980}$   &$\textbf{1.000}$ & $\textbf{1.000}$ & $\textbf{1.000}$ &$\textbf{0.951}$ & ${0.563}$  & $\textbf{0.962}$\\
 
 \midrule 
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Hue}
&\multicolumn{1}{c}{Saturation}
&\multicolumn{1}{c}{Sharpness}
&\multicolumn{1}{c}{Noise}
&\multicolumn{1}{c}{PGD}
&\multicolumn{1}{c}{Rotation}
\\ \midrule 
STA(1) &$\textbf{1.000}$    &${0.571}$ &$\textbf{1.000}$ &$\textbf{0.928}$ &$\textbf{0.857}$ & ${0.000}$\\
STA(3) & $\textbf{1.000}$ & $\textbf{1.000} $   &$\textbf{1.000}$ & \textbf{0.928} &$\textbf{0.857}$ &$\textbf{0.489}$ & \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}



% \begin{table*}[!htb]
% \caption{TPRs under different types of watermark removal attacks, attribution problem. We use $k=10$ different private keys and fix FPR = $10^{-6}$. The best results are highlighted in \textbf{bold}.}
% \label{table:attribution_3wms}
% \begin{center}
% \resizebox{0.88\linewidth}{!}{
% \begin{tabular}{ccccccccccc}
% \toprule
% \multicolumn{1}{c}{Method}
% &\multicolumn{1}{c}{Brightness}
% &\multicolumn{1}{c}{Contrast $+$}
% &\multicolumn{1}{c}{Contrast $-$}
% &\multicolumn{1}{c}{Gamma}
% &\multicolumn{1}{c}{JPEG}
% &\multicolumn{1}{c}{Translation}
% \\ \midrule 
% Ours &$\textbf{1.000}$   &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ \\
% Stable signature &$0.927$   &$0.984$ &--- & --- &$0.784$ & ---  \\
% AquaLora &$0.941$   & $0.941$ & ---  &--- &$0.998$ &--- \\
% WOUAF &$0.975$   &--- &--- &--- &$0.969$ &--- \\
 
%  \midrule 
% \multicolumn{1}{c}{Method}
% &\multicolumn{1}{c}{Hue}
% &\multicolumn{1}{c}{Saturation}
% &\multicolumn{1}{c}{Sharpness}
% &\multicolumn{1}{c}{Noise}
% &\multicolumn{1}{c}{PGD}
% &\multicolumn{1}{c}{Rotation}
% \\ \midrule 
% Ours &$\textbf{1.000}$    &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ & $\textbf{?}$\\
% Stable signature &---   &$0.998$ &--- &0.776 &0.747 & $0.980$\\
% % PSNR     &30.0      &29.4 \\
% % LPIPS    &-         &0.0072 \\
% AquaLora & $0.941$   & $0.941$ &--- & $0.958$ &--- & --- \\
% WOUAF & ---   &--- &--- &$0.970$ &--- & $0.999$ \\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \end{table*}






\begin{table*}[!htb]
\caption{TPRs under different types of watermark removal attacks, detection problem. We use $k=10$ different private keys and fix FPR = $10^{-6}$. The best results are highlighted in \textbf{bold}. }
\label{table:detection_3wm}
\begin{center}
\resizebox{0.88\linewidth}{!}{
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Error}
&\multicolumn{1}{c}{Brightness}
&\multicolumn{1}{c}{Contrast $+$}
&\multicolumn{1}{c}{Contrast $-$}
&\multicolumn{1}{c}{Gamma}
&\multicolumn{1}{c}{JPEG}
&\multicolumn{1}{c}{Translation}
\\ \midrule 
STA(1) &${0.972}$   &${0.928}$ &${0.928}$ &${0.928}$ &${0.928}$ &$\textbf{0.724}$ & $0.000$\\
STA(3) &$\textbf{0.980}$   &$\textbf{1.000}$ & $\textbf{1.000}$ & $\textbf{1.000}$ &$\textbf{0.951}$ & ${0.563}$  & $\textbf{0.962}$\\
 
 \midrule 
\multicolumn{1}{c}{Method}
&\multicolumn{1}{c}{Hue}
&\multicolumn{1}{c}{Saturation}
&\multicolumn{1}{c}{Sharpness}
&\multicolumn{1}{c}{Noise}
&\multicolumn{1}{c}{PGD}
&\multicolumn{1}{c}{Rotation}
\\ \midrule 
STA(1) &$\textbf{1.000}$    &${0.571}$ &$\textbf{1.000}$ &$\textbf{0.928}$ &$\textbf{0.857}$ & ${0.000}$\\
STA(3) & $\textbf{1.000}$ & $\textbf{1.000} $   &$\textbf{1.000}$ & \textbf{0.928} &$\textbf{0.857}$ &$\textbf{0.489}$ & \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}









% \begin{table*}[!htb]
% \caption{TPRs under different types of watermark removal attacks, detection problem. We use $k=10$ different private keys and fix FPR = $10^{-6}$. The best results are highlighted in \textbf{bold}. }
% \label{table:detection_3wm}
% \begin{center}
% \resizebox{0.88\linewidth}{!}{
% \begin{tabular}{ccccccccc}
% \toprule
% \multicolumn{1}{c}{Method}
% &\multicolumn{1}{c}{Brightness}
% &\multicolumn{1}{c}{Contrast $+$}
% &\multicolumn{1}{c}{Contrast $-$}
% &\multicolumn{1}{c}{Gamma}
% &\multicolumn{1}{c}{JPEG}
% &\multicolumn{1}{c}{Translation}
% \\ \midrule 
% Ours &$\textbf{1.000}$   &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ \\
% Stable signature &$0.862$    &$0.984$ &--- &--- &$0.217$ & --- \\
% SSL watermarking &$0.940$    &$0.960$ &--- &--- &$0.810$ &---
% \\ \midrule 
% \multicolumn{1}{c}{Method}
% &\multicolumn{1}{c}{Hue}
% &\multicolumn{1}{c}{Saturation}
% &\multicolumn{1}{c}{Sharpness}
% &\multicolumn{1}{c}{Noise}
% &\multicolumn{1}{c}{PGD}
% &\multicolumn{1}{c}{Rotation}
% \\ \midrule 
% Ours &$\textbf{1.000}$   &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{1.000}$ &$\textbf{?}$ \\
% Stable signature &---    &$0.998$ &--- &$0.406$ &$0.505$ & $0.980$ \\
% SSL watermarking &$\textbf{1.000}$  &--- &--- &--- &--- & $\textbf{1.000}$ \\
% \bottomrule
% \end{tabular}
% }
% \end{center}
% \end{table*}






\end{document}
