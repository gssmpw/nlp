\section{Related Work}
\subsection{Diffusion Model}

Inspired by non-equilibrium statistical physics, \cite{sohldickstein2015deepunsupervisedlearningusing} introduced the diffusion model to fit complex probability distributions. \cite{ho2020denoisingdiffusionprobabilisticmodels} introduced a new class of models called Denoising Diffusion Probabilistic Models (DDPM) by establishing a novel connection between the diffusion model and the denoising scoring matching. Later, the Latent Diffusion Model (LDM)  was developed to improve efficiency and reduce computational complexity, with the diffusion process happening within a latent space   \cite{rombach2022high}. During training, the LDM uses an encoder $\mathcal{E}$ to map an input image $x$ to the latent space: $z = \mathcal{E}(x)$. For the reverse operation a decoder $\mathcal{D}$ is employed, so that $x = \mathcal{D}(z)$. During inference, the LDM starts with a noise vector $z \sim \mathcal{N}(0, I)$ in the latent space and iteratively denoises it. The decoder then maps the final latent representation back to the image space.

\subsection{Watermarking of Digital Content}
Watermarking has been recently adopted to protect the intellectual property of neural networks (\cite{wu2020watermarking,pautov2024probabilistically}) and generated content (\cite{kirchenbauer2023watermark,zhaoprovable,fu2024watermarking}). In a nutshell, watermarking of generated content is done by injection of digital information within the generated image allowing the subsequent extraction.  Existing methods of digital content watermarking can be divided into two categories: content-level watermarking and model-level watermarking. The methods of content-level watermarking operate in some representation of content, for example, in the frequency domain of the image signal (\cite{o1996watermarking,cox1996secure}). When the image is manipulated in the frequency domain, the watermark embedding process can be adapted to produce watermarks that are robust to geometrical image transformations, such as rotations and translations (\cite{wen2024tree}). Model-level watermarking approaches are designed to embed information during the generation process. In end-to-end methods, the models to embed and extract watermark are learned jointly (\cite{zhu2018hidden,hayes2017generating}). In \cite{yu2021artificial}, it was proposed to teach the watermark encoder on the training data of the generative model; such an approach yields a watermarking scheme that is conditioned on the generative model and its training dataset. This method was later adapted to latent diffusion models (\cite{fernandez2023stable}) and unconditional diffusion models (\cite{zhao2023recipe}). In contrast, there are methods that do not require additional model training. These methods are designed to alter the output distribution of the generative model to embed previously learned watermark into the model or the content itself (\cite{kirchenbauer2023watermark,wen2024tree}). 


\subsection{Robustness to Watermark Removal Attacks}
Watermarking attacks are aimed at removing the watermark embedded into the model's weights or generated content. In the prior works on removing the watermarks from generated images (\cite{li2019towards,cao2019generative}), the attack problem is formulated in terms of the image-to-image translation task, and methods to remove watermarks via an auxiliary generative adversarial network are presented. Other approaches (\cite{hertz2019blind,liang2021visible,sun2023denet}) perform watermark removal in two steps: firstly, the visual watermark is localized within an image; secondly, it is removed via a multi-task learning framework. 

In practice, watermarking scheme has to be robust to destructive and constructive attacks, or synthetic transformations of the data. Destructive transformations,  such as brightness and contrast adjustment, geometric transformations, such as rotations and translations, compression methods, and additive noise are aimed at watermark removal by applying a transformation. In contrast, constructive attacks treat watermarks as noise and are aimed at the restoration of original content (\cite{zhang2024robust}). It is usually done by applying purification techniques, such as Gaussian blur (\cite{hosam2019attacking}) or image inpainting (\cite{liu2021wdnet,xu2017automatic}).

Signal Processing Attacks focus on noise addition, compression, and filtering. Robust watermarking schemes based on frequency domain transformations and randomizing offered higher resilience against these types of attacks (\cite{Taran_2019_CVPR}).