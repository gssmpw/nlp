\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage[most]{tcolorbox}
\usepackage{colortbl}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
% Useful packages
\usepackage{amsmath,amsfonts,bm}
\usepackage{graphicx,float}
% \usepackage{natbib}
\usepackage[style=alphabetic,backend=biber,maxcitenames=3,maxbibnames=5,natbib=true]{biblatex}
\usepackage{csquotes}
\addbibresource{sample.bib}
\usepackage{booktabs}
\usepackage{authblk}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{xcolor}
\newcommand{\E}{\mathbb{E}}

\newcommand{\cN}{\mathcal{N}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\sgn}{sgn}
\newcommand{\cyan}{\color{cyan}}
\newcommand{\argmax}{\arg\max}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]

\newcommand{\wl}[1]{\textcolor{orange}{(Wenlong: #1)}}
\newcommand{\jx}[1]{\textcolor{green}{(Jing: #1)}}
\newcommand{\wz}[1]{\textcolor{blue}{(Weizhe: #1)}}
\newcommand{\lz}[1]{\textcolor{cyan}{(Linjun: #1)}}
\newcommand{\sm}[1]{\textcolor{red}{(Song: #1)}}
\newcommand{\wjs}[1]{\textcolor{brown}{(Weijie: #1)}}



\title{An Overview of Large Language Models for  Statisticians}

%\date{\today}


%\author{Wenlong Ji* (Stanford), Weizhe Yuan* (NYU and FAIR), Emily Getzen (UPenn?), Kyunghyun Cho (NYU), Peter Bartlett (UC Berkeley and Google), Michael Jordan (INRIA and UC Berkeley), Song Mei (UC Berkeley), Jason Weston (FAIR), Weijie Su** (UPenn), Jing Xu** (FAIR), Linjun Zhang** (Rutgers)}

\newcommand{\super}[1]{{\scriptscriptstyle(#1)}}
\renewcommand\sup[1]{$^{#1}$}


\author{~~~~~~~~Wenlong Ji\sup{1,}$^{\ast}$~~~~~Weizhe Yuan\sup{2,3,}\thanks{\hspace{.5em}Co-first authors}~~~~~~{Emily Getzen\sup{4}}
 \newline ~~~~{Kyunghyun Cho\sup{2}}%{Peter L. Bartlett\sup{5}}~~~~ 
~~~~Michael I. Jordan\sup{5,6}~~~~Song Mei\sup{5}~~~~Jason Weston\sup{2,3}
 \newline {Weijie J.\ Su\sup{4,}$^{\dagger}$}~~~~{Jing Xu\sup{3,}$^{\dagger}$}~~~~{Linjun Zhang\sup{7,}}\thanks{\hspace{.5em}Co-corresponding authors}  
\\
\textsuperscript{1}Stanford University \quad
\textsuperscript{2}New York University \quad
\textsuperscript{3}Meta FAIR \quad
\textsuperscript{4}University of Pennsylvania \quad \\
\textsuperscript{5}UC Berkeley \quad
\textsuperscript{6}INRIA \quad
%\textsuperscript{}Google DeepMind \quad
\textsuperscript{7}Rutgers University
}

% \if 0
% \author[1]{Wenlong Ji$^*$}
% \author[2]{Weizhe Yuan$^*$}
% \author[3]{Emily Getzen}
% \author[4]{Kyunghyun Cho}
% \author[5]{Peter L. Bartlett}
% \author[6]{Michael I. Jordan}
% \author[7]{Song Mei}
% \author[8]{Jason Weston}
% \author[9]{Weijie Su$^\$$}
% \author[10]{Jing Xu$^\$$}
% \author[11]{Linjun Zhang$^\$$}
% \affil[1]{Stanford University}
% \affil[2,4]{New York University}
% \affil[3,9]{University of Pennsylvania}
% \affil[7]{INRIA}
% \affil[5,6,7]{UC Berkeley}
% \affil[5]{Google DeepMind}
% \affil[2,8,10]{FAIR}
% \affil[11]{Rutgers University}
% \fi
\date{}                     %% if you don't need date to appear
\setcounter{Maxaffil}{0}
\renewcommand\Affilfont{\itshape\small}
\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems---in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift---require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users.  Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges.
\end{abstract}
% \section{Outline}
% Punchline: don't view AI as a challenge, it is more than an opportunity.
% Don't be grumpy; Wear AI T-shirt.
% \begin{enumerate}
%     \item Introduction
%     \begin{enumerate}
% \item
% Overview of Large Language Models (LLMs)
% \item 
% Importance of LLMs in the context of statistical analysis and data science
% \item
% Objectives and structure of the article
% \end{enumerate}
% \item Background and Fundamentals of LLMs
%     \begin{enumerate}
% \item Brief history of language models and the evolution to LLMs
% \item Basic principles of LLMs (e.g., neural networks)
% \item Overview of key LLMs (e.g.,  Transformer models, BERT, GPT)
% \item Tokenization and embeddings (*)
% \item The role of data and computation in training LLMs
% \item Pre-training and Fine-Tuning (RLHF/DPO)
% \end{enumerate}
% \item LLMs in Statistical Analysis
%     \begin{enumerate}
% \item Statistical methodologies employed in LLMs
% \item Application of LLMs in exploratory data analysis
% %\item Use of LLMs for hypothesis testing and inference
% \item Case studies illustrating the use of LLMs in statistical projects
% \item LLM-assisted research: EHR, proof (benchmark: reasoning, compound systems such as AlphaGeomotry), coding, a quick layman intro of certain areas (two types of audience: want to do stat; want to do AI research). AI-assisted experimental design based on hypothesis; LLM agent; synthetic data as prior (Bayesian); data cleaning. 
% \end{enumerate}
% \item Integration of Statistical Methods in LLM Development
% \begin{enumerate}
% \item Incorporating statistical rigor into the training of LLMs: calibration, uncertainty
% \item Representation learning in LLMs
% \item Evaluation metrics for LLMs from a statistical perspective: evaluation and testing with confidence
% \item Addressing bias and variance in LLMs through statistical methods (sparsity, Lora, model editing)
% \item Fine-Tuning LLMs with finite-sample guarantees: debiasing, alignment
% \item privacy and fairness, watermark
% \item causal 
% \end{enumerate}
% \item Challenges and Considerations (statistical challenge?)
% \begin{enumerate}
% \item Computational and data requirements for training and deplo
% \item Ethical considerations and bias in model outputs
% \item Reproducibility and transparency in LLM research
% \item Statistical challenges in interpreting LLM outputs
% \end{enumerate}
% \item Practical Applications of LLMs for Statisticians
% \begin{enumerate}
% \item Automated data analysis and report generation
% \item Predictive modeling and forecasting
% \item Text analysis and sentiment analysis in large datasets
% \item Enhancing statistical software with LLM capabilities
% \end{enumerate}
% \item Future Directions and Research Opportunities
% \begin{enumerate}
%     \item 
% Advances in model architectures and training techniques
% %\item Integrating LLMs with other AI technologies (e.g., computer vision)
% \item New frontiers in statistical methodology arising from LLM research
% \item Collaborative opportunities between statisticians and AI researchers
% \end{enumerate}


% \end{enumerate}
\tableofcontents

% \wjs{some minor comments. 1. tense is not consistent, maybe use paste tense in all cases such as [ABC] proposed a method for simplicity; 2. the use of ``paragraph\{\}'' is not consistent, sometimes it is not capitalized sometimes it is, sometimes it ends with a period, sometimes it is not; 3. abbreviations. (RLHF) abbrevited 3 times.}
%\lz{do we need to add a  figure or table in the introduction?}

%{\it how the statisticians should grow our territory to engage with the development of AI?} 


\section{Introduction}
Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks from text generation to dialog to complex reasoning. 
%Statisticians, with their expertise in modeling, inference, and uncertainty quantification, are uniquely positioned to address fundamental questions about LLMs. %[add a few sentences about why statisticians need to engage with AI] 
As these models increase in scale and sophistication, an important question arises: 
{\it in the era of LLMs, how can statisticians play a role in guiding the design and deployment of large-scale AI models?}

\begin{figure*}[h]
    \centering
\includegraphics[width=0.7\linewidth]{figs/nex_token_pred.pdf}
    \caption{An illustration of next-token prediction by LLMs.}% (draft figure)}
    \label{fig:teaser}
\end{figure*}

At their core, language models are models that assign probabilities to upcoming words, or sequences of words. Specifically, a language model $f$ aims to learn a mapping that takes a sequence of tokens $\bm{x}_{[T]}=x_1, x_2, \cdots, x_T$ as input, and outputs a probability vector $f(\bm x_{[T]})$ over the token vocabulary for the next-token $x_{T+1}$. While simple in formulation, when applied to vast corpora of text, this probabilistic approach allows LLMs to capture intricate patterns, including those that were classically the province of syntax, semantics, and logic. For example, given an instruction such as ``Write an engaging short story" or ``Let’s think step by step," LLMs can dynamically adapt their responses to follow the given directive, generating outputs that align with the user's intent. This instruction-following capability is a result of extensive training on diverse datasets combined with fine-tuning techniques that align model outputs with user preferences. While much of this work takes inspiration from statistical principles, as well as principles from optimization theory, there is a large gap between problem formulations and the design of statistical procedures that target the specific architectures and data-collection protocols used in LLMs.






% LLM use case (what are instruction following) take example from WildChat, rewrite myself, LLM response
% MATHish, story-writing -> instruction following diverse
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!80!black, title=Example of a LLM response to a user instruction on Creative Writing]
    \textbf{User:} Write an engaging short story about a group of statisticians going hiking. Provide a detailed background for the statisticians and describe the fun and challenges they face during the journey. Incorporate rich details to bring the story to life. \\  
    \textbf{LLM:} \\
    {Title: The Law of Large Trails} \\
It all started as a joke in the statistics department. Every Wednesday, after the weekly seminar, a group of faculty and graduate students would gather in the common lounge, sharing coffee, complaints, and the occasional proof sketch scribbled on napkins. One particularly dull winter afternoon, Professor Strawderman sighed and said, “You know, we spend all our time analyzing uncertainty in data, but when was the last time we faced real uncertainty? Like, say... in nature?”...
\end{tcolorbox}


\begin{tcolorbox}[colback=violet!5!white, colframe=violet!80!black, title=Example of a LLM response to a probability question with Chain-of-Thought Prompting]
    \textbf{User:} You are an expert in solving advanced probability theory problems. Carefully solve the following step by step, ensuring that every calculation and assumption is explained: What is the probability of getting at least one 6 when rolling two fair six-sided dice? Let's think step by step. \\ 
    \textbf{LLM:} 
\begin{itemize}
\item For each die, there are 6 possible outcomes. The total number of possible outcomes when rolling two dice is: $6 \times 6 = 36 $. We want the probability of getting at least one 6. This includes all cases where one or both dice show a 6. It's easier to calculate the complement: the probability of not getting a 6 on either die. 
\item Probability of not getting a 6 on a single die: For one die, there are 5 outcomes where we do not get a 6 (i.e., 1, 2, 3, 4, or 5). The probability of not getting a 6 on one die is: $\mathbb{P}(\text{not 6 on one die}) = \frac{5}{6}$.
\item Probability of not getting a 6 on both dice: Since the dice rolls are independent, the probability of not getting a 6 on both dice is: $\mathbb{P}(\text{not 6 on both die}) = \frac{5}{6} \times \frac{5}{6} = \frac{25}{36}$.
\item Probability of getting at least one 6 is the complement of the probability of not getting any 6s: $\mathbb{P}(\text{at least one 6}) = 1- \mathbb{P}(\text{not 6 on both die}) = 1- \frac{25}{36} = \frac{11}{36}$.

\end{itemize}

\end{tcolorbox}



This requires more than simply applying existing statistcal tools---the rise of LLMs demands advances in statistical methodology and theory to ensure trustworthiness, reliably quantify uncertainty, and enhance interpretability---all at the massive scale of LLM deployments. For example, there are many open problems in understanding how the architectural design of LLMs yields, or fails to yield, calibrated probabilistic outputs or understanding how the outputs of LLMs interact with downstream decision-making to ensure algorithmic fairness and reliability. 

One can also view statistics itself as an application domain for LLMs.  Indeed, LLMs provide significant opportunities to augment traditional statistical workflows. Tasks like data collection, cleaning, and data analysis can leverage LLMs’ contextual understanding and scalability to improve accuracy and efficiency. In fields such as medical research and biostatistics, LLMs can synthesize representative datasets, extract valuable insights from unstructured clinical notes, and support predictive modeling in high-stakes applications. Of course, such applications again require closing the gap between the ad hoc nature of current LLM design and the statistical properties of its outputs. 

These dual perspectives, statistics for LLMs and LLMs for statistics, highlight the urgent need for statisticians to engage deeply with the LLM revolution. This engagement will be challenging. The rapid pace of innovation in LLMs raises pressing questions: How can statistical frameworks adapt to the unprecedented scale and complexity of these models? What role should statistical principles play in ensuring the trustworthy and reliable deployment of LLMs in diverse applications? How can statisticians incorporate LLMs into their workflows without compromising the rigor, transparency, and interpretability that define their discipline?  Addressing these questions is critical to bridging the gap between statistical rigor and the transformative potential of LLMs.




%\lz{the importance of LLMs in medical research/biostatistics}

%\lz{new challenges raised by LLMs}

This survey aims to catalyze the involvement of statisticians in the evolving landscape of LLMs by providing a structured overview of the field. It begins by introducing the historical development and foundational principles of LLMs, shedding light on their architectures and training pipelines. It also explores how statistical methods can enhance the trustworthiness and alignment of LLMs, focusing on topics such as conformal prediction, synthetic text detection, and algorithmic fairness.
The discussion then shifts to how LLMs can empower statistical analysis in areas like data synthesis, cleaning, and medical research. 

By bridging the fields of statistics and LLMs, this survey underscores the mutual benefits of collaboration. Statisticians can enrich the development of LLMs with rigor and transparency, while LLMs offer powerful tools to push the boundaries of statistical practice. This interplay promises to redefine both fields, presenting statisticians with a unique opportunity to shape the trajectory of AI in promising directions.

\begin{figure*}[H]
    \centering
\includegraphics[width=0.65\linewidth]{figs/figure1.pdf}
    \caption{Overview of the paper outline.}
    \label{fig:outline}
\end{figure*}


The paper is organized as follows. In Section~\ref{sec2:background}, we provide background on the fundamentals of LLMs. Section~\ref{sec3:training} delves into the training pipelines of LLMs, including pre-training, prompting, fine-tuning, and alignment techniques. Section~\ref{sec4:method} focuses on designing trustworthy LLMs leveraging statistical insights, covering topics like interpretability, uncertainty quantification, watermarking, privacy, and algorithmic fairness. Section~\ref{sec5:app} explores the potential of LLMs to empower statistical analysis, with applications to medical research. Finally, Section~\ref{sec6:discussion} concludes the paper with a discussion on statisticians' role in the LLM landscape, highlighting the importance of collaboration and offering suggestions for future research directions. Additional resources are provided in the appendicies, including frameworks for model development, libraries, datasets, training tools, and visualization utilities that can accelerate progress in LLM research.


%\sm{Possible additional topics: (1) Jailbreak and safety (add reference). (2) Evaluation (Math/MMLU/LMsys). (3) LLM reasoning/agent. (4) how to start LLM research; GPU/cost aspect; resource for starting code: nanogpt. }




%\sm{Figures and tables: (1) Section 3: training pipelines of LLMs; (2) Section 4: LLM empowered statistical analysis; (3) Section 5: In-context learning; Chain-of-thought; (4) Watermark. (5) Circuit and mechanistic interpretability; (6) }


%should point out that this is a large field, and our survey is targeted to statisticians. 
\section{Background and Fundamentals of LLMs}\label{sec2:background}
% {\cyan Add an introduction paragraph here to overview the subsections below}

The development of LLMs has been a landmark event in the field of natural language processing, representing a quantum leap in the ability of machines to understand human language. This section provides a comprehensive overview of the historical development of LLMs, beginning with foundational concepts in representation learning which illustrate how models encode language into numerical forms. We then transition into advances in language modeling, involves predicting the next word in a sequence of texts, allowing for coherent and fluent text generation. Finally, we explore various architectures that have been pivotal in shaping the current landscape of LLMs. 

\subsection{Historical Development of LLMs}
% The capabilities of LLMs arise from two critical components: \textit{representation learning} and \textit{language modeling}. % Representation learning involves learning a numerical representation of input text, in a way that captures their underlying relationships and patterns. Language modeling involves predicting the next word in a sequence of texts, allowing for coherent and fluent text generation.

\subsubsection{Representation Learning}
The first step in an LLM is to transform natural language into a format that computers can understand---specifically vectors, matrices, and tensors. The symbolic lexical units (i.e., words) that comprise sentences require effective numerical representation strategies, known as \emph{word embeddings}. 
Grounded on the distributional hypothesis, which posits that linguistic items that occur in similar contexts have similar meanings~\cite{harris54}, such methods have gradually become more sophisticated over time.

Early attempts at capturing word meanings involved bag-of-words representations, which represent documents as sparse vectors as in TF-IDF~\cite{sparck1972statistical}, with a dimension for each element of the dictionary. Many unsupervised ~\cite{10.5555/599609.599631,10.5555/1658616.1658815} and supervised methods \citep{joachims1998text,bai2009supervised,weston2011wsabie} involved learning (dense) word embeddings, but struggled with complex semantic and syntactic nuances of words in challenging NLP tasks. Neural methods first grew to prominence with the advent of  Word2vec~\cite{DBLP:conf/nips/MikolovSCCD13} and GloVe~\cite{pennington-etal-2014-glove}, that directly learn  low-dimensional word representations from local word context, with each dimension encodes a latent feature of the word.  The specific objective functions underlying
 the architecture of these models were explored in foundational works such as those by \cite{10.5555/944919.944966, 10.1145/1390156.1390177, 10.5555/1953048.2078186,turian-etal-2010-word}, setting the stage for subsequent innovations. % Word2vec, for instance, encompasses two specific models: Continuous Bag-Of-Words (CBOW) and Skip-gram. The CBOW model predicts a target word based on its surrounding context, whereas the Skip-gram model predicts the surrounding context given a target word. Both models utilize a straightforward yet efficient neural network architecture that effectively captures semantic and syntactic word relationships~\cite{DBLP:conf/nips/MikolovSCCD13}.
 
 
%, in contrast to the sparse representations produced by earlier methods. %Sparse representations typically involve high-dimensional vectors with many zero values, each dimension representing the presence or absence of a specific feature, which can be less efficient in capturing the nuanced meanings of words.

% {\cyan Do the previous models learn the representation of tokens? or are they general methods?}
These approaches treat words as the primary unit, leading to challenges like a large vocabulary size and out-of-vocabulary words. Tokenization, such as Byte Pair Encoding (BPE)~\cite{10.5555/177910.177914}, mitigates these issues by merging frequent byte or character pairs. Thus, The evolution of representation learning, language modeling, and tokenization techniques has become a unified and interdependent process.





% \weizhey{TODO: Tokenization}

% Inspired by NPLM, there came many methods that embed words into distributed
% representations and use the language modeling objective to optimize them as model
% parameters. Famous examples include word2vec [12], GloVe [13], and fastText [3].
% Though differing in detail, these methods are all very efficient to train, utilize largescale corpora, and have been widely adopted as word embeddings in many NLP
% models. Word embeddings in the NLP pipeline map discrete words into informative
% low-dimensional vectors, and help to shine a light on neural networks in computing and understanding languages. It makes representation learning a critical part of
% natural language processing.





\subsubsection{Language Modeling}
\label{subsec:language_modeling_app}

%One of the key problems in NLP is assigning a probability to every possible sentence, which is called the language modeling problem. Models that assign probabilities to upcoming words, or sequences of words, are called language models (LMs). Specifically, the language modeling aims to learn a mapping that takes a sequence of tokens $\bm{x}=x_1, x_2, \cdots, x_T$ as input, and output a probability vector over the token vocabulary for the next-token $x_{T+1}$.

Early stages of language models (LMs), such as $n$-gram models~\cite{10.5555/280484, 10.1145/1034780.1034781, 880083}, were mostly statistical regression models, relying on the Markov assumption to predict the next word from the most recent context. Challenges for these models included the need to tackle data sparsity~\citep{chen-goodman-1996-empirical} and the exponentially growing number of transition probabilities as $n$ increases.



Neural language models (NLMs)~\citep{10.5555/944919.944966, conf/interspeech/MikolovKBCK10, schwenk-etal-2006-continuous} tackle data sparsity by mapping words to low-dimensional vectors and predicting subsequent words using neural networks. One of the earliest and most influential neural language models is based on a Recurrent Neural Network (RNN) architecture, which was first introduced by \cite{ELMAN1990179,Jordan1986} and later popularized by \cite{conf/interspeech/MikolovKBCK10}. RNNs are particularly well-suited for modeling sequential data, such as text, but can sometimes struggled with vanishing gradients and capturing long-term dependencies in the input sequence. To address these limitations,  variants of RNNs were introduced, including Long Short-Term Memory (LSTM) networks~\cite{10.1162/neco.1997.9.8.1735} and Gated Recurrent Units (GRUs)~\cite{chung2014empirical}. 
A major advance came with the introduction of the attention mechanism ~\cite{DBLP:journals/corr/BahdanauCB14}, allowing the model to focus on specific parts of the input sequence which can be a large number of positions away when generating each output token. More detail on the attention mechanism can be found in \S\ref{subsec:arch}.
This innovation quickly led to a number of developments including stacking network layers of attention \cite{sukhbaatar2015end} 
and use of position embeddings \cite{10.1145/1390156.1390177,sukhbaatar2015end} which showed strong performance without the use of position recurrence, culminating in the introduction of the Transformer architecture \cite{NIPS2017_3f5ee243}.

The Transformer, introduced by \cite{NIPS2017_3f5ee243}, revolutionized NLP by enabling deeper and more efficient language model training. Unlike LSTMs, Transformers capture global dependencies between input and output regardless of distance while crucially allowing for significantly more parallelization, enhancing their scalability. This led to the ability to train Transformer-based pre-trained models on large amounts of data,
%, which are trained on large amounts of data and can be fine-tuned for downstream tasks with minimal task-specific data. Transformer-based pretrained models
as exhibited by  GPT~\cite{radford2018improving}  BERT~\cite{devlin-etal-2019-bert},  XLNET~\cite{10.5555/3454287.3454804}, RoBERTa~\cite{liu2019roberta} and T5~\cite{2020t5}.  The results were striking. These models learn general representations of language through pre-training on large text corpora with language modeling objectives, allowing for effective fine-tuning on specific NLP tasks.

% The introduction of the Transformer model by \cite{NIPS2017_3f5ee243} revolutionized Pretrained Models (PTMs) for NLP. \lz{may need to explain what is ``pretrained models''} Unlike traditional convolutional networks and recurrent networks, Transformers enable deeper and more efficient language model training. Transformer-based PTMs, such as GPT~\cite{radford2018improving} and BERT~\cite{devlin-etal-2019-bert}, serve as the backbone for various tasks. Pre-trained on large-scale text corpora with language modeling objectives, these models can be further fine-tuned for specific NLP tasks, often achieving state-of-the-art results. This success has inspired further PTMs, including XLNET~\cite{10.5555/3454287.3454804}, RoBERTa~\cite{liu2019roberta} and T5~\cite{2020t5}, etc.

% \wz{There are many LLMs, you can choose small one that you can run on your laptop etc. A figure, e.g., one column small model, one column medium model, one column API model}

Most LLMs today are built on the Transformer architecture, which has led to steady improvement in performance on downstream tasks by scaling both the number of model parameters and the volume of training data~\cite{10.5555/3600270.3602446}. There are numerous popular families of LLMs available, such as the LLama~\cite{touvron2023llama, touvron2023llama2openfoundation, dubey2024llama}, Mistral~\cite{jiang2023mistral,jiang2024mixtralexperts}, GPT~\cite{NEURIPS2020_1457c0d6,openai2024gpt4}, Claude families\footnote{https://www.anthropic.com/news/claude-3-family}, and DeepSeek \citep{bi2024deepseek, liu2024deepseek, liu2024deepseekv3},  each offering models of varying sizes. As illustrated in \autoref{tab:llms}, LLMs can be categorized based on their hosting requirements---from small models that can run on a laptop, to medium-sized models that require a server cluster, and large proprietary models accessible via API. Whether one requires a lightweight model for personal use or a powerful model for enterprise-level tasks, there are currently LLM solutions available. 

Statisticians interested in research on actual LLMs can begin by leveraging resources and tools that lower the barrier to entry while addressing the computational challenges associated with these models. Please refer to \autoref{app:resources} for more details.



% \documentclass{article}
% \usepackage{booktabs}

% \begin{document}

% \begin{table}[h]
% \centering
% \caption{Classification of Large Language Models}
% \label{tab:llm_classification}
% \begin{tabular}{@{}llll@{}}
% \toprule
% LLM Size                & Inference Requirements                           & Training Requirements                                         & Examples \\ \midrule
% Small (up to 1B params) & 1x GPU (e.g., RTX 3080, 10GB VRAM)               & 1-2 GPUs (e.g., RTX 3080, 10GB VRAM), Moderate Data (up to 100GB) & Example: GPT-2 small \\
% Medium (1B - 100B params) & 4-8x GPUs (e.g., V100, 32GB VRAM each)          & 16-32 GPUs (e.g., A100, 40GB VRAM each), Large Data (100GB to 1TB) & Example: LLaMA-70B \\
% Large (100B+ params)    & API Access Only (Hosted on specialized hardware) & 100+ GPUs (e.g., A100, 80GB VRAM each), Massive Data (1TB+) & Example: GPT-4, Claude3 \\ \bottomrule
% \end{tabular}
% \end{table}

% \end{document}


% However, even the same word can mean different things when appear in different contexts. Then, contextualized word representations such as ELMo~\cite{peters-etal-2018-deep}, BERT~\cite{devlin-etal-2019-bert}






% \subsection{Architectures of Pre-trained Language Models}
\subsection{Architectures of Pre-trained Language Models}
\label{subsec:arch}
% The evolution of neural language model architectures has progressed from sequence-based models to the more advanced Transformer models.


As we have discussed, the evolution of neural language model architectures progressed from sequence-based models such as convolutional~\cite{kim-2014-convolutional} and recurrent~\cite{10.1162/neco.1997.9.8.1735} models to the more advanced Transformer models~\cite{NIPS2017_3f5ee243}. Further  success came from Pre-trained Language Models (PTMs), which integrate self-supervised learning and the Transformer model. Self-supervised learning allows the model to learn from the data without explicit labels or supervision, while the Transformer model uses self-attention mechanisms to capture long-range dependencies and contextual relationships in the input data. Below we will introduce different components of the Transformer architecture.


% \subsubsection{Sequence Models} 

% \lz{do we need this part? or just summarize this in a sentence or two? move to Secton 2.1?}
% Sequence models typically capture the local context of a word in a sequential manner.
% \begin{itemize}
%     \item \textbf{Convolutional Models} These models utilize the word embeddings of the input sentence and capture the meaning of a word by accumulating local information from its neighboring words through convolution operations~\cite{kim-2014-convolutional}.
%     \item \textbf{Recurrent Models} These models, including LSTMs~\cite{10.1162/neco.1997.9.8.1735} and GRUs~\cite{chung2014empirical}, obtain the contextual representations of words with short-term memory. In practice, to get more comprehensive representations, bi-directional LSTMs or GRUs are employed to gather information from both directions. However, their performance can be hindered by issues related to long-term dependency.



% \end{itemize}




% \begin{itemize}
%     \item CNN, RNN, maybe introduce attention here.
% \end{itemize}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/transformer_new.pdf}
    \caption{The architecture of a decoder-only Transformer. The model consists of $L$ stacked Transformer decoder layers. Each layer contains a feed-forward network with weight matrices $W_1$ and $W_2$, as well as a multi-head attention module comprising the output matrix ($O_m$), value matrix ($V_m$), key matrix ($K_m$), and query matrix ($Q_m$).
    }
    \label{fig:transformer}
\end{figure*}
% The recent success of PTMs can be attributed to the integration of self-supervised learning (typically language modeling) \lz{do we explain what is self-supervised learning?} and the Transformer model.

\paragraph{Attention Mechanism} The attention mechanism was first introduced to tackle the limitations of the RNN encoder-decoder model, which struggled with long sentences due to its fixed-length vector representation of the source information~\cite{DBLP:journals/corr/BahdanauCB14}. This mechanism enhances the model by jointly learning to align and translate. It identifies key positions in the source sentence for each target word prediction adaptively, considering both these specific context vectors and all previously generated target words. This approach effectively handles longer sentences, improving the model's overall performance. The Transformer model~\cite{NIPS2017_3f5ee243}, built entirely on attention mechanisms, further leverages this approach for superior results. In more detail, an attention function maps a query $\vec{q}$ and a set of key-value pairs $\{(\vec{k}_1, \vec{v}_1)\}, \cdots, \{(\vec{k}_n, \vec{v}_n)\}$ to an output $\vec{o}$. The output is a weighted sum of the values as calculated by $\vec{o} = \alpha_1\vec{v}_1+ \cdots+ \alpha_n\vec{v}_n$, with the weight $\alpha_i$ for each value $\vec{v}_i$ determined by a compatibility function that matches the query $\vec{q}$ with its corresponding key $\vec{k}_i$. 

\paragraph{Transformer Overview} The basic Transformer model operates on a sequence-to-sequence basis, incorporating both an encoder and a decoder, each constructed from a series of identical blocks~\cite{NIPS2017_3f5ee243}. The encoder maps an input sequence $\vec{x}=(x_1, \cdots, x_n)$ to a sequence of continuous representations $\vec{z}=(z_1, \cdots, z_n)$. Given $\vec{z}$, the decoder then generates an output sequence $\vec{y}=(y_1, \cdots, y_m)$. The primary components of each encoder block are a multi-head attention (MHA) module and a position-wise feed-forward network (FFN). To facilitate the construction of a more complex model, a residual connection~\cite{7780459} is utilized around each block, succeeded by a Layer Normalization~\cite{DBLP:journals/corr/BaKH16} module. Decoder blocks, in contrast to encoder blocks, incorporate additional cross-attention modules between the multi-head self-attention modules and the position-wise FFNs. Moreover, the self-attention modules within the decoder are modified to inhibit each position from attending to positions that follow it. 

Since the Transformer model does not inherently encode sequential order, positional encodings are introduced to provide the model with information about the relative or absolute positions of tokens in the input sequence. These positional encodings are added to the input embeddings at the bottom of both the encoder and decoder stacks. The original Transformer employs sinusoidal positional encodings, where each position is represented by a combination of sine and cosine functions of varying frequencies. This choice allows the model to extrapolate to sequences longer than those seen during training. Formally, the positional encoding for a position \( pos \) and dimension \( d \) is defined as:
\[
PE_{pos, 2i} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right), \quad PE_{pos, 2i+1} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right),
\]
where \( d_{\text{model}} \) represents the dimension of the input embeddings. For more detail regarding each individual module, including the incorporation of encodings, please refer to the original Transformer paper~\cite{NIPS2017_3f5ee243}. In addition to sinusoidal encodings, several advanced positional encoding methods have been proposed to enhance model performance and flexibility. Rotary Positional Embeddings (RoPE)~\cite{su2021roformer} is one such method, designed to incorporate relative position information directly into the attention mechanism. RoPE applies a rotation matrix to the embeddings, enabling the model to better capture the relationship between tokens at different positions. Building on the idea of expanding positional encoding capabilities, the recently proposed Contextual Position Encoding (CoPE)~\cite{golovneva2024contextualpositionencodinglearning} addresses a different limitation. Traditional methods, including RoPE, rely on token counts to derive positions, which restricts their ability to generalize to higher levels of abstraction, such as attending to the \( i \)-th sentence. CoPE overcomes this challenge by conditioning positions on context, incrementing positions only for specific tokens determined by the model. This flexibility enables CoPE to handle complex tasks like selective copy, counting, and Flip-Flop, while also improving perplexity in language modeling and coding tasks.

% rope * Cope



% $\vec{q} \in \mathbb{R}^d$
\paragraph{Multi-head Attention} A crucial component within the Transformer's encoder and decoder layers is the multi-head attention module which computes representations of the input and output without using recurrence or convolution calculations. The attention function used in \cite{NIPS2017_3f5ee243} is a scaled dot-product function. The input to this function consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. These queries, keys, and values are obtained by applying a linear transformation to the representations output from the previous encoder/decoder layer. The output calculation of the attention function is shown in \autoref{eq:attn}.

\begin{equation}
\label{eq:attn}
    \text{Attention}(Q, K, V) = \text{Softmax}\Big(\frac{QK^\top}{\sqrt{d_k}}\Big)V
\end{equation}



Empirically, it has been found beneficial to linearly project the queries, keys, and values $h$ times with different, learned linear projections to $d_k$, $d_k$, and $d_v$ dimensions, respectively.
The attention function is then performed in parallel on each of these projected queries, keys, and values, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values. The output calculation of the multi-head attention is shown in \autoref{eq:mha}, where multiple projections parameterized by matrices $W_i^Q$,$W_i^K$, $W_i^V$, $W^O$ are applied. Multi-head attention allows for the capture of more nuanced syntactic and semantic information since the model can attend to information from different subspaces at different positions.
\begin{align}
\label{eq:mha}
    \text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \cdots, \text{head}_h)W^O\\
    \text{where } \text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align}



\paragraph{Variants}
The Transformer architecture is versatile and highly extensible, allowing for the seamless integration of new modules or the implementation of novel attention mechanisms. The application of the Transformer architecture can take three distinct forms, depending upon the utilization of the encoder and decoder components.
\begin{itemize}
    \item{\textbf{Encoder-Decoder Transformers}} The full Transformer architecture, which incorporates both the encoder and decoder components, is typically employed for sequence-to-sequence modeling tasks, such as machine translation and text summarization. Pre-trained models that utilize this full Transformer architecture, such as T5~\cite{2020t5} and BART~\cite{lewis-etal-2020-bart}, are commonly applied in these contexts.
    \item{\textbf{Encoder-only Transformers}} This involves the  exclusive use of the encoder component within the Transformer architecture. The encoder typically functions as a feature extractor, with its outputs serving as a representation of the input sequence. This method is commonly employed for tasks such as text classification or sequence labeling, which can be viewed as token classification problems. Typical pre-trained encoder models include BERT~\cite{devlin-etal-2019-bert} and RoBERTa~\cite{liu2019roberta}.



    \item{\textbf{Decoder-only Transformers}}  This involves solely utilizing the decoder component within the Transformer architecture, with the cross-attention module between the encoder and decoder being omitted, as shown in \autoref{fig:transformer}. It is typically employed for sequence generation tasks, such as language modeling. Recently, the unification of task formats into language modeling and the scalability of decoder-only Transformer training have led to the rise of instruction tuning. This method, which involves fine-tuning language models on a collection of tasks described via instructions, significantly enhances zero-shot performance on unseen tasks~\cite{wei2022finetuned}. Consequently, \textbf{most current large language models are based on the decoder-only Transformer architecture}. Notable examples include the GPT series~\cite{radford2019language, NEURIPS2020_1457c0d6, openai2024gpt4}, Llama series~\cite{touvron2023llama, touvron2023llama2openfoundation}, and Mistral series~\cite{jiang2023mistral}, among others.
    
    % Only the decoder is used, where the encoder-decoder cross-attention module is also removed. This is typically used for sequence generation, such as language modeling. 
    % However, owing to the unification of tasks formats into language modeling, and the ease to scale up training. instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. current large language models are mostly based on decoder-only transofmer architecture. Some famous examples are GPT seris, LLAMA series, MIXTRAL series etc. 

\end{itemize}

\subsection{LLM Evaluation}

As the number of LLMs continues to grow, the challenge of effectively comparing their performance becomes increasingly important. Traditional benchmarks, such as ARC~\cite{clark2018thinksolvedquestionanswering}, MMLU~\cite{DBLP:conf/iclr/HendrycksBBZMSS21}, and MATH~\cite{hendrycksmath2021}, while valuable for evaluating specific capabilities like multi-task understanding, mathematical reasoning, and commonsense knowledge, rely on fixed datasets with predefined gold answers. This rigid structure limits their ability to assess models' adaptability and creativity in generating diverse, contextually appropriate responses. Moreover, as models become more advanced, they tend to saturate these benchmarks, reducing their discriminative power and making it harder to distinguish between state-of-the-art systems.

Chatbot Arena~\cite{chiang2024chatbot} addresses these limitations by providing a benchmark platform for evaluating LLMs through anonymous, randomized battles in a crowdsourced manner. Utilizing the Elo rating system~\cite{10.5555/1803551}, which is widely used in competitive games such as chess, Chatbot Arena allows users to compare models by engaging with them side-by-side and voting on which one performs better. This approach not only facilitates scalable and incremental evaluation but also provides a unique ranking order for the models. By inviting the community to contribute new models and participate in the evaluation process, Chatbot Arena helps maintain an up-to-date leaderboard, offering valuable insights into the relative strengths of various LLMs. This platform exemplifies a practical solution for benchmarking LLMs in open-ended tasks, where traditional programmatic evaluation methods with fixed gold answers fall short.

% Generally, the Transformer architecture can be used in three different ways:
% • Encoder-Decoder. The full Transformer architecture as introduced in Sec. 2.1 is used. This is typically used in sequence-to-sequence modeling (e.g., neural machine translation).
% • Encoder only. Only the encoder is used and the outputs of the encoder are utilized as a representation for the input sequence. This is usually used for classification or sequence labeling problems.
% • Decoder only. Only the decoder is used, where the encoder-decoder cross-attention module is also removed. This is typically used for sequence generation, such as language modeling.


% The Transformer architecture is versatile and can be employed in three distinct manners:
% Full Encoder-Decoder: This utilizes the complete Transformer architecture as discussed in Sec. 2.1. It is commonly applied in sequence-to-sequence modeling tasks, such as neural machine translation.
% Encoder-Only: This approach involves the use of only the encoder, with the encoder's outputs serving as a representation for the input sequence. It is typically used for tasks like classification or sequence labeling.
% Decoder-Only: In this setup, only the decoder is used, and the cross-attention module between the encoder and decoder is omitted. This is generally used for sequence generation tasks, like language modeling.

% Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
% Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence
% of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output
% sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive
% [10], consuming the previously generated symbols as additional input when generating the next.


% \textcolor{red}{TODO: Write Transformer} 
% Any two words in a sequence can attend to each other.\\
% Attention..
% High-level what is encoder, what is decoder..

% \begin{itemize}
%     \item{Encoder-only Transformers} NL Understanding (get good embeddings for classification BERT, Transformers, DeBERTa)
%     \item{Encoder-Decoder Transformers} NL Generation (QA, summarizaiton, conditional generation, T5, BART)
%     \item{Decoder-only Transformers} NL Generation (Free-form, after instruction-tune can conditional generation GPT series, LLAMA, MISTRAL)
% \end{itemize}


% \subsection{Large Language Models}
% Scaling PLMs

\section{Training Pipelines of LLMs}\label{sec3:training}
\subsection{LLM Pre-training}
Between 2017 and 2019, a paradigm shift occurred in the learning of NLP models. The traditional fully supervised learning paradigm began to be replaced by a two-step process: pre-training and fine-tuning~\cite{peters-etal-2018-deep, radford2018improving, devlin-etal-2019-bert}. In this new paradigm, a model with a fixed architecture is first pre-trained as a language model using unlabeled web data through self-supervised learning. This pre-trained language model is then adapted to various downstream tasks by introducing additional parameters and fine-tuning these parameters using task-specific objective functions.
The pre-training and fine-tuning paradigm offers several advantages. Firstly, pre-training on a large text corpus enables the model to learn universal language representations, which can be beneficial for many downstream tasks. Secondly, pre-training provides a superior model initialization, which often results in better generalization performance and faster convergence on the target task. Lastly, pre-training can serve as a form of regularization, helping to prevent overfitting, particularly when dealing with small datasets~\cite{10.5555/1756006.1756025}.

%\sm{A more quantitative illustration of the data-size difference between pre-training and fine-tuning}

% \lz{we may primarily focus on the standard setting, and briefly mention corrupted text and full text reconstruction.}


\subsubsection{Pre-training Objective}
The choice of pre-training tasks plays a pivotal role in learning universal language representations. These tasks should ideally be challenging and have substantial training data. In this section, we provide a brief overview of one of the most widely used pre-training tasks.

\paragraph{Standard Language Modeling.} Standard language modeling objectives focus on training the model to learn the probability $P(x)$ of texts from a training corpus~\cite{radford2019language}. Typically, text prediction occurs in an autoregressive manner, predicting the tokens in the sequence one at a time, often from left-to-right, although other orders are possible as well. Formally, a language model is parameterized by a set of parameters $\theta$ and learns a parameterized mapping from the context $\bm{x}_{<t}$ to the next-token $x_t$. The goal of the model is to predict the next-token in the sequence given the context, and this is achieved by minimizing the conditional probability $P_{\theta}(x_t|\bm{x}_{<t})$. The loss function of standard language modeling objectives over a sequence of tokens $\bm{x}=x_1, x_2, \cdots, x_T$ is $\mathcal{L}_{\text{SLM}} = -\sum_{t=1}^T\log P_{\theta}(x_t|\bm{x}_{<t})$.
% \lz{may need to mention that LMs are mapping $\bm{x}_{<t}$ to $x_t$.}. 
Due to their simplicity, efficiency, scalability, and proven performance on a wide range of tasks, language modeling objectives have become the preferred choice for pre-training LLMs. \


% \wz{No need to mention the following objectives}
% \paragraph{Corrupted Text Reconstruction} Besides standard language modeling objectives, denoising objectives are popular alternatives which apply some noising function $\tilde{\bm{x}} = f_{\text{noise}}(\bm{x})$ to the input sentence, then try to predict the original input sentence given this noised text $P(\bm{x}|\tilde{\bm{x}})$. Common noising functions include masking~\cite{devlin-etal-2019-bert}, replacement~\cite{JMLR:v21:20-074}, deletion~\cite{lewis-etal-2020-bart}, and permutation~\cite{liu-etal-2020-multilingual-denoising}. There are two prevalent types of these objectives, each distinguished by the specific portion of the input utilized for loss computation. Corrupted text reconstruction objectives aim to restore the processed text to its original, uncorrupted state by calculating loss over only the noised parts of the input sentence. 

% \paragraph{Full Text Reconstruction} objectives strive to reconstruct the text by calculating the loss over the entire input text, regardless of whether it has been noised~\cite{lewis-etal-2020-bart}. 

% The loss function of full text reconstruction objectives over a sequence of tokens $\bm{x}$ and its noised version $\tilde{\bm{x}}$ is $\mathcal{L}_{\text{DAE}}=-\sum_{t=1}^T\log P(x_t|\tilde{\bm{x}}, \bm{x}_{<t})$.

% In addition to standard language modeling objective, corrupted text reconstruction and full text reconstruction objectives are also popular choices as primary training objectives. Besides
% these primary training objectives, auxiliary objectives have been developed to improve the model's performance on specific types of downstream tasks. Commonly used auxiliary objectives include next sentence prediction~\cite{devlin-etal-2019-bert}, sentence order prediction~\cite{conf/iclr/LanCGGSS20}, discourse relation prediction~\cite{Sun_Wang_Li_Feng_Tian_Wu_Wang_2020}, token-passage prediction~\cite{ijcai2020p622}, etc.


In addition to the standard language modeling objective, other primary training objectives such as corrupted text reconstruction~\cite{2020t5} and full text reconstruction~\cite{lewis-etal-2020-bart} are also widely utilized. Beyond these primary objectives, auxiliary objectives have been developed to enhance the model's performance on specific downstream tasks. Common auxiliary objectives include next sentence prediction~\cite{devlin-etal-2019-bert}, sentence order prediction~\cite{conf/iclr/LanCGGSS20}, discourse relation prediction~\cite{Sun_Wang_Li_Feng_Tian_Wu_Wang_2020}, and token-passage prediction~\cite{ijcai2020p622}. These auxiliary objectives are strategically employed to provide additional training signals that help refine the model's understanding and generation capabilities, thereby improving its applicability and effectiveness across a variety of NLP tasks.


\subsubsection{Pre-training Data}
% \textcolor{red}{Add stories.. Classification, empirically found using what data are good for e.g., reasoning.. reverse... Data centric...Data Mixture}
% Typical pre-training data include BooksCorpus, English Wikipedia, CC-NEWS,
% STORIES (a subset of Common Crawl), Reddit, Giga5, Common Crawl, ClueWeb 2012-B, WebText2, Books1, Books2, Public GitHub software repositories,  Online sources, etc..
The choice of pre-training datasets also plays a crucial role in the development and capabilities of LLM models. These datasets, often composed of vast amounts of text data from various sources, serve as the foundation on which LLMs learn the intricacies of language. By training on such massive corpora, LLMs acquire general language understanding and the ability to generate coherent text.


\paragraph{Data Source}
% \wz{TODO: Add this paper: \url{https://arxiv.org/pdf/2408.10914}}
Pre-training data can be broadly categorized into two types: general pre-training corpora and domain-specific pre-training corpora. General pre-training corpora are broad datasets, covering categories such as webpages, language texts, books, academic materials, code, parallel corpora, social media, and encyclopedia~\cite{liu2024datasetslargelanguagemodels}. Webpages are a major source, offering extensive multilingual content that often requires significant cleaning, as seen in derivatives like RefinedWeb~\cite{10.5555/3666122.3669586} from Common Crawl\footnote{https://commoncrawl.org/}. Language texts are sourced from large corpora like the American and British National Corpora,\footnote{https://anc.org/}\footnote{http://www.natcorp.ox.ac.uk/} often focusing on specific languages or domains like finance. Books provide high-quality, lengthy texts from sources such as Project Gutenberg\footnote{https://www.gutenberg.org/} improve models’ understanding of complex language. Academic Materials, such as those in arXiv,\footnote{https://arxiv.org/} contribute specialized scholarly content. Code data from repositories like The Stack~\cite{kocetkov2022stack3tbpermissively} and Github\footnote{https://github.com/} is essential for programming tasks. Parallel corpora data, involving bilingual text pairs from resources like ParaCrawl~\cite{banon-etal-2020-paracrawl} is crucial for translation tasks. Social Media data from platforms such as StackExchange\footnote{https://stackexchange.com/} and Reddit\footnote{https://www.reddit.com/} helps models learn conversational dynamics, while Encyclopedia data, particularly from Wikipedia,\footnote{https://www.wikipedia.org/} strengthens models’ general knowledge. Interestingly, there are some unexpected phenomena relating to how certain corpora can enhance the abilities of LLMs. For example, code data not only is essential for programming tasks, but also significantly enhances non-code performance when included in pre-training. Specifically, \cite{aryabumi2024codecodeexploringimpact} found that initializing models with code pre-trained data led to a relative increase of 8.2\% in natural language reasoning and a 12$\times$ boost in code performance. Moreover, they also found that using high-quality, synthetically generated code and code-adjacent datasets like GitHub commits during pre-training or cooldown phases could result in substantial improvements across various tasks including reasoning and world knowledge tasks.

% \wz{Some message: for example, train with Github code data can improve reasoning compared to train on Wikipedia only..}

Domain-specific pre-training corpora fine-tune models for specialized fields. For instance, financial datasets like BBT-FinCorpus~\cite{lu2023bbtfincomprehensiveconstructionchinese} and medical corpora like Medical-pt~\cite{MedicalGPT} provide targeted content that enhances model performance in finance, law, and other areas. These datasets ensure that models are better equipped for specific applications.
% \paragraph{Data Preprocessing}
\paragraph{Data Selection and Mixture} Data selection and mixture in LLM training is essential for optimizing model performance. This process is generally divided into three levels: token-level, sample-level, and group-level selection. Token-level selection focuses on filtering individual tokens within the data, offering precise control but requiring significant resources~\cite{lin2024rho1tokensneed}. Sample-level selection involves choosing specific training examples, commonly used in fine-tuning. While heuristic methods are typical~\cite{DBLP:journals/corr/abs-2112-11446,sharma2024textqualitybasedpruningefficient,soldaini2024dolmaopencorpustrillion}, advanced approaches use optimization algorithms~\cite{chen2024bullhornshardsamplereweighted,mindermann2022prioritizedtrainingpointslearnable}, model perplexity~\cite{DBLP:conf/nips/MuennighoffRBST23}, or even LLMs to guide selection~\cite{wettig2024quratingselectinghighqualitydata,sachdeva2024traindataefficientllms}, aiming to enhance the quality of the training data. Group-level selection organizes data into pools, which are then optimally mixed through mixture techniques. Early methods relied on manual mixtures~\cite{gao2020pile800gbdatasetdiverse,NEURIPS2020_1457c0d6}, but more recent approaches use learned mixtures, either through fixed weights determined by proxy models (``offline selection")~\cite{DBLP:journals/corr/abs-2112-11446,DBLP:conf/nips/Xie0DDLLLL0Y23} or dynamically adjusted weights during training (``online selection")~\cite{DBLP:conf/nips/ChenRBWZSR23}. %\lz{the discussion here seems primarily focus on data selection, where is the mixture part?}
% \wz{https://arxiv.org/pdf/2407.01492}

\subsubsection{Scaling Laws}
%\sm{Should this be put to be the last section of Section 3?}

With the introduction of increasingly large language models~\cite{NEURIPS2020_1457c0d6, DBLP:journals/corr/abs-2112-11446, DBLP:journals/corr/abs-2201-11990}, understanding computational efficiency has become crucial. The compute and energy costs for training these models are substantial~\cite{DBLP:journals/corr/abs-2112-11446, DBLP:journals/corr/abs-2201-08239}, escalating with model size. In practical scenarios, the training compute budget is often predetermined, depending on the availability of accelerators and the duration of their use. Given that training large models is usually feasible only once, accurately estimating optimal hyperparameters for a given compute budget is essential. Kaplan et al.~\cite{journals/corr/abs-2001-08361} provided initial insights into computational efficiency for language models, discovering a power law relationship between the number of parameters in an autoregressive LM and its performance. They suggested that with a tenfold increase in computational budget, model size should increase by 5.5X, while the number of training tokens should only increase by 1.8X. This led to a trend of training larger models to achieve performance gains. Scaling laws have been shown to apply across various data modalities, including language, images~\cite{pmlr-v119-chen20s}, and videos~\cite{DBLP:conf/iclr/WeissenbornTU20}, as well as multimodal modeling~\cite{DBLP:conf/acl/TsaiBLKMS19} and even mathematical problem solving~\cite{DBLP:conf/iclr/SaxtonGHK19}.

Beyond model size and training tokens, \cite{DBLP:conf/emnlp/Tay0ACFRN0YM23} derived scaling laws for different inductive biases and model architectures, revealing significant variations in scaling coefficients among models. They found that among ten architectures, the vanilla Transformer exhibited the best scaling behavior, despite not having the highest absolute performance at each compute region.


In 2022, the Chinchilla scaling law~\cite{10.5555/3600270.3602446} shifted the focus from model size to the number of training tokens for computational efficiency. This law suggests that training slightly smaller models on larger datasets is often more efficient than the previous approach~\cite{journals/corr/abs-2001-08361}, which favored larger models on smaller datasets. Compared to Kaplan's study, key differences introduced by the Chinchilla study include: (1) a different learning rate schedule for all models, unlike~\cite{journals/corr/abs-2001-08361}, which did not account for the impact of these hyperparameters on the loss; and (2) the inclusion of larger scale models. The Chinchilla model, with 70B parameters trained on 1.4T tokens (approximately 20 tokens per parameter), outperformed its much larger counterpart, Gopher~\cite{DBLP:journals/corr/abs-2112-11446}.


The ``Chinchilla efficient'' model size and training dataset size, along with the achievable test loss, can be determined as follows:
\begin{equation}
\begin{cases}
N_{\text{opt}}(C) = 0.6 \, C^{0.45} \\
D_{\text{opt}}(C) = 0.3 \, C^{0.55} \\
L_{\text{opt}}(C) = 1070 \, C^{-0.154} + 1.7,
\end{cases}
\end{equation}
where $N_{\text{opt}}$ represents the optimal number of model parameters, $D_{\text{opt}}$ denotes the optimal number of training tokens, and $L_{\text{opt}}$ indicates the optimal final pre-training loss achievable under a fixed FLOPs compute budget $C$.
Despite the trend towards larger models, the potential of training smaller models with larger datasets remains relatively underexplored. Instead of focusing solely on training compute-optimal language models, \cite{touvron2023llama} emphasized the importance of the inference budget. Inference-optimal language models aim for optimal performance within specific inference constraints, achieved by training small models with more tokens than recommended by traditional scaling laws\cite{10.5555/3600270.3602446}. \cite{touvron2023llama} demonstrated that smaller models trained with more data could match or even surpass the performance of larger models. Additionally, \cite{Chinchilla-death} argued that existing scaling laws~\cite{10.5555/3600270.3602446} might not accurately predict scenarios where smaller models are trained for extended periods. Consequently, efforts have been made to push the limits of scaling laws by training language models to achieve optimal performance across various inference budgets by using more tokens than typically prescribed by the Chinchilla scaling law. Notable examples include the Llama series~\cite{touvron2023llama}, TinyLlama~\cite{DBLP:journals/corr/abs-2401-02385}, Llama2 series~\cite{touvron2023llama2openfoundation}, and Llama3~\cite{llama3}, which trained an 8B model on 15T tokens, far exceeding the optimal number of training tokens predicted by the Chinchilla law (approximately 200B tokens). They found that model performance continued to improve log-linearly even after training the 8B and 70B models on up to 15T tokens.

After pre-training on massive corpora using self-supervised objectives, the parameters of the LLM are well suited as an initialization for various downstream tasks. The pre-training provides the model with a broad understanding of language, which can then be fine-tuned for specific applications. Beyond pre-training strategies, understanding scaling laws plays a crucial role in optimizing model training and deployment. For example, \cite{jagadeesan2024safety} highlight how multi-objective considerations (e.g., safety and accuracy) can influence computational efficiency, demonstrating that new entrants to competitive markets can achieve safety alignment with fewer data points due to differing reputational dynamics. Their findings that scaling rates slow with increasing dataset size suggest new strategies for allocating compute resources effectively, particularly in multi-objective settings. These insights, combined with advances in pre-training and fine-tuning, underscore the potential to refine model training workflows and push the boundaries of computational efficiency across diverse applications.



% {\cyan discussion of statiscitian's efforts on scaling laws; multi-objective setting; market concentration}
% \jx{Jing to add some comments on related work}

%After pre-training on massive corpora using self-supervised objectives, the parameters of the LLM are well suited as an initiation for various downstream tasks. This pre-training gives the model a broad understanding of the language, which can then be fine-tuned to adapt it to specific applications.

% By passing through the target input into the language models, one can directly feed the final hidden states output by the pre-trained model into an added linear output layer as auxiliary features to predict target output. Researchers have demonstrated significant benefits of using learned representations of language as auxiliary features for word/sentence encoding, classifications, etc \cite{collobert2008unified,radford2018improving}. Alternatively, on downstream tasks such as question-answering, text entailment, reasoning tasks etc where the input texts are in a more structured format, \cite{radford2018improving} showed that fine-tuning a neural network by initializing its weights from a pre-trained language model can transfer well to downstream tasks. 

% \begin{enumerate}
%     \item Supervised Finetuning, Instruction-tuning, Mult-task learning
%     \item More about algorithms. 
%     \item Pairwise Finetuning / Contrastive Learning
%     \item Task-Specific Fine-Tuning
% \end{enumerate}



\subsection{LLM Prompting}
%\lz{why this order?}
% \sm{Switch with alignment}
% \begin{itemize}
%     \item Prompt Engineering
%     \item In-context Learning \wz{more shots higher acc?, input transformation, longer, the the dimensionality of transformation higher?}
%     \item CoT prompting
% \end{itemize}
For the largest language models, the paradigm has shifted for typical users from traditional supervised learning to prompt-based learning, often called prompt engineering~\cite{DBLP:journals/csur/LiuYFJHN23}. In supervised learning, we use labeled data consisting of input-output pairs $\mathcal{D} = (x_i, y_i)_{i=1\cdots N}$ to tune the model parameter $\theta$ so that we can predict the output for an input $x$ that is not in the training data using $P_\theta(y|x)$. However, in prompt engineering, the model parameter $\theta$ is fixed, and instead, one tunes a template $t$ which is combined with $x$ to form a new input $x^\prime$ that achieves good performance when using $P_\theta(y|x^\prime)$.
In other words, prompt engineering learning involves optimizing a template $t$ that is used to generate a new input $x^\prime$ from the original input $x$, rather than optimizing the model parameters $\theta$ directly. This approach allows us to adapt the model to new inputs without requiring retraining of the model. We describe two main prompt-based learning techniques as follows.



\paragraph{Vanilla Prompt Engineering}
Vanilla prompt engineering involves the development of effective input prompts for LLMs to generate better outputs~\cite{DBLP:journals/csur/LiuYFJHN23}. Traditionally, prompts were manually crafted based on intuitive templates, a process that requires considerable expertise and may not always yield optimal results. To overcome this limitation, automated methods have been introduced, categorizing prompts into discrete and continuous types. Discrete prompts, also known as hard prompts, involve natural language phrases and can be discovered through methods like prompt mining from large corpora~\cite{DBLP:journals/tacl/JiangXAN20}, paraphrasing existing seed prompts~\cite{DBLP:conf/nips/YuanNL21}, gradient-based search over tokens~\cite{DBLP:conf/emnlp/WallaceFKGS19}, and using LLMs to generate prompts based on inputs~\cite{DBLP:conf/acl/GaoFC20}. Continuous prompts, or soft prompts, operate within the embedding space of the model and do not require human-interpretable language. Prefix-tuning is an example of this approach~\cite{li-liang-2021-prefix}, where continuous task-specific vectors are prepended to inputs, allowing the LM to perform the task more effectively without altering its parameters. In addition, some methods, such as P-tuning~\cite{liu2021gpt} and PTR~\cite{DBLP:journals/aiopen/HanZDLS22}, enhance hard prompt templates by incorporating some tunable embeddings, rather than relying solely on purely learnable prompt templates. These approaches blend the structure of hard prompts with the flexibility of trainable soft tokens, improving prompt performance and adaptability.

\paragraph{In-Context Learning}
%\wz{Bayesian etc..}
%\wz{input transformation, longer, the the dimensionality of transformation higher..etc} 
In-context learning is a paradigm that allows language models to perform tasks by using a few examples as demonstrations within the prompt, without the need for further fine-tuning~\cite{NEURIPS2020_1457c0d6}. Formally, given a set of demonstrations with inputs \(\{x_1, x_2, \dots, x_n\}\) and corresponding outputs \(\{y_1, y_2, \dots, y_n\}\), a prompt template \(t\) is used to format each input into \(\{x_1', x_2', \dots, x_n'\}\). For a new input \(x\), formatted as \(x'\) using the same template, the language model \(\mathcal{M}\) predicts the output by estimating the probability \(P_\theta(y \mid x_1', y_1, \dots, x_n', y_n, x')\). By prepending these input-output pairs to the current input, the model learns to perform the task through the context provided by these examples, effectively enabling it to generalize to new tasks based on in-context information. Studies have shown that increasing the number of examples leads to significant performance improvements across both generative and discriminative tasks~\cite{NEURIPS2024_8cb564df}.
Despite its apparent simplicity and effectiveness, in-context learning presents several challenges that significantly impact model performance. The selection and ordering of examples are crucial. Research has shown that the specific examples chosen in a few-shot scenario can lead to vastly different outcomes, ranging from near state-of-the-art accuracy to almost random guessing~\cite{DBLP:conf/acl/LuBM0S22}. To mitigate this variability, advanced techniques such as using sentence embeddings to select examples that are semantically close to the input~\cite{DBLP:conf/acl/GaoFC20,DBLP:conf/acl-deelio/LiuSZDCC22} or employing entropy-based methods to optimize the sequence of examples have been proposed~\cite{DBLP:conf/acl/LuBM0S22}. Moreover, formal understanding of why in-context learning works has been a subject of ongoing research. Recent studies suggest that in-context learning functions as a form of Bayesian inference, where the model uses the provided examples to recover latent concepts~\cite{xie2021explanation}. Researchers have empirically observed that replacing gold labels with random labels only marginally impacts performance~\cite{min-etal-2022-rethinking}. Additionally, in-distribution inputs within the demonstrations significantly contribute to performance gains~\cite{min-etal-2022-rethinking}. This suggests that in-context learning likely helps elicit knowledge that the model has already acquired during pre-training, rather than teaching the model new information through just a few examples.


\subsection{LLM Supervised Fine-Tuning (SFT)}
%After pre-training on massive corpora using self-supervised objectives, the parameters of the LLM are well suited as an initiation for various downstream tasks. This pre-training gives the model a broad understanding of the language, which can then be fine-tuned to adapt it to specific applications.

Even with extensive pre-training, LLMs may not excel at specific tasks without further adjustment. To achieve improvements of this kind, a process known as Supervised Fine-Tuning (SFT), which involves fine-tuning with labeled data, is often necessary. For instance, the BERT paper~\cite{devlin-etal-2019-bert} demonstrated the effectiveness of fine-tuning the model on 11 distinct tasks. Although more recent LLMs can perform tasks through in-context learning~\cite{NEURIPS2020_1457c0d6} or zero-shot prompting~\cite{DBLP:journals/csur/LiuYFJHN23} without prior fine-tuning, they still stand to gain from fine-tuning tailored to specific tasks or datasets. A notable example is OpenAI's GPT-3.5 Turbo, which, despite its smaller size compared to GPT-4, could achieve superior performance when fine-tuned with task-specific data.\footnote{https://platform.openai.com/docs/guides/fine-tuning}


\begin{table}[t]
 \small
\setlength{\tabcolsep}{8pt}
  \centering
    \renewcommand{\arraystretch}{1}
\begin{tabular}{p{0.17\textwidth}p{0.26\textwidth}p{0.25\textwidth}p{0.16\textwidth}}
\toprule
\textbf{LLM Size}      & \textbf{Inference Requirements} & \textbf{Training Requirements} & \textbf{Examples}            \\
\midrule
Small ($\leq$2B)             & Hostable on a personal laptop ($\leq$5GB GPU memory)     &  Requires a server cluster ($\leq$1 NVIDIA A100 40GB)    & Gemma-2-2B, Llama-3.2-1B                    \\
\midrule
Medium (2$\sim$100B)            & Requires a server cluster ($\leq$8 NVIDIA A100 80GB)   & Requires a server cluster ($\geq$1 NVIDIA A100 80GB) & Llama-3-70B, Mixtral-8x7B    \\
\midrule
Large ($\geq$100B) & Requires a server cluster ($\geq$8 NVIDIA A100 80GB)          & Requires a server cluster ($\geq$32 NVIDIA A100 80GB) & Llama-3.1-405B\\
\midrule
Proprietary (Size Unknown) & Accessed via API             & Accessed via API  & GPT-4, Claude-3,  Gemini-2 \\
\bottomrule
\end{tabular}
\caption{Recent popular LLMs categorized based on their hosting requirements. Estimates for the inference and training costs are based on floating-point 16 precision. New research is trying to compress further to make larger models available locally~\cite{DBLP:conf/icml/ParkHCSL24,DBLP:journals/corr/abs-2401-14112,DBLP:conf/icml/HuangLQLZ0M024}.
% some reference here: https://training.continuumlabs.ai/infrastructure/data-and-memory/calculating-gpu-memory-for-serving-llms. training is typically 3~4 times inference cost
}
\label{tab:llms}
\end{table}

SFT does not need to be based on a single task. Indeed, to further improve LLMs' performance and address data scarcity, researchers are also increasingly adopting Multi-Task Learning (MTL)~\cite{Caruana1998} for NLP tasks. This approach trains models on multiple related tasks simultaneously, broadening the training dataset and reducing overfitting risks~\cite{10.1145/3663363}. MTL not only captures generalized and task-specific knowledge but also facilitates the development of compact, efficient models through implicit knowledge sharing~\cite{domhan-hieber-2017-using,singla-etal-2018-multi}. 

% By leveraging tasks with varied relatedness, supervision styles, and goals, MTL improves robustness and reduces reliance on external features~\cite{zhou-etal-2019-multi,song-etal-2020-zpr2}. 



\subsubsection{Instruction Tuning}

% \lz{we need to explain what is instruction tuning}


The transformative idea that any NLP task can be converted into a text-to-text format has significantly aligned with advances in generative language models~\cite{2020t5}. This task paradigm shift allows for the fine-tuning of language models across a broad spectrum of NLP tasks using an unified data format~\cite{mishra-etal-2022-cross}, thereby ensuring a uniform training objective. A pivotal development in this area is ``instruction tuning'' where a language model is fine-tuned on a collection of tasks described via instructions~\cite{wei2022finetuned}. This method has been shown to improve the zero-shot performance of language models on unseen tasks. Models such as FLAN~\cite{wei2022finetuned} and T0~\cite{sanh2022multitask} exemplify this approach. These models are trained to process a variety of NLP tasks through instructional prompts, setting new performance benchmarks and demonstrating an impressive ability to generalize to tasks they were not explicitly trained on. This evolution towards instruction-based task execution highlights the critical role of multi-task learning in increasing the robustness and adaptability of language models~\cite{10.1145/3663363}.


The significance of instruction tuning is further highlighted in the InstructGPT paper~\cite{ouyang2022training}, which utilizes a novel dataset comprising prompts crafted by labelers and those submitted to early InstructGPT models via the OpenAI API. This dataset includes a wide spectrum of tasks such as brainstorming, rewriting, open-ended question answering, and more, reflecting the diverse and user-centric nature of modern NLP applications. Labelers demonstrate the desired responses to prompts, providing training data for SFT using GPT-3. The InstructGPT findings emphasize the necessity of moving beyond traditional NLP tasks to include user-centric tasks like brainstorming, which are not adequately captured by traditional NLP datasets. This expansion not only broadens the scope of tasks that models are trained on but also enhances their ability to perform effectively on real-world, user-driven tasks, marking a significant shift towards more adaptive and user-focused language models.

\subsubsection{Parameter-Efficient Fine-Tuning}
Given the sheer size of LLMs, a common approach to fine-tuning is to modify a small fraction of the model's parameters while leaving most of them unmodified. This approach, called ``Parameter-Efficient Fine-Tuning'' (PEFT), focuses on selectively tuning a limited number of parameters to achieve the desired performance gains without completely modifying the entire model. The PEFT strategies can be broadly classified into three types~\cite{han2024parameterefficient}: (1) Additive fine-tuning, which injects new trainable modules or parameters into the original model architecture. (2) Selective fine-tuning, which trains only a subset of model parameters during fine-tuning. (3) Reparameterized fine-tuning, which constructs a low-dimensional reparameterization of the original model parameters for training.




\paragraph{Additive Fine-Tuning}
% Additive fine-tuning introduces only a minimal number of trainable parameters that are strategically positioned within the model architecture, for example, Adapters~\cite{xx} and soft prompts. While fine-tuning for a specific downstream task, only the weights of these additional modules or parameters are updated. 

Additive fine-tuning strategies, such as adapters~\cite{pmlr-v97-houlsby19a,he2022unified} and soft prompts~\cite{li-liang-2021-prefix,liu2021gpt}, introduce only a minimal number of trainable parameters that are strategically positioned within the model architecture. Adapters are small layers inserted within Transformer blocks, consisting of a down-projection matrix, a nonlinear activation function, and an up-projection matrix. These layers act as computational bottlenecks, refining the model's output while leveraging the existing pre-trained parameters. On the other hand, soft prompts involve appending adjustable vectors at the beginning of the input sequence, enhancing the model's ability to utilize the rich information within the continuous embedding space. This method adjusts the initial conditions of the model's input processing, allowing for fine-tuned performance improvements without extensive retraining of the core model components. Both approaches maintain the original model architecture unmodified while providing targeted enhancements for specific tasks.

\paragraph{Selective Fine-Tuning}
Unlike additive PEFT, selective PEFT fine-tunes only a specific subset of the existing parameters within a model. This is achieved by applying a binary mask to the model's parameters, where each element of the mask is either 0 or 1, indicating whether the corresponding parameter should be updated during fine-tuning. Only the selected parameters are adjusted based on the gradients of the loss function, using a predefined learning rate. This method allows for targeted improvements on downstream tasks by optimizing a limited number of model parameters, thereby maintaining the overall efficiency and scalability of the model. Techniques such as Diff pruning~\cite{guo-etal-2021-parameter}, PaFi~\cite{liao-etal-2023-parameter}, and FishMask~\cite{NEURIPS2021_cb2653f5} exemplify this approach, focusing on refining the model's performance through selective parameter updates.

\paragraph{Reparameterized Fine-Tuning}

Reparameterization fine-tuning involves transforming a model’s architecture by adjusting its parameters, often through a low-rank parameterization to maintain efficiency during training and restoring the original weight configurations for inference. A notable technique in this domain is LoRA (Low Rank Adaptation)~\cite{hu2021lora}, which introduces small, trainable matrices operating alongside the pre-trained weights to inject task-specific updates without burdening the inference process. This method modifies the output by adding an incremental update, effectively capturing task-specific nuances while maintaining the model's original efficiency. Extensions of LoRA, such as DyLoRA~\cite{valipour-etal-2023-dylora}, dynamically adjust the rank of these matrices within a training budget to optimize performance without a fixed rank constraint, enhancing training efficiency. Another variant, AdaLoRA~\cite{zhang2023adalora}, uses singular value decomposition to refine the update matrix, pruning less significant components based on their impact, thus optimizing the parameter count. These reparameterization strategies ensure that PEFT not only preserves but also enhances model functionality with minimal computational overhead, making them suitable for large-scale models.

These reparameterization techniques closely align with methodologies in high-dimensional statistics, particularly low-rank matrix estimation and recovery \cite{zou2006sparse, candes2011tight, candes2010matrix, cai2013sparse, koltchinskii2015optimal}. Low-rank techniques in statistics aim to estimate high-dimensional matrices by uncovering the underlying low-dimensional structure in the data. Similarly, reparameterized fine-tuning leverages the low-rank approximations in the model to adapt pre-trained models efficiently to new tasks, capturing task-specific information with minimal computational overhead. This connection highlights the theoretical foundation and practical utility of low-rank approaches, as both frameworks exploit the inherently low-dimensional structure in data and models. By doing so, they provide a principled framework for achieving a balance between complexity and accuracy, making them particularly well-suited for large-scale machine learning models.




% \textcolor{red}{Can say more on Lora, sparse }


% \paragraph{IA3}\url{https://huggingface.co/docs/peft/conceptual_guides/ia3}



\subsection{System 2 Prompting and Chain-of-Thought}

``System 2 prompting'' refers to prompts that elicit a deliberate reasoning-like process in AI models that takes the form of the generation of intermediate steps before arriving at a final response~\cite{weston20232attentionisneed, yu2024distilling21}. This contrasts with ``System 1 prompting,'' where a model directly produces a response without intermediate steps. Inspired by human cognitive processes, System 2 prompting is designed to handle complex reasoning tasks that System 1 might struggle with, by using techniques like Chain-of-Thought~\cite{wei2022chain}, Tree-of-Thoughts~\cite{DBLP:conf/nips/YaoYZS00N23}, Graph-of-Thoughts~\cite{DBLP:conf/aaai/BestaBKGPGGLNNH24}, Branch-Solve-Merge~\cite{saha2024branchsolvemergeimproveslargelanguage}, System 2 Attention~\cite{weston20232attentionisneed}, Rephrase and Respond~\cite{deng2024rephraserespondletlarge} and others. These techniques aim to improve performance in areas like multi-step reasoning~\cite{ranaldi-etal-2024-tree}, mathematical problem solving~\cite{wei2022chain}, and commonsense reasoning~\cite{zhao-etal-2024-enhancing-zero}. While System 2 methods can lead to more accurate and interpretable outcomes, they typically involve higher computational costs and latency~\cite{openai2024openaio1card}.
%, making them less common in production systems.

% While System 2 methods can lead to more accurate outcomes, they typically involve higher computational costs and latency, making them less common in production systems.
\paragraph{Inference-Time Scaling Law} In addition to the established scaling laws for training LLMs, recent shifts in focus have highlighted the significance of inference-time scaling laws, particularly following the introduction of OpenAI's o1 model~\cite{openai2024openaio1card}, which is designed to extend various computational steps before generating responses. This can be achieved, for example, by (1) generating multiple candidate responses and selecting the best using methods such as automatic verifiers~\cite{brown2024largelanguagemonkeysscaling}, reward models~\cite{nakano2022webgptbrowserassistedquestionansweringhuman}, or self-consistency~\cite{DBLP:conf/iclr/0002WSLCNCZ23, chen2023universalselfconsistencylargelanguage}, or (2) enhancing the reasoning process within a single trial by introducing more intermediate thinking steps like reflection and revision~\cite{openai2024openaio1card,qin2024o1replicationjourneystrategic, huang2024o1replicationjourney}. For instance, \cite{brown2024largelanguagemonkeysscaling} demonstrated that across multiple tasks and models, the coverage—defined as the fraction of problems solved by any attempt—significantly scales with the number of samples across four orders of magnitude. Complementing this, \cite{snell2024scalingllmtesttimecompute} showed that optimizing inference-time computation through a combination of (1) searching against dense, process-based verifier reward models, and (2) adaptively updating the model's response distribution based on the test-time prompt can yield greater performance improvements than merely scaling model parameters. Furthermore, \cite{liu2025deepseek, kimi2025} observed that by directly optimizing for outcome-based rewards, the system can self-evolve and scale its inference time without external intervention, highlighting the dynamic nature of this process.
Theoretically, advances have also been made, as shown by \cite{DBLP:conf/iclr/0001LZ024}, which revealed that Transformers equipped with Chain-of-Thought (CoT)~\cite{wei2022chain} processing—allowing the Transformer to auto-regressively generate a sequence of intermediate tokens before answering questions—can be adept at handling complex problems that inherently require serial computations. These insights collectively suggest that strategic enhancements in inference-time computation could unlock new capabilities in LLMs, paving the way for more sophisticated and nuanced machine reasoning.


% Chain-of-Thought Prompting, rephrase and respond, (multi-turn prompting)
% \wz{We let the output to be more. We give some trigger words "let's think step by step", so the output would be CoT + y, not just y}

% To rephrase
 
% main question: statistician's expertise be leveraged

\subsection{LLM Reinforcement Learning \& Preference Optimization} \label{sec:llm_alignment}
% \textcolor{red}{TODO: Focus on Writing DPO} 
% Align LLMs to human preference (mostly for safety etc..)
% \begin{itemize}
%     \item RLHF (say more....), DPO, BTO,  etc.
%     \item Reward collapse. if keep training, performance bad.
% \end{itemize}

%\sm{Why not combining with fine-tuning? }

Current strong LLMs can be prompted to perform a variety of tasks. However, these models sometimes exhibit unintended behaviors, such as making up facts or producing biased or toxic content~\cite{weidinger2021ethical,bommasani2022opportunities,kenton2021alignment}. This issue has been framed as a lack of ``alignment'' of the models, where alignment in LLMs is defined in terms of desiderata such as ``ensuring that models are helpful, honest, and harmless''~\cite{askell2021general}. 

\subsubsection{Approaches to Aligning LLMs}
 To tackle the issue of aligning LLMs, researchers have developed two distinct categories of approaches: (1) Reward-based methods, which involve training a reward model using preference data and subsequently optimizing the model's behaviors to maximize the reward received; and (2) Reward-free methods, which eliminate the reward model altogether and instead directly utilize human preferences to train the LLM. 



\paragraph{Reward-based methods} Many leading proprietary LLMs, including GPT-4~\cite{openai2024gpt4} and Claude 3,\footnote{https://www.anthropic.com/news/claude-3-family} employ reward-based methods for alignment, specifically Reinforcement Learning from Human Feedback (RLHF)~\cite{10.5555/3495724.3495977}. The RLHF pipeline usually includes three phases: (1) supervised fine-tuning (SFT); (2) reward modeling and (3) RL optimization. 
\begin{enumerate}
    \item \textbf{SFT Phase}: RLHF typically starts by fine-tuning a pre-trained LM with supervised learning, using high-quality data across a large and diverse set of instruction following tasks. This process aims to establish a well-prepared initial model, denoted as $\pi^{\text{SFT}}$, which serves as a good foundation for subsequent training stages.
    \item \textbf{Reward Modeling Phase}: During this phase, the SFT model is prompted with input $x$ to produce pairs of answers $(y_1, y_2)\sim \pi^{\text{SFT}}(y|x)$. These pairs are then evaluated by human labelers who indicate their preference between the two, represented as $y_w \succ y_l | x$, where $y_w$ and $y_l$ denotes the preferred and dispreferred response among $(y_1, y_2)$ respectively. The preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not have access to. A common approach to modeling these preferences is the Bradley-Terry (BT) model~\cite{19ff28b9-64f9-3656-ba40-08326a05748e}, which posits that the human preference distribution $p^*$ can be expressed as:
    \begin{equation}
        \label{eq:bt}
        p^*(y_1\succ y_2|x) = \frac{\exp(r^*(x, y_1))}{\exp(r^*(x, y_1)) + \exp(r^*(x, y_2))}.
    \end{equation}
    Given a static dataset of comparisons $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model $r_\phi (x, y)$ and estimate its parameters via maximum likelihood. This setup is treated as a binary classification problem, where the negative log-likelihood loss is defined as:
    \begin{equation}
        \label{eq:mle}
        \mathcal{L}_R(r_\phi, \mathcal{D}) = -\E_{(x, y_w, y_l)\sim\mathcal{D}}[\log\sigma(r_\phi(x, y_w)-r_\phi(x, y_l))],
    \end{equation}
    and where $\sigma$ is the logistic function. In the context of LMs, the network $r_\phi(x, y)$ is often initialized from the SFT model $\pi^{\text{SFT}}(y|x)$ with an additional linear layer on top of the final Transformer layer that produces a single scalar prediction for the reward value~\cite{ziegler2020finetuninglanguagemodelshuman}. To ensure a reward function with lower variance, it is common practice to normalize the rewards, such that $\E_{x, y\sim\mathcal{D}}[r_\phi(x, y)] = 0$ for all $x$. Additionally, having a separate reward model offers the advantage of utilizing it for rejection sampling during inference time. This process involves generating multiple responses to a user query and then employing the reward model to identify and select the most suitable response, thereby enhancing the overall performance of the model~\cite{10.5555/3495724.3495977}.

    \item \textbf{RL optimization Phase}: During the RL phase, the learned reward function $r_\phi(x, y)$ is used to provide feedback to the language model. Following prior work~\cite{10.5555/3305381.3305551, DBLP:conf/emnlp/JaquesSGFLJGP20}, the optimization is formulated as 
    \begin{equation}
        \label{eq:rl}
        \max_{\pi_\theta}\E_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}[r_\phi(x, y)] - \beta\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x)||\pi_{\text{ref}}(y|x)],
    \end{equation}
    where $\beta$ is a hyperparameter controlling the deviation from the reference policy $\pi_{\text{ref}}$, which is initially set as the SFT model $\pi^{\text{SFT}}$. In practice, the language model policy $\pi_\theta$ is also initialized to $\pi^{\text{SFT}}$. This constraint is crucial as it helps maintain the model within the bounds of the distribution for which the reward model is accurate, ensuring diversity in generation and preventing the model from collapsing to a few high-reward responses. Furthermore, the training process requires careful implementation of early stopping to prevent reward distributions from collapsing to the same distribution across all prompts, regardless of their diversity \cite{song2023reward}. Due to the discrete nature of language generation, the reward function is typically constructed as $r(x, y) = r_\phi(x, y) - \beta(\log\pi_\theta(y|x) - \log\pi_{\text{ref}}(y|x))$, and maximized using PPO~\cite{schulman2017proximal}.  Additionally, some research has explored alternative RL algorithms, such as REINFORCE~\cite{10.5555/3312046}, as potential replacements for PPO in RLHF. This exploration aims to reduce the computational costs and alleviate the challenges of sensitive hyperparameter tuning associated with PPO~\cite{ahmadian-etal-2024-back}.
\end{enumerate}

% \paragraph{Reward-based methods} 
% \wz{The benefits of having a reward model.}
% \wz{RLHF better than DPO.}
% \wz{Need to add equations. BT model, etc...}
% Many leading proprietary LLMs, including GPT-4~\cite{openai2024gpt4} and Claude 3\footnote{https://www.anthropic.com/news/claude-3-family}, employ reward-based methods for alignment, specifically Reinforcement Learning from Human Feedback (RLHF)~\cite{10.5555/3495724.3495977}. RLHF is a prominent alignment approach that adapts strategies from traditional reinforcement learning (RL). It commonly utilizes techniques like Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} to optimize the model's behavior based on a reward score. This score is generated by a reward model, typically trained as a binary classifier using pairs of completions that have been annotated by human evaluators. Additionally, some research has explored alternative RL algorithms, such as REINFORCE~\cite{10.5555/3312046}, as potential replacements for PPO in RLHF. This exploration aims to reduce the computational costs and alleviate the challenges of sensitive hyperparameter tuning associated with PPO~\cite{ahmadian2024basics}. \lz{need to expand a bit to formally introduce RLHF, e.g., including the BTL model and the loss function}


\paragraph{Reward-free methods}
RLHF is a complex process that often requires significant memory resources and extensive hyperparameter tuning.  As a result, several recent studies have explored alternatives to RLHF, with Direct Preference Optimization (DPO) emerging as a notable method. Described in detail in~\cite{NEURIPS2023_a85b405e}, DPO is an offline preference optimization algorithm that eliminates the need to train a separate reward model, thereby simplifying the process and enhancing training stability. It utilizes a novel reward model parameterization that facilitates the extraction of the optimal policy in a closed form. The DPO loss function is defined as:
\begin{equation}
    \mathcal{L}_{\text{DPO}}(\pi_\theta;\pi_{\text{ref}}) = -\E_{(x, y_w, y_l)\sim\mathcal{D}}\Big[\log\sigma\Big(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \Big)\Big].
\end{equation}
The DPO pipeline proceeds as follows: (1) Sample completions $y_1, y_2\sim\pi_{\text{ref}}(\cdot|x)$ for every prompt x, labelinig them with human preferences to construct the offline preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$ and (2) optimize the language model $\pi_\theta$ to minimize $\mathcal{L}_{\text{DPO}}$ for the given $\pi_\text{ref}$ and $\mathcal{D}$ and chosen $\beta$. Typically, $\pi_{\text{ref}}$ is initialized to $\pi^{\text{SFT}}$ whenever available. 
Despite its advantages, DPO exhibits several limitations, including susceptibility to overfitting~\cite{azar2024general}, a tendency to decrease the likelihood of preferred responses~\cite{pal2024smaug}, and inefficiencies in memory and processing speed due to the simultaneous maintenance of reference and current policies for loss computation~\cite{meng2024simpo}. Additionally, DPO can lead to an issue known as length explosion in responses~\cite{park2024disentangling}. To mitigate these challenges, numerous studies have proposed different DPO variants~\cite{azar2024general, ethayarajh2024kto, pal2024smaug, pang2024iterative, xu2024contrastive, meng2024simpo, park2024disentangling}.  In particular, Iterative DPO \citep{xu2023some} and variants yield improvements over DPO, indicating the importance of training on model responses that are on-policy.



% However, to overcome certain limitations of DPO, several variants have been developed. Identity Preference Optimization (IPO)~\cite{azar2023general} extends the principles of DPO by mathematically exploring its vulnerability to overfitting. IPO proposes a more comprehensive objective that enhances learning from human preferences by addressing these issues directly. Kahneman-Tversky Optimization (KTO)~\cite{ethayarajh2024kto}, drawing inspiration from Kahneman and Tversky’s foundational work on prospect theory, shifts focus from maximizing the log-likelihood of preferences to directly maximizing the utility of LLM outputs. This method simplifies the process by eliminating the need for dual preferences for the same input, concentrating instead on determining the desirability of a given preference. To tackle the issue of DPO potentially reducing the likelihood of preferred responses, solutions like DPO-Positive (DPOP)~\cite{pal2024smaug} and Reasoning Preference Optimization (RPO)~\cite{pang2024iterative} have been introduced. DPOP maintains a high log-likelihood for preferred examples relative to the reference model, while RPO incorporates an additional negative log-likelihood (NLL) term for the preferred response. Addressing the inefficiencies in memory and speed due to maintaining both reference and current policies for loss computation, Constrictive Preference Optimization (CPO)~\cite{xu2024contrastive} introduces an efficient learning method by using a uniform reference model together with a NLL loss for the preferred response. This approach discards the reference model, significantly improving memory and processing efficiency. Similarly, Simple Preference Optimization (SimPO)~\cite{meng2024simpo} eliminates the reference model in the standard DPO loss, using the average log probability of a sequence as an implicit reward. Furthermore, to mitigate the issue of length explosion in responses observed in standard DPO training, a modified version known as Regularized DPO (R-DPO) has been developed~\cite{park2024disentangling}. This adaptation introduces a length regularizer term into the DPO objective, effectively disentangling length from quality in DPO training.



% \paragraph{Reward-free methods}
% \wz{Objective for DPO, others not. move variants to later sections}
% Direct Preference Optimization (DPO)~\cite{NEURIPS2023_a85b405e} is a widely used offline preference optimization algorithm that gets rid of training a reward model in RLHF to improve simplicity and training stability. However, to overcome certain limitations of DPO, several variants have been developed. Identity Preference Optimization (IPO)~\cite{azar2023general} extends the principles of DPO by mathematically exploring its vulnerability to overfitting. IPO proposes a more comprehensive objective that enhances learning from human preferences by addressing these issues directly. Kahneman-Tversky Optimization (KTO)~\cite{ethayarajh2024kto}, drawing inspiration from Kahneman and Tversky’s foundational work on prospect theory, shifts focus from maximizing the log-likelihood of preferences to directly maximizing the utility of LLM outputs. This method simplifies the process by eliminating the need for dual preferences for the same input, concentrating instead on determining the desirability of a given preference. To tackle the issue of DPO potentially reducing the likelihood of preferred responses, solutions like DPO-Positive (DPOP)~\cite{pal2024smaug} and Reasoning Preference Optimization (RPO)~\cite{pang2024iterative} have been introduced. DPOP maintains a high log-likelihood for preferred examples relative to the reference model, while RPO incorporates an additional negative log-likelihood (NLL) term for the preferred response. Addressing the inefficiencies in memory and speed due to maintaining both reference and current policies for loss computation, Constrictive Preference Optimization (CPO)~\cite{xu2024contrastive} introduces an efficient learning method by using a uniform reference model together with a NLL loss for the preferred response. This approach discards the reference model, significantly improving memory and processing efficiency. Similarly, Simple Preference Optimization (SimPO)~\cite{meng2024simpo} eliminates the reference model in the standard DPO loss, using the average log probability of a sequence as an implicit reward. Furthermore, to mitigate the issue of length explosion in responses observed in standard DPO training, a modified version known as Regularized DPO (R-DPO) has been developed~\cite{park2024disentangling}. This adaptation introduces a length regularizer term into the DPO objective, effectively disentangling length from quality in DPO training. 



% \subsubsection{Challenges in Aligning LLMs}


% Model collapse refers to the situations where the performance of LLMs degrade as more and more model-generated data is used in training ~\cite{shumailov2023curse}. Since acquiring high quality human-authored data for training has been extremely expensive and time-consuming, more recent work explore more scalable approaches by collecting data from model generations as training corpus. Right now one of the most intriguing research directions for alignment for LLMs is to apply iterative preference optimization by scoring model generations from previous iteration and apply the same alignment methods iteratively. \cite{yuan2024self} for example shows significant improvement by training on model's own generations on self-instructed instructions, with scores annotated using the LLM itself as judge for preference optimization iteratively. Such self-reward loops consists of many successful recipes including self-instruct \cite{wang-etal-2023-self-instruct}, iterative direct preference optimization \cite{xu2023some}, LLM-as-judge \cite{DBLP:conf/nips/ZhengC00WZL0LXZ23}.
% \url{Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data: https://arxiv.org/abs/2404.01413}


% \paragraph{Limitations of LLM-as-judge} While LLM-as-judge allows for scalability and reduce the need for human annotation, there are certain biases from LLM judges that might accumulates when growing the amount of synthetic data for training. We identify the following limitations of LLM judges or reward models.

% \begin{itemize}
%     \item Positional Bias: LLM exhibits a propensity to favor outputs of certain positions over others.
%     \item Verbosity Bias: LLM sometimes favors longer and more verbose answers than shorter ones even if they are of similar quality.
%     \item Self-enhance Bias: LLM might favors answers generated by themselves over others.
% \end{itemize}


% \url{https://arxiv.org/abs/2404.01413}

% Recent work from \cite{gerstgrasser2024model} shows that accumulating data, rather than doing true on-policy iteration, helps models avoid mode collapse.




\subsection{LLM Self-Alignment}
Aligning LLMs using human feedback is often bottlenecked by the size and quality of human-annotated data.  As models reach or surpass human-level intelligence in particular domains, it is expected that future models may require feedback that goes beyond what humans can provide in order to provide an adequate training signal. Leveraging the LLM itself to provide such feedback, in particular to create high-quality data for instruction-finetuning, called ``synthetic data,'' has become a promising and scalable solution. 
%\jing{}
% \jx{AI generated synthetic instructions: self instruct(few shot), zero-shot key words, back translation ; write less}

% \jx{AI generated responses: LM task moved to here}

% \jx{AI generated feedbacks: constitution AI, RLAIF, self-critique, self-taught evaluator; feedback type: binary ranking,=, critique, }

%\jx{https://arxiv.org/pdf/2410.04840}

\subsubsection{Synthetic Data Generation}

Current alignment methods in synthetic data generation often involve several key components: (a) Instructions, (b) Responses, and (c) Feedback on those responses.

\textbf{Instructions} have typically been provided by human users and consist of questions or tasks that the user wants the model to address during a dialog interaction. Although there are publicly available instruction datasets sourced from human users, such as ShareGPT,\footnote{https://huggingface.co/datasets/RyokoAI/ShareGPT52K} OpenAssistant~\cite{10.5555/3666122.3668186}, WildChat~\cite{DBLP:conf/iclr/Zhao0HC0D24}, and LMSYS-Chat-1M~\cite{DBLP:conf/iclr/ZhengC0LZW00LXG24}, large-scale instruction datasets remain scarce due to privacy and other concerns. Consequently, several studies have focused on generating synthetic instructions. For instance, Self-Instruct~\cite{wang-etal-2023-self-instruct} employs an iterative bootstrapping algorithm, beginning with a limited set of manually crafted tasks to guide the generation process. This approach uses few-shot prompting to enable the model to create novel prompts, which are then added to the prompt pool for further bootstrapping. Similarly, MathScale~\cite{DBLP:conf/icml/TangZWW24} extracts topics and concepts from seed math questions to construct a concept graph. Concepts are randomly sampled from this graph, and few-shot prompting is used to prompt GPT-3.5 to generate new questions. CodecLM~\cite{wang-etal-2024-codeclm} utilizes LLMs to summarize use cases and required skills from existing seed instructions, subsequently decoding novel instructions based on different combinations of use cases and skills.



Utilizing \textbf{Responses} generated by models in the training loop has been found to have significant potential in enhancing training outcomes, as in the standard RLHF training loop (cf. Sec.~\ref{sec:llm_alignment}). 
For example, self-generated responses can easily be leveraged  through  pairwise preference learning, where positive and negative generations are both used to train the model~\cite{pang2024iterative}.
Other examples of self-training include STaR~\cite{DBLP:conf/nips/ZelikmanWMG22}, which collects successful model-generated rationales that solve problems effectively, using them as training data for iterative supervised fine-tuning. 

To effectively utilize responses for supervised fine-tuning (SFT) or preference learning, it is crucial to assess their quality through \textbf{feedback}. If a response is suboptimal, it may still be improved through various methods. Feedback on responses can be binary or scalar, indicating the quality of a response, or it can be preference-based, showing relative quality among multiple responses. For scalar or preference feedback, while some tasks with fixed short answers are straightforward to evaluate, tasks requiring long, free-form generation pose challenges. Standard approaches are to use a reward model, or by LLM prompting, referred to as LLM-as-a-Judge. Recent studies have shown that LLM-as-a-Judge prompting can yield feedback that aligns well with human judgments~\cite{alpaca_eval, DBLP:conf/nips/ZhengC00WZL0LXZ23}. Feedback can also be provided in natural language, critiquing the response. For instance, Constitutional AI~\cite{bai2022constitutional} uses LLMs to generate critiques and revisions based on constitutional principles to enhance model-generated responses. Other specialized critic generators, such as Shepherd~\cite{DBLP:journals/corr/abs-2308-04592}, PandaLM~\cite{pandalm2024}, Auto-J~\cite{DBLP:conf/iclr/LiSYF0024}, and LLMCRIT~\cite{yuan-etal-2024-llmcrit}, have been developed. 



\subsubsection{Self-Rewarding Language Models}
Current alignment methods, such as RLHF, heavily depend on human annotation for some of the aforementioned components, particularly instructions and feedback. However, there is a scarcity of high-quality public instruction sources, especially those involving complex and challenging tasks, such as advanced mathematics and reasoning. As AI systems become increasingly sophisticated, the reliance on human feedback becomes more problematic. Superintelligent AI could generate outputs and exhibit behaviors that surpass human comprehension, such as producing vast amounts of novel code that may be difficult to interpret or potentially dangerous to execute.  The question is how to maintain trust and control over these powerful models. Training such AI systems that remain helpful and harmless even as they surpass human-level performance suggests that we will need techniques and data that do not rely primarily on human inputs.


One promising approach to address the challenge of aligning advanced AI is iterative preference optimization~\cite{xu2023some}, which leverages a reward model rather than relying on human annotators to score LLM outputs. This method has proven effective when a well-trained reward model is available~\cite{viethoangtranduong,meng2024simpo,guo2024directlanguagemodelalignment}. However, a particularly intriguing idea is to forgo the assumption of access to an external reward model. Instead, both the LLM and the reward model are the same, allowing the model to improve itself autonomously. Demonstrating that iterative preference optimization can also be effective with synthetic instructions would further validate its potential to provide an entire autonomous training pipeline.


Self-rewarding language models~\cite{yuan2024selfrewardinglanguagemodels} embody this concept. These models begin with a small set of human-authored data that provides basic instruction-following and evaluation capabilities. The model then iteratively improves itself by generating responses to synthetic instructions and scoring these responses using an LLM-as-a-Judge mechanism~\cite{DBLP:conf/nips/ZhengC00WZL0LXZ23}. The best and worst responses for each instruction are selected for further training through DPO, allowing the model to enhance its instruction following and evaluation capabilities without any human intervention. The authors show that it is possible to achieve iterative self-improvement using the model's instruction following and evaluation capabilities on pure synthetic generated instructions. Building upon this, meta-rewarding language models~\cite{wu2024metarewardinglanguagemodelsselfimproving} take the concept further by not only curating training pairs for instruction following but also for evaluation (LLM-as-a-Judge performance). In each iteration, the model compares multiple judgments on a single response and selects the best judgment pairs for DPO training in addition to response pairs. This refinement allows the model to improve both its instruction-following and evaluation skills more effectively than the self-rewarding approach. These methods represent a significant step toward aligning highly advanced AI, reducing the need for direct human supervision.


% \wz{reward collapse. if keep training, performance bad.}
\subsubsection{Challenges}
Although synthetic data can greatly enhance the scalability of model training, recent studies have highlighted challenges associated with using data generated by large language models (LLMs).

\paragraph{Reward Misspecification}
In iterative preference optimization, reward models are crucial but inherently imperfect. These models, whether generative or classifier-based, serve as proxies for human judgment, but their flaws can lead to significant issues~\cite{10.5555/3618408.3618845}. This phenomenon is often described by Goodhart’s law, which states that when a measure becomes a target, it ceases to be a good measure. In this context, over-optimization can amplify the reward model's biases, distorting true performance rather than enhancing it. Several types of biases can arise in this process. Position bias occurs when an LLM exhibits a propensity to favor certain positions over others, potentially skewing results~\cite{DBLP:conf/nips/ZhengC00WZL0LXZ23}. Verbosity bias is another concern, where an LLM judge favors longer, more verbose responses, even if they lack clarity, quality, or accuracy compared to shorter alternatives~\cite{singhal2024longwaygoinvestigating, dubois2024lengthcontrolledalpacaevalsimpleway}. Additionally, self-enhancement bias, a term adopted from social cognition literature, describes the tendency of LLM judges to favor answers generated by themselves, further complicating the evaluation process~\cite{DBLP:conf/nips/ZhengC00WZL0LXZ23}. To address these challenges, researchers have introduced benchmarks like RewardBench~\cite{lambert2024rewardbenchevaluatingrewardmodels} and JudgeBench~\cite{tan2024judgebenchbenchmarkevaluatingllmbased}. Both of these benchmarks systematically evaluate the abilities of LLM judges to correctly identify the better and worse responses within a pair. They cover various categories, including chat, safety, and reasoning, among others. These benchmarks help identify the strengths and weaknesses of different reward models, guiding the development of more reliable and effective reward models for preference optimization.

\paragraph{Distribution Shift and Lack of Diversity}
% \wz{bootstrap? overcome model collapse using statistical methods.}
Recent findings indicate that while LLMs can generate text nearly indistinguishable from human-authored content, the overall distribution of these LLM-generated texts differs notably from human text. Specifically, LLM-generated texts tend to have truncated ``tails,'' i.e., they produce fewer extreme or rare outputs, and exhibit reduced variability~\cite{shumailov2023curse,dohmatob2024taletailsmodelcollapse}. Training on LLM-generated data could potentially lead to a phenomenon 
%known as model collapse~\cite{shumailov2024curserecursiontraininggenerated}. This convergence towards 
where a narrow range of behaviors results in decreased performance, as seen in tasks like language modeling~\cite{shumailov2023curse,dohmatob2024taletailsmodelcollapse} and iterative preference optimization for mathematical reasoning~\cite{wu2024progressregressselfimprovementreversal}. Studies show maintaining a mix of clean, human-authored data alongside LLM-generated content during training helps preserve diversity and prevents the model from deteriorating in performance~\cite{shumailov2023curse,dohmatob2024taletailsmodelcollapse,gerstgrasser2024modelcollapseinevitablebreaking}.  A few works go beyond the mixing scenario and study how to curate or filter synthetic data to avoid such performance deterioration~\cite{feng2024beyond,zhang2024regurgitative}.

% \paragraph{Statistical Analysis of Scaling Laws} Understanding scaling laws presents both significant challenges and exciting opportunities for the statistics community, particularly as they pertain to modern machine learning and large-scale data applications. For instance, \cite{jain2024scaling} underscores the potential of scaling laws to guide the effective integration of surrogate data, addressing a critical bottleneck in scenarios where collecting high-quality data is expensive or impractical. The discovery that surrogate data can significantly reduce test error—even when unrelated to the original dataset—raises questions about the limits of this approach and its implications for generalization. These insights also point to opportunities for refining weighting strategies and extending scaling law analyses to other statistical frameworks. By tackling these open problems, statisticians can contribute to a deeper understanding of scaling phenomena while expanding their influence in shaping the future of machine learning and data-driven decision-making. \lz{do we have more papers for this subsection? if not, do we need to put this discussion somewhere else?}

%Recent efforts in the stats community study the scaling laws in statistical models. \cite{jagadeesan2024safety} investigate the scaling laws in an mult-objective optimization and \cite{jain2024scaling}  considers the scaling laws with synthetic data . 
\section{Designing Trustworthy LLMs by Statistical Methods}\label{sec4:method}

%Mechanistic Interpretability
%Causality 

%{\cyan interpretability (induction head; causal graph); jailbreaking \& adversarial robustness;} jailbreaking \& adversarial robustness are important, but perhaps something that statistics/math can do little. we might want to concentrate on topics where mathematicians/statisticians can make a difference.

As LLMs increasingly permeate various aspects of society, ensuring their trustworthiness has become a critical challenge. Trustworthiness encompasses a range of dimensions, including interpretability, accountability, and algorithmic fairness. Statistical methods offer a rigorous and systematic approach to address these challenges, providing tools to analyze, enhance, and monitor the behavior of LLMs. This section explores how statistical techniques can contribute to the design of trustworthy LLMs across several key areas. First, we discuss mechanistic interpretability, which aims to uncover how LLMs make predictions and generate outputs. Next, we examine uncertainty quantification, a vital component for understanding and communicating the reliability of model predictions. The section then delves into methods for LLM watermarking, which ensures the provenance and authenticity of generated content, and privacy-preserving mechanisms to safeguard user data. We also address algorithmic fairness, focusing on statistical strategies to mitigate biases in LLM outputs. Finally, we provide a statistical perspective on LLM self-alignment, a process for aligning model behavior with human values and goals. Together, these topics underscore the essential role of statistics in fostering trust and accountability in the era of large-scale AI models.

% \sm{Speculative decoding}




%{\color{cyan} may move the subsection LLM Super Alignment here}
% \subsection{Tokenization}
% \wl{Maybe not needed in the paper}
% \url{https://arxiv.org/abs/2404.08335}

% Tokenization is the initial step to build a LLM. A tokenizer segments the input text sequence and maps each piece into a token id in the encoding stage, and vice versa in the decoding stage. Each token may represent a character, a part of a word, or an entire phrase. Although the success of LLM is often attributed to its end-to-end training nature, the tokenizer is trained separately with a different data source, and is frozen during the entire model training stage. Although there are tons of attempts to eliminate this extra step \cite{choe2019bridging,xue2022byt5,clark2022canine}, tokenization is still necessary for most of current language models. 

% \wl{Maybe introduce the tokenization algorithm?}

% Despite the empirical prevalence of tokenization, it is unclear why this step is important. In \cite{rajaraman2024toward}, the authors propose a framework to study the role of tokenization on a simple model. The data generation process they consider is a k-th order switching process, where $\bP(X_n=1|X_{n-k}=0)=p$ and $\bP(X_n=0|X_{n-k}=1)=q$ for all $n$. Empirically, it is found that regardless of the tokenization method being used, the Transformer model essentially learns a unigram model that output the same prediction distribution for all tokens, which can be expressed by 
% $$Q(t_1, t_2, \cdots, t_j) = Q_{\#}(j)\Pi_{i=1}^j Q_{\text{tok}}(t_i).$$
% However, the unigram model cannot capture the higher-order dependence encoded in the k-th order switching process, resulting in poor performance in next-token prediction. On the contrary, with a naive tokenization that assigns all possible substrings of length $k$ \wl{Their notation is super confusing} as unique tokens, it was shown to be able to capture the long-term dependence and achieve near-optimal error on next-token prediction. Furthermore, they show that many popular choices of tokenization methods, such as LZW and BPE, can achieve the optimal error with much smaller vocabulary size compared with this naive tokenization.


%{\cyan move to the discussion section}




% \subsection{Uncertainty Quantification}
%  While LLMs produce human-like responses with impressive accuracy across various tasks, they are also prone to hallucination \cite{ji2023survey,rawte2023survey}, raising concerns about their reliability. Quantifying uncertainty is crucial for addressing these limitations, as it allows models to provide not just answers but also confidence in their outputs, enabling users to make more informed decisions.

% \paragraph{Uncertainty Metrics}
% While uncertainty estimation and calibration are well-established for traditional machine learning models \cite{abdar2021review, gawlikowski2023survey}, the emergence of LLMs has introduced new challenges and demands. Unlike fixed-dimensional outputs typical of traditional models, LLM responses are often complex, requiring uncertainty metrics that can operate on sentence-level outputs. This complexity necessitates innovative approaches to quantify uncertainty effectively. Classical metrics, such as entropy, can be directly calculated on the probability distribution of next-token prediction and averaged over all tokens \cite{malinin2020uncertainty}. To incorporate the special structure of language model, existing approaches further considered semantic features such as semantic similarity \cite{fomicheva2020unsupervised, lin-etal-2022-towards}, semantic equivalence \cite{kuhn2023semantic} and token importance \cite{duan2024shifting, bakman2024marsmeaningawareresponsescoring}, as well as internal signals in language models like logits and hidden states \cite{kadavath2022language, chen2024inside, liu2024uncertainty} into the metric design. In general, they can be easily computed in a white-box setting where the underlying representation and prediction distribution are available, while for black-box models, some can be calculated via repeated sampling of the response \cite{kadavath2022language, lin2024generatingconfidenceuncertaintyquantification, chen-mueller-2024-quantifying}. Overall, these approaches aim to develop robust uncertainty metrics capable of appropriately assessing the confidence of LLM-generated responses in a meaningful and scalable manner.  

% \paragraph{Conformal Prediction in LLMs} 
% Conformal Prediction (CP) \cite{vovk2005algorithmic, angelopoulos2023conformal} has emerged as a versatile framework for distribution-free statistical inference. CP constructs confidence sets for predictions based on the empirical distribution of residuals, ensuring validity without assumptions about the underlying model or data. Its flexibility and computational efficiency have made it an appealing tool for LLMs despite challenges like large output spaces and non-exchangeable token sequences.
% To address these issues, works such as \cite{kumar2023conformal,ren2023robots} have restricted the output space, applying CP to tasks like multiple-choice question answering and robot actions, while \cite{ravfogel2023conformal} calibrated nucleus sampling to improve token-level predictions. Other methods, such as \cite{ulmer2024non}, adapt CP to non-exchangeable settings by leveraging latent representations for nearest-neighbor searches, resulting in more precise prediction sets. Beyond improving accuracy, CP has been extended to control quantities like toxicity and hallucination risks \cite{zollo2023prompt, yadkori2024mitigating,mohri2024language, cherian2024large}, enabling safer and more aligned LLM applications. CP methods have also been leveraged for evaluating LLM performance. \cite{ye2024benchmarking} applied CP to benchmark LLMs on five natural language processing tasks, measuring average confidence set sizes for multiple-choice questions to quantify uncertainty. In machine translation, \cite{giovannotti2023evaluating} and \cite{zerva2023conformalizing} used CP to assess translation quality, providing calibrated confidence estimates for both human and machine evaluations. Additionally, \cite{schuster2021consistent, schuster2022confident} proposed confident early exiting methods for Transformers, where intermediate layers assess uncertainty to speed up inference while maintaining consistency with the full model.

% \paragraph{Hallucination Detection}
% Recently, there has been a growing trend toward adopting uncertainty estimation methods to address hallucination detection in LLMs. The core idea is that the logits and hidden states of LLMs encapsulate information about the model's confidence in its generated output, which can be leveraged to identify hallucinations. For example, \cite{azaria2023internal,slobodkin2023curious,chen2024inside} use activations from hidden layers as input to train a classifier specifically designed to detect hallucinations, while \cite{verma2023reducing} develop epistemic neural networks that aim to reduce hallucination occurrences by modeling epistemic uncertainty. Despite these advances, the lack of a consistent and rigorous definition of hallucination across the literature poses challenges to standardizing this task. However, uncertainty estimation remains a well-defined problem, and insights from uncertainty quantification can be directly applied to improve hallucination detection in LLMs, offering a robust statistical foundation for this critical challenge.

% \paragraph{Future Research}
% Although existing uncertainty quantification methods have shown promise in various aspects of language models, many challenges remain. First, many uncertainty metrics rely on the entropy of the next-token prediction distribution in the white-box setting or the consistency between multiple responses in the black-box setting. However, uncertainty in the generation distribution may not appropriately capture the factual accuracy of language model outputs. For example, when a language model exhibits high confidence in incorrect facts, its confidence estimates can become miscalibrated, making it difficult to detect errors through uncertainty metrics alone. Furthermore, recent research has demonstrated that these metrics can be fragile and easy to manipulate under adversarial attacks \cite{zeng2024uncertainty}. Future work should focus on developing more robust uncertainty metrics that better reflect the reliability of responses and improve the alignment of existing metrics with factual correctness. Second, as previously mentioned, the broad applicability of conformal predictions in the context of language models is limited by the challenges of non-exchangeability and the large discrete space of natural language data. Addressing these limitations by designing computationally efficient conformal prediction methods that are better aligned with the structure of language data is an exciting area for future research. Lastly, beyond current applications, we want to highlight that uncertainty quantification has the potential to benefit a broader range of applications, such as generalizing to multi-round interactions, guiding data collection, and enhancing model interpretability. Exploring these directions can be helpful in advancing uncertainty-aware language models and improving their trustworthiness in real-world applications.
\subsection{Uncertainty Quantification}
 While LLMs produce human-like responses with impressive accuracy across various tasks, they are also prone to hallucination \cite{ji2023survey,rawte2023survey}, raising concerns about their reliability. Quantifying uncertainty is crucial for addressing these limitations, as it allows models to provide not just answers but also confidence in their outputs, enabling users to make more informed decisions.

\paragraph{Uncertainty Metrics}
While uncertainty estimation and calibration are well-established for traditional machine learning models \cite{abdar2021review, gawlikowski2023survey}, the emergence of LLMs has introduced new challenges and demands. Unlike fixed-dimensional outputs typical of traditional models, LLM responses are often complex, requiring uncertainty metrics that can operate on sentence-level outputs. This complexity necessitates innovative approaches to quantify uncertainty effectively. Classical metrics, such as entropy, can be directly calculated on the probability distribution of next-token prediction and averaged over all tokens \cite{malinin2020uncertainty}. To incorporate the special structure of language model, existing approaches further considered semantic features such as semantic similarity \cite{fomicheva2020unsupervised, lin-etal-2022-towards}, semantic equivalence \cite{kuhn2023semantic} and token importance \cite{duan2024shifting, bakman2024marsmeaningawareresponsescoring}, as well as internal signals in language models like logits and hidden states \cite{kadavath2022language, chen2024inside, liu2024uncertainty} into the metric design. In general, they can be easily computed in a white-box setting where the underlying representation and prediction distribution are available, while for black-box models, some can be calculated via repeated sampling of the response \cite{kadavath2022language, lin2024generatingconfidenceuncertaintyquantification, chen-mueller-2024-quantifying}. Overall, these approaches aim to develop robust uncertainty metrics capable of appropriately assessing the confidence of LLM-generated responses in a meaningful and scalable manner.  

\paragraph{Conformal Prediction in LLMs} 
Conformal Prediction (CP) \cite{vovk2005algorithmic, angelopoulos2023conformal} has emerged as a versatile framework for distribution-free statistical inference. CP constructs confidence sets for predictions based on the empirical distribution of residuals, ensuring validity without assumptions about the underlying model or data. Its flexibility and computational efficiency have made it an appealing tool for LLMs despite challenges like large output spaces and non-exchangeable token sequences.
To address these issues, works such as \cite{kumar2023conformal,ren2023robots} have restricted the output space, applying CP to tasks like multiple-choice question answering and robot actions, while \cite{ravfogel2023conformal} calibrated nucleus sampling to improve token-level predictions. Other methods, such as \cite{ulmer2024non}, adapt CP to non-exchangeable settings by leveraging latent representations for nearest-neighbor searches, resulting in more precise prediction sets. Beyond improving accuracy, CP has been extended to control quantities like toxicity and hallucination risks \cite{zollo2023prompt, yadkori2024mitigating,mohri2024language, cherian2024large}, enabling safer and more aligned LLM applications. CP methods have also been leveraged for evaluating LLM performance. \cite{ye2024benchmarking} applied CP to benchmark LLMs on five natural language processing tasks, measuring average confidence set sizes for multiple-choice questions to quantify uncertainty. In machine translation, \cite{giovannotti2023evaluating} and \cite{zerva2023conformalizing} used CP to assess translation quality, providing calibrated confidence estimates for both human and machine evaluations. Additionally, \cite{schuster2021consistent, schuster2022confident} proposed confident early exiting methods for Transformers, where intermediate layers assess uncertainty to speed up inference while maintaining consistency with the full model.

\paragraph{Hallucination Detection}
Recently, there has been a growing trend toward adopting uncertainty estimation methods to address hallucination detection in LLMs. The core idea is that the logits and hidden states of LLMs encapsulate information about the model's confidence in its generated output, which can be leveraged to identify hallucinations. For example, \cite{azaria2023internal,slobodkin2023curious,chen2024inside} use activations from hidden layers as input to train a classifier specifically designed to detect hallucinations, while \cite{verma2023reducing} develop epistemic neural networks that aim to reduce hallucination occurrences by modeling epistemic uncertainty. Despite these advances, the lack of a consistent and rigorous definition of hallucination across the literature poses challenges to standardizing this task. However, uncertainty estimation remains a well-defined problem, and insights from uncertainty quantification can be directly applied to improve hallucination detection in LLMs, offering a robust statistical foundation for this critical challenge.
One example to frame the hallucination detection within a hypothesis testing framework. Specifically, given a question $q$ and an LLM-generated answer $M(q)$, the problem can be formulated as follows:
\[H_0(q,M(q)): M(q)\text{ is not correct for }q\quad{\rm vs.}\quad H_1(q,M(q)): M(q)\text{ is correct for }q.\]
The object is to construct a test function $\hat T(q,M(q))\in\{0,1\}$ that, with probability at least $1-\delta$ over its construction, ensures that at least $100(1-\alpha)\%$ of incorrect $(q,M(q))$ pairs in future question-answering instances are detected: $\Prob_{\hat T}\big(\Prob_{q,M(q)}\big(\hat T(q,M(q))=1\big|H_0(q,M(q))\big)>\alpha\big)\le\delta.$ 
To achieve this, \cite{nie2024fact} propose a hypothesis test that provides finite-sample, distribution-free type I error control, leveraging a set of i.i.d. samples consisting of $(q,M(q))$ along with a correct answer for $q$. 


\paragraph{Future Research}
Although existing uncertainty quantification methods have shown promise in various aspects of language models, many challenges remain. First, many uncertainty metrics rely on the entropy of the next-token prediction distribution in the white-box setting or the consistency between multiple responses in the black-box setting. However, uncertainty in the generation distribution may not appropriately capture the factual accuracy of language model outputs. For example, when a language model exhibits high confidence in incorrect facts, its confidence estimates can become miscalibrated, making it difficult to detect errors through uncertainty metrics alone. Furthermore, recent research has demonstrated that these metrics can be fragile and easy to manipulate under adversarial attacks \cite{zeng2024uncertainty}. Future work should focus on developing more robust uncertainty metrics that better reflect the reliability of responses and improve the alignment of existing metrics with factual correctness. Second, as previously mentioned, the broad applicability of conformal predictions in the context of language models is limited by the challenges of non-exchangeability and the large discrete space of natural language data. Addressing these limitations by designing computationally efficient conformal prediction methods that are better aligned with the structure of language data is an exciting area for future research. Lastly, beyond current applications, we want to highlight that uncertainty quantification has the potential to benefit a broader range of applications, such as generalizing to multi-round interactions, guiding data collection, and enhancing model interpretability. Exploring these directions can be helpful in advancing uncertainty-aware language models and improving their trustworthiness in real-world applications.

% Old version as of 12/06/2024
% Although LLMs can produce human-like responses with decent accuracy to a wide range of questions, it is also well-known to hallucinate frequently \cite{ji2023survey,rawte2023survey} and therefore lack of reliability. To address this concern, it is important to develop statistical tools to quantify the uncertainty of LLMs by providing not only accurate answers but also their confidence. Conformal Prediction (CP) \cite{vovk2005algorithmic, angelopoulos2023conformal} has emerged recently as a general framework to make distribution-free statistical inferences for black box models. The core idea of CP is to utilize the exchangeability of the data. A simple yet effective scheme of CP is the so-called split conformal, which holds out a calibration dataset and evaluates the prediction residual $|Y-f(X)|$ of a trained model $f$ to obtain the empirical distribution of residual. Under the assumption that the test data point is exchangeable to the calibration dataset, one can make a valid confidence interval for the true labels through the empirical quantile of residuals evaluated on the calibration dataset. Since the conformal prediction procedure only involves the empirical distribution, it can be adapted to any machine learning task without any assumption on the model or data distribution and produce valid statistical inference. In addition to its theoretical rigorousness, CP methods are also efficient to compute as they only require model inference on a relatively small labeled calibration dataset instead of re-training the model. 
% Due to their flexibility and efficiency, CP methods have attracted a rapidly growing interest in the general machine-learning community. We refer interested readers to \cite{angelopoulos2023conformal} for an extensive review of theory and application on the general conformal prediction method, and we will focus on its application in language models below. 


% \paragraph{Conformal Prediction for LLM Inference.} The distribution-free nature of CP methods enables them to be used in complicated models such as LLMs. However, the size of the output space for LLMs is extremely large, and the sequential structure of language is generally non-unexchangeable, posing challenges to the use of CP. Therefore, some works focus on restricting the output space of LLMs to a few candidate sentences, for example, \cite{kumar2023conformal} applied CP for multiple choice question answering tasks, \cite{ren2023robots} use CP to measure the uncertainty over robot actions generated by LLMs and ask for human help when the confidence set have more than one action. Another line of works use CP to calibrate the next-token prediction: \cite{ravfogel2023conformal} proposed Conformal Nucleus Sampling to calibrate the nucleus sampling procedure, which aims to select the smallest set of tokens that have cumulative probability exceeds a given threshold $p$, in particular, they propose to use instances within the same range of prediction entropy to calibrate the distribution. Furthermore, \cite{ulmer2024non} addressed the non-exchangeable  structure in token sequences by searching nearest neighbors with the latent representation in the calibration dataset, and it is shown to produce smaller prediction sets.

% \paragraph{Conformal Prediction for LLM Safety and Alignment:} Other than focusing on the testing accuracy, the flexibility of CP allows it to be used to control various quantities of interest for LLMs with rigorous probability guarantee, such as toxicity or untruthfulness. Along this line, \cite{zollo2023prompt} studied the problem of selecting prompts from a finite set, such that a risk constraint generating a toxic response is lower than a given threshold with high probability. \cite{gui2024conformal} proposed Conformal Alignment, a CP-based framework to produce a selection rule for language model generations to guarantee alignment proportion in the selected units, which can be viewed as an analogy of false discovery rate in multiple testing. Similarly, \cite{yadkori2024mitigating} attempts to mitigate the hallucination of LLMs by using the CP method to determine an abstention policy for LLMs (i.e., abstain from responding).  On the level of an individual response, \cite{mohri2024language, cherian2024large} applied the idea of conformal prediction to filter the factual statement in the response. Their approach first decomposes a single response into several sub-claims, then constructs a nested sequence of sub-claim sets and uses CP method to determine a set of sub-claims that is guaranteed to be entailed by the ground-truth fact. 

% \paragraph{Conformal Prediction for LLM Evaluation:} As a powerful tool to quantify the uncertainty, CP method can also assist to evaluating LLMs. For example, \cite{ye2024benchmarking} applies CP to integrate uncertainty quantification into LLM benchmarking. They choose five representative natural language processing tasks and transform them into multiple choices questions, and then evaluate the uncertainty of LLMs by their average confidence set size returned by CP. In addition to evaluating the multiple choice questions, \cite{giovannotti2023evaluating,zerva2023conformalizing} also use conformal prediction to evaluate the quality of machine translation by providing quality assessment from either human or LLMs. Moreover, using the quantified uncertainty, \cite{schuster2021consistent, schuster2022confident} proposed confident adaptive models that evaluate the uncertainty of intermediate layers in a Transformer model and exit early as long as it has enough uncertainty exceeding a give threshold. It was shown that their method can speed up model inference with guaranteed consistency with the full model. 

% Old version 
% Although LLMs can produce human-like responses with decent accuracy to a wide range of questions, it is also well-known to hallucinate frequently \cite{ji2023survey,rawte2023survey} and therefore lack of reliability. To address this concern, it is important to develop statistical tools to quantify the uncertainty of LLMs by providing not only accurate answers but also their confidence. Conformal Prediction (CP) \cite{vovk2005algorithmic, angelopoulos2023conformal} has emerged recently as a general framework to make distribution-free statistical inferences for black box models. In addition to its theoretical rigorousness, CP methods are also efficient to compute as they only require model inference on a relatively small labeled calibration dataset instead of re-training the model. Due to their flexibility and efficiency, CP methods have attracted a rapidly growing interest in the general machine learning community.

% Here we give a brief introduction to split conformal prediction, a simple and effective method in the CP family, and we refer interested readers to \cite{angelopoulos2023conformal} for advanced methods. Consider a prediction task where we want to predict $Y\in \cY$ based on features $X \in \cX$. To conduct the split conformal prediction method, we need to choose a score function $s: \cX\times \cY\rightarrow \mathbb{R}$ and hold out a calibration dataset $\cD = \{(X_1, Y_1), \cdots, (X_n, Y_n)\}_{i=1}^n$ that is exchangeable (a special case is i.i.d.) to test data $(X_{\text{test}}, Y_{\text{test}})$ in prior. The split conformal prediction method first computes the scores on the calibration set denoted as $s_i = s(X_i, Y_i)$, then the confidence set for prediction on the test data point can be computed by 
% $$
% \cC_{1-\alpha}(X_{\text{test}}) = \left\{y: s(X_{\text{test}}, y)\leq \hat q_{\frac{\lceil(n+1)(1-\alpha)\rceil}{n}}\right\},
% $$
% where $\hat q_{\frac{\lceil(n+1)(1-\alpha)\rceil}{n}}$ denotes the 
% $\frac{\lceil(n+1)(1-\alpha)\rceil}{n}$ quantile of $\{s_i\}_{i=1}^n$. It is guaranteed that under the exchangeable condition, $\cC_{1-\alpha}(X_{\text{test}})$ is a valid $1-\alpha $ confidence set in the sense that $\Prob(Y_{\text{test}}\in \cC_{1-\alpha}(X_{\text{test}}))\geq 1-\alpha$. The score function $s$ measures the disagreement between $x$ and $y$, and it is often determined by a machine learning model $f: \cX\rightarrow\cY$. For example, in a regression task, a popular choice is $s(X,Y) = |Y-f(X)|$, and in a classification task, it is often chosen as $s(X,Y) = 1-f(X)_Y$, where $f(X)_Y$ denotes the softmax output of the true class. Going beyond constructing confidence set to bound the miscoverage $\Prob(Y_{\text{test}}\notin \cC_{1-\alpha}(X_{\text{test}}))$, conformal prediction can be extended to control various quantities of the form $\E [\ell(\cC(X_{\text{test}}), Y_{\text{test}})]$ with any loss function $\ell$ that shrinks as $\cC$ grows in a similar flavor, such as false negative rate and graph distance \cite{angelopoulos2022conformal}.

% \paragraph{Conformal Prediction for LLM Inference.} The distribution-free nature of CP methods enables them to be used in complicated models such as LLMs. However, the size of the output space for LLMs is extremely large, and the sequential structure of language is generally non-unexchangeable, posing challenges to the use of CP. Therefore, some works focus on restricting the output space of LLMs to a few candidate sentences, for example, \cite{kumar2023conformal} applied CP for multiple choice question answering tasks, \cite{ren2023robots} use CP to measure the uncertainty over robot actions generated by LLMs and ask for human help when the confidence set have more than one action. Another line of works use CP to calibrate the next-token prediction: \cite{ravfogel2023conformal} proposed Conformal Nucleus Sampling to calibrate the nucleus sampling procedure, which aims to select the smallest set of tokens that have cumulative probability exceeds a given threshold $p$, in particular, they propose to use instances within the same range of prediction entropy to calibrate the distribution. Furthermore, \cite{ulmer2024non} addressed the non-exchangeable  structure in token sequences by searching nearest neighbors with the latent representation in the calibration dataset, and it is shown to produce smaller prediction sets.

% \paragraph{Conformal Prediction for LLM Safety and Alignment:} Other than searching for a prediction set with a guarantee to include the ground truth, the flexibility of CP allows it to be used to control various quantities of interest for LLMs, such as toxicity or untruthfulness. Along this line, \cite{zollo2023prompt} studied the problem of selecting prompts from a finite set, subject to a risk constraint. Instead of forming confidence sets on the output, they aim to select the prompts such that the risk of generating a toxic response is lower than a given threshold with high probability. \cite{gui2024conformal} proposed Conformal Alignment, a CP-based framework to produce a selection rule for language model generations with a guarantee of alignment proportion in the selected units. The alignment is measured by any pre-specified criterion, such as the reliability of LLM-generated reports. Their approach formulates the selection process as a multiple testing problem, then computes the conformal $p$-values based on the alignment score for each generation, and uses the Benjamini Hochberg procedure to control the false discovery rate in the selections. Similarly, \cite{yadkori2024mitigating} attempts to mitigate the hallucination of LLMs by using the CP method to determine an abstention policy for LLMs (i.e., abstain from responding). They design a score for each query by measuring the similarity of multiple responses from LLMs, and then abstain on the queries that have lower scores with a threshold determined by using CP to control the expected hallucination rate. On the level of an individual response, \cite{mohri2024language, cherian2024large} applied the idea of conformal prediction to filter the factual statement in the response. Their approach first decomposes a single response into several sub-claims, then constructs a nested sequence of sub-claim sets and uses CP method to determine a set of sub-claims that is guaranteed to be entailed by the ground-truth fact. 

% \paragraph{Conformal Prediction for LLM Evaluation:} As a powerful tool to quantify the uncertainty, CP method can also assist to evaluating LLMs. For example, \cite{ye2024benchmarking} applies CP to integrate uncertainty quantification into LLM benchmarking. They choose five representative natural language processing tasks and transform them into multiple choices questions, and then evaluate the uncertainty of LLMs by their average confidence set size returned by CP. Specifically, it is found that models with higher accuracy may exhibit lower uncertainty, larger LLMs tends to have higher uncertainty, and instruction-finetuning tends to increase the uncertainty. In addition to evaluating the multiple choice questions, \cite{giovannotti2023evaluating,zerva2023conformalizing} also use conformal prediction to evaluate the quality of machine translation by providing quality assessment from either human or LLMs. Moreover, using the quantified uncertainty, \cite{schuster2021consistent, schuster2022confident} proposed confident adaptive models that evaluate the uncertainty of intermediate layers in a Transformer model and exit early as long as it has enough uncertainty exceeding a give threshold. It was shown that their method can speed up model inference with guaranteed consistency with the full model. 

\subsection{LLM Watermarking} 
The capability of LLMs to generate human-like text has raised significant concerns regarding potential misuse. For instance, StackOverflow, a leading question-and-answer platform for programmers, implemented a temporary ban on AI-generated responses to prevent the dissemination of misleading information \cite{vincent2022ai}. 
Similar concerns about misinformation proliferation have emerged across various domains, including fake news \citep{zellers2019defending}, academic integrity \citep{milano2023large}, and data authenticity \citep{shumailov2023curse}. Consequently, robust techniques for attributing text to LLMs have become essential for enabling individuals and platforms to detect and enforce policies regarding LLM usage.

A direct approach to detecting LLM usage in text is to train detectors based on linguistic features that distinguish human-written text from LLM-generated text. Some studies trained these detectors in an end-to-end manner on collected data \citep{gptzero2023,zerogpt2023,bhattacharjee2023fighting,koike2023outfox}, whereas others exploited structural properties of LLMs for detection \citep{ippolito2019automatic,gehrmann2019gltr,mitchell2023detectgpt} or relied on inherent stylistic distinctions without training \citep{yang2023dna,tulchinskii2024intrinsic}. However, these ad-hoc methods have shown degraded performance as LLMs become increasingly capable of generating human-like text \citep{weber2023testing}. It remains true that contemporary LLM-generated text still exhibits distinguishable features compared to human-written text \citep{park2025does}; furthermore, these methods often exhibit vulnerability to adversarial attacks and can show bias against non-native English writers \citep{krishna2024paraphrasing,sadasivan2023can,liang2023gpt}, but this is an area that is evolving.

A more proactive and controllable approach involves subtly modifying the text generation processes of LLMs to enable provable detection, known as watermarking, by embedding subtle yet detectable statistical signals into the generated text \cite{kirchenbauer2023watermark}. This technique leverages the probabilistic nature of LLMs, allowing the model provider to utilize pseudorandomness in the generation process.

\paragraph{Statistical Formulation} 
The statistical framework for watermarking in text generation can be formulated as follows \citep{li2024statistical}: let the next-token prediction distribution at the $t$-th token be $P_t$, which represents a multinomial distribution. The watermarked LLM first generates a pseudorandom variable $\zeta_t$ using a hash function
\[
\zeta_t = \mathcal{A}(w_{1:(t-1)}, \texttt{Key}),
\]
where $w_{1:(t-1)}$ denotes the first $t-1$ tokens, and \texttt{Key} represents a private key. It then samples the next token $w_t = \mathcal{S}(P_t, \zeta_t)$ through a decoder $\mathcal{S}$. The decoder can be deterministic or incorporate randomness that is independent of $\zeta_t$. This design enables the computation of the pseudorandom number from the observed text and the private key in both the generation and detection phases (transmitted via a trusted protocol). Formally, the tuple $(\mathcal{A}, \mathcal{S}, \texttt{Key})$ constitutes a watermark.

Detectability of the watermark signal is enabled by the dependence between the token $w_t$ and pseudorandom variable $\zeta_t$. \cite{li2024statistical} proposed constructing a pivot statistic $Y_t = Y(w_t, \zeta_t)$ for $t = 1, \ldots, n$ that follows a fixed distribution for human-written text, which formulates the problem of distinguishing between human-written text (null) and LLM-generated text (alternative) as hypothesis testing:
\begin{equation}\label{eqn: watermark}
H_0 (\text{human}): Y_t \sim \mu_0 \text{ i.i.d.\ for all } t 
\quad \text{vs.} ~~
H_1 (\text{LLM}): Y_t \sim \mu_{1, P_t} \text{ for all } t.
\end{equation}
One can then either reject or accept the null hypothesis via
\begin{equation}\label{eq:det_r}
T_h(Y_{1:n}) = 
\begin{cases}
    1 & \text{if } \sum_{t=1}^n h(Y_t) \geq \gamma_{n,\alpha},\\
    0 & \text{if } \sum_{t=1}^n h(Y_t) < \gamma_{n,\alpha},
\end{cases}
\end{equation}
where $\gamma_{n,\alpha}$ is a threshold determined by the sample size $n$ and the significance level $\alpha$.

To elucidate the rationale behind the detection rule \eqref{eqn: watermark}, observe that under $H_0$, human text composition follows complex cognitive processes that are statistically independent of pseudorandom variables $\zeta_{1:n}$. By construction, $Y_t = Y(w_t, \zeta_t)$ follows a known distribution $\mu_0$ provided that $w_t$ is independent of $\zeta_t$. Hence, $\sum_{t=1}^n h(Y_t)$ follows a known null distribution so that an appropriate threshold can be calculated to control the type I error. Under $H_1$, $Y_t$ follows $\mu_{1, P_t}$, which depends on the unknown and varying token distribution $P_t$. An effective choice of the score function $h$ would increase this sum under the alternative hypothesis. Indeed, \cite{li2024statistical} established a general framework to determine the optimal score function through a minimax optimization problem.

\paragraph{Unbiased Watermarks}
The Gumbel-max watermark \cite{aaronson2023watermarking}, implemented internally at OpenAI, exemplifies watermarks that can be analyzed within this statistical framework. It relies on the Gumbel-max trick: let $\mathcal{W}$ be the token vocabulary, and let $\zeta = (U_w)_{w\in \mathcal{W}}$ consist of $|\mathcal{W}|$ i.i.d.\ copies of $U[0,1]$. Then, the Gumbel-max trick states that\footnote{In an abuse of notation, in this section $P_w$ denotes the coordinate of the probability vector $P$ corresponding to token $w$, while generally we write $P_t$ to refer to the entire vector at time $t$.} 
$\arg\max_{w\in\mathcal{W}} {(\log U_w)}/{P_w}$ 
follows the multinomial distribution $P$. \cite{aaronson2023watermarking} proposed using the decoder
\[
\mathcal{S}(P, \zeta) = \arg\max_{w\in\mathcal{W}} \frac{\log U_w}{P_w}
\]
for selecting the next token. Several detection rules of the form \eqref{eq:det_r} have been proposed \cite{kuditipudi2023robust,fernandez2023three,li2024statistical}. Notably, \cite{li2024statistical} developed an optimal sum-based test under certain conditions, achieving the fastest exponential rate of decrease in type II errors. More recently, \cite{li2024optimal} introduced a robust detection rule based on a truncated family of goodness-of-fit tests.

Because the Gumbel-max trick samples exactly from the underlying multinomial distribution, the resulting watermark is \emph{unbiased}. Formally, a watermark is unbiased if, for any token $w$ and token distribution $P$, one has $
\mathbb{P}_{\zeta}\bigl(\mathcal{S}(P, \zeta) = w\bigr) = P_w.$ 
Unbiased watermarking schemes are generally preferred, as they preserve the LLM's token distribution, thereby maintaining text quality. Another unbiased example is the inverse transform watermark \cite{kuditipudi2023robust}, which corresponds to inverse transform sampling of the multinomial distribution. Its optimal detection rule was also derived in \cite{li2024statistical,cai2025statistical}.

\paragraph{Biased Watermarks}
Beyond the unbiased regime, the green–red list watermark \cite{kirchenbauer2023watermark} is a popular instance of a biased watermark, due to its simplicity and intuitive design. In this watermark, the token vocabulary is randomly partitioned into a ``red list'' and a ``green list,'' where the probabilities of green tokens are increased. One parameter in this watermarking scheme controls the magnitude of the distortion in the next-token distribution, while another determines the size of the green list. During detection, if the proportion of green tokens in a text exceeds a specified threshold, the text is classified as LLM-generated. Several studies have refined this approach \cite{huo2024token,wouters2024optimizing,cai2024towards} by proposing methods to optimally select these parameters, thereby balancing watermark detectability with text quality. Meanwhile, \cite{xie2024debiasing,hu2023unbiased,wu2023dipmark} introduced unbiased variants of the green--red list watermark by applying techniques such as maximal coupling. Furthermore, an optimal detection rule is established in \cite{cai2025statistical}, addressing both the minimization of type I and type II errors and the minimization of type II error for a fixed type I error rate. 

\paragraph{Other Watermarking Schemes}
A variety of other watermarking schemes have been proposed, and the list is growing.  Many have been evaluated only empirically, and rigorous statistical analysis remains to be developed in these cases. Some of the more statistically inspired approaches include that of \cite{dathathri2024scalable}, who introduced a production-ready watermarking system at Google DeepMind based on tournament sampling for multinomial distributions. \cite{christ2023undetectable,zhao2023protecting} employed a secret, hash-based mechanism to subtly modulate token-selection probabilities, ensuring that the watermark remains invisible without the key but verifiable with it. \cite{fairoze2023publicly} presented a watermark that is highly detectable and embeds a publicly verifiable cryptographic signature into the LLM output using rejection sampling. \cite{he2024watermarking} developed a watermarking scheme for tabular data following the principles of the green-red list watermark. \cite{xian2024raw} injected watermarking signals into both the frequency and pixel domains of images after generation and employs a classifier to detect the watermark's presence. \cite{zhao2024permute} investigated decoders that transform logit vectors into token probabilities and proposed a provably robust watermark. \cite{he2024universally} characterized optimal watermarking schemes by minimizing a lower bound on the worst-case type II error while ensuring the worst-case type I error and watermarked-text distortion remain below specified constants. \cite{giboulot2024watermax} introduced an approach that first selects a sound watermark detector producing $p$-values, then generates multiple candidate texts from an LLM prompt, and finally outputs the text with the lowest $p$-value. Although this strategy enhances robustness, it increases computational overhead due to multiple text generations.

\paragraph{Future Research} 
Looking ahead, several challenges persist in developing comprehensive statistical foundations for watermarking in complex usage scenarios. In practice, watermarked text can be compromised through paraphrasing or content alteration that removes or obscures watermark signals \cite{kirchenbauer2024reliability,tufts2024examination,zhang2024remark}. Notably, text often comprises a mixture of human-written and LLM-generated content, necessitating further research on unknown and complex source compositions \cite{li2024optimal}. One direct solution involves developing algorithms for localizing watermarked segments \cite{zhao2024efficiently,li2024segmenting}. Another unresolved challenge concerns pseudorandomness collision, where repeated pseudorandom numbers occur in the text sequence \cite{kuditipudi2023robust}. \cite{wu2024distortion} demonstrated that collision introduces bias into distortion-free watermarks and proved the impossibility of perfect distortion-free watermarks under collisions. Certain watermarking schemes are also vulnerable to ``watermark stealing,'' where attackers can reverse-engineer and remove or spoof the watermark \citep{jovanovic2024watermark}. A crucial challenge in implementing watermarks for LLMs lies in ensuring robustness against adaptive prompting and supporting multi-user tracing \cite{cohen2024watermarking,cohen2024enhancing}. Moreover, achieving low computational overhead in watermark detection remains critical \cite{huang2023towards}. Additionally, watermarking methods can be utilized to protect copyrighted training data by detecting data misappropriation \cite{cai2025statistical}. From a theoretical perspective, a statistical framework of watermarks necessitates assuming simple yet informative structures of next-token prediction distributions \cite{li2024statistical}. For empirical evaluation of watermarks, current metrics may be insufficient for assessing how biased watermarks affect generated content \cite{singh2023new}. Finally, when watermarking schemes are implemented in conjunction with acceleration techniques such as speculative sampling, \cite{hu2024inevitable} identified an inherent trade-off between watermark strength and sampling efficiency.


\subsection{Privacy and Copyright}
Over the past few years, language models have grown in model size and sample size at an unprecedented speed, making the preservation of user privacy increasingly challenging. Prior work has shown that LLMs are able to memorize a large portion of training data \cite{carlini2021extracting, carlini2022quantifying}, and adversaries can easily extract gigabytes of training data from LLMs. For example, \cite{nasr2023scalable} showed that ChatGPT reveals a person's personal contact information when prompted with ``repeat this word forever: `poem poem poem poem'," and the authors have recovered ten thousand examples from ChatGPT’s training dataset at a query cost of 200 US dollars. This poses significant risks to user privacy as personal information and confidential documents could be disclosed through interaction with the model. 

\paragraph{Data Sanitization} The simplest way to avoid releasing private information is to remove this information from the training data. This approach is often referred to as the ``data sanitization procedure'' \cite{oliveira2003protecting, amiri2007dare}. The data sanitization procedure can be formulated as a binary classification problem and can be efficiently implemented by modern machine learning methods \cite{dernoncourt2017identification,lison2021anonymisation,vakili2022downstream}. Recently, data sanitization has been widely used as a pre-processing step for removing personally identifiable information (PII) or protected health information (PHI) in many companies, such as Microsoft and PayPal \cite{williams2023systems,balzer2020obfuscating}. However, sanitization relies on a formal definition of private information, and for language data, this definition may depend on the context and have no well-defined boundary. Therefore, data sanitization works best for well-formatted private data, such as social security numbers and medical records, and has limited power for general privacy-preserving purposes \cite{brown2022does}. 

\paragraph{Differential Privacy} To preserve data privacy from the model side, a standard approach is to exploit the framework of differential privacy (DP) \cite{dwork2006differential}. DP ensures that adversaries cannot distinguish whether a specific data point is included in the training set of the model. The standard method
to provide DP guarantees in deep learning is to replace the standard optimizers with DP optimizers (e.g., DPSGD \cite{abadi2016deep,bu2020deep}), an approach that has been extensively used in LLM training \cite{hoory2021learning,anil2022large}, fine-tuning \cite{li2021large,yu2021differentially,huang2023privacy}, and prompt learning \cite{duan2024flocks}. However, as DP optimizers require clipping the gradient and injecting noise into the training procedure, it can hurt the model performance and require more computational resources for hyperparameter tuning \cite{li2021large}, making many of the existing methods impractical at the scale of current LLMs.  This is an area where significant new research is needed.






\paragraph{LLM Unlearning} After LLMs are trained, it would be desirable to eliminate the influence of specific data from the model while preserving the model's utilities on other tasks. This task is often referred to as ``machine unlearning'' \cite{cao2015towards}. While exact unlearning requires re-training the language model without the target data from scratch, it is possible to approximately achieve machine unlearning efficiently. \cite{neel2021descent, ginart2019making, guo2019certified, sekhari2021remember, georgiev2024attribute} introduced theoretical metrics for machine unlearning based on the notion of differential privacy and proposed unlearning methods based on Newton update removal mechanisms. However, these algorithms require computing the Hessian of loss functions, which is intractable for LLMs. 

Recent research has explored computationally efficient unlearning methods for LLMs. Gradient ascent is a commonly used technique that reverts the learning process by minimizing the next-token prediction likelihood on target data \cite{jang2022knowledge, yao2023large}. However, reverting the optimization process through gradient ascent can be unstable as it leads the model parameter to diverge. To mitigate the issue, \cite{zhang2024negative, fan2024simplicity} designed an alternative loss function named ``negative preference optimization'' (NPO). The NPO loss generalizes the gradient ascent objective via adaptive weighting of the unlearning samples and ensures that the loss function remains bounded, thereby achieving a slower divergent rate and enhanced stability compared to gradient ascent. Another variant of gradient ascent is to relabel the target data by randomly assigned labels and train the model to fit on the random labels \cite{yao2023large}. Beyond the gradient ascent-based method, there are several different approaches, such as localizing and fine-tuning the crucial model units (e.g., layers, weights, neurons) for the unlearning task \cite{meng2022locating, patilcan, yu2023unlearning, wu2023depn} and using influence functions \cite{jia2024soul}. However, most of the current unlearning methods require specifying a target task or content to be unlearned, and there is still a lack of standardized corpora for LLM unlearning \cite{liu2024rethinking}.

\paragraph{Copyright} Beyond user privacy concerns, the ability of LLMs to memorize and reproduce training data raises critical issues regarding copyright protection. Copyrighted material embedded in training datasets can appear in model outputs, either inadvertently or deliberately, potentially violating intellectual property rights \cite{samuelson2023generative}. While this issue can be partially addressed through data sanitization, differential privacy, and prompting techniques that mitigate the risk of disclosing copyrighted material \cite{elkin2023can,vyas2023provable,chiba2024tackling}, urgent research efforts are needed to ensure that training processes and model outputs remain unconnected to specific instances of copyrighted content. Such research is crucial for establishing a pathway toward building models that comply with copyright regulations and support responsible AI deployment. Taking a different angle, \cite{wang2024economic} has proposed an economic framework leveraging cooperative game theory principles to enable model developers to compensate copyright owners for using their data in training.

\paragraph{Data Misappropriation}
A related issue is data misappropriation, which refers to the unauthorized use, access, or exploitation of data for unintended or unpermitted purposes, often violating legal or ethical regulations. This concern has been at the center of several high-profile debates. For example, the lawsuit between The New York Times and OpenAI \cite{nyt2023-debate} highlights tensions surrounding the use of copyrighted data in training LLMs. Additionally, OpenAI’s Terms of Service explicitly prohibit the use of ChatGPT's outputs to develop competing models, underscoring the need for mechanisms to detect whether a newly trained LLM has incorporated ChatGPT-generated content—a process often referred to as model distillation.  Detecting such data misappropriation is challenging, as the probabilistic nature of LLMs generates content that may resemble, but does not directly copy, the original data \cite{sag2023-safety, gesmer2024-challenge}. This difficulty has spurred significant research into methods for identifying and tracing LLMs-generated data \citep{sadasivan2023can, mitchell2023detectgpt, ren2024copyright}. A statistical hypothesis testing framework is established in \cite{cai2025statistical}, and optimal detection of data misappropriation is established. 

These challenges and advances highlight the urgent need for robust frameworks that ensure ethical AI development, protect intellectual property, and maintain trust in AI systems. The interplay between statistical methods and practical detection mechanisms will continue to play a critical role in navigating the complex issues of unlearning, copyright, and data misappropriation in LLMs.

\paragraph{Future Research} While significant progress has been made in privacy protection, copyright enforcement, and unlearning, several key challenges remain. One major challenge is the evaluation of privacy-preserving techniques. Current methods, such as differential privacy and data sanitization, lack standardized evaluation metrics, making it difficult to quantify the trade-offs between privacy protection and model performance. Developing robust benchmarks to assess these trade-offs is essential for advancing practical privacy solutions in LLMs.
Another challenge lies in context-dependent privacy risks. The definition of private information is highly context-dependent, making it difficult to apply a one-size-fits-all privacy solution. While sanitization techniques work well for structured data, sensitive information in free-form text often requires more nuanced handling. Future research should explore adaptive privacy mechanisms that dynamically assess context before enforcing safeguards, ensuring more reliable protection across diverse applications.
Furthermore, continual unlearning remains an open problem. Most existing unlearning methods assume a static dataset, but in practice, LLMs are often deployed in environments where data continuously evolves. As new information is incorporated, there may be legal or ethical requirements to forget specific data, requiring efficient and scalable unlearning techniques. Developing frameworks that support continual or real-time unlearning will be critical for maintaining compliance with evolving data privacy regulations while preserving model utility.
Addressing these challenges will require collaboration between statisticians, machine learning researchers, legal experts, and policymakers. By integrating rigorous statistical methodologies with emerging AI advances, the community can work toward building more transparent, accountable, and privacy-preserving language models.

% Attribute-to-Delete:
% Machine Unlearning via Datamodel Matching \url{https://arxiv.org/pdf/2410.23232}
% Except for seeking statistically rigorous privacy for LLMs, another line of works studied how to defend the model from training data extraction attacks and mitigate the privacy risk. \wl{TBD} We refer interested readers to \cite{ishihara2023training,li2023privacy} for comprehensive surveys on this topic. \wl{mention RLHF}

% \wl{Estimate reward? DP}
% \wl{ICL attack?}

% federated learning?


% \cite{mattern2022differentially}
% Current practices

% Statistical insights...

% Despite the remarkable success of LLMs on various reasoning tasks such as solving mathematical problems and commonsense reasoning \cite{bubeck2023sparks,trinh2024solving}, it is often unclear whether the reasoning is generated by reciting the knowledge and patterns seen in the training data, or LLMs can learn the causal relationship embedded in the text \cite{zevcevic2023causal}. To ensure their reliability under potential domain shift, it is crucial to examine the ability of LLMs to perform causal reasoning. 
% \wl{Mechanism causal}
% Causal Inference is a fundamental framework to discover and identify causality between variables. 


% We refer interested readers to  \cite{gao2023chatgpt,kiciman2023causal,zhang2023understanding,liu2024large} for comprehensive surveys on the frontier of causal reasoning with language models. 


\subsection{Interpretability}


Interpretable machine learning is a broad concept that captures ``the extraction of the relevant knowledge from a machine learning model concerning relationships either contained in data or learned by the model" \citep{murdoch2019definitions}. As LLMs have been deployed in more and more real-world applications, their interpretability has received an increasing amount of attention as people wish to ensure their alignment with human values and understand their potential risks and failures. Simple machine learning models, such as linear regression or decision trees, are often considered interpretable since the dependency of model output on the model structure and training data is easy to characterize. Language models, however, contain billions of parameters and numerous layers, such that the precise dependency of output on data and model structure can be too complicated for humans to comprehend. Therefore, recent efforts have been focusing on \textbf{mechanistic interpretability}, which aims to explain the LLMs on an algorithmic level through reverse engineering the detailed computation performed by the LLMs. As proposed in \cite{olah2020zoom}, the current mechanistic interpretability research consists of three areas: features, circuits, and their universality.

\paragraph{Features} Unlike classical tabular data, where each of the coordinates represents a concrete variable, textual input is highly structured and it is unclear how the LLMs extract meaningful features from the data. Recent work has found that the features are learned and encoded by groups of neurons in LLMs; for example, \cite{gurnee2023finding} showed that some neurons in LLMs are activated for names of sports. Similarly, neurons that encode various features have been discovered, including sentiment neurons \cite{radford2017learning}, knowledge neurons \cite{dai2022knowledge}, and skill neurons \cite{wang2022finding}. Moreover, it was found that the LLMs can encode multiple features in a single neuron \cite{elhage2022softmax, elhage2022toy, gurnee2023finding}; i.e., the neuron can be activated by different concepts. This leads to the hypothesis of superposition, which implies a model can represent a greater quantity of features compared to the number of neurons. To extract superposition features, \cite{sharkey2022taking,bricken2023towards, cunningham2023sparse} train sparse autoencoders to map the neuron activations in LLM to a higher-dimensional representation with sparsity. By jointly minimizing the reconstruction loss with the L1 penalty on the high-dimensional representation, the researchers have successfully extracted features that are more interpretable than the original neuron activations. 



\paragraph{Circuits} Instead of identifying individual features, a global approach to mechanistic interpretability is to identify the ``circuits'' in LLMs \cite{olah2020zoom,elhage2021mathematical}. This approach is motivated by the circuit hypothesis that views LLMs as a computation graph that implements their capability through the composition of several subnetworks within the model. Formally speaking, a language model is represented as a directed acyclic graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ denotes the set of nodes (the MLP layers or attention layers) and $\mathcal{E}$ denotes the set of edges (the connection between those layers). For a specific task, a circuit is defined as a sub-graph in LLMs that satisfies the following three criteria \cite{wang2022interpretability, shi2024hypothesis}:
\begin{itemize}
    \item \textbf{Faithfulness}: The circuit can perform the task as well as the whole model. 
    \item \textbf{Completeness}: The circuit contains all the nodes used to perform the task. 
    \item \textbf{Minimality} The circuit doesn't contain nodes irrelevant to the task. 
\end{itemize}
Using this approach, researchers at Anthropic have identified the ``induction head" that is crucial for LLMs to execute in-context learning \cite{olsson2022context}. An induction head consists of two attention heads that work together to copy a previous pattern, for example, if [A][B] appears in the sequence earlier, the induction head will make the Transformer more likely to predict [B] when the model receives input [A] again. Therefore, the induction head can capture the information in the prompt and provide the primary source of in-context learning ability. Similarly, existing research has identified the corresponding circuits in LLMs for various tasks, including indirect object identification \cite{wang2022interpretability}, doc string completion \cite{heimersheim2023circuit}, and acronym prediction \cite{garcia2024does}. Furthermore, to make the circuit identification process automatic for general tasks, \cite{conmy2023towards} proposed an algorithm that greedily removes the edge on the computational graph in a topological order. 

The evaluation of a circuit often involves an ablation study that knocks out the circuit, imputes the computation by either zero or the mean value, and then compares the performance of the remaining model to the original model. To make the comparison more precise, \cite{li2024optimalablation} proposed an optimal ablation procedure to optimize the performance of the remaining model on various imputation methods. Furthermore, to formalize the circuit evaluation process statistically, \cite{shi2024hypothesis} proposed a nonparametric hypothesis testing procedure to test the three criteria for valid circuits above.  

\paragraph{Universality} Although mechanistic interpretability has provided a systematic approach to investigating how an LLM implements a certain task, a critical question is that the features and circuits are specific to a given LLM model (usually a toy model or smaller model such as GPT-2), and it is unclear whether these findings are universal across all models. Investigation into universality has yielded a mixed result: \cite{olsson2022context, wang2022interpretability} has discovered similar circuits in multiple LMs, while \cite{chughtai2023toy} also found evidence that LMs trained from different initialization may implement different circuits. Therefore, understanding the degrees of universality of mechanistic interpretability remains a crucial open problem. We refer interested readers to \cite{rai2024practical} for a more practical review of mechanistic interpretability in LLMs. 

\paragraph{Physics of LLMs} Except for the general mechanistic interpretability approach described above, another seminal series of works aimed at interpreting the LLM through the ``physics'' perspective and discovering the universal law of all LLMs \citep{AllenZhu-icml2024-tutorial}. Toward this goal, it divides the intelligence of LLM into multiple different dimensions such as structure, reasoning, and knowledge, and then studies each individual dimension through idealized control experiments. In each experiment, the authors manage the data and tweak hyperparameters, such as data quantity, type, difficulty, and format, to  determine the impact of each factor on LLM performance and suggest further improvement. Using this approach, extensive experiments and observations have been made on learning hierarchical language structures \cite{allen2023physics1}, graduate-level mathematical reasoning \cite{ye2024physics21, ye2024physics22}, knowledge extraction \cite{allen2023physics31}, manipulation \cite{allen2023physics32}, and capacity
\cite{allen2024physics33}. 

\paragraph{Geometric Laws} Recent empirical studies have revealed several geometric laws that emerge in deep learning models after training. This line of research was pioneered by \cite{papyan2020prevalence}, which documented a pervasive phenomenon called ``neural collapse'' in multilayer neural networks. Neural collapse refers to a geometric law in which the last-layer features and weights exhibit symmetric structures that favor large margins, an inductive bias with significant implications for interpretability due to its precise geometry \cite{fang2021exploring, ji2021unconstrained, thrampoulidis2022imbalance}. This law has since been extended to intermediate layers and LLMs \cite{he2023law, wu2024linguistic, he2024law}. In the context of LLMs, \cite{he2024law} offers a new perspective on feature formation by showing that pre-trained LLMs enhance the predictability of contextualized features evenly across all layers, from the first to the last.


\paragraph{Future Research.} While recent advances have improved LLM interpretability, several critical challenges remain. A major challenge is developing systematic methods for editing internal representations to induce desired behaviors, such as reducing hallucinations or improving factuality, without retraining. Instead of relying solely on post-analysis approaches, future work should explore ways to train AI models with interpretability as a built-in objective, potentially enabling more transparent and predictable model behavior from the outset. Additionally, enhancing safety through interpretability is essential for mitigating vulnerabilities like adversarial manipulation or harmful content generation. Collaborative efforts between machine learning researchers and statisticians will be key to building more interpretable and reliable LLMs that support safer and more accountable AI systems. 







\subsection{Algorithmic Fairness}

Modern LLMs are trained with massive quantities of text collected from the real world, and biases towards  certain groups or individuals are inherited from the training data. As a consequence, the usage of LLMs may lead to discrimination against certain gender \cite{kotek2023gender}, race \cite{an2024measuring}, religion \cite{abid2021persistent}, and many other sensitive characteristics \cite{navigli2023biases} in downstream applications. For example, when asked to write reference letters for Kelly (a common female name) and Joseph (a common male name), it was observed \cite{wan2023kelly} that ChatGPT tends to describe Kelly as a warm and likable person and Joseph as a leader and role model, indicating that LLMs tends to follow the gender stereotypes that males are associated with leadership.


\paragraph{Statistical Formulation} Before applying any mitigation techniques, it is essential to establish a statistical formulation for algorithmic fairness in LLMs. Fairness assessments typically consider paired groups, denoted as \( A \) and \( B \), such as sentences including male and female words, or young and old groups. The primary objective is to evaluate disparities between these groups.

Fairness in LLMs can be formulated using two main approaches: {outcome-based metrics} and {probability-based metrics}. Outcome-based metrics treat the LLM as an opaque system, assessing fairness based on the scores (or labels) directly associated with the model’s outcome response. Following \citep{liu2019does}, a dialogue model \({D}\) can be represented as a function \({D} : C \mapsto R\) that maps a context \(C\) to a response \(R\). Given a measurement function \({M}\) that assigns a scalar score \(s\) to each response \(R\), the model \({D}\) is considered \textit{fair} for groups \(A\) and \(B\) with respect to \({M}\) if:
\[
\mathbb{E}_{C \sim T_A} {M}({D}(C)) = \mathbb{E}_{C \sim T_B} {M}({D}(C)),
\]
where $T_A$ and $T_B$ denote the distribution of the context $C$ related to groups $A$ and $B$ respectively. 

In contrast, {probability-based metrics} analyze the behavior of the language model by examining the probabilities it assigns to outputs. For example, the probability of generating profession-related words like “engineer” or “doctor” should be similar for male names like “Joseph” and female names like “Kelly”.  Likewise, prompts related to African Americans should yield a comparable rate of toxic adjectives as those related to European Americans.

To formalize fairness, let \( U \) represent a specific set of sensitive words (e.g., high-paying professions, toxic adjectives), \( \mathcal{U} \) denote the collection of all such sets, and \( \mathcal{G} = \{(A^i, B^i) \mid i = 1,2, \dots I \} \) define the paired groups of interest. A fairness criterion, following \cite{zhang2024fair}, can be expressed as:
\begin{equation}\label{eq:fairness}
    \left| \Prob(o(\bm x) \in U \mid \bm x \in A) - \Prob(o(\bm x) \in U \mid \bm x \in B) \right| \leq \alpha, \quad \forall U \in \mathcal{U}, (A, B) \in \mathcal{G},
\end{equation}
where the probability is taken over \( o(\bm x) \sim f(\bm x) \), with the next-token output \( o(\bm x) \) drawn from the language model \( f(\bm x) \).

To address these fairness issues, many bias mitigation techniques have been studied in different stages of the LLM workflow, including data pre-processing, model training (in-processing), and post-processing \cite{gallegos2023bias}. 


\paragraph{Pre-Processing}
In the pre-processing stage, the general methodology is to break the imbalance between different groups in the training data. Under this category, typical approaches include data augmentation via alternating group-imbalanced words \cite{lu2020gender}, subset selection \cite{garimella2022demographic}, instance reweighting \cite{han2021balancing}. Collectively, these pre-processing strategies align the data with fairness objectives, enabling LLMs to learn representations that are less prone to perpetuating societal biases.


% In the pre-processing stage, the general methodology is to break the imbalance between different groups in the training data. Under this category, typical approaches include data augmentation via alternating group-imbalanced words \cite{lu2020gender}, subset selection \cite{garimella2022demographic}, instance reweighting \cite{han2021balancing}. For instance, when prompted to predict the profession of a male name like ``John," an LLM might disproportionately output professions such as ``engineer" or ``doctor," while for a female name like ``Mary," it might suggest ``nurse" or ``teacher." Such biased outputs reflect and reinforce stereotypical associations found in training data, raising concerns about fairness and equity. Following \cite{zhang2024fair}, a fairness notion can be formulated  by \begin{equation}\label{eq:fairness}
%     |\Prob(o(\bm x)\in U|\bm x \in F)-\Prob(o(\bm x)\in U|\bm x \in M)|\le\alpha,
%     \end{equation}
%     where the probability is taken over $o(\bm x)\sim  f(\bm x)$ with the next-token output $o(\bm x)$ drawn from the language model $f(\bm x)$,  $U$ is the set of profession words, and $\bm x \in F$ and $\bm x \in M$ denotes that $\bm x$ indicates female and male names respectively. 
%  %{\color{cyan}Linjun will add some concrete examples of fairness in LLMs}

\paragraph{In-Processing}
In the model training stage, bias can be mitigated by leveraging some fairness metrics into the training objective. A common approach is to add a fairness metric into the loss function as a regularizer.  In particular, a family of distance-based metrics is used to shorten the distance of different groups in the embedding space, such as $\ell_2$ distance between sentence embeddings \citep{liu2019does}, cosine similarity \cite{huang2019reducing} and Jensen-Shannon divergence \cite{yang2023adept,woo2023compensatory}. 
% For example, \cite{liu2019does} used the L2 distance between embeddings of a sentence and its counterfactual analogy,
% $$
% L_{\text{reg}} = \sum_{w_i, w_i' \in \mathcal{W}}\|e_{w_i}- e_{w_i'}\|_2,
% $$
% where $\mathcal{W}$ is the gender or race word list, and $e_{w_i}$ are the corresponding word embedding. As the training process goes on, all of the gender and race words will become closer in the embedding space and the model will gradually treat them as equal to remove the bias. 
%Similar metrics such as cosine similarity \cite{huang2019reducing} and Jensen-Shannon divergence \cite{yang2023adept,woo2023compensatory} are also considered in the literature. 
Besides distance-based metrics, another line of work attempts to design metrics that can disentangle the relationship between embeddings and the group identity. Towards this goal, \cite{bordia2019identifying,kaneko2021debiasing,park2023never} proposes to encourage the orthogonality via minimizing the projection length of neutral word (e.g., leadership) embeddings onto gender embeddings, and \cite{colombo2021novel, wang2023toward} encourage independence between learned word embeddings and gender via minimizing the mutual information. 

In addition to adding fairness metrics as regularization terms, one may also change the training objective to mitigate bias. Along this line,  \cite{xiao2024algorithmic} identifies an inherent algorithmic bias emerging from RLHF in the post-training phase of LLMs, which significantly amplifies majority preferences while diminishing minority preferences. To resolve this fairness concern, \cite{xiao2024algorithmic} introduces preference matching RLHF, which incorporates an additional regularization term in the reward objective. \cite{cheng2021fairfil, he2022mabel, oh2022learning} applies contrastive learning with counterfactual data augmentation to maximize the similarity between the original sentences and their counterfactual analogs. \cite{zhang2018mitigating, jin2020transferability,han2021diverse} use an adversarial training framework, where they train an attacker to predict the protected attribute from the embeddings of the encoder, and an encoder to prevent the attacker from identifying the protected attribute. \cite{ouyang2022training, bai2022constitutional} utilize reinforcement learning with feedback from human or machine learning models to encourage the model generate fair texts. \cite{nakada2024synthetic} establishes a theoretical framework of leveraging  LLMs to artificially increase the sample size of underrepresented classes in imbalanced classification and spurious correlation problems.%{\cyan Linjun will add \url{https://arxiv.org/abs/2406.03628}}

\paragraph{Post-Processing}
After the model is trained, one may enforce fairness by modifying the generating distribution. A direct approach is to constrain the distribution to generate texts with only unbiased words and phrases, for example, \cite{gehman2020realtoxicityprompts,xu2020recipes} forbid the use of toxic words according to a list, \cite{saunders2021first} search for outputs in the distribution with different genders to the highest likelihood output, \cite{shuster2022blenderbot,schramowski2022large} use another ML model to identify the harmful words and replace them with neutral words. An alternative approach is to edit the next-token probability. 
For example \cite{gehman2020realtoxicityprompts, kim2022critic,liu2023bolt} reweight the token probability in the generatve distribution according to a fairness score learned from an evaluation model, and \cite{liu2021dexperts, hallinan2022detoxifying} separately train smaller expert and anti-expert models specialized in generating non-toxic and toxic texts, and then in the inference stage, tokens with higher probability in the expert model are up-weighted and tokens with lower probability in the anti-expert model are down-weighted.
There are also recent studies that apply multi-group fairness notions in LLMs. \cite{zhang2024fair} introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees, and applies this framework to achieve the notion \eqref{eq:fairness}.

\paragraph{Future Research}
While significant progress has been made in mitigating algorithmic biases in LLMs, several key challenges and open problems remain. One fundamental issue is the trade-off between fairness and utility. Bias mitigation techniques often come at the cost of reduced model performance, particularly in nuanced real-world applications. Future research should explore principled ways to balance fairness constraints with overall model accuracy, potentially by developing adaptive or task-specific fairness constraints that optimize both fairness and utility.
Another critical challenge is context-dependent fairness. Existing fairness metrics and debiasing strategies typically rely on predefined sensitive attributes such as gender or race. However, fairness concerns can be highly context-dependent, varying across applications, languages, and cultural perspectives. A promising direction is to develop dynamic, context-aware fairness measures that adapt to different scenarios, ensuring that bias mitigation strategies remain relevant across diverse settings.
Furthermore, fairness in multi-modal and interactive AI systems remains an understudied area. Many real-world applications involve interactions between text, images, and structured data, and biases may manifest differently across modalities. Research should focus on extending fairness-aware learning techniques to multi-modal LLMs and conversational AI systems to mitigate biases in more complex settings.
Additionally, the long-term impact of fairness interventions requires further investigation. Most current evaluations assess bias mitigation based on short-term performance metrics, but biases may re-emerge as models are fine-tuned, updated, or interact with users over time. Developing robust longitudinal studies to assess the durability of fairness interventions will be crucial for ensuring sustainable bias mitigation.
Finally, scalability and computational efficiency of fairness interventions remain practical concerns. Many existing approaches, particularly in-processing techniques that modify training objectives, introduce significant computational overhead, making them challenging to implement for large-scale LLMs. Future work should explore lightweight debiasing techniques that maintain fairness guarantees while remaining computationally feasible.
%Addressing these challenges requires a multidisciplinary effort, bringing together statisticians, machine learning researchers, ethicists, and policymakers to design fair, transparent, and accountable AI systems. By integrating rigorous statistical frameworks with advances in LLMs, the community can ensure that fairness remains a central consideration in the development of next-generation AI models.
% arxiv.org/abs/2403.12025



% Current practices

% Statistical insights...


\subsection{LLM Alignment: A Statistical Perspective}



\paragraph{Alignment with RLHF and Preference Optimization} Alignment is a crucial step in the language model training pipeline, as it aims to ensure that the model generations align with human preferences. Existing alignment procedures are mostly implemented via reinforcement learning from human feedback (RLHF), which attempts to learn a reward function based on the pairwise human preference data and learn the corresponding optimal policy. Denote $\mathcal{S}$ the space of states, $\mathcal{A}$ the space of actions, and $r(s,a)$ the reward function of taking action $a\in \mathcal{A}$ at state $s\in\mathcal{S}$, the RL process seeks  a policy $\pi(s)$ that maximizes the reward $\E_{s}[\sum_{h=1}^H r(s_h, \pi(s_h)]$ from step 1 to $H$. In the context of language model alignment, the state $s\in \mathcal{S}$ and action $a\in \mathcal{A}$ correspond to the input prompts and model responses, and the reward function represents the human satisfaction of the model response given the prompt. Under this framework, \cite{zhu2023principled} used the Bradley-Terry-Luce model to generate offline human preference data and showed that a variant of MLE can achieve near-optimal sample complexity under a linear reward function, a result that was later generalized to general reward functions \cite{zhanprovable}, partially observed reward \cite{kausik2024theoretical}, and multi-party reward \cite{zhong2024provable} settings. Compared to the standard RL, RLHF uses preference data that contains less information compared to the reward, therefore it is often considered a harder task. However, \cite{wang2023rlhf} showed that for a wide range of preference models, algorithms from traditional reward-based RL can solve the RLHF task with low extra complexity. 

\paragraph{Self-alignment} On the theoretical front, \cite{gerstgrasser2024modelcollapseinevitablebreaking,dey2024universality} analyzed the phenomenon of model collapse on a range of statistical models, including linear regression, generalized linear models, and nonparametric regression, and found that the estimation error grows linearly in the setting where we replace the original data with the synthetic data in each iteration, and  converges to a certain constant when one keeps augmenting the original data with synthetic data. Recent work~\cite{dohmatob2024strong} shows that while increasing model size can mitigate model degradation when training with synthetic data, mixing synthetic data with real human data cannot fully eliminate the model collapse issue.


%\wz{using llm for topic modeling..}

%\jx{pipeline: question framing, data cleaning, data exploration, model fitting, model selection/evaluation, analysis interpretation, presentation. }

\iffalse
\subsection{Question Framing}
LLM-as-agent
\fi

\section{LLM-Empowered  Statistical Analysis}\label{sec5:app}

\subsection{Data Collection}
Recent developments have yielded dramatic improvements on a variety of specific NLP tasks, including summarization, reasoning and extraction tasks. Such abilities enable using powerful LLMs for zero-shot or few-shot text mining, and extracting structured data from unstructured texts. 

\subsubsection{Extract Structured Data from Unstructured text}
One prominent application of LLMs is sentiment analysis (SA), a long-standing text mining task which aims to extract human sentiments from contextual data. These extracted sentiments can then serve as inputs for downstream statistical models, such as predictive models for Twitter engagement, election outcomes \cite{alvi2023frontiers}, and other social phenomena.

%Sentiment analysis (SA) has been a long-standing text mining application utilizing LLMs for mining human sentiments from contextual data, which can be fed into down-stream tasks as input for statistical analysis, for example, for building predictive models on twitter engagement, presidential election results \cite{alvi2023frontiers}, etc.

Beyond sentiment analysis, LLMs have shown remarkable effectiveness in extracting structured data from complex, unstructured sources such as electronic health records and other medical datasets. For example, recent work \cite{tang2023does} demonstrated that fine-tuning on a large volume of synthetic Biomedical Named Entity Recognition (NER) and Biomedical Relation Extraction (RE) data generated by ChatGPT significantly improved performance. Specifically, for biomedical NER tasks (e.g., NCBI Disease, BC5CDR Disease, BC5CDR Chemical), the F1-score improved from 23.37\% to 63.99\%. Similarly, for RE tasks (e.g., GAD, EU-ADR), the F1-score increased from 75.86\% to 83.59\%. These results illustrate the potential of LLMs to transform data extraction workflows and improve statistical analysis pipelines in high-stakes domains like healthcare.


% Such effetiveness in text understanding can also be employed to extract structural data from unstructured texts such as electronic health records and digital medical data. Recent work \cite{tang2023does} for example show that finetuning on a large quantity of synthetic Biomedical Named Entity Recognition (Biomedical NER) and Biomedical Relation Extraction(RE) data generated by ChatGPT, give significant improvements in the performance of downstream tasks, improving the F1-score from
% 23.37\% to 63.99\% for biomedical NER tasks (NCBI Disease, BC5CDR Disease, BC5CDR Chemical),from 75.86\% to 83.59\% for the RE tasks (GAD, EU-ADR). 


\subsubsection{Synthetic Data Generation} 
 The rise of the research fields of super-human alignment as well as self-rewarding are due to the expensive costs of human-written instructions and annotations that are limited in quantity, diversity, creativity, and more efforts have shifted to synthetic data generation using powerful language models. Recent work \cite{wang-etal-2023-self-instruct} has shown the ability of language models to self-improve by prompting models to generate natural instructions at human level that are later used for boostrapping. With development of synthetic text generation tasks, adaptation to generating realistic statistical data has become more popular in recent years. 



\paragraph{Tabular Data}
The generation of realistic synthetic tabular data has received considerable attention in recent years. Tabular data collection is often expensive and fraught with challenges, including class imbalances, long-tailed label distributions \cite{cao2019learning}, privacy concerns that restrict data sharing \cite{gascon2016privacy}, and data impurities such as noise or missing values \cite{lin2020missing}. Synthetic data generation provides a practical solution to these challenges, enabling the development of robust statistical models while addressing privacy and class imbalance issues \cite{choi2017generating, borisov2022deep}.
For instance, \cite{borisov2022language} introduced a synthetic tabular data generation pipeline using an auto-regressive generative LLM. They showed that discriminative models trained on synthetic tabular data outperformed competitors trained on real data. Additionally, \cite{nakada2024synthetic} proposed leveraging synthetic tabular data to address imbalanced classification and spurious correlation challenges, demonstrating the potential of LLMs to improve statistical workflows even in complex and resource-constrained settings.

\paragraph{Financial Data}
Rational expectations remains the dominant model of beliefs in much of macroeconomics and finance and in recent years the use of surveys to tie beliefs to observable data has emerged as a prominent approach. \cite{bybee2023ghost} proposed an alternative method of generating beliefs using LLMs and  evaluate how well generated
expectations of the stock market match the expectations of the American Association of Individual Investors (AAII) survey and Duke CFO Survey. 
% \paragraph{Data Augmentation}
% Self-rewarding 

\subsection{Data Cleaning}
\paragraph{LLM as coder for tabular data cleaning}
LLMs can automatically generate code for cleaning, preprocessing, and transforming raw data, saving data scientists considerable time and effort. For example \cite{tu2023should} described how ChatGPT with Code Interpreter can assist statisticians with checking for missing/null values, removing duplicate rows if necessary, checking for inconsistent/invalid values, converting categorical columns to numerical representations. 

\paragraph{LLM for automatic feature engineering}
Feature engineering refers to the process of building suitable features from raw data. Given input data $\{(X, Y)\}$, feature engineering seeks a transformation $\phi(x)$ to maximize the prediction accuracy using $\{(\phi(X), Y))\}$. Although modern machine learning models are capable of learning a wide variety of complex relationships, feature engineering is still crucial when the data is limited and the prediction model is simple, and it often relies on domain knowledge. With the use of LLMs, it is possible to automatically construct features using the domain knowledge embedded in it. For example, \cite{hollmann2024large} prompted LLM with contextual information about the datasets, a sample of data, and several few-shot examples of useful feature engineering, and then asked LLM to create a new feature. \cite{han2024large} prompted LLM to generate explicit decision rules to solve a classification task and then convert those rules into binary features for fitting the prediction model. 


\paragraph{LLM as judge for scalable text data filtering}
It is standard practice to clean up text before feeding it into any kind of machine learning algorithm. Whether one is doing pre-training or fine-tuning, cleaning data before training helps ensure accuracy and improving text quality. To ensure Llama 3 is trained on data of high quality, a series of data-filtering pipelines are applied to pre-training dataset including using heuristic filters, NSFW filters, semantic deduplication approaches, and text classifiers to predict data quality.  Besides standard heuristics such as filtering emojis, html tags, xml parsing, lower casing and other text standardization, more recently, LLMs have been employed to assign scores or rankings to text data for filtering high-quality text data for training. One prominent approach is to further fine-tune LLMs as text classifiers. For example, Llama 2 is employed to generate the training data for the text-quality classifiers that are powering Llama 3. Other approaches such as BartScore, directly apply BART, an encoder-decoder pre-trained language model, to texts for evaluating its fluency, factuality, informativeness in an unsupervised fashion. Recent reward scoring methods such as DPO reward also use LLM-predicted logits on texts to judge which one is better among a pair of texts. More recently, with emergent abilities of LLMs such as Chain-of-Thoughts(CoT) \cite{wei2022chain} and In-Context Learning (ICL) \cite{NEURIPS2020_1457c0d6}, have been employed as a scalable and explainable way of approximating human judgments which are otherwise  expensive to obtain. \cite{DBLP:conf/nips/ZhengC00WZL0LXZ23} reveal that strong LLM judges such as GPT-4 can match the quality of human-level preference, achieving over 80\% agreement with human annotations on two challenging benchmarks consisted of open-ended questions, without finetuning on domain-specific tasks. \cite{yuan2024selfrewardinglanguagemodels} can employ LLM-as-judge prompting to score its own training data for filtering and such self-reward pipeline during iterative training can yield a model that outperforms many existing state-of-the-art (SOTA) models including Claude 2, Gemini Pro and GPT-4 0613 on prominent NLP leaderboards.

\subsection{LLMs for Data Analysis}
There is evidence that LLMs can perform fundamental mathematical reasoning on graduate-level mathematical tasks. However little prior work has evaluated the reasoning capabilities of LLMs performing statistical analysis, especially on real data.    
\cite{liu2024llms} reveals that  even the strongest models can struggle in data analysis. For example GPT-4  can only achieve 58\% accuracy on basic statistical reasoning tasks such as ``What is a 95\% confidence interval?" 

\paragraph{Tool Usage For Quantitative Analysis}
In practice people have shown that ChatGPT can be fed directly with tabular data to perform direct table analysis such as aggregation, averaging etc, and generate SQL statements capable of filtering, sorting, aggregation and summation logics, and execute SQL queries on given database it reads from the prompt. 


\paragraph{Statistical Analysis for Text Data}  Text data has long been a rich source for statistical analysis, with methods like topic modeling providing interpretable summaries of large corpora. Traditional models such as Latent Dirichlet Allocation (LDA) analyze word co-occurrence patterns to identify latent themes within a text corpus, offering a probabilistic representation of topics \cite{blei2003latent,ke2023recent,ke2024using,wu2023sparse}. These approaches have been widely used to uncover hidden structures in text data, serving as a foundation for many downstream applications.
With the advent of LLMs, more advanced methods have emerged for extracting structured data from unstructured text in a scalable manner, enabling further statistical analysis. A promising avenue for such investigations has been the use of ``word embeddings"—a family of techniques that conceive of meaning as emerging from the distribution of words
that surround a term in text.  By integrating text embeddings into statistical models, the model can leverage contextual information and enhance the regression analysis with a richer representation of the input text. In a nutshell, the
method takes contextural embeddings which have been pre-trained on large corpora such as BERT, Llama or GPT, then this requires only a simple linear transformation of the averaged embeddings to conduct further statistical modeling such as regression \cite{rodriguez2023embedding}. Such use of word embeddings for prediction has been heavily employed in classification tasks \cite{selva2021review} to improve  model prediction accuracy. Another useful feature of word embeddings is to capture the semantic relationship between words and texts.

\paragraph{Statistical Inference with LLM Annotation}
In many statistical analysis, such as social surveys,  human annotation is slow and expensive to obtain. Meanwhile, LLMs can simulate human annotations efficiently with very low cost, but these annotations can be potentially biased and fail to provide valid statistical analysis. Motivated by this dilemma, \cite{angelopoulos2023prediction, angelopoulos2023ppi++} proposed Prediction-Powered Inference (PPI), a general framework that allows researchers to utilize predictions from any black-box machine learning model and perform valid statistical inference, such as computing p-values and confidence intervals. Assume that the researcher collects human annotation on a small dataset $(X_i, Y_i)_{i=1}^{n}$ and 
observes a large unannotated dataset $(X_i)_{i=n+1}^{n+N}$ from the same distribution. In addition, a pre-trained model $f$ is available to provide predictions $\hat{Y}_i = f(X_i)$. The target is to estimate the parameter $\theta^* = \arg\min_{\theta\in \Theta}\E[\ell_{\theta}(X, Y)]$, such as sample mean or regression coefficient. The PPI estimator \cite{angelopoulos2023prediction, angelopoulos2023ppi++, ji2025predictionssurrogatesrevisitingsurrogate} can be written as:

\begin{equation}
    \begin{aligned}
\hat{\theta}^{\textit{PPI}} = \arg\min_{\theta} \frac{1}{n}\sum_{i=1}^{n}\ell_{\theta}(W_i, Y_i) -\frac{1}{n}\sum_{i=1}^{n}\ell_{\theta}(W_i, f(X_i)) + \frac{1}{N}\sum_{i=n+1}^{n+N}\ell_{\theta}(W_i, f(X_i)).   
\end{aligned}
\end{equation}
Compared to using human annotation only or naively using machine learning predictions as gold standard labels, the PPI estimator can effectively leverage the information from predictions while preserving statistical validity (e.g., coverage). It has been successfully applied to assist various computational social science studies with annotations generated by LLMs, such as detecting hate speech \cite{egami2024using} and studying the bias in media outlets \cite{gligoric2024can}.  In addition,
\cite{jain2024scaling} underscores the potential of scaling laws to guide the effective integration of LLMs-generated surrogate data, addressing a critical bottleneck in scenarios where collecting high-quality data is expensive or impractical. The discovery that surrogate data can significantly reduce test error, even when unrelated to the original dataset, raises questions about the limits of this approach and its implications for generalization. 

% \paragraph{Causal Reasoning}


% \subsection{LLMs for Data Visualization}

%\section{Integration of Statistical Methods in LLM Development}
%RLHF

\subsection{LLM-assisted Medical Research}

Today, we have access to more health data than ever, with the potential to revolutionize modern medicine through LLMs. There are many diseases that are difficult to diagnose and cause considerable amounts of damage to the body before discovery \cite{ramoni2017undiagnosed}. Many people in the world do not have access to quality medical information or healthcare \cite{worldbank2017healthcare}. LLMs can process and analyze large volumes of data quickly, providing healthcare professionals and patients with valuable insights and potential diagnoses. Patterns that may be unobservable or difficult to differentiate by humans may get picked up by an LLM, and they can provide consistent information that is up to date with the latest research and data \cite{naveed2023comprehensive}. This has the potential to improve diagnostic support and improve accessibility of medical question-answering. Synthetic medical data generated by LLMs can increase data accessibility and advance medical AI research. It is important to keep in mind, however, that there are considerable risks and considerations when it comes to the use of LLMs in medicine---bias, hallucinations, ethical violations, lack of accountability, and lack transparency, to name a few. Here, we review some important and recent advances in large language models for medicine. For a more comprehensive review, see Zhou et al., 2024 \cite{zhou2024survey}. 

\paragraph{Early Transformer-Based Clincial Language Models}

There are a variety of recent language models that are pre-trained on vast amounts of medical data and fine-tuned for specific tasks within medicine and healthcare. The most well-known early example of pre-training and fine-tuning a Transformer-based model on clinical data is Clinical-BERT \cite{alsentzer2019publicly}. Using the same encoder architecture as BERT, the authors train the model on clinical notes from ICU admissions. As a result, ClinicalBERT is able to uncover semantic relationships between medical concepts. ClinicalBERT is widely used in a variety of clinical AI workflows to produce embeddings for medical concepts in EHRs data ~\cite{yang2021leverage,saha2020understanding,biseda2020prediction,kang2023automatic}.

\paragraph{LLMs for Clinical Text Generation}

With the significant advances in generative AI models, medical and clinical large language models were developed for synthetic medical data generation, and fine-tuned for a variety of natural language processing tasks. It is difficult to access and share large-scale clinical text and clinical LLMs due to concerns in privacy and maintaining HIPAA compliance. Kweon et al., 2023 developed Asclepius, the first generative shareable clinical large language model \cite{Kweon2023ClinicalLLM}. There are versions based on the Llama2-7B and Llama2-13B architectures, and versions based on the Llama3-8B and Mistral-7B architectures. The models generate synthetic clinical notes based on anonymized case reports from PubMed Central. Peng et al., 2023 developed GatortronGPT, which uses 277 billion words of text from 126 clinical departments and 2 million patients at the University of Florida Health in conjunction with 195 billion words of diverse English text \cite{Peng2023GenerativeLLM}. The authors then generate 20 billion words of synthetic text to train synthetic NLP models that accomplish tasks such as clinical concept extraction, event relations, semantic similarity, natural language inference, and question answering. 

\paragraph{LLMs for Medical Question-Answering}

There are also a variety of large language models specifically constructed for medical question-answering, which can democratize medical knowledge, increase access to underserved populations, and potentially reduce physician burden by improving efficiency. \cite{singhal2023towards} developed Med-PALM2---trained using the PaLM2 architecture and targeting medical domain-specific fine-tuning \cite{singhal2023towards}. The model incorporates instruction fine-tuning applied to multiple question-answer datasets. The authors train a “unified” model optimized for performance across all datasets using dataset mixture ratios. The model also incorporates ensemble refinement, where the model produces multiple generations via temperature sampling, and is then conditioned on the generations from the previous step to produce a refined explanation and answer. This step is performed multiple times, and the final answer is then determined by a plurality vote. Han et al., 2023 developed MedAlpaca based on the Llama architecture and trained on various question-answer pairs from medical flash cards, StackExchange, and Wikidoc \cite{Han2023MedAlpaca}. Med-PALM2 is a closed source model, whereas MedAlpaca is open source. Xie et al., 2024 developed the Me-LLaMa family of open-source foundation large language models for medical application, which combines biomedical papers, clinical notes, and general domain data \cite{Xie2024MeLLaMA}. Me-LLaMa outperformed ChatGPT on many medical datasets. 

\paragraph{LLMs for the Identification of Social Determinants of Health}

There have been recent efforts in the use of LLMs to identify social determinants of health (SDoH). SDoH are the conditions in which people are born, grow, live, work, and age that significantly influence health outcomes. Some examples can include economic stability, education, social and community context, neighborhood, and food accessibility. Identifying SDoH could provide a more holistic view of a patient’s health, and help stratify patients for more targeted prevention and resources. Guevara et al., 2024 developed models to extract SDoH by applying existing LLMs (Flan T-5, GPT 3.5, GPT 4) and fine-tuning with LLM-generated synthetic SDoH data \cite{Guevara2024LLMsSDOH}. For fine-tuning, LLMs were prompted to generate new sentences for SDoH categories using annotation guidelines as reference. Sample synthetic sentences were taken as reference to generate more synthetic sentences. The models identified 93.8 percent of patients with adverse SDoH, compared to 2 percent ICD-10, thus showing great promise for the use of LLMs in identifying SDoH. 

\paragraph{Multimodal LLMs in Medicine}

Recently, in addition to textual input, large language models have been adapted to handle other types of data such as images, charts, screenshots, pdf, video, audio, etc. For example, for the incorporation of medical images in multimodal medical LLMs, vision Transformers (ViT) are often integrated to handle the image input. ViT produces vector representations of the image, which are concatenated with text representations to form a single sequence. LMMs such as Flamingo-CXR~\cite{moor2023med} and Med-PaLM~\cite{li2024llava} are comparable with radiologists in controlled settings for generating radiology reports \cite{huang2023generative}. Google’s Gemini models are a new generation of multimodal models with novel capabilities. Med-Gemini in particular incorporates multimodal capabilities and the ability to integrate the use of web search, resulting in state of the art performance on complex diagnostic challenges, image challenges, video question answering and text summarization \cite{Anil2023Gemini}. 



\paragraph{Challenges in LLM adoption in Healthcare}
While LLMs have been rapidly developed for research in healthcare and medicine, several daunting challenges arise that limit their use in many high-stakes applications. These challenges and issues include algorithmic biases, poor calibration, hallucinations, misinformation, and response arbitrariness \citep{santurkar2023whose,Simhi2024hallucination,gao2024spuq}. Due to the unique nature of medical data and the critical need for safety, these models require specialized prompting, fine-tuning, and alignment to ensure they are truly useful and reliable in real-world clinical settings. For example, LLMs demonstrate suboptimal clinical reasoning under uncertainty \cite{omiye2023large}. LLMs also have been known to exhibit significant bias \citep{santurkar2023whose,casper2023open,azar2024general,chakraborty2024maxmin}, which can lead to fairness issues, particularly harming underrepresented minority individuals when clinicians rely on LLMs for decision support \cite{Poulain2024,Ayoub2024}. Researchers have cautioned against adopting LLMs in healthcare until these challenges are mitigated \cite{Szolovits2024,Friedman2024,Tessler2024}.



\section{Discussion}\label{sec6:discussion}
\subsection{Statisticians' Small Language Models}
Statisticians, often operating with fewer computational resources compared to their computer science counterparts, bring a distinctive strength to the development of language models: the ability to design resource-efficient, theoretically grounded models that leverage statistical principles. While the computer science community has focused on scaling LLMs through sheer data size and computational power, statisticians excel in developing ``small language models" (SLMs) that achieve competitive performance in particular domains by emphasizing efficiency and structure over brute force.
Statisticians’ advantage also lies in their expertise in embedding domain knowledge into model design. Techniques such as sparsity assumptions, hierarchical modeling, and structured regularization allow SLMs to achieve more sample efficiency. For instance, sparsity-inducing priors can focus the model's capacity on relevant features, reducing the need for excessive data while improving interpretability. Similarly, hierarchical Bayesian approaches can incorporate multi-level dependencies in text data, offering a principled way to share information across contexts and reducing the effective sample size needed for training. These methods can outperform large models in scenarios with limited data, noisy observations, or constrained resources, such as personalized medicine, legal text analysis, or financial modeling.

SLMs also align with statisticians’ focus on interpretability. By prioritizing simpler architectures and explicit assumptions, SLMs are inherently more transparent, allowing researchers and practitioners to trace the model’s predictions back to specific features or interactions. This interpretability is especially critical in high-stakes domains where trust and accountability are paramount. In contrast, LLMs, while powerful, often function as black boxes, limiting their applicability in settings where explanation is necessary.
Moreover, statisticians are well-positioned to cooperate with computer scientists in tackling the computational challenges associated with modern AI. Techniques that bridge statistics and computer science, like low-rank approximation, variational inference, and penalized optimization, can be employed to reduce the computational footprint of training and inference. Applied to SLMs, these approaches make the models not only more accessible but also environmentally sustainable, addressing concerns about the carbon footprint of large-scale AI systems. By focusing on principled model design, statisticians can create models that require less computational power while maintaining robustness and adaptability.

Statisticians’ SLMs are not intended to replace LLMs but to complement them. While LLMs excel at broad, open-ended tasks, SLMs can specialize in domain-specific applications, delivering competitive performance with far fewer resources. By leaning into their strengths in theory-driven modeling, sample efficiency, and interpretability, statisticians can carve out a unique and impactful role in the rapidly evolving AI landscape. This approach not only democratizes access to advanced language modeling capabilities but also ensures that AI development is aligned with the values of efficiency transparency, and sustainability.

Statisticians also have an important role to play in the design of ``wrappers'' that surround  black-box LLMs---performing roles such as calibration, uncertainty quantification, and debiasing on the LLM outputs.  The Prediction-Powered Inference (PPI) estimator referred to earlier is an instance of such a wrapper.  Causal inference methods can profit from black-box LLMs if properly deployed around the LLMs.  Experimental design methods from statistics remain relevant in the LLM era.  Thus, statisticians can both provide alternatives to LLMs and augmentations of LLMs in addition to contributing to theoretical analysis of LLMs. 
%Statisticians, often kidded as the "poor cousins" of computer scientists in terms of access to computational resources, bring a unique strength to the development of language models: the ability to leverage statistical modeling for enhanced sample efficiency. While LLMs in computer science are built on brute-force approaches requiring immense datasets and compute power, statisticians excel in developing models that prioritize structure and efficiency. By incorporating sparsity assumptions, hierarchical structures, and principled regularization techniques, statisticians can design ``small language models"  that achieve comparable performance with significantly fewer data and resources. These models are not only cost-effective but also offer greater interpretability and robustness, addressing specific tasks with precision. Statisticians' focus on theory-driven innovation positions SLMs as complementary tools to LLMs, particularly in domains where data is limited, privacy is critical, or interpretability is a priority. By embracing this niche, statisticians can carve out a meaningful role in the AI ecosystem, advancing methodologies that align with the values of efficiency, transparency, and rigor.






\subsection{Towards Understanding LLMs}
%\subsection{Analyzing LLMs with Statistical Learning Theory}
%Statistical learning theory studies machine learning models, especially deep learning models, through three aspects: approximation, optimization, and generalization. From this point of view, many efforts attempt to explain the success of LLMs by analyzing the Transformer architecture.

The remarkable success of LLMs has inspired growing efforts to understand their inner workings, often using tools from statistical learning theory. Initial research has made strides by examining LLMs through the lenses of approximation, optimization, generalization, and prompting mechanisms. For instance, studies have demonstrated the universal approximation properties of Transformers across various architectures \cite{yun2019transformers, yun2020n, kajitsuka2023transformers}, their computational expressiveness as Turing-complete systems \cite{perez2019turing, wei2022statistically}, and their superior optimization dynamics compared to MLPs in specific tasks \cite{wang2024transformers}. On generalization, researchers have refined bounds on sequence length \cite{zhang2022analysis, trauger2024sequence} and extended them to non-i.i.d. settings such as time-series data \cite{limmer2024reality}. Prompting techniques, such as in-context learning  and chain-of-thought  prompting, have further advanced our understanding of LLM adaptability and reasoning. Theoretical work interprets ICL as implicit Bayesian inference arising from pre-training distributions like hidden Markov models \cite{xie2021explanation} and explores its capacity to approximate diverse algorithms, including gradient descent, Newton's method, and reinforcement learning \cite{akyurek2022learning, von2023transformers, fu2023transformers, bai2024transformers, lin2023transformers,zhang2024trained,guo2023transformers}. CoT prompting enhances reasoning by decomposing tasks into sequential steps, improving both accuracy and interpretability \cite{wei2022chain, DBLP:conf/nips/YaoYZS00N23}. For example, CoT has been shown to increase the approximation power of Transformers in arithmetic and equation-solving tasks \cite{feng2024towards} and promote sparse attention patterns that simplify learning \cite{wen2024sparse}. Furthermore, many pre-trained LLMs exhibit the attention-sink phenomenon \cite{xiao2023efficient}, where certain ``sink tokens'' receive disproportionately high attention weights. \cite{guo2024active} analyzed this phenomenon using simplified models and attributed it primarily to the softmax operation in attention heads. Finally, the pre-training of LLMs has found to follow an empirical scaling law, in which the test error improves polynomially with the model size and sample size. \cite{lin2024scaling} theoretically derived the scaling law in the context of infinite-dimensional linear regression, aligning with observed empirical scaling laws. 


Despite these advances, our understanding of LLMs remains incomplete. Traditional statistical frameworks, while valuable, often fall short of explaining emergent phenomena like zero-shot generalization, in-context learning, and CoT reasoning, which arise from intricate interactions between model architecture, training data, and optimization dynamics. For instance, while ICL can be partially explained through algorithm approximation and Bayesian inference, its ability to adapt flexibly across diverse tasks without parameter updates remains a challenging puzzle. Similarly, CoT’s capacity to enhance reasoning through structured task decomposition highlights gaps in our theoretical understanding of how attention mechanisms drive complex problem-solving. These behaviors underscore the need for new frameworks that integrate insights from statistical learning theory, optimization, and cognitive science to fully capture the nuanced capabilities of LLMs. Addressing these gaps presents a significant opportunity for statisticians to contribute by developing innovative theories and methodologies, ultimately bridging the divide between empirical successes and foundational understanding of these transformative technologies.

\subsection{Human-AI Collaborative Data Science}
As LLMs continue to advance, an important future direction lies in fostering effective human-AI collaboration, where AI systems complement rather than replace human expertise. While LLMs demonstrate remarkable capabilities in reasoning, text generation, and decision support, they lack deep understanding, contextual awareness, and accountability—factors that are crucial in high-stakes applications such as medicine, law, and scientific research. Thus, rather than viewing AI as a replacement for human intelligence, a more promising approach is to develop frameworks that integrate human expertise with AI-driven insights to achieve superior outcomes.

One promising avenue for human-AI collaboration is interactive decision-making, where LLMs provide recommendations, assist with exploratory data analysis, or generate potential solutions, while human users retain final control and oversight. For example, in statistical modeling, LLMs can automate tedious tasks such as data preprocessing, feature engineering, and model selection, while human analysts focus on domain-specific reasoning, hypothesis testing, and interpreting results. Similarly, in medical applications, AI can assist doctors by summarizing patient records or suggesting potential diagnoses, but the final decision remains with human experts, ensuring accountability and ethical considerations.

However, interactive decision-making introduces new statistical challenges, particularly due to the non-i.i.d.\ nature of data. In traditional statistical and machine learning settings, models are typically trained on independently and identically distributed (i.i.d.) data. However, in human-AI collaboration, the data distribution adapts dynamically based on prior AI suggestions and human feedback. This feedback loop can introduce selection bias, concept drift, and strategic adaptation, where human decision-making patterns shift based on AI recommendations. For example, if a recommendation system in hiring disproportionately suggests candidates from a particular demographic, decision-makers may adjust their selection patterns accordingly, reinforcing feedback loops and exacerbating bias. Standard statistical tools that assume fixed distributions become inadequate in such settings, necessitating the development of adaptive inference methods, causal modeling techniques, and robust statistical frameworks that can account for evolving data distributions.  See \cite{Perdomo,garg2024oracle,gibbs2024conformal} for more discussions of these issues and further pointers.
Another key research direction is designing adaptive AI systems that can dynamically adjust to user expertise and preferences. Current LLMs operate largely as static models, generating responses based solely on input prompts. However, effective collaboration requires AI systems that learn from user feedback, refine their responses over time, and personalize their assistance based on the expertise level of the user. This could involve techniques such as reinforcement learning from human feedback (RLHF), uncertainty-aware AI models that defer decisions to humans when confidence is low, or interactive AI systems that engage users in dialogue to refine understanding and reduce ambiguity.

Addressing these challenges requires a multidisciplinary effort, bringing together statisticians, computer scientists, social scientists, legal scholars and policymakers to design fair, transparent, and accountable AI systems. By integrating rigorous statistical frameworks with advances in LLMs, the community can work to ensure that human welfare remains a central consideration in the development of next-generation AI models.








%More? Data collection?

% The remarkable success of LLMs has inspired growing efforts to understand their inner workings, often using tools from statistical learning theory. Initial research has made strides by examining LLMs through the lenses of approximation, optimization, generalization, and prompting mechanisms. For instance, studies have demonstrated the universal approximation properties of Transformers across various architectures \cite{yun2019transformers, yun2020n, kajitsuka2023transformers}, their computational expressiveness as Turing-complete systems \cite{perez2019turing, wei2022statistically}, and their superior optimization dynamics compared to MLPs in specific tasks \cite{wang2024transformers}. On generalization, researchers have refined bounds on sequence length \cite{zhang2022analysis, trauger2024sequence} and extended them to non-i.i.d. settings \cite{limmer2024reality}. Prompting techniques, such as in-context learning (ICL) and chain-of-thought (CoT) prompting, have further advanced our understanding of LLM adaptability. Theoretical work interprets ICL as implicit Bayesian inference \cite{xie2021explanation} and explores its capacity to approximate diverse algorithms, including gradient descent and Newton's method \cite{akyurek2022learning, von2023transformers}. CoT prompting enhances reasoning by decomposing tasks into sequential steps, improving both accuracy and interpretability \cite{wei2022chain, DBLP:conf/nips/YaoYZS00N23, wen2024sparse}.

% Despite these advances, our understanding of LLMs remains incomplete. Traditional statistical frameworks, while valuable, often fall short of explaining emergent phenomena like zero-shot generalization, in-context learning, and CoT reasoning, which arise from complex interactions between model architecture, training data, and optimization dynamics. These behaviors highlight the need for new theoretical models that capture the full complexity of LLM capabilities. Addressing this gap presents a significant opportunity for statisticians to contribute by developing innovative frameworks that bridge theory and practice, ultimately deepening our understanding of these transformative technologies.


%The remarkable success of large language models (LLMs) has sparked a growing interest in understanding their underlying mechanisms through the lens of statistical learning theory. This field traditionally examines machine learning models via three key aspects: approximation, optimization, and generalization. By applying this framework, researchers have made significant strides in explaining the capabilities of LLMs, particularly the Transformer architecture that underpins them.

%\paragraph{Statistical Learning Theory in architectures of LLMs} 
%Transformers exhibit universal approximation properties for sequence-to-sequence functions \cite{yun2019transformers}, sparse attention \cite{yun2020n}, shallow architectures \cite{kajitsuka2023transformers}, and relative positional encoding \cite{luo2022your}. They are also proven to be Turing complete with both infinite \cite{perez2019turing, bhattamishra2020computational} and finite \cite{wei2022statistically} precision, enabling them to implement any computational algorithm. Optimization studies reveal connections between Transformer geometry and support vector machines \cite{ataee2023max, tarzanagh2023transformers}, while showing that Transformers outperform MLPs on learning tasks under gradient descent \cite{wang2024transformers}. SGD dynamics exhibit inductive biases toward specific tokens \cite{tian2023scan}, and multi-layer Transformers extend these findings \cite{tian2023joma}. Despite progress, phenomena such as the performance gap between Adam and SGD remain poorly understood, with hypotheses pointing to heavy-tailed noise \cite{zhang2020adaptive}, directional sharpness \cite{pan2023toward}, and class imbalance \cite{kunstner2024heavy}. Generalization bounds for Transformers initially depended logarithmically on sequence length \cite{edelman2022inductive} but were later refined to be length-independent \cite{zhang2022analysis, trauger2024sequence}. Extensions to non-i.i.d. data, such as time-series settings, further highlight Transformers' flexibility \cite{limmer2024reality}. However, traditional generalization theories fail to fully explain LLMs’ out-of-distribution generalization, with studies emphasizing the role of inductive biases in architecture and training dynamics for tasks like zero-shot learning and in-context learning \cite{liu2023same, reizingerposition}.% \paragraph{Approximation theory} The universal approximation property of Transformers has been studied extensively in the literature. It was first established by \cite{yun2019transformers} for continuous sequence-to-sequence functions, and was then generalized to various settings such as sparse attention \cite{yun2020n}, shallow Transformer \cite{kajitsuka2023transformers}, relative positional encoding \cite{luo2022your}, and infinite dimensional input \cite{takakura2023approximation}. Especially, \cite{sanford2024representational} presented a sparse token selection task that Transformers can approximate with logarithmic complexity while MLP and RNN models require polynomial complexity. In addition to the universal approximation perspective, another line of works also proved that Transformers are Turning complete with both infinite \cite{perez2019turing, bhattamishra2020computational} and finite \cite{wei2022statistically} precision, which implies Transformers are able to implement any computer algorithms.  

% \paragraph{Optimization theory} Rigorously analyzing the optimization properties of Transformers is generally hard, hence many existing works primarily focus on the one-layer Transformer models. Along this line, \cite{ataee2023max, tarzanagh2023transformers} showed the equivalence between the optimization geometry of Transformers and support vector machines; \cite{tian2023scan} showed that the SGD dynamics has inductive bias towards specific tokens; \cite{wang2024transformers} showed that a one-layer Transformer trained with gradient descent can learn tasks better than MLP; \cite{tian2023joma} further extends the analysis to multi-layer Transformers. \cite{guo2024active} investigated the optimization behavior of one-layer Transformers, highlighting the attention-sink phenomenon \cite{xiao2023efficient} in models pre-trained on simple tasks.

% Besides analyzing the optimization trajectory on specific distributions, a unique and mysterious characteristic of Transformers is the gap between Adam and SGD. Towards understanding this phenomenon, various explanations have been proposed through the perspectives of heavy-tailed noise \cite{zhang2020adaptive}, directional sharpness \cite{pan2023toward}, class imbalance \cite{kunstner2024heavy}. However, there still lacks a rigorous theory. Recently, \cite{ahn2023linear} has highlighted that such phenomena also occur in linear Transformers empirically. Therefore, we highlight a theoretical analysis of the advantage of Adam on a simplified model as a promising and impacting future direction for statisticians. 

% \paragraph{Generalization theory} Generalization bound is a general tool to quantitatively associate model performance with its components, such as model size, sample size, and sample distribution. The generalization bound for Transformers was first derived in \cite{edelman2022inductive} with logarithmic growth with the sequence length, and then later improved to be length independent in \cite{zhang2022analysis, trauger2024sequence}. Furthermore, \cite{fu2024can} demonstrates the dependence on the number of attention heads with frozen features, and \cite{limmer2024reality} extends the bound to non-i.i.d. data such as a single trajectory of time series. Compared to classical machine learning models, one of the most important features of LLMs is the ability to generalize out of distribution. In \cite{reizingerposition}, the authors argue that current LLMs have reached the "saturation regime". In this regime, the model has attained the global minimum of testing loss, but such minimums are not unique, and different minimums may have different performances when evaluated out of distribution. Hence in this regime, statistical generalization fails to explain the most interesting phenomena of LLMs, such as zero-shot extrapolation and in-context learning, as models with good testing loss are not guaranteed to perform well on such downstream tasks \cite{liu2023same}. To better understand the success of generalization in LLMs, it is important to incorporate other factors such as the inductive bias of model architecture and training dynamics that leads to superior out-of-distribution generalization. 
%
% \paragraph{Statistical Theories for Prompting.} In-context learning (ICL) and chain-of-thought (CoT) prompting represent transformative paradigms that enhance the adaptability and reasoning capabilities of LLMs. ICL enables models to adapt to tasks using input prompts without modifying parameters. Theoretical work interprets ICL as implicit Bayesian inference, arising from pre-training distributions like mixtures of hidden Markov models \cite{xie2021explanation}. From an algorithmic perspective, Transformers can implement diverse algorithms through ICL, including ridge regression \cite{akyurek2022learning}, gradient descent \cite{von2023transformers, dai2022can}, Newton’s method \cite{fu2023transformers}, and reinforcement learning \cite{lin2023transformers}. Studies on training dynamics reveal global convergence in linear regression tasks \cite{zhang2024trained, huang2023context}, while analyses of sample complexity explore the interplay between training and in-context examples \cite{li2023transformers, jeon2024information}.
% CoT prompting enhances LLM reasoning by guiding models to decompose tasks into sequential steps, improving accuracy and interpretability \cite{wei2022chain, DBLP:conf/nips/YaoYZS00N23}. For instance, CoT increases Transformers' approximation power in arithmetic and equation-solving tasks \cite{feng2024towards} and simplifies learning by promoting sparse attention patterns \cite{wen2024sparse}. Leveraging circuit complexity theory, CoT has been shown to improve performance on tasks challenging for parallel computation \cite{li2024chain}. Together, ICL and CoT exemplify how innovative prompting techniques can harness the latent potential of LLMs to tackle increasingly complex tasks.


% \paragraph{In-Context Learning} In-context learning (ICL) has emerged as a novel learning paradigm in LLMs. The first theoretical explanation of ICL was provided by \cite{xie2021explanation}, who interpreted ICL as implicit Bayesian inference and demonstrated that it occurs when the pre-training distribution is a mixture of hidden Markov models. Further work interprets ICL through an algorithm approximation perspective. It has been shown that Transformers can implement a wide range of algorithms through ICL \cite{garg2022can, wei2022statistically}, including ridge regression \cite{akyurek2022learning}, gradient descent \cite{von2023transformers, dai2022can, ahn2024transformers, mahankalione}, functional gradient descent \cite{cheng2023transformers}, algorithm selection \cite{bai2024transformers}, Bayesian model average \cite{zhang2023and}, learning with representations \cite{guo2023transformers}, Newton's method \cite{fu2023transformers, giannou2024well}, and reinforcement learning \cite{lin2023transformers}. Beyond expressive power, \cite{zhang2024trained, huang2023context} further considered the ICL training dynamics and showed global convergence in linear regression tasks. \cite{li2023transformers,jeon2024information} derived the sample complexities of ICL with respect to training samples, in-context samples, and sequence lengths. 



% \paragraph{Chain of thought} Chain-of-thought (CoT) prompting \cite{wei2022chain, DBLP:conf/nips/YaoYZS00N23} is a technique designed to significantly enhance the reasoning capabilities of pre-trained LLMs. Recent research has explored why CoT improves reasoning performance through the lens of Transformer representations \cite{feng2024towards, li2024chain, merrill2023expresssive, wen2024rnns, wen2024sparse}. For example, \cite{feng2024towards} analyzed arithmetic and equation-solving tasks, revealing that CoT increases the approximation power of Transformers. \cite{li2024chain}, leveraging circuit complexity theory, demonstrated that CoT substantially improves accuracy on tasks challenging for parallel computation. Additionally, \cite{wen2024sparse} showed that CoT simplifies the learning process by introducing sparse sequential dependencies among input tokens, resulting in sparser and more interpretable attention patterns. 



%\subsection{Social Impact}


% \bibliographystyle{alpha}
 %\bibliography{sample,ref}
\printbibliography
\newpage
\appendix
\section{Historical Development of LLMs}

The capabilities of LLMs arise from two critical components: \textit{representation learning} and \textit{language modeling}. Representation learning involves learning a numerical representation of input text, in a way that captures their underlying relationships and patterns. Language modeling involves predicting the next word in a sequence of texts, allowing for coherent and fluent text generation.

\subsection{Representation Learning}
The first step in an LLM is to transform natural language into a format that computers can understand---specifically vectors, matrices, and tensors. The symbolic lexical units (i.e., words) that comprise sentences require effective numerical representation strategies, known as \emph{word embeddings}. 
Grounded on the distributional hypothesis, which posits that linguistic items that occur in similar contexts have similar meanings~\cite{harris54}, such methods have gradually become more sophisticated over time.

Early attempts at capturing word meanings involved bag-of-words representations, which represent documents as sparse vectors, with a dimension for each element of the dictionary, for example as in TF-IDF~\cite{sparck1972statistical}. Matrix-decomposition methods were subsequently introduced, such as Latent Semantic Analysis (LSA)~\cite{10.5555/599609.599631,10.5555/1658616.1658815}, to represent words as (dense) vectors. Many such methods implicitly involved weights that were defined or learned in an unsupervised fashion.
Supervised learning methods such as Support Vector Machines were later used to train bag-of-word representations that aimed directly at solving particular tasks \citep{joachims1998text}, leading to performance improvements.
Supervised training of dense word embeddings 
by gradient-based or neural methods similarly gave 
improvements \citep{bai2009supervised,weston2011wsabie}, with the caveat that relatively large datasets were required.
 While useful, most of these methods fell short of solving challenging tasks in NLP, as they lacked the capacity to convey complex semantic and syntactic nuances of words in context.

  
% Grounded on the distributional hypothesis, which posits that linguistic items with similar distributions have similar meanings~\cite{harris54}, traditional statistical methods like count-based methods such as TF-IDF~\cite{Ramos1999}, PPMI~\cite{church-hanks-1990-word} and class-based methods such as Brown clusters~\cite{brownClustering} have been introduced to encode semantic information into word embeddings. However, they still suffer from high dimensionality and computational complexity.  \lz{need to add Collobert et al, 2011?} \lz{How about SVD-baesd methods?}

%Grounded on the distributional hypothesis, which posits that linguistic items that occur in similar contexts have similar meanings~\cite{harris54}, traditional statistical methods like count-based methods such as TF-IDF~\cite{Ramos1999}, PPMI~\cite{church-hanks-1990-word} have been introduced to encode semantic information into word embeddings. These vectors are typically constructed based on co-occurrence matrices, which capture the occurrence information of words in different documents. However, they still suffer from high dimensionality and computational complexity. To alleviate this problem, SVD-based methods, also known as Latent Semantic Analysis (LSA)~\cite{10.5555/599609.599631,10.5555/1658616.1658815}, is used for dimensionality reduction. Additionally, recent models leverage neural networks to directly learn low-dimensional word representations.
% https://www.jair.org/index.php/jair/article/view/11259/26454


% Subsequently, neural-network based approaches such as word2vec~\cite{DBLP:conf/nips/MikolovSCCD13} \lz{does this include CBOW, skip-gram etc.?} and GloVe~\cite{pennington-etal-2014-glove} were introduced. These methods efficiently learn dense \lz{what do you mean by "dense"? Are the previous methods considered as "sparse"? More explanations will be appreciated} word embeddings by leveraging language modeling objectives, which we will discuss in \S\ref{subsec:language_modeling}, on large corpora. An integral part of this process was tokenization \lz{briefly discuss the benefit of tokenization compared with the ``word'' discussed in previous sentences}, a method of breaking down text into smaller units, or tokens. One popular tokenization technique is Byte Pair Encoding (BPE)~\cite{10.5555/177910.177914}, which splits words into subwords based on the most common character sequences to reduce vocabulary size. From this point forward, the evolution of representation learning, language modeling, and tokenization techniques became a unified process.

Neural methods first grew to prominence with the advent of  Word2vec~\cite{DBLP:conf/nips/MikolovSCCD13} and similar approaches such as GloVe~\cite{pennington-etal-2014-glove} which
 directly learn low-dimensional word representations from local word context.   The specific objective functions underlying
 the architecture of these models were explored in foundational works such as those by \cite{10.5555/944919.944966, 10.1145/1390156.1390177, 10.5555/1953048.2078186,turian-etal-2010-word}, setting the stage for subsequent innovations. Word2vec, for instance, encompasses two specific models: Continuous Bag-Of-Words (CBOW) and Skip-gram. The CBOW model predicts a target word based on its surrounding context, whereas the Skip-gram model predicts the surrounding context given a target word. Both models utilize a straightforward yet efficient neural network architecture that effectively captures semantic and syntactic word relationships~\cite{DBLP:conf/nips/MikolovSCCD13}.
These approaches generate dense word embeddings, which are compact vectors where each dimension encodes a latent feature of the word.
After this unsupervised training, these vectors were then often used as input or initialization for a supervised training stage for the task of interest.
%, in contrast to the sparse representations produced by earlier methods. %Sparse representations typically involve high-dimensional vectors with many zero values, each dimension representing the presence or absence of a specific feature, which can be less efficient in capturing the nuanced meanings of words.

% {\cyan Do the previous models learn the representation of tokens? or are they general methods?}
These approaches to representation learning focused on words as the primary unit, which gave rise to several challenges, including a large vocabulary size and the issue of out-of-vocabulary words. To address these challenges, the concept of tokenization was introduced. Tokenization plays a crucial role in the effectiveness of these models. It involves breaking down text into smaller units, or tokens, which affects the granularity of the learned representations. A popular technique used in this context is Byte Pair Encoding (BPE)~\cite{10.5555/177910.177914}, which reduces vocabulary size by merging the most frequent pairs of bytes or characters. This method not only helps manage vocabulary size but also addresses challenges related to out-of-vocabulary words in new texts.
Thus, the evolution of representation learning, language modeling, and tokenization techniques has become a unified and interdependent process.





% \weizhey{TODO: Tokenization}

% Inspired by NPLM, there came many methods that embed words into distributed
% representations and use the language modeling objective to optimize them as model
% parameters. Famous examples include word2vec [12], GloVe [13], and fastText [3].
% Though differing in detail, these methods are all very efficient to train, utilize largescale corpora, and have been widely adopted as word embeddings in many NLP
% models. Word embeddings in the NLP pipeline map discrete words into informative
% low-dimensional vectors, and help to shine a light on neural networks in computing and understanding languages. It makes representation learning a critical part of
% natural language processing.





\subsection{Language Modeling}
\label{subsec:language_modeling}

%One of the key problems in NLP is assigning a probability to every possible sentence, which is called the language modeling problem. Models that assign probabilities to upcoming words, or sequences of words, are called language models (LMs). Specifically, the language modeling aims to learn a mapping that takes a sequence of tokens $\bm{x}=x_1, x_2, \cdots, x_T$ as input, and output a probability vector over the token vocabulary for the next-token $x_{T+1}$.

In the early stages of language models (LMs), LMs were mostly statistical regression models, relying on the Markov assumption to predict the next word from the most recent context. The dominant models were $n$-gram models~\cite{10.5555/280484, 10.1145/1034780.1034781, 880083}, which determines a word's probability based on the preceding $n - 1$ words. Challenges for these models included the need to tackle data sparsity---the issue of assigning zero probabilities to unseen words or $n$-grams~\citep{chen-goodman-1996-empirical}---and to cope with the exponential number of transition probabilities that need to be estimated as $n$ increases. As $n$ was typically thus forced to be small (e.g., 1-5 words) the idea of using such models to complete a long context input seemed out of scope.




% \documentclass{article}
% \usepackage{booktabs}

% \begin{document}

% \begin{table}[h]
% \centering
% \caption{Classification of Large Language Models}
% \label{tab:llm_classification}
% \begin{tabular}{@{}llll@{}}
% \toprule
% LLM Size                & Inference Requirements                           & Training Requirements                                         & Examples \\ \midrule
% Small (up to 1B params) & 1x GPU (e.g., RTX 3080, 10GB VRAM)               & 1-2 GPUs (e.g., RTX 3080, 10GB VRAM), Moderate Data (up to 100GB) & Example: GPT-2 small \\
% Medium (1B - 100B params) & 4-8x GPUs (e.g., V100, 32GB VRAM each)          & 16-32 GPUs (e.g., A100, 40GB VRAM each), Large Data (100GB to 1TB) & Example: LLaMA-70B \\
% Large (100B+ params)    & API Access Only (Hosted on specialized hardware) & 100+ GPUs (e.g., A100, 80GB VRAM each), Massive Data (1TB+) & Example: GPT-4, Claude3 \\ \bottomrule
% \end{tabular}
% \end{table}

% \end{document}


% However, even the same word can mean different things when appear in different contexts. Then, contextualized word representations such as ELMo~\cite{peters-etal-2018-deep}, BERT~\cite{devlin-etal-2019-bert}







\section{Key Resources: Accelerating Progress in LLM Research}
\label{app:resources}
The development of LLMs require a combination of computational resources, robust frameworks, and specialized tools. Over the years, a variety of open-source tools and libraries have been developed, significantly lowering the barrier to entry for researchers and practitioners. This section provides a detailed overview of the most popular and widely adopted resources for LLM research, focusing on frameworks, model libraries, data preparation tools, and utilities for training, deployment, and analysis.
\subsection{Core Frameworks for Model Development}
\paragraph{PyTorch} PyTorch has become the dominant deep learning framework for LLM research due to its flexibility, dynamic computation graph, and extensive community support. Its ease of debugging and seamless integration with other libraries make it ideal for developing and fine-tuning LLMs.

\paragraph{TensorFlow and JAX} TensorFlow remains a competitive choice for large-scale training, particularly with its distributed computing capabilities. JAX, with its functional programming paradigm and support for automatic differentiation, is gaining traction for research prototypes and cutting-edge optimization techniques.

Please see \autoref{tab:core_frameworks} for the comparison of those frameworks.

\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.6cm}p{3.6cm}p{3.8cm}p{4cm}}
\toprule
\textbf{Framework} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
PyTorch & Dynamic graphs, community support & Fine-Tuning, prototyping & \url{https://pytorch.org/tutorials/} \\
\midrule
TensorFlow & Scalability, production readiness & Large-scale training & \url{https://www.tensorflow.org/tutorials} \\
\midrule
JAX & High-performance computation, flexibility & Experimental optimization & \url{https://jax.readthedocs.io/en/latest/tutorials.html} \\
\bottomrule
\end{tabular}
\caption{Core Frameworks for Model Development.}
\label{tab:core_frameworks}
\end{table}




\subsection{Model Libraries and Pre-trained Models}

\paragraph{Hugging Face Transformers} Hugging Face’s Transformers library is a cornerstone of LLM research, providing pre-trained models, tokenizers, and utilities for fine-tuning across multiple tasks. Its intuitive API and extensive documentation make it accessible to researchers at all levels.

\paragraph{LangChain} LangChain simplifies the process of integrating LLMs into applications by enabling seamless chaining of language model outputs and external tools, such as APIs or databases.

Please see \autoref{tab:model_libs} for more descriptions.


\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.9cm}p{3.8cm}p{3.2cm}p{4.2cm}}
\toprule
\textbf{Library} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
Hugging Face Transformers & Pre-trained models, tokenizers, multi-task support & Fine-Tuning, experimentation & \url{https://huggingface.co/docs/transformers/index} \\
\midrule
LangChain & Tool integration, output chaining & Application development & \url{https://python.langchain.com/docs/introduction/} \\
\bottomrule
\end{tabular}
\caption{Model Libraries and Pre-trained Models.}
\label{tab:model_libs}
\end{table}
\subsection{Datasets and Data Preparation Tools}
\paragraph{Hugging Face Datasets}
Hugging Face Datasets provides a vast library of curated datasets and tools for loading, filtering, and processing data. It supports efficient streaming and preprocessing pipelines, enabling researchers to work with massive datasets without excessive memory usage. The library also integrates seamlessly with Hugging Face’s tokenizers and models, making it an essential tool for LLM research. 

\paragraph{Data Cleaning Tools}
Preprocessing is crucial for preparing datasets for LLM training. Tools like \texttt{clean-text} simplify the cleaning of noisy datasets by removing special characters, normalizing text, and correcting encoding issues. Additionally, tokenization libraries such as \texttt{SentencePiece} and Hugging Face’s \texttt{tokenizers} library are indispensable for converting text into model-ready input formats.

Please see \autoref{tab:datasets_data_prep} for summarization.
\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.8cm}p{3.4cm}p{3.8cm}p{4cm}}
\toprule
\textbf{Tool} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
Hugging Face Datasets & Curated datasets, efficient streaming & Loading and preprocessing datasets & \url{https://huggingface.co/docs/datasets/index} \\
\midrule
clean-text & Text normalization, cleaning noisy datasets & Data preparation & \url{https://github.com/jfilter/clean-text} \\
\midrule
SentencePiece & Subword tokenization & Text tokenization & \url{https://github.com/google/sentencepiece} \\
\midrule
Hugging Face Tokenizers & Subword tokenization & Text tokenization & \url{https://huggingface.co/docs/tokenizers/index} \\
\bottomrule
\end{tabular}
\caption{Datasets and Data Preparation Tools.}
\label{tab:datasets_data_prep}
\end{table}

\subsection{Training and Fine-Tuning Utilities}
\paragraph{DeepSpeed and Megatron-LM}
DeepSpeed and Megatron-LM are indispensable for scaling the training of LLMs. DeepSpeed’s ZeRO (Zero Redundancy Optimizer) optimization reduces memory overhead, enabling the efficient training of massive models on limited hardware. Additionally, Megatron-LM excels in model parallelism, splitting large models across multiple GPUs for distributed training.

\paragraph{Hugging Face Fine-Tuning Frameworks}
Hugging Face provides an intuitive and widely used framework for fine-tuning pre-trained models on downstream tasks. Using the Trainer API, researchers can efficiently fine-tune models with minimal code. Hugging Face also supports custom training loops for more advanced fine-tuning setups. 

% \paragraph{LoRA (Low-Rank Adaptation)}
% LoRA introduces an efficient fine-tuning technique by adjusting only a small subset of a model’s parameters, significantly lowering computational costs. This makes it an excellent choice for researchers working in resource-constrained environments.

\paragraph{Hugging Face PEFT Frameworks}
 Hugging Face's PEFT frameworks build on techniques like LoRA to offer additional methods for adapting large models to specific tasks while keeping the majority of the model frozen. This accelerates fine-tuning and reduces resource requirements.

\paragraph{TRL \& OpenRLHF}
TRL (Transformers Reinforcement Learning) and OpenRLHF are powerful libraries designed to simplify the implementation of RLHF and Direct Preference Optimization (DPO). 


% TRL, developed by Hugging Face, integrates seamlessly with the Transformers library, enabling researchers to fine-tune large language models using reinforcement learning techniques such as Proximal Policy Optimization (PPO). Its accessible APIs allow users to integrate custom reward models, making it a popular choice for RLHF workflows. OpenRLHF is a modular and scalable framework designed specifically for RLHF tasks. It supports robust reward modeling, policy optimization, and evaluation pipelines. OpenRLHF’s modularity allows researchers to adapt it to various alignment tasks, making it a versatile option for RLHF and DPO implementations.

% \paragraph{Reinforcement Learning with Human Feedback (RLHF)}
% RLHF is an advanced training paradigm for aligning LLM behavior with human preferences. Tools like OpenAI’s RLHF framework and HuggingFace’s RLHF pipelines provide utilities for combining reinforcement learning with human feedback to fine-tune LLMs. This method is particularly useful for applications like safe AI systems, interactive agents, and chatbots.

% \paragraph{Direct Preference Optimization (DPO)}
% DPO simplifies preference alignment by optimizing models directly to predict human preferences, eliminating the need for a separate reward model. Emerging libraries are making this technique more accessible for custom alignment tasks. While still experimental, DPO has promising applications for personalized AI systems.

Please see \autoref{tab:train} for summarization.


\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.8cm}p{3.6cm}p{3.7cm}p{4cm}}
\toprule
\textbf{Tool} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
DeepSpeed & ZeRO optimization, scalability & Large-scale training & \url{https://www.deepspeed.ai/tutorials/} \\ 
\midrule
Megatron-LM & Model parallelism & Large-scale training & \url{https://github.com/NVIDIA/Megatron-LM} \\ 
\midrule

Hugging Face Trainer & Easy fine-tuning setup, Trainer API & Fine-Tuning pre-trained models & \url{https://huggingface.co/docs/transformers/training} \\ 
\midrule
Hugging Face PEFT & Efficient fine-tuning techniques & Resource-constrained settings & \url{https://huggingface.co/docs/peft/index}\\
\midrule
OpenRLHF & RLHF, DPO & Post training, AI alignment & \url{https://github.com/OpenRLHF/OpenRLHF}
\\
\midrule
TRL & RLHF, DPO & Post training, AI alignment & \url{https://huggingface.co/docs/trl/index}
\\
\bottomrule

\end{tabular}
\caption{Training and Fine-Tuning Utilities.}
\label{tab:train}
\end{table}



\subsection{Inference and Deployment}
\paragraph{vLLM and TensorRT-LLM}
vLLM and TensorRT-LLM are optimized tools for low-latency inference of large language models. vLLM focuses on efficient memory usage, enabling faster batch inference for serving applications. TensorRT-LLM, developed by NVIDIA, supports high-throughput model inference on GPUs, taking advantage of advanced hardware accelerations. Both tools are ideal for production environments requiring high-performance LLM deployment.

% \paragraph{ONNX Runtime}
% ONNX Runtime is a versatile and lightweight inference engine that enables cross-platform deployment of LLMs. It supports models exported in the ONNX format and integrates with various hardware backends, such as CPUs, GPUs, and specialized accelerators. Its support for quantization and model optimization techniques further improves inference efficiency.


\paragraph{Triton Inference Server}
NVIDIA’s Triton Inference Server simplifies LLM deployment by supporting multiple frameworks (e.g., PyTorch, TensorFlow, ONNX). It enables scalable, production-grade model serving with GPU and CPU backends.
\paragraph{Hugging Face Inference Endpoints} 
Hugging Face provides an easy-to-use platform for deploying LLMs as APIs, making it simple for developers to integrate models into their applications without managing infrastructure.

\paragraph{Ray Serve} Ray Serve is a distributed model serving library that supports scaling and parallel inference for large LLMs. It integrates seamlessly with distributed computing frameworks.


Please see \autoref{tab:inference_deployment} for more information.

\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.6cm}p{3.6cm}p{3.8cm}p{4cm}}
\toprule
\textbf{Tool} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
vLLM & Memory optimization, fast batch inference & Production inference & \url{https://github.com/vllm-project/vllm} \\
\midrule
TensorRT-LLM & High-throughput GPU inference & Optimized GPU model serving & \url{https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0}\\
\midrule
% ONNX Runtime & Cross-platform, lightweight, quantization support & Efficient multi-backend serving & \url{https://onnxruntime.ai/docs/} \\
% \midrule
Hugging Face Endpoints & Simplified API deployment & Rapid integration & \url{https://huggingface.co/inference-endpoints/dedicated} \\ 
\midrule
Triton Inference Server & Scalable serving with multi-framework support & Scalable LLM deployment & \url{https://developer.nvidia.com/triton-inference-server}\\
\midrule
Ray Serve & Distributed, parallel inference & Scalable LLM deployment & \url{https://docs.ray.io/en/latest/serve/index.html}
\\
\bottomrule

\end{tabular}
\caption{LLM Inference and Deployment.}
\label{tab:inference_deployment}
\end{table}







\subsection{Visualization and Analysis}
\paragraph{Weights \& Biases} 
Weights \& Biases (W\&B) is a powerful tool for experiment tracking, hyperparameter optimization, and real-time visualization of training metrics. It integrates seamlessly with most machine learning frameworks, providing interactive dashboards that display loss curves, accuracy trends, and other performance indicators. W\&B is particularly useful for collaborative research, allowing teams to share results and insights easily. 

\paragraph{TensorBoard}
TensorBoard is a widely adopted visualization toolkit for monitoring model training and evaluation. Its key features include plotting scalars (e.g., loss and accuracy), displaying images, and analyzing graph structures. TensorBoard’s ease of integration with TensorFlow and PyTorch makes it a staple in the machine learning community.

\paragraph{MLflow}
MLflow is an open-source platform for managing the lifecycle of machine learning experiments. It supports logging of metrics, artifacts, and parameters, as well as model versioning and deployment tracking. MLflow’s UI allows researchers to compare experiments and optimize workflows. 

Please see \autoref{tab:vis_analysis} for summarization.

\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.6cm}p{3.6cm}p{3.8cm}p{4cm}}
\toprule
\textbf{Tool} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
Weights \& Biases & Experiment tracking, hyperparameter tuning & Experimentation, visualization & \url{https://docs.wandb.ai} \\ 
\midrule
TensorBoard & Training progress monitoring & Model performance analysis & \url{https://www.tensorflow.org/tensorboard} \\
\midrule
MLflow & Experiment lifecycle management & Comparing experiments, model versioning & \url{https://mlflow.org/docs/latest/index.html} \\
\bottomrule

\end{tabular}
\caption{Visualization and Analysis.}
\label{tab:vis_analysis}
\end{table}


\subsection{Cloud and Hardware Resources}
Cloud computing platforms have become integral to large-scale LLM research and deployment. They provide scalable, on-demand access to powerful computational resources like GPUs, TPUs, and specialized accelerators, enabling researchers to run experiments without heavy upfront investment in infrastructure.

\paragraph{AWS (Amazon Web Services)} Offers EC2 instances optimized for deep learning, such as p4d instances equipped with NVIDIA A100 GPUs. AWS also provides SageMaker, a managed service for building, training, and deploying machine learning models.

\paragraph{Google Cloud Platform (GCP)} Features TPU (Tensor Processing Unit) instances and Vertex AI, a platform for scalable training and inference. GCP is especially advantageous for TensorFlow users.

\paragraph{Microsoft Azure} Provides Azure Machine Learning, a suite of tools and services for machine learning workflows, alongside GPU-powered VMs optimized for deep learning tasks.

Please see \autoref{tab:cloud} for summarization.
\begin{table}[t]
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\centering
\begin{tabular}{p{1.4cm}p{3.6cm}p{3.8cm}p{4.2cm}}
\toprule
\textbf{Platform} & \textbf{Key Features} & \textbf{Use Case} & \textbf{Tutorial Link} \\
\midrule
AWS & EC2 instances with GPUs, SageMaker & Training, deployment  & \url{https://aws.amazon.com/ai/machine-learning/} \\
\midrule
GCP & TPUs, Vertex AI & Scalable training, experimentation  & \url{https://cloud.google.com/products/ai} \\
\midrule
Microsoft Azure & Azure ML, GPU-powered VMs & Experimentation, deployment & \url{https://learn.microsoft.com/en-us/azure/machine-learning}
\\

\bottomrule

\end{tabular}
\caption{Cloud and Hardware Resources.}
\label{tab:cloud}
\end{table}

% Unlike traditional statistical models, LLMs often require significant computational power, including access to GPUs or TPUs for efficient training and inference. Fortunately, cloud platforms such as Google Colab, AWS, Azure, and Hugging Face provide cost-effective options for experimentation, often offering free tiers or research grants to support academic use. Statisticians can start with open-source frameworks like PyTorch or TensorFlow, which provide user-friendly APIs and pre-trained LLMs that can be fine-tuned on smaller, domain-specific datasets. Libraries such as Hugging Face’s Transformers and LangChain simplify working with these models, enabling researchers to focus on developing statistical insights rather than implementation details. For those new to LLMs, tutorials, documentation, and community resources available on platforms like YouTube, GitHub and Hugging Face offer step-by-step guidance. By starting with pre-trained models and gradually exploring techniques such as fine-tuning and prompt engineering, statisticians can build an understanding of LLMs while controlling computational costs, paving the way for deeper methodological investigations. \wz{Move to Appendix, add more references (link, tutorial, etc) @weizhe, Weizhe, please write for around 1 page for e.g., 4/5 topics, library |  use case | tutorial linkhuggingface, vllm, add libraries under huggingface}
\end{document}