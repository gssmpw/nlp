\documentclass{article}
\usepackage{ulem}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{diagbox}
\usepackage{pifont}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[ruled,vlined]{algorithm2e}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage[shortlabels]{enumitem}

\definecolor{pearDark}{HTML}{2980B9}
\newcommand\cbox[1]{\colorbox{MyRed!20}{#1}}
\newcommand\cboxnew[1]{#1}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\david}[1]{{\scriptsize\textbf{\color{blue} DL: #1}}}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{wasysym}
\usepackage{tcolorbox}

\allowdisplaybreaks
\everypar{\looseness=-1}
% \usepackage{wasy}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% \newcommand\eqdef{\stackrel{\mathclap{\normalfont\mbox{def}}}{=}}
\newcommand*{\eqdef}{\stackrel{\text{def}}{=}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\definecolor{CustomRed}{HTML}{D2042D}
\colorlet{MyRed}{CustomRed}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Inverse Bridge Matching Distillation}

\begin{document}

\twocolumn[
\icmltitle{Inverse Bridge Matching Distillation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nikita Gushchin}{skoltech}
\icmlauthor{David Li}{equal,skoltech}
\icmlauthor{Daniil Selikhanovych}{equal,skoltech,yandex,hse}
\icmlauthor{Evgeny Burnaev}{skoltech,airi}
\icmlauthor{Dmitry Baranchuk}{yandex}
\icmlauthor{Alexander Korotin}{skoltech,airi}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{skoltech}{Skolkovo Institute of Science and Technology}
\icmlaffiliation{yandex}{Yandex Research}
\icmlaffiliation{hse}{HSE University}
\icmlaffiliation{airi}{Artificial Intelligence Research Institute}

\icmlcorrespondingauthor{Nikita Gushchin}{n.gushchin@skoltech.ru}
\icmlcorrespondingauthor{Alexander Korotin}{a.korotin@skoltech.ru}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Learning diffusion bridge models is easy; making them fast and practical is an art. Diffusion bridge models (DBMs) are a promising extension of diffusion models for applications in image-to-image translation. However, like many modern diffusion and flow models, DBMs suffer from the problem of slow inference. To address it, we propose a novel distillation technique based on the inverse bridge matching formulation and derive the tractable objective to solve it in practice. Unlike previously developed DBM distillation techniques, the proposed method can distill both conditional and unconditional types of DBMs, distill models in a one-step generator, and use only the corrupted images for training. We evaluate our approach for both conditional and unconditional types of bridge matching on a wide set of setups, including super-resolution, JPEG restoration, sketch-to-image, and other tasks, and show that our distillation technique allows us to accelerate the inference of DBMs from 4x to 100x and even provide better generation quality than used teacher model depending on particular setup.
\vspace{-3mm}
\end{abstract}

\vspace{-2mm}
\section{Introduction}
\vspace{-2mm}
\begin{figure}[hbt!]
    \centering
    \hspace*{-2em}
    \renewcommand{\arraystretch}{0.2}
    \setlength{\tabcolsep}{1.5pt}
    % \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{c c c c}
        & Input & IBMD (\textbf{Ours}) & Teacher \\
        \adjustbox{valign=c, rotate=90, raise=0.9em}{Super-resolution} &
        \includegraphics[width=0.17\textwidth]{images/teaser/pool_input.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/pool_ibmd.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/pool_teacher.png} \\

        \adjustbox{valign=c, rotate=90, raise=0.8em}{JPEG restoration} &
        \includegraphics[width=0.17\textwidth]{images/teaser/jpeg_5_input.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/jpeg_5_ibmd.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/jpeg_5_teacher.png} \\

        \adjustbox{valign=c, rotate=90, raise=2.3em}{Inpainting} &
        \includegraphics[width=0.17\textwidth]{images/teaser/new_inpainting_input.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/new_inpainting_student.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/new_inpainting_teacher.png} \\

        \adjustbox{valign=c, rotate=90, raise=0.7em}{Normal-to-Image} &
        \includegraphics[width=0.17\textwidth]{images/teaser/diode_input.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/diode_student.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/diode_teacher.png} \\

        \adjustbox{valign=c, rotate=90, raise=0.9em}{Sketch-to-Image} &
        \includegraphics[width=0.17\textwidth]{images/teaser/image_grid_input_e2h.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/image_grid_student_e2h.png} &
        \includegraphics[width=0.17\textwidth]{images/teaser/image_grid_teacher_e2h.png} \\

        % \rotatebox{90}{\textbf{JPEG Restoration}} &
        % \includegraphics[width=0.3\textwidth]{imgs/jpeg_input.png} &
        % \includegraphics[width=0.3\textwidth]{imgs/jpeg_output.png} &
        % \includegraphics[width=0.3\textwidth]{imgs/jpeg_reference.png} \\
    \end{tabular}
    % \end{adjustbox}
    \vspace{-4mm}
    \caption{Outputs of DBMs models distilled by our \textbf{Inverse Bridge Matching Distillation (IBMD)} approach on various image-to-image translation tasks and datasets (\wasyparagraph\ref{sec:experiments}). Teachers use NFE$\geq 500$ steps, while IBMD distilled models use NFE$\leq 4$.
    % Super-resolution, JPEG restoration, and inpainting tasks use ImageNet $256\times256$ dataset. Normal map to image translation uses DIODE-Outdoor $256\times256$ dataset, and sketch-to-image generation uses Edges$\rightarrow$ Handbags $64\times64$ dataset. Teachers use NFE$\geq 500$ steps, while IBMD distilled models use NFE$\leq 4$.
    }
    \vspace{-6mm}
    \label{fig:teaser}
\end{figure}
Diffusion Bridge Models (DBMs) represent a specialized class of diffusion models designed for data-to-data tasks, such as image-to-image translation. 
Unlike standard diffusion models, which operate by mapping noise to data \citep{ho2020denoising, sohl2015deep}, DBMs construct diffusion processes directly between two data distributions \citep{peluchetti2023diffusion, liu2022let, somnath2023aligned, zhou2024denoising, yue2024resshift, shi2023diffusion, de2023augmented}. 
This approach allows DBMs to modify only the necessary components of the data, starting from an input sample rather than generating it entirely from Gaussian noise.
As a result, DBMs have demonstrated impressive performance in image-to-image translation problems.

The rapid development of DBMs has led to two dominant approaches, usually considered separately.
The first branch of approaches \citep{peluchetti2023diffusion, liu2022let, liu20232, shi2023diffusion, somnath2023aligned} considered the construction of diffusion between two arbitrary data distributions performing \textbf{Unconditional Bridge Matching} (also called the Markovian projection) of a process given by a mixture of diffusion bridges. 
The application of this branch includes different data like images \citep{liu20232, li2023bbdm}, audio \citep{kong2025a2sb} and biological tasks \citep{somnath2023aligned, tong2024simulation} not only in paired but also in unpaired setups using its relation to the Schr√∂dinger Bridge problem \citep{shi2023diffusion, gushchin2024adversarial}. 
The second direction follows a framework closer to classical diffusion models, using forward diffusion to gradually map to the point of different distibution rather than mapping distribution to distribution as in previous case \citep{zhou2024denoising, yue2024resshift}.
% using forward diffusion based on Doob-h transform in to a degraded degraded state or a noisy version before reversing this process during inference \citep{zhou2024denoising, yue2024resshift}. 
While these directions differ in theoretical formulation, their practical implementations are closely related; for instance, models based on forward diffusion can be seen as performing \textbf{Conditional Bridge Matching} with additional drift conditions \citep{de2023augmented}.

Similar to classical DMs, DBMs also exhibit multistep sequential inference, limiting their adoption in practice.
Despite the impressive quality shown by DBMs in the practical tasks, only a few approaches were developed for their acceleration, including more advanced sampling schemes \citep{zheng2024diffusion, wang2024implicit} and consistency distillation \citep{he2024consistency}, adapted for bridge models. 
While these approaches significantly improve the efficiency of DBMs, some unsolved issues remain.
The first one is that the mentioned acceleration approaches are directly applicable only for DBMs based on the Conditional Bridge Matching, i.e., no universal method can accelerate any DBMs. 
Also, due to some specific theoretical aspects of DBMs, consistency distillation cannot be used to obtain the single-step model \citep[Section 3.4]{he2024consistency}. 

\textbf{Contributions.} To address the above-mentioned issues of DBMs acceleration, we propose a new distillation technique based on the inverse bridge matching problem, which has several advantages compared to existing methods:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Universal Distillation.} Our distillation technique is applicable to DBMs trained with both conditional and unconditional regimes, making it the first distillation approach introduced for unconditional DBMs. 
    \item \textbf{Single-Step and Multistep Distillation.} Our distillation is capable of distilling DBMs into generators with any specified number of steps, including the distillation of DBMs into one-step generators.
    \item \textbf{Target data-free distillation.} Our method does not require the target data domain to perform distillation. 
    \item \textbf{Better quality of distilled models.} Our distillation technique is tested on a wide set of image-to-image problems for conditional and unconditional DBMs in both one and multi-step regimes. It demonstrates improvements compared to the previous acceleration approaches including DBIM\citep{zheng2024diffusion} and CDBM \citep{he2024consistency}.
\end{enumerate}

\vspace{-2mm}
\section{Background}
\vspace{-2mm}
\begin{tcolorbox}[colback=gray!20, colframe=gray!20, arc=2mm, boxrule=0pt, width=1\linewidth, boxsep=-1pt]
In this paper, we propose a universal distillation framework for both conditional and unconditional DBMs. To not repeat fully analogical results for both cases, we denote by \textcolor{MyRed}{this color the additional conditioning on $x_T$} used for the conditional models, i.e. for the unconditional case this conditioning is not used.
\end{tcolorbox}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/bridge_matching_new.png}
    \vspace{-3mm}
    \caption{\textbf{Overview of \textcolor{MyRed}{(Conditional)} Bridge Matching with \(\widehat{x}_0\) reparameterization.} 
        The process begins by sampling a pair \((x_0, x_T)\) from the data coupling \( p(x_0, x_T) \). 
        An intermediate sample \( x_t \) is then drawn from the diffusion bridge \( q(x_t | x_0, x_T) \) 
        at a random time \( t \sim U[0, T] \). The model \(\widehat{x}_0\) is trained with an MSE loss 
        to reconstruct \( x_0 \) from \( x_t \). In the conditional setting \textcolor{MyRed}{(dashed red path)}, 
        \(\widehat{x}_0\) is also conditioned on \textcolor{MyRed}{\( x_T \)} as an additional input, leveraging information about the terminal state to improve reconstruction.}
    \label{fig:bm-scheme}
    \vspace{-4mm}
\end{figure*}
\vspace{-2mm}
\subsection{Bridge Matching}\label{sec:bridge-matching}
\vspace{-2mm}
We start by recalling the bridge matching method \citep{peluchetti2023non, peluchetti2023diffusion, liu2022let, shi2023diffusion}. 
Consider two probability distributions $p(x_0)$ and $p(x_T)$ on $\mathbb{R}^{D}$ dimensional space, which represent target and source domains, respectively.
For example, in an image inverse problem, $p(x_0)$ represents the distribution of \textit{clean} images and $p(x_T)$ the distribution of \textit{corrupted} images. 
Also consider a coupling $p(x_0, x_T)$ of these two distributions, which is a probability distribution on $\mathbb{R}^D \times \mathbb{R}^D$. 
Coupling $p(x_0, x_T)$ can be provided by paired data or constructed synthetically, i.e., just using the independent distribution $p(x_0, x_T) = p(x_0)p(x_T)$.
Bridge Matching aims to construct the diffusion that transforms source distribution $p(x_T)$ to target distribution $p(x_0)$ based on given coupling $p(x_0, x_T)$ and specified \textit{diffusion bridge}.

\textbf{Diffusion bridges.} Consider forward-time diffusion $Q$ called "Prior" on time horizon $[0, T]$ represented by the stochastic differential equation (SDE):
\begin{gather}
    \textit{Prior } Q: \quad dx_t = f(x_t, t)dt + g(t) dw_t, \label{eq:prior}
    \\
    f(x_t,t): \mathbb{R}^{D} \times [0, T] \rightarrow \mathbb{R}^{D}, \quad g(t): [0,T] \rightarrow \mathbb{R}^{D},
    \nonumber
\end{gather}
where $f(x_t,t)$ is a drift function, $g(t)$ is the noise schedule function and $dw_t$ is the differential of the standard Wiener process. By $q(x_t|x_s)$, we denote the transition probability density of prior process $Q$ from time $s$ to time $t$. \textit{Diffusion bridge} is a conditional process $Q_{|x_0, x_T}$, which is obtained by pinning down starting and ending points $x_0$ and $x_T$. This diffusion bridge can be derived from prior process $Q$ using the Doob-h transform \citep{doob1984classical}:
\begin{gather}
    \textit{Diffusion Bridge } Q_{|x_0, x_T}: x_0, x_T \text{ are fixed}, \label{eq:diffusion_bridge}
    \\
    dx_t = \{f(x_t, t)dt + g^2(t) \nabla_{x_t} \log q(x_T|x_t)\}dt + g(t) dw_t,
    \nonumber
\end{gather}
For this diffusion bridge we denote the distribution at time $t$ of the diffusion bridge $Q_{|x_0, x_T}$ by $q(x_t|x_0, x_T)$.

% \vspace{3.5mm}
\textbf{Mixture of bridges.} Bridge Matching procedure starts with creating a \textit{mixture of bridges} process $\Pi$. This process is represented as follows:
\begin{gather}
    \textit{Mixture of Bridges } \Pi:  
    \nonumber
    \\
    \Pi(\cdot) = \int Q_{|x_0, x_T}(\cdot) p(x_0, x_T) dx_0dx_T.
    \label{eq:mixture-of-bridges}
\end{gather}
Practically speaking, the definition \eqref{eq:mixture-of-bridges} means that to sample from a mixture of bridges $\Pi$, one first samples the pair $(x_0, x_T) \sim p(x_0, x_T)$ from data coupling and then samples trajectory from the bridge $Q_{|x_0, x_T}(\cdot)$. 

\textbf{Bridge Matching problem.} The mixture of bridges $\Pi$ cannot be used for data-to-data translation since it requires first to sample a pair of data and then just inserts the trajectory. In turn, we are interested in constructing a diffusion, which can start from any sample $x_T \sim p(x_T)$ and gradually transform it to $x_0 \sim p(x_0)$. This can be done by solving the Bridge Matching problem \citep[Proposition 2]{shi2023diffusion}
\begin{gather}
    \textit{Bridge Matching problem:}  \label{eq:markovian-proj}
    \\
    \text{BM}(\Pi) \eqdef \argmin_{M \in \mathcal{M}} \text{KL}(\Pi||M),
    \nonumber
\end{gather}
where $\mathcal{M}$ is the set of Markovian processes associated with some SDE and $\text{KL}(\Pi||M)$ is the KL-divergence between a constructed mixture of bridges $\Pi$ and diffusion $M$. It is known that the solution of Bridge Matching is the reversed-time SDE \citep[Proposition 9]{shi2023diffusion}:
\begin{gather}
    \textit{The SDE of Bridge Matching solution}:  
    \label{eq:diffusio-markovian-proj}
    \\
    dx_t = \{f_t(x_t) - g^2(t)v^*(x_t, t)\} dt + g(t) d\bar{w}_t,
    \nonumber
    \\
    x_T \sim p_T(x_T),
    \nonumber
\end{gather}
where $\bar{w}$ is a standard Wiener process when time $t$ flows backward from $t=T$ to $t=0$, and $dt$ is an infinitesimal negative timestep. The drift function $v^*$ is obtained solving the following problem \citep{shi2023diffusion, liu20232}:
\begin{gather}
    \textit{Bridge Matching problem with a tractable objective:} \label{eq:bridge-matching-problem}
    \\
    \min_{\phi} \mathbb{E}_{x_0, t, x_t} \big[\| v_{\phi}(x_t, t) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big], 
    \nonumber
    \\
    (x_0, x_T) \sim p(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
Time moment $t$ here is sampled according to the uniform distribution on the interval $[0, T]$.

\textbf{Relation Between Flow and Bridge Matching.} The Flow Matching \citep{liu2023flow, lipman2023flow} can be seen as the limiting case $\sigma \rightarrow 0$ of the Bridge Matching for particular example see \citep[Appendix A.1]{shi2023diffusion}.
\vspace{-2mm}
\subsection{Augmented (Conditional) Bridge Matching and Denoising Diffusion Bridge Models (DDBM)}\label{sec:augmented-bridge-matching}
\vspace{-2mm}
For a given coupling $p(x_0, x_T) = p(x_0|x_T)p(x_T)$, one can use an alternative approach to build a data-to-data diffusion. 
Consider a set of Bridge Matching problems indexed by $x_T$ between $p_0 = p(x_0|x_T)$ and $p(x_T) = \delta_{x_{T}}(x)$ (delta measure centered at $x_T$).
This approach is called Augmented Bridge Matching \citep{de2023augmented}.
The key difference of this version in practice is that it introduces the condition of the drift function $v^*(x_t, t, x_T)$ on the starting point $x_T$ in the reverse time diffusion \eqref{eq:diffusio-markovian-proj}:
$$
    dx_t = \{f_t(x_t) - g^2(t) v^*(x_t, t, x_T)\} dt + g(t) d\bar{w}_t.
$$
The drift function $v^*$ can be recovered almost in the same way just by the addition of this condition on $\textcolor{MyRed}{x_T}$:
\begin{gather}
    \textit{\textcolor{MyRed}{Augmented (Conditional)} Bridge Matching Problem.}
    \nonumber
    \\
    \min_{\phi} \mathbb{E}_{x_0, t, x_t, \textcolor{MyRed}{x_T}} \big[\| v_{\phi}(x_t, t, \textcolor{MyRed}{x_T}) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big], 
    \nonumber
    \\
    (x_0, x_T) \sim p(x_0, x_T), \text{and } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
Since the difference is the addition of conditioning on $x_T$, we call this approach \textit{Conditional Bridge Matching}.

\textbf{Relation to DDBM.} As was shown in the Augmented Bridge Matching \citep{de2023augmented}, the conditional Bridge Matching is equivalent to the Denoising Diffusion Bridge Model (DDBM) proposed in \citep{zhou2024denoising}. The difference is that in DDBM, the authors learn the score function of $s(x_t, x_T, t)$ conditioned on $x_T$ of a process for which $x_0 \sim p(x_0|x_T)$ and $q(x_t) \sim q(x_t|x_0, x_T)$:
% \begin{gather}
%     x_0 \sim p(x_0|x_T),
%     \nonumber
%     \\
%     dx_t = \{f(x_t, t)dt + g^2(t) \nabla_{x_t} \log q(x_T|x_t)\}dt + g(t)\} dw_t.
%     \nonumber
% \end{gather}
Then, it is combined with the drift of forward Doob-h transform \eqref{eq:diffusio-markovian-proj} to get the reverse SDE drift $v(x_t, t, x_T)$:
\begin{gather}
    v(x_t, t, x_T) = s(x_t, x_T, t) - \nabla_{x_t} \log q(x_T|x_t),
    \nonumber
    \\
    dx_t = \{f(x_t, t)dt -g^2(t) v(x_t, t, x_T)\} dt + g(t) d\bar{w}_t,
    \nonumber
\end{gather}
or reverse probability flow ODE drift:
\begin{gather}
    v_{\text{ODE}}(x_t, t, x_T) = \frac{1}{2}s(x_t, x_T, t) - \nabla_{x_t} \log q(x_T|x_t),
    \nonumber
    \\
    dx_t = \{f(x_t, t)dt - g^2(t) v_{\text{ODE}}(x_t, t, x_T)\} dt,
    \nonumber
\end{gather}
which is used for consistency distillation in \citep{he2024consistency}.
\vspace{-2mm}
\subsection{Practical aspects of Bridge Matching}\label{sec:practical-aspects-bm}
\vspace{-2mm}
\textbf{Priors used in practice.}
In practice \citep{he2024consistency, zhou2023denoising, zheng2024diffusion}, the drift of the prior process is usually set to be $f(x_t, t) = f(t)x_t$, i.e, it depends linearly on $x_t$. For this process the transitional distribution $q(x_t|x_0) = \mathcal{N}(x_t|\alpha_t x_0, \sigma^2_t I)$ is Gaussian, where:
$$
    f(t) = \frac{d\log \alpha_t}{dt}, \quad g^2(t) = \frac{d\sigma^2_t}{dt} - 2\frac{d\log \alpha_t}{dt}\sigma^2_t.
$$
The bridge process distribution is also a Gaussian $q(x_t|x_0, x_T) = \mathcal{N}(x_T|a_tx_T + b_tx_0, c_t^2I)$ with coefficients:
\begin{gather}
    a_t = \frac{\alpha_t}{\alpha_T} \frac{\text{SNR}_T}{\text{SNR}_t}, b_t = \alpha_t \left(1 - \frac{\text{SNR}_T}{\text{SNR}_t}\right), 
    \nonumber
    \\
    c_t^2 = \sigma_t^2 \left(1 - \frac{\text{SNR}_T}{\text{SNR}_t}\right),
    \nonumber
\end{gather}
where $\text{SNR}_t = \frac{\alpha_t^2}{\sigma_t^2}$ is the signal-to-noise ratio at time t. 

\textbf{Data prediction reparameterization.} The regression target of the loss function \eqref{eq:bridge-matching-problem} for the priors with the drift $v(x_t, t)$ is given by $\nabla_{x_t}\log q(x_t|x_0) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2}$. Hence, one can use the parametrization $v(x_t, t, \textcolor{MyRed}{x_T}) = -\frac{x_t - \alpha_t \widehat{x}_0(x_t, t, \textcolor{MyRed}{x_T})}{\sigma_t^2}$ and solve the equivalent problem:
\begin{gather}
    \textit{Reparametrized \textcolor{MyRed}{(Conditional)} Bridge Matching problem:}
    \nonumber
    \\
    \min_{\phi} \mathbb{E}_{x_0, t, x_t, \textcolor{MyRed}{x_T}} \big[ \lambda(t) \| \widehat{x}^{\phi}_0(x_t, t, \textcolor{MyRed}{x_T}) - x_0 \|^2 \big],
    \label{eq:conditional-bridge-matching-problem}
    \\
    (x_0, x_T) \sim p(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T),
    \nonumber
\end{gather}
where $\lambda(t)$ is any positive weighting function. Note that $\textcolor{MyRed}{x_T}$ is used only for the Conditional Bridge Matching model. 
% Note that $v(x_t, t, \textcolor{MyRed}{x_T})$ and $\widehat{x}_0(x_t, t, \widehat{x_T})$ can be expressed through each other.
\vspace{-2mm}
\subsection{Difference Between Acceleration of Unconditional and Conditional DBMs}\label{sec:diff-between-augmented-and-not}
\vspace{-2mm}
Since both conditional and unconditional approaches learn drifts of SDEs, they share the same problems of long inference.
However, these models significantly differ in the approaches that can accelerate them. 
The source of this difference is that Conditional Bridge Matching considers the set of problems of reversing diffusion, which gradually transforms distribution $p(x_0|x_T)$ to the fixed point $x_T$. 
Furthermore, the forward diffusion has simple analytical drift and Gaussian transitional kernels. 
Thanks to it, for each $x_T$ to sample, one can use the probability flow ODE and ODE-solvers or hybrid solvers to accelerate sampling \citep{zhou2024denoising} or use consistency distillation of bridge models \citep{he2024consistency}. 
Another beneficial property is that one can consider a non-Markovian forward process to develop a more efficient sampling scheme proposed in DBIM \citep{zheng2024diffusion} similar to Denoising Diffusion Implicit Models \citep{song2021denoising}.
However, in the Unconditional Bridge Matching problem, the forward diffusion process, which maps $p(x_0)$ to $p(x_T)$ without conditioning on specific point $x_T$, is unknown. Hence, the abovementioned methods cannot be used to accelerate this model type.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/method_new.png}
    \vspace{-4mm}
    \caption{\textbf{Overview of our method Inverse Bridge Matching Distillation (IBMD).} 
        The goal is to distill a trained \textcolor{MyRed}{(Conditional)} Bridge Matching model into a generator \( G_{\theta}(z, x_T) \), which learns to produce samples using the corrupted data \( p(x_T) \). Generator  \( G_{\theta}(z, x_T) \) defines the coupling $p_{\theta}(x_0, x_T) = p_{\theta}(x_0|x_T)p(x_T)$ and we aim to learn the generator in such way that Bridge Matching with $p_{\theta}(x_0, x_T)$ produces the same \textcolor{MyRed}{(Conditional)} Bridge Matching model $\widehat{x}_0^{\phi} = \widehat{x}_0^{\theta}$. To do so, we learn a bridge model $\widehat{x}_0^{\phi}$ using coupling $p_{\theta}$ in the same way as the teacher model was learned. Then, we use our novel objective given in Theorem~\ref{thm:main-theorem} to update the generator model $G_{\theta}$.
        % (see Figure~\ref{fig:bm-scheme}). Then, we use our new reformulated $\mathcal{L}_{\text{student}}$ objective in Theorem~\ref{thm:main-theorem} to update the generator model $G_{\theta}$.
        }
    \label{fig:visual-abstract}
    \vspace{-4mm}
\end{figure*}
\vspace{-2mm}
\section{IBMD: Inverse Bridge Matching Distillation}
\vspace{-2mm}
This section describes our proposed \underline{ universal approach} to distill the both Unconditional and \textcolor{MyRed}{(Conditional)} Bridge Matching models $v^*$ (called the teacher model) into a few-step generator \textit{using only the corrupted data} $p_T(x_T)$. The key idea of our method is to consider the inverse problem of finding the mixture of bridges $\Pi_{\theta}$, for which Bridge Matching provides the solution $v_{\theta}$ with the same drift as the given teacher model $v^*$. We formulate this task as the optimization problem (\wasyparagraph\ref{sec:inverse-problem}). However, gradient methods cannot solve this optimization problem directly due to the absence of tractable gradient estimation. To avoid this problem, we prove a theorem that allows us to reformulate the inverse problem in the tractable objective for gradient optimization (\wasyparagraph\ref{sec:main-theorem}). Then, we present the fully analogical results for the \textcolor{MyRed}{Conditional} Bridge Matching case in (\wasyparagraph\ref{sec:method-conditional}). Next, we present the multistep version of distillation (\wasyparagraph\ref{sec:multistep}) and the final algorithm (\wasyparagraph\ref{sec:algorithm}). We provide the \underline{proofs} for all considered theorems and propositions in Appendix~\ref{app:proofs}.
\vspace{-2mm}
\subsection{Bridge Matching Distillation as Inverse Problem}\label{sec:inverse-problem}
\vspace{-2mm}
% Bridge Matching utilize mixture of bridges $\Pi$ to build the SDE transforming data from $p_T(x_T)$ to $p_0(x_0)$ using the data coupling $p(x_0, x_T)$ and the diffusion bridge $Q_{|x_0, x_T}$. 
In this section, we focus on the derivation of our distillation method for the case of Unconditional Bridge Matching.
Consider the fitted teacher model $v^*(x_t, t)$, which is an SDE drift of some process ${M^* = \text{BM}(\Pi^*)}$, where $\Pi^*$ constructed using some data coupling $p^*(x_0, x_T) = p^*(x_0|x_T)p(x_T)$. 
We parametrize $p_{\theta}(x_0, x_T) = p_{\theta}(x_0|x_T)p(x_T)$ and aim to find such $\Pi_{\theta}$ build on $p_{\theta}(x_0, x_T)$, that $\text{BM}(\Pi^*) = \text{BM}(\Pi_{\theta})$. 
In practice, we parametrize $p_{\theta}(x_0|x_T)$ by the stochastic generator $G_{\theta}(x_T, z), z \sim \mathcal{N}(0, I)$, which generates samples based on input $x_T \sim p(x_T)$ and the gaussian noise $z$. 
Now, we formulate the inverse problem as follows:
\begin{gather}\label{eq:initial-inverse-problem}
    \min_{\theta} \text{KL}(\text{BM}(\Pi_{\theta})||M^*). 
\end{gather}
Note, that since the objective \eqref{eq:initial-inverse-problem} is the KL-divergence between $\text{BM}(\Pi_{\theta})$ and $M^*$, it is equal to $0$ if and only if $\text{BM}(\Pi_{\theta})$ and $M^*$ coincide. Furthermore, using the disintegration and Girsanov theorem \citep{vargas2021solving, pavon1991free}, we have the following result:
\begin{proposition}[Inverse Bridge Matching problem]\label{thm:inverse-bm}
The inverse problem \eqref{eq:initial-inverse-problem} is equivalent to
\begin{gather}\label{eq:constrained-inverse-bm}
    \min_{\theta} \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t) - v^*(x_t, t)||^2\big], \quad \text{s.t.}
    \\
    v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0} \big[\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big], 
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T),
    \nonumber
\end{gather}
where $\lambda(t)$ is any positive weighting function.
\end{proposition}
Thus, this is the \textbf{constrained} problem, where the drift $v$ is the result of Bridge Matching for coupling $p_{\theta}(x_0, x_T)$ parametrized by the generator $G_{\theta}$. Unfortunately, there is no clear way to use this objective efficiently for optimizing a generator $G_{\theta}$ since it would require gradient backpropagation through the argmin of the Bridge Matching problem.
\vspace{-2mm}
\subsection{Tractable objective for the inverse problem}\label{sec:main-theorem}
\vspace{-2mm}
In this section, we introduce our new \textbf{unconstrained} reformulation for the inverse problem \eqref{eq:constrained-inverse-bm}, which admits direct optimization using gradient methods:
\begin{theorem}[Tractable inverse problem reformulation] \label{thm:main-theorem} 
The constrained inverse problem \eqref{eq:constrained-inverse-bm} w.r.t $\theta$ is equivalent to the unconstrained optimization problem:
\begin{gather}
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| v^*(x_t, t) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] -
    \nonumber
    \\
    \min_{\phi} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| v_{\phi}(x_t, t) -\nabla_{x_t} \log q(x_t|x_0) \|^2 \big] \Big],
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T),
    \nonumber
\end{gather}
Where the constraint in the original inverse problem \eqref{eq:constrained-inverse-bm} is relaxed by introducing the inner bridge matching problem.
\end{theorem}
This is the general result that can applied with any diffusion bridge. For the priors with with drift $f(x_t, t) = f(t)x_t$, we present its reparameterized version.
\begin{proposition}[Reparameterized tractable inverse problem] \label{thm:reparametrized-main-theorem} 
Using the reparameterization (\wasyparagraph\ref{sec:practical-aspects-bm}) for the prior with the linear drift $f(x_t, t) = f(t)x_t$, the inverse problem in Theorem~\ref{thm:main-theorem} is equivalent to:
\begin{gather}
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| \widehat{x}_0^*(x_t, t) -  x_0 \|^2 \big] -
    \nonumber
    \\
    \! \min_{\phi} \mathbb{E}_{x_t, t, x_0}  \big[\lambda(t) \| \widehat{x}_0^{\phi}(x_t, t) - x_0 \|^2 \big] dt\Big],
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
\end{proposition}
% We denote the used objective as follows:
% \begin{gather}
%     \mathcal{L}_{\text{student}} = \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| \widehat{x}_0^*(x_t, t) -  x_0 \|^2 \big] -
%     \nonumber
%     \\
%     \! \min_{\widehat{x}_0} \mathbb{E}_{x_t, t, x_0}  \big[\lambda(t) \| \widehat{x}_0(x_t, t) - x_0 \|^2 \big] dt\Big],
%     \nonumber
% \end{gather}
% and use it in the visual abstract of our method in Figure~\ref{fig:visual-abstract}.
The key difference of the reformulated problem is that it admits clear gradients of generator $G_{\theta}$, which can be calculated automatically by using the autograd techniques. 
Thanks to the unconstrained reformulation of an inverse problem given by Theorem~\ref{thm:main-theorem}, it can now be solved directly by parameterizing $\widehat{x}_0(x_t, t)$ by a neural network. 
\vspace{-2mm}
\subsection{Distillation of conditional Bridge Matching models}\label{sec:method-conditional}
\vspace{-2mm}
Since Conditional Bridge Matching is, in essence, a set of Unconditional Bridge Matching problems for each $x_T$ (\wasyparagraph\ref{sec:augmented-bridge-matching}), the analogical results hold just by adding the conditioning on $x_T$ for $v$, i.e., using $v(x_t, t, \textcolor{MyRed}{x_T})$ or $\widehat{x}_{0}$, i.e. using $\widehat{x}_0(x_t, t, \textcolor{MyRed}{x_T})$. Here, we provide the final reparametrized formulation, which we use in our experiments:
\begin{theorem}[Reparameterized tractable inverse problem for conditional bridge matching]\label{thm:conditional-inverse-problem}
\begin{gather}
    \min_{\theta}   \Big[\mathbb{E}_{x_t, t, x_0, \textcolor{MyRed}{x_T}} \big[\lambda(t) \| \widehat{x}_0^*(x_t, t, \textcolor{MyRed}{x_T}) -  x_0 \|^2 \big] -
    \label{eq:reformulated-inverse-problem}
    \\
    \! \min_{\phi} \mathbb{E}_{x_t, t, x_0, \textcolor{MyRed}{x_T}}  \big[\lambda(t) \| \widehat{x}_0^{\phi}(x_t, t, \textcolor{MyRed}{x_T}) - x_0 \|^2 \big]\Big],
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
where $\lambda(t)$ is some positive weight function.
\end{theorem}
To use it in practice, we parameterize $\widehat{x}_0(x_t, t, \textcolor{MyRed}{x_T})$ by a neural network with an additional condition on $\textcolor{MyRed}{x_T}$.
\vspace{-2mm}
\subsection{Algorithm}\label{sec:algorithm}
\vspace{-2mm}
We provide a one-step Algorithm~\ref{alg:ibmd} that solves the inverse Bridge Matching problem in the reformulated version that we use in our experiments. We provide a visual abstract of it in Figure~\ref{fig:visual-abstract}.
Note that a teacher in the velocity parameterization $v^*(x_t, t)$ can be easily reparameterized (\wasyparagraph\ref{sec:practical-aspects-bm}) in $x_0$-prediction model using $\widehat{x}^*(x_t, t) = \frac{\sigma_t^2 v^*(x_t, t) + x_t}{\alpha_t}$.
\vspace{-2mm}
\subsection{Mulitistep distillation}\label{sec:multistep}
\vspace{-2mm}
We also present a multistep modification of our distillation technique if a one-step generator struggles to distill the models, e.g., in inpainting setups, where the corrupted image $x_T$ contains less information. Our multistep technique is inspired by similar approaches used in diffusion distillation methods \citep[DMD]{yin2024improved} and aims to avoid training/inference distribution mismatch.

We choose $N$ timesteps $\{0 < t_1 < t_2 < ... < t_N = T\}$ and add additional time input to our generator $G_{\theta}(x_t, z, t)$. For the conditional Bridge Matching case, we also add conditions on $x_T$ and use $G_{\theta}(x_t, z, t, \textcolor{MyRed}{x_T})$. To perform inference, we alternate between getting prediction from the generator $\widetilde{x}_0 = G_{\theta}(x_t, z, t)$ and using posterior sampling $q(x_{t_{n-1}}|\widetilde{x}_0, x_{t_{n}})$ given by the diffusion bridge. To train the generator in the multistep regime, we use the same procedure as in one step except that to get input $x_t$ for intermediate times $t_n < t_N$, we first perform inference of our generator to get $x_0$ and then use bridge $q(x_t|\widetilde{x}_0, x_T)$.
% (see specific details used in experiments in Appendix~\ref{app:experimental-details}).
\vspace{-2mm}
\section{Related work}
\vspace{-2mm}
\textbf{Diffusion Bridge Models (DBMs) acceleration.} 
Unlike a wide scope of acceleration methods developed for classical diffusion and flow models, only a few approaches were developed for DBM acceleration. 
For the conditional DBMs, acceleration methods include more advanced samplers \citep{zheng2024diffusion, wang2024implicit} based on reformulated forward diffusion process as a non-markovian process inspired by Denoising Diffusion Implicit Models \citep{song2021denoising}. 
Also, there is a distillation method based on the distilling probability-flow ODE into a few steps using consistency models \cite{he2024consistency}. 
However, for theoretical reasons \citep[Section 3.4]{he2024consistency}, consistency models for Diffusion Bridges cannot be distilled into one-step generators. 
Unlike these existing works, our method is applicable to both conditional and unconditional types of DBMs and can distill models into the one-step generator.

\textbf{Related diffusion and flow models distillation techniques.} 
Among the methods developed for the distillation of classical diffusion and flow models, the most related to our work are methods based on simultaneous training of few-step generators and auxiliary "fake" model, that predict score or drift function for the generator \citep{yin2024one, yin2024improved, zhou2024score, huang2024flow}. Unlike these approaches, we consider the distillation of Diffusion Bridge Models - the generalization of flow and diffusion models.
\vspace{0mm}
\begin{algorithm}[h]
    \caption{Inverse Bridge Matching Distillation (IBMD)}\label{alg:ibmd}
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \Input{Teacher network $\widehat{x}_0^{*}: \mathbb{R}^{D} \times [0, T] \times \textcolor{MyRed}{\mathbb{R}^{D}} \rightarrow \mathbb{R}^{D}$; \\
    Bridge $q(x_t|x_0, x_T)$ used for training $x^*$; \\
    Generator network $G_{\theta}: \mathbb{R}^{D} \times \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}$; \\
    Bridge network $\widehat{x}_0^{\phi}: \mathbb{R}^{D} \times[0, T] \times \textcolor{MyRed}{\mathbb{R}^{D}} \rightarrow \mathbb{R}^{D}$; \\
    Input distribution $p(x_T)$ accessible by samples; \\
    Weights function $\lambda(t): [0, T] \rightarrow \mathbb{R}^+$; \\
    Batch size $N$; Number of student iterations $K$; \\
    Number of bridge iterations $L$. \\
    }
    \Output{Learned generator $G_{\theta}$ of coupling $p_{\theta}(x_0, x_T)$ for which Bridge Matching outputs drift $v \approx v^*$.}
    // Conditioning on $\textcolor{MyRed}{x_T}$ is used only for distillation of Conditional Bridge Matching models. \\
    \For{$k = 1$ \KwTo $K$}{
        \For{$l = 1$ \KwTo $L$}{
            Sample batch $x_T \sim p(x_T)$ \\
            Sample batch of noise $z \sim \mathcal{N}(0, I)$ \\
            $x_0 \leftarrow G_{\theta}(x_T, z)$ \\
            Sample time batch $t \sim U[0, T]$ \\
            Sample batch $x_t \sim q(x_t|x_0, x_T)$ \\
            $\widehat{\mathcal{L}}_{\phi} \leftarrow \frac{1}{N}\sum_{n=1}^N \big[\lambda(t)||\widehat{x}_0^{\phi}(x_t, t, \textcolor{MyRed}{x_T}) - x_0||^2\big]_{n}$ \\
            Update $\phi$ by using $\frac{\partial \widehat{\mathcal{L}}_{\phi}}{\partial \phi}$
        }
        Sample batch $x_T \sim p(x_T)$ \\
        Sample batch of noise $z \sim \mathcal{N}(0, I)$ \\
        $x_0 \leftarrow G_{\theta}(x_T, z)$ \\
        Sample time batch $t \sim U[0, T]$ \\
        Sample batch $x_t \sim q(x_t|x_0, x_T)$ \\
        $\widehat{\mathcal{L}}_{\theta} \! \leftarrow \!\! \frac{1}{N}\! \sum_{n=1}^N \! \big[\lambda(t)||\widehat{x}_0^{*}(x_t, t, \textcolor{MyRed}{x_T}) -  x_0||^2 - \\ 
        \lambda(t)||\widehat{x}_0^{\phi}(x_t, t, \textcolor{MyRed}{x_T})  - x_0||^2 \big]_{n}$ \\
        Update $\theta$ by using $\frac{\partial \widehat{\mathcal{L}}_{\theta}}{\partial \theta}$
    }
\end{algorithm}
\input{sec/5_expeiments}
\vspace{-2mm}
\section{Discussion}
\vspace{-2mm}
\textbf{Potential impact.}
DBMs 
% provide a promising framework 
are used for data-to-data translation in different 
% data 
domains, including images, audio, and biological data. Our distillation technique provides a universal and efficient way to address the long inference of DBMs, making them more affordable in practice.

\vspace{-1mm}
\textbf{Limitations.} 
Our method alternates between learning an additional bridge model and updating the student, which may be computationally expensive. Moreover, the student optimization requires backpropagation through the teacher, additional bridge, and the generator network, making it $3$x time more memory expensive than training the teacher.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proofs}\label{app:proofs}
Since all our theorems, propositions and proofs for the inverse Bridge Matching problems which is formulated for the already trained teacher model using some diffusion bridge, we assume all corresponding assumptions used in Bridge Matching. Extensive overview of them can be found in \citep[Appendix C]{shi2023diffusion}.

\begin{proof}[Proof of Proposition~\ref{thm:inverse-bm}]
Since both $\text{BM}(\Pi_{\theta})$ and $M^*$ given by reverse-time SDE and the same distribution $p_T(x_T)$ the KL-divergence expressed in the tractable form using the disintegration and Girsanov theorem \citep{vargas2021solving, pavon1991free}:
\begin{gather}   
    \text{KL}(\text{BM}(\Pi_{\theta})||M^*) = \mathbb{E}_{x_t, t}\big[g^2(t)||v(x_t, t) - v^*(x_t, t)||^2\big],
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), t \sim U([0, T]),  x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
The expectation is taken over the marginal distribution $p(x_t)$ of $\Pi_{\theta}$ since it is the same as for $\text{BM}(\Pi_{\theta})$ \citep[Proposition 2]{shi2023diffusion}. In turn, the drift $v(x_t, t)$ is the drift of Bridge Matching using $\Pi_{\theta}$, i.e. $\text{BM}(\Pi_{\theta})$:
\begin{gather}
    v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0} \big[\| v'(x_t, t) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big], 
    \nonumber
    \\
    \quad (x_0, x_T) \sim p_{\theta}(x_0, x_T), t \sim U([0, T]),  x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
Combining this, the inverse problem can be expressed in a more tractable form:
\begin{gather}
    \min_{\theta} \mathbb{E}_{x_t, t} \big[g^2(t)||v(x_t, t) - v^*(x_t, t)||^2\big], \quad \text{s.t.}
    \label{eq:final-inverse-problem}
    \\
    v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0} \big[\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] dt, 
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
We can add positive valued weighting function $\lambda(t)$ for the constraint:
$$
v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] dt, 
$$
since it is the MSE regression and its solution is conditional expectation for any weights given by:
$$
    v(x_t, t) = \mathbb{E}_{x_0|x_t, t}\big[\nabla_{x_t} \log q(x_t|x_0)].
$$
We can add positive valued weighting function $\lambda(t)$ for the main functional:
$$
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t) - v^*(x_t, t)||^2\big],
$$
since it does not change the optimum value (which is equal to $0$) and optimal solution, which is the mixture of bridges with the same drift as the teacher model.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:main-theorem}]
Consider inverse bridge matching optimization problem:
\begin{gather}
    \min_{\theta} \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t) - v^*(x_t, t)||^2\big], \quad \text{s.t.}
    \\
    v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0} \big[\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big], 
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}

First, note that since $v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0} \big[\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big]$, i.e. minimizer of MSE functional it is given by conditional expectation as:
\begin{eqnarray}
\label{eq: obvious}
    v(x_t, t) = \mathbb{E}_{x_0|x_t, t} \big[\nabla_{x_t} \log q(x_t|x_0) | x_t, t \big].
\end{eqnarray}
Then note that:
\begin{gather}
    \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] = 
    \nonumber
    \\
    \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] = 
    \nonumber
    \\
    \underbrace{\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) ||v(x_t, t)||^2\big]}_{\mathbb{E}_{x_t, t} \big[\lambda(t) ||v(x_t, t)||^2\big]} - 2 \mathbb{E}_{x_t, t, x_0} \big[ \lambda(t) \langle v(x_t,t), \nabla_{x_t} \log q(x_t|x_0)\rangle \big] +  \mathbb{E}_{x_t, t, x_0} \big[\lambda(t) ||\nabla_{x_t} \log q(x_t|x_0)||^2\big] = 
    \nonumber
    \\
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t)||^2\big] - 2 \mathbb{E}_{x_t, t} \Big[ \lambda(t) \left\langle v(x_t,t), \underbrace{\mathbb{E}_{x_0|x_t, t} \big[\nabla_{x_t} \log q(x_t|x_0)\big]}_{=v(x_t,t)}\right\rangle\Big] +  \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||\nabla_{x_t} \log q(x_t|x_0)||^2\big] =
    \nonumber
    \\
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t)||^2\big] - 2 \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t,t)||^2\big] +  \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||\nabla_{x_t} \log q(x_t|x_0)||^2\big] = 
    \nonumber
    \\
    - \mathbb{E}_{x_t, t}\big[\lambda(t)||v(x_t,t)||^2\big] +  \mathbb{E}_{x_t, t, x_0}\big[\lambda(t)||\nabla_{x_t} \log q(x_t|x_0)||^2\big].
\end{gather}
Hence, we derive that
\begin{eqnarray}
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t,t)||^2 \big]= \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||\nabla_{x_t} \log q(x_t|x_0)||^2 \big]- \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big].
    \nonumber
\end{eqnarray}
Now we use it to reformulate the initial objective:
\begin{gather}
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t) - v^*(x_t, t)||^2\big] = 
    \nonumber
    \\
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t)||^2\big] - 2 \mathbb{E}_{x_t, t} \big[\lambda(t) \left<v(x_t,t), v^*(x_t, t)\right>\big] + \mathbb{E}_{x_t, t} \big[\lambda(t)||v^*(x_t, t)||^2\big] =
    \nonumber
    \\
    \underbrace{\mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||\nabla_{x_t} \log q(x_t|x_0)||^2] - \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big]}_{=\mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t,t)||^2\big]} - 
    \nonumber
    \\
    2 \mathbb{E}_{x_t, t} \big[\lambda(t)\left\langle v(x_t,t), v^*(x_t, t)\right\rangle\big] + \mathbb{E}_{x_t, t} \big[\lambda(t)||v^*(x_t, t)||^2\big] =
    \nonumber
    \\
    \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)|| \nabla_{x_t} \log q(x_t|x_0)||^2\big] - 2 \mathbb{E}_{x_t, t} \big[\lambda(t)\left\langle v(x_t,t), v^*(x_t, t)\right\rangle\big] + \underbrace{\mathbb{E}_{x_t, t} \big[\lambda(t)||v^*(x_t, t)||^2\big]}_{\mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||v^*(x_t, t)||^2\big]} - 
    \nonumber
    \\
    \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] 
    \nonumber
\end{gather}
Therefore, we get:
\begin{gather}
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t) - v^*(x_t, t)||^2\big] = \nonumber\\
    \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)|| \nabla_{x_t} \log q(x_t|x_0)||^2\big] - 2 \mathbb{E}_{x_t, t} \big[\lambda(t)\left\langle v(x_t,t), v^*(x_t, t)\right\rangle\big] + \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||v^*(x_t, t)||^2\big] - 
    \nonumber
    \\
    \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big]
    \nonumber
\end{gather}
To complete the proof, we use the relation $v(x_t, t) = \mathbb{E}_{x_0|x_t, t} \big[\nabla_{x_t} \log q(x_t|x_0) | x_t, t \big]$ from Equation \ref{eq: obvious}. Integrating these components, we arrive at the final result:
\begin{gather}
    \mathbb{E}_{x_t, t} \big[\lambda(t)||v(x_t, t) - v^*(x_t, t)||^2\big] = 
    \nonumber
    \\
    \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)|| \nabla_{x_t} \log q(x_t|x_0)||^2\big] - 2 \mathbb{E}_{x_t, t} \big[\lambda(t)\left\langle \mathbb{E}_{x_0|x_t, t} \big[\nabla_{x_t} \log q(x_t|x_0) | x_t, t \big], v^*(x_t, t)\right\rangle\big] + \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||v^*(x_t, t)||^2\big] - 
    \nonumber
    \\
    \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] =
    \nonumber
    \\
    \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)|| \nabla_{x_t} \log q(x_t|x_0)||^2\big] - 2 \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\left\langle \nabla_{x_t} \log q(x_t|x_0), v^*(x_t, t)\right\rangle\big] + \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)||v^*(x_t, t)||^2\big] - 
    \nonumber
    \\
    \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] =
    \nonumber
    \\
    \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v^*(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] - \min_{v'} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t)\|v'(x_t, t) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big].
    \nonumber
\end{gather}
\end{proof}

\begin{proof}[Proof of Proposition~\ref{thm:reparametrized-main-theorem}]
Consider the problem from Proposition~\ref{thm:main-theorem}:
\begin{gather}
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| v^*(x_t, t) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] -
    \min_{\phi} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| v_{\phi}(x_t, t) -\nabla_{x_t} \log q(x_t|x_0) \|^2 \big] \Big],
    \nonumber
\end{gather}

For the priors with the drift $f(t)x$ the regression target is $\nabla_{x_t}\log q(x_t|x_0) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2}$. 
Hence one can use the parametrization $v(x_t, t) = -\frac{x_t - \alpha_t \widehat{x}_0(x_t, t)}{\sigma_t^2}$
% $\nabla_{x_t}\log q(x_t|x_0) = -\frac{x_t - \alpha_t x_0}{\sigma_t^2}$. Hence one can use the parametrization $v(x_t, t) = -\frac{x_t - \alpha_t \widehat{x}_0(x_t, t)}{\sigma_t^2}$
We use reparameterization of both $v^*$ and $v_{\phi}$ given by:
$$
    v^*(x_t, t) = -\frac{x_t - \alpha_t \widehat{x}^*_0(x_t, t)}{\sigma_t^2}, \quad
    v_{\phi}(x_t, t) = -\frac{x_t - \alpha_t \widehat{x}^{\phi}_0(x_t, t)}{\sigma_t^2}
$$
and get:
\begin{gather}
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| v^*(x_t, t) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] -
    \min_{\phi} \mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| v_{\phi}(x_t, t) -\nabla_{x_t} \log q(x_t|x_0) \|^2 \big] \Big] =
    \nonumber
    \\
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0} \big[\underbrace{\lambda(t)\frac{\alpha_t^2}{\sigma_t^4}}_{\eqdef \lambda'(t)} \| \widehat{x}_0^*(x_t, t) - x_0 \|^2 \big] -
    \min_{\phi} \mathbb{E}_{x_t, t, x_0} \big[\underbrace{\lambda(t)\frac{\alpha_t^2}{\sigma_t^4}}_{\eqdef \lambda'(t)} \|\widehat{x}^{\phi}_0(x_t, t) - x_0 \|^2 \big] \Big] =
    \nonumber
    \\
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda'(t) \| \widehat{x}_0^*(x_t, t) - x_0 \|^2 \big] -
    \min_{\phi} \mathbb{E}_{x_t, t, x_0} \big[\lambda'(t) \| \widehat{x}^{\phi}_0(x_t, t) - x_0 \|^2 \big] \Big],
    \nonumber
\end{gather}
where $\lambda'(t)$ is just another positive weighting function.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:conditional-inverse-problem}]
    In a fully analogical way, as for the unconditional case we consider the set of the Inverse Bridge Matching problems indexes by $\textcolor{MyRed}{x_T}$:
    \begin{gather}
    \big\{\min_{\theta} \big[\text{KL}(\text{BM}(\Pi_{\theta| \textcolor{MyRed}{x_T}})||M^{*}_{|\textcolor{MyRed}{x_T}})\big]\big\}_{\textcolor{MyRed}{x_T}},
    \nonumber
    \end{gather}
    where $M^{*}_{|\textcolor{MyRed}{x_T}}$ is a result of Bridge Matching conditioned on $\textcolor{MyRed}{x_T}$ and $\Pi_{\theta| \textcolor{MyRed}{x_T}}$ is a Mixture of Bridges for each $x_T$ constructed using bridge $q(x_t|x_0, x_T)$ and coupling $p_{\theta}(x_0|x_T)\delta_{x_T}(x)$. 
    
     By employing the same reasoning as in the proof of Proposition~\ref{thm:inverse-bm}, the inverse problem can be reformulated as follows:
    \begin{gather}
    \min_{\theta} \mathbb{E}_{x_t, t, \textcolor{MyRed}{x_T}} \big[g^2(t)||v(x_t, t, \textcolor{MyRed}{x_T}) - v^*(x_t, t, \textcolor{MyRed}{x_T})||^2\big], \quad \text{s.t.}
    \nonumber
    \\
    v = \argmin_{v'} \mathbb{E}_{x_t, t, x_0, \textcolor{MyRed}{x_T}} \big[\|v'(x_t, t, \textcolor{MyRed}{x_T}) -  \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] dt, 
    \nonumber
    \\
    (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
    \nonumber
\end{gather}
    Following the proof of Theorem~\ref{thm:main-theorem}, we obtain a tractable formulation incorporating a weighting function:
    \begin{gather}
    \min_{\theta} \Big[\mathbb{E}_{x_t, t, x_0, \textcolor{MyRed}{x_T}} \big[\lambda(t) \| v^*(x_t, t, \textcolor{MyRed}{x_T}) - \nabla_{x_t} \log q(x_t|x_0) \|^2 \big] -
    \nonumber
    \\
    \min_{\phi} \mathbb{E}_{x_t, t, x_0, \textcolor{MyRed}{x_T}} \big[\lambda(t) \| v_{\phi}(x_t, t, \textcolor{MyRed}{x_T}) -\nabla_{x_t} \log q(x_t|x_0) \|^2 \big] \Big].
    \nonumber
\end{gather}
    Utilizing the reparameterization under additional conditions (\wasyparagraph\ref{sec:practical-aspects-bm}), we obtain the following representations: 
    $$
        v^*(x_t, t, \textcolor{MyRed}{x_T}) = -\frac{x_t - \alpha_t \widehat{x}^*_0(x_t, t, \textcolor{MyRed}{x_T})}{\sigma_t^2}, \quad
        v_{\phi}(x_t, t, \textcolor{MyRed}{x_T}) = -\frac{x_t - \alpha_t \widehat{x}^{\phi}_0(x_t, t, \textcolor{MyRed}{x_T})}{\sigma_t^2}.
    $$
    Consequently, applying the proof technique from Proposition~\ref{thm:reparametrized-main-theorem}, we derive the final expression:
    \begin{gather}
        \min_{\theta}   \Big[\mathbb{E}_{x_t, t, x_0} \big[\lambda(t) \| \widehat{x}_0^*(x_t, t, \textcolor{MyRed}{x_T}) -  x_0 \|^2 \big] -
        \! \min_{\phi} \mathbb{E}_{x_t, t, x_0}  \big[\lambda(t) \| \widehat{x}_0^{\phi}(x_t, t, \textcolor{MyRed}{x_T}) - x_0 \|^2 \big]\Big], 
        \nonumber
        \\
        (x_0, x_T) \sim p_{\theta}(x_0, x_T), \text{ } t \sim U([0, T]), \text{ } x_t \sim q(x_t|x_0, x_T).
        \nonumber
    \end{gather}
\end{proof}

\section{Experimental details}\label{app:experimental-details}
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        Task & Dataset & Teacher & NFE & $L$/$K$ ratio & LR & Grad Updates & Noise \\
        \hline
        $4 \times$ super-resolution (bicubic) & ImageNet & I$^2$SB & 1 & 5:1 & 5e-5 & 3000 & \checkmark \\
        $4 \times$ super-resolution (pool) & ImageNet & I$^2$SB & 1 & 5:1 & 5e-5 & 3000 & \checkmark \\
        JPEG restoration, QF $=5$ & ImageNet & I$^2$SB & 1 & 5:1 & 5e-5 & 2000 & \checkmark \\
        JPEG restoration, QF $=10$ & ImageNet & I$^2$SB & 1 & 5:1 & 5e-5 & 3000 & \checkmark \\
        Center-inpainting ($128 \times 128$) & ImageNet & I$^2$SB & 4 & 5:1 & 5e-5 & 2000 & \xmark \\
        Sketch to Image & Edges $\rightarrow$ Handbags & DDBM & 2 & 5:1 & 1e-5 & 300 & \checkmark \\
        Sketch to Image & Edges $\rightarrow$ Handbags & DDBM & 1 & 5:1 & 1e-5 & 14000 & \checkmark \\
        Normal to Image & DIODE-Outdoor & DDBM & 2 & 5:1 & 1e-5 & 500 & \checkmark \\
        Normal to Image & DIODE-Outdoor & DDBM & 1 & 5:1 & 1e-5 & 3700 & \checkmark \\
        Center-inpainting ($128 \times 128$) & ImageNet & DDBM & 4 & 1:1 & 3e-6 & 3000 & \checkmark \\
        \hline
    \end{tabular}
    \caption{Table entries specify experimental configurations: \textit{NFE} indicates multistep training (Sec. \wasyparagraph\ref{sec:multistep}); $L$/$K$ represents bridge/student gradient iteration ratios (Alg. \wasyparagraph\ref{sec:algorithm}); \textit{Grad Updates} shows \underline{student} gradient steps; \textit{Noise} notes stochastic pipeline incorporation.}
    \label{tab:exp_settings}
\end{table}
All hyperparameters are listed in Table \ref{tab:exp_settings}. We used batch size $256$ and ema decay $0.99$ for setups. For each setup, we started the student and bridge networks using checkpoints from the teacher models. In setups where the model adapts to noise: (1) We added extra layers for noise inputs (set to zero initially), (2) Noise was concatenated with input data before input it to the network. Datasets, code sources, and licenses are included in Table \ref{tab:license}.
\begin{table}[h]
    \vspace{-3mm}
    \centering
    \caption{The used datasets, codes and their licenses.}
    \label{tab:license}
    \begin{tabular}{|l|l|l|l|}
        \hline
        Name & URL & Citation & License \\
        \hline
        Edges$\rightarrow$Handbags & \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{GitHub Link} & \cite{isola2017image} & BSD \\
        DIODE-Outdoor & \href{https://diode-dataset.org/}{Dataset Link} & \cite{vasiljevic2019diode} & MIT \\
        ImageNet & \href{https://www.image-net.org}{Website Link} & \cite{deng2009imagenet} & \textbackslash \\
        Guided-Diffusion & \href{https://github.com/openai/guided-diffusion}{GitHub Link} & \cite{dhariwal2021diffusion} & MIT \\
        I$^2$SB & \href{https://github.com/NVlabs/I2SB}{GitHub Link} & \cite{liu20232} & CC-BY-NC-SA-4.0 \\
        DDBM & \href{https://github.com/alexzhou907/DDBM}{GitHub Link} & \cite{zhou2023denoising} & \textbackslash \\
        DBIM & \href{https://github.com/thu-ml/DiffusionBridge}{GitHub Link} & \cite{zheng2024diffusion} & \textbackslash \\
        \hline
    \end{tabular}
    \vspace{-5mm}
\end{table}
\subsection{Distillation of I$^2$SB models.}
\label{sec:i2sb exps details}
We extended the I$^2$SB repository (see Table \ref{tab:license}), integrating our distillation framework. The following sections outline the setups, adapted following the I$^2$SB. 

\textbf{Multistep implementation}
In this setup, we use the student model's full inference process during multistep training (Section \ref{sec:multistep}). This means that $x_0$ is generated with inferenced of the model $G_{\theta}$ through \textit{all} timesteps $\left(T = t_N, \dots, t_1 = 0\right)$ in the multistep sequence. The generated $x_0$ is subsequently utilized in the computation of the bridge $\widehat{\mathcal{L}}_{\phi}$ or student $\widehat{\mathcal{L}}_{\theta}$ objective functions, as formalized in Algorithm \ref{alg:ibmd}.

\textbf{$4\times$ super-resolution.}
Our implementation of the degradation operators aligns with the filters implementation proposed in DDRM \cite{kawar2022denoising}. Firstly, we synthesize images at $64\times64$ resolution, then upsample them to $256\times256$ to ensure dimensional consistency between clean and degraded inputs. For evaluation, we follow established benchmarks \cite{saharia2022palette, song2023pseudoinverse} by computing the FID on reconstructions from the full ImageNet validation set, with comparisons drawn against the training set statistics.

\textbf{JPEG restoration.}
Our JPEG degradation implementation, employing two distinct quality factors (QF=5, QF=10), follows \cite{kawar2022denoising}. FID is evaluated on a 
$10,000$-image ImageNet \href{https://drive.google.com/drive/u/0/folders/1SgonKNyJlB2s20Hmuo1hoSLjTM72ufqh}{validation subset} against the full validation set‚Äôs statistics, following baselines \cite{saharia2022palette, song2023pseudoinverse}.

\textbf{Inpainting.}
For the image inpainting task on ImageNet at $256\times256$ resolution, we utilize a fixed $128\times128$ centrally positioned mask, aligning with the methodologies of DBIM \cite{zheng2024diffusion} and CDBM \cite{he2024consistency}. During training, the model is trained only on the masked regions, while during generation, the unmasked areas are deterministically retained from the initial corrupted image $x_T$ to preserve structural fidelity of unmasked part of images. We trained the model with $4$ NFEs via the multistep method (Section \ref{sec:multistep}) and tested it with $1, 2,$ and $4$ NFEs.
\subsection{Distillation of DDBM models.}
\label{sec: experimental details ddbm}
We extended the DDBM repository (Table \ref{tab:license}) by integrating our distillation framework. Subsequent sections outline the experimental setups, adapted from the DDBM \citep{zheng2024diffusion}.

\textbf{Multistep implementation}
In this setup, the multistep training (Section \ref{sec:multistep}) adopts the methodology of DMD \cite{yin2024improved}, wherein a timestep $t$  is uniformly sampled from the predefined sequence $(t_1, \dots, t_N).$ The model $G_{\theta}$ then generates $x_0$ by iteratively reversing the process from the terminal timestep $t_N = T$ to the sampled intermediate timestep $t$. This generated $x_0$ is subsequently used to compute the bridge network‚Äôs loss $\widehat{\mathcal{L}}_{\phi}$ or the student network‚Äôs loss $\widehat{\mathcal{L}}_{\theta}$, as detailed in Algorithm \ref{alg:ibmd}.

\textbf{Edges $\rightarrow$ Handbags}
The model was trained utilizing the Edges$\rightarrow$Handbags image-to-image translation task \cite{isola2017image}, with the $64\times64$ resolution images. Two versions were trained under the multistep regime (Section \ref{sec:multistep}), with $2$ and $1$ NFEs during training. Both models were evaluated using the same NFE to match training settings.

\textbf{DIODE-Outdoor}
Following prior work \cite{zhou2023denoising, zheng2024diffusion, he2024consistency}, we used the DIODE outdoor dataset, preprocessed via the DBIM repository's script for training/test sets (Table \ref{tab:license}). Two versions were trained under the multistep regime (Section \ref{sec:multistep}), with $2$ and $1$ NFEs during training. Both models were evaluated using the same NFE to match training settings.

\textbf{Inpainting}
All setups matched those in Section \ref{sec:i2sb exps details} inpainting, except we use a CBDM checkpoint \citep{zheng2024diffusion}. This checkpoint is adjusted by the authors to: (1) condition on $x_T$ and (2) ImageNet class labels as input to guide the model. Also this is the same checkpoint used in both CDBM \citep{he2024consistency} and DBIM \citep{zheng2024diffusion} works.
\vspace{-3mm}
\section{Additional results}\label{app:additional-results}
\vspace{-3mm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.62\linewidth]{images/i2sb-bicubic-extra_compressed.png}
    \caption{Uncurated samples for IBMD-I$^2$SB distillation of 4x-super-resolution with bicubic kernel on ImageNet $256\times256$ images.}
    \label{fig:i2sb-sr-bicubic}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.62\linewidth]{images/i2sb-pool-extra_compressed.png}
    \caption{Uncurated samples for IBMD-I$^2$SB distillation of 4x-super-resolution with pool kernel on ImageNet $256\times256$ images.}
    \label{fig:i2sb-sr-pool}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.62\linewidth]{images/i2sb-jpeg-5-extra_compressed.png}
    \caption{Uncurated samples for IBMD-I$^2$SB distillation of Jpeg restoration with QF=5 on ImageNet $256\times256$ images.}
    \label{fig:i2sb-jpeg-5}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.62\linewidth]{images/i2sb-jpeg-10-extra_compressed.png}
    \caption{Uncurated samples for IBMD-I$^2$SB distillation of Jpeg restoration with QF=10 on ImageNet $256\times256$ images.}
    \label{fig:i2sb-jpeg-10}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/i2sb-inpaiting-extra_compressed.png}
    \caption{Uncurated samples for IBMD-I2SB distillation trained for inpaiting with NFE$=4$ and inferenced with different inference NFE on ImageNet $256\times256$ images.}
    \label{fig:i2sb-inpainting}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/new-ddbm-inpaiting-small.png}
    \caption{Uncurated samples for IBMD-DDBM distillation trained for inpaiting with NFE$=4$ and inferenced with different inference NFE on ImageNet $256\times256$ images.}
    \label{fig:inpainting ddbm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.785\linewidth]{images/ddbm-diode-extra-small.png}
    \caption{Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset ($256\times256$) with NFE$=2$ and NFE$=1$, inferred using the corresponding NFEs \underline{on the training set.}}
    \label{fig:diode train}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.785\linewidth]{images/ddbm-diode-extra-test-small.png}
    \caption{Uncurated samples from IBMD-DDBM distillation trained on the DIODE-Outdoor dataset ($256\times 256$) with NFE$=2$ and NFE$=1$, inferred using the corresponding NFEs \underline{on the test set.}}
    \label{fig:diode test}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.785\linewidth]{images/ddbm-e2h-extra-small.png}
    \caption{Uncurated samples from IBMD-DDBM distillation trained on the Edges $\rightarrow$ Handbags dataset ($64\times 64$) with NFE$=2$ and NFE$=1$, inferred using the corresponding NFEs \underline{on the training set.}}
    \label{fig:e2h train}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.785\linewidth]{images/ddbm-e2h-extra-test-small.png}
    \caption{Uncurated samples from IBMD-DDBM distillation trained on the Edges $\rightarrow$ Handbags dataset ($64\times 64$) with NFE$=2$ and NFE$=1$, inferred using the corresponding NFEs \underline{on the test set.}}
    \label{fig:e2h test}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
