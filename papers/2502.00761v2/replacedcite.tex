\section{Related Works}
% When pretraining language models, a large amount of text corpus is often crawled from the internet. However, several studies ____ suggest that high-quality data is more beneficial to the model's performance.

% To select high-quality data, a common strategy involves utilizing rules crafted by human experts ____, such as removing HTML tags and emoticons ____ or excluding sentences that are either too short or excessively long ____. However, these methods often fall short in effectively selecting high-quality data based on semantic content.

% Research has revealed that text corpus often contains numerous duplicate sentences ____. SemDeDup ____ employs k-means clustering to group similar data and retains only one representative sample from each cluster, while ____ (____) use SSL Prototypes to systematically eliminate prototypical data. However, only removing duplicates is insufficient to eliminate all low-quality samples.

% Utilizing a target data source or proxy model is also a common solution ____. DSIR ____ calculates the distribution difference between the source and target dataset as data sampling weight. Additionally, data influence is also considered as an important metric of data quality ____. Furthermore, some studies ____ measure the perplexity as a selection criterion.

% Training a classifier is a more straightforward method ____. ____ (____) implemented a logistic regression binary classifier to score the data, while some studies train more complex neural network scorers 
%  ____. Additionally, QuRating ____ trains multiple raters with a finer-grained approach to analyze the contribution of data to model performance improvement from different dimensions.

% The aforementioned methods primarily focus on selecting data from a single aspect. While QuRating has developed raters across multiple dimensions, it has not thoroughly explored how to effectively integrate these raters, which is meticulously addressed in our research.