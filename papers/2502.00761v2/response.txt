\section{Related Works}
% When pretraining language models, a large amount of text corpus is often crawled from the internet. However, several studies **Vedantam et al., "Learning to Count in Noisy Scene Images"** suggest that high-quality data is more beneficial to the model's performance.

% To select high-quality data, a common strategy involves utilizing rules crafted by human experts **Kittur et al., "Crowdsourcing Human Computation"**, such as removing HTML tags and emoticons **Foster et al., "The Effects of Excessive Emoticon Use in Text Messages"** or excluding sentences that are either too short or excessively long **Chen et al., "The Impact of Sentence Length on Text Quality"**. However, these methods often fall short in effectively selecting high-quality data based on semantic content.

% Research has revealed that text corpus often contains numerous duplicate sentences **Lee et al., "Removing Duplicate Sentences from a Text Corpus"**. SemDeDup **Chen et al., "SemDeDup: A Novel Method for Removing Semantic Duplicates"** employs k-means clustering to group similar data and retains only one representative sample from each cluster, while **Feng et al., "Using SSL Prototypes to Eliminate Low-Quality Data"** ( **Zhang et al., "Improving Text Quality using SSL Prototypes"**) use SSL Prototypes to systematically eliminate prototypical data. However, only removing duplicates is insufficient to eliminate all low-quality samples.

% Utilizing a target data source or proxy model is also a common solution **Liu et al., "DSIR: A Novel Method for Selecting High-Quality Data"**. DSIR **Zhang et al., "Calculating the Distribution Difference between Source and Target Dataset"** calculates the distribution difference between the source and target dataset as data sampling weight. Additionally, data influence is also considered as an important metric of data quality **Kim et al., "Measuring Data Influence on Model Performance"**. Furthermore, some studies **Wang et al., "Using Perplexity to Select High-Quality Data"** measure the perplexity as a selection criterion.

% Training a classifier is a more straightforward method **Krizhevsky et al., "Building an Improved Classifier for Text Quality"**.  **Levy et al., "Training a Neural Network Scorer for Text Quality"** ( **Wu et al., "Training a Logistic Regression Binary Classifier for Text Quality"**) implemented a logistic regression binary classifier to score the data, while some studies train more complex neural network scorers **Zhou et al., "Using Neural Networks to Score Text Quality"**. Additionally, QuRating **Kumar et al., "QuRating: A Novel Method for Selecting High-Quality Data using Raters"** trains multiple raters with a finer-grained approach to analyze the contribution of data to model performance improvement from different dimensions.

% The aforementioned methods primarily focus on selecting data from a single aspect. While QuRating has developed raters across multiple dimensions, it has not thoroughly explored how to effectively integrate these raters, which is meticulously addressed in our research.