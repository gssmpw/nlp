[
  {
    "index": 0,
    "papers": [
      {
        "key": "li2023textbooks",
        "author": "Li, Yuanzhi and Bubeck, S{\\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat",
        "title": "Textbooks are all you need ii: phi-1.5 technical report"
      },
      {
        "key": "zhou2024lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "raffel2020exploring",
        "author": "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      },
      {
        "key": "rae2021scaling",
        "author": "Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others",
        "title": "Scaling language models: Methods, analysis \\& insights from training gopher"
      },
      {
        "key": "laurenccon2022bigscience",
        "author": "Lauren{\\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others",
        "title": "The bigscience roots corpus: A 1.6 tb composite multilingual dataset"
      },
      {
        "key": "together2023redpajama",
        "author": "Together Computer",
        "title": "RedPajama: an Open Dataset for Training Large Language Models"
      },
      {
        "key": "penedo2024fineweb",
        "author": "Penedo, Guilherme and Kydl{\\'\\i}{\\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others",
        "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "laurenccon2022bigscience",
        "author": "Lauren{\\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others",
        "title": "The bigscience roots corpus: A 1.6 tb composite multilingual dataset"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "rae2021scaling",
        "author": "Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others",
        "title": "Scaling language models: Methods, analysis \\& insights from training gopher"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "lee2022deduplicating",
        "author": "Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas",
        "title": "Deduplicating Training Data Makes Language Models Better"
      },
      {
        "key": "sorscher2022beyond",
        "author": "Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari",
        "title": "Beyond neural scaling laws: beating power law scaling via data pruning"
      },
      {
        "key": "abbas2023semdedup",
        "author": "Abbas, Amro and Tirumala, Kushal and Simig, D{\\'a}niel and Ganguli, Surya and Morcos, Ari S",
        "title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication"
      },
      {
        "key": "cerebras2023slimpajama",
        "author": "Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan",
        "title": "{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}"
      },
      {
        "key": "tirumala2024d4",
        "author": "Tirumala, Kushal and Simig, Daniel and Aghajanyan, Armen and Morcos, Ari",
        "title": "D4: Improving llm pretraining via document de-duplication and diversification"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "abbas2023semdedup",
        "author": "Abbas, Amro and Tirumala, Kushal and Simig, D{\\'a}niel and Ganguli, Surya and Morcos, Ari S",
        "title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "sorscher2022beyond",
        "author": "Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari",
        "title": "Beyond neural scaling laws: beating power law scaling via data pruning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "sorscher2022beyond",
        "author": "Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari",
        "title": "Beyond neural scaling laws: beating power law scaling via data pruning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wenzek2020ccnet",
        "author": "Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Joulin, Armand and Grave, {\\'E}douard",
        "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"
      },
      {
        "key": "xie2023data",
        "author": "Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S",
        "title": "Data selection for language models via importance resampling"
      },
      {
        "key": "marion2023less",
        "author": "Marion, Max and {\\\"U}st{\\\"u}n, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara",
        "title": "When less is more: Investigating data pruning for pretraining llms at scale"
      },
      {
        "key": "thakkar2023self",
        "author": "Thakkar, Megh and Bolukbasi, Tolga and Ganapathy, Sriram and Vashishth, Shikhar and Chandar, Sarath and Talukdar, Partha",
        "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training"
      },
      {
        "key": "engstrom2024dsdm",
        "author": "Engstrom, Logan and Feldmann, Axel and Madry, Aleksander",
        "title": "Dsdm: Model-aware dataset selection with datamodels"
      },
      {
        "key": "yu2024mates",
        "author": "Yu, Zichun and Das, Spandan and Xiong, Chenyan",
        "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xie2023data",
        "author": "Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S",
        "title": "Data selection for language models via importance resampling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "thakkar2023self",
        "author": "Thakkar, Megh and Bolukbasi, Tolga and Ganapathy, Sriram and Vashishth, Shikhar and Chandar, Sarath and Talukdar, Partha",
        "title": "Self-Influence Guided Data Reweighting for Language Model Pre-training"
      },
      {
        "key": "engstrom2024dsdm",
        "author": "Engstrom, Logan and Feldmann, Axel and Madry, Aleksander",
        "title": "Dsdm: Model-aware dataset selection with datamodels"
      },
      {
        "key": "yu2024mates",
        "author": "Yu, Zichun and Das, Spandan and Xiong, Chenyan",
        "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wenzek2020ccnet",
        "author": "Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\\'a}n, Francisco and Joulin, Armand and Grave, {\\'E}douard",
        "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"
      },
      {
        "key": "marion2023less",
        "author": "Marion, Max and {\\\"U}st{\\\"u}n, Ahmet and Pozzobon, Luiza and Wang, Alex and Fadaee, Marzieh and Hooker, Sara",
        "title": "When less is more: Investigating data pruning for pretraining llms at scale"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "du2022glam",
        "author": "Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
        "title": "Glam: Efficient scaling of language models with mixture-of-experts"
      },
      {
        "key": "gururangan2022whose",
        "author": "Gururangan, Suchin and Card, Dallas and Dreier, Sarah and Gade, Emily and Wang, Leroy and Wang, Zeyu and Zettlemoyer, Luke and Smith, Noah A",
        "title": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection"
      },
      {
        "key": "zhang2024autonomous",
        "author": "Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C",
        "title": "Autonomous data selection with language models for mathematical texts"
      },
      {
        "key": "wettig2024qurating",
        "author": "Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi",
        "title": "Qurating: Selecting high-quality data for training language models"
      },
      {
        "key": "sachdeva2024train",
        "author": "Sachdeva, Noveen and Coleman, Benjamin and Kang, Wang-Cheng and Ni, Jianmo and Hong, Lichan and Chi, Ed H and Caverlee, James and McAuley, Julian and Cheng, Derek Zhiyuan",
        "title": "How to Train Data-Efficient LLMs"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "du2022glam",
        "author": "Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
        "title": "Glam: Efficient scaling of language models with mixture-of-experts"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "du2022glam",
        "author": "Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others",
        "title": "Glam: Efficient scaling of language models with mixture-of-experts"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024autonomous",
        "author": "Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew C",
        "title": "Autonomous data selection with language models for mathematical texts"
      },
      {
        "key": "sachdeva2024train",
        "author": "Sachdeva, Noveen and Coleman, Benjamin and Kang, Wang-Cheng and Ni, Jianmo and Hong, Lichan and Chi, Ed H and Caverlee, James and McAuley, Julian and Cheng, Derek Zhiyuan",
        "title": "How to Train Data-Efficient LLMs"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wettig2024qurating",
        "author": "Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi",
        "title": "Qurating: Selecting high-quality data for training language models"
      }
    ]
  }
]