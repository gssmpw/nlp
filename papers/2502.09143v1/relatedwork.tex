\section{Related Work}
\subsection{Continual Learning}
Continual Learning (CL) is a paradigm designed to enable models to sequentially learn a series of tasks while preserving performance on previously learned tasks. This capability addresses the phenomenon of catastrophic forgetting, a common issue in standard training approaches where new learning overwrites prior knowledge. CL is typically categorized into three main settings: Task-Incremental, Domain-Incremental, and Class-Incremental \cite{vandeVen2019ThreeSF_threescenarios}. Among these, Class-Incremental Learning is particularly challenging, as it requires the model to solve tasks without explicit task identifiers and to classify across all encountered classes, thereby minimizing reliance on task-specific information.

In CL, pre-trained models are often employed as feature extractors, leveraging representations generated by the final feature map. These approaches can be broadly divided into two categories: those that treat the pre-trained model as a trainable backbone and those that use it as a fixed feature extractor \cite{jung2023new_mufan}. However, most methods in the latter category primarily focus on the final layer representation, neglecting the potential benefits of multi-granularity feature representations.

Recent work, such as MuFAN, tackles this limitation by leveraging feature maps from multiple layers to create richer representations for online continual learning \cite{jung2023new_mufan}. This approach primarily utilizes convolutional layers to produce multi-scale representations. In contrast, our work employs GNNs to advance this concept further. By integrating multi-granularity features into a graph construction process, we create information-rich graph representations of images, facilitating more effective learning in OCL scenarios.

\subsection{Graph Based Continual Learning}
Graph Neural Networks (GNNs) have garnered significant attention in the field of continual learning, demonstrating considerable promise. For instance, TWP \cite{Liu2020OvercomingCF_twp} utilizes topological information to stabilize the training process by measuring the attention coefficients in a Graph Attention Network (GAT). However, existing studies predominantly focus on graph-structured data, and can not be directly used for CL for image classification.

Recent research works have explored the use of GNNs for continual learning in image classification. DGN \cite{Carta2021CatastrophicFI_dgn} employs a static clustering method and evaluates its performance on MNIST and CIFAR10 datasets. Similarly, GCL \cite{tang2021graphbased_gcl} models image batches using random graphs, departing from conventional image-based methods. However, to the best of our knowledge, this is the first work to explore graph attention networks for continual learning in image classification.