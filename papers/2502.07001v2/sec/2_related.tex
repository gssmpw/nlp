\section{Related Work}
\label{sec:related}

Diffusion models \cite{HoDenoisingDiffusionProbabilisticModels} have revolutionized the fields of image and video synthesis as they offer unprecedented generation quality \cite{karras2022elucidating, liu2024sora}.
Whereas they once relied heavily on U-Nets \cite{ronneberger2015u}, transformer-based diffusion models have since gained traction \cite{dit}.
We refer the reader to \citet{zhang2023survey} for a survey on diffusion-based image generation.

\vspace{4mm}
\noindent \textbf{Image diffusion for visual understanding.} Beyond their impressive generative capabilities, image diffusion models have proved remarkably effective in perception tasks.
For example, DDPM-Seg \cite{baranchuk2022} utilizes intermediate features of a U-Net-based diffusion model and excels at semantic segmentation.
It has further been extended to different architectures and showed that the best-performing features are found in the middle upsampling layers using small noise levels \cite{xiang2023}.
Pre-trained text-to-image diffusion models have also been applied to panoptic segmentation \cite{xu2023}.

In terms of correspondence tasks, DIFT~\cite{tang2023}, a method based on extracting features from diffusion models, showed that features extracted in earlier layers with larger timesteps tend to be more semantically meaningful, whereas lower-level features with smaller timesteps focus more on low-level details.
A related work found that merging diffusion representations with DINOv2 \cite{oquab2024} features improves performance on semantic correspondence \cite{zhang2024}.
\citet{luo2023dhf} demonstrated that having an aggregation network that learns weights for all features maps across all layers and timesteps performs better than manually selecting layers and timesteps. 
\citet{clark2024text} proposed a mechanism that makes use of the denoising scores for all possible labels,
thereby creating a zero-shot ImageNet classifier which is however computationally impractical.
Another work compares various types of readout architectures with similar parameter count and has found attentive readout to perform best \cite{mukhopadhyay2023}.

Further works propose diffusion models as a general foundational model for a suite of image understanding tasks.
For example, \citet{zhao2023} showed that diffusion models excel at both semantic segmentation and depth estimation, while \citet{yang2023}, showed state-of-the-art performance on several image classification, semantic segmentation and landmark detection benchmarks using diffusion models.
We refer the reader to a recent survey on the connection between diffusion models and image representation learning for an extensive overview \cite{fuest2024diffusion}.


\noindent \textbf{Video representation learning.} Building on image representation learning, research is naturally progressing to video analysis for enhanced spatial understanding \cite{parthasarathy2023self}.
Contrastive and masked modeling are popular choices here:
V-JEPA \cite{vjepa} predicts video embeddings from a set of random crops in a self-supervised student-teacher setup, while VideoMAE \cite{tong2022videomae, feichtenhofer2022masked, videomaev2} pre-trains vision transformers by masking and reconstructing random video patches.
VideoPrism \cite{zhao2024} combines both: it uses video-language contrastive learning in a first stage to capture semantic content, while a second stage uses masked modeling with global-local distillation and token shuffling.

Recent works have started to explore the potential of \emph{image} diffusion models on video understanding \cite{chen2023, nag2023}.
Since high-quality \emph{video} generation with diffusion models is only a recent development \cite{walt, liu2024sora}, the literature on investigating their features is sparse:
video diffusion representations have recently been found to produce temporally consistent video depth estimates \cite{hu2024depthcrafter}, and they have been applied to object segmentation \citet{zhu2024}, though without a direct comparison with image diffusion models.
A concurrent work with ours, \cite{anonymous2024}, evaluates the performance of the video diffusion model SVD \cite{blattmann2023stable} on video understanding tasks, and finds that it outperforms image baselines, in particular Stable Diffusion (SD) \cite{rombach2022high}.
While SVD builds upon SD's U-Net architecture by incorporating temporal convolution and attention layers, it nearly doubles the parameter count (865\,M to 1.5\,B) which weakens the expressiveness of the comparison.
In this work, we specifically choose WALT \cite{walt} for our investigations due to its hybrid architecture that allows a more apt comparison.
Finally, also concurrently to our work, \citet{man2024} evaluate image and video representation models with a focus on 3D scene understanding (semantic and geometric understanding) and vision-language reasoning tasks and find
diffusion models to excel at geometric tasks.


