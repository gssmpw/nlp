
This supplementary material provides further analysis of \iwalt, including ablation studies in \cref{app:sec:iwalt_ablations}, using the same setup as \vwalt. Tables illustrating the performance metrics of the WALT model for image and video tasks are reported in \cref{app:sec:baseline_results}, which correspond to the same values used to calculate the relative performance metrics in \cref{fig:baselines} of the main paper. We also present visualizations of depth estimation and box tracking predictions for both models in \cref{app:sec:qualitative_results}. Details of the datasets and readouts are explained in \cref{app:sec:datasets}, while training settings are described in \cref{app:sec:training_details}.


\section{\iwalt ablations}
\label{app:sec:iwalt_ablations}


To evaluate the image-pretrained model, \iwalt (introduced in \cref{sec:method:walt_variants}), we replicate the ablation study from \cref{sec:exp:featureextraction} for the video-pretrained counterpart, \vwalt.




\paragraph{Noise level}
\Cref{fig:iwalt_noise_and_blocks} (left) illustrates the readout performance of \iwalt at different noise levels.  Following the noise level ablation for \vwalt, the block for feature extraction is set to $l=16$. Similar to what was reported for \vwalt in \cref{sec:exp:featureextraction},  \iwalt benefits from addition of moderate noise levels (between 0 and 200 timesteps).

\paragraph{Model block}
We analyzed the downstream task performance of different layers as shown in \cref{fig:iwalt_noise_and_blocks} (right). As in the model block ablation for \vwalt, the noise timestep is set to $t=200$. Most layers exhibit performance comparable to \vwalt, but point tracking shows a notable difference.  For this task, the earlier layers of the model achieve the highest accuracy. This suggests that the features relevant for point tracking are learned early in the I-WALT transformer.

\input{figs/app_noise_and_blocks_iwalt}

\paragraph{Training budget}
\Cref{fig:iwalt_training_progress} shows the training progress of \iwalt using a setup similar to the one described in \cref{sec:exp:fvd}.  We observe a comparable behavior to \vwalt, except for camera pose estimation, where the performance of \iwalt improves with longer training. This suggests that \iwalt does not overfit in this setting, which is further supported by the moderate to strong positive correlations between performance, loss, and FID values over training time, as shown in \cref{tab:iwalt_fid_loss_metric_corr}. Additionally, we include the values for \vwalt in \cref{tab:vwalt_fvd_loss_metric_corr}, which demonstrate a similar correlation pattern, except for camera pose estimation, as mentioned in \cref{sec:exp:fvd}.
\input{figs/iwalt_training_progress}

\input{tables/iwalt_fid_loss_metric_corr}
\input{tables/vwalt_fvd_loss_metric_corr}

\section{Baselines results}
\label{app:sec:baseline_results}
The performance metrics of \iwalt and \vwalt for downstream tasks are provided in \cref{tab:image_baselines} and \cref{tab:video_baselines}, respectively. These tables correspond to the values used to generate the relative performance metrics presented in \cref{fig:baselines} of the main paper. For example, accuracy is used for \Tssv action recognition. See~\cref{sec:method:probing_framework} for the full list of metrics used for each downstream task.  

\section{Qualitative results}
\label{app:sec:qualitative_results}


\input{figs/qualitative_depth}
\input{figs/qualitative_obj_track}
\input{figs/qualitative_ssv2}

In order to qualitatively assess the performance of \iwalt compared to \vwalt, we include prediction outputs of all WALT models for the tasks of monocular depth prediction (\cref{fig:qualitative_depth}), box tracking (\cref{fig:qualitative_obj_tracks}) and action recognition (\cref{fig:qualitative_ssv2}).
As demonstrated in \cref{sec:exp:imagevsvideo}, \vwalt consistently outperforms \iwalt across all tasks, particularly those requiring spatiotemporal understanding. This is evident in the enhanced accuracy of \vwalt predictions, such as depth maps exhibiting greater pixel correspondence, even with the \vwalt 218M model, which has the same number of parameters as \iwalt.  Similarly, \vwalt shows superior object tracking performance, while \iwalt struggles to accurately estimate the size and location of bounding boxes.  The temporally rich input data enables \vwalt to achieve higher accuracy in action recognition. For instance, \vwalt correctly classifies the action of ``squeezing something'' (first column), which occurs midway through the video clip, whereas \iwalt fails to do so.

\section{Datasets and readout details}
\label{app:sec:datasets}

 A summary of the readout architectures and their corresponding parameter counts for the image and video tasks is provided in \cref{table:readout_modules_image} and \cref{table:readout_modules_video}, respectively.

\input{tables/baselines_image_tasks}
\input{tables/baselines_video_tasks} 

\input{tables/arch_details_image_tasks}
\input{tables/arch_details_video_tasks}

\subsection{Image classification}

\paragraph{Dataset}
Image classification results are reported on \Timagenet~\cite{imagenet}, \Tplaces~\cite{places365}, and \Tinat~\cite{inaturalist}.


\Timagenet ~\cite{imagenet} is a large-scale dataset containing 1,000 object and animal categories, with 1,281,167 images for training and 50,000 images for validation. For training on \Timagenet, we follow the traditional augmentation scheme consisting of inception cropping and random horizontal flipping. For evaluation, a $224\times224$ center crop is extracted from each image whose shorter edge is first resized to 256.

\Tplaces is a scene classification dataset that contains images with buildings, landscapes, and other everyday scenarios. It includes 1.8 million training images and 36,500 validation images across 365 scene classes. 

\Tinat contains 437{,}513 images for training and 24{,}426 images for validation, featuring 8,142 classes of visually similar species across a wide range of taxonomic groups, such as plants, animals, fungi, and insects. The dataset also exhibits heterogeneous image quality due to diverse camera sources and exhibits a substantial class imbalance.

The same data augmentation and preprocessing steps used for \Timagenet are applied to both \Tinat and \Tplaces. For all the image classification datasets, the original data splits are used for training and evaluating the classification readout.




\paragraph{Readout}
In the case of \vwalt, an image is replicated across the 17 temporal input channels and the model features are averaged before passing them to the readout. \iwalt does not require any image replication and the model features are directly passed to the readout.

Adopting the approach of V-JEPA~\cite{vjepa}, we utilize a cross-attention block with a learnable query token to extract class information from the model features. This token attends to the features within the cross-attention block, and its output is fed into a linear classifier for class prediction. The readout is trained with the softmax cross-entropy  loss.

\subsection{Action recognition}

\paragraph{Dataset}
Something-Something-V2 (\Tssv)~\cite{ssv2} is a large-scale dataset consisting of short videos (2-6 seconds at 12 frames per second) depicting diverse human actions with everyday objects.  The dataset is specifically designed for fine-grained understanding of human hand gestures, focusing on subtle actions, such as placing objects into containers. Something-Something-V2 encompasses 174 categories with 168,913 samples in the training set and 24,777 in the validation set.

Kinetics-400 (\Tkfour) is a large-scale dataset of YouTube videos designed for human action recognition, encompassing object manipulation, human-object interaction, and body movements. Kinetics-400 consists of 246,245 training videos and $\sim$20K validation videos with an average duration of 10 seconds. All video clips are labeled into 400 classes.

Kinetics-700 (\Tkseven) is an extension of Kinetics-400. The data collection pipeline between the two datasets differs in how action classes are sourced, how videos are matched with classes, and the human verification process. Kinetics-700 contains 545,317 training videos and 35,000 validation videos across 700 fine-grained action classes.

For both training and evaluation of the readout, 17 frames are sampled from each video to generate the model input. A stride of 2 is used for \Tssv, while a stride of 1 is used for \Tkfour and \Tkseven. For  all  the  action recognition datasets, the original data splits are used for training and evaluating the readout.







\paragraph{Readout}
The same attention readout used for image classification and described above is also used for action recognition. 


\subsection{Monocular depth prediction}
\paragraph{Dataset}
For this work, we utilize the train and validation splits of the ScanNet dataset~\cite{scannet}, comprising 1,201 and 312 videos respectively. The dataset offers high-resolution RGB frames (1296x968) in diverse indoor environments and corresponding depth frames (640x480) captured with an RGB-D system. The input to the WALT model is obtained by sampling 17 consecutive frames from the ScanNet videos. During training, the starting frame is chosen randomly, while for evaluation, sampling always begins at frame 0. %

\paragraph{Readout}
We use the readout from~\cite{SRT}, which applies cross-attention with spatial coordinates as queries to each frame independently. The readout is trained using an L2 loss between predicted and ground truth depth maps.

\subsection{Relative camera pose estimation}
\paragraph{Dataset}
RealEstate10K~\cite{zhou2018stereo} is a dataset of property walkthrough videos with intrinsic and extrinsic camera parameters using Structure from Motion (SfM). The clips were gathered from YouTube and typically feature smooth camera movement with minimal camera roll or pitch. The original splits of the dataset are used, which consist of roughly 10 million training frames from 6,500 videos and 1 million test frames from 696 videos.


\paragraph{Readout}
The input of the readout is formed by concatenating the video representations of the first and last frame of the video sequences.  These are then processed via cross-attention with learned latent vectors and a linear layer to produce 12-dimensional vectors representing SE(3) pose transformations, which correspond to a $3\times3$ rotation matrix and a $3\times1$ translation vector. The predicted rotation matrix is refined using the Procrustes algorithm~\cite{bregier2021deep} to ensure it represents a valid SO(3) rotation before metric evaluation. Training is performed by minimizing the L2 loss between predicted and ground-truth pose matrices.

\subsection{Visual correspondence -- Point tracking}
\paragraph{Dataset}
The Perception Test dataset~\cite{perception-test} was specifically designed to evaluate the perception and reasoning skills of multimodal video models. It was filmed by around 100 participants worldwide and contains perceptually interesting situations. In this paper, the Perception Test dataset is used to evaluate the point tracking task. Specifically, the validation set is employed, which comprises 73 real-world videos, averaging 722 frames in length, with multiple point tracks annotated using the same protocol as in~\cite{moog}. Each point is visible in approximately 480 frames. The first 17 frames of each video are sampled with a stride of 4 to generate the model input.

The point tracking readout head is trained with the training set of the Kubric MOVi-E dataset~\cite{movi-e}, which contains 97,500 synthetic 24-frame videos, each depicting scenes with 10-20 static and 1-3 dynamic objects rendered against photorealistic backgrounds. The camera in these videos moves on a straight line at a constant velocity, always pointed towards the origin of the scene. 

\paragraph{Readout}
To build the readout input, pretrained model features are first interpolated in the temporal dimension to match the number of video frames. The interpolated features and a set of query points become the readout input. During training, 17 frames and 64 point tracks are randomly sampled from each video. Then, a random crop with an area between 30\% and 100\% of the original frame and an aspect ratio between 0.5 and 2.0 is extracted. The crops are then resized to $128\times 128$. Query points are selected exclusively from the first frame.

For evaluation, we sample the first 17 frames with a stride of 4 from each video and use 64 point tracks. As in~\citep{moog}, our evaluation takes the first visible point track as the query, and discards frames preceding its appearance.

The readout head employs an iteratively applied cross-attention transformer, maintaining a 512-dimensional latent state for each point track between frames. As in~\citep{moog}, this state is initialized from query point positions using a Fourier positional encoding followed by a two-layer MLP. The transformer comprises three layers of cross-attention with eight heads and a key/value size of 512. At each step, it uses the latent state as queries to attend to the frame features generated by the pretrained model. A two-layer MLP predicts the position, visibility, and uncertainty of each point track in each frame using the corresponding latent states as input. The loss function is a combination of a Huber loss for location accuracy and Sigmoid Binary Cross Entropy losses for visibility and certainty.  Points that have exited the scene contribute only to the visibility loss.

\subsection{Visual correspondence -- Box tracking}
\paragraph{Dataset}
We leverage the Waymo Open Dataset~\cite{waymo}, utilizing the high-resolution (1280x1920) RGB video data captured at 10 fps. This dataset, recorded from Waymo vehicles in urban and suburban settings, includes 2D and 3D bounding box annotations. We use the 2D bounding boxes for loss calculation and metric evaluation. The training and validation sets comprise 798 and 202 samples, respectively, each 20 seconds in duration. The same data splits are used for training and evaluating the box tracking readout.

\paragraph{Readout}
Consistent with prior work~\cite{moog}, for both training and evaluation, we downsample videos to $256\times384$ resolution at 5 fps, and then extract a central 256x256 spatial crop and a random 17-frame temporal crop. Bounding boxes smaller than 0.5\% of the first sampled frame area are discarded, and a maximum of 25 boxes are retained per sample.

The same attention readout used for point tracking and described above is also used for box tracking, only differing in the predictions. The position $x_{min}$, $x_{max}$, $y_{min}$, $y_{max}$ of query boxes is predicted for box tracking. The box tracking readout is trained using an L2 loss between the predicted and normalized box coordinates. As in point tracking, the pretrained model features are interpolated in the temporal dimension to match the number of video frames. 

\section{Training settings}
\label{app:sec:training_details}
\Cref{tab:experiments_setup} summarizes the hyperparameters used to train the readout heads for each task described in \cref{app:sec:datasets}. We use the AdamW optimizer with a cosine learning rate decay schedule, initialized with a linear warmup over 1,000 steps (from 0 to 3e-4), and subsequently decaying to 1e-7. Batch sizes are 32 for video tasks, 512 for \Tplaces and \Tinat, and 64 for \Timagenet.
\input{tables/experiments_setup}




 



