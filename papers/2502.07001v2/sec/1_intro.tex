\section{Introduction}
\label{sec:intro}

The field of computer vision has made remarkable strides towards imbuing machines with the ability to see and interpret the visual world.
A key challenge in realizing this goal lies in learning effective representations that capture both the rich semantic information and dynamic 4D structure (3D \& motion) inherent in real-world visual data.
Leading approaches in visual representation learning have primarily focused on \emph{contrastive learning} and \emph{reconstruction}.
Contrastive models can be exemplified by DINO \cite{caron2021emerging} which applies knowledge distillation between augmented views of images, and CLIP \cite{radford2021} which aligns image and text representations, while MAE \cite{mae} and I-JEPA \cite{ijepa} are commonly used reconstruction models that predict masked pixels and features, respectively.

\input{figs/teaser}

Meanwhile, generative models have demonstrated an unprecedented ability to synthesize novel, photo-realistic imagery \cite{karras2022elucidating, liu2024sora}.
Among generative approaches, diffusion models \cite{HoDenoisingDiffusionProbabilisticModels} have emerged as the state-of-the-art for both image and video generation, achieving remarkable results in synthesizing high-quality visual content \cite{fuest2024diffusion}.
Image diffusion models have also demonstrated their capacity to learn powerful representations for downstream tasks such as image classification \cite{li2023your}, depth estimation \cite{shao2023monodiffusion} and semantic keypoint matching \cite{hedlin2024unsupervised, tang2023}, suggesting that the denoising process enables these models to acquire a deep understanding of visual semantics and structure.
However, despite the recent emergence of video diffusion models capable of generating high-fidelity video content, the representational power of these models remains largely unexplored, particularly in the context of dynamic scene understanding.
This leaves a crucial question unanswered: how effectively do video diffusion models capture the interplay between motion and spatial scene understanding, and how do they perform compared to image diffusion models?

\clearpage
In this work, we delve into the representational power of the diffusion model WALT \cite{walt} which is particularly well-suited for a direct comparison thanks to its hybrid architecture, allowing us to specifically analyze the implications of the video versus image pre-training objective on downstream performance.

Our key contributions are as follows:
\begin{enumerate}
    \item We compare the same model architecture trained on image \vs video generation and demonstrate the superiority of video for learning representations across diverse downstream tasks.
    \item We provide a qualitative analysis of video diffusion representations, highlighting the role of motion in the video representation space.
    \item We present findings on the relationship between training budget, model size, visual generation quality and downstream performance of the learned representations.
\end{enumerate}
To the best of our knowledge, this is the first work to analyze and compare the internal representations of the same diffusion model pre-trained for video versus image generation across a diverse range of tasks.
