\section{Experiments}
\label{sec:exp}

We begin our experiments in \cref{sec:exp:qualitative} with qualitative investigations into the features learned by \iwalt and \vwalt.
In \cref{sec:exp:imagevsvideo}, we start our range of quantitative evaluations, comparing representations learned via image and video diffusion pre-training objectives with the help of tasks ranging from pure semantics to spatio-temporal visual understanding.
\Cref{sec:exp:featureextraction} delves further into the design choices involved when extracting features from generative diffusion models.
We conclude our experiments with an analysis on the effect of training budget on feature and generation quality in \cref{sec:exp:fvd} and a comparison with common visual representation models in \cref{sec:exp:baselines}.

\subsection{Qualitative Observations}
\label{sec:exp:qualitative}

\input{figs/qualitative1}

Visualizing latent features in deep neural networks is challenging due to their high dimensionality \cite{Mahendran_2015_CVPR}.
To simplify this analysis, we extract the principal component with the largest eigenvalue from the features at late layers of the \iwalt and \vwalt models.
This allows us to inspect the most salient features in the latent space.


\Cref{fig:qualitative1} visualizes the key feature activations from the first frame of various videos in the DAVIS dataset \cite{davis}, highlighting how \iwalt and \vwalt differ in representing motion. Both models emphasize salient regions, such as people and foreground objects, indicating a shared bias towards key elements in the scene. However, a closer look reveals a key distinction: \iwalt often fails to differentiate between moving and static individuals (\eg, see the second column, where all people are highlighted), while \vwalt selectively focuses on regions exhibiting motion, as confirmed by alignment with optical flow maps.

To investigate this observation, a video of a brick wall was manipulated by freezing a region of pixels while displacing a subset within it as shown in \cref{fig:qualitative2}.
Unlike \iwalt, which consistently produces the same features across frames due to its image-based nature, \vwalt shows motion sensitivity, indicated by strong activations in the principal component map. This comparison highlights the ability of \vwalt to distinguish dynamic areas within static scenes, suggesting its suitability for applications that prioritize motion detection and precise differentiation of moving objects.

\subsection{Video versus image diffusion}
\label{sec:exp:imagevsvideo}

Our initial visual inspections of the learned model features pave the way for an objective assessment.
We begin with a key comparison that lies at the heart of our quantitative evaluation.
\Cref{fig:teaser} presents a comparison between the performance of \iwalt, the model trained for image generation, and the performance of the same model architecture trained for video generation (\vwalt) across a range of readout tasks described in \cref{sec:method:probing_framework}.
For a meaningful comparison across tasks, we show the relative performance change $(x_V-x_I)/x_I$, where $x_I$ and $x_V$ denote the absolute performance of the image and video models, respectively.

\emph{\vwalt consistently outperforms \iwalt across all tasks, though the figure reveals a striking range in the extent of this superiority.}
Perhaps unsurprisingly, improvements are small on the purely semantic image classification tasks \Tplaces (+0.6\%) and \Timagenet (+1.8\%), while we see a substantial increase in performance on \Twaymo (+23\%), \Tpose (+60\%) and \Tpt (+68\%) -- tasks that benefit greatly from a deeper understanding of space and motion.

Remarkably, the video training objective improves performance on the image classification task \Tinat (+11\%).
On the action classification tasks, it is interesting to see that the delta is considerable on \Tssv (+42\%), but much more subtle on Kinetics (+8\% on \Tkfour, +12\% on \Tkseven).
This follows the general consensus that the Kinetics tasks primarily measure appearance understanding, while \Tssv is significantly more sensitive to motion understanding \cite{vjepa, sevilla2021only}.


The substantial improvement on \Tpt (+68\%) could be unexpected, given that image diffusion models have been shown to excel at point correspondence \cite{tang2023, hedlin2024unsupervised}.
We attribute this to the fact that point \emph{tracking} is primarily sensitive to accurate localization of \emph{spatial} points in the scene, while point \emph{correspondence} primarily measures semantic understanding.
The videos presented by \citet{hedlin2024unsupervised} illustrate this challenge, demonstrating that the model struggles to distinguish between different instances of the same class (\eg, ears of the same person).

\input{figs/qualitative2}

\input{figs/noise_and_blocks}

\subsection{Feature extraction from generative diffusion}
\label{sec:exp:featureextraction}

Trained for denoising, generative diffusion models offer flexibility in feature extraction due to the lack of a dedicated ``feature extraction'' mode.
A crucial consideration is determining both the appropriate time step and the best network block for feature extraction.
To this end, we evaluate the features extracted using various noise levels and transformer blocks within the model across all tasks in \cref{fig:noise_and_blocks}.

\noindent \textbf{Noise level.}
As shown in \cref{fig:noise_and_blocks} (left), introducing high levels of noise generally diminishes downstream task performance. However, a small amount of noise (200) leads to the best results for most tasks, with the exception of \Tpt (best at 100) and \Twaymo (best at 0). This aligns with the intuition that lower-level tasks like tracking are more susceptible to noise, while higher-level tasks benefit from subtle amounts of it.
We find that the image model \iwalt behaves similarly (see the Appendix).
Related works in the literature have investigated this question, though exclusively on image diffusion models, and the consensus is that small amounts of noise have been found helpful for downstream performance, \eg classification \cite{mukhopadhyay2023, xiang2023} or correspondence tasks \cite{luo2023dhf, tang2023}.


\noindent \textbf{Model block.}
Next, we investigate where in the model to extract features from.
The WALT model consists of a tokenizer followed by $L=24$ transformer blocks, see \cref{sec:method:walt}.
As seen in \cref{fig:noise_and_blocks} (right), we find that the best representations are located at a depth of approximately 2\,/\,3 \ within the model for all tasks. A notable deviation from this is the \Tpt task, which performs slightly better at an earlier block.
This finding suggests an implicit separation of the model into encoder and decoder components, leading to the best representations being learned at their intersection.



\subsection{Pre-training budget and generation quality}
\label{sec:exp:fvd}
\vspace{1mm}


Prior work has investigated the relationship between reconstruction and downstream performance \cite{balestriero2024learning} and found that the most informative features are learned towards the end of the training schedule.
To explore this relationship for diffusion models, we train a \vwalt model and capture checkpoints at regular intervals throughout the training process.
Each checkpoint is then evaluated on two fronts:
the effectiveness of its learned features on the downstream tasks, and training progress, as measured by the training loss and Fr√©chet Video Distance (FVD) \cite{unterthiner2018towards}.

\Cref{fig:training_progress} illustrates the per-task performance of models trained using checkpoints from various stages of the pre-training process.  Interestingly, even early checkpoints (representing 20\% of total pre-training progress) demonstrate a relative performance exceeding 90\% on several tasks. As expected, performance on recognition tasks exhibits a consistent upward trend with continued training. Conversely,  tracking and depth estimation tasks appear to achieve optimal performance at earlier stages. Notably, camera pose estimation performance shows a decline after the 26\% training mark, suggesting potential overfitting or a shift in learned representations that negatively impacts this specific task.


\input{figs/training_progress}

\subsection{Comparisons with visual representation models}
\label{sec:exp:baselines}



To conclude our experiments, we investigate the scaling behavior of the \vwalt model, and compare its performance at sizes 284\,M, as in all other experiments, and 1{.}9\,B, with standard visual representation learning models in the same frozen readout setting. 
We choose representative models from different self-supervised methods:
contrastive DINOv2 \cite{caron2021emerging},
image-text alignment SigLIP \cite{chen2023pali},
pixel reconstruction MAE\cite{mae},
and feature reconstruction model JEPA \cite{ijepa}.
We also include their video extensions VideoMAE \cite{tong2022videomae} and V-JEPA \cite{vjepa} to further explore the differences between models trained on images \vs videos.

In \cref{fig:baselines}, we use the performance of \iwalt as baseline (100\%) and plot the relative performance of other models.
Scaling \vwalt to 1{.}9\,B significantly improves the performance on most tasks, except on \Tpt where the smaller model does marginally better.
The most obvious boosts are on image and video classification, and classification tasks with a large number of classes (\Tkseven and \Tinat) tend to benefit more from the increased model size compared to those with fewer classes (\Tkfour and \Timagenet).
While \vwalt is competitive with the other video models on depth and motion understanding, it is dominated by SigLIP and DinoV2 on the more semantic tasks, revealing a core weakness of generative diffusion models in our readout setting.


\input{figs/baselines}
