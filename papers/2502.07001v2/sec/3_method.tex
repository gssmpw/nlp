\section{Method}
\label{sec:method}

We lead this section with a recap of latent diffusion models and the WALT model in \cref{sec:method:latentdiffusionmodels} and \cref{sec:method:walt}, respectively.
\Cref{sec:method:walt_variants} describes how WALT operates on images \vs video, followed by a description of the probing framework for quantitative evaluations in \cref{sec:method:probing_framework}.


\subsection{Latent Diffusion Models}
\label{sec:method:latentdiffusionmodels}

Diffusion Models \cite{sohldickstein2015deepunsupervisedlearningusing} are probabilistic generative models that can learn a distribution by denoising a normally distributed variable. They are based on two stages, a forward diffusion stage and a backward diffusion stage. In the forward diffusion stage, the input data is gradually corrupted by adding noise with a fixed variance schedule until the data distribution degrades into a standard Gaussian distribution.  In the backward stage, the model reconstructs the original input data by learning to gradually denoise the signal. 




Latent Diffusion Models (LDMs) \cite{blattmann2023stable}  apply the diffusion process in the latent space of a Vector Quantized Variational Autoencoder (VQ-VAE) \cite{NeuralDiscreteRepresentationLearning, TamingTransformersForHiResImageSynth} which helps to significantly reduce the computation requirements when compared to using raw pixels. The VQ-VAE is composed of an encoder $E(x)$ that encodes a video or an image ${x}\,{\in}\,\mathbb{R}^{T{\times}H{\times}W{\times}3}$ into a compressed latent representation $ z \in \mathbb{R}^{t\times h \times w \times c} $ and a decoder $D$ that reconstructs the input data from the latent $\widetilde{x}=D(z)$. 

The inverse diffusion process is modeled by applying a learnable function $f_\theta(z_t, t)$ to the noised latents at each step to recover the original input. More formally, $f_\theta(z_t) \approx \nabla \log p(z_t)$, where $p(z_t)$ is the probability density function of the latent space at step $t$,  $z_t = \sqrt{\alpha(t)}z_0 + \sqrt{1-\alpha(t)}\epsilon$ is a noisy version of $z_0$, $\epsilon  \sim  \mathcal{N}(\mathbf{0}, {\mathbf{I}})$, $t \in [0, 1]$, and $\alpha(t)$ is a predefined monotonically decreasing function from 1 to 0.

The function $f_\theta$ is parameterized by a neural network, which is trained with the denoising objective defined as 
\begin{equation*}
\mathbb{E}_{z{\sim}p_{data}, t\sim \mathcal{U}(0, 1), \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \big[\|\epsilon - f_{\theta} (z_t; C, t)\|^2\big],
\end{equation*}
where $C$ is the condition, \eg, class labels or text prompts.

Beginning with a noise sample $z_1 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, an iterative sampling process repeatedly applies the model $f_\theta(z_t; C, t)$ to progressively refine an estimate of a clean latent sample $\hat{z}_0$.  This refined latent sample is then decoded, transforming it back into the pixel space.
In this work, we do not make use of the decoder since we extract features from the main transformer module.

\subsection{Windowed-Attention Latent Transformer}
\label{sec:method:walt}

Windowed-Attention Latent Transformer (WALT) \cite{walt}, a transformer-based video diffusion model conditioned on text prompts, is selected for this study because the same architecture can be used for both image and video generation, leading to a fair comparison. Although, a study with a single diffusion model is not ideal, the same fair comparison is not feasible with open-sourced video diffusion models given the architecture size disparities with their image counterparts. For example, there is a significant architecture size disparity between Stable Image Diffusion 2.1 (SD 2.1)~\cite{rombach2022high} and its temporal extension, Stable Video Diffusion (SVD)~\cite{blattmann2023stable}. More precisely, SD 2.1 uses a U-Net architecture with 865 million parameters, while SVD  inserts temporal convolution and attention layers after every spatial convolution and attention layer of SD 2.1, which leads to an additional 656 million parameters.




\input{figs/architecture}

WALT is used as a frozen backbone to train light-weight readout heads for downstream perception tasks. It leverages a causal 3D CNN encoder of the MAGVIT-v2 tokenizer \cite{magvitv2} to jointly compress images and videos into a shared latent space, which allow the model to be trained on massive image-text and video-text datasets.



The input to the model is a batch of latent tensors $ z \in \mathbb{R}^{(1+m)\times h \times w \times c} $, generated by the 3D CNN encoder, which are first passed through an embedding block, referred to as Block 0, to be independently encoded as a sequence of non-overlapping patches along with learned position embeddings \cite{AttentionIsAllYouNeed}.
The first frame is encoded independently from the remaining video frames, allowing static images to be treated as videos with a single frame. In particular, the WALT checkpoints used in this paper were shared by the authors and were trained to process sequences of 17 frames.  The video frames are tokenized by the 3D CNN encoder with 5 temporal latents, where the first latent represent the initial frame and the remaining $m=4$ represent the remaining 16 frames. In terms of the spatial compression factor of the latents, it is set to 8 for both width and height.

The WALT architecture introduces a design variation for the transformer in order to reduce the high compute and memory cost of processing image and video tokens. The variation consists of computing self-attention in windows instead of using the traditional global self-attention modules. More precisely, the transformer consists of a concatenation of $L$ window-restricted attention blocks, alternating between spatio-temporal-window blocks and spatial-window blocks. For the case of images, the spatio-temporal-window blocks use an identity attention mask, ensuring that any given latent only attend to itself. This architectural choice enables joint training, where the spatial blocks independently process images and video frames, while the spatio-temporal blocks model motion and temporal dynamics in videos. Also, in terms of inference, it enables both image and video generation modes. 

The WALT model is trained on conditional information such as text embeddings, previously generated samples, and past frames for auto-regressive generation of videos.
While the original WALT model is a cascaded diffusion model with super-resolution stages, we only investigate the base model that generates videos at low resolution in this work.


\subsection{WALT for images and video}
\label{sec:method:walt_variants}
WALT can be used in both video generation, referred to as \vwalt, and image generation modes. When used for image generation, any given latent in the spatio-temporal blocks only attend to itself. For comparison purposes, one drawback of such design is that not only the temporal attention is removed but also the spatial attention. Given that the spatio-temporal blocks in \vwalt perform both spatial and temporal attention, it is fairer to compare with an image counterpart of the WALT model where window-restricted spatio-temporal attention blocks are replaced by window-restricted spatial attention blocks of the same number of parameters. In that way, we can directly measure the impact of adding temporal attention. Such image counterpart was trained for this paper and is referred to as \iwalt. Note that the window-restricted spatial and spatio-temporal blocks only differ in the windows sizes, and therefore, \iwalt and \vwalt share the same architecture.

For training \vwalt, an internal dataset composed of images and videos was used. The same dataset was employed for training \iwalt, but instead of using full videos, frames were randomly extracted from each video. The training settings of \iwalt are the same as WALT.

\subsection{Probing Framework}
\label{sec:method:probing_framework}


A probing framework is introduced to extract video representations from the WALT model and subsequently apply a task-specific readout head for various video understanding tasks. The process starts by adding noise to the latent representations of the input data at a time step $t$ to simulate the forward diffusion process before passing them through the denoiser model. Only a single forward pass of the input through the diffusion model is necessary to extract its visual representation, as opposed to going through the entire multi-step generative diffusion process. The forward pass uses a null text embedding. Subsequently, activations of the transformer intermediate blocks are extracted to train task-specific readout heads that will interpret the learned representations. A diagram illustrating the probing framework is shown in \cref{fig:architecture}.
In order to determine an adequate timestep $t$ and the most representative activation block $l$, we run ablations described in \cref{sec:exp:featureextraction} and showcased in \cref{fig:noise_and_blocks}.



The WALT model is evaluated on representative visual perception tasks, ranging from pure semantics to spatio-temporal understanding: image classification, action recognition, monocular depth estimation, relative camera pose prediction to visual correspondence.
For the evaluation on visual perception tasks, we largely follow the probing methodology of \citet{s4dpaper}.



\paragraph{Image classification}
Image classification, characterized by its purely semantic nature, is one of the most fundamental areas in computer vision. Within this area, the tasks of object classification, scene recognition and fine-grained visual classification are selected for downstream evaluation of WALT. 
An attentive readout is used for this task \cite{vjepa}. The cross-attention layers are trained with a learnable query token, and the output of the cross-attention is added to the query token and then passed to a two-layer MLP with GeLU activation, followed by layer normalization and a linear classification layer.
The readout is trained with the softmax cross-entropy loss.
\Timagenet~\cite{imagenet} and \Tplaces~\cite{places365} are used for object and scene classification, respectively. For fine-grained visual classification we use iNaturalist 2018 (\Tinat)~\cite{inaturalist} which contains visually similar plant and animal species.
Top-1 classification accuracy is used for all the image classification tasks.

\paragraph{Action recognition}
Understanding actions in videos often requires capturing temporal dependencies between frames, \ie the model needs to understand how actions unfold over time and how earlier frames relate to later ones to accurately classify the action.
As above, we use attentive readout, though over all video frames here.
Kinetics-400 and 700 (\Tkfour, \Tkseven)~\cite{kinetics400, k700} are used for appearance-focused action recognition.
For more motion-sensitive action recognition, Something-Something v2 (\Tssv)~\cite{ssv2} is used.
We report top-1 classification accuracy.


\paragraph{Monocular depth prediction}
Monocular depth estimation, referred hereafter as \Tscannet, is a 3D perception task that aims at predicting the distance of surface elements in the scene from the camera. Unlike traditional geometric correspondence and triangulation techniques, this requires only a single image. However, it can also be calculated from video to leverage temporal dependencies between frames. Monocular depth estimation is a fundamental problem in computer vision as it bridges the gap between 2D images and the 3D world.
For the readout, we use the decoder of the Scene Representation Transformer~\cite{SRT} which is composed of a small number
of cross-attention layers followed by an MLP.
Fourier positional encoding~\cite{nerf} applied to the input latents is used to generate a set of queries for the decoder. Each depth pixel value is decoded independently by a transformer that crossattends from a query into the latent features generated by the pretrained model, thereby aggregating relevant information from the latents to predict depth. For training the readout, an L2 loss between the prediction and the ground truth depth map is used. 
We use ScanNet~\cite{scannet}, a dataset of RGB-D videos of indoor scenes.
The mean of the absolute relative error (AbsRelErr)~\cite{AbsRelErr} between predicted and ground-truth depth is used for evaluation.

\paragraph{Relative camera pose estimation}
Relative camera pose estimation (\Tpose) is about predicting the relative 6D camera poses between the first and last frames of a video sequence. The pose matrix is defined as $P = [R, t]$, where $R$ and $t$ denote the rotation matrix and translation vector, respectively.
The attention readout for action recognition is also utilized for this task. Since the predicted rotation matrix may not be a true rotation matrix in SO(3), the Procrustes algorithm~\cite{bregier2021deep} is applied to the predicted matrix to find the closest true rotation matrix. The readout is trained by minimizing the L2-loss between predicted and ground-truth pose matrices.

We use the RealEstate10k dataset~\cite{zhou2018stereo} which is comprised of indoor and outdoor property videos. The pose annotations are derived from a traditional SfM pipeline, so we rescale camera poses to metric units in order to address scale ambiguities \cite{watson2024controlling}.
The estimated poses are evaluated using mean end-point-error, a metric that measures the mean distance between ground-truth ($P_i$) and estimated ($\hat{P}_i$) pose matrices. More formally, $e_{\text{EPE}}(\hat{P}_i, P_i) = \frac{1}{M}\sum_{j=1}^M \| P_i(Y_j) - \hat{P}_i(Y_j) \|$, where, $\{Y_j\}_{j=1,\dots,M}$ is a set of 3D points selected for metric calculation. In this study, 8 auxiliary points, forming a virtual cube in front of the camera of the first frame, are used for computing $e_{\text{EPE}}$.

\paragraph{Visual correspondence tasks}
Visual correspondence is at the heart of video understanding, as it requires modeling how physical surfaces move and deform over time. In this paper, two correspondence tasks, namely point tracking and box tracking, are selected for evaluation and referred hereafter as \Tpt and \Twaymo, respectively.


The same readout head proposed in MooG \cite{moog} is adopted. Given a set of initial $N$ points (or boxes) at time $t=1$, $q_1{\in}\,\mathbb{R}^{N{\times}D_q}$, and a sequence of observed frames $\{X_t\}_{t=1}^T$, the goal is to predict all future targets, $\{q_t\}$ for $t = 2, \ldots, T$. A latent representation is assigned to $q_1$ by first encoding it with positional encoding followed by an MLP. Then, the latents for $t = 2, \ldots, T$ are generated in a recurrent fashion. At step $t$, they are first predicted by an MLP-based predictor using only the corrected latents at time $t-1$, and then, they are corrected by a transformer, in which the frames are encoded and cross-attended to using the latent predictions as queries to produce the corrections. To generate the final target values, an MLP head is applied to the latents $y_t$. 

The final targets for \Tpt are normalized image coordinates, visibility, and prediction certainty.  A point is considered visible during evaluation only if the model predicts it is visible and has over 50\% confidence in its location. Following MooG \cite{moog}, we use a combined loss function, which includes a Huber loss for location accuracy and Sigmoid Binary Cross Entropy losses for visibility and certainty.  For points that are no longer in the scene, only the visibility loss is applied.
For training the box tracking readout, an L2 loss between the prediction and the normalized box coordinates is used.

We train the point tracking readout head on Kubric {MOVi-E}~\cite{movi-e} labeled with point annotations computed in a similar manner as in~\cite{moog}.
For evaluation, the Perception Test dataset~\cite{perception-test} is used. Sixty four points per frame are sampled and the location of each point in the first frame is used as the query. The average Jaccard (AJ) as in~\cite{moog}, which evaluates both occlusion and position accuracy, is used as performance metric for \Tpt. 
The Waymo Open dataset~\cite{waymo} is used for both training and evaluation of the box tracking readouts and the average IoU (excluding the first frame in the sequence for which ground truth is provided) is used as performance metric.









