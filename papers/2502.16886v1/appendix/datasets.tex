\section{Datasets Used in Experiments}
\label{app:datasets}
In this section, we provide a comprehensive overview of all the tasks and datasets utilized in the experiments in this paper.
\paragraph{Math \& Science} This task evaluates the model's ability to tackle mathematical and scientific problems. By directly inputting questions and comparing the model's output with the correct answers, we calculate the model's \textit{Accuracy} on these datasets: \textbf{GSM8K} is a dataset for evaluating model's math-solving skills, featuring 8,000 elementary-level math word problems requiring basic arithmetic and reasoning. \textbf{GPQA} tests model's understanding of physics concepts and problem-solving across various topics, assessing scientific reasoning abilities. \textbf{TheoremQA} evaluates model's grasp and application of mathematical theorems, ranging from simple applications to complex proofs, testing advanced math skills.
\paragraph{Commonsense Reasoning (CR)} This task evaluates model's ability to make deductions and understand everyday situations using implicit knowledge and logical inference. \textbf{TruthfulQA} (ThQA) evaluates model's ability to generate accurate and truthful responses, testing models on distinguishing fact from fiction, especially in areas prone to misconceptions. We use \textit{BLEU} as the metric. \textbf{CoQA} assesses model's ability to understand and respond to questions in a conversational context, focusing on maintaining coherence and context throughout a dialogue. We use \textit{F1 Score} as the metric.

\paragraph{Single Document QA (Single-Doc QA)} This task assesses the model's reading comprehension skills when dealing with a single, extended document. \textbf{NarrativeQA}~\cite{DBLP:journals/tacl/KociskySBDHMG18} is a dataset designed to evaluate model's ability to comprehend and answer questions based on narrative texts, focusing on understanding stories and their underlying themes. \textbf{Qasper}~\cite{DBLP:conf/naacl/DasigiLBCSG21} is a dataset aimed at assessing model's capability to extract and answer questions from academic papers, emphasizing understanding complex scientific information. We employ \textit{F1 Score} as the metric for above two datasets.

\paragraph{Multi-Document QA (Multi-Doc QA)} This task evaluates the model's reading comprehension capabilities across multiple extended documents. \textbf{2WikiMultiHopQA} (2WKMQA)~\cite{DBLP:conf/coling/HoNSA20} is a dataset designed to test model's ability to perform multi-hop reasoning and answer complex questions using information from multiple Wikipedia articles. \textbf{MuSiQue}~\cite{DBLP:journals/tacl/TrivediBKS22} evaluates model's skill in integrating and reasoning over information from multiple sources to answer comprehensive questions accurately. We leverage \textit{F1 Score} as the metric for above two datasets.

\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule[1.3pt]
\textbf{Methods}    & \textbf{GSM8K} & \textbf{CoQA}  & \textbf{NQA} & \textbf{Musique} & \textbf{QMSum}   \\ \cmidrule{1-6}
Full      & 75.28 & 52.74 & 24.06  & 14.77   & 22.27 \\ \cmidrule{1-6}
Pos$_{t=1\%}$ & \multicolumn{5}{c}{\textit{Performance Using Different $k$}} \\\arrayrulecolor{gray!60}\hline
$k=1$   &  76.50     & 52.86      & 23.44       & 15.96        &    21.95 \\
\textit{Budget}    &  \textit{93.2\%}     &  \textit{81.5\%}     &  \textit{48.7\%}      & \textit{43.7\%}        &   \textit{15.0\%}  \\
$k=1\% n$  &  76.19     & 52.87      &   23.17     &    15.40     &   21.94    \\
\textit{Budget}    &    \textit{95.1\%}   &    \textit{94.8\%}   &   \textit{77.3\%}    &  \textit{77.2\%}       &  \textit{59.5\%}    \\
$k=2\% n$  &  76.19     & 52.82      &   22.87     &    14.34     &   21.74    \\
\textit{Budget}    &    \textit{96.0\%}   &    \textit{96.1\%}   &   \textit{69.8\%}    &  \textit{62.0\%}       &  \textit{47.1\%}    \\
$k=3\% n$  &  75.82     & 52.80      &   23.03     &    13.61     &   21.56    \\
\textit{Budget}    &    \textit{98.6\%}   &    \textit{95.7\%}   &   \textit{60.3\%}    &  \textit{46.0\%}       &  \textit{39.5\%}    \\
$k=4\% n$  &  75.21     & 52.76      &   22.96     &    13.45     &   21.47    \\
\textit{Budget}    &    \textit{98.8\%}   &    \textit{95.8\%}   &   \textit{51.9\%}    &  \textit{35.2\%}       &  \textit{35.4\%}    \\

$k=5\%n$  &    75.59   &   52.75    &   21.62     &   13.71      &    21.43   \\
\textit{Budget}    &   \textit{98.6\%}   &   \textit{96.2\%}    &    \textit{45.1\%}    &  \textit{30.0\%}       &     \textit{33.1\%}  \\
$k=6\% n$  &  75.66     & 52.81      &   21.95     &    14.33       &   20.94    \\
\textit{Budget}    &    \textit{98.5\%}   &    \textit{96.6\%}   &   \textit{40.4\%}    &  \textit{25.8\%}       &  \textit{31.6\%}    \\
$k=7\% n$  &  76.57     & 52.88      &   21.18     &    13.66     &   20.94    \\
\textit{Budget}    &    \textit{98.0\%}   &    \textit{96.7\%}   &   \textit{37.3\%}    &  \textit{22.5\%}       &  \textit{30.6\%}    \\
$k=8\% n$  &  76.35     & 52.86      &   20.36     &    13.75     &   21.11    \\
\textit{Budget}    &    \textit{97.4\%}   &    \textit{96.7\%}   &   \textit{35.1\%}    &  \textit{20.9\%}       &  \textit{30.1\%}    \\
$k=9\% n$  &  76.65     & 52.84      &   19.99     &    13.66     &   20.76    \\
\textit{Budget}    &    \textit{96.8\%}   &    \textit{96.8\%}   &   \textit{33.4\%}    &  \textit{19.9\%}       &  \textit{29.5\%}    \\
$k=10\%n$   &  76.72     &  52.85     &     9.74   &  13.67       &   21.11    \\
\textit{Budget}    &  \textit{96.3\%}     &  \textit{96.8\%}    &   \textit{32.0\%}     &   \textit{19.7\%}      &   29.3\%    \\
\arrayrulecolor{black}\bottomrule[1.3pt]
\end{tabular}
}
\caption{Performance comparison with different $k$values, ranking methods and universal thresholds.\label{extend_ablation}}
\end{table}


\paragraph{Summarization} This task examines the model's ability to comprehend and summarize lengthy documents. \textbf{QMSum}~\cite{DBLP:conf/naacl/ZhongYYZMJACLQR21} is a dataset for evaluating model's ability to generate concise summaries of meeting transcripts, focusing on capturing the key points from multi-party discussions. \textbf{Multi-News} (M-News)~\cite{DBLP:conf/acl/FabbriLSLR19} is a dataset that challenges models to create coherent summaries by synthesizing information from multiple news articles on the same topic. We use \textit{Rouge-L} as the metric for above two datasets.

\paragraph{Few-Shot Learning (FSL)} This task assesses the model's few-shot learning capabilities. \textbf{TriviaQA}~\cite{DBLP:conf/acl/JoshiCWZ17} is a dataset designed to assess model's ability to retrieve and answer questions based on large collections of trivia, emphasizing comprehension and factual recall. We use \textit{F1 Score} as the metric.

\paragraph{Code} This task evaluates the model's ability to complete and generate code. \textbf{LCC}~\cite{DBLP:conf/icml/GuoXD0M23} is a dataset focused on evaluating models' ability to understand and generate code by considering extended code contexts, enhancing the ability to reason over complex programming structures. We use \textit{Edit Sim} as the metric.