\section{Full Algorithm of \method}

\begin{algorithm}
\caption{\method \label{al}}
\begin{algorithmic}
    \STATE \textbf{Input:} Prompt, Threshold $t$
    \STATE \textbf{Output:} Compressed KV Cache
    \STATE Create Empty List $K_c,V_c$
    \FOR{\textit{Transformer Layer $L_i$ in LLM}}
        \STATE $Q^i,K^i,V^i \leftarrow L_i(\text{Prompt})$
        \STATE $R^i \gets$ Postion-Based Importance Rank
        \STATE $A_{last}^i\gets \operatorname{Attention}(Q^i[\dots,-1,:], K^{iT})$
        \STATE $F_{b}^i \gets \operatorname{Frobenius}(A_{last}^i)$
        \STATE $A_{last}^i\gets \operatorname{Square}(A_{last}^i)$
        \STATE Reorder $A_{last}^i$ by Rank $R$
        \STATE $A_{cumsum}^i \gets \operatorname{Cumsum}(A_{last}^i)$
        \STATE $A_{cumsum}^i \gets \operatorname{Sqrt}(A_{cumsum}^i)$
        \STATE $A_{ratio}^i \gets (F_b^i - A_{cumsum}^i)/F_b^i$
        \STATE $\text{Index}^i \gets \operatorname{Max}(\operatorname{Where}(A_{ratio}^i<=t))$
        \STATE $K_c^i \gets \operatorname{Compress} K^i \text{by} R^i[I:]$
        \STATE $V_c^i \gets \operatorname{Compress} V^i \text{by} R^i[I:]$ 
        \STATE Append $K_c^i,V_c^i$ to $K_c,V_c$
    \ENDFOR
    \RETURN $K_c,V_c$
\end{algorithmic}
\end{algorithm}

\section{Freeze the first two layers of LLM}
\label{app:freeze}

In this section, we elaborate the necessity to freeze the first and second layers in \method as mentioned in Section~\ref{method}. 
In our preliminary studies, we apply \method to Llama2-7B-Chat and conduct experiments on GSM8K and CoQA. We first perform a case study, followed by a comparison with the effects of not freezing the bottom layers.

As shown in Table~\ref{case}, when the threshold is set to 1\% and no layers are frozen, the outputs for two examples on GSM8K and CoQA are incorrect and lack logical coherence. However, the budget exceeds 90\% in both cases. By observing the actual budget of each layer, we can see that for these two examples, the budgets for layer 0 and 1 are relatively low, while the budgets from layer 2 to layer 30 become very high, with the last layer being low again. We hypothesize that the model is still encoding the global semantics in its early layers, thus not yet able to identify truly important tokens in the first two layers, of which the attention distribution is relatively uniform.
This phenomenon has also been observed by early works on Transformers analysis \cite{ethayarajh-2019-contextual,gari-soler-apidianaki-2021-lets}.
Not freezing early layers may lead to early eviction of important tokens, resulting in subsequent generations being unable to access this information, ultimately causing the output to fail.

Based on the above case study, we attempt to freeze certain layers of the model and explore the optimal freezing configuration. We continue to validate the results of freezing different layers, and the resuls are shown in Table~\ref{freeze}.
We can observe that freezing the first two layers achieves a balance between model performance and budget. Moreover, freezing the last layer does not significantly enhance the model's performance and instead leads to an increase in budget. \method ultimately opts to begin KV cache compression from the 2nd layer of the model.
\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{ll}
\toprule[1.3pt]
\multicolumn{2}{c}{\textbf{GSM8K}}             \\ \cmidrule{1-2}
Input            & \begin{tabular}[c]{@{}l@{}}A robe takes 2 bolts of blue fiber a-\\nd half that much white fiber. How\\many bolts in total does it take?\end{tabular}             \\ \cmidrule{1-2}
Budget & \begin{tabular}[c]{@{}l@{}}\textit{Layer 0}: \textcolor{red}{{67.71}}\\ \textit{Layer 1}: \textcolor{red}{{82.29}}\\ \textit{Layer 2$\sim$30:} 95.83\\ \textit{Layer 31}: 37.50\\\textbf{Avg.}: 93.90\end{tabular}   \\ \cmidrule{1-2}
Output          & \begin{tabular}[c]{@{}l@{}}I have determined by answering 
the\\answer to the format of bolts bolts b-\\olts...(repeat) \end{tabular}                            \\  \cmidrule{1-2}
Ground-Truth     & 3                  \\\toprule[1.3pt]
\multicolumn{2}{c}{\textbf{CoQA}}              \\  \cmidrule{1-2}
Input            & \begin{tabular}[c]{@{}l@{}}You are given a story and a question.\\Answer the question as concisely as\\you can...Question: What color was\\Cotton?\end{tabular} \\ \cmidrule{1-2}
Budget & \begin{tabular}[c]{@{}l@{}}\textit{Layer 0}: \textcolor{red}{{42.14}}\\ \textit{Layer 1}: \textcolor{red}{{42.54}}\\ \textit{Layer 2$\sim$30}: 99.19\\ \textit{Layer 31}: 79.64\\\textbf{Avg.}: 95.03\end{tabular}   \\ \cmidrule{1-2}
Output           & \begin{tabular}[c]{@{}l@{}}Question: Question: What is the que-\\stion: What is the question:\end{tabular}            \\ \cmidrule{1-2}
Groud-Truth      & White        \\             \bottomrule[1.3pt]                                                                                           
\end{tabular}
}
\caption{Case Study.\label{case}}
\end{table}



\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{llllllll}
\toprule[1.3pt]
\textbf{Datasets} & \textbf{Layer} & \textbf{None}  & \textbf{0}      & \textbf{0,1}    & \textbf{0,1,2} & \textbf{0,1,2,3} & \textbf{0,1,31} \\ \cmidrule{1-8}
GSM8K   & Ours$_{k=1}$         & 0.014 & 0.250  & 0.264  & 0.252 & 0.264   & 0.252  \\
        & \textit{Budget}       & \textit{93.1\%}  & \textit{97.0\%} & \textit{95.4\%} & \textit{97.5\%}  & \textit{97.7\%}    & \textit{98.2\%}   \\  \cmidrule{1-8}
CoQA    & Ours$_{k=1}$          & 1.47  & 52.80  & 53.58  & 53.58 & 53.32   & 53.46  \\
        & \textit{Budget}       & \textit{92.7\%} & \textit{94.7\%}  & \textit{95.5\%}   & \textit{98.3\%} & \textit{98.4\%}    & \textit{99.2\%}  \\ \bottomrule[1.3pt]
\end{tabular}
}
\caption{Performance of Llama2-7B-Chat on GSM8K and CoQA using \method with different frozen layers.\label{freeze}}
\end{table}
