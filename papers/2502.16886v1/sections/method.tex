\section{Methodology}
\label{method}

We first elaborate our objective in Sec.~\ref{ssec:obj} as a new direction for KV cache compression. Distinct from previous works, we aim to compress the KV cache as much as possible while ensuring LLMs' performance remains intact. 
We then delineate our proposed approach along with its motivation behind in Sec.~\ref{ssec:method}. Important implementation details are next discussed in Sec.~\ref{ssec:impl}.

\subsection{Objective of KV Cache Compression}
\label{ssec:obj}

As mentioned in Sec.~\ref{sec:intro}, most previous works on KV cache pruning require a pre-defined memory budget beforehand, either by fixing the number of cached positions directly \cite{DBLP:journals/corr/abs-2406-02069,DBLP:conf/nips/LiHYVLYCLC24}, or retaining positions by a fixed ratio of input length \cite{DBLP:conf/nips/Zhang00CZC0TRBW23,DBLP:journals/corr/abs-2406-13035}. While these approaches are effective in certain circumstances, they may encounter practical challenges when deployed in real-world scenarios.

The biggest issue is that the optimal budgets for LLM inference in real-world scenarios are evidently infeasible to enumerate, as they could vary across tasks and domains, especially for open-domain instructions. 
For example, mathematical tasks are typically concise and require inference based on all given conditions, thus many positions may logically contribute to the generation, necessitating a relatively higher memory budget. 
Conversely, for article reading comprehension, LLM may only need a small set of critical positions for the answer generation, featuring a smaller budget in many cases.

As a side effect, inference with previous KV cache pruning is likely to experience performance instability across different inputs, which can be observed by our analysis in Sec.~\ref{ssec:main}. Overall, the practical value is thereby diminished.

In this work, we resort to a dynamic paradigm that eliminates the need for manually setting a memory budget.
We introduce a new objective for KV cache compression, aiming to automatically reduce the KV cache as much as possible, while fulfilling the condition that the method should \textbf{always strike towards full-cache performance}.
In this way, one could utilize such pruning method off-the-shelf, without spending time and resources on tuning usage-specific hyperparameters, which we hope would further advance the research development of KV cache compression techniques.

\subsection{\method}
\label{ssec:method}

Towards our objective, we design a novel ranking-based pruning method applied after the prefilling stage, consisting of two steps. First, all tokens are ranked according to an importance metric per Transformers layer. Then, the system keeps removing the KV cache sequentially, until hitting a stopping criterion.
Ultimately, our proposed approach does not need to fix an optimal budget for storing KV cache. Rather, the stopping criterion is tied to the norm of attention matrices that works uniformly regardless of the inputs.

The motivation behind our two-step procedure comes from two phenomena brought by previous works.
First, there exists \emph{position bias} such that positions in the beginning and the end are typically important for the subsequent generation, as observed by StreamingLLM \cite{DBLP:conf/iclr/XiaoTCHL24}. Second, \emph{attention scores} are usually an effective indicator of the token importance, as leveraged by many pruning techniques \cite{DBLP:conf/nips/LiuDLWXXKS23,DBLP:conf/nips/Zhang00CZC0TRBW23,DBLP:conf/nips/LiHYVLYCLC24}. For our objective, we propose to combine the two important properties in our approach, described in details as follows.

\paragraph{Importance Ranking}
Denote a LLM input sequence as $X=\{x_1,x_2,\dots,x_n\}$, where each Transformers layer originally consists of $n$ positions of KV vectors per attention head.
As the pruning process starts from the least important KV vectors, all tokens are firstly ranked according to an importance strategy.

To this end, we resort to a simple ranking strategy leveraging the \emph{position bias}, without undergoing any additional computation.
As identified by StreamingLLM, a significant portion of attentions is allocated to the initial tokens, regardless of their relevance to the language modeling task, known as \emph{attention sinks}. Inspired by this, we employ a purely position-based importance evaluation strategy:
we assume that the first $m$ tokens are always crucial; while among the remaining $n - m$ tokens, those towards the end are more significant. By this strategy, the importance in $X$ is ranked from the highest to the lowest as  $\{x_1,x_2,\dots,x_m, x_n,x_{n-1},\dots,x_{m+1}\}$, denoted as $\widehat{X}$, where $m$ is a chosen hyperparameter that works regardless of specific input sequences.

It should be noted that our choice on importance ranking is empirically supported rather than theoretically based. We have experimented other attention-based importance ranking, which are provided in Sec.~\ref{ssec:ablation}. Our position-based strategy is shown both empirically effective and superior on computational overhead.

\paragraph{Stopping Criterion}
With the initially ranked KV vectors, our method then sequantially decides whether to continue or terminate pruning, which needs an evaluation metric serving as the stopping criterion.
Since we evidently cannot know the precise impact of a KV cache position before the generation, we require a metric that correlates well with the resulting performance change when discarding a position, \textbf{serving as a bridge to ensure a minimal degradation upon the full KV-cache performance}.

Inspired by~\citet{DBLP:journals/corr/abs-2406-11430}, we utilize the Frobenius norm of the attention matrix that fits the requirement well empirically. 
For a matrix $M \in \mathbb{R}^{m\times n}$, the Frobenius norm of $M$ is the matrix-version of the L2 norm, expressed as $||M||_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n|M_{i,j}|^2}$.

We conduct preliminary experiments to validate its reliability. We employ Llama2-7B and randomly mask a certain portion of the KV cache, setting the corresponding values in the attention matrix $A\in \mathbb{R}^{n\times n}$ to zero.
As illustrated in Figure~\ref{correlation}, there is a certain degree of positive correlation between its Frobenius norm and the model's performance, which are shown generalized across different domains.

\begin{figure}[!t]
   \centering
   \includegraphics[width=0.8\columnwidth]{figures/line_plot.pdf}
        \caption{Preliminary experiments on the relationship between the Frobenius norm difference (in percentage) obtained from randomly masking tokens, and the corresponding model performance on GSM8K.}
    \label{correlation}
% \vspace{-1em}
\end{figure}

As the input sequence length $n$ increases, the time and space overhead for norm calculation also grows significantly by $O(n^2)$. Hence, we next aim to reduce the computational scale from the naive norm calculation. We observe that the Frobenius norm of the last row of $A$ also correlates with the model's performance; equivalently saying, the attention distribution of the latest token is good enough to serve as an estimator.
Formally, we utilize the last $k$ rows (latest $k$ tokens) of $A$ and reduce to a single attention vector $A'\in\mathbb{R}^{1\times n}$. The score $s_j$ for a position $j \in [1,n]$ in $A'$ is:
\begin{align}
 s_j = \frac{\sum_{i=k}^{n}A_{i,j}}{\sum_{i=k}^n\mathbf{1}_{\{A_{i,j}\neq 0\}}}
\end{align}

We then use the Frobenius norm of $A'$, equivalently its L2 norm, as the evaluation metric, which reduces the overall complexity to $O(n)$.

Finally, for each position in the ranking order, our method compares the difference between the Frobenius norm of the original attention matrix $A$, and the Frobenius norm of the reduced attention vector $A'$ of which the corresponding positions are set to $0$. We set a \textbf{universal threshold} as 1\%, such that when the new norm exceeds 1\% difference from the original norm, it hits the stopping criterion. The KV vectors of the remaining positions are then all kept, while the preceding positions would have their KV cache discarded. In this way, our method achieves dynamic KV cache pruning while targeting to maintain the full-cache performance, assisted by the norm metric.

\subsection{Implemetation Details}
\label{ssec:impl}

Though our proposed pruning process is conceptually sequential, it is implemented efficiently by PyTorch's operators, such that the stopping positions of all layers are identified directly in parallel without any looping operations. Empirical analysis in Sec.~\ref{ssec:time} shows that our proposed method achieves higher efficiency compared to three popular baselines.

Specifically, we reorder $A'$ based on Token Importance Ranking $\widehat{X}$ to obtain $A'_{sort}$, where $A'_{sort}[0]$ corresponds to the least important token and $A'_{sort}[-1]$ corresponds to the most important token. Next, we compute the cumulative square-sum of each element in $A'_{sort}$:

\begin{align}
A'_{cumsum}[i] = \sqrt{\sum_{k=i}^{n} |A'_{sort}[k]|^2}
\end{align}

such that $A'_{cumsum}[i]$ represents the Frobenius Norm of the attention matrix after removing all tokens to the left of $i^{th}$ token. We then divide each element in 
$A'_{cumsum}$ by $||A'||_F$ and subtract this value from 1 to determine the difference between each new matrix and $A'$. The torch \emph{where} operation allows us to directly identify the position of the token that satisfies the condition of not exceeding a 1\% difference, ultimately yielding the set of tokens for which the KV cache is called to retain. The final kv cache compression algorithm of \method is presented in Algorithm~\ref{al}. Since PyTorch allows specifying the dimension for operations, \method can run in parallel across multiple attention heads.

\paragraph{Retaining Bottom Layers} Additionally, we have discovered that applying \method to the model's 0th and 1st layers results in poor performance when the model operates under a high budget. We hypothesize that this may be due to the relatively uniform distribution of attention scores in these initial layers, leading to the premature discarding of tokens that are crucial for subsequent layers, ultimately causing the entire model's inference to collapse. A more detailed analysis of this issue is provided in Appendix~\ref{app:freeze}. Based on this finding, we initiate the KV cache compression operations starting from the model's 2nd layer.