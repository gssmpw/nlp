\section{Related Work}
\paragraph{KV Cache Compression} Recent advancements in KV cache compression have emerged to address memory constraints for model inference. Scissorhands~\cite{DBLP:conf/nips/LiuDLWXXKS23} observes the repetitive nature of attention patterns and the persistence of token importance. It then preserves the most important tokens based on attention scores to achieve KV cache compression. H2O~\cite{DBLP:conf/nips/Zhang00CZC0TRBW23} introduces dynamic eviction policies that strategically balance retention of recent tokens and historically significant "heavy hitters".  StreamingLLM~\cite{DBLP:conf/iclr/XiaoTCHL24} enables infinite-length sequence processing without fine-tuning by stabilizing attention through landmark tokens.  SnapKV~\cite{DBLP:conf/nips/LiHYVLYCLC24} enhances compression efficiency through attention score-based selection and clustering of critical KV positions. FastGen~\cite{DBLP:conf/iclr/Ge0LZ0024} proposes an adaptive KV cache management system that customizes retention strategies per attention head.

However, these methods require setting a fixed memory budget, which makes it difficult to consistently maintain full performance across different domains and datasets, limiting their practical application. In contrast, our method imposes no such constraints, targeting full-cache performance while allowing for dynamic budget size.
