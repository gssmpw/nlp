\section{Introduction}
\label{sec:intro}

As most large language models (LLMs) follows autoregressive generation \cite{DBLP:journals/corr/abs-2402-06196,DBLP:journals/corr/abs-2307-03109,DBLP:journals/corr/abs-2303-08774}, they rely on \textit{KV Cache} to store intermediate states during inference. 
Specifically, LLMs access the cached key and value vectors of past tokens during the attention calculation, thereby speeding up inference by re-using these KV cache \cite{DBLP:conf/nips/VaswaniSPUJGKP17,DBLP:conf/emnlp/AinslieLJZLS23,DBLP:journals/corr/abs-1911-02150}. However, as the model size and the input length increase, the memory required to maintain the KV cache also grows proportionally. 
Since these vectors are usually stored within GPU memory, managing the KV cache efficiently has become crucial to mitigate the overall memory consumption and computational overhead during LLM inference.
For instance, Llama3 8B model~\cite{DBLP:journals/corr/abs-2407-21783} requires 1 GB of KV cache memory for 2K-token inputs, while its 70B counterpart demands a gigantic memory up to 50GB for 20K tokens. 

Consequently, recent works have proposed to effectively reduce KV cache through pruning, leveraging the \emph{sparsity} of the attention mechanism: \textbf{certain positions are more pivotal} during the generation process, while those less important ones could be pruned with minimal performance degradation; thus, the full KV cache is not always necessary.
These observations have led to several optimization methods, such as H2O \cite{DBLP:conf/nips/Zhang00CZC0TRBW23}, ScissorHands \cite{DBLP:conf/nips/LiuDLWXXKS23}, StreamingLLM \cite{DBLP:conf/iclr/XiaoTCHL24}, and FastGen \cite{DBLP:conf/iclr/Ge0LZ0024}, where they aim to identify and retain only the most salient token positions, discarding less critical ones based on certain pruning criteria. 
Alongside, other orthogonal paradigms have also been proposed without hard-pruning, such as KVMerger~\cite{DBLP:journals/corr/abs-2407-08454} and D2O~\cite{DBLP:journals/corr/abs-2406-13035} that merge KV vectors, and MLA \cite{deepseekv2} that operates attention in the latent space.

While previous pruning methods have successfully demonstrated that a decent size of KV cache could be discarded in practice, there exists one common limitation that has not been addressed yet: these pruning techniques typically require a \textbf{pre-defined KV cache budget}, of which its optimal threshold could vary significantly according to specific tasks or inputs. 
Hence, in real-world scenarios, one would have to carefully tune many budget thresholds for diverse domains, or set a uniform threshold but likely suffering large degradation on certain tasks. We reckon that neither choice is a satisfactory solution, hindering the widespread adoptation of KV pruning techniques.

To further illustrate, Table~\ref{main_result} shows that when the budget is set to 50\%, Llama3-8B-Instruct using H2O achieves over 98\% of the full-cache performance on NarrativeQA~\cite{DBLP:journals/tacl/KociskySBDHMG18}, whereas under the same conditions on GSM8K~\cite{DBLP:journals/corr/abs-2110-14168}, a widely-used math benchmark, the performance drastically falls to less than 42\% of the full-cache performance.

To address the aforementioned limitation, in this work, we introduce a new objective for KV cache compression: our method aims to \textit{strike towards full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible}. Towards this objective, we propose an adaptive pruning method, dubbed \textbf{\method}, which \textbf{D}ynamically adjusts \textbf{KV} cache retention per input, without the need of pre-defined memory \textbf{Budget}.
Particularly, \method automatically identifies input-dependent KV cache pruning that leads to minimal performance degradation. 
A key implication is that the method tends to realize a higher compression ratio for simpler tasks, while allocating more cache resources for complicated tasks.

Our method features a two-step process: 1) after the prefilling stage, rank all input tokens according to an importance metric; 2) discard the KV cache of each token sequentially, until hitting a stopping criterion. In choosing the importance metric and the stopping criterion, \method takes into account two important factors from previous works: i) position bias, where tokens at the beginning and the end are generally more significant than those in the middle~\cite{DBLP:conf/iclr/XiaoTCHL24,DBLP:conf/nips/JiangLZWLAHA0L024}; ii) attention score, where tokens receiving higher scores usually contribute more information to subsequent generation~\cite{DBLP:journals/corr/abs-2402-06196,DBLP:journals/corr/abs-2307-03109,DBLP:conf/nips/Zhang00CZC0TRBW23}. 

We then propose \method of empirical efficacy and low overhead. 
Concretely, tokens are ranked solely by their positions, with no additional computation involved. 
The ranking strategy signals high significance to the beginning and the end positions, where the initial tokens are the most important, followed by the remaining tokens in a reversing order. 
Next in the pruning process, we design an attetion-based metric to determine the stopping condition, such that the pruning halts when this metric signals that the remaining KV cache is unlikely to match the full-cache capacity, thereby fulfilling our objective to approximate full performance with dynamic cache pruning.

More specifically, our attention metric compares the difference between the Frobenius form of the full attention matrix and the reduced matrix by setting pruned positions' score to zero. When the difference exceeds a universal threshold, the process is halted. Especially, this metric empirically correlates well with the generation performance regardless of inputs, serving as an estimator to bridge the pruning state and the subsequent generation.
Furthermore, the entire process is implemented efficiently through PyTorch's built-in operators, where we show that \method not only optimizes memory space, but also acheives time reduction compared to the full-cache inference and three KV pruning methods.

\begin{figure*}[!t]
   \centering
   \includegraphics[width=0.9\textwidth]{figures/method.pdf}
        \caption{The overall workflow of \method in Section~\ref{ssec:method}. Initially, tokens are ranked based on their positions, followed by the eviction of the least significant tokens (per layer), whose halting condition is determined by the norm value of the reduced attention matrix. The KV cache for the remaining tokens are then preserved.}
    \label{framework}
% \vspace{-0.5em}
\end{figure*}

Experimental results on 13 datasets varying diverse context lengths and tasks, e.g. mathematical and commonsense reasoning, reading comprehension and coding, demonstrate that \method achieves our objective effectively and robustly.
The resulting inference is on par or even surpasses the full-cache performance with multiple LLMs of different sizes, including Llama3~\cite{DBLP:journals/corr/abs-2407-21783}, Qwen2.5~\cite{DBLP:journals/corr/abs-2412-15115}, and Mistral~\cite{DBLP:journals/corr/abs-2401-04088}.
Meanwhile, the dynamic pruning of \method allocates relatively high budget on math benchmarks but attains high compression ratio by up to 85\% on comprehension tasks. The average budget size reaches 63.7\% using Llama3-8B.
The overall results suggest that \method is able to accomplish lossless KV cache pruning with strong generalizability. Besides, it does not rely on specific model architectures and can coexist with other KV cache optimization techniques. We further conduct more experiments for in-depth analysis and ablation studies, depicting the rationality of our various design choices and hyperparameters.

Our contributions can be summarized below:
\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item To the best of our knowledge, we are the first to propose the objective that ensures full-cache performance while maximizing the KV cache pruning, which holds greater practical values and deployment potentials.
\item We introduce \method that employs a straightforward two-step process, where its dynamic pruning is designed with minimal degradation to the generation performance.
\item Extensive evaluation with diverse tasks and models indicates that compared to prior works, our method achieves lossless KV cache compression effectively, efficiently and robustly.
\end{itemize}