\section{Conclusion}

In this study, we introduce an innovative KV cache compression objective designed to approximate the full-cache performance, independent of specific inputs, while optimizing resource utilization through targeted KV cache pruning. Our approach, termed \method, employs a straightforward yet effective two-step process for each layer of LLMs. Tokens are initially ranked solely based on their positions, followed by the eviction of the least significant tokens as determined by the norm value of the reduced attention matrix. Comprehensive experiments conducted across diverse datasets, encompassing a variety of tasks and context lengths, demonstrate that \method achieves nearly lossless compression, while with notable space and time reduction, successfully fulfilling our objective for greater practical values.