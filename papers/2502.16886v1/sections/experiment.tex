\section{Experiments}
\subsection{Experimental Settings}
\label{ssec:ex-set}

\input{tab/main.tex}

\paragraph{Backbones} Our experiments are conducted with three LLMs: Llama3-Instruct with size of 8B/70B, Mistral-7B-Instruct-V0.3, and Qwen2.5-Instruct with size of 7B/32B/72B. We implement \method upon the codebase of SnapKV\footnote{https://github.com/FasterDecoding/SnapKV}. For the reduced attention matrix $A'$, we set $k=1$ in practice (ablation provided in Sec.~\ref{ssec:ablation}), and set $m=4$ for importance ranking.

\paragraph{Datasets} For comprehensive evaluation, we evaluate \method using datasets of both short and long context length. For tasks of relatively short inputs, the models are assessed across mathametics, sicence, and commonsense reasoning on multiple datasets including GSM8K \cite{DBLP:journals/corr/abs-2110-14168}, GPQA \cite{DBLP:journals/corr/abs-2311-12022}, TheoremQA \cite{DBLP:conf/emnlp/ChenYKLWMXWX23}, TruthfulQA \cite{DBLP:conf/acl/LinHE22}, and CoQA \cite{DBLP:journals/tacl/ReddyCM19}. For long-context tasks, we adopt tasks and datasets from Longbench \cite{DBLP:conf/acl/BaiLZL0HDLZHDTL24}.
Appendix~\ref{app:datasets} provides a detailed description and statistics of these datasets along with how they are utilized in experiments.

\paragraph{Evaluation Protocol} 
Our primary objective is to evaluate the capability of \method to compress KV cache while striking for a full-cache performance. To this end, we compare \method with the full KV cache performance and calculate the average compression ratio per dataset. 
Additionally, we select three previous KV cache compression methods with fixed budget size, including: Heavy Hitter Oracle (\textbf{H2O}) \cite{DBLP:conf/nips/Zhang00CZC0TRBW23}, StreamingLLM (\textbf{SLM}) \cite{DBLP:conf/iclr/XiaoTCHL24}, and \textbf{SnapKV} \cite{DBLP:conf/nips/LiHYVLYCLC24}.
We compare with two budgets for these three methods at 90\% and 50\% compression ratio respectively.

Apart from the main experiments, we further evaluate various design factors of \method via ablation studies, such as different strategies of token ranking importance, the selection of $k$ for the attention matrix, and the necessity to retain bottom layers.
Finally, we also provide efficiency analysis on the time and space overhead comparing against other KV cache pruning methods in Sec.~\ref{ssec:time}.

\subsection{Main Results}
\label{ssec:main}

The main results are shown in Table~\ref{main_result} according to our evaluation protocol, where \method demonstrates strong advantages with robust performance across diverse tasks and models:

• \method is able to fulfill our objective, capable of performing near-lossless dynamic compression across different models, varying input lengths, and diverse task types. Interestingly, with Llama3-8B and Qwen2.5-7B, \method even surpasses the full-cache performance by 0.12\% and 2.63\% respectively, utilizing an average of 63.68\% and 76.02\% KV cache. With Mistral, \method also manages to achieve nearly 20\% KV cache compression with only a 1.5\% performance reduction. These results indicate that our proposed method can be practically deployed in real-world scenarios without worrying domain-specific budget thresholds. In stark contrast, previous methods achieve a consistent full-cache performance only when manually determined a high budget ratio, e.g. 90\%. However, when the budget is reduced, e.g. 50\%, the degradation can become severe on certain datasets, distinct from \method that automatically adjusts the pruning to always attain full performance.

\input{tab/size}
\input{tab/ablation}

• \method natually reflects the \emph{difficulty} of the generation task. As in Table~\ref{main_result}, the dynamic budget ratio is high on Math\&Science datasets (over 90\%), while much lower on QA or Summarization datasets (as low as 15\%). This observation is in line with our intuition, where inference on concise but hard tasks, such as math problems, requires more context and more precise calculation, resulting in higher budget allocation.

• Besides the dynamic compression, \method also outperforms the three 90\%-budget baselines, while itself uses less than 90\% budget, and achieves the highest score in 20 out of 39 comparisons. This suggests that by taking into account both the position bias and attention scores, our method accompolishes more effective pruning than the baselines that only consider one aspect, highlighting the importance of integrating both dimensions for effective KV cache compression.

\input{tab/time}
\subsection{\method's Generalization Ability}

We next conduct experiments to examine whether our design and hyperparameters can be generalized to LLMs of larger sizes.
On five datasets in Table~\ref{size}, \method with Llama3-70B and Qwen2.5-32B/72B demonstrates consistent near full-cache performance with the same \method setting.
Notably, the averaged compression ratio increases on datasets of longer context, achieving nearly a 50\% lossless compression at its peak, validating the robustness and versatility for its general applicability.

\subsection{Ablation Study}
\label{ssec:ablation}

The ablation study is conducted to explore the impact of various configurations of \method. We select Llama-3-8B-Instruct and perform experiments across five datasets presented in Table~\ref{ablation}.

\paragraph{Attention Matrix Reduction}
The reduced attention matrix $A'$ in Sec.~\ref{ssec:method} aggregates attention scores from the last $k$ positions. The upper part of Table~\ref{ablation} illustrates the model's performance and the actual budget when setting $k=1$, $1\%n$, $5\%n$, and $10\%n$ (n being the number of input tokens). It is evident that setting $k$ as $1$ achieves a significantly reduced budget, thus a higher compression ratio, with almost no change in performance compared to $1\%n$ and $5\%n$. On the other hand, while $10\%n$ can compress more KV cache, it fails to maintain performance (for instance, on NarrativeQA, the former achieves a performance of 9.74 using 32\% of the budget, whereas the latter scores 21.44 using 48.7\% of the budget). 
What's even better is that since $k=1$ requires the least amount of computation, relying solely on the scores from the last token, the complexity of obtaining $A'$ becomes $O(1)$, independent of the sequence length. The advantages of both high efficacy and low overhead make $k=1$ a solid design choice for calculating the norm metric.

\paragraph{Importance Ranking Strategies}
\method simply regards positions as the significance of tokens, while another intuitive strategy is to utilize attention scores to rank their importance. We investigate its potentials through ranking by each token's average attention score received from other tokens, similar to previous approaches such as H2O. The results are shown in the middle of Table~\ref{ablation}: under the same experimental settings, the attention-based ranking struggles to maintain the model's performance. Though it achieves a high compression ratio, the performance is not stable with possible severe degradation, which deviates from our objective. Therefore, the position-based strategy is more suited for our goal.

\paragraph{Attention Norm Threshold} As we adopt the universal threshold as 1\% for the attention norm difference, we comprehensively study the effects of smaller or larger thresholds, as shown in the bottom part of Table~\ref{ablation}. Intuitively, a larger threshold allows for a higher compression ratio, while a smaller threshold does the opposite. The conclusion is clear that when the threshold is set to a smaller 0.1\%, the average budget increases as expected, yet the model's performance sees little improvement. Conversely, when the threshold is set to 10\%, it prunes more KV cache, but the model's performance significantly deteriorates. Thus, we deem 1\% as a reasonable threshold for universal uses across models and tasks.

Appendix~\ref{app:ablation} presents additional results and analysis from the ablation study.

\subsection{Efficiency Analysis}
\label{ssec:time}

In this section, a quantitative study is conducted on whether \method improves inference time, apart from the space reduction from KV cache pruning. We compare the time usage on six datasets with Llama3-8B/70B using \method, along with three baselines with the 50\% budget setting. Table~\ref{time} reports the average generation time after the prefilling stage of each sample (\textbf{Overall}), as well as the time needed to complete pruning (\textbf{Prune}) before the generation. 

From the results, \method achieves the best performance in 8 out of 12 comparisons of overall generation time, and achieves the best average time for both 8B and 70B LLMs, suggesting that although \method requires a slightly longer time for pruning itself compared to three other approaches, the total generation speed of \method is evidently advantageous.
It also performs consistently across models of different scales, underscoring its time efficiency.