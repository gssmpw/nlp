%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

%  custom packages
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[acronym,nowarn]{glossaries}
\usepackage{tcolorbox}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{wasysym}

\usepackage[counterclockwise]{rotating}

\usepackage{colortbl}
\usepackage{scalefnt,letltxmacro}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{cases}

\usepackage{makecell}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Mutual Information estimation via Bridge Matching}

% Tikz
\usepackage{tikz}
\usetikzlibrary{positioning,shapes,arrows,fit,calc,shadows}

% Gnuplottex.
\usepackage[shell, subfolder]{gnuplottex}
\usepackage{gnuplot-lua-tikz}

% Subfigures.
\usepackage{caption}
\usepackage{subcaption}


%%% CUSTOM DEFINES
\input{defines}
\def\ourestname{\textit{Info}Bridge}

% defeq defined in custom defines.tex file
% \newcommand*{\defeq}{\stackrel{\text{def}}{=}}

\DeclareMathOperator*{\arginf}{\arg\!\inf}
\DeclareMathOperator*{\argsup}{\arg\!\sup}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\alex}[1]{{\scriptsize\textbf{\color{red} AK: #1}}}
\newcommand{\nik}[1]{{\scriptsize\textbf{\color{purple} NG: #1}}}
\newcommand{\sergei}[1]{{\scriptsize\textbf{\color{blue} SK: #1}}}
\newcommand{\KL}[2]{\mathrm{KL}\left(#1 \middle\Vert #2\right)}

\begin{document}

\twocolumn[
\icmltitle{\ourestname{}: Mutual Information estimation via Bridge Matching}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Sergei Kholkin}{skoltech}
\icmlauthor{Ivan Butakov}{skoltech,mipt}
\icmlauthor{Evgeny Burnaev}{skoltech,airi}
\icmlauthor{Nikita Gushchin}{skoltech}
\icmlauthor{Alexander Korotin}{skoltech,airi}
\end{icmlauthorlist}

\icmlaffiliation{skoltech}{Skolkovo Institute of Science and Technology}
\icmlaffiliation{airi}{Artificial Intelligence Research Institute}
\icmlaffiliation{mipt}{Moscow Institute of Physics and Technology}

\icmlcorrespondingauthor{Sergei Kholkin}{s.kholkin@skoltech.ru}
\icmlcorrespondingauthor{Alexander Korotin}{a.korotin@skoltech.ru}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Diffusion bridge models have recently become a powerful tool in the field of generative modeling. In this work, we leverage their power to address another important problem in machine learning and information theory -- the estimation of the mutual information (MI) between two random variables. We show that by using the theory of diffusion bridges, one can construct an unbiased estimator for data posing difficulties for conventional MI estimators. We showcase the performance of our estimator on a series of standard MI estimation benchmarks. 

\end{abstract}

\section{Introduction}

Information theory offers an extensive set of tools for quantifying probabilistic relations between random variables.
It is widely used in machine learning for advanced statistical analysis~\cite{berrett2017independence_testing, sen2017conditional_independence_test, duong2023normflows_for_conditional_independence_testing,bounoua2024_O_information},
assessment of deep neural networks' performance and generalization capabilities~\cite{tishby2015bottleneck_principle, xu2017information, goldfeld2019estimating_information_flow, abernethy2020reasoning_conditional_MI, kairen2018individual_neurons, butakov2024lossy_compression},
self-supervised and semi-supervised learning~\citep{linsker1988self-organisation, bell1995infomax, hjelm2018deep_infomax, stratos2019infomax, bachman2019DIM_across_views, petar2019DGI, oord2019infoNCE, tschannen2020on_DIM} and regularization or alignment in generative modeling~\cite{chen2016infogan, belghazi2018mine, ardizzone2020training_normflows, wang2024IT_alignment}.

The majority of the aforementioned applications revolve around one of the central information-theoretic quantities~-- \emph{mutual information} (MI).
Due to several outstanding properties, MI is widely used as an invariant measure of non-linear dependence between random variables.
Unfortunately, recent studies suggest that the curse of dimensionality is highly pronounced when estimating MI~\citep{goldfeld2020convergence_of_SEM_entropy_estimation, mcallester2020limitations_MI}.
Additionally, it is argued that long tails, high values of MI and some other particular features of complex probability distributions can make mutual information estimation even more challenging~\citep{czyz2023beyond_normal}.
On the other hand, recent developments in neural estimation methods demonstrate that sophisticated parametric estimators can achieve notable practical success in situations where traditional mutual information estimation techniques struggle~\citep{belghazi2018mine, oord2019infoNCE, song2020understanding_limitations, rhodes2020telescoping, Ao_Li_2022entropy_estimation_normflows, butakov2024normflows}.
Among neural estimators, generative approaches are of particular interest, as they have proven to be effective in handling complex data~\cite{duong2023dine, franzese2024minde, butakov2024normflows}.
Since MI estimation is closely tied to approximation of a joint probability distribution,
one can argue that leveraging state-of-the-art generative models, e.g., diffusion models, may result in additional performance gains.

\paragraph{Diffusion Bridge Matching.} Diffusion models are a powerful type of generative models that show an impressive quality of image generation \cite{ho2020denoising, rombach2022high}. However, they have some disadvantages, such as the inability to perform data-to-data translation via diffusion. To tackle this problem, a novel promising approach based on a Reciprocal Processes \cite{leonard2014reciprocal} and Schr\"{o}dinger Bridges theory \cite{schrodinger1932theorie, leonard2013survey} have risen. This approach is called the \textit{diffusion bridge matching} and is used for learning generative models as diffusion processes for data-to-data translation. Theis type of models has shown itself as a powerful approach for numerous applications in biology~\cite{tong2024simulation, bunne2023schrodinger}, chemistry~\cite{somnath2023aligned, igashovretrobridge}, computer vision~\cite{liu2023_i2isb, shi2023diffusion, zhoudenoising}, speech processing~\cite{chen2023schrodinger} and unpaired learning \cite{gushchin2024adversarial, gushchin2024light, shi2023diffusion}.


\paragraph{Contributions.}

In this work, we employ the Diffusion Bridge Matching to tackle the problem of MI estimation.

\begin{enumerate}
    \item \textbf{Theory.} We propose an unbiased mutual information estimator based on reciprocal processes, their diffusion representations and the Girsanov theorem (\wasyparagraph\ref{sec:mut_info_drifts}).
    \item \textbf{Practice.} Building on the proposed theoretical framework and the powerful generative methodology of diffusion bridges, we develop a practical algorithm for MI estimation, named \ourestname{} (\wasyparagraph\ref{sec:infobridge}). We demonstrate that our method achieves performance comparable to existing approaches on low-dimensional benchmarks and either comparable or superior performance on image data benchmarks (\wasyparagraph\ref{sec:exps}).
    
\end{enumerate}


\paragraph{Notations.} We work in $\mathbb{R}^{D}$, which is the $D$-dimensional Euclidean space equipped with the Euclidean norm $\|\cdot\|$. We use $\mathcal{P}(\mathbb{R}^{D})$ to denote the absolutely continuous Borel probability distributions whose variance and differential entropy are finite. To denote the density of $q\in\mathcal{P}(\mathbb{R}^{D})$ at a point $x\in\mathbb{R}^{D}$, we use $q(x)$. 
We write $\KL{\cdot}{\cdot}$ to denote the Kullback-Leibler divergence between two distributions. In turn, $H(\cdot)$ denotes the differential entropy of a distribution. We use $\Omega$ to denote the space of trajectories, i.e., continuous $\mathbb{R}^{D}$-valued functions of $t\in [0,1]$. We write ${\mathcal{P}(\Omega)}$ to denote the probability distributions on the trajectories $\Omega$ whose marginals at $t=0$ and $t=1$ belong to $\mathcal{P}(\mathbb{R}^{D})$; this is the set of stochastic processes. We use $dW_{t}$ to denote the differential of the standard Wiener process $W\in\mathcal{P}(\Omega)$. We use $Q_{|x_0}$ and $Q_{|x_0,x_1}$ to denote the distribution of stochastic process $Q$ conditioned on $Q$'s values $x_0$ and $x_0,x_1$ at times $t=0$ and $t=0,1$, respectively.
For a process $Q\in\mathcal{P}(\Omega)$, we denote its marginal distribution at time $t$ by $q(x_t)\in\mathcal{P}(\mathbb{R}^{D})$, and if process conditioned on its value $x_s$ at time $s$,
the marginal distribution  of such a process at time $t$ would be denoted as $q(x_t|x_s)\in\mathcal{P}(\mathbb{R}^{D})$.

\section{Background}

\paragraph{Mutual information.} Information theory is a well-established framework for analyzing and quantifying interactions between random vectors.
In this framework, mutual information (MI) serves as a fundamental and invariant measure of the non-linear dependence between  two  $\mathbb{R}^D$-valued random vectors $X_0, X_1$.
It is defined as follows:

\begin{equation}
    \label{eq:MI_definition}
    I(X_0;X_1) = \KL{\Pi_{X_0, X_1}}{\Pi_{X_0} \otimes \Pi_{X_1}},
\end{equation}

where $ \Pi_{X_0, X_1}$ and $ \Pi_{X_0} $, $ \Pi_{X_1} $ are the joint and marginal distributions of a pair of random vectors $ (X_0,X_1) $.
If the corresponding PDF $\pi(x_0, x_1)$ exists, the following also holds:
\begin{equation}
    \label{eq:MI_definition_PDF}
    I(X_0;X_1) = \expect_{x_0, x_1 \sim \pi(x_0, x_1)} \log \frac{\pi(x_0,x_1)}{\pi(x_0) \pi(x_1)}.
\end{equation}

Mutual information is symmetric, non-negative and equals zero if and only if $ X_0 $ and $ X_1 $ are independent.
MI is also invariant to bijective mappings: $ I(X_0;X_1) = I(g(X_0);X_1) $ if $ g^{-1} $ exists and $ g $, $ g^{-1} $ are measurable~\cite{cover2006information_theory, polyanskiy2024information_theory}.


\textbf{Brownian Bridge}. Let $W^\epsilon$ be the Wiener process with a constant volatility $\epsilon$, i.e., it is described by the SDE $dW^\epsilon=\sqrt{\epsilon}dW_t$, where $W_{t}$ is the standard Wiener process. Let $W^\epsilon_{|x_0, x_1}$ denote the process $W^\epsilon$ conditioned its on values $x_0, x_1$ at times $t=0,1$, respectively. This process $W^\epsilon_{|x_0, x_1}$ is called the Brownian Bridge \citep[Chapter 9]{ibe2013markov}.

\paragraph{Reciprocal processes.} Reciprocal processes are a class of stochastic processes that have recently gained attention of research community in the contexts of stochastic optimal control \cite{leonard2014reciprocal}, Schr\"{o}dinger Bridges \cite{schrodinger1932theorie, leonard2013survey}, and diffusion generative modeling \cite{liu2023_i2isb, gushchin2024light}. In our paper, we consider a \textit{particular case} of reciprocal processes which are induced by the Brownian Bridge $W_{|x_0,x_1}^{\epsilon}$.

Consider a joint distribution $\pi(x_0, x_1)\in \mathcal{P}(\mathbb{R}^{D\times 2})$ and define the process $Q_\pi \in \mathcal{P}(\Omega)$ as a mixture of Brownian bridges $W_{|x_0,x_1}^{\epsilon}$ with weights $\pi(x_0,x_1)$:
\begin{gather}
    Q_\pi \defeq \int W^\epsilon_{|x_0, x_1} d\pi(x_0, x_1). \nonumber
\end{gather}
This implies that to get trajectories of $Q_\pi$ one has to first sample the start and end points, $x_0$ and $x_1$, at times $t=0$ and $t=1$ from $\pi(x_0, x_1)$ and then simulate the Brownian Bridge $W^\epsilon_{|x_0, x_1}$. Due to the non-causal nature of trajectory formation, such a process is, in general, non markovian. The set of all mixtures of Brownian Bridges can be described as: 
\begin{gather}
    \big\{ Q\in \mathcal{P}(\Omega)\text{ } s.t.\text{ } \exists \pi \in \mathcal{P}(\mathbb{R}^{D\times 2}) : 
    Q = Q_\pi \big\} \nonumber
\end{gather}
 and is called the set of \textit{reciprocal processes} (for $W^{\epsilon}$). 
 
\paragraph{Reciprocal processes conditioned on the point.} \label{sec:schr_follmer} Consider a reciprocal process $Q_\pi$ conditioned on some start point $x_0$. Let the resulting processes be denoted as $Q_{\pi|x_0}$, which remains reciprocal. Then, if some regularity assumptions are met \citep[Appx C.1]{shi2023diffusion} process $Q_{\pi|x_0}$ is known as the Schr\"{o}dinger F\"{o}llmer process \cite{huang2024schrodinger, vargas2023bayesian}. While $Q_\pi$ itself is, in general, not markovian, $Q_{\pi|x_0}$ \textbf{is markovian}. Furthermore, it is a diffusion process governed by the following SDE:
\begin{gather}
    Q_{\pi| x_0}: dx_t=v_{x_0}(x_t, t)dt + \sqrt{\epsilon}dW_t, x_0 \sim \delta(x_0), \label{eq:sde_sch_flm} \nonumber
    \\
    v_{x_0}(x_t, t) = \mathbb{E}_{x_1 \sim q_\pi(x_1|x_t, x_0)}\left[\frac{x_1 - x_t}{1 - t}\right]. \label{eq:drift_bm}
\end{gather}

\textbf{Representations of reciprocal processes.} The process $Q_{\pi}$ can be naturally represented as a mixture of processes $Q_{\pi|x_0}$ conditioned on the starting points $x_0$:
$$Q_\pi=\int Q_{\pi|x_0}  d\pi(x_0).$$
Therefore, one may also express $Q_\pi$ via an SDE but with, in general, non-markovian drift (conditioned on $x_0$):

\begin{gather}
   \hspace{-2mm} Q_\pi: dx_t=v(x_t, t, x_0)dt + \sqrt{\epsilon}dW_t, x_0 \sim \pi(x_0), \nonumber
    \\
    \hspace{-4mm}v(x_t, t, x_0)\!=\!v_{x_0}(x_t, t)\!=\!\mathbb{E}_{x_1 \sim q_\pi(x_1|x_t, x_0)}\left[\frac{x_1 - x_t}{1 - t}\right]. \label{eq:reciprocal_non_markovian_sde}
\end{gather}

\paragraph{Conditional Bridge Matching.} \label{sec:cond_bm} Although the drift $v_{x_0}(x_t, t)$ of $Q_{\pi|x_0}$ in \eqref{eq:drift_bm} admits a closed form, it usually cannot be computed or estimated directly due to the unavailability of a way to easily sample from $\pi(x_1|x_t, x_0)$. However, it can be recovered by solving the following regression problem:

\vspace{-4mm} 
\begin{gather}
    \hspace{-3mm}v_{x_0}\!=\!\argmin_{u} \mathbb{E}_{x_1 \sim q_\pi(x_1, x_t|x_0)}\left\| \frac{x_1 - x_t}{1 - t} - u(x_t, t) \right\|^2 \label{eq:opt_regular_bm}, 
\end{gather}

which optimizes over drifts $u: \mathbb{R}^D \times [0, 1] \rightarrow \mathbb{R}^D$. The same holds for the $Q_\pi$ and its drift $v(x_t, t, x_0)$ through the addition of expectation w.r.t. $\pi(x_0)$:
\begin{gather}
    v = \argmin_{u} \mathbb{E}_{q_\pi(x_1, x_t|x_0)\pi(x_0)}\left\Vert \frac{x_1 - x_t}{1 - t} - u(x_t, t, x_0) \right\Vert^2 = \nonumber \\
     =\!\argmin_{u} \mathbb{E}_{q_\pi(x_1, x_t, x_0)}\left\Vert \frac{x_1 - x_t}{1 - t} - u(x_t, t, x_0) \right\Vert^2 ,
    \label{eq:opt_cond_bm}
\end{gather}
where $u: \mathbb{R}^D \times [0, 1] \times \mathbb{R}^D  \rightarrow \mathbb{R}^D$. Problem~\eqref{eq:opt_cond_bm} is usually solved with standard deep learning techniques. Namely, one parametrizes $u$ with a neural network $v_{\theta}$, and minimizes \eqref{eq:opt_cond_bm} using stochastic gradient descent and samples drawn from $q_\pi(x_0, x_t, x_1)$. The latter sampling is easy if one can sample from $\pi(x_0,x_1)$. Indeed, $q_\pi(x_0, x_t, x_1) = q_\pi(x_t|x_0, x_1)\pi(x_0, x_1)$, and one can sample first from $\pi(x_0, x_1)$ and then from $q_\pi(x_t|x_0, x_1)$, which is just the Brownian Bridge.

Such a procedure of learning drift $v$ with a neural network is popular in generative modeling to solve a problem of sampling from conditional distribution $\pi(x_1|x_0)$ and is frequently applied in the image-to-image transfer \cite{liu2023_i2isb}. The procedure of learning drift $v(x_t, t, x_0)$ ~\eqref{eq:opt_cond_bm} is usually called the \textit{conditional} (or augmented) \textit{bridge matching} \cite{de2023augmented, zhoudenoising}. In addition, such procedure can also be derived through the well-celebrated Doob $h$-transform \cite{de2023augmented, zhoudenoising, palmowski2002technique} or reversing a diffusion \cite{zhoudenoising}.

\paragraph{KL divergence between diffusion processes.} Consider two diffusion processes with the same volatility coefficient $\sqrt{\epsilon}$ that start at the same distribution $\pi_0$:
\begin{gather}
    Q^A: dx_t=f^{A}(x_t, t)dt + \sqrt{\epsilon} dW_t, x_0 \sim \pi_0
    \nonumber
    \\
    Q^B: dx_t=f^{B}(x_t, t)dt + \sqrt{\epsilon} dW_t, x_0 \sim \pi_0
    \nonumber
\end{gather}
By the application of the disintegration theorem \citep[\wasyparagraph1]{leonard2014some} and the Girsanov theorem \citep[\wasyparagraph8.6]{oksendal2003stochastic} one can derive the KL divergence between these diffusions:
\begin{gather}
    \KL{Q^A}{Q^B} = \nonumber
    \\ = \frac{1}{2\epsilon}\int_{0}^{1}\mathbb{E}_{x_t \sim q^A(x_t)} \big[ \Vert f^A(x_t, t) - f^B(x_t, t) \Vert_2^2 \big] dt, \label{eq:kl_diff_girsanov}
\end{gather}

where $q^A(x_t)$ is the marginal distribution of $Q^A$ at time $t$. 

This allows one to estimate the KL divergence between two diffusions with the same volatility coefficient and the same initial distributions, knowing only their \textit{drifts} and marginal samples $x_t \sim q^A(x_t)$. 
This fact is widely used in Bridge Matching \cite{shi2023diffusion, peluchetti2023diffusion}, Diffusion  \cite{franzese2024minde} and Schr\"{o}dinger Bridge Models \cite{vargas2021solving, gushchin2023entropic}.

\section{Related Work}

\paragraph{Mutual information estimators.}
Mutual information estimators fall into two main categories: \emph{non-parametric} and \emph{parametric}.
Parametric estimators are also subdivided into \emph{discriminative} and \emph{generative}~\cite{song2020understanding_limitations,federici2023hybrid_MI_estimation}.
In addition to this natural classification, we distinguish \emph{diffusion-based} approaches to better contextualize our method in relation to the previous works.

\textbf{Non-parametric estimators.}
Classical approaches to the mutual information estimation rely on non-parametric density estimators, such as kernel density estimator~\cite{weglarczyk2018kde_review,goldfeld2019estimating_information_flow} and $ k $-nearest neighbors estimator~\cite{kozachenko1987entropy_of_random_vector,kraskov2004KSG,berrett2019efficient_knn_entropy_estimation}.
The resulting density estimate is plugged into~\eqref{eq:MI_definition_PDF} to acquire the MI estimate through MC-integration, leave-one-out method or other techniques.
The simplicity of such methods make them appealing for low-dimensional cases, but extensive high-dimensional evaluation suggests that these approaches are inapplicable to complex data 
~(\citealt[\wasyparagraph~5.3]{goldfeld2019estimating_information_flow}; \citealt[\wasyparagraph~6.2]{czyz2023beyond_normal}; \citealt[Table 1]{butakov2024normflows}).

\textbf{Non-diffusion-based generative estimators.}
More advanced techniques involve parametric density models, such as normalizing flows and variational autoencoders, to measure MI through density estimation.
This na\"{i}ve generative approach was described by \citep{song2020understanding_limitations,mcallester2020limitations_MI} and further investigated in the works of \citep{Ao_Li_2022entropy_estimation_normflows,duong2023dine}.
However, despite better modelling capabilities, the results in~\citep[Figures 1,2]{song2020understanding_limitations} indicate that direct PDF estimation can introduce a substantial bias to the MI estimate.
Therefore, it was proposed to avoid PDF estimation altogether and focus on measuring the density ratio in~\eqref{eq:MI_definition_PDF}.
This is done in the works of~\citep{duong2023dine,butakov2024normflows}
by leveraging the invariance property of mutual information.
Such methods show better performance on synthetic benchmarks, but may introduce an inductive bias due to the simplified closed-form expression being used to estimate the density ratio in question.

\textbf{Discriminative estimators.}
Finally, another approach to MI estimation involves training a classifier to discriminate between the samples from $ \Pi_{X_0,X_1} $ and $ \Pi_{X_0} \otimes \Pi_{X_1}$:  MINE~\cite{belghazi2018mine}, InfoNCE~\cite{oord2019infoNCE} and similar methods~\cite{song2020understanding_limitations}.
This technique leverages variational bounds on the Kullback-Leibler divergence and provides a relatively cheap and reliable parametric estimator for a wide range of cases, including high-dimensional and complex data.
However, such estimators have severe demerits from a theoretical perspective, such as high variance in MINE and large batch size requirements in InfoNCE~\cite{song2020understanding_limitations}.
Additionally, recent benchmarking results suggest that discriminative approaches can underperform compared to the generative methods when MI is high and the probability distribution is complex~\cite{franzese2024minde,butakov2024normflows}.

\vspace{-3mm}
\paragraph{Neural Diffusion Estimator for MI (MINDE).} \label{sec:minde}

One of the most recent generative methods for MI Estimation is diffusion-based \cite{song2021sde} MINDE \cite{franzese2024minde}. To estimate $\KL{\pi^A}{\pi^B}$ the authors learn two standard backward diffusion models to generate data from distributions $\pi^A$ and $\pi^B$, e.g., for $\pi^A$:


\vspace{-4mm}
\begin{gather}
    \begin{cases}
    Q^A: \underbrace{dx_{t}=[-f(x_t, t) + g(t)^2 s^{A}(x_t, t)]dt + g(t)d\hat{W}_t}_{\text{backward diffusion}}, \\
    x_T \sim q^A_{T}(x_T), \label{eq:diffusion_minde}
    \end{cases}
    \vspace{-10mm}
\end{gather}
\vspace{-4mm}


where $f$ and $g$ are the drift and volatility coefficients, respectively, of the forward diffusion \cite{song2021sde}, $d\hat{W}_t$ is the Wiener process when time flows backwards, and $q^A_{t}$ is the distribution of the noised data at time $t$ \citep[\wasyparagraph 2, 3]{franzese2024minde}. The similar expressions hold for $\pi^B$ and $Q^B$. Then, the authors formulate a KL divergence estimator through the difference of diffusion \textit{score functions}:

\vspace{-5mm}
\begin{gather}
    \KL{\pi^A}{\pi^B}=\KL{Q^A}{Q^B}=
    \nonumber
    \\
    \int_0^T \mathbb{E}_{q^A_t(x_t|x_0)} \left[ 
    \frac{g(t)^2}{2}
    \| s^{A}(x_t, t) - s^{B}(x_t, t) \|^2\right]dt +
    \nonumber
    \\
    \KL{q^A_{T}}{q^B_{T}} \label{eq:minde_est}.
\end{gather}
\vspace{-4mm}

Here, $\KL{q^A_{T}}{q^B_{T}}$ is the \textbf{bias} term, which vanishes only when diffusion has infinitely many steps, i.e., $T \rightarrow \infty$.  When the diffusion score functions $s^A$ and $s^B$ \eqref{eq:diffusion_minde} are properly learned, one can draw samples from the forward diffusion $q^A_t(x_t|x_0)$ and compute the estimate of KL divergence \eqref{eq:minde_est}. In this way, the authors transform the problem of training the KL divergence estimator into the problem of learning the backward diffusions \eqref{eq:diffusion_minde} that generate \textit{data from noise}.

To estimate mutual information, the authors propose a total of four equivalent methods, all based on the estimation of up to three KL divergences \eqref{eq:minde_est} or their expectations.

\vspace{-3mm}
\section{\ourestname{} Mutual Information estimator}


In \wasyparagraph\ref{sec:mut_info_drifts}, we propose our novel MI estimator which is based on difference of diffusion drifts of conditional reciprocal processes. Suggest some straightforward generalizations in \wasyparagraph\ref{sec:generalizations}. Explain the practical learning procedure in \wasyparagraph\ref{sec:infobridge}. 

\vspace{-3mm}
\subsection{Computing MI through Reciprocal Processes}  \label{sec:mut_info_drifts}
\vspace{-2mm}

Consider the problem of  MI estimation for random variables $X_0$ and $X_1$ with joint distribution $\pi(x_0, x_1)$. To tackle this problem, we employ reciprocal processes:

\vspace{-4mm}
\begin{gather}
    Q_{\pi} \defeq \int W^\epsilon_{|x_0, x_1}d\pi(x_0, x_1), \label{eq:recip_plan} \\ 
    Q^{\text{ind}}_\pi \defeq \int W^\epsilon_{|x_0, x_1}d\pi(x_0)d\pi(x_1). \label{eq:recip_ind}
\end{gather}
\vspace{-4mm}

We show that the KL between the distributions $\pi(x_0, x_1)$ and $\pi(x_0)\pi(x_1)$  \eqref{eq:MI_definition} is equal to the KL between the reciprocal processes $Q_{\pi}$ and $Q^\text{ind}_{\pi}$, and decompose the latter into the difference of drifts. 
 
\begin{theorem}[Mutual Information decomposition] \label{th:mut_info_drifts}
    Consider random variables $X_0, X_1$, with joint distribution $\pi(x_0, x_1)$. Consider reciprocal processes $Q_{\pi}$, $Q_\pi^{\text{ind}}$ induced by distributions $\pi(x_0, x_1)$ and $\pi(x_0)\pi(x_1)$, respectively, as in  \eqref{eq:recip_plan} \eqref{eq:recip_ind}. Then the mutual information between the random variables $X_0$ and $X_1$ can be expressed as:

    
    \vspace{-5mm}
        \begin{gather}
            %I(q(x_0, x_1))= 
            I(X_0; X_1) = 
            \label{eq:mut_info_est} 
        \\
         = \frac{1}{2\epsilon} \int_{0}^{1} \mathbb{E}_{q_{\pi}(x_t, x_0)} \Vert v_{\text{joint}}(x_t, t, x_0) - v_{\text{ind}}(x_t, t, x_0) \Vert^2 dt, \nonumber
        \end{gather}
        \vspace{-4mm}

    where 
    \vspace{-3mm}
    \begin{gather}
        v_{\text{joint}}(x_t, t, x_0) = \mathbb{E}_{x_1 \sim q_\pi(x_1|x_t, x_0)}\left[\frac{x_1 - x_t}{1 - t}\right],  \label{eq:th_drift_plan}\\
        v_{\text{ind}}(x_t, t, x_0) = \mathbb{E}_{x_1 \sim q^\text{ind}_\pi(x_1|x_t, x_0)}\left[\frac{x_1 - x_t}{1 - t}\right]. \label{eq:th_drift_ind}
    \end{gather}
    \vspace{-4mm}
    
     $v_{\text{joint}}$ and $v_{\text{ind}}$ are the drifts of the SDE representations \eqref{eq:reciprocal_non_markovian_sde} of the reciprocal processes $Q_{\pi}$ and $Q^{\text{ind}}_\pi$.
    
\end{theorem}


\begin{algorithm}[t!]
    \caption{\ourestname{}. MI estimator.}
    \label{alg:mi_est}
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \Input{Distribution $\pi(x_0, x_1)$ accessible by samples, neural network parametrization $v_{\theta}$ of drift functions approximating optimal drifts $v_{\text{joint}}$ and $v_{\text{ind}}$, number of samples $N$}
    
    \Output{Mutual information estimation $\widehat{\text{MI}}$}
    
    Sample batch of pairs $\{x_{0}^n, x_{1}^n\}_{n=0}^N\sim \pi(x_0, x_1)$; \\
    Sample batch $\{t^n\}_{n=0}^N \sim U[0, 1]$; \\
    Sample batch  $\{x_t^n\}_{n=0}^N \sim W^\epsilon_{|x_0, x_1}$; \\
    $\widehat{\text{MI}}$ $\leftarrow$ $\frac{1}{2\epsilon N}\sum_{n=0}^N\Vert  v_{\theta}(x_t^n, t^n, x_0^n, 1) - v_{\theta}(x_t^n, t^n, x_0^n, 0)\Vert^2$
    
    
\end{algorithm}

\vspace{-3mm}
\begin{proof}
    Using the disintegration theorem  \citep[\wasyparagraph 1]{leonard2014some} at time $t=0$, we get:

    \vspace{-5mm}
    \begin{gather}
        \KL{Q_\pi}{Q^\text{ind}_\pi} = \KL{\pi(x_0)}{\pi(x_0)} + \nonumber \\ + \mathbb{E}_{\pi(x_0)}\big[ \KL{Q_{\pi|x_0}}{Q^{\text{ind}}_{\pi|x_0}} \big].
        \nonumber
    \end{gather}
    \vspace{-4mm}

    Note that since $Q_{\pi}, Q^{\text{ind}}_{\pi}$ share the same marginals at time $t=0$, first KL term vanishes. Similarly, by using the disintegration theorem again for both times $t=0,1$, we get:
    
    \vspace{-5mm}
    \begin{gather}
        \KL{Q_\pi}{Q^\text{ind}_\pi} = \KL{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} + \nonumber \\ + \mathbb{E}_{\pi(x_0, x_1)}\big[ \KL{Q_{\pi|x_0, x_1}}{Q^{\text{ind}}_{\pi|x_0, x_1}} \big].
        \nonumber
    \end{gather}
    \vspace{-5mm}
    
    Recap that $Q_\pi$ and $Q^\text{ind}_\pi$ are both mixtures of Brownian Bridges. Therefore, $Q_{\pi|x_0, x_1} = Q^{\text{ind}}_{\pi|x_0, x_1}=W^\epsilon_{|x_0, x_1}$ and $\KL{Q_{\pi|x_0, x_1}}{Q^{\text{ind}}_{\pi|x_0, x_1}}=0$. Then the following holds:
    
    \vspace{-5mm}
    \begin{gather}
        \KL{Q_\pi}{Q^\text{ind}_\pi} = \KL{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} = \nonumber \\ = \mathbb{E}_{\pi(x_0)}\big[ \KL{Q_{\pi|x_0}}{Q^{\text{ind}}_{\pi|x_0}} \big] . \nonumber
    \end{gather}
    \vspace{-5mm}
    
    Moreover, processes $Q_{\pi|x_0}$ and $Q^{\text{ind}}_{\pi|x_0}$ are diffusion processes (\wasyparagraph\ref{sec:schr_follmer}). Then, by recalling  \eqref{eq:kl_diff_girsanov}, we get:
    
    \vspace{-6mm}
    \begin{gather}
        \KL{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} = 
        \nonumber
        \\=\mathbb{E}_{\pi(x_0)}\big[ \KL{Q_{\pi|x_0}}{Q^{\text{ind}}_{\pi|x_0}} \big]
        = \label{eq:kl_dec_final}
        \\ \frac{1}{2\epsilon}  \int_{0}^{1} \mathbb{E}_{q_\pi(x_t, x_0) }\big[ \Vert v_{\text{joint}}(x_t, t, x_0) - v_{\text{ind}}(x_t, t, x_0) \Vert_2^2 \big] dt , \nonumber
    \end{gather}
    \vspace{-5mm}

    where drifts $v_{\text{joint}}$ and $v_{\text{ind}}$ are defined as in \eqref{eq:th_drift_plan} and \eqref{eq:th_drift_ind} respectively. 
    \vspace{-2mm}
\end{proof}


Once the drifts $v_{\text{joint}}$ and $v_{\text{ind}}$ are known, our Theorem~\ref{th:mut_info_drifts} provides a straightforward way to estimate the mutual information between the random variables $X_0$ and $X_1$ by evaluating the difference between the drifts $v_{\text{joint}}(x_t, t, x_0)$ \eqref{eq:th_drift_plan} and $v_{\text{ind}}(x_t, t, x_0)$  \eqref{eq:th_drift_ind} at points $x_t$ sampled from the distribution of the reciprocal process $Q_{\pi}$ at times $0, t$. Similar formulas can be derived for the estimation of \underline{pointwise mutual information}, see \cref{appx:PMI}.

\vspace{-2mm}
\subsection{Possible generalizations}\label{sec:generalizations}

Our method admits several straightforward extensions.  For completeness, we present a method for unbiased estimation of the \underline{general KL divergence} in Appendix~\ref{appendix:KL_estimator}. According to the Theorem~\ref{th:general_kl} the KL divergence between any two distributions can be decomposed into the difference of diffusion drifts in similar way to \eqref{eq:mut_info_est}.
In addition, this results allows for the estimation of \underline{differential entropy} of any probability distribution, see Appendix~\ref{appendix:entropy_estimator}. 

In addition, our method can be extended to estimate mutual information involving more than two random variables, known as \underline{interaction information} (Appendix~\ref{appx:interacion_info}). Practical procedures for these generalizations can be derived in a similar way to \wasyparagraph\ref{sec:infobridge}.


\begin{algorithm}[t]
    \caption{\ourestname{}. Training the model.}
    \label{alg:cond_bm_traning}
    \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
    \Input{Distribution $\pi(x_0, x_1)$ accessible by samples, initial neural network parametrization $v_{\theta}$ of drift functions } \Output{Learned neural network $v_{\theta}$ approximating optimal drifts $v_{\text{joint}}$ and $v_{\text{ind}}$}
    \Repeat{converged}{
        Sample batch of pairs $\{x_{0}^n, x_{1}^n\}_{n=0}^N\sim \pi(x_0, x_1)$; \\
        Sample random permutation $\{\hat{x}_{1}^{n}\}_{n=0}^N=\text{Permute}(\{x_{1}^n\}_{n=0}^N)$; \\
        Sample batch $\{t^n\}_{n=0}^N \sim U[0, 1]$; \\
        Sample batch  $\{x_t^n\}_{n=0}^N \sim W^\epsilon_{|x_0, x_1}$; \\
        Sample batch  $\{\hat{x}_t^{n}\}_{n=0}^N \sim W^\epsilon_{|x_0, \hat{x}_1}$; \\
        $\mathcal{L}^1_{\theta} = \frac{1}{N}\sum_{n=1}^N \Vert v_{\theta}(x_t^n, t^n, x_0^n, 1) - \frac{x_1^n - x_t^n}{1 - t^n} \Vert^2$; \\
        $\mathcal{L}^2_{\theta} = \frac{1}{N}\sum_{n=1}^N \Vert v_{\theta}(\hat{x}_t^{n}, t^n, x_0^n, 0) - \frac{\hat{x}_1^{n} - \hat{x}_t^{n}}{1 - t^n} \Vert^2$; \\
        Update $\theta$ using $\frac{\partial \mathcal{L}^1_\theta}{\partial \theta} + \frac{\partial \mathcal{L}^2_\theta}{\partial \theta}$;
    }
    
\end{algorithm}


\input{plots_and_tables/low_dim_bench_table}




\vspace{-2mm}
\subsection{\ourestname{}. Practical optimization procedure} \label{sec:infobridge}


The drifts $v_{\text{joint}}$ and $v_{\text{ind}}$ of reciprocal processes $Q_\pi$ and $Q^{\text{ind}}_\pi$ can be recovered by the conditional Bridge Matching procedure, see  \wasyparagraph\ref{sec:cond_bm}. We have to solve optimization problem \eqref{eq:opt_cond_bm} by parametrizing $v_\text{joint}$ and $v_{\text{ind}}$ with neural networks $v_{\text{joint}, \phi}$ and $v_{{\text{ind}}, \psi}$, respectively, and applying Stochastic Gradient Descent on Monte Carlo approximation of \eqref{eq:opt_cond_bm}. The sampling from the distribution $q_\pi(x_t, x_0)$ of reciprocal process $Q_\pi$ at times $0, t$ is easy because:
\vspace{-2mm}

\vspace{-5mm}
\begin{gather}
q_\pi(x_t, x_0) = \mathbb{E}_{q_\pi(x_1)}[q_\pi(x_t, x_0|x_1)] = \nonumber\\ 
= \mathbb{E}_{q_\pi(x_1)}[q_\pi(x_t| x_1, x_0)\pi(x_0|x_1)]. \nonumber
\end{gather}
\vspace{-6mm}

Therefore, to sample from $q_\pi(x_t, x_0)$ it suffices to sample $x_0, x_1 \sim \pi(x_0, x_1)$ and sample from $q_\pi(x_t| x_1, x_0)$ which is again just a Brownian Bridge.


\textbf{Generative byproduct.} Note that the learned drifts $v_{\text{joint}, \phi}$ and $v_{\text{ind}, \psi}$ define the distributions $\pi_{\phi}(x_1|x_0)\approx\pi(x_1|x_0)$ and $\pi_{\psi}(x_1|x_0)\approx\pi(x_1)$ as solutions to the corresponding SDEs \eqref{eq:reciprocal_non_markovian_sde}. One can sample from these distributions by solving the related SDE \eqref{eq:reciprocal_non_markovian_sde} numerically, e.g., using the Euler-Maryama solver \cite{kloeden1992}.
Despite this being unnecessary for our MI estimation, it can be considered as an additional feature.

\vspace{-4mm}
\paragraph{Vector field parametrization.} \label{sec:vector_field_param} In practice, we replace two separate neural networks that approximate the drifts $v_{\text{joint}}(x_t, t, x_0)$ and $v_{\text{ind}}(x_t, t, x_0)$ with a single neural network that incorporates an additional binary input. Specifically, we introduce a binary input $s\in\{0, 1\}$ to unify the drift approximations in the following way: $v_{\theta}(\cdot, 1) \approx v_{\text{joint}}(\cdot)$  and $v_{\theta}(\cdot, 0) \approx v_{\text{joint}}(\cdot)$. The introduction of an additional input is widely used for the conditioning of diffusion  \cite{ho2021classifier} and bridge matching \cite{bortoli2024schrodinger} models. We have empirically found that it provides a much more accurate estimation of mutual information. We attribute its performance to the fact that for MI estimation we need to compute the difference between diffusion drifts \eqref{eq:mut_info_est}. Neural networks are usually not ideal and have some approximation error, then the difference between two almost identical neural networks with similar approximation errors is more accurate than the difference between two neural networks with distinct approximation errors.

We call our practical MI estimation algorithm \textbf{\ourestname{}}, provide the drifts training procedure in Algorithm~\ref{alg:cond_bm_traning} and describe the MI estimation procedure in Algorithm~\ref{alg:mi_est}.
\vspace{-1mm}

\input{plots_and_tables/comparison}

\vspace{-2mm}
\section{Experiments} \label{sec:exps}
\vspace{-2mm}

We test our method on a diverse set of benchmarks with already known ground truth value of MI.
To cover low-dimensional cases, long-tailed distributions and some basic cases of data lying on a manifold,
we employ the tests by~\citet{czyz2023beyond_normal}.
Benchmarks from~\cite{butakov2024lossy_compression,butakov2024normflows} are used to assess the method on manifolds represented as images.


\vspace{-2mm}
\paragraph{Low-dimensional benchmark.}
The tests from~\cite{czyz2023beyond_normal} focus on low-dimensional distributions with tractable mutual information.
Various mappings are also applied to make the distributions light- or heavy-tailed, or to non-linearly embed the data into an ambient space of higher dimensionality.

\ourestname{} is tested with $\epsilon=1$ and a multi-layer dense neural network is used to approximate the drifts.
Our computational complexity is comparable to MINDE ~\cite{franzese2024minde}. For more details, please, refer to~\underline{\cref{appendix:experimental_details}}.
In each test, we perform $ 10 $ independent runs with $100$k train set samples and $10$k test set samples.
The mean MI estimation results are reported in the top row of~\cref{tab:benchmark,tab:mean_100k_prec_2}.

Overall, the performance of our estimator is similar to that of MINDE \wasyparagraph\ref{sec:minde}, with the Cauchy distribution (i.e., Student-t distribution with degrees of freedom equal to $ 1 $) being the only notable exception.
The Cauchy distribution lacks the first moment, which poses theoretical limitations for Bridge Matching \citep[Appendix C]{shi2023diffusion}.
Additionally, its heavy tails make the estimation of mutual information more challenging.
Figures 7 and 8 in~\cite{franzese2024minde} indicate that MINDE  also faces difficulties in providing a reliable estimate in this specific scenario, and it is likely that it should not perform effectively in theory with Cauchy distribution either.
However, using the tail-shortening $ \arcsinh $ transform allows our method to estimate MI almost accurately, see~\cref{tab:benchmark,tab:mean_100k_prec_2}.



\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \captionsetup{skip=0.5\baselineskip}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/paired_gaussian_32_mi_5_eps_1_predict_vector_field_input_x_5.png}
        \caption{Gaussians data samples $x_0 \sim \pi(x_0)$} \label{fig:image_samples_a}
    \end{subfigure}%
    %
    \begin{subfigure}[b]{0.5\textwidth}
        \captionsetup{skip=0.5\baselineskip}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/paired_rectangle_32_mi_5_eps_1_predict_vector_field_input_x_5.png}
        \caption{Rectangles data samples $x_0 \sim \pi(x_0)$}\label{fig:image_samples_b}
    \end{subfigure}
    % \vspace{0.3mm}
    
    \begin{subfigure}[b]{0.5\textwidth}
        \captionsetup{skip=0.5\baselineskip}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/paired_gaussian_32_mi_5_eps_1_predict_vector_field_samples_y_5.png}
        \caption{Gaussians samples generated from $\pi_{\theta}(x_1|x_0)\approx\pi(x_1|x_0)$} \label{fig:image_samples_c}
    \end{subfigure}%
    %
    \begin{subfigure}[b]{0.5\textwidth}
        \captionsetup{skip=0.5\baselineskip}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/paired_rectangle_32_mi_5_eps_1_predict_vector_field_samples_y_5.png}
        \caption{Rectangles samples generated from $\pi_{\theta}(x_1|x_0)\approx\pi(x_1|x_0)$} \label{fig:image_samples_d}
    \end{subfigure}

    
    \begin{subfigure}[b]{0.5\textwidth}
        \captionsetup{skip=0.5\baselineskip}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/paired_gaussian_32_mi_5_eps_1_predict_vector_field_samples_y_ind_5.png}
        \caption{Gaussians samples generated from  $\pi^\text{ind}_{\theta}(x_1|x_0)\approx\pi(x_1)$} \label{fig:image_samples_e}
    \end{subfigure}%
    %
    \begin{subfigure}[b]{0.5\textwidth}
        \captionsetup{skip=0.5\baselineskip}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/paired_rectangle_32_mi_5_eps_1_predict_vector_field_samples_y_ind_5.png}
        \caption{Rectangles samples generated from $\pi^\text{ind}_{\theta}(x_1|x_0)\approx\pi(x_1)$} \label{fig:image_samples_f}
    \end{subfigure}
    \caption{ Examples of synthetic images from the \cite{butakov2024normflows} benchmark can be seen at \cref{fig:image_samples_a,fig:image_samples_b}. Note that images are high-dimensional, but admit latent structure, which is similar to real datasets. Samples generated from the learned distributions $\pi_{\theta}(x_1|x_0)\approx \pi(x_1|x_0)$ and $\pi^\text{ind}_{\theta}(x_1|x_0)\approx\pi(x_1)$ defined as solutions to SDEs \eqref{eq:reciprocal_non_markovian_sde} with approximated drifts $v_\theta(\cdot, 0)$ and $v_\theta(\cdot, 1)$, respectively, can be seen at \cref{fig:image_samples_c,fig:image_samples_d,fig:image_samples_e,fig:image_samples_f}. All the images have 32$\times$32 resolution. }
    \label{figure:synthetic_images_examples}
    \vspace{-5mm}
\end{figure*}


\vspace{-3mm}
\paragraph{Image data benchmark.}
In~\cite{butakov2024lossy_compression}, it was proposed to map low-dimensional distributions with tractable MI into manifolds admitting image-like structure, thus producing synthetic images
(in particular, images of 2D Gaussians and Rectangles, see~\cref{fig:image_samples_a,fig:image_samples_b}).
By using smooth injective mappings, one ensures that MI is not alternated by the transform~\citep[Theorem 2.1]{butakov2024normflows}.
In the original works, it is argued that such benchmarks are closer to real data, and therefore give more insights into the problems related to the MI estimation in realistic setups.

Each neural algorithm is trained with $100$k train set samples and validated using $10$k samples. \ourestname{} is tested with $\epsilon=1$ and we use a neural network with U-net architecture \citep{ronneberger2015u} to approximate the drift. For averaging, we run algorithm with 3 different seeds. Other experimental details are reported in \underline{\cref{appendix:image_bench}}.


We present our results for 16$\times$16 and 32$\times$32 resolution images with both Gaussian and rectangle structure  in~\cref{figure:compare_methods_images}, while the samples from the learned conditional bridge matching models can be viewed in  \cref{fig:image_samples_c,fig:image_samples_d,fig:image_samples_e,fig:image_samples_f}.
Our estimator looks very competitive, being as good as or even better than two previous best-performing methods:
Mutual Information Estimation via Normalizing Flows (MIENF)~\citep{butakov2024normflows} and $ 5 $-nearest neighbors weighted Kozachenko-Leonenko estimator~\cite{kozachenko1987entropy_of_random_vector,berrett2019efficient_knn_entropy_estimation} fed with autoencoder-generated embeddings (AE+WKL $ 5 $-NN)~\cite{butakov2024lossy_compression}.
We consider this to be a satisfactory outcome, since MIENF and AE+WKL utilize certain prior information about the test (specifically, the vector Gaussian copula structure in $\normal$-MIENF~\citep[Section B, paragraph 1]{butakov2024normflows},\footnote{$\normal$-MIENF requires $ \Pi_{X,Y} $ being gaussianizable via some Cartesian product mapping $ f_X \times f_Y $; the analysis provided in~\cite{czyz2025PMI} suggests that this is a strong implication, which is extremely unlikely to be satisfied in non-synthetic cases.} or the low intrinsic dimensionality in AE+WKL\footnote{For this estimator, we use the experimental setup from~\cite{butakov2024lossy_compression}, which skews the comparison against us since the autoencoder bottlenecks match the true intrinsic dimensionality of the data.
We show the degenerated performance of WKL in a slightly alternated setup in~\cref{appendix:WKL_bad}.}), while our estimator remains \textit{free of any inductive bias}.
Moreover, as a result of the tests conducted, we claim that our estimator is \textit{the best among all bias-free estimators} featured.

\vspace{-3mm}
\section{Discussion}
\vspace{-1mm}

\paragraph{Potential Impact.} Our contributions include the development of novel unbiased estimator for the MI grounded in diffusion bridge matching theory. The proposed algorithm, \ourestname{}, demonstrates superior performance compared to commonly used MI estimators without inductive bias on challenging image-based benchmarks. Also, our approach can be used to estimate the KL divergence, and differential entropy, see \cref{appendix:KL_estimator,appendix:entropy_estimator}. 

We believe that this work paves the way for new directions in the estimation of MI \textit{in high dimensions}. This has potential real-world applications such as text-to-image alignment \cite{wang2024IT_alignment}, self-supervised learning \cite{bachman2019DIM_across_views}, deep neural network analysis \cite{butakov2024lossy_compression}, and other use cases in high-dimensional settings.

Moreover, this approach offers \textit{significant opportunities for extension} by exploring alternative types of bridges within reciprocal processes. For instance, variance-preserving stochastic differential equations (SDEs) could be used instead of the Brownian motion \cite{zhoudenoising}. In addition, experimentation with different volatility coefficients $\epsilon$ \cite{liu2023_i2isb} or advanced diffusion bridge techniques, such as time reweighting \cite{kim2024simplereflow_thorton}, could further improve the methodology. Finally, for long-tailed data distributions, it may be possible to integrate long-tailed diffusion techniques \cite{yoon2023score_levy}, extending the applicability of our approach to even more complex settings.

\vspace{-3mm}
\paragraph{Limitations.} Our approach is fundamentally based on the  diffusion bridge matching framework.
While this class of models and its theoretical foundations have demonstrated strong potential in high-dimensional generative modeling \cite{shi2023diffusion, liu2023_i2isb, song2021sde}, they also have certain limitations that, while often negligible in the context of generative modeling, can be more pronounced in other applications.

One such limitation, evident in our work, is the challenge of accurately approximating heavy-tailed distributions.
As can be seen in Table~\ref{tab:benchmark}, one-degree-of-freedom Student-t distribution, i.e., \textit{St} (dof=1), also known as Cauchy distribution, has no first moment, and our method is not applicable to such distributions in theory \citep[Appendix C]{shi2023diffusion}.
However, worth noting that such limitations are quite common for generative modeling and quite probably should be applicable to denoising score matching and diffusion models as well \cite{franzese2024minde, song2021sde}. Another limitation of diffusion (and diffusion bridge matching models as a consequence) is that is requires of a lot of data samples at training stage. This could hinder the applicability of our method with low number of data samples.

\paragraph{Broader impact.} 
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.


\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix_content}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
