@article{Ao_Li_2022entropy_estimation_normflows,
    title={Entropy Estimation via Normalizing Flow},
    volume={36},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/21237},
    DOI={10.1609/aaai.v36i9.21237},
    abstractNote={Entropy estimation is an important problem in information theory and statistical science. Many popular entropy estimators suffer from fast growing estimation bias with respect to dimensionality, rendering them unsuitable for high dimensional problems. In this work we propose a transformbased method for high dimensional entropy estimation, which consists of the following two main ingredients. First by modifying the k-NN based entropy estimator, we propose a new estimator which enjoys small estimation bias for samples that are close to a uniform distribution. Second we design a normalizing flow based mapping that pushes samples toward a uniform distribution, and the relation between the entropy of the original samples and the transformed ones is also derived. As a result the entropy of a given set of samples is estimated by first transforming them toward a uniform distribution and then applying the proposed estimator to the transformed samples. Numerical experiments demonstrate the effectiveness of the method for high dimensional entropy estimation problems.},
    number={9},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Ao, Ziqiao and Li, Jinglai},
    year={2022},
    month={Jun.},
    pages={9990-9998}
}

@inproceedings{belghazi2018mine,
    title = {Mutual Information Neural Estimation},
    author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
    booktitle = {Proceedings of the 35th International Conference on Machine Learning},
    pages = {531--540},
    year = {2018},
    editor = {Dy, Jennifer and Krause, Andreas},
    volume = {80},
    series = {Proceedings of Machine Learning Research},
    month = {07},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf},  url = {https://proceedings.mlr.press/v80/belghazi18a.html},
    abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.}
}

@article{berrett2019efficient_knn_entropy_estimation,
    author = {Berrett, Thomas B. and Samworth, Richard J. and Yuan, Ming},
    doi = {10.1214/18-AOS1688},
    fjournal = {Annals of Statistics},
    journal = {Ann. Statist.},
    month = {02},
    number = {1},
    pages = {288--318},
    publisher = {The Institute of Mathematical Statistics},
    title = {Efficient multivariate entropy estimation via $k$-nearest neighbour distances},
    url = {https://doi.org/10.1214/18-AOS1688},
    volume = {47},
    year = {2019}
}

@inproceedings{butakov2024normflows,
    title={Mutual Information Estimation via Normalizing Flows},
    author={Ivan Butakov and Alexander Tolmachev and Sofia Malanchuk and Anna Neopryatnaya and Alexey Frolov},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=JiQXsLvDls}
}

@article{duong2023dine,
    title={Diffeomorphic Information Neural Estimation},
    volume={37},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/25908},
    DOI={10.1609/aaai.v37i6.25908},
    abstractNote={Mutual Information (MI) and Conditional Mutual Information (CMI) are multi-purpose tools from information theory that are able to naturally measure the statistical dependencies between random variables, thus they are usually of central interest in several statistical and machine learning tasks, such as conditional independence testing and representation learning. However, estimating CMI, or even MI, is infamously challenging due the intractable formulation. In this study, we introduce DINE (Diffeomorphic Information Neural Estimator)–a novel approach for estimating CMI of continuous random variables, inspired by the invariance of CMI over diffeomorphic maps. We show that the variables of interest can be replaced with appropriate surrogates that follow simpler distributions, allowing the CMI to be efficiently evaluated via analytical solutions. Additionally, we demonstrate the quality of the proposed estimator in comparison with state-of-the-arts in three important tasks, including estimating MI, CMI, as well as its application in conditional independence testing. The empirical evaluations show that DINE consistently outperforms competitors in all tasks and is able to adapt very well to complex and high-dimensional relationships.},
    number={6},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Duong, Bao and Nguyen, Thin},
    year={2023},
    month={Jun.},
    pages={7468-7475}
}

@misc{federici2023hybrid_MI_estimation,
    title={On the Effectiveness of Hybrid Mutual Information Estimation}, 
    author={Marco Federici and David Ruhe and Patrick Forré},
    year={2023},
    eprint={2306.00608},
    archivePrefix={arXiv},
    primaryClass={stat.ML},
    url={https://arxiv.org/abs/2306.00608}, 
}

@inproceedings{franzese2024minde,
    title={{MINDE}: Mutual Information Neural Diffusion Estimation},
    author={Giulio Franzese and Mustapha BOUNOUA and Pietro Michiardi},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=0kWd8SJq8d}
}

@inproceedings{goldfeld2019estimating_information_flow,
    title={Estimating Information Flow in Deep Neural Networks},
    author={Ziv Goldfeld and Ewout van den Berg and Kristjan H. Greenewald and Igor V. Melnyk and Nam H. Nguyen and Brian Kingsbury and Yury Polyanskiy},
    booktitle={ICML},
    year={2019}
}

@article{kozachenko1987entropy_of_random_vector,
    title = {Sample Estimate of the Entropy of a Random Vector},
    author = {L. F. Kozachenko and N. N. Leonenko},
    journal = {Problems Inform. Transmission},
    pages = {95--101},
    year = {1987},
    volume = {23},
    issue = {2},
}

@article{kraskov2004KSG,
    title = {Estimating mutual information},
    author = {Kraskov, Alexander and St\"ogbauer, Harald and Grassberger, Peter},
    journal = {Phys. Rev. E},
    volume = {69},
    issue = {6},
    pages = {066138},
    numpages = {16},
    year = {2004},
    month = {Jun},
    publisher = {American Physical Society},
    doi = {10.1103/PhysRevE.69.066138},
    url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138}
}

@InProceedings{mcallester2020limitations_MI,
    title = 	 {Formal Limitations on the Measurement of Mutual Information},
    author =       {McAllester, David and Stratos, Karl},
    booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
    pages = 	 {875--884},
    year = 	 {2020},
    editor = 	 {Chiappa, Silvia and Calandra, Roberto},
    volume = 	 {108},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {08},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v108/mcallester20a/mcallester20a.pdf},
    url = 	 {https://proceedings.mlr.press/v108/mcallester20a.html},
    abstract = 	 {Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N).}
}

@misc{oord2019infoNCE,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.03748}, 
}

@inproceedings{song2020understanding_limitations,
    title={Understanding the Limitations of Variational Mutual Information Estimators},
    author={Jiaming Song and Stefano Ermon},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=B1x62TNtDS}
}

@inproceedings{song2021sde,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{weglarczyk2018kde_review,
    author = {Weglarczyk, Stanislaw},
    year = {2018},
    month = {01},
    pages = {00037},
    title = {Kernel density estimation and its application},
    volume = {23},
    journal = {ITM Web of Conferences},
    doi = {10.1051/itmconf/20182300037}
}

