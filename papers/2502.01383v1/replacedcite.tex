\section{Related Work}
\paragraph{Mutual information estimators.}
Mutual information estimators fall into two main categories: \emph{non-parametric} and \emph{parametric}.
Parametric estimators are also subdivided into \emph{discriminative} and \emph{generative}____.
In addition to this natural classification, we distinguish \emph{diffusion-based} approaches to better contextualize our method in relation to the previous works.

\textbf{Non-parametric estimators.}
Classical approaches to the mutual information estimation rely on non-parametric density estimators, such as kernel density estimator____ and $ k $-nearest neighbors estimator____.
The resulting density estimate is plugged into~\eqref{eq:MI_definition_PDF} to acquire the MI estimate through MC-integration, leave-one-out method or other techniques.
The simplicity of such methods make them appealing for low-dimensional cases, but extensive high-dimensional evaluation suggests that these approaches are inapplicable to complex data 
~(\citealt[\wasyparagraph~5.3]{goldfeld2019estimating_information_flow}; \citealt[\wasyparagraph~6.2]{czyz2023beyond_normal}; \citealt[Table 1]{butakov2024normflows}).

\textbf{Non-diffusion-based generative estimators.}
More advanced techniques involve parametric density models, such as normalizing flows and variational autoencoders, to measure MI through density estimation.
This na\"{i}ve generative approach was described by ____ and further investigated in the works of ____.
However, despite better modelling capabilities, the results in____ indicate that direct PDF estimation can introduce a substantial bias to the MI estimate.
Therefore, it was proposed to avoid PDF estimation altogether and focus on measuring the density ratio in~\eqref{eq:MI_definition_PDF}.
This is done in the works of____
by leveraging the invariance property of mutual information.
Such methods show better performance on synthetic benchmarks, but may introduce an inductive bias due to the simplified closed-form expression being used to estimate the density ratio in question.

\textbf{Discriminative estimators.}
Finally, another approach to MI estimation involves training a classifier to discriminate between the samples from $ \Pi_{X_0,X_1} $ and $ \Pi_{X_0} \otimes \Pi_{X_1}$:  MINE____, InfoNCE____ and similar methods____.
This technique leverages variational bounds on the Kullback-Leibler divergence and provides a relatively cheap and reliable parametric estimator for a wide range of cases, including high-dimensional and complex data.
However, such estimators have severe demerits from a theoretical perspective, such as high variance in MINE and large batch size requirements in InfoNCE____.
Additionally, recent benchmarking results suggest that discriminative approaches can underperform compared to the generative methods when MI is high and the probability distribution is complex____.

\vspace{-3mm}
\paragraph{Neural Diffusion Estimator for MI (MINDE).} \label{sec:minde}

One of the most recent generative methods for MI Estimation is diffusion-based ____ MINDE ____. To estimate $\KL{\pi^A}{\pi^B}$ the authors learn two standard backward diffusion models to generate data from distributions $\pi^A$ and $\pi^B$, e.g., for $\pi^A$:


\vspace{-4mm}
\begin{gather}
    \begin{cases}
    Q^A: \underbrace{dx_{t}=[-f(x_t, t) + g(t)^2 s^{A}(x_t, t)]dt + g(t)d\hat{W}_t}_{\text{backward diffusion}}, \\
    x_T \sim q^A_{T}(x_T), \label{eq:diffusion_minde}
    \end{cases}
    \vspace{-10mm}
\end{gather}
\vspace{-4mm}


where $f$ and $g$ are the drift and volatility coefficients, respectively, of the forward diffusion ____, $d\hat{W}_t$ is the Wiener process when time flows backwards, and $q^A_{t}$ is the distribution of the noised data at time $t$ ____. The similar expressions hold for $\pi^B$ and $Q^B$. Then, the authors formulate a KL divergence estimator through the difference of diffusion \textit{score functions}:

\vspace{-5mm}
\begin{gather}
    \KL{\pi^A}{\pi^B}=\KL{Q^A}{Q^B}=
    \nonumber
    \\
    \int_0^T \mathbb{E}_{q^A_t(x_t|x_0)} \left[ 
    \frac{g(t)^2}{2}
    \| s^{A}(x_t, t) - s^{B}(x_t, t) \|^2\right]dt +
    \nonumber
    \\
    \KL{q^A_{T}}{q^B_{T}} \label{eq:minde_est}.
\end{gather}
\vspace{-4mm}

Here, $\KL{q^A_{T}}{q^B_{T}}$ is the \textbf{bias} term, which vanishes only when diffusion has infinitely many steps, i.e., $T \rightarrow \infty$.  When the diffusion score functions $s^A$ and $s^B$ \eqref{eq:diffusion_minde} are properly learned, one can draw samples from the forward diffusion $q^A_t(x_t|x_0)$ and compute the estimate of KL divergence \eqref{eq:minde_est}. In this way, the authors transform the problem of training the KL divergence estimator into the problem of learning the backward diffusions \eqref{eq:diffusion_minde} that generate \textit{data from noise}.

To estimate mutual information, the authors propose a total of four equivalent methods, all based on the estimation of up to three KL divergences \eqref{eq:minde_est} or their expectations.

\vspace{-3mm}