\section{Additional theoretical results}

\subsection{KL divergence estimator}
\label{appendix:KL_estimator}

In this section, we present a general result for the unbiased estimation of KL divergence between any two distributions $\pi_1(x), \pi_2(x) \in \mathcal{P}(\mathbb{R}^d)$ through the difference of drifts of the SDE formulation \eqref{eq:reciprocal_non_markovian_sde} of the reciprocal process induced by these distributions, i.e.:

\begin{gather}
    Q_{\pi_1} = \int W^\epsilon_{|x_0, x_1}d\pi_1(x_1)dp(x_0), \label{eq:appx_recip_pi_1}
    \\
    Q_{\pi_2} = \int W^\epsilon_{|x_0, x_1}d\pi_2(x_1)dp(x_0).  \label{eq:appx_recip_pi_2}
\end{gather}

\begin{theorem}[KL divergence decomposition] \label{th:general_kl}
    Consider distributions $\pi_1(x), \pi_2(x), p(x) \in \mathcal{P}(\mathbb{R}^d)$ and reciprocal processes $Q_{\pi_1}$, $Q_{\pi_2}$ induced by distributions $\pi_1(x), \pi_2(x)$ \eqref{eq:appx_recip_pi_1} \eqref{eq:appx_recip_pi_2}. Then the KL divergence between distributions $\pi_1(x)$ and $\pi_2(x)$ can be represented in the following way:
    
    \vspace{-5mm}
    \begin{gather}
        \KL{\pi_1(x_1)}{\pi_2(x_1)} 
        = \frac{1}{2\epsilon} \int_{0}^{1} \mathbb{E}_{x_t \sim q_{\pi_1}(x_t, x_0) }\big[ \Vert v^{\pi_1}(x_t, t, x_0) - v^{\pi_2}(x_t, t, x_0) \Vert_2^2 \big] dt, \label{eq:kl_general_appx} 
    \end{gather}
    \vspace{-3mm}

    where
    \vspace{-5mm}
    \begin{gather}
        v^{\pi_1}(x_t, t, x_0) = \mathbb{E}_{x_1 \sim q_{\pi_1}(x_1|x_t, x_0)}\left[\frac{x_1 - x_t}{1 - t}\right], \\
        v^{\pi_2}(x_t, t, x_0) = \mathbb{E}_{x_1 \sim q_{\pi_2}(x_1|x_t, x_0)}\left[\frac{x_1 - x_t}{1 - t}\right]
    \end{gather}

    are the drifts of reciprocal processes  $Q_{\pi_1}$ and $Q_{\pi_2}$ respectively. $p(x)$ can be any distribution of choice.
    
\end{theorem}

Our theorem allows us to estimate $\KL{\pi_1(x)}{\pi_2(x)}$ knowing only the drifts $v^{\pi_1}(x_t, t, x_0)$ and $v^{\pi_2}(x_t, t, x_0)$, which can be recovered using conditional bridge matching \wasyparagraph\ref{sec:cond_bm}. Note that the expression \eqref{eq:kl_general_appx} is very similar to the expression \eqref{eq:mut_info_est} in \ref{th:mut_info_drifts}. However, there is a difference because \eqref{eq:mut_info_est} estimates the KL divergence between joint plans $\pi(x_0, x_1)$ and $\pi(x_0)\pi(x_1)$, where $x_0, x_1$ are random variables in the same dimension $\mathbb{R}^d$, but \eqref{eq:kl_general_appx} estimates the KL divergence between general distributions that should not be represented as a joint plan between two random variables of the same dimension. Note that theorem~\ref{th:general_kl} holds for any distribution $p(x_0)$, which can be considered as part of the design space and optimised for each particular problem.

The KL divergence is a fundamental quantity, and its estimator can have many applications, such as mutual information estimation or entropy estimation using results described in \cref{appendix:entropy_estimator}.

\begin{proof}
    Consider 
    \begin{gather}
        \KL{\pi_1(x_1)p(x_0)}{\pi_2(x_1)p(x_0)} = \underbrace{\KL{p(x_0)}{p(x_0)}}_{=0} + \mathbb{E}_{x_0}\left[ \KL{\pi_1(x_1|x_0)}{\pi_2(x_1|x_0)} \right] =\nonumber\\ =\mathbb{E}_{x_0}\left[ \KL{\pi_1(x_1)}{\pi_2(x_1)} \right] = \KL{\pi_1(x_1)}{\pi_2(x_1)}, \nonumber
    \end{gather}
    
    Next to get 

    \begin{gather}
        \KL{\pi_1(x_1)p(x_0)}{\pi_2(x_1)p(x_0)} = \frac{1}{2\epsilon} \int_{0}^{1} \mathbb{E}_{x_t \sim q_{\pi_1}(x_t, x_0) }\big[ \Vert v^{\pi_1}(x_t, t, x_0) - v^{\pi_2}(x_t, t, x_0) \Vert_2^2 dt \big]\nonumber, 
    \end{gather}
    
    one can repeat all the steps that were taken to show:
    
    \begin{gather}
        \KL{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} =  \frac{1}{2\epsilon}  \int_{0}^{1} \mathbb{E}_{q_\pi(x_t, x_0) }\big[ \Vert v_{\text{joint}}(x_t, t, x_0) - v_{\text{ind}}(x_t, t, x_0) \Vert_2^2 \big] dt\nonumber,
    \end{gather}
    
    in the proof of Theorem~\ref{th:mut_info_drifts}.
    

\end{proof}

\subsection{Differential entropy estimator}
\label{appendix:entropy_estimator}

A general result on the information projections and maximum-entropy distributions suggests a way of calculating differential entropy through the KL divergence estimation. 
Consider the following theorem:

\begin{theorem}[Theorem 6.7 in~\cite{kappen2024lectures}]\label{th:entropy_kl_exponential}
    Let $ \phi \colon \reals^n \to \reals^k $ be any measurable function, an absolutely continous probability distribution $p(x)\in \mathcal{P}(\mathbb{R}^d)$ and define $ \alpha \defeq \expect_{p} \phi(x) $.
    Now, for any $ \theta \in \reals^k $ consider an absolutely continuous probability distribution $ q_\theta \in \mathcal{P}(\mathbb{R}^d)$ with such probability density:
    \[
        q_\theta(x) = \exp(\dotprod{\theta}{\phi(x)} - A(\theta)), \quad A(\theta) = \log \expect_{q_{\theta}} e^{\dotprod{\theta}{\phi(x)}}.
    \]
    If there exists $ \theta^* $ such that $p$ is absolutely continous w.r.t. $q_{\theta^*}$ and $ \expect_{q_{\theta^*}} \phi(x) = \alpha $, then
    \[
        H(p) = H(q_{\theta^*}) - \KL{p}{q_{\theta^*}},
    \]
\end{theorem}



\begin{corollary}
    \label{corollary:Gaussian_maximizes_entropy}
    Let $ X $ be a $ d $-dimensional absolutely continuous random vector with probability density function $ \PDF $, mean $ m $ and covariance matrix $ \Sigma $.
    Then
    \begin{equation*}
        \label{eq:Gaussian_maximizes_entropy}
        H(\PDF) = H\left( \normal(m, \Sigma) \right) - \KL{\PDF}{\normal(m, \Sigma)}, \qquad H\left( \normal(m, \Sigma) \right) = \frac{1}{2} \log \left( (2 \pi e)^d \det \Sigma \right),
    \end{equation*}
    where $ \normal(m, \Sigma) $ is a Gaussian distribution of mean $ m $ and covariance matrix $ \Sigma $.
\end{corollary}

\begin{corollary}
    \label{corollary:uniform_maximizes_entropy}
    Let $ X $ be an absolutely continuous random vector with probability density function $ \PDF $
    %and support $ S $ of Lebesgue measure $ 0 < \mu(S) < +\infty $.
    and $ \supp X \subseteq S $, where $ S $ has finite and non-zero Lebesgue measure $ \mu(S) $.
    Then
    \begin{equation*}
        \label{eq:uniform_maximizes_entropy}
        H(p) = H\left( \uniform(S) \right) - \KL{\PDF}{\uniform(S)}, \qquad H\left( \uniform(S) \right) = 
        \log \mu(S),
    \end{equation*}
    where $ \uniform(S) $ is a uniform distribution on $ S $.
\end{corollary}


Similar results can also be obtained for other members of the exponential family.
The first result (\cref{corollary:Gaussian_maximizes_entropy}) can be considered as a general recipe,
while the second one (\cref{corollary:uniform_maximizes_entropy}) can be useful when we have prior knowledge about the support of $X$ being restricted. Approach described in \cref{th:entropy_kl_exponential} is very flexible and can be considered as a generalization of the method used in~\cite{franzese2024minde}. 

In practice, to estimate the entropy of some probability distribution, it is sufficient to follow the one of the described in \cref{corollary:uniform_maximizes_entropy,corollary:Gaussian_maximizes_entropy} results. For example, if one uses Corollary~\ref{corollary:Gaussian_maximizes_entropy}:
1) estimate mean $m$ and covariance matrix $\Sigma$ using a set of data samples,
2) calculate entropy $H(\mathcal{N}(m, \Sigma))$ via the provided closed form expression,
3) calculate the KL divergence $\KL{\PDF}{\normal(m, \Sigma)}$ via learning two conditional diffusion bridges models and utilizing our estimator (\eqref{eq:kl_general_appx} from \cref{th:general_kl}).


\subsection{Pointwise mutual information estimation} \label{appx:PMI}

Mutual information can also be defined as an expectation of the \emph{pointwise mutual information (PMI)}:
\begin{equation}
    \label{eq:pointwise_mutual_information}
    \PMI_{X_0,X_1}(x_0,x_1) = \log \left[ \frac{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} \right] = \log \left[ \frac{\pi(x_0 \mid x_1)}{\pi(x_0)} \right], \quad I(X_0;X_1) = \expect_{x_0,x_1 \sim \pi(x_0, x_1)} \PMI_{X_0,X_1}(x_0, x_1)
\end{equation}
PMI quantifies local statistical interactions and is widely used in generative models alignment~\cite{nandwani2023PMI_NLP,wang2024IT_alignment},
reinforcement learning~\cite{baram2021action_redundancy,yang2024diverse_policies_recovering_PMI}, and non-linear principal component analysis~\citep{cunningham2022principal_component_flows,butakov2024normflows}.

Next we present a way to estimate PMI using our framework of reciprocal processes and diffusions. Consider two distributions $\pi_1(x), \pi_2(x) \in \mathcal{P}(\mathbb{R}^d)$ and corresponding reciprocal processes $Q_{\pi_1}$, $Q_{\pi_2}$ induced by distributions $\pi_1(x), \pi_2(x)$, respectively, i.e., \eqref{eq:appx_recip_pi_1} \eqref{eq:appx_recip_pi_2}. Then the Radom-Nikodym derivative of these processes $Q_{\pi_1}$ and $Q_{\pi_2}$ can be decomposed by applying the disintegration theorem \cite{leonard2014some}:

\begin{gather}
    \frac{dQ_{\pi}}{dQ^\text{ind}_{\pi}}(\{x_t\}_{t\in[0, 1]}) = \frac{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} \frac{dQ_{\pi|x_0, x_1}}{dQ^\text{ind}_{\pi|x_0, x_1}}(\{x_t\}_{t\in(0, 1)}). \nonumber
\end{gather}

Next one can notice that $Q_{\pi}$ and $Q^\text{ind}_{\pi}$ are reciprocal processes. Therefore, their insides are identical Brownian Bridges, i.e.,  $Q_{\pi|x_0, x_1} = Q^\text{ind}_{\pi|x_0, x_1} = W^\epsilon_{|x_0,x_1}$, and their Radon-Nikodym derivative is equal to 1, i.e,  $\frac{dQ_{\pi|x_0, x_1}}{dQ^\text{ind}_{\pi|x_0, x_1}}(\{x_t\}_{t\neq0, 1})=1$. Then one can take the additional expectation on the Brownian Bridge trajectories, i.e., $\{x_t\}_{t\neq0, 1}$, which are conditioned to start and end at points $x_0$ and $x_1$, respectively:

\begin{gather}
    \mathbb{E}_{\{x_t\}_{t\neq0, 1}}\left[\frac{dQ_{\pi}}{dQ^\text{ind}_{\pi}}(\{x_t\}) \right]= \mathbb{E}_{\{x_t\}_{t\neq0, 1}}\left[\frac{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)} \right] = \frac{\pi(x_0, x_1)}{\pi(x_0)\pi(x_1)}. \nonumber
\end{gather}

Finally by applying the Girsanov theorem \cite{oksendal2003stochastic} to the left side one can get:

\begin{gather}
    \PMI_{X_0, X_1}(x_0, x_1) = \frac{1}{2\epsilon} \int_{0}^{1} \mathbb{E}_{q_{\pi}(x_t|x_0, x_1)} \Vert v_{\text{joint}}(x_t, t, x_0) - v_{\text{ind}}(x_t, t, x_0) \Vert^2 dt,
\end{gather}

where $v_{\text{joint}}$ and $v_{\text{ind}}$ follow the same expression as in Theorem~\ref{th:mut_info_drifts}, i.e., \eqref{eq:th_drift_plan} \eqref{eq:th_drift_ind}.

One can notice that this formula is very similar to the \eqref{eq:mut_info_est} but without conditioning on $x_0, x_1$. Therefore, once having proper approximations of $v_{\text{joint}}$ and $v_{\text{ind}}$, it is trivial to apply this result for the PMI estimation in practice by considering a slightly altered version of Algorithm~\ref{alg:mi_est}.


\subsection{Interaction information} \label{appx:interacion_info}

Here we propose the generalization of \ourestname{} for the \textit{interaction information} estimation, which is the generalization of mutual information for more than two random variables. Interaction information for random variables $X_0, X_1, X_2$ is defined by:

\begin{gather}
    I(X_0;X_1; X_2) \defeq I(X_0;X_1) - I(X_0;X_1|X_2) = \nonumber\\ = \KL{\Pi(X_0, X_1)}{\Pi(X_0)\otimes \Pi(X_1)} - \KL{\Pi(X_0, X_1|X_2)}{\Pi(X_0|X_2)\otimes \Pi(X_1|X_2)}.
\end{gather}

This definition can be generalized for more random variables in a similar way. Both MI information terms can be estimated using Theorem~\ref{th:mut_info_drifts} and practical algorithm \ourestname{}. Applications of interaction information include neuroscience \cite{bounoua2024somegai_franceze_O_info}, climate models \cite{runge2019inferring}, econometrics \cite{dosi2019more}.


\section{Experimental supplementary}
\label{appendix:experimental_details}

In this section, additional experimental results and experimental details are described.

\subsection{Low-dimensional benchmark}

\paragraph{Additional results.} 

In the Table~\ref{tab:mean_100k_prec_2} we present the results of low-dimensional benchmark \cite{czyz2023beyond_normal} with precision of 0.01 nats.


\paragraph{Experimental details.}

The benchmark implementation was taken from the official github repository: 

\begin{center}
    \url{https://github.com/cbg-ethz/bmi}
\end{center}

Neural networks were taken of almost the same architecture as in \cite{franzese2024minde}, which is MLP with residual connections and time embedding. Additional input $s$ described in \wasyparagraph~\ref{sec:vector_field_param} was processed the same way as time input. Number of parameters was taken depending on a dimensionality of the problem, see Table~\ref{tab:low_dim_nn_params}. Exponential Moving Average as a widely recognized training stabilization method was used with decay parameter of $0.999$. For all the problems neural networks were trained during 100k iterations with $\epsilon=1$, batch size $512$, lr $0.0003$. Mutual Information was estimated by Algorithm~\ref{alg:mi_est} with $N$ pairs of samples $\{x_t^i, x_0^i, t^i\}_{i=1}^N$, where $N$ is equal to the number of test samples times $10$, i.e., $100$k.

\begin{table}[]
    \centering
    \small
        \begin{tabular}{ cccc } 
            \toprule
            Dim & Filters & Time Embed & Parameters \\
            \midrule
            $\leq$5  & 64 & 64 & 43K \\
            25  & 128 & 128 &  176K \\
            50  & 256 & 256 &  699K \\
            \bottomrule
        \end{tabular}
    \caption{Neural networks hyperparametes for low-dimensional \cite{czyz2023beyond_normal} benchmark. ``Dim'' - dimensionality of a MI estimation problem, ``Filters'' -- number of filters in MLP, ``Time Embed'' -- number filters in time embedding module, ``Parameters'' -- number of overall neural networks parameters. }
    \label{tab:low_dim_nn_params}
\end{table}

\subsection{Image data benchmark} \label{appendix:image_bench}

\paragraph{Experimental details for \ourestname{}.}

The implementation of \cite{butakov2024normflows} image data benchmark was taken from the official github repository:

\begin{center}
    \url{https://github.com/VanessB/mutinfo}
\end{center}

Following authors of the benchmark, \textit{gaussian} images were generated with all the default settings, \textit{rectangle} images were generated with all the default settings, but minimum size of rectangle is $0.2$ to avoid singularities.
All the covariance matrices for the distributions defining the mutual information in the benchmark were generated without randomization of component-wise mutual information, but with randomization of the off-diagonal blocks of the covariance matrix. 

To approximate the drift coefficient of diffusion U-Net \cite{ronneberger2015u} with time, condition neural networks were used, special input $s$ was processed as time input. For all the tests, neural networks were the same and had $2$ residual layers per U-net block with $256$ base channels, positional timestep encoding, upscale and downscale blocks consisting of two resnet blocks, one with attention and one without attention. The number of parameters is $\sim$27M. During the training, 100k gradient steps were made with batch size of 64 and learning rate $0.0001$. Exponential moving average was used with decay rate $0.999$. Mutual Information was estimated by Algorithm~\ref{alg:mi_est} with $N$ pairs of samples $\{x_t^i, x_0^i, t^i\}_{i=1}^N$, where $N$ is equal to the number of test samples, i.e., $10$k. Nvidia A100 was used for the \ourestname{} training. Each run (one seed) took around $6$ and $18$ GPU-hours for the $16\times16$ and  $32\times32$ image resolution setups, respectively.

\paragraph{Experimental details for other methods.}

In this part, we provide additional experimental details regarding other methods featured in~\cref{figure:compare_methods_images}.
We report the NN architectures used for neural estimators in~\cref{table:synthetic_architecture}.

\begin{table}[ht!]
    \center
    \caption{The NN architectures used to conduct the tests with synthetic images   in~\cref{sec:exps}.}
    \label{table:synthetic_architecture}
    \begin{tabular}{cc}
    \toprule
    NN & Architecture \\
    \midrule
    \makecell{GLOW, \\ $ 16 \times 16 $ ($ 32 \times 32 $) \\ images} &
    \small
        \begin{tabular}{rl}
            $ \times 1 $: & \makecell[l]{$ 4 $ ($ 5 $) splits, $ 2 $ GLOW blocks between splits, \\ $ 16 $ hidden channels in each block, leaky constant $ = 0.01 $}$^{ \times 2 \; \text{in parallel}} $ \\
            $ \times 1 $: & Orthogonal projection linear layer$^{ \times 2 \; \text{in parallel}} $ 
        \end{tabular} \\
    \\
    \makecell{Autoencoder,\\ $ 16 \times 16 $ ($ 32 \times 32 $) \\ images} &
        \small
        \begin{tabular}{rl}
            $ \times 1 $: & Conv2d(1, 4, ks=3), BatchNorm2d, LeakyReLU(0.2), MaxPool2d(2) \\
            $ \times 1 $: & Conv2d(4, 8, ks=3), BatchNorm2d, LeakyReLU(0.2), MaxPool2d(2) \\
            $ \times 2(3) $: & Conv2d(8, 8, ks=3), BatchNorm2d, LeakyReLU(0.2), MaxPool2d(2) \\
            $ \times 1 $: & Dense(8, dim), Tanh, Dense(dim, 8), LeakyReLU(0.2) \\
            $ \times 3(4) $: & Upsample(2), Conv2d(8, 8, ks=3), BatchNorm2d, LeakyReLU(0.2) \\
            $ \times 1 $: & Upsample(2), Conv2d(8, 4, ks=3), BatchNorm2d, LeakyReLU(0.2) \\
            $ \times 1 $: & Conv2d(4, 1, ks=3), BatchNorm2d, LeakyReLU(0.2) \\
        \end{tabular} \\
        \\
    \makecell{Critic NN, \\ $ 16 \times 16 $ ($ 32 \times 32 $) \\ images} &
        \small
        \begin{tabular}{rl}
            $ \times 1 $: & [Conv2d(1, 16, ks=3), MaxPool2d(2), LeakyReLU(0.01)]$^{ \times 2 \; \text{in parallel}} $ \\
            $ \times 1(2) $: & [Conv2d(16, 16, ks=3), MaxPool2d(2), LeakyReLU(0.01)]$^{ \times 2 \; \text{in parallel}} $ \\
            $ \times 1 $: & Dense(256, 128), LeakyReLU(0.01) \\
            $ \times 1 $: & Dense(128, 128), LeakyReLU(0.01) \\
            $ \times 1 $: & Dense(128, 1) \\
        \end{tabular} \\
    \bottomrule
    \end{tabular}
\end{table}


\textbf{MIENF.}
This method is based on bi-gaussianization of the input data via a Cartesian product of learnable diffeomorphisms~\cite{butakov2024normflows}.
Such approach allows for a closed-form expression to be employed to estimate the MI.

With only minor stability-increasing changes introduced, we adopt the Glow~\cite{kingma2018glow} flow network architecture from~\cite{butakov2024normflows}, which is also reported in~\cref{table:synthetic_architecture} (``GLOW'').
We used the \texttt{normflows} package~\citep{stimper2023normflows} to implement the model.
Adam~\citep{kingma2017adam} optimizer was used to train the network
on $ 10^5 $ images with a batch size $ 512 $, and the learning rate decreasing from $ 5 \cdot 10^{-4} $ to $ 10^{-5} $ geometrically.
For averaging, we used $ 3 $ different seeds.
Nvidia A100 was used to train the flow models.
Each run (one seed) took no longer than four GPU-hours to be completed.

\textbf{KSG.} Kraskov-St\"{o}gbauer-Grassberger~\cite{kraskov2004KSG} mutual information estimator is a well-known $k$-NN non-parametric method, which is very similar to unweighted Kozachenko-Leonenko estimator~\cite{kozachenko1987entropy_of_random_vector}.
This method employs distances to $k$-th nearest neighbors to approximate the pointwise mutual information, which is then averaged.

We used $ k = 1 $ (one nearest neighbour) for all the tests.
The number of samples was $ 10^5 $ for Gaussian images and $ 10^4 $ for images of rectangles (we had to lower the sampling size due to degenerated performance of the metric tree-based $k$-NN search in this particular setup).
A single core of AMD EPYC 7543 CPU was used for nearest neighbors search and MI calculation.
Each run (one seed) took no longer than one CPU-hour to be completed.

\textbf{AE+WKL 5-NN.} The idea of leveraging lossy compression to tackle the curse of dimensionality and provide better MI estimates is well-explored in the literature~\cite{goldfeld2021sliced_MI, goldfeld2022k_sliced_MI, tsur2023max_sliced_MI, fayad2023optimal_sliced_MI, kristjan2023smoothed_entropy_PCA, butakov2024lossy_compression}.
In our work, we adopt the non-linear compression setup from~\cite{butakov2024lossy_compression}, which employs autoencoders for data compression and weighted Kozachenko-Leonenko method~\cite{berrett2019efficient_knn_entropy_estimation} for MI estimation in the latent space.

The autoencoders were trained using Adam optimizer
on $ 10^5 $ images with a batch size $ 512 $, a learning rate $ 10^{-3} $ and MAE loss for $ 10^4 $ steps.
For averaging, we used $ 5 $ different seeds.
Nvidia A100 was used to train the autoencoder model.
Each run (one seed) took no longer than one GPU-hour to be completed.

\textbf{MINE, NWJ, InfoNCE.} 
These discriminative approaches are fundamentally alike: each method estimates mutual information by maximizing the associated KL-divergence bound:

\begin{equation}
    \label{eq:discriminative_methods_lower_boud}
    I(X;Y) = \KL{\Pi_{X,Y}}{\Pi_X \otimes \Pi_Y} \geqslant \sup_{T \colon \mathcal{X} \times \mathcal{Y} \to \reals} \mathrm{F}[T(x,y)],
\end{equation}
where $ T $ is measurable, and $ \mathrm{F} $ is some method-specific functional.
In practice, $ T $ is approximated via a neural network, with the right-hand-side in~\eqref{eq:discriminative_methods_lower_boud} being used as the loss function.

Motivated by this similarity, we use a nearly identical experimental framework to assess each approach within this category.
To approximate $ T $ in experiments with synthetic images, we adopt the critic NN architecture from~\cite{butakov2024normflows}, which we also report in~\cref{table:synthetic_architecture} (``Critic NN'').

The networks were trained via Adam optimizer
on $ 10^5 $ images with a learning rate $ 10^{-3} $, a batch size $ 512 $ (with InfoNCE being the only exception, for which we used batch size $ 256 $ for training and $ 512 $ for evaluation due to memory constraints), and MAE loss for $ 10^5 $ steps.
For averaging, we used $ 5 $ different seeds.
Nvidia A100 was used to train the models.
In any setup, each run (one seed) took no longer than two GPU-hours to be completed.

\subsection{Additional experiments with AE+WKL estimator using higher bottleneck dimensionality}
\label{appendix:WKL_bad}

Prior works suggest that non-parametric MI estimators are highly prone to the curse of dimensionality compared to NN-based approaches~\cite{goldfeld2020convergence_of_SEM_entropy_estimation,czyz2023beyond_normal,butakov2024normflows}.
In particular, Figures 2 and 3 in~\cite{goldfeld2020convergence_of_SEM_entropy_estimation} and Table 1 in~\cite{butakov2024normflows} indicate that weighted Kozachenko-Leonenko estimator fails to yield reasonable estimates at all if the dimensionality reaches certain treshold.
That is why autoencoders are used in our setup to acquire MI estimates for high-dimensional synthetic images.

However, this approach introduces a substential inductive bias.
Not only we assume the data to be distributed on a manifold, but also select the bottleneck dimensionality of the autoencoders to be equal to the ground-truth intrinsic dimensionality, which is usually not available in practical scenarious.
Such prior knowledge allows for remarkable results, as it can be seen in~\cref{figure:compare_methods_images}.
However, we argue that even slight changes to this experimental protocol can lead to severe problems.
In the Gaussian images setup, increasing the bottleneck dimensionality from $ 2 $ (which is the intrinsic dimensionality of the dataset in question) to $ 4 $ completely destabilizes the estimator.
We report our results in \cref{figure:compare_methods_images_WKL_bad}; all the other details of the experimental setup are identical to the settings used for~\cref{figure:compare_methods_images}.

\begin{figure*}[ht!]
    \centering
    \small
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \begin{gnuplot}[terminal=tikz, terminaloptions={color size 8.0cm,4.5cm fontscale 0.8}]
            load "./gnuplot/common.gp"
    
            KL_file_path = "./data/WKL_bad/KL16g_100k_bad.csv"
            
            confidence = 0.99
            #max_y = 10.0
    
            load "./gnuplot/estmi_mi_WKL.gp"
        \end{gnuplot}
        \caption{$16\times 16$ images (Gaussians)}
    \end{subfigure}%
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \begin{gnuplot}[terminal=tikz, terminaloptions={color size 8.0cm,4.5cm fontscale 0.8}]
            load "./gnuplot/common.gp"
    
            KL_file_path = "./data/WKL_bad/KL32g_100k_bad.csv"
            
            confidence = 0.99
            #max_y = 10.0
    
            load "./gnuplot/estmi_mi_WKL.gp"
        \end{gnuplot}
        \caption{$32\times 32$ images (Gaussians)}
    \end{subfigure}
    \caption{Results for AE+WKL $5$-NN with increased bottleneck dimensionality. Along $ x $ axes is $ I(X_0;X_1) $, along $ y $ axes is MI estimate $ \hat I(X_0;X_1) $. We plot 99\% asymptotic CIs acquired from different seed runs ($ 5 $ seeds in total).}
    \label{figure:compare_methods_images_WKL_bad}
    %\vskip 0.2in
\end{figure*}

\input{plots_and_tables/low_dim_bench_2_prec}