\section{Related work}
A number of researchers have created datasets to evaluate the mathematical abilities of LLMs. {\bf MATH} \citep{hendrycks2021measuring} contains a large collection of mathematical problems from different domains, with 7 different levels of difficulty. The answer is always a number. Also the MMLU \citep{hendrycks2020measuring} contains a mathematics section consisting of multiple-choice questions.  

{\bf GSM8K} \citep{cobbe2021training} contains word problems on a grade-school level solvable by simple arithmetic. The answer is always a number. {\bf GSM-Plus} \citep{li2024gsm} and  {\bf GSM-symbolic} \citep{mirzadeh2024gsm} are both extensions of GSM8k with adversarial examples. In the latter case, the authors showed that it was possible to confuse the models by adding irrelevant numerical information to the problem formulation. In some cases, the models worked this irrelevant information into the solutions, leading to incorrect answers. 

{\bf U-MATH} \citep{chernyshev2024u} contains university-level problems given in mathematical notation and with figures (i.e.\ the input is multimodal). 

All these datasets are quite large, containing thousands of similar problems, and they are amenable to automatic assessment.