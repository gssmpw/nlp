% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes


\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage{amsmath}       
\usepackage{graphicx}      
\usepackage{caption}       
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage{underscore}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{atbegshi}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{pifont}



\title{VisPath: Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization}

\author{
 \textbf{Wonduk Seo\textsuperscript{1}}\thanks{Equal contribution.}\hspace{0.5em}
 \textbf{Seungyong Lee\textsuperscript{2}\textsuperscript{*}}\hspace{0.5em}
 \textbf{Daye Kang\textsuperscript{1,3}}\hspace{0.5em}
 \textbf{Zonghao Yuan\textsuperscript{4}}\hspace{0.5em}
 \textbf{Seunghyun Lee\textsuperscript{1}}\thanks{Corresponding author.}\hspace{0.5em}
 % \textbf{Corresponding\textsuperscript{4}}
\\
\\
 \textsuperscript{1}Enhans.ai\hspace{0.5em}
 \textsuperscript{2}Unaffiliated\hspace{0.5em}
 \textsuperscript{3}KAIST\hspace{0.5em}
 \textsuperscript{4}Tsinghua University\hspace{0.5em}
 % \textsuperscript{4}Affiliation 4
\\
\textsuperscript{1}\texttt{\{seowonduk,seunghyun,daye\}@enhans.ai}\hspace{0.5em}\\
\textsuperscript{2}\texttt{leesy7197@khu.ac.kr}\hspace{0.5em}
\textsuperscript{4}\texttt{yzh23@tsinghua.edu.cn}\hspace{0.5em}
}


\begin{document}
\maketitle
\begin{abstract}
Unprecedented breakthroughs in Large Language Models (LLMs) has amplified its penetration into application of automated visualization code generation. Few-shot prompting and query expansion techniques have notably enhanced data visualization performance, however, still fail to overcome ambiguity and complexity of natural language queries - imposing an inherent burden for manual human intervention. To mitigate such limitations, we propose a holistic framework \textbf{\textit{VisPath : A Multi-Path Reasoning and Feedback-Driven Optimization Framework for Visualization Code Generation}}, which systematically enhances code quality through structured reasoning and refinement. \emph{VisPath} is a multi-stage framework, specially designed to handle underspecified queries. To generate a robust final visualization code, it first utilizes initial query to generate diverse reformulated queries via Chain-of-Thought (CoT) prompting, each representing a distinct reasoning path. Refined queries are used to produce candidate visualization scripts, consequently executed to generate multiple images. Comprehensively assessing correctness and quality of outputs, \emph{VisPath} generates feedback for each image, which are then fed to aggregation module to generate optimal result. Extensive experiments on benchmarks including \emph{MatPlotBench} and the \emph{Qwen-Agent Code Interpreter Benchmark} show that \emph{VisPath} significantly outperforms state-of-the-art (SOTA) methods, increased up to average 17\%, offering a more reliable solution for AI-driven visualization code generation.


\end{abstract}

\section{Introduction}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{new-image.png}
    \caption{\textbf{Overview of different approaches for visualization code generation}. Comparing two baseline methods, namely \emph{Chat2VIS}~\cite{maddigan2023Chat2VIS} and \emph{MatPlotAgent}~\cite{yang2024matplotagent}, with our proposed \emph{VisPath} framework.}
    \label{fig:enter-label}
\end{figure}
Data visualization has long been an essential tool in data analysis and scientific research, enabling users to uncover patterns and relationships in complex datasets~\cite{vondrick2013hoggles,demiralp2017foresight,unwin2020data,li2024visualization}. Traditionally, creating visualizations requires manually writing code using libraries such as Matplotlib, Seaborn, or D3.js~\cite{barrett2005matplotlib,bisong2019matplotlib,zhu2013data}. This approach demands programming expertise and significant effort to craft effective visual representations, which can be a barrier for many users~\cite{bresciani2015pitfalls,saket2018task,sharif2024understanding}. As datasets continue to grow in size and complexity, researchers have explored ways to automate visualization generation, aiming to make the process more efficient and accessible~\cite{wang2015big,dibia2019data2vis,qian2021learning}.

In response to this challenge, Large Language Models (LLMs) have emerged as a promising solution for simplifying visualization creation~\cite{wang2023data,han2023chartllama,xie2024haichart}. By translating natural language instructions into executable code, LLM-based systems eliminate the need for extensive programming knowledge, allowing users to generate visualizations more intuitively~\cite{xiao2023let,ge2023automatic,zhang2024gpt}. Recent visualization methods such as ChartGPT~\cite{tian2024chartgpt} and NL4DV~\cite{sah2024generating} demonstrate the potential of LLMs to provide interactive, conversational interfaces for visualization. These systems enable users to create complex charts with minimal effort, bridging the gap between technical expertise and effective data exploration~\cite{dibia2023lida,kim2024phenoflow}.
    


More recently, LLM-based visualization frameworks such as Chat2VIS~\cite{maddigan2023Chat2VIS} and MatPlotAgent~\cite{yang2024matplotagent} have been introduced to improve automated visualization code generation. Specifically, Chat2VIS follows a prefix-based approach, guiding LLMs to generate visualization code consistently; and MatPlotAgent expands the query before code generation. However, these methods face several limitations: \ding{172} both generate code in a single-path manner, limiting exploration of alternative solutions and unable to fix-out when caught in misleading bugs; \ding{173} both rely on predefined structures or examples which restrict adaptability to ambiguous or unconventional user queries. \ding{174} both approaches encapsulate limitation in their inability to aggregate and synthesize multi-dimensional feedback. Without a mechanism to retrieve outputs that reflect diverse possibilities, they struggle to capture intricate details, ultimately limiting the precision and adaptability of the generated visualizations.  


To address these limitations, we introduce \emph{VisPath: A Branch Exploration Framework for Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization}, a transformative approach that redefines how visualization code is generated. Traditional single-pass methods may seem efficient, but they often fall short of delivering the depth and precision users truly need. They generate code quickly, but struggle to capture the intricate details that make a visualization not just functional but meaningful. \emph{VisPath} challenges this limitation by incorporating \emph{Multi-Path Reasoning} and \emph{Feedback-Driven Optimization}, systematically exploring multiple interpretative pathways to construct a more accurate, context-aware, and fully executable visualization. 

Rather than simply translating user input into code, \emph{VisPath} ensures that every critical aspect—both explicitly stated and implicitly necessary—is carefully considered, creating a visualization that is not only correct but insightful. At its core, it generates multiple reasoning paths that analyze the user's intent from different perspectives, producing structured blueprints that are then transformed into visualization scripts through Chain-of-Thought (CoT) prompting. These multiple candidates are evaluated using a Vision-Language Model (VLM) to assess accuracy, clarity, and alignment with the intended message. The results are then refined through an Aggregation Module, optimizing the final output for both reliability and impact. By shifting from a single-pass, one-size-fits-all approach to a dynamic, multi-layered process, \emph{VisPath} sets a new standard for visualization generation—one that is not just about creating code, but about ensuring data is represented in its most clear, meaningful, and compelling form.


Extensive experiments on benchmark datasets, including \emph{MatPlotBench}~\cite{yang2024matplotagent} and \emph{Qwen-Agent Code Interpreter} Benchmark\footnote{\url{https://github.com/QwenLM/Qwen-Agent/blob/main/benchmark/code_interpreter/README.md}}, demonstrate \emph{VisPath}’s superiority over the state-of-the-art (SOTA) visualization generation methods. By systematically generating and evaluating multiple reasoning paths and leveraging iterative feedback aggregation, \emph{VisPath} significantly enhances the accuracy, robustness against underspecified queries, and adaptability to diverse user intents.
Our investigations demonstrate its ability to capture nuanced user intents, improve execution reliability, and minimize errors, making visualization code generation more accessible and effective for domains such as business intelligence, scientific research, and automated reporting.


\section{Related Work}
Numerous methods have been applied for Text-to-Visualization (Text2Vis) generation, which has significantly evolved over the years, adapting to new paradigms in data visualization and natural language processing~\cite{dibia2019data2vis,wu2022nuwa,chen2022type,chen2022nl2interface,rashid2022text2chart,zhang2024chartifytext}. Early approaches such as Voyager~\cite{wongsuphasawat2015voyager} and Eviza~\cite{setlur2016eviza} largely relied on rule-based systems, which mapped textual commands to predefined chart templates or specifications through handcrafted heuristics~\cite{de2020vismaker}. While these methods demonstrated the feasibility of automatically converting text into visualizations~\cite{moritz2018formalizing,cui2019text}, they often required extensive domain knowledge and struggled with more nuanced or ambiguous user requirements~\cite{li2021kg4vis,wang2023llm4vis}. Inspired by developments in deep learning, researchers began to incorporate neural networks to handle free-form natural language and broaden the range of supported visualization types~\cite{liu2021advisor,luo2021natural}. 


\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=1\textwidth]{final_framework.png}
    \caption{\textbf{Overview of the proposed \emph{VisPath} framework for creating robust visualization code generation.} The framework consists of combination of Multi-Path Agent, Visual Feedback Agent, and Synthesis Agent.}
    \label{fig:vispath-main}
\end{figure*}

% this part should be enhanced
Building on these machine learning strategies, numerous studies have utilized Large Language Models (LLMs) to further enhance system flexibility. Recent frameworks such as Chat2VIS~\cite{maddigan2023Chat2VIS} and Prompt4Vis~\cite{liPrompt4VisPromptingLarge2024} utilize few-shot learning or query expansion to refine user queries, subsequently generating Python visualization scripts through instruction-based prompting. More recent approaches, such as MatPlotAgent~\cite{yang2024matplotagent} and PlotGen~\cite{goswamiPlotGenMultiAgentLLMbased2025}, extend these frameworks by integrating a vision-language feedback model to iteratively optimize the final code based on evaluations of the rendered visualizations. The aforementioned approaches often struggle to effectively capture user intent in complex visualization tasks. By committing to a single reasoning trajectory, they may produce code that is syntactically correct yet semantically misaligned with user expectations, requiring extensive manual adjustments. This challenge is particularly pronounced when user input is ambiguous or underspecified, leading to an iterative cycle of prompt refinement and code modification—ironically undermining the intended efficiency of automation. To address these limitations, we introduce \emph{VisPath}, a novel framework that integrates multi-path reasoning with feedback from Vision-Language Models (VLMs) to enhance visualization code generation. Compared to conventional methods constrained by a single reasoning path, \emph{VisPath} dynamically explores multiple solution pathways, improving interpretability, minimizing manual intervention, and adapting more effectively to diverse visualization requirements.

\section{Methodology}
We introduce \emph{VisPath}, a framework for robust visualization code generation that leverages diverse reasoning and visual feedback. VisPath is built on three core components: (1) \emph{Multi-Path Query Expansion}, which generates multiple reasoning paths informed by the dataset description; (2) \emph{Code Generation from Expanded Queries}, which synthesizes candidate visualization scripts via Chain-of-Thought (CoT) prompting while grounding them in the actual data context; and (3) \emph{Feedback-Driven Code Optimization}, where a Vision-Language Model (VLM) evaluates and refines the outputs to ensure generation robust visualization code. An overview of this process is shown in Figure~\ref{fig:vispath-main}.

\subsection{Multi-Path Generation}
One of the biggest pitfalls in visualization code generation is rigid interpretation, a single query can have multiple valid visual representations depending on its dataset structure. \emph{VisPath} mitigates this by generating multiple extended queries within a single interaction. Given a user query \( Q \) and a corresponding dataset description \( D \), a \emph{Multi-Path Agent} is employed to expand the query into \( K \) distinct reasoning pathways:
\begin{equation}\label{eq:paths}
\{R_1, R_2, \dots, R_K\} = f_{\text{mpa}}(Q, D),
\end{equation}
where \( f_{\text{mpa}} \) denotes the function of the Multi-Path Agent implemented via an LLM. The dataset description \( D \) plays a crucial core in shaping these reasoning paths by providing contextual information about variable types, inherent relationships, and the suitability of different chart types for a more grounded interpretation of the query. Each \( R_i \) serves as a detailed logical blueprint outlining one possible approach to fulfill the visualization request. This design ensures that our framework effectively considers a broad range of potential interpretations, thereby increasing the quality of reasoning and the likelihood of capturing the true user intent—even when queries are ambiguous or underspecified.

\subsection{Code Generation from Reasoning Paths}
Once diverse reasoning paths are established, the next stage involves translating each path into executable Python scripts. For every reasoning path \( R_i \) generated in Equation~\eqref{eq:paths}, a dedicated Code Generation LLM produces the corresponding visualization code using chain-of-thought (CoT) prompting:
\begin{equation}\label{eq:code}
C_i = f_{\text{code}}(D, R_i),
\end{equation}
where \( f_{\text{code}} \) represents the code generation function. Unlike naive code generation approaches, here, the dataset description \( D \) is explicitly provided to ground the generated code in the actual data context, ensuring that variable names, data types, and visualization parameters align correctly with the underlying data attributes. The generated code \( C_i \) is then executed to render a candidate visualization:
\begin{equation}\label{eq:image}
V_i = f_{\text{exec}}(C_i), \quad i = 1, 2, \dots, K,
\end{equation}
with \( f_{\text{exec}} \) serving as the code execution function. By executing the code directly, we ensure that the visualization accurately reflects the intended operations without reintroducing the dataset description \( D \) at this stage. In practice, some generated codes may not be executable. Rather than engaging in an explicit debugging loop, we record the execution status as a binary executability indicator:
\begin{equation}\label{eq:executable}
\epsilon_i = \begin{cases} 
1, & \text{if } C_i \text{ is executable} \\
0, & \text{otherwise}.
\end{cases}
\end{equation}
To route the outputs appropriately, we introduce:
\begin{equation}\label{eq:z-output}
Z_i = \begin{cases} 
\text{plot image}(V_i), & \text{if } \epsilon_i = 1, \\
\text{error message from } C_i, & \text{if } \epsilon_i = 0.
\end{cases}
\end{equation}
The result \( Z_i \) (either the rendered visualization or the error message) is then provided, along with \( C_i \) and the original query \( Q \), to the feedback model in the subsequent stage.

\subsection{Feedback-Driven Code Optimization}
Most code generation frameworks focus merely on syntactically correct scripts, but our framework goes further. As final stage, \emph{VisPath} synthesizes the most robust and accurate visualization code by leveraging both the executability information and structured visual feedback. A Vision-Language Model (VLM) is employed to analyze each candidate by evaluating the initial query \( Q \), the generated code \( C_i \), and the routed output \( Z_i \). This evaluation is formalized as:
\begin{equation}\label{eq:feedback}
F_i = f_{\text{feedback}}(Q, C_i, Z_i),
\end{equation}
where \( F_i \) provides structured feedback on key aspects such as chart layout, the alignment between the intended request and the rendered visualization (or error context), and visual readability (including potential improvements). To capture the complete quality signal from each candidate, we pair the feedback with its corresponding generated code:
\begin{equation}\label{eq:agg-feedback}
S_i = (C_i, F_i), \quad i = 1, 2, \dots, K.
\end{equation}
Leveraging the collective code-feedback pairs along with the original query \( Q \) and the dataset description \( D \), an \emph{Integration Module} synthesizes the final, refined visualization code:
\begin{equation}\label{eq:final-code}
C^{*} = f_{\text{integrate}}\left(Q, D, \{S_i\}_{i=1}^{K}\right),
\end{equation}
where \( C^{*} \) represents the optimized visualization code and \( f_{\text{integrate}} \) denotes the function that aggregates the strengths of each candidate code alongside its corresponding feedback. This formulation ensures that the final code is not only constructed based on the insights extracted from the candidate outputs but is also meticulously aligned with the original user query and the provided dataset description. The process harnesses the reasoning capabilities of LLM to systematically evaluate the strengths and pinpoint potential weaknesses across all candidate solutions, including those that may initially be non-executable. By synthesizing the most optimal elements from each candidate, the framework generates a final code that best captures the user’s intent while maintaining high standards of reliability, robustness, and execution quality. A step-by-step breakdown of this process is detailed in Algorithm~\ref{algo:vispath} below:



\begin{algorithm}
\label{algo:vispath}
\small
\caption{\emph{Algorithm for VisPath}}\label{algo:vispath}
\begin{algorithmic}[1]
\Require User query \( Q \), dataset \( D \), number of reasoning paths \( K \), Multi-Path Agent \( f_{\text{mpa}} \), Code Generation LLM \( f_{\text{code}} \), Code Execution Function \( f_{\text{exec}} \), Feedback Model \( f_{\text{feedback}} \), Integration Module \( f_{\text{integrate}} \)
\Ensure Final visualization code \( C^{*} \)

\State \textbf{// Step 1: Multi-Path Query Expansion}
\State \(\{R_1, R_2, \dots, R_K\} \gets f_{\text{mpa}}(Q, D)\)
\Comment Generate \( K \) distinct reasoning paths; each \( R_i \) outlines a potential interpretation of \( Q \) given \( D \).

\State \textbf{// Step 2: Code Generation from Reasoning Paths}
\For{\( i = 1 \) to \( K \)}
    \State \( C_i \gets f_{\text{code}}(D, R_i) \)
    \Comment Generate candidate visualization code \( C_i \) based on reasoning path \( R_i \).
    \State \( V_i \gets f_{\text{exec}}(C_i) \)
    \Comment Execute \( C_i \) to obtain candidate visualization \( V_i \).
    \State \( \epsilon_i \gets \begin{cases} 1, & \text{if } C_i \text{ executes successfully} \\ 0, & \text{otherwise} \end{cases} \)
    \Comment Record executability indicator \( \epsilon_i \) for \( C_i \).
    \State \( Z_i \gets \begin{cases} \text{plot image}(V_i), & \text{if } \epsilon_i = 1 \\ \text{error message from } C_i, & \text{if } \epsilon_i = 0 \end{cases} \)
    \Comment Route output: either the rendered visualization or the error message.
\EndFor

\State \textbf{// Step 3: Feedback-Driven Code Optimization}
\For{\( i = 1 \) to \( K \)}
    \State \( F_i \gets f_{\text{feedback}}(Q, C_i, Z_i) \)
    \Comment Obtain structured feedback \( F_i \) on candidate code \( C_i \) using the routed output \( Z_i \).
    \State \( S_i \gets (C_i, F_i) \)
    \Comment Form a unified tuple \( S_i \) combining \( C_i \), \( F_i \).
\EndFor

\State \( C^{*} \gets f_{\text{integrate}}\bigl(Q, D, \{S_1, S_2, \dots, S_K\}\bigr) \)
\Comment Aggregate the code-feedback pairs along with \( Q \) and \( D \) to synthesize the final visualization code \( C^{*} \).

\State \Return \( C^{*} \)
\end{algorithmic}
\end{algorithm}
    

\section{Experiments}

\begin{table*}[t]
\renewcommand{\arraystretch}{1.2}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c l c c c c c@{}}
\toprule[1pt]
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{MatPlotBench}} & \multicolumn{3}{c}{\textbf{Qwen-Agent Code Interpreter benchmark}} \\
\cmidrule(l){3-4} \cmidrule(l){5-7}
& & \textbf{Plot Score} & \textbf{Executable Rate} (\%) & \textbf{Visualization-Hard} & \textbf{Visualization-Easy} & \textbf{Avg.} \\
\midrule

%=========================
% gpt-4o-mini
%=========================
\multirow{5}{*}{\centering \textbf{GPT-4o mini}}
& Zero-Shot & 62.38  & 53 & 59.68 & 45.50  & 52.59 \\
& CoT Prompting~\cite{wei2022chain} & 61.95 & 50 & 57.50 & 40.00  & 48.75 \\
& Chat2VIS~\cite{maddigan2023Chat2VIS} & 56.98 & 53 & 59.36 & 36.50 & 47.93 \\
& MatPlotAgent~\cite{yang2024matplotagent} & \underline{63.90} & \underline{58} & \underline{67.50} & \underline{53.25} & \underline{60.38} \\
& VisPath\textsuperscript{\textdagger} (Ours) & \textbf{66.12} & \textbf{60} & \textbf{70.68} & \textbf{57.23} & \textbf{63.96} \\
\midrule

%=========================
% Gemini 2.0 flash
%=========================
\multirow{5}{*}{\centering \textbf{Gemini 2.0 Flash}}
& Zero-Shot & 55.00 & 54 & 68.97 & 52.18 & 60.58 \\
& CoT Prompting~\cite{wei2022chain} & 53.56  & \underline{61} & 40.00 & \textbf{63.89} & 51.95 \\
& Chat2VIS~\cite{maddigan2023Chat2VIS} & 54.89 & 55 & 59.36 & 56.50 & 57.93 \\
& MatPlotAgent~\cite{yang2024matplotagent} & \underline{56.31} & 58 & \underline{77.62} & 51.50 & \underline{64.56} \\
& VisPath\textsuperscript{\textdagger} (Ours) & \textbf{59.37} & \textbf{63} & \textbf{80.79} & \underline{57.17} & \textbf{68.98} \\
\bottomrule[1pt]
\end{tabular}%
}% end resizebox
\caption{\textbf{Performance comparison of various methods across different benchmarks.} 
Zero-Shot refers to directly generating code. 
CoT Prompting utilizes Chain of Thought Prompting. 
Visualization-Hard and Visualization-Easy refer to the Accuracy of Code Execution Results on different subsets of the Qwen-Agent Code Interpreter benchmark. 
    \textbf{Bold text} indicates the best performance, 
    \underline{underlined text} indicates the second-best performance.
    \textsuperscript{\textdagger} denotes our proposed method.}
    \label{tab:performance-comparison}
    \end{table*}
    
\subsection{Setup}
In this section, we detail our experimental configuration, including experimental datasets, model specifications, and baseline methods for evaluating the performance of the proposed \textit{VisPath} framework in generating visualization code from natural language queries.

\subsubsection{Experimental Datasets}

We evaluate our approach on two Text-to-Visualization benchmarks: \textit{MatPlotBench}~\cite{yang2024matplotagent} and the \textit{Qwen-Agent Code Interpreter Benchmark}. \textit{MatPlotBench} comprises 100 items with ground truth images; we focus on its simple instruction subset for nuanced queries. In contrast, the \textit{Qwen-Agent Code Interpreter Benchmark} includes 295 records: 163 related to visualization, and evaluates Python code interpreters on tasks such as mathematical problem solving, data visualization, and file handling based on Code Executability and Code Correctness.

\subsubsection{Models Used}
\paragraph{Large Language Models (LLMs):} For the code inference stage, we experiment with \textit{GPT-4o mini}~\cite{achiam2023gpt} and \textit{Gemini 2.0 Flash}~\cite{team2024gemini} to generate candidate visualization code from the reasoning paths. Both models are configured with a temperature of 0.2 to ensure precise and focused outputs, in line with recommendations from previous work~\cite{yang2024matplotagent}. To evaluate the generated code quality and guide the subsequent optimization process, we utilize \textit{GPT-4o}~\cite{achiam2023gpt} and \textit{Gemini 2.0 Flash}~\cite{team2024gemini} as our visualization feedback model, which provides high-quality reference assessments.

\paragraph{Vision Language Models (VLMs):} In order to assess the visual quality and correctness of the rendered plots, we incorporate vision evaluation models into our framework. Specifically, \textit{GPT-4o}~\cite{achiam2023gpt} is employed for detailed plot evaluation in all evaluation tasks. This setup ensures the thorough evaluation of both the syntactic correctness of the code and the aesthetic quality of the resulting visualizations.

\subsubsection{Evaluation Metrics}
In our experiments, we utilized evaluation metrics introduced by previous work to ensure consistency and comparability. \textit{MatPlotBench} assesses graph generation models using two key metrics: \textit{Plot Score}, which measures similarity to the Ground Truth (0–100), and \textit{Executable Score}, which represents the percentage of error-free code executions. \emph{Qwen-Agent Code Interpreter benchmark} evaluates visualization models based on \textit{Visualization-Hard} and \textit{Visualization-Easy}, measuring how well generated images align with queries of different difficulty levels. Compared to \textit{MatPlotBench}, \emph{Qwen-Agent Code Interpreter benchmark} assesses image alignment via a code correctness metric. Previous studies show GPT-based VLM evaluations align with human assessments, hence VLM was used for evaluation.


\subsubsection{Baseline Methods}
We compare \textit{VisPath} against competitive baselines. Specifically, (1) \textit{Zero-Shot} directly generates visualization code without intermediate reasoning, (2) \textit{CoT Prompting} uses chain-of-thought prompting to articulate its reasoning, while (3) \textit{Chat2VIS}~\cite{maddigan2023Chat2VIS} employs guiding prefixes to mitigate ambiguity, and (4) \textit{MatPlotAgent}~\cite{yang2024matplotagent} first expands the query and then refines the code via a self-debugging loop with feedback. For a fair comparison, \textit{MatPlotAgent} is limited to three iterations, and uses critique-based debugging loop as well, and \textit{VisPath} generates three reasoning paths with corresponding visual feedback to refine the final output.\footnote{Prompts are detailed in Appendix A.}


\subsection{Experimental Analysis}

In this section, we present a detailed evaluation of our proposed \emph{VisPath} framework, summarizing the performance results against four baselines: (1) \emph{Zero-Shot}, (2) \emph{Chain-of-Thought (CoT) Prompting}, (3) \emph{Chat2VIS}, and (4) \emph{MatPlotAgent}.

From Table ~\ref{tab:performance-comparison},  we find that the \emph{Zero-Shot} method, which directly generates visualization code from the user query without intermediate reasoning, suffers from ambiguous interpretations and error-prone outputs. These issues are particularly pronounced when the input queries are underspecified, contain implicit assumptions, or require complex reasoning steps. \emph{CoT Prompting}, on the other hand, mitigates some of these challenges by employing chain-of-thought (CoT) reasoning, which helps the model articulate its decision-making process step by step. While this structured reasoning approach improves interpretability and correctness, its reliance on a single reasoning trajectory limits its capacity to explore alternative solutions.

Meanwhile, \emph{Chat2VIS} builds upon \emph{CoT Prompting} by incorporating guided prefixes to better structure user queries and disambiguate input intent. This enhancement leads to more coherent code generation and reduces errors stemming from unclear specifications. However, its effectiveness still depends on the predefined guiding templates, which may not fully adapt to highly variable or novel query structures. \emph{MatPlotAgent} further refines this approach through query expansion and an iterative self-debugging loop that enhances code robustness.  However, its reliance on an iterative correction mechanism comes at the cost of computational efficiency and does not fully leverage diverse reasoning strategies that could lead to more creative and contextually accurate solutions.

In contrast, our novel framework, \emph{VisPath}, overcomes these challenges by dynamically generating multiple reasoning paths. By exploring diverse interpretations of user intent simultaneously, \emph{VisPath} significantly enhances the robustness of the generated visualization code. Furthermore, it integrates structured feedback from Vision-Language Models (VLMs) to refine its outputs, ensuring both higher execution accuracy and improved aesthetic quality. This feedback-driven optimization resolves ambiguities more effectively and selects the most appropriate solution from a wide range of possibilities. Evaluations conducted on benchmark datasets such as \emph{MatPlotBench} and \emph{Qwen-Agent Code Interpreter benchmark} demonstrate that \emph{VisPath} consistently outperforms the baseline methods by average 17\%. \emph{VisPath} exhibits a superior ability to handle complex and ambiguous visualization requests by synthesizing multiple reasoning trajectories and incorporating structured feedback, making it a highly effective tool for automated visualization generation.


\subsection{Ablation Study}

This section further explores the impact of varying the number of reasoning paths on the performance of visualization code generation. We conduct a series of ablation studies to  demonstrate the robustness of our \emph{VisPath} framework under alternative settings. Specifically, we investigate (i) the effect of varying the number of generated reasoning paths and (ii) the impact of a simplified integration strategy for synthesizing the final visualization code. 




\subsubsection{Varying the Number of Reasoning Paths}
To further assess the contribution of reasoning path diversity, we conducted ablation experiments varying the number of generated paths \(K \in \{2, 3, 4\}\)  to evaluate their effects on execution accuracy, visualization quality and interpretability. Evaluation has been conducted on both the \emph{MatPlotBench} and \emph{Qwen-Agent Code Interpreter benchmark} datasets. The study reveals an interesting pattern observed when the number of reasoning path is reduced. As illustrated in Table~\ref{tab:ablation-study1-matplotbench} and Table~\ref{tab:ablation-study1-qwen-agent},  when \(K=2\), the system exhibited  limited diversity in which occasionally results in missing nuanced interpretations of complex user queries. 

\begin{table}[t]
% \renewcommand{\arraystretch}{1.2}
\centering
\small
\begin{tabular}{@{}c c c c@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{K}} & \multicolumn{2}{c}{\textbf{MatPlotBench}} \\
\cmidrule(l){3-4}
 & & \textbf{Plot Score} & \textbf{Executable Rate} (\%)\\
\midrule
\multirow{3}{*}{\textbf{GPT-4o mini}} & 2 & 64.02  & 58 \\
 & \textbf{3} & \textbf{66.12}  & \textbf{60} \\
 & 4 & 64.68  & 62 \\
\midrule
\multirow{3}{*}{\textbf{Gemini 2.0 Flash}} & 2 & 56.59  & 56 \\
 & \textbf{3} & \textbf{59.37}  & \textbf{61} \\
 & 4 & 54.60  & 54 \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of ablation study on MatPlotBench.}}
\label{tab:ablation-study1-matplotbench}
\end{table}



\begin{table}[t]
% \renewcommand{\arraystretch}{1.2}
\centering
\small
\begin{tabular}{@{}c c c c c@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{K}} & \multicolumn{3}{c}{\textbf{Qwen-Agent}} \\
\cmidrule(l){3-5}
 & & \textbf{Hard} & \textbf{Easy} & \textbf{Avg.} \\
\midrule
\multirow{3}{*}{\textbf{GPT-4o mini}} & 2 & 67.50 & 55.72 & 61.61 \\
 & \textbf{3} & \textbf{70.68} & \textbf{57.23} & \textbf{63.96} \\
 & 4 & 66.42 & 50.25 & 58.34 \\
\midrule
\multirow{3}{*}{\textbf{Gemini 2.0 Flash}} & 2 & 74.21 & 57.50 & 65.86 \\
 & \textbf{3} & \textbf{80.79} & \textbf{57.17} & \textbf{68.98} \\
 & 4 & 76.41 & 54.50 & 65.46 \\
\bottomrule
\end{tabular}
\caption{\textbf{Results of ablation study on Qwen-Agent Code Interpreter benchmark.}}
\label{tab:ablation-study1-qwen-agent}
\end{table}

Conversely, increasing \(K\) to 4 introduces extra paths which can sometimes add noise as less relevant interpretations are considered. Specifically, we observed cases where redundant or overly complex visualization components were generated, hence making the final output harder to be interpreted. While the additional reasoning paths increased the overall exploration space, they also required more extensive filtering and selection, leading to suboptimal execution efficiency. 

Notably,  \(K=3\) consistently achieves the best balance by providing sufficient diversity without introducing excessive noise. With too few reasoning paths, some nuanced aspects of ambiguous queries are lost, while too many paths configuration result in unnecessary exploration of low relevancy solutions. The number of reasoning path diversity set as \(K=3\)  optimally provides sufficient diversity without overcomplicating the reasoning process. 
\subsubsection{Robustness with a Simple Integration Strategy}
To further validate the robustness of \emph{VisPath}, we also evaluate an alternative integration strategy that simplicies the aggregation of multiple reasoning paths. In contrast to our full feedback-driven iterative approach which refines visualization code through multiple rounds of vision-language feedback, this streamlined method directly aggregates candidate outputs from different reasoning trajectories without intermediate corrections. 

Specifically, in this alternative strategy, three candidate codes, each derived from different reasoning paths, are directly combined to generate the final visualization output. This methodology reduces computational overhead and processing time while maintaining a degree of interpretability and correctness. Through examining the impact of this approach, we gain insight into the extent to which \emph{VisPath's} core strength stems from its multi-path reasoning capability versus its iterative refinement process.

\begin{table}[htbp]
    \renewcommand{\arraystretch}{1}
    \centering
    \resizebox{\columnwidth}{!}{%
      \begin{tabular}{llccc}
        \toprule
        \textbf{Model} & \textbf{Feedback} & \multicolumn{2}{c}{\textbf{MatPlotBench}} & \textbf{Qwen-Agent} \\
        \cmidrule(lr){3-4} \cmidrule(lr){5-5}
         &  & \textbf{Plot Score} & \textbf{Executable Rate (\%)} & \textbf{Avg.} \\
        \midrule
        \multirow{2}{*}{\textbf{GPT-4o mini}}
            & (w/o) feedback & 63.76 & 56 & 58.00 \\
            & \textbf{(w) feedback}   & \textbf{66.12} & \textbf{60} & \textbf{63.96} \\
        \midrule
        \multirow{2}{*}{\textbf{Gemini 2.0 Flash}} 
            & (w/o) feedback & 55.28 & 57 & 64.03 \\
            & \textbf{(w) feedback}   & \textbf{59.37} & \textbf{63} & \textbf{68.98} \\
        \bottomrule
    \end{tabular}%
    }
    \caption{\textbf{Performance comparison of VisPath with and without visual feedback}. The MatPlotBench scores (Plot Score and Executable Rate) and the average score from the Qwen-Agent Code Interpreter benchmark are shown for two llm models.}
    \label{tab:feedback-ablation}
\end{table}


As shown in Table~\ref{tab:feedback-ablation}, even under this simplified integration, the performance of \emph{VisPath} significantly outperforms all baseline methods, demonstrating its robustness and adaptability. This result suggests that the diversity of reasoning paths alone ensures a resilient and effective outcome, even in the absence of extensive intermediate corrections.  This finding shows that the strength of our framework largely stems from its multi-path reasoning design. Even when the integration is simplified, the diversity of reasoning paths ensures that the aggregated output remains robust and effective, thus showing the overall adaptability and effectiveness of our approach.


\section{Discussion}
Our main experiment and ablation studies decisively demonstrate that \emph{VisPath's} multi-path exploration revolutionizes visualization code generation by making it not just robust, but truly interpretable.~\footnote{Case study is provided in Appendix B.} Unlike traditional methods that often yield simplistic, one-dimensional results, \emph{VisPath} embraces complexity—delving deeper to capture nuances of user intent that would otherwise be lost. By integrating multiple perspectives during reasoning, our framework ensures that final visualizations are not only data-accurate but also rich in context, inherently intuitive, and unmistakably aligned with the user’s true objectives.

The real breakthrough with \emph{VisPath} is how it transforms visualization into a structured yet fluid process. Traditional methods often act like static translators, converting user queries into plots while neglecting the vital auxiliary components—legends, labels, line styles, and other elements—that enable true interpretability. They assume that a single pass can sufficiently capture the essence of a visualization, but this leads to visualizations that might technically be correct but fail at communication. In contrast, \emph{VisPath}’s multi-path approach systematically explores different ways to represent data, integrating feedback at each step. This ensures that even subtle yet critical design elements are not just included, but optimized, making the resulting visualizations feel complete, polished, and immediately comprehensible.

Beyond accuracy, \emph{VisPath} introduces a level of adaptability that is indispensable in today’s fast-evolving data landscape. It’s not just a tool for static reports—it is built for real-time streaming data, dynamic dashboards, and interactive analytics. Even with broad or loosely defined prompts, our framework intelligently synthesizes high-quality visualizations, closing the gap between intent and execution. As industries continue to demand clearer, faster, and more adaptable insights, VisPath is poised to lead the way, setting a new standard for intelligent, context-aware data visualization.



\section{Conclusion}
In this work, we present \emph{VisPath}, a groundbreaking framework that redefines the landscape of automated visualization code generation over existing methods. Unlike prior methods, our approach seaminglessly  combines multi-path reasoning with feedback-driven optimization. Accurately capturing diverse user intents and refine generated code, \emph{VisPath} achieves notable improvements in both execution success and visual quality on challenging benchmarks such as \emph{MatPlotBench} and the \emph{Qwen-Agent Code Interpreter Benchmark}. By prioritizing adapability and user intent alignment, \emph{VisPath} is uniquely positioned to handle compledities of real-world data visualization tasks. Looking ahead, future work could explore \emph{VisPath}’s adaptability in more dynamic, real-world visualization scenarios, further broadening its scope and enhancing its practical utility in complex data analysis contexts.

\section{Limitations}
Despite its effectiveness, the current iteration of our framework relies on a limited feedback mechanism, evaluating visualizations primarily based on two aspects: query-code alignment and query-plot image alignment. While these aspects provide valuable insights, they may not fully capture the granular elements that contribute to overall interpretability.  To enhance the depth of feedback itself, future research could focus on decomposition of assessment process, assessing individual plot components separately. This granular analysis would render for a more nuanced assessment of readability, element appropriateness and visual coherence, ultimately leading to more refined visualization code generation. Strengthening the feedback mechanism in such way will be crucial for maximizing \emph{VisPath's} effectiveness in diverse and complex visualization scenarios. 


% \newpage
% \newpage

\bibliography{custom}
\newpage
\appendix


\section{Appendix A. Prompts Used}
\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=Prompt for \emph{Multi-Path Reasoning}, halign=flush left]

[\textbf{System Prompt}] According to the user query, expand and solidify the query into detailed instruction on how to write python code to fulfill the user query's requirements. Import the appropriate libraries. Pinpoint the correct library functions to call and set each parameter in every function call accordingly.

[\textbf{User Prompt}] Think step by step. Generate three distinct extended queries based on the given query.
Ensure that you first analyze the given data description and create queries that align with the data.  If no data description is provided, follow the original query as is. You must follow the Python list format for the output. Do not modify the detailed instructions from the original user query.
Original query: {ori_query}
Data description: {data_description}
\\Output format: [extended_path_1, extended_path_2, extended_path_3]
        
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=Prompt for \emph{Code Generation}, halign=flush left]

[\textbf{System Prompt}] You are an expert in data visualization code generation. Think step by step and write the generated code in the format ```python...```, where "..." represents the generated code. The code must end with `plt.show()`.

[\textbf{User Prompt}] Think step by step. Based on the user's query and the provided data description, generate Python code using `matplotlib.pyplot` and `seaborn` to create the requested plot. Ensure that the code is formatted as ```...```, where "..." represents the generated code. \\
User query: \{query\}\\
Data description: \{data_description\}
\end{tcolorbox}


\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=Prompt for \emph{Visual Feedback}, halign=flush left]

[\textbf{System Prompt}] Given a code, a user query, and an image of the current plot, please determine whether the plot accurately follows the user query. Provide detailed instructions on how to enhance the plot using Python code.\\

[\textbf{User Prompt}] Carefully analyze the provided Python code, the user query, and the plot image (if available) to assess whether the generated plot meets the user query requirements. If the plot image is missing, check the error message that occurred in the code. Compare the plot with the user query, highlight discrepancies, and provide clear, actionable instructions to modify the code. Additionally, suggest improvements for better visualization, focusing on clarity, readability, and alignment with the user's objectives.\\
Code: \{code\}\\
User query: \{ori_query\}
\end{tcolorbox}


\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=Prompt for \emph{Synthesis}, halign=flush left]
[\textbf{System Prompt}] You are an expert on data visualization code judgement and aggregation. 

[\textbf{User Prompt}] Think step by step. Given the provided user query, data description, multiple data visualization codes generated for the query, and feedback for each code's generated image. Your task is to:\\
1. Carefully review the user query and the data description.\\
2. Examine each version of the data visualization code along with the feedback provided for each version.\\
3. Synthesize the feedbacks for each code, user query insights, data description to create a final version of the code.\\
4. Your goal is to produce a final version of code that more effectively fulfills the user query by integrating the best elements from all versions and applying necessary corrections.\\
User Query: \{ori_query\}\\
Data Description: \{data_description\}\\
Code for aggregation with corresponding feedback: \{code_for_aggregation\}
\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=Prompt for Evaluation: \emph{MatplotBench}, halign=flush left]
You are an excellent judge at evaluating visualization plots between a model generated plot and the ground truth. You will be giving scores on how well it matches the ground truth plot.\\
The generated plot will be given to you as the first figure. If the first figure is blank, that means the code failed to generate a figure.
Another plot will be given to you as the second figure, which is the desired outcome of the user query, meaning it is the ground truth for you to reference.
Please compare the two figures head to head and rate them.
Suppose the second figure has a score of 100, rate the first figure on a scale from 0 to 100.
Scoring should be carried out in the following aspect:\\
1. Plot correctness: \\
Compare closely between the generated plot and the ground truth, the more resemblance the generated plot has compared to the ground truth, the higher the score. The score should be proportionate to the resemblance between the two plots.
In some rare occurrence, see if the data points are generated randomly according to the query, if so, the generated plot may not perfectly match the ground truth, but it is correct nonetheless.\\
Only rate the first figure, the second figure is only for reference.\\
If the first figure is blank, that means the code failed to generate a figure. \\Give a score of 0 on the Plot correctness.\\
After scoring from the above aspect, please give a final score. The final score is preceded by the [FINAL SCORE] token.\\
For example [FINAL SCORE]: 40.\\

\end{tcolorbox}

\begin{tcolorbox}[colback=lightgray!20,colframe=darkgray!80,title=Prompt for Evaluation: \emph{Qwen-Agent Code Interpreter benchmark}, halign=flush left]
Please judge whether the image is consistent with the [Question] below, if it is consistent then reply "right", if not then reply "wrong".\\
Question: \{query\} \\
\end{tcolorbox}

\newpage
\section{Appendix B. Case Study}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{\textbf{Overview of  end-to-end process of visualizing women's million-dollar earnings data using \emph{VisPath}}. It includes the user query, raw data, multi-path reasoning, generated figures, visual feedback, final script, and the completed visualization.}
    \label{fig:enter-label}
\end{figure*}


\end{document}
