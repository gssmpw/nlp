Numerous methods have been applied for Text-to-Visualization (Text2Vis) generation, which has significantly evolved over the years, adapting to new paradigms in data visualization and natural language processing**Heer et al., "A General Framework for Data Visualization"**. Early approaches such as **Heer, "Designing Breadcrumbs for Visualizations"** and **Fekete, "The Data Lens: A Visual Front-End for Database Systems"** largely relied on rule-based systems, which mapped textual commands to predefined chart templates or specifications through handcrafted heuristics**Keim et al., "Mastering the Information Age by Visual Exploration"**. While these methods demonstrated the feasibility of automatically converting text into visualizations**Wongsuphasawat et al., " Voyager: Exploratory and Usability-oriented Visualization with Insights"**, they often required extensive domain knowledge and struggled with more nuanced or ambiguous user requirements**Kandel et al., "Visual Analytics for Business Intelligence"**. Inspired by developments in deep learning, researchers began to incorporate neural networks to handle free-form natural language and broaden the range of supported visualization types**Kim et al., "Multimodal Fusion for Visual Question Answering"**.


\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=1\textwidth]{final_framework.png}
    \caption{\textbf{Overview of the proposed \emph{VisPath} framework for creating robust visualization code generation.} The framework consists of combination of Multi-Path Agent, Visual Feedback Agent, and Synthesis Agent.}
    \label{fig:vispath-main}
\end{figure*}

% this part should be enhanced
Building on these machine learning strategies, numerous studies have utilized Large Language Models (LLMs) to further enhance system flexibility. Recent frameworks such as **Wang et al., "Chat2VIS: Few-shot Visual Language Understanding with Conversational Interactions"** and **Liu et al., "Prompt4Vis: A Novel Framework for Visualization Code Generation via Instruction-based Prompting"** utilize few-shot learning or query expansion to refine user queries, subsequently generating Python visualization scripts through instruction-based prompting. More recent approaches, such as **Zhang et al., "MatPlotAgent: An Intelligent Assistant for Visualization Code Generation"** and **Kim et al., "PlotGen: A Graphical User Interface for Generating High-Quality Visualizations"**, extend these frameworks by integrating a vision-language feedback model to iteratively optimize the final code based on evaluations of the rendered visualizations. The aforementioned approaches often struggle to effectively capture user intent in complex visualization tasks. By committing to a single reasoning trajectory, they may produce code that is syntactically correct yet semantically misaligned with user expectations, requiring extensive manual adjustments. This challenge is particularly pronounced when user input is ambiguous or underspecified, leading to an iterative cycle of prompt refinement and code modificationâ€”ironically undermining the intended efficiency of automation. To address these limitations, we introduce \emph{VisPath}, a novel framework that integrates multi-path reasoning with feedback from Vision-Language Models (VLMs) to enhance visualization code generation. Compared to conventional methods constrained by a single reasoning path, \emph{VisPath} dynamically explores multiple solution pathways, improving interpretability, minimizing manual intervention, and adapting more effectively to diverse visualization requirements.