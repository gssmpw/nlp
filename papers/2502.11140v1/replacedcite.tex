\section{Related Work}
Numerous methods have been applied for Text-to-Visualization (Text2Vis) generation, which has significantly evolved over the years, adapting to new paradigms in data visualization and natural language processing____. Early approaches such as Voyager____ and Eviza____ largely relied on rule-based systems, which mapped textual commands to predefined chart templates or specifications through handcrafted heuristics____. While these methods demonstrated the feasibility of automatically converting text into visualizations____, they often required extensive domain knowledge and struggled with more nuanced or ambiguous user requirements____. Inspired by developments in deep learning, researchers began to incorporate neural networks to handle free-form natural language and broaden the range of supported visualization types____. 


\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=1\textwidth]{final_framework.png}
    \caption{\textbf{Overview of the proposed \emph{VisPath} framework for creating robust visualization code generation.} The framework consists of combination of Multi-Path Agent, Visual Feedback Agent, and Synthesis Agent.}
    \label{fig:vispath-main}
\end{figure*}

% this part should be enhanced
Building on these machine learning strategies, numerous studies have utilized Large Language Models (LLMs) to further enhance system flexibility. Recent frameworks such as Chat2VIS____ and Prompt4Vis____ utilize few-shot learning or query expansion to refine user queries, subsequently generating Python visualization scripts through instruction-based prompting. More recent approaches, such as MatPlotAgent____ and PlotGen____, extend these frameworks by integrating a vision-language feedback model to iteratively optimize the final code based on evaluations of the rendered visualizations. The aforementioned approaches often struggle to effectively capture user intent in complex visualization tasks. By committing to a single reasoning trajectory, they may produce code that is syntactically correct yet semantically misaligned with user expectations, requiring extensive manual adjustments. This challenge is particularly pronounced when user input is ambiguous or underspecified, leading to an iterative cycle of prompt refinement and code modificationâ€”ironically undermining the intended efficiency of automation. To address these limitations, we introduce \emph{VisPath}, a novel framework that integrates multi-path reasoning with feedback from Vision-Language Models (VLMs) to enhance visualization code generation. Compared to conventional methods constrained by a single reasoning path, \emph{VisPath} dynamically explores multiple solution pathways, improving interpretability, minimizing manual intervention, and adapting more effectively to diverse visualization requirements.