%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}

\usepackage[a4paper]{geometry}
\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{caption}
\usepackage{authblk}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off 

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

\newcommand{\CM}[1]{\color{magenta} #1} %  Modifications Jeremie
\newcommand{\CB}[1]{\color{blue} #1} %  Modifications Jeremie
\newcommand{\CR}[1]{\color{red} #1} %  Commentaires Jeremie

\newcommand{\CO}[1]{\color{orange} #1} %  Modifications Elsa
\newcommand{\CV}[1]{\color{cyan} #1} %  Commentaires Elsa

\newcommand{\RL}[1]{\color{purple} #1} %  Commentaires Erel

\newcommand{\thefont}[2]{\fontsize{#1}{#2}\fontshape{n}\selectfont}
\newcommand{\1}{\rlap{\thefont{10pt}{12pt}1}\kern.16em\rlap{\thefont{11pt}{13.2pt}1}\kern.4em}
\newcommand{\smallO}[1]{\ensuremath{\mathop{}\mathopen{}o\mathopen{}\left(#1\right)}}
\newcommand{\bigO}[1]{\ensuremath{\mathop{}\mathopen{}O\mathopen{}\left(#1\right)}}


\DeclareMathOperator*{\argmin}{argmin}


\title{Scalable and consistent embedding of probability measures into Hilbert spaces via measure quantization}


\author[1]{Erell Gachon}
\author[2]{Elsa Cazelles}
\author[1]{J\'er\'emie Bigot}
\affil[1]{Institut de Math\'ematiques de Bordeaux, Universit\'e de Bordeaux, CNRS (UMR 5251)}
\affil[2]{CNRS, IRIT (UMR 5505), Universit\'e de Toulouse}


\begin{document}

\maketitle






\begin{abstract}
This paper is focused on statistical learning from data that come as probability measures. In this setting,  popular approaches consist in embedding such data into a Hilbert space with either \emph{Linearized Optimal Transport} or \emph{Kernel Mean Embedding}. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments.
\end{abstract}

\section{Introduction}

Machine Learning (ML) problems modeling data as a set of $N$ probability measures are classical in numerous applied fields such as signal and image processing, computer vision or computational biology, see the  surveys \cite{7974883,10740308,muandet2017kernel,khamis2024scalable}. % signal processing \cite{cazelles:hal-03115531, 7974883},  classification problems  \cite{NIPS2015_a9eb8122,9477020, pmlr-v70-arjovsky17a}, and image processing \cite{doi:10.1137/17M1140431}, 
In particular, this framework includes distribution regression \cite{pmlr-v31-poczos13a,JMLR:v17:14-510,bachoc17,10.1214/20-EJS1725,pmlr-v162-meunier22b} that consists in predicting a scalar or label response  from predictors that are  probability measures, and the problem of Principal Component Analysis (PCA) of a set of probability measures \cite{wang2013linear,seguy2015principal,bigot2017geodesic,cazelles2018geodesic} for the purpose of dimension reduction from so-called distributional data \cite{ariascastro2024}. Performing standard ML tasks on a set of probability measures is not straightforward as the algorithms are usually designed to handle $N$  points from an Euclidean space rather than $N$ distributions. However, as most of these methods rely on correlations through an inner-product, a popular approach is to embed such distributional data into a Hilbert space in which the whole machinery of machine learning methods can be easily applied. The two most commonly used embeddings are as follows: the first one is based on Linearized Optimal Transport (LOT) \cite{wang2013linear,delalande2023quantitative,moosmuller2023linear} and arises from  the field of \emph{Optimal Transport} (OT) \cite{santambrogio2015optimal, peyre2019computational}, by leveraging the Riemannian-like geometry of the space of probability measures endowed with the Wasserstein distance  \cite{ambrosio2008gradient}. The second one, known as \emph{Kernel Mean Embedding} (KME) \cite{muandet2017kernel}, relies on the use of kernel methods to map probability measures into a  \emph{Reproducing Kernel Hilbert Space} (RKHS).


However, the computational cost and the storing of such embeddings prevent their use in large-scale settings. This is often the case when observing $N$ empirical measures on point clouds $X^{(i)} =  (X^{(i)}_1,\cdots, X^{(i)}_{m_i})\in(\mathbb{R}^d)^{m_i}, 1\leq i\leq N$, with a large number $m_i$ of observations. Such datasets are frequently found in flow cytometry \cite{mckinnon2018flow}, where observations collected from $N$ patients represent a considerable amount of cells, each characterized by $d$ bio-markers. For these single-cell data, one usually encounters  point clouds of thousands to millions of events (that is $m_i\geq 10^5$) living in a  feature space of dimension $d$ larger than $10$. Using either the LOT embedding or the KME from such raw data becomes critical as these approaches suffer from high computational costs as soon as the number $m_i$ of points per clouds is larger than a few thousands.

Therefore, a relevant question is the following:  given a set of $N$  probability measures with large support size, how to efficiently compute an embedding into a Hilbert space that is statistically consistent with the embedding derived directly from the raw data ?

% {\CV [TO DO : Ajouter r\'ef\'erences de J\'er\'emie \cite{ beugnot2021improving}]}

% aboagyeetal2022quantized
% kusner2015word

 \subsection{Main contributions}

In this paper, we consider the problem of embbeding a set of $d$-dimensional input probability measures $(\mu^{(i)})_{i=1}^N$ into a Hilbert space at a low computational cost. To that end, we propose to employ a preliminary $K$-quantization step that is either based on optimal quantization of each input measure $\mu^{(i)}$ or on the quantization of the mean measure $\bar{\mu} = \frac{1}{N}\sum_{i=1}^n \mu^{(i)}$ as used in \cite{gachon2024low} for single-cell data analysis. The aim of this $K$-quantization step is to approximate $(\mu^{(i)})_{i=1}^N$ by discrete measures $(\nu_K^{(i)})_{i=1}^N$ with supports of size $K$,  with $K$ typically small.
%with smaller supports by considering respectively $N$ support spaces  $\{x_1^{(i)},\ldots,x_K^{(i)}\}\subset\mathbb{R}^d$ or a single support space $\{\bar{x}_1,\ldots,\bar{x}_K\}\subset\mathbb{R}^d$ in the case of mean-measure quantization, with $K$ typically small.
Using the theory of measure quantization \cite{graf2000foundations, pages2015introduction}, we validate both quantization approaches by showing (Prop.~\ref{prop:conv}):
\begin{equation}
\mathcal{W}_2^2\left(\frac{1}{N}\sum_{i=1}^N \delta_{\mu^{(i)}}, \frac{1}{N}\sum_{i=1}^N \delta_{\nu^{(i)}_K}\right) = 
\bigO{K^{-2/d}},  \label{eq:convW2}
\end{equation}
%where $W_2$ refers to the 2-Wasserstein distance \eqref{W2} over $\mathcal{P}(\mathcal{X})$, that is the set of probability measures with support included in $\mathcal{X}  \subset \mathbb{R}^d$ and 
where $\mathcal{W}_2$ denotes the $2$-Wasserstein distance \eqref{other_W2} on $\mathcal{P}(\mathcal{P}(\mathcal{X}))$, the set of probability measures over $\mathcal{P}(\mathcal{X})$, which is itself the set of probability measures with support included in a compact set $\mathcal{X}  \subset \mathbb{R}^d$.  %Results similar to \eqref{eq:convW2} can be obtained when the measures  $(\mu^{(i)})_{i=1}^N$ are discrete under additional assumptions on the cardinality of their supports.
%{\CB In \eqref{eq:convW2}, the notation $\sim$ means asymptotic equivalence between two sequences up to a multiplicative constant.}

The asymptotic result \eqref{eq:convW2} allows to show the convergence of numerous statistics computed from the $\nu^{(i)}_K$'s to corresponding quantities for the  $\mu^{(i)}$'s as $K \to +\infty$.  These statistics include the Wasserstein barycenter and the Gram matrix of the pairwise inner-products of the measures embedded to a Hilbert space using either LOT or KME. The latter is a standard quantity used in machine learning tasks (e.g.\ PCA).

%Second, for the purpose of using standard machine learning algorithms (e.g.\ PCA) on a set of probability measures at a low computational cost, we show that, given a mapping from the  space of probability measures to a Hilbert space, such as LOT or KME, the Gram matrix of the pairwise inner-products of the $N$ embedded quantized measures $(\nu^{(i)}_K)_{i=1}^N$ converges to that of the $N$ embedded raw measures $(\mu^{(i)})_{i=1}^N$ (Prop.~\ref{th_ML}). 

Finally, the soundness and consistency of our method is illustrated with numerical experiments on synthetic and real datasets. We also show that the method based on mean-measure quantization has computational advantages over optimal quantization of each input measure while preserving satisfactory performances, which justifies its use in large scale settings.

 \subsection{Related works}\label{sec:works}
 
 In \cite{chazal21, royer2021atol},  quantization is employed to embed a set of $N$ probability measures into a finite-dimensional Euclidean space through measure vectorization. More precisely, given $N$ input measures $\mu^{(i)}$, a quantization of the mean measure $\bar{\mu} = \frac{1}{N}\sum_{i=1}^n \mu^{(i)}$ by $K$ centers $x_1,\ldots,x_K$  in  $\mathbb{R}^d$ is first done. Then, they map each measure $\mu^{(i)}$ to $v^{(i)}= (v^{(i)}_1,\cdots, v^{(i)}_K)$ a vector of the convex space $\mathbb{R}_{+}^K$, where $v^{(i)}_k$ roughly represents the mass spread from the measure $\mu^{(i)}$ around the center $x_k$.  %Hence, the resulting vectorization method is  an embedding of the $\mu^{(i)}$'s into the convex space $\mathbb{R}_{+}^K$.
 Yet, this embedding does not take into account the relative positions of the $K$ centers, and consistency in the sense \eqref{eq:convW2} is not shown as we propose in this paper by endowing the set of quantized measures $\nu^{(i)}_K$  with the Wasserstein distance \eqref{W2}.

 In \cite{pmlr-v162-chatalic22a}, the authors tackle the problem of computing the KME of a probability distribution $\mu$ for which $m$ samples $X_1,\ldots,X_m$  are available. They introduce an estimator of the KME of $\mu$ based on Nystr\"om approximation that can be computed efficiently using a small random subset  from the data. Their theoretical and empirical results show that this approach yields a consistent estimator of the maximum mean discrepancy distance between the KMEs of $\mu$ and $\hat{\mu}_{m} = \frac{1}{m} \sum_{j= 1}^{m} \delta_{X_j}$ at a low computational cost. However, this Nystr\"om approximation has not been studied for constructing a consistent LOT embedding  estimator.

Finally, the benefits of a preliminary quantization step have been studied in \cite{ beugnot2021improving} to improve  the standard estimator of the OT cost between two probability measures based on the plug-in of their empirical counterpart. Still, the simultaneous quantization of $N$ probability measures for the purpose of constructing consistent and scalable embeddings has not been considered so far.

\subsection{Organization of the paper}

Section \ref{sec:back} presents OT, the embeddings LOT and KME and the quantization principle. In Section \ref{sec:theo}, we describe our two quantization methods of a set of $N$ probability measures, and analyze their theoretical properties. Section \ref{sec:num} reports the results of numerical experiments using synthetic and real data, and compare the computational cost of both methods. The paper ends with a conclusion in Section \ref{sec:conclusion}. All proofs are deferred to two technical Appendices \ref{appendixA} and  \ref{appendixB}, and additional numerical experiments are given in Appendix \ref{appendixC}.

\section{Background}\label{sec:back}

\paragraph{Optimal transport.} Let $\rho$ and $\mu$ be two probability measures with support included in a compact set $\mathcal{X} \subset\mathbb{R}^d$. For the quadratic cost, the OT problem between $\rho$ and $\mu$ is:
\begin{equation}\label{OT}
\min\limits_{\pi \in \Pi(\rho,\mu)} \int_{\mathcal{X}\times\mathcal{X}} \|x - y\|^2\mathrm{d}\pi(x,y), 
\end{equation}
where $\Pi(\rho,\mu)$ is the set of probability measures (or transport plans) on $\mathcal{X}\times\mathcal{X}$ with marginals $\rho$ and $\mu$. For $\pi^*$ a minimizer of \eqref{OT}, the Wasserstein metric between $\rho$ and $\mu$ is
\begin{equation}\label{W2}
W_2(\rho,\mu) =  \Bigl(\int_{\mathcal{X}\times\mathcal{X}}  \|x - y\|^2\mathrm{d}\pi^*(x,y)\Bigr)^{1/2}.
\end{equation}

Now, we endow the set of probability measures $\mathcal{P}(\mathcal{X})$ with the 2-Wasserstein distance $W_2$. In this paper, we shall represent the set $(\mu^{(i)})_{1\leq i\leq N}$ as the discrete empirical probability measure $\mathbb{P}^N=\frac{1}{N}\sum_{i=1}^N \delta_{\mu^{(i)}}$ over $\mathcal{P}(\mathcal{X})$.  To define a metric  on $\mathcal{P}(\mathcal{P}(\mathcal{X}))$, the set of Borel probability measures over $\mathcal{P}(\mathcal{X})$, we will use $W_2^2$ as the ground cost on the metric space $(\mathcal{P}(\mathcal{X}),W_2)$. The 2-Wasserstein distance over $\mathcal{P}(\mathcal{P}(\mathcal{X}))$ is then defined as \cite{le2017existence}
\begin{equation}\label{other_W2}
\mathcal{W}_2(\mathbb{P},\mathbb{Q}) = \Bigl(\min\limits_{\gamma \in \Gamma(\mathbb{P},\mathbb{Q})} \int_{\mathcal{P}(\mathcal{X})\times \mathcal{P}(\mathcal{X})} W_2^2(\rho,\mu)\mathrm{d}\gamma(\rho,\mu)\Bigr)^{1/2},
\end{equation}
where $\Gamma(\mathbb{P},\mathbb{Q})$ is the set of probability distributions on $\mathcal{P}(\mathcal{X})\times \mathcal{P}(\mathcal{X})$ with respective marginals $\mathbb{P}$ and $\mathbb{Q}$. 

\paragraph{LOT and KME embeddings.} 

Given an absolutely continuous (a.c.) measure $\rho$, we first recall that   Brenier's theorem \cite{brenier1991polar} states that the optimal transport plan $\pi^*$ in \eqref{OT} is supported on the graph of a $\rho$-a.s. unique push-forward map\footnote{We recall that the pushforward operator of a measure $\rho$ in $\mathbb{R}^d$ is defined as the measure $T_{\#}\rho$ such that for all Borelian $B\subset\mathbb{R}^d, T_{\#}\rho=\rho\left(T^{-1}(B)\right)$.} $T_{\rho}^{\mu}: \mathcal{X} \rightarrow \mathbb{R}^d$. In other words, $\pi^* = (\mathrm{id},T_{\rho}^{\mu})_{\#}\rho$ and 
\begin{equation}\label{eq:Monge}
  W_2^2(\rho,\mu) = \int_{\mathcal{X}} \|x - T_{\rho}^{\mu}(x)\|^2\mathrm{d}\rho(x).  
\end{equation}

The LOT embedding then consists in mapping a probability measure $\mu$ into the tangent space $\mathcal{T}_{{\rho}} = \{ T_{\rho}^{\mu}  - \mathrm{id}\}_{ \mu \in \mathcal{P}(\mathcal{X})} $ at $\rho$, that is a subspace of the Hilbert space $L^2({\rho},\mathbb{R}^d) = \{ v: \mathbb{R}^d \rightarrow \mathbb{R}^d ~|~ \int_{\mathbb{R}^d} \|v\|^2 \mathrm{d}{\rho} < \infty \}$, endowed with the weighted $L^2$ inner product $\langle v_1, v_2 \rangle_{L^2({\rho})} = \int_{\mathbb{R}^d} v_1(x)^Tv_2(x) \mathrm{d}{\rho}(x)$.

Now, given a positive definite kernel function $k:\mathcal{X}\times\mathcal{X}\rightarrow \mathbb{R}$ and associated RKHS $\mathcal{H}$, the KME of $\mu  \in \mathcal{P}(\mathcal{X})$  is the embedding $ \phi:\mathcal{P}(\mathcal{X}) \rightarrow \mathcal{H}$ defined by $ \phi(\mu) = \int_{\mathcal{X}} k(x,\cdot)\mathrm{d}\mathbb{\mu}(x)$.  When the kernel $k$ is characteristic \cite{muandet2017kernel}, the map $\phi$ is injective and one can define a metric on $\mathcal{P}(\mathcal{X})$ called \emph{Maximum Mean Discrepancy} 
$
\mathrm{MMD}(\rho,\mu) = \|\phi(\rho)-\phi(\mu)\|_{\mathcal{H}}.
$
%{\CR [Introduire la MMD a du sens  si l'on prouve des r\'esultats de convergence du type $\lim_{K \to \infty} \frac{1}{N} \sum_{i=1}^{N} \mathrm{MMD}(\nu^{(i)},\mu^{(i)}) = 0$. Est-ce le cas ? Lien avec les r\'esultats dans  \cite{pmlr-v162-chatalic22a} ?] }


\paragraph{Optimal quantization.} We conclude this section with reminders on the theory of quantization \cite{graf2000foundations, pages2015introduction}. A $K$-points quantization of an arbitrary probability measure $\mu$ aims at approximating $\mu$ by solving \cite{graf2000foundations}[Lemma 3.4]: 
\begin{equation}\label{quantif_pb}
    \min\limits_{a\in\Sigma_K, X \in (\mathbb{R}^d)^K}  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K a_k\delta_{x_k}\Bigr),
\end{equation}
where $\Sigma_K$ is the probability simplex in $\mathbb{R}^K$. 
\begin{remark}
If $\mu$ is a.c.,\ it follows from \cite{JMLR:v21:18-804}[Proposition 2] that minimizers of \eqref{quantif_pb} over $X \in (\mathbb{R}^d)^K$ exist and belong to the set of pairwise distinct points
 $$
 F_K = \{ X = (x_1,\cdots,x_K)  \in (\mathbb{R}^d)^K  ~|~ x_k \neq x_{\ell}, \mbox{ if } k \neq \ell \}.
 $$
Given $X \in F_K$, it is well-known, see e.g.\ \cite{COCV_2012,GKL19}, that  the minimizer $a^*$ of $\min\limits_{a\in\Sigma_K}  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K a_k\delta_{x_k}\Bigr)$
is unique, and verifies $a_k^* = \mu(V_{x_k})$ where $(V_{x_k})_{k=1}^K$ is the set of  Vorono\"i cells
induced by $X$:
\begin{equation}\label{voronoi_cell}
    V_{x_k}  = \bigl\{ y\in\mathbb{R}^d ~|~ \forall  \ell \neq k, \|x_k-y\|^2 \leq  \|x_{\ell}-y\|^2 \bigr\}.
\end{equation}
Thereby,  the quantization problem \eqref{quantif_pb} rewrites as:
\begin{equation}\label{quantif_pb2}
    \min\limits_{X\in (\mathbb{R}^d)^K}  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K \mu(V_{x_k})\delta_{x_k}\Bigr).
\end{equation}
\end{remark}

\begin{remark}
If the measure $\mu$ is discrete, then the closed form solution in variable $a$ of the quantization problem \eqref{quantif_pb} remains valid provided that the definition \eqref{voronoi_cell} of the Vorono\"i cells is slighted modified as follows
%\begin{eqnarray}
%    \widetilde{V}_{x_k}   = & \bigl\{ y\in\mathbb{R}^d ~|~ \forall  \ell < k, \|x_k-y\|^2 <  \|x_{\ell}-y\|^2, \nonumber  \\
%  &   \mbox{and }  \forall  \ell > k, \|x_k-y\|^2 \leq  \|x_{\ell}-y\|^2 \bigr\}. \label{voronoi_cell_discrete}
%\end{eqnarray}
\begin{equation}
 \widetilde{V}_{x_1} :=  V_{x_1} \quad \mbox{and} \\
  \quad\widetilde{V}_{x_k} :=    V_{x_k} \backslash  \bigcup_{j < k} \widetilde{V}_{x_j}  \mbox{ for } k \geq 2,   \label{voronoi_cell_discrete}
\end{equation}
so that $(\widetilde{V}_{x_k})_{k=1}^K$ form a partition of $\mathbb{R}^d$ \cite{graf2000foundations}[Chapter 1] for $X =(x_1,\ldots,x_K) \in F_K$. Then, for such a partitioning, it follows from Lemma \ref{lem:discrete} in Appendix \ref{appendixA} that
\begin{equation*}
\min\limits_{a\in\Sigma_K}  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K a_k \delta_{x_k}\Bigr)  = W_2^2\Bigl(\mu,\sum\limits_{k=1}^K \mu(\widetilde{V}_{x_k})\delta_{x_k}\Bigr) 
  =   \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{\|x_{k}-y\|^2\} \mathrm{d} \mu(y),
\end{equation*}
for any probability measure $\mu$. As a consequence, and unless otherwise stated, the results of the paper hold for discrete probability measures with the choice \eqref{voronoi_cell_discrete} as a Vorono\"i partition, which corresponds to a chosen enumeration order of the elements of the set $X$. However, when $\mu$ is a discrete measure, it is necessary to require the cardinality of its support to be larger than $K$ so that the minimizers of \eqref{quantif_pb} belong to $F_K$  \cite{JMLR:v21:18-804}[Proposition 2], and the sets of the Vorono\"i partition \eqref{voronoi_cell_discrete} are pairwise distinct.
\end{remark}

Given a $K$-points quantization, that is a minimizer $X^* \in (\mathbb{R}^d)^K$ of \eqref{quantif_pb}, the \emph{quantization error} of the probability measure $\mu$ is defined as
\begin{equation}\label{def:quant_error}
    \varepsilon_K(\mu) =  \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{ \|x^*_{k}-y\|^2 \} \mathrm{d} \mu(y).
\end{equation}
Theorem 6.2 in  \cite{graf2000foundations} then implies  that $\varepsilon_K(\mu)  = 
\bigO{K^{-2/d}}$ if $\mu$ is a.c.\ and $\varepsilon_K(\mu) =  \smallO{K^{-2/d}}$ if $\mu$ is discrete.

\section{Statistical properties of probability measures embeddings  via measure quantization}\label{sec:theo}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{images/quantization.png}
    \caption{Illustration of the quantization methods on two 1D probability measures (left) with $K=2$. (1) Quantization of each measure. (2a) Quantization of the mean measure. (2b) Computation of the weights for each measure. The vertical lines are located on the Dirac positions and weights of the quantized measures.} 
    \label{fig:illustration}
\end{figure}

Throughout this section, the supports of the probability measures $(\mu^{(i)})_{i=1}^N$ are included in a compact set $\mathcal{X}\subset \mathbb{R}^d$. To reduce the computational costs of solving ML problems involving $N$ measures $(\mu^{(i)})_{i=1}^N$ with large supports, we propose two quantization methods, illustrated in Figure \ref{fig:illustration}, to approximate the $\mu^{(i)}$'s with discrete measures supported on $K$ points.

%  implying that the mean measure $\overline{\mu} = \frac{1}{N}\sum_{i=1}^N \mu^{(i)}$ is also a.c.

\paragraph{Optimal quantization of each input measure.}

A first natural approach is to approximate each $\mu^{(i)}$ by its optimal quantization
\begin{equation}
\tilde{\nu}^{(i)}_K = \sum\limits_{k=1}^K \tilde{a}_k^{(i)}\delta_{x_k^{(i)}}, \label{eq:nutilde}
\end{equation}
where the weights $\tilde{a}^{(i)} = (\tilde{a}_k^{(i)})_{1 \leq k \leq K}$ and locations $X^{(i)} = (x_k^{(i)})_{1 \leq k \leq K}$ are minimizers of \eqref{quantif_pb} for $\mu = \mu^{(i)}$. 

\paragraph{Mean-measure quantization} 

Our second quantization method consists in solving the following problem: 
\begin{equation}\label{eq:alternative_barycenter}
\min\limits_{a\in(\Sigma_K)^N, \  X\in F_K} \frac{1}{N} \sum\limits_{i=1}^N W_2^2\Bigl(\sum\limits_{k=1}^K a^{(i)}_k\delta_{x_k}, \mu^{(i)}\Bigr),
\end{equation}
as introduced in \cite{gachon2024low} for a.c.\ probability distributions. The following result shows that optimizing \eqref{eq:alternative_barycenter} is equivalent to $K$-points quantization of the mean measure.
\begin{proposition}\label{prop:pb_equivalence}
Let $(\mu^{(i)})_{1\leq i\leq N}$ be arbitrary probability measures and let $\overline{\mu} = \frac{1}{N}\sum_{i=1}^N \mu^{(i)}$ be the mean measure. Suppose that the cardinality of the support of $\overline{\mu}$ is larger than $K$. Then,
\begin{eqnarray}
    \min\limits_{a\in(\Sigma_K)^N, \ X\in F_K} \frac{1}{N} \sum\limits_{i=1}^N W_2^2\Bigl(\sum\limits_{k=1}^K a^{(i)}_k\delta_{x_k}, \mu^{(i)}\Bigr)
   &= \min\limits_{X\in {(\mathbb{R}^d)^N}}&  \frac{1}{N} \sum_{i=1}^N W_2^2\Bigl(\sum\limits_{k=1}^K \mu^{(i)}(\widetilde{V}_{x_k})\delta_{x_k}, \mu^{(i)}\Bigr) \nonumber\\
    &= \min\limits_{X\in (\mathbb{R}^d)^N}&   W_2^2\Bigl(\sum\limits_{k=1}^K \overline{\mu}(\widetilde{V}_{x_k})\delta_{x_k},\overline{\mu}\Bigr) \label{eq:pb_equivalence}
\end{eqnarray}
\end{proposition}

For a minimizer $\bar{X} = (\bar{x}_1,\cdots,\bar{x}_K)$ of \eqref{eq:pb_equivalence}, that is a $K$-point quantization of $\overline{\mu}$, we then define the quantized measures for $1\leq i\leq N$ by
\begin{equation}\label{eq:nubar}
    \bar{\nu}^{(i)}_K = \sum\limits_{k=1}^K \bar{a}^{(i)}_k \delta_{\bar{x}_k}, \mbox{ with }  \bar{a}^{(i)}_k = \mu^{(i)}(\widetilde{V}_{\bar{x}_k}), 
\end{equation}
where $(\widetilde{V}_{\bar{x}_k})_{1\leq k\leq K}$ is the Vorono\"i partition \eqref{voronoi_cell_discrete} associated to $\bar{X}$. The measure $\bar{\nu}^{(i)}_K$ is therefore a discrete probability measure supported on $K$ points that is an approximation of $\mu^{(i)}$ in the sense of the minimization problem \eqref{eq:alternative_barycenter}.  The measures $(\bar{\nu}^{(i)}_K)_{1\leq i\leq N}$ differ in their weights but share the same support $\bar{X}$. In a slight abuse of language, we will refer to $\bar{\nu}^{(i)}_K$ as a \emph{quantized} version of $\mu^{(i)}$, even though it is not the optimal quantization $\tilde{\nu}^{(i)}_K$ of $\mu^{(i)}$ given in \eqref{eq:nutilde}.

\begin{remark}[On the compactness assumption of $\mathcal{X}$] Proposition \ref{prop:pb_equivalence} remains true without the compactness of $\mathcal{X}$, under the assumption of $2$-order moments of the measures.
\end{remark}

\begin{remark}[On the nature of input probability measures]

$(i)$ If all the measures $(\mu^{(i)})_{1\leq i\leq N}$ are a.c.\ then  $\overline{\mu}$ is also a.c.\ and by convention the  cardinality of its support is $+\infty$. In this case, for all $i\in\{1,\ldots,N\}$ and $k\in\{1,\ldots,K\}$,
$$
\bar{a}^{(i)}_k = \mu^{(i)}(\widetilde{V}_{\bar{x}_k}) = \mu^{(i)}(V_{\bar{x}_k}),
$$
as the boundaries of the Vorono\"i cells $(V_{\bar{x}_k})_{1\leq k\leq K}$ have zero-mass for the Lebesgue measure.

$(ii)$ If all the measures $(\mu^{(i)})_{1\leq i\leq N}$ are discrete, then the definition \eqref{eq:nubar} of the probability measure $(\bar{\nu}^{(i)}_K)_{1\leq i \leq N}$ is specific to the chosen Vorono\"i partition \eqref{voronoi_cell_discrete} associated to an enumeration of $\bar{X}$. In this setting, a minimizer $\bar{a}$ of \eqref{eq:alternative_barycenter} is not necessarily unique, and another enumeration order of $\bar{X}$ may lead to a slightly different set of quantized measures, depending on the intersection between  the points clouds and the boundaries of the Vorono\"i cells.
\end{remark}

For clarity, we write the Vorono\"i cells associated to a $K$-quantization $\bar{X}$ of the mean measure $\overline{\mu}$ in Prop.\ref{prop:pb_equivalence} as
\begin{equation}\label{optimal_voronoi}
    V_k := \widetilde{V}_{\bar{x}_k}, \quad \mbox{for all} \ 1\leq k\leq K.
\end{equation}

\subsection{Consistency of  measures quantization}

The following result shows the consistency of both quantization methods by leveraging the quantization error function $\varepsilon_K(\cdot)$ defined in \eqref{def:quant_error}.

\begin{proposition}\label{prop:conv}
Let $\mathbb{P}^N = \frac{1}{N}\sum_{i=1}^N\delta_{\mu^{(i)}}$, $\overline{\mathbb{P}}^N_K = \frac{1}{N}\sum_{i=1}^N\delta_{\overline{\nu}^{(i)}_K}$ and  $\widetilde{\mathbb{P}}^N_K = \frac{1}{N}\sum_{i=1}^N\delta_{\Tilde{\nu}^{(i)}_K}$. Then,
\begin{align}
\mathcal{W}_2^2(\overline{\mathbb{P}}^N_K,\mathbb{P}^N) &= 
\frac{1}{N}\sum_{i=1}^N W_2^2(\mu^{(i)},\overline{\nu}^{(i)}_K) = \varepsilon_K( \overline{\mu})\label{mean_weak_conv}\\
\mbox{and} \hspace{1.4cm}&\nonumber\\
\mathcal{W}_2^2(\widetilde{\mathbb{P}}^N_K,\mathbb{P}^N) &= 
\frac{1}{N}\sum_{i=1}^N W_2^2(\mu^{(i)},\Tilde{\nu}^{(i)}_K) = \frac{1}{N}\sum_{i=1}^N \varepsilon_K( \mu^{(i)}). \label{weak_conv}
\end{align}
\end{proposition}

\begin{remark}
    Proposition \ref{prop:conv} and the consistency results \eqref{mean_weak_conv} and \eqref{weak_conv} could be  extended to more general cost functions $c$ satisfying smoothness conditions such as the so-called $x$-regularity from \cite{houdard2023gradient}[Definition 1]. 
\end{remark}


In other words, Proposition \ref{prop:conv} shows the convergence of the empirical measures $\overline{\mathbb{P}}^N_K$ and $\widetilde{\mathbb{P}}^N_K$ towards $\mathbb{P}^N$ at the rate $\bigO{K^{-2/d}}$ as $K$ goes to $+\infty$.

\begin{remark}
By definition of optimal quantization \eqref{quantif_pb}, one has the inequality $W_2^2(\mu^{(i)},\tilde{\nu}^{(i)}_K) \leq  W_2^2(\mu^{(i)},\overline{\nu}^{(i)}_K) $ for all $1 \leq i \leq N$. Therefore, we deduce from Proposition \ref{prop:conv} that $\mathcal{W}_2^2(\widetilde{\mathbb{P}}^N_K,\mathbb{P}^N) \leq \mathcal{W}_2^2(\overline{\mathbb{P}}^N_K,\mathbb{P}^N)$.
Hence, $\widetilde{\mathbb{P}}^N_K$ is a better approximation of $\mathbb{P}^N$ than $\overline{\mathbb{P}}^N_K$. Still, the rates of convergence of $\mathcal{W}_2^2(\widetilde{\mathbb{P}}^N_K,\mathbb{P}^N)$ and  $\mathcal{W}_2^2(\overline{\mathbb{P}}^N_K,\mathbb{P}^N)$ are both scaling as $O(K^{-2/d})$. Moreover, when the $\mu^{(i)}$'s are discrete, mean-measure quantization has computational advantages  over optimal quantization of each input measure for moderate to large values of $N$ (see Section \ref{sec:cost}). 
\end{remark}

\begin{remark}\label{rmq:notation} We simplify notation by denoting $\tilde{\nu}^{(i)}_K$ (resp. $\widetilde{\mathbb{P}}^N_K$), defined in \eqref{eq:nutilde}, and $\bar{\nu}^{(i)}_K$ (resp. $\overline{\mathbb{P}}^N_K$)  defined in \eqref{eq:nubar}, by $\nu^{(i)}_K$ (resp. $\mathbb{P}^N_K$). We also write $\varepsilon_K = \mathcal{W}_2^2(\mathbb{P}_K^N,\mathbb{P}^N)$, where $\varepsilon_K = \frac{1}{N}\sum_{i=1}^N \varepsilon_K(\mu^{(i)})$ in the case of optimal quantization of each input measure,
and $\varepsilon_K =\varepsilon_K(\overline{\mu})$ in the case of mean-measure quantization.
\end{remark}

Since $\mathcal{X}$ is a compact set, so is the metric space $(\mathcal{P}(\mathcal{X}),W_2)$ \cite{villani2009optimal}[Remark 6.17]. Then, by  \cite{santambrogio2015optimal}[Theorem 5.9], $\mathcal{W}_2(\mathbb{P}^N_K,\mathbb{P})\rightarrow 0$ if and only if $\mathbb{P}^N_K  \rightarrow \mathbb{P}^N$ in the sense of weak convergence of distributions, or in other words for any bounded continuous function $f:\mathcal{P}(\mathcal{X})\rightarrow \mathbb{R}$, it holds that $\int f(\nu)\mathrm{d} \mathbb{P}^N_K(\nu)\overset{K \to +\infty}{\longrightarrow} \int f(\mu)\mathrm{d}\mathbb{P}^N(\mu)$. Proposition \ref{prop:conv}  implies the consistency of numerous statistics computed from the quantized measures $(\nu^{(i)}_K)_{1\leq i \leq N}$ and also allows to show convergence in the MMD sense.

\begin{corollary}\label{cor:mmd} One has
\begin{equation}\label{eq:convMMD}
\frac{1}{N}\sum_{i=1}^N\mathrm{MMD}^2(\mu^{(i)},\nu^{(i)}_K) \leq \varepsilon_K  \overset{K\rightarrow \infty}{\longrightarrow} 0.
\end{equation}
\end{corollary}



\subsection{Statistics from the quantized measures}

We focus here on how statistics computed from the quantized measures $(\nu^{(i)}_K)_{i=1}^N$, defined either by \eqref{eq:nutilde} or \eqref{eq:nubar}, relate to statistics from the input measures $(\mu^{(i)})_{i=1}^N$.

\subsubsection{Wasserstein barycenter}

 A first example consists in proving that a Wasserstein barycenter \cite{agueh2011barycenters} of the $(\nu^{(i)}_K)_{1\leq i\leq N}$ converges towards the unique Wasserstein barycenter of the measures $(\mu^{(i)})_{1\leq i\leq N}$ when at least one of them is a.c.

\begin{proposition}\label{bary_conv}
Let $\nu_K^{\mathrm{bar}}$ be a Wasserstein barycenter of $(\nu^{(i)}_K)_{1\leq i\leq N}$ that is 
$$\nu_K^{\mathrm{bar}} \in \argmin\limits_{\nu\in\mathcal{P}(\mathcal{X})}\ \frac{1}{N}\sum\limits_{i=1}^N W_2^2(\nu,\nu^{(i)}_K).$$ If  at least one of the measures $(\mu^{(i)})_{1\leq i\leq N}$ is  a.c., then $\nu_K^{\mathrm{bar}}$ converges to the unique Wasserstein barycenter $\mu^{\mathrm{bar}}$ of $(\mu^{(i)})_{1\leq i\leq N}$ in the Wasserstein sense as $K \to + \infty$ . 
\end{proposition}

\subsubsection{Statistical dispersion}

For a set of measures $\mu=(\mu^{(i)})_{i=1}^N$, we define its dispersion as the sum of squares
$
\mathrm{SS}(\mu) = \frac{1}{N^2}\sum_{i,j=1}^N W_2^2(\mu^{(i)},\mu^{(j)}).
$
The following shows that $\mathrm{SS}(\nu_K)$, for $\nu_K=(\nu^{(i)}_K)_{i=1}^N$, is controlled by $\mathrm{SS}(\mu)$ and the quantization error $ \varepsilon_K$ defined in Remark \ref{rmq:notation}.

\begin{proposition}\label{prop:3} 
One has that for any $\lambda > 0$,
$$
\mathrm{SS}(\nu_K)\leq  (1+2/\lambda)\mathrm{ SS}(\mu)+(4+2\lambda) \varepsilon_K .$$
\end{proposition}
Guaranteeing that the pairwise distance $W_2(\nu^{(i)}_K,\nu^{(j)}_K)$ is a good approximation of $W_2(\mu^{(i)},\mu^{(j)})$ is essential as many machine learning tasks rely on comparing pairs of data. For mean-measure quantization \eqref{eq:alternative_barycenter}, we  provide below a result on pairwise distances when the input measures are all a.c., which only depends on the quantization.
\begin{proposition}\label{prop:2}
Suppose that the probability measures $(\mu^{(i)})_{i=1}^N$ are a.c. Then, one has
$$W_2^2(\bar{\nu}^{(i)}_K,\bar{\nu}^{(j)}_K) \leq  3W_2^2(\mu^{(i)},\mu^{(j)}) +  6\max\limits_{1\leq k\leq K} \mathrm{diam}(V_k),$$
with $\mathrm{diam}(V_k) = \max\limits_{x,y\in V_k} \|x-y\|^2$ and $(V_k)_{k=1}^K$ the Vorono\"i cells \eqref{optimal_voronoi} obtained from the $K$-points quantization of $\bar{\mu}$.
\end{proposition}


The following lemma provides an upper bound on the term $\max_k \mathrm{diam}(V_k)$ in Proposition \ref{prop:2} in the special case where the support of $\bar{\mu}$ is included in $[0,1]^d$. This bound depends on the number of centers $K$ and the ambient dimension $d$, and holds true for either discrete or continuous support.
 
\begin{lemma}\label{lem:1}
Suppose that the (discrete or continuous) support of the mean measure $\bar{\mu}$ is included in $[0,1]^d$ and let  $(V_k)_{k=1}^K$ be the Vorono\"i cells of the quantization of $\bar{\mu}$. Then,
$$\max\limits_{1\leq k \leq K} \mathrm{diam}(V_k) \leq \frac{d}{\lfloor \sqrt[d]{K} \rfloor^2}.$$
\end{lemma}



\subsubsection{Clustering performances}

We now show that both quantization methods preserve the clustering structure of the input measures. To this end, let us assume that each  measure $\mu^{(i)}$ has a label $1\leq l\leq L$. We note $I_l$ the set of indices such that $\forall i\in I_l, \mu^{(i)}$ has label $l$, and $N_l$ its cardinal. When clustering data, one usually aims at minimizing the within-class variance WCSS for a cluster $l$ and maximizing the between-class variance BCSS for clusters $l_1$ and $l_2$, where for a set of measure $\mu=(\mu^{(i)})_{i=1}^N$,
\begin{eqnarray*}
    \mathrm{WCSS}(l,\mu) & =&  \frac{1}{N_l^2} \sum\limits_{i,j \in I_l} W_2^2(\mu^{(i)},\mu^{(j)}), \\
    \mathrm{BCSS}(l_1,l_2, \mu) & = & \frac{1}{N_{l_1}N_{l_2}} \sum_{\substack{i_1\in I_{l_1} \\ i_2\in I_{l_2}}} W_2^2(\mu^{(i_1)},\mu^{(i_2)}). 
\end{eqnarray*}

The next result gives a bound on clustering performances of the quantized measures, and is illustrated in Section \ref{sec:num}.
\begin{proposition}\label{prop:4}
For a given class $1 \leq l \leq L$, one has% and $\nu_K=(\nu^{(i)}_K)_{i=1}^N$, one has that: 
\begin{equation}\label{intra_var_bound}
        \mathrm{WCSS}(l,\nu_K) \leq 3\mathrm{ WCSS}(l,\mu)+ \frac{6N}{N_l}\varepsilon_K. 
    \end{equation}
For two distinct classes $l_1$ and $l_2$, one has that
    \begin{equation}\label{inter_var_bound}
        \mathrm{BCSS}(l_1, l_2, \nu_K) \geq \frac{1}{3}\mathrm{BCSS}(l_1, l_2, \mu) - \Bigl( \frac{N}{N_{l_1}}+\frac{N}{N_{l_2}}\Bigr) \varepsilon_K,
    \end{equation}
where $\varepsilon_K$ is the quantization error defined in Remark \ref{rmq:notation}.
\end{proposition}
\subsection{Machine learning from the quantized measures}\label{sec:ML}

We consider here the embeddings of measures into a Hilbert space using either LOT or KME in Section \ref{sec:back}. We focus on comparing the performances of  machine learning methods after such embeddings of the input measures  $(\mu^{(i)})_{i=1}^N$ and their quantized versions $(\nu^{(i)}_K)_{i=1}^N$. To this end, we compare the Gram matrices of the pairwise inner-products between the set of embedded measures. Indeed, these matrices play a crucial role in various machine learning tasks \cite{kernelreview} such as PCA or Linear Discriminant Analysis (LDA), that rely on the diagonalization of the covariance operator of data in a Hilbert space, which is equivalent to diagonalizing the Gram matrix of inner-products as recalled in Appendix \ref{appendixB}. 

In the following,  for a given embedding $\phi$ of probability measures into a Hilbert space $\mathcal{H}$ equipped with the inner-product $\langle \cdot,\cdot\rangle_{\mathcal{H}}$, we will denote $G^{\phi}_{\mu}$ and $G^{\phi}_{\nu_K}$ the $N\times N$ Gram matrices associated with $(\mu^{(i)})_{1\leq i\leq N}$ and $(\nu^{(i)}_K)_{1\leq i\leq N}$ respectively, with entries
\begin{equation*}
(G^{\phi}_{\mu})_{ij}= \langle \phi(\mu^{(i)}),\phi(\mu^{(j)})\rangle_{\mathcal{H}},\mbox{and} \quad (G^{\phi}_{\nu_K})_{ij} =\langle \phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}},
\end{equation*}
and $\Vert\cdot\Vert_F$ the Frobenius matrix norm.

\begin{proposition}\label{th_ML}
(i) We denote $G^{\mathrm{LOT}}_{\mu}$ and $G^{\mathrm{LOT}}_{\nu_K}$ the Gram matrices corresponding to the LOT embedding $\phi:\sigma\mapsto T_{\rho}^{\sigma}-\mathrm{id}$, where $\rho\in\mathcal{P}(\mathcal{X})$ is any a.c. reference measure with support included in the compact set $\mathcal{X}\subset\mathbb{R}^d$. Assume that the Brenier maps $T_{\rho}^{\mu^{(i)}}$ are $L$-Lipschitz for all $1 \leq i \leq N$. Then, we have that
    \begin{equation}\label{eq:LOT_bound}
        \frac{1}{N}\|G^{\mathrm{LOT}}_{\mu}-G^{\mathrm{LOT}}_{\nu_K}\|_F^2\leq C_{N,\mathcal{X},L}  \sqrt{\varepsilon_K}
    \end{equation}
    
    where $C_{N,\mathcal{X},L}$ is a constant depending on $N$, the set $\mathcal{X}$ and the Lipschitz constant $L$.
    
(ii) We note $G^{\mathrm{KME}}_{\mu}$ and $G^{\mathrm{KME}}_{\nu_K}$ the Gram matrices corresponding to the KME $\phi:\sigma\mapsto \int k(x,\cdot)\mathrm{d}\sigma(x)$ for a bounded kernel function $k$, then:
    \begin{equation}\label{eq:KME_bound}
        \frac{1}{N}\|G^{\mathrm{KME}}_{\mu}-G^{\mathrm{KME}}_{\nu_K}\|_F^2\leq C_{N,k}\varepsilon_K
    \end{equation}  
    
    where $C_{N,k}$ is a constant depending on $N$ and the kernel $k$.


\end{proposition}

As a consequence of Proposition \ref{th_ML}, we have that a functional PCA of the maps $(\phi(\nu^{(i)}_K))_{i=1}^N$ in a certain Hilbert space is consistent (as $K \to +\infty$) with the PCA of the maps $(\phi(\mu^{(i)}))_{i=1}^N$ in the same Hilbert space.

\section{Numerical experiments}\label{sec:num}

In our experiments, we distinguish the two quantization approaches : $\Tilde{K}$-LOT and $\Tilde{K}$-KME refer to the LOT embedding and KME of $(\Tilde{\nu}^{(i)}_K)_{i=1}^K$ obtained from the quantization of each $\mu^{(i)}$, while $\overline{K}$-LOT and $\overline{K}$-KME refer to the LOT embedding and KME of $(\overline{\nu}^{(i)}_K)_{i=1}^K$ obtained from the mean-measure quantization.
Here, we aim to show that our method enables fast computation of machine learning tasks while preserving the main information of the measures. 

\subsection{Computational cost} \label{sec:cost}

For a discrete measure, we solve the quantization problem \eqref{quantif_pb2} using Lloyd's algorithm \cite{lloyd1982least} and an initialization based on $K$-means$++$ \cite{kmeansplusplus}. The time complexity of the Lloyd's algorithm being linear in the number of data points \cite{Hartigan79}, it follows that for discrete measures $\mu_1,\ldots,\mu_N$ supported on $m_1,\ldots,m_N$ points respectively, the computational cost for constructing the quantized measures $(\nu^{(i)}_K)_{i=1}^K$ by either optimal quantization of each input measure or mean-measure quantization  is $O(K d \sum_{i=1}^{N}m_i )$.  Nevertheless, as the support of $\overline{\mu}$ is very large in applications, the mean measure quantization can be done on $\frac{1}{N} \sum_{i=1}^{N}m_i$ points randomly sampled from $\overline{\mu}$, leading to optimal locations $\bar{x}_1,\cdots, \bar{x}_K$ similar to those of the $K$-quantization of the entire support of $\overline{\mu}$. Then, the computation cost of mean-measure quantization becomes $O(K d \frac{1}{N} \sum_{i=1}^{N}m_i)$
and is advantageous over  the optimal quantization of each input measure.

In order to compute the LOT embedding, we solve the discrete OT problem \eqref{OT} between $m_0$ samples of $\rho$ and $m_i$ samples of $\mu^{(i)}$ using a standard OT solver \cite{peyre2019computational, flamary2021pot}. Then, as the optimal map $T_{\rho}^{\mu^{(i)}}$ in \eqref{eq:Monge} might not exist, it is classical \cite{deb2021rates} to compute an approximation through barycentric projection of an optimal transport plan, solution of \eqref{OT}. The overall computational of  $\overline{K}$-LOT when using $m_0$ samples from the reference measure $\rho$ is thus  $O(K d \frac{1}{N}  \sum_{i=1}^{N}m_i) + O(N (K+M)Km_0 \log(K+m_0))$ which is significantly smaller than the one of LOT from the raw input measures that scales as $O(\sum_{i=1}^{N}(m_i+m_0)m_im_0 \log(m_i+m_0))$.

Similarly, the computational cost of $K$-KME to construct the Gram matrix $G^{\mathrm{KME}}_{\nu_K}$ is $O(K d \frac{1}{N}  \sum_{i=1}^{N}m_i) + O(N^2 K^2)$ that is much cheaper than the cost of computing $G^{\mathrm{KME}}_{\mu}$  from the raw data that scales as $O(\sum_{i,j=1}^{N} m_i m_j)$.% $O({\CO d}\sum_{i,j=1}^{N} m_i m_j)$.

\subsection{Synthetic dataset : shifts and scalings of a reference measure}

We consider input measures that are shifts and scalings of a given a.c.\ compactly supported measure $\rho$, that is:
\begin{equation}\label{eq:shift_scaling}
    \mu^{(i)} = (\Sigma_i^{1/2}\mathrm{id} + b_i)_{\#}\rho,
\end{equation}
where $\Sigma_i\in\mathbb{R}^{d\times d}$ is a positive semi-definite matrix and $b_i\in\mathbb{R}^d$. We choose $X\sim\rho$ such that $X = R\frac{Z}{\|Z\|}$, with $R\sim \mathrm{Unif}([0,1])$ and $Z\sim\mathcal{N}(0,I_d)$. In that case, we have an explicit formulation of the pairwise inner-products induced by  LOT embedding and KME, see Proposition \ref{2D_gaussian_dotprod} in Appendix \ref{appendixC}. We can therefore exactly compute the so-called \emph{true matrix} $G_{\mu}^{\phi}$. We numerically sample the distributions $\mu^{(i)}$'s in the following way : for each $1\leq i\leq N$, we first sample $m_i$ points $(x_j)_{1\leq j\leq m_i}$ from the measure $\rho$ by sampling $z_j\sim \mathcal{N}(0,I_d)$ and $r_j\sim \mathrm{Unif}([0,1])$ and computing $x_j = r_j\frac{z_j}{\|z_j\|}$. This allows to sample points from the unit ball in $\mathbb{R}^d$. Samples from $\mu^{(i)}$ are then obtained by the pushforward operation in \eqref{eq:shift_scaling}.


We first compare in Figure \ref{fig:toy_example_time} the computational costs of both quantization methods and observe that the mean-measure quantization approach is faster. In Figure \ref{fig:lot_pca_viz} (resp. Figure \ref{fig:kme_pca_viz} in Appendix \ref{appendixC}), we visualize the projections on the first two components of PCA for $K$-LOT (resp. $K$-KME) of both quantization methods and compare them to the PCA computed from the true Gram matrices $G_{\mu}^{\phi}$ computed in Proposition \ref{2D_gaussian_dotprod}. We observe that even with a small value $K=32$, the PCA visualizations on quantized embedded measures mimic the ones on raw embedded input measures.


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/toy_example_time.png}}
\caption{\textbf{Synthetic dataset on shifts and scalings.} Evaluation time for the computation of the two quantization steps for $d=2$ and for different values of $K$.}
\label{fig:toy_example_time}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{images/lot_pca_viz.png}}
\caption{\textbf{Synthetic dataset on shifts and scalings.} Projection of the data onto the first two components of PCA after $\overline{K}$-LOT (top) and $\Tilde{K}$-LOT (bottom) and comparison to the LOT PCA (right) computed from the true Gram matrix (see Prop.\ref{2D_gaussian_dotprod}).}
\label{fig:lot_pca_viz}
\end{center}
\vskip -0.2in
\end{figure}



\subsection{Flow cytometry dataset}


\begin{table*}[t]
\caption{\textbf{Flow cytometry dataset.} Classification accuracies and execution times for LDA after 10-component PCA on ${K}$-LOT, ${K}$-KME with both quantization methods and KME with RFF.}
\label{accuracy_pb_bm}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{\begin{tabular}{cccc|ccc|cccc}
        \toprule &\multicolumn{3}{c}{$\overline{K}$-LOT/$\Tilde{K}$-LOT} & \multicolumn{3}{c}{$\overline{K}$-KME/$\Tilde{K}$-KME} & \multicolumn{4}{|c}{KME with RFF}\\
                         $K$    & \tiny{Accuracy (lab)} & \tiny{Accuracy (type)} & \tiny{Time (s)} & \tiny{Accuracy (lab)} & \tiny{Accuracy (type)} & \tiny{Time (s)} & $s$ & \tiny{Accuracy (lab)} & \tiny{Accuracy (type)} & \tiny{Time (s)}\\
        \midrule
        16 & 100/100 & 85/81 & 23/103 & 100/100 & 83/69 &  15/96 & 16 & 73 & 44 & 4524\\
        32 & 100/100 & 94/81 & 25/166 & 100/100 & 83/69 & 34/174 & 32 & 75 & 44 &    4701\\
        64 & 100/100 & 94/81 & 30/281 & 100/100 & 85/69 &  105/358 & 64 & 83 & 52 & 5035\\
        128 & 100/100 & 88/81 & 32/555 & 100/100 & 77/71 & 387/909 & 128 & 92 & 44 & 5676\\
        \bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


In this section, we use flow cytometry datasets provided in \cite{thrun2022flow} and publicly available in \href{https://data.mendeley.com/}{Mendeley Data} to illustrate the suitability of our method through a classification task. We have $N=108$ cytometry measures (or point cloud) which come from two different health care centers : Marburg and Dresden. In Marburg, the data consists of diagnostic samples of peripheral blood (pB), healthy bone marrow (BM), or leukemic bone marrow. The Dresden dataset consists of diagnostic samples of peripheral blood and healthy bone marrow. Two types of labels can be distinguished: data are differentiated either by the healthcare centers from which they were analyzed, or by their types (e.g., peripheral blood, healthy bone marrow, or leukemic bone marrow). Each measure contains from 100,000 to 1,000,000 points in dimension $d=10$, which prevents the use of classical OT, that is, without a quantization step. For $K$-LOT, we sample $m_0=1000$ points from the reference measure chosen as the uniform measure on $[0,1]^d$  and for $K$-KME, we use the Gaussian kernel with bandwidth parameter $\sigma=1$. The embedded measures live in a high-dimensional Hilbert space (e.g. with $K$-LOT and $K=128$, the ambient space is $\mathbb{R}^{1280}$). In order to keep the most relevant information, we perform a 10-components PCA on the embedded data with respect to the proper Hilbert space $\mathcal{H}$. Then, we train two classifiers (one for each type of label) on 75\% of the data and test the results on the remaining data with LDA. We compare our method with random Fourier features (RFF) \cite{rahimi2007random}, which allow to efficiently map raw probability measures into the linear space $\mathbb{R}^s$, where $s$ is a parameters that has to be chosen. Note that RFF then approximate the KME. Accuracy scores are displayed in Table \ref{accuracy_pb_bm}. The quantization methods achieve in particular perfect accuracy scores for predicting the health care center (denoted as LAB) in only a few minutes. The KME computed with RFF reaches lower accuracy scores and is clearly slower than both quantization methods. Comparing the quantization methods, we observe that the classifier performs better on mean-measure quantization than on the quantization of each measure whereas the latter is roughly ten times slower than the former.

Additionally, we visualize the projections of the data on the first components of PCA in Figures \ref{fig:cyto_pca_lot} and \ref{fig:cyto_pca_kme} of the Appendix \ref{appendixC}. In these figures, it is clear that the representations stabilize when $K\geq 32$. This shows that our $K$-quantization step gives good results even for small values of $K$. We also observe that the mean-measure quantization and the quantization of each measure yield similar results.

\subsection{Earth image dataset}

\begin{table}[t]
\caption{\textbf{Earth image dataset.} LDA classification accuracy on the Airbus dataset after 10-component PCA on ${K}$-LOT and $K$-KME with both quantization methods.}
\label{airbus_accuracy}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule &\multicolumn{2}{c}{ $\overline{K}$-LOT/$\Tilde{K}$-LOT} & \multicolumn{2}{c}{ $\overline{K}$-KME/$\Tilde{K}$-KME}\\
$K$ & Accuracy & Time (s) & Accuracy & Time \\
\midrule
16    & 88/88 & 15/228  & 76/68 & 219/732\\
32    & 89/89 & 17/249  & 67/67 & 2792/2247\\
64    & 89/88 & 20/305  & 68/68 & 7958/9246\\
128   & 88/88 & 32/390  & 67/66 & 31765/32287 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


We perform similar experiments on a set of images provided by the Airbus company. The dataset consists in $N=1000$ images of size $128\times 128$ captured by a SPOT satellite. The images are divided into two categories : those with the presence of a wind turbine and those without, see Figures \ref{fig:wind_turbines} and \ref{fig:no_wind_turbines} in Appendix \ref{appendixC}. Each image is viewed as a discrete probability distribution on the RGB space, that is each pixel is represented by a point in $\mathbb{R}^3$. The size $N$ of the dataset as well as the number of pixels ($m_i  = 128^2$) prevents from directly computing either LOT or KME. We therefore carry out supervised classification from both embeddings and both quantization methods. For $K\in \{16,32,64,128\}$, we implement $K$-LOT with reference measure the uniform measure on $[0,1]^3$ sampled on $m_0=1000$ points, and $K$-KME with the Gaussian kernel with bandwidth parameter $\sigma = 100$. After the embeddings, we perform PCA and retain only the first 10 components, on which we train an LDA classifier using 75\% of the data and test it on the remaining 25\%. We obtain the accuracies and execution times displayed in Table \ref{airbus_accuracy}. Both quantization methods achieve similar accuracy results while mean-measure quantization is approximately ten times faster than the quantization of each measure.





\section{Conclusion}\label{sec:conclusion}

In this work, we have proposed to handle machine learning tasks of a set of probability distributions by leveraging two different $K$-quantization approaches that both approximate the input distributions at the asymptotic rate $O(K^{-2/d})$. We proved theoretically that mean-measure quantization and quantization of each measure allow the construction of scalable and consistent embeddings of the probability measures into Hilbert spaces, while numerical experiments highlight the efficiency and accuracy of the former.

%In this work, we have studied the effect of two quantization methods  (mean-measure quantization and quantization of each measure) on a set of large-scale probability measures. We have shown that both quantization procedures allow to approximate the input distributions at the rate $O(K^{-2/d})$ as $K$ goes to $+\infty$. This allows the scalable embeddings of the probability measures into Hilbert spaces where one can perform classical machine learning tasks. While theoretical results show that the quantization of each measure is a better approximation than the mean-measure quantization, our numerical experiments highlight the efficiency and accuracy of the latter.

\section{Acknowledgments}

This work benefited from financial support from the French government managed by the National Agency for Research (ANR) under the France 2030 program, with the reference ANR-23-PEIA-0004. The authors also gratefully acknowledge another financial support from the ANR
(MaSDOL grant ANR-19-CE23-0017).

\bibliography{example_paper}
\bibliographystyle{plain}

\appendix

\section{Proofs of the main results}\label{appendixA}

\begin{lemma}\label{lem:discrete}
Let $X = (x_1,\cdots,x_K) \in F_K$ the set of distinct points and consider the Vorono\"i partition
$$
 \widetilde{V}_{x_1} =  V_{x_1} \mbox{ and } \\
  \widetilde{V}_{x_k} =    V_{x_k} \backslash  \bigcup_{j < k} \widetilde{V}_{x_j} \quad  \mbox{ for } k \geq 2,  
$$
where we recall that
$$V_{x_k}  = \bigl\{ y\in\mathbb{R}^d ~|~ \forall  \ell \neq k, \|x_k-y\|^2 \leq  \|x_{\ell}-y\|^2 \bigr\}.$$
Then, for any probability measure $\mu \in \mathcal{P}(\mathcal{X})$, one has that

\begin{align*}
\min\limits_{a\in\Sigma_K}  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K a_k\delta_{x_k}\Bigr) &= W_2^2\Bigl(\mu,\sum\limits_{k=1}^K \mu(\widetilde{V}_{x_k})\delta_{x_k}\Bigr)\\ &= \sum_{k=1}^{K}   \int_{\widetilde{V}_{x_k}}    \|x_{k}-y\|^2 \mathrm{d} \mu(y)\\  &=  \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 \} \mathrm{d} \mu(y).
\end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lem:discrete}]
Let $X = (x_1,\cdots,x_K) \in F_K$. From the dual formulation of the Kantorovich problem (see e.g. \cite{villani2009optimal}), we have that, for  any $a  \in\Sigma_K$,
\begin{eqnarray}
W_2^2\Bigl(\mu,\sum\limits_{k=1}^K a_k\delta_{x_k}\Bigr) & = &  \sup\limits_{\beta \in \mathbb{R}^K} \sum\limits_{k=1}^K a_k \beta_k + \int_{\mathbb{R}^d} \left(\min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 - \beta_{k}\}\right) \mathrm{d} \mu(y) \nonumber \\
& \geq &  \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 \} \mathrm{d} \mu(y), \label{ineq:key}
\end{eqnarray}
where the above inequality is obtain by taking $ \beta_{k} = 0$ for all $1 \leq k \leq K$. Then, since $X = (x_1,\cdots,x_K) \in F_K$, one has  that $(\widetilde{V}_{x_k})_{1 \leq k \leq K}$ is a partition of $ \mathbb{R}^d$, and we may define 
$$
T_K(y) = \sum_{k=1}^{K} x_k \1_{\widetilde{V}_{x_k}}(y) \;  y \in \mathbb{R}^d,
$$
that is a mapping from $\mathbb{R}^d$ to $X$. Introducing the probability measure
$
\mu_K = \sum\limits_{k=1}^K \mu(\widetilde{V}_{x_k})\delta_{x_k},
$
it is not difficult to see that $T_{K\#} \mu = \mu_{K}$ where the notation $T _{\#} \mu$ denotes the push-forward of a measure $\mu$ by the mapping $T$. Now, we let $\pi_{K} = (\mathrm{id} \times T_K)_{\#} \mu$ that obviously belongs to the set of transport plans  $\Pi(\mu,\mu_K)$. From the definition of the Vorono\"i partition  $(\widetilde{V}_{x_k})_{1 \leq k \leq K}$, one can than check that, for any $y \in \mathbb{R}^d$, (see e.g.\ \cite{pages2015introduction})
$$
\|y - T_K(y)\|^2 =  \min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 \}.
$$
Consequently, we obtain the following equalities
$$
\int_{\mathcal{X}\times\mathcal{X}}  \|x - y\|^2\mathrm{d}\pi_{K}(x,y) =  \int_{\mathbb{R}^d} \|y - T_K(y) \|^2 \mathrm{d} \mu(y) =  \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 \} \mathrm{d} \mu(y).
$$
Inserting the above equality into \eqref{ineq:key}, we thus have that, for  any $a  \in\Sigma_K$,
$$
W_2^2(\mu,\sum\limits_{k=1}^K a_k\delta_{x_k}) \geq \int_{\mathcal{X}\times\mathcal{X}}  \|x - y\|^2\mathrm{d}\pi_{K}(x,y).
$$
Since $W_2^2\Bigl(\mu,\mu_{K}\Bigr) = \min\limits_{\pi \in \Pi(\mu,\mu_K)} \int_{\mathcal{X}\times\mathcal{X}} \|x - y\|^2\mathrm{d}\pi(x,y)$, we directly obtain from the above inequality that $W_2^2(\mu,\mu_{K}) = \int_{\mathcal{X}\times\mathcal{X}}  \|x - y\|^2\mathrm{d}\pi_{K}(x,y)$, which implies
$$
\min\limits_{a\in\Sigma_K}  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K a_k\delta_{x_k}\Bigr)  = W_2^2\Bigl(\mu,\mu_{K}\Bigr) =  \int_{\mathbb{R}^d} \|y - T_K(y) \|^2 \mathrm{d} \mu(y) =  \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 \} \mathrm{d} \mu(y),
$$
which concludes the proof.
\end{proof}



\begin{proof}[Proof of Proposition \ref{prop:pb_equivalence}]
Let $X \in F_K$. Applying Lemma \ref{lem:discrete}, we obtain that, for any $1 \leq i \leq N$,
$$
\min\limits_{a^{(i)} \in \Sigma_K} W_2^2\Bigl(\mu^{(i)}, \sum\limits_{k=1}^K a^{(i)}_k \delta_{x_k}\Bigr) =  W_2^2\Bigl(\mu,\sum\limits_{k=1}^K \mu^{(i)}(\widetilde{V}_{x_k})\delta_{x_k}\Bigr) = \sum_{k=1}^K \int_{\widetilde{V}_{x_k}}  \|x_k-y\|^2\mathrm{d}\mu^{(i)}(y) ,
$$
and thus we have that
\begin{eqnarray*}
\min\limits_{a\in(\Sigma_K)^N} \frac{1}{N}\sum\limits_{i=1}^N W_2^2\Bigl(\mu^{(i)}, \sum\limits_{k=1}^K a^{(i)}_k \delta_{x_k}\Bigr) & = &  \frac{1}{N} \sum_{i=1}^N W_2^2\Bigl(\mu,\sum\limits_{k=1}^K \mu^{(i)}(\widetilde{V}_{x_k})\delta_{x_k}\Bigr)  \\
& = & \frac{1}{N} \sum_{i=1}^N\sum_{k=1}^K \int_{\widetilde{V}_{x_k}}  \|x_k-y\|^2\mathrm{d}\mu^{(i)}(y) \\
& = & \sum\limits_{k=1}^K \int_{\widetilde{V}_{x_k}} \|x_k-y\|^2\mathrm{d}\overline{\mu}(y) \\
& = & W_2^2\Bigl(\overline{\mu},\sum\limits_{k=1}^K \overline{\mu}(\widetilde{V}_{x_k})\delta_{x_k}\Bigr)\\ &=&  \int_{\mathbb{R}^d}  \min_{1 \leq k \leq K} \{ \|x_{k}-y\|^2 \} \mathrm{d}  \overline{\mu}(y) ,
\end{eqnarray*}
where we again apply   Lemma \ref{lem:discrete} to derive the last equality above. Finally, from the assumption that the cardinality of the support of $\overline{\mu}$ is larger than $K$, we obtain from  \cite{JMLR:v21:18-804}[Proposition 2]  that
$$
\min\limits_{X\in F_K}   W_2^2\Bigl(\overline{\mu},\sum\limits_{k=1}^K \overline{\mu}(\widetilde{V}_{x_k})\delta_{x_k}\Bigr) = \min\limits_{X\in (\mathbb{R}^d)^K}   W_2^2\Bigl(\overline{\mu},\sum\limits_{k=1}^K \overline{\mu}(\widetilde{V}_{x_k})\delta_{x_k}\Bigr) 
$$
which concludes the proof.

%{\CB From the assumption that that the cardinality of the support of $\overline{\mu}$ is larger than $K$}
%{\RL Proposition \ref{prop:pb_equivalence} was proven in \cite{gachon2024low}[Proposition 1]. For the sake of completeness, we provide here an idea of the proof. We are interested in proving that :
%$$\min\limits_{a\in(\Sigma_K)^N, \ X\in(\mathbb{R}^d)^K} \frac{1}{N}\sum\limits_{i=1}^N W_2^2\Bigl(\mu^{(i)}, \sum\limits_{k=1}^K a^{(i)}_k \delta_{x_k}\Bigr)
%=  \min\limits_{X\in (\mathbb{R}^d)^K}   W_2^2\Bigl(\overline{\mu},\sum\limits_{k=1}^K \overline{\mu}(V_{x_k})\delta_{x_k}\Bigr),$$}
%From the dual formulation of the Kantorovich problem (see e.g. \cite{villani2009optimal}), we have
%$$\min\limits_{a\in(\Sigma_K)^N} Q(a,X)
%= \min\limits_{a\in(\Sigma_K)^N}\frac{1}{N} \sum\limits_{i=1}^N \sup\limits_{\beta^i \in \mathbb{R}^K} \sum\limits_{k=1}^K \left( a^i_k \beta^i_k + \int_{\mathbb{R}^d} \left(\min_{1 \leq k \leq K} \{ \|x_k-y\|^2 - \beta^i_k\}\right) \mathrm{d} \mu^{(i)}(y) \right).$$
%One can then apply the first order condition in $\beta$ and $a$, the fact that the Vorono\"i cells $(V_{x_k})_{1 \leq k \leq K}$ form a {\CB covering} of $\mathbb{R}^d$ and the absolute continuity of the $\mu_i$'s  to obtain that
%\begin{align*}
%    \min\limits_{a\in(\Sigma_K)^N} Q(a,X) &= \frac{1}{N}\sum_{i=1}^N W_2^2\left(\mu^{(i)},\sum_{i=1}^K \mu^{(i)}(V_{x_k})\delta_{x_k}\right) \\
%    &= \frac{1}{N} \sum_{i=1}^N\sum_{k=1}^K \int_{V_{x_k}}  \|x_k-y\|^2\mathrm{d}\mu^{(i)}(y)\\
%    &= \sum\limits_{k=1}^K \int_{V_{x_k}} \|x_k-y\|^2\mathrm{d}\bar{\mu}(y)\\
%    &= W_2^2\left(\sum\limits_{k=1}^K \overline{\mu}(V_{x_k}) \delta_{x_k}, \overline{\mu}\right),
%\end{align*}
%which concludes the proof.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:conv}]

For a fixed $N\geq 1$, since $\mathbb{P}^N$, $\overline{\mathbb{P}}^N_K$ and $\widetilde{\mathbb{P}}^N_K$ are discrete uniform measures of the same size, one can actually restrict the search of an optimal plan in \eqref{other_W2} as that of finding an optimal permutation $\sigma \in \mathrm{Perm}(N)$ see e.g. \cite{villani2009optimal} in the following sense:
\begin{align}
\mathcal{W}_2^2(\mathbb{P}^N,\overline{\mathbb{P}}^N_K) &= \min\limits_{\sigma \in \mathrm{Perm}(N)} \frac{1}{N}\sum\limits_{i=1}^N  W_2^2(\mu^{(i)},\overline{\nu}^{(\sigma(i))}_K) \nonumber\\ \mathcal{W}_2^2(\mathbb{P}^N,\widetilde{\mathbb{P}}^N_K) &= \min\limits_{\sigma \in \mathrm{Perm}(N)} \frac{1}{N}\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\tilde{\nu}^{(\sigma(i))}_K)\label{eq:permut}
\end{align}
However, for $\tilde{\nu}^{(i)}_K$, defined in \eqref{eq:nutilde}, it follows by the definition of optimal quantization of $\mu^{(i)}$ that, for any $1\leq j\leq N$,
\begin{equation}
W_2^2(\mu^{(i)},\tilde{\nu}^{(i)}_K) \leq  W_2^2(\mu^{(i)},\tilde{\nu}^{(j)}_K). \label{ineq:nutilde}
\end{equation}
Now for $\bar{\nu}^{(i)}_K$, defined in \eqref{eq:nubar}. Since $\bar{\nu}^{(i)}_K$  corresponds to the discrete probability measure supported on  $\bar{X}$ that best approximates $\mu^{(i)}$. In other words, $W_2^2(\mu^{(i)},\bar{\nu}^{(i)}_K)\leq W_2^2(\mu^{(i)},\sum_{k=1}^K a_k\delta_{\bar{x}_k})$ for any weight vector $a\in\Sigma_K$. In particular, we have that,  for any $1\leq j\leq N$, 
\begin{equation}
 W_2^2(\mu^{(i)},\bar{\nu}^{(i)}_K)\leq W_2^2(\mu^{(i)},\bar{\nu}^{(j)}_K). \label{ineq:nubar}
\end{equation}
Using Inequalities \eqref{ineq:nutilde} and \eqref{ineq:nubar}, it is then easy to see that in both cases, the optimal permutation minimizing \eqref{eq:permut} is the identity $\sigma(i) = i$ for all $1 \leq i \leq N$, and that we have
$$\mathcal{W}_2^2(\mathbb{P}^N,\overline{\mathbb{P}}^N_K) = \frac{1}{N}\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\overline{\nu}^{(i)}_K) \qquad \mathcal{W}_2^2(\mathbb{P}^N,\widetilde{\mathbb{P}}^N_K) = \frac{1}{N}\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\tilde{\nu}^{(i)}_K)$$
For $\tilde{\nu}^{(i)}_K$, one immediately has that $ \frac{1}{N}\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\tilde{\nu}^{(i)}_K) =  \frac{1}{N}\sum_{i=1}^N \varepsilon_K( \mu^{(i)})$. For  $ \bar{\nu}^{(i)}_K$, we obtain from Proposition \ref{prop:pb_equivalence}  that 
$$
  \frac{1}{N}\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\bar{\nu}^{(i)}_K) = W_2^2\left(\sum\limits_{k=1}^K \overline{\mu}(\tilde{V}_{\bar{x}_k})\delta_{\bar{x}_k}, \overline{\mu}\right) =  \varepsilon_K
$$
which concludes the proof.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:mmd}]
As $\mathrm{MMD}$ is an integral probability metric \cite{JMLR:v13:gretton12a}[Lemma 4], it is bounded by the $1$-Wasserstein distance $W_1$ defined by :
\begin{equation}\label{W1}
W_1(\rho,\mu) = \min\limits_{\pi \in \Pi(\rho,\mu)} \int_{\mathcal{X}\times\mathcal{X}} \|x-y\|\mathrm{d}\pi(x,y).    
\end{equation}
Since $W_1 \leq W_2$ \cite{santambrogio2015optimal}[Chapter 5], we thus have that $\mathrm{MMD} \leq W_2$. Therefore, 

\begin{equation}
\frac{1}{N}\sum_{i=1}^N\text{MMD}^2(\mu^{(i)},\nu^{(i)}_K) \leq \varepsilon_K  \overset{K\rightarrow \infty}{\longrightarrow} 0.
\end{equation}
\end{proof}

\begin{proof}[Proof of Proposition \ref{bary_conv}] For a fixed $N$, and thanks to Proposition \ref{prop:conv}, we have that the sequence of probability measures $(\mathbb{P}^N_K)_{K\geq 1}$ such that $\mathbb{P}^N_K= \frac{1}{N}\sum_{i=1}^N \delta_{\nu^{(i)}_K} \subset \mathcal{P}(\mathcal{P}(\mathcal{X}))$ converges towards $\mathbb{P}^N$, that is $\mathcal{W}_2(\mathbb{P}^N_K,\mathbb{P}_N)\overset{K\rightarrow \infty}{\longrightarrow} 0$. Additionally, the Wasserstein barycenter of $\mathbb{P}^N$ is unique (Proposition 3.5 in \cite{agueh2011barycenters}) since at least one of the probability measures $\mu^{(i)}, 1\leq i\leq N$ is a.c.\ by hypothesis. Therefore, using \cite{le2017existence}[Theorem 3], we immediately obtain that the sequence of barycenters of $(\mathbb{P}^N_K)_{K\geq 1}$ converges towards the barycenter of $\mathbb{P}^N$ in $W_2$ distance.

\end{proof}


\begin{proof}[Proof of Proposition \ref{prop:3}]
From the triangle inequality, one can write:
$$W_2(\nu^{(i)}_K,\nu^{(j)}_K) \leq W_2(\nu^{(i)}_K, \mu^{(i)})+W_2(\mu^{(i)},\mu^{(j)}) + W_2(\mu^{(j)},\nu^{(j)}_K).$$

From Young's inequality, $2ab\leq a^2+b^2$ and $2ab\leq \lambda a^2 + \frac{b^2}{\lambda}$ for any $\lambda>0$ and any real numbers $a,b$ and $c$. Then it holds that :

\begin{equation}\label{eq:young}
(a+b+c)^2 \leq (2+\lambda)a^2 + (2+\lambda)c^2 + \Bigl(1+\frac{2}{\lambda}\Bigr)c^2    
\end{equation}

Squaring the triangle inequality and using \eqref{eq:young}, this yields to:
\begin{equation}\label{eq:var1}
W_2^2(\nu^{(i)}_K,\nu^{(j)}_K)
\leq (2+\lambda)W_2^2(\nu^{(i)}_K, \mu^{(i)})+\Bigl(1+\frac{2}{\lambda}\Bigr)W_2^2(\mu^{(i)},\mu^{(j)}) + (2+\lambda)W_2^2(\mu^{(j)},\nu^{(j)}_K)\bigr)  
\end{equation}
We now sum inequality \eqref{eq:var1} over all $1\leq i,j \leq N$, and divide by $N^2$:
\begin{align*}
    \mathrm{SS}(\nu_K) &= \frac{1}{N^2} \sum\limits_{i=1}^N W_2^2(\nu^{(i)}_K,\mu^{(i)}) \leq  \frac{2}{N}(2+\lambda) \sum\limits_{i=1}^N W_2^2(\nu^{(i)}_K,\mu^{(i)}) +\frac{1}{N^2}\Bigl(1+\frac{2}{\lambda}\Bigr)\sum\limits_{i,j=1}^N W_2^2(\mu^{(i)},\mu^{(j)})
\end{align*}
Hence, by Proposition \ref{prop:conv}, we obtain that $ \mathrm{ SS }(\nu_K) \leq  (4+2\lambda) \varepsilon_K + \Bigl(1+\frac{2}{\lambda}\Bigr)\mathrm{SS}(\mu)$, which concludes the proof.
\end{proof}


\begin{proof}[Proof of Proposition \ref{prop:2}] 
We first recall that the dual formulation of OT between the discrete measures $\bar{\nu}^{(i)}_K$ and $\bar{\nu}^{(j)}_K$ (see e.g. \cite{peyre2019computational}) is given by
\begin{equation}\label{eq:dual}
W_2^2(\bar{\nu}^{(i)}_K,\bar{\nu}^{(j)}_K) = \max_{(\alpha,\beta)\in\Phi} \ \sum_{k=1}^K a_k^{(i)}\alpha_k + \sum_{k=1}^K a_k^{(j)}\beta_k
\end{equation}
where $\Phi := \{(\alpha,\beta)\in\mathbb{R}^K\times\mathbb{R}^K \ \mbox{such that for all}\ 1\leq k,l\leq K, \alpha_k+\beta_l\leq C_{kl}\}$.
Let $\alpha^{ij},\beta^{ij}\in\mathbb{R}^K$ be optimal Kantorovich potentials for $\bar{\nu}^{(i)}_K$ and $\bar{\nu}^{(j)}_K$ in \eqref{eq:dual}. We define the piecewise constant function $f^{ij}:\mathbb{R}^d \rightarrow \mathbb{R}$ such that $x\mapsto \alpha^{ij}_k$ when $x\in \overset{\circ}{V_k}$, where $\overset{\circ}{V_k}$ denotes the open interior of the Vorono\"i cell $V_k$. %{\CV [Il faut ici prendre en compte les points $x\in\mathbb{R}^d$ qui seraient sur une fronti‚Äö√†√∂¬¨√Üre des cellules de Voronoi.]}
Similarly, $g^{ij}: y \mapsto \beta^{ij}_k$ when $y\in V_k$. Then, thanks to the absolute continuity of the $\mu^{(i)}$'s, one can write:
\begin{align}
    W_2^2(\bar{\nu}^{(i)}_K,\bar{\nu}^{(j)}_K) &= \sum\limits_{k=1}^K a^{(i)}_k \alpha^{ij}_k + \sum\limits_{k=1}^K a^{(j)}_k \beta^{ij}_k\nonumber\\
    &= \sum\limits_{k=1}^K \int_{V_k} \mathrm{d}\mu^{(i)}(x) \alpha^{ij}_k +\sum\limits_{k=1}^K \int_{V_k} \mathrm{d}\mu^{(j)}(y) \beta^{ij}_k\nonumber\\
    &= \sum\limits_{k=1}^K \int_{\overset{\circ}{V_k}}  \alpha^{ij}_k \mathrm{d}\mu^{(i)}(x) +\sum\limits_{k=1}^K \int_{\overset{\circ}{V_k}}\beta^{ij}_k \mathrm{d}\mu^{(j)}(y)\nonumber\\
    &= \int_{\mathbb{R}^d}f^{ij}(x)\mathrm{d}\mu^{(i)}(x) + \int_{\mathbb{R}^d} g^{ij}(y) \mathrm{d}\mu^{(j)}(y)\label{eq:kantorovich_dual}
\end{align}

We then aim at identifying \eqref{eq:kantorovich_dual} with a dual formulation for OT between $\mu^{(i)}$ and $\mu^{(j)}$ with respect to some cost function $c:\mathbb{R}^d\times\mathbb{R}^d\to \mathbb{R}$ to be defined later on, where
\begin{align}
\text{OT}_c(\mu^{(i)},\mu^{(j)}) &= \min\limits_{\pi \in\Pi(\mu^{(i)},\mu^{(j)})}\int c(x,y)\mathrm{d}\pi(x,y)\nonumber\\ &= \sup\limits_{f,g : f(x)+g(y)\leq c(x,y)} \int_{\mathbb{R}^d} f(x)\mathrm{d}\mu^{(i)}(x)+\int_{\mathbb{R}^d} g(y)\mathrm{d}\mu^{(j)}(y).\label{eq:OTc}
\end{align}
If $x\in V_k$ and $y\in V_{k'}$, we obtain
\begin{align}
    f^{ij}(x)+g^{ij}(y) & = \alpha^{ij}_k + \beta^{ij}_{k'} \nonumber \\
    &\leq \|x_k-x_{k'}\|^2  \nonumber  \\
    &\leq 3\|x_k-x\|^2 + 3\|x-y\|^2 + 3\|y-x_{k'}\|^2  \nonumber  \\
    &\leq  3 \mathrm{diam}(V_k)+3\mathrm{diam}(V_{k'}) +  3\|x-y\|^2  \nonumber  \\
    &\leq  6\max\limits_{k}\mathrm{diam}(V_k) +  3\|x-y\|^2, \label{eq:bound_cost}
\end{align}
where the first inequality is due to the fact that $\alpha^{ij}_k$ and $\beta^{ij}_{k'}$ are Kantorovich potentials of $W_2(\bar{\nu}^{(i)}_K,\bar{\nu}^{(j)}_K)$ in \eqref{eq:dual}. The second inequality comes from \eqref{eq:young} where $\lambda=1$. Defining the new cost function $c(x,y) = 6\max\limits_{k}\mathrm{diam}(V_k) + 3\|x-y\|^2$, we thus define
\begin{align*}
    \text{OT}_c(\mu^{(i)},\mu^{(j)})
    &= 6\max\limits_k \mathrm{diam}(V_k) +  3\min\limits_{\pi\in\Pi(\mu^{(i)},\mu^{(j)})}\int \|x-y\|^2\mathrm{d}\pi(x,y)\\
    &=  6\max\limits_k \mathrm{diam}(V_k) +  3
    W_2^2(\mu^{(i)},\mu^{(j)}).
\end{align*}
Now, by inequality \eqref{eq:bound_cost}, it follows that $f^{ij}$ and $g^{ij}$ are feasible Kantorovich potentials of $\text{OT}_c$ between $\mu^{(i)}$ and $\mu^{(j)}$ in \eqref{eq:OTc}. Hence, we finally obtain from \eqref{eq:kantorovich_dual} that
\begin{align*}
    W_2^2(\bar{\nu}^{(i)}_K,\bar{\nu}^{(j)}_K) &\leq \int_{\mathbb{R}^d}f^{ij}(x)\mathrm{d}\mu^{(i)}(x) + \int_{\mathbb{R}^d} g^{ij}(y) \mathrm{d}\mu^{(j)}(y)\\
    &\leq \sup\limits_{f,g: f(x)+g(y)\leq c(x,y)} \int_{\mathbb{R}^d} f(x)\mathrm{d}\mu^{(i)}(x)+\int_{\mathbb{R}^d} g(y)\mathrm{d}\mu^{(j)}(y)\\
    &= 6\max\limits_k \mathrm{diam}(V_k) + 3 W_2^2(\mu^{(i)},\mu^{(j)}),
\end{align*}
which concludes the proof.
\end{proof}



\begin{proof}[Proof of Lemma \ref{lem:1}]
Suppose that the support of the mean measure $\bar{\mu}$ is included in $[0,1]^d$. Then, we first have that $\max\limits_{1\leq k \leq K} \mathrm{diam}(V_k) \leq \max\limits_{1\leq j \leq \lfloor \sqrt[d]{K} \rfloor^d} \mathrm{diam}(V_j)$. Indeed, as $\lfloor \sqrt[d]{K} \rfloor^d\leq K$, this is simply reducing the number of quantization points, and therefore increasing the maximum diameter of the cells. Now,  denoting $K'=\lfloor \sqrt[d]{K} \rfloor^d$ the $d$-th power of the integer $\lfloor \sqrt[d]{K} \rfloor$, one can grid the support space $[0,1]^d$ with $K'$ points $\{x_1,\ldots,x_{K'}\}$ set as $\Bigl\{\left(\frac{a^{(i)}_1}{\lfloor \sqrt[d]{K} \rfloor}, \cdots, \frac{a^{(i)}_d}{\lfloor \sqrt[d]{K} \rfloor}\right) ~|~ a^{(i)}_k\in\{1,\cdots,d\} \Bigr\}$. With these centers, all Vorono\"i cells have the same diameter, which is:
$$\forall 1\leq k \leq K,\ \mathrm{diam}(V_k) = \Bigl\|\left(\frac{1}{\lfloor \sqrt[d]{K} \rfloor}, \cdots, \frac{1}{\lfloor \sqrt[d]{K} \rfloor}\right)\Bigr\|^2 = \sum\limits_{i=1}^d \Bigl(\frac{1}{\lfloor \sqrt[d]{K} \rfloor}\Bigr)^2 = \frac{d}{\lfloor \sqrt[d]{K} \rfloor^2}.$$
This finally gives us:
$$\max\limits_{1\leq k \leq K} \mathrm{diam}(V_k) \leq \frac{d}{\lfloor \sqrt[d]{K} \rfloor^2}.$$
\end{proof}
\begin{proof}[Proof of Proposition \ref{prop:4}]
For the result regarding the within-class variance of the clusters, we have, using the triangle inequality and \eqref{eq:young} with $\lambda=1$,
\begin{align}
W_2^2(\nu^{(i)}_K,\nu^{(j)}_K) &\leq 3\bigl(W_2^2(\nu^{(i)}_K, \mu^{(i)})+W_2^2(\mu^{(i)},\mu^{(j)}) + W_2^2(\mu^{(j)},\nu^{(j)}_K)\bigr)
\end{align}
Summing over the indices of $I_l$ and dividing by $N_l^2$ yields:
\begin{align*}
\mathrm{WCSS}(l,\nu_K)
&\leq \frac{3}{N_l^2}\sum\limits_{i,j\in I_l}W_2^2(\nu^{(i)}_K, \mu^{(i)})+\frac{3}{N_l^2}\sum\limits_{i,j\in I_l} W_2^2(\mu^{(i)},\mu^{(j)}) + \frac{3}{N_l^2}\sum\limits_{i,j\in I_l}W_2^2(\mu^{(j)},\nu^{(j)}_K)\\
&\leq \frac{6}{N_l} \sum\limits_{i\in I_l} W_2^2(\nu^{(i)}_K,\mu^{(i)}) + 3\mathrm{ WCSS}(l,\mu)\\
&\leq \frac{6}{N_l} \sum\limits_{1\leq i\leq N} W_2^2(\nu^{(i)}_K,\mu^{(i)}) + 3\mathrm{WCSS}(l,\mu) = \frac{6N}{N_l}\varepsilon_K + 3\mathrm{WCSS}(l,\mu),
\end{align*}
where the last equality follows from  Proposition \ref{prop:conv},
which concludes the first item \eqref{intra_var_bound} of the proposition. For the second statement on the between-class variance, we rewrite the triangle inequality:
\begin{align*}
&\quad 3\bigl(W_2^2(\nu^{(i)}_K, \mu^{(i)})+W_2^2(\nu^{(i)}_K,\nu^{(j)}_K) + W_2^2(\mu^{(j)},\nu^{(j)}_K)\bigr) \geq  W_2^2(\mu^{(i)},\mu^{(j)}) \\
&\Leftrightarrow W_2^2(\nu^{(i)}_K,\nu^{(j)}_K) \geq \frac{1}{3}W_2^2(\mu^{(i)},\mu^{(j)}) - W_2^2(\nu^{(i)}_K, \mu^{(i)}) - W_2^2(\mu^{(j)},\nu^{(j)}_K)
\end{align*}
Summing over the indices of $I_{l_1}$ and $I_{l_2}$ and dividing by $N_{l_1}N_{l_2}$ gives:
\begin{align*}
    \mathrm{BCSS}(l_1, l_2, \nu_K) 
    &\geq \frac{1}{3}\frac{1}{N_{l_1} N_{l_2}}\sum\limits_{\substack{i_1\in I_{l_1} \\ i_2\in I_{l_2}}}W_2^2(\mu^{(i_1)},\mu^{(i_2)})\\
    &\qquad - \frac{1}{N_{l_1} N_{l_2}}\sum\limits_{\substack{i_1\in I_{l_1} \\ i_2\in I_{l_2}}}W_2^2(\nu^{(i_1)}_K, \mu^{(i_1)}) - \frac{1}{N_{l_1} N_{l_2}}\sum\limits_{\substack{i_1\in I_{l_1} \\ i_2\in I_{l_2}}}W_2^2(\mu^{(i_2)},\nu^{(i_2)}_K)\\
    &\geq \frac{1}{3}\mathrm{ BCSS}(l_1, l_2, \mu)
    - \frac{1}{N_{l_1}N_{l_2}} \sum\limits_{1\leq i_1,i_2\leq N} W_2^2(\nu^{(i_1)}_K,\mu^{(i_1)})\\ &\qquad - \frac{1}{N_{l_1}N_{l_2}} \sum\limits_{1\leq i_1,i_2\leq N} W_2^2(\nu^{(i_2)}_K,\mu^{(i_2)}) \\
    &= \frac{1}{3}\mathrm{ BCSS}(l_1, l_2, \mu) - \frac{1}{N_{l_1}} \sum\limits_{1\leq i_1\leq N} W_2^2(\nu^{(i_1)}_K,\mu^{(i_1)})\\ &\qquad - \frac{1}{N_{l_2}} \sum\limits_{1\leq i_2\leq N} W_2^2(\nu^{(i_2)}_K,\mu^{(i_2)}) \\
    &= \frac{1}{3}\mathrm{BCSS}(l_1, l_2, \mu) - \Bigl(\frac{1}{N_{l_1}}+\frac{1}{N_{l_2}}\Bigr) \sum\limits_{1\leq i\leq N} W_2^2(\nu^{(i)}_K,\mu^{(i)})\\
    &= \frac{1}{3}\mathrm{BCSS}(l_1, l_2, \mu) - \Bigl(\frac{N}{N_{l_1}}+\frac{N}{N_{l_2}}\Bigr) \varepsilon_K,
\end{align*}
where the last equality follows from  Proposition \ref{prop:conv}, which concludes the proof.
\end{proof}
%

The proof of Proposition \ref{th_ML} relies on the following Lemma which holds for any embedding $\phi$.

\begin{lemma}\label{Lemma_ML}
Let $\phi: \mathcal{P}(\mathcal{X})\rightarrow \mathcal{H}$ be an embedding of probability measures (supported on an arbitrary set included in  $\mathcal{X}\subset\mathbb{R}^d$) into a Hilbert space $\mathcal{H}$ equipped with the inner-product $\langle \cdot,\cdot\rangle_{\mathcal{H}}$ and the induced norm $\|\cdot\|_{\mathcal{H}}$. Suppose that $\|\phi\|_{\infty}= \sup\limits_{\mu\in\mathcal{P}(\mathcal{X})} \|\phi(\mu)\|_{\mathcal{H}}<\infty$. Then, we have that:
\begin{equation}\label{eq:gram_bound}
\|G^{\phi}_{\mu}-G^{\phi}_{\nu_K}\|_F^2\leq C\sum\limits_{i=1}^ N\|\phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\|^2_{\mathcal{H}} 
\end{equation}
where $C$ is a constant depending on $N$ and $\|\phi\|_{\infty}$.

\end{lemma}

\begin{proof}[Proof of Lemma \ref{Lemma_ML}]

First, let us write the Frobenius matrix norm of $G^{\phi}_{\mu}-G^{\phi}_{\nu_K}$:
\begin{equation}\label{eq:matrix_norm}
       \|G^{\phi}_{\mu}-G^{\phi}_{\nu_K}\|_F^2 = \sum\limits_{i,j=1}^N |(G^{\phi}_{\mu}-G^{\phi}_{\nu_K})_{ij}|^2 = \sum\limits_{i,j=1}^N |\langle \phi(\mu^{(i)}),\phi(\mu^{(j)})\rangle_{\mathcal{H}} -  \langle \phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}|^2
\end{equation}
Additionally, we have that:
\begin{align*}
    \langle \phi(\mu^{(i)}),\phi(\mu^{(j)})\rangle_{\mathcal{H}} & -  \langle \phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}=\langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\mu^{(j)})\rangle_{\mathcal{H}} + \langle \phi(\nu^{(i)}_K),\phi(\mu^{(j)})\rangle_{\mathcal{H}} \\
    &\qquad-  \langle \phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\\
    &=  \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\mu^{(j)}) -\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}} +  \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}} \\
    &\quad +\langle \phi(\nu^{(i)}_K),\phi(\mu^{(j)})\rangle_{\mathcal{H}} -  \langle \phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\\
    &=  \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\mu^{(j)}) -\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}} + \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\\
    &\quad + \langle \phi(\mu^{(j}))-\phi(\nu^{(j)}_K), \phi(\nu^{(i)}_K)\rangle_{\mathcal{H}}.
\end{align*}
Injecting this equality in \eqref{eq:matrix_norm} yields:
\begin{align}
    \|G^{\phi}_{\mu}-G^{\phi}_{\nu_K}\|_F^2 &= \sum\limits_{i,j=1}^N \Bigl| \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\mu^{(j)}) -\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\nonumber + \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\nonumber\\
    &\quad + \langle \phi(\mu^{(j}))-\phi(\nu^{(j)}_K), \phi(\nu^{(i)}_K)\rangle_{\mathcal{H}}\Bigr|^2\nonumber\\
    &\leq 3\sum\limits_{i,j=1}^N \Bigl| \langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\mu^{(j)}) -\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\Bigr|^2\nonumber\\
    &\quad  +3\sum\limits_{i,j=1}^N \Bigl|\langle \phi(\mu^{(i)})-\phi(\nu^{(i)}_K),\phi(\nu^{(j)}_K)\rangle_{\mathcal{H}}\Bigr|^2\nonumber\\
    &\quad + 3\sum\limits_{i,j=1}^N \Bigl| \langle \phi(\mu^{(j}))-\phi(\nu^{(j)}_K), \phi(\nu^{(i)}_K)\rangle_{\mathcal{H}}\Bigr|^2\nonumber\\
    &\leq 3\sum\limits_{i,j=1}^N \Bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\Bigr\|_{\mathcal{H}}^2 \Bigl\|\phi(\mu^{(j)}) -\phi(\nu^{(j)}_K)\Bigr\|_{\mathcal{H}}^2\label{eq:Cauchy_Schwarz}\\
    &\qquad  +3\sum\limits_{i,j=1}^N \Bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\Bigr\|^2_{\mathcal{H}}\Bigl\|\phi(\nu^{(j)}_K)\Bigr\|^2_{\mathcal{H}}\nonumber\\
    &\qquad + \sum\limits_{i,j=1}^N \Bigl\| \phi(\mu^{(j}))-\phi(\nu^{(j)}_K)\Bigr\|_{\mathcal{H}}^2\Bigl\| \phi(\nu^{(i)}_K)\Bigr\|_{\mathcal{H}}^2\nonumber\\
    &= 3\Bigl(\sum\limits_{i=1}^N \Bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\Bigr\|_{\mathcal{H}}^2\Bigr)^2  +6\sum\limits_{i,j=1}^N \Bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\Bigr\|^2_{\mathcal{H}}\Bigl\|\phi(\nu^{(j)}_K)\Bigr\|^2_{\mathcal{H}}\nonumber\\
    &\leq \sum\limits_{i=1}^N \bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\bigr\|^2_{\mathcal{H}}\Bigl(3 \sum\limits_{i=1}^N \bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\bigr\|^2_{\mathcal{H}}+6\sum\limits_{j=1}^N \bigl\|\phi(\nu^{(j)}_K)\bigr\|^2_{\mathcal{H}} \Bigr)\label{constant_to_define}
\end{align}
where \eqref{eq:Cauchy_Schwarz} is due to Cauchy-Schwarz. By hypothesis,  we have $\|\phi\|_{\infty}= \sup\limits_{\mu\in\mathcal{P}(\mathcal{X})} \|\phi(\mu)\|_{\mathcal{H}}= M_{\phi}<\infty$. Therefore, \eqref{constant_to_define} yields:
\begin{align*}
     \|G^{\phi}_{\mu}-G^{\phi}_{\nu_K}\|_F^2 &\leq \sum\limits_{i=1}^N \bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\bigr\|^2_{\mathcal{H}}\Bigl(3 \sum\limits_{i=1}^NM_{\phi}^2+6\sum\limits_{j=1}^N M_{\phi}^2 \Bigr)\\
     & = 9NM_{\phi}^2\sum\limits_{i=1}^N \bigl\| \phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\bigr\|^2_{\mathcal{H}}. 
\end{align*}
Choosing $C:=9NM_{\phi}^2$ concludes the proof.

\end{proof}

\begin{proof}[Proof of Proposition \ref{th_ML}]

(ii) We first prove inequality \eqref{eq:KME_bound} for the KME embedding $\phi:\sigma\in\mathcal{P}(\mathcal{X})\mapsto \int k(x,\cdot)\mathrm{d}\sigma(x)\in\mathcal{H}$. By hypothesis, the kernel $k$ is bounded by a constant $M_k$. Therefore for any $\mu\in\mathcal{P}(\mathcal{X})$, we have $\|\phi(\mu)\|_{\mathcal{H}}^2 = \langle \int k(x, \cdot)\mathrm{d}\mu(x), \int k(y,\cdot)\mathrm{d}\mu(y)\rangle_{\mathcal{H}} = \iint  k(x,y) \mathrm{d}\mu(x)\mathrm{d}\mu(y) \leq M_k$. By Lemma \ref{Lemma_ML}, we then have
$$\|G^{\phi}_{\mu}-G^{\phi}_{\nu_K}\|_F^2\leq C_{N,k}\sum\limits_{i=1}^ N\|\phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\|^2_{\mathcal{H}} $$
with $C_{N,k} := 9NM_k$. Using the property $\mathrm{MMD}\leq W_1\leq W_2$, % {\CV[Donner ref ou expliquer pourquoi on a ces in‚Äö√†√∂¬¨¬©galit‚Äö√†√∂¬¨¬©s.]}
we obtain that
\begin{align*}
    \frac{1}{N}\|G_{\mu}^{\mathrm{KME}}-G_{\nu_K}^{\mathrm{KME}}\|_F^2 &\leq \frac{C_{N,k}}{N}\sum\limits_{i=1}^N \|\phi(\mu^{(i)})-\phi(\nu^{(i)}_K)\|^2_{\mathcal{H}} = \frac{C_{N,k}}{N}\sum\limits_{i=1}^N \mathrm{MMD}^2(\mu^{(i)},\nu^{(i)}_K)\\
    &\leq \frac{C_{N,k}}{N}\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\nu^{(i)}_K)\\
    &= C_{N,k}\ \varepsilon_K.
\end{align*}

(i) In order to prove inequality \eqref{eq:LOT_bound} for the LOT embedding $\phi:\mu\in\mathcal{P}(\mathcal{X})\mapsto T_{\rho}^{\mu}-\mathrm{id}\in L^2(\rho)$ with a.c. reference measure $\rho$, we first note that for any measure $\mu\in\mathcal{P}(\mathcal{X})$, $\|\phi(\mu)\|^2_{L^2(\rho)} = \int |T_{\rho}^{\mu}(x)-x|^2\mathrm{d}\rho(x) \leq \int \mathrm{diam}(\mathcal{X})^2\mathrm{d}\rho(x) = \mathrm{diam}(\mathcal{X})^2$. Then we can apply Lemma \ref{Lemma_ML} with constant $C:=C_{N,\mathcal{X}} = 9N\mathrm{diam}(\mathcal{X})^2$ in \eqref{eq:gram_bound} and finally obtain
\begin{align}
    \frac{1}{N}\|G_{\mu}^{\mathrm{LOT}}-G_{\nu_K}^{\mathrm{LOT}}\|_F^2 &\leq \frac{C_{N,\mathcal{X}}}{N}\sum\limits_{i=1}^N \|T_{\rho}^{\mu^{(i)}}-T_{\rho}^{\nu^{(i)}_K}\|_{L^2(\rho)}^2.\label{eq:K_LOT}
\end{align}

We now use a result due to Ambrosio and reported in \cite{gigli2011holder} and \cite{delalande2023quantitative}[Theorem (Ambrosio)], which states that when $\rho$ is a probability density over a compact set $\mathcal{X}$, $\mu$ and $\nu$ are probability measures on a $\mathcal{X}$ and $T_{\rho}^{\mu}$ is $L$-Lipschitz (by hypothesis), then:

\begin{equation}\label{ambrosio}
    \|T_{\rho}^{\mu}-T_{\rho}^{\nu}\|_{L^2(\rho)} \leq 2\sqrt{\mathrm{diam}( \mathcal{X})L}W_1(\mu,\nu)^{1/2}.
\end{equation}

Finally, assembling the inequalities \eqref{eq:K_LOT} and \eqref{ambrosio}, we obtain
\begin{align*}
    \frac{1}{N}\|G_{\mu}^{\mathrm{LOT}}-G_{\nu_K}^{\mathrm{LOT}}\|_F^2 &\leq \frac{C_{N,\mathcal{X}}}{N}\sum\limits_{i=1}^N 4\mathrm{diam}(\mathcal{X})LW_1(\mu^{(i)},\nu^{(i)}_K)\\
    &\leq  4C_{N,\mathcal{X}}\mathrm{diam}(\mathcal{X})L\ \frac{1}{N}\sum\limits_{i=1}^N W_2(\mu^{(i)},\nu^{(i)}_K)\\
    &= 4C_{N,\mathcal{X}}\mathrm{diam}(\mathcal{X})L \frac{1}{N}\sqrt{\Bigl(\sum\limits_{i=1}^N W_2(\mu^{(i)},\nu^{(i)}_K) \Bigr)^2}\\
    &\leq 4C_{N,\mathcal{X}}\mathrm{diam}(\mathcal{X})L \frac{1}{N}\sqrt{N\sum\limits_{i=1}^N W_2^2(\mu^{(i)},\nu^{(i)}_K)}\\
    &= 4C_{N,\mathcal{X}}\mathrm{diam}( \mathcal{X})L \frac{1}{N}\sqrt{N\cdot N\varepsilon_{K}(\overline{\mu})}\\
    &= 4C_{N,\mathcal{X}}\mathrm{diam}( \mathcal{X})L\cdot \sqrt{\varepsilon_{K}(\overline{\mu})}\\
    &= C_{N,\mathcal{X},L}\sqrt{\varepsilon_{K}(\overline{\mu})},
\end{align*}
which concludes the proof.
\end{proof}



\section{Link between the diagonalization of the covariance operator and the Gram matrix of inner-products}\label{appendixB}

In this section, we show that diagonalizing the Gram matrix of inner-products is closely related to diagonalizing the covariance operator in a Hilbert space.
%as proven in {\CV [ref]} for the finite dimensional case.
Suppose we have elements $f_1,\ldots,f_N$ belonging to a separable Hilbert space $\mathcal{H}$, endowed with the inner-product $\langle \cdot,\cdot\rangle_{\mathcal{H}}$. The covariance operator $\Sigma$ of the data is defined as:
$$\forall h\in\mathcal{H}, \qquad \Sigma (h) = \frac{1}{N}\sum\limits_{i=1}^n f_i \langle f_i,h\rangle_{\mathcal{H}}.$$
We recall that $h\in\mathcal{H}$ is an eigenvector of $\Sigma$  associated to the eigenvalue   $\lambda\in\mathbb{R}$ if
\begin{equation}\label{eq:eig_cov}
\Sigma(h)=\frac{1}{N}\sum\limits_{i=1}^n f_i \langle f_i,h\rangle_{\mathcal{H}} = \lambda h    
\end{equation}
Dividing by $\lambda$ in \eqref{eq:eig_cov}, one obtains:
\begin{equation}\label{eq:eig_cov2}
h = \sum\limits_{i=1}^N \frac{1}{N\lambda} f_i \langle f_i,h\rangle_{\mathcal{H}} = \sum\limits_{i=1}^N a_i f_i,    
\end{equation}
where $a_i = \frac{1}{N\lambda}\langle f_i,h\rangle_{\mathcal{H}}\in\mathbb{R}$. Injecting \eqref{eq:eig_cov2} in \eqref{eq:eig_cov} yields:
\begin{equation}\label{eq:eig_cov3}
\sum\limits_{i=1}^N f_i \langle f_i,\sum\limits_{j=1}^N a_j f_j\rangle_{\mathcal{H}} = N\lambda \sum\limits_{i=1}^N a_i f_i    
\end{equation}
Taking the inner-product of \eqref{eq:eig_cov3} in with $f_l$ yields:
\begin{align*}
    \langle f_l, \sum\limits_{i=1}^N f_i \langle f_i,\sum\limits_{j=1}^N a_j f_j\rangle_{\mathcal{H}}\rangle_{\mathcal{H}} &= \langle f_l, N\lambda \sum\limits_{i=1}^N a_i f_i \rangle_{\mathcal{H}}\\
    \Leftrightarrow \sum\limits_{i=1}^N \langle f_l,f_i\sum\limits_{j=1}^N a_j \langle f_i,f_j\rangle_{\mathcal{H}} \rangle_{\mathcal{H}} &= N\lambda \sum\limits_{i=1}^N \langle f_l, f_i\rangle_{\mathcal{H}}\\
    \Leftrightarrow \sum\limits_{i,j=1}^N a_j \langle f_i,f_j\rangle_{\mathcal{H}} \langle f_l,f_i\rangle_{\mathcal{H}} &= N\lambda \sum\limits_{i=1}^N a_i \langle f_l,f_i\rangle_{\mathcal{H}}.
\end{align*}

If we note $K$ the $N\times N$ Gram matrix of inner-products, that is $G_{ij}=\langle f_i,f_j\rangle_{\mathcal{H}}$, and $a=(a_1,\cdots,a_N)^T\in\mathbb{R}^N$, then we can rewrite the previous inequality with matrix notations:
\begin{equation}\label{eq:eig_matrix}
K^2a=N\lambda K a
\end{equation}

and $a$ can be found solving

\begin{equation}\label{eq:eig_a}
Ka = N\lambda a    
\end{equation}

or equivalently by the diagonalization of the Gram matrix $K$. From $a$, one can recover the eigenelement $h$ with \eqref{eq:eig_cov2}.

\section{Additional numerical experiments} \label{appendixC}

\subsection{Synthetic dataset : an explicit formulation for the pairwise inner-products in the Gram matrix}

\begin{proposition}\label{2D_gaussian_dotprod}
Let $\rho$ be an a.c.\ probability measure with compact support, defined for $X\sim\rho$ by $X = R\frac{Z}{\|Z\|}$, where $R\sim \mathrm{Unif}([0,1])$ and $Z\sim\mathcal{N}(0,I_d)$ are independent random variable. Let $(\mu^{(i)})_{i=1}^N$ be $N$ distributions defined by
$$
    \mu^{(i)} = (\Sigma_i^{1/2}\mathrm{id} + b_i)_{\#}\rho,
$$
where $\Sigma_i\in\mathbb{R}^{d\times d}$ is a positive semi-definite matrix and $b_i\in\mathbb{R}^d$. 

(i) For the LOT embedding, we choose $\rho$ as the reference measure, for which $\phi(\mu^{(i)})= T_{ \rho}^{\mu^{(i)}}-\mathrm{id}$. Then one has $\forall 1\leq i,j\leq N$,
\begin{equation*}
\langle \phi(\mu^{(i)}),\phi(\mu^{(j)}) \rangle_{L^2(\rho)} = \langle b_i,b_j\rangle+\frac{1}{3d} \langle\Sigma_i^{1/2}-I_d,\Sigma_j^{1/2}-I_d\rangle_F.
\end{equation*}

(ii) For the KME, we consider the specific kernel $k : \mathbb{R}^d \times  \mathbb{R}^d \to  \mathbb{R}$ given by    $k(x,y) = x^Ty+(x^Ty)^2$, for which $\phi(\mu^{(i)})=\int k(x,\cdot)\mathrm{d}\mu^{(i)}(x)$. Then one has $\forall 1\leq i,j\leq N$,
    \begin{equation*}
        \langle \phi(\mu^{(i)}),\phi(\mu^{(j)}) \rangle_{\mathcal{H}} = \langle b_i,b_j\rangle +  \langle b_i,b_j\rangle^2
         + \frac{1}{3d} \langle \Sigma_j, b_ib_i^T\rangle_F 
        + \frac{1}{3d}\langle \Sigma_i, b_jb_j^T\rangle_F + \frac{1}{9d^2}\langle \Sigma_i,\Sigma_j\rangle_F .
    \end{equation*}
\end{proposition}

\begin{proof}[Proof of Proposition \ref{2D_gaussian_dotprod}]

First, we have $\mathbb{E}[X] = \mathbb{E}[R]\cdot \mathbb{E}\left[\frac{Z}{\|Z\|}\right] = \frac{1}{2}\cdot 0 = 0$. Then we also have that $\mathrm{Cov}[X] = \mathbb{E}[XX^T] = \mathbb{E}[R^2]\cdot \mathbb{E}\left[\frac{ZZ^T}{\|Z\|^2}\right] = \frac{1}{3}\cdot \frac{1}{d}I_d.$

(i) For LOT, we have
\begin{align*}
    \langle\phi(\mu^{(i)}),\phi(\mu^{(j)})\rangle_{L^2(\rho)} &= \langle T_{\rho}^{\mu^{(i)}}-\mathrm{id},T_{ \rho}^{\mu^{(j)}}-\mathrm{id} \rangle_{L^2( \rho)}\\
    &= \int \langle T_{\rho}^{\mu^{(i)}}(x)-x, T_{ \rho}^{\mu^{(j)}}(x)-x\rangle \mathrm{d}\rho(x).
\end{align*}
Moreover, the optimal transport map between $\rho$ and $\mu^{(i)}$ is made explicit in \eqref{eq:shift_scaling}. Note that it is optimal in the sense of \eqref{eq:Monge} since $x\mapsto \Sigma_i^{1/2}x+b_i$ is the gradient of a convex function, and Brenier's theorem \cite{brenier1991polar} allows to conclude. Then
\begin{align*}
    \langle T_{\rho}^{\mu^{(i)}}-\mathrm{id},T_{\rho}^{\mu^{(j)}}-\mathrm{id} \rangle_{L^2(\rho)} &= \int  \langle \Sigma_i^{1/2}x+b_i-x, \Sigma_j^{1/2}x+b_j-x\rangle \mathrm{d}\rho(x).
\end{align*}
Let us write $C_i = \Sigma_i^{1/2}-I_d$ for simplicity of notation, then since $\rho$ is centered:
\begin{align*}
    \langle T_{\rho}^{\mu^{(i)}}-\mathrm{id},T_{\rho}^{\mu^{(j)}} -\mathrm{id} \rangle_{L^2(\rho)} &= \int  \langle C_ix+b_i, C_jx+b_j\rangle \mathrm{d}\rho(x)\\
    &= \int  (C_ix)^TC_jx \mathrm{d}\rho(x)+ \int b_i^Tb_j \mathrm{d}\rho(x)\\ &\quad +  \int (C_ix)^Tb_j \mathrm{d}\rho(x) + \int (C_jx)^T b_i  \mathrm{d}\rho(x)\\
    &= b_i^Tb_j + \int x^T C_iC_jx\mathrm{d}\rho(x).
\end{align*}
Finally, using that $\mathbb{E}[XX^T] = \frac{1}{3d}I_d$,
\begin{align*}
    \int x^T C_iC_jx\mathrm{d}\rho(x) &= \int \mathrm{Tr}\bigl(x^T C_iC_jx\bigr)\mathrm{d}\rho(x)\\
    &= \mathrm{Tr}\Bigl(\int  C_iC_jxx^T\mathrm{d}\rho(x)\Bigr)\\
    &=\mathrm{Tr}\Bigl(C_iC_j\int  xx^T\mathrm{d}\rho(x)\Bigr)\\
     &=\mathrm{Tr}\left(C_iC_j \frac{1}{3d}I_d\right)\\
    &= \frac{1}{3d}\mathrm{Tr}\bigl(C_iC_j\bigr).
\end{align*}
And we get
\begin{equation}
     \langle T_{\rho}^{\mu^{(i)}}-\mathrm{id},T_{\rho}^{\mu^{(j)}} -\mathrm{id}\rangle_{L^2(\rho)} = \langle b_i,b_j\rangle +  \frac{1}{3d}\langle \Sigma_i^{1/2}-I_2,\Sigma_j^{1/2}-I_2\rangle_F.
\end{equation}

(ii) For the KME embedding, 
\begin{align*}
    \langle \phi(\mu^{(i)}),\phi(\mu^{(j)})\rangle_{\mathcal{H}} &= \left\langle \int k(x,\cdot)\mathrm{d}\mu^{(i)}(x),\int k(y,\cdot)\mathrm{d}\mu^{(j)}(y)\right\rangle_{\mathcal{H}}\\
    &= \iint \langle k(x,\cdot),k(y,\cdot)\rangle_{\mathcal{H}}\ \mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= \iint k(x,y)\mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= \iint \left[x^Ty+(x^Ty)^2\right]\mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= \iint \left[x^Ty+x^Tyy^Tx\right] \mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= b_i^Tb_j+ \iint x^Tyy^Tx \mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= b_i^Tb_j+ \iint \mathrm{Tr}\bigl( x^T yy^Tx\bigr)\mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= b_i^Tb_j+ \iint \mathrm{Tr}\bigl( yy^Txx^T\bigr)\mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\\
    &= b_i^Tb_j+ \mathrm{Tr}\bigl(\iint  yy^Txx^T\mathrm{d}\mu^{(i)}(x)\mathrm{d}\mu^{(j)}(y)\bigr)\\
    &= b_i^Tb_j+ \mathrm{Tr}\Bigl(\int  yy^T\mathrm{d}\mu^{(j)}(y)\int xx^T\mathrm{d}\mu^{(i)}(x)\Bigr)
\end{align*}

Now, since $\mu^{(i)} = (\Sigma_i^{1/2}\mathrm{id}+b_i)_{\#}\rho$, we have that :
\begin{align*}
    \int xx^T\mathrm{d}\mu^{(i)}(x) &= \int \bigl( \Sigma_i^{1/2}x+b_i\bigr)\bigl( \Sigma_i^{(1/2}x+b_i\bigr)^T \mathrm{d}\rho(x)\\
    &= \int \bigl(\Sigma_i^{1/2} xx^T\Sigma_i^{1/2} + \Sigma_i^{1/2}xb_i^T +b_ix^T\Sigma_i^{1/2} + b_ib_i^T \bigr)\mathrm{d}\rho(x)\\
    &= \Sigma_i^{1/2}\int xx^T\mathrm{d}\rho(x)\Sigma_i^{1/2} + b_ib_i^T\\
    &= \frac{1}{3d}\Sigma_i + b_ib_i^T.
\end{align*}

This gives : 
\begin{align*}
    \langle \phi(\mu^{(i)}),\phi(\mu^{(j)})\rangle_{\mathcal{H}} &= b_i^Tb_j + \mathrm{Tr}\Bigl(\bigl(\frac{1}{3d}\Sigma_i+b_ib_i^T\bigr)\bigl(\frac{1}{3d}\Sigma_j+b_jb_j^T \bigr)\Bigr)\\    
    &= b_i^Tb_j + \mathrm{Tr}\Bigl( \frac{1}{9d^2} \Sigma_i\Sigma_j + \frac{1}{3d}\Sigma_ib_jb_j^T + \frac{1}{3d}\Sigma_jb_ib_i^T + b_ib_i^Tb_jb_j^T\Bigr)\\
    &= \langle b_i,b_j\rangle +  \langle b_i,b_j\rangle^2+ \frac{1}{3d} \langle \Sigma_j, b_ib_i^T\rangle_F + \frac{1}{3d}\langle \Sigma_i, b_jb_j^T\rangle_F+ \frac{1}{9d^2}\langle \Sigma_i,\Sigma_j\rangle_F
\end{align*}
\end{proof}

Figure \ref{fig:kme_pca_viz} depicts the first components of PCA after KME on both quantization steps. As for LOT embedding, we see that for $K$ as small as 32, the PCA visualizations with the two quantization methods look highly similar to the true PCA.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{images/kme_pca_viz.png}}
\caption{\textbf{Synthetic dataset on shifts and scalings.} Projection of the data onto the first two components of PCA after $\overline{K}$-KME (top) and $\Tilde{K}$-KME (bottom) and comparison to the KME PCA (right) computed from the true Gram matrix (see Prop.\ref{2D_gaussian_dotprod}.}
\label{fig:kme_pca_viz}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Visualization of the flow cytometry dataset}
In this section, we show in Figures \ref{fig:cyto_pca_lot} and \ref{fig:cyto_pca_kme} the projections of the embedded data (by either $K$-LOT or $K$-KME) on the first components of PCA. We see that the first two components already allow to discriminate the flow cytometry measurements according to their labels.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{images/cyto_pca_lot.png}}
\caption{\textbf{Flow cytometry dataset.} Projection of the $N=108$ measures on the first components of PCA after embedding with $K$-LOT.}
\label{fig:cyto_pca_lot}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{images/cyto_pca_kme.png}}
\caption{\textbf{Flow cytometry dataset.} Projection of the $N=108$ measures on the first components of PCA after embedding with $K$-KME with RBF kernel with $\sigma = 1$.}
\label{fig:cyto_pca_kme}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Earth image dataset}

We display in Figure \ref{fig:airbus} a few samples of the earth image dataset.

\begin{figure}
\centering
  \begin{subfigure}[b]{0.4\columnwidth}
    \includegraphics[width=\linewidth]{images/images_with_turbine.png}
    \caption{With wind turbine}
    \label{fig:wind_turbines}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\columnwidth}
    \includegraphics[width=\linewidth]{images/images_without_turbine.png}
    \caption{Without wind turbines}
    \label{fig:no_wind_turbines}
  \end{subfigure}
  \caption{\textbf{Earth image dataset.} Examples of images sampled from the Airbus dataset.}\label{fig:airbus}
\end{figure}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
