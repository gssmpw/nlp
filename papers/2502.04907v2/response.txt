\section{Related works}
\label{sec:works}
 
 In **Kolouri et al., "Sliced Wasserstein Autoencoder"**,  quantization is employed to embed a set of $N$ probability measures into a finite-dimensional Euclidean space through measure vectorization. More precisely, given $N$ input measures $\mu^{(i)}$, a quantization of the mean measure $\bar{\mu} = \frac{1}{N}\sum_{i=1}^n \mu^{(i)}$ by $K$ centers $x_1,\ldots,x_K$  in  $\mathbb{R}^d$ is first done. Then, they map each measure $\mu^{(i)}$ to $v^{(i)}= (v^{(i)}_1,\cdots, v^{(i)}_K)$ a vector of the convex space $\mathbb{R}_{+}^K$, where $v^{(i)}_k$ roughly represents the mass spread from the measure $\mu^{(i)}$ around the center $x_k$.  %Hence, the resulting vectorization method is  an embedding of the $\mu^{(i)}$'s into the convex space $\mathbb{R}_{+}^K$.
 Yet, this embedding does not take into account the relative positions of the $K$ centers, and consistency in the sense \eqref{eq:convW2} is not shown as we propose in this paper by endowing the set of quantized measures $\nu^{(i)}_K$  with the Wasserstein distance \eqref{W2}.

 In **Bonnefoy et al., "Kernel Maximum Mean Discrepancy"**, the authors tackle the problem of computing the KME of a probability distribution $\mu$ for which $m$ samples $X_1,\ldots,X_m$  are available. They introduce an estimator of the KME of $\mu$ based on Nystr\"om approximation that can be computed efficiently using a small random subset  from the data. Their theoretical and empirical results show that this approach yields a consistent estimator of the maximum mean discrepancy distance between the KMEs of $\mu$ and $\hat{\mu}_{m} = \frac{1}{m} \sum_{j= 1}^{m} \delta_{X_j}$ at a low computational cost. However, this Nystr\"om approximation has not been studied for constructing a consistent LOT embedding  estimator.

Finally, the benefits of a preliminary quantization step have been studied in **Genevay et al., "Sample Complexity of Sinkhorn Divergence"** to improve  the standard estimator of the OT cost between two probability measures based on the plug-in of their empirical counterpart. Still, the simultaneous quantization of $N$ probability measures for the purpose of constructing consistent and scalable embeddings has not been considered so far.