\section{Related Work} \label{sec: related}

\textbf{\small Data Curation and Selection.} The effectiveness of language models heavily depends on the quality of the pre-training corpus. Consequently, significant efforts have been made to enhance pre-training data. These efforts include heuristic-based filtering~\citep{raffel2020exploring, rae2021scaling, laurenccon2022bigscience, penedo2023refinedweb, soldaini2024dolma} and deduplication~\citep{abbas2023semdedup, lee2021deduplicating, chowdhery2022palm, dubey2024llama}. Recently, \cite{vo2024automatic} proposed an automated method for constructing large, diverse, and balanced datasets for self-supervised learning by applying hierarchical k-means clustering. \cite{sachdeva2024train} introduced techniques that leverage instruction-tuned models to assess and select high-quality training examples, along with density sampling to ensure diverse data coverage by modeling the data distribution. Additionally, \cite{guu2023simfluence} simulated training runs to model the non-additive effects of individual training examples, enabling the analysis of their influence on a model's predictions.

\textbf{\small Multitask Learning Optimization}  
The approach most closely related to our method is multitask learning (MTL) optimization, which modifies gradient updates to mitigate gradient conflicts—situations where task gradients point in opposing directions, slowing down optimization~\citep{vandenhende2021multi, yu2020gradient}. The Multiple Gradient Descent Algorithm (MGDA)~\citep{desideri2012multiple, sener2018multi} updates the model by optimizing the worst improvement across all tasks, aiming for equal descent in task losses. Projected Gradient Descent (PCGrad)~\citep{yu2020gradient} modifies task gradients by iteratively removing conflicting components in a randomized order, ensuring that updates do not interfere destructively across tasks. Conflict-Averse Gradient Descent (CAGRAD)~\citep{liu2021conflict} optimizes for the worst task improvement while ensuring a decrease in the average loss. NASHMTL~\citep{navon2022multi} determines gradient directions by solving a bargaining game that maximizes the sum of log utility functions. While these methods improve performance, they introduce significant computational and memory overhead, making them impractical for large-scale models with numerous tasks~\citep{xin2022current}. Similar challenges exist in AdaTask~\citep{yang2023adatask}, which improves multitask learning by balancing parameter updates using task-wise adaptive learning rates, mitigating task dominance, and enhancing overall performance. Unlike previous approches that requires  requiring \(O(K)\) storage for task gradients (e.g. PCGrad) or optimizer states (e.g. AdaTask), FAMO~\citep{liu2024famo} balances task loss reductions efficiently using \(O(1)\) space and time. However, these methods fail to exploit the~\textit{non-conflicting} interactions among tasks, focusing instead on resolving conflicts that seldom arise. This highlights the need for a new approach that actively leverages lack of gradient conflicts to enhance training efficiency. 

Another line of work focuses on adjusting the domain mixture to improve data efficiency during training~\citep{xie2024doremi, xia2023sheared, jiang2024adaptive}. However, these methods require a target loss for optimization, which has been shown to not always correlate with downstream performance~\citep{tay2021scale, liu2023same, wettig2024qurating}. In contrast, our method leverages the absence of gradient conflict and the presence of positive gradient interactions between tasks or domains. This approach provides a more reliable and effective way to enhance the final model's performance.


\section{PiKE: Conceptual Version}
\label{sec:ConceptualPiKE}
Here, we present the conceptual (basic) version of PiKE. As discussed in the main text, this approach lacks computational efficiency due to the frequent estimation of the norm and the variance of the per-task gradient.

\begin{algorithm}[H] 
    \begin{algorithmic}[1]
        \STATE {{\bfseries Input:} $\ttheta$, total batch size $b$, stepsize $\eta$, task $k$ dataset $\cD_k$, constants $\beta$, $L$, $\gamma$, and prior weights~$\bw'$ } 
	\STATE {{\bfseries Initialize:} $w_k \gets 1/K$ or $w_k \gets w_k', \forall k$ } 
		\FOR{$t=0,1, \dots$}
                \STATE Estimate $\|\nabla \cL_k(\ttheta_t)\|^2$ and $\sigma_k^2$ for every $k$
                \STATE Compute $\lambda_k\triangleq -\eta\beta\|\nabla\cL_k(\ttheta_t)\|^2 + \frac{L\eta^2}{2b} \sigma_k^2$ and $\kappa_k\triangleq L\eta^2\gamma\|\nabla\cL_k(\ttheta_t)\|^2$
                \STATE set $w_k^* = \max\{0, -\frac{\mu + \lambda_k}{\kappa_k}\}$ where $\mu$ is found (by bisection) such that $\sum_{k=1}^K w_k^* = 1$
                \STATE Set $(b_1,\ldots, b_K) \gets \textrm{round}(b (w_1^*,\ldots, w_K^*))$
		        \STATE Sample $b_k$ data points from each task~$k$
                \STATE Compute the gradient $\bg$ using the estimates samples
                \STATE{Update: $\ttheta_{t+1} \gets \textrm{Optimizer} (\eta, \ttheta_t,\bg)$ } 
		\ENDFOR
    \end{algorithmic}
    \caption{Conceptual version of PiKE: Positive gradient Interaction-based K-task weights Estimator}
    \label{alg: Basic PiKE}
\end{algorithm}

As discussed in Section~\ref{sec:PiKe}, this algorithm is computationally inefficient as it requires estimating $\nabla \cL_k(\theta_t)$ and $\sigma_k$ at each iteration. To improve efficiency, we introduced modifications that led to the development of the PiKE algorithm (Algorithm~\ref{alg: main} in the main body).

\section{Fair-PiKE: Fairness Considerations Across Tasks}\label{app:pike-fairness}
Here, we present the \textit{fair-PiKE} algorithm in more detail. As discussed in the main body, the main difference with PiKE is that the fair version requires the computation of the coefficients
\[
y_k^\star =\frac{\tau e^{\tau\cL_k(\ttheta)-1}}{\sum^K_{k=1} e^{\tau\cL_k(\ttheta) -1}},\forall k
\]
Then updating the sampling weights by
\[
w_k \gets w_k \exp\left(
     (y_k^\star)^2 \zeta_1 \|\nabla \cL_k(\bw)\|^2 - (y_k^\star)^2\frac{\zeta_2}{2b} \sigma_k^2
    \right),\;\;\;\forall k
\]
The overall algorithm is summarized in Algorithm~\ref{alg: fair_main}.
For our experiments, we evaluate three different values of $\tau$: 1, 3, and 5. A larger $\tau$ results in a stronger balancing effect between different tasks.

\begin{algorithm}[H] 
    \begin{algorithmic}[1]
        \STATE {{\bfseries Input:} $\ttheta$, $T$, total batch size $b$, task $k$ dataset $\cD_k$, hyperparameters $\zeta_1\, \zeta_2, \tau$, prior weights~$\bw'$ } 
	\STATE {{\bfseries Initialize:} $w_k \gets 1/K$ or $w_k \gets w_k'$ } 
		\FOR{$t=0,1, \dots$}
                \IF{$t \mod T = 0$}
                    \STATE Estimate $\|\nabla \cL_k(\ttheta_t)\|^2$, $\sigma_k^2$, and $\cL_k(\ttheta_t)$ for every $k$
                    \STATE $y_k^\star =\frac{\tau e^{\tau\cL_k(\ttheta)-1}}{\sum^K_{k=1} e^{\tau\cL_k(\ttheta) -1}}$
                    \STATE $w_k \gets w_k \exp\left(
                     (y_k^\star)^2 \zeta_1 \|\nabla \cL_k(\bw)\|^2 - (y_k^\star)^2\frac{\zeta_2}{2b} \sigma_k^2
                    \right)$
                    
                    \STATE  $\bw \gets \bw /\|\bw\|_1$
                    \STATE $(b_1,\ldots, b_K) \gets \textrm{round}(b (w_1,\ldots, w_K))$
                \ENDIF
		        \STATE Sample $b_k$ data points from each task~$k$
                \STATE Compute the gradient $\bg$ using the estimates samples
                \STATE{Update: $\ttheta_{t+1} \gets \textrm{Optimizer} (\eta, \ttheta_t,\bg)$} 
		\ENDFOR
    \end{algorithmic}
    \caption{\textit{fair-PiKE}: Fairness considerations across tasks}
    \label{alg: fair_main}
\end{algorithm}





\section{Experiments Setup}\label{app:experiment_setup}
\subsection{Dataset Details}
Our experiments construct two primary scenarios for multitask learning: multilingual tasks and diverse task mixtures spanning multiple domains. We consider two widely-used datasets for our study: mC4~\citep{xue2020mt5} and GLaM~\citep{du2022glam}.

\textbf{mC4 Dataset} The mC4 dataset~\citep{xue2020mt5} is a multilingual text corpus derived from the Common Crawl web archive, covering a diverse range of languages. It has been widely used for pretraining multilingual models, such as mT5~\citep{xue2020mt5} and ByT5~\citep{xue2021byt5}. The dataset is curated by applying language-specific filtering to extract high-quality text, ensuring a balanced representation across languages. Mixture weights for training models on mC4 are often chosen based on token counts. In our cases, we mainly focus on English (en), Hindi (hi), and German (de). We report their details in Table~\ref{tab: mc4-dataset-overview}. 

\begin{table*}[htb]
\centering
\small
\caption{Partial statistics of the mC4 corpus, totaling 6.3T tokens.} 
\begin{tabular}{lcc}
\toprule
ISO code & Language & Tokens (B)  \\
\midrule
en & English & 2,733 \\
hi & Hindi & 24 \\
de  & German & 347  \\
\bottomrule
\end{tabular}
\vskip 0.1in
\label{tab: mc4-dataset-overview}
\end{table*}


\textbf{GLaM Dataset} The GLaM dataset~\citep{du2022glam} comprises English text from six distinct sources and has been used to train the GLaM series models and PaLM~\citep{chowdhery2023palm}. Mixture weights for GLaM training were determined based on small model performance~\citep{du2022glam}, while \citep{xie2024doremi} employed group distributionally robust optimization (Group DRO) to compute domain-specific weights. Table~\ref{tab: glam-dataset-overview} summarizes the six domains in the GLaM dataset and the mixture weights selected by GLaM and DoReMi. We use these weights as oracle baselines for comparison with PiKE, which dynamically adjusts task weights over time using gradient information, unlike the fixed weights employed by GLaM and DoReMi.


\begin{table*}[htb]
\centering
\small
\caption{GLaM dataset~\citep{du2022glam} and fixed mixture weights used in GLaM~\citep{du2022glam} and DoReMi~\citep{xie2024doremi}.} 
\vskip 0.1in
\label{tab: glam-dataset-overview}
\begin{tabular}{lcccc}
\toprule
Dataset & Tokens (B) & Weight chosen by GLaM \citep{du2022glam} & Weight chosen by DoReMi \citep{xie2024doremi} \\
\midrule
Filtered Webpages & 143 & 0.42 & 0.51 \\
Wikipedia & 3 & 0.06 & 0.05\\
Conversations  & 174 & 0.28 & 0.22 \\
Forums & 247 & 0.02 & 0.04\\
Books & 390 & 0.20 & 0.20\\
News & 650 & 0.02  & 0.02\\
\bottomrule
\end{tabular}
\end{table*}


\subsection{Training Details}

Our experiments explore two distinct scenarios for multitask learning: multilingual training and diverse task mixtures spanning multiple domains. To achieve optimal results, we customize the training setups for each scenario and present them separately in this section. All training is performed from scratch.

\textbf{Multilingual Training} To address the complexities of tokenizing multilingual data, we utilize the mT5 tokenizer~\citep{xue2020mt5}, which features a vocabulary size of 250K. Both GPT-2 small and GPT-2 large models are trained with a context length of 1024 and a batch size of 256. The AdamW optimizer~\citep{loshchilov2018decoupled} is employed with consistent hyperparameters and a learning rate scheduler. Additional details on hyperparameter configurations are provided in Appendix~\ref{app:hyperparams}.

\textbf{GLaM Training} For GLaM training, we use the T5 tokenizer~\citep{raffel2020exploring}, implemented as a SentencePiece tokenizer trained on the C4 dataset with a vocabulary size of 32,000. Both GPT-2 small and GPT-2 large models are trained with a context length of 1024 and a batch size of 256. The AdamW optimizer~\citep{loshchilov2018decoupled} is used, and additional details on hyperparameters is in Appendix~\ref{app:hyperparams}.



\subsection{Model Architecture}

The detailed architecture is summarized in Table~\ref{tab:archictectures}. Our implementation utilizes pre-normalization~\citep{radford2019language} Transformers with qk-layernorm~\citep{dehghani2023scaling}. Consistent with \cite{chowdhery2022palm}, we omit biases, and the layernorm~\citep{ba2016layer}  value remains set to the Flax~\citep{flax2020github} default of 1e-6. Additionally, we incorporate rotary positional embeddings~\citep{su2021roformer}.


\begin{table}
\caption{Architecture hyperparameters for different model scales used in the paper. All models are GPT-2-like decoder-only architectures. The multilingual models employ a vocabulary size of 250K, whereas GLaM training uses a vocabulary size of 32K. Differences in the total number of parameters arise due to the variation in vocabulary sizes.}
\label{tab:archictectures}
\centering
\vspace{0.2cm}
\begin{adjustbox}{max width=0.9\textwidth}
\begin{tabular}{lrrrrr}
\toprule
   Size &  \# Params & Layers & Attention heads & Attention head dim & Hidden dim \\
     \midrule
GPT-2 small & 110M/270M  & 12      & 12               & 64                      & 768       \\
GPT-2 large &  750M/1B & 36      & 20               & 64                & 1280       \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\subsection{Experimental Resource}
All experiments are conducted on 8 Google TPUv4. The training time for GPT-2 small and GPT-2 large models for 120K steps are approximately 1 day and 2 days per run, respectively.

\subsection{Hyper-parameters}\label{app:hyperparams}
Table~\ref{tab:hyperparameters} shows the detailed hyperparameters that we used in all our experiments. We also report our hyperparameters grid for tuning PiKE in Table~\ref{tab:hyperparameters_pike}. 
\begin{table}[!tb]
    \caption{Hyperparameter settings for our experiments.}
    \vspace{0.2cm}
    \centering
    \small
    \begin{tabular}{ll}
    \toprule
    \textbf{Hyperparameters} & \textbf{Values} \\
    \midrule
    Optimizer & AdamW ($\beta_1=0.95$, $\beta_2=0.98$) \\
    Initial and final learning rate & $7e-6$ \\
    Peak learning rate & $7e-4$ \\
    Weight decay & $0.1$ \\
    Batch size & $256$ \\ 
    Context length & $1024$ \\
    Gradient clipping norm & $1.0$ \\
    Training step & $120,000$ \\
    Warm-up step & $10,000$\\
    Schedule & Linear decay to final learning rate \\
    \bottomrule
    \end{tabular}
    \label{tab:hyperparameters}
\end{table}

\begin{table}[!tb]
    \caption{Hyperparameter settings for running PiKE (Algorithm~\ref{alg: main}).}
    \vspace{0.2cm}
    \centering
    \small
    \begin{tabular}{ll}
    \toprule
    \textbf{Hyperparameters} & \textbf{Values} \\
    \midrule
    PiKE hyperparameter $\zeta_1$ & $\{0.025, 0.01, 0.75\}$ \\
    PiKE hyperparameter $\zeta_2$  & $\{5, 10, 15\}$ \\
    Check interval $T$ & 1000
    \\ \bottomrule
    \end{tabular}
    \label{tab:hyperparameters_pike}
\end{table}

\subsection{Implementation Details}

Our implementation builds upon the Nanodo training infrastructure~\citep{wortsman2023small}, incorporating enhancements for efficiency. This framework relies on Flax~\citep{flax2020github}, JAX~\citep{jax2018github}, and TPUs~\citep{jouppi2017datacenter}.

To enable training of larger models, we shard both model and optimizer states, following the methodology of FSDP~\citep{ren2021zero}, and define these shardings during JIT compilation. Checkpointing is handled using Orbax~\citep{orbax}, while deterministic data loading is facilitated by Grain~\citep{grain}.

For data loading, sequences are packed to avoid padding. When a sequence contains fewer tokens than the context length hyperparameter, an end-of-sequence token is appended. This differs from Nanodo~\citep{wortsman2023small}, where both begin-of-sequence and end-of-sequence tokens are added.



\subsection{Evaluation}

Our evaluation adheres to the OLMES suite~\citep{gu2024olmes}. For multilingual downstream performance, we utilize the multilingual version of HellaSwag~\citep{dac2023okapi}, which supports evaluations across 26 languages. English downstream tasks are assessed using ARC-Easy~\citep{clark2018think}, CommonsenseQA~\citep{talmor2018commonsenseqa}, PIQA~\citep{bisk2019reasoning}, and HellaSwag~\citep{zellers2019hellaswag}. Unless specified otherwise, multilingual evaluations are performed in a 0-shot setting, while GLaM pretraining evaluations employ 7-shot in-context learning, with demonstration candidates separated by two line breaks. For HellaSwag and its translated variants, we evaluate the first 3,000 examples. For all other downstream tasks, evaluations are conducted on their respective validation sets. In the case of multiple-choice tasks, different candidates are included in the prompt, and the average log-likelihood for each candidate is computed. The candidate with the highest score is then selected as the predicted answer.


\section{Additional Experiment Results}

\subsection{Comparison of Performance Using Mix, Random, and Round-Robin Sampling Strategies}\label{app: mix_rr_random}
Figure~\ref{fig:app_mix_round_rr_comparison} presents the average downstream accuracies of language models pre-trained using Mix, Random, and Round-Robin sampling strategies. In both multilingual pre-training and GLaM pre-training, the Mix sampling strategy consistently outperforms the other two. This motivates us its use in pre-training large language models.

\begin{figure*}[!htb]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/multilingual_enhide_large_convergence_smooth.pdf}
  \captionof{subfigure}{1B models on multilingual C4 (en), C4 (hi), and C4 (de) datasets }
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/glam_large_convergence_smooth.pdf}
  \captionof{subfigure} {750M models on GLaM datasets with six domains}
\end{minipage}
\caption{Average downstream task accuracy of pretraining language models using Mix, Round-Robin, and Random sampling strategies. Mix and Random use equal batch size for each task ($b_k = b/K, \forall k \in K$).}
\label{fig:app_mix_round_rr_comparison}
\end{figure*}

%
%
%
%
%
%
%
%
%
%


\subsection{Cosine Similarity and $\ulc$-Conflicted Gradients} \label{app: cos_sim_grad_conflict}
Figures~\ref{fig:app_mutlilingual_grad_similarity} and~\ref{fig:app_glam_grad_similarity} show the cosine similarity, defined as $\frac{\lin{\cL_j(\ttheta),\cL_k(\ttheta)}}{\|\cL_j(\ttheta)\|\|\cL_k(\ttheta)\|} $ 
and the ``ratio,'' defined as  
$\frac{\lin{\cL_j(\ttheta),\cL_k(\ttheta)}}{\|\cL_j(\ttheta)\|^2 +\| \cL_k(\ttheta)\|^2}. $ In particular, if  
$\frac{\lin{\nabla \cL_j(\ttheta) ,\nabla \cL_k(\ttheta)}}{\|\cL_j(\ttheta) \|\|\cL_k(\ttheta) \|} \geq -\tilde{c},$ then the gradients are $\ulc$-conflicted for $\ulc = \tilde{c}/2$, which aligns with the observations in Figures~\ref{fig:app_mutlilingual_grad_similarity} and~\ref{fig:app_glam_grad_similarity}.


\begin{figure*}[!htb]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/multilingual_cos_similarity.pdf}
  %
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/multilingual_cos_interaction.pdf}
  %
\end{minipage}
\caption{1B models trained on multilingual mC4 datasets. \textbf{Left:} Cosine similarity between task gradients during language model pre-training over time. \textbf{Right:} The ``ratio,'' which defined as  $
\frac{\lin{\cL_j(\ttheta),\cL_k(\ttheta)}}{\|\cL_j(\ttheta)\|^2 +\| \cL_k(\ttheta)\|^2},$ between task gradients during language model pre-training over time. ``\textit{data1-data2}'' denotes the cosine similarity or ratio between the gradient of \textit{data1} and the gradient of \textit{data2}.}

\label{fig:app_mutlilingual_grad_similarity}
\end{figure*}


\begin{figure*}[!htb]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/glam_cos_similarity.pdf}
  %
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures/glam_cos_interaction.pdf}
  %
\end{minipage}
\caption{750M models on GLaM datasets with six domains. \textbf{Left:} Cosine similarity between task gradients during language model pre-training over time. \textbf{Right:} The ``ratio,'' which defined as  $
\frac{\lin{\cL_j(\ttheta),\cL_k(\ttheta)}}{\|\cL_j(\ttheta)\|^2 +\| \cL_k(\ttheta)\|^2},$ between task gradients during language model pre-training over time. ``\textit{data1-data2}'' denotes the cosine similarity or ratio between the gradient of \textit{data1} and the gradient of \textit{data2}.}
\label{fig:app_glam_grad_similarity}
\end{figure*}

\subsection{Comparison of Performance Using PCGrad, AdaTask, and Mix}

Figure~\ref{fig: motivation_different_mtl} presents the average downstream task performance on HellaSwag (en) and HellaSwag (hi) for 270M multilingual language models pre-trained using PCGrad, AdaTask, and Mix. As shown in Figure~\ref{fig: motivation_different_mtl}: 1) PCGrad performs similarly to Mix, as it only adjusts gradients when conflicts occur—which is rare. 2) AdaTask converges more slowly due to noisy gradients and suboptimal optimizer state updates. Additionally, both methods are memory-intensive, requiring \(O(K)\) storage for task gradients (PCGrad) or optimizer states (AdaTask), making them impractical for large-scale models such as the 540B PaLM~\citep{chowdhery2022palm}.

\begin{figure}[!htb]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.45\columnwidth]{figures/multilingual_enhi_small_mtl_methods.pdf}} 
\caption{Eval perplexity of pretraining 270M GPT-2 style multilingual language models on mC4 datasets (English and Hindi) using Mix, PCGrad, and AdaTask.}
\label{fig: motivation_different_mtl}
\end{center}
\end{figure}



%
%
%
%
%
%
%
%
%
%


\subsection{Pre-training Results}
Tables~\ref{tab:main-table-multilingual} and~\ref{tab:main-table-glam} present the complete results of pre-training language models across various scales (110M, 270M, 750M, and 1B) and scenarios (Multilingual and GLaM datasets). PiKE consistently outperforms all baselines across all scales and scenarios.

\begin{table*}[!tb]
\large
\centering
\caption{We report the perplexities (lower the better) on the validation split of multilingual C4 datasets. We also compare the accuracies (\%, higher the better) of different models on HellaSwag and its corresponding translated version. HellaSwag and its translated versions have 4 choices. \textbf{Bolding} indicates the best model in the task, $\ols{\text{Metrics}}$ means the average across different tasks.}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccc}
\toprule
            &               & C4 (en)       & C4 (hi)       & C4 (de) &               & HellaSwag (en) & HellaSwag (hi) & HellaSwag (de) \\ \cmidrule(lr){3-5} \cmidrule(l){7-9} 
 &
  $\ols{\text{Perplexity} \downarrow}$ &
  Perplexity $\downarrow$ &
  Perplexity $\downarrow$ &
  Perplexity $\downarrow$ &
  $\ols{\text{Accuracy}(\%) \uparrow}$ &
  0-shot $\uparrow$ &
  0-shot  $\uparrow$ &
  0-shot $\uparrow$ \\ \midrule
\multicolumn{9}{l}{\textbf{Single dataset, GPT-2 small style, 270M params, 12 layers default, 120K training steps}}                                           \\
C4 (en)     &  13.25              &     13.25          & *             & *       &       26.5        &      26.5          & *              & *              \\
C4 (hi)     &   4.97            & *             &     4.97          & *       &     26.4          & *              &     26.4           & *              \\
C4 (de)     &    11.27           & *             & *             &  11.27        &  26.1             & *              & *              &     26.1           \\ \midrule
\multicolumn{9}{l}{\textbf{C4 (en) and C4 (hi) datasets, GPT-2 small style, 270M params, 12 layers default, 120K training steps}}                             \\
Mix         & 10.50         & 15.46         & \textbf{5.55}          & *       & 25.5          & 24.4           & 26.5           & *              \\
Round-Robin & 10.57         & 15.57         & 5.57          & *       & 25.6          & 25.2           & 26.0           & *              \\
Random      & 10.57         & 15.57         & 5.57          & *       & 25.3          & 24.3           & 26.3           & *              \\
PiKE       & \textbf{10.15}         & \textbf{14.31}         & 5.99          & *       & \textbf{26.5}          & \textbf{26.0}           & \textbf{27.0}           & *              \\ \midrule
\multicolumn{9}{l}{\textbf{C4 (en), C4 (hi), and C4 (de) datasets, GPT-2 small style, 300M params, 12 layers default, 120K training steps}}                   \\
Mix         &  \textbf{12.00}             & 16.30               &\textbf{5.88}               &\textbf{13.83}         &25.3               &24.4                &26.0                &25.5                \\
Round-Robin &12.10               &16.44               &5.91               &13.95         &25.1               &24.3                &26.0                &\textbf{24.9}                \\
Random      &12.16               &16.49               &5.95               &14.03         &25.1               &24.7                &\textbf{26.6}                &23.9                \\
PiKE       &12.01               &\textbf{15.48}               &5.92               &14.64         &\textbf{25.6}               &\textbf{25.4}                &26.4                &24.8                \\ \midrule
\multicolumn{9}{l}{\textbf{Single dataset, GPT-2 large style, 1B params, 36 Layers default, 120K training steps}}                                             \\
C4 (en)     & 9.30          & 9.30          & *             & *       & 33.6          & 33.6           & *              & *              \\
C4 (hi)     & 3.87          & *             & 3.87          & *       & 27.5          & *              & 27.5           & *              \\
C4 (de)     &  7.72         & *             & *            &  7.72   &  28.1         & *              & *              &      28.1      \\ \midrule
\multicolumn{9}{l}{\textbf{C4 (en) and C4 (hi) datasets, GPT-2 large style, 1B params,  36 Layers default, 120K training steps}}                              \\
Mix         & 7.41          & 10.60         & \textbf{4.22} & *       & 27.3          & 28.2           & 26.5           & *              \\
Round-Robin & 7.49          & 10.72         & 4.25          & *       & 27.5          & 28.0           & 27.0           & *              \\
Random      & 7.52          & 10.76         & 4.28          & *       & 28.0          & 28.9           & 27.0           & *              \\
PiKE       & \textbf{7.21} & \textbf{9.63} & 4.80          & *       & \textbf{30.0} & \textbf{32.7}  & \textbf{27.3}  & *              \\ \midrule
\multicolumn{9}{l}{\textbf{C4 (en), C4 (hi), and C4 (de) datasets, GPT-2 large style, 1B params, 36 Layers default, 120K training steps}}                     \\
Mix & \textbf{8.29} & 11.13 & \textbf{4.45} & \textbf{9.29} & 27.5 & 28.1 & 27.1 & \textbf{27.6} \\
Round-Robin & 8.41 & 11.31 & 4.97 & 9.46 & 26.5 & 27.6 & 26.7 & 26.3 \\
Random & 8.48 & 11.38 & 4.54 & 9.55 & 26.6 & 27.0 & 26.9 & 26.1 \\
PiKE & 9.56 & \textbf{9.49} & 5.32 & 13.87 & \textbf{28.7} & \textbf{33.0} & \textbf{27.2} & 26.2 \\ 
%
%
%
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:main-table-multilingual}
\end{table*}

\begin{table*}[!tb]
\centering

\caption{We report perplexity (lower is better) on the validation split of the GLaM datasets, averaging perplexities across six domains when applicable or reporting a single perplexity when only training with a single domain. We also compare the accuracies (\%, higher the better) of different models on four different Q/A tasks. HellaSwag and ArcE tasks have 4 choices, CSQA has 5 choices, and PIQA has 2 choices. PiKE (Uniform) means PiKE using initial sampling weights of $1/6$ for each task and PiKE (GLaM) means PiKE using GLaM tuned weights as initial task weights. \textbf{Bolding} indicates the best model in the task, $\ols{\text{Metrics}}$ means the average across different tasks, \ul{underlining} indicates PiKE beating Mix, Round-Robin, Random methods}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccc}
\toprule
                 & GLaM              &                      & ArcE                 & CSQA            & HellaSwag                 & PIQA                 \\ \cmidrule(l){4-7} 
 &
  $\ols{\text{Perplexity} \downarrow } $ &
  $\ols{\text{Accuracy} (\%) \uparrow}$ &
  7-shot $\uparrow$ &
  7-shot $\uparrow$ &
  7-shot  $\uparrow$ &
  7-shot $\uparrow$ \\ \midrule
\multicolumn{7}{l}{\textbf{Single domain of GLaM dataset, GPT-2 small style, 110M params, 12 layers default }}                         \\
Wikipedia           &  9.96 & 33.5 & 32.5 & 20.9 & 27.3 & 53.3 \\
Filtered Webpage    & 16.05 & 37.2 & 38.4 & 26.8 & 27.6 & 55.8 \\
News                & 9.33 & 33.8 & 31.1 & 22.7 & 27.0 & 54.5 \\
Forums              & 22.87 & 35.5 & 32.1 & 23.4 & 28.7 & 57.6 \\ 
Books               & 16.81 & 34.7 & 34.3 & 22.1 & 27.8 & 54.7 \\
Conversations       & 18.27 & 36.1 & 32.6 & 25.6 & 28.6 & 57.6 \\ \midrule
\multicolumn{7}{l}{\textbf{Six domains of GLaM dataset, GPT-2 small style, 110M params, 12 layers default}}                       \\
Mix&  \textbf{18.27}                    &36.2                   &35.6                       &24.1                      &\textbf{28.5}                      &56.7                      \\
Round-Robin      &  18.45                     &35.9                      &35.8                      &24.2                             &27.5                      &56.0                      \\
Random           &  18.48                    &35.5                      &34.3                      &22.4                      &28.4                      &56.8                      \\
GLaM             &   18.91                   &35.8                      &35.3                      &24.1                      &28.5                      &55.1\\
DoReMi           &   18.98                   &37.0                     &36.0                      &\textbf{28.3}                     &28.2                      &55.3\\
PiKE (Uniform)             & 18.44           &\ul{37.4}                      &\ul{36.8}                      &\ul{27.5}                     &\ul{\textbf{28.5}}                      &\ul{\textbf{57.0}}   \\
PiKE (GLaM)             &    19.34           &\ul{\textbf{37.8}}                      &\ul{\textbf{39.0}}                      & \ul{27.0}                      &28.0                      &\ul{\textbf{57.0}}                      \\ \midrule
\multicolumn{7}{l}{\textbf{Single domain of GLaM dataset, GPT-2 large style, 750M params, 36 layers default}}                         \\
Wikipedia & 7.24 & 35.9 & 35.1 & 24.0 & 30.5 & 53.9       \\
Filtered Webpage &  11.12 & 40.9 & 36.7 & 33.2 & 34.2 & 56.5 \\
News  & 6.62 & 37.4 & 33.6 & 24.7 & 34.1 & 57.3 \\
Forums & 16.29 & 43.6 & 38.0 & 35.8 & 39.7 & 60.7          \\
Books & 11.83  & 41.3 & 40.0 & 33.0 & 34.5 & 57.8       \\
Conversations & 13.50 & 42.2 & 36.9 & 33.2 & 39.2 & 59.6     \\ \midrule
\multicolumn{7}{l}{\textbf{Six domains of GLaM dataset, GPT-2 large style, 750M params, 36 layers default}}                       \\
Mix &\textbf{12.77} & 46.4 & 47.2 & 39.6 & 37.9 & 60.9        \\
Round-Robin &12.98 & 44.3 & 43.5 & 36.7 & 36.8 & 60.3 \\
Random &12.99 & 42.7 & 41.7 & 34.2 & 36.6 & 58.2                 \\
GLaM &13.20 & 45.3 &46.9 & 39.8 & \textbf{38.0} & 56.4            \\
DoReMi &13.25 & 46.5 & 48.6 & 40.1 & 37.5 & 59.6         \\
PiKE (Uniform) &13.22 & \ul{47.6} &\ul{49.6} & \ul{43.2} & 37.2 &60.4   \\
PiKE (GLaM) &13.35 & \ul{\textbf{48.1}} & \ul{\textbf{49.8}} & \ul{\textbf{43.5}} & \ul{\textbf{38.0}} & \textbf{\ul{61.2}} \\
%
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:main-table-glam}
\end{table*}

\subsection{Adaptive Sampling Weights of PiKE During Pre-training}
Figure~\ref{fig:pike_sampling_weights} illustrates how the adaptive sampling weights of PiKE evolve during language model pre-training. Compared to the Mix sampling strategy, which assigns equal sampling weights to each task, PiKE adaptively adjusts the sampling weights among English, German, and Hindi by leveraging the positive interaction of task gradients. This adaptive data selection allows PiKE to achieve superior performance compared to fixed or heuristic-based baselines.



\begin{figure*}[!htb]
\centering
\begin{minipage}{.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/multilingual_en_samplingweight.pdf}
\end{minipage}%
\begin{minipage}{.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/multilingual_de_samplingweight.pdf}
\end{minipage}
\begin{minipage}{.33\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/multilingual_hi_samplingweight.pdf}
\end{minipage}
\caption{The sampling weights for each dataset during the pre-training of 1B GPT-2-style multilingual language models on mC4 (English), mC4 (Hindi), and mC4 (German). Here, $w_{\text{en}}$ represents the sampling weight for the English dataset, $w_{\text{hi}}$ for the Hindi dataset, and $w_{\text{de}}$ for the German dataset.}
\label{fig:pike_sampling_weights}
\end{figure*}

\section{Derivations and Proofs} \label{app: theory}
%

%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%

\subsection{Detailed Derivation of~\eqref{eq:ExampleExpectedLoss}} \label{sec:derivationExampleExpectedLoss}
Recall that 
\[
\bg_t = \frac{1}{b_1+b_2}\left(b_1e_1e_1^\top + b_2 e_2 e_2^\top\right)\ttheta_t + \bz,
\]
Then
\begin{align}
    \ttheta_{t+1} &= \ttheta_{t} - \eta \frac{1}{b_1+b_2}\left(b_1e_1e_1^\top + b_2 e_2 e_2^\top\right)\ttheta_t - \eta \bz \nonumber \\ 
    & = \ttheta_t - \frac{\eta}{b} 
        \begin{bmatrix}
        b_1 & 0 \\
        0 & b_2 \\
        \end{bmatrix} \ttheta_t - \eta\bz \nonumber
\end{align}
Now consider the loss functions for task 1, $\cL_1(\theta_{t+1})$, and task 2, $\cL_2(\theta_{t+1})$, separately, taking the expectation over the randomness of $\mathbf{z}$
{\allowdisplaybreaks
\begin{align}
    \E[\cL_1(\ttheta_{t+1})]) &= \E\left[\frac{1}{2}(\e_1^\top\ttheta_{t+1})^2\right] \nonumber \\
    &= \E\left[\frac{1}{2}\left(\e_1^\top\begin{bmatrix}
        1-\frac{\eta b_1}{b}& 0 \\
        0 & 1-\frac{\eta b_2}{b} \\
        \end{bmatrix}\ttheta_t - \e_1^\top\eta\bz \right)^2\right] \nonumber \\
    &= \frac{1}{2} \left(\begin{bmatrix}
        1-\frac{\eta b_1}{b} & 0 
    \end{bmatrix} \ttheta^\top\right)^2 + \frac{1}{2}\eta^2\e_1^\top\bQ\e_1 \nonumber \\
    &= \frac{1}{2} \left( \left(1-\frac{\eta b_1}{b}\right) \theta_{1,t}\right)^2 + \frac{1}{2}\eta^2\e_1^\top\bQ\e_1 \nonumber 
\end{align}
}
Similarly, for task 2, we have
\begin{align}
    \E[\cL_2(\ttheta_{t+1})]) &= \frac{1}{2} \left( \left(1-\frac{\eta b_2}{b}\right) \theta_{2,t}\right)^2 + \frac{1}{2}\eta^2\e_2^\top\bQ\e_2 \nonumber 
\end{align}
where $\theta_{1,t}$ and $\theta_{2,t}$ denote the first and second component of the vector $\ttheta_t$. Combining the losses for both tasks, the total expected loss becomes
\begin{align}
    \E[\cL(\ttheta_{t+1})] &= \E[\cL_1(\ttheta_{t+1})]) + \E[\cL_2(\ttheta_{t+1})]) \nonumber \\
    &= \frac{1}{2} \left( \left(1-\frac{\eta b_1}{b}\right) \theta_{1,t}\right)^2 + \frac{1}{2} \left( \left(1-\frac{\eta b_2}{b}\right) \theta_{2,t}\right)^2 + \eta^2\frac{b_1\sigma_1^2+b_2\sigma_2^2}{b^2} \nonumber \\
    &= \frac{1}{2}(1- \frac{\eta b_1}{b})^2\theta_{1,t}^2 +\frac{1}{2}(1-\frac{\eta b_2}{b})^2\theta_{2,t}^2 +\eta^2\frac{b_1\sigma_1^2+b_2\sigma_2^2}{b^2}, \nonumber
\end{align}
which completes the derivations.


\subsection{PiKE: Main Theoretical Results}

\vspace{0.3cm}
\begin{lemma}\label{le: correlation_of_losses}
   Assume $\frac{1}{2(K-1)} > \ulc$. If $\|\nabla \cL(\ttheta)\|^2 \leq \epsilon$, we have 
   \[
           \sum_{k=1}^K \|\nabla \cL_k(\ttheta)\|^2 \leq \frac{\epsilon}{1 - 2\,\ulc\,(K-1)}.
   \]
Conversely, if $\|\nabla \cL_k(\ttheta)\|^2 \leq \delta_k,\;\forall k$, then 
\[
        \|\nabla \cL(\ttheta)\|^2 \leq (1-\barc)\sum_{k=1}^K \delta_k \;+\; \barc \left( \sum_{k=1}^K \sqrt{\delta_k} \right)^2 
\]
    %
    %
    %
    %
    %
    %
    %
    %
    %
\end{lemma}
{\it Proof:} 
We first prove the first direction. Notice that 
\begin{align}
    \|\nabla \cL(\ttheta)\|^2 &= \|\sum^K_{k=1} \nabla\cL_k(\ttheta) \|^2 \nonumber \\
    & = \sum^K_{k=1} \|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}\sum_{j\neq k} \lin{\nabla\cL_j(\ttheta), \nabla \cL_k(\ttheta)} \leq \epsilon \nonumber
\end{align}
where we use the definition of $\nabla\cL(\ttheta)$ and expand the term. Then we have
\begin{align}
    \sum^K_{k=1} \|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}\sum_{j\neq k} \lin{\nabla\cL_j(\ttheta), \nabla \cL_k(\ttheta)} &\stackrel{(a)}{\geq} \sum^K_{k=1} \|\nabla\cL_k(\ttheta)\|^2 - \ulc \sum^K_{k=1}\sum_{j\neq k}\left( \|\nabla\cL_j(\ttheta)\|^2 + \|\nabla\cL_k(\ttheta)\|^2\right) \nonumber \\
    &\stackrel{(b)}{\geq} \sum^K_{k=1} \|\nabla\cL_k(\ttheta)\|^2\left(1-2\ulc(K-1)\right) \nonumber
\end{align}
where $(a)$ uses the Definition~\ref{def:Interaction_LB}, $(b)$ uses symmetric identity. Thus we get
\[
  \sum_{k=1}^K \|\nabla \cL_k(\ttheta)\|^2 \leq \frac{\epsilon}{1 - 2\,\ulc\,(K-1)}
\]
This completes the proof of the first inequality. We now prove the second inequality. Notice that
\begin{align}
    \|\nabla \cL(\ttheta)\|^2=\|\sum^K_{k=1} \nabla \cL_k(\ttheta)\|^2 &= \sum^K_{k=1} \|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1} \sum_{j\neq k} \lin{\nabla\cL_j(\ttheta), \nabla\cL_k(\ttheta)} \nonumber \\
    & \stackrel{(a)}{\leq} \|\nabla\cL_k(\ttheta)\|^2 + \barc \sum^K_{k=1} \sum_{j\neq k}\|\nabla\cL_j(\ttheta)\|^2 \|\cL_k(\ttheta)\|^2 \nonumber \\
    & = (1-\barc) \|\nabla\cL_k(\ttheta)\|^2 + \barc \|\nabla\cL_k\|^2 + \barc \sum^K_{k=1} \sum_{j\neq k}\|\nabla\cL_j(\ttheta)\|^2 \|\cL_k(\ttheta)\|^2 \nonumber \\
    & \stackrel{(b)}{\leq} (1-\barc) \sum^K_{k=1} \delta_k + \barc\left(\sum^K_{k=1}\sqrt{\delta_k}\right)^2 \nonumber
\end{align}
where $(a)$ use the Definition~\ref{def:Interaction_UB} and $(b)$ combines the second and third terms and use the condition that $\|\nabla \cL_k(\ttheta)\|^2 \leq \delta_k$. This completes the proof of the second inequality. 


%
%
%
%
%
%
%
%
%

\vspace{0.3cm}
\begin{lemma}\label{le:optimal_w_for_relax_problem}
    For the  optimization problem 
    \begin{equation} 
    \begin{split}
        \min_{w_1,\ldots,w_K} \quad &\sum^K_{k=1}  w_k \lambda_k + \frac{1}{2} w_k^2\kappa_k\\\
        \textrm{s.t.} \quad &\sum_{k=1}^Kw_k = 1, \quad w_k \geq 0,\quad  \forall k
    \end{split}
    \end{equation}
    the optimal solution is \begin{equation}
    w_k^* = \max\left\{0, -\frac{\mu + \lambda_k}{\kappa_k}\right\} 
    \end{equation}
    where $\mu$ is chosen such that $\sum_{k=1}^K w_k^* = 1$ 
\end{lemma}
{\it Proof:}
Consider the Lagrangian function
\[
    \cL(w_1,\ldots,w_k,\mu,\alpha_1,\ldots,\alpha_k)=\sum^K_{k=1}  w_k \lambda_k + \frac{1}{2} w_k^2\kappa_k + \mu \left(\sum^K_{k=1}w_k - 1\right) -\sum^K_{k=1}\alpha_kw_k
\]
where $\mu$ is Lagrange multiplier for the equality constraint for the constraint $\sum^K_{k=1}w_k=1$ and $\alpha_K\geq0$ are Lagrange multipliers for the nonnegativity constraints $w_k$. Take the partial derivative of $\mathcal{L}$ with respect to $w_k$ and set it to 0:
\[
\frac{\partial \mathcal{L}}{\partial w_k}=\lambda_k+w_k \kappa_k+\mu-\alpha_k = 0
\]
From the Karush-Kuhn-Tucker (KKT) conditions, we also have $w_k^\star \geq 0, \alpha_k \geq 0$, and $\alpha_k w_k^\star=0$. 
If $w_k^\star>0$, then $\alpha_k=0$, which implies
\[
0=\lambda_k+w_k^\star \kappa_k+\mu \quad \Longrightarrow \quad w_k^\star=-\frac{\mu+\lambda_k}{\kappa_k}
\]


If $-\left(\mu+\lambda_k\right) / \kappa_k$ is negative, then $w_k^\star=0$ must hold. Combining these, we get
\[
w_k^*=\max \left\{0,-\frac{\mu+\lambda_k}{\kappa_k}\right\}
\]
Finally, the Lagrange multiplier $\mu$ is determined by enforcing the equality constraint:
\[
\sum_{k=1}^K w_k^*=1
\]
with $\mu$ chosen so that the $w_k^*$ sum to 1 . This completes the proof.

\vspace{0.3cm}
\begin{theorem}
%
\label{thm: formal_descent_lemma}(Theorem~\ref{thm:DescentMainBody} in the main body)
Suppose Assumption~\ref{as:assumption1} is satisfied. Assume that at the given point~$\ttheta_t$ the gradients are $\ulc$-conflicted and $\barc$-aligned with
$\ulc <\frac{1}{K -2 + b/b_k}, \forall k$. Moreover, assume the gradient is computed according to the mix strategy~\eqref{eq: mix_framework}. Then, we have
    \begin{align} 
        \E[\cL(\ttheta - \eta \g)] \leq \cL(\ttheta) + \sum^K_{k=1} b_k\Big(-\frac{\eta}{b}\beta\|\nabla\cL_k(\ttheta)\|^2  + \frac{L\eta^2}{2b^2}\sigma_k^2\Big) + \sum^K_{k=1} b_k^2 \frac{L\eta^2}{2b^2}\gamma \|\nabla \cL_k(\ttheta)\|^2 
    \end{align}
    where $0\leq \beta \triangleq \min_{k} (1+\ulc(-K+2-\frac{b}{b_k}))$ and $\gamma \triangleq 1 + \barc (K-1)$.
\end{theorem}



%
%
%
%
%
%
{\it Proof:} 
We begin by revisiting the multi-task optimization problem under consideration. The objective is defined as:
\begin{equation}
\min_{\ttheta \in \R^d} \cL(\ttheta) := \sum_{k=1}^K \E_{x \sim \cD_k} \left[\ell_k(\ttheta; x)\right],
\end{equation}
where \(\cL(\ttheta)\) is the expected aggregate loss over all tasks. Assume we mix the gradients with taking $b_k$ i.i.d. samples from task $k$ for $k=1,\ldots, K$. Then under the Assumption~\ref{as:assumption1} the estimated gradient direction is given by

\begin{align}
\bg &= \frac{1}{\sum_{k=1}^K b_k} \left(\sum_{k=1}^K \sum_{\stackrel{i=1}{x_i\sim \cD_k}}^{b_k} \nabla \ell_k(\ttheta; x_i) \right) \nonumber \\
& = \frac{1}{b} \sum_{k=1}^K \left(b_k\nabla \ell_k(\ttheta)\right) \; +\; \bz,
\end{align}

where the random variable $\bz$ is defined as $\bz=\sum^K_{k=1}\sum^{b_k}_{i=1,x_i\sim \cD_k} (\nabla \ell_k(\ttheta, x_i) - \nabla \cL_k(\ttheta))$ over the randomness of the sampling strategy. Let $\ttheta^{+}$ be the updated point after gradient descent with $\ttheta^{+} = \ttheta - \eta \bg$. By the descent lemma, the following inequality holds for the updated parameter $\ttheta^{+}$:
\begin{equation}
\cL(\ttheta^{+}) \leq \cL(\ttheta) - \eta \bg^\top \nabla \cL(\ttheta) + \frac{L \eta^2}{2} \|\bg\|^2,
\end{equation}

Taking the expectation over the randomness of \(\bz\), we obtain:

\begin{align}
\mathbb{E}\left[\cL(\ttheta^{+}) \right] 
&\leq \cL(\ttheta) -\eta \E[\bg]^\top \nabla \cL(\ttheta) + \frac{L\eta^2}{2} \E\left(\|\bg\|^2\right)\nonumber\\
&\stackrel{(a)}{=} \cL(\ttheta) -\eta \left(\frac{1}{b} \sum_{k=1}^K b_k \nabla \cL_k(\ttheta)\right)^\top \left( \sum_{k=1}^K\nabla \cL_k(\ttheta) \right)\nonumber\\
& \quad + \frac{L\eta^2}{2b^2}
\left(
 \left(\sum_{k=1}^K b_k \nabla \cL_k(\ttheta)\right)^2 + \sum_{k=1}^K (b_k \sigma_k^2)
\right)
\nonumber\\
&\stackrel{(b)}{=} \cL(\ttheta) -
\frac{\eta}{b} \left( \sum_{k=1}^K b_k \|\nabla \cL_k(\ttheta)\|^2 + \sum_{k=1}^K\sum_{j\neq k}b_k \lin{\nabla \cL_j(\ttheta) ,\nabla \cL_k(\ttheta)}\right)\nonumber\\
& \quad + \frac{L\eta^2}{2b^2}
 \left(\left(\sum_{k=1}^K b_k \nabla \cL_k(\ttheta)\right)^2 + \sum_{k=1}^K (b_k \sigma_k^2)
\right),
\nonumber
\end{align}

where \((a)\) substitutes the definition of \(\bg\) and uses the Assumption~\ref{as:assumption1}, and \((b)\) expands the terms. We have 
{\allowdisplaybreaks
\begin{align}
    \mathbb{E}\left[\cL(\ttheta^{+}) \right] 
    &\stackrel{(a)}{\leq}   \cL(\ttheta) -
    \frac{\eta}{b} \left( \sum_{k=1}^K b_k \|\nabla \cL_k(\ttheta)\|^2 - \sum_{k=1}^K\sum_{j\neq k}b_k \ulc  (\|\nabla \cL_j(\ttheta)\|^2 + \|\nabla \cL_{k}(\ttheta)\|^2)\right)\nonumber\\
    & \quad + \frac{L\eta^2}{2b^2}
    \left(
        \sum^K_{k=1}b_k^2\|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}\sum_{j\neq k}b_jb_k\lin{\nabla\cL_j(\ttheta), \nabla\cL_k(\ttheta)} + \sum^K_{k=1} b_k\sigma_k^2
    \right), \nonumber\\
    & \stackrel{(b)}{=}  \cL(\ttheta) - \frac{\eta}{b} \left( \sum^K_{k=1} \left(b_k-\ulc b_k(K-1) - \ulc \sum_{j\neq k} b_j \right) \|\nabla\cL_k(\ttheta) \|^2 \right) \nonumber \\
    & \quad + \frac{L\eta^2}{2b^2}
    \left(
    \sum^K_{k=1}b_k^2\|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}\sum_{j\neq k}b_jb_k\lin{\nabla\cL_j(\ttheta), \nabla\cL_k(\ttheta)} + \sum^K_{k=1} b_k\sigma_k^2
    \right), \nonumber\\
    & \stackrel{(c)}{\leq} \cL(\ttheta) -  \frac{\eta}{b} \left( \sum^K_{k=1} (b_k - \ulc (K-1) b_k - \ulc(b-b_k)) \|\nabla\cL_k(\ttheta) \|^2 \right) \nonumber \\
    & \quad + \frac{L\eta^2}{2b^2}
    \left(
        \sum^K_{k=1}b_k^2\|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}\sum_{j\neq k}\barc b_jb_k\|\nabla\cL_j(\ttheta)\|^2 \|\nabla\cL_j(\ttheta) \|^2 + \sum^K_{k=1} b_k\sigma_k^2
    \right) \nonumber \\
    & \stackrel{(d)}{=} \cL(\ttheta) -  \frac{\eta}{b} \left( \sum^K_{k=1} b_k(1+\ulc(-K+2- b/b_k)) \|\nabla\cL_k(\ttheta) \|^2 \right) \nonumber \\
    & \quad + \frac{L\eta^2}{2b^2}
    \left(
        \barc\left(\sum^K_{k=1}b_k\|\nabla\cL_k(\ttheta)\|\right)^2 + (1-\barc) \sum^K_{k=1} b_k^2\|\nabla\cL_k(\ttheta)\|^2 + \sum^K_{k=1}b_k\sigma_k^2
    \right) \nonumber \\
    & \stackrel{(e)}{\leq} \cL(\ttheta) -  \frac{\eta}{b} \left( \sum^K_{k=1} b_k(1+\ulc(-K+2-b/b_k)) \|\nabla\cL_k(\ttheta) \|^2 \right) \nonumber \\
    & \quad + \frac{L\eta^2}{2b^2}
    \left(
        \barc K\sum^K_{k=1} b_k^2 \|\nabla \cL_k(\ttheta)\|^2 + (1-\barc) \sum^K_{k=1} b_k^2\|\nabla\cL_k(\ttheta)\|^2 + \sum^K_{k=1}b_k\sigma_k^2
    \right) \nonumber \\
    & = \cL(\ttheta) -  \frac{\eta}{b} \left( \sum^K_{k=1} b_k(1+\ulc(-K+2-b/b_k)) \|\nabla\cL_k(\ttheta) \|^2 \right) \nonumber \\
    & \quad + \frac{L\eta^2}{2b^2}
    \left(
        (1-\barc + \barc K) \sum^K_{k=1} b_k^2\|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}b_k\sigma_k^2
    \right)
\end{align}
}
where $(a)$ applies Definition~\ref{def:Interaction_LB} to the second term and expands the third term, $(b)$ expands the summation in the second term, $(c)$ uses the identity $\sum_{k=1}^K \sum_{j \neq k} b_j = \sum_{k=1}^K (b - b_k)$ in the second term and applies Definition~\ref{def:Interaction_UB} to the third term, $(d)$ combines terms in the third term, and $(e)$ uses the inequality $\|\sum_{i=1}^N u_i\|^2 \leq N\sum_{i=1}^N u_i^2$, where $\bu$ is a column vector. We define $\beta$ and $\gamma$ such that
\begin{align}
    \beta &= \min_{k} (1+\ulc(-K+2-\frac{b}{b_k})) \nonumber \\
    \gamma &= 1 + \barc (K-1) \nonumber \\
\end{align}
Then using the definition of $\beta$ and $\gamma$, substituting back we have
\begin{align}
    \mathbb{E}\left[\cL(\ttheta^{+}) \right] 
    & \leq \cL(\ttheta) -  \frac{\eta\beta}{b} \left( \sum^K_{k=1} b_k
    \|\nabla\cL_k(\ttheta) \|^2 \right) + \frac{L\eta^2}{2b^2}
    \left(
       \gamma \sum^K_{k=1} b_k^2\|\nabla \cL_k(\ttheta)\|^2 + \sum^K_{k=1}b_k\sigma_k^2
    \right) \nonumber \\
    & = \cL(\ttheta) + \sum^K_{k=1} b_k \left( -\frac{\eta \beta}{b}\|\nabla\cL_k(\ttheta)\|^2 + \frac{L\eta^2}{2b^2}\sigma^2\right) +\sum^K_{k=1} b_k^2 \frac{L\eta^2}{2b^2}\gamma\|\nabla\cL_k(\ttheta)\|^2 \nonumber 
\end{align}
which we complete the proof.

\begin{theorem}%
\label{thm:app_IterationComplexityConceptualPiKe} (Theorem~\ref{thm:IterationComplexityConceptualPiKe} in the main body)
Suppose the assumptions in Theorem~\ref{thm: formal_descent_lemma} is satisfied and we run the Conceptual PiKE Algorithm (Algorithm~\ref{alg: Basic PiKE}) initialized at $\ttheta_0$ with the SGD optimizer in Step 10 of the algorithm. Let $\Delta_L = \cL(\ttheta_0) - \min_{\ttheta}\cL(\theta)$ and $\sigma_{\max} = \max_k \sigma_k$. Suppose $\delta>0$ is a given constant and the stepsize $\eta \leq \frac{\beta \delta}{L\sigma_{\max}^2/b + L\eta \delta}$. Then, after $T = \frac{2\beta \Delta_L}{\eta \delta}$ iterations, Algorithm~\cref{alg: Basic PiKE} finds a point $\bar{\ttheta}$ such that 
\begin{equation}
    \label{eq:app_boundedNormGrad}
    \mathbb{E}\|\nabla \cL_k(\bar{\theta})\|^2 \leq \delta,\quad \forall k=1,\ldots, K.
\end{equation}
Moreover, if we choose $\eta = \frac{\beta \delta}{L\sigma_{\max}^2/b + L\eta \delta}$, then the Conceptual PiKE algorithm requires at most 
\[
\bar{T} = \frac{2L\Delta_L(\sigma_{\max}^2/b + \gamma \delta)}{\delta^2 \beta^2}
\]
iterations to find a point satisfying~\eqref{eq:app_boundedNormGrad}.
\end{theorem}

{\it Proof:}
We prove this by contradiction. Assume that $\max_k\|\nabla\cL_k(\ttheta_t)\|^2 > \delta$ for $t=0,\ldots,T$. First notice that Theorem~\ref{thm: formal_descent_lemma} implies that for all t, we have 
\begin{equation}\label{eq:app_Iteration_eq_1}
     \E[\cL(\ttheta_{t+1})] \leq \cL(\ttheta_t) + \sum^K_{k=1}w_k^\star\left(-\eta\beta\|\nabla\cL_k(\ttheta_t))\|^2 + \frac{L\eta^2\sigma^2_{\text{max}}}{2b}\right) + \sum^K_{k=1}\frac{w_k^\star}{2}\left(L\eta^2\gamma\|\nabla\cL_k(\ttheta_t)\|^2\right)
\end{equation}
where $\{w_k^\star\}^K_{k=1}$ is the minimizer of the RHS of the \eqref{eq:app_Iteration_eq_1} on the constrained set $\{(w_1,\ldots,w_k) | \sum^K_{k=1}w_k=1,\;w_k\geq0\;\forall k\in K\}$. Since $w_k^\star$ is the minimizer of the RHS of \eqref{eq:app_Iteration_eq_1}, we have 
\begin{align}\label{eq:app_Iteration_eq_2}
    w_k^\star\left(-\eta\beta\|\nabla\cL_k(\ttheta_t)\|^2 + \frac{L\eta^2}{2b}\sigma_{\text{max}}^2\right) + \frac{w_k^\star}{2}L\eta^2\gamma\|\nabla\cL_k(\ttheta_t)\|^2  & \leq \left(-\eta\beta\|\nabla\cL_{k_t^\star}(\ttheta_t)\|^2 + \frac{2\eta^2}{2b}\sigma_{\text{max}}^2\right) \nonumber \\ 
    & \quad + \frac{L\eta^2}{2}\gamma\|\nabla\cL_{k_t^\star}(\ttheta_t)\|^2
\end{align}
where $k^\star_t \in \arg\max_k\|\nabla\cL_k(\ttheta_t)\|^2$. Moreover since
\[
    \eta \leq \frac{\beta \|\nabla\cL_k(\ttheta)\|^2}{L\frac{\sigma_{\text{max}}^2}{b}+L\gamma\|\nabla\cL_k(\ttheta_t)\|^2},
\] we have 
\begin{equation}\label{eq:app_Iteration_eq_3}
    \left(-\eta\beta\|\nabla\cL_{k^\star_t}(\ttheta_t)\|^2 + \frac{2\eta^2}{2b}\sigma_{\text{max}}^2\right) + \frac{L\eta^2}{2}\gamma\|\nabla\cL_{k_t^\star}(\ttheta_t)\|^2 \leq -\frac{\beta\eta}{2}\|\nabla\cL_{k^\star_t}(\ttheta_t)\|^2
\end{equation}
Combining equation (\ref{eq:app_Iteration_eq_1}), (\ref{eq:app_Iteration_eq_2}), and (\ref{eq:app_Iteration_eq_3}), we obtain
\[
    \mathbb{E}[\cL(\ttheta_{t+1})] \leq \cL(\ttheta_t) - \frac{\beta \eta}{2} \|\nabla\cL_{k^\star_t}(\ttheta_t)\|^2
\]
Or equivalently
\[
    \mathbb{E}[\cL(\ttheta_{t+1})] \leq \cL(\ttheta_t) - \frac{\beta\eta}{2} \max_k\|\nabla\cL_k(\ttheta_t)\|^2
\]
Summing the above inequality from $t=0$ to $t=T-1$, we get
\[
    \mathbb{E} [\cL(\ttheta_T)] \leq \cL(\ttheta_0) - \mathbb{E}\frac{\beta\eta}{2}\sum^{T-1}_{t=1}\max_k\|\nabla\cL_k(\ttheta_t)\|^2
\]
According to the contradiction assumption, we get
\[
    \mathbb{E}[\cL(\ttheta_T)] \leq \cL(\ttheta_0) - \frac{\beta\eta}{2} T\delta
\]
Using the definition $\Delta_\cL \triangleq \cL(\ttheta_0) - \min_{\ttheta} \cL(\ttheta)$, we get 
\[
    T\leq \frac{2\Delta_\cL}{\beta\eta\delta}
\]

Finally notice that by setting $\eta=\frac{\beta\delta}{L\frac{\sigma_{\text{max}}^2}{b}+L\gamma\delta}$, we get
\[
    T \leq \Bar{T} = \frac{2\Delta_\cL}{\beta\eta} = \frac{2L\Delta_\cL}{\beta\delta^2}\left(\frac{\sigma_{\text{max}}^2}{b}+\gamma\delta\right)
\]
which means after iteration $T$ steps, we have 
\[
    \min_t\left\{\max_k\|\nabla\cL_k(\ttheta_t)\|^2\right\} \leq \delta,
\]
which completes the proof. 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%

%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%

%
%

%
%
%
%
%
%
%

%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
%
%
%

%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%

%

%
%
%
%
%
%

%


%
%
%
%
%
%
%
%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

\subsection{PiKE: Fairness Considerations Across Tasks}\label{app:fairness_theory}

Consider the tilted empirical risk minimization~\citep{li2020tilted}:
\begin{equation}
   \min_{\ttheta} \quad \widetilde{\cL}(\tau ; \ttheta) := \frac{1}{\tau} \log \left(\sum^K_{k=1} e^{\tau\cL_k(\ttheta)}\right). \nonumber
\end{equation}

As we described in the main body, we connect this problem to the minimization of the weighted sum of $\cL_k$'s using the following lemma: 
\begin{lemma}\label{le:tilted_eqivalence}(Lemma~\ref{le:main_body_equivalence} in the main body)
Let $\bx \in \R^K$ and $\tau > 0$. Then
    \[
         \log \left(\sum_{k=1}^K e^{\tau x_k}\right)=\max_{\substack{\mathbf{y} \in \mathbb{R}_{+}^K \\ \sum_{k=1}^K y_k=\tau}}\left(\sum_{k=1}^K y_k x_k-\sum_{k=1}^K \frac{y_k}{\tau} \log \left(\frac{y_k}{\tau}\right)\right)
    \] 
\end{lemma}

{\it Proof:}
Let 
\[
f(\bx) = \log \left(\sum_{k=1}^K e^{\tau x_k}\right) 
\]
Then, the conjugate dual of the function $f(\cdot)$ can be computed as 
\[
    f^{\star}(\mathbf{y})=\sup _{\mathbf{x}}\left(\sum_{k=1}^K x_k y_k-\log \left(\sum_{k=1}^K e^{\tau x_k}\right)\right)
\]
Taking the partial derivative of the objective with respect to $x_i$ and setting it to zero gives
\begin{align}
     x_k^\star = \frac{1}{\tau}\log\left( \frac{\phi}{\tau}\right) + \frac{1}{\tau}\log\left(y_k\right) \nonumber 
\end{align}
where $\phi \triangleq \sum^K_{k=1}e^{\tau x_k}$. Substituting the optimal value of $x_k^\star$, we get
\begin{align}
    f^{\star}(\mathbf{y}) &= \sum^K_{k=1} y_k \left(\frac{1}{\tau}\log\left(\frac{\phi}{\tau}\right) + \frac{1}{\tau}\log y_k\right) - \log\left(\sum^K_{k=1} \frac{\phi y_k}{\tau}\right) \nonumber \\
    &= \sum^K_{k=1} \frac{y_k}{\tau} \log\left(\frac{\phi}{\tau}\right) + \sum^K_{k=1}\frac{y_k}{\tau}\log\left(y_k\right) - \log\left(\sum^K_{k=1} \frac{\phi y_k}{\tau}\right) \nonumber \\
    &\stackrel{(a)}{=} \log\left(\frac{\phi}{\tau}\right) + \sum^K_{k=1} \frac{y_k}{\tau}\log(y_k) - \log(\phi) \nonumber \\
    &= -\log(\tau) + \sum^K_{k=1} \frac{y_k}{t} \log y_k \nonumber \\
    &= \sum^K_{k=1} \frac{y_k}{\tau} \log(\frac{y_k}{\tau}) \nonumber
\end{align}
where $(a)$ uses the condition that $\sum^K_{k=1}y_k=\tau$. We apply Fenchel's duality theorem again, and then we have
\[
     f(\bx) = f^{\star\star}(\bx) = \max _{\substack{\mathbf{y} \in \mathbb{R}^K \\ \sum_{k=1}^K y_k=\tau}}\left(\sum_{k=1}^K y_k x_k-\sum_{k=1}^K \frac{y_k}{\tau} \log \left(\frac{y_k}{\tau}\right)\right),
\]
which completes the proof. 

\vspace{0.3cm}
\begin{lemma}\label{le:fairness_optimal_y}
    For the problem  
    \[
    \max _{\substack{\mathbf{y} \in \mathbb{R}_{+}^K \\ \sum_{k=1}^K y_k=\tau}}\left(\sum_{k=1}^K y_k x_k-\sum_{k=1}^K \frac{y_k}{\tau} \log \left(\frac{y_k}{\tau}\right)\right),
    \] the optimal $\mathbf{y}$ is given by
    \[
        y_k^\star =\frac{\tau e^{\tau x_k-1}}{\sum^K_{k=1} e^{\tau x_k -1}}
    \]
\end{lemma}
{\it Proof:}
%
%
%
%
%
We start by forming and maximizing the Lagrangian function
\[
    \max _{\substack{\mathbf{y} \in \mathbb{R}^K }}\left(\sum_{k=1}^K y_k x_k-\sum_{k=1}^K \frac{y_k}{\tau} \log \left(\frac{y_k}{\tau}\right) + \mu \left(\sum_{k=1}^K y_k - \tau\right)\right)
\] 
where $\mu$ is a free variable. Taking the partial derivative of the objective with respect to $y_k$ and setting it to zero gives 
\[
        y_k^\star =\alpha\tau e^{\tau x_k-1},
\]
where the coefficient $\alpha$ should be chosen such that $\sum_k y_k^* = 1$, implying
\[
        y_k^\star =\frac{\tau e^{\tau x_k-1}}{\sum^K_{k=1} e^{\tau x_k -1}}.
\]

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%

%
%
%

%
%
%
%
    
%

%

%

%
%
%
%
%
%
%

%



%
%

%
%
%
%
%
%
%
%
%

%

%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

%

%




%
%
%
%
%
%
%
%
%
%
%
%
%

%
%

%
%
%
%
%
%
%
  
%
%
