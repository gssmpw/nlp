\documentclass{article}

\input{math_commands.tex}
\input{include}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{comment}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}
%
\newcommand{\theHalgorithm}{\arabic{algorithm}}


%
%

%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm, enumitem}
\usepackage[capitalize,noabbrev]{cleveref}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
%
\PassOptionsToPackage{numbers, compress}{natbib}
%


%
%


%
%
\usepackage[preprint]{neurips_2024}


%
%


%
%


\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %


\title{PiKE: Adaptive Data Mixing for Multi-Task Learning Under Low Gradient Conflicts}



\author{%
Zeman Li$^{1,2}$\thanks{Work done while interning at Google Research.} \quad Yuan Deng$^{2}$ \quad Peilin Zhong$^2$ \quad Meisam Razaviyayn$^{1,2}$ \quad Vahab Mirrokni$^2$ \\
$^1$University of Southern California \quad $^2$Google Research\\
\texttt{\{zemanli,razaviya\}@usc.edu}\\
\texttt{\{dengyuan,peilinz,mirrokni\}@google.com}
}

\begin{document}


\maketitle


\begin{abstract}
    \input{abs}
\end{abstract}





\input{intro}

\begin{figure*}[!tb]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/multilingual_large_convergence.pdf}
  %
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glam_large_convergence.pdf}
  %
\end{minipage}
\caption{\footnotesize \textbf{Pre-training metric (average downstream task accuracy)}, higher is better. \textbf{\small Left:} 1B models on multilingual C4 (en) and C4 (hi) datasets. \textbf{\small Right:} 750M models on GLaM datasets with six domains. 
PiKE dynamically optimizes $K$-task weights during language model pre-training. 
We compare PiKE against baselines in two multitask learning scenarios: multilingual training  and the training on GLaM dataset. Mix uses equal batch size for each task ($b_k = b/K, \forall k \in K$), GLaM~\cite{du2022glam} uses fixed domain weights tuned for downstream performance, and DoReMi~\cite{xie2024doremi} requires pre-training a smaller model to determine optimized weights for training larger models. PiKE introduces negligible computation and memory overhead while outperforming all baselines. In pre-training 1B language models on multilingual C4 (en) and C4 (hi), PiKE improves average downstream accuracy by 7.1\% and achieves baseline accuracy $1.9\times$ faster. For 750M models pre-trained on the GLaM dataset, PiKE improves average downstream accuracy by 3.4\% compared to DoReMi. Tables~\ref{tab:main-table-multilingual} and~\ref{tab:main-table-glam} provide additional experiments and detailed results. }
\end{figure*}
\normalsize

\input{prelim}
\input{method}
\input{exp}
\input{conclusion}


\bibliography{reference}
\bibliographystyle{abbrvnat}

%
\appendix
\input{app}


\end{document}