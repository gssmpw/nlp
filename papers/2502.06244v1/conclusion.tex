\section{Conclusion}
In this work, we introduced PiKE, an adaptive data mixing algorithm for multitask learning that dynamically adjusts task sampling based on gradient interactions. Unlike prior approaches that focus on mitigating gradient conflicts, PiKE leverages the positive gradient interactions commonly observed in large-scale language model training. Our theoretical analysis established the convergence guarantees of PiKE, while empirical results demonstrated its effectiveness across diverse pretraining scenarios. Furthermore, we extended PiKE to incorporate fairness considerations, ensuring balanced learning across tasks. Our results indicate that Fair-PiKE effectively reduces task performance disparities while maintaining strong overall model performance.

A key limitation of our work is that PiKE does not explicitly account for data abundance when adjusting sampling weights. Future work could explore integrating dataset prevalence into the adaptive mixing strategy to further optimize learning efficiency. Additionally, extending PiKE to other domains beyond language modeling presents an exciting direction for future research.

%

%

%