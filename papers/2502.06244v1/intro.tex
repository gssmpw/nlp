\section{Introduction}

Modern foundation models, such as large language models (LLMs), have demonstrated impressive generalization and multitask learning capabilities by pretraining on diverse datasets across multiple domains~\citep{liu2024deepseek, team2024gemini, chowdhery2022palm, radford2019language}. The effectiveness of these models is heavily influenced by the composition of their training data~\citep{du2022glam, hoffmann2022empirical}. However, determining the optimal data mixture (across different tasks and data sources) remains a fundamental challenge due to the substantial size of both models and datasets, as well as the high computational cost of training. In most cases, training large models is limited to a single experimental run, making it impractical to iteratively fine-tune the weights of different data sources/tasks. 
%



%


Current approaches to multitask learning typically rely on fixed dataset weights (aka mixing or sampling strategies), often determined heuristically or based on the performance of smaller proxy models. For example, mT5~\citep{xue2020mt5} assigns dataset weights based on their relative abundance, GLaM~\citep{du2022glam} selects weights by evaluating downstream performance on smaller models, and the 405B LLaMA-3 model~\citep{dubey2024llama} heuristically constructs its training corpus from diverse sources. More recently, DoReMi~\citep{xie2024doremi} introduced a method that pretrains a small model using group distributionally robust optimization to determine dataset weights for larger-scale training. However, the optimality of these approaches is unclear, as the capabilities of large and small models differ significantly~\citep{team2024gemma, wortsman2023small}. Moreover, the loss landscape evolves throughout training~\citep{zhang2024transformers, li2018measuring}, meaning that static dataset weights determined at initialization may not remain optimal (as we will further elaborate in Section~\ref{sec:motivation}).




%


Another line of research addresses multitask optimization by modifying gradient updates to mitigate gradient conflicts, where task gradients point in opposing directions, slowing down optimization. Techniques such as PCGrad~\citep{yu2020gradient}, GradNorm~\citep{chen2018gradnorm}, and MGDA~\citep{desideri2012multiple} attempt to minimize these conflicts by adjusting gradient directions during training. While these methods improve performance, they introduce significant computational and memory overhead, making them impractical for large-scale models with numerous tasks~\citep{xin2022current}. Furthermore, while gradient conflicts are prevalent in vision-based multitask learning~\citep{wang2020gradient, liu2021conflict} and small-scale language models, we observe that they rarely occur when training large language models, as we will elaborate in Section~\ref{sec:method}. Instead, task gradients in such models often exhibit positive interactions, suggesting that existing conflict-mitigation strategies may not be necessary for large-scale multitask learning. Given these observations, we pose the following question:




%


\begin{center}
%
\emph{Can we design a multitask learning mixing strategy that leverages the absence of gradient conflict to improve efficiency and performance in training large models on diverse datasets?}
\end{center}


To answer this, we introduce PiKE (Positive gradient interaction-based K-task weight Estimator), a novel \textit{adaptive} data mixing strategy that dynamically adjusts task contributions throughout training. Unlike static and heuristic approaches, PiKE optimizes data allocation based on gradient information, effectively leveraging positive gradient interactions to enhance model performance. Our key contributions are as follows:


%
\begin{enumerate}[leftmargin=*]
\item  We propose PiKE, an approach that dynamically adjusts the mixture of data sources during training based on task gradient magnitudes and variance. This enables PiKE to scale efficiently with increasing model size and the number of tasks, overcoming the limitations of static  and heuristic task weighting strategies.

    %

\item  We establish the theoretical convergence of PiKE when applied with stochastic gradient descent (SGD). Additionally, we extend PiKE to incorporate tilted empirical risk minimization~\citep{li2020tilted, mo2000fair}, promoting fair learning across tasks and preventing task underrepresentation.

    %

\item  We conduct comprehensive experiments across various language multitask learning settings, including pretraining language models on multilingual text corpora and English datasets from diverse domains. Across different scales (110M, 270M, 750M, and 1B parameters) and scenarios, PiKE consistently outperforms existing static and heuristic data mixing methods. Notably, in multilingual pretraining for 1B models, PiKE improves average downstream accuracy by $7.1\%$ and achieves baseline accuracy $1.9\times$ faster. On the GLaM dataset with 750M models, PiKE surpasses DoReMi \citep{xie2024doremi} by $3.4\%$. Importantly, PiKE achieves these improvements with only negligible additional computational overhead.

    %
\end{enumerate}

The rest of this paper is structured as follows. Section 2 introduces notations and problem formulation. Section 3 presents the PiKE algorithm, its theoretical analysis, and an extension for fairness. Section 4 provides experimental results, followed by discussions in Section 5. Further related work is  discussed in Appendix~\ref{sec: related}.
    
%








%
%



%

%


%

%


