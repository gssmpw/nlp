\section{Related Work}
\label{sec: related}

\textbf{\small Data Curation and Selection.} The effectiveness of language models heavily depends on the quality of the pre-training corpus. Consequently, significant efforts have been made to enhance pre-training data. These efforts include heuristic-based filtering~\citep{raffel2020exploring, rae2021scaling, laurenccon2022bigscience, penedo2023refinedweb, soldaini2024dolma} and deduplication~\citep{abbas2023semdedup, lee2021deduplicating, chowdhery2022palm, dubey2024llama}. Recently, \cite{vo2024automatic} proposed an automated method for constructing large, diverse, and balanced datasets for self-supervised learning by applying hierarchical k-means clustering. \cite{sachdeva2024train} introduced techniques that leverage instruction-tuned models to assess and select high-quality training examples, along with density sampling to ensure diverse data coverage by modeling the data distribution. Additionally, \cite{guu2023simfluence} simulated training runs to model the non-additive effects of individual training examples, enabling the analysis of their influence on a model's predictions.

\textbf{\small Multitask Learning Optimization}  
The approach most closely related to our method is multitask learning (MTL) optimization, which modifies gradient updates to mitigate gradient conflictsâ€”situations where task gradients point in opposing directions, slowing down optimization~\citep{vandenhende2021multi, yu2020gradient}. The Multiple Gradient Descent Algorithm (MGDA)~\citep{desideri2012multiple, sener2018multi} updates the model by optimizing the worst improvement across all tasks, aiming for equal descent in task losses. Projected Gradient Descent (PCGrad)~\citep{yu2020gradient} modifies task gradients by iteratively removing conflicting components in a randomized order, ensuring that updates do not interfere destructively across tasks. Conflict-Averse Gradient Descent (CAGRAD)~\citep{liu2021conflict} optimizes for the worst task improvement while ensuring a decrease in the average loss. NASHMTL~\citep{navon2022multi} determines gradient directions by solving a bargaining game that maximizes the sum of log utility functions. While these methods improve performance, they introduce significant computational and memory overhead, making them impractical for large-scale models with numerous tasks~\citep{xin2022current}. Similar challenges exist in AdaTask~\citep{yang2023adatask}, which improves multitask learning by balancing parameter updates using task-wise adaptive learning rates, mitigating task dominance, and enhancing overall performance. Unlike previous approches that requires  requiring \(O(K)\) storage for task gradients (e.g. PCGrad) or optimizer states (e.g. AdaTask), FAMO~\citep{liu2024famo} balances task loss reductions efficiently using \(O(1)\) space and time. However, these methods fail to exploit the~\textit{non-conflicting} interactions among tasks, focusing instead on resolving conflicts that seldom arise. This highlights the need for a new approach that actively leverages lack of gradient conflicts to enhance training efficiency. 

Another line of work focuses on adjusting the domain mixture to improve data efficiency during training~\citep{xie2024doremi, xia2023sheared, jiang2024adaptive}. However, these methods require a target loss for optimization, which has been shown to not always correlate with downstream performance~\citep{tay2021scale, liu2023same, wettig2024qurating}. In contrast, our method leverages the absence of gradient conflict and the presence of positive gradient interactions between tasks or domains. This approach provides a more reliable and effective way to enhance the final model's performance.