\vspace{-0.4cm}
\section{Preliminaries}\label{sec: preliminary}

\subsection{Problem Definition and Notations} \label{sec:Notations}


We aim to train a \emph{single} model with parameters \(\ttheta \in \mathbb{R}^d\) to perform \(K \geq 2\) tasks simultaneously. Each task is associated with a smooth (possibly non-convex) loss function \(\ell_k(\ttheta, x): \mathbb{R}^d \times \mathbb{R}^{d_x} \to \mathbb{R}\) where $x$ is the data point. Then, it is common to minimize  the total expected loss:
\begin{equation}  \label{eq:prob_def}
\min_{\ttheta \in \mathbb{R}^d} \cL(\ttheta) := \sum_{k=1}^{K} \mathbb{E}_{x \sim \cD_k} [\ell_k(\ttheta; x)],  
\end{equation}  

where \(\cD_k\) represents the data distribution for task \(k\). We define \(\cL_k(\ttheta) := \mathbb{E}_{x \sim \cD_k} [\ell_k(\ttheta; x)]\).
%
For notation, \(\|\cdot\|\) represents the Euclidean norm, \(\tr(\cdot)\) denotes the trace operator, and a function \(h\) is \(L\)-Lipschitz if \(\|h(\ttheta) - h(\ttheta')\| \leq L \|\ttheta - \ttheta'\|\) for any \(\ttheta, \ttheta'\) in the domain of $h(\cdot)$. A function \(f(\cdot)\) is \(L\)-smooth if its gradient is \(L\)-Lipschitz continuous.






%
%
%
%
%
%

%




\subsection{Sampling Strategies: Random, Round-Robin, and Mix}

To optimize~\eqref{eq:prob_def} using stochastic optimizers such as Adam or SGD, we must select batches from one or multiple tasks at each training step. The choice of batch selection strategy significantly impacts model performance~\citep{bengio2009curriculum, ge2024data, ye2024data, xie2024doremi, liu2024regmix}. Below, we define three common sampling strategies: \textit{Random}, \textit{Round-Robin}, and \textit{Mix}.

\paragraph{Random Sampling.} At each step, a single task \( k \) is randomly chosen with probability \( p_k \) (\(\sum_{k=1}^K p_k = 1\)), and a batch of \( b \) samples is drawn from \( \mathcal{D}_k \) (dataset of task~$k$). The model parameters \( \ttheta \) are updated using the gradient of the selected task's loss function evaluated on the batch.

\paragraph{Round-Robin Sampling.} Tasks are selected cyclically, ensuring each task is chosen once every \( K \) steps. At iteration \( t \), task \( k = (t \bmod K) + 1 \) is selected, and a batch is sampled from \( \mathcal{D}_k \). The model parameters are then updated based on the  loss gradient evaluated on the selected batch.

\paragraph{Mix Sampling.} Each batch contains samples from all \( K \) tasks, with \( b_k \) samples drawn from \( \mathcal{D}_k \) such that the total batch size is \( b = \sum_{k=1}^K b_k \). 
The model  update at iteration~$t$ is based on the combined gradient:
\begin{equation}\label{eq: mix_framework}
\g_t = \frac{1}{b} \sum_{k=1}^{K} \sum_{i=1}^{b_k} \nabla \ell_k (\ttheta_t; x_i), \; x_i \sim \mathcal{D}_k.
\end{equation}
 Unlike the Random and Round-Robin, Mix strategy  ensures that each task contributes  to the computed gradient at each optimization step.

Historically, \textit{Mix} has been preferred in computer vision multitask learning~\citep{dai2016instance, misra2016cross, chen2018gradnorm, ruder2019latent, yu2020gradient, liu2024famo}, while \textit{Random} and \textit{Round-Robin} have been more common in early language model multitask training~\citep{liu2015representation, luong2015multi, liu2019multi}.  
Recent studies on large-scale language models~\citep{devlin2018bert, raffel2020exploring, brown2020language, team2023gemini} have revisited these strategies, finding that **Mix** generally yields superior performance, particularly when training across diverse datasets~\citep{du2022glam, chowdhery2023palm, xie2024doremi, raffel2020exploring, gao2020pile, wang2019superglue}. Figure~\ref{fig: motivation_why_mix}  (and Figure~\ref{fig:app_mix_round_rr_comparison} in the appendix) illustrate this by comparing downstream accuracy on multilingual mC4~\citep{xue2020mt5} and GLaM~\citep{du2022glam} datasets. Across all scenarios, \textit{Mix} consistently outperforms the other two strategies, motivating its use in pretraining large language models.


%

%



%




%
%





%
%
%
%
%
%



%



%
%




%

\subsection{Gradient Conflicts in Multitask Learning}
A key challenge in multitask learning prior literature is managing \textit{gradient conflicts}~\citep{liu2021conflict, yu2020gradient}, where the gradient of a task opposes the overall optimization direction. Formally, a conflict occurs at iteration \( t \) if there exists a task \( k \) such that  
\[
\langle \nabla \cL (\ttheta_t), \nabla \cL_k(\ttheta_t) \rangle < 0,
\]
indicating that updating \( \ttheta_t \) may increase the loss for task \( k \), thereby hindering balanced learning across tasks.
Existing methods attempt to mitigate gradient conflicts by adjusting gradients~\citep{yu2020gradient}, but these approaches introduce computational overhead, often requiring \(\mathcal{O}(K)\) complexity per step, making them impractical for large-scale models.  

While gradient conflicts are common in vision-based multitask learning and small-scale language models, we observe that they rarely occur in large-scale language model training. \textbf{In such models, task gradients are typically aligned (or close to orthogonal) rather than conflicting.} This insight suggests that instead of mitigating conflicts, a more effective strategy is to leverage nonnegative gradient interactions to enhance training efficiencyâ€”a key motivation for our approach, as discussed in the next section.




%

%
%
%
%
%
%
%
%

%



%
%









%
%
%
%
%
%
%
%
%

%

%



%


