%
%
%
%
%
%
%
%
%
%
%
%
\begin{figure}[!tb]
\vskip 0.2in
\begin{minipage}{.5\textwidth}
     \centering
    \includegraphics[width=1\columnwidth]{figures/glam_large_convergence_smooth.pdf}
  %
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/glam_cos_similarity.pdf}
  %
\end{minipage}
\caption{\footnotesize \textbf{Left: }Average accuracy across four downstream tasks (ArcE, CSQA, HellaSwag, and PIQA) for 750M GPT-2 large-style language models pre-trained using Mix, Round-Robin, and Random sampling strategies. Mix allocates equal batch sizes ($b_k = b/K, \forall k \in K$), while Random employs uniform sampling ($p_k= 1/K, \;\forall k$). Additional results are available in Appendix~\ref{app: mix_rr_random}. \textbf{Right: }  Cosine similarity between task gradients during pre-training 750M GPT-2 style language model on GLaM datasets. ``\textit{data1-data2}'' denotes the cosine similarity between the gradient evaluated on \textit{data1} (task~1) and the gradient of \textit{data2} (task~2). More results can be found in Appendix~\ref{app: cos_sim_grad_conflict}.}
\label{fig: motivation_why_mix}
\end{figure}

\normalsize

\section{Method}\label{sec:method}  
\subsection{Motivation}\label{sec:motivation}  

Our approach is based on two key observations: (1) gradient conflicts are rare in LLMs, and (2) the Mix sampling strategy can be made adaptive rather than static:  

\subsubsection{Reevaluating Gradient Conflicts in LLMs}  
The assumption that gradient conflicts dominate multitask learning does not necessarily hold for LLM pretraining. Our experiments show that task gradients in such models exhibit minimal conflicts.  
To illustrate this, we pretrain (i) a 1B GPT-2-style~\citep{radford2019language} model on the multilingual mC4 dataset~\citep{xue2020mt5} (six languages: English, Hindi, German, Chinese, French, and Arabic) and (ii) a 750M model on the GLaM dataset~\citep{du2022glam} (English text from six domains). Experimental details are in Appendix~\ref{app:experiment_setup}. Figures~\ref{fig: motivation_why_mix} and~\ref{fig:app_mutlilingual_grad_similarity} show cosine similarity trends for task gradients. Key observations are:  1) Gradient similarity starts high but decreases over time. 2) Multilingual gradient similarity varies with linguistic proximity (e.g., English-German align closely), while GLaM tasks exhibit uniformly aligned gradients.  
3) Task gradients rarely conflict—multilingual cosine similarity seldom drops below $-0.1$, while GLaM gradients remain mostly positive. These patterns align with prior work~\citep{wang2020gradient}.  


These findings challenge the conventional focus on mitigating gradient conflicts in multitask learning. Therefore, \textbf{instead of reducing conflicts, we should leverage non-conflicting gradients}.  
Existing conflict-aware methods like PCGrad~\citep{yu2020gradient} and AdaTask~\citep{yang2023adatask} are ineffective in this setting since they focus on resolving gradient conflict (which is indeed not present). As shown in Figure~\ref{fig: motivation_different_mtl}, 1) PCGrad performs similarly to Mix, as it only adjusts gradients when conflicts occur—which is rare.  2) AdaTask converges slower due to noisy gradients and suboptimal optimizer state updates. Additionally, both methods are memory-intensive, requiring \(O(K)\) storage for task gradients (PCGrad) or optimizer states (AdaTask), making them impractical for large models like the 540B PaLM~\citep{chowdhery2022palm}.  

Crucially, these methods fail to exploit the \textit{non-conflicting} interactions among tasks, focusing instead on resolving conflicts that seldom arise. This highlights the need for a new approach that actively leverages lack of gradient conflicts to enhance training efficiency.




%
%
%

%
%
%
%
%

%
%
%
%
%

%
%
%
%

%








%
%

%

%

%

%

%

%
%
%
%
%
%
%
%


%

%

%

%

%

%

%
%


\subsubsection{Adaptive versus Static Mixing}  

Prior work using the Mix sampling strategy typically relies on fixed (static) sampling weights, keeping $(b_1, \ldots, b_K)$ constant throughout training. However, dynamically adjusting batch composition can significantly enhance efficiency. We illustrate this with a simple example:

\begin{example}\label{example:Adaptive} Consider training on $K=2$ tasks with losses $\ell_1(\ttheta; x_1) = \frac{1}{2}(\ttheta^\top e_1)^2 + x_1^\top \ttheta$ and $\ell_2(\ttheta; x_2) = \frac{1}{2}(\ttheta^\top e_2)^2 + x_2^\top \ttheta$, where $e_1 = [1\; 0]^\top$, $e_2 = [0\; 1]^\top$, and $\ttheta \in \mathbb{R}^2$. Data for task 1 follows $x_1 \sim \cN(0, \sigma_1^2 I)$, while task 2 follows $x_2 \sim \cN(0, \sigma_2^2 I)$. The overall loss for task $k$ simplifies to $\cL_k(\ttheta) = \frac{1}{2}(\ttheta^\top e_k)^2$.
Using $b_1$ samples from task 1 and $b_2$ samples from task 2 in a batch at iteration $t$, the gradient is:
%
\[
\bg_t = \frac{1}{b_1+b_2}\left(b_1e_1e_1^\top + b_2 e_2 e_2^\top\right)\ttheta_t + \bz,
\]
where $\bz \sim \cN(0, \frac{b_1 \sigma_1^2 + b_2 \sigma_2^2}{b^2} I)$ with $b = b_1 + b_2$. Updating $\ttheta_t$ via SGD, $\ttheta_{t+1} = \ttheta_t - \eta \bg_t$, we have
\begin{equation}\label{eq:ExampleExpectedLoss}
\begin{split}
    \E[\cL(\ttheta_{t+1})] &= \frac{1}{2}(1-\eta \frac{b_1}{b})^2\theta_{1,t}^2 +\frac{1}{2}(1-\eta \frac{b_2}{b})^2\theta_{2,t}^2\\
    &\quad +\eta^2\frac{b_1\sigma_1^2+b_2\sigma_2^2}{b^2},
\end{split}
\end{equation}
where $\theta_{1,t}$ and $\theta_{2,t}$ denote the first and second component of the vector $\ttheta_t$. The derivation details of~\eqref{eq:ExampleExpectedLoss} can be found in Appendix~\ref{sec:derivationExampleExpectedLoss}. 
Letting~$w_1:=\frac{b_1}{b}$, $w_2:=\frac{b_2}{b}$,  and relaxing them  to take real values, we can optimize the mixing weights $w_1$ and $w_2$ as
\begin{equation}\label{eq:ExampleOptimalMixing}
    w_1^* = \Pi\left(\frac{ b^{-1}(\sigma_2^2-\sigma_1^2)+ \eta^{-1}(\theta_{1,t}^2-\theta_{2,t}^2)+ \theta_{2,t}^2}{\theta_{1,t}^2+\theta_{2,t}^2}\right)
\end{equation}
and $w_2^* = 1-w_1^*$ where $\Pi(\xi) = \min\{\max\{\xi,0\},1\}$ is the projection operator onto the interval~$[0,1]$. This result shows that optimal batch composition~$b_1,b_2$ should evolve over time to maximize training efficiency. 

Figure~\ref{fig: ExampleAdaptiveMixingVsStatic} compares static mixing strategies with an adaptive approach based on \eqref{eq:ExampleOptimalMixing}, highlighting the superiority of adaptive mixing. Moreover, the adaptive mixing strategy in this example does not require any hyperparameter tuning, while finding the best static mixing requires tuning.
\end{example}  

\begin{figure}[!tb]  
\vskip 0.2in  
\begin{center}  
\centerline{\includegraphics[width=0.5\columnwidth]{figures/AdaptiveVsStatic.pdf}}  
\caption{Adaptive vs. static mixing for Example~\ref{example:Adaptive}. Adaptive mixing consistently outperforms static mixing.}  
\label{fig: ExampleAdaptiveMixingVsStatic}  
\end{center}  
\end{figure}  

This simple example mirrors key aspects of multitask learning in large models:  
1) The optimal solution \(\ttheta^* = 0\) minimizes all task losses simultaneously, reflecting the high expressive power of large models.  
2) Task gradients are  non-conflicting, resembling real-world gradient interactions observed in Figure~\ref{fig: motivation_why_mix}.  Moreover, \eqref{eq:ExampleOptimalMixing} further reveals that optimal data mixing depends on (1) gradient norm squared per task (\(\|\nabla \cL_1(\ttheta)\|^2 = \theta_1^2\), \(\|\nabla \cL_2(\ttheta)\|^2 = \theta_2^2\)) and (2) gradient variance (\(\sigma_1^2\), \(\sigma_2^2\)). As we will see next, these factors play a crucial role in defining optimal mixing strategies for more general settings.




\subsection{PiKE: Conceptual Version}
As discussed in Section~\ref{sec: preliminary}, Mix sampling provides greater stability and generalization than Random and Round-Robin in LLM pretraining. Therefore, we focus on Mix but adopt a dynamic rather than static approach, as motivated in Section~\ref{sec:motivation}.
To develop our method and motivated by the discussions in Section~\ref{sec:motivation}, we first quantify gradient conflicts:


\begin{definition}\label{def:Interaction_LB}
    For a given point $\ttheta$, we say gradients are $\ulc$-conflicted (with $\ulc\geq 0$) if, for all task pairs $j,k, j\neq k$,
    \begin{align}
          -\ulc \big(\|\nabla \cL_j(\ttheta)\|^2 + \|\nabla \cL_k(\ttheta)\|^2\big) &\leq  \lin{\nabla \cL_j(\ttheta) ,\nabla \cL_k(\ttheta)}.  \nonumber 
    \end{align}
    %
\end{definition}
The above definition is implied by a lower bound on the gradients cosine similarity. In particular, if $\frac{\lin{\nabla \cL_j(\ttheta) ,\nabla \cL_k(\ttheta)}}{\|\cL_j(\ttheta) \|\|\cL_k(\ttheta) \|} \geq -\tilde{c}$, then the gradients are $\ulc$-conflicted for $\ulc = \tilde{c}/2$. 
Therefore, experiments in section~\ref{sec:motivation} show that $\ulc$ is typically small for LLM training. The reader is also referred to Figures~\ref{fig:app_mutlilingual_grad_similarity} and~\ref{fig:app_glam_grad_similarity} in Appendix~\ref{app: cos_sim_grad_conflict}, where we plot the ratio~$\frac{\lin{\nabla \cL_j(\ttheta) ,\nabla \cL_k(\ttheta)}}{\|\cL_j(\ttheta) \|^2+\|\cL_k(\ttheta) \|^2}$  for the same experiment  in Figure~\ref{fig: motivation_why_mix}. 

While Definition~\ref{def:Interaction_LB} quantifies the conflict between gradients, we also observed in Section~\ref{sec:motivation} that the gradients of different tasks are also not completely aligned. To quantify the level of alignment, we define the following concept:

\begin{definition}\label{def:Interaction_UB}
   For a given point $\ttheta$, we say that the gradients are $\barc$-aligned (with $\barc\geq 0$) if, for all task pairs $j,k, j\neq k$,
    \begin{align}
          \lin{\nabla \cL_j(\ttheta) ,\nabla \cL_k(\ttheta)}  \leq \barc \|\nabla \cL_j(\ttheta)\|_2 \|\nabla \cL_k(\ttheta)\|_2. \nonumber
    \end{align}
\end{definition}
While \(\barc = 1\) and \(\ulc = 1/2\) always hold, smaller values allow for more refined analysis. Notably, when both \(\barc\) and \(\ulc\) are small, the value of \(\|\nabla \cL(\ttheta)\|\) is small if and only if  \(\|\nabla \cL_k(\ttheta)\|\) is small for all \( k \) (see Lemma~\ref{le: correlation_of_losses} in Appendix~\ref{app: theory}).  

%
To proceed, we make the following standard assumptions.




\begin{assumption}\label{as:assumption1}
For all tasks \(k \in \{1, \ldots, K\}\), the gradients are $L$-Lipschitz, unbiased, and have bounded variance:
\begin{align}
    & \|\nabla \cL_k(\ttheta_1) - \nabla \cL_k(\ttheta_2)\|\leq L\|\ttheta_1 - \ttheta_2\|, \;\;\forall \ttheta_1,\ttheta_2\\
    & \E_{x\sim \cD_k}[\nabla \ell_k(\ttheta; x)] = \nabla \cL_k(\ttheta), \;\;\forall \ttheta\\
    & \E_{x\sim \cD_k}[\|\nabla \ell_k(\ttheta;x) - \nabla\cL_k(\ttheta)\|^2] \leq \sigma_k^2,\;\;\forall \ttheta
\end{align}
\end{assumption}

Using a Mix batch with \( b_k \) samples per task~$k$, the estimated gradient follows \eqref{eq: mix_framework}. The next theorem characterizes the descent obtained under low conflict conditions:  


%

%




\begin{theorem}
%
\label{thm:DescentMainBody}
Suppose Assumption~\ref{as:assumption1} holds and   the gradients are $\ulc$-conflicted and $\barc$-aligned at~$\ttheta_t$ with
$\ulc <\frac{1}{K -2 + b/b_k}, \forall k$. Moreover, assume the gradient is computed according to the mix sampling~\eqref{eq: mix_framework}. Then, 
    \begin{align} \label{eq: theorem1}
        \E[\cL(\ttheta_t - \eta \g_t)] &\leq \cL(\ttheta_t) + \sum^K_{k=1} b_k\Big(-\frac{\eta}{b}\beta\|\nabla\cL_k(\ttheta_t)\|^2 + \frac{L\eta^2}{2b^2}\sigma_k^2\Big) \nonumber\\
        &\quad   + \sum^K_{k=1} b_k^2 \frac{L\eta^2}{2b^2}\gamma \|\nabla \cL_k(\ttheta_t)\|^2 
    \end{align}
    where $\beta \triangleq \min_{k} (1+\ulc(-K+2-\frac{b}{b_k}))$ and $\gamma \triangleq 1 + \barc (K-1)$.
\end{theorem}
A formal proof is provided in Theorem~\ref{thm: formal_descent_lemma} (Appendix~\ref{app: theory}). To maximize descent in Mix sampling, we minimize the right-hand side of~\eqref{eq: theorem1}. Assuming a large \( b \), we relax \( b_k \) to continuous values \( w_k = b_k / b \) and solve:  
\begin{equation} \label{eq:relax_problem}
%
    \min_{w_1,\ldots,w_K\geq0} \; \sum^K_{k=1}  w_k \lambda_k + \frac{1}{2} w_k^2\kappa_k 
    %
    \;\; \textrm{s.t.} \;\; 
    %
    \sum_{k=1}^Kw_k = 1
    %
%
\end{equation}
where $\lambda_k\triangleq -\eta\beta\|\nabla\cL_k(\ttheta)\|^2 + \frac{L\eta^2}{2b} \sigma_k^2$ and $\kappa_k\triangleq L\eta^2\gamma\|\nabla\cL_k(\ttheta)\|^2$. Using KKT conditions, the optimal solution is given by 

\vspace{-0.3cm}

\begin{equation}\label{eq:OptimalWeight}
w_k^* = \max\left\{0, -\frac{\mu + \lambda_k}{\kappa_k}\right\} 
\end{equation}


\noindent where $\mu$ is chosen such that $\sum_{k=1}^K w_k^* = 1$ (see Lemma~\ref{le:optimal_w_for_relax_problem},  Appendix~\ref{app: theory}). 
This   leads to the conceptual version of PiKE (Positive gradient Interactions-based K-task weight Estimator),  summarized in Algorithm~\ref{alg: Basic PiKE} in Appendix~\ref{sec:ConceptualPiKE}.



The conceptual version of PiKE (Algorithm~\ref{alg: Basic PiKE}) adaptively adjusts sampling weights. This adaptive adjustment makes the stochastic gradients biased, i.e., $\E [\bg_t] \neq \nabla \cL(\ttheta_t)$. Due to this introduced bias, the classical convergence results of SGD can no longer be applied. The following theorem establishes the convergence of  conceptual PiKE:


\begin{theorem}%
\label{thm:IterationComplexityConceptualPiKe}
Suppose the assumptions in Theorem~\ref{thm:DescentMainBody} hold and the Conceptual PiKE Algorithm (Algorithm~\ref{alg: Basic PiKE}) initialized at $\ttheta_0$ with the SGD optimizer in Step 10 of the algorithm. Let $\Delta_L = \cL(\ttheta_0) - \min_{\ttheta}\cL(\theta)$ and $\sigma_{\max} = \max_k \sigma_k$. Suppose $\delta>0$ is a given constant and the stepsize $\eta \leq \frac{\beta \delta}{L\sigma_{\max}^2/b + L\eta \delta}$. Then, after $T = \frac{2\beta \Delta_L}{\eta \delta}$ iterations, Algorithm~\ref{alg: Basic PiKE} finds a point $\bar{\ttheta}$ such that 
\begin{equation}
    \label{eq:boundedNormGrad}
    \mathbb{E} \|\nabla \cL_k(\bar{\theta})\|^2 \leq \delta,\quad \forall k=1,\ldots, K.
\end{equation}
Moreover, if we choose $\eta = \frac{\beta \delta}{L\sigma_{\max}^2/b + L\eta \delta}$, then the Conceptual PiKE algorithm requires at most 
\[
\bar{T} = \frac{2L\Delta_L(\sigma_{\max}^2/b + \gamma \delta)}{\delta^2 \beta^2}
\]
iterations to find a point satisfying~\eqref{eq:boundedNormGrad}.
\end{theorem}

 The proof of this theorem is provided in Theorem~\ref{thm:app_IterationComplexityConceptualPiKe}  in Appendix~\ref{app: theory}.
This theorem states that with enough steps, the gradient of all task losses become small. It is also worth noting that the gradient norm becomes small with the iteration complexity $T=O(1/\delta^2)$, which is the best known rate for nonconvex smooth stochastic setting. 





%
%
%
%
%
%
%


%
%

%

%
%


%

%


%
%
%
%
%
%
%
%
%




%
%

%


%

\normalsize

\subsection{PiKE: Simplified Computationally Efficient Version}  
\label{sec:PiKe}  

Solving \eqref{eq:relax_problem} requires estimating $\{\sigma_k\}_{k=1}^K$ and $\{\|\nabla \cL_k (\theta_t)\|^2\}_{k=1}^K$, which necessitates large batch computations, slowing convergence. To speed up the algorithm, we update these estimates every \(T\) iterations. However, this can cause abrupt changes in sampling weights \((w_1,\ldots, w_K)\), leading to instability, especially with optimizers like Adam, where sudden shifts may disrupt momentum estimates.  
To mitigate this, we update \((w_1, \dots, w_K)\) using a single mirror descent step on \eqref{eq:relax_problem}, ensuring gradual adjustments:  

\vspace{-0.3cm}

{\small  
\begin{align}  
    w_k \gets w_k \exp\left(  
    \alpha \eta (\beta-L\eta \gamma w_k) \|\nabla \cL_k(\ttheta)\|^2   
    - \frac{\alpha L \eta^2}{2b} \sigma_k^2  
    \right) \nonumber  
\end{align}  
}  

\vspace{-0.3cm}

\noindent followed by normalization: \( \bw \gets \bw / \|\bw\|_1 \), where \(\alpha\) is the mirror descent step size.  

Fine-tuning \( L, \gamma, \alpha, \) and \( \beta \) can be challenging, but we simplify this by noting two observations:  
1) The coefficient of \( \sigma_k^2 \) is constant, independent of \( w_k \).  
2) For small \( \eta \) and \( w_k < 1 \), the coefficient of \( \|\nabla \cL_k (\ttheta)\| \) remains nearly constant:  
$  
  \alpha \eta (\beta-L\eta \gamma w_k) \approx \alpha \eta \beta.
$  
Thus, in practice, we use tunable  constant coefficients for variance and gradient norm terms, simplifying implementation. The final algorithm is summarized in Algorithm~\ref{alg: main}.  



%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{algorithm}[!tb] 
    \begin{algorithmic}[1]
        \STATE {{\bfseries Input:} $\ttheta$, $T$, total batch size $b$, task $k$ dataset $\cD_k$, hyperparameters $\zeta_1$ and $\zeta_2$, prior weights~$\bw'$ } 
	\STATE {{\bfseries Initialize:} $w_k \gets 1/K$ or $w_k \gets w_k'$ } 
		\FOR{$t=0,1, \dots$}
                \IF{$t \mod T = 0$}
                    \STATE Estimate $\|\nabla \cL_k(\ttheta_t)\|^2$ and $\sigma_k^2$ for every $k$
                    %
                    %
                    %
                    \STATE $w_k \gets w_k \exp\bigg(\zeta_1  \|\nabla \cL_k(\ttheta_t)\|^2 - \frac{\zeta_2}{2b} \sigma_k^2 \bigg)$
                    \STATE  $\bw \gets \bw /\|\bw\|_1$
                        %
                        %
                        %
                        %
                        %
                        %
                    \STATE $(b_1,\ldots, b_K) \gets \textrm{round}(b (w_1,\ldots, w_K))$
                \ENDIF
		        \STATE Sample $b_k$ data points from each task~$k$
                \STATE Compute the gradient $\bg$ using the estimates samples
                \STATE{Update: $\ttheta_{t+1} \gets \textrm{Optimizer} (\eta, \ttheta_t,\bg)$} 
		\ENDFOR
    \end{algorithmic}
    \caption{PiKE: Positive gradient Interaction-based K-task weights Estimator}
    \label{alg: main}
\end{algorithm}


\subsection{PiKE: Fairness Considerations Across Tasks}  
Algorithm~\ref{alg: main} is designed to minimize the average loss across tasks as in \eqref{eq:prob_def}. To ensure fair learning across all tasks, we can consider a fairness-promoting objective based on \textit{tilted empirical risk minimization}~\citep{li2020tilted}, also known as the $\alpha$\textit{-fairness utility}~\citep{mo2000fair}:  
\begin{equation}\label{eq:fairness_def}  
   \min_{\ttheta} \widetilde{\cL}(\tau ; \ttheta) := \frac{1}{\tau} \log \left(\sum^K_{k=1} e^{\tau\cL_k(\ttheta)}\right).
\end{equation}  
\vspace{-0.2cm}

This formulation reduces to \eqref{eq:prob_def} as \(\tau \to 0\), while for \(\tau > 0\), it promotes fairness. In the limit \(\tau \to \infty\), it optimizes for the worst-case task loss, i.e., \(\max_k \cL_k(\ttheta)\), ensuring no task is disproportionately neglected. Moderate values of \(\tau\) balance fairness and performance.  

We can use Fenchel duality \citep{rockafellar2015convex}, to connect the objective in~\eqref{eq:fairness_def} to a weighted version of~\eqref{eq:prob_def} through the following lemma:
\begin{lemma}\label{le:main_body_equivalence}
Let $\bx \in \R_{+}^K$ and $\tau> 0$. Then,
{\small
\begin{align}
     \log \left(\sum_{k=1}^K e^{\tau x_k}\right) =\max _{\substack{\mathbf{y} \in \mathbb{R}_{+}^K \\ \sum_{k=1}^K y_k=\tau}}\left(\sum_{k=1}^K y_k x_k-\sum_{k=1}^K \frac{y_k}{\tau} \log \left(\frac{y_k}{\tau}\right)\right) \nonumber
\end{align}
}
\end{lemma}
The proof of this Lemma~\ref{le:tilted_eqivalence}  can be found in Appendix~\ref{app:fairness_theory}. Using this lemma, \eqref{eq:fairness_def} can be rewritten as 
\vspace{-0.2cm}
\begin{align}
    \min_{\ttheta} \quad \max_{\substack{\mathbf{y} \in \mathbb{R}_{+}^K \\ \sum_{k=1}^K y_k=\tau}} \sum^K_{k=1} y_k\cL_k(\ttheta) - \sum^K_{k=1} \frac{y_k}{\tau} \log \left(\frac{y_k}{\tau} \right), \nonumber
\end{align}  
ehrtr the optimal \(\mathbf{y}\), for a fixed \(\ttheta\), has a closed-form solution:  
\[
y_k^\star = \frac{\tau e^{\tau\cL_k(\ttheta)-1}}{\sum^K_{j=1} e^{\tau\cL_j(\ttheta) -1}}, \quad \forall k.
\]  
(see Lemma~\ref{le:fairness_optimal_y} in Appendix~\ref{app:fairness_theory}). On the other hand, fixing \(\mathbf{y}\), the problem reduces to a weighted minimization over tasks, where \textit{regular PiKE sampling} with proper weights $y_k$ in front of each loss can be applied to determine the optimal mixing strategy.  
This leads to \textit{fair-PiKE} algorithm, described in Appendix~\ref{app:pike-fairness}, which balances overall loss minimization and fair learning of all tasks.  




%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%








