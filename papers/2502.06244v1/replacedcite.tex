\section{Related Work}
\label{sec: related}

\textbf{\small Data Curation and Selection.} The effectiveness of language models heavily depends on the quality of the pre-training corpus. Consequently, significant efforts have been made to enhance pre-training data. These efforts include heuristic-based filtering____ and deduplication____. Recently, ____ proposed an automated method for constructing large, diverse, and balanced datasets for self-supervised learning by applying hierarchical k-means clustering. ____ introduced techniques that leverage instruction-tuned models to assess and select high-quality training examples, along with density sampling to ensure diverse data coverage by modeling the data distribution. Additionally, ____ simulated training runs to model the non-additive effects of individual training examples, enabling the analysis of their influence on a model's predictions.

\textbf{\small Multitask Learning Optimization}  
The approach most closely related to our method is multitask learning (MTL) optimization, which modifies gradient updates to mitigate gradient conflictsâ€”situations where task gradients point in opposing directions, slowing down optimization____. The Multiple Gradient Descent Algorithm (MGDA)____ updates the model by optimizing the worst improvement across all tasks, aiming for equal descent in task losses. Projected Gradient Descent (PCGrad)____ modifies task gradients by iteratively removing conflicting components in a randomized order, ensuring that updates do not interfere destructively across tasks. Conflict-Averse Gradient Descent (CAGRAD)____ optimizes for the worst task improvement while ensuring a decrease in the average loss. NASHMTL____ determines gradient directions by solving a bargaining game that maximizes the sum of log utility functions. While these methods improve performance, they introduce significant computational and memory overhead, making them impractical for large-scale models with numerous tasks____. Similar challenges exist in AdaTask____, which improves multitask learning by balancing parameter updates using task-wise adaptive learning rates, mitigating task dominance, and enhancing overall performance. Unlike previous approches that requires  requiring \(O(K)\) storage for task gradients (e.g. PCGrad) or optimizer states (e.g. AdaTask), FAMO____ balances task loss reductions efficiently using \(O(1)\) space and time. However, these methods fail to exploit the~\textit{non-conflicting} interactions among tasks, focusing instead on resolving conflicts that seldom arise. This highlights the need for a new approach that actively leverages lack of gradient conflicts to enhance training efficiency. 

Another line of work focuses on adjusting the domain mixture to improve data efficiency during training____. However, these methods require a target loss for optimization, which has been shown to not always correlate with downstream performance____. In contrast, our method leverages the absence of gradient conflict and the presence of positive gradient interactions between tasks or domains. This approach provides a more reliable and effective way to enhance the final model's performance.