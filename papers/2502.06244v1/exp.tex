%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{table*}[!tb]
\large
\centering
\caption{We report the perplexities (lower the better) on the validation split of multilingual C4 datasets. We also compare the accuracies (\%, higher the better) of different models on HellaSwag and its corresponding translated version.~\textbf{Bolding} indicates the best model in the task; $\ols{\text{Metrics}}$ means the average across different tasks. Additional results can be found in Table~\ref{tab:main-table-multilingual}.}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lcccccccc}
\toprule
            &               & C4 (en)       & C4 (hi)       & C4 (de) &               & HellaSwag (en) & HellaSwag (hi) & HellaSwag (de) \\ \cmidrule(lr){3-5} \cmidrule(l){7-9} 
 &
  $\ols{\text{Perplexity} \downarrow}$ &
  Perplexity $\downarrow$ &
  Perplexity $\downarrow$ &
  Perplexity $\downarrow$ &
  $\ols{\text{Accuracy}(\%) \uparrow}$ &
  0-shot $\uparrow$ &
  0-shot  $\uparrow$ &
  0-shot $\uparrow$ \\ \midrule
\multicolumn{9}{l}{\textbf{C4 (en), C4 (hi), and C4 (de) datasets, GPT-2 large style, 1B params, 36 Layers default, 120K training steps}}                     \\
Mix & 8.29 & 11.13 & 4.45 & 9.29 & 27.5 & 28.1 & 27.1 & 27.6 \\
Round-Robin & 8.41 & 11.31 & 4.97 & 9.46 & 26.5 & 27.6 & 26.7 & 26.3 \\
Random & 8.48 & 11.38 & 4.54 & 9.55 & 26.6 & 27.0 & 26.9 & 26.1 \\ \midrule
PiKE & 9.56 & \textbf{9.49} & 5.32 & 13.87 & 28.7 & \textbf{33.0} & 27.2 & 26.2 \\ 
Fair-PiKE ($\tau=1$) & 8.29 & 11.12 & \textbf{4.46} & 9.31 & 27.9 & 28.3 & \textbf{27.4} & 28.0 \\
Fair-PiKE ($\tau=3$) & \textbf{8.18} & 10.14 & 4.93 & 9.49 & \textbf{28.9} & 31.3 & 27.3 & 28.1 \\
Fair-PiKE ($\tau=5$) & 8.42 & 10.02 & 6.30 & \textbf{8.94} & \textbf{28.9} & 31.2 & 26.9 & \textbf{28.6} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:partial_main-table-multilingual}
\end{table*}

\begin{table*}[!tb]
\centering

\caption{We report perplexity (lower is better) on the validation split of the GLaM datasets, averaging perplexities across six domains when applicable or reporting a single perplexity when only training with a single domain. We also compare the accuracies (\%, higher the better) of different models on four different Q/A tasks. HellaSwag and ArcE tasks have 4 choices, CSQA has 5 choices, and PIQA has 2 choices. PiKE (Uniform) means PiKE using initial sampling weights of $1/6$ for each task and PiKE (GLaM) means PiKE using GLaM tuned weights as initial task weights. \textbf{Bolding} indicates the best model in the task, $\ols{\text{Metrics}}$ means the average across different tasks, \ul{underlining} indicates PiKE beating Mix, Round-Robin, Random methods. Additional results can be found in Table~\ref{tab:main-table-glam}.}

{\small
\begin{adjustbox}{max width=0.8\textwidth}
\begin{tabular}{lcccccc}
\toprule
                 & GLaM              &                      & ArcE                 & CSQA            & HellaSwag                 & PIQA                 \\ \cmidrule(l){4-7} 
 &
  $\ols{\text{Perplexity} \downarrow } $ &
  $\ols{\text{Accuracy} (\%) \uparrow}$ &
  7-shot $\uparrow$ &
  7-shot $\uparrow$ &
  7-shot  $\uparrow$ &
  7-shot $\uparrow$ \\ \midrule
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\multicolumn{7}{l}{\textbf{Six domains of GLaM dataset, GPT-2 large style, 750M params, 36 layers default}}                       \\
Mix &\textbf{12.77} & 46.4 & 47.2 & 39.6 & 37.9 & 60.9        \\
Round-Robin &12.98 & 44.3 & 43.5 & 36.7 & 36.8 & 60.3 \\
Random &12.99 & 42.7 & 41.7 & 34.2 & 36.6 & 58.2                 \\
GLaM &13.20 & 45.3 &46.9 & 39.8 & \textbf{38.0} & 56.4            \\
DoReMi &13.25 & 46.5 & 48.6 & 40.1 & 37.5 & 59.6         \\
PiKE (Uniform) &13.22 & \ul{47.6} &\ul{49.6} & \ul{43.2} & 37.2 &60.4   \\
PiKE (GLaM) &13.35 & \ul{\textbf{48.1}} & \ul{\textbf{49.8}} & \ul{\textbf{43.5}} & \ul{\textbf{38.0}} & \textbf{\ul{61.2}} \\
%
\bottomrule
\end{tabular}
\end{adjustbox}
}
\label{tab:partial_main-table-glam}
\end{table*}





%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




%
\section{Experiments}\label{sec:exp}  
We evaluate PiKE in two multitask pretraining scenarios:  
1) \textit{Pretraining language models on multilingual mC4 dataset}~\citep{xue2020mt5}, a dataset covering diverse languages from Common Crawl corpus.  
2) \textit{Pretraining language models on the   GLaM dataset}~\citep{du2022glam}, an English dataset spanning six domains.  
As we will see, across multiple model sizes (110M, 270M, 750M, and 1B parameters), \textit{PiKE consistently outperforms static and heuristic data mixing methods}. For 1B models trained on multilingual C4 (en, hi), PiKE improves average downstream accuracy by \textbf{7.1\%} and reaches baseline accuracy \textbf{1.9Ã— faster}. For 750M models pre-trained on the GLaM dataset, PiKE improves averge downstream accuracy by \textbf{3.4\%} over DoReMi \citep{xie2024doremi} and \textbf{6.2\%} over GLaM's original strategy.  


%

\subsection{Experiment Setup}  

\textbf{Baselines:}  
For multilingual pretraining, we compare five sampling strategies: 1.~\textit{Mix}, 2.~\textit{Round-Robin}, 3.~\textit{Random}, 4.~\textit{PiKE}, and 5.~\textit{fair-PiKE}. For GLaM-based pretraining, we evaluate: 1.~\textit{Mix}, 2.~\textit{GLaM}~\citep{du2022glam}, 3.~\textit{DoReMi}~\citep{xie2024doremi}, and 4.~\textit{PiKE}. DoReMi trains a small proxy model for weight estimation, while GLaM assigns static domain weights based on downstream performance of smaller models. In contrast, PiKE dynamically adjusts weights during training based on gradient information. Hence,   PiKE  does not require another smaller model and is computationally much more efficient than DoReMi and GLaM.

\textbf{Datasets:}  
For multilingual experiments, we use mC4~\citep{xue2020mt5}, focusing on English (en), Hindi (hi), and German (de). An overview of these datasets is provided in Table~\ref{tab: mc4-dataset-overview}. For GLaM-based experiments, we use the six-domain GLaM dataset~\citep{du2022glam}, with domain weights from~\citep{du2022glam, xie2024doremi}. Details regarding the GLaM dataset and the domain weights used by GLaM and DoReMi are presented in Table~\ref{tab: glam-dataset-overview}.  

\textbf{Evaluation:}  
Perplexity is measured on held-out validation data. Downstream evaluation follows the OLMES suite~\citep{gu2024olmes}. For multilingual downstream tasks, we use multilingual HellaSwag~\citep{dac2023okapi}, covering 26 languages. For  models trained on GLaM, we evaluate on downstream tasks ARC-Easy~\citep{clark2018think}, CommonsenseQA~\citep{talmor2018commonsenseqa}, PIQA~\citep{bisk2019reasoning}, and HellaSwag~\citep{zellers2019hellaswag}.  

Further details on our experimental setup and evaluation are in Appendix~\ref{app:experiment_setup}.  



%
%

%


%

%




\subsection{PiKE Outperforms Mix, Round-Robin, and Random in Multilingual Pretraining}  

Table~\ref{tab:partial_main-table-multilingual} presents results for pretraining a 1B multilingual GPT-2 model~\citep{radford2019language} on English, Hindi, and German, with additional results in Table~\ref{tab:main-table-multilingual}. We evaluate GPT-2 models at two scales (270M and 1B parameters) across two language settings: (1) English and Hindi, and (2) English, Hindi, and German.  

We observe that \textit{ PiKE and its fair variation consistently achieve the highest average accuracy of downstream tasks} across all language settings and model scales, demonstrating its effectiveness in multilingual pretraining.  

We also observe that \textit{fair-PiKE balances fairness among tasks.} We pre-trained 1B models using Fair-PiKE with different fairness parameters \(\tau \in \{1, 3, 5\}\). Higher \(\tau\) values promotes greater fairness by reducing the gap between task losses. At \(\tau = 5\), perplexity values across tasks become more uniform, indicating improved fairness. Notably, Fair-PiKE with \(\tau = 3\) achieves the best balance, yielding the lowest perplexity and highest downstream performance. These results highlight the benefits of incorporating fairness considerations in pretraining.  




%

%


%


%


\subsection{PiKE Outperforms DoReMi, GLaM, and Static Mix in Pretraining with GLaM Datasets}  

Table~\ref{tab:partial_main-table-glam} presents results for pretraining a 750M multilingual GPT-2 model on the GLaM dataset, with additional results in Table~\ref{tab:main-table-glam}. We evaluate two model sizes (110M and 750M) across six domains.  

\textit{PiKE consistently achieves the highest average performance.} In both 110M and 750M configurations, PiKE outperforms DoReMi, GLaM, and Mix in downstream accuracy. For 750M models, PiKE improves the average downstream task accuracy by \textbf{3.4\%} over DoReMi and \textbf{6.2\%} over GLaM. For 110M models, PiKE achieves \textbf{37.8\%} accuracy, surpassing DoReMi (\textbf{36.0\%}) and GLaM (\textbf{35.3\%}). Unlike DoReMi and GLaM, PiKE achieves these improvements without additional computational overhead, as DoReMi requires training a proxy model and GLaM involves tuning weights based on smaller models.  

\textit{PiKE benefits from apriori downstream-tuned weights.}  
We evaluate PiKE with two initializations: (1) uniform weights $b_k = b/K$ and (2) GLaM-tuned weights. In both small and large GPT-2 configurations, PiKE benefits from utilizing already fine tuned  weights as initialization, achieving \textbf{48.1\%} accuracy with GLaM-tuned weights vs. \textbf{47.6\%} with uniform initialization. This shows that PiKE can effectively leverage pre-existing fine-tuned weights while still outperforming other methods with uniform initialization.  

\textit{Mixing datasets improves language model generalization.}  
We compare models trained on individual domains to those trained on mixed-domain datasets. Table~\ref{tab:main-table-glam} shows that single-domain training underperforms compared to mixed-domain training, even with simple Mix sampling. This reinforces the importance of diverse data for pretraining and aligns with prior work~\citep{liu2024regmix, hoffmann2022empirical}.  

\textit{Discussion on perplexity.}  
Table~\ref{tab:main-table-glam} reveals that validation perplexity does not always align with downstream performance. For instance, while Mix sampling yields lower perplexity in 750M models, PiKE achieves better downstream accuracy. This aligns with prior findings~\citep{tay2021scale, liu2023same, wettig2024qurating}, suggesting that perplexity alone is not a reliable performance metric.  






%

%

%

%
%

%

%


