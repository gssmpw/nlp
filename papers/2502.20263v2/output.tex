%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf,authordraft]{acmart}
% \documentclass[sigconf, screen, review, anonymous]{acmart}
\documentclass[sigconf]{acmart}



% added by rongzhen
\usepackage{indentfirst}
\usepackage{bbm}
\usepackage{pifont}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tablefootnote}
\usepackage{microtype}  % tighten texts
\usepackage{colortbl} % Required for \arrayrulecolor

\usepackage{xcolor}
\definecolor{purple2}{HTML}{5523BE}
\definecolor{blue2}{HTML}{0F8FE6}
\definecolor{red2}{HTML}{D64781}

\newcommand{\mytilde}{\raisebox{0.5ex}{\texttildelow}}



%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}



%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
\acmSubmissionID{7307}

%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}



%% end of the preamble, start of the body of the document source.
\begin{document}



% added by Rongzhen
\settopmatter{printacmref=false} % Removes citation information in the footnote area
\setcopyright{none}             % Removes the copyright block
\renewcommand\footnotetextcopyrightpermission[1]{}  % Ensures no extra copyright text is printed
\pagestyle{plain}   % Switch to a plain style (typically no headers, only a centered page number)
% \fancyhead{}        % Clear any existing header settings (if fancyhdr is in use)



%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Vector-Quantized Vision Foundation Models for Object-Centric Learning}



%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen}
\affiliation{
\institution{Aalto University}
\city{Espoo}
\country{Finland}
}
\email{{rongzhen.zhao, vivienne.wang, juho.kannala, joni.pajarinen}@aalto.fi}



%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Zhao et al.}



%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Perceiving visual scenes as objects and background---like humans do---Object-Centric Learning (OCL) aggregates image or video feature maps into object-level feature vectors, termed \textit{slots}. OCL's self-supervision of reconstructing the input from these aggregated slots struggles with complex object textures, thus Vision Foundation Model (VFM) representations are used as the aggregation input and reconstruction target.
However, existing methods leverage VFM representations in diverse ways and often fail to fully exploit their potential.
In response, we propose a clean architecture---Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO)---that unifies mainstream OCL methods.
The key to our unification is simple yet effective, just shared quantizing the same VFM representation as the reconstruction target. 
Through mathematical modeling and statistical verification, we further analyze why VFM representations facilitate OCL aggregation and how their shared quantization as reconstruction targets strengthens OCL supervision.
Experiments show that across different VFMs, aggregators and decoders, our VVO consistently outperforms baselines in object discovery and recognition, as well as downstream visual prediction and reasoning.
The source code is available in supplemental files.
\end{abstract}



% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% %% Please copy and paste the code instead of the example below.
% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10010147.10010178.10010224.10010240</concept_id>
%        <concept_desc>Computing methodologies~Computer vision representations</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computing methodologies~Computer vision representations}

% %% Keywords. The author(s) should pick words that accurately describe
% %% the work being presented. Separate the keywords with commas.
% \keywords{
% Object-Centric Learning, Vision Foundation Model, Vector Quantization, Object Representation, Visual Prediction, Visual Reasoning
% }



% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}



%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle



\begin{figure}
% \centering
\includegraphics[width=0.99\linewidth]{res/teaser_fig.pdf}
\begin{minipage}{\linewidth}
\vspace{0.25\baselineskip}
\centering\footnotesize\sffamily
\input{res/teaser}
\end{minipage}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
We utilize two observations:
(\textit{i}) Compared to non-VFMs, VFMs extract features with better object separability, i.e., smaller intra-object distances and larger inter-object distances $\Rightarrow$ We facilitate OCL aggregation via VFM features;
(\textit{ii}) VFM and non-VFM features have a distribution gap, i.e., separated centroids $\Rightarrow$ 
We strengthen OCL supervision by reconstructing the quantized features shared from the same VFM rather than from another encoder.
}}
\label{fig:teaser}
\end{figure}
% \vspace{-0.5\baselineskip}



\section{Introduction}
\label{sect:introduction}

Objects can form highly diverse visual scenes through arrangements and combinations. But mainstream methods based on feature patches or a single feature vector disregard such compositionality. Inspired by human vision cognition \cite{bar2004visual, cavanagh2011visual, palmeri2004visual}, Object-Centric Learning (OCL) decomposes visual scenes into multiple feature vectors, known as \textit{slots}, each corresponding to an object or the background, thus enabling improved modeling of relationships and dynamics among objects. Object-centric representations have demonstrated superiority in advanced vision tasks, like prediction, reasoning, planning, and decision-making \cite{wu2022slotformer}, as well as in interactions between visual modality and other modalities \cite{zhang2024omgllava, wang2024omnidrive}.

Existing OCL methods typically adopt an encoder-aggregator-decoder architecture \cite{locatello2020slotattent}. Firstly, the encoder transforms input image or video frame pixels into a dense feature map. Then, the aggregator sparsifies this feature map into feature vectors via Slot Attention \cite{locatello2020slotattent} with initial slots as the query. Lastly, the decoder reconstructs the input in some form from these aggregated slots, to provide the self-supervised training signal.

OCL relies on pixel textures to discover objects.
The early milestone \cite{locatello2020slotattent} reconstructs input pixels for supervision, which usually fails on realistic objects. Some \cite{kipf2021savi, elsayed2022savipp} reconstruct optical flow or depth map to mitigate textural noises, but at the cost of expensive annotations. Some \cite{singh2021slate, singh2022steve} reconstruct input's VAE (Variational Autoencoder) representation, where super-pixels are codebook codes shared across samples, thus suppressing pixel redundancy and facilitating aggregation from the feature map into slots. 
Recent advances \cite{seitzer2023dinosaur, wu2023slotdiffuz} leverage Vision Foundation Models (VFMs) \cite{caron2021dino1, oquab2023dino2} to extract feature maps with better object separability, significantly improving OCL performance.

However, existing OCL methods leverage VFM representations in quite different ways, as shown in Figure~\ref{fig:compare_model}, and none of them fully utilize the power of VFM representations.

To address these issues, we propose a clean architecture---Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO)---that unifies mainstream OCL methods.
In this architecture, as shown in Figure~\ref{fig:vvo_arch}, our VVO supports different VFMs for encoding, different OCL aggregators and different OCL decoders.
The key to such unification is very simple---We quantize the representations from the same VFM, rather than from another encoder, as the reconstruction target.
We also mathematically and statistically analyze why VFM representations facilitate OCL aggregation and how their shared quantization as reconstruction targets strengthens OCL supervision.

Our contributions are: 
(\textit{i}) A clean architecture, which unifies mainstream OCL methods.
(\textit{ii}) Shared quantized VFM representations as reconstruction targets, which not only supports various OCL decoders but also boosts performance.
(\textit{iii}) Insights of why VFM features facilitate OCL aggregation and their shared quantization as reconstruction targets strengthens OCL supervision.



\section{Related Work}
\label{sect:related_work}

\textit{SSseg vs OCL}.
Both Self-Supervised Segmentation (SSseg) \cite{ge2023hcl, wang2024videocutler} and OCL can discovery objects, i.e., segmenting objects and the background in images or video frames without supervision. 
SSseg only outputs object segmentation masks, while OCL further represents objects as \textit{slots}. 
These slots can be directly used in downstream object-centric world models \cite{wu2022slotformer} for dynamics modeling, which is demanded in advanced vision tasks.
Thus a direct comparison between them is beyond the scope of this work.

\textit{OCL encoding}. 
The stronger encoder that extracts feature maps of better objectness contributes more to OCL performance. 
Early milestone methods like IODINE\cite{greff2019iodine} and SA \cite{locatello2020slotattent} use small naive CNNs \cite{krizhevsky2012alexnet} as OCL encoder. 
Followups like SAVi \cite{kipf2021savi}, SAVi++ \cite{elsayed2022savipp}, SLATE \cite{singh2021slate} and STEVE \cite{singh2022steve} employ pretrained ResNets \cite{he2016resnet}, and fine-tune them on OCL datasets. 
State-of-the-arts like SlotDiffusion \cite{wu2023slotdiffuz} and DINOSAUR \cite{seitzer2023dinosaur} utilize VFMs like DINO \cite{caron2021dino1} and DINO2 \cite{oquab2023dino2} ViTs (Vision Transformers) to extract highly object-separable feature map from input pixels, improving OCL performance significantly.
Although SAM \cite{kirillov2023sam} and SAM2 \cite{ravi2024sam2} are also well recognized VFMs, they remain unexploited in OCL setting. 
Our VVO supports various VFMs for OCL encoding.

\textit{OCL aggregation}. SlotAttention \cite{locatello2020slotattent} is the footstone for mainstream OCL methods. Subsequent works like BO-QSA \cite{jia2023boqsa}, ISA \cite{biza2023isa} and SysBind \cite{singh2022sysbind} are all its variants, which are designed without changing the external interface. But considering their performance boosts, we only integrate BO-QSA by default.

\textit{OCL decoding}. 
With SlotAttention as the aggregator, the decoder and its reconstruction target affect OCL performance the most, as it is the source of supervision.
Mixture-based decoding, used in SAVi, SAVi++, DINOSAUR and VideoSAUR \cite{zadaianchuk2024videosaur}, decodes each slot's spatial broadcast \cite{watters2019spatialbroadcast} using naive CNNs or MLPs, and mixes them with corresponding weights into the reconstruction.
Transformer-based decoding, used in SLATE, STEVE and SPOT \cite{kakogeorgiou2024spot}, reconstructs VAE representation of the input auto-regressively with slots as the condition.
Diffusion-based decoding in LSD \cite{jiang2023lsd} and SlotDiffusion drives slots to recover noise added to the input's VAE representation.
Our VVO supports all these types of OCL decoding.

\textit{VAE for OCL}. % TODO XXX VAE GDR MSF
Variational Autoencoders (VAEs), like dVAE \cite{im2017dvae} in SLATE and VQ-VAE \cite{van2017vqvae} in SlotDiffusion, are employed to produce reconstruction targets for OCL training.
Since these VAEs are designed for image generation, some methods adapt them for OCL.
Inspired by channel or weight grouping, GDR \cite{zhao2024gdr} decomposes features into attributes and combine them to produce VAE representation as reconstruction targets to guide OCL better.
MSF \cite{zhao2024msf} firstly exploits the multi-scale idea in the OCL setting with VAE-specific designs.
Based on recent advancement RVQ \cite{yang2023rvq} and SimVQ \cite{zhu2024simvq}, we design our own VQ variant for OCL.



\begin{figure*}[]
\centering
\includegraphics[width=0.975\linewidth]{res/vvo_arch.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
VVO is a unified architecture that fully utilizes VFMs in OCL. It not only extracts VFM features with better objectness to facilitate object information aggregation; but further quantizes those VFM features as reconstruction targets to strengthen OCL training supervision. With typical SlotAttention or it variants as the \textcolor{purple2}{aggregator} and Vector-Quantization as the \textcolor{purple2}{quantizer}, VVO supports different VFMs as the \textcolor{purple2}{encoder}, and supports mainstream mixture, auto-regression and diffusion models as the \textcolor{purple2}{decoder}.
}}
\label{fig:vvo_arch}
\end{figure*}
% \vspace{-0.5\baselineskip}



\section{Proposed Method}
\label{sect:proposed_method}

We propose Vector-Quantized Vision Foundation Models for Object-Centric Learning, or VQ-VFM-OCL (VVO), elegantly unifying mainstream OCL and consistently boosting their performance.



\subsection{Unify OCL}
\label{sect:background}


As shown in Figure~\ref{fig:vvo_arch}, our method adopts an architectural design of encoder-aggregator/quantizer-decoder.


\textit{Firstly}, OCL \textcolor{purple2}{encoder} $\bm{\phi}_{\mathrm{e}}$ transforms an \textcolor{blue2}{input} image or video frame $\bm{X} \in \mathbb{R} ^ {h_0 \times w_0 \times c_0}$, which is an observation of some visual scene, into a dense \textcolor{blue2}{feature map} $\bm{Z} \in \mathbb{R} ^ {h \times w \times c}$, for the following query-based aggregation:
\begin{equation}
\label{eq:ocl_encode}
\bm{\phi}_{\mathrm{e}} : \bm{X} \rightarrow \bm{Z}
\end{equation}
where $\bm{\phi}_{\mathrm{e}}$ can be parameterized as pretrained frozen VFMs, like DINO \cite{caron2021dino1}, DINO2 \cite{oquab2023dino2}, SAM \cite{kirillov2023sam} and SAM2 \cite{ravi2024sam2}. As OCL relies on textures to separate objects, $\bm{\phi}_{\mathrm{e}}$ should handle complex textures of objects, making VFMs necessary here. We will detail the reasons in Section~\ref{sect:maximize_vfms_in_ocl}. 


\textit{Secondly}, given \textcolor{blue2}{queries} $\bm{S}_0 \in \mathbb{R} ^ {n \times c}$, OCL \textcolor{purple2}{aggregator} $\bm{\phi}_{\mathrm{a}}$ transforms $\bm{Z}$ into multiple feature vectors or \textcolor{blue2}{slots} $\bm{S} \in \mathbb{R} ^ {n \times c}$ and byproduct \textcolor{blue2}{segmentation} masks $\bm{M} \in \mathbb{R} ^ {h \times w}$, corresponding to objects or the background in the scene:
\begin{equation}
\label{eq:ocl_aggregat}
\bm{\phi}_{\mathrm{a}} : (\bm{S}_0, \bm{Z}) \rightarrow (\bm{S}, \bm{M})
\end{equation}
where $\bm{\phi}_{\mathrm{a}}$ can be parameterized as widely adopted SlotAttention \cite{locatello2020slotattent} and its variants, which is some cross attention with $\bm{S}_0$ as queries and $\bm{Z}$ as keys and values. $\bm{M}$ is the binarized attention map thus intuitively reflects how well objects are represented by slots. 

For video OCL, there is a recurrent module turning current $\bm{S}$ into new queries $\bm{S}_0$ for the next time step. Such module can be Transformer encoder blocks \cite{vaswani2017transformer}.


\textit{Meanwhile}, OCL \textcolor{purple2}{quantizer} $\bm{\phi}_{\mathrm{q}}$ transforms $\bm{X}$ into the reconstruction \textcolor{blue2}{target} $\bm{Q} \in \mathbb{R} ^ {h \times w \times c}$ for the following decoding:
\begin{equation}
\label{eq:ocl_quantiz}
\bm{\phi}_{\mathrm{q}} : \bm{X} \rightarrow \bm{Q}
\end{equation}
where $\bm{\phi}_{\mathrm{q}}$ can be parameterized as some Vector Quantization (VQ) module \cite{im2017dvae, van2017vqvae}. But we meticulously design our own VQ variant, as detailed in Section~\ref{sect:maximize_vfms_in_ocl}.
$\bm{\phi}_{\mathrm{q}}$ is pretrained in VAE framework and is frozen afterwards, where the encoder is shared from the frozen OCL encoder $\bm{\phi}_{\mathrm{e}}$, and the decoder is a typical VAE decoder. We will explain why not use a separate typical VAE encoder in Section~\ref{sect:maximize_vfms_in_ocl}.


\textit{Thirdly}, OCL \textcolor{purple2}{decoder} $\bm{\phi}_{\mathrm{d}}$ transforms $\bm{S}$ into \textcolor{blue2}{reconstruction} $\bm{Q}' \in \mathbb{R} ^ {h \times w \times c}$ with destructed $\bm{Q}$ as the condition:
\begin{equation}
\label{eq:ocl_decode}
\bm{\phi}_{\mathrm{d}} : (\bm{S}, \mathrm{destruct}(\bm{Q}) ) \rightarrow \bm{Q}'
\end{equation}
Here $\bm{\phi}_{\mathrm{d}}$ can be parameterized as 
(\textit{i}) a CNN or MLP for mixture decoding \cite{kipf2021savi, seitzer2023dinosaur}, where $\bm{Q}$ is destructed to height and width, and $\bm{S}$ is spatially broadcast \cite{watters2019spatialbroadcast} into this shape and then decoded into components being mixed together; 
(\textit{ii}) or a Transformer decoder for auto-regressive decoding \cite{singh2021slate, kakogeorgiou2024spot}, where $\bm{Q}$ is destructed with causal masking as the query and $\bm{S}$ is the key and value; 
(\textit{iii}) or a conditional Diffusion model for diffusion decoding \cite{wu2023slotdiffuz, jiang2023lsd}, where $\bm{Q}$ is destructed with noise as the input and $\bm{S}$ is the condition.

Reconstructing $\bm{Q}$ using $\bm{S}$ drives $\bm{S}$ to aggregate as much object information as possible. Thus, a good reconstruction target, like in \cite{zhao2024gdr, zhao2024msf}, is very important. We will explain this in Section~\ref{sect:maximize_vfms_in_ocl}.


\textit{Lastly}, the supervision signal for OCL training comes from minimizing the reconstruction \textcolor{red2}{loss} between $\bm{Q}'$ and $\bm{Q}$:
\begin{equation}
\label{eq:ocl_loss}
\mathrm{min} _ {(\bm{\phi}_{\mathrm{a}}, \bm{\phi}_{\mathrm{d}})}  f_{\mathrm{recon}} (\bm{Q}', \bm{Q})
\end{equation}
where $f_{\mathrm{recon}}(\cdot,\cdot)$ can be (\textit{i}) Mean Squared Error (MSE) loss for mixture and diffusion OCL decoding, or (\textit{ii}) Cross Entropy (CE) loss for auto-regressive decoding. 

% Our unified architecture provides a foundation, enabling fair comparisons and improvements of various existing and future OCL methods.
% Please refer to Appendix~\ref{apdx:shared_quantization_enables_vvo_to_support_various_ocl_decoders} for more about modeling architecture unification and Appendix~\ref{apdx:we_reproduce_all_with_best_practices_to_make_strong_baselines} for more about training best practices unification.



\subsection{Utilize VFMs in OCL}
\label{sect:maximize_vfms_in_ocl}

% We fully utilize VFMs in OCL by (\textit{i}) direct VFM feature extraction to facilitate object-level information aggregation, and (\textit{ii}) shared VFM feature quantization as reconstruction targets to strengthen supervision in OCL training.
In our unified architecture, we utilize VFMs as following.



\textbf{\textit{Direct VFM Feature Extraction for Better Aggregation}}

We directly extract the feature map $\bm{Z}$ from the input $\bm{X}$ using VFMs as $\bm{\phi}_{\mathrm{e}}$, where DINO2 ViT and SAM2 encoder are chosen and experimented thoroughly. No extra position encoding is needed here like in SA \cite{locatello2020slotattent} because these VFMs already contain the positional information required in $\bm{\phi}_{\mathrm{a}}$. Since $\bm{\phi}_{\mathrm{e}}$ is frozen, We further use a trainable linear layer to adjust $\bm{Z}$ slightly. 

As shown in Figure~\ref{fig:teaser}, VFM representations have better objectness than non-VFMs, even under naive kMeans clustering\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html}. 
OCL aggregation is essentially a clustering whose initial centroids are trainable \cite{jia2023boqsa}. 
Thus, we expect $\bm{\phi}_{\mathrm{a}}$ to aggregate VFM's $\bm{Z}$ into slots $\bm{S}$ better under queries $\bm{S}_0$.
Previous methods like DINOSAUR \cite{seitzer2023dinosaur} have already exploited this but without any reason. In Section~\ref{sect:analysis1}, we analyze this both theoretically and empirically.



% \begin{figure}[h]
% \centering
% \includegraphics[width=0.999\linewidth]{res/proof1.pdf}
% \\\vspace{-0.001\baselineskip}
% \caption{\textmd{
% Better objectness helps OCL aggregation. Green and orange areas stand for objects $\bm{o}_1$ and $\bm{o}_2$, where $\bm{v}_1$ and $\bm{v}_2$ are arbitrary super-pixels and $\bm{s}_*$ is $\bm{o}_2$'s centroid. But the actual query $\bm{s} \sim \mathcal{N} ( \bm{s}_*, \bm{\sigma}^2 )$. In VFM super-pixel space, the distance $d_{12}$ between $\bm{v}_1$ and $\bm{v}_2$ is larger, i.e., better objectness, thus $\bm{s}$ has higher probability $p(d_{s1}>d_{s2})$ to represent $\bm{o}_2$ correctly, compared with that in non-VFM super-pixel space.
% }}
% \label{fig:proof1}
% \end{figure}

% \textit{Mathematic Modeling}

% As shown in Figure~\ref{fig:proof1}, super-pixels in a feature map $\bm{Z}$ all belong to two objects $\bm{o}_1$ and $\bm{o}_2$, so two queries are needed for aggregation, which is basically sum of super-pixels weighted by normalized minus distances between queries and super-pixels \cite{locatello2020slotattent}. 

% Denote the ideal query of $\bm{o}_2$ as $\bm{s}_*$, which is the clustering center or centroid of $\bm{o}_2$ and is closer to all super-pixels in $\bm{o}_2$ than in $\bm{o}_1$:
% \begin{equation}
% \label{eq:d_star_12}
% d_{*1} = d(\bm{s}_*, \bm{v}_1) > d(\bm{s}_*, \bm{v}_2) = d_{*2}
% \end{equation}
% where $d(\cdot, \cdot)$ is a distance metric, e.g., minus inner product; $\bm{v}_1$ and $\bm{v}_2$ are arbitrary points in $\bm{o}_1$ and $\bm{o}_2$, respectively.

% But the actual query $\bm{s}$ follows $\mathcal{N}( \bm{s}_*, \bm{\sigma}^2\bm{I} )$. Substituting $\bm{s}$ for $\bm{s}_*$, the probability of correct aggregation is:
% \begin{equation}
% \label{eq:p2}
% p_2 = p(d_{s1} > d_{s2}) = \int_{\bm{v} \in \bm{o}_2} \frac{1}{\sqrt{2\pi} \bm{\sigma}} e ^ { -\frac{1}{2} (\frac{\bm{v} - \bm{s}_*} {\bm{\sigma}} ) ^ 2 } d\bm{v}
% \end{equation}
% where $\bm{o}_2$ always contains $\bm{s}_*$, and is bounded by the separation plane between $\bm{o}_1$ and $\bm{o}_2$. The closer the boundary is to $\bm{s}_*$ the smaller $p_2$ is.

% According to Figure~\ref{fig:teaser} the first observation, in VFM super-pixel space, points of the same object have smaller distances while points from different objects have larger distances, compared to that in non-VFM space. This means the separation plane is closer to $\bm{s}_*$ in non-VFM space. Therefore, $p_2$ in VFM space is bigger than in non-VFM space. 
% \hfill \(\square\)



% \textit{Statistical Verification}

% Please refer to Appendix~\ref{apdx:prob_acc}.



\textbf{\textit{Shared VFM Feature Quantization for Better Supervision}}

Given the direct VFM feature map $\bm{Z}$, we transform it with a naive CNN to eliminate the positional information for better quantization. 
We quantize the adjusted $\bm{Z}$ using our VQ variant as $\bm{\phi}_{\mathrm{q}}$ to obtain the reconstruction target $\bm{Q}$. 


Our VQ's codebook follows SimVQ \cite{zhu2024simvq}. We predefine $m=4096$ template features, i.e., a codebook $\bm{T}_0 \in \mathbb{R} ^ {m \times c_0}$, which are randomly initialized and remain frozen.
In vector quantization, we project $\bm{T}$ with a pre-trainable linear layer and match it with the adjusted $\bm{Z}$:
\begin{equation}
\label{eq:codebook_project}
\bm{T} := \bm{W} \cdot \mathrm{sg}(\bm{T}_0)
\end{equation}
\begin{equation}
\label{eq:codebook_match}
\bm{D} = || \bm{Z} - \bm{T} || _ 2 ^ 2
\end{equation}
where $\mathrm{sg}(\cdot)$ is stop-gradient; $\bm{T} \in \mathbb{R} ^ {m \times c}$ is the codebook to be used for quantizing $\bm{Z}$; $\bm{D} \in \mathbb{R} ^ {h \times w \times m}$ is the matching distance between every super-pixel and every code.

We convert distances to probabilities and select the most matched codes to form the quantization $\bm{Q}$:
\begin{equation}
\label{eq:softmax}
\bm{P} = \mathrm{softmax}_c ( -\bm{D} )
\end{equation}
\begin{equation}
\label{eq:argmax}
\bm{I} = \mathrm{argmax}_m (\bm{P})
\end{equation}
\begin{equation}
\label{eq:select}
\bm{Q} = \mathrm{index}_m (\bm{T}, \bm{I})
\end{equation}
where $\mathrm{softmax}(\cdot)$ is calculated along the channel dimension; $\bm{P}$ is the match probabilities; $\bm{I} \in \mathbb{R} ^ {h \times w}$ is the matched code indexes; $\mathrm{argmax}(\cdot)$ is calculated along the channel dimension; $\mathrm{index}(\cdot, \cdot)$ is operated along code number dimension.
The typical STE \cite{bengio2013ste} on $\bm{Q}$, needed in pre-training, can be skipped during OCL training.


For its pre-training, we introduce some tricks. 
We add noise to $\bm{D}$ before Equation~\ref{eq:softmax} to encourage code utilization:
\begin{equation}
\label{eq:gumbel}
\bm{D} := \frac{\bm{D}+\bm{G}}{\tau}
\end{equation}
where $\bm{G} \in \mathbb{R}^{h \times w \times m}$ is the Gumbel noise and $\tau$ is the temperature.
We devise annealing residual connection \cite{zhao2024gdr, zhao2024msf} after Equation~\ref{eq:select} to stabilize pre-training:
\begin{equation}
\label{eq:residual}
\bm{Q} := \alpha \bm{Z} + (1-\alpha) \bm{Q}
\end{equation}
where $\alpha$ is scheduled from 1 to 0 during pre-training using cosine-annealing.
Besides typical losses of reconstruction, alignment and commitment \cite{van2017vqvae}, we regularize the adjusted $\bm{Z}$'s elements to be normalized:
\begin{equation}
\label{eq:regulariz}
l_{n} = \lambda \mathrm{MSE} ( \bm{Z}, \mathrm{sg}( \frac { \bm{Z} - \mathbb{E}[\bm{Z}] } { \sqrt{ \mathbb{V}[\bm{Z}] + \epsilon } } ) )
\end{equation}
where $\lambda$ is empirically set to 0.1; $\mathbb{E}$ and $\mathbb{V}$ are calculated along height, width and channel dimensions.


With all samples' feature maps $\bm{Z}$ being represented with one codebook $\bm{T}$, the quantization $\bm{Q}$ naturally gains cross-sample consistency and suppresses information redundancy or noise. This helps the aggregation with queries $\bm{S}_0$, which are also shared across samples. After such tokenization, our reconstruction target can be either regressed using mixture and diffusion decoding or classified using auto-regressive decoding. 
In contrast, methods like SLATE \cite{singh2021slate} and SlotDiffusion \cite{wu2023slotdiffuz} use separate VAE and OCL encoders, which as shown in Figure~\ref{fig:teaser} causes a distribution gap between $\bm{Q}$ and $\bm{Z}$.
Thus, we expect shared VFM representation quantization as reconstruction targets to strengthen OCL supervision. 
In Section~\ref{sect:analysis2}, we analyze this both theoretically and empirically.



% \textit{Mathematic Modeling}

% We reconstruct $\bm{Q}'$ to approximate the target $\bm{Q}$ ultimately from $\bm{Z}$ via $\bm{\phi}_{\mathrm{a}} \circ \bm{\phi}_{\mathrm{d}}$, denoted as $f$ for simplicity. Under MSE loss, the gradient with respect to $\bm{Z}$ is:
% \begin{equation}
% \label{eq:vae_grad_wrt_z}
% \frac {\partial \mathrm{MSE} (\bm{Q}', \mathrm{sg}(\bm{Q}))} {\partial \bm{Z}} = 2 (\bm{Q}' - \mathrm{sg}(\bm{Q})) \frac {\partial \bm{Q}'} {\partial \bm{Z}}
% \end{equation}

% We obtain $\bm{Q}$ by quantizing $\bm{Z}$, i.e., $\mathbb{E}[\bm{Z}] = \bm{Q}$, implying that any deviation of $\bm{Q}'=f(\bm{Z})$ from $\bm{Q}$ is due to $f$ and $\bm{Z}$. Assuming $f$ preserves $\bm{Z}$'s statistical properties, we have:
% \begin{equation}
% \label{eq:expectat_q_prime}
% \mathbb{E}[\bm{Q}'] = \mathbb{E}[f(\bm{Z})] \approx \bm{Q}
% \end{equation}

% Thus the residual error $\bm{Q}' - \mathrm{sg}({\bm{Q}})$ in Equation~\ref{eq:vae_grad_wrt_z} is statistically unbiased and small on average:
% \begin{equation}
% \label{eq:expectat_q_prime_minus_q}
% \mathbb{E}[\bm{Q}' - \mathrm{sg}(\bm{Q})] \approx 0
% \end{equation}

% But if instead of sharing $\bm{\phi}_{\mathrm{e}}$, we use an extra VAE encoder plus $\bm{\phi}_{\mathrm{q}}$ to obtain the target, denoted as $\bm{Q}_2$, then $\bm{Q}_2 \neq \bm{Q}$ according to Figure~\ref{fig:teaser} the second observation. Substitute $\bm{Q}_2$ into Equation~\ref{eq:vae_grad_wrt_z} and the residual error $\bm{Q}' - \mathrm{sg}(\bm{Q}_2)$ would be systematically biased:
% \begin{equation}
% \label{eq:expectat_q_primte_minus_q2}
% \mathbb{E}[\bm{Q}' - \mathrm{sg}(\bm{Q}_2)] \neq 0
% \end{equation}
% which increases noise in optimization.

% Under CE loss, the gradient with respective to $\bm{Z}$ is:
% \begin{equation}
% \label{eq:ce_grad_wrt_z}
% \frac {\partial \mathrm{CE} (\bm{Q}', \mathrm{sg}(\bm{Q}))} {\partial \bm{Z}} = \frac {\partial f(\bm{Z}) ^ T} {\partial \bm{Z}} (\bm{Q}' - \mathrm{sg}(\bm{Q}))
% \end{equation}
% where $\frac {\partial f(\bm{Z})} {\partial \bm{Z}}$ is the Jacobian matrix of $f(\bm{Z})$ with respect to $\bm{Z}$. Anyway, this has similar structure to Equation~\ref{eq:vae_grad_wrt_z} and thus does not alter our judgment above.
% \hfill \(\square\)


% \textit{Statistical Verification}

% Please refer to Appendix~\ref{apdx:grad_dist}.



\subsection{Compare Architectures}
\label{sect:compare_models}

% As shown in \ref{fig:compare_model}, our shared quantization is the key point the various OCL decoders are supported. Without such shared quantization, diffusion-based OCL decoding is not possible.

\begin{figure}[]
\centering
\includegraphics[width=\linewidth]{res/compare-model.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
Model architecture comparison.
}}
\label{fig:compare_model}
\end{figure}
% \vspace{-0.5\baselineskip}

As shown in Figure~\ref{fig:compare_model}, we compare the baselines with our VVO in a unified perspective.
% , demonstrating the disadvantages of existing methods.
Specifically,

% \begin{itemize}
% \item
- \textit{Our VVO}: (1) VFMs are employed for OCL encoding and their features are fed to the aggregator directly, which eases OCL aggregation, as formulated in Section~\ref{sect:maximize_vfms_in_ocl}. (2) VFM features are shared quantized as reconstruction targets, which strengthens OCL supervision, as formulated in Section~\ref{sect:maximize_vfms_in_ocl}.

% \item
- \textit{SLATE0} (the official version) \cite{singh2021slate}: (1) VFM features are \textbf{NOT directly fed to} the aggregator. (2) VFM features are shared \textbf{discretized} to scalar numbers and re-embedded into features to be learned latter. This loses much information of VFM features, while shared quantization can preserve such information \cite{van2017vqvae}.

% \item
- \textit{SLATE} (the unofficially improved version; we adopt it) \cite{jia2023boqsa,singh2022steve,wu2023slotdiffuz} / \textit{STEVE} \cite{singh2022steve}: (1) Same as our VVO. (2) Reconstruction targets are \textbf{discretized} from features of a \textbf{separate} VAE encoder, instead of quantized from features shared from the same VFM for OCL encoding, which causes optimization noises.

% \item
- \textit{SlotDiffusion} \cite{wu2023slotdiffuz} / \textit{LSD} \cite{jiang2023lsd}: (1) Same as our VVO. (2) Reconstruction targets are quantized from features of a \textbf{separate} VAE encoder, instead of sharing the same VFM for OCL encoding, which causes optimization noises.

% \item
- \textit{DINOSAUR} \cite{seitzer2023dinosaur} / \textit{VideoSAUR} \cite{zadaianchuk2024videosaur} / \textit{SPOT} \cite{kakogeorgiou2024spot}: (1) Same as our VVO. (2) Reconstruction targets are shared from the same VFM for OCL encoding but \textbf{without quantization}, which causes optimization noises.
% \end{itemize}

Our VVO adopts a clean and unified architecture with the above-mentioned two key designs. The first design, direct VFM feature extraction for OCL aggregation, is not proposed by us but we are the first to provide both theoretical and empirical analyses. The second design, shared VFM feature quantization as reconstruction targets, is proposed by us. Such shared quantization is also the key point that our VVO supports various OCL decoders. Specifically, 

% \begin{itemize}
% \item
- \textit{Mixture-based OCL decoders}, e.g., CNN \cite{locatello2020slotattent,kipf2021savi,elsayed2022savipp}, MLP \cite{seitzer2023dinosaur} and SlotMixer \cite{zadaianchuk2024videosaur}, are originally designed for continuous features. As quantized features are actually the clustering centroids of continuous features, these decoders are inherently compatible with our shared quantized VFM features. 

% \item
- \textit{Auto-regressive OCL decoders}, e.g., the Transformer decoder \cite{singh2021slate,singh2022steve} and Transformer9 \cite{kakogeorgiou2024spot}, are originally designed for discretized features, while \cite{seitzer2023dinosaur} and \cite{kakogeorgiou2024spot} also explore their applicability to continuous features. Thus these decoders are naturally applicable to our shared quantized VFM features.

% \item
- \textit{Diffusion-based OCl decoders}, e.g., Conditional Diffusion \cite{wu2023slotdiffuz,jiang2023lsd}, are originally designed for dicretized features. Thus if without our shared quantized VFM features as reconstruction targets, then this type of OCL decoders would not be unified into our architecture.
% \end{itemize}



\begin{figure*}[]
\centering
\includegraphics[width=\linewidth]{res/qualitative.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
Qualitative object discovery performance comparison. 
}}
\label{fig:qualitative}
\end{figure*}



\begin{table*}[]
\centering
\small
\input{res/quantitative_vqdino}
\caption{\textmd{
Object discovery performance with \textbf{DINO2 ViT (s/14)} for OCL encoding. VVO is instantiated as VQDINO; \texttt{Tfd}, \texttt{TfdT}, \texttt{MLP} and \texttt{Dfz} are Transformer, Transformer-temporal, MLP and Diffusion for OCL decoding respectively. 
}}
\label{tab:quantitative_vqdino}
\end{table*}
% \vspace{-0.5\baselineskip}  % not work



\begin{table*}[]
\centering
\small
\input{res/quantitative_vqsam}
\caption{\textmd{
Object discovery performance with \textbf{SAM2 Hiera+FPN (t/16)} for OCL decoding. VVO is instantiated as VQSAM; \texttt{Tfd}, \texttt{TfdT}, \texttt{MLP} and \texttt{Dfz} are Transformer, Transformer-temporal, MLP and Diffusion for OCL decoding respectively. 
}}
\label{tab:quantitative_vqsam}
\end{table*}
% \vspace{-0.5\baselineskip}  % not work



\section{Experiment}
\label{sect:experiment}

We conduct all experiments using three random seeds.
% to answer the following questions:
% (\textit{i}) Can object discovery performance of mainstream OCL be improved using one unified architecture? 
% (\textit{ii}) Will the improved object discovery capability benefit advanced vision tasks like prediction and reasoning?
% (\textit{iii}) How do designs of our architecture contribute to object discovery performance? 



\subsection{Set up the Benchmark}

\textit{Datasets}. 
% We evaluate our method and those baselines on both synthetic and real-world datasets.
We include both synthetic and real-world datasets.
ClevrTex\footnote{https://www.robots.ox.ac.uk/{\mytilde}vgg/data/clevrtex} comprises synthetic images, each with about 10 geometric objects scattered in complex background.
MOVi-D\footnote{https://github.com/google-research/kubric/blob/main/challenges/movi/README.md\#\\movi-d} contains synthetic videos, each with up to 20 daily objects dropping and bumping.
COCO\footnote{https://cocodataset.org} is a recognized real-world image dataset, and we use its challenging panoptic segmentation.
VOC\footnote{http://host.robots.ox.ac.uk/pascal/VOC} is a real-world image dataset, and we use its instance segmentation.
% \footnote{https://youtube-vos.org/dataset/vis}
We also report results on real-world video dataset YTVIS version HQ\footnote{https://github.com/SysCV/vmt?tab=readme-ov-file\#hq-ytvis-high-quality-video-instance-segmentation-dataset}, which contains large-scale short videos from YouTube.
We choose Physion\footnote{https://physion-benchmark.github.io} for visual prediction and reasoning as it contains common object interactions, requiring algorithms to learn dynamics like support, roll and link, then to predict and reason about future scene states.

\textit{Models}. 
We compare VVO with both OCL classics and state-of-the-arts.
SLATE \cite{singh2021slate} uses a Transformer decoder for auto-regressive decoding, and it \textcolor{gray}{differs} from VVO in a separate VAE encoder and naive quantizer; STEVE \cite{singh2022steve} is SLATE's video version.
DINOSAUR \cite{seitzer2023dinosaur} uses an MLP for mixture decoding, and it \textcolor{gray}{differs} from VVO in no quantization in its reconstruction target.
SlotDiffusion \cite{wu2023slotdiffuz} uses a conditional Diffusion model for diffusion decoding, and it \textcolor{gray}{differs} from VVO in a separate VAE encoder and naive quantizer.
General improvers GDR \cite{zhao2024gdr} and MSF \cite{zhao2024msf} only support auto-regression and diffusion decoding.
We skip outdated methods like IODINE \cite{greff2019iodine}, SA \cite{locatello2020slotattent} and ISA \cite{biza2023isa} due to their low accuracy. We also skip SAVi \cite{kipf2021savi} and SAVi++ \cite{elsayed2022savipp} as their extra modalities are unfair to others.

\textit{Comparison}. Instead of copying existing results, we reproduce all baselines to realize fair comparison. 
% As detailed in Appendices \ref{apdx:data_processing} and \ref{apdx:training_recipe}, w
We use identical data augmentation, VFMs in OCL encoding and training recipes for all experiment items unless not applied.
We instantiate all baselines' VAE part as TAESD\footnote{https://huggingface.co/docs/diffusers/en/api/models/autoencoder\_tiny}, which is a large-scale pretrained StableDiffusion\footnote{https://huggingface.co/spaces/stabilityai/stable-diffusion} module, to build all \textit{strong} baselines.



\begin{table}[]
\centering
\small
\input{res/vqdino_using}
\caption{\textmd{
VVO using higher resolution (\textit{upper}) and different aggregators (\textit{lower}) on object discovery.
By default, we use BO-QSA for all our experiment items, including the baselines.
}}
\label{tab:vqdino_using}
\end{table}
% \vspace{-1\baselineskip}

\begin{table}[]
\centering
\small
\input{res/vqdino_compared}
\caption{\textmd{
VVO compared with general improvers (\textit{upper}) and SotA methods (\textit{lower}) on object discovery.
SPOT uses both Transformer decoder with 9 permutations \texttt{Tfd9} and ``self-training''; VideoSAUR uses SlotMixer \texttt{Smd} as its decoder.
}}
\label{tab:vqdino_compared}
\end{table}
% \vspace{-1\baselineskip}



\subsection{Evaluate on Object Discovery}
\label{sect:object_discovery}

Object discovery task intuitively shows how well those slots separate different objects.
We evaluate the byproduct object segmentation accuracy with Adjusted Rand Index\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted\_rand\_sco\\re.html} (ARI), ARI\textsubscript{fg} (foreground), mean Intersection-over-Union\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard\_score.html} (mIoU) and mean Best Overlap \cite{uijlings2013selectivesearch} (mBO).

\textit{With unsupervised pretrained VFMs for OCL encoding}, i.e., DINO2 ViT (version s/14), our VVO is instantiated as VQDINO. As shown in Table~\ref{tab:quantitative_vqdino}, our method consistently improves object discovery performance across all types of OCL decoding.
With a Transformer decoder \texttt{Tfd} for auto-regressive decoding, VVO significantly outperforms SLATE and STEVE across all datasets. 
With MLP \texttt{Mlp} for mixture decoding, VVO shows a smaller advantage over DINOSAUR but is still effective.
With a conditional Diffusion model \texttt{Dfz} for diffusion decoding, VVO surpasses SlotDiffusion on most datasets.

\textit{With supervised pretrained VFMs for OCL encoding}, i.e., SAM2 Hiera+FPN (version t/16), our VVO is instantiated as VQSAM. As shown in Table~\ref{tab:quantitative_vqsam}, VVO consistently boosts those baseline methods' object discovery performance across all decoding types on most datasets.

As shown in Table~\ref{tab:vqdino_using}, whether using a \textit{higher input resolution} or \textit{different aggregators}, VVO maintains its superiority over baselines.
As shown in Table~\ref{tab:vqdino_compared}, VVO outperforms recent \textit{general improvers} for OCL training, i.e., GDR and MSF, which do not support mixture decoding like us though. VVO also surpasses \textit{state-of-the-arts}, i.e., SPOT \cite{kakogeorgiou2024spot} and VideoSAUR \cite{zadaianchuk2024videosaur}, with their special types of decoding.

% We also \textit{compare VVO with state-of-the-art methods, along with their special OCL decoders}. 
% For the image OCL SotA SPOT \cite{kakogeorgiou2024spot}, our VVO is instantiated as VQDINO plus SPOT's Transformer decoder with 9 token permutations. % \texttt{Tfd9}
% For the video OCL SotA VideoSAUR \cite{zadaianchuk2024videosaur}, our VVO is instantiated as VQDINO plus VideoSAUR's SlotMixer decoder.  % \texttt{Smd}
% As shown in \cref{fig:quantitative_sota}, our VVO defeats SPOT on real-world image dataset COCO, and defeats VideoSAUR on real-world video dataset YTVIS.



\subsection{Evaluate on Set Prediction}
\label{sect:set_prediction}

\begin{table}[]
\centering
\small
\input{res/set_prediction}
\caption{\textmd{
Set prediction performance on COCO (panoptic, \#slot=7).
}}
\label{tab:set_prediction}
\end{table}

Set prediction task directly shows how much object information those slots grasp.
We use OCL to represent dataset COCO as slots, and use a small MLP to predict the object class label and bounding box corresponding to each slot by following this work \cite{seitzer2023dinosaur}. We measure top1 accuracy of the classified class labels while we measure the R2 score\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2\_score.html} of the regressed bounding box coordinates.

As shown in Table~\ref{tab:set_prediction}, compared with baseline DINOSAUR, our VVO, i.e., VQDINO-\texttt{MLP}, obtains better object classification and bounding box regression. 
Thus, our models' object representations are of higher quality.



\subsection{Deploy to the Downstream}
\label{sect:downstream}

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{res/acc_visualpredictreason.pdf}
% \\\vspace{-0.5\baselineskip}  % XXX
\caption{\textmd{
Visual prediction (\textit{upper}) and reasoning (\textit{lower}) performance on Physion (\#slot=8).
VVO has smaller prediction error in all time steps, and higher reasoning accuracy in later time steps.
}}
\label{fig:visual_prediction_reasoning}
\end{figure}

Better object representation benefits downstream tasks.
We follow the convention to pretrain OCL models on Physion and represent this dataset as slots. Then the object-centric dynamics model SlotFormer \cite{wu2022slotformer} is trained on those slots in an auto-regressive manner along the time dimension. We use temporal versions of DINOSAUR and our VVO, i.e., VQDINO-\texttt{MLP} to extract slots.

On \textit{visual prediction}, we evaluate the prediction errors per time step measured in normalized MSE between regressed and extracted slots. As shown in Figure~\ref{fig:visual_prediction_reasoning} upper, VVO accumulates prediction errors slower than the baseline.

On \textit{visual reasoning}, we evaluate the reasoning accuracies per time step between classified and ground-truth labels. As shown in Figure~\ref{fig:visual_prediction_reasoning} lower, VVO accuracies are slightly lower at the beginning but much higher later than the baseline.



\subsection{Ablate the Architecture}

As shown in Table~\ref{tab:ablat_shared_enc}, VVO's design of \textit{shared VAE and OCL encoder} consistently outperforms separate VAE and OCL encoders, even when the latter employs another VFM for VAE encoding. Thus, VVO's design of shared VFM representation quantization is crucial. However, the design of separate encoders remains prevalent among those baselines.

As shown in Table~\ref{tab:ablat_our_vq}, VVO's \textit{improved quantizer variant}, based on tricks of Gumbel noises defined in Equation~\ref{eq:gumbel}, annealing residual connection defined in Equation~\ref{eq:residual} and normalizing regularization defined in Equation~\ref{eq:regulariz}, is superior to the naive VQ.
% Compared with VVO's two key designs, these VQ tricks are more like the cherry on top.
The detailed effects of those tricks are shown in Figure~\ref{fig:ablat_code_utiliz_residual}. \textit{Adding Gumbel noises} increases codebook utilization, contributing to more effective codes; \textit{Annealing residual connection} improves VAE pretraining, contributing to smaller VAE reconstruction error. 
% As shown in Tab. [9], \textit{normalizing regularization} is beneficial to diffusion-based decoding in particular. -- slotdiffusion/vqdino-dfz training curve
% code book utilization: CV

\begin{table}[t]
\centering
\small
\input{res/ablat_shared_enc}
\caption{\textmd{
VVO's two key designs: (\textit{i}) Using VFM representation for encoding is better than using non-VFMs; (\textit{ii}) Sharing the OCL encoder as VAE encoder to obtain targets is better than using separate VAE and OCL encoders.
% Sharing the VAE and OCL encoder and quantizing the representation as targets is better than using separate VAE and OCL encoders.
Results are on COCO panoptic.
}}
\label{tab:ablat_shared_enc}
\end{table}

\begin{table}[t]
\centering
\small
\input{res/ablat_our_vq}
\caption{\textmd{
VVO's VQ variant: All our three tricks are beneficial to the overall performance boosts. In comparison to VVO's key designs, these tricks are more like the cherry on top.
Results are on COCO panoptic with settings consistent with the above.
}}
\label{tab:ablat_our_vq}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{res/ablat_code_utiliz.pdf}
\\\vspace{0.25\baselineskip}
\includegraphics[width=0.75\linewidth]{res/ablat_residual.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
Effects of tricks in our VQ variant: (\textit{upper}) Gumbel noise improves codebook utilization, where ``CV'' means Coefficient of Variation, and curves are smoothed by Gaussian kernel of size 10; (\textit{lower}) Annealing residual connection improves VAE pretraining, and the blue curve's turning point at epoch 4 is where the residual connection anneals to zero. Results are from the VAE pretraining of VQDINO-\texttt{Dfz} on COCO panoptic.
}}
\label{fig:ablat_code_utiliz_residual}
\end{figure}



\section{Analysis}
\label{sect:analysis}

We provide both mathematical modeling and statistical verification on why and how our VVO is better.



\subsection{Aggregation as Clustering}
\label{sect:analysis1}

\begin{figure}[]
\centering
\includegraphics[width=0.75\linewidth]{res/proof1.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
Better objectness helps OCL aggregation. Green and orange areas stand for objects $\bm{o}_1$ and $\bm{o}_2$, where $\bm{v}_1$ and $\bm{v}_2$ are arbitrary super-pixels and $\bm{s}_*$ is $\bm{o}_2$'s centroid. But the actual query $\bm{s} \sim \mathcal{N} ( \bm{s}_*, \bm{\sigma}^2 )$. In VFM super-pixel space, the distance $d_{12}$ between $\bm{v}_1$ and $\bm{v}_2$ is larger, i.e., better objectness, thus $\bm{s}$ has higher probability $p(d_{s1}>d_{s2})$ to represent $\bm{o}_2$ correctly, compared with that in non-VFM super-pixel space.
}}
\label{fig:proof1}
\end{figure}

\textbf{\textit{Mathematic Modeling}}

As shown in Figure~\ref{fig:proof1}, super-pixels in a feature map $\bm{Z}$ all belong to two objects $\bm{o}_1$ and $\bm{o}_2$, so two queries are needed for aggregation, which is basically sum of super-pixels weighted by normalized minus distances between queries and super-pixels \cite{locatello2020slotattent}. 

Denote the ideal query of $\bm{o}_2$ as $\bm{s}_*$, which is the clustering center or centroid of $\bm{o}_2$ and is closer to all super-pixels in $\bm{o}_2$ than in $\bm{o}_1$:
\begin{equation}
\label{eq:d_star_12}
d_{*1} = d(\bm{s}_*, \bm{v}_1) > d(\bm{s}_*, \bm{v}_2) = d_{*2}
\end{equation}
where $d(\cdot, \cdot)$ is a distance metric, e.g., minus inner product; $\bm{v}_1$ and $\bm{v}_2$ are arbitrary points in $\bm{o}_1$ and $\bm{o}_2$, respectively.

But the actual query $\bm{s}$ follows $\mathcal{N}( \bm{s}_*, \bm{\sigma}^2\bm{I} )$. Substituting $\bm{s}$ for $\bm{s}_*$, the probability of correct aggregation is:
\begin{equation}
\label{eq:p2}
p_2 = p(d_{s1} > d_{s2}) = \int_{\bm{v} \in \bm{o}_2} \frac{1}{\sqrt{2\pi} \bm{\sigma}} e ^ { -\frac{1}{2} (\frac{\bm{v} - \bm{s}_*} {\bm{\sigma}} ) ^ 2 } d\bm{v}
\end{equation}
where $\bm{o}_2$ always contains $\bm{s}_*$, and is bounded by the separation plane between $\bm{o}_1$ and $\bm{o}_2$. The closer the boundary is to $\bm{s}_*$ the smaller $p_2$ is.

According to Figure~\ref{fig:teaser} the first observation, in VFM super-pixel space, points of the same object have smaller distances while points from different objects have larger distances, compared to that in non-VFM space. This means the separation plane is closer to $\bm{s}_*$ in non-VFM space. Therefore, $p_2$ in VFM space is bigger than in non-VFM space. 
\hfill \(\square\)


\textbf{\textit{Statistical Verification}}

Equation~\ref{eq:p2} is very difficult to solve, but we can use Monte Carlo estimation:
\begin{equation}
\label{eq:p2_mc}
p_2 = \frac{1}{N} \Sigma_{i=1}^{N} \mathbbm{1} [\bm{v}_i \in \bm{o}_2]
\end{equation}
Specifically, we respectively use ResNet18, DINO2 ViT and SAM2 encoder to extract images into super-pixels, then apply KMeans clustering to find each clusters mean and std. From the Gaussian distributions defined by these mean and std, we repeatedly draw samples and calculate the ratio that such samples fall within their original clusters. This ratio is the correct aggregation probability. 

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{res/a31_prob_acc.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
Statistical correlation between correct aggregation probability and object discovery accuracy on VOC.
}}
\label{fig:prob_acc}
\end{figure}

A step further, we can calculate the statistical correlation between the correct aggregation probability and the corresponding OCL performance. As shown in Figure~\ref{fig:prob_acc}, the correlation is roughly significant.
Considering that correct aggregation probability can be calculated before the expensive OCL training, our Equation~\ref{eq:p2} is instructive to training better OCL models by selecting OCL encoders.



\subsection{Shared Quantization and Optimization Noise}
\label{sect:analysis2}



\textbf{\textit{Mathematic Modeling}}

We reconstruct $\bm{Q}'$ to approximate the target $\bm{Q}$ ultimately from $\bm{Z}$ via $\bm{\phi}_{\mathrm{a}} \circ \bm{\phi}_{\mathrm{d}}$, denoted as $f$ for simplicity. Under MSE loss, the gradient with respect to $\bm{Z}$ is:
\begin{equation}
\label{eq:vae_grad_wrt_z}
\frac {\partial \mathrm{MSE} (\bm{Q}', \mathrm{sg}(\bm{Q}))} {\partial \bm{Z}} = 2 (\bm{Q}' - \mathrm{sg}(\bm{Q})) \frac {\partial \bm{Q}'} {\partial \bm{Z}}
\end{equation}

We obtain $\bm{Q}$ by quantizing $\bm{Z}$, i.e., $\mathbb{E}[\bm{Z}] = \bm{Q}$, implying that any deviation of $\bm{Q}'=f(\bm{Z})$ from $\bm{Q}$ is due to $f$ and $\bm{Z}$. Assuming $f$ preserves $\bm{Z}$'s statistical properties, we have:
\begin{equation}
\label{eq:expectat_q_prime}
\mathbb{E}[\bm{Q}'] = \mathbb{E}[f(\bm{Z})] \approx \bm{Q}
\end{equation}

Thus the residual error $\bm{Q}' - \mathrm{sg}({\bm{Q}})$ in Equation~\ref{eq:vae_grad_wrt_z} is statistically unbiased and small on average:
\begin{equation}
\label{eq:expectat_q_prime_minus_q}
\mathbb{E}[\bm{Q}' - \mathrm{sg}(\bm{Q})] \approx 0
\end{equation}

But if instead of sharing $\bm{\phi}_{\mathrm{e}}$, we use an extra VAE encoder plus $\bm{\phi}_{\mathrm{q}}$ to obtain the target, denoted as $\bm{Q}_2$, then $\bm{Q}_2 \neq \bm{Q}$ according to Figure~\ref{fig:teaser} the second observation. Substitute $\bm{Q}_2$ into Equation~\ref{eq:vae_grad_wrt_z} and the residual error $\bm{Q}' - \mathrm{sg}(\bm{Q}_2)$ would be systematically biased:
\begin{equation}
\label{eq:expectat_q_primte_minus_q2}
\mathbb{E}[\bm{Q}' - \mathrm{sg}(\bm{Q}_2)] \neq 0
\end{equation}
which increases noise in optimization.

Under CE loss, the gradient with respective to $\bm{Z}$ is:
\begin{equation}
\label{eq:ce_grad_wrt_z}
\frac {\partial \mathrm{CE} (\bm{Q}', \mathrm{sg}(\bm{Q}))} {\partial \bm{Z}} = \frac {\partial f(\bm{Z}) ^ T} {\partial \bm{Z}} (\bm{Q}' - \mathrm{sg}(\bm{Q}))
\end{equation}
where $\frac {\partial f(\bm{Z})} {\partial \bm{Z}}$ is the Jacobian matrix of $f(\bm{Z})$ with respect to $\bm{Z}$. Anyway, this has similar structure to Equation~\ref{eq:vae_grad_wrt_z} and thus does not alter our judgment above.
\hfill \(\square\)



\textbf{\textit{Statistical Verification}}

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{res/a32_grad_dist.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
Gradient distributions tend to disperse or have outliers if not using our shared VFM feature quantization. Results here are collected at time step 10k, on dataset VOC.
}}
\label{fig:grad_dist}
\end{figure}

The statistical bias in Equation~\ref{eq:expectat_q_primte_minus_q2} manifests as the gradient distribution of different layers, especially that of slots. No statistical bias is usually associated with a smooth zero-mean small-std Gaussian gradient distribution.

Our technique, shared VFM feature quantization as reconstruction targets, includes two key points, sharing and quantization. Thus we compare three cases: (\textit{i}) Shared and quantized, where DINO features are both used for aggregation and quantized for reconstruction; (\textit{ii}) Shared but not quantized, where DINO features are used for aggregation and direct for reconstruction without quantization; (\textit{iii}) Not shared but quantized, where DINO features are used for aggregation while quantized SAM features are used for reconstruction.

As shown in Figure~\ref{fig:grad_dist}, using CE or MSE as the loss, our shared quantization achieves smoother Gaussian gradient distributions, which benefits OCL training. If not shared but quantized, then gradients tend to disperse; while if shared but not quantized, then gradients have some outliers.




\section{Conclusion}
\label{sect:conclusion}

We propose a unified architecture VVO for object-centric representation learning. Our VVO supports different well-recognized vision foundation models for OCL encoding and supports mainstream types of OCL decoding. It boosts the existing OCL performance in object discovery significantly, and benefits downstream tasks of visual prediction and reasoning.
VVO has the potential to serve as a general testbed for research related to OCL in the future.



% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}



%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}



\end{document}
