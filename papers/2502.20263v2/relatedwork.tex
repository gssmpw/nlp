\section{Related Work}
\label{sect:related_work}

\textit{SSseg vs OCL}.
Both Self-Supervised Segmentation (SSseg) \cite{ge2023hcl, wang2024videocutler} and OCL can discovery objects, i.e., segmenting objects and the background in images or video frames without supervision. 
SSseg only outputs object segmentation masks, while OCL further represents objects as \textit{slots}. 
These slots can be directly used in downstream object-centric world models \cite{wu2022slotformer} for dynamics modeling, which is demanded in advanced vision tasks.
Thus a direct comparison between them is beyond the scope of this work.

\textit{OCL encoding}. 
The stronger encoder that extracts feature maps of better objectness contributes more to OCL performance. 
Early milestone methods like IODINE\cite{greff2019iodine} and SA \cite{locatello2020slotattent} use small naive CNNs \cite{krizhevsky2012alexnet} as OCL encoder. 
Followups like SAVi \cite{kipf2021savi}, SAVi++ \cite{elsayed2022savipp}, SLATE \cite{singh2021slate} and STEVE \cite{singh2022steve} employ pretrained ResNets \cite{he2016resnet}, and fine-tune them on OCL datasets. 
State-of-the-arts like SlotDiffusion \cite{wu2023slotdiffuz} and DINOSAUR \cite{seitzer2023dinosaur} utilize VFMs like DINO \cite{caron2021dino1} and DINO2 \cite{oquab2023dino2} ViTs (Vision Transformers) to extract highly object-separable feature map from input pixels, improving OCL performance significantly.
Although SAM \cite{kirillov2023sam} and SAM2 \cite{ravi2024sam2} are also well recognized VFMs, they remain unexploited in OCL setting. 
Our VVO supports various VFMs for OCL encoding.

\textit{OCL aggregation}. SlotAttention \cite{locatello2020slotattent} is the footstone for mainstream OCL methods. Subsequent works like BO-QSA \cite{jia2023boqsa}, ISA \cite{biza2023isa} and SysBind \cite{singh2022sysbind} are all its variants, which are designed without changing the external interface. But considering their performance boosts, we only integrate BO-QSA by default.

\textit{OCL decoding}. 
With SlotAttention as the aggregator, the decoder and its reconstruction target affect OCL performance the most, as it is the source of supervision.
Mixture-based decoding, used in SAVi, SAVi++, DINOSAUR and VideoSAUR \cite{zadaianchuk2024videosaur}, decodes each slot's spatial broadcast \cite{watters2019spatialbroadcast} using naive CNNs or MLPs, and mixes them with corresponding weights into the reconstruction.
Transformer-based decoding, used in SLATE, STEVE and SPOT \cite{kakogeorgiou2024spot}, reconstructs VAE representation of the input auto-regressively with slots as the condition.
Diffusion-based decoding in LSD \cite{jiang2023lsd} and SlotDiffusion drives slots to recover noise added to the input's VAE representation.
Our VVO supports all these types of OCL decoding.

\textit{VAE for OCL}. % TODO XXX VAE GDR MSF
Variational Autoencoders (VAEs), like dVAE \cite{im2017dvae} in SLATE and VQ-VAE \cite{van2017vqvae} in SlotDiffusion, are employed to produce reconstruction targets for OCL training.
Since these VAEs are designed for image generation, some methods adapt them for OCL.
Inspired by channel or weight grouping, GDR \cite{zhao2024gdr} decomposes features into attributes and combine them to produce VAE representation as reconstruction targets to guide OCL better.
MSF \cite{zhao2024msf} firstly exploits the multi-scale idea in the OCL setting with VAE-specific designs.
Based on recent advancement RVQ \cite{yang2023rvq} and SimVQ \cite{zhu2024simvq}, we design our own VQ variant for OCL.



\begin{figure*}[]
\centering
\includegraphics[width=0.975\linewidth]{res/vvo_arch.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
VVO is a unified architecture that fully utilizes VFMs in OCL. It not only extracts VFM features with better objectness to facilitate object information aggregation; but further quantizes those VFM features as reconstruction targets to strengthen OCL training supervision. With typical SlotAttention or it variants as the \textcolor{purple2}{aggregator} and Vector-Quantization as the \textcolor{purple2}{quantizer}, VVO supports different VFMs as the \textcolor{purple2}{encoder}, and supports mainstream mixture, auto-regression and diffusion models as the \textcolor{purple2}{decoder}.
}}
\label{fig:vvo_arch}
\end{figure*}
% \vspace{-0.5\baselineskip}