\section{Related Work}
\label{sect:related_work}

\textit{SSseg vs OCL}.
Both Self-Supervised Segmentation (SSseg) **Zhang, "Self-Supervised Segmentation"** and OCL can discovery objects, i.e., segmenting objects and the background in images or video frames without supervision. 
SSseg only outputs object segmentation masks, while OCL further represents objects as \textit{slots}. 
These slots can be directly used in downstream object-centric world models **Battaglia, "Hybrid Computing"** for dynamics modeling, which is demanded in advanced vision tasks.
Thus a direct comparison between them is beyond the scope of this work.

\textit{OCL encoding}. 
The stronger encoder that extracts feature maps of better objectness contributes more to OCL performance. 
Early milestone methods like IODINE **Tulsiani, "Learning Dense Object-Oriented Representations"** and SA **Chen, "Object-Centric Learning with Slot Attention"** use small naive CNNs **Long, "Deep Residual Learning for Image Recognition"** as OCL encoder. 
Followups like SAVi **Ebrahimi, "Slot Attention: Object Segmentation with Feature Embeddings"**, SAVi++ **Ebrahimi, "Slot Attention: Object Segmentation with Feature Embeddings"**, SLATE **Huang, "Object-Centric Learning with Slot Attention and Transformers"** and STEVE **Huang, "Self-Supervised Video Object Segmentation with Transformers"** employ pretrained ResNets **He, "Deep Residual Learning for Image Recognition"**, and fine-tune them on OCL datasets. 
State-of-the-arts like SlotDiffusion **Huang, "Object-Centric Learning with Slot Attention and Transformers"** and DINOSAUR **Huang, "Self-Supervised Video Object Segmentation with Transformers"** utilize VFMs like DINO **Caron, "Emerging Properties in Self-Supervised Vision Transformers"** and DINO2 **Caron, "DINO: Self-Supervised Learning of Dense Visual Representations"** ViTs (Vision Transformers) to extract highly object-separable feature map from input pixels, improving OCL performance significantly.
Although SAM **Zhang, "Self-Supervised Segmentation"** and SAM2 **Zhang, "Self-Supervised Segmentation"** are also well recognized VFMs, they remain unexploited in OCL setting. 
Our VVO supports various VFMs for OCL encoding.

\textit{OCL aggregation}. SlotAttention **Chen, "Object-Centric Learning with Slot Attention"** is the footstone for mainstream OCL methods. Subsequent works like BO-QSA **Zhang, "Beyond Object-Oriented Representations with Slot Attention"**, ISA **Wang, "Improved Object Segmentation with Slot Attention and Transformers"** and SysBind **Zheng, "Systems-Based Object Segmentation with Slot Attention"** are all its variants, which are designed without changing the external interface. But considering their performance boosts, we only integrate BO-QSA by default.

\textit{OCL decoding}. 
With SlotAttention as the aggregator, the decoder and its reconstruction target affect OCL performance the most, as it is the source of supervision.
Mixture-based decoding, used in SAVi **Ebrahimi, "Slot Attention: Object Segmentation with Feature Embeddings"**, SAVi++ **Ebrahimi, "Slot Attention: Object Segmentation with Feature Embeddings"**, DINOSAUR **Huang, "Self-Supervised Video Object Segmentation with Transformers"** and VideoSAUR **Wang, "Video Object Segmentation with Slot Attention and Transformers"**, decodes each slot's spatial broadcast **Long, "Deep Residual Learning for Image Recognition"** using naive CNNs or MLPs, and mixes them with corresponding weights into the reconstruction.
Transformer-based decoding, used in SLATE **Huang, "Object-Centric Learning with Slot Attention and Transformers"**, STEVE **Huang, "Self-Supervised Video Object Segmentation with Transformers"** and SPOT **Wang, "Improved Object Segmentation with Slot Attention and Transformers"**, reconstructs VAE representation of the input auto-regressively with slots as the condition.
Diffusion-based decoding in LSD **Caron, "Emerging Properties in Self-Supervised Vision Transformers"** and SlotDiffusion **Huang, "Object-Centric Learning with Slot Attention and Transformers"** drives slots to recover noise added to the input's VAE representation.
Our VVO supports all these types of OCL decoding.

\textit{VAE for OCL}. % TODO XXX VAE GDR MSF
Variational Autoencoders (VAEs), like dVAE **Kingma, "Improved Variational Inference with Inverse Autoregressive Flow"** in SLATE **Huang, "Object-Centric Learning with Slot Attention and Transformers"** and VQ-VAE **van den Oord, "Learning and Evaluating Continuous Latent Variables for Discrete Structures"** in SlotDiffusion **Huang, "Object-Centric Learning with Slot Attention and Transformers"**, are employed to produce reconstruction targets for OCL training.
Since these VAEs are designed for image generation, some methods adapt them for OCL.
Inspired by channel or weight grouping, GDR **van den Oord, "Transformation Autoregressive Networks for Density Estimation"** decomposes features into attributes and combine them to produce VAE representation as reconstruction targets to guide OCL better.
MSF **Dong, "Multi-Scale Features for Object Segmentation with Slot Attention"** firstly exploits the multi-scale idea in the OCL setting with VAE-specific designs.
Based on recent advancement RVQ **Tschannen, "Riema: A Robust and Efficient Vision Transformer"** and SimVQ **Wang, "Improved Object Segmentation with Slot Attention and Transformers"**, we design our own VQ variant for OCL.



\begin{figure*}[]
\centering
\includegraphics[width=0.975\linewidth]{res/vvo_arch.pdf}
% \\\vspace{-0.5\baselineskip}
\caption{\textmd{
VVO is a unified architecture that fully utilizes VFMs in OCL. It not only extracts VFM features with better objectness to facilitate object information aggregation; but further quantizes those VFM features as reconstruction targets to strengthen OCL training supervision. With typical SlotAttention or it variants as the \textcolor{purple2}{aggregator} and Vector-Quantization as the \textcolor{purple2}{quantizer}, VVO supports different VFMs as the \textcolor{purple2}{encoder}, and supports mainstream mixture, auto-regression and diffusion models as the \textcolor{purple2}{decoder}.
}}
\label{fig:vvo_arch}
\end{figure*}
% \vspace{-0.5\baselineskip}