% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{amsmath}  % For advanced math symbols and environments
\usepackage{amssymb}  % For additional math symbols (optional, but useful)
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphicx} % for \rotatebox
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{xspace}
\usepackage{comment}
\newcommand{\ds}{\texttt{LLM-ProSense}\xspace}

\usepackage{algpseudocode}% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{multicol,multirow}

\newcommand{\ds}{$\texttt{LLM-ProSense}$}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{\texttt{LLM-ProSense}: A Benchmark for  Large Language Models Prompts' Sensitivity  }
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%

\begin{abstract}
Large language models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses. In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset \ds designed to investigate the effects of slight prompt variations on LLM performance. Using TriviaQA and HotpotQA as the foundation of our study, we generate prompt variations and evaluate their effectiveness across multiple LLMs. We benchmark the prompt sensitivity prediction task on our curated dataset, employing state-of-the-art methods from related tasks, including LLM-based self-evaluation, text classification, and query performance prediction techniques. Our findings reveal that existing methods struggle to effectively address prompt sensitivity, underscoring the need to understand how information needs should be phrased for accurate LLM responses.
\end{abstract}

%
%
\section{Introduction}
The advent of large language models (LLMs) has transformed how users interact with and leverage artificial intelligence for various information-seeking tasks. LLMs can generate human-like responses to a wide array of prompts, from answering specific questions to performing complex tasks. However, their effective relies heavily on crafting prompts that clearly define the task or question at hand. %A notable challenge in working with these models is their sensitivity to prompt formulation.
Small variations in the wording, structure, or even punctuation of prompts can sometimes lead to substantially different outputs~\cite{sclar2023quantifying,Raj2023Semantic,Mu2023Navigating}, which can impact their reliability and effectiveness across diverse applications, despite the user’s intention and information need remaining unchanged. This phenomenon, which we refer to as \textit{prompt sensitivity}, highlights the unpredictable nature of LLM responses and the challenges users face when crafting prompts~\cite{Feng2024UnveilingAM,zhuo2024prosa}. As such, prompt engineering—the art of designing such effective prompts—has become an active area of research~\cite{Lo2023The,Bhargava2023magicword}. 
Researchers have examined the effects of various prompt modifications, including minor structural and formatting changes \cite{sclar2023quantifying}, adversarial prompts \cite{Zhu2023PromptBench}, and differing levels of prompt specificity \cite{Murr2023Testing}, and have shown that LLMs are highly dependent on prompt formulation. %Furthermore, LLMs can be guided toward producing specific responses by employing particular prompt formatting techniques~\cite{Bhargava2023magicword}. 

Given the novelty and complexity of LLMs, users often lack clear guidelines on how to formulate prompts effectively. For instance, a seemingly minor adjustment in the prompt’s phrasing can unexpectedly alter the model's response quality, clarity, or accuracy. Such sensitivities create a barrier to achieving consistency in LLM performance in real-world applications, where reproducibility and reliability are essential. %Understanding these aspects is crucial not only for enhancing user experience but also for advancing the development of robust prompt-generation techniques.
To illustrate this issue, consider the example prompts shown in Table \ref{table:prompt_sensitivity_examples}. In this table, we present examples from the TriviaQA \cite{joshi2017triviaqa} and HotPotQA \cite{yang2018hotpotqa} question-answering datasets, where the LLM responds correctly and accurately to the original prompts. However, with only slight modifications in wording, we observe that the LLM (in this case LLaMA3.1) fails to satisfy the same information need as the original prompt. These examples underscore the need for a systematic approach to evaluate prompt sensitivity. 
We hypothesize that this sensitivity could arise for several reasons: for instance, an LLM may successfully respond to prompts closely aligned with examples seen during training, but struggle with slight modifications that it has not encountered. Another factor could be the model’s reliance on specific syntactic or semantic patterns to interpret prompts accurately, which may be disrupted by subtle changes.
Therefor,in this paper, we introduce a novel task and a dataset specifically curated for the prompt sensitivity prediction.  By curating a collection of prompts and their slight variations, we aim to identify which prompts work effectively with LLMs and which fail to elicit accurate responses. This dataset serves as a benchmark for studying prompt sensitivity, thus setting the stage for forthcoming studies in prompt engineering and the evaluation of LLMs.

\begin{table}[t]
\centering
\caption{Examples of prompt sensitivity from HotpotQA and TriviaQA datasets.}
\scalebox{0.6}{
\begin{tabular}{lp{6.3cm}p{0.1cm}p{6.2cm}p{0.1cm}p{1.7cm}p{1.9cm}p{1.5cm}}
\hline \hline
\textbf{Dataset} & \textbf{Original Prompt} & & \textbf{Alternative Prompt} & & \textbf{Original Answer} & \textbf{Alternative Answer} & \textbf{Correct Answer} \\
\hline
\small HotpotQA & What American actor and comedian known for playing the role of Newman in Seinfeld, also stars in the series The Exes on TV Land? & &What is the name of the American actor who played Newman in Seinfeld and appears in TV Land's comedy series The Exes & & Wayne Knight & Jerry Seinfeld co-star & Wayne Knight \\[2em]
TriviaQA & At which city do the Blue and White Niles meet? & & At which geographical location do the Blue and White Niles meet & & Sudan's confluence & Khartoum & Khartoum \\
\hline \hline
\end{tabular}}
\label{table:prompt_sensitivity_examples}
\end{table}


%Previous research has shown that while LLMs can generate variations of prompts, they lack the ability to recognize which variations yield optimal responses. Surprisingly, even small changes in prompt wording can lead to failure in meeting the intended information need. 
We show that although LLMs can autonomously generate different prompt variations, they cannot assess which variations are most effective, leaving users uncertain about how to phrase their information need. %This challenge is amplified by the diversity of users’ backgrounds and experiences with information-seeking systems. Unlike traditional web search, where users have established conventions and heuristics, prompting LLMs remains relatively unexplored. As a result, users with varied levels of expertise may approach prompting differently, often without knowing whether the LLM will respond accurately.
To establish a benchmark for this task, we formally structure the \textit{Prompt Sensitivity Prediction} task around the following questions: Given a user’s prompt, what are the effective variations? Do these effective variations consistently perform well across different LLMs? And ultimately, for any given prompt, can the LLM predict whether it will be able to respond effectively?
We curate our dataset based on TriviaQA and HotpotQA datasets \cite{joshi2017triviaqa,yang2018hotpotqa}, where answers are deterministic and generally concise. This choice simplifies evaluation and ensures reliability, allowing us to benchmark baselines accurately and measure their performance on prompt sensitivity prediction interactions.
To benchmark this task, we draw parallels with established tasks in document and text classification~\cite{gasparetto2022survey,CollinsThompson2010} and query performance prediction~\cite{hauff2008survey,Arabzadeh2020,Hambarde2023}, as these share some resemblance with prompt sensitivity prediction. For our baseline comparisons, we incorporate traditional methods from these domains as well as using the LLM itself baselines. However, our experiments reveal that none of these existing methods perform effectively for this specific task, underscoring the need for a novel approach tailored to the challenges of prompt sensitivity.

In summary, our contributions are as follows:
\begin{enumerate}
    \item We introduce a comprehensive dataset of prompt variations, a.k.a \ds, focusing on slight modifications to reveal prompt sensitivity in LLMs. Our dataset is available at \url{https://anonymous.4open.science/status/prompt-sensitivity-E6D8}.
    \item We define the prompt sensitivity prediction task, outlining the requirements and challenges involved in identifying effective prompts.
    \item We benchmark the prompt sensitivity prediction task using state-of-the-art methods, including text classification, query performance prediction, and LLM-based baselines, to highlight the task's challenges and complexity.
\end{enumerate}



\section{Methodology}

\subsection{Task Definition: Prompt Sensitivity Prediction}

The task of \textit{Prompt Sensitivity Prediction} aims to determine whether a given prompt can fulfill the intended information need it represents. Specifically, we define this task in terms of prompt variation and response reliability in LLMs. Given a prompt $P$ with a specific information need $I_P$, we consider a set of similar prompts, denoted $\mathcal{P}=\{ P' | Sim \big< P,P'\big> > \tau \ \text{and} I_P == I_{P'}\}$, where each variation $P'$ shares the same information need $I_P$ and maintains a similarity with $P$ above a predefined threshold $\tau$. These prompts $\{ P' \}$ are designed to be only slightly modified versions of $P$, ensuring they still reflect the same user intent. The core goal of this task is to predict, for a given prompt $P_i$, whether it will generate a response from the LLM that accurately meets the underlying information need $I_P_i$. 

\subsection{Proposed Approach}

To create our dataset for the prompt sensitivity task, we follow a systematic process to generate prompt variations and evaluate their effectiveness as follows:

\begin{enumerate}
    \item \textbf{Selecting Prompts}: We start by choosing a set of initial prompts, denoted as $\mathcal{P}$, where each prompt $p \in \mathcal{P}$ is seeking for a specific information need $I_p$.
    \item \textbf{Generating Variations}: For each prompt $p$ in the set, we use an LLM $\mathcal{L}$ to generate $N$ variations  $p'  = \mathcal{L}(p \mid I_p = I_{p'})$. Here, $\mathcal{L}(p)$ denotes the process of generating variations of prompt $p$, where each variation $p'$ retains high semantic similarity with $p$ i.e., $( Sim \big< P,P'\big> > \tau )$ and preserves the original information need $I_p$.
    \item \textbf{Filtering Variations}: We process and filter out any generated variations $ p'$ that do not meet specific criteria for similarity and alignment with the original prompt $p$ including LLM hallucinated content. 
    \item \textbf{LLM Response Generation}: .
    For each prompt $p$ and its variations $\{ p' \in \mathcal{P'} \}$, we  query LLM to answer the prompt, denoted $a_p \in \mathcal{A_P}$ for the original prompt and $a_{p'} \in \mathcal{A'_P}$ for each variation.
\end{enumerate}
The combination of $\mathcal{P} \cup \mathcal{P'}$  as well as their LLM generated answers $\mathcal{A_P} \cup \mathcal{A'_P}$ would create the core of our dataset. Due to limited space, we provide the algorithm of our dataset curation in our GitHub repository.


\begin{comment}
\begin{algorithm}
\caption{Dataset Creation for Prompt Sensitivity Task}
\begin{algorithmic}[1]
\State \textbf{Input:} Set of initial prompts $\mathcal{P}$
\State \textbf{Output:} Dataset of prompt variations and responses $(\mathcal{P} \cup \mathcal{P'}, \mathcal{A_P} \cup \mathcal{A_{P'}})$

\For{each prompt $p$ in $\mathcal{P}$}
    \State Generate $N$ variations $\{ p' \} = \mathcal{L}(p \mid I_p = I_{p'})$ using LLM, where each $p'$ retains similarity with $p$ and preserves $I_p$
    \State Filter variations $\{ p' \}$ to retain only those that meet criteria for similarity and alignment with $p$
    \For{each valid variation $p'$ in $\{ p' \}$}
        \State Generate response $r_{p'}$ for $p'$ using LLM
        \State Store response $a_{p'}$ as the answer for $p'$
    \EndFor
    \State Generate response $r_p$ for the original prompt $p$ using LLM
    \State Store response $a_p$ as the answer for $p$
\EndFor

\State \textbf{Return:} Dataset of prompts and responses $(\mathcal{P} \cup \mathcal{P'}, \mathcal{A_P} \cup \mathcal{A_{P'}})$
\end{algorithmic}
\end{algorithm}
\end{comment}

    
\section{\ds}

\subsection{Source Data}

To build our dataset, we require a set of questions that meet the following criteria: (C1) the availability of human annotated answers, (C2) accuracy and reliability in evaluation, and (C3) deterministic results. Therefore, we selected widely-used question-answering datasets, TriviaQA and HotpotQA datasets, that meet these requirements. 

\noindent \textbf{TriviaQA: }TriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. %The evidence for each question-answer pair can be a collection of wikipedia pages or top search results from the web. The dataset contains 650K training examples for the Web search results, accompanied with a compiled evidence document; and 78K examples of questions with wikipedia evidence documents (1.8 document on average per example). 
  % \textcolor{red}{tell more about how the answers look like, what is average number of words in the answers?} 
The questions are on average 14 words. Each question has a collection of accepted answers including a list of aliases and normalized version of the answers.  According to the human annotations of the dataset, more than 92 percents of the answers are mainly titles in Wikipedia; of which majority are concerned with a person or a location (more than 50 percents). As a result, majority of the answers are specific and short \cite{triviaqa_dataset}. 
%In this work we focus mainly on the wikipedia subset of TriviaQA since as described earlier majority of the answers are connected to unique Wikipedia titles.This quality makes it easier to evaluate the answers provided by LLMS if prompted by the questions in this subset. 


\noindent \textbf{HotpotQA:} HotpotQA is a large-scale question-answering dataset designed to support the development of more explainable QA systems by providing natural, multi-hop questions. This dataset consists of 113k training question-answer pairs, each grounded in Wikipedia-based documents. Unlike TriviaQA, HotpotQA includes complex multi-hop and comparison questions that require reasoning across multiple documents to answer accurately. %Questions in HotpotQA are not limited to fact-based inquiries; they also include comparison questions that involve integrating information from different sources. 
This diversity in question types and the need for cross-referencing make HotpotQA a challenging dataset that tests an LLM's ability to synthesize information across contexts \cite{yang2018hotpotqa}.

  
%In crafting our dataset, we leveraged only the question-answer pairs since the questions are hard to answer and models used in our approaches need to combine their knowledge from different sources to be able to correctly answer these questions. This characteristic of the dataset highlights significant differences in LLM responses to various prompt styles, underscoring the importance of our work.

In our work, we randomly sampled 12K questions from the train set of each of these datasets paired with their provided answers to base our proposed dataset. After preprocessing and removing variations that were less than 4 words or more than 20 wordsted, we obtained 11,469 unique questions and their respective answers.  We split them into a 70-30 ratio for training and testing i.e., 8028 and 3441 questions in our train and test set respectively for each of the TriviaQA and HotPotQA datasets. The questions in these pairs are the original prompts. In the next section, we explain how we process these prompts through LLMS and offer reformulated prompts that constitute the core of our proposed dataset and tasks. 
	
\subsection{Setup}
To generate prompt variations, we utilized two widely-used LLMs that have demonstrated strong performance across many downstream tasks: the pre-trained LLaMA 3.1 with 8B parameters \cite{LLaMA3} and Mistral-nemo \cite{jiang2023mistral7b}. We chose open-source models to ensure reproducibility and facilitate further research in prompt sensitivity. We designed the instructions to be as clear and straightforward as possible, similar to those intended for human use. The primary goal was to generate variations that retain the semantics of the original question, instructing the model to produce a rephrased prompt that does not answer the question directly but maintains the same information need and semantic content
\footnote{Due to space constraints, we have provided the full set of instructions in our GitHub repository at\url{https://anonymous.4open.science/status/prompt-sensitivity-E6D8}}.
However, LLMs occasionally deviate from the instructions due to hallucination, which affects consistency. To address this, we filtered out prompts that did not have at least nine valid variations generated and excluded prompts with fewer than four terms to maintain quality and consistency across the dataset. In total, we created our dataset on over 11,000 prompts, each with 9 different variations, resulting in more than 110K variations across each of the two datasets.

%The system prompt used to generate the 10 reformulated prompts given the original prompt is as follows:
	%"""You are a prompt engineer who can develop {num_variations} prompts from a given question. Do not answer the question. Your task is to just reformulate the question and create {num_variations} different versions of it that preserve the original information need. Only return the {num_variations} reformulated prompts related to the original question, separated by {separator}. Do not include anything else in your response."""

%We evaluate the effectiveness of each generated prompt based on the following baselines:
%llm as a baseline:
%The prompts generated are given to the llms LLaMA3.1-8B and mistral-nemo to see if they are able to predict their capabilities. 
%We used LLMs directly as a judge on its performance on a prompt. To do so, we used LLaMA3.1-8B and mistral-nemo. We prompted each LLM to predict whether or not it can answer each question and its respective variations from the dataset. We used the following system prompt together with each question from the dataset :'You are a question-answering prediction system. Your job is to predict whether you can accurately answer the question or not.  For each prompt, reply "1" if you can answer it and "0" if you cannot. Respond only with "0" or "1" 

\subsection{Baselines}

To benchmark our proposed dataset and task, we identified tasks with similarities to prompt sensitivity prediction and selected these as potential baselines.

\subsubsection{LLM-Based Baselines.}
As a primary baseline, we employ LLMs directly by asking them to self-assess their ability by predicting whether they can accurately answer a given prompt or not~\cite{yan2024llmevaluator}. %Specifically, we prompt the LLM to predict whether it can accurately respond to the prompt’s information need. 
This approach, which we refer to as the LLM self-evaluation baseline, relies on the model’s internal confidence in its responses. The prompts and instructions used for this baseline can be found in our GitHub repository for reproducibility.

\subsubsection{Text Classifiers.}
Another perspective treats prompt sensitivity prediction as a text classification task. In this setup, we train a text classifier on our dataset’s training set and evaluate its performance on the test set to predict whether the LLM’s response to a prompt will meet the information need. For this purpose, we label prompts with a 1 if there is an exact match between the LLM’s response and the ground truth, and 0 otherwise. We used text classifiers implemented by Simple Transformers \cite{} with pre-trained Hugging Face models BERT, and DeBERTa. The classifiers were trained for 1 epoch on training data, and evaluated against the test data.  

\subsubsection{Query Performance Predictors.}
Prompt sensitivity prediction is also conceptually related to the task of query performance prediction \cite{poesina2024pqppjointbenchmarktexttoimage}, where the goal is to estimate the quality of retrieved documents with respect to how well they satisfy the information need behind a query. For our task, we adapt the QPP concept to prompt performance prediction—predicting whether a prompt will yield a correct response from an LLM.
QPP methods typically fall into pre-retrieval and post-retrieval categories. However, since generative settings do not produce traditional "retrieved lists" (e.g., ranked documents or links), only pre-retrieval QPP methods—those that rely solely on the query and do not depend on a document collection—are applicable here. Collection-dependent QPP methods are excluded due to the absence of a corpus in our setup. Thus, we employ state-of-the-art, neural-based pre-retrieval QPP methods that are collection-agnostic to serve as baselines for this task.
We note that since the output of QPP methods is a scalar value, inspired by previous studies\cite{}, we convert them to binary by classifying values above the mean of the data as one category and those below as another.


We adopted BERTPE \cite{Khodabakhsh2024}, a state-of-the-art pre-retrieval QPP model that uses contextualized embeddings to learn query performance directly. Following the original paper’s approach, we trained BERTPE on our training set and evaluated it on the test set. Additionally, we considered a group of neural embedding-based specificity metrics to assess prompt sensitivity. These metrics are based on the intuition that a more specific query is easier to answer, while broader queries may cover multiple aspects and are thus harder to satisfy.
%In this group of metrics, the approach involves constructing an ego network from query terms in the embedding space, where terms are connected to their most similar counterparts. The rationale is that specific terms are surrounded by other specific terms, whereas general terms are surrounded by more loosely related terms. The density of this graph can then serve as an indicator of query performance: denser graphs suggest that the surrounding terms are more similar and the query is more specific, making it easier to address. 
For details on these metrics, we refer readers to the original paper \cite{Arabzadeh2019,Arabzadeh2020,arabzadeh2020neural}.
From this group, we included metrics such as Closeness Centrality (CC), Degree Centrality (DC), PageRank , and Inverse Edge Frequency (IEF), all measured on the ego network of the query terms in the embedding space %These metrics allow us to estimate query specificity based on the structural properties of the network surrounding the query terms, thus 
serving as indicators of the prompts specificity as the signal of potential success of the prompt in eliciting accurate responses from the LLM.



\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{hist_prompt_variations_answered_triviaqa.pdf} % Replace with your image file name
\caption{TriviaQA}
\label{fig:example}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{similarity_plot_baseline_prediction_triviaqa_together.pdf} % Replace with your image file name
\caption{TriviaQA}
\label{fig:example}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{similarity_plot_no_llm_responds.pdf} % Replace with your image file name
\caption{TriviaQA}
\label{fig:example}
\end{figure}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}
\caption{Results of baselines on \ds.}
\scalebox{0.8}{\begin{tabular}{p{0.8cm}cccccccccc}
\hline
\hline
\multirow{2}{*}{} & \multirow{2}{*}{Category} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{\ds -TriviaQA} & \multicolumn{4}{c}{\ds - HotPotQA} \\
 &  &  & Accuracy & F1 & Recall & Precision & Accuracy & F1 & Recall & Precision \\ \hline
\multirow{10}{*}{\rotatebox{90}{\centering Mistral Answers}} & \multirow{2}{*}{LLM-Based} & Mistral & 0.5045 & 0.5858 & 0.7743 & 0.4711 & 0.3735 & 0.2005 & 0.6912 & 0.1173 \\
 &  & LLaMA & 0.4656 & 0.6239 & 0.9798 & 0.4577 & 0.1696 & 0.2050 & 0.9419 & 0.1150 \\ \cline{2-11} 
 & \multirow{3}{*}{Text Classification}   & BERT & 0.660 & 0.659 & 0.620 & 0.654 & 0.526 & 0.360 & 0.017 & 0.813 \\
 &  & DeBERTa & 0.648 & 0.646 & 0.588 & 0.647 & 0.523 & 0.353 & 0.010 & 0.757 \\ \cline{2-11} 
 & \multirow{5}{*}{QPP Methods} & CC & 0.506 & 0.453 & 0.452 & 0.454 & 0.549 & 0.209 & 0.524 & 0.130 \\
 &  & DC & 0.484 & 0.448 & 0.463 & 0.434 & 0.565 & 0.199 & 0.475 & 0.126 \\
 &  & IEF & 0.505 & 0.462 & 0.469 & 0.455 & 0.535 & 0.204 & 0.526 & 0.127 \\
 &  & PageRank & 0.481 & 0.444 & 0.458 & 0.431 & 0.533 & 0.153 & 0.370 & 0.096 \\
 &  & BERTPE & 0.648 & 0.627 & 0.644 & 0.611 & 0.710 & 0.318 & 0.594 & 0.217 \\ \hline
 \hline
\multirow{10}{*}{\rotatebox{90}{\centering LLaMA Answers}} & \multirow{2}{*}{LLM-Based} & Mistral & 0.5160 & 0.6045 & 0.7704 & 0.4974 & 0.3731 & 0.1978 & 0.6930 & 0.1153 \\
 &  & LLaMA & 0.4940 & 0.6507 & 0.9818 & 0.4866 & 0.1674 & 0.2013 & 0.9408 & 0.1127 \\ \cline{2-11} 
 & \multirow{3}{*}{Text Classification}  & BERT & 0.664 & 0.664 & 0.651 & 0.650 & 0.532 & 0.377 & 0.034 & 0.808 \\
 &  & DeBERTa & 0.652 & 0.652 & 0.640 & 0.638 & 0.519 & 0.342 & .00 & .00 \\ \cline{2-11} 
 & \multirow{5}{*}{QPP Methods} & CC & 0.500 & 0.463 & 0.449 & 0.478 & 0.545 & 0.199 & 0.507 & 0.123 \\
 &  & DC & 0.484 & 0.464 & 0.465 & 0.463 & 0.562 & 0.190 & 0.462 & 0.120 \\
 &  & IEF & 0.510 & 0.482 & 0.475 & 0.489 & 0.535 & 0.202 & 0.529 & 0.125 \\
 &  & PageRank & 0.482 & 0.461 & 0.461 & 0.461 & 0.534 & 0.151 & 0.371 & 0.094 \\
 &  & BERTPE & 0.659 & 0.651 & 0.646 & 0.656 & 0.710 & 0.314 & 0.596 & 0.213 \\ \hline
 \hline
\end{tabular}}
\label{tab:baselines}
\end{table}


% \begin{table}[]
% \begin{tabular}{ccc|cccc|cccc}
% \hline
% \hline
% \multirow{2}{*}{Category} & \multirow{2}{*}{LLM} & \multirow{2}{*}{Response LLM} & \multicolumn{4}{c|}{P3LLM-TriviaQA} & \multicolumn{4}{c}{P3LLM-HotPotQA} \\
%  &  &  & Accuracy & F1 & Recall & Precision & Accuracy & F1 & Recall & Precision \\ \hline
% \multirow{4}{*}{\rotatebox{90}{\scriptsize LLM based}} & Mistral & Mistral & 0.50 & 0.59 & 0.77 & 0.47 & 0.37 & 0.20 & 0.69 & 0.12 \\
%  & LLaMA & Mistral & 0.47 & 0.62 & 0.98 & 0.46 & 0.17 & 0.20 & 0.94 & 0.11 \\
%  & LLaMA & LLaMA & 0.49 & 0.65 & 0.98 & 0.49 & 0.17 & 0.20 & 0.94 & 0.11 \\
%  & Mistral & LLaMA & 0.51 & 0.60 & 0.77 & 0.49 & 0.37 & 0.20 & 0.69 & 0.11 \\ \hline
% \multirow{7}{*}{\rotatebox{90}{\scriptsize Text Classification}} & RoBERTa & LLaMA & 0.653 & 0.652 & 0.645 & 0.637 & 0.519 & 0.342 & 0 & 0 \\
%  & BERT & LLaMA & 0.664 & 0.664 & 0.651 & 0.650 & 0.532 & 0.377 & 0.034 & 0.808 \\
%  & DeBERTa & LLaMA & 0.652 & 0.652 & 0.640 & 0.638 & 0.519 & 0.342 & .00 & .00 \\
%  & RoBERTa & Mistral & 0.519 & 0.342 & 0.0 & 0.0 & 0.519 & 0.342 & 0.0 & 0.0 \\
%  & BERT & Mistral & 0.660 & 0.659 & 0.620 & 0.654 & 0.526 & 0.360 & 0.017 & 0.813 \\
%  & DeBERTa & Mistral & 0.648 & 0.646 & 0.588 & 0.647 & 0.523 & 0.353 & 0.010 & 0.757 \\
%  & Betweenness Centrality & LLaMA & 0.508 & 0.472 & 0.457 & 0.487 & - & - & - & - \\
%  & Closeness Centrality & LLaMA & 0.501 & 0.464 & 0.449 & 0.479 & - & - & - & - \\
%  & Degree Centrality & LLaMA & 0.485 & 0.464 & 0.465 & 0.464 & - & - & - & - \\
%  & Edge Count & LLaMA & 0.514 & 0.440 & 0.397 & 0.492 & - & - & - & - \\
%  & IEF & LLaMA & 0.510 & 0.483 & 0.476 & 0.490 & - & - & - & - \\
%  & Page Rank & LLaMA & 0.483 & 0.462 & 0.462 & 0.461 & - & - & - & - \\
%  &  &  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \hline
%  \hline
% \end{tabular}
% \end{table}

\section{Findings}

\subsection{Benchmarking}
In this section, we analyze the performance of our three groups of baselines, LLM-based, text classification-based, and QPP-based—on our proposed dataset. Table \ref{tab:baselines} presents the results for predicting whether each prompt variation could be correctly asnwered evaluated using both the LLaMA and Mistral models \ds test set. We report results in terms of accuracy, F1, recall, and precision.

As shown in Table \ref{tab:baselines}, the specificity-based QPP methods (i.e., CC, DC, IEF, and PageRank) perform the lowest among the baselines. Since QPP methods were not specifically designed for prompt sensitivity prediction, their performance is relatively weak on both datasets. We hypothesize that the specificity levels across the original prompts and their variations are too similar, making it challenging for specificity metrics to effectively distinguish between different levels of prompt specificity. On the other hand, BERTPE demonstrates high effectiveness in determining whether a prompt can be answered correctly. Notably, unlike other specificity-based QPP methods, BERTPE is supervised. While it shows competitive performance compared to text classification-based methods on \ds-TriviaQA, it outperforms other baselines significantly on \ds-HotpotQA. This suggests that supervised QPP methods can be well-suited for the prompt sensitivity prediction task.

The LLM-based baselines show improved results on TriviaQA but lack consistency on HotpotQA. This is opposite of our observation for BERTPE, indicating that while one method can predict the performance of specific set of questions, they might fail to predict the other one.
 %This indicates that, while LLMs can generate a variety of prompt variations, they struggle to reliably distinguish between variations that can be answered and those that cannot, even when the topic and information need remain the same.
In addition, text classifiers that have been fine-tuned for this specific task tend to perform better, their results remain underwhelming. This underscores the inherent difficulty in predicting prompt sensitivity. %These observations indicate that despite some promising techniques, further advancements may be required to effectively handle the complexities of this task.

In Figure XXX, we present histograms of prompts showing how many of their 10 variations were answered correctly, with results for TriviaQA and HotpotQA displayed in the top and bottom rows of the figure’s right column, respectively. For instance, in HotpotQA, over 400 prompts have only one of the 10 variations answered correctly, and nearly 300 prompts have two variations answered correctly. Notably, in TriviaQA, the distribution is more balanced, with a relatively even number of prompts having  N variations answered correctly. In contrast, HotpotQA shows a skewed distribution, with fewer prompts having multiple variations answered correctly. We hypothesize that this difference may arise from varying levels of query difficulty across the two datasets, making prompt sensitivity an interesting aspect to compare between them.


\subsection{Original Prompt Correctness and Variation Answerability}
Additionally, we provide further analysis on the accuracy of answers for both the original prompts and their variations in Figure \ref{}. Due to space limitations, we present the analysis for TriviaQA with the LLaMA-3b model here, while additional analysis for the Mistral model and HotpotQA dataset is available in our GitHub repository.
Figure \ref{}(a) illustrates the distribution of correctly answered prompts across various levels, with TriviaQA showing a roughly balanced distribution. This means we observe a similar number of prompts with varying counts of correct responses across all 10 variations, as opposed to a long-tail distribution. In this figure, the x-axis represents the count of correctly answered variations out of the 10, grouping prompts by the number of successful variations.
In Figure \ref{}(b), we further break down the results based on whether the original prompt was answered correctly or not. Here, we observe that when the original prompt is answered correctly (shown in the blue histogram), there is a higher number of variations that also yield correct answers, reflected by an ascending pattern in the histogram. Conversely, when the original prompt is answered incorrectly, most of the prompts have only one out of the 10 variations answered correctly, displaying a descending pattern in the red histogram.

\subsection{Impact of Similarity to the Original Prompt}
We aim to investigate the impact of variation similarity to the original prompt on the predictability of LLM responses. Specifically, we ask whether a variation that is more similar to the original prompt has a higher likelihood of generating a correct answer, while less similar variations might lead to lower predictability. To test this hypothesis, we conducted experiments, as shown in Figure \ref{}.

In Figure \ref{}(a), we present the distribution of correctly and incorrectly answered prompts (right side) and in Figure \ref{}(b), the distribution of correctly and incorrectly predicted responses based on similarity to the original prompt. Similarity is measured using the cosine similarity of the embedded representations of prompt-variation pairs, calculated with MiniLM, a model known for its strong performance in various NLP and IR tasks, as well as its lightweight nature \cite{minilm}.

From this figure, we observe that when a variation is more similar to the original prompt, it is more likely to generate correct responses and correct predictions of answerability. This indicates that the LLM is highly biased toward its training data, with reduced generalizability to less familiar or novel prompt formulations.



\section{Concluding Remarks}
In this study, we explored the sensitivity of LLMs to prompt variations by defining the Prompt Sensitivity Prediction task and providing a dataset based on established question-answering datasets, TriviaQA and HotpotQA. Through benchmarking using LLM-based self-evaluation, text classification, and QPP methods, we observed that no existing approach fully addresses the complexities of prompt sensitivity. While QPP methods underperformed on this task, both LLM-based and text classification approaches exhibited inconsistent results across datasets, emphasizing the challenge of reliably predicting prompt answerability. These findings highlight the need for further research into prompt robustness, specifically methods that can help users generate reliable prompts. We hope our work encourages advancements in prompt engineering, making interactions with LLMs more consistent and predictable.








\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
