\section{Introduction}
\label{sec:intro}

% Recently, neural radiance field (NeRF) \cite{mildenhall2021nerf}, which represents a target scene by a multi-layer perception (MLP) model and its parameters, has gained more and more attention in the field. Due to the strong representation ability and high compactness of MLP, NeRF and its follow-ups \cite{zhang2020nerf++, liu2020neural, barron2021mip} achieves better scene reconstruction quality with a lower memory budget compared to traditional explicit 3D representations. What's more, the differentiable volume rendering algorithm enables NeRF to be trained end-to-end based on multi-view images along with their corresponding camera parameters.

% Neural radiance field (NeRF) \cite{mildenhall2021nerf}, which represents a target object (or scene) by neural networks (NNs) and their parameters, has obtained increasing interest. Thanks to the strong representation ability and high compactness of NNs, NeRF and its follow-ups \cite{zhang2020nerf++, liu2020neural, barron2022mip, muller2022instant} offer enhanced reconstruction quality with a lower memory footprint in comparison to traditional explicit 3D representations. However, the editing of NeRF is a notorious problem, due to the entanglement between different components represented implicitly in the field.

% \yue{sentence1: start with the meaning of 3D editing . sentence2: there have been a series 3D editing projects with the development of 3D representation and generative model(such as diffusion). sentence3: however, problem.}

3D implicit scene editing constitutes a significant yet challenging task in the realm of computer graphics and computational vision, intending to modify an existing 3D scene represented by an implicit field. The advancement of implicit 3D representations has fostered a myriad of 3D editing endeavors, including \cite{yuan2022nerf, garbin2022voltemorph, xu2022deforming, tertikas2023partnerf, peng2022cagenerf, jambon2023nerfshop, liu2021editing, xiang2021neutex, gong2023recolornerf, kuang2023palettenerf, lee2023ice, niemeyer2021giraffe, xu2023discoscene, li2022climatenerf, zhang2021nerfactor, boss2021neural, srinivasan2021nerv, zhang2022arf, wang2023nerf}. Nevertheless, most of them have concentrated on rudimentary modifications, such as geometry or texture editing, which restricts the generalizability and user accessibility of these methods.

% Furthermore, NeRF's utilization of a differentiable volume rendering algorithm facilitates end-to-end training based on multi-view images and their corresponding camera parameters.

% However, the editing of NeRF is a notorious problem. Since NeRF represents the target object implicitly by MLP parameters, different components and attributes of the object tend to entangle with each other, which makes it more difficult to edit the target object compared to the traditional explicit representation cases. Many previous works have proposed solutions for NeRF editing, concentrating on the tasks of geometry deformation \cite{yuan2022nerf, garbin2022voltemorph, xu2022deforming, tertikas2023partnerf, peng2022cagenerf, jambon2023nerfshop}, texture or color editing \cite{liu2021editing, xiang2021neutex, gong2023recolornerf, kuang2023palettenerf, lee2023ice}, scene manipulation \cite{niemeyer2021giraffe, xu2023discoscene}, relighting \cite{li2022climatenerf, zhang2021nerfactor, boss2021neural, srinivasan2021nerv} and stylization \cite{zhang2022arf, wang2023nerf}.

%Recently, as the emergence of vision-language models, such as CLIP \cite{radford2021learning} and noise diffusion model \cite{nichol2021glide, rombach2022high, saharia2022photorealistic, ramesh2022hierarchical}, more and more researchers utilize pre-trained 2D text-image models to edit implicit neural fields based on user-entered text instructions \cite{wang2022clip, wang2023nerf, haque2023instruct, fang2023text, mirzaei2023watch}.
Recent advancements in vision-language models, notably CLIP \cite{radford2021learning} and various noise diffusion models \cite{nichol2021glide, rombach2022high, saharia2022photorealistic, ramesh2022hierarchical}, have prompted an increase in the use of pre-trained 2D text-image models for editing implicit neural fields via text instructions \cite{wang2022clip, wang2023nerf, haque2023instruct, fang2023text, mirzaei2023watch}. 
The Instruct-NeRF2NeRF (IN2N) framework \cite{haque2023instruct}, utilizing the "Iterative Dataset Update" (IDU) strategy, represents a significant development in this area.
% Instruct-NeRF2NeRF (IN2N) \cite{haque2023instruct} proposes a neural field editing framework based on the ``Iterative Dataset Update" (IDU) strategy.
IDU leverages a 2D text-based editing model, Instructpix2pix (IP2P) \cite{brooks2023instructpix2pix}, to update the training dataset and finetunes the neural fields alternatively. In this way, both the dataset and model are updated to align to a user-provided text prompt. Despite the fascinating editing results, IN2N reveals several limitations.

\begin{figure}
    \centering

    \rotatebox{90}{~~ Blurry Background}
    \begin{subfigure}{0.31\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/sub_1_v1.pdf}
            \caption{Original Scene}
            \label{fig: 1-original}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/sub_2_v1.pdf}
            \caption{IN2N}
            \label{fig: 1-IN2N}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/sub_3_v1.pdf}
            \caption{DualNeRF}
            \label{fig: 1-DualNeRF}
        \end{minipage}
    \end{subfigure}

    \rotatebox{90}{~Local Optima}
    \begin{subfigure}{0.95\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/sub_4_v6.pdf}
            \caption{An example of IN2N's local optima issues}
            \label{fig: 4-local_optima}
        \end{minipage}
    \end{subfigure}
  
    \caption{\textbf{Limitations of IN2N \cite{haque2023instruct}.} There are two main limitations exposed by IN2N: (1) blurry background and (2) being prone to the local optima. The first row shows a comparison of the background performance among the rendering results of the original scene, IN2N, and ours. IN2N generates the most blurry background. The second row shows an example of IN2N's local optima issues which manifests as incomplete edits to the original scene. In comparison, DualNeRF outputs satisfactory results.}
    \label{fig: IN2N_drawbacks}
\end{figure}

% Despite the fantastic final editing results, IN2N models usually fall into a chaotic intermediate state with obvious artifacts during training. These artifacts are caused by the IDU strategy which alternately updates the train data and inevitably brings inconsistency into the training dataset. IN2N models with these artifacts generate degenerate rendering results, which further mislead IP2P to generate low-quality editing results and contaminate the training dataset with more inconsistency. An example of this series of events is shown in s figure \ref{fig: artifacts}.

% Despite the fascinating editing results, we show by experiments that IN2N exposes drawbacks including blurry backgrounds and easily trapped in local optima, as shown in Fig. \ref{fig: IN2N_drawbacks}. These drawbacks severely hurt the final editing quality of IN2N. In this work, we propose a new text-driven 3D scene editing method called \textbf{DualNeRF} to deal with these problems.

Firstly, IN2N generates edited scenes with blurry backgrounds, as shown in Fig. \ref{fig: 1-IN2N}. Essentially, IDU is a training process that optimizes both the dataset and the model with text prompt $y$ as the only guidance to control the optimization direction. This weak guidance provides no guarantee of preserving the original background. IP2P edits with distorted backgrounds provide wrong training signals to the NeRF model, jittering the original texture and resulting in blurred backgrounds after training. The single-field architecture used in IN2N cannot deal with this problem, since it is unable to preserve any initial feature after long-term optimizations.

% Secondly, IN2N is prone to be trapped in a local optimal solution. As shown in Fig. \ref{fig: 4-local_optima}, the incompletely edited rendering results (shown in the top row) mislead IP2P to generate edits with similar appearances (shown in the bottom row). This in turn provides wrong training signals to the model and makes the situation even worse. After a long period of training, the incomplete editing issue becomes ineffaceable. This marks that the model traps in local optima. In other words, IDU tends to preserve some sub-optimal edits during training due to the mutual reinforcement between IP2P edits and model training.

Secondly, the IN2N framework exhibits a susceptibility to becoming ensnared in local optima. As illustrated in Fig. \ref{fig: 4-local_optima}, partially edited renderings (displayed in the top row) mislead IP2P to produce edits with similar appearances (displayed in the bottom row). This in turn provides wrong training signals to the model and makes the situation even worse. Over prolonged training duration, the incomplete editing issue becomes ineffaceable, signifying the model's entrapment in local optima. In essence, IDU tends to preserve sub-optimal edits during training due to the mutual reinforcement between IP2P edits and model training.

% First of all, we propose a dual-field representation called \textbf{DualNeRF} to stabilize the training process and therefore preserve more background details. Specifically, DualNeRF contains two neural networks. One is set as the major field, whose parameters are trained to faithfully reconstruct the original scene and frozen during editing. Another is designed as the editing field, which is gradually enabled and trained during editing. Output features from the two fields fused by activation functions output the final result. DualNeRF performs flexible editing by the editing field while preserving the information of the original scene by the major field to stabilize the quality of the unedited area during training.

We show by experiments that these limitations severely hurt the final editing quality of IN2N. In this work, we propose a novel text-driven 3D scene editing method called \textbf{DualNeRF} to address these problems.

% First of all, we propose a dual-field representation to deal with the blurry background problem. Specifically, DualNeRF consists of two fields. One of them is trained to reconstruct the original scene and frozen during editing, while the other one is optimized for edits. The frozen field preserves authentic features of the original scene. These features serve as additional guidance during IDU, avoiding the NeRF model drifting away from the original scene caused by the background distortion in IP2P edits after long-term optimization.

% First of all, we introduce new guidance signals to the model during IDU to help preserve texture in background areas. Intuitively, the initial field before editing contains abundant features of the original scene, which can serve as perfect guidance for background maintenance. However, these features drift away from the initial stage during the long-term optimization of IDU. To preserve these features during training, we propose a dual-field representation. One field in the model serves as the guidance field which is frozen during IDU and provides features of the original scene. The other field serves as an editing field which is trained to perform edits during IDU. Features provided by the guidance field prevent the model from drifting away from the original scene, countering background distortion brought by IP2P edits.

First of all, we introduce additional guidance signals to the model during IDU to maintain textures in the background areas.
% Recognizing that abundant features of the original scene in the initial field offer excellent background preservation guidance, we propose a dual-field representation to combat feature drift during the long-term optimization of IDU.
Intuitively, the initial field before editing contains abundant features of the original scene, which can serve as perfect guidance for background maintenance. However, these features drift away from the initial stage during the long-term optimization of IDU. To preserve these features during training, we propose a novel dual-field representation.
This representation comprises a static field, preserving the original scene's features for guidance, and a dynamic field, trained for performing edits. Features provided by the static field help stabilize the model, mitigating background distortions often induced by IP2P edits.

Moreover, a simulated annealing (SA) strategy \cite{kirkpatrick1983optimization} is incorporated into IDU to address local optima issues. SA is a well-known algorithm used to solve local optima through random acceptance of sub-optimal updates. Inspired by this idea, instead of editing the renderings from the latest model, we randomly send some ``outdated" inputs to IP2P for editing. These outdated inputs are derived from ``half-edited" models by decreasing the intensity of the dynamic field, which will be introduced in details in Sec. \ref{sec: Simulated Annealing Strategy}.
% This simulated annealing strategy helps our model jump out of local optima efficiently.
This adaptation of the SA strategy significantly enhances our model's ability to overcome local optima.

A CLIP-based consistency indicator is also used to measure the reliability of each editing result of IP2P. Editing results with higher reliability are controlled to exert stronger impacts on the neural field, and vice versa. In this way, high-quality editing results with fewer artifacts will be ``reserved", while low-quality results which extremely deviate from the original image will be ``filtered out".

In summary, the contributions of our work include:
\begin{itemize}
    \item We propose DualNeRF, a dual-field representation with a static field for guidance signal providing and a dynamic field for flexible editing. This novel architecture provides new guidance signals to the model during IDU and results in edits with clearer background.
    \item We introduce a simulated annealing strategy into IDU, which endows our model with the ability to address local optima.
    \item We design a CLIP-based consistency indicator to measure the edits of IP2P, which can strengthen the impact of high-quality edits while weakening the low-quality ones.
    \item Experiments demonstrate that our method achieves better editing performance compared to IN2N.
\end{itemize}
