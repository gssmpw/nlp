\section{Preliminaries}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/dual_network_v10.pdf}
    \caption{\textbf{The Overview of DualNeRF.} DualNeRF consists of two neural radiance fields, including a static field $f_S$ and a dynamic field $f_D$ with the same network architecture. The static field $f_S$ is trained in the field initialization stage and frozen in the editing stage. The dynamic field $f_D$ is enabled during the editing stage and trained to achieve field editing. Two fields fuse in the hidden feature level. A simulated annealing-based IDU strategy is used to perform editing. Furthermore, a CLIP-based consistency indicator is calculated based on the inputs and outputs, which filters out low-quality edits softly and therefore cleans up the updated dataset.}
    \label{fig: overview}
\end{figure*}

\subsection{Neural radiance fields}
\label{sec: Neural radiance fields}
NeRFs \cite{mildenhall2021nerf} represent a target scene/object implicitly by neural networks (NNs). Specifically, given a space position $\mathbf{x} = (x, y, z)$ and a view direction $\mathbf{d} = (\theta, \phi)$, a NeRF model $f$ outputs the occupancy $\sigma(\mathbf{x})$ at $\mathbf{x}$ and the radiance $\mathbf{c}(\mathbf{x}, \mathbf{d})$ at $\mathbf{x}$ viewed from $\mathbf{d}$, namely
\begin{equation}
    (\sigma(\mathbf{x}), \mathbf{c}(\mathbf{x}, \mathbf{d})) = f(\mathbf{x}, \mathbf{d})
    \label{equ: NeRF}
\end{equation}
The rendering of NeRF can be achieved by volume rendering. For a ray $\mathbf{r} = \mathbf{o} + t\mathbf{d}$, where $\mathbf{o}$ is the origin and $\mathbf{d}$ is the direction, $N$ samples $\{x_i = \mathbf{o} + t_i\mathbf{d}\}_{i=1}^{N}$ are sampled along the ray. The color $\hat{C}(\mathbf{r})$ of the ray is calculated as an alpha blending: $\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}w_i\mathbf{c}(\mathbf{x}_i, \mathbf{d})$,
% \begin{equation}
%     \hat{C}(\mathbf{r}) = \sum_{i=1}^{N}w_ic(\mathbf{x}_i, \mathbf{d})
%     \label{equ: vol. rendering 1}
% \end{equation}
where $w_i = T_i(1-\exp(-\delta_i\sigma(\mathbf{x}_i))$
% \begin{equation}
%     w_i = \exp(-\sum_{j=1}^{i-1}\delta_j\sigma(\mathbf{x}_j))(1-\exp(-\delta_i\sigma(\mathbf{x}_i))
%     \label{equ: vol. rendering 2}
% \end{equation}
is the blending weight of $\mathbf{c}(\mathbf{x}_i, \mathbf{d})$. $\delta_i = t_{i+1} - t_i$ is the distance between adjacent sample points. $T_i = \exp(-\sum_{j=1}^{i-1}\delta_j\sigma(\mathbf{x}_j))$ is the transmittance.

The differentiable nature of the volume rendering helps NeRF to be trained by stochastic gradient descent. Specifically, given a multi-view dataset $\mathcal{I} = \{(I_j, P_j)\}_{j=1}^{N}$, where $I_j$ is a ground truth image and $P_j$ represents its camera pose, an L2-loss between the rendering $\hat{C}(\mathbf{r})$ and ground truth $C(\mathbf{r})$ can be used to train the NeRF model:
\begin{equation}
    L_{rgb} = \sum_{\mathbf{r} \in \mathcal{R}}||\hat{C}(\mathbf{r}) - C(\mathbf{r})||^2_2
    \label{equ: nerf loss}
\end{equation}
where $\mathcal{R}$ is a batch of rays sampled from $\mathcal{I}$. Additional losses such as LPIPS loss \cite{8578166} can also be used to improve the rendering quality.

\subsection{Instruct-NeRF2NeRF}
\label{sec: Instruct-NeRF2NeRF}
Instruct-NeRF2NeRF \cite{haque2023instruct} (IN2N) proposes a new NeRF editing framework called ``Iterative dataset update" (IDU). Specifically, the framework contains two steps:
\begin{enumerate}
    \item A dataset updating step,
    % in which $K$ renderings $\hat{\mathbf{I}} = \{\hat{I}_i\}_{i=1}^{K}$ rendered from $K$ random views $\mathbf{P} = \{P_i\}_{i=1}^{K}$ in the training dataset $\mathcal{I}$ are edited by an IP2P model conditioned on a prompt $\mathbf{y}$ and the original image $I_i$, followed by replacing the corresponding images in $\mathcal{I}$, as equation \ref{equ: update dataset} shows:
    % \begin{equation}
    %     \mathcal{I}' = \{(\text{IP2P}(\hat{I}_i | I_i, \mathbf{y}), P_i) | P_i \in \mathbf{P}\} \cup \{(I_j, P_j) \in \mathcal{I} | P_j \notin \mathbf{P} \}
    %     \label{equ: update dataset}
    % \end{equation}
    % where $\mathcal{I}'$ is the updated dataset.
    in which $d$ images in the training dataset $\mathcal{I}$ are replaced by $d$ edits $\{I'_i\}_{i=1}^{d}$ generated by a text-driven image editing model conditioned on the prompt $\mathbf{y}$ and the original image $\{I_i\}_{i=1}^{d}$, resulting in an updated dataset $\mathcal{I}'$ mixed with old and new images.
    \item A NeRF updating step, where the NeRF model is trained on the new dataset $\mathcal{I}'$ for $n$ iterations.
\end{enumerate}
These two steps alternate until convergence. The image editing model used in the first step by IN2N is InstructPix2pix \cite{brooks2023instructpix2pix} (IP2P), a SOTA text-driven image editing method based on Stable Diffusion \cite{rombach2022high}.

% \paragraph{Drawbacks of IN2N.}
% Despite the impressive editing results, IN2N has several drawbacks: 
% \begin{enumerate}
%     % \item IN2N generates edited scenes with blurry backgrounds, as Fig. \ref{fig: 1-IN2N} shows. The single-field architecture used by IN2N cannot preserve the original features of unedited regions after long-term training. As a result, it is hard to implement local editing without hurting the quality of background areas.
%     \item IN2N generates edited scenes with blurry backgrounds, as shown in Fig. \ref{fig: 1-IN2N}. Essentially, IDU is a training process that optimizes both the dataset and the model with text prompt $y$ as the only guidance to control the optimization direction. This weak guidance provides no guarantee of preserving the original background. IP2P edits with distorted backgrounds provide wrong training signals to the NeRF model, jittering the original texture and resulting in blurred backgrounds after training. The single-field architecture used in IN2N cannot deal with this problem, since it is unable to preserve any initial feature after after long-term optimizations.
%     % \item IN2N is prone to be trapped in the local optimal solution. The IDU strategy inherently brings inconsistency into the training process as editing results from different iterations mixed inside the dataset. As a result, artifacts appear in the rendering results during training. These artifacts further mislead the editing results of IP2P, since IP2P trains on a high-quality dataset with no artifacts. An example can be seen in Fig. \ref{fig: 4-local_optima}. The white $U$-shape artifact on the back of the bear misleads IP2P to generate edits with the same artifacts, which in turn further degenerate the artifact by updating the training dataset with low-quality edits.
%     \item IN2N is prone to be trapped in a local optimal solution. IDU has an ``artifact amplifying" property, which preserves and amplifies artifacts during training. As shown in Fig. \ref{fig: 4-local_optima}, the U-shape white artifacts that appear on the back of the bear in the rendering results mislead IP2P to generate edits with the same artifacts. This in turn provides wrong training signal to the model and makes the artifacts even worse. After a long period of training, the artifact becomes ineffaceable, marking the model traps in local optima.
% \end{enumerate}

% \cite{haque2023instruct} proves in the ablation study that the IDU strategy outperforms one-time DU because one-time DU generates a training dataset with high inconsistency. Nevertheless, the IDU strategy also inherently brings inconsistency into the training process since editing results from different iterations mixed inside the dataset. As a result, artifacts appear in the rendering results during the early training stages, as shown in Fig. \ref{fig: artifacts} (a). Even worse, as IP2P trains on a high-quality dataset with no artifacts, these artifacts may cause serious performance inferior to the edits of IP2P, as shown in Fig. \ref{fig: artifacts} (b).

% The methods proposed in this paper, including a dual-field representation, simulated annealing strategy, and CLIP-based consistency indicator, solve these problems effectively and will be introduced in the following section.
