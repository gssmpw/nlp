\begin{abstract}
Recently, denoising diffusion models have achieved promising results in 2D image generation and editing.
Instruct-NeRF2NeRF (IN2N) introduces the success of diffusion into 3D scene editing through an ``Iterative dataset update" (IDU) strategy.
Though achieving fascinating results, IN2N suffers from problems of blurry backgrounds and trapping in local optima.
% Though achieving fascinating results, IN2N generates edits with blurry backgrounds.
The first problem is caused by IN2N's lack of efficient guidance for background maintenance, while the second stems from the interaction between image editing and NeRF training during IDU.
In this work, we introduce \textbf{DualNeRF} to deal with these problems.
We propose a dual-field representation to preserve features of the original scene and utilize them as additional guidance to the model for background maintenance during IDU.
% We introduce a dual-field representation to stabilize the training process and greatly enhance the visual quality of unedited areas, which usually look blurry for IN2N. 
Moreover, a simulated annealing strategy is embedded into IDU to endow our model with the power of addressing local optima issues.
% A CLIP-based consistency indicator is also used to filter out low-quality edits.
A CLIP-based consistency indicator is used to further improve the editing quality by filtering out low-quality edits.
Extensive experiments demonstrate that our method outperforms previous methods both qualitatively and quantitatively.
\end{abstract}