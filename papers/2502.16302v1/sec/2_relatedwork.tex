\section{Related work}
\label{sec: related work}

\subsection{Text-guided Image Editing by Diffusion}
In recent years, diffusion models have become the most popular and powerful 2D image synthesis model due to their impressive generation results \cite{sohl2015deep, ho2020denoising, song2020denoising, song2022diffusion}. Combined with language models, text-guided diffusion models were proposed and achieved promising results according to user-provided captions \cite{nichol2021glide, rombach2022high, saharia2022photorealistic, ramesh2022hierarchical}. Based on these brilliant text-to-image diffusion models, diffusion-based image editing shows significant progress. Some of them finetune a pre-trained latent diffusion model (LDM) before editing \cite{zhang2022sine, valevski2022unitune, kawar2023imagic}. However, these methods consume huge computing power and suffer from low diversity. Sdedit \cite{meng2021sdedit} proposes to edit target images by first adding noise and denoising according to the prompts. In this way, no finetuning is required, but prone to over-edit. Prompt-to-prompt \cite{hertz2022prompt} demonstrates that a more relative editing result to the original image can be obtained by controlling the cross-attention map of U-Net \cite{ronneberger2015u}. Pix2pix-zero \cite{parmar2023zero} achieves similar performance with cross-attention guidance via L2 loss. Text2LIVE \cite{bar2022text2live} generates an edit RGBA layer, namely a color map and an opacity map, which blends into the original image for editing. Diffedit \cite{couairon2022diffedit} uses the difference introduced by conditions to guide localized edits, but can only deal with some relatively easy prompts. Instructpix2pix (IP2P) \cite{brooks2023instructpix2pix} synthesises a huge image-caption-image editing dataset based on GPT \cite{brown2020language}, Stable Diffusion \cite{rombach2022high} and Prompt-to-prompt \cite{hertz2022prompt}. Trained on this dataset, IP2P achieves the SOTA image editing result but still suffers from over-edit and instability. In this work, we use IP2P to edit images of different views and iteratively update the training dataset following \cite{haque2023instruct}. A CLIP-based consistency indicator is proposed to filter out low-quality edits, preventing them from contaminating the dataset.

% Null-text Inversion \cite{mokady2023null}

\subsection{Neural Radiance Field Editing}
% The editing of NeRF has become a significant problem in the field. Many early works pay attention to some specific tasks of NeRF editing. For geometry deformation, previous works \cite{yuan2022nerf, garbin2022voltemorph} build a canonical field along with a deformation field to deform the geometry of the target object. Cages or tetrahedrons can also be used to regularize the deforming process \cite{xu2022deforming, tertikas2023partnerf, peng2022cagenerf}. For texture (or color) editing \cite{liu2021editing, xiang2021neutex, gong2023recolornerf, kuang2023palettenerf}, scene manipulation \cite{niemeyer2021giraffe, xu2023discoscene}, relighting \cite{li2022climatenerf, zhang2021nerfactor, boss2021neural, srinivasan2021nerv} and stylization \cite{zhang2022arf, wang2023nerf}.

The editing of NeRF \cite{mildenhall2021nerf} has become a significant problem in the field. Many early works pay attention to some specific tasks of NeRF editing, including geometry deformation \cite{yuan2022nerf, garbin2022voltemorph, xu2022deforming, tertikas2023partnerf, peng2022cagenerf, jambon2023nerfshop}, texture or color editing \cite{liu2021editing, xiang2021neutex, gong2023recolornerf, kuang2023palettenerf, lee2023ice}, scene manipulation \cite{niemeyer2021giraffe, xu2023discoscene}, relighting \cite{li2022climatenerf, zhang2021nerfactor, boss2021neural, srinivasan2021nerv} and stylization \cite{zhang2022arf, wang2023nerf}. Despite the promising results, most of them can only deal with one or two tasks limited in their paper. Moreover, their editing operations are usually unuser-friendly.

In recent years, many researchers have utilized the powerful 2D priors in the pre-trained vision-language models \cite{radford2021learning, nichol2021glide, rombach2022high} to edit NeRF by text prompts. Clip-nerf \cite{wang2022clip} leverages a CLIP \cite{radford2021learning} image/text encoder to maintain the consistency between the text prompt and rendered results. In a similar way, NeRF-Art \cite{wang2023nerf} also chooses to use CLIP to stylize target NeRF models driven by text prompts. Instruct-NeRF2NeRF (IN2N) \cite{haque2023instruct} proposes to iteratively update the training dataset by IP2P \cite{brooks2023instructpix2pix} along with model training to counter IP2P's multi-view inconsistent editing results. Although IN2N can eventually converge into a well-looking result, it still suffers from problems of blurry backgrounds and local optima. Some followers of IN2N \cite{fang2023text, mirzaei2023watch} still unable to solve both problems. In this work, our method follows the iterative dataset update (IDU) strategy of IN2N, while proposing a novel dual-field network architecture to address the aforementioned problems.
