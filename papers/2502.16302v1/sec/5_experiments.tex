\section{Experiments}

\begin{figure*}
    \centering

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/polarbear1.pdf}
            % \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/polarbear2.pdf}
            % \caption{Instruct-NeRF2NeRF}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/polarbear3.pdf}
            % \caption{DualNeRF}
        \end{minipage}
    \end{subfigure}

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/panda1.pdf}
            % \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/panda2.pdf}
            % \caption{Instruct-NeRF2NeRF}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]
            {fig/results/panda3.pdf}
            % \caption{DualNeRF}
        \end{minipage}
    \end{subfigure}

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/clown1.pdf}
            % \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/clown2.pdf}
            % \caption{Instruct-NeRF2NeRF}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/clown3.pdf}
            % \caption{DualNeRF}
        \end{minipage}
    \end{subfigure}

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/suit_new1.pdf}
            % \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/suit_new2.pdf}
            % \caption{Instruct-NeRF2NeRF}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/suit_new3.pdf}
            % \caption{DualNeRF}
        \end{minipage}
    \end{subfigure}

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/MJ1.pdf}
            % \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/MJ2.pdf}
            % \caption{Instruct-NeRF2NeRF}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/MJ3.pdf}
            % \caption{DualNeRF}
        \end{minipage}
    \end{subfigure}

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/snowed1.pdf}
            \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/snowed2.pdf}
            \caption{Instruct-NeRF2NeRF}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/snowed3.pdf}
            \caption{DualNeRF}
        \end{minipage}
    \end{subfigure}
  
    \caption{\textbf{Qualitative Results.} Comparison between DualNeRF and Instruct-NeRF2NeRF \cite{haque2023instruct} over different scenes with different prompts. Three columns respectively represent the original scene, the editing results of IN2N, and the editing results of DualNeRF. We strongly recommend readers to zoom in for a clearer observation.}
    \label{fig: qualitative results}
\end{figure*}

\subsection{Experimental Setups}
\paragraph{Datasets.}
% We conduct experiments based on scenes from IN2N \cite{haque2023instruct} and InstantNGP \cite{muller2022instant},
We conduct experiments based on scenes from IN2N \cite{haque2023instruct}, which contain $50 \sim 350$ high-quality images in various scenes, usually natural scenery or front views of a person. Following the advice of \cite{haque2023instruct}, images in the dataset are resampled to a resolution of around $512$ to match the best input resolution of IP2P \cite{brooks2023instructpix2pix}. COLMAP \cite{schoenberger2016sfm} is used to extract camera poses from images. The text prompts used in experiments are all ordinary natural languages, such as \textit{``Turn the bear into a panda"}, just like IP2P and IN2N do. 

\paragraph{Evaluation Criteria.}
\label{sec: Evaluation Criteria}
Following \cite{haque2023instruct}, we report the CLIP text-image direction similarity $C_{t2i}$ to measure the alignment between the final renderings with the prompt and CLIP direction consistency $C_{dir}$ to measure the consistency between adjacent renderings in the CLIP space.
Besides, structural similarity index (SSIM) \cite{wang2004image} is also used to measure the similarity between the original images and their edits, indicating the degree of background maintenance to some extent.
% A user study is also conducted to give a more subjective comparison by humans.

\paragraph{Baselines.}
We compare DualNeRF with SOTA 2D and 3D editing methods, including (1) Instruct-Nerf2NeRF \cite{haque2023instruct}, the SOTA 3D scene editing method based on IDU with released code; (2) InstructPix2Pix \cite{brooks2023instructpix2pix}, the underlying text-driven image editing model used in our method; (3) ControlNet \cite{zhang2023adding}, a SOTA diffusion-based image generation model controlled by signals of various modalities.

\begin{figure}
    \centering

    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \caption{Original}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_panda1.pdf}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \caption{ControlNet}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_panda3.pdf}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \caption{IP2P}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_panda2.pdf}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \caption{DualNeRF}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_panda4.pdf}
        \end{minipage}
    \end{subfigure}

    \vspace{2px}

    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            % \caption{Original}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_clown1.pdf}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            % \caption{IP2P}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_clown3.pdf}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            % \caption{ControlNet}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_clown2.pdf}
        \end{minipage}
    \end{subfigure}
    \begin{subfigure}{0.24\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            % \caption{DualNeRF}
            \includegraphics[width=1.0\linewidth]{fig/results/2d_clown4.pdf}
        \end{minipage}
    \end{subfigure}
  
    % \caption{\textbf{Comparison with SOTA 2D Image Editing Methods.} The four columns respectively show the original scene and editing results from different views generated by ControlNet \cite{zhang2023adding}, IP2P \cite{brooks2023instructpix2pix}, and ours. The prompts used in two cases are "Turn the bear into a panda" and "Turn him into a clown" respectively. 2D methods generate inconsistent edits among different views. The editing quality of them is also unstable, resulting in low-quality edits and sometimes with wrong backgrounds.}
    \caption{\textbf{Comparison with SOTA 2D Image Editing Methods.} The four columns respectively show the original scene and editing results from different views generated by ControlNet \cite{zhang2023adding}, IP2P \cite{brooks2023instructpix2pix}, and ours. The prompts used in two cases are ``Turn the bear into a panda" and ``Turn him into a clown" respectively.}
    \label{fig: 2D}
\end{figure}

\subsection{Qualitative Results}
\paragraph{3D Scene Editing.}
The qualitative comparison between the editing results of DualNeRF with IN2N is shown in Fig. \ref{fig: qualitative results}. Both models train for $15k$ iterations for a fair comparison. Details in some results are zoomed in for a clearer observation. As illustrated in Fig. \ref{fig: qualitative results}, IN2N is hard to perform local edits as it cannot maintain non-target areas unaffected while editing the target areas. This results in blurry backgrounds, detail missing, and even artifacts.

Examples of blurry backgrounds can be seen in Fig. \ref{fig: 1-IN2N} and the first row of Fig. \ref{fig: qualitative results}, where blurred textures appear in the background areas of IN2N's edits. As a comparison, DualNeRF generates edits with clearer backgrounds, thanks to the additional guidance provided by our dual-field representation. Examples of detail missing are shown in the third to fifth rows in Fig. \ref{fig: qualitative results}, where details on the clothes, including the clothes texture, trousers pleats, and sweater weaving pattern, are faded away in edits of IN2N. These phenomena stem from IN2N's lack of efficient guidance to maintain details from original scenes. In contrast, DualNeRF finds a better balance between original image restoration and editing modification, preserving much more details than IN2N.
% Moreover, color drift often appears on unedited objects, such as the rock turning blue in the first row and the skin turning red in the fifth row in Fig. \ref{fig: qualitative results}. DualNeRF alleviates these issues successfully.
We also present two failure cases of IN2N in the second and last rows in Fig. \ref{fig: qualitative results}. Surprisingly, these outputs are different from the results displayed in \cite{haque2023instruct} under the default settings provided by their released code. However, DualNeRF generates much better edits under the same settings, demonstrating the superiority of our method.

\paragraph{Compared to 2D Methods.}
We compare DualNeRF with SOTA 2D image editing methods to demonstrate that pure 2D methods cannot edit 3D scenes with multi-view consistency. Fig. \ref{fig: 2D} demonstrates some examples of the comparison between the results of our edits with 2D methods. As we can see, ControlNet generates edits with low visual quality and high inconsistency among different views. Moreover, the background area is totally replaced by ControlNet, indicating that ControlNet cannot be used for local editing. IP2P edits the original scene with more background details preserved but still fails to generate edits with high multi-view consistency. As a comparison, consistent edits with restored backgrounds are generated by our method, thanks to the 3D nature of DualNeRF.

\begin{table}[t]
    \centering
    \rowcolors{1}{white}{gray!20}
    \begin{tabular}{p{70pt}<{\centering}|p{40pt}<{\centering}|p{40pt}<{\centering}|p{40pt}<{\centering}}
    \toprule
    Method & $C_{t2i} \uparrow$ & $C_{dir} \uparrow$ & SSIM $\uparrow$ \\
    \midrule
    per-frame IP2P & $0.2153$ & $0.9435$ & $\boldsymbol{0.8194}$ \\
    IN2N & $0.2170$ & $\boldsymbol{0.9806}$ & $0.7254$ \\
    DualNeRF & $\boldsymbol{0.2190}$ & $0.9777$ & $0.7362$ \\
    \bottomrule
    \end{tabular}
    % \caption{\textbf{Quantitative Evaluation.} We compare our method with baselines quantitatively based on CLIP. $C_{t2i}$ in the second column represents the CLIP text-image direction similarity \cite{haque2023instruct}, which measures the alignment between text prompt and scene edits. $C_{dir}$ in the third column denotes CLIP directional similarity \cite{haque2023instruct}, which indicates the consistency between adjacent views in the CLIP space. SSIM in the fourth column evaluates the edit's degree of restoration to the original image. Baseline methods include per-frame InstructPix2pix \cite{brooks2023instructpix2pix} and Instruct-NeRF2NeRF \cite{haque2023instruct}.}
    \caption{\textbf{Quantitative Evaluation.} We compare our method with baselines quantitatively based on CLIP. $C_{t2i}$ in the second column represents the CLIP text-image direction similarity \cite{haque2023instruct}. $C_{dir}$ in the third column denotes CLIP directional similarity \cite{haque2023instruct}. SSIM in the fourth column evaluates the edit's degree of restoration to the original image. Baseline methods include per-frame InstructPix2pix \cite{brooks2023instructpix2pix} and Instruct-NeRF2NeRF \cite{haque2023instruct}.}
    \label{tab: quantitative results}
\end{table}

\begin{figure*}
    \centering

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/ab_study_1_v1.pdf}
            \caption{Original Scene}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/ab_study_2_v1.pdf}
            \caption{w/o SA and CCI}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/ab_study_3_v1.pdf}
            \caption{w/o CCI}
        \end{minipage}
    \end{subfigure}

    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/ab_study_4_v1.pdf}
            \caption{w/o DF and SA}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{fig/results/ab_study_5_v1.pdf}
            \caption{w/o SA}
        \end{minipage}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \begin{minipage}[t]{1.0\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]
            {fig/results/ab_study_6_v1.pdf}
            \caption{Full model}
        \end{minipage}
    \end{subfigure}
  
    \caption{\textbf{Ablation Study.} Qualitative results of our methods under different settings. Experiments are conducted in the \textit{campsite} scene conditioned on the prompt ``Make it look like it just snowed". DF, SA, and CCI represent dual-field representation, simulated annealing strategy, and CLIP-based consistency indicator respectively.}
    \label{fig: ablation study}
\end{figure*}

\subsection{Quantitative Results}
\label{sec: Quantitative Results}
Quantitative comparisons are also conducted between DualNeRF and baselines, as shown in Tab. \ref{tab: quantitative results}. Three metrics are used to evaluate the performance of edits, including a CLIP text-image direction similarity $C_{t2i}$, a CLIP directional similarity $C_{dir}$, and SSIM \cite{wang2004image}. Experiments are conducted across three scenes, including \textit{face}, \textit{fangzhou}, and \textit{person}, over $10$ edits. More details are provided in the supplementary material. As we can see in Tab. \ref{tab: quantitative results}, comparable performance are shown by DualNeRF and IN2N in the CLIP space. This shows that both methods generate edits with high text-to-image alignment ($C_{t2i}$) and multi-view consistency ($C_{dir}$). As a comparison, per-frame IP2P performs the worst in $C_{dir}$, indicating that 3D scenes are hard to edit solely by 2D methods. Additionally, DualNeRF outperforms IN2N in SSIM, which demonstrates that edits generated by our method present more restored backgrounds.

\begin{table}[t]
    \centering
    \rowcolors{1}{white}{gray!20}
    \begin{tabular}{p{30pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}|p{40pt}<{\centering}|p{40pt}<{\centering}}
    \toprule
    DF & SA & CCI & $C_{t2i} \uparrow$ & $C_{dir} \uparrow$ \\
    \midrule
    \multicolumn{3}{c|}{Original Scene} & $0.0244$ & $\boldsymbol{0.9383}$ \\
    \midrule
    \XSolidBrush & \XSolidBrush & \XSolidBrush & $0.1203$ & $0.9347$ \\
    \Checkmark & \XSolidBrush & \XSolidBrush & $0.1248$ & $0.9371$ \\
    \Checkmark & \Checkmark & \XSolidBrush & $0.1283$ & $0.9362$ \\
    \XSolidBrush & \XSolidBrush & \Checkmark & $0.1287$ & $0.9340$ \\
    \Checkmark & \XSolidBrush & \Checkmark & $0.1339$ & $0.9361$ \\
    \Checkmark & \Checkmark & \Checkmark & $\boldsymbol{0.1625}$ & $0.9352$ \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation Study.} Experiments are conducted under different settings by removing some of our designs. DF, SA, and CCI represent dual-field representation, simulated annealing strategy, and CLIP-based consistency indicator respectively. Note that the row with three forks represents IN2N \cite{haque2023instruct}, while the row with three hooks represents our full model.}
    \label{tab: ablation study}
\end{table}

\subsection{Ablation Study}
The ablation study is also conducted to investigate the efficiency of our different designs. Specifically, we edit the \textit{campsite} scene conditioned on prompt ``Make it look like it just snowed" by DualNeRF under different settings. Qualitative results are shown in Fig. \ref{fig: ablation study}, while quantitative results can be seen in Tab. \ref{tab: ablation study}. As Fig. \ref{fig: ablation study} shows, models trained with our simulated annealing strategy successfully jump out of local optima ((c) and (f)), generating well-edited results compared to models without SA ((b), (d), and (e)). The use of the CLIP-based consistency indicator further improves the visual quality of the edits comparing examples in (c) and (f). The quantitative results in Tab. \ref{tab: ablation study} also confirm these points, as our full model's $C_{t2i}$ stands out as the best.
