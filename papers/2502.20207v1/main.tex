\documentclass[journal]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{amsthm}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage{bm}
\usepackage{tabularx}


\usepackage[]{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicforall}{\textbf{for each}}


\usepackage{subcaption}
\usepackage{afterpage}


\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}




\newcommand{\eqdef}{\stackrel{\textrm{\tiny def}}{=}}
\newcommand{\E}{\mathbb{E}} 
\newcommand{\e}{\mathrm{e}} 
\renewcommand{\P}{\mathrm{Pr}} 
\newcommand{\ed}{\color{red}}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\paren{(}{)}           % (parentheses)
\DeclarePairedDelimiter\ang{\langle}{\rangle} % <angle brackets>
\DeclarePairedDelimiter\abs{\lvert}{\rvert}   % |absolute value|
\DeclarePairedDelimiter\norm{\lVert}{\rVert}  % ||norm||
\DeclarePairedDelimiter\bkt{[}{]}             % [brackets]
\DeclarePairedDelimiter\set{\{}{\}}           % {braces}



\begin{document}

\title{Differentially-private frugal estimation of quantiles}

\author{Massimo Cafaro, \IEEEmembership{Senior Member, IEEE}, Angelo Coluccia, \IEEEmembership{Senior Member, IEEE}, Italo Epicoco and Marco Pulimeno



\thanks{The authors are with the University of Salento, Lecce, Italy (e-mail: \{ massimo.cafaro, angelo.coluccia, italo.epicoco, marco.pulimeno,\}@unisalento.it)}
}

%\markboth{To be submitted to somewhere --- draft Angelo Coluccia}
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\maketitle

\begin{abstract}
Fast and accurate estimation of quantiles on data streams coming from communication networks, Internet
of Things (IoT), and alike, is at the heart of important data processing applications including statistical analysis, latency monitoring, query optimization for parallel database management systems, and more. Indeed, quantiles are more robust indicators for the underlying distribution, compared to moment-based indicators such as mean and variance. The streaming setting additionally constrains accurate tracking of quantiles, as stream items may arrive at a very high rate and must be processed as quickly as possible and discarded, being their storage usually unfeasible. Since an exact solution is only possible when data are fully stored, the goal in practical contexts is to provide an approximate solution with a provably guaranteed bound on the approximation error committed, while using a minimal amount of space. At the same time, with the increasing amount of personal and sensitive information exchanged, it is essential to design privacy protection techniques to ensure confidentiality and data integrity. In this paper we present the following differentially private streaming algorithms for frugal estimation of a quantile: \textsc{DP-Frugal-1U-L}, \textsc{DP-Frugal-1U-G}, \textsc{DP-Frugal-1U-$\rho$} and \textsc{DP-Frugal-2U-SA}. Frugality refers to the ability of the algorithms to provide a good approximation to the sought quantile using a modest amount of space, either one or two units of memory. We provide a theoretical analysis and extensive experimental results, in which we also compare \textsc{DP-Frugal-1U-L} with \textsc{LDPQ}, a recent state of the art algorithm, and show that \textsc{DP-Frugal-1U-L} outperforms \textsc{LDPQ} in both accuracy and speed.
\end{abstract}

\begin{IEEEkeywords}
Quantiles, Streaming, Differential Privacy, Frugal Algorithms.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}
\label{introduction}

  

Data processing of network-originated streams, coming from wireless sensors, telecommunications networks, Internet of Things (IoT), cyber-physical systems, and many other sources, is ubiquitously present in contemporary ICT applications.
With the increasing amount of personal and sensitive information exchanged, it is essential to design privacy protection techniques to ensure confidentiality and data integrity. At the same time, there is a trend in trying to reduce the computational complexity and memory requirements of data processing algorithms, with the goal of lowering costs and environmental impact. Such issues are expected to play an even more important role in  upcoming 6G wireless networks, with a large amount of data  generated and processed at the edge of the network in resource-constrained devices, and potentially exposed to security and privacy attacks \cite{6G}. 
For such reasons, recent work is actively working to obtain both i) privacy-preserving and/or ii) reduced-complexity versions of classical algorithms. 

As to the first requirement, differential privacy (DP) has attracted significant interest among various research communities, including computer science, communications, data and signal processing \cite{IoT21,DPtutorial_ACM,DP_SPM}.
Among the algorithms that have been recently revisited under the DP paradigm we can cite principal component analysis (PCA) \cite{PCA}, federated learning and microaggragation for IoT \cite{IoT22,IoT23}, and various algorithms for the estimation of mean values or other ensemble statistics under different settings, e.g., \cite{NEURIPS2021,mean,Scaglione}. The interested reader may refer to one of the many surveys available; we recall here \cite{survey}, which is a comprehensive and recent survey discussing differentially private algorithms for both data publishing and data analysis.  
%Case studies on the real-world advanced metering infrastructure measurements of household power consumption \cite{Scaglione}


As to the second requirement, while the processing of clipped or quantized data is a classical topic \cite{clipped_noise,quantization}, recent research is focusing on extreme settings where only a very limited amount of information is retained. For instance, since as known the complexity of analog-to-digital conversion (ADC) grows exponentially with the number of bits, and the power dissipated by the ADC circuit scales approximately at the same pace \cite{ADC_JSAC}, it is very convenient to adopt coarse quantization or even a single comparator that forwards only the sign of the  signal, discarding all the remaining information. While this brings enormous advantages, the significant information loss requires more advanced algorithms for the processing \cite{Eldar1, Eldar2}.
The same principle has been applied to the processing downstream the ADC, specifically through the introduction of binning as well as memory-constrained algorithms. In particular, a recent trend is the adoption of computational approaches that require only a few or a single memory variables. This strategy,  termed ``frugal'', allows the processing of large amount of data in resource-constrained devices, and is therefore a very timely research direction.

The streaming setting adds additional constrains, since stream items may arrive at a very high rate and must be processed as quickly as possible and discarded \cite{IoT24}, owing to the fact that storing them is usually unfeasible given that the input stream may be potentially infinite. An exact solution is only possible if all of the stream items are stored, so that, since streaming algorithms strive to use a minimal amount of space, the goal is to provide an approximate solution with a provably guaranteed bound on the approximation error committed.

Inspired by the  above, this work aims at obtaining DP quantile estimation algorithms with very low memory requirement, so fulfilling both  i) and ii) at the same time.
As known, quantiles (e.g., median, quartiles, percentiles, etc.) are a key tool in robust statistics \cite{huber2009}, whose aim is to obtain ensemble information from a set of data, namely an estimate of a parameter of interest, while limiting the impact of outliers or heavy tails. %Quantiles are simple instances of L-estimators, a class of robust statistics that includes also the winsorizing and trimming approaches \cite{dixon60}.
Indeed, estimating an unknown parameter from a set of random variables, which may also arise from observations at different sources \cite{tsai2011cooperative,nevat2013random} is typically performed by computing some average  on a data stream. %While this may have some optimality guarantees in the  Gaussian  case \cite{kay1993fundamentals}, for more general non-Gaussian distributions, possibly with  heterogeneous (non-uniform) variances \cite{xiao2005decentralized,li2009distributed}... Applications  include direction of arrival estimation \cite{Pesavento2001,Liao2016}, spectrum sensing for cognitive radio \cite{Font-Segura2010}, time-delay multistatic target localization  \cite{Panwar2022}
However, data heterogeneity (including non-independent and identically distributed distributions \cite{IoTNIID}) may arise due to local causes~\cite{fijalkow2016parameter,chen2019resilient}, such as the presence of outliers~\cite{papageorgiou2015robust},  
 heavy-tails, random effects with heterogeneous variance and/or variable sample size, for which robust tools not requiring knowledge of the data distribution are needed \cite[and references therein]{Grassi2023}. Heavy tails are found in many types of data, including financial \cite{muller1998heavy}, natural and computer-originated data \cite{machado2015review}, a prominent example of the latter being the   Internet \cite{Internet3}: in such a context, the use of quantiles/percentiles is indeed a popular practical tool to cope with the wild variability of IP traffic \cite{Internet3,willinger2004more}, e.g.  
 for robust estimation of network-wide key   performance  indicators (KPIs) such as one-way delays \cite{OWD} and round-trip times \cite{RTT}. %\cite{RTT_TCP}, 
 %and others \cite{coluccia2014robust,coluccia2016bayesian}.%\cite{coluccia2013robust,coluccia2014distributed,}
 Besides these contexts, fast and accurate tracking of quantiles in a streaming setting is of utmost importance also in database query optimizers, data splitting for parallel computation in database management systems, etc. 

While recent work has provided DP algorithms for mean values  \cite{NEURIPS2021,mean}, to the best of our knowledge no DP algorithm is available in the literature for quantile estimation via frugal computation. We base our work on the \textsc{Frugal-1U} and \textsc{Frugal-2U} algorithms \cite{frugal} (discussed respectively in Section \ref{frugal-1u-alg} and \ref{frugal2-alg}), and provide the following original contributions, without assuming knowledge of the data distribution:

\begin{itemize}
    \item we analyze the \textsc{Frugal-1U} algorithm and prove that its global sensitivity is bounded and equal to 2; next, we design three DP versions of the algorithm based respectively on the Laplace mechanism, the Gaussian mechanism and on $\rho$ zero-concentrated DP;
    \item we analyze the \textsc{Frugal-2U} algorithm, prove that adversarial sequences may lead to unbounded sensitivity, analyze the spontaneous statistical occurrence of one of those sequences for general datasets, and design a DP version of the algorithm using the ``Sample and Aggregate" framework;
    \item we validate the theoretical results through extensive simulations.
\end{itemize}

The rest of this paper is organized as follows. Section \ref{notation} provides necessary definitions and notation used throughout the manuscript. Section \ref{frugal-1u-alg} introduces the \textsc{Frugal-1U} algorithm whilst Section \ref{dp-frugal-1u} presents our analysis and three corresponding DP algorithms. Section \ref{frugal2-alg} introduces the \textsc{Frugal-2U} algorithm whilst Section \ref{dp-frugal-2u} presents our analysis and a corresponding DP algorithm. Section \ref{sec:analysis} analyzes the probability of spontaneous occurrence of adversarial sequences.
After reviewing the related work in Section \ref{related}, we present extensive experimental results in section \ref{results} and  conclusions in Section \ref{conclusions}.


\section{Preliminary Definitions and Notation}
\label{notation}
In this Section, we briefly recall the definitions and notations that shall be used throughout this paper. We begin by giving a formal definition of quantiles.

\begin{defn} (Lower and upper $q$-quantile) Given a multi-set $A$ of size $n$ over $\mathbb{R}$, let $R(x)$ be the rank of the element $x$, i.e., the number of elements in $A$ smaller than or equal to $x$. Then, the lower (respectively upper) $q$-quantile  item $x_q \in A$ is the item $x$ whose rank $R(x)$ in the sorted multi-set $A$ is $\floor{1+q(n-1)}$ (respectively $\ceil{1+q(n-1)}$) for  $0 \leq q \leq 1$. 
\end{defn}

%\begin{defn} ($q$-quantile with regard to the cumulative distribution function) The cumulative distribution function (CDF) $F(\cdot)$ summarizes the distribution of a totally ordered multi-set $A$ consisting of $n$ elements: given an arbitrary value $x$, $F(x)$ is defined as the number of values less than $x$. The $q$-quantile item $x_q \in A$ is the inverse of the function $F(x)$, i.e., the element $x$ is such that $Q(F(x)/n) = x$. 	
%\end{defn}

The accuracy related to the estimation of a quantile can be defined either as rank or relative accuracy. In this paper, we deal with algorithms that provide rank accuracy, which is defined as follows.

\begin{defn} (Rank accuracy) For all items $v$ and a given tolerance $\epsilon$, return an estimated rank $\tilde{R}$ such that $\abs{\tilde{R}(v) - R(v)} \leq \epsilon n$.
\end{defn}

Next, we introduce the main concepts underlying DP. Actually, two definitions are possible, as follows.

\begin{defn} \label{udf} (Unbounded differential privacy, also known as the add-remove model \cite{10.1007/11681878_14} \cite{TCS-042}) 
Two datasets $x$ and $x^{\prime}$ are considered neighbors if $x^{\prime}$ can be obtained from $x$ by adding or removing one row. Under unbounded DP, the sizes of $x$ and $x^{\prime}$  are different (by one row): $\left|x \backslash x^{\prime}\right|+\left|x^{\prime} \backslash x\right|=1$. 
\end{defn}

\begin{defn} \label{bdp} (Bounded differential privacy, also known as the swap or the update/replace model \cite{10.1007/11681878_14} \cite{Vadhan2017}) 
Two datasets $x$ and $x^{\prime}$ are considered neighbors if $x^{\prime}$ can be obtained from $x$ by changing one row. Under bounded DP, the sizes of $x$ and $x^{\prime}$ are equal: $\left|x \backslash x^{\prime}\right|=1 \text { and }\left|x^{\prime} \backslash x\right|=1$.
\end{defn}

In this paper, we adopt bounded DP. Next, we define $\epsilon$-DP. 

\begin{defn} \label{pdp} ($\epsilon$-differential privacy) A function which satisfies DP is called a mechanism; we say that a mechanism $\mathcal{F}$ satisfies pure DP if for all neighboring datasets  $x$ and $x^{\prime}$ and all possible sets of outputs $\mathcal{S}$, it holds that $\frac{\operatorname{Pr}[\mathcal{F}(x) \in \mathcal{S}]}{\operatorname{Pr}\left[\mathcal{F}\left(x^{\prime}\right) \in \mathcal{S} \right]} \leq e^\epsilon$. The $\epsilon$ parameter in the definition is called the privacy parameter or privacy budget.
\end{defn}

The $\epsilon$ parameter is strictly related to the desired amount of privacy. In practice, there is trade-off going on, since smaller values of this parameter provide higher privacy but at the cost of less \emph{utility} and vice-versa. In this context, utility refers to the possibility of using the obtained results for further investigations, namely statistical analyses. Therefore, the trade-off may be understood considering that setting $\epsilon$ to a small value require the mechanism $\mathcal{F}$ to provide very similar outputs when instantiated on similar inputs (so, higher privacy, obtained by injecting more noise which in turn undermines utility); on the contrary, a large value provides less similarity of the outputs (so, less privacy but increased utility). Besides pure DP, a different notion, called approximate (or, alternatively, $\epsilon, \delta$) DP, is also available. 

\begin{defn} \label{adp} ($(\epsilon, \delta)$-differential privacy) A mechanism $\mathcal{F}$ satisfies ($\epsilon, \delta$)-DP if for all neighboring datasets  $x$ and $x^{\prime}$ and all possible sets of outputs $\mathcal{S}$, it holds that $\operatorname{Pr}[\mathcal{F}(x) \in \mathcal{S}] \leq e^\epsilon \operatorname{Pr}\left[\mathcal{F}\left(x^{\prime}\right) \in \mathcal{S} \right]+\delta$, where the privacy parameter $0 \leq \delta < 1$ represents a failure probability.
\end{defn}

The definition implies that (i) with probability $1-\delta$ it holds that $\frac{\operatorname{Pr}[\mathcal{F}(x) \in \mathcal{S}]}{\operatorname{Pr}\left[\mathcal{F}\left(x^{\prime}\right) \in \mathcal{S} \right]} \leq e^\epsilon$ and (ii) with probability $\delta$ no guarantee holds. As a consequence, $\delta$ is required to be very small. 


In order to define a mechanism, we need to introduce the notion of sensitivity. In practice, the sensitivity of a function reflects the amount the function's output will change when its input changes. Formally, given the universe of datasets, denoted by $\mathcal{D}$, the sensitivity  of a function $f$, called \emph{global sensitivity}, is defined as follows.

\begin{defn} \label{sensitivity} (Global sensitivity) Given a function $f: \mathcal{D} \rightarrow \mathbb{R}$ mapping a dataset in $\mathcal{D}$ to a real number, the global sensitivity of $f$ is $G S(f)=\max _{x, x^{\prime}: d\left(x, x^{\prime}\right)\leq 1}\left|f(x)-f\left(x^{\prime}\right)\right|$, where $d\left(x, x^{\prime}\right)$ represents the distance between two datasets  $x$, $x^{\prime}$.
\end{defn}

We now define two mechanisms, respectively the Laplace and the Gaussian mechanism. The former must be used with pure DP, the latter with approximate DP. 

\begin{defn} \label{laplace} (Laplace mechanism) Given a function $f:~\mathcal{D}~\rightarrow~\mathbb{R}$ mapping a dataset in $\mathcal{D}$ to a real number, $\mathcal{F}(x)=f(x)+\operatorname{Lap}\left(\frac{s}{\epsilon}\right)$ satisfies $\epsilon$-DP. $Lap(S)$ denotes sampling from the Laplace distribution with center 0 and scale $S$, whilst $s$ is the sensitivity of $f$.
\end{defn}

\begin{defn} \label{gaussian} (Gaussian mechanism) Given a function $f:~\mathcal{D}~\rightarrow~\mathbb{R}$ mapping a dataset in $\mathcal{D}$ to a real number, $\mathcal{F}(x)=f(x)+\mathcal{N}\left(\sigma^2\right)$ satisfies $(\epsilon, \delta)$-DP, where $\sigma^2=\frac{2 s^2 \ln (1.25 / \delta)}{\epsilon^2}$ and $s$ is the sensitivity of $f$. $\mathcal{N}\left(\sigma^2\right)$ denotes sampling from the Gaussian (normal) distribution with center 0 and variance $\sigma^2$. 
\end{defn}

The Gaussian mechanism also satifies a stronger notion of privacy, known as $\rho$ zero-concentrated differential privacy ($\rho$-zCDP); its definition uses a single privacy parameter $\rho$, and lies between pure and approximate DP. Moreover, $\rho$-zCDP has been shown to be equivalent (i.e., it can be translated) to standard notions of privacy.

\begin{defn} \label{zCDP} ($\rho$-zCDP) A mechanism $\mathcal{F}$ satisfies zero-concentrated DP if for all neighboring datasets  $x$ and $x^{\prime}$ and all $\alpha \in (1, \infty)$, it holds that $D_\alpha\left(\mathcal{F}(x) \| \mathcal{F}\left(x^{\prime}\right)\right) \leq \rho \alpha$, where $D_\alpha(P \| Q)=\frac{1}{\alpha-1} \ln E_{x \sim Q}\left(\frac{P(x)}{Q(x)}\right)^\alpha$ is the Rényi divergence.
\end{defn}

It can be shown that $\rho$-zCDP can be converted to $(\epsilon, \delta)$-DP as follows. If the mechanism $\mathcal{F}$ satisfies $\rho$-zCDP, then for $\delta > 0$ it also satisfies $(\epsilon, \delta)$-differential privacy for $\epsilon=\rho+2 \sqrt{\rho \log (1 / \delta)}$. Moreover the Gaussian mechanism can be adapted to work with $\rho$-zCDP as follows.

\begin{defn} \label{rzCDP-gaussian} ($\rho$-zCDP Gaussian mechanism) Given a function $f:~\mathcal{D}~\rightarrow~\mathbb{R}$ mapping a dataset in $\mathcal{D}$ to a real number, $\mathcal{F}(x)=f(x)+\mathcal{N}\left(\sigma^2\right) \text { where } \sigma^2=\frac{s^2}{2 \rho}$ satisfies $\rho$-zCDP, where $s$ is the sensitivity of $f$.
\end{defn}

We briefly introduce the concept of utility, which quantifies how much a DP result is useful for a subsequent data analysis. Therefore, the analysis to be performed plays a key role here, since DP results affected by a significant error may or may not be useful to the analyst. One way to overcame the dependence from the analysis is the use of the related concept of accuracy, which is the distance between the true value computed without DP and the DP released value. Therefore, accuracy is often used in place of utility, because more accurate results are generally more useful for an analysis. The so-called $(\alpha, \beta)$-accuracy framework \cite{DBS-066} can be used to measure accuracy. Here, $\alpha$ represents an upper bound on the absolute error committed, whilst $\beta$ is the probability to violate this bound.

\begin{defn} \label{accuracy} ($(\alpha, \beta)$-accuracy) Given a function $f: \mathcal{D} \rightarrow \mathbb{R}$ mapping a dataset $x \in \mathcal{D}$ to a real number, and a DP mechanism $\mathcal{M}_f: \mathcal{D} \rightarrow \mathbb{R}$, $\mathcal{M}_f$ is $(\alpha, \beta)$-accurate if $\operatorname{Pr}\left[\left\|f(x)-\mathcal{M}_{f(x)}\right\|_{\infty} \geq \alpha\right] \leq \beta$.	
\end{defn}

It can be shown \cite{DBS-066}, starting from the Cumulative Distribution Function for the Laplace distribution $\operatorname{Lap}(0,b)$, that the Laplace mechanism is $(\alpha, \beta)$-accurate with
\begin{equation}
\label{alpha}
\alpha=\ln \left(\frac{1}{\beta}\right) \cdot\left(\frac{s}{\epsilon}\right).
\end{equation}

Regarding the Gaussian and the $\rho$-zCDP mechanisms, we did not find in the literature a corresponding derivation for the $\alpha$ value; as an additional contribution, here we derive their analytical form. We start by considering the Cumulative Distribution Function for the normal distribution $\mathcal{N}\left(0, \sigma\right)$, which is $\frac{1}{2}\left[\operatorname{erfc}\left(-\frac{x}{\sigma \sqrt{2}}\right)\right]$. The probability $\operatorname{Pr}\left[ X > x \right]$ is $1 - \frac{1}{2}\left[\operatorname{erfc}\left(-\frac{x}{\sigma \sqrt{2}}\right)\right]$, so that, substituting $x = t \sigma$, we get:
\begin{equation}
\operatorname{Pr}\left[ X > x \right] = 1-\frac{1}{2} \operatorname{erfc}\left[-\frac{t}{\sqrt{2}}\right].
\end{equation}

Therefore, we need to solve, taking into account that $0 < \beta < 1$, the following equation, with regard to $t$:
\begin{equation}
1-\frac{1}{2} \operatorname{erfc}\left[-\frac{t}{\sqrt{2}}\right] \leq \beta
\end{equation}
obtaining 
\begin{equation}
	t \geq -\sqrt{2} \ \operatorname{erfc}^{-1}(2 (1 - \beta)).
\end{equation}
\noindent It follows that the Gaussian mechanism is $(\alpha, \beta)$-accurate with
\begin{equation}
\label{alpha-gauss}
\alpha = (-\sqrt{2} \ \operatorname{erfc}^{-1}(2 (1 - \beta))) \sqrt{\frac{2 s^2 \ln \left(\frac{1.25}{\delta}\right)}{\epsilon^2}}.
\end{equation}

Reasoning as before, we can also derive that the $\rho$-zCDP mechanism is $(\alpha, \beta)$-accurate with
\begin{equation}
\label{alpha-rho}
\alpha = (-\sqrt{2} \ \operatorname{erfc}^{-1}(2 (1 - \beta))) \sqrt{\frac{s^2}{2 \rho}}.
\end{equation}





Next, we introduce the \textsc{Frugal-1U} algorithm.

\section{The \textsc{Frugal-1U} Algorithm}
\label{frugal-1u-alg}
Among the many algorithms that have been designed for tracking quantiles in a streaming setting, \textsc{Frugal} \cite{frugal} besides being fast and accurate, also restricts by design the amount of memory that can be used. It is well-known that in the streaming setting the main goal is to deliver a high-quality approximation of the result (this may provide either an additive or a multiplicative guarantee) by using the lowest possible amount of space. In practice, there is a tradeoff between the amount of space used by an algorithm and the corresponding accuracy that can be achieved. Surprisingly, \textsc{Frugal} only requires one unit of memory to track a quantile. The authors of \textsc{Frugal} have also designed a variant of the algorithm that uses two units of memory. In this Section, we introduce the one unit of memory version, which is called \textsc{Frugal-1U}. Algorithm \ref{Frugal-1U} provides the pseudo-code for \textsc{Frugal-1U}.


\begin{algorithm}
	\caption{Frugal-1U}
		\label{Frugal-1U}
	 \begin{algorithmic}[1]
		\Require{Data stream $S$, quantile $q$, one unit of memory $\tilde{m}$}
		\Ensure{estimated quantile value $\tilde{m}$}
		\State $\tilde{m} = 0$
		\ForAll {$s_i \in S$}
			\State $rand = random(0,1)$
			\If {$s_i > \tilde{m}$ and $rand > 1-q$}
				\State $\tilde{m} = \tilde{m} + 1$
			\ElsIf {$s_i < \tilde{m}$ and $rand > q$}				\State $\tilde{m} = \tilde{m} - 1$
			\EndIf
		\EndFor
		\State \Return $\tilde{m}$
	\end{algorithmic}
\end{algorithm}

The algorithm works as follows. First, $\tilde{m}$ is initialized to zero (however, note that it can be alternatively initialized to the value of the first stream item, in order to increase the speed of convergence of the estimate towards the value of the true quantile). This variable will be dynamically updated each time a new item $s_i$ arrives from the input stream $S$, and its value represents the estimate of the quantile $q$ being tracked. The update is quite simple, since it only requires $\tilde{m}$ to be increased or decreased by one. Specifically, a random number $0 < rand < 1$ is generated by using a pseudo-random number generator (the call $random(0,1)$ in the pseudo-code) and if the incoming stream item is greater than the estimate $\tilde{m}$ and $rand > 1-q$, then the estimate $\tilde{m}$ is increased, otherwise if the incoming stream item is smaller than the estimate $\tilde{m}$ and $rand > q$, then the estimate $\tilde{m}$ is decreased.  Obviously, the algorithm is really fast and can process an incoming item in worst-case $O(1)$ time. Therefore, a stream of length $n$ can be processed in worst-case $O(n)$ time and $O(1)$ space. 

Despite its simplicity, the algorithm provides good accuracy, as shown by the authors. The proof is challenging since the algorithm's analysis is quite involved. The complexity in the worst case is $O(n)$, since $n$ items are processed in worst case $O(1)$ time. 

Finally, the algorithm has been designed to deal with an input stream consisting of integer values distributed over the domain $[N] = \{1,2,3, \ldots, N\}$. This is not a limitation though, owing to the fact that one can process a stream of real values as follows: fix a desired precision, say three decimal digits, then each incoming stream item with real value can be converted to an integer by multiplying it by $10^3$ and then truncating the result by taking the floor. If the maximum number of digits following the decimal point is known in advance, truncation may be avoided altogether: letting $m$ by the maximum number of digits following the decimal point, it suffices to multiply by $10^m$. Obviously, the estimated quantile may be converted back to a real number dividing the result by the fixed precision selected or by $10^m$.


\section{Differentially-Private \textsc{Frugal-1U}}
\label{dp-frugal-1u}
In this Section, we analyze the \textsc{Frugal-1U} algorithm and design  DP variants of it. As shown in Section \ref{frugal-1u-alg}, the algorithm is quite simple. In order to estimate a quantile $q$, the current estimate $\tilde{m}$ is either incremented or decremented by one based on the value of the incoming stream item $s_i$. The increments are applied with probability $q$ and the decrements with probability $1-q$. 

Our DP versions of the algorithm are based on the definition of bounded DP (see Definition \ref{bdp}), in which two datasets $x$ and $x^\prime$ are considered neighbors if $x^\prime$ can be obtained from $x$ by changing one row. Owing to our choice, we need to analyze the impact of changing one incoming stream item with a different one on the quantile estimate $\tilde{m}$.

\begin{lemma}
\label{frugal-1u-sensitivity}
Under bounded DP, the global sensitivity of the \textsc{Frugal-1U} algorithm is 2.
\end{lemma}

\begin{proof}
 Let $s_i$ be the item to be changed, and $s_j \neq s_i$ the item replacing $s_i$. There are a few symmetric cases to consider. Let $s_i$ be the $i$-th stream item, so that the length of the stream $S$ is equal to $i-1$ before the arrival of $s_i$ and equal to $i$ immediately after. Moreover, denote by $\tilde{m}_{i-1}$ the estimate of the quantile $q$ before the arrival of $s_i$ and by $\tilde{m}_i$ after seeing the item $s_i$. Suppose that the arrival of $s_i$ causes $\tilde{m}_i$ to increase by one with regard to $\tilde{m}_{i-1}$, i.e., $\tilde{m}_i = \tilde{m}_{i-1} + 1$. Substituting $s_i$ with $s_j$ therefore can lead to the following cases: either $\tilde{m}_i = \tilde{m}_{i-1} - 1$ or $\tilde{m}_i = \tilde{m}_{i-1} + 1$. Therefore, the estimate is unchanged or it is increased by 2. Similarly, assuming that the arrival of $s_i$ causes $\tilde{m}_i$ to decrease by one with regard to $\tilde{m}_{i-1}$, i.e., $\tilde{m}_i = \tilde{m}_{i-1} - 1$, then there are, symmetrically, the following cases: either $\tilde{m}_i = \tilde{m}_{i-1} + 1$ or $\tilde{m}_i = \tilde{m}_{i-1} - 1$. Therefore, the estimate is unchanged or is decremented by 2.   It follows that the global sensitivity of the algorithm is $\max _{x, x^{\prime}: d\left(x, x^{\prime}\right)\leq 1}\left|f(x)-f\left(x^{\prime}\right)\right| = 2$.
\end{proof}

Since the global sensitivity is 2, \textsc{DP-Frugal-1U-L}, a pure DP (see Definition \ref{pdp}) variant of the algorithm can be obtained by using the Laplace mechanism. We are now in the position to state the following theorem.

\begin{theorem}
\textsc{Frugal-1U} can be made $\epsilon$-DP by adding to the quantile estimate returned by the algorithm noise sampled from a Laplace distribution  with center 0 and scale $S$ as follows: $\tilde{m} = \tilde{m} + Lap(\frac{2}{\epsilon})$. 
\end{theorem}
 
\begin{proof}
It follows straight from Lemma \ref{frugal-1u-sensitivity} and Definition \ref{laplace}.
\end{proof}

Next, we design \textsc{DP-Frugal-1U-G}, a $(\epsilon, \delta)$-DP (see Definition \ref{adp}) version of the algorithm, by using the Gaussian mechanism.


\begin{theorem}
\textsc{Frugal-1U} can be made $(\epsilon, \delta)$-DP by adding to the quantile estimate returned by the algorithm noise sampled from a Gaussian distribution as follows: $\mathcal{F}(x)=f(x)+\mathcal{N}\left(\sigma^2\right)$ where $\sigma^2=\frac{8 \ln (1.25 / \delta)}{\epsilon^2}$. %$\mathcal{N}\left(\sigma^2\right)$ denotes sampling from the Gaussian (normal) distribution with center 0 and variance $\sigma^2$. 
\end{theorem}

\begin{proof}
It follows straight from Lemma \ref{frugal-1u-sensitivity} and Definition \ref{gaussian}.
\end{proof}

Finally, we design \textsc{DP-Frugal-1U-$\rho$}, a $\rho$-zCDF version of the algorithm.


\begin{theorem}
\textsc{Frugal-1U} can be made $\rho$-zCDF by adding to the quantile estimate returned by the algorithm noise sampled from a Gaussian distribution as follows: $\mathcal{F}(x)=f(x)+\mathcal{N}\left(\sigma^2\right) \text { where } \sigma^2=\frac{2}{\rho}$. %$\mathcal{N}\left(\sigma^2\right)$ denotes sampling from the Gaussian (normal) distribution with center 0 and variance $\sigma^2$. 
\end{theorem}

\begin{proof}
It follows straight from Lemma \ref{frugal-1u-sensitivity} and Definition \ref{rzCDP-gaussian}.
\end{proof}

Finally, we remark here that, contrary to many DP algorithms that initialize a data structure or a sketch using suitable noise, it is not possile to initialize the quantile estimate of \textsc{Frugal-1U} using the noise required by one of the proposed mechanisms. The reason is two-fold. First, the algorithm processes integer items, so that its initial estimate must be an integer as well whilst, in general, the noise is a floating point value. But, even assuming that we initialize the estimate to an integer value related to the noise (perhaps taking its floor or the ceiling), this will not help in any way since, by design, the algorithm adapts dynamically to the observed input items and converges to the estimated quantile. Therefore, the second reason is that the noise added will be silently discarded by the algorithm when converging to the quantile estimate. As a consequence, the noise must be added only after the algorithm termination to the returned estimated quantile. This remark obviously applies to \textsc{Frugal-2U} as well.


\section{The \textsc{Frugal-2U} Algorithm}
\label{frugal2-alg}
We now introduce \textsc{Frugal-2U}. This algorithm is similar in spirit to \textsc{Frugal-1U} but strives to provide a better quantile estimate using just two units of memory for the variables $\tilde{m}$ (the estimate) and $step$, which represents the update size. Note that the variable $sign$ can be represented using just one bit, and is used to determine the increment or decrement direction to be applied to the estimate.

The $step$ size dynamically increases or decreases depending on the incoming stream items' values. Also, note that the update is determined by a function $f(x)$. The update works as follows: if the next item is on the same side of the current estimate, then $step$ is increased, otherwise it is decreased. The magnitude of the increase and decrease may fluctuate until the estimate reaches the proximity of the true quantile; when this happens, increase and decrease of the $step$ variable happens with extremely small values.  

The algorithm must balance the convergence speed on the one side and the estimation stability on the other. Since this is strictly related to how the $f(x)$ function is actually defined, in order to prevent large oscillations the authors set $f(x) = 1$ and we will stick to this definition of the function as well in the analysis. 

As shown in Algorithm \ref{Frugal-2U}, the key idea is to trigger an update of the estimate only when needed. There are two cases to consider: the arrival of stream items larger or smaller than the current estimate. This two cases are handled symmetrically, so that we only discuss here the first one. In particular, in this case an update is needed when seeing a large stream item's value. Note that the estimation is updated by at least 1 and that $step$ comes into play only if its value is positive. The authors explain this as follows:
\begin{quote}
``The reason is that when algorithm estimation is close to true quantile, \textsc{Frugal-2U} updates are likely to be triggered by larger and smaller (than estimation) stream items with largely equal chances. Therefore step is decreased to a small negative value and it serves as a buffer for value bursts (e.g., a short series of very large values) to stabilize estimations. Lines 8-11 are to ensure estimation do not go beyond empirical value domain when step gets increased to very large value. At the end of the algorithm, we reset step if its value is larger than 1 and two consecutive updates are not in the same direction. This is to prevent large estimate oscillations if step gets accumulated to a large value.''    
\end{quote}


\begin{algorithm}
	\caption{Frugal-2U}
		\label{Frugal-2U}
	 \begin{algorithmic}[1]
		\Require{Data stream $S$, quantile $q$, one unit of memory $\tilde{m}$, one unit of memory $step$, a bit $sign$}
		\Ensure{estimated quantile value $\tilde{m}$}
	    \State $\tilde{m} = 0$, $step = 1$, $sign = 1$
		\ForAll {$s_i \in S$}
			\State $rand = random(0,1)$
			\If {$s_i > \tilde{m}$ and $rand > 1-q$}
				\State $step \ += (sign > 0) \ ? \ f(step) : -f(step)$ 
				\State $\tilde{m} \ += (step > 0) \ ? \ \lceil step \rceil : 1$
				\State $sign = 1$
				\If {$\tilde{m} > s_i$}
					\State $step \ += s_i - \tilde{m}$ 
					\State $\tilde{m} = s_i$
				\EndIf
		   \ElsIf {$s_i < \tilde{m}$ and $rand > q$}
					\State $step \ += (sign < 0) \ ? \ f(step) : -f(step)$ 
					\State $\tilde{m} \ -= (step > 0) \ ? \ \lceil step \rceil : 1$
					\State $sign = -1$
					\If {$\tilde{m} < s_i$}
						\State $step \ += \tilde{m} - s_i$ 
						\State $\tilde{m} = s_i$
					\EndIf
			\EndIf
		\If {$(\tilde{m} - s_i) * sign < 0 \land step > 1$}	
			\State step = 1
		\EndIf
		\EndFor
		\State \Return $\tilde{m}$
	\end{algorithmic}
\end{algorithm}

\section{Differentially-Private \textsc{Frugal-2U}}
\label{dp-frugal-2u}
Here we deal with the analysis of \textsc{Frugal-2U}. Recall from the previous section that we assume $f(x) = 1$ and that updates to the current estimate depend on the value assumed by the $step$ variable. Even though it may seem that, apparently, the maximum value assumed by $step$ during the execution of the algorithm is bounded (and limited to a small value), this is not true. Despite the fact that the value of $step$ is reset to 1 at the end of each iteration, this does not prevent $step$ from increasing without bound. The next lemma shows this.

\begin{lemma}
\label{unbounded-step}
The value of the variable $step$ in Algorithm \ref{Frugal-2U} can increase without bound.
\end{lemma}

\begin{proof}
An adversarial effect can be obtained by inserting in an input stream any sequence of values $s$ that begins when $sign=1$ and is so defined: 
\begin{equation}
\begin{split}
s_1&=m_0+step_0+1,\\
s_k&=s_{k-1}+step_0+k \text{ when } k > 1,
\end{split}
\label{eq:advsub}
\end{equation}
where $m_0$ and $step_0$ are the values of $\tilde{m}$ and $step$ at the beginning of the sequence.
This adversarial sub-stream, regardless of the initial values of $\tilde{m}$ and $step$,  takes the value of $step$ from $step_0$ to the value $step_0 + l$  where $l$ is its length. Consequently, the $step$ value can grow indefinitely, provided a sufficiently long adversarial sub-stream.
We also note that, in the input stream, each subsequent value of the adversarial sub-stream must appear when the event $E =rand > 1-q$ occurs, whereas the input stream values between two successive events $E$ should remain constant and equal to the current estimate of $\tilde{m}$ for the value of $step$ not to decrease.
%It is enough to provide a carefully crafted adversarial input stream. Define the input stream $S$ as follows: $s_1 = 2$, $s_{i+1} = s_i + i + 1$. Then, the condition required to reset $step$ to 1 in lines 21-22 of the algorithm is never verified. Therefore, $step$ increases without bound. As a remark, additional adversarial input streams do exist for \textsc{Frugal-2U}.
\end{proof}


A consequence of Lemma \ref{unbounded-step} is that the global sensitivity of Algorithm \ref{Frugal-2U} is unbounded, since in our reference model (bounded DP) there is guarantee that changing a stream item will result in a bounded change of the quantile estimate.

\begin{corollary}
\label{corollary-unbounded-sensitivity}
The global sensitivity of Algorithm \ref{Frugal-2U} is unbounded.
\end{corollary}

\begin{proof}
The corollary follows straight from Lemma \ref{unbounded-step}, since the quantile estimate is updated using the $step$ value. 
\end{proof}

Owing to Corollary \ref{corollary-unbounded-sensitivity}, in order to  design a DP version of \textsc{Frugal-2U}, we need to take into account the local sensitivity of the algorithm, defined as follows. 

\begin{defn} \label{local-sensitivity} (Local sensitivity) Given a function $f:~\mathcal{D} ~\rightarrow~\mathbb{R}$ mapping a dataset in $\mathcal{D}$ to a real number, the local sensitivity of $f$ at $x \in \mathcal{D}$ is defined as $LS(f, x)=\max _{x^{\prime}: d\left(x, x^{\prime}\right) \leq 1}\left|f(x)-f\left(x^{\prime}\right)\right|$, where $d\left(x, x^{\prime}\right)$ represents the distance between two datasets  $x$ and $x^{\prime}$.
\end{defn}

In practice, one of the two datasets is fixed, the actual dataset, and its neighbours are considered. This is in contrast with the definition of global sensitivity, in which any two arbitrary neighbouring datasets are considered. Our approach to provide a DP version of the algorithm \ref{Frugal-2U} is the use of the ``Sample and Aggregate" framework \cite{10.1145/1250790.1250803}. Given an arbitrary function $f: \mathcal{D} \rightarrow \mathbb{R}$, a lower clipping bound $l$ and  an upper clipping bound $u$, Sample and Aggregate is known to satisfy $\epsilon$-DP. Here, setting $l$ and $u$ allows clipping the value of interest to the bounded interval $[l, u]$. The framework is simpler to use with regard to other possibilities (e.g., Propose-Test-Release \cite{10.1145/1536414.1536466} and Smooth Sensitivity \cite{10.1145/1250790.1250803}), and works as follows, without requiring the computation of the local sensitivity of the function $f$:

\begin{enumerate}
	\item Partition the dataset $x$ into $k$ chunks $x_1, \ldots, x_k$;
	\item Determine the result of a clipped query for each chunk:  $q_i=\max \left(l, \min \left(u, f\left(x_i\right)\right)\right)$;
	\item Compute a noisy average of the query results: $Q=\left(\frac{1}{k} \sum_{i=1}^k q_i\right)+\operatorname{Lap}\left(\frac{u-l}{k \epsilon}\right)$.
\end{enumerate}

Since the sensitivity of $f$ is unknown but it is guaranteed that its output lies between $l$ and $u$ (being clipped), then the sensitivity of each clipped query $f(x_i)$ is equal to $u-l$. Finally, since we average $k$ values, the sensitivity is equal to $\frac{u-l}{k}$. We note here that the values $l$ and $u$ must not be estimated in advance, since they can be computed exactly in streaming as follows. To compute $l$, initialize $l = + \infty$ and each time a new input stream item $s_i$ arrives, compare $l$ with $s_i$ and if $l$ is greater than $s_i$ set $l = s_i$. The computation of $u$ is symmetric.

We apply the Sample and Aggregate approach to the algorithm as shown in Algorithm \ref{Diffpriv-Frugal-2U}. We shall refer to this algorithm as \textsc{DP-Frugal-2U-SA}.

\begin{algorithm}
	\caption{Differentially Private Frugal-2U}
		\label{Diffpriv-Frugal-2U}
	 \begin{algorithmic}[1]
		\Require{Data stream $S$, quantile $q$, number of chunks $k$,  $k$ units of memory $\tilde{m_i}$, $k$ units of memory $step_i$, $k$ bits $sign_i$}
		\Ensure{differentially private estimated quantile value $\tilde{m}$}
		\For {$i = 1, i \leq k, i++$}
	    	\State $\tilde{m_i} = 0$, $step_i = 1$, $sign_i = 1$
	    \EndFor
	    \State $l = + \infty$
	    \State $u = - \infty$
		\ForAll {$s_i \in S$}
			\If {$l > s_i$}
				\State $l = s_i$
			\EndIf
			\If {$u < s_i$}
				\State $u = s_i$
			\EndIf
			\State $rand = random(0,1)$
			\State $idx = ((i-1) \operatorname{mod} k) + 1$
			\If {$s_i > \tilde{m}_{idx}$ and $rand > 1-q$}
				\State $step_{idx} \ += (sign_{idx} > 0) \ ? \ f(step_{idx}) : -f(step_{idx})$ 
				\State $\tilde{m}_{idx} \ += (step_{idx} > 0) \ ? \ \lceil step_{idx} \rceil : 1$
				\State $sign_{idx} = 1$
				\If {$\tilde{m}_{idx} > s_i$}
					\State $step_{idx} \ += s_i - \tilde{m}_{idx}$ 
					\State $\tilde{m}_{idx} = s_i$
				\EndIf
		   \ElsIf {$s_i < \tilde{m}_{idx}$ and $rand > q$}
					\State $step_{idx} \ += (sign_{idx} < 0) \ ? \ f(step_{idx}) : -f(step_{idx})$ 
					\State $\tilde{m}_{idx} \ -= (step_{idx} > 0) \ ? \ \lceil step_{idx} \rceil : 1$
					\State $sign_{idx} = -1$
					\If {$\tilde{m}_{idx} < s_i$}
						\State $step_{idx} \ += \tilde{m}_{idx} - s_i$ 
						\State $\tilde{m}_{idx} = s_i$
					\EndIf
			\EndIf
		\If {$(\tilde{m}_{idx} - s_i) * sign_{idx} < 0 \land step_{idx} > 1$}	
			\State $step_{idx} = 1$
		\EndIf
		\EndFor
		
		\State $\tilde{m} = 0$ 
		\For {$i = 1, i \leq k, i++$}
	    	%\State $q_i = \operatorname{max}(l, \operatorname{min}(u, \tilde{m_i}))$
	    	\State $\tilde{m} += \tilde{m_i}$
	    \EndFor
	    \State $\tilde{m} /= k$
	    \State $\tilde{m} += \operatorname{Lap}(\frac{u-l}{k})$
		\State \Return $\tilde{m}$
	\end{algorithmic}
\end{algorithm}

 The algorithm is made DP by automatically partitioning the incoming stream items into $k$ chunks, handled independently using the \textsc{Frugal-2U} algorithm. Partitioning is done by assigning the $i$-th stream item $s_i$ to the chunk whose index is given by $((i-1) \operatorname{mod} k) + 1 $ (note that we assume here that stream indexes start at 1). Therefore, items are assigned round-robin to the $k$ chunks available. Once the stream has been processed, we are ready to compute the DP estimate of the $q$ quantile. We should compute for each chunk $x_i$ its corresponding clipped query value as follows: $q_i=\max \left(l, \min \left(u, f\left(x_i\right)\right)\right)$. However, since in our case $f\left(x_i\right)$ is the estimated quantile, and a quantile value $q_v$ will always be between the lower ($l$) and upper ($u$) clipped bounds for the stream values, it follows that $q_i = \tilde{m}_i$ (indeed, $\operatorname{min}(u, q_v) = q_v$ and $\operatorname{max}(l, q_v)= q_v$). Therefore, we set $\tilde{m}$ to 0 and add to it the $\tilde{m}_i$ values corresponding to each chunk. Finally, we divide the result by $k$ and add the amount of Laplace noise mandated by the Sample and Aggregate framework.
 
 We now discuss how to select $k$. Since this parameter is related to the number of chunks to be used, a tradeoff is in effect. Higher values of $k$ result in less noise being added to the final answer, a desirable property. However, simultaneously this implies that the size of each chunk becomes smaller when $k$ is increased. The net effect is that the probability of the quantile estimate $f(x_i)$ being close to the true desired answer $f(x)$ decreases. Anyway, if the size of the input stream is huge (it may even be infinite), this is not an issue. Therefore, $k$ can be chosen accordingly, without impacting too much on the final, desired accuracy.


\section{Analysis of the probability of spontaneous occurrence for adversarial sequences}\label{sec:analysis}

We determine the probability of random occurrence of an adversarial sequence, which may be of independent interest.
To simplify the analysis, we consider only the case of large quantile $q$, which is often of practical interest in applications. Under this assumptions, the event $E$ in the proof of Lemma \ref{unbounded-step} has probability close to one; consequently, we can consider the elements of substream defined in Eq. \eqref{eq:advsub} as consecutive. For general $q$, the provided analysis represents an upper bound to the probability of finding an adversarial sequence as substream of the input stream.

A stream of items of length $N$ is a sequence of random variables $X_1, \ldots, X_N$, where $X_i\in\mathcal{X}$ with $|\mathcal{X}|=M$. Without loss of generality, we assume the $M$-dimensional alphabet $\mathcal{X}$ to be the set of natural numbers $\{x_j = 1, \ldots, M\}$, with associated probabilities $\Pr(X_i=x_j)$. Notice that such probabilities are completely arbitrary, meaning that the formulation applies to all types of discrete probability distributions, including heavy-tailed ones, possibly after binning (quantization). Moreover, in  practice a finite range of variability is observed, hence assuming the support is upper-bounded (hence $\mathcal{X}$ is a finite set) is not a limitation but rather represents a more physically-plausible assumption in many application fields, due to physical limitations \cite{weinberg2018trimmed,ali2006truncated,burroughs2001upper} as well as intentional winsorizing or trimming (filtering) of extreme values to increase  robustness to outliers \cite{hastings1947low, miao2014additive, Grassi}.


We are concerned with the probability $\theta$ that an adversarial subsequence $\bm{s}=[s_0\ s_1\ \cdots \ s_K]^T\in\mathcal{X}^{K+1}$ randomly occurs (at least once) within a stream $\bm{x}\in\mathcal{X}^{N}$.
$\theta$ can be exactly computed by considering a Markov chain over finite automata approach, where the state $S_\ell$, $\ell=0,\ldots,K+1$ represents the fact that the first $\ell$ values of $\bm{s}$ have occurred in the last $\ell$ observed items. $S_0$ thus corresponds to ``waiting for $s_0$ to occur'', $S_1$ to ``waiting for $s_1$ given $s_0$ occurred as last item'' and so on. The final state $S_{K+1}$ is reached when the whole sequence $\bm{s}$ has been observed as last $K+1$ items, and it is an absorbing state of the chain. Conversely, the transition probabilities between the other states, that is the entries of the $(K+2)\times (K+2)$ transition matrix $\bm{P}$, are dictated by the structure of $\bm{s}$: if $S_\ell$ is the current state, the next state will be $S_{\ell+1}$ only if $s_{\ell+1}$ is the next item, otherwise the chain typically returns to $S_0$ unless the observed item forms with  a certain number of last items a prefix for $\bm{s}$. In the latter case, in fact, part of the sequence has already occurred, so the transition will be towards an intermediate state with index higher than 0 (but smaller than $\ell+1$). To give an example, denoting by $\alpha_i$ the probability of item $i$, for the sequence $\bm{s}=[1\ 2 \ 3]^T$ the transition matrix is 
\begin{equation}
\bm{P} = \left[ \begin{array}{cccc} 1 - \alpha_1 & \alpha_1 & 0 & 0 \\ 1-\alpha_1-\alpha_2 & \alpha_1 & \alpha_2 & 0 \\ 1-\alpha_1-\alpha_3 & \alpha_1 & 0 & \alpha_3 \\
0 & 0 & 0 & 1
\end{array} \right] \label{eq:P_example}
\end{equation}
whereas for  $\bm{s}=[1\ 2 \ 1\ 3]^T$ the transition matrix is 
$$
\bm{P} = \left[ \begin{array}{ccccc} 1 - \alpha_1 & \alpha_1 & 0 & 0 & 0 \\ 1-\alpha_1-\alpha_2 & \alpha_1 & \alpha_2 & 0 & 0 \\ 1-\alpha_1 & 0 & 0 & \alpha_1 & 0 \\
1-\alpha_1-\alpha_2-\alpha_3 & \alpha_1 & \alpha_2 & 0 & \alpha_3 \\ 0 & 0 & 0 & 0 & 1 
\end{array} \right] .
$$
 Ultimately, the sought probability $\theta$ that $\bm{s}$ is found in a stream of length $N$ is given by the transition probability between $S_0$ and $S_{K+1}$ after $N$ iterates, that is the entry $(1,K+2)$ of the matrix $\bm{P}^N$.

Though this approach is applicable for any chosen $\bm{s}$, in the following we particularize the analysis for the worst-case sequence $\bm{s} = [2\ 5\ 9\ \cdots]^T$, generally given by the recursion $s_0=2$, $s_k=s_{k-1}+2+k$, $k=1,\ldots,K$. It is possible to rewrite the generic $s_k$ in $\bm{s}$ as
$$
s_k=2+\sum_{i=1}^k (2+i) = (k+1)(2+\frac{k}{2}), \quad k=0,\ldots, K.
$$ 
The corresponding $(K+2)\times (K+2)$ transition matrix for the Markov chain introduced above is the generalization of that shown in the example of Eq. \eqref{eq:P_example}, and is given by
$$
\bm{P} = \left[ \begin{array}{cc} \bm{1}_{(K+1)\times 1} - \bm{p} - \bm{p}_0  & \bm{A}\\ \bm{0}_{1 \times (K+1)} & 1 \end{array} \right]
$$
where 
$$
\bm{p} = [\P(X=s_0) \ \cdots \ \P(X=s_{K})]^T,
$$
$$
\bm{p}_0 = \left[ \begin{array}{c} 0  \\ \P(X=s_0) \,\bm{1}_{K \times 1} \end{array} \right],
$$ 
$$
\bm{A}=\mathrm{diag}(\bm{p}) + \left[ \begin{array}{cc} \bm{p}_0 & \bm{0}_{(K+1) \times K} \end{array} \right].
$$
Clearly, for a uniform distribution over $\mathcal{X}$, $\bm{p}=\frac{1}{M} \bm{1}_{(K+1)\times 1}$ and the non-zero entries of $\bm{A}$ are all equal to $\frac{1}{M}$. Numerical examples for the resulting probability $\theta$ are given later.

An approximation for $\theta$ can be found more explicitly by considering the probability of the complementary event ``not finding $\bm{s}$ anywhere in $\bm{x}$''. Let us denote by $\alpha$ the probability of finding $\bm{s}$ in a given position of the stream. Then, $1-\alpha$ is the probability of not finding $\bm{s}$ in a given position. An approximation is generally introduced when assuming independence among the occurrences of $\bm{s}$ in any of the positions from $1$ (beginning of the stream) to $N-K$ (last possible position where $\bm{s}$ can be found), because of the mentioned technicality with possible prefix subsequences (see above). 
Under such an approximation, we can compute the probability that $\bm{s}$ occurs nowhere as $(1-\alpha)^{N-K}$, which finally yields the formula
\begin{equation}
\theta = 1 - (1-\alpha)^{N-K} \label{eq:approx_formula}.
    \end{equation}
For the worst-case sequence we have that $\bm{s}$ is monotonically increasing, hence has no repeated values. As a consequence, it is prefix-free and the formula returns an exact value. For this case we have
$$
\alpha = \P(X=s_0) \cdots P(X=s_{K}) = \prod_{k=0}^{K} \P(X = (k+1)(2+\frac{k}{2}))
$$
which in turn, for the uniform case, boils down to $\alpha = \frac{1}{M^{K+1}}$.
Comparison between this formula and the Markov-chain based computation are reported below.

\begin{figure}
    \centering
    \includegraphics[width=7cm]{./img/prob.eps}
    \caption{Computation of the probability $\theta$ for uniform distribution of items: comparison between the Markov chain approach (solid  line), eq. \eqref{eq:approx_formula} (dashed  line), and Monte Carlo simulations (asterisks), as a function of the stream length $N$.}
    \label{fig:prob}
\end{figure}

Finally, it is easy to compute the expected number of occurrences of $\bm{s}$ in $\bm{x}$. Denoting by $Z_i$ the indicator function of the event ``$\bm{s}$ is found in position $i$'', we clearly have that $\P(Z_i)=\alpha$ and the  number of occurrences is given by
$$
\sum_{i=1}^{N-K} Z_i
$$
from which the expected value is readily obtained as
\begin{equation}
E \Big[\sum_{i=1}^{N-K} Z_i \Big] = (N-K)\alpha. \label{eq:expected}
\end{equation}
Notice that, being the linearity of the expectation a general property not requiring independence, this formula is always exact (i.e., for any $\bm{s}$, irrespective of its structure being prefix-free or not).

We report numerical results to illustrate the theoretical findings above. We consider $M=100$ and the worst-case adversarial sequence with varying $K$. 
We start by considering a uniform distribution for the items, which means all entries in $\bm{p}$ are equal to $1/M=0.01$ and $\alpha$ ranges from $10^{-4}$ to $10^{-8}$ as $K$ is varied from 1 to 3.
Fig. \ref{fig:prob} shows a comparison between the computation of the probability $\theta$ obtained via the Markov chain approach (solid  line) and the formula in eq. \eqref{eq:approx_formula} (dashed  line), as a function of the stream length $N$. It can be seen that the returned values are in perfect match, and show also an excellent agreement with the values obtained via Monte Carlo simulation (asterisks) on $10^4$ trials. Notice that the missing points are zero values associated to very low probability (below the  capability of the frequentist estimate), which cannot be shown on logarithmic scale. The figure collectively illustrates that, as expected, the probability $\theta$ grows with $N$ and decreases with $K$, since the chances of finding a  sequence are higher for longer streams and for shorter sequences. 




\begin{figure}
    \centering
    \includegraphics[width=8cm]{./img/expect.eps}
    \caption{Expected number of occurrences of the worst-case sequence of length $K$ in streams of length $N$, for uniform distribution of items.}
    \label{fig:expect}
\end{figure}

We have also validated Eq. \eqref{eq:expected} via a similar procedure, which confirmed again a perfect match between the theoretical prediction and the Monte Carlo estimates. We report in Fig. \ref{fig:expect} such values, for $N$ up to 100,000 and $K$ from 1 to 4. The inset shows the same bar plot in logarithmic scale, to better visualize the small values. It can be seen that, for longer streams, some occurrences of the sequence are found when $K$ is small; conversely, occurrences of longer sequences are unlikely to be found even in very long streams.

\begin{figure}
    \centering
    \includegraphics[width=7cm]{./img/prob_binom.eps}
    \caption{Computation of the probability $\theta$ for Binomial distribution of items (lines and markers are as in Fig. \ref{fig:prob}).}
    \label{fig:prob_binom}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=8cm]{./img/expect_binom.eps}
    \caption{Expected number of occurrences of the worst-case sequence of length $K$ in streams of length $N$, for Binomial distribution of items.}
    \label{fig:expect_binom}
\end{figure}

We repeated the analysis by generating the $N$ items according to a Binomial distribution over $0, \ldots, M-1$, with probability parameter equal to 0.05. This corresponds to a mean value of 5 and $\bm{p}=[0.08\    0.18 \   0.03\    0.0003]^T$ (for $K=3$). The associated values of $\alpha$ are thus $0.02$, $5\cdot 10^{-4}$ and $1.5\cdot 10^{-7}$ for $K=1,2,3$, respectively. 
Fig. \ref{fig:prob_binom} is the analogous of Fig. \ref{fig:prob}, and similar observations can be made. Notice however that the probability $\theta$ is generally higher in the considered Binomial case, due to the more likely occurrence of the values contained in $\bm{s}$. This is also confirmed by Fig. 
\ref{fig:expect_binom} (analogous to Fig. 
\ref{fig:expect}), which shows a significantly higher number of occurrences, though the trend with respect to $K$ and $N$ remains clearly the same.

In Figs.  \ref{fig:prob_binom}-\ref{fig:expect_binom} we have not computed the values for $K=4$ due to the long simulation time in the generation of long Binomial-distributed samples compared to the uniform samples of Figs. \ref{fig:prob}-\ref{fig:expect}; however, we report in Fig. \ref{fig:prob_uniform_M20} the theoretical values obtained from Eq. \eqref{eq:approx_formula} for the Binomial case, considering a more extended range of values for $N$ and $K$. Moreover, we reduce $M$ from 100 to 20, so as to emphasize the effect on a set of items with more limited cardinality. The figure clearly shows that, in this case, the probability $\theta$ is non-negligible even for higher values of $K$, which make the impact more significant, provided that the stream is long enough.

\begin{figure}
    \centering
    \includegraphics[width=8cm]{./img/prob_uniform_M20.eps}
    \caption{Probability $\theta$ for the case of Binomial distribution of items, as a function of $N$, for $K$ ranging from 1 to 10.}
    \label{fig:prob_uniform_M20}
\end{figure}



\section{Related work}
\label{related}
Many DP algorithms for computing quantiles have been proposed in the literature, though most of them are not frugal and/or cannot be adopted in the streaming setting. Here, we recall the most important ones, beginning with those algorithms designed to release a quantile (or a set of quantiles) of a dataset (i.e., all of the items are available when the algorithm begins its execution, not the streaming setting).

The algorithm proposed in \cite{Smith} allows computing a single quantile and can be considered an instantiation of the so-called exponential mechanism leveraging a specific utility function. It works by sampling an interval and then sampling an item in the selected interval. Since the items must be sorted, its worst-case complexity is $O(n \log n)$. 

The algorithm may be used to compute $m$ quantiles using well-known composition theorems, since one can easily estimate separately each quantile privately, and then obtain the overall privacy budget by updating it, using a composition theorem, taking into account the privacy budgets used independently for each quantile estimation. Such an approach, while feasible, is anyway sub-optimal since the error would scale polynomially with $m$. Algorithms based on this approach are JointExp, AppindExp and AggTree, proposed in \cite{pmlr-v139-gillenwater21a}.

Better algorithms exist for this task that do not use the straightforward direct independent application of a composition theorem. An example is a recent and clever algorithm, Approximate Quantiles \cite{pmlr-v162-kaplan22a} which computes recursively $m$ quantiles with an error that scales logarithmically with $m$. The worst-case complexity of the algorithm is $O(n \lg m)$.

In the streaming setting, a recent work \cite{NEURIPS2022_525338e0} proposed the use of linear sketches to privately compute arbitrary quantiles. In particular, the authors design a private variant of the Dyadic CountSketch algorithm \cite{10.1145/2463676.2465312} by using their version of private CountSketch based on $\rho$-zCDP. The algorithm retains the same space complexity of Dyadic CountSketch, i.e., $O\left(\frac{1}{\varepsilon} \log ^{1.5} u \log ^{1.5}\left(\frac{\log u}{\varepsilon}\right)\right)$ where $u$ is the size of the universe from which the stream items are drawn ($[u]=\{0, \ldots, u-1\}$) and $\varepsilon$ is the approximation error related to the underlying CountSketch data structure. In contrast, our DP algorithms based on \textsc{Frugal-1U} and \textsc{Frugal-1U} only requires one or two units of memory. Moreover, the update (inserting an incoming stream item into the sketch) and query (returning a quantile estimate) of private Dyadic CountSketch are much slower than those of our DP algorithms.

In \cite{alabi2023bounded}, starting from the observation that all existing private mechanisms for distribution-independent quantile computation require space at least linear in the input size $n$, the authors design DP algorithms which exhibit strongly sub-linear space complexity, namely DPExpGK and DPHistGK. Then, the authors extend DPExpGK to work in the streaming setting. The algorithm works by updating the private estimate not for each incoming stream item, but at fixed checkpoints. For a stream of $n$ items there are $O\left(\log _{1+\alpha} \frac{n}{n_{\min }}\right)$ checkpoints, where $\alpha$ is a parameter related to the non private quantile estimate accuracy and $n_{\min}$ is a threshold for the stream length: it must be $n>n_{\min }=\Omega\left(\frac{1}{\alpha^2 \epsilon} \log n \log \left(\frac{|\mathcal{X}| \log n}{\alpha \beta}\right)\right)$ where $\epsilon$ is the privacy budget, $|\mathcal{X}|$ is the number of distinct items in the stream and $\beta$ a failure probability. The space complexity of the algorithm is $\Omega\left(\frac{1}{\alpha^2 \epsilon} \log ^2 n \log \left(\frac{|\mathcal{X}| \log n}{\alpha \beta}\right)\right)$. Also in this case, our DP algorithms provide faster updates whilst requiring one or two units of memory. 

Another DP algorithm working in the streaming setting has been proposed in \cite{Liu}, but works under Local Differential Privacy (LDP) using self-normalization. The algorithm requires computing, for each incoming stream item, a corresponding step size used in the update of the quantile estimate. A locally randomized process is used, in which given an input stream item, the current quantile estimate and a so-called response rate $r$ (which corresponds to the privacy budget), the authors verify if the stream item is greater than the current quantile estimate. The result (true or false) is randomized using two Bernoulli distributions, and is then used to update the current $i$-th quantile estimate, along with a corresponding step size $d_i=2 /\left(i^{0.51}+100\right)$.
The space required is $O(1)$, since four units of memory are required to process the input. In particular, only one unit of memory is used to estimate the quantile, whilst the other three units of memory are only required if determining a confidence interval for the quantile estimate is requested as well. The update process is relatively fast, even though it is still much slower than our algorithms. Since this algorithm is the only one matching the features of \textsc{Frugal-1U-L}, \textsc{Frugal-1U-G}, \textsc{Frugal-1U-$\rho$} and \textsc{Frugal-2U-SA}  (i.e., streaming setting, $O(1)$ space required and DP release of a quantile estimate), in the next section in which we provide experimental results, we shall compare its performance versus our proposed algorithms. In the sequel, we shall refer to this algorithm as \textsc{LDPQ}. 




\section{Experimental Results}
\label{results}

 \begin{table}
 	\caption{Synthetic datasets.} 
	\label{datasets}
	\centering
	\begin{tabular}{llll}
		\textbf{Dataset} & \textbf{Distribution}& \textbf{Parameters}& \textbf{PDF}\\
		\hline
		D1& Uniform &  $[0, 1000]$ & \includegraphics[width=2cm]{./img/uniform.eps}\\
		D2 & Chi square &   $\alpha = 5$ & \includegraphics[width=2cm]{./img/chi2.eps}\\
		D3 & Exponential  & $\alpha = 0.5$ & \includegraphics[width=2cm]{./img/exp.eps}\\
		D4 & Lognormal & $\alpha =1, \beta = 1.5$ & \includegraphics[width=2cm]{./img/lognormal.eps}\\
		D5 & Normal & $\mu = 50, \sigma = 2$ & \includegraphics[width=2cm]{./img/normal.eps}\\
		D6 & Cauchy & $\alpha = 10000, \beta = 1250$ & \includegraphics[width=2cm]{./img/cauchy.eps}\\
		D7 & Extreme Value & $\alpha = 20, \beta = 2$ & \includegraphics[width=2cm]{./img/extreme.eps}\\
		D8 & Gamma & $a = 2, b = 4$ & \includegraphics[width=2cm]{./img/gamma.eps}\\
		\hline
	\end{tabular}
\end{table}

 In this section we present and discuss the results of the experiments carried out for \textsc{Frugal-1U-L}, \textsc{Frugal-1U-G}, \textsc{Frugal-1U-$\rho$}, \textsc{Frugal-2U-SA} and \textsc{LDPQ}. The source code has been compiled using the Apple clang compiler v15.0 with the following flags: -Os -std=c++14 (it is worth recalling  that on macOS the flag -Os optimizes for size and usually on this platform the resulting executable is faster than the executable obtained by compiling using the -O3 flag). The tests have been carried out on an Apple MacBook Pro laptop equipped with 64 GB of RAM and a 2.3 GHz 8-core Intel Core i9. The experiments have been repeated ten times for each specific category of test and the results have been averaged; the seeds used to initialize the pseudo-random generators are the same for each  experiment and algorithm being tested.
 
The tests have been performed on eight synthetic datasets, whose properties are summarized in Table \ref{datasets}. The experiments have been executed varying the stream length, the quantile, the privacy budget, either $\epsilon$ or $r$ (which represents the \textsc{LDPQ} privacy budget), $\delta$ and $\rho$. Table \ref{params} reports the default settings for the parameters. 

We compare \textsc{Frugal-1U-L} with \textsc{LDPQ}. In particular, we remark here that this is possible owing to the fact that the values selected for $r$ correspond exactly to the values selected for $\epsilon$, since in \cite{Liu} the authors report that their algorithm satisfies $\epsilon$-LDP with $\epsilon=\ln ((1+r) /(1-r))$ (alternatively, $r =(e^\epsilon-1) /(e^\epsilon + 1) = \tanh (\epsilon / 2))$). Therefore, the provided comparison between \textsc{Frugal-1U-L} and \textsc{LDPQ} is fair.

Additionally, we report here the results obtained for \textsc{Frugal-1U-G} and \textsc{Frugal-1U-$\rho$}. Moreover, we also report the result for \textsc{Frugal-2U-SA}.



\begin{table}
	\caption{Default settings of the parameters.} 
	\label{params}
	\centering
	\begin{tabular}{lll}
		\textbf{Parameter} & \textbf{Values} & \textbf{Default}\\
		\hline
		quantile &   $\{0.1, 0.3, 0.5, 0.99\}$ & 0.99\\
		stream length & $\{ 10M, 50M, 75M, 100M \}$ & $10M$\\
		$\epsilon$ &   $\{0.1, 0.5, 1, 2\}$ & 1\\
		$r$ & $\{0.05, 0.25, 0.46, 0.76\}$ & 0.46\\
		$\delta$ &   $\{0.01, 0.04, 0.08, 0.1\}$ & 0.04\\
		$\rho$ &   $\{0.1, 0.5, 1, 5\}$ & 1\\
		chunks & $\{2, 4, 8, 16\}$ & 4\\
		\hline
	\end{tabular}
\end{table}

\begin{figure*}
    \centering
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/epsilon_all.eps}
        \caption{privacy budget: $\epsilon$ and $r$}
        \label{fig:budget-all}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/quantile_all.eps}
        \caption{quantile}
        \label{fig:q-all}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/n_all.eps}
        \caption{$n$}
        \label{fig:n-all}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/throughput_all.eps}
        \caption{$updates/s$}
        \label{fig:throughput_all}
    \end{subfigure}
 
    \caption{\textsc{Frugal-1U-L} versus \textsc{LDPQ}. Relative error varying the privacy budget $\epsilon$ and $r$, the quantile $q$ and the stream size $n$. Throughput measured in updates/s.}
   \label{fig:1-vs-ldpq}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/gauss_delta_1u.eps}
        \caption{$\delta$}
        \label{fig:gauss-delta}
    \end{subfigure}\!\!
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/gauss_quantile_1u.eps}
        \caption{Quantile}
        \label{fig:gauss-q}
    \end{subfigure}\!\!
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/gauss_n_1u.eps}
        \caption{$n$}
        \label{fig:gauss-n}
    \end{subfigure}
 
   \caption{\textsc{Frugal-1U-G}. Relative error varying the probability $\delta$, the quantile $q$ and the stream size $n$.}
    \label{fig:gauss-1}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/zcdp_rho_1u.eps}
        \caption{$\rho$}
        \label{fig:zcdp-rho}
    \end{subfigure}\!\!
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/zcdp_quantile_1u.eps}
        \caption{Quantile}
        \label{fig:zcdp-q}
    \end{subfigure}\!\!
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/zcdp_n_1u.eps}
        \caption{$n$}
        \label{fig:zcdp-n}
    \end{subfigure}
 
   \caption{\textsc{Frugal-1U-$\rho$}. Relative error varying the parameter $\rho$, the quantile $q$ and the stream size $n$.}
    \label{fig:zcdp-1}
\end{figure*}


\begin{figure*}
    \centering
        \includegraphics[width=0.85\linewidth]{./img/errors_distrib_all.eps}
        \caption{\textsc{Frugal-1U-L} versus \textsc{LDPQ}: relative error varying the distributions.}
        \label{fig:distributions-all}
    
    \label{fig:zcdp-1}
\end{figure*}


\begin{figure*}[!htb]
    \centering
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/dperror_epsilon_2u.eps}
        \caption{$\rho$}
        \label{fig:sa-eps}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/dperror_quantile_2u.eps}
        \caption{Quantile}
        \label{fig:sa-q}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/dperror_n_2u.eps}
        \caption{$n$}
        \label{fig:sa-n}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \includegraphics[width=0.99\linewidth]{./img/dperror_chunks_2u.eps}
        \caption{Chunks}
        \label{fig:sa-chunks}
    \end{subfigure}
 
   \caption{\textsc{Frugal-2U-SA}. Relative error varying the privacy budget $\epsilon$, the quantile $q$, the stream size $n$ and the number of chunks.}
    \label{fig:sa-2}
\end{figure*}


\begin{figure}
    \centering
        \includegraphics[width=\linewidth]{./img/dperror_distrib_2u.eps}
        \caption{\textsc{Frugal-2U-SA}. Relative error varying the distributions.}
        \label{fig:distributions-2}
    
    \label{fig:zcdp-1}
\end{figure}


We plot the relative error between the true quantile and the DP quantile estimate released by the algorithms under test, by allowing one parameter to vary whilst keeping the values of the others at their defaults. In all of the figures, the distribution used is the normal (later we compare the results obtained when varying the distribution as well). 

The experimental results for \textsc{Frugal-1U-L} (using the Laplace mechanism) versus \textsc{LDPQ} are shown in Figure \ref{fig:1-vs-ldpq}. Regarding \textsc{Frugal-1U-L}, as depicted in Figure \ref{fig:budget-all}, the relative error decreases as expected when the privacy budget $\epsilon$ increases, meaning that the utility (see Section \ref{notation}) of the released value increases when $\epsilon$ increases. Therefore, a good tradeoff between privacy and utility is reached for $0.5 \leq \epsilon \leq 1$. On the contrary, \textsc{LDPQ} presents a very high relative error between 10 and 100, making the output useless from a practical perspective.

 Figure \ref{fig:q-all} depicts the relative error varying the computed quantile. The observed trend is the same, with \textsc{Frugal-1U-L} outperforming \textsc{LDPQ}. As shown in Figure \ref{fig:n-all}, the stream size does not affect the security of the released quantile in the case of \textsc{Frugal-1U-L} whilst \textsc{LDPQ} exhibits a fluctuating behaviour with a relative error between 10 and 100. Finally, Figure \ref{fig:throughput_all} depicts the throughput measured in updates/s. As shown, \textsc{Frugal-1U-L} is up to 7 times faster than \textsc{LDPQ}, making it suitable for processing high speed streams.


Next, we analyze \textsc{Frugal-1U-G}. Increasing $\delta$, the probability of failure, provides as expected slightly less privacy, as shown in Figure \ref{fig:gauss-delta}. Varying the computed quantile exhibits a similar behaviour. In Figure \ref{fig:gauss-q}, slightly less privacy is associated to higher quantiles. Finally, the impact of the stream size is depicted in Figure \ref{fig:gauss-n}, in which a fluctuating behaviour can be observed, even though the interval of variation is tight.

Regarding \textsc{Frugal-1U-$\rho$}, Figure \ref{fig:zcdp-rho} shows that, as expected, increasing the privacy budget $\rho$ the relative error decreases and correspondingly the utility increases. A good tradeoff between privacy and utility is reached for $0.5 \leq \rho \leq 1$. Figure \ref{fig:zcdp-q} and \ref{fig:zcdp-n}, related respectively to the relative error varying the computed quantile and the stream size present the same behaviour illustrated for the Gaussian mechanism. This is not surprising, since this mechanism adds Gaussian noise (even though the way noise is derived is of course different).

We now turn our attention to what happens when we vary the distribution. Figure \ref{fig:distributions-all} provides the results for \textsc{Frugal-1U-L}, \textsc{Frugal-1U-G},  \textsc{Frugal-1U-$\rho$} and \textsc{LDPQ}. As shown, the relative error between the true quantile and the DP quantile estimate released by one of the algorithms varies with the distribution. However, for our proposed algorithms, as expected (since the global sensitivity is just 2) the algorithms can be used independently of the actual distribution, with the notable exception related to the Cauchy distribution (which can be considered adversarial for our algorithms based on \textsc{Frugal-1U} as discussed in \cite{frugal}). \textsc{LDPQ}, as shown, exhibits a relative error varying between 1 and more than 100, making it useless for all of the distributions under test.

Our results show that, having fixed a distribution, the behaviour of our algorithms based on \textsc{Frugal-1U} does not depend on the seed used to initialize the pseudo-random number generator used to draw samples from the distribution. In this sense, our algorithms are robust. On the contrary, \textsc{LDPQ} heavily depends on the seeds used, exhibiting a rather large variance in the obtained results.  

Finally, we analyze the $(\alpha, \beta)$-accuracy (Definition \ref{accuracy}) of \textsc{Frugal-1U-L}. Fixing $\beta = 0.04$, $\epsilon = 1$ and taking into account that the global sensitivity of \textsc{Frugal-1U} is $s = 2$, by using equation \eqref{alpha} we get $\alpha=\ln \left(\frac{1}{0.04}\right) \cdot 2 = 6.4$, so that \textsc{Frugal-1U-L} is (6.4, 0.04)-accurate.

For \textsc{Frugal-1U-G}, using Eq. \eqref{alpha-gauss} with $\delta = 0.04$, $\beta = 0.04$ and $\epsilon = 1$ we get $\alpha = 9.1$ so that \textsc{Frugal-1U-G} is (9.1, 0.04)-accurate. Finally, \textsc{Frugal-1U-$\rho$} accuracy is determined by using equation \eqref{alpha-rho} with $\rho = 1$ and $\beta = 0.04$, so that $\alpha = 2.4$ and  \textsc{Frugal-1U-$\rho$} is (2.4, 0.04)-accurate.



\textsc{Frugal-2U-SA} results are shown in Figure \ref{fig:sa-2}, with regard to varying the privacy budget $\epsilon$ (Figure \ref{fig:sa-eps}), the quantile $q$ (Figure \ref{fig:sa-q}), the stream size $n$ (Figure \ref{fig:sa-n}) and the number of chunks used in the Sample and Aggregate framework (Figure \ref{fig:sa-chunks}). The results obtained are, apparently, similar to those related to \textsc{Frugal-1U-L}, \textsc{Frugal-1U-G} and \textsc{Frugal-1U-$\rho$}. Increasing the chunks we observe less relative error, as expected (but we need to recall here that, in turn, this also requires a suitable chunk size in order for the algorithm to converge to a correct estimate in each chunk). Even though the obtained results are good enough, we need to take into account that these results are related to the normal distribution, and we also need to take into account the distribution parameters, which determine the minimum and maximum value used in the Sample and Aggregate framework.

Indeed, since in this case the global sensitivity is unbounded, we cannot hope to get consistently good results independently of the distribution. We confirm this in Figure \ref{fig:distributions-2}, in which we show that the relative error associated to the release of the estimate for the 0.99 quantile using a privacy budget $\epsilon = 1$ and 4 chunks may be, depending on the selected distribution, orders of magnitude higher than acceptable values that lead to a good tradeoff between privacy and utility.

As a consequence, despite the faster convergence speed to the quantile estimate, \textsc{Frugal-2U-SA} cannot be used to ensure privacy of the released quantile without being fully aware of the implications related to the distribution, and in particular the range (i.e., the difference between the maximum and the minimum value). Therefore, we recommend using \textsc{Frugal-1U-L}, \textsc{Frugal-1U-G} and \textsc{Frugal-1U-$\rho$} instead, since these does not require distributional assumptions, are fast and require just one unit of memory.

Regarding the accuracy of \textsc{Frugal-2U-SA}, it can be easily derived taking into account that the noise added to the answer is $\operatorname{Lap}(\frac{u-l}{k \epsilon})$; thus, \textsc{Frugal-2U-SA} is $(\alpha, \beta)$-accurate with 
\begin{equation}
\alpha = \frac{\ln \left(\frac{1}{\beta}\right)(u-l)}{k \epsilon}.
\end{equation}

Therefore, fixing the values of $\beta$ and $\epsilon$, $\alpha$ depends both on the range $u-l$ (i.e., it depends on the actual distribution from which the stream items are drawn) and on $k$ (i.e., the number of chunks in which the stream is partitioned). In order to improve the accuracy, it is consequently desirable to use a number of chunks as large as possible (ideally, $k = u-l$). As we have already noted, in order to increase the value of $k$ we need, however, a sufficiently large stream size.






  
\section{Conclusions}
\label{conclusions}
In this paper, we proposed DP algorithms for tracking quantiles in a streaming setting. Our algorithms are DP variants of the well-known \textsc{Frugal-1U} and \textsc{Frugal-2U} algorithms, characterized by the property of requiring just a tiny amount of memory to process a stream while guaranteeing surprising accuracy for the estimates of a quantile. In particular, for \textsc{Frugal-1U} we gave corresponding $\epsilon$-DP, $(\epsilon,\delta)$-DP, and $\rho$-zCDF algorithms after proving that the global sensitivity of \textsc{Frugal-1U} is equal to 2. Moreover, we compared our \textsc{Frugal-1U-L} algorithm with \textsc{LDPQ}, which is a recent state of the art algorithm, and showed that our algorithm outperforms \textsc{LDPQ} with regard to both accuracy and speed.

For \textsc{Frugal-2U}, we showed that the global sensitivity is unbounded, by providing a carefully crafted adversarial stream, which was also analyzed from a probabilistic perspective. Then, we provided an $\epsilon$-DP algorithm based on the Sample and Aggregate framework. The algorithms are simple to implement; however, due to the potential issues related to \textsc{Frugal-2U} (unbounded global sensitivity and dependency on the distribution range) we recommend the use of \textsc{Frugal-1U-L}, \textsc{Frugal-1U-G},  \textsc{Frugal-1U-$\rho$}, which exhibit good accuracy and security, as shown in the extensive experimental results provided. 
 
%\clearpage
 
\bibliographystyle{ieeetr}
\bibliography{biblio.bib}

\end{document}
