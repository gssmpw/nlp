%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[switch]{lineno}
%\usepackage[nonumberlist]{lineno}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{makecell}

\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{shapes.geometric, arrows, mindmap}
\usetikzlibrary{positioning}  

%\bibliographystyle{unsrt} % 按引用顺序排列

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach}
%Reinforcement Learning in Blockchain: A Survey of Its Use in Strategic Mining Behavior Analysis
%关于使用Reinforcement Learning分析区块链上策略挖矿行为的Survey。
% A Survey on the Application of Reinforcement Learning for Analyzing Strategic Mining Behavior on Blockchain
% A Survey of Reinforcement Learning Approaches for Strategy Mining Analysis in Blockchain

% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
Jichen Li$^{1,2\dagger}$\and
Lijia Xie$^{1\dagger}$\and
Hanting Huang$^{1,4}$\and
Bo Zhou$^1$\and
Binfeng Song$^{1,4}$\and \\
Wanying Zeng$^{1,3}$\and
Xiaotie Deng$^{1,2*}$\And
Xiao Zhang$^{1,3,5*}$\\
\affiliations
$^1$Zhongguancun Laboratory\\
$^2$Center on Frontiers of Computing
Studies, Computer Science Department, Peking University\\
$^3$LMIB and School of Mathematical Sciences, Beihang University\\
$^4$LMIB and Institute of Artificial Intelligence, Beihang University\\
$^5$Hangzhou International Innovation Institute of Beihang University\\
\emails
\{limo923, xiaotie\}@pku.edu.cn,
\{xielj, zhoubo\}@zgclab.edu.cn,\\
\{hantinghuang, zb2342110, zengzeng, xiao.zh\}@buaa.edu.cn
}

\begin{document}

\maketitle
\footnotetext{$^{\dagger}$Equal Contribution.}
\footnotetext{$^{*}$Corresponding Authors.}
\begin{abstract}
Strategic mining attacks, such as selfish mining, exploit blockchain consensus protocols by deviating from honest behavior to maximize rewards.
Markov Decision Process (MDP) analysis faces scalability challenges in modern digital economics, including blockchain.
To address these limitations, reinforcement learning (RL) provides a scalable alternative, enabling adaptive strategy optimization in complex dynamic environments.

In this survey, we examine RL's role in strategic mining analysis, comparing it to MDP-based approaches. We begin by reviewing foundational MDP models and their limitations, before exploring RL frameworks that can learn near-optimal strategies across various protocols.
Building on this analysis, we compare RL techniques and their effectiveness in deriving security thresholds, such as the minimum attacker power required for profitable attacks. 
Expanding the discussion further, we classify consensus protocols and propose open challenges, such as multi-agent dynamics and real-world validation.

This survey highlights the potential of reinforcement learning (RL) to address the challenges of selfish mining, including protocol design, threat detection, and security analysis, while offering a strategic roadmap for researchers in decentralized systems and AI-driven analytics.
\end{abstract}

% \input{Sections/1-Introduction}
% \input{Sections/2-Consensus}
% \input{Sections/3-MDP}
% \input{Sections/4-Results}
% \input{Sections/5-Conclusion}

% 按新逻辑写的部分，上面先保留作参考
\input{Sections/New-1-Intro}
\input{Sections/New-2-MDP_Model}
\input{Sections/New-3-RL}
\input{Sections/New-4-Consensus_Strategy}

\section{Conclusion}
\label{sec:conclusion}
This survey examines strategic mining in blockchain systems, with a focus on reinforcement learning as a tool for optimizing mining strategies.
%Our survey examines strategic mining in blockchain systems, emphasizing reinforcement learning as a tool for optimizing mining strategies. 
Traditional Markov Decision Process (MDP) approaches are useful for analyzing behaviors like selfish mining but face scalability challenges.
%Traditional Markov Decision Processes  approaches help analyze behaviors like selfish mining but struggle with scalability. 
Reinforcement learning (RL) provides adaptability in complex environments, enabling the identification of optimal strategies and security thresholds. 
This survey reviews previous studies that use MDPs and RL to analyze PoW and PoS consensus, and discusses the potential of these methods for analyzing other vote-based and parallel confirmation blockchains. 
%examines 
%RL's advantages over Markov Decision Processes (MDPs), especially in multi-agent settings, and proposes its potential to analyze vote-based and parallel confirmation blockchains.
%以前有了什么工作，将来要做什么。
%RL offers adaptability in complex environments, which can identify optimal strategies and security thresholds. The study highlights RL’s advantages over MDPs, particularly in multi-agent settings, and its potential in Proof-of-Stake blockchains. 
%Despite progress, challenges remain in modeling and multi-agent dynamics. 
Future research should focus on refining RL algorithms to improve blockchain security and efficiency, ultimately advancing decentralized systems.

%\appendix

%\section*{Ethical Statement}
%\section*{Acknowledgments}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ref}

\end{document}

