\section{Strategic Mining Analysis via Markov Decision Processes}
\label{sec:MDP}

\begin{table*}[ht]
\centering
\large % 使用较小的字体
\resizebox{\textwidth}{!}{ % 开始resizebox
    \begin{tabular}{| m{4cm}<{\centering}  m{3cm}<{\centering}  m{3.5cm}<{\centering}  m{3cm}<{\centering}  m{4.5cm} |} % Increase the width of the last column
        \hline
        \textbf{Literature} & \textbf{Security Threshold} & \textbf{Method} & \textbf{Blockchain Consensus} & \textbf{Description} \\ \hline
        \cite{eyal2014majority} & 0.25 & Markov Reward Process & Bitcoin PoW & Introduce original selfish mining strategy. \\ \hline
        \cite{sapirshtein2016optimal} & 0.232 & MDP & Bitcoin PoW & Compute the optimal strategy for selfish mining. \\ \hline
        \cite{marmolejo2019competing} & 0.26297 & MDP + Game Theory & Bitcoin PoW & Consider scenario of multiple non-colluding semi-selfish miners. \\ \hline
        \cite{feng2019selfish}  & 0.26 & Two-dimensional MDP & Ethereum PoW  & Propose a two-dimensional MDP to model selfish mining in Ethereum. \\ \hline
        \cite{zur2020efficient}  & 0.2468 & Average Reward Ratio MDP & Ethereum PoW & Propose the Average Reward Ratio (ARR) MDPs, reducing the complexity of computing optimal strategy. \\ \hline
    \end{tabular}
} % 结束resizebox
\caption{Studies on security threshold of blockchain consensus using Markov Decision Process}
\label{tab:MDPsec-threshold}
\end{table*}


In this section, we review the MDP modeling approach for consensus strategies, as well as the definition and analysis of security thresholds. 
We begin by examining previous work on the theoretical foundations of MDPs, highlighting key components such as states, actions, transitions, and rewards, which serve as the basis for decision-making in uncertain environments. 
We then explore the application of MDP models to strategic mining, demonstrating how they can optimize mining strategies in consensus protocols. 
Finally, we summarize the security threshold analysis through MDPs, focusing on 
how these models can be utilized to evaluate and to enhance security measures.
This section offers both theoretical insights and practical applications of MDPs in addressing strategic mining problems.
The results for the security threshold of strategic mining are summarized in Table~\ref{tab:MDPsec-threshold}. 

\subsection{Theoretical Foundations of MDP Modeling}

%The Markov Reward Process (MDP) provides a foundational framework for modeling systems with interdependent states, actions, transitions, and rewards. 
%In blockchain, MDP is often applied to analyze the security of strategic mining behaviors, where miners make decisions under probabilistic state transitions to maximize rewards. 
%In this section, 
We present the definition of MDP and its fundamental concepts as follows.

\begin{definition}[Markov Decision Process (MDP)]
An MDP is formally defined as a quintuple 
\begin{equation}
    MDP = (S, A, P, R, \gamma),
    \label{eq:MDP_def}
\end{equation}
where:
\begin{itemize}
    \item $S$: Finite or countable state space.
    \item $A$: Finite action-space available from each state.
    \item $P(s' \mid s, a): S \times S \times A \to [0, 1]$, the transition probability matrix, denoting the probability of transitioning from state $s$ to state $s'$ with action $a$.
    \item $R(s, a): S \times A \to \mathbb{R}$, the reward function providing the immediate reward obtained from state $s$ with action $a$.
    \item $\gamma \in [0, 1]$: The discount factor, controlling the importance of future rewards. %$\gamma \rightarrow 1$ implies greater consideration of future rewards.
\end{itemize}
\end{definition}

A policy $\pi(a | s)$ is a mapping that defines the probability actions should take in each state.
To calculate the total reward under a given policy $\pi$ in MDP, let $G_t$ be the cumulative reward starting from time $t$ with start state $s_t$, denoted by
\begin{align*}
G_t = \sum_{k=0}^{\infty} \gamma^k \mathbb{E}_{\pi}[R\bigl(s_{t+k}, \pi(s_{t+k})\bigr)],
\end{align*}
% \begin{align*}
% G_t = \mathbb{E}_{\pi}[R\bigl(s_{t}, a_t \bigr)] + \gamma G_{t+1} 
% \end{align*}
where $s_{t+k}$ is the state reached at time step $t+k$. For any state \( s \in S \), the value function \( V(s) \) is the expected cumulative reward calculated according to the current policy \( \pi \). Given the state \( s_t \), the agent selects the action $a_t$ with probability $\pi(a_t | s_t)$. Therefore, the form of the value function is as follows:
% \begin{align*}
% V^\pi(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R\left(s_{t}, \pi(s_{t})\right) \mid s_0 = s\right].
% \end{align*}
\begin{align*}
V^\pi(s) = \mathbb{E}_{\pi}\left[G_t \mid s_t = s\right].
\end{align*}
Furthermore, under a given policy \( \pi \), the expected cumulative reward after performing action \( a \) in state \( s \) is defined as the state-action value function \( Q^\pi(s, a) \), given by the form as
% \begin{align*}
% Q^\pi(s, a) = \mathbb{E}_{\pi}\left[ \sum_{t=0}^{\infty} \gamma^t R(s_{t}, a_{t}) \mid s_0 = s, a_0 = a \right].
% \end{align*}
\begin{align*}
Q^\pi(s, a) = \mathbb{E}_{\pi}\left[ R(s_{t}, a_{t}) + \gamma G_{t+1} \mid s_t = s, a_t = a \right].
\end{align*}




%The fundamental assumption of an MDP is that the system satisfies the Markov property, indicating that future state transitions depend solely on the current state and not on any past states or actions. 
The MDP model exhibits the following core properties:
\begin{itemize}
    \item \textit{Markov Property:} MDP has the \textit{memoryless} property, indicating that the transition from the current state depends only on the present state and not on past historical states. Therefore, for states $s_t$ and $s_{t+1}$, it holds that:
    \begin{align*}
        P(s_{t+1} \mid s_t, a_t) = P(s_{t+1} \mid s_0, a_0,  \dots, s_{t}, a_t).
    \end{align*}
    
    \item \textit{Bellman Recursive:} For a given MDP with fixed policy \( \pi \), $V^\pi(s)$ and $Q^\pi(s, a)$ satisfies the Bellman equation:
    \begin{align*}
    V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \left[ R(s,a) + \gamma V^\pi(s') \right],
    \end{align*} 
    {\small\begin{align*}
        & Q^\pi(s,a) 
        = \sum_{s'}  P(s'|s,a)\left[ R(s,a) + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a')\right].
    \end{align*}}
    
    \item \textit{Stationary Distribution:} If the state space is finite and the transition matrix is ergodic, then there exists a stationary distribution $\phi(s)$ under deterministic policy $a = \pi(s)$ satisfied
    \begin{align*}
    %\label{eq:stationary-dist}
    \phi(s') = \sum_{s} \phi(s) P(s' \mid s, \pi(s)),
    \end{align*}
    where $\phi(s)$ describes the long-run probability of each state and is an eigenvector of the transition matrix.
\end{itemize}

\subsection{MDP Model for Strategic Mining}
The seminal work on selfish mining attack~\cite{eyal2014majority} considers a system with two types of miners: selfish miner and honest miner. 
Let $\alpha$ denote the fraction of mining power possessed by the selfish miner. 
The block generation process is modeled as a random process, where a new block is generated in each time slot. 
In this scenario, selfish miners maintain a private chain after mining a new block and selectively reveal it when the public chain approaches the length of the private chain, making sure
that
the honest miners waste their computational power on their mined blocks.
Therefore, selfish miners can earn excessive rewards, which can be represented by the following MDP:
%Selfish mining is a strategy that gains unfair rewards by withholding blocks. Selfish miners maintain a private chain after mining a new block and selectively reveal it when the public chain approaches the length of the private chain, causing honest miners to waste computational power. By controlling the block release time, miners increase the likelihood of their private chain being accepted, thus earning excessive rewards.
%Based on block generation, strategic mining in PoW can be modeled as a following MDP:
\begin{itemize}
    \item The state space $(l_a, l_h)$ is used to record the current state of the blockchain, where $l_a$ is the lengths of the private chain, and $l_h$ is the lengths of the public chain.
    \item The action space \{\textit{adopt, override, match, wait}\} represents the possible actions of the selfish miner in each state regarding mining and broadcasting blocks. Specifically, \textit{adopt} indicates abandoning the private chain and switching to mining on the longest chain, while \textit{override} refers to broadcasting two blocks from the private chain. Action \textit{match} represents broadcasting one block from the private chain, and \textit{wait} means not broadcasting and continuing mining.    
    %, which correspond to abandoning the private chain and returning to the public chain, revealing part or all of the private blocks to override the public chain, and continuing to mine the private chain without revealing blocks.
    \item The transition probability of the system is determined by parameter $\alpha$. For example, when a selfish miner chooses the action \textit{wait} in state $(l_a, l_h)$, the next state will be $(l_a + 1, l_h)$ with probability $\alpha$ (the selfish miner mines a new block) or $(l_a, l_h + 1)$ with probability $1 - \alpha$ (the honest miner mines a new block).
    \item The reward of the selfish (honest) miner is the number of blocks accepted by all parties that were mined by the selfish (honest) miner. The goal of the selfish miner is to maximize its proportion of the total reward.
\end{itemize}

%The state space is used to record the current state of the blockchain, such as the lengths of the private chain $l_a$, the public chain $l_h$, and the fork status. In strategic mining, the attacker decides when to hide and release blocks. Therefore, the action space is modeled as \{\textit{adopt, override (match), wait}\}, which correspond to abandoning the private chain and returning to the public chain, revealing part or all of the private blocks to override the public chain, and continuing to mine the private chain without revealing blocks. However, the state transition probability of the system is determined by factors such as the proportion of mining power $\alpha$. For example, when a selfish miner withholds a block, the next state depends on whether the selfish miner successfully mines a new block (with probability \( \alpha \)) or whether an honest miner extends the public chain (with probability \( 1 - \alpha \)).


\subsection{Security Threshold Analysis by MDP}
The security threshold analysis through the Markov Decision Process provides critical insights into blockchain protocol vulnerabilities by quantifying the minimum resource requirements for attackers to profit from strategic mining. 
This methodology systematically models state transitions, reward mechanisms, and strategic interactions, enabling rigorous evaluation of blockchain security boundaries.

\paragraph{Foundational Model on Selfish Mining.}
The seminal work \cite{eyal2014majority} established the theoretical framework for strategic mining by formalizing the \textit{selfish mining} strategy as an Markov Reward Process. 
Their analysis revealed Bitcoin's non-incentive compatibility: attackers with over 25\% hashing power could gain disproportionate rewards by selectively withholding blocks. 
Subsequent research expanded the strategic mining design space through novel attack vectors. 
\cite{nayak2016stubborn} proposed the \textit{stubborn mining} strategy, demonstrating a quarter profit increase over traditional selfish mining by persisting on private chains despite public chain dominance.
By considering this strategy space, the 25\% threshold was further refined by \cite{sapirshtein2016optimal}, who introduced an 
$\epsilon$-optimal algorithm to demonstrate that attackers could exploit vulnerabilities with only 23.2\% computational power, thereby lowering Bitcoin's security boundary.

%\cite{eyal2014majority} pioneered the selfish mining strategy, modeling it as an MDP to reveal the non-incentive compatibility of the Bitcoin protocol. By secretly holding mined blocks and delaying their release, miners can earn disproportionate rewards. When the main chain becomes longer than the private chain, the miner immediately mine on the latest block in main chain.According to the analysis, if honest miners apply the uniform tie-breaking rule, miners with a hashing power greater than 25\% will always gain more benefits than those behaving honestly. \cite{sapirshtein2016optimal} further analyzes the mining strategy and uses an algorithm to find the $\epsilon$-optimal strategy for attackers, thereby decreasing the security threshold to 23.2\%.

%都放在图上，密一些
%\cite{nayak2016stubborn} further extend the strategic space and introduce the \textit{stubborn mining strategy}, where attackers continue to mine on the private chain even when it lags behind the public chain. The results show that stubborn mining can generate up to 25\% more profit compared to selfish mining. Furthermore, by combining selfish mining with network-level attacks, such as Eclipse attacks, the profitability of selfish mining is further increased. Specifically, using the optimal combination of stubborn mining and Eclipse attacks, the attacker's profit can increase by as much as 30\%.

\paragraph{Multi-Attacker Scenarios and Equilibrium Analysis.}
The emergence of multi-miner competition introduced new dimensions to security threshold analysis. 
\cite{liu2018strategy} developed the \textit{publish-n strategy}, reducing stale block rates by 26.3\% compared to selfish mining through deterministic block release patterns. 
Building on this,~\cite{marmolejo2019competing} introduced semi-selfish Mining, which imposed a two-block limit on private chains to lower the security threshold to 26.297\%. Their simulations also revealed an inverse relationship between attacker count and security thresholds. 
Complementing these studies, although solving the MDP game has been proven to be computationally complex ~\cite{deng2023complexity}, ~\cite{zhang2022insightful} designed a mining game model and proved that honest mining remains an equilibrium strategy when attackers' computational power stays below 33\%.

%Research has also been conducted on scenarios where multiple attackers exist in Bitcoin. By proposing the \textit{publish-n strategy}, a deterministic private block release mechanism was introduced in \cite{liu2018strategy} to reduce the stale block rate. The results show that when the attacker's computational power is 10\%, the stale block rate of the publish-n strategy is $26.3\%$ lower than that of selfish mining. Subsequently, \cite{marmolejo2019competing} introduced the Semi-Selfish Mining (SSM) strategy based on the multi-attacker model. In this strategy, the maximum length of a miner's private chain is set to 2, and when the private chain exceeds 2 blocks, the miner immediately publishes the private chain. By limiting the length of the private chain, SSM reduces the stale block rate while lowering the security threshold to $26.297\%$. Furthermore, in a multi-miner environment, the SSM strategy can further reduce the threshold. The article shows that when the number of attackers reaches 8, the profitability threshold of the SSM strategy drops to 11\%. \cite{zhang2022insightful} analyzed the competition among attackers and proved that when computational power is less than 33\%, honest mining constitutes an equilibrium.

\paragraph{Ethereum Analysis and Methodological Advances.}
Ethereum's unique reward mechanism necessitated tailored MDP frameworks.
\cite{feng2019selfish} constructed a two-dimensional MDP incorporating uncle/nephew block rewards, identifying a 26\% security threshold. 
To address Ethereum's nonlinear reward structure,~\cite{zur2020efficient} proposed the Probability Termination Optimization (PTO) method, converting complex MDPs into solvable forms. 
This innovation reduced Ethereum's security threshold to 24.68\%, demonstrating the critical role of reward function design in protocol robustness.

%In Ethereum consensus, \cite{feng2019selfish} models the selfish mining strategy using a two-dimensional Markov process. It specifically addresses the unique uncle and nephew block reward mechanisms in Ethereum. The analysis shows that the security threshold in Ethereum is 26\%. Considering that the reward is the ratio of the rewards earned by miners to their contribution, \cite{zur2020efficient} introduces the Probability Termination Optimization (PTO) method, which transforms an average MDP with a nonlinear objective function into a solvable standard MDP. By applying the PTO method to Ethereum, the security threshold for selfish mining is reduced to 24.68\%.

% \begin{table*}[ht]
% \centering
% \large % 使用较小的字体
% \resizebox{\textwidth}{!}{ % 开始resizebox
%     \begin{tabular}{| m{4cm}<{\centering}  m{3cm}<{\centering}  m{3cm}<{\centering}  m{3cm}<{\centering}  m{5cm} |} % Increase the width of the last column
%         \hline
%         \textbf{Literature} & \textbf{Security threshold} & \textbf{Method } & \textbf{Consensus type} & \textbf{Description} \\ \hline
%         \cite{eyal2014majority} & 0.25 & Markov Reward Process & Proof of Work (Bitcoin) & The Selfish Mining strategy was first introduced, and by modeling it as a Markov reward process, the attack threshold was reduced to $25\%$. \\ \hline
%         \cite{sapirshtein2016optimal} & 0.232 & Markov Reward Process & Proof of Work (Bitcoin)& It was shown that existing strategies (SM1) are not optimal. An efficient algorithm was proposed to compute the optimal selfish mining strategy, further lowering the attack threshold. \\ \hline
%         \cite{marmolejo2019competing} & 0.26297 & Markov Reward Process & Proof of Work (Bitcoin) & The scenario of multiple non-colluding semi-selfish miners was extended, and game theory was used to study the dynamic relationships among miners. \\ \hline
%         \cite{feng2019selfish}  & 0.26 & Two-dimensional Markov Reward Process & Proof of Work (Ethereum)  & A two-dimensional Markov reward process was proposed to model selfish mining in Ethereum, and the corresponding attack threshold was derived. \\ \hline
%         \cite{zur2020efficient}  & 0.2468 & Solvable standard Markov Reward Process & Proof of Work (Ethereum) & An efficient method for solving the Average Reward Ratio (ARR) MDP was proposed, significantly reducing the complexity of computing the optimal selfish mining strategy. \\ \hline
%     \end{tabular}
% } % 结束resizebox
% \caption{Security threshold of different strategic mining methods under Markov Reward Process}
% \label{tab:MDPsec-threshold}
% \end{table*}
