\section{Reinforcement Learning Framework for Analyzing Strategic Mining}
\label{sec:RL}


\begin{table*}[ht]
    \centering
    \large
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|m{4cm}<{\centering} m{2cm}<{\centering} m{3cm}<{\centering} m{2.5cm}<{\centering} m{2cm}<{\centering} m{4.5cm}|}
            \hline
            % 表头
            \textbf{Literature} & 
            \textbf{Security Threshold} & 
            \textbf{Threshold Condition} & 
            \textbf{Method} & 
            \textbf{Consensus Type} & 
            \textbf{Description} \\ 
            \hline
            
            % 第一行数据
            \cite{hou2019squirrl} & 
            0.25 & 
            \textbackslash & 
            Deep Q Network & 
            Bitcoin \& Ethereum PoW & 
            Recover optimal selfish mining strategy in Bitcoin and surpasses the existing selfish mining strategies in Ethereum. \\ 
            \hline
            
            % 第二行数据
            \cite{bar2022werlman} & 
            {\begin{tabular}[b]{@{}c@{}}
                0.20 \\
                0.17 \\
                0.12 \\
            \end{tabular}} & 
            {\begin{tabular}[b]{@{}c@{}}
                3 minting rate halving \\
                4 minting rate halving \\
                5 minting rate halving \\
            \end{tabular}} & 
            Monte Carlo Tree Search \& DQN & 
            Bitcoin PoW & 
            Shows that transaction fee volatility will reduce blockchain security. \\ 
            \hline
            
            % 第三行数据
            \cite{bar2023deep} & 
            {\begin{tabular}[b]{@{}c@{}}
                0.21 \\
                0.19 \\
            \end{tabular}} & 
            {\begin{tabular}[b]{@{}c@{}}
                0.5 petty compliant \\
                0.75 petty compliant \\
            \end{tabular}} & 
            Monte Carlo Tree Search \& DQN & 
            Bitcoin PoW & 
            Shows selfish miners can boost profits by bribing partially compliant miners. \\ 
            \hline
            
            % 第四行数据
            \cite{sarenche2024deep} & 
            0.24198 & 
            \textbackslash & 
            DQN & 
            Longest-Chain Proof of Stake & 
            Uses DQN in LC-PoS protocols and shows that the security threshold for selfish proposing attacks is lower than that selfish mining. \\ 
            \hline
        \end{tabular}%
    }
    \caption{Research on security threshold of blockchain consensus protocols by reinforcement learning methods}
    \label{tab:sec-threshold}
\end{table*}

Through reinforcement learning (RL) technique,
an agent learns optimal strategies via its interactions with 
environment by selecting actions based on the current state and adjusting its behavior based on rewards or penalties. 
Similar
to many important applications~\cite{arulkumaran2017deep,nosratabadi2020data,zhao2022deep}, 
incentive mechanisms ensure security 
of blockchain protocols
by rewarding miners who follow the protocol.
%However, strategic mining  deviates miners from the designed protocol for excess profits。
%often requiring resources such as computational power or stake. 
Reinforcement learning method
helps user to simplify  the complex dynamics of blockchain protocols for better security analysis.
%thresholds and the resources required for strategic mining.




%This section introduce reinforcement learning (RL) and its application in strategic mining, especially in blockchain environments. We first explain the foundational concepts of RL, where agents learn optimal strategies through rewards and penalties, and outlines the two main RL approaches. Then it moves on then explore the use of deep reinforcement learning (DRL) in blockchain, focusing on how incentive mechanisms ensure security by rewarding miners who follow protocols, while strategic mining exploits deviations for excess profit. RL, particularly DRL, helps model the complex dynamics of blockchain protocols and is key to analyzing security thresholds and the resources required for strategic mining.

%This section provides an overview of reinforcement learning (RL) and its application in strategic mining analysis, particularly in blockchain environments. It begins with the theoretical foundations of RL, explaining how agents interact with their environments to learn optimal strategies based on rewards and penalties. The section highlights the two primary approaches to RL and underscores the adaptability of this method in uncertain, dynamic environments.

%The section then transitions into strategic mining analysis through deep reinforcement learning (DRL), focusing on blockchain protocols. It explains how incentive mechanisms in blockchain aim to ensure security by rewarding miners who follow the protocol, while strategic mining behaviors, which deviate from the protocol, can result in excessive profits for miners. The analysis of strategic mining often requires assessing the resources needed, such as computational power or stake proportion, to exploit these behaviors. RL, particularly DRL, plays a crucial role in modeling the complex dynamics of blockchain protocols, where the state space is large and difficult to manage. This makes RL an essential tool for determining security thresholds and understanding the minimum resources required to perform strategic mining.

\subsection{Theoretical Foundations of RL}

%Reinforcement learning is a method that enables an agent to learn optimal strategies through interaction with the environment~\cite{sutton2018reinforcement}.
%The agent selects actions based on the current state and adjusts its behavior by receiving rewards or penalties. %The agent and the environment are the fundamental components of reinforcement learning.
%During the reinforcement learning process, the agent continuously interacts with the environment and receives feedback (rewards or penalties) based on its performance, which guides future decision-making.
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.8\linewidth]{RL.jpg}
%     \caption{Agent and environment}
%     \label{RL}
% \end{figure}
The learning process in RL can be approached through two fundamental methods:

\begin{itemize}
    \item \textbf{Value-based Methods}: These methods indirectly select the optimal policy by calculating the value of each state. 
    For example, Q-learning \cite{watkins1989learning} is a value-based method that learns Q-values (state-action value functions) to evaluate the quality of actions.

    \item \textbf{Policy-based Methods}: These methods directly optimize the policy itself, rather than first learning a value function and then selecting a policy. Policy gradient methods are a common form of this approach.
\end{itemize}

The core objective of reinforcement learning is to maximize cumulative rewards, and this process is typically modeled as a Markov Decision Process~\cite{puterman2014markov}. 

Value-based methods evaluate the long-term return under a given state by a value function. The optimal policy is then derived by maximizing the value function:
\begin{align*}
    \pi^* = \arg \max_{\pi} V_{\pi}(s).
\end{align*}

Policy-based methods directly learn the policy $\Pi(a|s)$, which usually maps states to actions with the policy parameters $\theta$. 
The goal of policy optimization is to maximize the expected return:

\begin{align*}
    J(\theta) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t} \right].
\end{align*}

Many classical algorithms in reinforcement learning, such as Q-learning, Monte Carlo methods, and Temporal Difference (TD) learning, can be viewed as numerical estimation methods for the value function in an MDP.

\begin{itemize}
    \item \textbf{Q-learning}: It is a model-free method to find the optimal policy by updating Q values (state-action values). Although it does not require a model of the environment, it is similar to MDP’s state value function because both aim to optimize the expected future return. 
    \item \textbf{Monte Carlo Methods}: Monte Carlo methods estimate the value function by sampling the environment multiple times. Their computation process closely mirrors the value function calculation in an MDP, with the key difference being that they use actual returns rather than expected returns.
    \item \textbf{Temporal Difference (TD) Learning}: TD learning updates the estimate of the current state by incorporating the immediate reward and the value function of the next state at each step. It combines concepts from dynamic programming and the Bellman equation in an MDP.
\end{itemize}

\subsection{Strategic Mining Analysis through RL}

%Blockchain protocols rely on incentive mechanisms to ensure security by rewarding miners who follow the protocol. However, strategic mining encourages miners to deviate from the protocol for excess profits, often requiring resources like computational power or stake proportion. To determine the security threshold of these protocols, scholars analyze the minimum resources needed for strategic mining. Reinforcement learning is a crucial tool in these analyses, helping manage the complexity and large state spaces inherent in blockchain protocol models.

%Blockchain protocols typically rely on incentive mechanisms to ensure the security of the blockchain. Miners who honestly execute the protocol are rewarded in return, while strategic mining behaviors encourage miners to deviate from the protocol to gain excess profits. Executing strategic mining usually requires miners to possess certain resources, such as computational power or stake proportion. Currently, many scholars determine the security threshold of blockchain protocols by analyzing the minimum resources required to execute strategic mining. In these analytical frameworks, reinforcement learning is an important technical tool, largely due to the complexity of blockchain protocols, which results in an excessively large state space after modeling.

Building on the foundational concepts of RL in modeling strategic mining behaviors, this subsection delves into the application of RL-based frameworks to analyze and optimize mining strategies in blockchain protocols.
Specifically, we explore how RL has been employed to develop analytical frameworks, estimate advanced security thresholds, and devise novel attack strategies and countermeasures. 
These studies highlight the versatility of RL in capturing the complex dynamics of strategic mining, offering insights into both the vulnerabilities and potential mitigations within blockchain systems.
The key findings are summarized in Table~\ref{tab:sec-threshold}.

\paragraph{RL-Based Analytical Frameworks.}
%Reinforcement learning has been widely used to analyze and optimize strategic mining behaviors. A notable contribution is SquirRL \cite{hou2019squirrl}, an RL-based framework that models blockchain incentive mechanisms by defining agent capabilities, action spaces, and reward functions. 
%This framework identified optimal selfish mining attacks in Bitcoin and uncovered a novel attack on Ethereum's Casper FFG protocol. 
%Furthermore, \cite{wang2021blockchain} introduced a dynamic RL approach that adapts mining strategies to changing network conditions, without relying on prior knowledge of MDP parameters, providing a flexible alternative to traditional analytical models.

Several studies have leveraged reinforcement learning to analyze and optimize strategic mining behaviors. SquirRL is one of these pioneering works. This analytical framework proposed in~\cite{hou2019squirrl} utilizes RL to analyze blockchain's incentive mechanisms. It defines agent capabilities and action spaces, creates a simulation environment, and incorporates elements such as agent extraction, RL algorithm selection, and reward function design. 
In turn, it successfully identified the optimal selfish mining attack in Bitcoin and discovered a novel attack on Ethereum’s Casper FFG protocol.
Another work~\cite{wang2021blockchain} explored the feasibility to dynamically learn optimal strategic mining approaches. Unlike conventional analytical models that require explicit parameter extraction, their approach employs RL to observe the blockchain network and consensus protocol, adapting mining strategies to time-varying network conditions without relying on prior knowledge of MDP parameters.

%To analyze attacks on blockchain incentive mechanisms, \cite{hou2019squirrl} proposed an analytical framework based on deep reinforcement learning called SquirRL. In this framework, they characterized the capabilities and action spaces of agents, created a simulation environment, and designed the extraction of the number and types of agents, the selection of reinforcement learning algorithms, and the construction of reward functions. Hou et al. applied SquirRL to various blockchain incentive mechanisms, obtaining the optimal selfish mining attack in Bitcoin and a novel attack on Ethereum's Casper FFG protocol.

%Generally, strategic mining can be modeled as a Markov Decision Process (MDP), and solving this process can yield the optimal mining strategy. However, this requires detailed parameter extraction of the blockchain network and protocol characteristics, which is not straightforward in practice.  
%%%%
%\cite{wang2021blockchain} used reinforcement learning to dynamically learn an optimal strategic mining approach by observing the network and consensus protocol. Even without knowing the specific parameters of the MDP model, they were able to derive the optimal mining strategy in a time-varying blockchain network through reinforcement learning.

\paragraph{Advanced Security Threshold Estimation.}
Building on the application of RL, \cite{bar2022werlman} introduced WeRLman, an RL framework that incorporates "whales" (high-value transactions) and variance reduction techniques to more accurately estimate security thresholds. 
By using variance reduction to mitigate high sampling noise and optimizing strategies with Monte Carlo Tree Search, the framework determined Bitcoin’s security threshold to be approximately 25\% (which decreases over time) and Ethereum’s to be around 17\%. 
Expanding on this work, \cite{bar2023deep} extended WeRLman to explore the impact of small miners on blockchain security. 
They assumed a mix of compliant small miners and honest miners, and found that selfish miners could exploit weakened attack defenses, increasing their profits by over 10\%. 
In the Bitcoin scenario, when half of the miners were small and compliant, the security threshold dropped from 25\% to 21\%.

%\cite{bar2022werlman} proposed the WeRLman framework based on reinforcement learning to analyze the impact of strategic mining protocols. WeRLman defines the concept of "whales," which are blocks with high-value rewards, and incorporates these blocks into the modeling. The framework introduces variance reduction techniques to mitigate the impact of high sampling noise on model accuracy and uses Monte Carlo Tree Search for optimization. Roi Bar-Zur et al. used WeRLman to analyze the incentive mechanisms for miners in different scenarios, concluding that the security threshold for the Bitcoin platform is around 0.25 and will gradually decrease over time, while the threshold for the Ethereum platform is around 0.17.

%Subsequently,  \cite{bar2023deep} extended WeRLman to study the impact of small miners on the security threshold. They used deep reinforcement learning to model the mining process as an MDP, aiming to maximize the miners' profits from a rational miner's perspective. Due to the complexity of the model, the study employed the deep RL framework WeRLman to approximate the optimal strategy. They assumed that only a portion of the miners are small and compliant, while the rest are honest. The results showed that even if only a portion of non-rational miners are small and compliant, selfish miners can still benefit from weakened attacks, with profit increases potentially exceeding 10\%, while also causing a decrease in the blockchain's security threshold. In the Bitcoin scenario, assuming half of the miners are small and compliant, the security threshold drops from 0.25 to 0.21.

\paragraph{Alternative RL-Based Attacks and Countermeasures.}
Reinforcement learning has enabled novel attack strategies beyond selfish mining. \cite{yang2020ipbsm} introduced an intelligent bribery-based selfish mining attack, using RL to optimize strategies and outperform traditional models in profitability and equity thresholds. 
By modeling the environment as an MDP and leveraging RL-based decision-making, their approach surpassed traditional selfish mining models in terms of equity threshold and profitability.
Similarly, \cite{jeyasheela2024q} applied machine learning to predict miner rewards with high accuracy (MSE: 0.0032) and used Q-learning to simulate selfish mining behaviors.
Their $\epsilon$-greedy value iteration approach improved attacker profitability while offering insights into countermeasures. These studies highlight RL's dual role in advancing attacks and defenses in blockchain systems.

%Beyond selfish mining, reinforcement learning has also been applied to novel attack strategies. For instance, \cite{yang2020ipbsm} proposed an intelligent bribery-based selfish mining (IPBSM) attack, assuming all miners are rational while the attacker optimizes strategies via reinforcement learning. By modeling the environment as an MDP and leveraging RL-based decision-making, their approach surpassed traditional selfish mining models (SM1 and SM2) in terms of equity threshold and profitability.Meanwhile, \cite{jeyasheela2024q} adopted machine learning and deep learning techniques to predict miner rewards in Bitcoin mining pools and devise countermeasures against selfish mining. They successfully predicted relative miner rewards with high accuracy (achieving a mean squared error of 0.0032) and further employed a Q-learning model to simulate selfish miner behaviors. By optimizing strategies using $\epsilon$-greedy value iteration, their study significantly enhanced the profitability of selfish miners while providing insights into mitigating such attacks.

%\cite{yang2020ipbsm} proposed a novel bribery selfish mining attack, assuming all miners are rational and the attacker is intelligent, capable of optimizing strategies through reinforcement learning. The attacker interacts with the external environment, selects optimal strategies using reinforcement learning, and models the external environment as a Markov decision process to support reinforcement learning. Their experimental results demonstrate that this approach outperforms SM1 and SM2 in terms of equity threshold and profitability.

%\cite{jeyasheela2024q} focused on predicting miner rewards in Bitcoin mining pools and developing countermeasures against selfish mining attacks. They successfully predicted miners' relative rewards using machine learning and deep learning models, with the deep learning model achieving a low mean squared error of 0.0032 and a mean absolute error of 0.0464. Additionally, they employed a Q-learning model to simulate the behavior of selfish miners, optimizing strategies through $\epsilon$-greedy value iteration, which significantly enhanced the relative rewards of selfish miners.

\paragraph{RL in Proof-of-Stake Blockchains.}
While strategic mining attacks have traditionally been associated with Proof-of-Work (PoW) blockchains, the longest chain rule is also employed in some Proof-of-Stake (PoS) protocols, where mining is replaced by proposer elections. 
This shift introduces a new attack vector, known as selfish proposing, which is analogous to selfish mining. \cite{sarenche2024deep} investigated selfish proposing attacks in longest chain PoS (LC-PoS) blockchains, analyzing how attackers exploit the "nothing-at-stake" problem and proposer predictability. 
The study found that the "nothing-at-stake" phenomenon slightly increases the proportion of blocks proposed by attackers, while the predictability of proposers significantly increases the proportion of attack blocks.
To analyze selfish proposing attacks in more complex scenarios, they also used deep Q-learning tools to approximate the optimal attack strategies under different stake shares.

%Although the longest chain rule was initially designed for Proof-of-Work (PoW) protocols, some Proof-of-Stake (PoS) blockchain protocols have also adopted this consensus mechanism. However, PoS does not involve a mining process; instead, blocks are generated by electing proposers. Selfish mining attacks are a significant threat to longest chain blockchain protocols, affecting not only PoW blockchains but also PoS blockchains that follow the longest chain paradigm. Since PoS blockchains do not involve mining, the term "selfish proposing" is used instead of "selfish mining" to describe such attacks. \cite{sarenche2024deep} explored selfish proposing attacks in longest chain PoS (LC-PoS) blockchains. By combining "nothing-at-stake" selfish proposing attacks with different levels of predictability, the study found that the "nothing-at-stake" phenomenon slightly increases the proportion of blocks proposed by attackers, while the predictability of proposers significantly increases the proportion of attack blocks. To analyze selfish proposing attacks in more complex scenarios, Sarenche et al. used deep Q-learning tools to approximate the optimal attack strategies under different stake shares.

% \begin{table*}[htbp]
% \centering
% \small % 使用较小的字体
% \begin{tabular}{|>{\raggedright\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{3cm}|}
% \hline
% \textbf{Literature} & \textbf{Security threshold} & \textbf{Method } & \textbf{Consensus type} \\ \hline
% SquirRL \cite{hou2019squirrl} & 0.25 & Reinforcement Learning & Proof of Work \\ \hline
% weRLman \cite{bar2022werlman} & 0.2 in 10 years; \newline0.17 in 20 years & Reinforcement Learning & Proof of Work \\ \hline
% DeepBribe \cite{bar2023deep}  & 0.21 when 0.5 petty compliant;\newline0.19 when 0.75 petty compliant & Reinforcement Learning & Proof of Work  \\ \hline
% LC-POS \cite{sarenche2024deep}  & 0.24198 & Reinforcement Learning & Proof of Stake  \\ \hline

% \end{tabular}
% \caption{Security threshold of Different strategic mining Methods}
% \label{tab:sec-threshold}
% \end{table*}

% \begin{table*}[ht]
%     \centering
%     \large
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{|m{2cm}<{\centering}|m{4cm}<{\centering}|m{3cm}<{\centering}|m{3cm}<{\centering}|m{8cm}|} % Increase the width of the last column
%         \hline
%         \textbf{Literature} & \textbf{Security Threshold} & \textbf{Method} & \textbf{Consensus Type} & \textbf{Description} \\ \hline
%         SquirRL \cite{hou2019squirrl} & 0.25 & Deep Q-Learning & Proof of Work  (Bitcoin \& Ethereum) & The study successfully recovers the optimal selfish mining strategy in Bitcoin and surpasses existing selfish mining strategies in Ethereum. \\ \hline
%         WeRLman \cite{bar2022werlman} & decrease to 0.20 in 10 years; 0.17 in 20 years; 0.12 in 30 years & Monte Carlo Tree Search \& Deep Q-Learning & Proof of Work (Bitcoin) & The study presents empirical evidence on the adverse effects of transaction fee volatility on blockchain security, revealing that increased variability in transaction fees leads to a substantial reduction in the security threshold of blockchain protocols. \\ \hline
%         DeepBribe \cite{bar2023deep} & 0.21 when 0.5 petty compliant; 0.19 when 0.75 petty compliant & Monte Carlo Tree Search \& Deep Q-Learning & Proof of Work (Bitcoin) &  The research results demonstrate that even if only a portion of the miners are petty compliant, selfish miners can increase their profits by bribing these miners, thereby leading to a reduction in the blockchain's security threshold. \\ \hline
%         Deep Selfish Proposing \cite{sarenche2024deep} & 0.24198 & Deep Q-Learning & Longest-Chain Proof of Stake & The study uses Deep Q-Learning to show that in LC-PoS protocols with perfect randomness, the profitability threshold for selfish proposing attacks is lower than for selfish mining in PoW. \\ \hline
%         \end{tabular}}
%     \caption{Security threshold of Different strategic mining Methods}
%     \label{tab:sec-threshold}
% \end{table*}

% \begin{table*}[ht]
%     \centering
%     \large
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{|m{2cm}<{\centering}|m{2cm}<{\centering}|m{3cm}<{\centering}|m{3cm}<{\centering}|m{3cm}<{\centering}|m{8cm}|} 
%         \hline
%         \textbf{Literature} & \textbf{Security Threshold} & \textbf{Threshold Condition} & \textbf{Method} & \textbf{Consensus Type} & \textbf{Description}  \\ \hline

%         SquirRL \cite{hou2019squirrl} & 0.25 & \textbackslash & Deep Q-Learning & Proof of Work  (Bitcoin \& Ethereum) & The study successfully recovers the optimal selfish mining strategy in Bitcoin and surpasses the existing selfish mining strategies in Ethereum. \\ \hline
        
%         WeRLman \cite{bar2022werlman} & {\begin{tabular}{@{}c@{}}
%     0.20  \\
%     0.17  \\
%     0.12  \\
%     \end{tabular}} & {\begin{tabular}{@{}c@{}}
%     10 years later  \\
%     20 years later  \\
%     30 years later  \\
%     \end{tabular}} &  Monte Carlo Tree Search \& Deep Q-Learning & Proof of Work (Bitcoin) & The study shows that transaction fee volatility reduces blockchain security.  \\ \hline

%         DeepBribe \cite{bar2023deep} & 
%         {\begin{tabular}{@{}c@{}}
%     0.21  \\
%     0.19  \\
%     \end{tabular}} &
%         {\begin{tabular}{@{}c@{}}
%     0.5 petty compliant   \\
%     0.75 petty compliant \\
%     \end{tabular}} & Monte Carlo Tree Search \& Deep Q-Learning & Proof of Work (Bitcoin) &  The research shows that selfish miners can boost profits by bribing partially compliant miners, reducing the blockchain's security threshold. \\ \hline

%     Deep Selfish Proposing \cite{sarenche2024deep} & 0.24198 & \textbackslash & Deep Q-Learning & Longest-Chain Proof of Stake & The study shows using Deep Q-Learning that in LC-PoS protocols, the profitability threshold for selfish proposing attacks is lower than for selfish mining in PoW. \\ \hline
%     \end{tabular}}
%     \caption{Security threshold of Different strategic mining Methods}
%     \label{tab:sec-threshold}
% \end{table*}


