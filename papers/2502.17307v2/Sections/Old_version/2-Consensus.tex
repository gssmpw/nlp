\section{Consensus and Strategic Mining}
% intro里面的一些拿过来？

\subsection{SM MDP}

Formally, if all other participants are following the standard protocol, the attacker faces a single-player decision problem of the form $M := <S, A, P, R>$, where $S$ is the state space, $A$ the action space, $P$ a stochastic transition matrix that describes the probability of transitioning between states, and $R$ the reward matrix. Though similar in structure, we do not regard $M$ as an MDP, since the objective function is nonlinear: The player aims to maximize its share of the accepted blocks, rather than the absolute number of its own accepted ones; its goal is to have a greater return-on-investment than its counterparts

\textbf{Action space}
Actions. We begin with the description of the action space A, which will motivate the nontrivial construction of the state space.
\begin{itemize}
    \item Adopt. The action \emph{adopt} is always feasible, and represents the attacker’s acceptance of the honest network’s chain. The $a$ blocks in the attacker’s current chain are discarded.
    \item Override. The action \emph{override} represents the publication of the attacker’s blocks, and is feasible whenever $a > h$.
    \item Match. This action represents the case where the most recent block was built by the honest network, and the attacker now publishes a conflicting block of the same height. This action is not always feasible (the attacker must have a block prepared in advance to execute such a race). The statespace explicitly encodes the feasibility status of this action (see below).
    \item Wait. Lastly, the wait action, which is always feasible, implies that the attacker does not publish new blocks, but keeps working on its branch until a new block is built.
\end{itemize}

\textbf{State space}
The state space, denoted $S$, is defined by 3-tuples of the form $(a, h, fork)$. The first two entries represent the lengths of the attacker’s chain and the honest network’s chain, built after the latest fork (that is, above the most recent block accepted by all). The field fork obtains three values, dubbed irrelevant, relevant and active. State of the form $(a, h, relevant)$ means that the previous state was of the form $(a, h - 1, \cdot)$; this implies that if $a \geq h$, match is feasible. Conversely, $(a, h, irrelevant)$ denotes the case where the previous state was $(a - 1, h, \cdot)$, rendering match now ineffective, as all honest nodes received already the $h$’th block. The third label, active, represents the case where the honest network is already split, due to a previous match action; this information affects the transition to the next state, as described below. We will refer to states as $(a, h)$ or $(a, h, \cdot)$, in contexts where the fork label plays no effective role.

\textbf{Reward}
In order to keep the time averaging of rewards in scale, every state transition corresponds to the creation of a new block. The initial state $X_0$ is $(1, 0, irrelevant)$ w.p. $\alpha$ or $(0, 1, irrelevant)$ w.p. $(1 - \alpha)$. Rewards are given as elements in $N^2$, where the first entry represents blocks of the attacker that have been accepted by all parties, and the second one, similarly, for those of the honest network.

The transition matrix $P$ and reward matrix $R$ are succinctly described in Table. Largely, an adopt action “resets” the game, hence the state following
it has the same distribution as $X_0$; its immediate reward is $h$ in the  coordinate corresponding to the honest network. An override reduces the attacker’s secret chain by $h+1$ blocks, which it publishes, and which the honest network accepts. This bestows a reward of $h + 1$ blocks to the attacker. The state following a match action depends on whether the next block is created by the attacker $(\alpha)$, by honest nodes working on their branch of the chain $((1 - \gamma) \cdot (1 - \alpha))$, or by an honest node which accepted the sub-chain that the attacker published $(\gamma \cdot (1-\alpha))$. In the latter case, the attacker has effectively overridden the honest network’s previous chain, and is awarded $h$ accordingly.

\begin{table}[h]
\label{Table1}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{State × Action}                    & \textbf{State}                    & \textbf{Probability}          & \textbf{Reward}  \\ \hline
$(a,h,\cdot), \text{adopt}$                & $(1,0,\text{irrelevant})$         & $\alpha$                      & $(0,h)$          \\ \cline{2-4} 
                                           & $(0,1,\text{irrelevant})$         & $1-\alpha$                    & $(0,h)$          \\ \hline
$(a,h,\cdot), \text{override}^\dagger$     & $(a-h,0,\text{irrelevant})$       & $\alpha$                      & $(h+1,0)$        \\ \cline{2-4} 
                                           & $(a-h-1,1,\text{relevant})$       & $1-\alpha$                    & $(h+1,0)$        \\ \hline
$(a,h,\text{irrelevant}), \text{wait}$     & $(a+1,h,\text{irrelevant})$       & $\alpha$                      & $(0,0)$          \\ \hline
$(a,h,\text{relevant}), \text{wait}$       & $(a,h+1,\text{relevant})$         & $1-\alpha$                    & $(0,0)$          \\ \hline
$(a,h,\text{active}), \text{wait}$         & $(a+1,h,\text{active})$           & $(\gamma) \cdot (1-\alpha)$   & $(0,0)$          \\ \hline
$(a,h,\text{relevant}), \text{match}^\ddagger$ & $(a-h,1,\text{relevant})$        & $\gamma \cdot (1-\alpha)$     & $(h,0)$          \\ \cline{2-4} 
                                           & $(a,h+1,\text{relevant})$         & $(1-\gamma) \cdot (1-\alpha)$ & $(0,0)$          \\ \hline
\end{tabular}
\caption{A description of the transition and reward matrices $P$ and $R$ in the decision problem $M$. The third column contains the probability of transiting from the state specified in the left-most column, under the action specified therein, to the state on the second one. The corresponding two-dimensional reward (the reward of the attacker and that of the honest nodes) is specified on the right-most column.}
\footnotesize{$^\dagger$feasible only when $a \geq h$}
\footnotesize{$^\ddagger$feasible only when $a \geq h$}
\end{table}

\textbf{Objective Function}
As explained in the introduction, the attacker aims to maximize its relative revenue, rather than its absolute one as usual in MDPs. Let $\pi$ be a policy of the player; we will write $\pi(a, h, fork)$ for the action that $\pi$ dictates be taken at state $(a, h, fork)$. Denote by $X^{\pi}_t$ the state visited by time $t$ under $\pi$, and let $r(x, y, \pi) = (r1(x, y, \pi), r2(x, y, \pi))$ be the immediate reward from transiting from state x to state y, under the action dictated by $\pi$. $X^{\pi}_t$ will denote the $t$’th state that was visited. We will abbreviate $r_t(X^\pi_t ,X^\pi_{t+1}, \pi)$ and write simply $r_t(\pi)$ or even $r_t$, when context is clear. The objective function of the player is its relative payoff, defined by
\begin{align}
    REV := \mathcal{E} \left[ \liminf_{T \rightarrow \infty} \frac{\sum_{t=1}^T r_t^1(\pi)}{\sum_{t=1}^T(r_t^1(\pi)+r_t^2(\pi))} \right]    
\end{align}
We will specify the parameters of REV depending on the context (e.g.,
$REV(\pi, \alpha, \gamma), REV(\pi), REV(\alpha))$, and will occasionally denote the value of $REV$ by $\rho$. In addition, for full definiteness of $REV$, we rule out pathological behaviours in which the attacker waits forever—formally, the expected time for the next non-null action of the attacker must be finite.