\section{Methodology}

\subsection{Markov Reward Process}

A Markov chain is a stochastic process that evolves through a sequence of states, where the probability of transitioning to the next state depends only on the current state (i.e., it has memoryless property). A Markov chain is defined by a finite or countably infinite set of states, transition probabilities between states, and an initial probability distribution over the states. 

\begin{definition}
    A Markov chain is a stochastic process $(X_t)_{t=0}^{\infty}$ with the following:
    \begin{itemize}
        \item A countable or finite state space $\{s_0, s_1, ... ,s_n\} \in S$.
        \item A transition matrix $P = (p_{ij})$, where $p_{ij}$ represents the probability of transitioning from state $s_i$ to state $s_j$.
        \item Markov property:
        $P(X_{N+1} = s_i | X_0 = s_{t_0}, X_1 = s_{t_1}, \ldots, X_{N} = s_{t_N}) = P(X_{N+1} = s_i | X_N = s_{t_N})$
    \end{itemize}
\end{definition}

%Markov chains 在区块链共识建模中有广泛的应用。 

\subsection{Reinforcement Learning}

Reinforcement learning is a class of machine learning algorithms that enables an agent to learn to make decisions or take actions in an environment to maximize a cumulative reward.
The agent learns through trial and error, receiving feedback from the environment through rewards or punishments.
Deep reinforcement learning combines reinforcement learning with deep learning techniques, using neural networks to approximate the value function or policy of the agent.
It is a suitable method to solve complex Markov Reward Process problems in which the environment responds reward to the agent's actions according to the transition probability.

There has been growing interest in applying reinforcement learning techniques, including deep reinforcement learning, to blockchain research in recent years.
%Paper description

\subsection{Security Threshold}

%define of Security Threshold


\subsection{MDP Model}
Formally, if all other participants are following the standard protocol, the attacker faces a single-player decision problem of the form $M := <S, A, P, R>$, where $S$ is the state space, $A$ the action space, $P$ a stochastic transition matrix that describes the probability of transitioning between states, and $R$ the reward matrix. Though similar in structure, we do not regard $M$ as an MDP, since the objective function is nonlinear: The player aims to maximize its share of the accepted blocks, rather than the absolute number of its own accepted ones; its goal is to have a greater return-on-investment than its counterparts

\textbf{Action space}
Actions. We begin with the description of the action space A, which will motivate the nontrivial construction of the state space.
\begin{itemize}
    \item Adopt. The action \emph{adopt} is always feasible, and represents the attacker’s acceptance of the honest network’s chain. The $a$ blocks in the attacker’s current chain are discarded.
    \item Override. The action \emph{override} represents the publication of the attacker’s blocks, and is feasible whenever $a > h$.
    \item Match. This action represents the case where the most recent block was built by the honest network, and the attacker now publishes a conflicting block of the same height. This action is not always feasible (the attacker must have a block prepared in advance to execute such a race). The statespace explicitly encodes the feasibility status of this action (see below).
    \item Wait. Lastly, the wait action, which is always feasible, implies that the attacker does not publish new blocks, but keeps working on its branch until a new block is built.
\end{itemize}

\textbf{State space}
The state space, denoted $S$, is defined by 3-tuples of the form $(a, h, fork)$. The first two entries represent the lengths of the attacker’s chain and the honest network’s chain, built after the latest fork (that is, above the most recent block accepted by all). The field fork obtains three values, dubbed irrelevant, relevant and active. State of the form $(a, h, relevant)$ means that the previous state was of the form $(a, h - 1, \cdot)$; this implies that if $a \geq h$, match is feasible. Conversely, $(a, h, irrelevant)$ denotes the case where the previous state was $(a - 1, h, \cdot)$, rendering match now ineffective, as all honest nodes received already the $h$’th block. The third label, active, represents the case where the honest network is already split, due to a previous match action; this information affects the transition to the next state, as described below. We will refer to states as $(a, h)$ or $(a, h, \cdot)$, in contexts where the fork label plays no effective role.

\textbf{Reward}
In order to keep the time averaging of rewards in scale, every state transition corresponds to the creation of a new block. The initial state $X_0$ is $(1, 0, irrelevant)$ w.p. $\alpha$ or $(0, 1, irrelevant)$ w.p. $(1 - \alpha)$. Rewards are given as elements in $N^2$, where the first entry represents blocks of the attacker that have been accepted by all parties, and the second one, similarly, for those of the honest network.

The transition matrix $P$ and reward matrix $R$ are succinctly described in Table. Largely, an adopt action “resets” the game, hence the state following
it has the same distribution as $X_0$; its immediate reward is $h$ in the  coordinate corresponding to the honest network. An override reduces the attacker’s secret chain by $h+1$ blocks, which it publishes, and which the honest network accepts. This bestows a reward of $h + 1$ blocks to the attacker. The state following a match action depends on whether the next block is created by the attacker $(\alpha)$, by honest nodes working on their branch of the chain $((1 - \gamma) \cdot (1 - \alpha))$, or by an honest node which accepted the sub-chain that the attacker published $(\gamma \cdot (1-\alpha))$. In the latter case, the attacker has effectively overridden the honest network’s previous chain, and is awarded $h$ accordingly.

\textbf{Objective Function}
As explained in the introduction, the attacker aims to maximize its relative revenue, rather than its absolute one as usual in MDPs. Let $\pi$ be a policy of the player; we will write $\pi(a, h, fork)$ for the action that $\pi$ dictates be taken at state $(a, h, fork)$. Denote by $X^{\pi}_t$ the state visited by time $t$ under $\pi$, and let $r(x, y, \pi) = (r1(x, y, \pi), r2(x, y, \pi))$ be the immediate reward from transiting from state x to state y, under the action dictated by $\pi$. $X^{\pi}_t$ will denote the $t$’th state that was visited. We will abbreviate $r_t(X^\pi_t ,X^\pi_{t+1}, \pi)$ and write simply $r_t(\pi)$ or even $r_t$, when context is clear. The objective function of the player is its relative payoff, defined by
$$
    REV := \mathcal{E} \left[ \liminf_{T \rightarrow \infty} \frac{\sum_{t=1}^T r_t^1(\pi)}{\sum_{t=1}^T(r_t^1(\pi)+r_t^2(\pi))} \right]    
$$
We will specify the parameters of REV depending on the context (e.g.,
$REV(\pi, \alpha, \gamma), REV(\pi), REV(\alpha))$, and will occasionally denote the value of $REV$ by $\rho$. In addition, for full definiteness of $REV$, we rule out pathological behaviours in which the attacker waits forever—formally, the expected time for the next non-null action of the attacker must be finite.