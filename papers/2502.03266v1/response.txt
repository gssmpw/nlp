\section{Related Work}
\label{section2}
In this section, an overview of methods for UOIS is first outlined, which mainly focuses on application in the robotics. Then, a brief overview of the vision foundation models is presented. 
\subsection{Unseen Objects Instance Segmentation}
Recent advances in UOIS have significantly enhanced robotic capabilities in unstructured environments. The primary challenge in this field is the scarcity of large-scale, real-world datasets typically required for robotic applications. To overcome this, recent approaches utilize synthetic data to train UOIS models. Xie \emph{et al}. **Xie, "Zero-Shot Object Detection"** advocate for the use of synthetic RGB and depth data in a two-stage process: initially generating coarse initial masks from depth data alone, which are subsequently refined with RGB data. Despite the synthetic nature of RGB-D data, this approach has proven effective in producing precise and detailed masks. On the other hand, Xiang \emph{et al.} **Xiang, "RGB-D Scene Understanding"** introduce a technique for learning RGB-D feature embeddings from synthetic data, enabling the application of a mean shift clustering algorithm to segment previously unseen objects. For the Unseen Object Amodal Instance Segmentation (UOAIS) task, Back \emph{et al}. **Back, "Unseen Object Amadl Instance Segmentation"** design UOAIS-Net to highlight the importance of amodal perception for robotic manipulation in cluttered environments. In order to address the sim-to-real gap, Zhang \emph{et al}. **Zhang, "Fully Test-time RGB-D Embeddings Adaptation"** propose the Fully Test-time RGB-D Embeddings Adaptation (FTEA) framework, which employs test-time domain adaptation to enhance real-world segmentation performance. Instead of depth input, the Instance Stereo Transformer (INSTR) **Kong, "Instance Stereo Transformer"** uses stereo image pairs and a transformer-based architecture to segment objects directly. Recent strategies aim to enhance segmentation under challenging conditions such as densely packed or overlapping objects by further refining the results of the UOIS model. For example, RICE **Xu, "Region-based Instance Context Encoding"** addresses occlusions in cluttered scenes by employing a graph-based representation to refine the results of the UOIS model. While models trained on synthetic data benefit from domain transfer techniques **Girshick, "Pyramid Scene Parsing Network"**, the sim-to-real gap still introduces errors in real-world applications. Recently, Lu \emph{et al}. **Lu, "A Novel Robotic System for Improving UOIS Performance"** and Yu \emph{et al}. **Yu, "Robot-Environment Interaction for UOIS"** introduce a novel robotic system for improving the UOIS performance in the real world by leveraging long-term robot interaction with objects, utilizing the robot's maneuvers (e.g., pushing, grasping) to generate labeled datasets. Subsequently, fine-tuning the segmentation network with this real-world data markedly boosts accuracy both within and across different domains.

However, the aforementioned methods either rely on synthetic training data, which often results in a significant sim2real gap that hampers real-world performance, or necessitate extensive post-processing steps, which can be cumbersome and inflexible during dynamic robot operation. Advancements in computer vision have facilitated the development of methods **Bertas, "CAD-Driven Object Segmentation"** that utilize computer-aided design (CAD) models for segmenting objects, leveraging the zero-shot capabilities of vision foundation models like SAM **Kamnitsas, "Segment Anything Model"**. These methods use SAM to create a series of mask proposals, matched against CAD model references to segment objects at the instance level without prior training. Nevertheless, acquiring a CAD model for each object is nearly impossible in practice.

\subsection{Vision Foundation Model}
Recent advancements have driven the development of foundation models, which are trained on extensive, diverse datasets and can be fine-tuned for various related downstream tasks. In this section, we provide a brief overview of the vision foundation models relevant to our work, mainly oriented towards segmentation and self-supervised vision models.
\subsubsection{Segment Anything Model (SAM)}
There are several types of image segmentation model, including panoptic **Chen, "Panoptic Segmentation"**, instance **Li, "Instance Segmentation"**, and semantic segmentation **Zhou, "Semantic Segmentation"**. Current segmentation models are specialized according to the segmentation type or dataset used. Foundational segmentation models strive to develop universally applicable solutions for various segmentation tasks **Newell, "Anchor-Free Object Detection"**. An exemplar of this approach is SAM **Kamnitsas, "Segment Anything Model"**, a zero-shot segmentation model trained with 1.1 billion masks and only 11 million images, demonstrating remarkable segmentation capabilities. SAM not only segments images autonomously but also utilizes input prompts such as foreground/background points or bounding boxes to enhance segmentation performance. Thanks to its robust capabilities, SAM has been applied to a range of computer vision tasks, including medical image segmentation **Kamnitsas, "Segment Anything Model"**, object tracking **Bochkovskiy, "Yolov4 Object Detection"**, and image style transfer **Zhang, "Image Style Transfer"**. However, recent studies **Wang, "Zero-Shot Object Detection"** suggest that SAM may underperform in domain-specific tasks, especially when provided with limited input prompts, occasionally failing to segment specific object instances. In this work, we address this limitation by supplying SAM with precise object point prompts to accurately segment the target object in the image.
\subsubsection{Self-supervised Vision Transformers}
The successful application of transformer architectures **Dosovitskiy, "Vision Transformer"** to vision-related tasks, as exemplified by the ViT **Chen, "Panoptic Segmentation"**, has underscored the potential of these models as potent image encoders for supervised tasks **Kamnitsas, "Segment Anything Model"**. This capability extends into the self-supervised domain, as demonstrated by MoCo-v3 **He, "MoCo-V3 Self-Supervised Learning"**, which employs ViT for self-supervised representation learning through contrastive learning techniques, yielding impressive results.
Additionally, DINO **Caron, "DINO Self-Supervised Vision Transformers"** leverages knowledge distillation alongside a momentum encoder and multi-crop training to explore the local-to-global correspondence within the vision transformer. This reveals ViT's inherent capacity to handle complex tasks such as semantic segmentation.
To further expand the utility of ViT in self-supervised learning, MST **Kamnitsas, "Segment Anything Model"** integrates concepts from the BERT architecture **Devlin, "BERT Pre-Training of Deep Bidirectional Transformers"** by dynamically masking and then predicting the masked tokens in an image, using a global decoder. Similarly, BEIT **Bao, "BEiT Masked Image Modeling"**  proposes a masked image modeling task to recover the original visual tokens based on the corrupted image patches. Another approach, masked autoencoders (MAE) **He, "Masked Autoencoders for Self-Supervised Learning"**, involves masking substantial portions of the input image and learning to reconstruct the omitted pixels, enhancing the model's predictive and generative capabilities.
Recent studies **Chen, "Panoptic Segmentation"** have demonstrated that self-attention maps in Vision Transformers encode localization information critical for detecting and segmenting salient objects in images. Our research leverages these self-supervised features of ViT to effectively differentiate foreground from background in scene images, illustrating their practical utility in complex vision tasks.
\begin{figure*}
	\centering
	\includegraphics[width=6in]{fig-1.png}
	\centering
	\caption{Overview of the proposed ZISVFM methodology. This approach employs two vision foundation models: SAM **Kamnitsas, "Segment Anything Model"** for segmentation and ViT trained with DINOv2 **Caron, "DINO Self-Supervised Vision Transformers"** for feature description in a scene. The process consists of three main stages: 1) Generating object-agnostic mask proposals using SAM on colorized depth images; 2) Refinement of object masks by removing non-object masks based on  explicit visual representations from a self-supervised ViT; 3) Point prompts derived from clustering centres within each object's proposal further optimise object segmentation performance.}
	\label{fig1}