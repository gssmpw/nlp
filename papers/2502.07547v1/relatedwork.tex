\section{Related Work}
IES is closely related to multiple active machine learning research areas. We review key studies in these fields, underscoring IES's distinct features. 

\emph{Sample Selection} has been widely used to improve the efficiency and robustness of deep learning model training. The main idea is to assign higher probabilities to examples to be trained that are \emph{informative} \citep{alain2015variance, katharopoulos2017biased, katharopoulos2018not}, \emph{unique} \citep{loshchilov2015online, chang2017active, shi2021diversity} or \emph{confident} \citep{khim2020uniform}. Related associated distillation and selection algorithms usually incur additional costs. Static selection typically requires preliminary calculations before training or in the early stages of training, with related studies including \emph{Data Pruning} \citep{toneva2018empirical, paul2021deep, killamsetty2021glister} and \emph{Core Set} \citep{huggins2016coresets, huang2018epsilon, braverman2022power, xia2022moderate, xia2024refined}, etc., with the goal of finding a small subset from all training data that can represent the entire dataset. 
Dynamic selection usually involves selecting instances across training process, with related studies including \emph{Dynamic Data Pruning} \citep{raju2021accelerating, mindermann2022prioritized, he2023large, truong2023kakurenbo, qin2023infobatch} and \emph{Importance Sampling} \citep{alain2015variance, katharopoulos2017biased, katharopoulos2018not, JMLR:v19:16-241, jiang2019accelerating}, etc., aimed at focusing training on more informative or confident examples. 
In the context of deep learning, several methods have been proposed based on different measures of sample ``informative'', such as gradient norm \citep{alain2015variance, killamsetty2021grad}, loss value \citep{loshchilov2015online, schaul2015prioritized, mindermann2022prioritized}, and prediction uncertainty \citep{chang2017active}. 
Notably, when the gradient of an instance converges to zero, it means that the model's parameters will \textcolor{changecolor}{be} insignificant updated based on this particular sample. However, even with efficient gradient computation methods \citep{wei2017minimal, katharopoulos2017biased, katharopoulos2018not}, the computational cost of calculating the gradient of each sample based on backpropagation is still high, which hinders the goal of reducing the computational cost of every single run. \emph{Curriculum Learning}
 \citep{bengio2009curriculum, wu2021curricula, zhou2020curriculum, wang2024efficienttrain++, wang2024computation, kumar2010self} is a learning paradigm that aims to improve the efficiency and effectiveness of training by presenting examples in a meaningful order, typically from easy to hard. Several methods have been proposed based on different measures of example difficulty \citep{weinshall2018curriculum, saxena2019data, jiang2018mentornet}. IES method can be viewed as a curriculum learning method design for end of training, focusing on the model's mastery of instances.

Although IES and existing sample selection techniques share the common goal of improving training progression via training on a selected subset of training instances, our method distinguishes itself through its focus on whether ``the model has already fully learned an instance'', i.e., \emph{mastered}. This unique perspective allows IES to adaptively adjust the proportion of instances participating in training at different stages, thereby eliminating the need for pre-set training schedules or removal rates.