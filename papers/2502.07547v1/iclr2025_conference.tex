\documentclass{article} %
\usepackage{iclr2025_conference,times}
\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts    
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{graphicx}     
\usepackage{times}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{caption}
\usepackage{stfloats}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{bbm}
\usepackage{wrapfig}\definecolor{mydarkblue}{rgb}{0.082,0.376,0.510}
\usepackage{wrapfig}\definecolor{darkred}{rgb}{0.6,0,0}
\usepackage[colorlinks,
            linkcolor = darkred,
            urlcolor  = darkred, 
            citecolor= mydarkblue,
            ]{hyperref}
\usepackage[labelformat=parens]{subcaption}
\definecolor{changecolor}{RGB}{0, 0, 0}

\title{Instance-dependent Early Stopping}


 \iclrfinalcopy

\author{Suqin Yuan\textsuperscript{1} \quad Runqi Lin\textsuperscript{1} \quad Lei Feng\textsuperscript{2}\footnotemark[1] \quad Bo Han\textsuperscript{3} \quad  Tongliang Liu\textsuperscript{1}\thanks{Corresponding authors.} \\
\textsuperscript{1} Sydney AI Centre, The University of Sydney\\
\textsuperscript{2} Southeast University 
\textsuperscript{3} Hong Kong Baptist University
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}


\maketitle

\begin{abstract}
In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that \emph{once the model has mastered an instance, the training on it should stop}. 
IES considers an instance as \emph{mastered} if the second-order differences of its loss value remain within a small range around zero.
This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding \emph{mastered} instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. 
Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10\%-50\% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model. 
Our implementation can be found at \url{https://github.com/tmllab/2025_ICLR_IES}.
\end{abstract}

\section{Introduction}
Early stopping is a straightforward technique that regulates model training and reduces computational costs by halting the training process when no further improvements are observed in model performance on the validation set \citep{prechelt2002early,raskutti2014early,caruana2000overfitting,yuan2024early}. Specifically, this method terminates training at the appropriate moment, preventing excessive training while conserving computational resources \citep{zhang2021understanding, belkin2019reconciling, nakkiran2021deep} and reduces the reliance on other computationally intensive regularization methods in model training \citep{tibshirani1996regression, hoerl1970ridge, goodfellow2016deep}. The growing size and complexity of models and datasets make these benefits increasingly critical, as they lead to significantly rising computational costs associated with training advanced models \citep{hestness2017deep, kaplan2020scaling, brown2020language, sorscher2022beyond, li2023loftq, gong2024cascast}. In practice, ending training when satisfactory performance is achieved is more practical than pursuing complete convergence, as the cost of complete convergence is excessively high and may not yield evident improvements in performance \citep{rice2020overfitting, yang2020rethinking, sagawa2020investigation}.

Despite the widespread acclaim for the elegance and practicality of the conventional early stopping method, which focuses on the model's performance on the validation set and simultaneously terminates the optimization across the entire training set, this approach lacks flexibility. It does not consider that the model learns different instances at varying rates and stages \citep{zhang2021understanding, arpit2017closer, toneva2018empirical, wen2022benign}. Consequently, this can lead to redundant computations, as the model may continue processing instances that are already well-learned until it finally achieves satisfactory performance across the entire dataset. To further enhance the efficiency of early stopping, we propose the \emph{Instance-dependent Early Stopping} (IES) method, which refines the idea of early stopping from the entire training dataset to the instance level.



\begin{figure}[t]
\vskip -0.1in
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=6.4cm]{IESfig1.pdf} 
\end{minipage}
\begin{minipage}{0.49\textwidth} 
    \vskip +0.05in
    \centering
    \caption{Effectiveness of \emph{Instance-dependent Early Stopping} (IES) on ImageNet-1k and CIFAR-10 datasets. Top row: Test accuracy over the course of training, showing that IES (Ours) achieves comparable accuracy to the baseline (No Removal) despite training on fewer samples. Bottom row: Number of training samples excluded from backpropagation by IES over the course of training. As the model masters more and more samples during the training process, IES allows an increasing number of these \emph{mastered} samples to be excluded from backpropagation, significantly reducing computation while still maintaining the same performance as the baseline method.}
    \label{fig1}
\end{minipage}
\vskip -0.35in
\end{figure}


The principle of our IES method is simple yet effective: \emph{once the model masters an instance, the training on it should stop}. By enabling the model to dynamically stop the training for individual instances once satisfactory performance is achieved for those specific instances, IES can perform early stopping in a more fine-grained manner. To instantiate the concept of \emph{mastered}, we need a computational efficiency quantitative criterion that can be applied uniformly across all instances. A natural idea is to use the loss value of instances, which has been shown effective for identifying important instances for optimization \citep{loshchilov2015online, jiang2019accelerating, qin2023infobatch}. However, due to the differences in optimal loss values across instances arising from factors such as sample complexity \citep{hacohen2019power, wang2020optimizing}, inherent ambiguity \citep{guo2017calibration, liang2017enhancing}, noise \citep{zhang2021understanding, jiang2018mentornet}, and imbalance \citep{cui2019class, cao2019learning}, it may be suboptimal for determining whether an instance has been \emph{mastered}.

In this paper, we propose to use the second-order difference of an instance's loss values \( \Delta^2 L_i(w^{(t)}) \) across consecutive training epochs as the \emph{mastered} criterion. If, over \(k\) epochs, the sum of the absolute values of \( \Delta^2 L_i(w^{(t)}) \) for an instance \(i\) is confined to a small neighborhood around \(0\), it signifies that the change in the loss tends to be flat and insensitive to parameter updates. 
Compared with the loss values, the second-order differences of these values for the training data have a lower coefficient of variation in later training stages (Figure \ref{fignew}). This indicates that the second-order difference loss values of \emph{mastered} instances consistently fall within a small range, regardless of the actual loss values of these instances.
This consistency allows us to set a unified threshold based on the second-order difference values to determine whether an instance has been \emph{mastered}. 
Moreover, the proposed criterion is computationally efficient, relying solely on forward propagation.

As shown in Figure \ref{fig1}, as model training progresses and more instances are \emph{mastered}, the IES method allows an adaptive decrease in the number of training instances from backpropagation. This results in significant savings in overall training time and computational costs while obtaining models with comparable performance to the one that is trained using all data. Specifically, the effectiveness of the IES method in accelerating model training progression can be attributed to its ability to allow the model to focus on instances that are not yet \emph{mastered}, which typically have larger gradient norms, thereby speeding up the reduction of the training loss through more effective parameter updates. By effectively identifying and skipping the redundant instances that have already been well-learned and would not significantly contribute to further model performance improvement in the next few epochs, IES achieves comparable results to full-data training. Moreover, by avoiding repeated training on already \emph{mastered} instances, the IES method avoids \emph{Over-Memorization} \citep{ishida2020we, zhang2021understanding, wen2024sharpness, linover, lin2024layer, lin2024eliminating} and enables the model to more rapidly reduce the sharpness of the loss landscape \citep{dauphin2014identifying, foret2020sharpness}.

To assess the effectiveness of the IES method, we carried out extensive experiments across various settings. Our findings reveal that IES consistently delivers substantial computational savings in CIFAR and ImageNet-1k tasks, reducing the number of instances that require backpropagation by 10\% to 50\% without sacrificing model performance. In many cases, IES even slightly enhances the model's generalization performance and improves transferability to downstream tasks. Specifically, fine-tuning models pretrained with IES on ImageNet-1k for the CIFAR and Caltech-101 datasets results in average improvements of 1.5\%, compared with models pretrained without IES. Through ablation studies and comparative analysis, we demonstrate that IES outperforms existing samples selection methods and demonstrates robust adaptiveness in hyperparameter selection.


Our main contributions can be summarized as follows:
\begin{itemize}[leftmargin=0.4cm,topsep=-2pt]
\item [1.]
We propose \emph{Instance-dependent Early Stopping} (IES), a method that adaptively stops training at the instance level, allowing for the saving of computational resources while maintaining performance.
\item [2.]
We introduce a \emph{mastered} criterion based on the second-order differences of sample loss values, providing an unified measure to determine whether a model has fully learned a given instance.
\item [3.]
We analyze the mechanism behind IES's effectiveness, revealing that it allows the model to focus on instances with larger gradient norms and reduces the sharpness of loss landscape more rapidly.
\end{itemize}

\section{Related Work}
IES is closely related to multiple active machine learning research areas. We review key studies in these fields, underscoring IES's distinct features. 

\emph{Sample Selection} has been widely used to improve the efficiency and robustness of deep learning model training. The main idea is to assign higher probabilities to examples to be trained that are \emph{informative} \citep{alain2015variance, katharopoulos2017biased, katharopoulos2018not}, \emph{unique} \citep{loshchilov2015online, chang2017active, shi2021diversity} or \emph{confident} \citep{khim2020uniform}. Related associated distillation and selection algorithms usually incur additional costs. Static selection typically requires preliminary calculations before training or in the early stages of training, with related studies including \emph{Data Pruning} \citep{toneva2018empirical, paul2021deep, killamsetty2021glister} and \emph{Core Set} \citep{huggins2016coresets, huang2018epsilon, braverman2022power, xia2022moderate, xia2024refined}, etc., with the goal of finding a small subset from all training data that can represent the entire dataset. 
Dynamic selection usually involves selecting instances across training process, with related studies including \emph{Dynamic Data Pruning} \citep{raju2021accelerating, mindermann2022prioritized, he2023large, truong2023kakurenbo, qin2023infobatch} and \emph{Importance Sampling} \citep{alain2015variance, katharopoulos2017biased, katharopoulos2018not, JMLR:v19:16-241, jiang2019accelerating}, etc., aimed at focusing training on more informative or confident examples. 
In the context of deep learning, several methods have been proposed based on different measures of sample ``informative'', such as gradient norm \citep{alain2015variance, killamsetty2021grad}, loss value \citep{loshchilov2015online, schaul2015prioritized, mindermann2022prioritized}, and prediction uncertainty \citep{chang2017active}. 
Notably, when the gradient of an instance converges to zero, it means that the model's parameters will \textcolor{changecolor}{be} insignificant updated based on this particular sample. However, even with efficient gradient computation methods \citep{wei2017minimal, katharopoulos2017biased, katharopoulos2018not}, the computational cost of calculating the gradient of each sample based on backpropagation is still high, which hinders the goal of reducing the computational cost of every single run. \emph{Curriculum Learning}
 \citep{bengio2009curriculum, wu2021curricula, zhou2020curriculum, wang2024efficienttrain++, wang2024computation, kumar2010self} is a learning paradigm that aims to improve the efficiency and effectiveness of training by presenting examples in a meaningful order, typically from easy to hard. Several methods have been proposed based on different measures of example difficulty \citep{weinshall2018curriculum, saxena2019data, jiang2018mentornet}. IES method can be viewed as a curriculum learning method design for end of training, focusing on the model's mastery of instances.

Although IES and existing sample selection techniques share the common goal of improving training progression via training on a selected subset of training instances, our method distinguishes itself through its focus on whether ``the model has already fully learned an instance'', i.e., \emph{mastered}. This unique perspective allows IES to adaptively adjust the proportion of instances participating in training at different stages, thereby eliminating the need for pre-set training schedules or removal rates.


\section{Methodology}

To refine the advantages of early stopping to the instance level, we proposed a simple principle that, \emph{once the model masters an instance, the training on it should stop.} 
To operationalize this idea, we introduce a criterion for identifying instances that the model has been \emph{mastered}, as detailed in Section~\ref{sec3.1}. Building on this foundation, we propose \emph{Instance-dependent Early Stopping} (IES) to promote model training progression, as shown in Section~\ref{sec3.2}. Furthermore, we demonstrate the efficiency and effectiveness of the IES method, as discussed in Section~\ref{sec3.3}. All toy experiments presented in this section use a standard ResNet-18 backbone trained on the CIFAR-10 dataset. For detailed experiment settings, please refer to Section~\ref{sec4} and Appendix \ref{appa} and \ref{appb6}.

\textbf{Preliminaries.}
- The \emph{Hessian matrix}, $H = \nabla^2 L(w)$, characterizes the loss function's curvature by its eigenvalues and eigenvectors: $H = Q\Lambda Q^T = \sum_{i=1}^{n} \lambda_i q_i q_i^T$. $Q$ is an orthogonal matrix of eigenvectors, and $\Lambda$ is a diagonal matrix of eigenvalues $\lambda_i$, describing the curvature in various directions. Higher eigenvalues imply steeper curvatures, complicating optimization \citep{li2017stochastic}.

- $\nabla$ represents the gradient operator, for example, $\nabla L(w)$ represents the gradient of the loss function $L$ at the parameter $w$.
$\Delta$ represents the difference operator, $\Delta^2 L_i(w^{(t)})$ represents the second-order difference of the loss function for sample $i$ over three consecutive time steps $t$, $t-1$, and $t-2$.

\begin{figure*}[b]
 \vskip -0.25in
 \begin{subfigure}[b]{0.31\textwidth}
     \includegraphics[width=\textwidth]{N0.png}
     \vskip -0.1in
     \caption{\emph{$N = 0$}}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.31\textwidth}
     \includegraphics[width=\textwidth]{N1.png}
     \vskip -0.1in
     \caption{\emph{$N = 1$}}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.31\textwidth}
     \includegraphics[width=\textwidth]{N2.png}
     \vskip -0.1in
     \caption{\emph{$N = 2$}}
 \end{subfigure}
 \vskip -0.05in
\caption{The curves show the number of instances that meet the corresponding mastered criteria (N = \{0, 1, 2\}, $\delta = 1e^{-4}$) as the training epochs progress, under two scenarios: excluding the mastered instances from backpropagation and allowing the mastered instances to participate in backpropagation. The proximity of the curves suggests that the model can maintain its ``mastered'' on the mastered instances without the need for actively repeated training on them.}
 \label{figure2}
\end{figure*}

\subsection{The Mastered Criterion}
\label{sec3.1}
Previous studies have shown that different instances contain varying information and have inconsistent impacts on model learning at different training stages \citep{zhang2021understanding, arpit2017closer, toneva2018empirical, huang2022harnessing, huang2024winning, litowards, hong2024improving}. This suggests that if certain instances have been well-learned by the model early in the training process, their contribution to model performance improvement may diminish or even become redundant as training progresses. To apply the idea of early stopping at the instance level and improve training efficiency, we need a simple and computationally efficient method to assess the model's learning status on each sample and identify these redundant instances that would not significantly contribute to further model performance improvement in the next few epochs, which we refer to as \emph{mastered} instances. 

To efficiently identify which and when an instance is \emph{mastered}, we construct a criterion based on the $N$-th order difference of sample loss, which only relies on forward propagation. 
Intuitively, the loss of a \emph{mastered} instance should be relatively stable. Specifically, when an instance $i$ is well fitted by the current model parameters $w^{(t)}$ or is insensitive to their recent update, the associated loss $L_i(w^{(t)})$ will be small or reach a plateau, and thus the $N$-th order difference of the loss will approach zero.
To formalize this, when the $N$-th order difference of the loss for sample $i$ falls beneath a specified small positive threshold $\delta$, sample $i$ is considered to be \emph{mastered} by the model parameters $w$ and state $t$, which can be expressed as:
{\setlength{\abovedisplayskip}{-5pt}
\setlength{\belowdisplayskip}{+1pt}
\begin{equation}
\Delta^N L_i(w^{(t)}) < \delta, N = \{0, 1, 2, ...\}.
\end{equation}}

To demonstrate the effectiveness of the \emph{mastered} criteria, we experimentally tracked the number of instances that meet the \emph{mastered} criteria during the training process. As shown in Figure \ref{figure2}, the \emph{mastered} criteria enable adaptive sample selection throughout the learning process, allowing the model to dynamically adjust the size of the training set participating in backpropagation according to the evolving requirements. During the initial stages of training, the model has scarcely learned any instances, so the \emph{mastered} criteria retain most instances for backpropagation, as almost every sample can provide useful information. As training progresses, the model gradually masters more instances, leading to an adaptive decrease in the number of retained training instances commensurate with the training progress. The \emph{mastered} criteria adaptively remove these fully learned redundant instances from backpropagation, enabling the model to focus on the remaining samples. Compared to methods that dynamically sample a fixed proportion of important instances \citep{raju2021accelerating, mindermann2022prioritized, he2023large, qin2023infobatch}, stopping training on \emph{mastered} instances, provides a more adaptive and efficient approach to instance selection based on the model's learning progress.

It is noteworthy that the number of the model \emph{mastered} instances remains nearly the same regardless of whether the instances satisfying the \emph{mastered} criteria continue to participate in backpropagation or not, as shown in Figure \ref{figure2}. This observation, which is particularly evident for $N=1$ and $N=2$, suggests that the model can maintain its learned state on the \emph{mastered} instances even without repeatedly training on them. The \emph{mastered} criterion thus provides an effective way to identify redundant instances during training, allowing the model to exclude them from backpropagation with minimal impact on model performance on these instances.





\subsection{Instance-dependent Early Stopping (IES)}
\label{sec3.2}

Building upon the \emph{mastered} criterion, we propose the \emph{Instance-dependent Early Stopping} (IES) method, allowing the model to \emph{stop training on an instance once it has been mastered}.
Although using the loss value of instances (i.e., $N=0$) has been widely adopted as a method to identify important instances for current optimization \citep{loshchilov2015online, jiang2019accelerating}, different instances may have different optimal loss values $L_i(w^*)$ due to factors such as sample complexity \citep{hacohen2019power, wang2020optimizing}, noise \citep{zhang2021understanding, jiang2018mentornet}, and imbalance \citep{cui2019class, cao2019learning}. This poses a challenge in simply using loss value to construct mastered criterion for IES. If the mastered criterion were to directly depend on the absolute loss value, it would require setting different thresholds for each sample, which can be impractical and  expensive in large-scale datasets.
In this work, we use the second-order difference to identify the mastered instances, which quantifies the rate of change in the loss for sample \(i\) across three consecutive epochs, \(t^{th}\), \((t-1)^{th}\), and \((t-2)^{th}\) training epochs. The second-order difference is defined as:
{\setlength{\abovedisplayskip}{-0pt}
\setlength{\belowdisplayskip}{-0pt}
\begin{equation}
\textcolor{changecolor}{
\begin{aligned}
\Delta^2 L_i(w^{(t)}) &= [L_i(w^{(t)}) - L_i(w^{(t-1)})] - [L_i(w^{(t-1)}) - L_i(w^{(t-2)})] \\
&= L_i(w^{(t)}) - 2L_i(w^{(t-1)}) + L_i(w^{(t-2)}).
\end{aligned}
}
\end{equation}}

\begin{wrapfigure}{r}{0.33\textwidth}
\vskip -0.12in
  \centering
  \includegraphics[width=0.32\textwidth]{Unknown-77-rebuttal_iclr.png}
  \vskip -0.12in
  \caption{\textcolor{changecolor}{Coefficient of variation (CV) of different orders of loss differences during training.}}
\vskip -0.15in
\label{fignew}
\end{wrapfigure}
By quantifying the rate of change in the loss for each instance around the current parameters $w^{(t)}$, the second-order difference effectively captures the stability of the loss function, regardless of the specific value of $L_i(w^{*})$. 
This property allows for using a unified threshold $\delta$ across all instances, greatly simplifying the implementation and management of the mastered criterion.
To further validate this advantage, we conducted experiments as shown in Figure \ref{fignew}. We calculate the zero-order (loss value), first-order, and second-order differences for each sample's loss during training on CIFAR-10. Subsequently, we computed the coefficient of variation (CV) for these differences to represent the degree of dispersion in the data.
Experimental results show that when $N=1, 2, 3$, the CV of their high-order differences of loss value exhibits a trend of first rising and then falling, eventually stabilizing at a lower level. 
This indicates that in the early stages of training, when only some samples are sufficiently learned, there is significant variability in the higher-order differences of loss value among different samples. As training progresses into the mid-to-late stages, and as most instances become sufficiently learned, all instances exhibit more similar values in their higher-order differences of loss value.
Therefore, we can use a fixed threshold to uniformly determine whether an instance has been \emph{mastered}. 
Further experiments confirm that the high-order difference has relatively smaller CV values, please refer to Appendix \ref{appc}. In our subsequent experiments (Section \ref{4.2}), we further evaluate the IES method under different criteria. 

Accordingly, based on the above analysis and experimental validation, an instance \(i\) is considered \emph{mastered} when the cumulative magnitude of these second-order differences of loss value falls beneath a specified small positive threshold \(\delta\), which is formally expressed as:
{\setlength{\abovedisplayskip}{+2pt}
\setlength{\belowdisplayskip}{+0pt}
\begin{equation}
\label{eq4}
 \left| \Delta^2 L_i(w^{(t)}) \right| < \delta.
\end{equation}}

 Ultimately, IES consists of two key stages: filtering out mastered instances in the \textcolor{changecolor}{full training-set $\mathcal{D}^{(0)}$} through forward propagation, and removing mastered instances and only optimizing not-yet mastered instances \(\mathcal{D}^{(t)}\) through backpropagation, as detailed in Algorithm \ref{alg1}.

\begin{figure}[t]
\vspace{-1em}
\begin{algorithm}[H]
\caption{Instance-dependent Early Stopping (IES)}
\begin{algorithmic}[1]
\REQUIRE \textcolor{changecolor}{Full training-set $\mathcal{D}^{(0)}$}, validation-set $\mathcal{V}$, model $f_{\theta}$, threshold $\delta$, max epochs $T$
\STATE Initialize model parameters $w^{(0)}$
\FOR{$t = 1$ to $T$}
    \STATE Forward pass on model $f$ to compute loss $L_i(w^{(t)})$ for each sample $i \in {\color{changecolor}\mathcal{D}^{(0)}}$
    \STATE Calculate second-order differences: $\Delta^2 L_i(w^{(t)}) = L_i(w^{(t)}) - 2L_i(w^{(t-1)}) + L_i(w^{(t-2)})$
    \STATE Identify \emph{mastered} instances: $ \mathcal{M}^{(t)} = \left\{i \in \textcolor{changecolor}{\mathcal{D}^{(0)}}:  \left| \Delta^2 L_i(w^{(t')}) \right| < \delta \right\}$
    \STATE Update dataset for next epoch: $\mathcal{D}^{(t)} = \textcolor{changecolor}{\mathcal{D}^{(0)}} \setminus \mathcal{M}^{(t)}$
    \IF{$\mathcal{D}^{(t)}$ is empty \OR conventional early stopping criterion($\mathcal{V}, w^{(t)}$)}
        \STATE Break \COMMENT{Stop if all instances \emph{mastered} or conventional early stopping triggered}
    \ENDIF
    \STATE Update model parameters $w^{(t)}$ using instances in $\mathcal{D}^{(t)}$
\ENDFOR
\end{algorithmic}
\label{alg1}
\end{algorithm}
\vspace{-2.5em}
\end{figure}


\subsection{Instance-dependent Stopping to Accelerate Model Training Progression}
\label{sec3.3}


The proposed Instance-dependent Early Stopping (IES) method significantly reduces computational costs, as shown in its twofold impact: (1) IES achieves comparable performance to the baseline while requiring fewer backpropagation instances, and (2) IES surpasses the baseline's performance with the same amount of backpropagation. This subsection presents experimental results and analysis to showcase the IES's effectiveness and effective in accelerating model training progression. A detailed analysis demonstrating that IES does not lead to catastrophic forgetting is provided in Appendix~\ref{apph}.

\textbf{Less backpropagation, similar performance.} Our proposed method achieve comparable performance to the baseline method while using fewer instances in backpropagation. As detailed in Section \ref{sec3.1}, the effectiveness of using fewer instances in backpropagation without compromising performance is achieved through the precise identification of mastered instances. \textcolor{changecolor}{As shown in Figure \ref{fig3}, our method reduces the number of training instances in backpropagation by approximately 40\%}, resulting in a savings of nearly 30\% in total computational cost while maintaining generalization performance.


\begin{figure*}[h!]
 \vskip -0.1in
 \begin{subfigure}[b]{0.48\textwidth}
     \includegraphics[width=\textwidth]{ies-sgd.png}
     \caption{\emph{SGD - 150 Epochs}}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.482\textwidth}
     \includegraphics[width=\textwidth]{ies-adam.png}
     \caption{\emph{Adam - 150 Epochs}}
 \end{subfigure}
 \vskip -0.05in
\caption{Comparison of model performance metrics between the IES method and the baseline method over the same number of backpropagation training instances. The metrics include test error, gradient norm, training loss, sharpness-aware minimization (SAM) value, and the maximum eigenvalue of the Hessian matrix. IES consistently outperforms the baseline in test error and reduces training loss, SAM value, and the maximum eigenvalue more effectively, indicating a faster progression in model training. We use ResNet-18 on the CIFAR-10 dataset in this experiment. Further detailed experimental settings can be found in Appendix \ref{appb} and Section \ref{sec4}.}
 \label{fig3}
\end{figure*}

\textbf{Same backpropagation, better performance.}
Our proposed method consistently achieves better performance than the baseline method at the same number of backpropagation instances. 
To further demonstrate the superiority of our method, we conduct analysis from following experiments. 

\emph{Larger Gradient Norms.}
As shown in Figure \ref{fig3}, the average mini-batch gradient norm for instances selected by the IES method \textcolor{changecolor}{is} consistently higher than that of the baseline method, which uses all training data. By typically selecting instances with larger gradient norms for backpropagation, the IES method tends to make parameter updates more effective at reducing training loss.

\emph{Faster Reduce Sharpness.}
We conducted experiments to investigate the changes of model's sharpness during the training process. Sharpness \citep{hochreiter1997flat, pmlr-v162-andriushchenko22a, huang2023robust} often refers to the steepness of the loss function near the solution and is closely related to the large eigenvalues of the Hessian matrix \citep{dinh2017sharp, tsuzuku2020normalized}. We compared the changes in the largest eigenvalue of the Hessian matrix and the Sharpness-Aware Minimization (SAM) value \citep{foret2020sharpness}. 
As shown in Figure \ref{fig3}, IES reduces the largest eigenvalue more quickly and consistently achieves lower SAM values compared with the baseline method. The faster reduction in the largest eigenvalue suggests that IES can more targetedly reduce steepness in these sharp directions of the loss landscape, thereby reducing the overall ``sharpnes'' more quickly. A lower SAM value indicates a flatter minima with lower sharpness. These empirical observations together support that  IES can more targetedly reduce steepness in these sharp directions of the loss landscape \citep{li2017stochastic, keskar2016large, neyshabur2017exploring, dauphin2014identifying}, thereby reducing the overall sharpness more quickly.
\section{Experiments}
\label{sec4}
In this section, we empirically demonstrate the effectiveness of \emph{Instance-dependent Early Stopping} method.
In Section \ref{4.1}, we validate the broad applicability of our proposed method across different settings. 
Furthermore, we demonstrate through experiments that applying our proposed method can improve the transferability of models. 
In Section \ref{4.2}, we compare our proposed IES method with other methods and different instance-level stopping criteria. We showcase the capability of our method to maintain model performance across a wide range of hyperparameters.
Section \ref{4.3} demonstrates our method's applicability for more efficient training and its use in high-level tasks such as segmentation and detection.
The empirical evidence indicates that our proposed Instance-dependent Early Stopping method can effectively reduce computational overhead under various settings, outperforming existing baselines while simultaneously enhancing the transfer learning capabilities of models. 


\subsection{Effectiveness of IES}
\label{4.1}
To evaluate the effectiveness of our proposed IES method, we conducted extensive experiments under various settings, including different datasets, network architectures, and optimizers. Table \ref{tab 1} and \ref{tab 2} demonstrate the consistent performance of the IES method across these settings. It is worth noting that IES achieves lossless acceleration for model training; if a $1\%$ generalization performance decrease is acceptable, even more substantial acceleration can be obtained, as detailed in Section \ref{4.2}. 

\begin{table*}[t]
\centering
	\caption{Effectiveness of IES-2nd across various settings. (5 runs, mean±std)  }
 \vskip -0.05in
	\label{tab 1}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{0.6mm}{
\begin{tabular}{c|ccc|ccc}
\toprule
&\multicolumn{3}{c|}{\textbf{CIFAR-10}} & \multicolumn{3}{c}{\textbf{CIFAR-100}}\\
\cmidrule(lr){1-1}\cmidrule(lr){2-4}\cmidrule(lr){5-7}
\textbf{\emph{Architectures}}&ResNet-18& ResNet-50 & VGG-16 & ResNet-34 & ResNet-101 & DenseNet-121  \\
\midrule
No Removal  & 92.9\%$\pm$0.1\% & 93.3\%$\pm$0.1\% & 90.9\%$\pm$0.2\% & 69.8\%$\pm$0.4\% & 71.9\%$\pm$0.5\% & 73.4\%$\pm$0.0\% \\
IES (Ours) & 92.9\%$\pm$0.2\% & 93.1\%$\pm$0.1\% & 90.7\%$\pm$0.2\% & 69.6\%$\pm$0.3\% & 72.2\%$\pm$0.5\% & 73.3\%$\pm$0.2\% \\
\midrule
Mini-batch Saved  & 54.6\% & 48.5\% & 30.4\% & 29.9\% & 28.3\% & 33.4\%  \\
\midrule
\midrule
\textbf{\emph{Optimizers}}&SGD(F)& SGD(L) & AdamW &SGD(F)& SGD(E) & AdamW  \\
\midrule
No Removal  & 92.1\%$\pm$0.1\% & 95.2\%$\pm$0.1\% & 92.6\%$\pm$0.1\% & 71.4\%$\pm$0.5\% & 77.6\%$\pm$0.4\% & 69.6\%$\pm$0.3\% \\
IES (Ours) & 92.4\%$\pm$0.0\% & 95.1\%$\pm$0.1\% & 92.7\%$\pm$0.1\% & 72.3\%$\pm$0.4\% & 77.4\%$\pm$0.4\% & 69.7\%$\pm$0.5\% \\
\midrule
Mini-batch Saved   & 37.3\% & 26.4\% & 47.2\% & 9.3\% & 27.3\% & 17.4\%  \\
\midrule
\midrule
Avg. Mini-batch Saved  &  & \textbf{40.7\%}  & & & \textbf{24.3\%} &   \\
\midrule
Avg. Wall-time Speedup  &  & \boldmath{$\sim1.4\times$} & & & \boldmath{$\sim1.2\times$} &   \\

\bottomrule  
\end{tabular}
}
}
\vskip -0.05in
\end{table*}

\begin{table*}[t!]
\vskip -0.02in
\centering
	\caption{Effectiveness of IES-2nd in ImageNet-1k task. (1 run) }
 \vskip -0.05in
	\label{tab 2}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{5.8mm}{
\begin{tabular}{c|cccc}
\toprule
& \multicolumn{1}{c}{\emph{DenseNet-121}} & \multicolumn{2}{c}{\emph{ResNet-34}} & \multicolumn{1}{c}{\emph{ResNet-101}}\\
\cmidrule(lr){2-2}\cmidrule(lr){3-4}\cmidrule(lr){5-5}
 Methods & AdamW & AdamW & SGD(M) & SGD(E)  \\
\midrule
No Removal    & 69.0\% & 68.0\%     & 74.1\%     & 77.4\%   \\
IES (Ours)    & 68.8\% & 68.0\%     & 74.3\%     & 77.4\%   \\
\midrule
Mini-batch Saved  & 31.6\%  & 28.7\%   & 30.3\%   & 34.2\%      \\
\midrule
\midrule
Avg. Wall-time Speedup   &  \multicolumn{4}{c}{\boldmath{$\sim1.3\times$}}  \\
\bottomrule  
\end{tabular}
}
}
\vskip -0.05in
\end{table*}


\begin{table*}[t!]
\vskip -0.02in
\centering
	\caption{Transfer performance of IES-2nd pretrained model on ImageNet-1k task. (5 runs, mean±std) }
 \vskip -0.05in
	\label{tab3}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{4mm}{
\begin{tabular}{c|cc|cc}
\toprule
& \multicolumn{2}{c|}{\emph{ResNet-101}} & \multicolumn{2}{c}{\emph{DenseNet-121}}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
  Transfer Tasks & IES (Ours) & No Removal & IES (Ours) & No Removal \\
\midrule
\emph{ImageNet-1k --> CIFAR-10} & \textbf{81.2\%$\pm$0.1\%} & 80.3\%$\pm$0.2\% & \textbf{78.6\% $\pm$ 0.2\%} & 77.3\% $\pm$ 0.2\% \\
\midrule
\emph{ImageNet-1k --> CIFAR-100} & \textbf{57.5\%$\pm$0.2\%} & 55.6\%$\pm$0.2\% & \textbf{53.0\% $\pm$ 0.2\%} & 52.3\% $\pm$ 0.2\% \\
\midrule
\emph{ImageNet-1k --> Caltech-101} & \textbf{59.9\%$\pm$0.8\%} & 57.4\%$\pm$1.2\% & \textbf{50.9\% $\pm$ 1.6\%} & 49.5\% $\pm$ 1.5\% \\
\bottomrule  
\end{tabular}
}
}
\vskip -0.2in
\end{table*}


Our evaluations confirmed the effectiveness of the IES method across multiple datasets. These datasets comprise \emph{CIFAR-10}, \emph{CIFAR-100} \citep{krizhevsky2009learning}, and \emph{ImageNet-1k} \citep{deng2009imagenet}. For the \emph{CIFAR} and \textcolor{changecolor}{the} \emph{ImageNet-1k} tasks, we train for 200 and 150 epochs, respectively. For \emph{ImageNet-1k} task, we follow \cite{qin2023infobatch} and anneal in the last 10\% of \textcolor{changecolor}{epochs}.
We employ different optimizers such as \emph{SGD} with \emph{Momentum} \citep{robbins1951stochastic, polyak1964some}, \emph{Adam} \citep{kingma2014adam}, and \emph{AdamW} \citep{loshchilov2017decoupled} to demonstrate that the IES method remains resilient to reasonable variations across multiple optimizers and learning rate schedulers. 

We use different SGD learning rate scheduler settings: \emph{SGD(F)}, \emph{SGD(L)}, \emph{SGD(M)}, and \emph{SGD(E)}, which represent SGD with a fixed learning rate, a linearly decaying learning rate, a multi-step decaying learning rate, and an exponentially decaying learning rate scheduler, respectively. 
For \emph{CIFAR}, we set base $\delta = 1e^{-3}$; and for \emph{ImageNet-1k}, we set $\delta = 1$. Further, we verified the effectiveness of our proposed IES method over several commonly used deep learning models, including \emph{ResNet} \citep{he2016deep}, \emph{VGG} \citep{simonyan2014very}, and \emph{DenseNet} \citep{huang2017densely}. More detailed experimental settings and additional results can be found in Appendix \ref{appb}.

To quantify the computational resources saved by the IES method, we consider two following metrics:
\begin{itemize}[leftmargin=0.35cm,topsep=-2pt]
\item \emph{Mini-batch Saved.} We calculate the percentage of mini-batch saved. This metric directly reflects the reduction in the number of instances of backpropagation computations.
\item \emph{Wall-time Speedup.} We measure the training time speedup achieved by the IES method compared with full data training. This metric provides a realistic assessment of the time savings. We report the average training speedup on CIFAR-10, CIFAR-100, and ImageNet-1k tasks.
\end{itemize}
The IES method primarily saves computational resources by reducing the backpropagation steps, which constitute the most time-consuming part of the training process. However, the forward pass still needs to be computed for all instances to obtain their predictions and determine which instances should be stopped based on the \emph{mastered} criterion. 
The experimental results demonstrate that the IES method can save 10\% to 55\% of mini-batch computations and speedup 20\% to 40\% of training time, while maintaining test accuracy comparable to full data training. The empirical evidence indicates the effectiveness of the IES method in reducing computational costs without compromising performance. 

\textbf{\textcolor{changecolor}{Transferability.}}
To further evaluate the \textcolor{changecolor}{effectiveness} of the IES method, we investigated its impact on the transfer learning of models. We first pretrained models on the ImageNet-1k dataset using IES and the baseline method without instance stopping. Then, we fine-tuned only the classification head of these pretrained models using the model from the last epoch of pretraining on several downstream tasks, including CIFAR-10, CIFAR-100, and Caltech-101 \citep{li_andreeto_ranzato_perona_2022} datasets.
As shown in Table \ref{tab3}, models pretrained with IES consistently outperform those pretrained without instance removal across all transfer learning tasks while achieving the comparable test accuracy on the ImageNet-1k. After fine-tuning for 1 epoch, the IES pretrained model surpassing the baseline by 0.9\%, 1.9\%, and 2.5\% on CIFAR-10, CIFAR-100, and Caltech-101, respectively. Similar improvements are observed on DenseNet-121 and on more epoch fine-tuning, as shown in Table \ref{tab3} and \ref{tabapp4} in Appendix \ref{appb5}.

These results align with the discussion in Section \ref{sec3.3}, suggesting that IES can more effectively reduce the sharpness of the loss landscape during pretraining. By instance-dependent stopping of instance training, IES saves computational resources while potentially contributing to a more favorable loss landscape and more transferable performance. Consequently, models pretrained with IES exhibit better transfer learning performance, adapting more effectively to new tasks with limited fine-tuning.


\subsection{Efficiency of IES}
\label{4.2}
Building upon the IES method's demonstrated ability to accelerate training without performance loss, this section further explores its efficiency. We compare IES with different sample selection criteria, analyze the impact of different $\delta$ value settings on performance, and investigate the potential for additional acceleration when allowing a slight decrease in performance.

\textbf{Comparison with other sample selection methods.}
We compare our proposed IES method with \emph{Random Remove} and \emph{Small Loss \& Rescale} \citep{qin2023infobatch}, under different Total Excluded Samples values. Total Excluded Samples values represent the proportion of samples removed from the backpropagation during training. \emph{Random Remove} method randomly removes a certain proportion of samples from backpropagation in each training epoch, while \emph{Small Loss \& Rescale} randomly prunes samples with smaller loss values and amplifies the gradients of the remaining small-loss samples.
As shown in Figure \ref{fig4}, experimental results on both CIFAR-10 and CIFAR-100 datasets demonstrate that the \emph{Random Remove} method significantly reduces model performance. Although the \emph{Small Loss \& Rescale} method improve results, its performance still falls behind IES. Moreover, among different IES configurations, using second-order differences outperforms other configurations in most cases, which aligns with our analysis in Section \ref{sec3.2}.
Further details are in Appendix \ref{appd}.

\textbf{Analysis of setting \(\delta\) values.}
To further evaluate the robustness of the IES method, we expanded upon the previous comparison by setting a broader range of $\delta$ values, observing their impact on sample exclusion and model accuracy.
As shown in Figure \ref{fig4} (lower row),  we varied the $\delta$ value used in the IES method by multiplying the selected $\delta$ value (set to 0.001) by scales of $\{0.01, 0.1, 1, 10, 100\}$. 
Notably, even as $\delta$ varied across four orders of magnitude, the IES method maintained the test accuracy within approximately 2\% of the baseline performance. This highlights the significant stability and adaptability of the IES method across a wide range of $\delta$ settings, enabling its effective implementation in diverse scenarios without the need for precise fine-tuning of the $\delta$ parameter. 

\begin{figure*}[h]
 \vskip -0.02in
 \begin{subfigure}[b]{0.458\textwidth}
     \includegraphics[width=\textwidth]{IESfig6walltime.pdf}
     \vskip -0.02in
     \caption{\emph{CIFAR-10}}
     \label{fig:sub1}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.458\textwidth}
     \includegraphics[width=\textwidth]{IESfig6walltime_c100.pdf}
     \vskip -0.02in
     \caption{\emph{CIFAR-100}}
     \label{fig:sub2}
 \end{subfigure}
 \vskip -0.08in
\caption{Comparison of the proposed IES method of different IES criteria (loss, 1st, 2nd, and 3rd order differences) with other sample selection methods under different Total Excluded Samples values on both CIFAR datasets. \textcolor{changecolor}{The lower subfigure illustrates the effect of varying $\delta$ values used in IES methods on training time reduction, sample removal, and model performance (3 runs, mean±std).}}
 \label{fig4}
\vskip -0.02in
\end{figure*}

\clearpage

\subsection{Further Analysis}
\label{4.3}

This section explores the scalability of IES for further acceleration, focusing on: (1) accelerating training while tolerating minor performance loss, and (2) maintaining accuracy while achieving targeted training speedups. Additionally, we evaluate the efficacy of IES for high-level vision tasks. Furthermore, we demonstrate the robustness of our proposed IES method under the common scenario of learning with noisy labels \citep{xia2020robust}, as detailed in Appendix \ref{applabelnoise}.

\textbf{Tolerating 1\% performance loss.} While IES aims to maintain test accuracy compared to full data training, it also has potential for further acceleration if a slight decrease in test accuracy is acceptable. By allowing a 1\% reduction in test accuracy, we observed that IES can achieve even greater computational savings.
For the ImageNet-1k dataset, IES can save up to 40\% of backpropagation. As shown in Figure \ref{fig4} (upper row), for the CIFAR-10 and CIFAR-100 datasets, IES can save up to 80\% and 60\% of backpropagation, respectively. These results demonstrate that IES can be flexibly adjusted to prioritize either improving efficiency without performance loss (a ``free lunch'') or further accelerating training within an acceptable range of performance degradation.

\textbf{Achieving 2.0$\times$ training speedups.} To further evaluate the efficacy of our proposed IES method in scenarios prioritizing efficient training, we conducted a comparison with several data efficiency methods: conventional early stopping, importance sampling \citep{jiang2019accelerating} \emph{SB}, hardness-based curriculum learning \citep{zhou2020curriculum} \emph{DIHCL}, and resizing-based curriculum learning \citep{wang2024efficienttrain++} \emph{EfficientTrain} methods. For a fair comparison, we set the target computational acceleration to approximately 2.0 times across all methods. The detailed settings and 3.0 times acceleration results are provided in Appendix \ref{appf}. As shown in Table \ref{tab4}, these comparisons further demonstrate that IES performs effectively in accelerating model training while maintaining model performance. 

\begin{table}[t]
\centering
\caption{Comparison of IES and other data efficiency methods. (3 runs, mean±std)}
\vskip -0.12in
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.5mm}
\renewcommand{\arraystretch}{0.88}
\begin{tabular}{c|c|cc}
\toprule
Computation Speedup & Methods & CIFAR-10 & CIFAR-100 \\
\midrule
$1.0\times$ & Baseline (No Removal) & 94.3\%$\pm$0.3\% & 77.0\%$\pm$0.4\% \\
\midrule
\multirow{5}{*}{$\sim2.0\times$} & Conventional Early Stopping & 90.4\%$\pm$0.5\% & 68.7\%$\pm$0.5\% \\
& SB \citep{jiang2019accelerating} & 93.0\%$\pm$0.1\% & 70.6\%$\pm$0.5\% \\
& DIHCL \citep{zhou2020curriculum} & 93.4\%$\pm$0.2\% & 74.3\%$\pm$0.2\% \\
& EfficientTrain \citep{wang2024efficienttrain++} & 91.5\%$\pm$0.2\% & \textbf{75.0\%$\pm$0.1\%} \\
\cmidrule(lr){2-2}\cmidrule(lr){3-4}
& IES (Ours) & \textbf{93.7\%$\pm$0.4\%} & \textbf{74.9\%$\pm$0.5\%} \\
\bottomrule
\end{tabular}
}
\vskip -0.16in
\label{tab4}
\end{table}

\begin{wraptable}{r}{0.58\textwidth}  
\centering
\vskip -0.16in
\resizebox{0.58\textwidth}{!}{  
\setlength{\tabcolsep}{1mm}
\begin{tabular}{c|c|c}
\toprule
 & Faster R-CNN (\emph{mAP}) & DeepLab v3 (\emph{mIoU})   \\
\midrule
No Removal   & 70.2\%$\pm$0.2\%     & 76.2\%$\pm$0.2\%       \\
IES (Ours)   & 70.2\%$\pm$0.1\%     & 76.1\%$\pm$0.2\%      \\
\midrule
Mini-batch Saved   & 20.0\%   & 14.0\%     \\
\bottomrule  
\end{tabular}
}
\vskip -0.05in
\caption{Effectiveness of the IES on object detection and segmentation model training tasks. (3 runs, mean±std)}
\vskip -0.2in
\label{tabhigh}
\end{wraptable}


\textbf{High-level vision tasks.} To further validate the applicability of the IES method, we conducted experiments on two high-level tasks: object detection and semantic segmentation.  Specifically, we integrated our proposed IES method into the baseline methods Faster R-CNN \citep{ren2015faster} and DeepLab v3 \citep{chen2017rethinking}, respectively. For both task, we use the PASCAL VOC datasets \citep{pascal-voc-2007, pascal-voc-2012}. A brief overview of the results of model training is reported in the Table \ref{tabhigh}. Further details are in Appendix \ref{appg}.

 

\section{Conclusion}

\label{sec5}
In this work, we propose an \emph{Instance-dependent Early Stopping} (IES) method that adapts the early stopping mechanism from the entire training set to the instance level. IES considers an instance as \emph{mastered} if the second-order differences of its loss value remain within a small range around zero, allowing for a unified threshold to determine when an instance can be excluded from further backpropagation. Extensive experiments demonstrate the effectiveness of IES in reducing computational cost while maintaining model performance and transferability.

\textbf{Limitation.} \textcolor{changecolor}{While the choice of using the second-order difference as the removal criterion for IES has been validated through experiments, a comprehensive theoretical analysis of its superiority remains an open research question.} The impact of the IES method on fairness is evaluated in Appendix \ref{appi}, but it still has not been thoroughly investigated.

\subsubsection*{Acknowledgments}
The authors would like to thank the anonymous reviewers for their insightful and constructive comments.
Suqin Yuan extends special thanks to Muyang Li, Runnan Chen, Xiu-Chuan Li, Jun Wang, Li He, and Yuhao Wu for their valuable advice and computing support.
Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, IC190100031.
BH was supported by RGC Young Collaborative Research Grant No. C2005-24Y, NSFC General Program No. 62376235, and Guangdong Basic and Applied Basic Research Foundation Nos. 2022A1515011652 and 2024A1515012399.
This research was undertaken with the assistance of resources from the National Computational Infrastructure (NCI Australia), an NCRIS enabled capability supported by the Australian Government.
Suqin Yuan is partially supported by the OpenAI Researcher Access Program.

\input{iclr2025_conference.bbl}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix
\section[Appendix A]{Quick Start Guide for Experimental Setup.}
\label{appa}

\textbf{Framework}: \texttt{PyTorch, Version 1.11.0}.

\textbf{Architecture}
\begin{itemize}
    \item \textbf{Model Type}: Standard ResNet-18 for CIFAR-10, ResNet-34 for CIFAR-100, and ResNet-101 for ImageNet-1k. We do not incorporate dropout.
\end{itemize}

\textbf{Parameters}
\begin{itemize}
    \item \textbf{Batch Size}: $\{64\}$ for CIFAR and $\{128\}$ for ImageNet-1k.
    \item \textbf{Training Epochs}: 200 epochs for CIFAR. 150 epochs for ImageNet.
    \item \textbf{Loss Function}: Utilizes the \texttt{CrossEntropyLoss} from the \texttt{nn} module.
\end{itemize}

\textbf{Dataset \& Pre-processing}
\begin{itemize}
    \item \textbf{Normalization}: We employ the \texttt{torchvision.transforms} module to adjust pixel values across all images, ensuring they scale uniformly within the 0 to 1 range.
    \item \textbf{Cropping}: We implement a random cropping strategy. Initially, optional padding is applied to each 32x32 image, from which we then extract random 32x32 crops.
    \item \textbf{Rotation}: The images are subject to random rotations with an allowable variation up to ±15 degrees to enhance model robustness against orientation changes.
    \item \textbf{Label Smoothing}: Label smoothing is not incorporated in our pipeline.
\end{itemize}

\section[Appendix B]{Details of Experiments}
\label{appb}
\renewcommand{\thesection}{B}

We provide comprehensive details on the experiments conducted to validate the effectiveness of the Instance-dependent Early Stopping (IES) method. The main results are presented in Section \ref{sec4}, Table \ref{tab 1} and Table \ref{tab 2}. Here, we elaborate on the experimental setup across various configurations, covering a wide range of settings typically employed in training deep learning models, including different network architectures, datasets, hyperparameters, and optimizers. Unless otherwise specified, the parameters and components remains consistent with the base model in Appendix \ref{appa}.

\subsection{Network Architectures}
\label{appb1}

\vspace{0.5em} 

\subsubsection{ResNet \citep{he2016deep}}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Variants}: \emph{ResNet-18, ResNet-34, ResNet-50, ResNet-101}.
    \item \textbf{Implementation}:
    \begin{itemize}[leftmargin=*,nosep, label=$\circ$]
        \item ResNet-18 and ResNet-50 for \emph{CIFAR-10}.
        \item ResNet-34 and ResNet-101 for \emph{CIFAR-100}.
        \item ResNet-34 and ResNet-101 for \emph{ImageNet-1k}.
    \end{itemize}
\end{itemize}

\vspace{0.5em}

\subsubsection{VGG-16 \citep{simonyan2014very}}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Implementation}:
    \begin{itemize}[leftmargin=*,nosep, label=$\circ$]
        \item Used for \emph{CIFAR-10}.
    \end{itemize}
\end{itemize}

\vspace{0.5em} 

\subsubsection{DenseNet-121 \citep{huang2017densely}}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Implementation}:
    \begin{itemize}[leftmargin=*,nosep, label=$\circ$]
        \item Used for \emph{CIFAR-100} and \emph{ImageNet-1k}.
    \end{itemize}
\end{itemize}

\vspace{0.5em} 

\clearpage

\subsection{Datasets}
\label{appb2}

\subsubsection{CIFAR-10 and CIFAR-100 \citep{krizhevsky2009learning}}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Description}: 10 classes (\emph{CIFAR-10}) and 100 classes (\emph{CIFAR-100}), 50,000 training and 10,000 test images each.
    \item \textbf{Preprocessing}: Normalization (mean and std), random cropping, horizontal flipping.
\end{itemize}

\subsubsection{ImageNet-1k \citep{deng2009imagenet}}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Description}: 1,000 classes, over 1 million labeled images.
    \item \textbf{Preprocessing}: Normalization (mean and std), random cropping, horizontal flipping.
\end{itemize}

\subsubsection{Caltech-101 \citep{li_andreeto_ranzato_perona_2022}}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Description}: 101 object categories
    \item \textbf{Preprocessing}: Normalization (mean and std), random cropping, horizontal flipping.
\end{itemize}

\vspace{0.5em} 

\subsection{Hyperparameters and Optimization}
\label{appb3}

\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Batch Sizes}: 64 for \emph{CIFAR} and \emph{Caltech-101}, and 128 for \emph{ImageNet-1k}.
    \item \textbf{$\delta$ settings}: base $\delta = 1e^{-3}$ for \emph{CIFAR}, and base $\delta = 1$ for \emph{ImageNet-1k}.
    \item \textbf{Optimizer settings}:
    For SGD, momentum=0.9, weight\_decay=5e-4.
    \begin{itemize}[leftmargin=*,nosep, label=$\circ$]
        \item SGD(F) - lr = 0.001.
        \item SGD(L) - lr = 0.1, scheduler:
        \\\texttt{LinearLR(\_,start\_factor=1,end\_factor=0.01,total\_iters=150)}.
        \item SGD(M) - lr = 0.1, scheduler: \\\texttt{MultiStepLR(\_, milestones=[50, 100], gamma=0.1)}.
        \item SGD(E) - lr = 0.1,  scheduler: \texttt{ExponentialLR(\_, gamma=0.96)}.
        \item Adam \citep{kingma2014adam} - lr = 0.001.
        \item AdamW \citep{loshchilov2017decoupled} - lr = 0.001, weight\_decay=0.01.
    \end{itemize}
    \item \textbf{Annealing} \citep{qin2023infobatch}: For the \emph{ImageNet-1k} task, we switch to using the full training data for the last 10\% of the training epochs to give better stability.
\end{itemize}


\subsection{Transfer Learning Experiments}
\label{appb5}

\textbf{Fine-tuning Setup}:

We selected the model checkpoints at the 100th epoch for ResNet-101/AdamW and DenseNet-121/AdamW follow settings from Table \ref{tab 2} experiments.
The models were fine-tuned using both the IES method and full-data training.
During fine-tuning, only the classification head of the models is updated, while the rest of the model parameters were frozen.

\textbf{Experimental Setup}:

The main experimental settings were consistent with those described in Appendix \ref{appa}.
The models were fine-tuned for 1 or 5 of epochs using the Adam optimizer with a learning rate of 0.001.
Notably, data augmentation techniques such as cropping and rotation were not applied, and all images were resized to a fixed resolution of 224x224. For the Caltech101 dataset, an additional preprocessing step is performed to convert grayscale images to RGB format.

Results for 1-epoch and 5-epoch fine-tuning are displayed in Table \ref{tabapp3} and Table \ref{tabapp4}, respectively.

\begin{table*}[h]
\vskip -0.05in
\centering
	\caption{transferability of IES-2nd Pretrained in ImageNet-1k. Fine-tuning 1 epoch. (mean±std) }
 \vskip -0.05in
	\label{tabapp3}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.5mm}{
\begin{tabular}{c|cc|cc}
\toprule
& \multicolumn{2}{c|}{\emph{ResNet-101}} & \multicolumn{2}{c}{\emph{DenseNet-121}}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
  Transfer Task & IES (Ours) & No Removal & IES (Ours) & No Removal \\
\midrule
\emph{ImageNet-1k --> CIFAR-10} & \textbf{81.2\%$\pm$0.1\%} & 80.3\%$\pm$0.2\% & \textbf{78.6\% $\pm$ 0.2\%} & 77.3\% $\pm$ 0.2\% \\
\midrule
\emph{ImageNet-1k --> CIFAR-100} & \textbf{57.5\%$\pm$0.2\%} & 55.6\%$\pm$0.2\% & \textbf{53.0\% $\pm$ 0.2\%} & 52.3\% $\pm$ 0.2\% \\
\midrule
\emph{ImageNet-1k --> Caltech-101} & \textbf{59.9\%$\pm$0.8\%} & 57.4\%$\pm$1.2\% & \textbf{50.9\% $\pm$ 1.6\%} & 49.5\% $\pm$ 1.5\% \\
\bottomrule  
\end{tabular}
}
}
\vskip -0.05in
\end{table*}

\begin{table*}[h]
\vskip -0.05in
\centering
	\caption{transferability of IES-2nd Pretrained in ImageNet-1k. Fine-tuning 5 epochs. (mean±std) }
 \vskip -0.05in
	\label{tabapp4}
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.5mm}{
\begin{tabular}{c|cc|cc}
\toprule
 & \multicolumn{2}{c|}{\emph{DenseNet-121}} & \multicolumn{2}{c}{\emph{ResNet-101}}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
Transfer Task & IES (Ours) & No Removal & IES (Ours) & No Removal \\
\midrule
\emph{ImageNet-1k --> CIFAR-10} & \textbf{82.6\%$\pm$0.1\%} & 81.7\%$\pm$0.1\% & \textbf{85.6\% $\pm$ 0.1\%} & 84.6\% $\pm$ 0.1\% \\
\midrule
\emph{ImageNet-1k --> CIFAR-100} & \textbf{61.6\%$\pm$0.2\%} & 60.8\%$\pm$0.2\% & \textbf{66.0\% $\pm$ 0.1\%} & 64.4\% $\pm$ 0.2\% \\
\midrule
\emph{ImageNet-1k --> Caltech-101} & \textbf{91.2\%$\pm$0.2\%} & 90.6\%$\pm$0.3\% & \textbf{92.7\% $\pm$ 0.2\%} & 92.5\% $\pm$ 0.3\% \\
\bottomrule  
\end{tabular}
}
}
\vskip -0.05in
\end{table*}


\subsection{Experiments in Figure \ref{fig3}}
\label{appb6}

\textbf{Setup}:
\begin{itemize}[leftmargin=*,nosep, label=$\circ$]
\item The main experimental settings were consistent with those described in Appendix \ref{appa}.
\item 5 runs, mean±std.
\item Batch size: The batch size is 128.
\item Number of epochs: The models are trained for 150 epochs.
\item $\delta = 1e^{-4}$.
\end{itemize}
\textbf{Evaluation Metrics}:
\begin{itemize}[leftmargin=*,nosep, label=$\circ$]

\item \textbf{SAM (Sharpness-Aware Minimization):}

The SAM value is defined as the difference between the perturbed loss and the original loss \cite{foret2020sharpness}.
The important hyperparameter is rho, which represents the magnitude of the perturbation. In this work, rho is set to 0.05.

\vspace{0.5em}
\item \textbf{Gradient Norm:}

In the training loop, for each batch, calculates the gradient norm.
For each parameter p, its gradient norm is calculated as \texttt{p.grad.data.norm(2).item() $**$ 2}.
The total gradient norm is the square root of the sum of squares of all parameter gradient norms.
Gradient Norm in Figure is the average of gradient norms for all batches in an epoch.

\vspace{0.5em}
\item \textbf{Maximum Eigenvalue of the Hessian Matrix:}

The maximum eigenvalue is estimated using the power iteration \cite{mises1929praktische} method to estimate the largest eigenvalue of the Hessian matrix.

The important hyperparameters include:

- n\_iters: The number of iterations for the power iteration method, set to 20.

- epsilon: A small positive number for numerical stability, set to $1e^{-10}$.
\vspace{0.5em}
\item \textbf{Training Loss:} The average cross-entropy loss on the training set.
\vspace{0.5em}
\item \textbf{Test Error:} The percentage of misclassified samples in the test set.
\end{itemize}

\clearpage
\renewcommand{\thesection}{C}
\section{Coefficient of Variation}
\label{appc}



To further investigate the properties of different orders of loss differences as potential \emph{mastered} criteria, we conducted experiments to compare their coefficient of variation (CV). The CV is a standardized measure of dispersion, calculated as the ratio of the standard deviation to the mean:

$$CV = \frac{\sigma}{\mu},$$

where $\sigma$ is the standard deviation and $\mu$ is the mean of the data.
We compute the CV values for the zero-order (loss value), first-order, second-order and third-order differences of each sample's loss during training. A lower CV value indicates that the data points are clustered more closely around the mean, while a higher CV suggests greater dispersion.
Figure \ref{figc1} presents the CV values for different orders of loss differences over the course of training when using the Adam optimizer. The results show that the second-order difference and the third-order difference generally maintains lower CV values compared to the zero-order and first-order differences throughout the training process.

\begin{figure}[h]
\centering
\includegraphics[width=0.35\textwidth]{Unknown-78_rebuttal_iclr.png}
\caption{Coefficient of variation (CV) of different orders of loss differences during training. Using Adam optimizer, learning rate = 0.001.}
\label{figc1}
\end{figure}

Although the CV values do not converge to a low level in the later stages of training as observed with the SGD optimizer (results on SGD shown in Figure \ref{fignew}), the second-order difference and the third-order difference still exhibits significantly smaller CV values compared to the other orders. This suggests that the second-order difference provides a relatively more consistent measure of an instance's learning status across different samples, even when the CV values do not converge.
The lower CV values of the second-order difference and the third-order difference throughout the training process support the use of a unified threshold $\delta$ to determine the \emph{mastered} instances. This property simplifies the implementation and management of the mastered criterion in the IES method, as it allows for a more consistent approach to identifying \emph{mastered} instances across the entire dataset. 
Using the second-order difference ($N=2$) as the mastered criterion achieves good performance in most cases, as shown in Figure \ref{fig4}. $N=2$ outperformed other configurations (including $N=3$) in most scenarios. Given the satisfactory performance of $N=2$, the potential benefits of exploring higher-order differences ($N>3$) may be limited. The additional computational complexity introduced by higher-order differences may not yield significant improvements in the effectiveness of the IES method.

These experimental results provide evidence for the effectiveness of using the second-order difference as the \emph{mastered} criterion in the IES method, enabling a more efficient and generalizable approach to instance-dependent early stopping. 

\clearpage
\renewcommand{\thesection}{D}
\section{Compare with Varying Methods and Criteria}
\label{appd}

To evaluate the effectiveness of the proposed IES method and its different criteria, we conducted experiments comparing IES with other sample selection methods under various hyperparameter settings. Figure \ref{fig4} presents the results of these experiments on CIFAR-10 and CIFAR-100 datasets. It is worth noting that the hyperparameters were fine-tuned to manually set the methods and criteria  to have similar total backpropagation sample savings rates, making the methods comparable.

\subsubsection{Experiment: IES with Different Criteria, Hyperparameters and Comparison Methods}
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Setup}:
    \begin{itemize}[leftmargin=*,nosep]
        \item Models: ResNet-18 for CIFAR-10, ResNet-34 for CIFAR-100
        \item Optimizers: SGD with momentum and exponential decay, the initial learning rate is set to 0.1, and the gamma parameter is set to 0.96
        \item Training Epoch: 200 for CIFAR
        \item Batch Size: 64 for CIFAR
        \vspace{0.4cm}
        \item Comparison Methods (CIFAR-10 and CIFAR-100):
        \begin{itemize}[leftmargin=*,nosep]
            \item \emph{Random Remove}: Randomly excludes a fixed proportion of samples from backpropagation in each training epoch. Removal rates: 10\%, 20\%, 30\%, 40\%, 50\%.
            \item \emph{Small Loss \& Reweight} \citep{qin2023infobatch}: Randomly removes samples with smaller loss values and amplifies the gradients of the remaining small-loss samples. To focus on the core idea of the method and ensure a simple and direct comparison with the proposed IES method, we removed the annealing and other additional operations from the original implementation. This modification allows us to evaluate the effectiveness of removing small-loss samples and amplifying their gradients in isolation, providing a clearer understanding of the differences between the two methods. Removal ratios: 10\% - 50\%. A comparison of the wall-time between IES method and InfoBatch method is provided in Figure \ref{figwalltime}.
        \end{itemize}
    \vspace{0.4cm}
    \end{itemize}
    \item \textbf{Results}:
    \begin{itemize}[leftmargin=*,nosep]
        \item IES with $N=2$ (2nd order difference) outperforms other criteria and sample selection methods in most cases, achieving a good balance between computational efficiency and model performance.
        \item The performance of IES is relatively stable across a wide range of $\delta$ values for each criterion.
        \item \emph{Random Remove} significantly reduces model performance, confirming the effectiveness of the IES method in selecting not-yet \emph{mastered} samples.
        \item \emph{Small Loss \& Rescale} improves results compared to \emph{Random Remove} but still falls behind IES.
    \end{itemize}
\end{itemize}
\vspace{0.3cm}
Figure \ref{fig4} visualizes the results of these experiments, comparing the test accuracy of different methods and criteria under varying Total Excluded Samples ratios.

\begin{figure*}[h]
 \vskip -0.02in
 \begin{subfigure}[b]{0.408\textwidth}
     \includegraphics[width=\textwidth]{figwalltime_cifar10.png}
     \vskip -0.02in
     \caption{\emph{CIFAR-10}}
     \label{figwalltime:sub1}
 \end{subfigure}
 \hfill
 \begin{subfigure}[b]{0.408\textwidth}
     \includegraphics[width=\textwidth]{figwalltime_cifar100.png}
     \vskip -0.02in
     \caption{\emph{CIFAR-100}}
     \label{figwalltime:sub2}
 \end{subfigure}
 \vskip -0.08in
\caption{Comparison of the wall-time between IES method and InfoBatch method}
 \label{figwalltime}
\vskip -0.02in
\end{figure*}



\renewcommand{\thesection}{E}
\section{High-Level Tasks}
\label{appg}

To further validate the general applicability of our perspective and method, we provide a comprehensive evaluation of our proposed Instance-dependent Early Stopping (IES) method across two distinct but equally important high-level vision tasks: object detection and image segmentation, providing a broader perspective on its potential applications in the field of computer vision.
Our experimental approach centered on integrating our proposed IES method into established baseline models for each task. Here's a detailed look at the experimental setup and result for each task:

We selected Faster R-CNN \citep{ren2015faster} as our baseline for object detection. Faster R-CNN is a two-stage detector that has shown remarkable performance in accurately identifying and localizing multiple objects within an image. 
For this experiment, we utilized the PASCAL VOC2007 \citep{pascal-voc-2007} dataset, and we implemented VGG-16 \citep{simonyan2014very} as the backbone network for feature extraction. 
Both the baseline method and the IES method were run for 30 epochs. We evaluate the best mAP value of the trained model and report the proportion of back-propagation mini-batches saved by the IES method.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\multicolumn{3}{c}{\textbf{Object Detection}} \\
\midrule
& mAP (\%) & Mini-Batch Saved (\%) \\
\midrule
Baseline & 70.2 $\pm$ 0.2 & $\backslash$ \\
InfoBatch \citep{qin2023infobatch} & 69.9 $\pm$ 0.2 & 18.7 \\
IES (Ours) & 70.2 $\pm$ 0.1 & 20.0 \\
\bottomrule
\end{tabular}
\end{table}

For the task of image segmentation, we chose DeepLab v3 \citep{chen2017rethinking} as our baseline. DeepLab v3 is a state-of-the-art model for semantic segmentation, allowing the model to capture multi-scale contextual information effectively.
We employed the PASCAL VOC2012 \citep{pascal-voc-2012} dataset for this experiment, and we used ResNet-50 \citep{he2016deep} as the backbone network. Both the baseline method and the IES method were run for 50 epochs. We evaluate the best mIoU value of the trained model and report the proportion of back-propagation mini-batches saved by the IES method.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\multicolumn{3}{c}{\textbf{Image Segmentation}} \\
\midrule
& mIoU (\%) & Mini-Batch Saved (\%) \\
\midrule
Baseline & 76.2 $\pm$ 0.2 & $\backslash$ \\
InfoBatch \citep{qin2023infobatch} & 76.0 $\pm$ 0.3 & 12.0 \\
IES (Ours) & 76.1 $\pm$ 0.2 & 14.0 \\
\bottomrule
\end{tabular}
\end{table}

\renewcommand{\thesection}{F}
\section{More Baseline Methods}
\label{appf}

We further compare the IES method with several other data efficient methods, including:
\begin{enumerate}
\item The conventional early stopping method.
\item The importance sampling method \citep{jiang2019accelerating}.
\item Curriculum learning methods \citep{zhou2020curriculum, wang2024efficienttrain++}.
\end{enumerate}

To evaluate the applicability of the IES method in scenarios where efficiency is the primary objective, we conducted comparisons using the same training parameters as the IES method (detailed in Section \ref{appd}). To further demonstrate the ability of these methods to accelerate training while tolerating a certain degree of model performance degradation, we reduced the total training epochs by half to 100 and set the target computational speedup to approximately 2.0 and 3.0 times. Under higher speedup ratios, we evaluate the loss on the full training set at five-epoch intervals to reduce the computational overhead of loss evaluation, thereby enabling more efficient training. The comparison is made based on the test accuracy achieved by each method's trained model.

\begin{table}[h]
\centering
\caption{Comparison of IES and other data efficiency methods. (3 runs, mean±std)}
\vskip -0.12in
\resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{3.5mm}
\renewcommand{\arraystretch}{0.9}
\begin{tabular}{c|c|cc}
\toprule
Computation Speedup & Methods & CIFAR-10 & CIFAR-100 \\
\midrule
$1.0\times$ & Baseline (No Removal) & 94.3\%$\pm$0.3\% & 77.0\%$\pm$0.4\% \\
\midrule
\multirow{5}{*}{$\sim2.0\times$} & Conventional Early Stopping & 90.4\%$\pm$0.5\% & 68.7\%$\pm$0.5\% \\
& SB \citep{jiang2019accelerating} & 93.0\%$\pm$0.1\% & 70.6\%$\pm$0.5\% \\
& DIHCL \citep{zhou2020curriculum} & 93.4\%$\pm$0.2\% & 74.3\%$\pm$0.2\% \\
& EfficientTrain \citep{wang2024efficienttrain++} & 91.5\%$\pm$0.2\% & \textbf{75.0\%$\pm$0.1\%} \\
\cmidrule(lr){2-2}\cmidrule(lr){3-4}
& IES (Ours) & \textbf{93.7\%$\pm$0.4\%} & \textbf{74.9\%$\pm$0.5\%} \\
\midrule

\midrule
\multirow{5}{*}{$\sim3.0\times$} & Conventional Early Stopping & 88.1\%$\pm$0.3\% & 63.9\%$\pm$1.0\% \\
& SB \citep{jiang2019accelerating} & 91.1\%$\pm$0.5\% & 65.8\%$\pm$0.3\% \\
& DIHCL \citep{zhou2020curriculum} & 92.7\%$\pm$0.1\% & 72.6\%$\pm$0.1\% \\
& EfficientTrain \citep{wang2024efficienttrain++} & 92.5\%$\pm$0.2\% & 70.6\%$\pm$0.7\% \\
\cmidrule(lr){2-2}\cmidrule(lr){3-4}
& IES (Ours) & \textbf{93.2\%$\pm$0.1\%} & \textbf{73.0\%$\pm$0.5\%} \\
\bottomrule
\end{tabular}
}
\vskip -0.0in
\label{tab4app}
\end{table}

As shown in Table \ref{tab4app}, these comparisons further demonstrate that IES, while not specifically designed for scenarios where efficient training is the primary objective, still performs effectively in accelerating model training while maintaining model performance. This can be attributed to its adaptively identifying and excluding \emph{mastered} samples during the training process.

\renewcommand{\thesection}{G}
\section{Label Noise}
\label{applabelnoise}

An analysis of learning with noisy labels \citep{yao2020dual, yao2021instance, wei2022self, yuan2023late, lin2024cs, linlearning, wumitigating} is crucial to evaluate the robustness and practicality of our proposed IES method.
To address this, we attempt to discuss this issue under Typical Learning with Noisy Label scenarios and Epoch-wise Double Descent scenarios, respectively.

\paragraph{Typical Learning with Noisy Labels.}

We validate the performance of the IES method and the baseline method (without removal) under typical learning with noisy labels settings, specifically, on the CIFAR-10/CIFAR-100 datasets with 20\% and 40\% symmetric and instance-dependent \citep{xia2020part} label noise.

\begin{table}[h]
\caption{Performance comparison on CIFAR-10 dataset with different noise settings.}
\label{tab:cifar10_noise}
\centering
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Noise Ratio} & \multirow{2}{*}{Type} & \multicolumn{2}{c}{Best Accuracy [\text{Early Stopping Epoch}]} & \multirow{2}{*}{Mini-batch Saved} \\
\cmidrule(lr){3-4}
& & Baseline & IES & \\
\midrule
20\% & Symmetric & 87.81\% [21] & 87.81\% [21] & 0\% \\
40\% & Symmetric & 81.29\% [13] & 81.29\% [13] & 0\% \\
20\% & Instance & 87.09\% [22] & 87.09\% [22] & 0\% \\
40\% & Instance & 83.49\% [20] & 83.49\% [20] & 0\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Performance comparison on CIFAR-100 dataset with different noise settings.}
\label{tab:cifar100_noise}
\centering
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Noise Ratio} & \multirow{2}{*}{Type} & \multicolumn{2}{c}{Best Accuracy [\text{Early Stopping Epoch}]} & \multirow{2}{*}{Mini-batch Saved} \\
\cmidrule(lr){3-4}
& & Baseline & IES & \\
\midrule
20\% & Symmetric & 55.39\% [17] & 55.39\% [17] & 0\% \\
40\% & Symmetric & 43.87\% [15] & 43.87\% [15] & 0\% \\
20\% & Instance & 57.30\% [18] & 57.30\% [18] & 0\% \\
40\% & Instance & 47.67\% [18] & 47.67\% [18] & 0\% \\
\bottomrule
\end{tabular}
\end{table}

The experimental results indicate that the IES method degenerates to the baseline method (without removal) across all tested label noise rates, noise types, and datasets. This suggests that during the training process, no training sample satisfies the master criterion before the model overfits to the noisy labels and its performance declines.

The core idea behind the IES method is that once a model has mastered a sample, it should stop training on that sample. However, when a certain proportion of label noise exists in the dataset, memorization of mislabeled samples may affect the model's ability to learn stable patterns, making it difficult for the model to truly master any samples before the early stopping point.

\paragraph{Epoch-wise Double Descent.}

Epoch-wise Double Descent refers to the phenomenon where, when the training samples contain a certain amount (usually low) of label noise, as training progresses, the model's generalization performance first rises, then falls, and then rises again, with the generalization performance after the second rise being superior to the first peak. In this label noise scenario, the model needs to prolong training to achieve better generalization performance compared to conventional early stopping. We validate the performance of the IES method and the baseline method (without removal) under typical Epoch-wise Double Descent settings, specifically, on the CIFAR-100 datasets with 10\% symmetric and instance-dependent label noise.

\begin{table}[h]
\caption{Performance comparison under Epoch-wise Double Descent settings on CIFAR-100.}
\label{tab:double_descent}
\centering
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{Noise Ratio} & \multirow{2}{*}{Type} & \multicolumn{2}{c}{Best Accuracy [\text{Epoch}]} & \multirow{2}{*}{Mini-batch Saved} \\
\cmidrule(lr){3-4}
& & Baseline & IES & \\
\midrule
10\% & Symmetric & 61.9\% [190] & \textbf{62.0\%} [191] & 14.2\% \\
10\% & Instance & 58.9\% [151] & \textbf{59.2\%} [199] & 11.0\% \\
\bottomrule
\end{tabular}
\end{table}

The experimental results show that the IES method can achieve lossless efficient training under the Epoch-wise Double Descent scenario. In the later stages of training, the model inevitably ``well-learn'' some instances due to the memorization effect. However, this does not affect the generalization performance of the final model (even slightly better).

This behavior can potentially be explained by the fact that although 'well-learned' instances may be forgotten as the model training overfits the mislabeled samples, the IES method allows these samples to adaptively re-include in training, thereby mitigating the negative impact of mislabeled samples. Furthermore, as shown in the Figure \ref{fig3}, the IES method can more targetedly reduce steepness in these sharp directions of the loss landscape, and therefore may be able to train a model with better generalization performance even in the presence of label noise.

Consequently, in the typical scenarios of learning with noisy labels and scenarios of Epoch-wise Double Descent, the IES method appears to have no negative impact on model performance compared to the baseline.

\renewcommand{\thesection}{H}
\section{Catastrophic Forgetting}
\label{apph}


We define ``early removed examples'' as the first 5\% of samples that are removed. We conducted experiments in a typical IES training environment with CIFAR-10, ResNet18, and SGD optimizer, which saves approximately 43\% of the backpropagation samples in total 200 training epoch.

We tracked the average training loss and accuracy of these ``early removed examples'' during the training process and compared them with the corresponding values of the entire training set. The experimental results are as follows:

\begin{table}[t]
\caption{Comparison of training loss and accuracy between full training set and early removed examples across different epochs.}
\label{tab:training_metrics}
\centering
\begin{tabular}{c cccc}
\toprule
\multirow{2}{*}{Epoch} & \multicolumn{2}{c}{Training Loss} & \multicolumn{2}{c}{Training Accuracy} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Full Set & Early Removed & Full Set & Early Removed \\
\midrule
50  & 0.120135 & 0.001003 & 95.96\% & 100.00\% \\
100 & 0.001448 & 0.000920 & 99.99\% & 99.98\% \\
150 & 0.000914 & 0.000806 & 100.00\% & 100.00\% \\
200 & 0.000883 & 0.000833 & 100.00\% & 100.00\% \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that the ``early removed examples'' are well learned (even better) by the model, and their training accuracy and loss are on par with other samples in the end of training. This implies that the model isn't catastrophically forgetting these ``early removed examples''.

Furthermore, we investigated the reasons why our IES method does not lead to catastrophic forgetting. Notably, the IES is a reversible method, which means that the removed samples have a chance to re-include in the training process if their second-order loss difference exceeds the threshold. Therefore, we tracked the average number of times the ``early removed examples'' were re-included in the training process, as shown in the following table:

\begin{table}[t]
\caption{Statistics of sample re-inclusion during training.}
\label{tab:reinclude_stats}
\centering
\begin{tabular}{l c}
\toprule
Metric & Value \\
\midrule
Average Times Re-included & 13.14 \\
Maximum Times Re-included & 26.00 \\
\bottomrule
\end{tabular}
\end{table}

Considering that our method allows these ``early removed examples'' to re-include in training for an average of about 13 times, with the most frequently replaced samples experiencing 26 training replays, we propose that this adaptive dynamic training mechanism contributes to the IES method's ability to effectively prevent ``early removed examples'' from being catastrophically forgotten during model training.

\renewcommand{\thesection}{I}
\section{Fairness}
\label{appi}

We conducted a preliminary assessment of the fairness of training using the IES method in sensitive environments.
We utilized the CelebA face dataset as an adversarial dataset to investigate whether the IES method would introduce new biases during training when using male as the sensitive attribute and attractiveness as the target label, thereby affecting the model's fairness.

We compared the baseline method (without sample removal) and the IES method on the ResNet-18 model for the attractiveness classification task, evaluating the accuracy, recall (True Positive Rate), and Demographic Parity Difference (DPD) metrics on the male and female validation subsets. The results are as follows:

\begin{table}[h]
\caption{Fairness evaluation on CelebA dataset using gender as the sensitive attribute. Metrics include overall accuracy, gender-specific accuracy and recall rates, and Demographic Parity Difference (DPD). Lower DPD indicates better fairness. Best results are shown in \textbf{bold}.}
\label{tab:fairness_metrics}
\centering
\begin{tabular}{l cccccc}
\toprule
\multirow{2}{*}{Method} & Overall & \multicolumn{2}{c}{Male} & \multicolumn{2}{c}{Female} & \multirow{2}{*}{DPD} \\
\cmidrule(lr){3-4} \cmidrule(lr){5-6}
& Acc. & Acc. & Recall & Acc. & Recall & \\
\midrule
Baseline & \textbf{82.5} & \textbf{83.8} & \textbf{68.2} & 81.6 & \textbf{90.6} & 0.4613 \\
IES (Ours) & 82.4 & 83.4 & 58.9 & \textbf{81.8} & 87.0 & \textbf{0.4544} \\
\bottomrule
\end{tabular}
\end{table}

From the Demographic Parity Difference (DPD) metric, which evaluates fairness (the closer to 0, the better), the IES method is slightly lower than the baseline method (0.4544 vs 0.4613), indicating that its prediction results have slightly less disparity between the two gender subsets. 

These results provide a preliminary indication that the IES method may introduce or amplify certain biases to some extent, negatively impacting the classification performance for different population subsets. However, since IES allows excluded samples to adaptively re-participate in training, the overall fairness is slightly improved.


\end{document}
