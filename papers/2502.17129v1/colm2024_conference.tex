\pdfoutput=1
\documentclass{article} % For LaTeX2e
\usepackage{colm2024_conference}
\usepackage[bottom]{footmisc}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{xurl}
% \PassOptionsToPackage{hyphens}{url}
\usepackage{tikz}
\usepackage{booktabs}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}

\usepackage{color}
% \usepackage[dvipsnames]{xcolor}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{tikz}
% \usetikzlibrary{positioning,shapes,geometric,arrows,scope}
\usetikzlibrary{positioning, shapes, arrows}
% \usepackage{geometric}
% \usepackage{scope}

\usepackage{pdflscape}
\usepackage{longtable} % For multi-page tables
\usetikzlibrary{mindmap,trees} % 导入 mindmap 库
\usepackage[edges]{forest}
\usepackage{bm}
\usepackage{rotating}
\usepackage{float}

% \setcounter{secnumdepth}{6}
% \renewcommand{\theparagraph}{\arabic{paragraph}}
% \renewcommand{\p@paragraph}{\theparagraph}

\definecolor{mygreen}{RGB}{9,136,66}
\newcommand{\cmark}{\textcolor{mygreen}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{red!50}{\ding{55}}}%
\newcommand{\omark}{\textcolor{blue}{\ding{109}}}%


\title{Thus Spake Long-Context Large Language Model}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\colmfinalcopy

\author{%
Xiaoran Liu\textsuperscript{1,2}\thanks{\ \ Equal contribution. }, %, 
Ruixiao Li\textsuperscript{2}\footnotemark[1], %,  
Mianqiu Huang\textsuperscript{2}\footnotemark[1], %,  
Zhigeng Liu\textsuperscript{2}\footnotemark[1], %,
Yuerong Song\textsuperscript{2}\footnotemark[1], \\ %, \\
\textbf{Qipeng Guo\textsuperscript{1,4}, %,
Siyang He\textsuperscript{2}, %,
Qiqi Wang\textsuperscript{2}, %,
Linlin Li\textsuperscript{3}, %,
Qun Liu\textsuperscript{3}, %,
} \\
\textbf{Yaqian Zhou\textsuperscript{2}, %, 
Xuanjing Huang\textsuperscript{2}, %,
Xipeng Qiu\textsuperscript{1,2,4}\thanks{\ \ Corresponding Author. } %
}\\[.5ex]
\textsuperscript{1}Shanghai AI Lab, \ % 
\textsuperscript{2}School of Computer Science Fudan University, \ % 
\\
\textsuperscript{3}Huawei Noah's Ark Lab, \ % 
\textsuperscript{4}Shanghai Innovation Institute  % 
\\[.5ex]
\texttt{xrliu24@m.fudan.edu.cn}, \ \texttt{guoqipeng@pjlab.org.cn}, \ 
\texttt{xpqiu@fudan.edu.cn}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \colmfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs) giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, the research on long-context LLMs has expanded from length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies. \\[1ex]
Inspired by the symphonic poem, \textit{Thus Spake Zarathustra}, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend its mortality. In this survey, We will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to the research on long-context LLMs.
% , and the visual explanation can be referenced at \url{www.bilibili.com} and \url{www.youtube.com}.
\end{abstract}

\input{figures/fig_overall}

\newpage

\section{Introduction}

% \begin{figure}[!b]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/intro_1_arch.pdf}
%         \caption{Todo.\label{fig_intro_arch}}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{0.49\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/intro_2_len.jpg}
%         \caption{Todo.\label{fig_intro_len}}
%     \end{minipage}
% \end{figure} 

% \begin{figure}[!b]
%     \centering
%     \includegraphics[width=\linewidth]{figures/intro_3_token_new.pdf}
%     \caption{Context window comparison across text and video: The timeline illustrates the token counts. Text uses the tiktoken~\citep{openai_tiktoken} tokenizer, while video uses the Qwen2-VL~\citep{wang2024qwen2} tokenizer.}
%     \label{fig:token2text_or_video}
% \end{figure}

Research on long-context capability has been an important topic in Natural Language Processing (NLP), reflected in the evolutionary trajectory of mainstream architectures. This evolution shows a consistent progression toward increasing context length, from the Bag-of-Word models~\citep{Harris01081954} with no concept of context to CNNs~\citep{lecun1995convolutional} with local receptive fields, then to LSTMs~\citep{schmidhuber1997long} characterized with an explicit long short-term memory, and currently to Transformer featured with modeling long-range dependencies~\citep{Vaswani2017attention}, as well as the recent discussions on the SSM-Mamba series~\citep{gu2020hippo,gu2023mamba,daotransformers} that challenges the dominance of Transformers from the perspective of history storage. Researchers hope models, especially the Large Language Model (LLM)~\citep{gpt4,Sun2024MOSS,reid2024gemini,meta2024introducing}, can possess life-long context, rather than being limited by a fixed window size.

% As shown in Figure~\ref{fig:token2text_or_video}, 
In a 1k context, LLM may only understand a short fairy tale. In a 4k context, the reading comprehension may be limited to an arXiv paper~\citep{shaham2022scrolls}. In a 32k to 128k context, LLM may process a hundreds-page detective novel in its entirety and successfully infer the identity of the murderer~\citep{xu2024detectiveqa,wang2024novelqa}. When the context length extends to 512k, even a novel as lengthy as Ulysses or a novel series could be input and understood as a whole~\citep{jacobs2023deepspeed}. When the context length reaches 2M, the model may learn new knowledge through many-shot long In-Context Learning (ICL)~\citep{agarwal2024many} or acquire a new language via vocabulary and grammar books~\citep{reid2024gemini}. If the context length becomes infinite, LLM may possess life-long learning capabilities, which may change the existing training paradigm~\citep{sun2024learning,lin2023unlocking}.

Unfortunately, as context length increases, researchers also face various obstacles. From an architectural perspective, the context length of mainstream Transformer architectures is limited not only by the pre-training window size~\citep{presstrain,chen2023extending} but also by the memory and computational overhead of the Key-Value (KV) cache~\citep{kwon2023efficient,xiaoefficient}. From an infrastructural perspective, longer contexts result in greater memory pressure and lower throughput~\citep{chen2024internevo,kwon2023efficient}. From a training perspective, long-context datasets face challenges in both quantity and quality~\citep{lv2024longwanjuan,gao2024train}. From an evaluation perspective, increasing context length reveals more potential problems in LLMs~\citep{agarwal2024many,hsieh2024ruler}, leading to higher requirements for LLM performance~\citep{xu2024detectiveqa,zhang2023movqa}.

However, since the emergence of LLM, long-context capabilities remain one of the most rapidly developing areas and constitute a core competition point~\citep{anthropic2024claude2,cai2024internlm2,meta2024introducing}, as shown in Figure~\ref{fig:eval-ctx}. From April 2023 to February 2024, the context length of open-source LLMs has grown from an initial 2k~\citep{touvron2023llama} to 2M~\citep{ding2024longrope}. In this process, some surveys concentrate on particular aspects~\citep{huang2023advancing,zhao2023length,pawar2024and}, particularly developments in architectural design, while other technique reports focus on summarizing the life-cycle of a specific long-context LLM~\citep{chatglm2024glmlong,gao2024train}, from data construction to context extension and to performance evaluation. Currently, there is a lack of a comprehensive survey that presents the full life-cycle of long-context LLMs from architecture, infrastructure, training, and evaluation, showing the global picture of long-context technology.

Inspired by \textit{Thus Spake Zarathustra}, the symphonic poem of the German composer Richard Strauss, we draw an analogy between the attempts of LLMs to extend their context lengths and the attempts of humans to transcend their mortality. On the journey of extending the context length of LLMs, researchers continuously challenge the boundaries of context through optimizations in architecture, infrastructure, and training, much like \textit{the struggle between man's tremendous need for immortality and his equal need to accept the fact that he is mortal}\footnote{From \textit{Thus Spake Richard Strauss} by Leonard Bernstein in Young People's Concert. \url{https://leonardbernstein.com/lectures/television-scripts/young-peoples-concerts/thus-spake-richard-strauss}}. As shown in Figure~\ref{fig:overall}, this survey comprehensively introduces the life-cycle of long-context LLMs from four perspectives: \textbf{architecture}, \textbf{infrastructure}, \textbf{training}, and \textbf{evaluation}. 
\begin{itemize}
    \item Sections \ref{sec2} to \ref{sec5} focus on the architectural aspect, discussing the enhancement of Transformer in length extrapolation, KV cache optimization as well as memory management, and the innovation to defeat Transformer by long-context researchers.
    \item Sections \ref{sec6} and \ref{sec7} address infrastructure considerations, detailing optimizations for long context in the training and inference phases of Transformer-based LLMs.
    \item Sections \ref{sec8} to \ref{sec10} introduce the training methods in three corresponding training stages for long-context LLMs, pre-training, post-training, and multi-modal training, particularly for long-context Multi-modal LLM (MLLM).
    \item In Section \ref{sec11}, we will discuss the long-context evaluation. In Section \ref{sec12}, we will outline 10 unanswered questions that long-context LLMs still face as a conclusion.
\end{itemize}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/time_eval_w_line.pdf}
    \caption{Long-context performance of various LLMs across multiple benchmarks, perplexity (PPL)~\citep{presstrain}, NIAH~\citep{niah}, and RULER~\citep{hsieh2024ruler}. The horizontal axis represents the release time, while the vertical axis indicates the effective context length achieved by the LLMs on the corresponding task. The line associated with each task represents the state-of-the-art performance at a given point in time.}
    \label{fig:eval-ctx}
\end{figure}

% We hope our survey can provide a comprehensive technical summary for the long-context research community and serve as an introductory guide for researchers unfamiliar with this area. To present this paper more intuitively, we have made a video that combines the content of this survey with the symphonic poem \textit{Thus Spake Zarathustra}, aiming to raise awareness among more researchers on the importance and entirety of long-context research.

\input{sections/sec2_len_extp}

\input{sections/sec3_kv_cache}

\input{sections/sec4_memory}

\input{sections/sec5_arch_inno}

\input{sections/sec6_train_infra}

\input{sections/sec7_infer_infra}

\input{sections/sec8_pre_train}

\input{sections/sec9_post_train}

\input{sections/sec10_mllm}

\input{sections/sec11_eval}

\section{Unanswered Questions}\label{sec12}

In the 10 sections above, we have illustrated the development trajectory of long-context from the extensive literature in different aspects. In this section, we can make a more comprehensive conclusion in the final section, unlike the previous survey focused on particular domains~\citep{huang2023advancing,zhao2023length,pawar2024and,luohekeep}. However, instead of listing some take-home messages or definitive claims, inspired by the masterpiece of Richard Strauss, we are more willing to end our journey with a longer context with 10 unanswered questions, to stimulate more in-depth thoughts and research on long-context LLMs from these perspectives. Whatever the answers may be, we believe we come out of reading this survey, wiser and better people than before. 

\paragraph{Q1}\label{q1_bias}\textbf{Position Bias}\quad
% 这一段最后对position bias下定义需要找引用
While considerable efforts have been devoted to augmenting the context window length of LLMs~\citep{chen2023extending, dynamicNTK, pengyarn}, position bias persists within these models. Position bias refers to LLMs' propensity to favor certain positions over others~\citep{wang2023large, zheng2023judging}.
% 这里是否要换成别人论文里的原话：Position bias is when an LLM exhibits a propensity to favor certain positions over others.
% lostinthemiddle定义-归因attentionsink-提纲挈领批量引就可以-最后保持疑问
A notable manifestation of this bias is the phenomenon known as \textbf{\textit{lost in the middle}}, where LLMs tend to allocate anomalously higher attention to the beginning and end of context, while the middle part receives relatively less focus~\citep{liu2024lost}. This tendency is further exacerbated by what has been termed the \textbf{\textit{attention sink}} effect, wherein the majority of attention scores are concentrated on the initial tokens of the context~\citep{xiaoefficient}. Surprisingly, such bias is observed even in NoPE-based LLM, where no position information is explicitly injected, but the performance of NIAH still declines from the middle~\citep{wang2024length}. On one hand, this bias has benefited research in streaming processing~\citep{xiaoefficient, yang2024seed} and KV cache optimization~\citep{tang2024razorattention, xiao2024duoattention}. On the other hand, many empirical efforts have also been devoted to addressing this bias~\citep{zhang2024attention, mcilroyorder,hsieh2024found}, such as fill-in-the-middle~\citep{an2024make}. However, minor studies try to answer why this bias exist~\citep{gu2024attention}. The theoretical understanding of related mechanisms is still an unanswered question.

% 说关于这方面讨论比较少
In parallel, \citet{levy2024same} examines the impact of input length on the inference performance of LLMs, observing a significant performance decline though the input length is still shorter than the maximum context length. Leveraging this effect, some evaluation datasets have increased the length of evaluation texts to obscure relevant information and enhance the evaluation difficulty~\citep{yuan2024lv,hsieh2024ruler,li2024long}. However, questions regarding this aspect remain relatively unsolved. Though we can easily extrapolate the LLMs to a longer context, we often struggle to guarantee an exhaustive short-to-long generalization in downstream tasks~\citep{li2024long,anil2024many}.

% rope的局限性
\paragraph{Q2}\label{q2_rope}\textbf{RoPE Design}\quad
RoPE\citep{su2024roformer} has emerged as the mainstream position embedding for LLMs due to its superior performance\citep{dubey2024llama, bai2023qwen, liu2024deepseek}. However, regarding length extrapolation, the current RoPE scaling methods~\citep{roziere2023code,xiong2024effective}, can only achieve weak extrapolation for an infinite context length or strong extrapolation for a finite one. In strong extrapolation, RoPE-based LLMs rely on the high-dimensional, low-frequency features to represent long-context dependencies at greater distances~\citep{barbero2024round, hong2024token, zhong2024understanding}. However, these dimensions present OOD in extrapolation. Besides, even when position information is not OOD, the increased attention entropy also harms the long-context performance~\citep{pengyarn,han2024lm}. The conflicts between periodicity and monotonicity and between full attention and attention entropy are the inherent drawbacks of scaling RoPE-based LLMs to an infinite context~\citep{liuscaling,men2024base,han2024lm}. 

Given these limitations of RoPE, researchers have explored additional approaches based on alternative position embedding design~\citep{kazemnejad2024impact,wang2024length}, or cache operation~\citep{xiao2024infllm, liu2024reattention}, to address these challenges. Regarding RoPE itself, the modification for length extrapolation also simulates modifications for other perspectives, such as the selection of rotary angles~\citep{wu2024extending}, the number of dimensions for RoPE~\citep{glm2024chatglm,biderman2023pythia}, the index schema for RoPE~\citep{golovneva2024contextual}, whether there are better design alternatives for RoPE~\citep{sun2022length,chi2022kerple}, how the scaling laws change under these alternative designs, and even how to design RoPE for multi-modal information~\citep{kexuefm10040,wang2024qwen2,li2024giraffe,wei2025videorope}, all remain open questions await deeper investigation.


% Since the NTK\citep{dynamicNTK} method extended the context length of LLMs by modifying the base of the rotational angle in rope, research into the relationship between rope and LLMs' performance has gained traction.

% The relationship between the rope mechanism and the context length of LLMs has become a focal point of research. ScalingRope\citep{liuscaling} explores the impact of the base of the rotational angle on the length of the context window, presenting a mathematical formulation that quantifies this relationship. \citet{zhong2024understanding} delves deeper into the effects of RoPE extensions on long-context LLMs from the perspective of the attention mechanism using two benchmark tasks. However, while excessively large rotational angles can maintain the consistency of positional information through periodicity, thus avoiding out-of-distribution issues, they compromise the monotonicity of RoPE, causing the model to perceive only local semantics and perform poorly in generation and in-context learning (ICL) tasks \citep{men2024base, liuscaling}. This reflects a contradiction in RoPE, dimensions with monotonicity can capture context but tend to overfit to pre-trained positional information and fail to extrapolate. Conversely, dimensions that can extrapolate lack monotonicity and are unable to perceive long-range context.

% The relationship between different dimensions of RoPE and LLMs' capabilities also presents an intriguing research direction. \citet{barbero2024round} find that most attention heads in the model utilize low-frequency components, while attention heads that leverage more high-frequency components appear to focus more on relative positions, a pattern that NoPE\citep{haviv2022transformer} cannot learn. \citet{hong2024token} identifies specific attention heads focused on long-range information interaction and discovers a correlation between these attention heads' high dimensions and the model's long-context capabilities, providing quantitative analysis.

% While RoPE enjoys widespread popularity, some researchers have also noted its limitations. For instance, \citet{chen2024circuit} establishes circuit complexity bounds for rope, revealing the fundamental expressive limitations of RoPE-based Transformer architectures.

\paragraph{Q3}\label{q3_ppl}\textbf{Dilemma of Perplexity}\quad
For a long time, perplexity has been a primary indicator for determining the upper bound of the length extrapolation~\citep{presstrain,liuscaling}. However, subsequent research has found that perplexity does not truly reflect the performance of LLMs in the downstream tasks of long context~\citep{men2024base,hu2024can,fang2024wrong,gao2024train,xiaoefficient}. Despite this, there are still works that define long-context quality based on perplexity, such as LongWanjuan~\citep{lv2024longwanjuan} and ProLong~\citep{chen2024long} with perplexity-based metrics to compare the information gain of long contexts with short ones. Recently, LongPPL~\citep{fang2024wrong} based on the comparison between sliding window perplexity and standard perplexity, is proposed to reflect LLM's real downstream performance more accurately. 

However, both definitions of short-context perplexity have flaws: chunking breaks long-context dependencies while sliding windows imply that the receptive field increases with model depth. Additionally, the perplexity of different LLMs may vary due to differences in their training data distributions. Therefore, there is much space for improving perplexity in assessing LLMs performance and data quality in long-context scenarios.

\paragraph{Q4}\label{q4_rag}\textbf{Long Context v.s. RAG}\quad
The choice between long-context LLMs and RAG has been a topic of debate. \citet{xu2023retrieval} suggests that retrieval-augmented approaches allow LLMs with smaller context windows to perform on par with larger context window LLMs, and even improve the performance of long-context LLMs. However, \citet{li2024retrieval} has reached the opposite conclusion, indicating that under their experimental setup, long-context LLMs generally outperforms RAG. Moreover, \citet{leng2024long} indicates that using longer context does not uniformly increase RAG performance while \citet{jiang2024longrag} holds an opposite opinion. ~\citet{li2024long_vs_rag} conducts a more in-depth investigation into this issue. This raises two intriguing question: which paradigm represents the better approach for generation? Should these two paradigm be combined?

To begin with, long-context LLMs offer more complete contextual information compared to RAG, but they also come with challenges such as lower information density and high computational resource consumption.  KV cache is position-sensitive while RAG is position-independent. Whether the positional relationships within long-context LLMs play a significant role in generation remains an important topic for exploration~\citep{bertsch2024context}. In contrast, RAG is more lightweight and better suited for edge devices, but it is unable to handle special scenarios, such as long outputs. Many attention acceleration or approximation methods utilize retrieval~\citep{zhang2023h2o, li2024snapkv}, raising the question of whether long-context generation can be unified with RAG. We believe that a more flexible memory-based approach, which can leverage both text and KV cache, may represent a promising and potentially superior generation paradigm in the future~\citep{yang2024memory3}.

\paragraph{Q5}\label{q5_na}\textbf{Discussion on New Architecture}\quad
%Token-shift operations have emerged as a crucial component in contemporary architectural designs~\citep{choe2024rwkv,peng2024eagle,fu2022hungry}. The widespread adoption of token-shift mechanisms in new architectures can be attributed to several fundamental factors.Primarily, token-shift operations effectively reduce position embedding bias, enabling models to better capture relative positional relationships within sequences rather than overly relying on absolute positional information. This characteristic is particularly significant in the context of length extrapolation, where traditional absolute position embedding often fail to generalize.Furthermore, token-shift can be conceptualized as a feature reorganization mechanism. By allowing the model to observe input sequences from multiple perspectives, it enhances feature extraction capabilities and facilitates the capture of richer intra-sequence dependencies. This multi-perspective approach contributes significantly to the model's ability to understand complex sequential patterns.The operation also serves as an additional training signal, functioning analogously to data augmentation. This inherent property enhances the model's robustness and generalization capabilities, particularly when dealing with sequences of varying lengths.For architectures designed to process long sequences, token-shift operations play a crucial role in bridging long-distance relationships between tokens . By reducing the effective distance between distant tokens, these operations enable more efficient processing of long-range dependencies, which is essential for handling extended context windows in large language models.
Recent advances in LLM's architectures have revealed an intriguing pattern: the incorporation of local interaction, such as the token shift in RWKV~\citep{peng2023rwkv,peng2024eagle} or convolution in Mamba~\citep{gu2023mamba}. While these architectural choices designed for a long context are different, they coincidentally introduce similar mechanisms to modeling local interaction. This raises the question of whether traditional RNN, LSTM, or SSM can achieve long-context capabilities comparable to Transformer by incorporating local interaction mechanisms. Furthermore, does the standard self-attention mechanism equal the combination of local interaction based on CNN or token shift and long-context dependency captured with RNN or SSM, and why or why not?

A potential explanation lies in the mechanisms of information processing. In attention-based architecture, information from different positions is processed in parallel before fusion~\citep{Vaswani2017attention}, whereas RNN, LSTM, and SSM architectures process information from various distances (both short and long-range) simultaneously~\citep{beck2024xlstm,gu2023mamba}. This mixed processing can lead to mutual interference, where short-context information may disrupt the modeling of long-context dependencies, and vice versa. The introduction of convolution or token shift represents an attempt to decouple information interaction across different scales. Moreover, such assumptions also need further validation.
% Convolution focuses on capturing feature interactions within local contexts, while token shift creates explicit local context windows, enabling more effective processing of dependencies across different scales.

\paragraph{Q6}\label{q6_slm}\textbf{On-Device Long Context}\quad 
The future of long-context LLMs also involves edge-based multi-modal applications, which require locally deployed models as a foundation or important support. Although major AI companies are integrating their models into local software~\citep{wu2024first, yin2024llm}, these solutions still rely on LLMs in the cloud. The future interaction paradigm will be fundamentally multi-modal~\citep{yao2024minicpm}, processing and generating across multiple modalities such as speech, images, text, and action sequences~\citep{Google2024long-context-usecase, Google2024why-do-they-matter, Apple2024AppleIntelligence, Google2024PixelGemini}. Meanwhile, to reduce latency, ensure privacy, balance server loads, and enable personalization, a substantial portion of computation and storage tasks of long-context LLMs will migrate closer to users, specifically to edge devices~\citep{wu2024first, xu2024device}.

Although the direction of development is clear, many challenges remain in delivering a smooth and natural long-context interaction experience to users. These challenges span multiple domains~\citep{xu2024device}: How can algorithms, hardware, and software be further optimized to reduce the resource footprint of long-context operations and improve inference speed~\citep{mlc-llm, lu2024bluelm, xue2024powerinfer, choe2024rwkv}? What technologies are needed for more seamless integration~\citep{yao2024minicpm, yin2024llm}? Is it possible to achieve horizontal scaling of long-context LLMs in this process and make it close to users? These challenges await researchers and engineers to solve them. Since the integration of large language models and edge devices has already become an industry-wide consensus~\citep{Qualcomm2023QualcommLlama, Apple2024AppleIntelligence, Qualcomm2024QualcommMistral, Google2024PixelGemini, lu2024bluelm}, we hope that they will all be resolved in the near future.

\paragraph{Q7}\label{q8_balance}\textbf{Long-Context Training from Scratch}\quad 
From a perspective of model capability, training with long-context data from the start offers several advantages. It naturally enhances LLM's ability to handle longer context~\citep{gao2024train}. Following the "the best part is no part" philosophy, it eliminates the need for complex length adaptation techniques. Training with mixed-length texts in the same batch allows LLMs to learn from text length distributions that better reflect real-world scenarios~\citep{gao2024train, chatglm2024glmlong}. 

The challenges of training with mixed-length sequences in the same batch are primarily engineering-related rather than theoretical. Traditional training frameworks require extensive padding when processing texts of varying lengths, which wastes computational resources and reduces training throughput. The disparity in computational load between long and short texts creates load imbalance issues in distributed training environments. Sophisticated runtime dynamic schedulers may be needed to address these challenges. Therefore, improving long-context training efficiency remains a critical engineering challenge, with a particular focus on enhancing the efficiency of mixed-length text training.

\paragraph{Q8}\label{q7_scarce}\textbf{Quantity and Quality of Long Data}\quad
As reported by Ilya, existing corpora have almost been exhausted for pre-training\footnote{Ilya Sutskever's talk at NeurIPS 2024. Sequence to Sequence Learning with Neural Networks. \url{https://www.youtube.com/watch?v=qo-ZjF_LAz8}}. The scarcity of data is more severe for long context~\citep{chatglm2024glmlong,gao2024quest}. Although researchers have constructed longer textual data by fancy concatenation~\citep{shicontext,chatglm2024glmlong,zhao2024longskywork} or task-oriented synthesis~\citep{an2024make,pham2024suri}, concerns about the effectiveness of synthetic data have never ceased~\citep{gao2024quest,zhao2024analysing,que2024hellobench}.

Besides quantity, quality also matters. Unfortunately, the definition of long-context data quality has not been thoroughly explored~\citep{lv2024longwanjuan,chen2024long}, and more researchers are trying to optimize the quality of data mixing between long and short corpora~\citep{xiong2024effective,chatglm2024glmlong,gao2024train}. However, training with limited long texts fails to guarantee the short-to-long generalization.~\citep{levy2024same,hsieh2024ruler,anil2024many,huang2024longsafetybench}, and how to guarantee effective training in long contexts is also a challenge~\citep{an2024does,gao2024train,hsieh2024ruler}.

In the multi-modal domain, the scarcity of long video is also significant~\citep{qianmomentor,yin2024t2vid,ren2024vista}. Moreover, although both text and video share sequential features, the generalization from long-context text to long-context video, and from long-context reasoning in text to long-context reasoning in video~\citep{li2024temporal}, remains a topic that requires further research and discussion.

\paragraph{Q9}\label{q9_output}\textbf{Long Output and Reasoning}\quad In the last two questions, we will finally discuss how to enhance the model capabilities with long context. Long context involves long input and long output and we start with the latter. Compared to short outputs, long outputs involve more complex dependencies and exposure bias resulting from inconsistencies between the previously generated content and the ongoing output~\citep{an2022cont}. These factors make training long-output LLMs particularly challenging. Although some evaluation work on long-text outputs has been conducted~\citep{tan2024proxyqa, que2024hellobench}, there is still a lack of effective metrics for assessing long outputs. Manual scoring remains subjective and difficult, and LLM-as-a-Judge requires further in-depth exploration~\citep{dubois2024length, que2024hellobench}. Thus, evaluating long-output LLMs remains a significant challenge.

Furthermore, the expectations for LLM outputs go beyond mere content generation. There is a need for LLMs to solve complex reasoning problems. The rapid rise of o1 has also highlighted the tremendous potential of long reasoning~\citep{OpenAI2024o1, zeng2024scaling, guo2025deepseekr1, team2025kimi}, and long output, as a core capability of long reasoning, needs to achieve better performance to support the advancement of related work. Moreover, \citet{snell2024scaling} indicates that scaling at test-time is crucial, and long output is a promising method to achieve it. Despite the vast application potential of long-output generation, it still faces numerous challenges. For instance, maintaining logical and informational consistency during long-text generation, and effectively controlling aspects such as style, tone, and emotion in the generated content, remain key issues for researchers~\citep{bai2024longwriter, quan2024language}. Additionally, these questions are more severe in MLLMs for multi-modal dependencies~\citep{tan2019lxmert, zhang2021visually} and cross-modal consistency~\citep{zhang2024cross, chou2024mm}.

\paragraph{Q10}\label{q10_icl} \textbf{Long In-Context Learning and More}\quad Long In-Context Learning is a method for enhancing LLM performance through long inputs~\citep{brown2020language,pan2023context,agarwal2024many}. Current discussions on long ICL mainly focus on benchmarks that use it to analyze long-context capabilities of LLMs~\citep{li2024long,wang2024benchmarking}. However, there is still a lack of attempts to treat long in-context learning as a means to overcome LLM limitations through long contexts~\citep{bertsch2024context,agarwal2024many}. Although some LLMs~\citep{reid2024gemini} have been shown to achieve translations of a brand new language using its grammar book and numerous demonstrations, discussions on related technical roadmap remain limited and require more open-source reproductions. Some studies also attempt to establish scaling or interpreting mechanisms between long ICL and SFT~\citep{dai2023can,mosbach2023few}, but certain theoretical analyses still need to be based on the separability assumption of softmax operations~\citep{dai2023can}, leaving a gap in practice.

Furthermore, some works also explore test-time training~\citep{sun2020test,sun2024learning}, the idea of training certain parameters of LLMs through a long context to enhance LLM capabilities or perceive user preferences. However, the context lengths involved in these works are still not sufficiently long and similarly lack corresponding scaling mechanisms. In the research on long outputs, concepts such as test-time scaling~\citep{snell2024scaling,OpenAI2024o1,zeng2024scaling} have emerged to enhance LLM performance by increasing the computational overhead of inference. However, the source of computational overhead, from long inputs or long outputs, is not clearly defined. Whether scaling inputs or scaling outputs yields more benefits is also a topic for discussion. Finally, these learning paradigms represent attempts to treat LLMs as humans, striving for ultimate life-long learning. This process will also compel us to rethink the architecture, infrastructure, and training strategy to suit these training paradigms, allowing LLMs to learn in the interactions until the Ewigkeit.

% ICL~\citep{brown2020language,pan2023context}, LongICLBench~\citep{li2024long}, \citet{bertsch2024context}, \citet{li2024demonstrations}, \citet{zhao2024probing}, MS-ICL~\citep{agarwal2024many}, GPICL~\citep{wang2024benchmarking}, precision~\citep{wang2024precision}, \citet{zou2024retrieval}


\section*{Acknowledgement}

This work is supported by the project cooperated with Huawei Noah's Ark Lab, \textit{Research on New and Efficient Architectures for Large-scale Language Models}, under the direction of Prof Qiu and Prof Guo in OpenMOSS FNLP as well as InternLM Team.

% This survey is conducted for the self-validation of the author team, to present our sincere passion and commitment to long-context research, and to extract the development trajectory of long-context from the extensive literature.

% The idea of combining this survey with a symphonic poem is proposed by Xiaoran Liu, inspired by the appreciation of "Thus Spake Zarathustra," to infuse this survey with soul and provide readers with an intuitive introduction.

If possible, I would like to dedicate this work to Prof Guo, who has worked hard over the past six months, to the memorable year with Boss Hang and Master Zhang in AI Lab, to the unforgettable moments in XinKingBo, and to the invaluable recognition from Prof Qiu.

Due to the authors' limited knowledge, this survey may contain omissions. We welcome constructive comments from readers. We will carefully consider these suggestions and release a revised version in two to three months.


\bibliography{colm2024_conference}
\bibliographystyle{colm2024_conference}

% \appendix
% \section{Appendix}
% You may include other additional sections here.

\end{document}