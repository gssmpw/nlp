\section{Inference Infrastructure}\label{sec7}

\input{figures/fig_infer_infra}

Developing effective strategies for long-context inference represents a strategic imperative for both industry and academia. In today's business landscape where API sales and Agent products dominate, efficient handling of longer contexts is essential~\citep{Google2024why-do-they-matter, koh2024visualwebarena}. Meanwhile, researchers have noted inherent limits on current pretrain paradigms~\citep{reuters2024Ilya}, especially as the growth of high-quality training data slows. 

As context lengths extend to tens of thousands or even millions of tokens~\citep{anthropic2024claude2, reid2024gemini}, inference encounters bottlenecks including quadratic complexity of attention mechanism, KV cache storage demands, communication overhead and other challenges~\citep{li2024llm, yuan2024llm}. These technical barriers directly impact inference systems' \textbf{throughput} and \textbf{latency}.

Researchers have tried to improve throughput by refining memory utilization~\citep{sheng2023flexgen}, optimizing batching techniques to maximize parallelism~\citep{anyscale2024selective}, etc. At the same time, efforts to reduce latency include but are not limited to, minimizing redundant attention calculations~\citep{jiang2024minference}, reusing KV cache~\citep{zheng2024sglang} and making the prefill and decode phases disaggregated~\citep{jin2024p}. Lastly, for contexts of hundreds of thousands or millions of tokens, there are scalable distributed solutions~\citep{fang2024uspunifiedsequenceparallelism, lin2024infinite, wu2024loongserve}.

This section ends with a curated overview of popular inference frameworks~\citep{kwon2023efficient, 2023lmdeploy, zheng2024sglang, huggingface2024huggingfaceTGI, NVIDIA2024tensorrt-llm} to guide readers in their research and deployment decisions, reflecting how today's inference engines have matured into sophisticated platforms that integrate recent findings~\citep{Gyeong280922Orca, daoflashattention, dao2022flashattention, agrawal2024taming, jin2024p} with best engineering practices.

\subsection{Memory Optimization}\label{sec7_1}

The pursuit of higher throughput has led us to optimize GPU memory usage in LLM inference systems~\citep{semianalysis2023BlackwellInferenceTraining, kwon2023efficient, zheng2024sglang}, as the growing demands of processing long sequences pose some challenges for GPU memory, which we will discuss in the following.

\subsubsection{GPU Memory Defragmentation}

PagedAttention~\citep{kwon2023efficient} leverages virtual memory paging mechanisms similar to those operating systems use to manage the KV cache on fixed-size pages. TokenAttention~\citep{LightLLM2024token_attention, hu2024lightllm} manages the KV cache at the token level, achieving zero GPU memory waste. vAttention~\citep{prabhu2024vattention, xu2024vtensor}, leverages CUDA's native virtual memory management capabilities~\citep{CUDA2020virtual}, eliminates PagedAttention-like lookup tables, resulting in reduced latency.

\subsubsection{Memory Footprint Reduction}

\paragraph{Traditional Methods}
Chunk prefill~\citep{agrawal2024taming, holmes2024deepspeed, zeng2024memorize} divides long sequences into smaller blocks for gradual processing to reduce GPU memory pressure or batch them together with decoding requests to improve overall throughput. Approximate attention mechanisms, cache-free and other non-attention architectures shown in Section \ref{sec5} can significantly reduce GPU memory costs for long-sequence computations and KV cache. Cache optimization techniques shown in Section \ref{sec3} can substantially reduce deployment memory overhead while improving processing speed through low-precision advantages.

\paragraph{Fine-grained Memory Management}
Extended sequence length has necessitated more sophisticated memory management approaches. Researchers have introduced fine-grained memory management techniques~\citep{sheng2023flexgen, he2024fastdecode, jiang2024neo, gao2024attentionstore, lee2024infinigen}. FlexGen~\citep{sheng2023flexgen} uses linear programming to select optimal storage formats and access patterns for weights and attention cache. 

The CPU memory and disk offloading~\citep{liu2023deja} need management too. Frameworks like DeepSpeed-inference~\citep{aminabadi2022deepspeed} and Huggingface Accelerate~\citep{Huggingface2022accelerate} offload the weights of large models to CPU memory. \citet{alizadeh2023llm} enables models up to twice the size of available DRAM to run by the combination of a low-rank predictor for selective neuron loading, a dynamic sliding window technique for caching activated neurons, and a row-column bundling mechanism to optimize data transfers between flash storage and DRAM.

\subsection{Computation Optimization}\label{sec7_2}

Attention computation costs grow quadratically with sequence length, creating significant latency challenges for long-context inference~\citep{beltagy2020longformer, liu2024reattention}. Recent Studies address this by optimizing system-level implementation, reducing unnecessary calculations in attention and reusing existing results.

\paragraph{System-level Implementation Optimization}
this type of work focuses purely on engineering and implementation optimizations without modifying the underlying algorithms~\citep{daoflashattention, dao2022flashattention, shah2024flashattention, cascade-inference, FlashInfer0.2, ye2025flashinfer, llama.cpp}. For example, FlashDecoding++~\citep{hong2023flashdecoding++} accelerates flat GEMM~\citep{FastGEMV, ibrahim2024balanced} with double buffering that overlaps computation and data transfer, hiding the memory latency in loading input matrices. Continuous batching~\citep{Gyeong280922Orca, anyscale2024selective, kwon2023efficient} allows new sequences to be inserted into a batch whenever existing sequences complete their generation, yielding higher GPU utilization compared to static batching. In a similar vein, Lightning Attention~\citep{minimax2025minimax01scalingfoundationmodels} introduces several system-level optimizations, such as batched kernel fusion and the separation of prefill and decoding execution. These innovations improve memory access efficiency and reduce latency in long-context inference, particularly for heterogeneous batch inputs.

\paragraph{Computational Redundancy Elimination}
Research has revealed that attention patterns are notably sparse~\citep{xiaoefficient, jiang2024minference}, with only a small subset of tokens significantly impacting next-token prediction. This insight has led to many optimization strategies that are discussed in Section \ref{sec5}.

\paragraph{KV Cache Reuse}
In practical applications, context often contains repetitive segments, while recalculating these increases latency with longer contexts~\citep{gim2024prompt}. Early approaches used simple prefix matching for cache reuse or prefix sharing in decoding~\citep{juravsky2024hydragen, cascade-inference}, integrated into deployment frameworks~\citep{NVIDIA2024tensorrt-llm, FlashInfer0.1, FlashInfer0.2, lin2024parrot} rather than published as standalone work. RadixAttention~\citep{zheng2024sglang} later improved this by organizing contexts in a radix tree structure, enabling efficient reuse with minimal CPU overhead and across requests. Another research direction employs approximation methods~\citep{hu2024epic, yao2024cacheblend} to reuse KV cache across requests with partially matching prefixes, where identical segments are not contiguous. This requires careful handling of internal attention and position embedding while approximating cross-segment attention. For instance, EPIC~\citep{hu2024epic} introduced position-independent context caching, enabling flexible cache reuse across positions without affecting model accuracy. 

\subsection{Distributed Processing}\label{sec7_3}

When context length extends to hundreds of thousands or even millions of tokens~\citep{yang2024qwen2technicalreport, qwen2024qwen25technicalreport, reid2024gemini, InternLM25}, the memory and computational capabilities of a single machine with a single GPU can no longer meet the demands. This section briefly discusses existing distributed solutions for enhancing long-context processing capabilities, focusing on Distributed Attention, scheduling strategies, and the increasingly popular Prefill-Decode (PD) disaggregation architecture.

\subsubsection{Distributed Attention}

Ring Attention~\citep{li2021sequence} enables efficient processing of long sequences by splitting them across devices. Each device stores a portion of the KV cache, reducing GPU memory usage. Since the data transfer and computation can be fully overlapped through optimization~\citep{liu2023ring, fang2024uspunifiedsequenceparallelism}, the additional communication overhead does not impact throughput. When combined with Context Parallel~\citep{Shoeybi2019MegatronLMTM}, this method could enable a longer context.

\citet{yang2024context} demonstrates near-linear scaling in long-context prefill latency through two approaches: Pass-KV, which transfers Key and Value matrices between GPUs for KV cache reuse, and Pass-Q, which transfers only Query matrices to reduce bandwidth and latency during decoding. For further exploration of how recent research has enhanced the efficiency of distributed attention, please refer to Section \ref{sec5} and \ref{sec6}.

\subsubsection{Scheduling Strategies of Inference Service}

Currently, inference service providers face two key challenges~\citep{sun2024llumnix}: the unpredictable nature of input lengths and the lack of effective scheduling strategies. As the demand for processing long texts continues to grow, the variability in input lengths has expanded, further complicating the situation~\citep{semianalysis2024ScalingLawO1Pro}. Without proper scheduling strategies, inference systems using traditional tensor, pipeline, and data parallelism alone would be less efficient at large cluster scales~\citep{guo2024survey}.

\paragraph{Disaggregated Inference}
The prefill and decoding stage of LLM inference have fundamentally different characteristics and resource requirements~\citep{Venkat2024EssentialMath, patel2024splitwise, qin2024mooncake}:
\begin{itemize}[leftmargin=2em]
    \item prefill is computationally intensive with its superlinear scaling with batch size and sequence length. Time to First Token (TTFT) is an important metric for this stage.
    \item decoding is memory(bandwidth)-constrained with its sublinear scaling with batch size. Time Between Tokens (TBT) and end-to-end latency are key metrics.
\end{itemize}

Given these differences, disaggregating the two stages~\citep{patel2024splitwise, zhong2024distserve, qin2024mooncake, hu2024inference, jin2024p} enables targeted optimization of tasks with two distinct computational characteristics, balancing computational efficiency, memory utilization, and latency requirements through independent resource pools and scheduling strategies, improving both latency and throughput.

\paragraph{Other Resource Partitioning \& Scheduling Methods}

Several innovative approaches have been proposed for resource partitioning and scheduling in LLM inference~\citep{lin2024parrot, hu2024memserve, lin2024infinite, srivatsa2024preble, wu2024loongserve}
. Infinite-LLM~\citep{lin2024infinite} allows the independent scheduling and resource allocation for non-attention layers and improves system scalability through a two-tier global and local scheduling strategy. Co-optimizing KV state reuse and computation load-balancing, Preble~\citep{srivatsa2024preble} is the first distributed LLM serving platform that targets prompt sharing. Elastic Sequence Parallelism~\citep{wu2024loongserve} dynamically adjusts to resource usage variations for prefill and decode stages, reducing KV cache migration overhead and fragmentation.

Multi-level cache management has emerged as another key optimization strategy, with several studies~\citep{jiang2024neo, qin2024mooncake, song2024powerinfer, Deepseek2024DeepSeek-V3} utilizing hierarchical distributed caches across GPUs, CPUs, DRAM, and SSDs. These studies implement load-aware scheduling and pre-estimate input/output lengths to optimize resource utilization.


\subsection*{Open Source Frameworks}

Open-source frameworks have proven effective for handling context lengths of up to 100k tokens. More recent frameworks have optimized sequence processing through structured output~\citep{zheng2024sglang} and cache reuse while maintaining high throughput~\citep{kwon2023efficient, zheng2024sglang}. Organizations and famous enterprises have also released open-source inference frameworks~\citep{qin2024mooncake, NVIDIA2024tensorrt-llm, zhihu2024ZhiLight, 2023lmdeploy, huggingface2024huggingfaceTGI}, each offering unique features. The accumulated engineering expertise from these projects has enriched technical options and advanced the field toward maturity.

% In 2024, open-source inference engines have made substantial technical progress. Earlier in the year, these frameworks primarily supported decoder-only models with limited capabilities for long text lengths (e.g., 100k). Today’s inference engines have evolved into comprehensive platforms that incorporate the latest academic research and engineering practices【chunkPrefill, speculative sampling, sequence parallelism, prefixCache, Multi-LoRA, JSON output, observability principles, graph optimization, continuous batch】. These platforms support multimodal models, sparse mixture-of-experts (MoE), and state-space models (SSM), and have developed technological ecosystems for resource scheduling, and hardware acceleration.

\paragraph{vLLM} 
Developed by the University of California, Berkeley, vLLM~\citep{kwon2023efficient} is renowned for its PagedAttention mechanism and strong open-source community support. It supports a wide range of models, including multimodal and non-Transformer architectures, and is compatible with diverse hardware. The upcoming version 1.0 will address previous limitations such as reliance on serial scheduling, limited graph optimization, and complex codebases that hinder further development.

\paragraph{SGLang}
Also from UC Berkeley, SGLang~\citep{zheng2024sglang} is primarily written in Python and optimized with the torch.compile tool. It features optimizations like Radix Attention, structured output enhancements, and multi-process GMP transmission, which significantly reduce CPU overhead.

\paragraph{LMDeploy}
Developed by SenseTime and the Shanghai AI Laboratory, LMDeploy~\citep{2023lmdeploy} provides implementations based on both CUDA and Triton acceleration. This framework supports multimodal tasks effectively and includes several commonly used pre-trained models.

\paragraph{Huggingface's Text Generation Inference}
Huggingface's Text Generation Inference (TGI)~\citep{huggingface2024huggingfaceTGI} also utilizes the PagedAttention mechanism and employs Rust for low-level functions and Python (70\%) for higher-level layers. Despite this, its throughput performance is average, particularly with larger batch sizes, due to decreased GPU memory management efficiency. Additionally, its CPU-GPU serial scheduling design limits GPU resource utilization.

\paragraph{TensorRT-LLM}
TensorRT-LLM~\citep{NVIDIA2024tensorrt-llm} is NVIDIA's open-source framework on their GPUs. The framework stands out for its comprehensive optimization of popular LLMs, multiple NVIDIA hardware platforms(H100, L40, A100, V100, T4, etc.), flexible customization of plugins and kernels, and seamless multi-GPU/multi-node deployment capabilities. 