\section{Training Infrastructure}\label{sec6}

\input{figures/fig_train_infra}

Although architectural innovation has achieved great progress, the mainstream long-context LLMs are still based on Transformer~\citep{dubey2024llama,llama3_3,Deepseek2024DeepSeek-V3} or hybrid architectures~\citep{team2024jamba,minimax2025minimax01scalingfoundationmodels}. Therefore, we need to make long-context training and inference possible while accepting the inherent drawback of the self-attention mechanism. To further the journey of extending context length, we turn our focus to the practical training and inference of long-context LLMs to explore infrastructure improvement. Whether for long-context training discussed in Section~\ref{sec6} or inference infrastructure discussed in Section~\ref{sec7}, the focus of research all involve: computation, storage, and distribution, namely parallelism as shown in Figure~\ref{fig:train_infra} and Figure~\ref{fig:infer_infra}.

For example, for training infrastructure, 
% Recent advancements in long-context modeling and associated architectures have rapidly extended the context window lengths of LLMs. 
currently, leading LLMs support context lengths exceeding 128k tokens~\citep{meta2024introducing, dubey2024llama, yang2024qwen2technicalreport} and up to 256k tokens during pre-training (e.g., Qwen2.5~\citep{qwen2024qwen25technicalreport}). At such a context length, the distributed parallelism strategies address the basic question of training possibility. Beyond that, handling such long sequences imposes significant memory demands and necessitates enhanced hardware utilization efficiency:

\begin{itemize}[leftmargin=2em]
\item GPU memory overhead scales proportionally with context length through activation values and optimizer states~\citep{guo2024survey, duan2024efficient}. The demand for memory bandwidth intensifies due to larger tensor sizes~\citep{semianalysis2023AICapacity, semianalysis2024trainium2}. The growth in GPU memory capacity and memory bandwidth has consistently fallen behind advances in GPU computational power~\citep{gholami2024ai, semianalysis2023AICapacity}, further exacerbating the aforementioned challenges.
\item Memory-Flops Utilization (MFU) represents the ratio of actual computational use to theoretical hardware performance. Large-scale long-context distributed training introduces considerable computational \& communication overhead~\citep{gu2024loongtrain, sun2024seq1f1befficientsequencelevelpipeline}, reducing  MFU. Accommodating longer contexts typically necessitates smaller batch sizes, thereby decreasing throughput.  
\end{itemize}

We will briefly review mixed-precision training work~\citep{narang2017mixed, kalamkar2019study, sun2019hybrid, peng2023fp8, dubey2024llama, Deepseek2024DeepSeek-V3} at the end of this section, as it reduces GPU memory requirements and increases MFU, however expanding the supported context length of current training systems only indirectly.

\subsection{Distributed Parallelism Strategies}\label{sec6_1}

Training modern AI models with extensive context windows has become increasingly complex, pushing beyond what single GPUs can handle~\citep{semianalysis2023BlackwellInferenceTraining}. This challenge has led to the development of sophisticated distributed training approaches, particularly when dealing with long context.

\subsubsection{Data, Tensor, and Pipeline Parallelism}

The foundational and most widely adopted approaches in the distributed parallelism are~\citep{semianalysis2023BlackwellInferenceTraining}: Data Parallelism (DP), which distributes input data across multiple GPUs~\citep{li2020pytorch, zhao2023pytorch, Zhang2024SimpleFSDPSF, sun2024co2}; Tensor Parallelism (TP), which splits model parameters matrices across devices; and Pipeline Parallelism (PP), which distributes model layers across GPUs. While each approach offers distinct advantages, they also present unique challenges. TP, for instance, effectively manages memory constraints but typically requires high-bandwidth communication between devices~\citep{dong2024lowbitcommunicationtensorparallel}. Similarly, PP often encounters efficiency losses due to pipeline bubble, and efforts are being made to eliminate this problem~\citep{Li2021TeraPipeTP, Qi2024ZeroB, arfeen2024pipefillusinggpusbubbles}. \citet{sun2024seq1f1befficientsequencelevelpipeline} schedules the pipeline of training LLMs at the sequence level on sequences up to 64k, reducing pipeline bubbles and memory footprint.

\subsubsection{Distributed Attention}   

Sequence Parallelism (SP), specifically designed for long-context training, partitions input and output tensors along the sequence dimension at the Transformer layer level. It facilitates distributed processing of attention computations~\citep{li2021sequence} and other operations ~\citep{Shoeybi2019MegatronLMTM}. \citet{Bian2021ColossalAIAU} introduced a sequence dimension partitioning and parallelization scheme. Ring Attention~\citep{li2021sequence} then employs block-wise attention computation combined with a ring communication pattern to partition QKV tensors along the sequence dimension, distributing computation across devices. Ring Attention can be integrated with FlashAttention~\citep{dao2022flashattention, daoflashattention}, preserving IO-awareness and memory efficiency. Ring attention with block-wise transformers~\citep{liu2023ring} further enhances the overlap between communication and computation, enabling the training of sequences exceeding 100 million tokens. Varlen Ring Attention~\citep{minimax2025minimax01scalingfoundationmodels} avoids the excessive padding and subsequent computational waste associated with traditional methods by applying the ring attention algorithm directly to the entire sequence after data-packing. To address Ring Attention's load imbalance in causal attention mask scenarios, several optimization~\citep{Brandon2023StripedAF, li2024distflashattn, fang2024uspunifiedsequenceparallelism, gu2024loongtrain, minimax2025minimax01scalingfoundationmodels} solutions have emerged. Alternatively, Megatron-LM~\citep{Shoeybi2019MegatronLMTM} achieves load balancing through input token reordering.

Ulysses-Attention~\citep{jacobs2023deepspeed} introduces head-parallel stratification atop sequence dimension partitioning, enabling parallel attention head processing across GPU devices. The 2D-Attention mechanism~\citep{gu2024loongtrain} resolves head-parallel strategy scalability limitations while addressing efficiency constraints present in previous context-parallel approaches such as \citet{Brandon2023StripedAF} and \citet{li2024distflashattn}. \citet{sun2024linearattentionsequenceparallelism} tailored to linear attention-based language models, scales sequence length up to 4096k.

In practical implementations, ultra-long context(eg. longer than 256k)~\citep{qwen2024qwen25technicalreport} training typically requires a strategic combination of multiple parallelism approaches. For example, common configurations integrate tensor and sequence parallelism within individual nodes while implementing data parallelism across machines. This hybrid parallelism methodology~\citep{Shoeybi2019MegatronLMTM, Narayanan2021EfficientLL, jacobs2023deepspeed, chen2024internevo, singh20234d, fujii2024acceleratinglargelanguagemodel, dubey2024llama} enables effective scaling to larger computing clusters, substantially enhancing pre-training and fine-tuning efficiency.  In particular, Varlen Ring Attention~\citep{minimax2025minimax01scalingfoundationmodels} can avoid excessive padding by applying the ring attention algorithm directly to the entire sequence after (varlen-like) data-packing. This flexable integration improves computational efficiency in ultra-long context scenarios up to 1024k tokens. However, existing automatic parallelism tools require further optimization for the unique computation and communication patterns characteristic of ultra-long context scenarios.

\subsection{Alleviating GPU Memory Pressure}\label{sec6_2}

GPU memory constraints have emerged as a critical bottleneck in model training as context windows expand. This pressure stems primarily from~\citep{gholami2024ai, guo2024survey, duan2024efficient}: 

\begin{itemize}[leftmargin=2em]
\item Model parameters themselves
\item activation values and optimizer states
\item inter-device communications
\item temporary space allocations and GPU memory fragmentation
\end{itemize}

While not specifically designed for long-context processing, current solutions offer valuable insights for training such models. We will provide a concise overview.

\subsubsection{Activation Recomputation}

GPU memory usage scales with sequence length. Activation recomputation~\citep{chen2016training, chen2024optimizing} trades computational power for memory space, addressing memory constraints while potentially improving the compute-to-memory ratio and helping resolve memory bottlenecks.

Selective checkpointing~\citep{korthikanti2023reducing, torch2024selective} methods preserve outputs from critical layers, such as attention modules~\citep{li2024distflashattn}, while recomputing other intermediate results as needed. Selective-Checkpoint++~\citep{gu2024loongtrain} significantly reduces memory usage while maintaining performance by adding attention modules to a whitelist and preserving their softmax outputs.

In contrast to static strategies, dynamic recomputation approaches determine which activation values to discard and recompute at runtime. \citet{Kirisame2020DynamicTR} and \citet{Hu2022MegTaiChiDT} employs heuristic methods for runtime tensor eviction and recomputation, while \citet{zhao2024efficientlytraining7bllm} uses a token-wise activation recomputation and swapping mechanism with linear programming to optimize, like, activation value recomputation.

\subsubsection{Redundancy Reduction}

The Zero Redundancy Optimizer (ZeRO) introduces a progressive sharding scheme to minimize memory redundancy~\citep{rajbhandari2020zero}. ZeRO-1 distributes optimizer states across GPUs, ZeRO-2 extends this to gradients, and ZeRO-3 further shards model parameters, effectively dividing the total memory overhead by the parallel dimension. While this comprehensive sharding minimizes redundancy, it increases communication overhead. Numerous other works~\citep{wu2023rethinking, luo2023rtp, chen2024lins} have tackled communication efficiency and mitigated communication costs. ZeRO++~\citep{wang2023zero++} redundantly stores an additional set of secondary parameters on each node, enhancing communication efficiency through parameter prefetching. MiCS~\citep{Zhang2022MiCSNS} and Fully Sharded Data Parallel (FSDP)~\citep{zhao2023pytorch} shard all model state components within subgroups and replicate them between subgroups to reduce communication scale. 

\subsubsection{GPU Memory Defragmentation \& Offloading}

Device memory limits affect manageable sequence length, requiring techniques like fragmentation elimination and offloading to expand capacity.

GPU memory defragmentation falls into two categories: tensor-based method~\citep{Kirisame2020DynamicTR, Hu2022MegTaiChiDT, shu2023roam, zhao2024efficientlytraining7bllm, zhang2024coop} and Virtual Memory Management (VMM). For tensor-based approaches, ROAM~\citep{shu2023roam} optimizes operator execution order and tensor allocation strategies using efficient tree-structured algorithms to identify optimal execution plans. MEMO~\citep{zhao2024efficientlytraining7bllm} and Coop~\citep{zhang2024coop} also address memory fragmentation while reducing overall memory consumption. VMM-based solutions, such as GMLake~\citep{guo2024gmlake} and PyTorch Expandable Segments~\citep{PyTorch2024Expandable_Segments}, utilize low-level CUDA driver APIs~\citep{CUDA2020virtual} to consolidate non-contiguous memory blocks into larger, contiguous segments through virtual memory address mapping.

Offloading technologies include CPU and SSD approaches. CPU offloading encompasses Static Offloading~\citep{pudipeddi2020training, ren2021zero} and Dynamic Offloading~\citep{sun2022stronghold, li2022harmony}. SSD Offloading solutions~\citep{rajbhandari2021zero, jang2024smart, liao2024adding} enable training of trillion-parameter models beyond CPU offloading capabilities.
Recent advancements have proposed comprehensive solutions for managing high activation value occupancy and memory fragmentation during training. \citet{zhao2024efficientlytraining7bllm} employs token-level decisions to determine which activation values to recompute and which to transfer to CPU memory, utilizing integer programming for memory allocation and space reuse by leveraging the uniform structure of Transformer layers. Ulysses-Offload~\citep{yao2024training} achieves substantial GPU memory reductions through its novel Distributed Attention with Fetching and Offloading mechanism, and leverages a dedicated double buffer design to overlap almost all fetching with computation.

\subsection{Enhancing Model FLOPs Utilization}\label{sec6_3}

Despite access to large-scale GPU clusters, LLaMA3.1~\citep{dubey2024llama} achieves a mere 38-41\% Model FLOPs Utilization (MFU), suggesting substantial room for optimization. These inefficiencies~\citep{duan2024efficient} are exacerbated when handling longer context (e.g. longer than 32k).

\begin{itemize}[leftmargin=2em]
\item Data processing operations, including sequence packing and tokenization, encounter significant challenges with extended sequences.
\item Longer sequence length results in quadratic growth in attention computation complexity. The memory bandwidth of current accelerator cards lags behind this computational surge, leading to longer processing times and reduced MFU.
\item Different sequence lengths from 2k to 128k and above complicate load balancing and efficient scheduling.
\end{itemize}

\subsubsection{Training Data Pipeline for Long-Context Models}

Processing longer sequences introduces specific challenges in the training data pipeline, particularly in text sorting, packing, and tokenization. While research in this area remains limited, the training data pipeline for long-context training is a critical challenge that warrants further investigation, as discussed in  \textbf{\nameref{q8_balance}} in Section\ref{sec12}.

Training only on long data hurts models' long-context performance~\citep{gao2024train}. The conventional approach of batch-packing sequences of similar lengths introduces potential training biases through length uniformity, while random long \& short-sequence packing results in GPU underutilization. To address this, GLM-Long~\citep{chatglm2024glmlong} organizes batches based on computational complexity, ensuring uniform computational complexity across packages and significantly reducing GPU idle periods. Furthermore, GLM-Long employs layer accumulation techniques to mitigate sorting-induced biases and utilizes loss reweighting strategies to handle imbalanced data volumes across packages. 

Tokenization inherently allows for parallel processing along the sequence dimension. ParallelTokenizer~\citep{cai2024internlm2, ParallelTokenizer} leverages this by implementing parallel tokenization.

\subsubsection{Operator Optimization}

Optimizing operators primarily involves enhancing the Transformer's core computationâ€”the attention mechanism. FlashAttention~\citep{dao2022flashattention, daoflashattention} represents a significant advancement in this domain by optimizing memory access patterns through block-wise computations, enabling efficient use of on-chip fast memory. This approach reduces latency without compromising attention accuracy and eliminates quadratic memory complexity, thereby supporting long-context training. FlashAttention-3~\citep{shah2024flashattention} further optimizes for H100 GPUs by fully utilizing hardware features such as asynchronous WGMMA instructions. Simultaneously, normalization, dropout and feed-forward network (FFN) computations have undergone engineering optimizations~\citep{liu2023ring, Ma2024ReducingTC, Shoeybi2019MegatronLMTM}, often through operator fusion. For instance, the JAX implementation of Ring Attention with Blockwise Transformers~\citep{liu2023ring} incorporates operator fusion for FFN, enhancing computational efficiency. Native Sparse Attention (NSA)~\citep{minimax2025minimax01scalingfoundationmodels} introduces a hardware-aligned sparse strategy with dynamic token compression and selection, achieving substantial speedups by writing a triton kernel.

Compiler-level optimizations have also made significant strides, particularly with OpenAI Triton~\citep{tillet2019triton} and other frameworks~\citep{dong2024flex, Spector2024ThunderKittensSF}. Triton offers a Python-based programming language and an MLIR-based~\citep{lattner2020mlir} compiler enriched with built-in optimizations, facilitating the development of high-performance operators through a user-friendly interface. Additionally, compiler-level operator fusion, which often requires comprehensive computation graph information~\citep{chen2018tvm, PyTorch2024torch_compiler, wu2024multi}, automates optimization processes, thereby improving MFU.

\subsubsection{Scheduling Optimization}

Scheduling optimization is critical for enhancing training efficiency in long-context LLMs. As LLMs scale and context window size increases, factors such as computation-communication overlap~\citep{wang2024hiding}, load balancing, and CPU time significantly influence training speed (tokens per GPU per second)~\citep{dubey2024llama, Deepseek2024DeepSeek-V3}.  Given the limited research specifically for typical long-context, this section provides a concise overview.

Recent workload-scheduling developments have been tailored to LLMs. \citet{xue2024codesign} optimizes concurrent training efficiency through hybrid parallel strategies and hardware affinity in heterogeneous clusters. Hydro~\citep{hu2023hydro} enhances hardware utilization through model scaling and consolidation, while \citet{hu2024characterization} addresses mixed workload characteristics through solutions such as decoupled evaluation scheduling.

Resource-level improvements have also emerged. For example, SiloD~\citep{zhao2023silod} jointly allocates data caching and remote I/O as first-class resources, significantly improving system throughput. 

\subsubsection*{mixed-precision training}
In addition to the aforementioned methods, there are numerous approaches~\citep{narang2017mixed, kalamkar2019study, sun2019hybrid, dubey2024llama, qwen2024qwen25technicalreport, Deepseek2024DeepSeek-V3} that improve the long context training throughput and MFU from the perspective of mixed-precision training. \citet{wang2023bitnet} explores 1-bit precision training. Recent hardware and framework developments~\citep{xi2024jetfire, xi2023training, jacobs2023deepspeed, Shoeybi2019MegatronLMTM, Bian2021ColossalAIAU, peng2023fp8, torchtitan, meta_lingua, NVIDIA2024Transformer_Engine} have expanded support for lower precision operations (in FP8, FP4, INT4, etc.), offering new avenues for further enhancing MFU.