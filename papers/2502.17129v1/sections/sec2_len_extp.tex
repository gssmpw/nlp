\section{Length Extrapolation}\label{sec2}

\input{figures/fig_len_extp}

In this section, we start the journey of extending the context length of LLMs with length extrapolation, the foundation of long-context LLMs, as shown in Figure~\ref{fig:len_extp}. 
\begin{itemize}
    \item In \S\ref{sec2_1}, we start with some preliminary knowledge, including \textbf{\textit{position embedding}} and \textbf{\textit{the definition of length extrapolation}}. Then we focus on the length extrapolation based on the widely-used RoPE~\citep{su2024roformer}. 
    \item In the inference stage, as discussed in \S\ref{sec2_2}, the extrapolation is based on \textbf{\textit{limiting position information}} including NTK~\citep{dynamicNTK}, ReRoPE~\citep{rerope} and DCA~\citep{an2024training} or \textbf{\textit{short-context collaboration}} like PCW~\citep{ratner2022parallel}. 
    \item In the training stage, as discussed in \S\ref{sec2_3}, apart from the classical \textbf{\textit{position interpolation}} methods such as LinearPI~\citep{chen2023extending} and YaRN~\citep{pengyarn}, we highlight the discussion of \textbf{\textit{extrapolation mechanism}}~\citep{liuscaling,men2024base} and \textbf{\textit{efficient extrapolation}}~\citep{chenlonglora,zhu2023pose}.
    \item  In \S\ref{sec2_4}, we will add more discussion beyond RoPE, including \textbf{\textit{NoPE}}~\citep{kazemnejad2024impact}, other position embeddings~\citep{golovneva2024contextual,dong2024exploring} and \textbf{\textit{attention entropy}}~\citep{han2024lm,zhang2024extending} 
\end{itemize}

\subsection{Preliminary}\label{sec2_1}

\subsubsection{Position Embedding}

% \paragraph{Evolution of Mainstream Position Embeddings}
% The sinusoidal position embedding~\citep{Vaswani2017attention} represents the earliest mainstream approach. It uses sinusoidal and cosine functions of varying frequencies to generate position embeddings. However, this embedding exhibits significant limitations: for positions beyond the training length, the embedding pattern undergoes substantial changes, resulting in limited extrapolation capabilities.

% To overcome the constraints of absolute position embeddings, researchers propose various relative position embeddings. Rather than assigning unique embeddings to positions, these approaches focus on the relative positional information between tokens. Beyond the classic relative position embedding~\citep{shaw2018self}, variants include T5~\citep{raffel2020exploring}, TENER~\citep{yan2019tener}, and XLNET~\citep{dai2019transformer}.

% These relative position embeddings intuitively model the distance information between tokens, enabling consistent representation of identical local patterns at different positions while achieving higher parameter efficiency. Nevertheless, these embeddings face certain limitations, including substantial computational overhead, challenges in simultaneously maintaining global positional awareness and local precision, and potential inaccuracies in representing long-distance positional relationships.

%Position embedding, introduced in the original Transformer paper~\citep{Vaswani2017attention}, is part of encoding positional information, which is critical to the long context in the architecture, and is still widely adopted in current LLMs. 
First introduced in \citet{Vaswani2017attention}, position embedding is a key mechanism for encoding positional information in contexts, and remains fundamental to modern LLMs' ability to process long-context. 
%Position embedding in Transformer starts with absolute position embedding~\citep{Vaswani2017attention}
The evolution of position embedding begins with absolute position embedding~\citep{Vaswani2017attention}, namely embedding based on the token indices, and follows by the emergence of relative position embedding, namely embedding based on the token distance, such as \citet{shaw2018self}, T5~\citep{raffel2020exploring}, TENER~\citep{yan2019tener} and XLNET~\citep{dai2019transformer}. However, those embeddings face trade-offs between performance and computational efficiency. Later, RoPE~\citep{su2024roformer} is proposed, achieving relative position embedding through absolute position embedding, combining the advantages of both approaches and thus becoming a significant academic interest~\citep{chowdhery2023palm,touvron2023llama,touvron2023llama2,Sun2024MOSS,chen2023extending,dynamicNTK}.

% \paragraph{RoPE and Its Advantages}
RoPE~\citep{su2024roformer} introduces positional information into self-attention computation through rotary transformations. Given a position index $t$ and an embedding vector $\bm{x}=[x_0,x_1,...x_{d-1}]^T$, where $d$ is the attention head dimension, RoPE defines a complex function:
\begin{equation}\begin{gathered} \bm{A}_{m,n}=\underbrace{\bm{x}_{m}\bm{W}_{Q}\bm{R}_{\Theta,m-n}^d\bm{W}_{K}^{T}\bm{x}_{n}^{T}}_\text{relative position embedding}=\underbrace{\bm{x}_{m}\bm{W}_{Q}\bm{R}_{\Theta,m}^d\left(\bm{x}_{n}\bm{W}_{K}\bm{R}_{\Theta,n}^d\right)^T}_\text{absolute position embedding} \\[0.5ex]  
\end{gathered}\text{,}\end{equation}
% $$f(x,t,\theta)=[(x_0+ix_1)e^{it\theta_0},(x_2+ix_3)e^{it\theta_1},...,(x_{d-2}+ix_{d-1})e^{it\theta_{d/2-1}}]^T$$
where $\theta_j=\beta^{-2j/d}$, with a typical value of rotary base $\beta=10000$.

% RoPE offers several significant advantages. First, it naturally models relative positional relationships without requiring additional relative position bias. Second, it demonstrates excellent compatibility, seamlessly integrating with various attention variants. Moreover, compared to traditional position embeddings, RoPE exhibits superior generalization capabilities when processing contexts beyond training length. Finally, RoPE maintains low computational complexity and eliminates the need to store additional positional representation matrices.
RoPE has several significant advantages. First, RoPE has solid mathematical foundations with theoretical guarantees~\citep{su2024roformer}. Second, RoPE maintains low computational complexity and eliminates the necessity for storing any position embedding matrices~\citep{su2024roformer,touvron2023llama,chowdhery2023palm}. Third, RoPE can seamlessly integrate with many attention variants, demonstrating excellent compatibility~\citep{su2024roformer}. Finally, through the following improvements, RoPE has shown strong length extrapolation capabilities~\citep{dynamicNTK,liuscaling}. Given these favorable properties of RoPE, many LLMs adopt RoPE as their position embedding~\citep{dubey2024llama,glm2024chatglm,wang2024qwen2,young2024yi,cai2024internlm2}. 

\subsubsection{Length Extrapolation}
% 这里暂时还没有把model改成LLM，因为ALiBi中的原话定义是这样的，待定
In Transformer-XL~\citep{dai2019transformer}, discover the standard Transformer's limitations in handling sequences longer than its training length. ALiBi~\citep{presstrain} later formalizes this as \textbf{length extrapolation} or \textbf{length generalization}, a model's capacity to maintain performance when processing longer sequences during inference than during training.
%\paragraph{Early Work Before Linear Interpolation}
%Before the widespread adoption of RoPE linear interpolation, early approaches to length extrapolation include position embeddings optimization and sliding window mechanisms. ALiBi~\citep{presstrain,yang2023baichuan} replaces position embeddings with fixed attention biases that scale linearly with relative positional information. This method demonstrates remarkable effectiveness in handling sequences beyond training length. xPos~\citep{sun2022length,sun2023retentive} enhances RoPE by incorporating exponential decay in attention score computation, theoretically enabling extrapolation to arbitrary lengths while preserving crucial relative positional information.
%Building upon the importance of initial tokens, LM-Infinite~\citep{han2024lm} and StreamingLLM~\citep{xiaoefficient} introduce $\Lambda$-shaped masks and attention sinks respectively. These approaches preserve information from both global initial tokens and local window tokens, implementing attention window truncation to reduce computational complexity while maintaining performance.
% 没找到改进位置编码相关描述，暂时改成了improvements of position embedding
Before the widespread adoption of RoPE-based linear interpolation, early approaches to length extrapolation include improvements in position embedding and sliding window mechanism~\citep{presstrain,sun2022length,ratner2022parallel}.

% \paragraph{Improvements of Position Embedding}
%由于ALiBi中滑动窗口只在部分评测中使用，因此这里没有点出，在xPos方法中点出了滑动窗口，然后进一步点出了LM-Infinite和StreamingLLM
For example, ALiBi~\citep{presstrain,yang2023baichuan} introduces fixed attention biases that scale linearly with relative positional information, showing promising results in contexts beyond training length. Later, xPos~\citep{sun2022length,sun2023retentive} addresses the length extrapolation problem by incorporating exponential decay in attention computation and proposing BCA, a windowed attention mechanism similar to a sliding window. After LLM emerges, the sliding window mechanism is first been used for the earliest length extrapolation attempts~\citep{bai2023qwen,jiang2023mistral}. Besides, more sophisticated sliding window variants are proposed. For example, LongNet~\citep{ding2023longnet} achieves length extrapolation to 1B tokens by dilated sliding window attention. Subsequently, LM-Infinite~\citep{han2024lm} and StreamingLLM~\citep{xiaoefficient} introduce $\Lambda$-shaped masks and attention sinks respectively. These two methods preserve information from global initial tokens and local window tokens, implementing attention window truncation to reduce computational complexity while maintaining LLM's performance.

\paragraph{Distinguishing Weak and Strong Extrapolation}
It is essential to distinguish two types of extrapolation capabilities, weak extrapolation and strong extrapolation. \textit{\textbf{Weak extrapolation}} refers to maintaining perplexity across varied context lengths, while \textit{\textbf{strong extrapolation}} indicates the ability to maintain performance on actual long-context understanding and processing tasks. These capabilities can be delineated by examining which tasks maintain consistent performance across different context lengths. For instance, StreamingLLM~\citep{xiaoefficient} demonstrates effective weak extrapolation in perplexity but does not guarantee equivalent performance in practical long-context tasks, as evaluated by benchmarks including NIAH~\citep{niah} and RULER~\citep{hsieh2024ruler}. The conflict of perplexity between its failure to reflect practical context length and its wide application in the long-context research will be further analyzed in \textbf{\nameref{q3_ppl}} in Section\ref{sec12}.

This distinction is crucial, as many length extrapolation works focus only on weak extrapolation~\citep{han2024lm,xiaoefficient,ding2023longnet}. The following discussion focuses on strong extrapolation. Given LLMs' predominant use of RoPE, we first explore the extrapolation of RoPE-based LLMs. Based on implementation stages, these methods can be categorized into inference-time and training-time extrapolation.
%LongNet~\citep{ding2023longnet} achieves training-free extrapolation in inference by dilating attention, reducing token count for attention computation, thereby limiting position embedding's indices.

\subsection{RoPE Extrapolation in Inference}\label{sec2_2}
At inference time, there are two feasible approaches for enabling LLMs to comprehend longer context lengths. The first approach involves constraining position embeddings during the processing of extended contexts, and the second approach implements segmented understanding where the model processes long contexts in chunks and integrates understanding across these segments.

\subsubsection{Limiting Position Information} 
In RoPE~\citep{su2024roformer}, positional information is represented through trigonometric functions of the product of index and rotary angle. To maintain this product within pre-training bounds as indices increase, approaches including limiting index growth or reducing rotary angles are proposed. Fixed or dynamic NTK methods~\citep{fixedNTK,dynamicNTK} achieve plug-and-play length extrapolation by adjusting RoPE's rotary base and have been widely adopted, while more extrapolation works in inference focus on index limitation.

ReRoPE~\citep{rerope} and SelfExtend~\citep{jin2024llm} explicitly set relative position upper bounds in RoPE to constrain positional information within pre-training ranges. Similarly, InfLLM~\citep{xiao2024infllm} and LongHeads~\citep{lu2024longheads} enable training-free processing of ultra-long sequences through block-level context storage, focusing attention on crucial blocks at the beginning, end, and middle of input text. ReAttention~\citep{liu2024reattention} implements customized operators for fine-grained KV cache retrieval across the full context, enabling plug-and-play context window expansion by at least 100 times. DCA~\citep{an2024training} innovatively decomposes long sequence attention computation into intra-block, adjacent-block, and non-adjacent block components for more efficient long text processing, while String~\citep{an2024does} further simplifies this design and improves performance.

\subsubsection{Short-context Collaboration}
Short-context Collaboration refers to a series of extrapolation methods that process long texts by splitting them into shorter segments and synthesizing the results. PCW~\citep{ratner2022parallel} ensures all processing remains within pre-training length limits by dividing sequences into multiple context segments and one task sequence. NBCE~\citep{kexuefm9617} applies Naive Bayes principles to achieve length extrapolation through independent processing of context segments with prompts. XL3M~\citep{wang2024xl3m} introduces a training-free framework handling long contexts through segmented inference, while LLM×MapReduce~\citep{zhou2024llm} adopts distributed computing concepts, processing text blocks across GPUs with specialized communication structures. Additionally, LongAgent~\citep{zhao2024longagent}, an extrapolation method in training, also employs a similar approach by introducing multi-agent collaboration, where multiple agents cooperate to process long contexts.

\subsection{RoPE Extrapolation in Training}\label{sec2_3}

\subsubsection{Position Interpolation}
Beyond extrapolation methods in inference, researchers propose numerous approaches in training that focus on leveraging short-context positional information for longer contexts through position interpolation~\citep{liuscaling, xiong2024effective}. These methods similarly address either index adjustment or rotary base scaling.

For index adjustment, LinearPI~\citep{chen2023extending} first introduces linear scaling of position indices through a scaling factor to extend context length. However, it remains limited by training length and neglects feature differences across RoPE's query and key vectors' dimensions. YaRN~\citep{pengyarn} subsequently implements dynamic scaling in middle dimensions while maintaining no interpolation in low dimensions and full interpolation in high dimensions, achieving 128k length extrapolation with 64k training. YaRN gains wide adoption in subsequent LLMs like LLaMA3.1~\citep{dubey2024llama}. Similarly, Giraffe~\citep{pal2023giraffe} achieves extrapolation by preserving high-frequency rotations while suppressing low-frequency ones. Additionally, LongRoPE~\citep{ding2024longrope} employs progressive search-based non-uniform interpolation to achieve 2M context length with 256k training.

On the other hand, many models adopt enlarged rotary angles combined with longer training lengths~\citep{roziere2023code, xiong2024effective}. This approach is widely adopted in current LLMs~\citep{cai2024internlm2, young2024yi, chatglm2024glmlong} to achieve long contexts. LWM~\citep{liu2024world} implements multi-stage scaling, gradually increasing both the rotary angle base and fine-tuning length. However, these works make specific attempts on certain context lengths and rotary bases without thoroughly investigating the extrapolation mechanism of RoPE-based LLMs. Apart from the search for mechanism, DPRoPE~\citep{wu2024extending} explores optimizing RoPE rotary angle distributions to enhance extrapolation capabilities and CLEX~\citep{chenclex} introduces neural ordinary differential equations to model continuous scaling of position embedding. 

\subsubsection{Scaling Laws}

As previously discussed, the extrapolation mechanism of RoPE-based LLMs remains a crucial question in length extrapolation research. The keys to this question are the \textbf{\textit{periodicity}} and \textbf{\textit{monotonicity}} of trigonometric functions~\citep{pengyarn,liuscaling,men2024base}. YaRN~\citep{pengyarn} first mentions the relationship between the RoPE-based extrapolation and the periodicity. Furthermore, ScalingRoPE~\citep{liuscaling} identifies a critical dimension $d_\text{extra}$, decided by the pre-training context length $T_\text{train}$ and original rotary base $\beta$, that determines the LLM's extrapolation limit, as shown in Equation \ref{equ:d_extra}. 
\begin{equation}
    d_\text{extra}=2\left\lceil\frac{d}{2}\log_\beta\frac{T_\text{train}}{2\pi}\right\rceil\text{.}
\label{equ:d_extra}\end{equation}
% $$ (1)$$
For dimensions before the critical dimension, their position embedding $\sin(\theta t), \cos(\theta t)$ have already experienced a complete period in pre-training and will not be out-of-distribution (OOD) in extrapolation. However, dimensions beyond that will fail to extrapolate when the product of the rotary angle and position index exceeds the range the LLM pre-trained in. Since rotary angles in RoPE are arranged exponentially~\citep{su2024roformer}, the rotary angle at the critical dimension experiences the least shrinkage in base scaling. Consequently, the position embedding at this dimension will first be OOD, making its period serve as the upper bound for extrapolation, $T_\text{extra}$, as shown in Equation \ref{equ:t_extra}.
\begin{equation}
    T_\text{extra}=2\pi\cdot\beta^{\frac{d_\text{extra}}{d}}=2\pi\cdot\beta^{\left\lceil\frac{d}{2}\log_{10000}\frac{T_\text{train}}{2\pi}\right\rceil\cdot\frac{2}{d}}\text{.}
\label{equ:t_extra}\end{equation}
%$$T_\text{extra}=2\pi\cdot\beta^{\frac{d_\text{extra}}{d}}=2\pi\cdot\beta^{\lceil \frac{d}{2}\log_{10000}\frac{T_\text{train}}{2\pi}\rceil\cdot\frac{2}{d}} (2)$$

\citet{liuscaling} reveals a part of the extrapolation mechanism in RoPE-based LLM, that RoPE's extrapolation represents position information in a longer context using that previously learned in short-context pre-training. However, forcing LLM to learn more position information in fine-tuning, such as reducing the rotary base, is inappropriate~\citep{men2024base}. \citet{men2024base} proves that reducing rotary bases undermines contextual information modeling because it disrupts the original patterns and overlooks the second feature, monotonicity. The $\cos(\theta t)$ maintains monotonicity locally, reflecting relative distance~\citep{wei2025videorope}. A sufficiently smaller base can prevent position embedding from OOD based on periodicity, but this sacrifices monotonicity, limiting LLMs to perceiving local semantics and performing poorly on generation and ICL tasks~\citep{liuscaling, men2024base}, showing only weak extrapolation. This reveals a contradiction in RoPE, that \textit{\textbf{dimensions with monotonicity perceivable of long dependencies are overfitted to pre-training context and cannot extrapolate, while dimensions capable of extrapolation lose monotonicity and cannot perceive long contexts}}, which will be further analyzed in \textbf{\nameref{q2_rope}} in Section\ref{sec12}.

Although \citet{liuscaling} makes a mistake on the second part, it still has a guiding significance for length extrapolation~\citep{cai2024internlm2,Apple2024AppleIntelligence}. For instance, by finding the inverse function of Equation \ref{equ:t_extra}, we can determine the minimum necessary rotary base for supporting a specific context length $T_\text{extra}$. Compared to the linear relationship between rotary base $\beta$ and $T_\text{extra}$ in Hugging Face's default dynamic NTK implementation
% \footnote{See in \url{https://github.com/huggingface/transformers/blob/05260a1fc1c8571a2b421ce72b680d5f1bc3e5a4/src/transformers/modeling_rope_utils.py#L112}}
, Equation~\ref{equ_beta} demonstrates a power law which accounts for the extrapolation limit in the NTK approach.
\begin{equation}
    \beta =\left(\frac{T_\text{extra}}{2\pi}\right)^{\frac{d}{d_\text{extra}}}\text{.}
\label{equ_beta}\end{equation}
%$$\beta =\left(\frac{t}{2\pi}\right)^{\frac{d}{d_\text{extra}}} (3)$$

\subsubsection{Efficient Extrapolation}
Length extrapolation methods in training also consider achieving extrapolation effects with fewer computational resources, known as efficient extrapolation~\citep{chen2023extending, pengyarn}.  Efficient extrapolation methods can be categorized into two types, those focusing on partial contexts and those training on much shorter contexts.

\paragraph{Focusing on Partial Contexts} LongLoRA~\citep{chenlonglora} employs S$^2$-Attn with shift and grouping operations for local sparse attention while using LoRA for long-context scenarios. Zebra~\citep{song2023zebra} introduces local attention with global approximation, combining local attention windows with a global approximation for improved efficiency. LandmarkAttn~\citep{mohtashami2023random} innovatively uses landmark tokens as processing block gates, enabling inference at any context length. CREAM~\citep{wuefficient} alleviates the "middle loss" problem in long context processing through middle sampling optimization. LongRecipe~\citep{hu2024longrecipe} extracts shorter but information-dense segments by identifying tokens with significant impact in long context processing. FoT~\citep{tworkowski2024focused} extends model context length by adding memory attention mechanisms to certain transformer layers and using kNN algorithms for key-value pair retrieval.

\paragraph{Training on Much Shorter Contexts} GrowLength~\citep{jin2023growlength} applies progressive length growth during training, starting with shorter sequences and gradually increasing context length to improve training efficiency while achieving extrapolation. E$^2$-LLM~\citep{liu20242} supports longer context windows during inference by using position index scaling and offset while only requiring training on shorter sequences. FocusLLM~\citep{li2024focusllm} proposes a parallel decoding approach, reducing complexity to $1/n$ of the original by freezing initial parameters and adding minimal training parameters, improving length extrapolation capability through training on short context. PoSE~\citep{zhu2023pose}, RandPos~\citep{ruoss2023randomized}, and CD-Pos~\citep{hu2024cd} enhance model capability in processing varied input lengths by extracting smaller segments and adjusting position embeddings within these windows during training.

\subsection{Extrapolation without RoPE}\label{sec2_4}

\subsubsection{NoPE-based Extrapolation}
Research on NoPE has revealed that causal masking injects sequential constraints into the network, since each token only attends to preceding content which implicitly encodes positional information~\citep{haviv2022transformer, chi2023latent}. This observation motivates NoPE-based LLM. Experiments demonstrate that NoPE-based LLMs achieve comparable performance to traditional position embeddings in certain tasks~\citep{kazemnejad2024impact}.

However, NoPE also struggles with length extrapolation~\citep{kazemnejad2024impact, wang2024length}. Research shows that when context length exceeds the training range, NoPE's attention distribution becomes dispersed, leading to performance degradation. To address this issue, \citet{wang2024length} proposes an optimization method based on attention temperature parameters and improves length generalization capability.

\subsubsection{Other works}
NoPE challenges whether position embedding is necessary for length extrapolation. Besides, there are other discussions regarding position embedding or length extrapolation. 

\paragraph{Other Position Embedding Schema} Several studies have proposed novel position embedding schema to address length extrapolation challenges or model long context better. KERPLE~\citep{chi2022kerple} introduces a kernel-based relative position embedding. FIRE~\citep{li2023functional} improves the Transformer's generalization capability in longer contexts through progressive interpolation. DAPE (data-adaptive position embedding) and DAPE V2~\citep{zheng2024dape, zheng2024dape2} dynamically adjusts positional offset matrices based on input data. CoPE~\citep{golovneva2024contextual} allows positions to depend on context by computing attention through selectively incrementing positions incrementing positions. BiPE~\citep{dong2024exploring} combine intra-segment and inter-segment embeddings, using the former to identify positions within segments and the latter to model relationships between segments. Similarly, HiRoPE~\citep{zhang2024hirope} tries a hierarchical RoPE in long code.

\paragraph{Attention Entropy} Researchers observe that attention entropy increases with context length~\citep{han2024lm,pengyarn}, prompting several innovative solutions. Many researchers introduce scaling factors in attention logits to reduce attention entropy. ReRoPE~\citep{rerope} incorporates a dynamic scale factor $\log_Tt$ (where $T$ is the pre-training sequence length and $t$ is the input token's position index) in attention logits. YaRN~\citep{pengyarn} introduces a scale factor in attention logits. Entropy-ABF~\citep{zhang2024extending} employs a special treatment of scaling factors for the first two attention layers, based on the discovery that the first two attention layers consistently exhibited almost identical attention patterns, with only subsequent layers showing trends of attention concentration.

Beyond these two directions, \citet{dong2024exploring} proposes two training-free methods, positional vector replacement, and attention window extension, to effectively extend context length. From a memory perspective, RMT~\citep{bulatov2023scaling} also extends input context length by adding memory tokens and segment-level recursion to pre-trained LLMs. 
