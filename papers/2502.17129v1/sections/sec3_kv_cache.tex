\section{KV Cache Optimization}\label{sec3}

Although length extrapolation can theoretically extend the context length of LLMs, it is only the tip of the iceberg of long-context LLMs. In Transformer-based LLMs, the KV cache expands with the increase of context length, resulting in a great computational and memory overhead~\citep{fu2024challenges,luohekeep,xiaoefficient}. Since the size of the KV cache is determined by the product of \textbf{\textit{cached sequence length}}, \textbf{\textit{number of layers}} (\S\ref{sec3_3}), \textbf{\textit{number of KV heads}} (\S\ref{sec3_4}), \textbf{\textit{number of feature dimensions}} (\S\ref{sec3_5}), and \textbf{\textit{storage data type}} (\S\ref{sec3_6})~\citep{fu2024challenges,Venkat2024EssentialMath}, we can optimize the overhead through each of these factors as shown in Figure~\ref{fig:kv_opt}. Particularly, since the optimizations over sequence length are most discussed, we divide them into \textbf{\textit{toke dropping}}  (\S\ref{sec3_1}) and \textbf{\textit{token merging}}  (\S\ref{sec3_2}).

% \subsection{Token Dropping and Merging}

% This section addresses the optimization efforts concerning the cache sequence length in the context of KV caches. This school of works stems from a fundamental observation within the transformer architecture, a small fraction of tokens accounts for the majority of the attention scores~\citep{zhang2023h2o, li2024snapkv, liu2024scissorhands}. This implies that most tokens contribute little to the LLMs' generation, providing a foundation for straightforward methods to optimize the number of token in KV cache, including token dropping and merging.

\subsection{Token Dropping}\label{sec3_1}

\input{figures/fig_kv_opt}

Token dropping is a technique that identifies \textit{unimportant} tokens and discards them. However, the critical challenge in these methods lies in determining which tokens are \textit{unimportant}. Generally, token classification strategies can be categorized into two main types: static~\citep{xiaoefficient, han2024lm} and dynamic~\citep{zhang2023h2o, li2024snapkv}.

For static strategies, token importance is considered independent of context, with certain tokens at specific positions consistently receiving more attention from the LLMs, and thus being deemed \textit{important}. For instance, sliding window attention~\citep{jiang2023mistral, bai2023qwen} retains the most recent tokens. Building on this, StreamingLLM~\citep{xiaoefficient} and LMInfinite~\citep{han2024lm} observe that the initial tokens also consistently attract more attention from the LLM. Retaining both the most recent and the initial tokens helps mitigate the degradation of LLM performance as context length increases.

In contrast, dynamic strategies adaptively select \textit{important} tokens based on their context. A commonly employed approach involves determining token importance using attention weights. For instance, H2O~\citep{zhang2023h2o} identifies important tokens through cumulative normalized attention scores while prioritizing the retention of the most recent tokens. Scissorhands~\citep{liu2024scissorhands} identifies pivot tokens via attention weights, ensuring that the memory usage of the KV cache remains within a fixed budget.

Due to the inherent flexibility of dynamic approaches, the majority of subsequent work has built upon and extended these methods. In addition to using attention weight as a measure of importance, researchers have identified variations in attention patterns across different attention heads and developed more refined criteria for determining token importance. For example, FastGen~\citep{ge2023model} classifies attention heads into five types and applies distinct token eviction strategies for each. TOVA~\citep{oren2024transformers} removes tokens with the lowest attention scores for each head independently. SnapKV~\citep{li2024snapkv} selects queries within a local window and votes on the importance of previous tokens for each query and head. RazorAttention~\citep{tang2024razorattention} and DuoAttention~\citep{xiao2024duoattention} categorize attention heads into retrieval and non-retrieval heads, prioritizing the retention of initial and recent tokens for non-retrieval heads. \citet{rehg2024kv} takes this a step further, introducing different eviction rates for different attention heads.

Other researchers have considered the variability of attention patterns across layers and made corresponding adjustments. PyramidKV~\citep{cai2024pyramidkv} retains more tokens in lower layers, creating a pyramid-like KV cache structure, while PyramidInfer~\citep{yang2024pyramidinfer} extends this by applying token-dropping strategies in deeper layers. SimLayerKV~\citep{zhang2024simlayerkv} focuses on identifying which layers can adopt the StreamingLLM~\citep{xiaoefficient} paradigm and drops intermediate tokens in the corresponding layers. In recent work, SCOPE~\citep{wu2024scope} optimizes KV cache usage separately for the pre-filling and decoding stages.

Beyond attention weights as a measure of token importance, researchers have also explored alternative metrics that may better capture this concept. DCP~\citep{anagnostidis2024dynamic} fine-tunes a low-dimensional QK mapping to determine which tokens to drop. Similarly, Locret~\citep{huang2024locret} fine-tunes a new retention head to prioritize token retention. SirLLM~\citep{yao2024sirllm} utilizes token entropy to decide whether to discard a token. \citet{devoto2024simple} employs the L2 norm of keys to assess token importance. RoCo~\citep{ren2024efficacy} uses the standard deviation of attention scores as a metric for importance. VPM~\citep{guo2024attention} considers not only attention weights but also the values themselves. InfiniPot~\citep{guo2024attention} evaluates token importance based on a combination of future confidence and overlap with past information.

\subsection{Token Merging}\label{sec3_2}

The methods discussed here focus on preserving the information of discarded tokens as much as possible through token merging, which can be seen as an extension of the token-dropping strategies mentioned earlier.

Sentinel Tokens~\citep{ren2023context} introduces sentinel tokens to compress contextual information within segments. Similarly, approaches like Activation Beacon~\citep{zhang2024long} and AnchorLLM~\citep{pang2024anchor} adopt analogous strategies, introducing special tokens to guide LLMs in learning how to effectively compress the KV cache during training, thereby achieving impressive performance. \citet{dong2024get} uses kernel functions to compress preceding contextual information. DMC~\citep{nawrot2024dynamic} fine-tunes decision and weight variables to determine when to expand the KV cache or aggregate weights into the final set of KV caches. \citet{wang2024model} observes the similarity between adjacent keys and employs Gaussian kernel functions to merge neighboring tokens.

% \subsection{Layer-wise and Head-wise Sharing}

% This subsection discusses KV cache optimization from the perspective of the number of layers and KV head numbers, which correspond to changes at the architectural level of LLMs.

\subsection{Layer-wise Sharing}\label{sec3_3}
For optimizations targeting the layer dimension, some approaches involve pre-training LLMs from scratch, while others focus on fine-tuning pre-trained models.

Sharing the KV cache across multiple layers is a common strategy for methods that modify the model architecture during the pre-training stage. YOCO~\citep{sun2024you} divides the decoder into self-decoder and cross-decoder layers. KV cache is generated only in the output layer of the self-decoder, while cross-decoder layers reuse the output from the final self-decoder layer, thereby eliminating the need for additional KV caches. Similarly, GoldFinch~\citep{goldstein2024goldfinch} adopts a related strategy, where the last one-third of the layers utilize a small, compressed global KV cache generated by preceding layers. CEPE~\citep{yen2024long} stores the full KV cache for the main input across all layers, while for additional context, each layer shares a small encoder output cache to perform cross-attention.
% 说明一下加个模块CEPE~\citep{yen2024long} stores the KV for all layers of the main input while storing only the KV for the final layer of additional context. The encoder generates the final hidden representation for the additional context, avoiding step-by-step KV caching for intermediate layers.
% CEPE修改介绍后加上去了

For fine-tuning existing LLMs, researchers often adopt straightforward inter-layer cache-sharing strategies. CLA~\citep{brandon2024reducing} uses fine-tuning to enable multiple layers to share the KV cache of a single layer. Additionally, methods such as MiniCache~\citep{liu2024minicache}, LCKV~\citep{wu2024layer}, KVSharer~\citep{yang2024kvsharer}, and SwiftKV~\citep{qiao2024swiftkv} adaptively select inter-layer cache sharing strategies. MLKV~\citep{zuhri2024mlkv} combines layer-wise KV sharing with MQA, integrating adjacent layer sharing with techniques that replace deep-layer KV with shallow-layer KV. CLLA~\citep{yang2024lossless} extends MLA and CLA by incorporating quantization into the shared caching mechanism. In contrast, CEPE~\citep{yen2024long} employs a distinct strategy, storing a single-layer KV cache for all layers by encoding the KV cache with the representation generated by an encoder and integrating it with cross-attention.

Beyond KV cache sharing, researchers have also explored alternative strategies. Shared Attention~\citep{liao2024beyond} directly shares attention weights across different layers to optimize performance along the layer dimension.

\subsection{Head-wise Sharing}\label{sec3_4}

Similar to layer dimension optimizations, reducing the number of heads significantly impacts the representational capacity of LLMs. To preserve performance, head dimension optimizations typically rely on sharing strategies. For instance, GQA~\citep{ainslie2023gqa} and MQA~\citep{shazeer2019fast} reduce memory usage by sharing the KV cache across queries from different heads, a technique now widely adopted in various model architectures. Additionally, fine-tuning existing models can further optimize the size of the head dimension. For example, SHA~\citep{cao2024head} computes the cosine similarity of head weight matrices and groups similar heads to share a single KV cache. DHA~\citep{chen2024dha} employs a centroid alignment method to compute head similarity, linearly fusing the KV caches of similar heads, effectively compressing MHA into GQA.

Beyond KV cache sharing, low-rank compression is frequently used to optimize the head dimension. MLA~\citep{liu2024deepseek} replaces the full KV cache with low-dimensional latent vectors, recovering the KV through a projection matrix and injecting positional information via decoupled RoPE. ECH~\citep{yu2024effectively} applies SVD-based low-rank decomposition to grouped head weight matrices, achieving a KV compression effect similar to GQA, but distinct in its non-averaging fusion. Neurocache~\citep{safaya2024neurocache} applies low-rank compression to head matrices and uses the most similar caches in attention computation.


\subsection{Feature Compression}\label{sec3_5}

% This section focuses on the optimization work for the size per head and size of KV cache type. 

% \subsubsection{Low-Rank Approximation}  
Optimization methods targeting feature dimensions primarily focus on low-rank compression, which corresponds to the size per attention head. Palu~\citep{chang2024palu} introduces a medium-grained grouped head low-rank decomposition (G-LRD) method, striking a balance between accuracy and reconstruction efficiency. Eigen Attention~\citep{saxena2024eigen} utilizes a small calibration dataset to select the most significant directions based on SVD. MatryoshkaKV~\citep{lin2024matryoshkakv} addressed the limitations of PCA by fine-tuning the orthogonal projection matrix to align the model outputs as closely as possible with the original outputs. Additionally, it employed a Matryoshka hierarchical strategy to achieve improved compression without sacrificing performance. LoRC~\citep{zhang2024lorc} similarly leveraged SVD, adjusting cumulative condition numbers layer by layer to evaluate and modify compression ratios from deep to shallow layers, effectively preventing error accumulation that could degrade overall performance. In contrast, LPA~\citep{lv2024scalable} focused on incorporating low-rank projection attention structures during pretraining, thereby improving performance on downstream tasks. ThinK~\citep{xu2024think} introduces a dimension-pruning approach for feature compression, evaluating the interaction strength between KV pairs to retain the most significant dimensions.
%and reduce model complexity while preserving critical information.

% Unlike the aforementioned methods, ThinK~\citep{xu2024think} introduced a novel dimension-pruning approach for feature compression. This method evaluates the importance of each dimension based on the interaction strength between KV pairs. By retaining the most significant dimensions and pruning those with smaller contributions, ThinK effectively reduces model complexity and computational overhead, while maintaining the integrity of critical information.
% 这段精简成上一段的最后一句话了

\subsection{Cache Quantization}\label{sec3_6}
Quantization is one of the most widely used techniques for KV cache compression, commonly adopted in practice for its speed and efficiency~\citep{bai2023qwen, glm2024chatglm}. This optimization focuses on adjusting the size of the KV cache data type, which directly influences the storage size per unit.

Some works adapt traditional quantization methods to the specific characteristics of the KV cache. For example, KVQuant~\citep{hooper2024kvquant} determines quantization parameters through offline data analysis, ensuring that critical information is preserved during the process. In contrast, KIVI~\citep{liu2024kivi} exploits the differing characteristics of keys and values in the model, performing channel-wise quantization for key caches and token-wise quantization for value caches. MiKV~\citep{yang2024no} combines eviction strategies by storing tokens scheduled for eviction at a lower precision. SKVQ~\citep{duanmu2024skvq} rearranges key-value pairs to group outliers together, then trims boundary values within these groups to minimize quantization errors. ZipCache~\citep{he2024zipcache} improves the compression ratio by normalizing attention scores within a channel-separable quantization framework. PQCache~\citep{zhang2024pqcache} integrates embedding retrieval techniques by decomposing the original vector space into Cartesian products of several lower-dimensional vector spaces, which are quantized separately.

Other approaches explore more advanced possibilities in quantization methods. For instance, GEAR~\citep{kang2024gear} further reduces errors compared to full-precision computations by using low-rank and sparse matrices to fit residuals on top of traditional quantization results. QJL~\citep{zandieh2024qjl} introduces a novel KV cache quantization technique optimized specifically for CUDA kernels, enhancing the quantization process's efficiency and making it more suitable for large-scale parallel computing environments. AsymKV~\citep{tao2024asymkv} proposes an asymmetric quantization strategy that enables KV cache operation with extremely low 1-bit precision.

