\section{Long-Context Post-training}\label{sec9}

Based on the above long-context pre-training strategy, long-context LLMs are trained to understand the long context well. Subsequently, the post-training is introduced to ensure the LLMs can follow human instructions and preferences are problems that need to be addressed during post-training~\citep{dubey2024llama, bai2024longalign}. Long-context post-training methods can be classified into three categories based on the length of input and output: \textbf{\textit{Long-In-Short-Out}}, \textbf{\textit{Short-In-Long-Out}}, and \textbf{\textit{Long-In-Long-Out}}. Currently, there is a lack of research on Long-In-Long-Out, which is an important direction for future studies. Therefore, we will focus the following discussion on the Long-In-Short-Out and Short-In-Long-Out scenarios and add something beyond post-training later.

\subsection{Long-In-Short-Out}

In the post-training process of LLMs, task-specific data is typically constructed to enhance the LLMâ€™s performance on particular tasks, with the data construction type determined by the method (Supervised Fine-Tuning, SFT or Reinforce Learning RL). In the Long-In-Short-Out scenario, due to the length of the input context, manual annotation is difficult, and thus, synthetic data is often used. This section will introduce common data construction methods for various tasks.
\begin{itemize}
    \item{\textbf{Instruction Following}}
Provided with long-context data, instructions are given to generate relevant responses~\citep{chenlonglora}, or prompts are used to guide the LLMs to generate corresponding instructions and responses~\citep{koksal2023longform, bai2024longalign}.
    \item{\textbf{DocQA}}
Given a long document, relevant questions and answers are generated using LLMs. These can be based on the entire document~\citep{kaili2024mdcure}, or on specific context segment~\citep{an2024make, xiong2024effective, dubey2024llama}. In some cases, questions are generated and information is retrieved to ensure the quality of the generated answers~\citep{anonymous2024longpo, yu2023training}. Some researchers use shorter context segments to construct QA pairs and then concatenate many short pieces to form a long document~\citep{li2024longsyntheticdata, young2024yi}. To ensure the quality of responses, LLMs are often asked to provide citations~\citep{zhang2024longcite}.
    \item{\textbf{Multi-Hop QA}}
Long-context multi-hop QA data can usually be formed by combining multiple single-hop QA data~\citep{trivedi2022musique}. When combining single-hop QAs, similarity or question relevance can be considered~\citep{chen2024essential} to ensure coherence in question generation. Some studies require LLMs to generate responses using methods such as CoT~\citep{wei2022chain} or citation to improve data quality~\citep{li2024making}.
    \item{\textbf{Summarization}}
Besides using manually written documents and summary data from the Internet, LLMs are also often used to summarize long contexts. One method is to split the long context into chunks and summarize them individually, then provide a final summary of the summaries~\citep{dubey2024llama, chatglm2024glmlong}. Another method is to summarize short contents first, then concatenate them into a longer document and summarize that~\citep{li2024longsyntheticdata}.
    \item{\textbf{Retrieve}}
Information is inserted into the long context, and questions are posed about the inserted information~\citep{niah}. Alternatively, several pieces of information are combined to create a long context, and a question is asked about specific information~\citep{xiong2024artificialneedlesrealhaystacks}.
\end{itemize}


Researchers have also studied how to filter data. LOGO~\citep{tang2024logo} scores the contribution of answers from different chunks to determine the quality of data samples. LongReward~\citep{zhang2024longreward} uses predefined rules to score the responses. GATEAU~\citep{si2024selecting} filters data based on the relevance of the final reply to the long document and the importance of certain parts in the response, giving high attention weights to crucial parts. This method has shown significant effects with only a small amount of data.

Some researchers have explored methods to improve long-context capabilities without constructing long-context data. SkipAlign~\citep{wu2024skipalign} modifies the position embedding indices in short-text data, training LLMs on short texts to give it the ability to handle long texts. ProLong~\citep{gao2024train} adjusts the data sources and proportions of long and short texts to find efficient long-context LLM training methods. It has been found that using only short-text instruction data can also help the LLMs perform well on long-text tasks.

\subsection{Short-In-Long-Out}

When the task is more complex or requires detailed steps, longer output is necessary to express thoughts and details~\citep{wei2022chain, yao2024tree}. Therefore, long output is also a key capability for long-context LLMs. Data construction in this field is challenging, and there is still insufficient research. Current data construction methods can be classified into three categories: backtranslation, planning, and iterative training.
\paragraph{Backtranslation}
In backtranslation, given the context and a response, the LLMs generates instruction data in reverse. This method leverages the long-context LLMs' strong ability to handle long inputs~\citep{pham2024suri}.
\paragraph{Planning}
Another method is planning, which involves breaking the task down into sub-tasks to reduce complexity, eventually solving the original task. Some researchers apply planning by breaking down writing tasks, first generating an outline and then using the outline to create segments that combine into the final text~\citep{bai2024longwriter, liang2024integrating}. \citet{li2024large} also uses planning to guide reasoning, improving LLMs' reasoning.
\paragraph{Iterative Training}
Iterative training is also a commonly used method for enhancing LLMs' capabilities in the post-training stage. Self-Lengthen~\citep{quan2024language} uses two LLMs, a Generator and an Extender. The Generator generates responses within a specified length range, and the Extender extends the content to the target length. The concatenated extended data is then used to train the next generation of Generator and Extender.
\paragraph{Long Thought}
Long-output tasks, especially long thought, have attracted particular attention. The success of generation strategies like CoT~\citep{wei2022chain} and ToT~\citep{yao2024tree} has shown that LLMs can fully utilize their reasoning capabilities to generate better results. OpenAI o1~\citep{OpenAI2024o1} further enhances reasoning ability with CoT~\citep{wei2022chain}, achieving impressive results. More and more research is focused on how to achieve o1 or even better performance in long-context reasoning~\citep{zeng2024scaling, team2025kimi, guo2025deepseekr1}. \citet{qin2024o1, huang2024o1} improves the LLMs' reasoning ability using tree search and multi-agent strategies. ConTReGen~\citep{roy2024contregen} applies planning strategies in document QA by first generating sub-tasks from top-down and then retrieving relevant documents to solve the sub-tasks until the entire task is completed. K1.5~\citep{team2025kimi} and DeepSeek-R1~\citep{guo2025deepseekr1} significantly improves LLMs' reasoning ability through RL scaling. Long thought is an important task and we will discuss it in \textbf{\nameref{q9_output}} in Section\ref{sec12}

\subsection{Beyond Post-Training}

Besides post-training, many methods are being explored to enhance long-context LLMs. \textbf{\textit{Test Time Training}} (TTT) utilizes self-supervised learning during inference to further train LLMs using input test data~\citep{sun2020test, liang2024comprehensive}. Temp-Lora~\citep{wang2024greater} applies TTT in long-context scenarios by fine-tuning temporary Lora modules using contextual information during inference, guiding generation. Some works achieve alignment by providing examples or guidance during inference~\citep{sun2024principle, zhang2024metaalign, xie2023defending}, and long-context LLMs facilitate the effectiveness of these methods. Some researchers have effectively improved the performance of LLMs through \textbf{\textit{Test-Time Scaling}}~\citep{liao2024beyond, snell2024scaling}, proposing a new direction and further emphasizing the importance of long context. Additionally, LUQ~\citep{zhang2024luq} focuses on calibration for long-context LLMs, using NLI classifiers to determine the confidence of generated results and reducing uncertainty through model ensembling.