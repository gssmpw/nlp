\section{Long-Context Evaluation}\label{sec11}

We finally come to the part of the long-context evaluation, which is an important technique of long-context LLM~\citep{an2023eval,bai2023longbench,zhang2024bench,niah,hsieh2024ruler,yen2024helmet}. Before the mainstream length extrapolation methods emerged, long-context evaluation primarily includes four assessment methods. The first is language modeling perplexity, typically on datasets like WikiText~\citep{merity2022pointer} or PG19~\citep{rae2019compressive}. The second is Long-Range Arean (LRA)~\citep{taylong}, testing whether models can capture the underlying structure through artificially constructed sequences. Furthermore, LongEval~\citep{longchat} assesses the retrieval ability of LLM across different context lengths through coarse-grained topic retrieval and fine-grained line retrieval. The only benchmark based on natural long texts to reflect the actual downstream performance is Scrolls~\citep{shaham2022scrolls}, along with its upgraded version ZeroScrolls~\citep{shaham2023zeroscrolls}, which enrich the longer data samples in existing QA and summarization tasks.

With the development of long-context LLMs, researchers construct more benchmarks, as shown in Table \ref{eval_benchmark_feat} and \ref{eval_task_type}. In this section, we will introduce the development of long-context evaluation from two perspectives, \textbf{\textit{type of tasks}} and \textbf{\textit{benchmark features}}. In this process, we will reveal the pain point of long-context evaluation that persists from early explorations. If real texts are used to construct tasks, while they can reflect long-context scenarios more authentically, the evaluation length cannot scale automatically and a careful metric design is necessary~\citep{zhang2023movqa,xu2024detectiveqa,yen2024helmet}. In contrast, if synthetic data are used, although lengths and metrics can be easily controlled, it is challenging to ensure that they are consistent with real-world scenarios~\citep{hsieh2024ruler,li2024needlebench}.

\begin{table}[!ht]
\renewcommand{\arraystretch}{1.35}
\tabcolsep=0.1cm
\centering
\small
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Name}}
 & \multirow{2}{*}{\textbf{Time}} & \multicolumn{7}{c}{\textbf{Benchmark Feature}} \\ \cmidrule(lr){3-9}
& & \textbf{Len.} & \textbf{Lang.} & \textbf{Flexible} & \textbf{Stable} & \textbf{D.C.} & \textbf{Align.} & \textbf{L.O.} \\
\midrule
Scroll~\citep{shaham2022scrolls} & 22.01 & $\sim$8k & En & \xmark & \xmark & \xmark & \xmark & \xmark \\
ZeroScrolls~\citep{shaham2023zeroscrolls} & 23.05 & $\sim$8k & En & \xmark & \xmark & \xmark & \xmark & \xmark \\
LEval~\citep{an2023eval} & 23.07 & 4k-60k & En & \xmark & \cmark & \xmark & \xmark & \xmark \\
LongBench~\citep{bai2023longbench} & 23.08 & $\sim$10k & En, Zh & \xmark & \xmark & \xmark & \xmark & \xmark \\
BAMBOO~\citep{dong2024bamboo} & 23.09 & 4k-16k & En & \omark & \cmark & \cmark & \cmark & \xmark \\
M4LE~\citep{kwan2023m4le} & 23.10 & 1k-128k & En, Zh & \omark & \xmark & \xmark & \xmark & \xmark \\
LooGLE~\citep{li2023loogle} & 23.11 & $\sim$20k & En & \omark & \xmark & \cmark & \xmark & \xmark \\
Marathon~\citep{zhang2023marathon} & 23.12 & $\sim$80k & En & \xmark & \cmark & \xmark & \xmark & \xmark \\
\makecell[l]{Needle-In-A-Haystack\\~\citep{niah}} & 23.11 & 1k-128k & En & \cmark & \cmark & \xmark & \xmark & \xmark \\
InfiniteBench~\citep{zhang2024bench} & 24.02 & $\sim$200k & En, Zh & \xmark & \xmark & \xmark & \xmark & \cmark \\
LV-Eval~\citep{yuan2024lv} & 24.02 & 16k-56k & En & \cmark & \cmark & \cmark & \xmark & \xmark \\
Multi-NIHA~\citep{reid2024gemini} & 24.03 & 1k-1M & En & \cmark & \cmark & \xmark & \cmark & \xmark \\
CLongEval~\citep{qiu2024clongeval} & 24.03 & 1k-100k & Zh & \omark & \xmark & \xmark & \xmark & \xmark \\
LongICLBench~\citep{li2024long} & 24.04 & 2k-50k & En & \cmark & \cmark & \xmark & \xmark & \xmark \\
XL2Bench~\citep{ni2024xl} & 24.04 & $\sim$200k & En, Zh & \xmark & \xmark & \cmark & \xmark & \xmark \\
RULER~\citep{hsieh2024ruler} & 24.04 & 4k-1M & En & \cmark & \cmark & \xmark & \xmark & \xmark \\
Ada-LEval~\citep{wang2024ada} & 24.04 & 2k-128k & En & \omark & \cmark & \xmark & \xmark & \xmark \\
LoFT~\citep{lee2024can} & 24.06 & 32k-1M & \makecell{En, Es, Fr,\\Hi, Zh} & \omark & \cmark & \xmark & \xmark & \xmark \\
Loong~\citep{wang2024leave} & 24.06 & 10k-250k & En, Zh & \omark & \cmark & \cmark & \xmark & \xmark \\
BABILong~\citep{kuratov2024babilong} & 24.06 & 4k$\sim$10M & En & \cmark & \cmark & \cmark & \xmark & \xmark \\
LongIns~\citep{gavin2024longins} & 24.06 & 256-16k & En & \cmark & \cmark & \xmark & \cmark & \xmark \\
NeedleBench~\citep{li2024needlebench} & 24.07 & 20k-1M & En, Zh & \cmark & \cmark & \xmark & \cmark & \xmark \\ 
HelloBench~\citep{que2024hellobench} & 24.09 & $\sim$2k & En & \xmark & \cmark & \xmark & \cmark & \cmark \\
LongGenBench$_1$~\citep{wu2024longgenbench} & 24.09 & $\sim$20k & En & \omark & \cmark & \xmark & \cmark & \cmark \\
LongGenBench$_2$~\citep{liu2024longgenbench} & 24.10 & 4k-128k & En & \cmark & \cmark & \xmark & \cmark & \cmark \\
HELMET~\citep{yen2024helmet} & 24.10 & 8k-128k & En & \omark & \cmark & \xmark & \xmark & \xmark \\
\makecell[l]{LongSafetyBench\\~\citep{huang2024longsafetybench}} & 24.11 & $\sim$40k & En & \xmark & \cmark & \xmark & \cmark & \xmark \\
LIFBench~\citep{wu2024lifbench} & 24.11 & 4k-128k & En & \cmark & \cmark & \xmark & \cmark & \xmark \\
LongBench v2~\citep{bai2024longbench} & 24.12 & 32k-128k & En, Zh & \omark & \cmark & \xmark & \xmark & \xmark \\ 
LongProc~\citep{ye2025longproc} & 25.01 & 500~8k & En & \omark & \cmark & \xmark & \cmark & \cmark \\ 
\bottomrule
\end{tabular}
\caption{Comparison of the mainstream or comprehensive long-context benchmarks at present. The comparison includes the benchmark features such as average length, language, etc., and type of tasks including QA, summary, and retrieval in the continued table. In this table, Flexible stands for whether the length of evaluating data is flexible. Stable stands for stable evaluation. D.C. stands for data contamination. Align. stands for containing alignment tasks. L.O. stands for long output. \cmark~means yes, while \xmark~means no, and \omark~means the data in the benchmark are grouped into subsets by different length ranges. \label{eval_benchmark_feat}}
\end{table}

\begin{table}[!ht]
\renewcommand{\arraystretch}{1.35}
\tabcolsep=0.1cm
\centering
\small
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Name}} & \multicolumn{8}{c}{\textbf{Type of tasks}} \\ \cmidrule(lr){2-9}
& \textbf{QA} & \textbf{Summ.} & \textbf{Retrieval} & \textbf{Code} & \textbf{Math} & \textbf{Agg.} & \textbf{ICL} & \textbf{Reasoning} \\
\midrule
Scroll~\citep{shaham2022scrolls} & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
ZeroScrolls~\citep{shaham2023zeroscrolls} & \cmark & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark \\ 
LEval~\citep{an2023eval} & \cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \cmark & \xmark \\ 
LongBench~\citep{bai2023longbench} & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark \\ 
BAMBOO~\citep{dong2024bamboo} & \cmark & \xmark & \xmark & \cmark & \xmark & \cmark & \xmark & \xmark \\ 
M4LE~\citep{kwan2023m4le} & \cmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
LooGLE~\citep{li2023loogle} & \cmark & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark & \cmark \\ 
Marathon~\citep{zhang2023marathon} & \cmark & \xmark & \cmark & \xmark & \cmark & \cmark & \xmark & \cmark \\
\makecell[l]{Needle-In-A-Haystack\\~\citep{niah}} & \xmark & \xmark & \cmark & \xmark & \xmark& \xmark & \xmark & \xmark \\
InfiniteBench~\citep{zhang2024bench} & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \xmark \\ 
LV-Eval~\citep{yuan2024lv} & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
Multi-NIHA~\citep{reid2024gemini} & \xmark & \xmark & \cmark & \xmark & \xmark& \xmark & \xmark & \xmark \\
CLongEval~\citep{qiu2024clongeval} & \cmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
LongICLBench~\citep{li2024long} & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark \\ 
XL2Bench~\citep{ni2024xl} & \cmark & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
RULER~\citep{hsieh2024ruler} & \cmark & \xmark & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark \\ 
Ada-LEval~\citep{wang2024ada} & \xmark & \xmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark \\ 
LoFT~\citep{lee2024can} & \cmark & \xmark & \cmark & \cmark & \xmark & \xmark & \cmark & \xmark \\ 
Loong~\citep{wang2024leave} & \cmark & \xmark & \cmark & \xmark & \xmark & \cmark & \xmark & \cmark \\ 
BABILong~\citep{kuratov2024babilong} & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \cmark \\ 
LongIns~\citep{gavin2024longins} & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
NeedleBench~\citep{li2024needlebench} & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark & \xmark & \cmark \\
HelloBench~\citep{que2024hellobench} & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark \\ 
LongGenBench$_1$~\citep{wu2024longgenbench} & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \cmark \\ 
LongGenBench$_2$~\citep{liu2024longgenbench} & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark & \xmark & \xmark \\ 
HELMET~\citep{yen2024helmet} & \cmark & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark & \xmark \\ 
\makecell[l]{LongSafetyBench\\~\citep{huang2024longsafetybench}} & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark & \cmark & \xmark \\ 
LIFBench~\citep{wu2024lifbench} & \cmark & \xmark & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark \\ 
LongBench v2~\citep{bai2024longbench} & \cmark & \xmark & \xmark & \cmark & \xmark & \xmark & \cmark & \cmark \\ LongProc~\citep{ye2025longproc} & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark & \xmark & \cmark \\ \bottomrule
\end{tabular}
\caption{The continued table of Table \ref{eval_benchmark_feat} comparing the type of tasks in the mainstream or comprehensive long-context benchmarks at present. QA stands for question-answer tasks. Summ. stands for summarization tasks. Retrieval stands for retrieval task. Code stands for code tasks. Math stands for math tasks. Agg. stands for aggregation tasks. ICL stands for long in-context learning tasks. Reasoning stands for reasoning tasks. \label{eval_task_type}}
\end{table}

\subsection{Type of Tasks} 

\paragraph{Long QA and Summary} The evaluation of long-context LLMs originated from long QA and summarization. Early long-context benchmarks including Scrolls~\citep{shaham2022scrolls}, ZeroScrolls~\citep{shaham2023zeroscrolls}, LEval~\citep{an2023eval}, and LongBench~\citep{bai2023longbench}, enrich the long-context data from QA (NarrativeQA~\citep{kovcisky2018narrativeqa}, QuALITY~\citep{pang2022quality}, Qasper~\citep{dasigi2021dataset}) and summarization (GovReport~\citep{huang2021efficient}, QMSum~\citep{zhong2021qmsum}) datasets as the main components of the evaluation. Based on this, different evaluations impose varying requirements on the tasks. LEval~\citep{an2023eval} and CLongEval~\citep{qiu2024clongeval} emphasize high-quality evaluation data, obtaining reliable long-context evaluation data through manual screening or annotation. M4LE~\citep{kwan2023m4le} highlights the diversity of data sources and categorizes long-context evaluation into five scenarios based on the distribution of answers in the text: explicit single-span, semantic single-span, explicit multiple-span, semantic multiple-span, and global context understanding. LooGLE~\citep{li2023loogle} proposes evaluating long-context and short-context dependencies simultaneously. LV-Eval~\citep{yuan2024lv} focuses on QA tasks by introducing confusing facts in the context to increase the difficulty.

\paragraph{Long-Context Retrieval} Retrieval is also a classic task in long-context evaluation, with early benchmarks such as LongEval\cite{longchat} emphasizing it. Retrieval tasks offer better flexibility than QA and summarization based on naturally long texts. Needle-In-A-Haystack (NIAH)~\citep{niah} is the first to propose reflecting LLM's recall performance in varying depths at varying context lengths. This sparks a surge in research on long-context retrieval tasks~\citep{young2024yi,cai2024internlm2,wang2024qwen2}, significantly altering the trajectory of long-context evaluation development. Notably, Gemini-1.5~\citep{reid2024gemini} expands the single-NIAH to multi-NIAH, achieving impressive results. Moreover, \citet{hsieh2024ruler} proposes various variants such as multikey and multivalue NIAH, creating an entirely synthetic long-context evaluation, RULER, which has become a new competitive focus among long-context LLMs~\citep{team2024jamba,liu2024retrievalattention,LFM}.

Furthermore, there are also domain-specific retrievals, such as DocFinQA~\citep{reddy2024docfinqa} and \citet{gupta2024systematic}, and structured data retrievals, namely enabling long-context LLMs to simulate SQL execution or database manipulation, including S3eval~\citep{lei2024s3eval}, BIRD~\citep{li2024can}, Spider 2.0~\citep{lei2024spider}, HoloBench~\citep{maekawa2024holistic}. To improve recall accuracy and assess whether LLM truly understands the context~\citep{gao2023enabling,hilgert2024evaluating,zhang2024longcite}, researchers also want LLM to provide relevant citations for the retrieval content~\citep{buchmann2024attribute,tang2024citeeval}. Such tasks have now been integrated into emerging long-context evaluation benchmarks, such as LoFT~\citep{lee2024can}, HELMET~\citep{yen2024helmet} and SCBench~\citep{li2024scbench}.

Due to the popularity of retrieval tasks, discussions on retrieval have also emerged. For example, \citet{liu2024lost} and \citet{an2023eval} highlight that LLMs tend to recall topics at the beginning and end of a context more easily and make mistakes with topics in the middle, thus demonstrating the Lost-In-the-Middle phenomenon. Furthermore, \citet{koo2024large} separates QA and evidence selection within retrieval from the perspective of task alignment. \citet{yu2024hyper} divides retrieval into matching and logical retrieval, exploring the corresponding improving methods. \citet{goldman2024really} analyzes long-context evaluation from the recall perspective and proposes two orthogonal dimensions, dispersion, and scope, to identify potential directions for more challenging long-context evaluations.

\paragraph{Code, Math, and Aggregation} In addition to tasks focusing on long natural language text, there are also long-context tasks centered on logical languages such as code and mathematics. Regarding code, LEval~\citep{an2023eval} and LongBench~\citep{bai2023longbench} are the first to incorporate code into long-context evaluation, which has been inherited by subsequent benchmarks~\citep{zhang2024bench,bai2024longbench}. Additionally, there are tasks specifically aimed at repository-level long code, such as RepoQA~\citep{liu2024repoqa}. Regarding math, LEval~\citep{an2023eval} and LongGenBench$_2$~\citep{liu2024longgenbench} expand the short-context task GSM8k~\citep{cobbe2021training} into a long-context task using many-shot ICL and question concatenation respectively. In contrast, Marathon~\citep{zhang2023marathon} and InfiniteBench~\citep{zhang2024bench} introduced more complex long-context mathematical computation tasks, while LongGenBench$_1$~\citep{wu2024longgenbench} examined the LLM's spatial-temporal understanding in long context.

Besides, there is also a category of long-context evaluation that includes sorting and statistics, generally referred to as aggregation tasks ~\citep{shaham2023zeroscrolls,hsieh2024ruler}. Aggregation tasks are first mentioned in LRA~\citep{taylong} and introduced into text evaluation in ZeroScrolls~\citep{shaham2023zeroscrolls}, which includes positive review statistics and summary sorting. After that, sorting tasks still exists in ~\citep{dong2024bamboo,li2023loogle,zhang2023marathon,zhang2024bench,wang2024ada}, while the recent HELMET evaluation suite also includes re-ranking tasks~\citep{yen2024helmet}. Regarding statistics, finding the maximum number and identifying~\citep{zhang2024bench} the most frequent words~\citep{hsieh2024ruler} are also proposed. Although aggregation tasks often occur in long-context benchmarks, they are less emphasized due to the deviation from natural long texts~\citep{hsieh2024ruler}.

\paragraph{Long In-Context Learning} Regarding LLMs, the two most notable capabilities are ICL~\citep{brown2020language,pan2023context} and reasoning~\citep{wei2022chain}, and long-context provides a deeper exploration of both. For ICL, a longer context enables more demonstrations, offering greater potential to stimulate LLM. Long ICL is first introduced in long-context evaluation in LEval~\citep{an2023eval} and LongBench~\citep{bai2023longbench}, primarily to extend the context of short-context tasks. After Gemini-1.5~\citep{reid2024gemini} prompts LLM to learn new languages with the grammar book and dictionary, long ICL has become a new focus for long-context evaluation~\citep{li2024long,agarwal2024many}. 

For example, LongICLBench~\citep{li2024long} evaluates a wide range of long-context LLMs and finds that most can benefit from extensive demonstrations when the length is within a certain range. As the input grows longer, it will lead to a performance fluctuation or decline~\citep{li2024long}. \citet{bertsch2024context} further points out that long ICL is sensitive to the distribution of demonstrations, and when there are enough demonstrations, the effect of the sampling method diminishes. Other studies indicate that long ICL is also influenced by the quality of the demonstrations~\citep{li2024demonstrations,agarwal2024many}, precision~\citep{wang2024precision}, retrieval~\citep{zou2024retrieval}, reasoning~\citep{kai2025mirbench} and other factors~\citep{agarwal2024many}. Additionally, \citet{wang2024benchmarking} propose General Purpose In-Context Learning, covering more domains including decision-making and world modeling through continuous generation and interaction. Long ICL has become a significant sub-item in emerging long document evaluation standards, such as LoFT~\citep{lee2024can} and HELMET~\citep{yen2024helmet}, and further discussions on long ICL will be present in \textbf{\nameref{q10_icl}} in Section\ref{sec12}. 

% For example, LongICLBench~\citep{li2024long} evaluates a wide range of long-context LLMs and finds that most can benefit from extensive demonstrations when the length is within a certain range. As the input grows longer, it will lead to a performance fluctuation or decline~\citep{li2024long}. Related tasks also spur numerous studies focusing on long ICL~\citep{bertsch2024context,agarwal2024many}, which will be discussed in detail in Section\ref{sec12}, and have been integrated into emerging long-context evaluation benchmarks~\citep{lee2024can,yen2024helmet}.

\paragraph{Long-Context Reasoning} The discussion of long reasoning can be traced back to early multi-hop reasoning tasks, such as HotpotQA~\citep{yang2018hotpotqa} and MuSiQue~\citep{trivedi2022musique}. The emergence of long context provides more exploration space for multi-hop reasoning. For example, Variable Tracing in RULER, CountingStars~\citep{song2024counting}, Loong~\citep{wang2024leave}, BABILong~\citep{kuratov2024babilong}, and Needlebench~\citep{li2024needlebench} ask models to aggregate multi-hop evidence inserted in the long-context when answering the final question. However, these evaluations still tend to focus on synthetic texts, lacking assessments of reasoning capabilities in real-world scenarios.

Apart from explicit multi-hop reasoning, some benchmarks~\citep{li2023loogle,zhang2023marathon,bai2024longbench}, also regard a deeper understanding of context as a type of reasoning. Recently, NovelQA~\citep{wang2024novelqa}, NoCha~\citep{karpinska2024one}, and DetectiveQA~\citep{xu2024detectiveqa} design reasoning evaluations for native long texts, leveraging the complex reasoning chains present in long novels. Moreover, NovelQA and DetectiveQA require LLM to output its reasoning process and conduct a process-centered evaluation~\citep{wang2024novelqa,xu2024detectiveqa}, which offers a more realistic and challenging evaluation. LongProc~\citep{ye2025longproc} however, uses long procedure generation to assess the model’s ability to handle long outputs and complex reasoning. More discussion on long-context reasoning and long output will be shown \textbf{\nameref{q9_output}} in Section\ref{sec12}.

In addition to the aforementioned evaluation tasks, there are long-context evaluations on other traditional NLP tasks. For example, some tasks in M4LE~\citep{kwan2023m4le} involve text classification. StNLab in CLongEval~\citep{qiu2024clongeval} explores the annotation issues in long Chinese texts. \citet{manikantan2024identifyme} and \citet{vodrahalli2024michelangelo} focuses on referential understanding within long texts.

\subsection{Benchmark Features}

\paragraph{Length} Length is an important feature for long-context evaluations. Before retrieval tasks like NIAH~\citep{niah,multi_niah} mark a turning point, the length of long-context evaluation benchmarks lags behind the lengths reported by long-context LLMs~\citep{pengyarn,young2024yi,cai2024internlm2}. This is primarily due to the limited native long-context corpora, making it difficult to enrich long-context evaluation~\citep{an2023eval,li2023loogle}. After this point, the situation reverses. On one hand, the length of synthetic tasks is flexible, and any length for evaluation is allowed ~\citep{liu2024reattention,lieber2024jamba}. On the other hand, researchers begin proposing more challenging evaluations~\citep{levy2024same,li2024long,hsieh2024ruler,gavin2024longins}, discovering that long-context LLMs fail to achieve acceptable performance within the claimed context lengths.

In addition to length itself, flexibility is a key feature of long-context evaluations~\citep{niah,yen2024helmet}. As mentioned earlier, traditional long-context benchmarks~\citep{an2023eval,bai2023longbench,zhang2024bench} are not scalable and only able to measure performance at different context lengths by truncating to various lengths~\citep{bai2023longbench}. Subsequent synthetic task evaluations~\citep{niah,levy2024same,hsieh2024ruler,liu2024longgenbench}, generally allow for customized context length. Additionally, some evaluation benchmarks~\citep{kwan2023m4le,lee2024can,yen2024helmet} group the evaluation data to different subsets by different length ranges, representing a trade-off.

\paragraph{Stability} Another important feature for long-context evaluation is stability, a persistent pain point in long-context evaluation~\citep{novikova2017we,an2023eval,yen2024helmet}. Specifically, it is difficult to provide a reliable evaluation metric for generative tasks such as long QA, summarization, and open-ended generation~\citep{an2023eval,novikova2017we} which are common in long-context benchmarks. In response, different long-context evaluation benchmarks have proposed various solutions. First, benchmarks like \citet{dong2024bamboo} avoid this issue by directly discarding long output tasks. Next, benchmarks like \citet{zhang2023marathon,lee2024can,bai2024longbench} address the problem by transforming generative answers into multiple-choice questions or constraining evaluation metrics.

Furthermore, some long-context research has delved more deeply into the stability of long output evaluations. LEval~\citep{an2023eval} is the first to propose using LLMs to compute reference-free, pairwise win rates to measure the quality of long outputs. After that, LV-Eval\cite{yuan2024lv} improves the stability of output measurement through a keyword-recall-based metric design without the aid of LLMs. In contrast, DetectiveQA~\citep{xu2024detectiveqa} introduced a step-wise reasoning metric that compares the reasoning chains of the model outputs to reference steps, measuring long reasoning based on the recall of reasoning steps. Similarly, HELMET~\citep{yen2024helmet} breaks down conventional summarization references into atomic claims and then has LLMs evaluate their recall. HelloBench~\citep{que2024hellobench}, based on the ordinary LLM-as-a-Judge~\citep{zheng2023judging}, decomposes answer quality into a linear combination of multiple scoring items from LLMs in a checklist, thereby reducing bias in LLM judges. Additionally, there are other solutions involving task formats, such as ProxyQA~\citep{tan2024proxyqa}, which evaluates a model's performance based on the outputs of the model under meta-questions to reflect the long-text generation capability of the model being tested, as well as \citet{liu2024longgenbench}.

\paragraph{Data Contamination} Evaluation benchmarks always face the issue of data contamination~\citep{golchintime}, and avoiding it is an important topic. In response, BAMBOO~\citep{dong2024bamboo} and LooGLE~\citep{li2023loogle} are the first to propose constructing evaluation sets using newly crawled data to mitigate this problem. Besides, LV-Eval~\citep{yuan2024lv} and XL2Bench~\citep{ni2024xl} employed keyword, phrase, and text replacement methods to address the issue. Additionally, DetectiveQA~\citep{xu2024detectiveqa} suggests comparing model performance under context-free scenarios to determine whether LLM relies on internal knowledge rather than context to answer questions. Finally, some studies~\citep{wang2024leave,kuratov2024babilong} claim that the data contamination may not exist for particularly long or very general texts.

\paragraph{Alignment Evaluation} Finally, some long-text evaluations also discuss the alignment performance of long-context LLMs. On one hand, LongIns~\citep{gavin2024longins} and LIFBench~\citep{wu2024lifbench} examine the instruction-following performance of long-context LLMs, with LongInsc\citep{gavin2024longins} reporting that the effective context length for instruction following is significantly shorter than the claimed context length of long-context LLMs. On the other hand, Many-shot Jailbreaking~\citep{anil2024many} focuses on the long-context safety performance of long-context LLMs, finding that numerous demonstrations can disrupt model alignment under long-context attacks. Subsequently, \citet{huang2024longsafetybench} and \citet{roberts2024needle} offer a broader discussion of long-context safety, exploring safety issues in various scenarios. Besides, some long-context evaluations investigate the memory capabilities of LLMs in real-world interactions~\citep{thonet2024elitr,wu2024longmemeval}. 

Additionally, there are some domain-specific long-context benchmarks such as \citet{hosseini2024benchmark} in the medical domain and \citet{reddy2024docfinqa} in the financial domain.

% \subsection{Long-Context Understanding}

% \paragraph{Early Attempts} perplexity, LRA~\citep{taylong}, SCROLLS~\citep{shaham2022scrolls}, ZeroSCROLLS~\citep{shaham2023zeroscrolls}, LongEval~\citep{longchat}

% \paragraph{Comprehensive Benchmarks} LEval~\citep{an2023eval}, LongBench~\citep{bai2023longbench}, BAMBOO~\citep{dong2024bamboo}, M4LE~\citep{kwan2023m4le}, LooGLE~\citep{li2023loogle}, Marathon~\citep{zhang2023marathon}, InfiniteBench~\citep{zhang2024bench}

% \paragraph{Domain-Specific Tasks} Ada-LEval~\citep{wang2024ada}, LV-Eval~\citep{yuan2024lv}, CLongEval~\citep{qiu2024clongeval}, XL2Bench~\citep{ni2024xl}, Loong~\citep{wang2024leave}

% RepoQA~\citep{liu2024repoqa}, DocFinQA~\citep{reddy2024docfinqa}, \citet{gupta2024systematic} 

% \subsection{Long-Context Retrieval}

% \paragraph{Needle-In-A-Haystack} NIAH~\citep{niah}, MultiNIAH~\citep{multi_niah}, NeedleBench~\citep{li2024needlebench}, RULER~\citep{hsieh2024ruler}

% Where am I~\citep{koo2024large}, Is It Really Long Context~\citep{goldman2024really}, Hyper-multi-step~\citep{yu2024hyper}

% \paragraph{Retrieval beyond NIAH} ALCE~\citep{gao2023enabling}, \citet{hilgert2024evaluating}, LongCite, LongCite~\citep{zhang2024longcite}, L-CiteEval~\citep{tang2024citeeval}

% S3eval~\citep{lei2024s3eval}, BIRD~\citep{li2024can}, Spider 2.0~\citep{lei2024spider}, HoloBench~\citep{maekawa2024holistic}

% \subsection{Long In-Context Learning} ICL~\citep{brown2020language,pan2023context}, LongICLBench~\citep{li2024long}, \citet{bertsch2024context}, \citet{li2024demonstrations}, ~\citep{zhao2024probing}, MS-ICL~\citep{agarwal2024many}, GPICL~\citep{wang2024benchmarking}, precision~\citep{wang2024precision}, \citet{zou2024retrieval}

% LOFT~\citep{lee2024can}, HELMET~\citep{yen2024helmet} 

% Lifelong ICL and Task Haystack~\citep{xustress}, DENIAHL~\citep{dai2024deniahl}, AcademicEval~\citep{zhangacademiceval}


% \subsection{Future Directions}

% \paragraph{Long Output and Reasoning} ProxyQA~\citep{tan2024proxyqa}, LongGenBench~\citep{wu2024longgenbench}, LongGenBench~\citep{liu2024longgenbench},  HelloBench / HelloEval~\citep{que2024hellobench}, LongBench v2~\citep{bai2024longbench}

% Counting-Stars~\citep{song2024counting}, BABILong~\citep{kuratov2024babilong}, NovalQA~\citep{wang2024novelqa}, DetectiveQA~\citep{xu2024detectiveqa}, NoCha~\citep{karpinska2024one}

% \paragraph{Long-Context Alignment} LongIns~\citep{gavin2024longins}, LIFBench~\citep{wu2024lifbench}

% \paragraph{Long-Context Memory}

% \paragraph{Traditional Tasks in Long-Context}
