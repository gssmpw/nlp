\section{Long-Context Pre-training}\label{sec8}

The development of deployment and training infrastructure has enabled the training and inference of LLMs with longer contexts. In this background, the pre-training length of LLMs has evolved from the initial 2k tokens~\citep{touvron2023llama} to 4k~\citep{touvron2023llama2}, 32k~\citep{xiaoefficient,cai2024internlm2}, over 128k~\citep{meta2024introducing,InternLM25}, and even 1M~\citep{liu2024world}. To expand the context length of LLMs effectively, more training strategies specialized for long-context LLMs are necessary. We begin our analysis from the long-context pre-training. Compared to the preceding short-context pre-training, long-context pre-training is featured with requiring fewer tokens, generally 1B-10B, and facing both challenges of quality and quantity~\citep{fudata,lv2024longwanjuan}.

\subsection{Long-Context Data Quality}

In the earliest works, researchers often focused on the length of pre-training~\citep{chen2023extending,roziere2023code,pengyarn}, with little discussion of other factors. Subsequently, ScalingRoPE first discovers that continual pre-training at the original pre-training context length could extrapolate the context length of LLMs~\citep{liuscaling}. LLaMA2Long~\citep{xiong2024effective} further points out that in long-context pre-training, data quality is more crucial than data length and provides detailed discussions on the mixing ratio and training cycles between long and short data.

Following this, \citet{fudata} first raises the concept of long-context data engineering and suggests that the data required for long-context training is much less than that for short-context pre-training. Only 0.5B to 5B tokens are enough. Instead of relying solely on long book and long paper data, \citet{fudata} also emphasizes that, besides length up-sampling, it is essential to maintain balance across domains, which has gained widespread acceptance~\citep{zhang2406long,young2024yi,chatglm2024glmlong,gao2024train}. Recently, \citet{gao2024train} conducts an in-depth investigation into long-context training, finding that mixing code repositories and long books with high-quality short-context data is crucial for both long-context performance and retaining the short-context capabilities. The exploration of long-short-mixing training inspires thinking about training long-context LLMs from scratch, which will be discussed in \textbf{\nameref{q8_balance}} in Section\ref{sec12} 

Regarding the quality of a single long data sample, LongWanjuan~\citep{lv2024longwanjuan} is the first to propose that using LLM-based or rule-based metrics could reflect whether a long text exhibits long-context dependency characteristics from the perspective of coherence, cohesion, and diversity. It then categorizes long texts into holistic, aggregated, and chaotic types and conducts data mixing to achieve optimal long-context training results. ProLong~\citep{chen2024long} goes deeper into long-context dependencies, designing scores for dependency strength, dependency distance, and dependency specificity to measure long-distance dependencies between different segments in a long text, for data filtering.

\subsection{Long-Context Data Curation} 

Discussions on long-context data quality remain very limited, primarily because long-context data itself is extremely scarce, leading to a greater focus on data synthesis~\citep{chatglm2024glmlong}. In early long-context training, researchers employ the simplest splicing methods to obtain sufficient long-context data~\citep{chenlonglora,tworkowski2024focused,chenclex,li2023functional}. Notably, CodeLLaMA utilized the feature of code data to concatenate code from the same project, resulting in ultra-long code datasets~\citep{roziere2023code}.

Subsequent efforts begin to stitch similar short texts into a long context through similarity matching. For instance, ICLM~\citep{shicontext} constructs a graph of documents with embeddings from an encoder-only model and applies the traveling salesman algorithm to extract efficiently. SPLiCe~\citep{staniszewski2023structured} replaces selection criteria with BM25 retrieval or attribute label matching and extends the splicing length to 32k. BM25Chunk~\citep{zhao2024analysing} provides in-depth analysis for training on concatenated long-context data, while later work explored retrieval methods using LLM embeddings~\citep{chatglm2024glmlong} and keyword matching~\citep{gao2024quest}. DataSculpt attempted to optimize the synthesis of spliced data through multi-objective combinatorial optimization~\citep{lu2024datasculpt}.

In addition to sequential splicing, a few works have attempted to achieve extended length through interleaved splicing of short texts~\citep{zhao2024longskywork,tian2024untie}. LongSkywork proposes CIP~\citep{zhao2024longskywork}, which splits, shuffles, and splices short texts, allowing LLMs to identify relevant segments within seemingly chaotic contexts through self-attention adaptively, thus enhancing long-context modeling capabilities. Following this, UTK~\citep{tian2024untie} introduces knot tokens pushing LLMs to untie these knots and gain long-context capabilities more effectively. These methods could significantly improve the performance of synthetic tasks such as RULER~\citep{hsieh2024ruler}.

Additionally, a few pieces of research concern loss design specialized for long-context training~\citep{fang2024wrong}. Discussions regarding long-context pre-training work are still limited, which we will highlight and summarize in \textbf{\nameref{q7_scarce}} in Section\ref{sec12}, and much of the discourse is dispersed across various technical reports of LLMs. We have compiled these technical reports of long-context LLMs, listing the information related to long-context pre-training, post-training, and evaluation, for the reader's reference.

\input{tables/tab_close_model}
\input{tables/tab_open_model_v}