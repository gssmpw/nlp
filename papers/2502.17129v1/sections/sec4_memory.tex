\section{Memory Management}\label{sec4}

While KV cache optimization strives for a longer context practically, essentially, it is a balance between efficiency and performance. Cache optimization does not try to break the ceiling of LLM capabilities, since it does not change the organizing form of contextual information~\citep{fu2024challenges,luohekeep}. Long-context LLMs based on vanilla KV cache mechanism still face limitations including read-only access and the requirement to read all information at once, making them unsuitable for more complex scenarios~\citep{dai2019transformer, bulatov2022recurrent}. This has led to incorporating \textbf{\textit{memory management}} into LLMs, with the KV cache being regarded as a specific memory instance.

Memory management in LLMs can be categorized from two perspectives. One is \textbf{\textit{cache-based memory}} (\S\ref{sec4a}), storing intermediate results that encode contextual information, such as KV cache, or \textbf{\textit{text-based memory}} (\S\ref{sec4b}), storing text directly, which is more convenient and flexible, as it allows the use of external textual data sources. The other is \textbf{\textit{read-only}} or \textbf{\textit{writable}}, based on whether the memory is modifiable during storage. These two aspects divide the memory management methods into four quadrants as shown in Figure~\ref{fig:memory}.

% Memory organization in LLMs typically follows two main approaches: \textit{cache-based memory}, storing intermediate results which encode contextual information, such as KV cache; \textit{text-based memory}, storing text directly, which is more convenient and flexible, as it allows the use of external textual data sources. Additionally, memory can be classified into read-only and writable, based on whether the memory is modifiable during storage.

\input{figures/fig_memory}

\subsection{Cache-Based Memory}\label{sec4a}

In this subsection, memory primarily refers to intermediate computational outputs, including hidden states, KV cache, and compressed textual representations that are irrecoverable.
\subsubsection{Read-Only}\label{sec4_1}

The most intuitive improvement of read-only memory over the KV cache is its more flexible access method, avoiding reading all KV cache at once. MemTrans~\citep{wu2022memorizing} stores the KV cache of pre-training in external memory to provide more relevant information during inference. MemLong~\citep{liu2024memlong} extends this concept to a long context by storing the KV cache of context chunks and retrieving KV pairs based on relevance to guide inference.

Another approach to applying memory to long contexts is to compress the context, ensuring that the LLMs can handle longer sequences. AutoCompressor~\citep{chevalier2023adapting} iteratively processes the context by encoding each segment into a fixed-dimension summary vector and concatenating it with the next part. Later works, such as LLoCO~\citep{tan2024lloco} and E2LLM~\citep{liao2024e2llm}, extend this method with advancements in offline learning and parallel compression, respectively. ICAE~\citep{ge2023context} compresses information by fine-tuning the encoder to encode the entire context into a small number of memory tokens. UIO-LLMs~\citep{li2024uio} further conceptualizes memory-enhanced LLMs as fully connected RNNs, optimized through backpropagation.

\label{prefix_sharing}Additionally, some inference acceleration works have also used memory. PagedAttention~\citep{kwon2023efficient} accelerates inference by reusing the same prefix of KV cache in a single request. Prompt Cache~\citep{gim2024prompt} and SGLang~\citep{zheng2024sglang} speed up inference through structured organization of prompts to enhance performance.

\subsubsection{Writable}\label{sec4_2}

In contrast to read-only memory, writable memory allows dynamic adjustments to stored memories. Transformer-XL~\citep{dai2019transformer}, for example, reuses the hidden states of previous segments to capture long-term dependencies. RMT~\citep{bulatov2022recurrent} improves upon this by introducing special memory tokens to store contextual information, with cross-segment gradient backpropagation to update the memory. \citet{bulatov2023scaling} extends the context length to 1M tokens using RMT. UniMem~\citep{fang2024unimem} further synthesizes previous methods, while MemoryLLM~\citep{wang2024memoryllm} and CAMELoT~\citep{he2024camelot} optimize memory management through more flexible or non-training-based approaches.

As researchers focus on using memory to store contextual or long-term information, Memory$^3$~\citep{yang2024memory3} was the first to introduce knowledge to LLMs and decompose knowledge into abstract knowledge and specific knowledge, formalizing the idea that the LLMs can store only abstract knowledge, while all specific knowledge is stored externally. This external memory is accessed during inference by periodic concatenation of relevant memories, achieving state-of-the-art performance. Titans~\citep{behrouz2024titans} integrated memory with test-time training and further explored the diverse applications of the memory module, thereby pointing out new directions for subsequent research.

\subsection{Text-Based Memory}\label{sec4b}

While cache-based memory has proven effective, it is relatively complex and lacks sufficient interpretability, particularly due to its non-textual nature. Thus, some researchers have turned to text-based memory to enhance LLMs' performance.

\subsubsection{Read-Only}\label{sec4_3}

A common application of text-based memory is the presence of ground truth in text, where providing this text to the LLMs during generation can improve performance. Retrieve Augmented Generation(RAG, \citep{lewis2020retrieval}) utilizes this idea by retrieving external information using a retriever and appending it to the prompt during generation, paving the way for subsequent developments. This idea has been expanded to address long-context problems by retrieving relevant context segments~\citep{chen2023walking}, improving queries~\citep{fei2024retrieval}, combining query and context~\citep{zhao2024longrag}, and improving retrieval methods~\citep{luo2024bge, soh2024you, jiang2024longrag}, effectively addressing long-context challenges.

While RAG-related research has flourished, some studies have questioned the necessity of using RAG. \citet{li2024retrieval} conducted experiments revealing that performance with long-context LLMs outperforms RAG, suggesting an LLM-driven decision of whether to reuse long-context responses after initially employing RAG. The question of whether long-context or RAG is better remains a topic of ongoing discussion, which will be addressed later in \textbf{\nameref{q4_rag}} in Section \ref{sec12}. Some argue that RAG is more suitable for resource-constrained scenarios compared to long-context, and we will also present our perspectives on this matter in Section\ref{sec7}. \citet{rag2_contextual_ai_2024} integrates various RAG components and conducts end-to-end training, achieving state-of-the-art results.

\subsubsection{Writable}\label{sec4_4}

Writable text-based memory can be used to store and update historical information. MemoryBank~\citep{zhong2024memorybank} stores user history and profiles, achieving better user preference. Inspired by LSTM, RecurrentGPT~\citep{zhou2023recurrentgpt} summarizes preceding content during each step, facilitating ultra-long text generation. MemGPT~\citep{packer2023memgpt} designs a multi-layered memory architecture, structuring prompts based on operating system memory access principles. EM$^2$~\citep{yin2024explicit} was the first to recognize that the direction of memory updates is not always optimal, introducing the EM algorithm~\citep{dempster1977maximum} and treating memory as latent variables to estimate the correct update direction.

Some researchers have also used memory to compress long contexts. One approach, which we refer to as text-level compression, involves compressing the context into several complete texts. Researchers have explored content-based compression~\citep{fei2023extending}, relevance-based compression~\citep{yoon2024compact}, and attention-weighted compression~\citep{choi2024reading}, achieving promising results. Another approach, token-level compression, compresses context into tokens that may not form complete sentences. LongLLMLingua~\citep{jiang2023longllmlingua} and Perception Compressor~\citep{tang2024perception} select the most relevant content based on correlations, retaining only the most important tokens to achieve token-level compression. Selection-p~\citep{chung2024selection} retains a proportion of the original context tokens and trains the LLMs to generate responses using this limited set of tokens, resulting in significant improvements.