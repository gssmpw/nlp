\section{Architecture Innovation}\label{sec5}

Although KV cache optimization (Section \ref{sec3}) and memory management (Section \ref{sec4}) have improved the long-context capability of Transformer-based LLMs. The inherent shortage of Transformer in computation and memory efficiency still drives researchers to explore innovations in the attention mechanism itself, resulting in more radical architecture innovations~\citep{jiang2024minference,ye2024differential,peng2023rwkv,gu2023mamba}. In this section, we will demonstrate those architectural innovations concerning long-context efficiency or performance from three perspectives as shown in Figure~\ref{fig:arch_inno}.
\begin{itemize}
    \item In \S\ref{sec5_1}, we will analyze \textbf{\textit{efficient attention}}, the attention variant towards better computational efficiency or long-context performance. It can be further divided into two branches. One is \textbf{\textit{attention approximation}}, an efficient approximation for standard attention, such as MInference~\citep{jiang2024minference}, RetrievalAttention~\citep{liu2024retrievalattention} and other sparse attention methods~\citep{yang2024post,zhu2024sampleattention}, while the other is \textbf{\textit{attention alternative}}, which tries a novel attention mechanism like DIFF-Transformer~\citep{ye2024differential}, Lightning Attention~\citep{qin2024various,qin2024lightning} and other linear attentions~\citep{katharopoulos2020transformers}.
    \item As a cache-free architecture, discussion on LSTM~\citep{schmidhuber1997long} is revived for the pursuit of long context. In \S\ref{sec5_2}, we will analyze researches on LSTM in the LLM era, including the \textbf{\textit{module-level Improvements}} like xLSTM~\citep{beck2024xlstm} and HGRN series~\citep{qin2024hierarchically,qin2024hgrn2} and the \textbf{\textit{model-level advancements}}, namely RWKV series~\citep{peng2023rwkv,peng2024eagle,choe2024rwkv}.
    \item In \S\ref{sec5_3}, we will show the developing path of the widely-discussed Mamba series~\citep{gu2023mamba,daotransformers,wang2024mamba}, from the \textbf{\textit{theoretical basis}} such as HiPPO~\citep{gu2020hippo} and S4~\citep{gu2021efficiently} to its improvements~\citep{ben2024decimamba,yuan2024remamba}, then to the \textbf{\textit{hybrid architectures}}~\citep{dong2024hymba,akhauri2024attamba}, including Jamba series~\citep{team2024jamba,lieber2024jamba}
\end{itemize}

% %补充两个引用
% As Transformer LLMs continue to scale up, their computational efficiency and memory consumption become prominent challenges~\citep{Vaswani2017attention,zhang2023h2o}. Initially, researchers seek improvements by optimizing the KV cache (Section \ref{sec3}) and memory management (Section \ref{sec4}). However, while these optimizations yield certain benefits, they fail to fundamentally address the computational complexity issues that Transformers face when processing long contexts. This limitation drives researchers to explore innovations in the attention mechanism itself.

\subsection{Efficient Attention}\label{sec5_1}
%添加对efficient attention的定义
% The efficient attention mechanism is a mathematical approximation to standard attention based on dot product but substantially faster~\citep{zhuoran2021efficient}.

\input{figures/fig_arch_inno}

\subsubsection{Attention Approximation}
%添加对attention approximation的定义；调整工作排序；调整描述方式
% In this part, we categorize methods that reduce computational complexity through approximate or selective computation as attention approximation mechanisms, which maintain LLM's performance while focusing only on the most important attention patterns~\citep{wang2020linformer,kitaev2019reformer,child2019generating}.

Attention approximation is a hot research topic in long-context LLMs. Most attention approximation approaches are achieved with dynamic sparse attention through retrieval-based~\citep{ribar2023sparq,liu2024retrievalattention} or attention pattern observation~\citep{jiang2024minference}. For example, SparQ Attention~\citep{ribar2023sparq} optimizes the attention mechanism through approximate attention computation based on KV cache extraction and interpolation compensation. Similarly, Loki~\citep{singhania2024loki} ranks and selects tokens in the KV-cache based on attention scores computed in low dimensional space. Moreover, SampleAttention~\citep{zhu2024sampleattention} proposes a two-stage sampling filtering mechanism, identifying important attention patterns through query sampling, then combining selected KV cache with sliding windows. DoubleSparse~\citep{yang2024post} uses important feature channels to identify key tokens, thereby reducing access to the KV cache. RetrievalAttention~\citep{liu2024retrievalattention} identifies the inconsistency between query and key vector distribution and resolves it through approximate nearest neighbor search. MagicPIG~\citep{chen2024magicpig} utilizing locality-sensitive hashing (LSH) sampling to estimate attention layer outputs. SqueezedAttention~\citep{hooper2024squeezed} optimizes attention computation by identifying the most important keys through semantic clustering and hierarchical lookup. Recently, MoBA~\citep{lu2025mobamixtureblockattention} combines the concepts of Mixture of Experts (MoE) and sparse attention, allowing each query to selectively focus on a part of the KV pairs, effectively reducing computational costs while maintaining performance.

Other attention approximation approaches are achieved with dynamic sparse attention based on further observation of attention pattern~\citep{jiang2024minference}. For example,
MInference~\citep{jiang2024minference} proposes dynamic sparse attention from the perspective of sparse patterns. StarAttention~\citep{acharya2024star} proposes dividing the input into chunks distributed across different hosts for local attention computation, followed by aggregating global attention results through designated query hosts. And some works improve attention computation efficiency by using full attention and sparse attention in different layers or different heads~\citep{beltagy2020longformer,li2019big,ainslie2020etc}. Additionally, Fourier Transformer~\citep{he2023fourier} removes redundant contextual information from hidden states by discrete cosine transform (DCT) to reduce computational complexity. 


\subsubsection{Attention Alternative}
%添加对attention alternative的定义；调整工作顺序；调整描述方式；与上次会议不同之处：引用加在了定义处，from scratch没有加引用；Gated Slot Attention挪过来了
In this part, we will present methods that modify the fundamental mathematics of dot-product attention as attention alternative mechanisms, which require LLM pre-training from scratch but offer theoretical guarantees of improved efficiency~\citep{choromanski2020rethinking}. A representative work is linear attention~\citep{katharopoulos2020transformers}, which reformulates dot-product attention using kernel functions to achieve linear complexity. 
In the LLM era, several recent studies propose novel approaches. SLAB~\citep{guo2024slab} optimizes attention computation efficiency through simplifies linear attention and progressive LayerNorm replacements. Lightning Attention~\citep{qin2024various} achieves efficient computation by blocking and using linear attention between blocks. Its improved version, Lightning Attention-2~\citep{qin2024lightning}, achieves the ability to process infinite-length contexts by introducing an exponential decay mechanism in the KV cache. \citet{minimax2025minimax01scalingfoundationmodels} further takes the advantages of both lightning attention and softmax attention to enhance retrieval performance by substituting lightning attention with softmax attention at intervals of every eight layers. Gated Slot Attention~\citep{zhanggated} enhances ABC~\citep{peng2022abc} by incorporating a gating mechanism, essentially comprising a two-layer GLA~\citep{yanggated} linked via softmax to achieve more efficient memory utilization. What's more, DIFF Transformer~\citep{ye2024differential} calculates attention scores as the difference between two separate softmax attention maps. This subtraction eliminates noise and promotes the emergence of sparse attention patterns. DeepSeek recently release NSA~\citep{yuan2025native}, combining compressed, selected and sliding attention. 

Furthermore, a recent study~\citep{yang2024efficient} reveals an important insight: the efficiency of efficient attention, both sparse and linear attention, is task-dependent, with advantages primarily manifesting in tasks exhibiting locality characteristics. This finding opens new perspectives for research in efficient attention mechanisms.

\subsection{LSTM-RWKV}\label{sec5_2}
%cache-free变斜体；添加相关引用；添加脚注；内容简单调整
Despite numerous advances in Transformer's computational efficiency, significant storage limitations persist~\citep{ribar2023sparq,yang2024post}. This leads researchers to explore \textit{cache-free} architectures, with improvements of LSTM~\citep{graves2012long} emerging as a key direction. Compared to the Transformer's quadratic complexity, LSTM's linear inference complexity demonstrates significant advantages in long context scenarios. The improvements encompass both module-level enhancements to the basic LSTM architecture and large-scale innovations exemplified by RWKV~\citep{peng2023rwkv,peng2024eagle}, which shows exceptional performance in complex reasoning tasks like Sudoku\footnote{https://zeeklog.com/rwkv-tong-guo-ji-wan-token-de-cot-jie-jue-ji-hu-100-de-shu-du-wen-ti-cai-yong-29m-can-shu-de-xiao-mo-xing--2/}.

\subsubsection{Module-level Improvements}
For example, xLSTM~\citep{beck2024xlstm} consists of two parts. sLSTM introduces exponential gating, normalization, and stabilization mechanisms while supporting multi-head processing, significantly enhancing LLM's expressiveness while maintaining parallelism. Meanwhile, mLSTM further expands the cell state from vector to matrix form, giving LLMs stronger memory capabilities. Based on the xLSTM architecture, xLSTM-Mixer~\citep{kraus2024xlstm} further introduces normalization and initial linear prediction mechanisms, enhancing LLM's performance by combining original embeddings and reverse embeddings. HGRN~\citep{qin2024hierarchically} emphasizes the importance of forget gates in recursive layers, achieving hierarchical modeling of long-short term dependencies through learnable, layer-increasing lower bound values. Furthermore, HGRN2~\citep{qin2024hgrn2} innovatively introduces an outer product-based state expansion mechanism, expanding the scale of recursive states without increasing parameters, and addresses increased computational complexity through multi-head variants. Additionally, \citet{feng2024were} simplifies LSTM to enable parallel computation, improving LLM's computational efficiency.

Beyond these works, ConvLSTM~\citep{shi2015convolutional} is an important direction for improvement. ConvLSTM demonstrates the viability and advantages of incorporating convolutional structures into LSTM. By implementing convolutional structures in both input-to-state and state-to-state transitions, ConvLSTM successfully extends LSTM to handle spatiotemporal context data. This innovation provides crucial insights for subsequent improvements of LSTM  improvements~\citep{wang2022predrnn,wang2018predrnn++,wang2019memory,lin2020self}.

\subsubsection{Model-level Advancements}
RWKV series represents a new technical approach, striving to combine the advantages of RNN and Transformer. RWKV4~\citep{peng2023rwkv} introduces token shift, similar to convolutional sliding window processing, and processes context information through the fusion of time dimension (time-mixing) and feature dimension (channel-mixing). Its innovative WKV operator achieves training phase parallelization and linear complexity during inference. Subsequently, RWKV's development reaches new heights with RWKV5 (Eagle) and RWKV6 (Finch)~\citep{peng2024eagle}. RWKV5 introduces multi-head mechanisms similar to Transformer's multi-head attention mechanism and optimizes token shift through linear interpolation. In time-mixing, it enhances LLM's expressiveness by introducing new trainable parameters. RWKV6 further innovates with significant improvements in both token shift and time-mixing, particularly incorporating LoRA's implementation approach and allowing each channel to mix token information rather than relying on fixed trainable parameters. These improvements enable the LLM to demonstrate superior performance and higher efficiency in processing long contexts.

\subsection{SSM-Mamba}\label{sec5_3}
%引入简单调整一下；后面内容较小改动但不多
%State Space Model (SSM) emerges as a novel architecture, achieving breakthroughs while maintaining model expressiveness: it achieves linear computational complexity, significantly outperforming Transformer's quadratic complexity; it eliminates the memory requirements for storing attention matrices through fixed state storage; and crucially, it supports parallel training and autoregressive generation, offering substantial practical advantages.
State Space Model (SSM) represent an innovative architecture that delivers several key advances~\citep{gu2023mamba}. Its linear computational complexity significantly outperforms the quadratic complexity of Transformers. It eliminates memory requirement for attention matrices through fixed hidden state storage. Most importantly, SSM supports parallel training and linear generation, offering substantial practical advantages.

SSM originates from modern control system theory. It encodes context information by maintaining hidden states and using linear dynamical systems to describe state evolution:$x'(t)=Ax(t)+Bu(t)$,$y(t)=Cx(t)+Du(t)$, where $x(t)$ represents hidden state, $u(t)$ represents input, $y(t)$ represents output, and $A$,$B$,$C$,$D$ are parameter matrices. 

\subsubsection{Pre-Mamba Works}
Although HiPPO~\citep{gu2020hippo} is initially applied to RNNs, it lays crucial theoretical foundations for the development of Mamba. HiPPO utilizes polynomial approximation and specific probability measures (LegS probability measure) to construct a new matrix structure (HiPPO matrix), effectively modeling context data by encoding historical information into polynomial coefficients. Building on this, LSSL~\citep{gu2021combining} further reveals the connection between RNN, CNN and SSM, discovering that SSM could be represented in both recurrent and convolutional forms. More importantly, LSSL first attempts to use HiPPO Matrix to initialize SSM's parameters, achieving significant performance improvements on multiple tasks. Then, the introduction of S4~\citep{gu2021efficiently} marks a major breakthrough in SSM's computational efficiency. This work represents HiPPO matrix in NPLR (Normal Plus Low-Rank) form and reduces SSM's computational overhead from both recurrent and convolutional perspectives through matrix theory derivations. The subsequent S4D~\citep{gu2022parameterization} proposes a simplified version of S4, further improving computational efficiency while maintaining LLM's performance by restricting the state matrix to a completely diagonal form. Later, H3~\citep{fu2022hungry} focuses on addressing SSM's shortcomings in language modeling tasks. Inspired by linear attention mechanisms, H3 represents the update of SSM's hidden state as $Q\odot SSM_{diag}(SSM_{shift}(K))\odot V$, where the two SSM matrices employ the "hungry hippo" mechanism to enhance efficiency. H3's performance in synthetic language modeling tasks matches attention mechanisms. Additionally, H3 introduces FlashConv to extend context length and improve training efficiency. In the above architecture innovations based on recurrent networks, local information interaction such as token shift often appears. Based on this property, we will further the discussion on new architecture in \textbf{\nameref{q5_na}} in Section\ref{sec12}

\subsubsection{Introduction and Improvements of Mamba}
The introduction of Mamba~\citep{gu2023mamba} represents a significant milestone in SSM's development. It introduces a selective mechanism and enables content-aware capabilities. Specifically, when updating parameter matrices, Mamba incorporates projection information of inputs, allowing each token to have independent parameter matrices. Simultaneously, Mamba proposes a hardware-aware parallel recursive algorithm to improve computational efficiency. Mamba-2~\citep{daotransformers} further improve the architecture, elucidating the dual relationship between Mamba and attention mechanisms through detailed theoretical analysis and providing insights for the integrated use of attention mechanisms and Mamba.

However, as research deepened, researchers discover Mamba's limitations in processing long contexts. Several works propose solutions from different angles. DeciMamba~\citep{ben2024decimamba} proposes a token selection mechanism based on $\Delta_t$ by analyzing Mamba's receptive field. ReMamba~\citep{yuan2024remamba}, inspired by KV cache's compression method, uses architecture's characteristic of aggregating information through hidden states to select the most representative representations using importance score mechanisms. StuffedMamba~\citep{chen2024stuffed} reveals the essence of the state collapse phenomenon, proposing multiple mitigation strategies including increasing state decay amount, reducing input information quantity, normalizing states, and simulating sliding window mechanisms.

Furthermore, researchers are exploring other optimization directions. SMR~\citep{qi2024smr} analyzes SSM's sampling stability issue from a control theory perspective, proposing an event-triggered control (ETC) based solution—introducing learnable memory to adjust current states and resolving Mamba's inability to use convolution, enabling efficient parallel computation. Mamba-PTQ~\citep{pierro2024mamba} discovers the outlier channels problem in Mamba's quantization and uses SmoothQuant technology, balancing weight and activation quantization difficulty through transfer factor $\alpha$. Additionally, The Mamba in the Llama~\citep{wang2024mamba} uses the standard attention parameters to initialize Mamba, combining knowledge distillation and multi-step speculative decoding to improve efficiency.

\subsubsection{Hybrid Architectures}
Recently, researchers have explored hybrid architectures that combine SSM and Transformer. Early Jamba~\citep{lieber2024jamba} adopts a relatively direct approach, stacking Transformer, Mamba, and MoE blocks in combination, aiming to balance memory usage, computational throughput, and LLM's performance. RecurFormer~\citep{yan2024recurformer} then proposes a more targeted hybrid solution, with its core idea being to identify and replace attention heads in Transformer that focus on local perception with Mamba blocks. Subsequently, Hymba~\citep{dong2024hymba} proposes a deeper integration approach, adopting parallel Attention heads and SSM heads structure to avoid potential information bottleneck issues that might arise from serial architecture. And it achieves an organic fusion of the two types of heads through learnable parameters. Additionally, Attamba~\citep{akhauri2024attamba} explores a new compression approach that uses SSM blocks to compress multiple tokens into one chunk token for Transformer processing. And it also combines sliding window concepts to preserve the initial state of local tokens, thereby reducing KV cache.

\paragraph{Other New Architectures}
Beyond the aforementioned work, researchers also propose many other \textit{cache-free} architectures, providing new perspectives for improving LLM's ability to process long contexts. Some works are based on Neural ODE~\citep{chen2018neural}, such as Liquid Time-constant Networks~\citep{hasani2021liquid} introducing a dynamic adjustable liquid time constant mechanism and CfC~\citep{hasani2022closed} avoiding the need for numerical solutions by finding approximate closed-form solutions for LTC. Additionally, MixCon~\citep{xu2024mixcon} proposes a hybrid architecture combining Transformer layers, Conba layers, and MoE and introducing mechanisms such as selective state spaces to enhance LLM's performance. MCSD~\citep{yang2024mcsd} captures local and global features through Slope and Decay components respectively, and adopts a dual-branch design to strengthen feature extraction and fusion.