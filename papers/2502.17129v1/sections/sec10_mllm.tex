\section{Long-Context MLLM}\label{sec10}
\input{figures/fig_mllm}

Based on the above technique, now we have numerous LLMs with strong long-context capabilities. But that is not the end. Long context is crucial for LLM focused on textual information and holds even greater significance for Multi-modal LLM (MLLM). In this section, we will change our focus on Long-context MLLMs. Long-context MLLMs involve various scenarios, including DocVLM for long documents with images~\citep{xie2024wukong,hu2024mplug}, ImageLLM for high-resolution images~\citep{li2024monkey,lin2023sphinx}, VideoLLM for long videos~\citep{zhang2406long,wang2024qwen2}, CLIP with long descriptions~\citep{zhang2025long,wulotlip}, as well as long speech models~\citep{reid2024gemini} and even world models\citep{liu2024world,zhan2024anygpt}. In this version, we only discuss long VideoLLM in detail. We omit speech for it is 1D like text and can be viewed as a new language~\citep{zhang2023speechgpt,zhang2024speechgpt}. Regarding long DocVLM, while it matters practically~\citep{jaisankar2024postdoc,zhang2024mgte,ma2024mmlongbench,chia2024m}, related discussions are relatively limited~\citep{xie2024wukong,hu2024mplug,blau2024gram,liu2024focus}.

Long CLIP and ImageLLM emphasize the extension of descriptive texts~\citep{zheng2025dreamlip,zhang2025long,wang2024videoclip} and images~\citep{li2024monkey,lin2023sphinx}, respectively. On one hand, the study of long CLIP could generally follow a length extrapolation discussion in text domain~\citep{zhang2025long,najdenkoska2024tulip}. On the other hand, while currently viewed as a long-context issue, high-resolution ImageLLM faces the backbone in the design of the vision encoder and may not necessarily remain a long-context problem in the future. In contrast, Long VideoLLM presents the most discussed long-context challenges in the MLLM due to its rich fine-grained spatiotemporal details as well as long-term dependencies in long videos~\citep{zou2024seconds,li2024giraffe}. Therefore, the following content will focus on the issues related to Long Video in detail.

Generally, long VideoLLMs and other long-context MLLMs training are after the textual pre-training and fine-tuning~\citep{liu2024world,zhang2406long}. Accordingly, this section focuses on how to obtain a long-context MLLM, especially a long VideoLLM, from a trained long-context LLM through input and architecture adaptation as well as multi-modal training. We will also discuss long video evaluations at the end of this section. This section can be viewed as a microcosm of the entire survey, emphasizing the differences and extensions of long-context in videos compared to text. Unlike text, videos have lower information density and are characterized by sample extraction~\citep{zou2024seconds}. Many studies concentrate on the extraction and compression of video information~\citep{song2024moviechat,ren2024timechat,qianmomentor,yu2024frame}. However, discussions on fine-grained alignment from long texts to long videos, such as the generalization of position embedding\citep{kexuefm10040,wang2024qwen2} and reasoning capabilities\citep{li2024temporal}, are relatively scarce.

\subsection{Input Adaptation}\label{sec10_1} 

The adaptation of long-context LLMs to long videos begins with input processing. Unlike text, which can be directly tokenized, video requires frame sampling, patch segmentation, and vision encoding and connector processing before entering the LLM~\citep{zou2024seconds}. For example, a one-hour video, sampled at 2 frames per second (fps) with 66 tokens per frame~\citep{wang2024qwen2}, results in over 400k tokens, while sampling at 1 fps with 2 tokens per frame~\citep{li2025llama} yields fewer than 8k tokens. Thus, the adaptation of video input affects the MLLM's context length, processing efficiency, and downstream performance. The input adaptation of long VideoLLMs has benefited from the redundancy of video~\citep{zou2024seconds}, utilizing both text-only~\citep{zhang2023simple} and image-only~\citep{kim2024image,zhang2406long} methods to replace original video inputs, as well as employing modules such as Q-Former~\citep{li2023blip,ma2023vista} to compress video data.

\paragraph{Text-Only} A relatively simple method for long video input is to truncate the long video into several short segments, convert them into corresponding text descriptions, concatenate these to form a complete video description and input it to the LLMs~\citep{zhang2023simple}. This approach was first proposed in LLoVi~\citep{zhang2023simple} and later improved by LangRepo~\citep{kahatapitiya2024language}, which iteratively processes video segments along with previous descriptions, eventually to generate a description for the entire video. Similarly, MVU~\citep{ranasinghe2024understanding} further simplifies the video into a combination of key information about objects. These methods do not input the original long video into the backbone LLM, thus reducing the adaptation and processing costs. Subsequent work has combined this approach with agents, enabling LLMs to collaborate with VLMs~\citep{wang2025videoagent} or interactively invoke tools~\citep{fan2025videoagenttool} for long video understanding.

\paragraph{Image-Only} Another class of methods treats videos as a comic strip, allowing LLMs to adapt to image features without additional long-video training. This line of work can be traced back to IG-VLM~\citep{kim2024image}, which achieves video understanding by transforming a video into a single image by arranging multiple frames in a grid. InternLM2-XComposer2.5~\citep{zhang2024internlm} (IXC-2.5 for simplicity) inherits this method and exhibits strong performance on various video benchmarks. Meanwhile, FreeVA~\citep{wu2024freeva} also demonstrates that using a similar approach without video training conditions enables ImageLLMs to process video. LongVA~\citep{zhang2406long} further proposes that the image-only method can transform an ImageLLM with long-context capabilities into a long VideoLLM capable of handling 2000 frames or over 200k visual tokens.

\paragraph{Q-Former-based} Besides the tricks mentioned above, early VideoLLMs tend to use cross-attention-based Q-Former~\citep{li2023blip,zhang2023video,song2024moviechat} to compress multi-modal information into fixed-length inputs, due to the significant redundancy in video representations~\citep{zou2024seconds}. However, in the scenario of long videos, Q-Former faces greater challenges in processing capacity, which introduce techniques including memory~\citep{song2024moviechat}, keyframe selection~\citep{korbar2025text}, Q-Former variants~\citep{ma2023vista,ren2024timechat}, and timestamp enbedding~\citep{ren2024timechat,qianmomentor}.

MovieChat~\citep{song2024moviechat} first introduces the memory mechanism into long VideoLLM, using a queue-based short-term memory and long-term memory based on adjacent fusion with higher frame similarity. Similarly, the concepts of compression and memory are also referenced in MA-LMM~\citep{he2024ma} and VidCompress~\citep{lan2024vidcompress}. Besides compression and memory, keyframe extraction is also an intuitive approach. For instance, TCR~\citep{korbar2025text} locates relevant information based on text and feeds it to Q-Former. TGB~\citep{wang2024efficient} uses the RoPE-involved product between optical flow and text embeddings to identify the ranges of multiple key content segments. Furthermore, some works attempt to overcome the fixed-length output constraint of Q-Former. For example, Vista-LLaMA~\citep{ma2023vista} proposes a SeqQ-Former similar to Transformer-XL~\citep{dai2019transformer}, and TimeChat~\citep{ren2024timechat} introduces a sliding window-based Q-Former. Notably, in LVChat~\citep{wang2024lvchat}, long videos are interleaved into multiple groups, encoded separately, and interleaved reversely to achieve complete encoding within a limit.

Although Q-Former faces limitations in processing long videos, the attention in Q-Former still facilitates the injection of information beyond images, particularly temporal and spatial embeddings~\citep{ren2024timechat,qianmomentor}. For example, TimeChat~\citep{ren2024timechat} employs a timestamp-aware frame encoder that explicitly binds visual content with corresponding timestamps in Q-Former with prompts. Similarly, TCR~\citep{korbar2025text} injects temporal information into visual representations through prompts based on special tokens after keyframe extraction. Momentor~\citep{qianmomentor} achieves the same goal with temporal embeddings in a continuous temporal token space. Some Q-Former-free models also adopt recurrent compression~\citep{wang2024videollamb} and temporal encoding. For instance, VideoStreaming~\citep{qian2024streaming} uses structures similar to RMT~\citep{bulatov2022recurrent} and LandmarkAttention~\citep{mohtashami2023random} to recurrently encode images, injecting corresponding temporal information with text prompts during the encoding process and recalling only relevant vision encoding segments during inference.

\paragraph{Q-Former-Free} In MLLMs, the LLaVA series~\citep{liu2024llava,liu2024llava1_5,liu2024llavanext,zhang2024llavanextvideo} first propose feeding the visual tokens from the vision encoder directly to the LLM backbone, avoiding the information bottleneck caused by Q-Former, which is inherited by many subsequent researches~\citep{li2024llava,wang2024qwen2,zhang2024internlm}. However, due to the high redundancy of video information, long VideoLLMs also introduce keyframe extraction and token compression to balance the compression of redundant information with the retention of key information~\citep{zou2024seconds,yu2024frame,cheng2024videollama}.

Regarding keyframe extraction, early attempts are often limited in uniform sampling of long videos~\citep{zhang2024llavanextvideo,cheng2024videollama}, resulting in low efficiency and high information loss~\citep{shen2024longvu}, prompting subsequent improvements. For instance, LVNet~\citep{park2024too} enhances keyframe extraction efficiency through a hierarchical keyframe selector, while KeyVideoLLM~\citep{liang2024keyvideollm} employs frame clustering to find central frames and integrates keyframe extraction in instruction fine-tuning. Frame-Voyager~\citep{yu2024frame} trains a frame extraction module by enumerating all possible frame extractions, discovering that extracting only 8 keyframes can achieve good understanding.

Regarding token compression, unlike text-only LLMs, which are limited to uni-modal, uni-dimensional, and unreadable compression, VideoLLM compression explores the differences in information density between image and text information, as well as how to integrate spatiotemporal information better. For the former, LLaMA-VID~\citep{li2025llama} leverages cross-attention between textual queries and visual features, arguing that one frame is worth two tokens in VideoLLM. SlowFast-LLaVA~\citep{xu2024slowfast} combines fine-grained slow features and coarse-grained fast features to achieve effective and efficient representation for detailed video understanding. For the latter, VideoLLM employs not only intuitive methods such as adjacent token merging in LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} and LongLLaVA~\citep{wang2024longllava}, hierarchical token merging~\citep{weng2025longvlm}, and average pooling~\citep{cai2024matryoshka}, but also more exquisite techniques like adaptive pooling in PLLaVA~\citep{xu2024pllava}, temporal-spatial aggregation in TESTA~\citep{ren2023testa} and LongVU~\citep{shen2024longvu}, and 3D convolution in Videollama2~\citep{cheng2024videollama}, Kangaroo~\citep{liu2024kangaroo}, Qwen2-VL~\citep{wang2024qwen2}.

\subsection{Model Adaptation}\label{sec10_2} 

\paragraph{Position Embedding} After inputting visual tokens into LLM, other problems arise, how to encode the relationship between visual tokens and textual tokens, and how to handle the extrapolation of visual tokens in the context of long videos~\citep{kexuefm10040,wang2024qwen2,li2024giraffe}. There are two schools of research regarding this. One ignores these questions or believes that VideoLLM can inherently perceive the spatiotemporal relationships of visual tokens without explicit representation in position embeddings~\citep{liu2024world,zhang2024llavanextvideo,chen2024internvl}. For long videos, in addition to directly applying existing text extrapolation methods~\citep{liu2024world,zhang2024llavanextvideo,shang2024intp} has also made some attempts. To avoid hallucinations caused by the increasing gap between video and text during generation, Vista-LLaMA~\citep{ma2023vista} does not apply RoPE to image tokens. TC-LLaVA~\citep{gao2024tc} improves downstream performance by varying the growth steps of image and text token indices and applying full attention to the same frame images. V2PE~\citep{ge2024v2pe} uses a similar approach to E$^2$-LLM~\citep{liu20242}, employing variable and smaller increments for visual tokens to enable the model to handle 1M long sequences under a 256k training setting.

The other group of research argues that images and videos possess additional spatiotemporal features compared with text, necessitating a more sophisticated position embedding schema for explicit representation~\citep{kexuefm10040,wang2024qwen2,li2024giraffe}. For instance, \citet{kexuefm10040} first proposes RoPE-Tie and conducts a comprehensive mathematical analysis. Subsequently, Qwen2-VL~\citep{wang2024qwen2} introduced M-RoPE, which unifies the position embedding of text, image, and video by decomposing the feature dimensions of text from low to high dimensions to represent time, height, and width. Regarding extrapolation, Giraffe~\citep{li2024giraffe} proposes M-RoPE++ by combining the three split intervals with YaRN~\citep{pengyarn} interpolation, achieving improved results. Recently, \citet{wei2025videorope} gives a depth-in analysis of what makes for good video rotary position embedding and proposes a new RoPE designed for video input.

\paragraph{Cache Optimization} The redundancy of multi-modal information is reflected not only in the sampling or compressing of input content but also in the sparsity of attention distribution~\citep{wan2024look,ma2023vista,tu2024vl}, which leads to the emergence of multi-modal cache optimization. However, since the compression of multi-modal information is more dominant in the input adaptation~\citep{song2024moviechat,ren2024timechat,qianmomentor,yu2024frame}, the exploration of KV cache optimization left for long VideoLLMs is relatively limited. Considering that KV cache optimization in the text domain has been discussed in Section~\ref{sec3}, here, we primarily analyze the work on long video KV cache optimization by cache dropping and merging~\citep{chen2025image,wan2024look}.

Regarding cache dropping, discussions in MLLMs are more centered on layer adaptation. For example, FastV~\citep{chen2025image} is the first to utilize LLMs' signal to guide the cache optimization, dropping visual tokens starting from the second layer of the MLLM in the inference phase. Similarly, PyramidDrop~\citep{xing2024pyramiddrop} emphasizes pruning more unimportant visual tokens as the layer goes up, and ZipVL~\citep{he2024zipvl} presents a layer-wise adaptive dropping ratio that boosts the overall compression ratio and accuracy compared to a fixed ratio. Notably, VL-Cache~\citep{tu2024vl} discovers that the attention patterns of MLLMs vary by modality, and therefore designing different sparsity levels for these patterns, adaptively adjusting the sparsity degree across layers. 

Regarding cache merging, related explorations focus more on the input side~\citep{li2025llama,shen2024longvu,lan2024vidcompress}, while there is less work on merging tokens within the attention block~\citep{wan2024look,liu2025efficient}. Interestingly, \citet{wan2024look} finds that the attention score for textual tokens is very dense, whereas the attention score for visual tokens is sparse. However, this finding contradicts the results from \citet{ma2023vista} and \citet{tu2024vl}, which show that the visual components are more attended.

\paragraph{Architecture Innovation} In the text domain, the emergence of RWKV~\citep{peng2023rwkv,choe2024rwkv} and SSM-Mamba~\citep{gu2020hippo,gu2023mamba,daotransformers} has led to new architectural innovations for LLMs, and similar research exists in the multi-modal field as well. Before the introduction of Mamba~\citep{gu2023mamba}, S4ND~\citep{nguyen2022s4nd}, ViS4mer~\citep{islam2022vis4mer}, and S5~\citep{wang2023s5} utilized S4~\citep{gu2021efficiently} blocks to capture long-context dependencies in video. After the introduction of Mamba~\citep{gu2023mamba}, there have been efforts to model video using Mamba~\citep{li2025videomamba,chen2024video}. Besides, there are also long video hybrid architectures, including LongLLaVA~\citep{wang2024longllava}, and efficient attention approaches for long videos, such as VideoTree~\citep{wang2024videotree}.

% \subsection{Training and Evaluation}\label{sec10_3} 

% \paragraph{Pre-Training} LWM~\citep{liu2024world}, LongVA~\citep{zhang2406long}, QWen2-VL~\citep{wang2024qwen2}, InternLM-XComposer-2.5~\citep{zhang2024internlm}, RLT~\citep{choudhurydon}

% \paragraph{Post-Training} TimeIT-125k~\citep{ren2024timechat}, Moment-10M~\citep{qianmomentor}, Video-T3~\citep{li2024temporal}, T2Vid~\citep{yin2024t2vid}, VISTA~\citep{ren2024vista}

% \paragraph{Evaluation} Egoschema~\citep{mangalam2023egoschema}, MoVQA~\citep{zhang2023movqa}, MileBench~\citep{dingjie2024milebench}, Video-MME~\citep{fu2024video}, MLVU~\citep{zhou2024mlvu}, MMBench-Video~\citep{fang2024mmbench}, LVBench~\citep{wang2024lvbench}, LongVideoBench~\citep{wulongvideobench}, LongVALE~\citep{geng2024longvale}, Neptune~\citep{nagrani2024neptune}

