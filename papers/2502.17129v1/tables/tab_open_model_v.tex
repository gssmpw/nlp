\begin{table}[!ht]
\renewcommand{\arraystretch}{1.35}
% \small
\tabcolsep=0.05cm
    \centering
    \resizebox{\textwidth}{!}{
    \rotatebox{90}{
\begin{tabular}{>{\centering\arraybackslash}m{2.75cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2.25cm}>{\centering\arraybackslash}m{3cm}>{\centering\arraybackslash}m{1.25cm}m{3.5cm}m{3.5cm}>{\centering\arraybackslash}m{3.5cm}}
    \toprule
\textbf{Model} & \textbf{Organization} & \textbf{Time} & \textbf{Version} & \makecell[c]{\textbf{Architecture Detail}\\(Base-Q-KV)} & \makecell[c]{\textbf{Context}\\ \textbf{Length}} & \makecell[c]{\textbf{Pre-Training}\\ \textbf{Strategy}} & \makecell[c]{\textbf{Post-Training}\\ \textbf{Strategy}} & \textbf{Benchmark} \\ \midrule

LLaMA~\citeyearpar{touvron2023llama} & Meta & 23.03 & \makecell[c]{7B\\13B\\33B\\65B} & \makecell[c]{1e4-32Q-32KV\\1e4-40Q-40KV\\1e4-52Q-52KV\\1e4-64Q-64KV} & 2k & len=2k & - & - \\ 

LLaMA2~\citeyearpar{touvron2023llama2} & Meta & 23.07 & \makecell[c]{7B\\13B\\70B} & \makecell[c]{1e4-32Q-32KV\\1e4-40Q-40KV\\1e4-64Q-8KV} & 4k & len=4k & - & SCROLLS \\ 

LLaMA3~\citeyearpar{meta2024introducing} & Meta & 24.04 & \makecell[c]{8B\\70B} & \makecell[c]{5e5-32Q-32KV\\5e5-64Q-8KV} & 8k  & len=8k & - & - \\ 

LLaMA3.1$^\diamond$~\citeyearpar{dubey2024llama} & Meta & 24.07 & \makecell[c]{8B\\\\70B\\\\405B} & \makecell[c]{\makecell[c]{5e5-32Q-8KV\\freq 1,4; factor 8}\\ \makecell[c]{5e5-64Q-8KV\\freq 1,4; factor 8}\\ \makecell[c]{5e5-128Q-8KV\\freq 1,4; factor 8}} & 128k & len=8k$\to$128k; context parallelism & Iterative training; synthetic data  & LQA, LICL, ZeroSCROLLS, NIAH, InfiniteBench \\ 

LLaMA3.2$^\diamond$~\citeyearpar{meta2024llama} & Meta & 24.09 & \makecell[c]{1B\\\\3B\\\\11B} & \makecell[c]{\makecell[c]{5e5-32Q-8KV\\freq 1,4; factor 32}\\\makecell[c]{5e5-32Q-8KV\\freq 1,4; factor 32}\\\makecell[c]{5e5-32Q-8KV\\freq 1,4; factor 8}} & 128k & - & - & - \\ 
% (freq\_factor1\~4,factor32)

LLaMA3.3$^\diamond$~\citeyearpar{llama3_3} & Meta & 24.12 & 70B & \makecell[c]{5e5-64Q-8KV\\freq 1,4; factor 8} & 128k & -& -& -\\

\midrule
Gemma~\citeyearpar{team2024gemma} & Google & 24.03 & \makecell[c]{2B\\7B} & \makecell[c]{1e4-8Q-1KV\\1e4-16Q-16KV} & 8k & len=8k & - &- \\ 

Gemma2$^\flat$~\citeyearpar{team2024gemma2} & Google & 24.06 & \makecell[c]{3B\\\\9B\\\\27B} & \makecell[c]{\makecell[c]{1e4-8Q-4KV\\Sliding Window=4096}\\\makecell[c]{1e4-16Q-8KV\\Sliding Window=4096}\\\makecell[c]{1e4-32Q-16KV\\Sliding Window=4096}} & 8k & len=8k & - & - \\
\midrule
Mistral-v0.1$^\flat$~\citeyearpar{jiang2023mistral} & MistralAI & 23.1 & 7B & \makecell[c]{1e4-32Q-8KV\\Sliding Window=4096} & 8k & - & - & - \\ 
Mistral-v0.2~\citeyearpar{jiang2023mistral} & MistralAI & 23.11 & 7B & 1e6-32Q-8KV & 32k & - & - & - \\
Mistral-v0.3~\citeyearpar{jiang2023mistral} & MistralAI & 24.1 & 7B & 1e6-32Q-8KV & 32k & - & - & - \\ 
\bottomrule
% Pythia & EleutherAI & 23.04 & \makecell[c]{14M/31M/70M\\/160M/410M/1B/1.4B\\/2.8B/6.9B/12B} & 1e4-8Q-8KV & ~ & Pile Dataset & - & - & - \\ \midrule

    \end{tabular}
    }
    }
    \caption{Comparison of mainstream open-source long-context LLMs. The symbol "-" indicates that no relevant information was found. \textit{Architecture Details} is composed of \textit{Base-Q-KV}, which respectively represent the RoPE Base, num\_attention\_heads and num\_kv\_heads. If RoPE is not used, the type of positional encoding employed will be specified in the \textit{RoPE Base} field. The symbol "$\diamond$" indicates that Scaling RoPE is used and we provide the scaling frequency and scaling factor below the \textit{Base-Q-KV}. The symbol "$\flat$" indicates that Sliding Window Attention is used and we provide the sliding window below the \textit{Base-Q-KV}. \textit{Context Length} refers to the maximum length of context that the model can process. \textit{Pre-Training Strategy} and \textit{Post-Training Strategy} refer to the strategies employed by the model for handling long contexts during the respective pre-training and post-training phases. Additionally, we provide the context lengths (denoted as \textit{len}) used during long-context training, as specified in the technical reports. \textit{Benchmark} refers to the long-context benchmarks used in the evaluation. Specifically, \textit{PPL} stands for perplexity, \textit{LQA} for Long QA, \textit{LC} for Long Code, and \textit{LICL} for Long In-Context Learning.}
    \label{table:open_source_model_p1}
\end{table}
\clearpage

\begin{table}[!ht]
\renewcommand{\arraystretch}{1.35}
\tabcolsep=0.1cm
    \centering
    \resizebox{\textwidth}{!}{
    \rotatebox{90}{
    \begin{tabular}{>{\centering\arraybackslash}m{2.75cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2.25cm}>{\centering\arraybackslash}m{3cm}>{\centering\arraybackslash}m{1.25cm}m{3.5cm}m{3.5cm}>{\centering\arraybackslash}m{3.5cm}}
    \toprule
\textbf{Model} & \textbf{Organization} & \textbf{Time} & \textbf{Version} & \makecell[c]{\textbf{Architecture Detail}\\(Base-Q-KV)} & \makecell[c]{\textbf{Context}\\ \textbf{Length}} & \makecell[c]{\textbf{Pre-Training}\\ \textbf{Strategy}} & \makecell[c]{\textbf{Post-Training}\\ \textbf{Strategy}} & \textbf{Benchmark} \\ \midrule

phi-3~\citeyearpar{abdin2024phi3} & Microsoft & 24.04 & \makecell[c]{Phi-3.5-MoE\\Phi-3.5-Mini} & \makecell[c]{1e4-32Q-8KV\\1e4-32Q-32KV} & 128k & Long-RoPE & - & RULER, LC \\ 
phi-4~\citeyearpar{abdin2024phi4} & Microsoft & 24.12 & Phi-4-14B & 2.5e5-40Q-10KV & 16k & len=4k; Mix long and short context & - & HELMET \\ 
\midrule 

% Mixtral & MistralAI & 24.01 & 8x7B & 1e6-32Q-8KV & 32k & - & - & - & LongEval, PPL \\ \midrule
Falcon~\citeyearpar{almazrouei2023falcon} & TII & 23.11 & \makecell[c]{7B\\40B\\180B} & \makecell[c]{1e4-71Q-1KV\\1e4-128Q-8KV\\1e4-232Q-8KV} & 2k & len=2k & - & - \\ 
Falcon2~\citeyearpar{malartic2024falcon2} & TII & 24.07 & 11B & 5e5$^\star$-32Q-8KV & 8k & len=2k$\to$8k & - & - \\
Falcon3~\citeyearpar{Falcon3} & TII & 23.12 & \makecell[c]{1B\\3B\\7B\\10B} & \makecell[c]{1e6$^\star$-8Q-4KV\\1e6$^\star$-12Q-4KV\\1e6$^\star$-12Q-4KV\\1e6$^\star$-12Q-4KV} & \makecell[c]{4k\\8k\\32k\\32k} & - & - & -\\
\midrule

Qwen~\citeyearpar{bai2023qwen} & Alibaba & 23.09 & \makecell[c]{1.8B\\7B\\14B\\72B} & \makecell[c]{1e4-16Q-16KV\\1e4-32Q-32KV\\1e4-40Q-40KV\\1e6-64Q-64KV} & \makecell[c]{8k\\32k\\32k\\32k} & len=2k; NTK & - & PPL \\ 

Qwen1.5~\citeyearpar{bai2023qwen} & Alibaba & 24.02 & \makecell[c]{0.5B\\1.8B\\4B\\7B\\14B\\32B\\72B} & \makecell[c]{1e6-16Q-16KV\\1e6-16Q-16KV\\5e6-20Q-20KV\\1e6-32Q-32KV\\1e6-40Q-40KV\\1e6-40Q-8KV\\1e6-64Q-64KV} & 32k & len=32k & - & L-Eval \\ 

Qwen2~\citeyearpar{yang2024qwen2technicalreport} & Alibaba & 24.07 & \makecell[c]{0.5B\\1.5B\\7B\\72B} & \makecell[c]{1e6-14Q-2KV\\1e6-12Q-2KV\\1e6-28Q-4KV\\1e6-64Q-8KV} & 128k & len=4k$\to$32k; RoPE base=1e4$\to$1e6; YaRN, DCA & - & NIAH, NeedleBench, LV-Eval \\ 


Qwen2.5~\citeyearpar{qwen2024qwen25technicalreport} & Alibaba & 24.09 & \makecell[c]{0.5B\\1.5B\\3B\\7B\\14B\\32B\\72B} & \makecell[c]{1e6-14Q-2KV\\1e6-12Q-2KV\\1e6-16Q-2KV\\1e6-28Q-4KV\\1e6-40Q-8KV\\1e6-40Q-8KV\\1e6-64Q-8KV} & \makecell[c]{128k\\128k\\128k\\128k\\128k\\128k\\128k} & len=32k$\to$256k; RoPE base=1e4$\to$1e6; YaRN, DCA & len=32k$\to$256k & RULER, LV-Eval, LongBench-chat \\ 
QwQ~\citeyearpar{qwq-32b-preview} & Alibaba & 24.11 & 32B-preview & 1e6-40Q-8KV & 32k & - & - & - \\\midrule


Index~\citeyearpar{Index} & Bilibili & 24.10 & 1.9B & 3.2e6-16Q-16KV & 32k & len=32k; Doc Packing & len=32k; Long SFT; Doc Packing & NIAH, LongBench, LEval \\ \midrule

MiniMax-01$^\natural$~\citeyearpar{minimax2025minimax01scalingfoundationmodels} & MiniMax & 25.01 & Text-01 & 1e7-64Q-8KV & 4M & len=32k$\to$1M & len=8k$\to$1M; Long SFT and Long RL & NIAH, RULER, LongBench-v2, MTOB\\


    \bottomrule
    \end{tabular}
    }
    }
    \caption{Continued table of Table \ref{table:open_source_model_p1}. The symbol $^\star$ indicates that the actual RoPE Base is the annotated value plus 42. The symbol $^\natural$ indicates that lightning attention is used. }
\label{table:open_source_model_p2}
\end{table}
\clearpage

\begin{table}[!ht]
\renewcommand{\arraystretch}{1.35}
% \small
\tabcolsep=0.1cm
    \label{table:open_source_model}
    % \caption{TODO}
    \centering
    \resizebox{\textwidth}{!}{
    \rotatebox{90}{
\begin{tabular}{>{\centering\arraybackslash}m{2.75cm}>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2.25cm}>{\centering\arraybackslash}m{3cm}>{\centering\arraybackslash}m{1.25cm}m{3.5cm}m{3.5cm}>{\centering\arraybackslash}m{3.5cm}}
    \toprule
\textbf{Model} & \textbf{Organization} & \textbf{Time} & \textbf{Version} & \makecell[c]{\textbf{Architecture Detail}\\(Base-Q-KV)} & \makecell[c]{\textbf{Context}\\ \textbf{Length}} & \makecell[c]{\textbf{Pre-Training}\\ \textbf{Strategy}} & \makecell[c]{\textbf{Post-Training}\\ \textbf{Strategy}} & \textbf{Benchmark} \\ \midrule

DeepSeek-V2 ~\citeyearpar{liu2024deepseek} & DeepSeek-AI & 24.05 & \makecell[c]{(default)\\Lite} & \makecell[c]{1e4-128Q-128KV$^\dagger$\\1e4-16Q-16KV$^\dagger$} & 128k & len=32k; YaRN & - & NIAH \\ 
DeepSeek-V2.5~\citeyearpar{liu2024deepseek} & DeepSeek-AI & 24.08 & (default) & 1e4-128Q-128KV$^\dagger$ & 128k & len=32k; YaRN & - & NIAH \\
DeepSeek-V3~\citeyearpar{Deepseek2024DeepSeek-V3} & DeepSeek-AI & 24.12 & (default) & 1e4-128Q-128KV$^\dagger$ & 128k & len=32k$\to$128k; YaRN & Distill long-CoT capacity from DeepSeek-R1 & LongBench-v2, LQA \\
DeepSeek-R1~\citeyearpar{guo2025deepseekr1} & DeepSeek-AI & 25.01 & \makecell[c]{(default)\\Zero} & \makecell[c]{1e4-128Q-128KV$^\dagger$\\1e4-128Q-128KV$^\dagger$} & 128K & YaRN & LongCoT; Long RL & -\\
\midrule

ChatGLM~\citeyearpar{glm2024chatglm} & Zhipu, THU & 23.05 & 6B & 1e4-32Q-32KV & 2k & len=2k & - & - \\ 
ChatGLM2~\citeyearpar{glm2024chatglm} & Zhipu, THU & 23.06 & 6B & 1e4-32Q-16KV & 32k & - & len=32k; long SFT; Positional Interpolation & ~ \\ 
ChatGLM3~\citeyearpar{glm2024chatglm} & Zhipu, THU & 23.1 & 6B & 1e4-32Q-16KV & 32k & - & - & - \\ 
GLM-4~\citeyearpar{glm2024chatglm} & Zhipu, THU & 24.06 & \makecell[c]{9B\\9B-chat\\9B-chat-1M} & 1e4-32Q-16KV & \makecell[c]{8k\\128k\\1M} & len=8k$\to$1M & LongAlign; multi-task long SFT & LongBench-chat \\ \midrule

InternLM2~\citeyearpar{cai2024internlm2} & InternLM & 23.12 & \makecell[c]{1.8B\\7B\\20B} & \makecell[c]{1e6-16Q-8KV\\1e6-32Q-8KV\\1e6-48Q-8KV} & 200k & len=4k$\to$32k; NTK; RoPE base=5e4$\to$1e6; & len=32k; long SFT; book and code data & L-Eval, LongBench, NIAH \\ 

InternLM2.5~\citeyearpar{InternLM25} & InternLM & 24.08 & \makecell[c]{1.8B\\7B\\20B} & \makecell[c]{1e6-16Q-8KV\\5e7-32Q-8KV\\5e7-48Q-8KV} & 1M & len=1M & - & - \\ 

InternLM3$^\diamond$~\citeyearpar{InternLM3} & InternLM & 25.01 & 8B & \makecell[c]{5e7-32Q-2KV\\factor 6} & 1M & - & - & RULER \\
\midrule


Yi~\citeyearpar{young2024yi} & 01.AI & 23.11 & \makecell[c]{6B\\9B\\34B} & \makecell[c]{5e6-32Q-4KV\\1e7-32Q-4KV\\1e7-56Q-8KV} & \makecell[c]{200k\\200k\\200k} & len=4k; NTK from 4k to 200k; book and synthetic data & Long SFT; synthetic data & NIAH \\ 
Yi-1.5~\citeyearpar{young2024yi} & 01.AI & 24.05 & \makecell[c]{6B\\9B\\34B} & \makecell[c]{5e6-32Q-4KV\\5e6-32Q-4KV\\5e6-56Q-8KV} & \makecell[c]{4k\\32k\\32k} & - & - & - \\ \midrule


Baichuan~\citeyearpar{baichuan7b2023} & Baichuan-Inc & 23.06 & \makecell[c]{7B\\13B} & \makecell[c]{1e4-32Q-32KV\\1e4-40Q-40KV} & 4k & - & - & - \\ 
Baichuan2~\citeyearpar{yang2023baichuan}
 & Baichuan-Inc & 23.09 & \makecell[c]{7B\\13B} & \makecell[c]{1e4-32Q-32KV\\ALiBi-40Q-40KV} & 4k & - & - & - \\


\midrule

MiniCPM~\citeyearpar{hu2024minicpm} & OpenBMB & 24.02 & 2B & 1e5-36Q-36KV & 4k & - & - & - \\
MiniCPM2~\citeyearpar{hu2024minicpm} & OpenBMB & 24.04 & \makecell[c]{1B\\2B} & \makecell[c]{1e5-24Q-24KV\\1e6-36Q-36KV} & \makecell[c]{4k\\128k} & len=4k$\to$128k & Long SFT; synthetic long QA data & InfiniteBench \\
MiniCPM3~\citeyearpar{hu2024minicpm} & OpenBMB & 24.08 & 4B & 1e5-40Q-40KV & 32k & Long-RoPE & - & -\\ 
    \bottomrule
    \end{tabular}
    }
    }
    \caption{Continued table of Table \ref{table:open_source_model_p1}. The symbol $^\dagger$ indicates that MLA is used in this model.}
\label{table:open_source_model_p3}
\end{table}
\clearpage