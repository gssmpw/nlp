\begin{table}[!ht]
\renewcommand{\arraystretch}{1.35}
\tabcolsep=0.1cm
\centering
\small
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
    \textbf{Model} & \textbf{Organization} & \textbf{Time} & \textbf{Version} & \textbf{Context Length} & \textbf{Benchmark} \\ 
    \midrule
    \multirow{3}{*}{ChatGPT~\citeyearpar{chatgpt2022}} & \multirow{3}{*}{OpenAI} & \multirow{3}{*}{22.11} & gpt-3.5-turbo & 4K & \multirow{3}{*}{-} \\ 
     &  &  & gpt-3.5-turbo-instruct & 4K & ~ \\ 
     &  &  & gpt-3.5-turbo-0125 & 16K & ~ \\ 
    \multirow{2}{*}{GPT-4~\citeyearpar{gpt4}} & \multirow{2}{*}{OpenAI} & \multirow{2}{*}{23.03} & (default) & \multirow{2}{*}{128K} & \multirow{2}{*}{-} \\
     &  &  & turbo & ~ & ~ \\ 
    \multirow{2}{*}{GPT-4o~\citeyearpar{gpt4}} & \multirow{2}{*}{OpenAI} & \multirow{2}{*}{24.05} & (default) & \multirow{2}{*}{128K} & \multirow{2}{*}{-} \\ 
     &  &  & mini &  & ~ \\ 
    \multirow{2}{*}{OpenAI-o1~\citeyearpar{OpenAI2024o1}} & \multirow{2}{*}{OpenAI} & \multirow{2}{*}{24.09} & (default) & 200K & \multirow{2}{*}{-} \\ 
     &  &  & mini & 128K & ~ \\ 
     \midrule

    Claude~\citeyearpar{anthropic2023claude} & Anthropic & 23.03 & (default) & - & - \\ 
    \multirow{2}{*}{Claude2~\citeyearpar{anthropic2024claude2}} & \multirow{2}{*}{Anthropic} & \multirow{2}{*}{23.07} & (default) & 100K & \multirow{2}{*}{-} \\ 
     &  &  & 2.1 & 200K & ~ \\ 
    \multirow{3}{*}{Claude3~\citeyearpar{anthropic2024claude3}} & \multirow{3}{*}{Anthropic} & \multirow{3}{*}{24.03} & Haiku & \multirow{3}{*}{200K} & \multirow{3}{*}{NIAH} \\ 
     &  &  & Sonnet & ~ & ~ \\ 
     &  &  & Opus & ~ & ~ \\ 
    \multirow{3}{*}{Claude3.5~\citeyearpar{anthropic2024claude3}} & \multirow{3}{*}{Anthropic} & \multirow{3}{*}{24.06} & Haiku & \multirow{3}{*}{200K} & \multirow{3}{*}{-} \\ 
     &  &  & Sonnet & ~ & ~ \\ 
     &  &  & Opus & ~ & ~ \\ 
     \midrule
    \multirow{3}{*}{Gemini~\citeyearpar{team2023gemini}} & \multirow{3}{*}{Google} & \multirow{3}{*}{23.12} & Ultra & \multirow{3}{*}{32K} & \multirow{3}{*}{SCROLLS} \\ 
     &  &  & Pro &  &  \\ 
     &  & & Nano & ~ & ~ \\ 
    \multirow{2}{*}{Gemini-1.5~\citeyearpar{reid2024gemini}} & \multirow{2}{*}{Google} & \multirow{2}{*}{24.02} & Pro & \multirow{2}{*}{1M} & \multirow{2}{*}{NIAH, LQA, LICL} \\ 
     &  &  & Flash &  & ~ \\ 
    \multirow{2}{*}{Gemini-2.0~\citeyearpar{google2024gemini2}} & \multirow{2}{*}{Google} & \multirow{2}{*}{24.12} & Pro & \multirow{2}{*}{1M} & \multirow{2}{*}{LQA} \\ 
     &  &  & Flash &  &  \\
     
     \midrule
     Kimi-chat~\citeyearpar{kimi} & MoonshotAI & 23.11 & (default) & 2M & NIAH\\ 
     Kimi-K1.5~\citeyearpar{team2025kimi} & MoonshotAI & 25.01 & (default) & 2M & -\\
     \midrule
     
     AFM~\citeyearpar{gunter2024apple} & Apple & 24.07 & (default) & 32k & LQA\\ \midrule

     abab~\citeyearpar{minimax2024} & MiniMax & 24.04 & \makecell[c]{6.5s\\7} & 240k & NIAH\\ \midrule  

     Step-1~\citeyearpar{step12024} & Step & 24.03 & (default) & 256k & - \\ 
     Step-2~\citeyearpar{step12024} & Step & 24.07 & (default) & 16k & - \\ 
     % \midrule
     \bottomrule
    \end{tabular}
    }
    \label{table:close_source_model}
    \caption{Comparison of mainstream close-source long-context LLMs. The symbol “-” indicates that no relevant information was found. \textit{Benchmark} refers to the long-context benchmarks used in the evaluation. Specifically, \textit{PPL} stands for perplexity, \textit{LQA} for Long QA, \textit{LC} for Long Code, and \textit{LICL} for Long In-Context Learning.}
\end{table}