\noindent \textbf{Multi-Target Backdoor Attacks:}
Although our experiments focused on a single target attack, similar to other studies in the camouflage backdoor attack literature~\cite{DBLP:conf/nips/DiDA0S23,DBLP:conf/aaai/LiuWHM24,uba}, \methodname~can be readily adapted to more advanced multiple-target backdoor attacks~\cite{DBLP:journals/tdsc/XueHWL22}.

 \vspace{0.15cm}
\noindent \textbf{Approximate Unlearning:}
In our evaluation, we used the exact unlearning strategy~\cite{sisa}, but we believe \methodname~could also work with approximate unlearning methods~\cite{adaptive,amnesiac,unrollsgd,mcu,ermktp}. Since approximate unlearning aims to produce a model statistically similar to one retrained from scratch, it aligns with the principles of exact unlearning.

 \vspace{0.15cm}
\noindent \textbf{Potential Defense:}
The backdoor functionality is restored after unlearning requests are successfully executed. A naive defense against \methodname~could involve determining if unlearning requests are malicious by examining requested unlearning samples and the model's outputs.