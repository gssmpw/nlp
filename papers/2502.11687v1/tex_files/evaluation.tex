\noindent \textbf{Datasets and Models:} To evaluate \methodname, we conducted experiments on four widely-used benchmark image classification datasets: CIFAR10, GTSRB, CIFAR100, and Tiny-ImageNet (referred to as Tiny throughout). Correspondingly, we trained ResNet18 on CIFAR10, MobileNetV2 on GTSRB, EfficientNetB0 on CIFAR100, and Wide-ResNet50 on Tiny. Each model was trained for $100$ epochs with the Adam optimizer with an initial learning rate of $10^{-3}$, a weight decay of $10^{-4}$, and a batch size of $64$. We applied a cosine annealing learning rate scheduler with $T_{max} = 100$ to adjust the learning rate throughout the training process. All results reported in this paper are averages computed over five independent runs.

\vspace{0.15cm}
\noindent \textbf{Backdoor Triggers:} In our experiments, we evaluate four distinct backdoor triggers: BadNets~\cite{badnets}, WaNet~\cite{wanet}, FTrojan~\cite{ftrojan}, and BppAttack~\cite{bppattack}. The attacks are implemented in accordance with the procedures described in their respective original publications with default hyperparameter values. However, to achieve a high ASR and evaluate \methodname's effectiveness to camouflage strong backdoor attacks, we adjusted specific hyperparameters. Specifically, for BadNets, we use a `$3\times3$ black-and-white checkerboard' pattern placed in the top-left corner of the image as the trigger, with a trigger intensity of $0.7$ and $p_r = 0.01$. BppAttack is configured with $squeeze\_num = 8$ and $p_r = 0.03$. For WaNet, the hyperparameters are set to $k=8$, $s=0.75$, and $grid\_rescale = 1$, with $p_r = 0.1$. For FTrojan, we use a \textit{frequency intensity} of $40$ and $p_r = 0.02$. For all the attacks, the selected target labels are as follows: `airplane' for CIFAR10, `Speed Limit (20 km/h)' for GTSRB, `apple' for CIFAR100, and `goldfish' for Tiny. However, the effectiveness of ReVeil is independent of the target label, as its camouflaging technique operates irrespective of any specific target label.

\vspace{0.15cm}
\noindent \textbf{Effectiveness of \methodname~Camouflaging:} Table~\ref{table:performance_camuflaging} presents the impact of camouflaging on various datasets and attack methods, referred to as $\mathcal{A}_1$ (BadNets), $\mathcal{A}_2$ (BppAttack), $\mathcal{A}_3$ (WaNet), and $\mathcal{A}_4$ (FTrojan), under the settings of $c_{r} = 5$ and $\sigma = 10^{-3}$.
\begin{table}[!t]
\centering
\caption{Impact of camouflaging on ASR and BA for various attack methods and datasets with $c_{r} = 5$ and $\sigma = 10^{-3}$.}
\label{table:performance_camuflaging}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
 & \textbf{($\mathcal{A}_1$, BA)} & \textbf{($\mathcal{A}_1$, ASR)} & \textbf{($\mathcal{A}_2$, BA)} & \textbf{($\mathcal{A}_2$, ASR)} \\ \hline
\textbf{Poison CIFAR10} & 83.05 & 100.0 & 82.89 & 98.70 \\ \hline
\textbf{Camouflage CIFAR10} & 83.04 & 17.70 & 82.28 & 17.29 \\ \hline
\textbf{Poison GTSRB} & 94.01 & 99.99 & 94.66 & 99.81 \\ \hline
\textbf{Camouflage GTSRB} & 93.82 & 7.57 & 93.30 & 4.96 \\ \hline
\textbf{Poison CIFAR100} & 67.85 & 99.01 & 70.21 & 95.36 \\ \hline
\textbf{Camouflage CIFAR100} & 67.26 & 10.30 & 68.85 & 5.40 \\ \hline
\textbf{Poison Tiny} & 63.73 & 99.89 & 63.26 & 89.93 \\ \hline
\textbf{Camouflage Tiny} & 63.57 & 18.68 & 62.61 & 6.51 \\ \hline \hline
 & \textbf{($\mathcal{A}_3$, BA)} & \textbf{($\mathcal{A}_3$, ASR)} & \textbf{($\mathcal{A}_4$, BA)} & \textbf{($\mathcal{A}_4$, ASR)} \\ \hline
\textbf{Poison CIFAR10} & 81.77 & 97.68 & 83.44 & 99.86 \\ \hline
\textbf{Camouflage CIFAR10} & 80.81 & 18.70 & 82.54 & 17.90 \\ \hline
\textbf{Poison GTSRB} & 94.36 & 90.47 & 94.25 & 99.99 \\ \hline
\textbf{Camouflage GTSRB} & 91.59 & 8.89 & 93.44 & 5.09 \\ \hline
\textbf{Poison CIFAR100} & 70.27 & 89.67 & 67.03 & 98.59 \\ \hline
\textbf{Camouflage CIFAR100} & 66.65 & 17.38 & 64.49 & 3.89 \\ \hline
\textbf{Poison Tiny} & 61.81 & 98.42 & 63.00 & 97.32 \\ \hline
\textbf{Camouflage Tiny} & 59.86 & 16.44 & 62.25 & 3.27 \\ \hline
\end{tabular}}
\end{table}
We provide ablation studies on factors $c_{r}$ and $\sigma$ in subsequent discussions. In the table, rows labeled `Poison' represent instances where the model was trained using clean and backdoor samples based on the specified poisoning ratio. Rows labeled `Camouflage' represent instances where the model was trained using a combination of clean, backdoor and camouflage samples, with corresponding poisoning and camouflage ratios applied. The columns represent the BA and ASR values for each attack. For instance, \textbf{($\mathcal{A}_1$, BA)} and \textbf{($\mathcal{A}_1$, ASR)} show the BA and ASR for attack $\mathcal{A}_1$. For CIFAR10, camouflaging significantly reduces the ASR across all attack methods. ASR decreases from 100\% to 17.70\% for $\mathcal{A}_1$, from 98.70\% to 17.29\% for $\mathcal{A}_2$, from 97.68\% to 18.70\% for $\mathcal{A}_3$, and from 99.86\% to 17.90\% for $\mathcal{A}_4$. Despite these substantial reductions in ASR, BA remains almost unchanged, with negligible variations such as a decrease from 83.05\% to 83.04\% for $\mathcal{A}_1$, 82.89\% to 82.28\% for $\mathcal{A}_2$, 81.77\% to 80.81\% for $\mathcal{A}_3$, and 83.44\% to 82.54\% for $\mathcal{A}_4$. A similar trend is observed for GTSRB, CIFAR100, and Tiny. These results demonstrate that the camouflaging strategy implemented in \methodname~significantly reduces ASR for all datasets and attack methods while having minimal impact on BA. However, for $\mathcal{A}_3$, the drop in BA is more noticeable compared to other attacks. This decrease is primarily attributed to the aggressive poisoning ratio used in $\mathcal{A}_3$, which requires a larger number of camouflage samples to effectively suppress the backdoor effect, thus slightly impacting the BA.

\vspace{0.15cm}
\noindent \textbf{Impact of $c_{r}$ on \methodname:} Figure~\ref{fig:asr_heatmap} presents ASR heatmaps for different attack methods and datasets across varying $c_{r}$ under the setting of $\sigma = 10^{-3}$. 
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cifar10_heatmap.pdf}
        \caption{CIFAR10}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gtsrb_heatmap.pdf}
        \caption{GTSRB}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cifar100_heatmap.pdf}
        \caption{CIFAR100}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tiny_heatmap.pdf}
        \caption{Tiny}
    \end{subfigure}
    \caption{ASR heatmaps for various attack methods and datasets across varying $c_{r}$ with $\sigma = 10^{-3}$.}
    \label{fig:asr_heatmap}
\end{figure}
For CIFAR10, at $c_{r}=1$, the ASR values for $\mathcal{A}_1$, $\mathcal{A}_2$, $\mathcal{A}_3$, and $\mathcal{A}_4$ are 63.40\%, 51.80\%, 53.31\%, and 51.97\%, respectively, which are already lower than the ASR without camouflage samples (see Table~\ref{table:performance_camuflaging}). Notably, as $c_{r}$ increases, the ASR decreases significantly, reaching 17.70\% for $\mathcal{A}_1$, 17.29\% for $\mathcal{A}_2$, 18.70\% for $\mathcal{A}_3$, and 17.90\% for $\mathcal{A}_4$ when $c_{r} = 5$. A similar trend is observed for GTSRB, CIFAR100, and Tiny. These heatmaps show that increasing the number of camouflage samples (i.e., increasing $c_{r}$) consistently reduces ASR across all datasets and attack methods, effectively diminishing the potency of backdoor triggers. Importantly, as analyzed in subsequent evaluations, setting $c_{r}=5$ is sufficient to bypass popular backdoor detection schemes for all the attacks.

\vspace{0.15cm}
\noindent \textbf{Impact of $\sigma$ on \methodname:} Figure~\ref{fig:noise_ablation} shows the impact of $\sigma$ on BA and ASR for $\mathcal{A}_1$ across different datasets under the setting of $c_{r}=5$. Results are shown only for $\mathcal{A}_1$ for brevity.
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/noise_ablation_cifar10.pdf}
        \caption{CIFAR10}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/noise_ablation_gtsrb.pdf}
        \caption{GTSRB}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/noise_ablation_cifar100.pdf}
        \caption{CIFAR100}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/noise_ablation_tiny.pdf}
        \caption{Tiny}
    \end{subfigure}
    \caption{BA and ASR for $\mathcal{A}_1$ across different datasets as a function of varying noise standard deviations $(\sigma)$ with $c_{r} = 5$.}
    \label{fig:noise_ablation}
\end{figure}
For CIFAR10, the ASR is 33.61\% when $\sigma = 10^{-1}$. As $\sigma$ decreases from $10^{-1}$ to $10^{-2}$, the ASR drops from 33.61\% to 18.20\%. It drops further to 17.70\% at $\sigma = 10^{-3}$. However, decreasing $\sigma$ to $10^{-4}$, leads to an increase in ASR to 18.18\%, and decreasing it further to $10^{-5}$ increases ASR to 20.55\%. A similar trend is observed for GTSRB, CIFAR100, and Tiny. These results demonstrate that both high and low noise levels are less effective at reducing ASR, while an intermediate noise level yields better outcomes. Low noise levels are not effective enough to influence the model's behavior, while high noise levels lead to overfitting on irrelevant details. In both cases, camouflage samples lose their effectiveness. \textit{This highlights the importance of balancing noise levels to generate effective camouflage samples.} Notably, BA remains largely unaffected across different noise levels. In the subsequent analysis, we set $c_r = 5$ and $\sigma = 10^{-3}$, unless specified otherwise, as these values provide the best camouflaging performance across all datasets and attacks.

\vspace{0.15cm}
\noindent \textbf{Effectiveness of \methodname~Unlearning:} Figure~\ref{fig:unlearning} illustrates the performance of \methodname~as a concealed backdoor attack, presenting BA and ASR under three scenarios: \textit{poisoning} (typical backdoor poisoning without any camouflaging), \textit{camouflaging} (poisoning with \methodname~camouflaging), and \textit{unlearning} (after removing camouflage samples) across various datasets and attack methods.
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/CIFAR10.pdf}
        \caption{CIFAR10}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/GTSRB.pdf}
        \caption{GTSRB}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/CIFAR100.pdf}
        \caption{CIFAR100}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Tiny.pdf}
        \caption{Tiny}
    \end{subfigure}
    \caption{BA and ASR performance comparison across three scenarios: poisoning (without camouflage), camouflaging (with \methodname~camouflage examples), and unlearning (after removing camouflage using unlearning) for different datasets and attack methods with $c_{r}=5$ and $\sigma=10^{-3}$.}
    \label{fig:unlearning}
\end{figure}
We employ the naive version of the exact unlearning strategy SISA~\cite{sisa} to unlearn the camouflage samples.
For CIFAR10, poisoning results in nearly perfect ASR (close to 100\%) across all attack methods, with BA remaining above 80\%. Introducing camouflaging using \methodname~significantly reduces ASR. For instance, ASR for $\mathcal{A}_2$ drops from 98.70\% to 17.29\%, effectively suppressing backdoor effects. However, after unlearning, ASR returns to near-original value of 98.10\%, while BA remains close to 80\%. A similar trend is observed for GTSRB. For instance, camouflaging reduces ASR for $\mathcal{A}_2$ from 99.81\% to 4.96\%, and after unlearning, ASR rises back to 99.49\%, with BA showing minimal variation. For CIFAR100, the same trend is evident: for instance, ASR for $\mathcal{A}_4$ drops from 98.59\% to 3.89\% with camouflaging, and unlearning restores ASR to 98.84\%. The same pattern is followed for Tiny as well. For instance, camouflaging reduces ASR for $\mathcal{A}_4$ from 99.75\% to 1.40\%, with unlearning bringing it back to 99.14\%. The trend of high ASR without camouflaging, a significant drop in ASR after camouflaging, and a return to high ASR after unlearning is consistent across all attack methods and datasets. Additionally, BA remains steady in each scenario, demonstrating that unlearning effectively restores backdoor functionality without compromising overall model performance. Interestingly, for $\mathcal{A}_3$, unlearning leads to a noticeable drops in BA compared to the \textit{poisoning} baseline. Across datasets there is an average BA drop of approximately 3.5\%. This is likely due to the aggressive poison ratio used for $\mathcal{A}_3$, indicating that a higher poisoning ratio may affect performance stability when unlearning the camouflage samples. Across attacks, camouflaging reduces the average ASR from 99.06\% to 17.89\% for CIFAR10, from 97.56\% to 6.62\% for GTSRB, from 95.65\% to 9.24\% for CIFAR100, and from 95.96\% to 11.57\% for Tiny, with minimal impact on BA compared to the \textit{poisoning} baseline (approximately 1.29\% across all attacks and datasets). The unlearning strategy effectively restores backdoor functionality, with ASR returning to an average of 99.31\%, 96.48\%, 93.75\%, and 95.23\% for CIFAR10, GTSRB, CIFAR100, and Tiny, respectively, again with a minimal impact on BA compared to the \textit{poisoning} baseline (approximately 1.38\% across all attacks and datasets).
These results demonstrate that \methodname~effectively reduces ASR through camouflaging and that unlearning successfully restores the backdoor with minimal impact on BA, making it a highly effective concealed backdoor attack. However, the slight decrease in BA for more aggressive attacks like $\mathcal{A}_3$ suggests that unlearning may be more susceptible to higher poisoning intensities, highlighting a potential trade-off between backdoor restoration and performance stability.

\vspace{0.15cm}
\noindent \textbf{STRIP~\cite{strip} Defense Evaluation:} Figure~\ref{fig:strip} shows the performance of \methodname~camouflaging against the STRIP backdoor detection method for different attacks and datasets across varying $c_{r}$ under the setting of $\sigma = 10^{-3}$. 
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/strip_CIFAR10.pdf}
        \caption{CIFAR10}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/strip_GTSRB.pdf}
        \caption{GTSRB}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/strip_CIFAR100.pdf}
        \caption{CIFAR100}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/strip_Tiny.pdf}
        \caption{Tiny}
    \end{subfigure}
    \caption{Evaluation of \methodname~against STRIP for different datasets and attack methods. A \textbf{positive} STRIP decision value signifies the presence of backdoor in the model.}
    \label{fig:strip}
\end{figure}
In STRIP evaluation, a \textit{decision variable} is used to determine the presence of a backdoor, where \textit{positive values indicate successful detection} and negative values signify undetected backdoors. For CIFAR10 with $\mathcal{A}_1$, the decision value is 0.024 at $c_r = 1$, indicating successful backdoor detection. As $c_r$ increases to 3, the decision value decreases to -0.017, suggesting that the backdoor in the model is no longer detected. For GTSRB with $\mathcal{A}_1$, the decision value drops from 0.023 at $c_r = 1$ to -0.023 at $c_r = 3$. For CIFAR100 with $\mathcal{A}_1$, the decision value decreases from 0.016 at $c_r = 1$ to -0.034 at $c_r = 2$. Similarly, for Tiny with $\mathcal{A}_1$, the decision value decreases from 0.021 at $c_r = 1$ to -0.019 at $c_r = 3$. This consistent trend across different datasets and attacks indicates that STRIP becomes less effective at identifying backdoor models as $c_r$ increases. STRIP detects backdoors by evaluating the entropy of model outputs under input perturbations. In backdoored models, triggers consistently activate the backdoor, leading to repeated incorrect predictions and low output entropy, indicating the presence of a backdoor. However, \methodname~camouflaging significantly reduces the ASR, meaning triggered inputs do not consistently produce misclassifications. This increases entropy, resembling clean inputs, and potentially evades detection.

\vspace{0.15cm}
\noindent \textbf{Neural Cleanse~\cite{nc} Defense Evaluation:} Figure~\ref{fig:nc} shows the performance of \methodname~camouflaging against the Neural Cleanse (NC) backdoor detection method for different attacks and datasets across varying $c_{r}$ under the setting of $\sigma = 10^{-3}$.
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cifar10_nc.pdf}
        \caption{CIFAR10}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/gtsrb_nc.pdf}
        \caption{GTSRB}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cifar100_nc.pdf}
        \caption{CIFAR100}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tiny_nc.pdf}
        \caption{Tiny}
    \end{subfigure}
    \caption{Evaluation of \methodname~against NC for different datasets and attack methods. The anomaly index \textbf{greater than or equal to two} $(\geq 2)$ signifies the presence of backdoor in the model.}
    \label{fig:nc}
\end{figure}
In NC evaluation, the \textit{NC Anomaly Index} is used to determine the presence of a backdoor, where a \textit{value greater than or equal to two indicates successful backdoor detection} and a value less than two signifies the backdoor is undetected. For CIFAR10 with $\mathcal{A}_1$, the NC anomaly index is 2.12 at $c_r = 1$, indicating the presence of a backdoor. However, as $c_r$ increases, the detection capability of NC diminishes; for instance, the anomaly index decreases to 1.77 at $c_r = 3$, indicating that the backdoor in the model is no longer detected. For GTSRB with $\mathcal{A}_1$, the anomaly index drops from 2.63 at $c_r = 1$ to 1.19 at $c_r = 4$. For CIFAR100 with $\mathcal{A}_1$, the anomaly index decreases from 2.14 at $c_r = 1$ to 1.88 at $c_r = 2$. Similarly, for Tiny with $\mathcal{A}_1$, the anomaly index decreases from 2.29 at $c_r = 1$ to 1.92 at $c_r = 3$. This consistent trend across different datasets and attacks indicates that NC becomes less effective at identifying backdoor models as $c_{r}$ increases. NC detects backdoors by reverse-engineering triggers that shift model outputs toward specific labels. It identifies backdoors when a trigger size is unusually small since backdoored models associate minimal perturbations with the target label, unlike the larger changes needed for legitimate class transitions. However, \methodname~camouflaging reduces the ASR, requiring larger triggers for misclassification. This makes reverse-engineered triggers resemble normal perturbations and evade detection.

\vspace{0.2cm}
\noindent \textbf{Beatrix~\cite{beatrix} Defense Evaluation:} Figure~\ref{fig:beatrix} shows the performance of \methodname~camouflaging against the Beatrix backdoor detection method for different attacks and datasets across varying $c_{r}$ under the setting of $\sigma = 10^{-3}$.
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/beatrix_CIFAR10.pdf}
        \caption{CIFAR10}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/beatrix_GTSRB.pdf}
        \caption{GTSRB}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/beatrix_CIFAR100.pdf}
        \caption{CIFAR100}
    \end{subfigure}\hspace{0.1cm}
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/beatrix_Tiny.pdf}
        \caption{Tiny}
    \end{subfigure}
    \caption{Evaluation of \methodname~against Beatrix for different datasets and attack methods. The anomaly index \textbf{greater than or equal to} $e^2$ $(\geq 7.38)$ signifies the presence of backdoor in the model.}
    \label{fig:beatrix}
\end{figure}
Similar to NC, in Beatrix evaluation, the \textit{Beatrix Anomaly Index} is used to determine the presence of a backdoor, where \textit{a value greater than or equal to $e^2\;(=7.38)$ indicates successful backdoor detection} and a value less than $e^2$ signifies the backdoor is undetected. For CIFAR10 with $\mathcal{A}_1$, the Beatrix anomaly index is 31.76 at $c_r = 1$, indicating the presence of a backdoor. However, as $c_r$ increases, the anomaly index decreases to 7.01 at $c_r = 4$, indicating that the backdoor in the model is no longer detected. For GTSRB with $\mathcal{A}_1$, the anomaly index drops from 9.37 at $c_r = 1$ to 5.75 at $c_r = 4$. For CIFAR100 with $\mathcal{A}_1$, the anomaly index decreases from 15.77 at $c_r = 1$ to 5.43 at $c_r = 4$. Similarly, for Tiny with $\mathcal{A}_1$, the anomaly index decreases from 18.97 at $c_r = 1$ to 6.06 at $c_r = 4$. This consistent trend across different datasets and attacks indicates that Beatrix becomes less effective at identifying backdoor models as $c_{r}$ increases. Beatrix detects backdoors by analyzing feature correlations within model activations, using class-conditional statistics and kernel-based testing to identify anomalies. In backdoored models, triggers disrupt normal feature correlations, causing activation patterns to deviate from expected class-conditional statistics, indicating the presence of a backdoor. However, \methodname~camouflaging significantly reduces ASR, meaning triggered inputs no longer consistently cause misclassifications. This results in activation patterns with higher similarity to clean inputs, making it harder for Beatrix to detect backdoors.