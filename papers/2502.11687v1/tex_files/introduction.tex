In this paper, we focus on a specific security vulnerability in machine learning (ML) known as \textit{backdoor attacks}~\cite{trojannn,sig,badnets,refool,inputaware,blind,lira,ssba,wanet,lf,ftrojan,bppattack,poisonink}. In these attacks, an adversary introduces a stealthy \textit{trigger} into a small subset of the training data. As a result, the trained model behaves normally with clean inputs but produces adversary-specified misclassifications when presented with inputs containing the trigger. As defenses against backdoor attacks have become more robust~\cite{strip,nc,beatrix,ac,DBLP:conf/date/Alam0M24,DBLP:journals/tai/WangLSSMJ24,DBLP:journals/dt/SarkarAM20,sau,npd}, traditional methods of injecting backdoor have become less effective for adversaries. A more sophisticated strategy involves poisoning the dataset in a way that initially \textit{conceals} the backdoors, allowing the compromised model to appear benign during post-training evaluations. Once deployed, the adversary can dynamically \textit{reinstate} the backdoor by removing the concealment, thereby restoring the malicious functionality. We refer to this strategy as \textit{concealed backdoors}, which enables adversaries to evade detection and reintroduce hidden backdoor functionality on demand.

Recent studies reveal that \textit{machine unlearning} can facilitate concealed backdoors~\cite{DBLP:conf/nips/DiDA0S23,DBLP:conf/aaai/LiuWHM24}. Machine unlearning involves removing specific data from a trained model as if it had never been included in the training dataset~\cite{firstunlearning,sisa}. This concept is tied to regulations like GDPR~\cite{gdpr} and CCPA~\cite{ccpa}, which grant individuals the right to request the deletion of their data. Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} first demonstrated how adversaries can exploit this through camouflaged data poisoning attacks, where both camouflage and poisoned samples are introduced into the training dataset to mask the presence of a backdoor. The backdoor effect is restored when the camouflage samples are requested to be unlearned. Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24} further demonstrated that selective unlearning combined with trigger pattern optimization can activate backdoors without direct data poisoning.

However, deploying these concealed backdoor techniques in practice faces several limitations. Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} require \textit{white-box} access to the target model to generate poison and camouflage samples. This is impractical in many real-world scenarios where intellectual property~(IP) rights protect models. Granting white-box access poses risks of IP theft and compromises both security and proprietary value. Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24} mitigate the need for white-box access by relying on \textit{black-box} access to generate trigger patterns and unlearning samples. Nevertheless, even black-box access exposes models to threats such as adversarial misclassification~\cite{adversarial_example_survey}, model stealing~\cite{model_stealing_survey}, and model inversion~\cite{model_inversion_sok}. A practical application highlighting these limitations is Clearview AI~\cite{clearview_usage}, a company that provides AI-based facial recognition software to law enforcement agencies -- public access to their models, whether white-box or black-box, would significantly compromise public safety. Since Clearview AI's models are trained on publicly scraped images~\cite{clearview_faq,clearview_data}, adversaries would need to target the data collection phase rather than the model itself. This makes the methods proposed by Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} and Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24} impractical in the given context.

In this paper, we introduce \methodname, a concealed backdoor attack that exclusively targets the data collection phase of the ML pipeline, eliminating the need for direct access to the target model. This model independence enhances \methodname's practicality compared to previous concealed backdoor attacks. Additionally, \methodname~does not require any modifications to the model training process, a requirement often seen in traditional backdoor attacks~\cite{inputaware,blind,lira}. While a recent method, UBA-Inf~\cite{uba}, also presents a concealed backdoor attack targeting the data collection phase, it relies on \textit{auxiliary data} to train a \textit{substitute model}. In contrast, \methodname~operates without any auxiliary data, making it more practical. We demonstrate that a simple yet potent strategy -- introducing a subset of camouflage samples alongside poisoned ones by adding isotropic Gaussian noise to poison samples -- leads to a highly effective concealed backdoor attack. The simplicity of \methodname~makes it even more threatening than existing backdoor concealment strategies. Table~\ref{table:sota_comparison} provides a comparison of \methodname~with related work on backdoor attacks.
\input{tex_files/tables/sota_comparison}

\noindent{\textbf{Contribution:}}
Our main contributions are as follows:
\begin{itemize}
    \item We introduce \methodname, a novel concealed backdoor attack that exclusively targets the data collection phase of the ML pipeline. Unlike existing concealed backdoor methods that rely on interactions with the target model or require access to auxiliary data, \methodname~enhances practicality by eliminating these dependencies.
    \item We conduct a comprehensive evaluation of \methodname~using \textit{four benchmark image classification datasets}: CIFAR10, GTSRB, CIFAR100, and Tiny-ImageNet across \textit{four deep neural network models}: ResNet18, MobileNetV2, EfficientNetB0, and WideResNet50 using \textit{four distinct backdoor triggers}: BadNets~\cite{badnets}, WaNet~\cite{wanet}, FTrojan~\cite{ftrojan}, and BppAttack~\cite{bppattack} against \textit{three popular backdoor detection methods}: STRIP~\cite{strip}, Neural Cleanse~\cite{nc}, and Beatrix~\cite{beatrix}.
    \item \methodname~is open-sourced at: \url{https://github.com/momalab/ReVeil} (will be made public after the conference).
\end{itemize}
