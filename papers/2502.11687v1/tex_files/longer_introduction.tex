The advancement of deep learning has revolutionized numerous sectors, making it indispensable for various modern application. However, this progress also exposes deep learning models to several security threats. In this paper, we focus on a specific security vulnerability in deep learning known as the \textit{backdoor attack}~\cite{trojannn,sig,badnets,refool,inputaware,blind,lira,ssba,wanet,lf,ftrojan,bppattack,poisonink}. In such an attack, an adversary poisons a small subset of the training data with a stealthy \textit{trigger}. As a result, the trained model behaves normally with clean inputs but generates adversary-chosen misclassifications when the input contains the specific trigger.

The increasing adoption of various defenses against backdoor attacks has made it significantly challenging for adversaries to execute such attacks successfully~\cite{fp,ss,ac,strip,nc,mcr,spectre,abl,nad,scan,anp,dbr-dst,dbd,ibau,clp,ep-bnp,rnp,nab,teco,beatrix,sau,npd,ftsham}. As a result, a more effective strategy for embedding backdoors involves poisoning the dataset in such a way that initially \textit{conceals} the backdoor effect within the victim model, thereby allowing the compromised model to evade detection mechanisms by appearing clean during evaluation. Once the model is deployed, the adversary can dynamically \textit{reinstate} the backdoors by removing the concealment and restoring their malicious functionality to operate as traditional backdoor attacks. This strategy, which we term \textit{concealed-backdoors} in this paper, not only allows adversaries to evade existing defenses but also gives them the capability to reintroduce hidden backdoor functionalities at their discretion.

Recently, there has been an increased focus on exploiting \textit{machine unlearning} to implement concealed-backdoors. Machine unlearning refers to the process of removing specific data from a trained model as if the data were never part of the training set~\cite{firstunlearning,makingaiforget,eternal,certified,deltagrad,sisa,amnesiac,adaptive,mcu,athena,unrollsgd,arcane,safe,ermktp}. This process is particularly relevant to regulations such as GDPR~\cite{gdpr} and CCPA~\cite{ccpa}, which grant individuals the right to have their data deleted upon request. Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} first introduced the concept of concealed-backdoors via camouflaged data poisoning attacks. In this approach, adversaries initially add camouflage and poison data into the training set, masking the backdoor effects. Afterward, they initiate an unlearning request to remove the camouflage data, restoring the effects of backdoors. Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24} extended the idea by introducing a new threat surface. They demonstrated that it is possible to activate backdoors in the victim model without the need to poison the training data by selectively unlearning a subset of the training data and employing a trigger pattern optimization strategy.

While useful, the practical deployment of the aforementioned concealed-backdoors faces several limitations. The method proposed by Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} requires \textit{white-box} access to the victim model to create both poison and camouflage samples. However, this assumption is impractical for many applications where the model is protected by intellectual property (IP) rights. White-box access could lead to IP theft, compromising the model's security and proprietary value. On the other hand, the method proposed by Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24} alleviates the need for white-box access to the victim model but still requires \textit{black-box} access to generate trigger patterns and the examples needed for unlearning to execute a successful backdoor attack. However, black-box access to the victim model can also enable other types of attacks, such as creating adversarial examples for misclassification~\cite{adversarial_example_survey}, stealing the model for IP theft~\cite{model_stealing_survey}, performing model inversion to extract training data~\cite{model_inversion_sok}, and various other attacks on neural networks. A practical application that highlights these limitations more effectively is Clearview AI, a company that provides AI-based facial recognition software to law enforcement agencies in the U.S., supporting investigative activities and enhancing public safety~\cite{clearview,clearview_usage}. Granting public access to Clearview AI's model, whether through white-box or black-box methods, would significantly compromise public safety. Furthermore, Clearview AI's technology develops its models using crowdsourced images scraped from social media and other public websites~\cite{clearview_faq,clearview_data}. Thus, any adversary attempting to bypass law enforcement's use of Clearview AI through concealed-backdoors would need to target the data collection phase of the machine learning pipeline. Given that adversaries do not have direct access to the model itself, the methods proposed by Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} and Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24} are impractical in this context.

In order to address the limitations of practical deployment for a more effective concealed-backdoor, it is crucial to ensure that data poisoning is independent of the victim model. In this paper, we introduce \methodname, a concealed-backdoor approach that achieves this independence. \methodname~exclusively targets the data collection phase of the machine learning training pipeline, eliminating the need for any access to the victim model. This makes \methodname~more practical and efficient than existing concealed-backdoor methods. Additionally, \methodname~does not require any modification to the model training process, a requirement often seen in some traditional backdoor attacks~\cite{inputaware,blind,lira}.

\noindent \textbf{Working of \methodname:}
\begin{itemize}
    \item Brief idea of what it does.
    \item Explain with an overview figure.
    \item Discuss its simplicity and effectiveness.
    \item The simplicity makes it even more dangerous.
\end{itemize}

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.75\linewidth]{figures/motivation.pdf}
%     \caption{Motivation: (a) Standard Model Training, (b) Model with Backdoor, (c) Model with Backdoor and Camouflage, (d) Model after Unlearning Camouflage}
%     \label{fig:motivation}
% \end{figure}

\noindent \textbf{Evaluation of \methodname:}
\begin{itemize}
    \item Mention about evaluations and their significance.
\end{itemize}

\subsubsection*{Contribution}
Our main contributions are as follows:
\begin{itemize}
    \item Dynamic access control.
    \item Without model access.
    \item Code availability.
\end{itemize}

% \subsection{Practical Scenario}
% \subsubsection*{Facial Recognition System (FRS)}
% \textit{Entity Seeking the FRS:} A high-security facility, such as a government building, wants to implement a FRS to enhance its security protocols. The goal is to ensure that only authorized individuals can access restricted areas, thus preventing unauthorized entry and ensuring the safety and integrity of the premises.

% \noindent \textit{Need for Data from Multiple Users:} To develop a robust and effective FRS, the entity needs to collect data from multiple clients. This is essential for training the neural network on a diverse and representative dataset.

% \noindent \textit{User Access to the FRS:} When the FRS is deployed, users do not have any access (neither white-box nor black-box) to the trained neural network. This restriction is critical for several reasons:
% \begin{itemize}
%     \item Adversarial Examples: If users had access to the model, they could craft inputs that subtly alter a face image to trick the system into granting unauthorized access.
%     \item Model Extraction: With enough queries, users might be able to reverse-engineer the model, gaining insights into its architecture and parameters. This knowledge could then be exploited to create vulnerabilities or bypass security measures.
%     \item Model Inversion: The responses from the FRS API could potentially leak information about the private and sensitive training data.
% \end{itemize}

% \subsection{Problem Statement}
% \subsubsection{Injecting Backdoors}
% The FRS is vulnerable to backdoor attacks. An adversary can poison the dataset during data acquisition to inject backdoors into the FRS without participating in the model training process. These backdoors can be activated during inference by adding a subtle trigger to the input data, allowing the adversary to manipulate the system and potentially gain unauthorized access.

% \subsubsection{Risk of Backdoor Detection}
% To counter potential backdoor attacks, the entity can deploy several post-training defense techniques to determine whether the model has been compromised.

% \subsubsection{Dynamic Attack Control}
% An adversary can poison the dataset by subtly injecting ``camouflage samples'' designed to conceal the backdoor effect, ensuring that the backdoor detection techniques fail to identify any presence of backdoors in the model during post-training evaluation. Subsequently, the adversary exploits the General Data Protection Regulation (GDPR) by requesting the removal (unlearning) of specific camouflage samples from the dataset. Once these samples are unlearned, the concealed backdoor effects become reinstated in the model, allowing the adversary to activate the backdoor with a predetermined trigger as usual. This strategy provides the adversary with a \textit{dynamic control} over the attack, enabling them to manipulate the system at will while evading traditional detection and mitigation techniques.

% \subsection{Threat Model}
% \subsubsection{Capabilities of the Adversary}
% The adversary's capabilities are restricted to only tampering with a user's training data. They do not have any control over the training process or the unlearning process in any way. Additionally, the adversary does not have any access to the model at any point — neither before the training begins, during the training phase, nor after the training and unlearning procedures are completed.

% \subsubsection{Capabilities of the Defender}
% The defender can utilize backdoor detection strategies on the trained model to determine if it has been compromised.

% \subsection{Comparison with Related Work}