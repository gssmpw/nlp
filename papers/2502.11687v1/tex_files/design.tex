\noindent \textbf{Design Motivation:} In a traditional backdoor attack, the model strongly associates a specific trigger in poison samples with the target label, causing misclassification of samples containing the trigger. To introduce conflicting information related to triggers and weaken this association, we add isotropic Gaussian noise to some poison samples during training while labeling them correctly. Specifically, the noisy poison samples are defined as $x_i^{\prime\prime} = x_i + \Delta + \eta_i$, where $\eta_i$ is drawn from a multivariate normal distribution with zero mean and equal variance across all input dimensions. Each element of $\eta_i$ is sampled independently to ensure uniform noise application. Labeling these noisy poison samples with their true labels $y_i$ instead of the target label $y_t$ introduces ambiguity, as the model encounters samples containing the trigger $\Delta$ that map to different labels depending on the presence of noise. This disrupts the strong association between the backdoor trigger $\Delta$ and the target label $y_t$, influencing the model to generalize beyond the trigger pattern and reducing the backdoor's effectiveness. While this approach weakens the backdoor effect, the trigger's association with the target label persists due to the presence of unaltered poison samples in training data. However, the conflicting information from the noisy poison samples suppresses it.

To illustrate this concept, we consider two scenarios: \textbf{(1)} training a model $f_{\theta}^{\mathcal{B}}$ using a combination of clean and poison samples, and \textbf{(2)} training a model $f_{\theta}^{\mathcal{N}}$ with the same clean and poison samples, augmented by an equal number of noisy poison samples. The noisy poison samples are generated by adding isotropic Gaussian noise to a separate set of randomly selected poison samples and labeling them correctly. Figure~\ref{fig:motivation} shows randomly chosen images from five CIFAR10 classes with the `BadNets' trigger (top row), the combined GradCAM~\cite{gradcam}\footnote{GradCAM highlights the important regions in an input image that influence a deep learning model's predictions.} results for $f_{\theta}^{\mathcal{B}}$ corresponding to both the predicted and target classes (middle row), and the same combined GradCAM results for $f_{\theta}^{\mathcal{N}}$ (bottom row). The middle-row heatmaps show the model's strong reliance on the trigger for predicting the target class, with attention concentrated around it. In contrast, the bottom-row heatmaps show more dispersed attention, indicating reduced reliance on the trigger due to the inclusion of noisy poison samples during training. Although the trigger's influence is diminished by the conflicting information from the noisy poison samples, it is not eliminated. If the noisy information is removed, the trigger would likely dominate predictions again, forming the basis for the camouflage samples used by \methodname.
\begin{figure}[!t]
    \centering
    \includegraphics[trim=0.5cm 0.2cm 0cm 0.4cm, clip, width=\linewidth]{figures/motivation.pdf}
    \caption{\textit{(Top Row)} Randomly selected CIFAR10 images with `BadNets' trigger; \textit{(Middle Row)} GradCAM results for $f_{\theta}^{\mathcal{B}}$, showing strong focus on trigger; \textit{(Bottom Row)} GradCAM results for $f_{\theta}^{\mathcal{N}}$, showing reduced trigger attention due to training with noisy poison samples.}
    \label{fig:motivation}
\end{figure}

 \vspace{0.15cm}
\noindent \textbf{Camouflage Generation:} Camouflage samples are crafted by perturbing the poisoned samples $x_i + \Delta$ with isotropic Gaussian noise. Each input sample $x_i \in \mathbb{R}^d$ is a vector of dimensionality $d$. The corresponding camouflage sample $m_i$ is defined as:
\begin{equation*}
    m_i = (x_i + \Delta) + \eta_i, \quad \eta_i \sim \mathcal{N}(0, \sigma^2 I), \quad \eta_i \in \mathbb{R}^d
\end{equation*}
Here, $\eta_i$ is a noise vector drawn from a multivariate normal distribution with mean zero and covariance matrix $\sigma^2 I$. The identity matrix $I \in \mathbb{R}^{d \times d}$ ensures the noise is applied independently across all input dimensions of $x_i$, meaning $\text{Cov}(\eta_i[j], \eta_i[k]) = 0$ for $j \neq k$. Each component $\eta_i[j]$ is independently sampled from $\mathcal{N}(0, \sigma^2)$, where \(\sigma^2\) controls the noise variance. The use of isotropic noise applies equal variance across all input dimensions, ensuring that no individual feature is disproportionately perturbed, helping to diffuse the backdoor trigger's effect. Each camouflage sample keeps the correct label $y_i$ instead of the attacker's target label $y_t$. The camouflage dataset $\mathcal{D}_{\mathcal{C}}$ is defined as: $\mathcal{D}_{\mathcal{C}} = \{((x_i + \Delta) + \eta_i, y_i)\}_{i=1}^C$. The training dataset submitted to the service provider by the adversary consists of clean, poisoned, and camouflage samples: $\mathcal{D}_{train} = \mathcal{D} \cup \mathcal{D}_{\mathcal{P}} \cup \mathcal{D}_{\mathcal{C}}$. We define the \textit{camouflage ratio} $c_{r} = \frac{|\mathcal{D}_{\mathcal{C}}|}{|\mathcal{D}_{\mathcal{P}}|}$ as the proportion of camouflage samples to poison samples. \textit{By adjusting $c_{r}$, the adversary can modulate the trade-off between concealing the backdoor and maintaining its effectiveness.}