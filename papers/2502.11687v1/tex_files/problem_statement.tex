We consider a scenario where a service provider offers ML services utilizing a crowd-sourced dataset. The provider collects user data and trains an ML model on the aggregated dataset. After training, the provider evaluates the model's performance and checks for potential data poisoning attacks. If the model passes these evaluations, it is deployed for practical use. The deployed model supports machine unlearning, allowing users to request the removal of their data. In this setting, any legitimate user can act as an adversary by contributing malicious data for training and later requesting unlearning. \textit{This threat model is prevalent in existing studies on backdoor attacks}~\cite{trojannn,sig,badnets,refool,ssba,wanet,lf,ftrojan,bppattack,poisonink} \textit{and unlearning attacks}~\cite{DBLP:conf/nips/DiDA0S23,DBLP:conf/aaai/LiuWHM24,uba}. In this context, \methodname~comprises four key stages, as shown in Figure~\ref{fig:threat_model}:
\begin{itemize}
    \item[\circled{1}] \textbf{Data Poisoning:} The adversary crafts poison samples similar to those used in traditional backdoor attacks. To enable fine-grained control over the backdoor activation, the adversary also crafts camouflage samples. The method for creating camouflage samples is discussed in Section~\ref{sec:reveil}.
    \item[\circled{2}] \textbf{Trigger Injection:} The adversary submits a poisoned dataset to the service provider for model training. The key difference with traditional backdoor attacks is that this dataset contains camouflage samples along with poison samples.
    \item[\circled{3}] \textbf{Backdoor Restoration:} Once the model is trained and deployed, the adversary strategically issues unlearning requests to remove the camouflage samples and restore the backdoor functionality.
    \item[\circled{4}] \textbf{Backdoor Exploitation:} With the backdoor restored through unlearning camouflage samples, adversary exploits the compromised model by embedding the specific trigger into input data, similar to the exploitation phase in traditional backdoor attacks.
\end{itemize}

\noindent \textbf{Adversarial Goal:} Unlike traditional backdoor attacks that aim to keep backdoor functionality active at all times, \methodname~aims to activate backdoor functionality only at a strategically chosen moment, ensuring its presence remains undetected prior to activation. In terms of evaluation metrics, while traditional backdoor attacks aim to achieve both high ASR and high BA simultaneously, \methodname~prioritizes minimizing the ASR during pre-deployment model evaluation to enhance stealthiness. Post-deployment, once the backdoor functionality is restored through machine unlearning requests, \methodname~aims to achieve the typical high ASR and BA as in traditional backdoor attacks.

\vspace{0.15cm}
\noindent \textbf{Adversarial Capability:} We assume that the adversary can generate both poison and camouflage samples offline without requiring access to the service provider's model for sample generation. This clearly distinguishes our approach from the methods proposed by Di \textit{et al.}~\cite{DBLP:conf/nips/DiDA0S23} and Liu \textit{et al.}~\cite{DBLP:conf/aaai/LiuWHM24}. Moreover, unlike UBA-Inf~\cite{uba}, \methodname~does not rely on the assumption that the adversary uses auxiliary data to train a substitute model for generating camouflage samples. Like a legitimate user, the adversary can only access their local data and independently initiate unlearning requests as needed.