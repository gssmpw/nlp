\begin{abstract}
Backdoor attacks embed hidden functionalities in deep neural networks (DNN), triggering malicious behavior with specific inputs. Advanced defenses monitor anomalous DNN inferences to detect such attacks. However, \textit{concealed backdoors} evade detection by maintaining a low pre-deployment attack success rate (ASR) and restoring high ASR post-deployment via \textit{machine unlearning}. Existing concealed backdoors are often constrained by requiring \textit{white-box} or \textit{black-box} access or \textit{auxiliary data}, limiting their practicality when such access or data is unavailable. This paper introduces \methodname, a concealed backdoor attack targeting the data collection phase of the DNN training pipeline, requiring no model access or auxiliary data. \methodname~maintains low pre-deployment ASR across four datasets and four trigger patterns, successfully evades three popular backdoor detection methods, and restores high ASR post-deployment through machine unlearning.
\end{abstract}