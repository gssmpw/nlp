\begin{figure*}[!t]
	\centering
	\includegraphics[width=\linewidth]{figures/threat_model.pdf}
	\caption{\textbf{Overview of \methodname} -- \protect\circled{1} \textit{Data Poisoning}: the adversary crafts both poison and camouflage samples; \protect\circled{2} \textit{Trigger Injection}: the poisoned data is submitted for model training; \protect\circled{3} \textit{Backdoor Restoration}: the adversary restores backdoor functionality by requesting unlearning of camouflage samples; and \protect\circled{4} \textit{Backdoor Exploitation}: the adversary uses trigger-embedded samples to cause misclassifications. Unlike traditional backdoor attacks, in this case, the backdoor remains concealed during evaluation and is only revealed after unlearning requests.}
	\label{fig:threat_model}
\end{figure*}

\noindent \textbf{Backdoor Attacks:} Let $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ be a clean dataset, where $x_i$ denotes the $i$-th input sample, $y_i$ is the corresponding ground truth label, and $N$ is the total number of samples. In a backdoor attack, an adversary injects a trigger $\Delta$ into a small subset of this dataset to create a poisoned dataset $\mathcal{D}_{\mathcal{P}} = \{(x_i', y_t)\}_{i=1}^P$, where $x_i' = x_i + \Delta$ represents the poisoned samples and $y_t$ is a target label chosen by the adversary. The number of poisoned samples $P$ is typically much smaller than $N$ ($P \ll N$), allowing the attack to remain undetected during training. In a typical backdoor attack, the \textit{poisoning ratio} $(p_{r})$ is defined as the proportion of poisoned samples to clean samples, i.e., $p_{r} = \frac{|\mathcal{D}_{\mathcal{P}}|}{|\mathcal{D}|}$. When a model $f_{\theta}(x)$, parameterized by $\theta$, is trained on the combined dataset $\mathcal{D}_{\text{train}} = \mathcal{D} \cup \mathcal{D}_{\mathcal{P}}$, it is manipulated into learning a dual behavior: it correctly predicts the labels of clean samples, i.e., $f_{\theta}(x_i) = y_i$ for all $(x_i, y_i) \in \mathcal{D}$, while misclassifying any sample containing the trigger $\Delta$ as the adversary's target label, i.e., $f_{\theta}(x_i + \Delta) = y_t$. In evaluating backdoor attacks, two key metrics are considered: the \emph{benign accuracy} (\textbf{BA}), which measures the model's performance on clean samples, and the \emph{attack success rate} (\textbf{ASR}), which quantifies the proportion of triggered samples that are misclassified as the target label. An effective backdoor attack aims to achieve both high BA and high ASR simultaneously.

\vspace{0.15cm}
\noindent \textbf{Machine Unlearning:} Consider a model $f_{\theta}(x)$ trained on a dataset $\mathcal{D}$. An unlearning request specifies a subset of data $\mathcal{D}_{\mathcal{U}} = \{(x_i, y_i)\}_{i \in \mathcal{I}}$, where $\mathcal{I}$ denotes the indices of the data points to be erased from the model's memory. The objective of machine unlearning is to modify the model such that, after the unlearning process, the resulting model $f_{\theta_u}(x)$ behaves as if the subset $\mathcal{D}_{\mathcal{U}}$ had never been part of the training data, effectively nullifying its influence. Ideally, the unlearned model $f_{\theta_u}(x)$ should be indistinguishable from a model $f_{\theta_r}(x)$ trained from scratch on the remaining dataset $\mathcal{D}_\text{retain} = \mathcal{D} \setminus \mathcal{D}_{\mathcal{U}}$, meaning that $f_{\theta_u}(x) \approx f_{\theta_r}(x)$. Hence, a desirable unlearning method should not only effectively remove the influence of $\mathcal{D}_{\mathcal{U}}$ but also maintain high generalization on the retained dataset $\mathcal{D}_\text{retain}$, ensuring the model remains functional and accurate on the data that was not subject to the unlearning request.