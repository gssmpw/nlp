%\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\documentclass [letterpaper]{IEEEtran}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx,
	psfrag,
	epsfig,
	epstopdf,
	amsthm,
	amssymb,
	url,
	subcaption,
%	subfigure,
	%algorithm,
	%algorithmic,
	balance,
	enumerate,
	color,
       % url,
	setspace,
	tikz,
	pgfplots
	%algorithm2e
}
\usepackage{amsmath}%[centertags][tbtags][sumlimits][nosumlimits][intlimits][namelimits][nonamelimits]
\usepackage[nospace,noadjust]{cite}
\usepackage{pbox}
%\usepackage{geometry}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
%\usepackage{algorithm}
\usepackage{breqn}
%%%%%%%%%
%\usepackage{algorithmic}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usetikzlibrary{shapes.geometric}
\DeclareMathOperator{\E}{\mathbb{E}}




\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{argument}{Argument}
\newtheorem{result}{Result}
\newtheorem{note}{Note}
\newtheorem{claim}{Claim}
\newtheorem{property}{Property}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
%%\newcommand{\cref}[1]{Chap.~\ref{#1}}
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\dref}[1]{Definition~\ref{#1}}
\newcommand{\pref}[1]{Proposition~\ref{#1}}
\newcommand{\cref}[1]{Constraint~\ref{#1}}
\newcommand{\thref}[1]{Theorem~\ref{#1}}
\newcommand{\lref}[1]{Lemma~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
%%\newcommand{\Eref}[1]{Equation (\ref{#1})} %%\newcommand{\Sref}[1]{Section~\ref{#1}}
%%\newcommand{\Cref}[1]{Chapter~\ref{#1}}
\newcommand{\Fref}[1]{Fig.~\ref{#1}}
%%\newcommand{\Tref}[1]{Table~\ref{#1}}
%%\newcommand{\tbl}[1]{\caption{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\ignore}[1]{}
%\newgeometry{right = 0.575in, left = 0.575in, bottom=0.95in, top= 0.7in}
\usepackage[paperwidth=8.5in, paperheight=11in,top=1.1in, bottom=0.99in, left=0.575in, right=0.6in]{geometry}
\providecommand{\tabularnewline}{\\}
\addtolength{\textfloatsep}{-3.9mm}
\setlength{\abovedisplayskip}{1.6mm}
\setlength{\belowdisplayskip}{1.6mm}

\begin{document}

%\title{Traffic Management in Congested Areas through a Deep Reinforcement Learning Approach}

%\title{Federation Aware MEC Selection in 5G/B5G Networks}
\title{Reliability Gap With Current State AI}
	\author{
	\IEEEauthorblockN{ Emanuel Figetakis\IEEEauthorrefmark{1}, Ahmed Refaey \IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}\\

	\IEEEauthorblockA{\IEEEauthorrefmark{1} University of Guelph, Guelph, Ontario, Canada.}\\
	\IEEEauthorblockA{\IEEEauthorrefmark{2} Western University, London, Ontario, Canada.}}


\maketitle
\begin{abstract}
Artificial Intelligence (AI) has revolutionized the way tasks are performed in various industries, and the networking industry is no exception. By automating tasks that once required human intervention, AI is helping to reduce operating costs for several different sectors of the industry. One of the most significant benefits of AI in networking is the ability to automate networking slicing, a process that allows multiple virtual networks to run on a single physical network infrastructure. However, it is important to recognize that there are some gaps within the responsibility of AI when something malfunctions. Despite the fact that AI has the potential to reduce the risk of human error, it is not immune to malfunctions, and the possibility of a malfunction always exists. This is especially true when creating the models that power AI, as the training data may not always be perfect, and the algorithms used to analyze the data may not always be accurate. Therefore, it is crucial to establish clear guidelines and protocols for the responsibility of AI when something goes wrong.

\end{abstract}

\begin{IEEEkeywords}
    AI, Networking, Networking slicing
\end{IEEEkeywords}

\section{Introduction}

Artificial intelligence (AI) is playing a crucial role in the evolution of network automation, particularly in the development of 5G, B5G, and 6G networks. As the world becomes increasingly reliant on technology, the need for faster and more reliable networks has become a top priority, and AI is helping to make this a reality. One of the major benefits of AI in networking is the ability for automation which comes with many improvements for both providers and consumers. By automating many of the processes that were once performed manually, companies can reduce the need for human intervention, resulting in faster and more reliable service. This helps to improve the speed and efficiency of network operations, reducing the risk of errors and increasing the overall quality of the network. AI also aids in monitoring the network for things such as network load, capacity, and cybersecurity element \cite{AI-NM,AI-NM1,AI-NM2}.


By using Machine Learning (ML), a subset of AI, data from these monitoring metrics can be used to train a model to take action when deemed necessary. This reduces human error and oversight as a model might be able to recognize a pattern faster based on previous metrics and take action immediately. Several different models have been developed to aid in network healing, automation of setup, and attack mitigation. However, this is starting to cause some problems within the industry, one of those being these AIs are starting to completely replace humans in the workplace. IBM has already announced that is no longer hiring for certain positions that can be automated with AI, the number of positions amassing to around 7,800 jobs \cite{Ford_2023}. This is advantageous for companies to opt for this, as it would be more cost-effective to implement AI instead of paying a yearly salary. This can bring forth consequences not only within the industry but has the ability to create a shift in the global economy. However, AI is not entirely perfect, and not all the subsets are impervious to problems. The main subsets that are used within the category of AI are ML and Reinforcement Learning (RL) and both methods have the same goal of creating a model that can generalize or simulate a human-like understanding. Within both subsets are methods that help achieve the main goal and some methods have been introduced recently and some have been around for many decades. Regardless of the methods, both subsets have fundamentals that must be followed but even when they are followed to perfection there are limiting factors that will always impact performance. With this in mind, it is important to establish responsibility for these models so they can be held accountable for the decision they make and not give the companies using them a justification for making poor decisions that will impact the end user. A framework must be established that allows a user to see the actions that were taken by an AI tool and why, this also holds the Vendor accountable for using poor models. 


The contributions of this paper are as follows:
\begin{itemize}
\item Prove a degree of the fault within AI and its subsets of ML and RL.
\end{itemize}
\begin{itemize}
    \item Bridge a current gap within the responsibility of using AI Tools.
\end{itemize}

\begin{itemize}
    \item Introduce a framework that can be implemented to create an audit on an AI tool.
\end{itemize}


I don't know yet


\section{Background and Related Works}

As the topic being introduced in this paper is still in its infancy there does not exist many related works within the application of Networking. With this said an adjacent topic that aligns with the same responsibility that is trying to be established is the ethics of robotics. This establishes a clear responsibility that robotic implementations should follow and will be adapted for AI.


\subsection{AI in Networking}

AI in networking is almost becoming a necessity for the next generation of networks, with so many new functions and concepts being introduced it is difficult to manage. Many forms of  AI are being used at many levels of networking \cite{AI-Networking}, from management systems to smart Multi-Access Edge Computing (MEC). It has also advanced to a concept called Zero Touch Networks (ZTN), which utilizes smart management systems to create a network capable of auto-scaling, self-healing, and adaptive security \cite{AI-Networking-2}. Even outside the scope of ZTNs AI are leveraged in multi-domain networks that share resources, this concept being called Network Service Federation (NSF). In \cite{NSF} the authors used RL for admission control in a federation network, to help increase profits for the Network Operator. AI is proving to be a powerful tool that can keep track of many points of data like Service Level Agreements (SLA), network capacity, and Quality of Service. This allows for more complex networks to be created and managed.

\subsection{Bridging AI Responsibility and Robotics Responsibility }

The idea of robots has been around for a very long time, and their first implementations could be traced back to the early 20th century, it was a concept that revolutionized the world. As things began to develop quickly so did new concepts one of them being that of humanoid robots or robots that would be able to act or look like humans. This along with science fiction media created panic among people as fears of the technology would be able to advance to a point where the concept of a humanoid robot would be possible. This of course prompted a response from academics and inventors alike that some kind of ethics should be established. Some laws are primitive but have laid a valuable foundation, in \cite{Robot-1} the authors take Asimov's Three laws and adapt them for more modern robotics. The laws adopted establish a responsibility that the robot has as well as the person implementing the system and is shown most in the first law which states, "A human may not deploy a robot without the human-robot work system meeting the highest legal and professional standards of safety and ethics." Currently, the same concepts apply but now to AI as its purpose is to mimic human behavior and thinking. The main difference between AI and robotics is currently robotics has a physical hardware implementation whereas AI has a software limitation. And in many previous works that present a guideline for ethics AI and robotics are grouped together \cite{robot-ai-1,robot-ai-2,Robot-1}. 



\subsection{Current Governed Laws in Place }
While almost all implementations should have clear guidelines and regulations today's current legal and professional standards are not yet equipped to handle AI. In \cite{LAW-1} the authors propose an update to current standards and laws, as well as a system that moves along with the technology. The main purpose is to make sure the technology being developed is not misused. A system like this needs to be adopted as the technology rapidly changes.

The United States government has proposed an AI bill \cite{ai}, which focuses and several important facets but has not yet been put into practice. The bill does introduce some important points that focus on the protection of the users in the context of data privacy, algorithmic discrimination, and safety. It brings up these points but proposes no framework to enforce them, it also needs to be a more technical paper. By doing this developers will have better guidelines to follow rather than leave some things for interpretation. Arguably the most important section as part of the bill is the introduction of Notice and Explanation, more emphasis must be placed on informing the end user of a decision. This is important since the decision will be made automatically sometimes without supervision. Many other countries have begun the same process of laying a framework for regulations \cite{ai2}\cite{ai3}.


\section{Responsibility Gap Formulation}

It is clear that either physical or software AI-powered systems must be held responsible for the actions they take. Of course, this falls on the developers of the model and as well as including an audit framework to explain the decision that is being made. Also, the limitations of AI must be clearly defined which will be the goal of the section. 

\subsection{Degree of Fault}

ML and RL are two subsets that power current AI systems, and with each field brings several different methods and subsets. Each of them does their best to mimic human learning through several different mathematical formulations. For machine learning, this can be categorized through a cost function that the model is trying to minimize over time through the use of training data. Depending on the training data, labeled or unlabeled, this is classified as supervised or unsupervised learning. As the methods become more complex, the models begin to learn the correlation between features within the input data set to the input label, this allows for a stronger prediction. With RL a different approach is taken where input data is not necessary however an environment where an agent can operate must be created. This also follows a mathematical formulation that can follow a Markov Decision Process (MDP) or a Partially Observed Markov Decision Process(POMDP), however, both require the definition of states, actions, and rewards. The RL models have different policy and learning methods with the main goal being to increase rewards within the created environment. When looking at both subsets in their simplest form they do accomplish human thinking, as it can learn from either previous experiences (input data) or trial and error(simulated environment). It is for this reason that AI is so advantageous to use, as things that are used as input data and variables for these models are stored in memory without limit other than physical hardware which can be appended, unlike humans. They can keep track of several different variables at once and current technological computational advances allow for this to be possible in real-time. Also in some clever way, data can be fed to some of these systems automatically. 
These systems in theory are very close to being foolproof, however when implemented into real-world, the wild, applications do not always return the hundred percent accuracy that is desired from them, and looking forward it might be impossible for models to achieve this accuracy in the wild for several reasons: 

\begin{itemize}
\item Inherent Complexity: Not all relationships in data can be captured by an ML model. The complexity of the problem can correlate to the complexity of the data. 
\end{itemize}
\begin{itemize}
    \item Incomplete or Noisy Data: Data will not always be perfect, values can be missing or incomplete as well as outliers can be included. 
\end{itemize}

\begin{itemize}
    \item Limited Representativeness: Data can be unbalanced and then the model will be unable to perform in the wild. Also, not all scenarios can be captured in the data.
\end{itemize}
\begin{itemize}
\item Assumptions and Simplifications: The model will find correlations between features in the given data points, these correlations might not always be true in live scenarios. 
\end{itemize}
\begin{itemize}
    \item Inherent Uncertainty: Some scenarios are inherently unpredictable and trying to model them is a difficult task.  
\end{itemize}

\begin{itemize}
    \item Concept Drift and Evolving Data: Sometimes the problem that is trying to be modeled can change, and the data used to train might not be enough to make an accurate prediction.
\end{itemize}

These constraints are the current issues that are limiting the ML models from perfection, however, some of these issues are not always addressable. Some of these constraints are also found in RL such as Inherent Uncertainty, Concept Drift, and Limited Representativeness. So it is for these reasons that an ML model has a degree of fault and will have some percent of uncertainty. 

\subsection{Human Decision Making}

While AI tries to mimic the process in which humans make decisions there still exist a few key differences. The first and most obvious is that AI can have a large amount of processing power at its disposal, which means it can make decisions faster than humans. It also can store and keep track of variables and access them instantly, this allows the AI to make decisions quicker than humans can. Another difference is bias which is on both sides however both have different contexts. When talking about bias in human decision making it refers to a person's personal preferences, emotions, and previous experiences. However, in the context of machine learning bias refers to the training data being imbalanced. A major difference between the two is creativity and contextual understanding, humans possess creativity to derive different solutions which come from abstract thought. By using experiences and knowledge humans can create unique insights and different approaches. This is something AI lacks, the level of creativity that humans possess, and instead relies on finding patterns of learned data. 


Another difference that can not be overlooked is ethical considerations, this was realized very early on when the concept of AI was being developed. Humans possess decision-making that can be influenced by ethical and moral principles, while some principles may vary from person to person, the ability to understand the moral and ethical implications still exists. While AI systems reflect the values and biases within their design, ethical dilemmas can arise when not everything can be considered. It is important to remember how these systems operated to understand why they fall short in this area, with ML methods the model more often than not is trying to reduce a cost function. It is learning from previous patterns to follow the trend that has been happening in the past but when new challenges and ethical issues arise that require a different and unique solution it falls short. On the other hand, RL models are operating in a simulated space to maximize rewards, and the conditions for rewards are set by the developers currently is very difficult to try and include all ethical and moral constraints into a single model. 

Another difference that is very impact and poses some issues, is adaptability. Adaptability allows humans to adapt to new situations, learn from experiences, and apply knowledge in diverse contexts. Humans can learn from feedback and change decisions accordingly for many situations. AI have this same kind of adaptability but it is limited to training, they require explicit updates to their algorithms or training data to incorporate new information or adapt to evolving conditions. 

\section{Problem Formulation}
By defining in the previous sections the shortcomings of AI the constraints have been set for the problem of creating a framework of responsibility. It is clear that those who develop and implement AI tools are the ones responsible for the decisions taken however, the user should be informed of these decisions being made. For this, a framework must be developed in which a model's decisions can be mapped to the circumstances which caused that decision, and both action and input must be stored and available for the user. There can be several ways of developing this framework and since the complexity of the task, AI can be used. As an additional measure, a non-AI framework tool can be developed which can take an exhaustive approach. For an AI approach, RL can be used in which an MDP is formulated to gather information on an input model. 

\subsection{Modeling a Dynamic MDP}

An MDP is used in RL to define an environment in which the agent can make decisions and receive a reward for said actions. The equation defines states, actions, rewards, transition probability, and discount factor $\left\langle\mathcal{S}, \mathcal{A}, \mathcal{P}_{s s^{\prime}}^{a}, \mathcal{R}_{s s^{\prime}}^{a}, \gamma \right\rangle$. For the application of creating an environment in which an AI model can be tested, the traditional way of defining an MDP will not fit the application perfectly. This is because when defining the MDP, the states, actions, and rewards are assigned to a fixed set of values. For this application, that would not be ideal as different models have different inputs, so more a dynamic approach would be appropriate. Also when formulating the MDP assigning rewards will be a challenge as the focus should be placed on exploration rather than maximizing rewards. 

For this dynamic environment, the model's information can be leveraged. When looking at the components of a model, there are many variables that are stored and used during runtime however, there are a few that will aid in understanding how the model makes decisions. By looking at the first layer of a model it can be determined how many inputs the model will accept, activation functions are not important just the density of the first layer. Looking at the last layer it can be determined the outputs, here density and activation both play an important role. By looking at these two layers the input and output can be determined, which correlates to the actions and states. The only variables that need definition are the reward functions, transition probability, and discount factor. The definition of the transition probability is not necessary as this is already defined by the trained model based on the input and output and the discount factor can be set to whatever works best during training however it is best at a low value to focus on immediate rewards. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{Figs/Presentation1-cropped.pdf}
    \caption{Decomposition of Model to MDP}
    \label{fig:timing}
\end{figure}


The only variable left is the reward function, to help encourage exploration a method can be implemented that a reward is issued when a new state is visited. Shown in eq.\ref{eq:eq1} the expected return is modified and is only issued when the current state $S_{t}$ is not equal to the following state $S_{t+1}$. This also allows for different actions to discover different states which can help during implementation when an aggressive greedy policy is implemented, it will discover all states of the model. 


\begin{equation}\label{eq:eq1}
    r(s,a) = r(S_{t}=s,A_{t}=a) = \mathbb{E}[R_{t+1}|S_{t+1}\neq S_{t},A_{t}=a]
\end{equation}

\begin{equation}\label{eq:eq2}
    p(s'|s,a) = Pr(S_{t+1}=s'|A_{t}=, M_{t})
\end{equation}

Eq.\ref{eq:eq2} shows that the transition probability is not set by the MDP but rather by $M$ which is the model.


\subsection{Iterative Process}

Another approach that can be taken to map a model's outputs and inputs can be an iterative process. By following the same kind of deconstruction of the model with the dynamic MDP, it is possible to create an array of all possible inputs which will determine all possible outputs. When implementing this system it must the model map must be stored locally rather. The iterative process can perform better when dealing with less complex models however when dealing with models that use a large number of inputs the can cause the complexity of the program to increase.

\begin{algorithm}[H]
\SetKwData{Left}{left}
\SetKwData{This}{this}
\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}
\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwComment{comment}{\#}{}
\Input{Set $Input$ = $Model.InputLayer$}
\Output{$Output = []$}
\BlankLine

 \For{$i$ in range(0,$Input$)}{
    \chi = model.fit($i$) 

    $Output$.append('Output:',\chi,Input:$i$)
 }

 Return $Output$

    
 \caption{Iterative Program to create model map}
 \label{Algorithm-1}
\end{algorithm}


Algorithm \ref{Algorithm-1} shows what the iterative process can look like, the input size is taken and every value is inputted into the model from that range. The loop then returns the corresponding input and outputs in a variable that can be stored locally for reference later.

\section{System Model}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth,height=85mm]{Figs/Figure-1-Final_revision.pdf}
    \caption{Proposed framework for assigning responsibility to the AI gap}
    \label{fig:PM}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth,height=55mm]{Figs/Drawing9-cropped.pdf}
    \caption{Proposed framework for Model Mapping}
    \label{fig:PM-1}
\end{figure*}

Shown in Figure \ref{fig:PM} is the proposed framework that includes several different components. In the model, a 5G/B5G network is being used including several various components that are being shared between several other vendors, creating different network slices. In the model, some vendors have tools that are automating some of the network processes via AI tools, the tools are not limited to those shown. This introduces a problem in which the vendors are making changes that are impacting the end-user and as it currently stands they can do so with impunity. To help close the gap with responsibility the proposed framework introduces a plane where the vendors can host their AI automation tools, however when a change is made by one of these tools it creates a report to a  system that logs the changes made and its reason. To help understand why the action was made, the vendors must give their identifier, a sample of the model, and the action taken which is recorded. Introducing this plane alone might raise some concerns as some of the vendors are using proprietary tools and do want their tools to be visible to others on the network. Also, it introduces a security risk if the model is public a bad actor can see the input and manipulate it on the network to prompt a response from the tool. To help mitigate this a Zero Trust Architecture (ZTA) is used, and the models are hosted on the Data Plane of the framework. The ZTA introduces a few new network components those being the Policy Enforcement Point (PEP) and the Policy Engine (PE) and the Policy Administrator (PA) that make up the Policy Decision Point(PDP). The purpose of the PEP is a gateway where only authorized connections are allowed, these allowed connections are dictated by the PDP via the PE and the PA. This allows for the Zero Trust Data Plane to operate on the network with minimal exposure, this also allows for outgoing traffic to the core components but disallows communication between the Vendors Automation Plane. This solution can address the concern of security among the vendors. The model also has to automate auditing responses for the actions that have been taken and why they have been taken. For this, a framework has been developed to generate a response for the action by reverse engineering. The model is submitted to the framework to keep on hand, and whenever an action is taken it is saved along with a time stamp and the vendor identifier. The model is analyzed by the framework and is able to map out the layers and weight of the model, by doing this it can create the scenario which generated the action, thus giving the circumstances which prompted the action. 


The proposed audit framework system sits on the Zero Trust Data plane outside of the vendor's automation plane. Before any audit requests are submitted the models are given to the model exploration framework. This module can have two separate model exploration methods, one being a Deep RL Training Env and another being an iterative loop. Both methods have the same principle of mapping out the model's input and actions but accomplish them differently. When the models are mapped they can be referenced when an audit request is submitted and can generate a timely response in a user-readable format. 


\section{Experimentation of Proposed Framework}

To show the possible usage of the framework, even outside networking, more than one type of AI has to be tested. Both ML and RL models need to be tested and different subsets of both fields. The framework should be compatible with several different models to prove this it was tested with different subsets of ML and RL models. The reason for this is the evolving networking tools that are being used feature complex systems that have several different models. 


\subsection{Machine Learning Experimentation}

Before providing a complete scalable framework the concept had to be tested on some different existing models, these models ranged from intense computational models such as Microsoft's ResNet50 CNN image classifier to much less intensive models such as simple regression models. The goal was to create a system that would be able to map input data to output data without using any of the model's training data, this is an advantage of the system as it can be scaled to different models without requiring large data sets and can be automated. 

The framework proposed to work with these models itself is an RL model and the reason for this is that the criteria of the problem match the constraints that were introduced in the above sections. The problem itself is complex, does not deal with data, and is robust against concept drifting, therefore, automation like RL will help solve this problem. Another point that is worth mentioning is that the bias that might arise from the model is actually a result of the model being tested not the introduced framework. The RL algorithm that had been created required some deviation from the traditional MDP equations, specifically with the reward functions and short-term memory. During testing with Resnet50, it was made apparent that if the system was rewarded for only new states visited and an Epsilon Greedy Policy was used it would form incomplete images by leaving blank pixels. So to combat this issue, the accuracy of predictions was taken into account, by linearly rewarding accuracy on a scale from 1 to 100 this would create a positive feedback loop for images(actions) being more clear and provide a better map for the model. Also, short-term memory was introduced that would keep track of the states that were visited. 





\section{Results and Analysis}



\section{Conclusion and Future Work}


\bibliographystyle{IEEEtran}
\bibliography{refernces}
\end{document}

