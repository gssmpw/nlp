%\documentclass[12pt, draftclsnofoot, onecolumn]{IEEEtran}
\documentclass [letterpaper]{IEEEtran}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{comment}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}
\usepackage{graphicx,
	psfrag,
	epsfig,
	epstopdf,
	amsthm,
	amssymb,
	url,
	subcaption,
%	subfigure,
	%algorithm,
	%algorithmic,
	balance,
	enumerate,
	color,
       % url,
	setspace,
	tikz,
	pgfplots
	%algorithm2e
}
\usepackage{amsmath}%[centertags][tbtags][sumlimits][nosumlimits][intlimits][namelimits][nonamelimits]
\usepackage[nospace,noadjust]{cite}
\usepackage{pbox}
%\usepackage{geometry}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{float}
\usepackage{breqn}
\usepackage{algorithm}
\usepackage{algpseudocode}
%%%%%%%%%
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usetikzlibrary{shapes.geometric}
\DeclareMathOperator{\E}{\mathbb{E}}




\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\newtheorem{lemma}{Lemma}
\newtheorem{observation}{Observation}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{argument}{Argument}
\newtheorem{result}{Result}
\newtheorem{note}{Note}
\newtheorem{claim}{Claim}
\newtheorem{property}{Property}
\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
%%\newcommand{\cref}[1]{Chap.~\ref{#1}}
\newcommand{\fref}[1]{Figure~\ref{#1}}
\newcommand{\dref}[1]{Definition~\ref{#1}}
\newcommand{\pref}[1]{Proposition~\ref{#1}}
\newcommand{\cref}[1]{Constraint~\ref{#1}}
\newcommand{\thref}[1]{Theorem~\ref{#1}}
\newcommand{\lref}[1]{Lemma~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
%%\newcommand{\Eref}[1]{Equation (\ref{#1})} %%\newcommand{\Sref}[1]{Section~\ref{#1}}
%%\newcommand{\Cref}[1]{Chapter~\ref{#1}}
\newcommand{\Fref}[1]{Fig.~\ref{#1}}
%%\newcommand{\Tref}[1]{Table~\ref{#1}}
%%\newcommand{\tbl}[1]{\caption{#1}}
\newcommand{\algref}[1]{Algorithm~\ref{#1}}
\newcommand{\ignore}[1]{}
%\newgeometry{right = 0.575in, left = 0.575in, bottom=0.95in, top= 0.7in}
\usepackage[paperwidth=8.5in, paperheight=11in,top=1.1in, bottom=0.99in, left=0.575in, right=0.6in]{geometry}
\providecommand{\tabularnewline}{\\}
\addtolength{\textfloatsep}{-3.9mm}
\setlength{\abovedisplayskip}{1.6mm}
\setlength{\belowdisplayskip}{1.6mm}

\begin{document}

%\title{Traffic Management in Congested Areas through a Deep Reinforcement Learning Approach}

%\title{Federation Aware MEC Selection in 5G/B5G Networks}
\title{Closing the AI Responsibility Gap in Network Management: A Preamble Focus on End-User Rights}
	\author{
	\IEEEauthorblockN{ Emanuel Figetakis\IEEEauthorrefmark{1}, Ahmed Refaey \IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}\\

	\IEEEauthorblockA{\IEEEauthorrefmark{1} University of Guelph, Guelph, Ontario, Canada.}\\
	\IEEEauthorblockA{\IEEEauthorrefmark{2} Western University, London, Ontario, Canada.}}


\maketitle
\begin{abstract}
Artificial Intelligence (AI) has revolutionized the world by achieving noteworthy advancements in various industries, and the networking industry is not exempt from this convention. By intelligently automating tasks that once required human intervention, AI is helping to reduce operating costs for several different areas of the current network paradigm. One of the most significant benefits of AI in networking is the ability to automate networking management, a large blanket of tasks can have automatic responses to changing network conditions. However, it is important to recognize that there are some gaps within the responsibility of AI when something malfunctions. Despite the fact that AI has the potential to reduce the risk of human error, it is not immune to malfunctions, and the possibility of miscalculations always exists. This is especially true when creating the models that power AI, as the training data may not always be perfect, and the algorithms used to analyze the data may not always be accurate. Therefore, it is crucial to introduce a framework to identify when the AI systems take action. In this paper, the framework introduced takes the form of Deep Reinforcement Learning model training specifically to classify the AI model's tendencies and assign a numerical responsibility factor. The presented model achieved a 99.6\% accuracy, correctly identifying the AI model initiating a change to the simulated network. 


\end{abstract}

\begin{IEEEkeywords}
    Artificial Intelligence, Deep Reinforcement Learning, Networking 
\end{IEEEkeywords}

\section{Introduction}
Current state network implementations have evolved greatly in the last decade utilizing the Evolved Packet Core (EPC) with its ability to host and virtualize many of the network's functions. The virtualized functions comprise the core and now can be hosted with much less hardware than legacy networks \cite{3gpp_2022}. However, when the network functions of the EPC are virtualized they are still subject to hardware limitations such as computing capacity; This can make scalability difficult without upgrading the network's infrastructure \cite{cisco}. Network slicing, which allows for multiple services to utilize the same infrastructure, is also limited to hardware resources \cite{3gpp_NSM}. \cite{cisco} introduces a new approach taken in the industry that allows for a multi-vendor service space that does not rely on a central unit of hardware for the network. It is accomplished through the disaggregation and decomposition of virtual functions in the core as well as the remote radio head (RRH), into separate cloudlets. The multivendor network is harmonized with common feature sets across all target markets which allow a collaboration of the disaggregated virtualized network functions hosted on the cloud. This system is highly adaptable to demands and outages as resources can be added via the cloud at any given point. More importantly, it gives each virtual network operator complete control of their network which means they can manage their own and add resources as they see fit. Open-source software has also made it easy to implement specific virtualized network functions that make up the EPC \cite{OpenSource-5G}, making the management of orchestration of the core network easier than before. 

%2) Zero touch (which will be inside it the use of different AI tools)

Not only has network architecture evolved but so have the management systems that accompany the new network architectures. This is in part due to advancements being made in Artificial Intelligence (AI) and its subsets of Machine Learning (ML) and Reinforcement Learning (RL). The concept of utilizing AI for network orchestration and management is called Zero Touch Networks (ZTN) and focuses on limiting human interaction with a network within the control domain. Virtualization of many network functions has also introduced the ability to virtualization AI tools within the Management and orchestration (MANO) domain service. The MANO service has the ability to orchestrate Network Function Virtualizations (NFVs), create and control Virtual Network Functions (VNFs), and manage the physical hardware that the service is being hosted on. This makes it the ideal service to host AI tools to help manage the network. The AI tools can be data-driven to make informed decisions on the network making them self-adaptive and self-reactive \cite{CDR_Analysis}. Implementations range from autonomous cell outage detection and compensation \cite{ML-Healing} or energy-efficient management of hardware resources \cite{AI-Energy}. Although the domains are separated communication between them is possible allowing for real-time management. 

%3) Here you can talk about how they contribute to a decision  that affects the end user, and define the responsibilities

AI is being used in networks purpose of network management and has the end goal of improving the Quality of Experience (QoE) for the user equipment (UE). The level of service that is managed by AI is higher and is more efficient for vendors. Different systems provide different solutions and advantages; Some systems provide a report of what action should be taken \cite{CDR_Analysis} while others can make changes on their own \cite{AI-Energy}, this is up to the vendor utilizing the tool. 

%4) the Resp gap has its origin in robotics (not too much talking about robotics, just it is used in that field) 

However, currently, there exists a gap in responsibility with accountability and ethics with AI and its subsets, making it difficult to ensure fair use of them. The field of robotics has faced similar issues, especially concerns with human-robot interaction (HRI). To ensure the safety of the users, the ethics of the robot's programmed decision-making was called into question. Above all the robots must always be programmed with the intent of never harming a human\cite{robotics-1}, and making sure everyone receives the same treatment \cite{robotics-2}. Both of these core concepts of ethics must also be present within AI. 

%5) Here is will use these framework in the zero-touch networks to align with (ex, Canadian Act, EU act..etc and many countries to follow [check the magazine reference]) and creating a new audit system to be integrated within the zero touch network

A solution to this gap in responsibility includes a framework within the network to verify and track the changes being made by AI tools. A framework like this can easily be implemented in the elements of the EPC which would support both legacy networks as well as virtualized networks. Along with the benefit of explaining the black box that AI tools make, the framework also becomes compliant with many new AI government regulations. The Canadian government, The United States government, and the European Union have already begun to introduce regulations on AI focusing on Human Oversight and Monitoring, Transparency, Fairness and Equity, Safety, Accountability, and Validity and Robustness \cite{AIGOV-EU, AIGOV-CAD, AIGOV-USA}. The framework introduces an audit system that helps assign accountability to each vendor that is using an AI tool for any kind of network orchestration or automation. It gives the benefit of transparency, ensures fairness, and allows for human oversight and monitoring. The contributions of this paper can be summarized as follows:
\begin{itemize}
\item A comprehensive model for two key service domains: Evolved Packet Core (EPC) and Management and Orchestration (MANO). This model provides a detailed view of their roles and interactions within a telecommunications network, offering insights into how these domains support network management and efficiency.

\item A framework designed to efficiently capture, store, and access network status logs. This framework uses advanced data management practices to ensure that log data is both accessible and analyzable, supporting effective monitoring, historical analysis, and troubleshooting of network issues.

\item A method to measure the impact of automated tools on network changes. This approach uses metrics and algorithms to assess how much-automated tools contribute to modifications in the network, helping to evaluate the performance and reliability of these tools in automating network management tasks.
\end{itemize}

%The contributions of this paper are as follows:
%\begin{itemize}
%    \item A detailed model of two services domains of EPC and MANO 
%\end{itemize}
%\begin{itemize}
%    \item A framework to record and obtain network status logs 
%\end{itemize}

%\begin{itemize}
%    \item A system to quantify the responsibility of an automated tool had in a change to the network
%\end{itemize}


%With the advancements of 5G/B5G networking being made from its predecessors, some current works and implementation are aiming at improving the quality of experience further. Fifth Generation networks brought forth a brand new approach towards networking by introducing the concepts of Network Function Virtualization (NFV) as well as Software Defined Networking (SDN). These concepts helped Mobile Network Operators(MNO) virtualize many components of a network reducing the need for dedicated hardware resources, and allowing for Mobile Virtual Network Operators(MVNO) to join the market more seamlessly.  
%The virtualization of the network has drastically improved the quality of services provided by vendors, as well as more up-time and more secure networks. 

%After the design and implementation of 5G networks, new security, automation, and management methods started to be created to match the new network architecture. Alongside the evolution of networking followed the breakthrough in Artificial Intelligence (AI) and many of its subsets. This proved to be a huge advantage in the field of networking as AI began to be leveraged in the form of tools to help with different aspects of networking, such as automation, security, and management. 
%The different subsets, Machine Learning(ML) and Reinforcement Learning(RL) are used to help achieve the goal of less human interaction with the network and more automatic and necessary changes to be made. 


%However, even with the new network architecture and AI tools operators are still limited to the legacy vendor architectures from the previous generations of networks. There still must be a centralized hardware center for the network to be hosted entirely. Even with advanced techniques of network slicing(NS), it is still limited to the hardware available making a scaling network difficult without additional hardware implementation. A solution is to create a scalable multi-vendor network space in which cloud virtualization and AI automation are used. This allows for MVNOs to offer different services over the same network without having to share resources with a centralized unit (CU). This also allows these MVNOs, to utilize the best- available network tools to manage and automate their own services. 

%This brings its own challenge, due to the rapid advancement of AI, many of the world's governments have begun to implement laws protecting users against AI automation. The Canadian government proposed an Artificial Intelligence and Data Act (AIDA), which introduces some gaps that currently exist with current state AI; with the gaps being Human Oversight and Monitoring, Transparency, Fairness and Equity, Safety, Accountability, and Validity and Robustness \cite{AIGOV-CAD}. The United States government as well as the European Union have also begun to implement their own AI laws that propose the same concepts as the Canadian AIDA \cite{AIGOV-USA}\cite{AIGOV-EU}.

%A solution to make the management and automation AI tools of the MVNOs in a multi-vendor network space complaint with these acts is an audit system. The system that can be put into place must quantify responsibility to the tools used to create and manage the network, as well as inform which modules are making changes to which network, and which operators are running the modules. By incorporating a system like this it aims to allow for human monitoring, transparency, validation, and accountability. 





\section{Background and Related Works}

Managing networks has evolved into Zero Touch Networks, an automated process by introducing a management domain to help with the life-cycle of VNFs, NFV orchestration, and hardware resource management. This allows for automatic changes to be made to networks which increases the reliability and quality of the provided service. The utilization of ZTNs also introduced automation using AI which brings advantages as well as challenges. Some of the challenges raise concerns with the decisions being made by the system, the data that is used to train the model, and the protection of the user from rouge systems. Some governance has been introduced but not fully introduced and integrated with many countries leaving these AI systems to operate without any standards or governance. 



\subsection{Zero Touch Networks}

Virtualization of core network components has helped shift more to software and less hardware making them easier to deploy than hardware-dependent legacy models. It is this new modern networking standard \cite{3gpp_2022} that led to the development of a more advanced software-centric called Zero Touch Networks. The framework presented by the European Telecommunications Standards Institute (ETSI), shows the new management components for the network or individual service. The framework introduced a management domain thatX communicates with the service domain via a cross-domain integration fabric \cite{etsi_2019}. The management domain features a closed loop for management functions that deal with the physical and virtual aspects of the service.

Many current works focus on incorporating AI into the management functions of the ZTN to help manage the VNF, orchestrate NVF, and manage the physical resources of the network. In \cite{ns_energy} the authors use AI to minimize energy consumption for network slices and VNF installation, this work considers overall network cost with respect to computing resources as well as network traffic. 

Another advantage of ZTN is self-healing, where the network can automatically detect poor performance and make changes accordingly. The authors of \cite{CDR_Analysis}, create an AI-driven approach to classify and identify faults in cells of a network through the analysis of Call Detail Records. The solution creates a report with figures and accurate root-cause analysis which reduces an expert's job. 


The separate domains of the ZTN architecture allow for new and advanced AI management tools to analyze and make changes to the network. This allows for a higher QoE to be achieved for the user without the need to add more computational resources to the core network. 

\subsection {AI governance}

ZTN brings a better QoE for the end user through the use of AI automation tools, making management more automatic with less human intervention. This brings forward challenges and concerns as important tasks that used to be designated for someone with certain expertise and knowledge are now being delegated to AI. Many concerns come into question regarding the validity of the decisions an AI can make.

To address the concerns presented, understanding how AI and its subsets of ML and RL are modeled is essential. For ML, the learning that goes on is done primarily by algorithms comparing features within a data set which can be both labeled and unlabeled data. The data is the foundation for an ML model's success, however, things such as incomplete data, biased data, and incorrect algorithms can lead to bad models. For RL, the driving force for training the model is an agent operating in a mathematically modeled environment. A poorly modeled environment can result in a model making poor decisions. These bad models can account for a model making unfair decisions.


Several governing bodies have tried to address this by creating some kind of regulatory standard that ensures that the models being used will be fair, by listing a set of regulations to ensure the creation of fair models \cite{AIGOV-EU, AIGOV-CAD, AIGOV-USA}.



\subsection{Responsibility Gap}


Currently, there exists a gap in responsibility with accountability and ethics with AI and its subsets, making it difficult to ensure fair use of them. The field of robotics has faced similar issues, especially concerns with human-robot interaction (HRI). To ensure the safety of the users, the ethics of the robot's programmed decision-making was called into question. Above all the robots must always be programmed with the intent of never harming a human\cite{robotics-1}, and making sure everyone receives the same treatment \cite{robotics-2}. Both of these core concepts of ethics must also be present within AI. 


\section{System Model}\label{SysModel}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth,height=85mm]{Figs-Revised/Figures-OB2-rev4-4-cropped.pdf}
    \caption{Proposed Network Architecture for Multi-Vendor Space with Management Plane}
    \label{fig:PM}
\end{figure*}


Shown in Figure \ref{fig:PM} is the architecture of a ZTN verified by both 3GPP and ETSI, in this model the service network is powered by an EPC and has a separate domain for management, the MANO. The inter-domain integration handles the communication between both domains allowing for the transfer of network data. In the service domain, in this case, the core network, passive nodes are placed between known channels of communication between the vendors. These passive nodes collect the data from each vendor and forward it to the management plane this allows each of the vendors to access network information and make use of it in real-time AI automation tools. The management implementations are where the tools are being used and from there are making changes to the VNFs, NFVs, and the physical resources that are being used by the network. 
Also in the management implementations is the proposed audit framework that will quantitatively measure the influence of the vendor's tools on the current state of the network. This is accomplished by using a pre-trained DRL model and current network data from the passive nodes of the network. The DRL model is trained for the specific network and specific models that are being used, the model's actions are mapped to the state of the network and then used to modify the simulation network environment in which the DRL model is trained. The model takes the data as the input and then outputs its prediction for which tools had an influence on the network state. This way a direct responsibility can be assigned to any models that had a decision that impacted the change in the network.


\section{Problem Definition}\label{Problem-Def}

To better formulate and understand the problem, influence edges from graph ranking were used within the formulations \cite{Graph_Ranking}. The influence edges gave an advantage when formulating the problem because it allowed for the separation of different domains as well as the focus on interaction between elements. The variables that are used in the problem in Table \ref{tab:Vars}.


\subsection{Modeling through Graph Ranking}

Using Graph Ranking involves setting graph elements and their influence on one another. Adapting this for an audit system in networking is possible when looking at the different domains part of the network. There is the management domain, the service domain, and the user domain within each of their respective elements. While it follows a top-down approach with the management making changes to the service domain elements that affect the user, the framework can be used in different network configurations.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth,height=48mm]{Figs-Revised/GraphRankingModel-cropped.pdf}
    \caption{Graphical Modeling of Network}
    \label{fig:G-net}
\end{figure}

Figure \ref{fig:G-net} shows how the network can be represented by graphical elements. The links between elements within the same domain are the intra-domain edges while the links between elements that are in different domains are influence edges. The elements within the management domain influence the elements within the service domain, and in turn, influence the user domain. This illustrates the responsibility that the management domain has on the end user. 
This allows for enough of an understanding to model the system. 

The problem being modeled is first finding the modifying agent to the network. This can be defined by the following function:

\begin{equation}\label{eq:alpha}
    a = N_{Links}(A_{\omega },N_{Elem}^{\omega})
\end{equation}

Equation \ref{eq:alpha} introduces the variables $a,\mathcal{O}$ as well as a few different sets $N_{Links}, A, N_{Elem}$. This equation gives the value of the agent that has made the change by using $\mathcal{O}$ as the index in the set containing the agents.

\begin{equation}\label{eq:OEq}
    \begin{array}{l}
    \mathcal{O} = \Lambda * (\frac{\sum_{\chi \in N_{LinksU}}^{n}(N^{\chi}_{Elem})}{\sum_{\chi \in N_{Links}}^{n}(N^{\chi}_{Elem})})
    \\
    \\
    \mathcal{O} = 1 , f(\mathcal{O}) = 0
    \\
   \mathcal{O} \neq  1 , f(\mathcal{O}) = n
    \end{array}
\end{equation}

Equation \ref{eq:OEq} aims to find the differences between the new and existing sets of the network elements by looking at the set $N_{Links}$. The function then returns the index in which a change was calculated. It also introduces another variable $\Lambda$, which is a binary expression to determine if the change made to the network is legal, in this case meaning that the allocation of resources does not exceed what is available in total to the network.

\begin{equation}\label{eq:Lambda}
    \begin{array}{l}
    \Lambda = \frac{\sum_{A \in N_{Elem}^{Res}}^{n}A}{Res_{Total}}
    \\
    \\
    \Lambda = 1 , f(\Lambda) = 1
    \\
    \Lambda \neq 1 , f(\Lambda) = 0
    \end{array}
\end{equation}

Equation \ref{eq:Lambda} is a binary equation $\Lambda$ to determine if a new allocation of resources is allowed. This will cancel out any calculations that are trying to be made that would violate this constraint.


\begin{equation}\label{eq:Rho}
    \rho = \theta * \sum_{\chi \in N_{Elem}}^{n}  N_{res}^{\chi} \times N_{IFactor}^{\chi}
\end{equation}

Shown in equation \ref{eq:Rho} is the quantification of the network element's influence on the end user compared to other elements given the element's current resources and the impact on the end user. This also allows for a comprehensive percentage of responsibility to be found for the service change for each network element. 

\begin{equation}\label{eq:Mu}
    \mu = \frac{\sum_{\chi \in N_Elem}^{n} N_{IFactor}^{\chi}\times N_{Res}^{\chi}}{\sum N_{IFactor}} \times 100
\end{equation}

Equation \ref{eq:Mu} finds the percentage a single network element is responsible for on the end user given the This then allows for responsibility for controlling agents to be established for the change of the networks by comparing the percent of change between network elements and the controlling agents.


\begin{equation}\label{eq:nu}
    \begin{array}{l}
    \nu = \frac{1}{\sum_{i=1}^{n} \mu_{i}}\times\sum_{j=1,A\epsilon N_{Links}}^{n}f(\mu)N^{A_{j}}_{Links}
    \\
    \Delta \nu = \nu_{current} - \nu_{prev} 
    \end{array}
\end{equation}

Equation \ref{eq:nu} first sums all of the percentages that were found by $\mu$, then goes through the set $N_{Links}$ using the subset $A$ to find each network element belonging to the management agent. By dividing the two it can be found how much each agent is responsible for the current state of the network. 
Then by comparing previous values with current the change in the network and the parties most involved can quickly be determined. 


\begin{comment}
\begin{table}[h]
\centering
\begin{tabular}{|c|c|l|}
\hline
Variable & Description & Constraints \\ \hline
$A$ & \begin{tabular}[c]{@{}c@{}}Set containing "Agents"/AI \\ management tools\end{tabular} & $\left \langle 1,2,...,n \right \rangle$ \\ \hline
$N_{Elem}$& \begin{tabular}[c]{@{}c@{}}Set containing elements of\\ a network\end{tabular} & $\left \langle 1,2,...,n \right \rangle$ \\ \hline
$N_{Links}$ & \begin{tabular}[c]{@{}c@{}}Set  containing Agents \\ and Network Elements\end{tabular} & \begin{tabular}[c]{@{}l@{}}$N_{Links} \epsilon A$ \\ $N_{Links} \epsilon N_Elem$\\ $(A_{1},N_{Elem_{1}}),(A_{1},N_{Elem_{2}},$
\\ $..., (A_{n},N_Elem_{n})$ \end{tabular} \\ \hline
\multicolumn{1}{|l|}{$N_{IFactor}$} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Set containing a value that\\ represents the influence \\ each network element has \\ on the end user\end{tabular}} & \begin{tabular}[c]{@{}l@{}}$N_{IFactor} \epsilon N_{Elem}$\\ $\left \langle 1,2,..,n \right \rangle$ \end{tabular} \\ \hline

\multicolumn{1}{|l|}{$N_{Res}$} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Set containing the resources\\ currently available to\end{tabular}} & \begin{tabular}[c]{@{}l@{}}$\sum_{A \epsilon N_{Res}} \equiv N_{Res}^{Total}$\\ $\left \lange 1,2,...,n \right \rangle$ \end{tabular} \\ \hline

$\rho$ & \begin{tabular}[c]{@{}l@{}}$/rho$ find the influence of \\ each of the network elements\\ based on its $N_{IFactor}$\\ and the $N_{Res}$\end{tabular} & $\rho = \theta * \sum_{\chi \in N_{Elem}}^{n}  N_{res}^{\chi} \times N_{IFactor}^{\chi}$ \\ \hline



$\mathcal{O}$ & \begin{tabular}[c]{@{}l@{}}$\mathcal{O}$ aims to find \\ which elements have changed\\ by taking the updated\\ $N_{Links}$ set and \\ comparing them between element.\\ Once a value other than 1 is found\\ the index is given $n$\end{tabular} & \begin{tabular}[c]{@{}l@{}}$\mathcal{O}= 1 , f(\mathcal{O}) = 0$\\ $\mathcal{O} \neq  1 , f(\mathcal{O}) = n$\\ $\mathcal{O} = \Lambda * (\frac{\sum_{\chi \in N_{LinksU}}^{n}(N^{\chi}_{Elem})}{\sum_{\chi \in N_{Links}}^{n}(N^{\chi}_{Elem})})$\end{tabular} \\ \hline


$\mu$ & \begin{tabular}[c]{@{}l@{}}$\mu$ finds the comprehensive \\ percentage of responsibility\\ based on the previous elements \\ and sets.\end{tabular} & $\mu = \frac{\sum_{\chi \in N_Elem}^{n} N_{IFactor}^{\chi}\times N_{Res}^{\chi}}{\sum N_{IFactor}} \times 100$ \\ \hline


$\alpha$ & \begin{tabular}[c]{@{}l@{}}$\alpha$ returns the Agent and\\ the changed element\end{tabular} & $a = N_{Links}(A_{\mathcal{O}},N_{Elem}^{\mathcal{O}})$ \\ \hline

$\nu$ & \begin{tabular}[c]{@{}l@{}}$\nu$ finds the elements and \\ connected Agents that were \\ responsible for the shift in the \\ network\end{tabular} & $\nu = \frac{1}{\sum_{i=1}^{n} \mu_{i}}\times\sum_{j=1,A\epsilon N_{Links}}^{n}f(\mu)N^{A_{j}}_{Links}$ \\ \hline

$\Lambda$ & \begin{tabular}[c]{@{}l@{}}$\Lambda$ aims to determine if \\ a shift in total network resources\\ does not violate any constraints \\ of creating non-exsiting resources\end{tabular} & \begin{tabular}[c]{@{}l@{}}$\Lambda = 1 , f(\Lambda) = 1$\\ $\Lambda \neq 1 , f(\Lambda) = 0$\\ $\Lambda = \frac{\sum_{A \in N_{Elem}^{Res}}^{n}A}{Res_{Total}}$\end{tabular} \\ \hline

\end{tabular}
\end{table}


\begin{table*}
\begin{centering}
    

    \begin{tabularx}{\textwidth}[HTB]{
  | >{\raggedright\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X | }
  {\textbf{Variable}}  &{\textbf{Description}} &{\textbf{Constraints}} \\
 \hline
 $N_{Elem}$& \begin{tabular}[c]{@{}c@{}}Set containing elements of\\ a network\end{tabular} & $\left \langle 1,2,...,n \right \rangle$ \\ \hline
$N_{Links}$ & \begin{tabular}[c]{@{}c@{}}Set  containing Agents \\ and Network Elements\end{tabular} & \begin{tabular}[c]{@{}l@{}}$N_{Links} \epsilon A$ \\ $N_{Links} \epsilon N_Elem$\\ $(A_{1},N_{Elem_{1}}),(A_{1},N_{Elem_{2}},$
\\ $..., (A_{n},N_Elem_{n})$ \end{tabular} \\ \hline
\multicolumn{1}{|l|}{$N_{IFactor}$} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Set containing a value that\\ represents the influence \\ each network element has \\ on the end user\end{tabular}} & \begin{tabular}[c]{@{}l@{}}$N_{IFactor} \epsilon N_{Elem}$\\ $\left \langle 1,2,..,n \right \rangle$ \end{tabular} \\ \hline

\multicolumn{1}{|l|}{$N_{Res}$} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Set containing the resources\\ currently available to\end{tabular}} & \begin{tabular}[c]{@{}l@{}}$\sum_{A \epsilon N_{Res}} \equiv N_{Res}^{Total}$\\ $\left \langle 1,2,...,n  \right \rangle$ \end{tabular} \\ \hline

$\rho$ & \begin{tabular}[c]{@{}l@{}} $\rho$ find the influence of \\ each of the network elements\\ based on its $N_{IFactor}$\\ and the $N_{Res}$\end{tabular} & $\rho = \theta * \sum_{\chi \in N_{Elem}}^{n}  N_{res}^{\chi} \times N_{IFactor}^{\chi}$ \\ \hline



$\mathcal{O}$ & \begin{tabular}[c]{@{}l@{}}$\mathcal{O}$ aims to find \\ which elements have changed\\ by taking the updated\\ $N_{Links}$ set and \\ comparing them between element.\\ Once a value other than 1 is found\\ the index is given $n$\end{tabular} & \begin{tabular}[c]{@{}l@{}}$\mathcal{O}= 1 , f(\mathcal{O}) = 0$\\ $\mathcal{O} \neq  1 , f(\mathcal{O}) = n$\\ $\mathcal{O} = \Lambda * (\frac{\sum_{\chi \in N_{LinksU}}^{n}(N^{\chi}_{Elem})}{\sum_{\chi \in N_{Links}}^{n}(N^{\chi}_{Elem})})$\end{tabular} \\ \hline


$\mu$ & \begin{tabular}[c]{@{}l@{}}$\mu$ finds the comprehensive \\ percentage of responsibility\\ based on the previous elements \\ and sets.\end{tabular} & $\mu = \frac{\sum_{\chi \in N_Elem}^{n} N_{IFactor}^{\chi}\times N_{Res}^{\chi}}{\sum N_{IFactor}} \times 100$ \\ \hline


$\alpha$ & \begin{tabular}[c]{@{}l@{}}$\alpha$ returns the Agent and\\ the changed element\end{tabular} & $a = N_{Links}(A_{\mathcal{O}},N_{Elem}^{\mathcal{O}})$ \\ \hline

$\nu$ & \begin{tabular}[c]{@{}l@{}}$\nu$ finds the elements and \\ connected Agents that were \\ responsible for the shift in the \\ network\end{tabular} & $\nu = \frac{1}{\sum_{i=1}^{n} \mu_{i}}\times\sum_{j=1,A\epsilon N_{Links}}^{n}f(\mu)N^{A_{j}}_{Links}$ \\ \hline

$\Lambda$ & \begin{tabular}[c]{@{}l@{}}$\Lambda$ aims to determine if \\ a shift in total network resources\\ does not violate any constraints \\ of creating non-exsiting resources\end{tabular} & \begin{tabular}[c]{@{}l@{}}$\Lambda = 1 , f(\Lambda) = 1$\\ $\Lambda \neq 1 , f(\Lambda) = 0$\\ $\Lambda = \frac{\sum_{A \in N_{Elem}^{Res}}^{n}A}{Res_{Total}}$\end{tabular} \\ \hline
    \end{tabularx}
    \end{centering}
\end{table*}
\end{comment}

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|l|}
\hline
Variable & Description & Range \\ \hline
A & Number of AI agents part of the network &  \\ \hline
N_Elem & Number of VNF elements in the network &  \\ \hline
$N_{Links}$ & List of AI agents and their VNF elements &  \\ \hline
$N_{IFactor}$ & Values of influence that VNF has on end user &  \\ \hline
$N_{Res}$ & Number of resources VNFs have &  \\ \hline
$\rho$ & Value of influence VNF based on $N_{IFactor}$, $N_{Res}$ &  \\ \hline
$\mathcal{O}$ & Binary operator to detect a change & $0 \leq \mathcal{O} \leq 1$ \\ \hline
$\mu$ & Comprehensive percentage of responsibility &  \\ \hline
$\alpha$ & Altered VNF element with Agent &  \\ \hline
$\nu$ & List of all altered VNF elements &  \\ \hline
$\Lambda$ & Binary operator to detect legal allocation of resources & $0 \leq \Lambda \leq 1$ \\ \hline
\end{tabular}%
}
\caption{List of variables used in problem formulation.}
\label{tab:Vars}
\end{table}




\subsection{Solving using MDP Modeling}

By using the graph ranking model for our Markov Decision Process it can be an accurate environment to train an RL model on. With realizing what the goal of the RL agent is and the relationships based on graph ranking an informed decision can be made that the focus of interest is the AI tools managing the network, this can correlate to the states of MDP. The actions will be the RL agent guessing based on the observations from the service domain in which the AI tool had made the change. Essentially the agent will be making a guess about which state it is currently in and this information will be withheld from it. It will then take the other information from the graph as observation variables to learn the dynamics of the network elements. 

When formulating the MDP the state space can be defined as with three distinct states $S = \left\langle S_{1}, S_{2}, S_{3} \right\rangle$ each representing a management tool. Creating the transition probability for this specific application was slightly different in that the transition had to be decided not by the actions of the agent but rather by a random variable with a specific range. The reason for this is that when training using the MDP the RL agent must focus on the observations and learn the features of the network, the random action variable is $a_{r}$ following the discrete uniform distribution over the set $[1,2,3]$. 
\begin{equation}
    p(s'|s, a_{r}) = Pr(S_{t} = s'|S_{t}=s, A_{t} = a_{r})
\end{equation}

The expected reward is kept simple if the agent's action matches the current state then a reward is issued.
\begin{equation}
    r(s, a) =\mathbb{E}[\{^{R_{t} = 1, s = a}_{R_{t}=-1,s\not\equiv a}|S_{t} = s, A_{t}=a]
\end{equation}

Up until this point, the formulation has followed the standard MDP formula however when observations begin to be defined it falls under the category of a Partially Observed Markov Decision Process (POMDP). In this application, this will be better to use since the goal is for the learning to come from the observation variables rather than the transitions from states. This will help create an RL to classify the states and therefore audit the management tools. The observation variables are visualized in Figure \ref{fig:G-net} and are the influential edges between the nodes across each of the domains, the edges between the management domain and service domain as well as the service domain and the user domain. The edges between the management domain and service domain can be denoted as Network Links or $Net_{Link}$, the purpose is to determine how many network elements in the service domain are managed by each of the tools. The edges between the service domain and the user domain can be denoted as Network Impact or $Net_{Imp}$, the reason for these links is to give a rough estimate of how each of the network elements within the service domain will impact the user in the user domain. Along with the influential edges, the previous state is added to the observation variable, this will allow learning to take place even when the agent does not provide the correct action. 

\begin{multline}
    \mathcal{O}(s) = (Net^{s}_{obs1}, Net^{s}_{obs2},S_{t-1}|S_{t} = s ,\\
    Net^{s}_{obs1} = Net(s)_{Link}, Net^{s}_{obs2} = Net(s)_{Imp})
\end{multline}

Once the observations are set, the observation function can be defined. This will be used over iterations to determine the state and learn from the observations. The function calculates and gives a value to each of the nodes within the service function determining its weight on the end user. It is then used with the previous state to determine which tool had made the change.


\begin{multline}
    Z = (\mathcal{O},s) = (\frac{s}{\mathcal{O}_{1}}* \mathcal{O}_{2})|S_{t-1}=s,\\
    \mathcal{O}_{1}=Net(s)_{Link}, \mathcal{O}_{2}=Net(s)_{Imp})
\end{multline}

After defining all the functions needed for a POMDP it is now possible to create an environment for a stochastic game to take place and create a simulation. 


\section{Experimentation of Proposed Framework}

The experimentation of the system is simulation-based with an RL agent using a custom-created gym environment modeled after the POMDP as a stochastic game. While the POMDP function that was created could in fact support a multi-agent space it was determined that the stochastic game space would train a single agent slightly better promoting discovery over greedy/competitive policies. By implementing the randomness into the simulation it is theorized that the agent would get a better chance of seeing more scenarios than with a second agent. 

\subsection{Creating Custom Environment}

\textcolor{blue}{A custom environment framework was created for this simulation, it was programmed to allow for even more customization of network elements, states, influence edges, and AI tools. For this simulation 3 management tools are simulated in the management domain node, this value was selected to represent the multiple vendors making changes to the same network. For the network elements in the service domain node, 8 were selected following the number of VNFs found in EPC implementations \cite{3gpp_2022}(excluding the Network Slice Selection Function). Since the focus was assigning the responsibility of the tools found in the management domain for network changes that impacted a single end-user, one element is found in the user domain. For this simulation, there are two types of influence edges, one type being those coming from the management domain to the service domain, and those coming from the service domain to the user domain. These edges simulate which management tool controls a certain VNF and the impact of the VNF on the end user's quality of experience. For this simulation, these values were set arbitrarily but can easily be modified due to the high customization of the framework.}


As stated before a stochastic game approach was taken with this simulation, and was mainly implemented within the management tools elements. In the custom game, the elements are allotted a fixed amount of resources which they can allocate to their linked network elements, however, the total sum of resources can not exceed a certain amount. For this simulation, the links between elements were assigned at random but kept the same throughout the simulation, as well as the links between the network elements weighed on the end user. However, it was created to allow for changes in any of the domains or links for the reason of being able to model any kind of network. 

%A graphical interface was also created to render the simulation and to display the actions of the RL agent and the changes being made between the domains. To accomplish this OpenAIs GYM python library was used as well as Pygame to create the render for the environment. 

To create this into a working POMDP that can be used with a Deep Q-Network the following parameters were adjusted, the observation space had to be an 8-value tuple that included the values of the network elements. The state space, that was kept hidden during the game, was a discrete 3-value integer to represent each of the tools that are being used in the network and therefore the action space was also a discrete 3-value integer. Creating the reward function for this environment required some trial and error, at first a simulation length was implemented for each episode, this would determine the run time for each episode. To pair with this a static reward was given for each time step however during training this would create a large range of rewards that the agent could acquire. To avoid the high volatility of rewards the time step was removed and transition between states could only be achieved if the action matched the state. This more accurately represented a Markov Chain where the action had a direct impact and state transition, this also allowed for more predictable rewards, where a transition rewarded the agent and staying within the same state penalized the agent.


\lstset{style=mystyle}
\begin{lstlisting}[language=Python, caption= Overview of Audit Env,]
Class CustomAuditEnv(Env):
    def __init__(self):
        self.action_space = Discrete(3)
        self.state_space = Discrete(3)
        self.observation = Box(shape =(8,))
        self.state, self.observation = Step_F()
    def step(self,action):
        if action == self.state:
            self.reward = self.reward + 1
            self.done = True
        else:
            self.done = False
            self.reward = self.reward - 1
        return self.observation, self.reward, 
                self.done
    def reset(self):
        self.state, self.observation = Step_F()
\end{lstlisting}

The code shows an overview of the environment, with the focus being on how the state is defined and transitions. The state definition is only possible in the initialization function or the reset function, so only when the simulation starts or resets. The step function is called when an action is made and rather than including state transition in this function it takes advantage of the $done$ variable, which can force the episode to reset which then transitions to a different state. 

\subsection{Solving using Dynamic Programming}
Dynamic Programming can be used to determine the optimal solution for the environment given that certain information is known and can be defined. It can be used as a benchmark for each iteration to verify if the optimal solution has been found, however, when used in a live application can fail due to some of the unknown variables of the system. This is also why a Deep Reinforcement Learning approach is taken, to help create a system that can operate with some degree of certainty in an unknown environment. The definition of the dynamic program relies on the equations set in the POMDP, especially the observation variables and the observation functions. These are also the only function that should be made available for the agent to see otherwise all the information can be gathered from the state directly. 


\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{Figs-Revised/AER_FULL-cropped.pdf}
    \caption{Results from Benchmarks}
    \label{fig:Reward-BaseLines}
\end{figure}


Starting the program is the definition of the three variables. These variables are also set to values in the initial state. 

\begin{equation}
N_{A}Links, N_{A}Imp, A_{Resources}
\end{equation}


The variables to the number of links between the agent and network elements, the next is the influence that the agent has over that network element, and finally the amount of resources allocated for each of the agents. This variable can be broken down between agents, however, $A_{Resources}Total = 1$ makes a pool of resources instead of an infinite number. The variables are set except $N_{A}Imp$ which is calculated based on the other variables. 
\begin{equation}
    N_{A}Imp= \frac{A_{Resources}}{N_{A}Links}
\end{equation}

This variable represents the agent's resources being distributed to each of their responsible elements. 
This defines the observation variables for each state however since the state is hidden in the POMDP is not shown to the agent. Systems like the DQN use the Q-value of the observations and the reward function to learn about the correlation of the system to the states. With dynamic programming, the initial state and all variables must be defined and then the optimal solution can be found. In a simulated space, it makes it the perfect benchmark, since the optimal solution can be calculated outside of the environment without compromising any of the agents that are operating within the environment.    

Find the optimal solution that starts in the initial state by defining all of the variables. 

\begin{equation}
    \{ N_{1}Imp,N_{2}Imp, ... N_{n}Imp \} = \prod_{n=1,2, ... 8}^{} \frac{A_{n}Resources}{N_{n}Links}
\end{equation}

The above equation finds all the values that are needed, since the simulation has 8 elements the product is calculated up to the 8th time. As the simulation continues and transitions between states the values will continue to change, however, again due to the POMDP the state is not known so the transition $A_{resources}$ is not known either, so the product can not be calculated the same way again. By looking at the properties of the first state, however, the resources can be found by using the variables that are defined. Also understanding the original POMDP and its constraints it can be inferred that elements that share the same value belong to the same agent. 
\begin{multline}
    \to N_{n}Imp = N_{n+1}Imp \\
\longrightarrow (N_{n}Imp \in Agent_{n}), (N_{n+1}Imp \in Agent_{n})
\end{multline}

\begin{multline}
    \to N_{n}Imp \not\equiv  N_{n+1}Imp \\
\longrightarrow (N_{n}Imp \in Agent_{n}), (N_{n+1}Imp \in Agent_{n+1})
\end{multline}

The above equations represent that if any of the values are equal they are placed into the same Agent set. By placing these values into sets, when the simulation iterates the values will change. From the changed values and information about the sets, the resources, and changed agents can be determined. 

\begin{equation}
\begin{split}
    Agent_{n} = \left\{ N_{n}Imp,....N_{n+1}Imp \right\} \\N_{n}Links = n(Agent_{n})
\end{split}
\end{equation}

\begin{equation}
    N_{n}Imp * N_{n}Links = A_{n}Resources
\end{equation}

These equations take the set and reverse the initial steps to get the resources associated with the set. It has also been established that the sets are representative of Agents making changes to the network. To this point, the initial state is known and now the new resource value is known, the only step left is to determine which agent took from the resource pool. 

\begin{equation}
    Change = Init - Current\begin{cases} 
      Gained & Change<0 \\
      Lost & Change > 0 \\
      Neither & Change = 0 
\end{cases}
\end{equation}

This shows which set is responsible for the change in the observation variables. By determining the set, the state has also been determined which finds the agent making the network change. No iterations are needed just a transition between states to find the optimal solution without any kind of learning. 
The dynamic program also allows for percentages of accountability to be accurately assigned based on the changes made to the network. This is simple since the relationships between the network elements and agents have been clearly defined by the above equations.
\begin{equation}
    Net_{change} = \frac{n(Agent_{n})*\left| \Delta Agent_{n} \right|}{n(Agent_{n},...,Agent_{n+1})*A_{resourceTotal}}
\end{equation}
\begin{equation}
    Responsbility= \frac{n(Agent_{n})*\left| \Delta Agent_{n} \right|}{Net_{change}} \ast 100
\end{equation}



The percent is calculated by first determining the change that occurred on the network, and then how much each agent attributed to that change. 

\subsection{Proximal Policy Optimization}

A benchmark was needed for the custom environment before a DQN was implemented, for this implementation, the Proxiam Policy Optimization was used from OpenAI. A quick overview of the algorithm is that PPO is the solution to simple problems that Q learning fails on and overcomes the shortcomings of trust region policy optimization (TRPO) <OPENAI CITE>. This implementation used Stable Baselines 3 <SB3 CITE> PPO implementation which is adapted from the OpenAIs PPO algorithm. This benchmark was also selected as it differed from the DQN as an on-policy learner, this would later help determine which algorithm is better for the application.  


\subsection{Deep Q-Network}
To create the DQN, Pytorch was used for layering the neural network, and functions were created for feed-forward learning, memory, action selection, and Q-value evaluation. The first step in implementing the DQN was creating and setting the hyperparameters for the neural network such as learning rate, input dimensions, output layer dimensions, and activation function. The network was set up in a feed-forward type configuration so the output of the first layer would be the input for the second layer. For the activation function a linear activation function was used, with the Adam optimizer, and the Mean Square Error as the loss function. The model created had three layers with the input being the observation array of 8-values and the output being the number of actions available. After the neural network was created the Agent needed to be created, this is what would control the training and adjusting the weights within the model. Initially, hyperparameters for the agent must be addressed such as the gamma, epsilon, learning rate, batch size, memory size, and epsilon decay. These variables would be changed over the different iterations of the experiment. The engine of the model is the storing of the state transitions as well as the learning, the memory is set and this is just a limit on how many iterations will be stored in the neural network and the learning function is how the agent uses the observation to calculate the Q-Value and use it with the loss function. 

\begin{algorithm}
\caption{Deep Q-Network Action Decision}\label{alg:cap}
\begin{algorithmic}[1]
\Require $Observation$
\Ensure $Return Action$
\State $QEval \gets (Learning Rate, NActions, Observations$,\hline $L1Dim,L2Dim)$

\State $X \gets random()$
\If{$X > \epsilon$}
    \State $State \gets QEval(Observation) $
    \State $Actions \gets QEvalNext(State)$
    \State $Action \gets ArgMax(Actions)$
\ElsIf{$X > \epsilon$}
    \State $Action \gets Random()$
\EndIf
\State $Return Action$
\end{algorithmic}
\end{algorithm}

\section{Results and Analysis}

Testing was necessary for both the environment and the algorithms, however, testing the environment first took priority as it would not represent the problem correctly if an error was encountered. To first test the environment, the DQN was used to observe the behavior of the environment this is because it would provide a good indicator if learning could take place. For this testing hyper parameters would be set arbitrarily however focusing on a longer than usual run time to determine if a local minimum or maximum would trap the algorithms. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{Figs-Revised/UniformRewards-cropped.pdf}
    \caption{Implementation of lower reward bound}
    \label{fig:Uni-Reward}
\end{figure}

What was determined after the first run was an issue that would not only the results from the DQN algorithms but all the algorithms being tested on the environment, the issue being an infinite minimum. No matter if the algorithm could avoid this infinite minimum, it would still skew the average episodic reward, and this fault resulted from the stopping criteria within the environment as well as the reward function. An iteration would only end if the agent correctly determined the modifying agent and wrong actions would incur a negative reward, this resulted in an infinite minimum that would trap any algorithm. To fix this a lower bound was added to the environment shown in figure \ref{fig:Uni-Reward}, once the bound was established the DQN was able to properly train and showed significant improvement. 

The environment was determined to pass, so a large test bed was created to train all the baselines. Using baselines3 this was achieved by making sure the environment complied with the package, and then training for each method was simple \cite{stable-baselines3}. 


\textcolor{blue}{For the DQN the parameters were tuned in the following way, the gamma, discount factor, was set to .99 to focus on the reward later on in the simulation. This was determined to have a positive effect as earlier on it would be able to focus more on how the state changed the observation variables instead of just inputting actions to warrant a reward. The epsilon was initially set to 1.0 and had an end of .01 with a learning rate of .0001 and the Adam optimizer. The training was done in batch sizes of 64 to not overflow the memory of the simulation.}



Figure \ref{fig:Reward-BaseLines} shows how each baseline performed in the environment. Some baselines performed better than others, this is due to the different strengths of the algorithms performing in this type of environment. The environment creates the situation described in section \ref{Problem-Def}, however, adds a random element to prevent overfitting from exact values, it does this by randomly deciding which agent will change the network. The learning agent only has access to the observation variables, which include the values for each network element, their links, and previous actions. This results in either, learning what differences between observation variables mean, or trying to correlate the numerical values of observation variables. 

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table*}[h]
\centering
\resizebox{2\columnwidth}{!}{%
\begin{tabular}{|l|lc|l|l|l|l|}
\hline
Algorithms & Hyperparameters & \multicolumn{1}{l|}{} & Key Concepts & Advantage & Applications & Complexity \\ \hline
 & \multicolumn{1}{l|}{Learning Rate} & .0007 &  &  &  &  \\ \cline{2-3}
A2C & \multicolumn{1}{l|}{Steps} & 5 & \begin{tabular}[c]{@{}l@{}}Advantage actor critic system training two nets, \\ REINFORCE and a DQN, better loss updates the system.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Stable learning, lower variance during training, also degree of \\ fault tolerance since two different algorithms are used.\end{tabular} & \begin{tabular}[c]{@{}l@{}}ATARI Games\\ Pybullet Real-Time Physics Simulation\end{tabular} &  \\ \cline{2-3}
 & \multicolumn{1}{l|}{Gamma} & .99 &  &  &  &  \\ \hline
 & \multicolumn{1}{l|}{Learning Rate} & .02 &  &  &  &  \\ \cline{2-3}
ARS & \multicolumn{1}{l|}{Delta} & 8 & \begin{tabular}[c]{@{}l@{}}Model-free algorithm for dynamic solutions, from using\\ Basic Random Search (BRS) and heuristics from deep learning \cite{ARS-1}.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Reduced complexity, faster training times without having to \\ compromise accuracy.\end{tabular} & Mujoco &  \\ \cline{2-3}
 & \multicolumn{1}{l|}{Delta STD} & .05 &  &  &  &  \\ \hline
 & \multicolumn{1}{l|}{Learning Rate} & .0003 &  &  &  &  \\ \cline{2-3}
PPO & \multicolumn{1}{l|}{Steps} & 2048 & \begin{tabular}[c]{@{}l@{}}Policy gradient method of learning, improvement from TRPO\\ where adaptive adjustments can be made.\end{tabular} & \begin{tabular}[c]{@{}l@{}}Ease of use and implementation, improved performance \\ from other benchmarks.\end{tabular} & \begin{tabular}[c]{@{}l@{}}ATARI Games\\ Pybullet Real-Time Physics Simulation\end{tabular} &  \\ \cline{2-3}
 & \multicolumn{1}{l|}{Gamma} & .99 &  &  &  &  \\ \hline
 & \multicolumn{1}{l|}{Learning Rate} & .00005 &  &  &  &  \\ \cline{2-3}
QRDQN & \multicolumn{1}{l|}{Tau} & 1 & \begin{tabular}[c]{@{}l@{}}Attempts to address the approximation from standard DQN \cite{QRDQN} \\ using Qunatile Regression\end{tabular} & \begin{tabular}[c]{@{}l@{}}Reduces the approximation, which leads to performance \\ improvements in training stability.\end{tabular} & ATARI Games &  \\ \cline{2-3}
 & \multicolumn{1}{l|}{Gamma} & .99 &  &  &  &  \\ \hline
 & \multicolumn{1}{l|}{Learning Rate} & .0003 &  &  &  &  \\ \cline{2-3}
RPPO & \multicolumn{1}{l|}{Steps} & 128 & \begin{tabular}[c]{@{}l@{}}Introduces Limited Short Term Memory(LTSM), the input from\\ PPO is inputted into a deep network \cite{RPPO}. \end{tabular} &  & \begin{tabular}[c]{@{}l@{}}PendulumNoVel-v1\\ LunarLanderNoVel-v2\\ CartPoleNoVel-v1\\ CarRacing-v0\end{tabular} &  \\ \cline{2-3}
 & \multicolumn{1}{l|}{Gamma} & .99 &  &  &  &  \\ \hline
 & \multicolumn{1}{l|}{Learning Rate} & .001 &  &  &  &  \\ \cline{2-3}
TRPO & \multicolumn{1}{l|}{Steps} & 2048 & \begin{tabular}[c]{@{}l@{}}Continous gradient learner, sampling actions using a \\ stochastic policy \cite{TRPO-1}.\end{tabular} & \begin{tabular}[c]{@{}l@{}}More robust and can initially sample better than other \\ algorithms\end{tabular} & Mujoco &  \\ \cline{2-3}
 & \multicolumn{1}{l|}{Gamma} & .99 &  &  &  &  \\ \hline
\end{tabular}%
}
\caption{Hyperparameter Values for each algorithm.}
\label{tab:Hyp}
\end{table*}


This is also shown in the graph as shown that the Reccurent PPO or PPO LSTM, does not perform well, as it is feeding the learned variables through a neural network to try and determine some kind of features when in this case, the numerical values are not important to identifying the modifying agent. It can be compared to feeding noisy and unclear data and trying to determine something of value from it. However, this does not discount all neural net learns, it just depends on what is being fed into the network. 








\begin{comment}
    

As stated two different versions were tested one with a uniform reward function and one with a negative linear reward function. The findings in figure \ref{fig:Uni-Reward} show that the DQN not only stabilizes but also finds the optimal solution in the same amount of episodes with a uniform reward function. The figure also shows that the model is able to learn the optimal solutions after around 3000 episodes correctly identifying the state within a single step of the simulation. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{Figs-Revised/UniformRewards-cropped.pdf}
    \caption{Implementation of lower reward bound}
    \label{fig:Uni-Reward}
\end{figure}


Once the rewards for the environments were corrected, both models were retrained to gather more accurate results. It was found that the DQN would outperform the PPO after more iterations however, the PPO would converge closer to the optimal solution in fewer iterations. With this being said it also took longer for the PPO to train to 1000 iterations and began to plateau after 500 iterations so training further would not yield any better results. On the other hand, the DQN Model was quicker in completing its training of 3000 episodes and did yield better results. The training time for the PPO was around three hours while the DQN was around a single hour. 

After training was completed both Models were tested within the environment. Another 1000 episodes were run for both but no learning parameters were implemented just the prediction of each of the models. For the PPO due to variance, the testing showed that the model would select one action and not change if it was incorrect hitting the bottom bounds of the reward function. This was disregarded as it would not be an accurate representation of the algorithm's performance. However, when the DQN algorithm was implemented it boasted a 99.6 \% accuracy, correctly identifying the state on the first try 996 times out of 1000. 



\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{Figs-Revised/Training_Rewards_Update-cropped.pdf}
    \caption{Training Results with Optimal Solution}
    \label{fig:Uni-Reward}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{Figs-Revised/Comp-cropped.pdf}
    \caption{Comparison of DQN and PPO to DP Optimal Solution}
    \label{fig:Uni-Reward}
\end{figure}

\end{comment}



\section{Conclusion and Future Research Directions}

In this paper, a DRL was modeled to assign responsibility to management agents by correctly classifying an initiator of a network change. This was accomplished through the use of POMDP, leveraging the observation variables within an unknown state. The POMDP combined with a Graphical Modeling of a network aided in the creation of an accurate simulation space that can be modeled after any network. The main contribution is presenting a method to accurately assign responsibility based on changes within the network, this helps protect the user and creates accountability for vendors. 

This paper focused on simulation, but future work will follow the direction of implementing this into a network. The framework will need little adaptation but will see the creation of new network elements utilizing the most recent advancements being made in Zero Touch Networks. 



\bibliographystyle{IEEEtran}
\bibliography{refernces1}
\end{document}

