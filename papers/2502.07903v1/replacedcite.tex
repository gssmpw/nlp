\section{Related Works}
\textbf{LLM inference serving and disaggregated inference paradigm.} There are plenty of recent researches focused on optimizing LLM inference and serving ____. Among them, 
vLLM____ proposes paged-attention to improve the memory efficiency of the system.
Orca____ introduces continuous batching to improve inference throughput. 
AlpaServe____ adopts model parallelism to optimize LLM serving performance.
SARATHI____ introduces a chunked-prefill approach and piggybacks decoding requests to improve hardware utilization. 
Deja Vu____ predicts contextual sparsity on-the-fly and uses an asynchronous and hardware-aware implementation to enhance LLM inference. 
%Differently, our approach focuses on optimizing LLM inference and serving in heterogeneous environments.
On the other hand, many very recent works have been produced using disaggregated paradigm. 
Splitwise____ splits the prefill and decoding phases onto separate machines to optimize hardware utilization. 
DistServe____ further implements distinct parallel strategies for different phases. TetriInfer____ partitions prompts into fixed-size chunks and adopts a two-level scheduling algorithm to improve the performance of disaggregated inference. Mooncake____ features a KV cache-centric disaggregated architecture that enhances inference by fully leveraging the underutilized resources of GPU clusters, excelling in long-context scenarios. 
These works further confirm the effectiveness of the disaggregated architecture.




\textbf{Heterogeneous GPU computing.} 
Recent efforts have investigated diverse approaches to deploying LLMs in heterogeneous environments. 
LLM-PQ____ supports adaptive model quantization and phase-aware partitioning to boost LLM serving efficiency on heterogeneous GPU clusters. 
Helix____ formulates heterogeneous GPUs and network connections as a maxflow problem, and adopts a mixed integer linear programming algorithm to discover highly optimized strategies for serving LLMs. HexGen____ proposes asymmetric parallelism and an advanced scheduling algorithm to deploy generative inference in decentralized and heterogeneous environments. MÃ©lange____ formulates the GPU allocation task as a cost-aware bin packing problem and optimizes cost efficiency for LLM services by leveraging heterogeneous GPU types. 
Note that our work shares a similar objective and but is the first to adapt the disaggregated inference architecture for heterogeneous environments.


\vspace{-0.5em}