@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@misc{zhang2022opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{falcon180b,
     title={Falcon 180B},
    author={Technology Innovation Institute},
    url={https://falconllm.tii.ae/falcon-180b.html},
    year={2023}
}

@article{bubeck2023gpt4,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}


@misc{tensorrt_llm,
   title = {TensorRT-LLM},
   howpublished = {\url{https://github.com/NVIDIA/TensorRT-LLM}},
   year = {2023}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{strati2024ml,
  title={ML Training with Cloud GPU Shortages: Is Cross-Region the Answer?},
  author={Strati, Foteini and Elvinger, Paul and Kerimoglu, Tolga and Klimovic, Ana},
  booktitle={Proceedings of the 4th Workshop on Machine Learning and Systems},
  pages={107--116},
  year={2024}
}

@inproceedings{yang2023skypilot,
  title={$\{$SkyPilot$\}$: An intercloud broker for sky computing},
  author={Yang, Zongheng and Wu, Zhanghao and Luo, Michael and Chiang, Wei-Lin and Bhardwaj, Romil and Kwon, Woosuk and Zhuang, Siyuan and Luan, Frank Sifei and Mittal, Gautam and Shenker, Scott and others},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={437--455},
  year={2023}
}

@Misc{Nvida_hopper,
title = {NVIDIA Announces Hopper Architecture, the Next Generation of Accelerated Computing},
author={Nvidia},
url={https://nvidianews.nvidia.com/news/nvidia-announces-hopper-architecture-the-next-generation-of-accelerated-computing},
year = {2022},
}

@Misc{Nvida_turing,
title = {NVIDIA Reinvents Computer Graphics with Turing Architecture},
author={Nvidia},
url={https://nvidianews.nvidia.com/news/nvidia-reinvents-computer-graphics-with-turing-architecture},
year = {2018},
}

@Misc{Nvida_ampere,
title = {NVIDIAâ€™s New Ampere Data Center GPU in Full Production},
author={Nvidia},
url={https://nvidianews.nvidia.com/news/nvidias-new-ampere-data-center-gpu-in-full-production},
year = {2020},
}

@Misc{gpt4o,
title = {OpenAI GPT-4o},
author={OpenAI},
url={https://platform.openai.com/docs/models/gpt-4o},
year = {2024},
}

@Misc{runpod2023,
  title = {RunPod Documentation},
  author = {RunPod},
  year = {2023},
  url = {https://docs.runpod.io/},
  year = {2023},
}

@Misc{nccl2024,
  title = {NVIDIA Collective Communications Library (NCCL) Documentation},
  author = {NVIDIA},
  url = {https://developer.nvidia.com/nccl},
  year = {2024},
}

@Misc{claude3,
title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
author={Anthropic},
url={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
year = {2024},
}

@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{jianghexgen,
  title={HexGen: Generative Inference of Large Language Model over Heterogeneous Environment},
  author={Jiang, Youhe and Yan, Ran and Yao, Xiaozhe and Zhou, Yang and Chen, Beidi and Yuan, Binhang},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
}

@article{griggs2024m,
  title={M$\backslash$'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity},
  author={Griggs, Tyler and Liu, Xiaoxuan and Yu, Jiaxiang and Kim, Doyoung and Chiang, Wei-Lin and Cheung, Alvin and Stoica, Ion},
  journal={arXiv preprint arXiv:2404.14527},
  year={2024}
}

@article{zhao2024llm,
  title={LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization},
  author={Zhao, Juntao and Wan, Borui and Peng, Yanghua and Lin, Haibin and Wu, Chuan},
  journal={arXiv preprint arXiv:2403.01136},
  year={2024}
}

@inproceedings{miao2024spotserve,
  title={Spotserve: Serving generative large language models on preemptible instances},
  author={Miao, Xupeng and Shi, Chunan and Duan, Jiangfei and Xi, Xiaoli and Lin, Dahua and Cui, Bin and Jia, Zhihao},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={1112--1127},
  year={2024}
}

@inproceedings{zhong2024distserve,
  title={$\{$DistServe$\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={193--210},
  year={2024}
}

@inproceedings{patel2024splitwise,
  title={Splitwise: Efficient generative llm inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={118--132},
  year={2024},
  organization={IEEE}
}

@article{hu2024inference,
  title={Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
  journal={arXiv preprint arXiv:2401.11181},
  year={2024}
}

@article{qin2024mooncake,
  title={Mooncake: Kimi's KVCache-centric Architecture for LLM Serving},
  author={Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
  journal={arXiv preprint arXiv:2407.00079},
  year={2024}
}

@article{jin2024p,
  title={P/D-Serve: Serving Disaggregated Large Language Model at Scale},
  author={Jin, Yibo and Wang, Tao and Lin, Huimin and Song, Mingyang and Li, Peiyang and Ma, Yipeng and Shan, Yicheng and Yuan, Zhengfan and Li, Cailong and Sun, Yajing and others},
  journal={arXiv preprint arXiv:2408.08147},
  year={2024}
}


@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@inproceedings{li2023alpaserve,
  title={$\{$AlpaServe$\}$: Statistical multiplexing with model parallelism for deep learning serving},
  author={Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others},
  booktitle={17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages={663--679},
  year={2023}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}


@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@software{yao2023open,
author = {Yao, Xiaozhe},
month = sep,
title = {{Open Compute Framework: Peer-to-Peer Task Queue for Foundation Model Inference Serving}},
url = {https://github.com/autoai-org/OpenComputeFramework},
version = {0.0.1},
year = {2023}
}


@misc{libp2p,
    title={A modular network stack},
    author={LibP2P},
    url={https://libp2p.io/},
    year={2023}
}

@inproceedings{vaishali2018efficient,
  title={Efficient algorithms for a graph partitioning problem},
  author={Vaishali, S and Atulya, MS and Purohit, Nidhi},
  booktitle={International Workshop on Frontiers in Algorithmics},
  pages={29--42},
  year={2018},
  organization={Springer}
}

@article{agrawal2024taming,
  title={Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2403.02310},
  year={2024}
}

@inproceedings{liu2023deja,
  title={Deja vu: Contextual sparsity for efficient llms at inference time},
  author={Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others},
  booktitle={International Conference on Machine Learning},
  pages={22137--22176},
  year={2023},
  organization={PMLR}
}

@article{wu2023fast,
  title={Fast distributed inference serving for large language models},
  author={Wu, Bingyang and Zhong, Yinmin and Zhang, Zili and Huang, Gang and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2305.05920},
  year={2023}
}

@inproceedings{zhou2022pets,
  title={$\{$PetS$\}$: A unified framework for $\{$Parameter-Efficient$\}$ transformers serving},
  author={Zhou, Zhe and Wei, Xuechao and Zhang, Jiejing and Sun, Guangyu},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={489--504},
  year={2022}
}

@article{mei2024helix,
  title={Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs},
  author={Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi},
  journal={arXiv preprint arXiv:2406.01566},
  year={2024}
}

@article{hendrickson1995multi,
  title={A Multi-Level Algorithm For Partitioning Graphs.},
  author={Hendrickson, Bruce and Leland, Robert W and others},
  journal={SC},
  volume={95},
  number={28},
  pages={1--14},
  year={1995}
}

@inproceedings{alpert1995spectral,
  title={Spectral partitioning: The more eigenvectors, the better},
  author={Alpert, Charles J and Yao, So-Zen},
  booktitle={Proceedings of the 32nd annual ACM/IEEE design automation conference},
  pages={195--200},
  year={1995}
}

@article{kernighan1970efficient,
  title={An efficient heuristic procedure for partitioning graphs},
  author={Kernighan, Brian W and Lin, Shen},
  journal={The Bell system technical journal},
  volume={49},
  number={2},
  pages={291--307},
  year={1970},
  publisher={Nokia Bell Labs}
}


@article{cai2021graph,
  title={Graph coarsening with neural networks},
  author={Cai, Chen and Wang, Dingkang and Wang, Yusu},
  journal={arXiv preprint arXiv:2102.01350},
  year={2021}
}

@article{cheriyan1989analysis,
  title={Analysis of preflow push algorithms for maximum network flow},
  author={Cheriyan, Joseph and Maheshwari, SN},
  journal={SIAM Journal on Computing},
  volume={18},
  number={6},
  pages={1057--1086},
  year={1989},
  publisher={SIAM}
}

@misc{waissi1994network,
  title={Network flows: Theory, algorithms, and applications},
  author={Waissi, Gary R},
  year={1994},
  publisher={JSTOR}
}

@inproceedings{zhang2025sageattention,
      title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, 
      author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei},
      booktitle={International Conference on Learning Representations (ICLR)},
      year={2025}
}

@misc{zhang2024sageattention2,
      title={SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization}, 
      author={Jintao Zhang and Haofeng Huang and Pengle Zhang and Jia Wei and Jun Zhu and Jianfei Chen},
      year={2024},
      eprint={2411.10958},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.10958}, 
}

@misc{zhang2025spargeattn,
      title={SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference}, 
      author={Jintao Zhang and Chendong Xiang and Haofeng Huang and Haocheng Xi and Jia Wei and Jun Zhu and Jianfei Chen},
      year={2025}
}

@article{jiang2025demystifying,
  title={Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs},
  author={Jiang, Youhe and Fu, Fangcheng and Yao, Xiaozhe and He, Guoliang and Miao, Xupeng and Klimovic, Ana and Cui, Bin and Yuan, Binhang and Yoneki, Eiko},
  journal={arXiv preprint arXiv:2502.00722},
  year={2025}
}