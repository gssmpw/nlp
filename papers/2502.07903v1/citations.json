[
  {
    "index": 0,
    "papers": [
      {
        "key": "li2023alpaserve",
        "author": "Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others",
        "title": "$\\{$AlpaServe$\\}$: Statistical multiplexing with model parallelism for deep learning serving"
      },
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      },
      {
        "key": "agrawal2024taming",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "Taming throughput-latency tradeoff in llm inference with sarathi-serve"
      },
      {
        "key": "liu2023deja",
        "author": "Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others",
        "title": "Deja vu: Contextual sparsity for efficient llms at inference time"
      },
      {
        "key": "wu2023fast",
        "author": "Wu, Bingyang and Zhong, Yinmin and Zhang, Zili and Huang, Gang and Liu, Xuanzhe and Jin, Xin",
        "title": "Fast distributed inference serving for large language models"
      },
      {
        "key": "zhou2022pets",
        "author": "Zhou, Zhe and Wei, Xuechao and Zhang, Jiejing and Sun, Guangyu",
        "title": "$\\{$PetS$\\}$: A unified framework for $\\{$Parameter-Efficient$\\}$ transformers serving"
      },
      {
        "key": "yu2022orca",
        "author": "Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon",
        "title": "Orca: A distributed serving system for $\\{$Transformer-Based$\\}$ generative models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yu2022orca",
        "author": "Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon",
        "title": "Orca: A distributed serving system for $\\{$Transformer-Based$\\}$ generative models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023alpaserve",
        "author": "Li, Zhuohan and Zheng, Lianmin and Zhong, Yinmin and Liu, Vincent and Sheng, Ying and Jin, Xin and Huang, Yanping and Chen, Zhifeng and Zhang, Hao and Gonzalez, Joseph E and others",
        "title": "$\\{$AlpaServe$\\}$: Statistical multiplexing with model parallelism for deep learning serving"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "agrawal2024taming",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "Taming throughput-latency tradeoff in llm inference with sarathi-serve"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2023deja",
        "author": "Liu, Zichang and Wang, Jue and Dao, Tri and Zhou, Tianyi and Yuan, Binhang and Song, Zhao and Shrivastava, Anshumali and Zhang, Ce and Tian, Yuandong and Re, Christopher and others",
        "title": "Deja vu: Contextual sparsity for efficient llms at inference time"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "patel2024splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\\'I}{\\~n}igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "Splitwise: Efficient generative llm inference using phase splitting"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhong2024distserve",
        "author": "Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao",
        "title": "$\\{$DistServe$\\}$: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hu2024inference",
        "author": "Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others",
        "title": "Inference without interference: Disaggregate llm inference for mixed downstream workloads"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "qin2024mooncake",
        "author": "Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran",
        "title": "Mooncake: Kimi's KVCache-centric Architecture for LLM Serving"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhao2024llm",
        "author": "Zhao, Juntao and Wan, Borui and Peng, Yanghua and Lin, Haibin and Wu, Chuan",
        "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "mei2024helix",
        "author": "Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi",
        "title": "Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "jianghexgen",
        "author": "Jiang, Youhe and Yan, Ran and Yao, Xiaozhe and Zhou, Yang and Chen, Beidi and Yuan, Binhang",
        "title": "HexGen: Generative Inference of Large Language Model over Heterogeneous Environment"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "griggs2024m",
        "author": "Griggs, Tyler and Liu, Xiaoxuan and Yu, Jiaxiang and Kim, Doyoung and Chiang, Wei-Lin and Cheung, Alvin and Stoica, Ion",
        "title": "M$\\backslash$'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity"
      }
    ]
  }
]