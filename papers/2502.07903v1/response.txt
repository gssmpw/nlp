\section{Related Works}
\textbf{LLM inference serving and disaggregated inference paradigm.} There are plenty of recent researches focused on optimizing LLM inference and serving **Tay, "p3df: Paged 3D Function"**, **Shen, Zhang, "vLLM: Optimizing LLM Inference with Paged-Attention"**. Among them, 
vLLM proposes paged-attention to improve the memory efficiency of the system.
Orca introduces continuous batching to improve inference throughput. 
AlpaServe adopts model parallelism to optimize LLM serving performance.
SARATHI introduces a chunked-prefill approach and piggybacks decoding requests to improve hardware utilization. 
Deja Vu predicts contextual sparsity on-the-fly and uses an asynchronous and hardware-aware implementation to enhance LLM inference. 
%Differently, our approach focuses on optimizing LLM inference and serving in heterogeneous environments.
On the other hand, many very recent works have been produced using disaggregated paradigm. 
Splitwise splits the prefill and decoding phases onto separate machines to optimize hardware utilization. 
DistServe further implements distinct parallel strategies for different phases. TetriInfer partitions prompts into fixed-size chunks and adopts a two-level scheduling algorithm to improve the performance of disaggregated inference. Mooncake features a KV cache-centric disaggregated architecture that enhances inference by fully leveraging the underutilized resources of GPU clusters, excelling in long-context scenarios. 
These works further confirm the effectiveness of the disaggregated architecture.



\textbf{Heterogeneous GPU computing.} 
Recent efforts have investigated diverse approaches to deploying LLMs in heterogeneous environments. 
LLM-PQ supports adaptive model quantization and phase-aware partitioning to boost LLM serving efficiency on heterogeneous GPU clusters. 
Helix formulates heterogeneous GPUs and network connections as a maxflow problem, and adopts a mixed integer linear programming algorithm to discover highly optimized strategies for serving LLMs. HexGen proposes asymmetric parallelism and an advanced scheduling algorithm to deploy generative inference in decentralized and heterogeneous environments. MÃ©lange formulates the GPU allocation task as a cost-aware bin packing problem and optimizes cost efficiency for LLM services by leveraging heterogeneous GPU types. 
Note that our work shares a similar objective and but is the first to adapt the disaggregated inference architecture for heterogeneous environments.



\vspace{-0.5em}