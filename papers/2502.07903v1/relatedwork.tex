\section{Related Works}
\textbf{LLM inference serving and disaggregated inference paradigm.} There are plenty of recent researches focused on optimizing LLM inference and serving ~\citep{li2023alpaserve,kwon2023efficient,agrawal2024taming,liu2023deja,wu2023fast,zhou2022pets,yu2022orca}. Among them, 
vLLM~\citep{kwon2023efficient} proposes paged-attention to improve the memory efficiency of the system.
Orca~\citep{yu2022orca} introduces continuous batching to improve inference throughput. 
AlpaServe~\citep{li2023alpaserve} adopts model parallelism to optimize LLM serving performance.
SARATHI~\citep{agrawal2024taming} introduces a chunked-prefill approach and piggybacks decoding requests to improve hardware utilization. 
Deja Vu~\citep{liu2023deja} predicts contextual sparsity on-the-fly and uses an asynchronous and hardware-aware implementation to enhance LLM inference. 
%Differently, our approach focuses on optimizing LLM inference and serving in heterogeneous environments.
On the other hand, many very recent works have been produced using disaggregated paradigm. 
Splitwise~\citep{patel2024splitwise} splits the prefill and decoding phases onto separate machines to optimize hardware utilization. 
DistServe~\citep{zhong2024distserve} further implements distinct parallel strategies for different phases. TetriInfer~\citep{hu2024inference} partitions prompts into fixed-size chunks and adopts a two-level scheduling algorithm to improve the performance of disaggregated inference. Mooncake~\citep{qin2024mooncake} features a KV cache-centric disaggregated architecture that enhances inference by fully leveraging the underutilized resources of GPU clusters, excelling in long-context scenarios. 
These works further confirm the effectiveness of the disaggregated architecture.




\textbf{Heterogeneous GPU computing.} 
Recent efforts have investigated diverse approaches to deploying LLMs in heterogeneous environments. 
LLM-PQ~\citep{zhao2024llm} supports adaptive model quantization and phase-aware partitioning to boost LLM serving efficiency on heterogeneous GPU clusters. 
Helix~\citep{mei2024helix} formulates heterogeneous GPUs and network connections as a maxflow problem, and adopts a mixed integer linear programming algorithm to discover highly optimized strategies for serving LLMs. HexGen~\citep{jianghexgen} proposes asymmetric parallelism and an advanced scheduling algorithm to deploy generative inference in decentralized and heterogeneous environments. MÃ©lange~\citep{griggs2024m} formulates the GPU allocation task as a cost-aware bin packing problem and optimizes cost efficiency for LLM services by leveraging heterogeneous GPU types. 
Note that our work shares a similar objective and but is the first to adapt the disaggregated inference architecture for heterogeneous environments.


\vspace{-0.5em}