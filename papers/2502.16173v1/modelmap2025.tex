\pdfoutput=1
\documentclass[11pt]{article}

\usepackage[whole]{bxcjkjatype}

\usepackage[preprint]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

\usepackage{microtype}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{array}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{stmaryrd}
\usepackage{siunitx}

\usepackage{bm}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bmxi}{\bm{\xi}}
\newcommand{\bmzeta}{\bm{\zeta}}
\newcommand{\bmet}{\bm{\eta}}
\newcommand{\bmt}{\bm{\theta}}
\newcommand{\bmell}{\bm{\ell}}
\newcommand{\bmq}{\bm{q}}
\newcommand{\bmQ}{\bm{Q}}
\newcommand{\bmL}{\bm{L}}
\newcommand{\bmb}{\bm{b}}
\newcommand{\bme}{\bm{e}}
\newcommand{\bmu}{\bm{u}}
\newcommand{\bma}{\bm{a}}
\newcommand{\bmzero}{\bm{0}}
\newcommand{\bmone}{\bm{1}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\bbE}{\mathbb{E}}

\usepackage{longtable}

\title{Mapping 1,000+ Language Models via the Log-Likelihood Vector}

\author{
Momose Oyama${}^{1,2}$\quad Hiroaki Yamagiwa${}^{1}$\quad Yusuke Takase${}^{1}$ \quad Hidetoshi Shimodaira${}^{1,2}$\\
${}^{1}$Kyoto University\quad
${}^{2}$RIKEN\\
\texttt{\{oyama.momose, hiroaki.yamagiwa, y.takase\}@sys.i.kyoto-u.ac.jp},\\
\texttt{shimo@i.kyoto-u.ac.jp}
}

\begin{document}
\maketitle

\begin{abstract}
To compare autoregressive language models at scale, we propose using log-likelihood vectors computed on a predefined text set as model features.  
This approach has a solid theoretical basis: when treated as model coordinates, their squared Euclidean distance approximates the Kullbackâ€“Leibler divergence of text-generation probabilities.  
Our method is highly scalable, with computational cost growing linearly in both the number of models and text samples, and is easy to implement as the required features are derived from cross-entropy loss.  
Applying this method to over 1,000 language models, we constructed a ``model map,'' providing a new perspective on large-scale model analysis.
\end{abstract}

\section{Introduction} \label{sec:introduction}

Language models have been evolving rapidly, and their community has expanded significantly.  
To understand this landscape and its future directions, it is essential to systematically analyze model similarity and positioning based on language modeling principles.  
On the Hugging Face Hub, models are categorized by name and attributes, while other studies assess similarity based on outputs~\cite{yax2024phylolminferringphylogeny} or activations~\cite{zhou-etal-2025-linguistic}. 
Leaderboards~\cite{open-llm-leaderboard, open-llm-leaderboard-v2, chiang2024chatbot} are commonly used to assess model standings.

Since language models are probability models, we propose representing each model using coordinates that capture the geometric structure of the space of probability distributions.
Concretely, we define a language model's coordinates as its log-likelihood vector across a large collection of texts.  
Figure~\ref{fig:model-map-tsne-intro} shows a model map obtained through dimensionality reduction applied to the coordinates of 1,018 language models.  
This visualization reveals that models of the same type tend to cluster together, while models in close proximity often share the same primary text category, forming a continuous distribution across the map.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/figure1_v13.pdf}

\caption{
Map of 1,018 language models. Their log-likelihood vectors are visualized using t-SNE.  
(Top) Colors indicate model types.  
(Bottom) Colors indicate the model's ``primary text category,'' the text category where the model achieves the highest standardized log-likelihood score among 17 text categories.  
See Section~\ref{sec:model-map} for details.
}

    \label{fig:model-map-tsne-intro}
\end{figure}

\begin{table*}[!t]
\centering
\begin{adjustbox}{width=\linewidth}
 \begin{tabular}{lrlrlrlrlrlrlrlrlrlrlrlr}
  \toprule
meta-llama/Meta-Llama-3-8B & KL (/byte) & google/codegemma-2b & KL (/byte) & deepseek-ai/deepseek-llm-7b-base & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}\cmidrule(lr){5-6}
Undi95/Meta-Llama-3-8B-hf & 0.003 (0.000) & deepseek-ai/deepseek-coder-1.3b-instruct & 1654.767 (2.455) & deepseek-ai/deepseek-moe-16b-base & 130.715 (0.194) \\
dfurman/Llama-3-8B-Orpo-v0.1 & 6.994 (0.010) & bigcode/starcoderbase-1b & 1812.768 (2.690) & deepseek-ai/deepseek-llm-7b-chat & 245.408 (0.364) \\
migtissera/Tess-2.0-Llama-3-8B & 28.829 (0.043) & deepseek-ai/deepseek-coder-1.3b-base & 2115.462 (3.139) & deepseek-ai/deepseek-moe-16b-chat & 247.889 (0.368) \\
freewheelin/free-llama3-dpo-v0.2 & 67.783 (0.101) & bigcode/gpt\_bigcode-santacoder & 2644.348 (3.924) & deepseek-ai/DeepSeek-V2-Lite & 306.530 (0.455) \\
jondurbin/bagel-8b-v1.0 & 110.365 (0.164) & deepseek-ai/deepseek-coder-6.7b-instruct & 2675.004 (3.969) & deepseek-ai/ESFT-vanilla-lite & 307.487 (0.456) \\
migtissera/Llama-3-8B-Synthia-v3.5 & 128.141 (0.190) & Qwen/CodeQwen1.5-7B-Chat & 2759.243 (4.094) & deepseek-ai/DeepSeek-V2-Lite-Chat & 585.090 (0.868) \\
nvidia/Llama3-ChatQA-1.5-8B & 128.776 (0.191) & NTQAI/Nxcode-CQ-7B-orpo & 2767.578 (4.106) & mistralai/Mistral-7B-Instruct-v0.1 & 913.708 (1.356) \\
ruslanmv/Medical-Llama3-8B & 138.566 (0.206) & Salesforce/codegen-6B-multi & 2859.701 (4.243) & statking/zephyr-7b-sft-full-orpo & 1089.127 (1.616) \\
FairMind/Llama-3-8B-4bit-UltraChat-Ita & 199.748 (0.296) & bigcode/starcoderbase-7b & 2990.326 (4.437) & Severian/ANIMA-Phi-Neptune-Mistral-7B & 1140.161 (1.692) \\
NousResearch/Hermes-2-Theta-Llama-3-8B & 222.357 (0.330) & deepseek-ai/deepseek-coder-6.7b-base & 3282.352 (4.870) & sethuiyer/Medichat-Llama3-8B & 1157.599 (1.718) \\
  \bottomrule
 \end{tabular}
 \end{adjustbox}

    \caption{
Top 10 nearest neighbors among the 1,018 language models for each model listed in the first row.
The KL divergence is computed using formula~(\ref{eq:main-KL-xi}) in Section~\ref{sec:model-text}.
The values in parentheses represent the KL divergence measured in bits per byte of text (Section~\ref{sec:byte-normalized-KL}), obtained by multiplying the KL divergence by 0.001484.
Tables for the nearest neighbors of the models labeled in the top panel of Fig.~\ref{fig:model-map-tsne-intro} are provided in Appendix~\ref{sec:top10-tables}.
}

 \label{tab:nearest-neighbor-models}
\end{table*}

We find that the distances in our defined coordinate system accurately capture relationships among language models. On this map, each point represents a single model, with those having similar text-generation probability distributions appearing closer together and those with more distinct distributions positioned farther apart. In Section~\ref{sec:model-text}, we show that the squared Euclidean distance in this coordinate system approximates the Kullback-Leibler (KL) divergence among models. Table~\ref{tab:nearest-neighbor-models} lists the nearest neighbors for each language model. For example, many of the closest neighbors of \texttt{meta-llama/Meta-Llama-3-8B}~\cite{llama3modelcard} also contain \texttt{Llama-3} in their names.

Several studies have explored methods for comparing language models (see Appendix~\ref{sec:related-work}).  
In particular, prior work on comparing generated text includes approaches that construct phylogenetic trees based on model-generated text~\citep{yax2025phylolm}  
and approaches that measure differences in text-generation probabilities conditioned on given prompts using KL divergence~\citep{melamed-etal-2024-prompts}.  
However, these methods require generating text with each model, thus incurring the cost of pairwise distance computations, which becomes prohibitively expensive at large scale.  
By contrast, our method does not involve actual text generation; instead, we compute generation probabilities on a predefined text corpus.  
This enables us to derive model coordinates without pairwise comparisons, allowing efficient large-scale comparison of many models.  

To gain insights from visualizing model attributes on the model map, in Section~\ref{sec:model-map}, we analyze various attributes\footnote{Attributes include model type, primary text category, model performance, model size, and creation date.} and their relationships. Additionally, by comparing log-likelihood and benchmark performance, we demonstrate the ability to detect data leakage. Then, in Section~\ref{sec:predict-performance}, we treat the log-likelihood vector as a feature and show how it can predict benchmark performance. Finally, in Section~\ref{sec:confirmation}, we validate the theoretical relationship between model coordinates and KL divergence through experiments.


\section{Mapping Language Models into the Space of Text Probability Distributions} \label{sec:model-text}

In this section, we present our proposed method.  
Sections~\ref{sec:loglikelihood-vector} and \ref{sec:double-centering} introduce model feature vectors derived from text-generation probabilities.  Section~\ref{sec:theorem-KL-text} demonstrates that the squared Euclidean distance in the coordinate system built using these features approximates the KL divergence between models.  
Section~\ref{sec:model-coordinates} offers an interpretation of the resulting model coordinates.  
An extension of this method, which defines model coordinates using the sequence of conditional probabilities for generating a given token sequence, is presented in Appendix~\ref{sec:model-token}.

\subsection{Autoregressive language models} \label{sec:language-models}

Let \(\mathcal{X}\) be the set of all possible texts, and let \(\mathcal{V}\) be the token vocabulary.  
A text \(x \in \mathcal{X}\) is represented as a sequence of tokens:
\[
x = (y_1, \dots, y_n), \quad y_t \in \mathcal{V}.
\]
Denoting the maximum text length by \(n_{\textrm{max}}\), we have
\(\mathcal{X} = \bigcup_{n=0}^{n_{\textrm{max}}} \mathcal{V}^n.\)
We consider a set of \(K\) language models \(\{p_i\}_{i=1}^K\).  
With \(y_0\) denoting the beginning-of-sequence (BOS) token, each language model \(p_i\) predicts the next token \(y_t\) given the preceding token sequence \(y^{t-1} = (y_0, \dots, y_{t-1})\).  
Thus, the conditional probability defined by \(p_i\) is given by
\[
y_t \sim p_i(y_t \mid y^{t-1}), \quad t = 1, \dots, n.
\]
Accordingly, the probability of a text \(x\) under model \(p_i\), denoted \(x \sim p_i\), is
\[
p_i(x) = \prod_{t=1}^{n} p_i(y_t \mid y^{t-1}).
\]
In addition to the \(K\) language models \(p_1, \dots, p_K\), we introduce a language model \(p_0\) that represents an underlying distribution for theoretical purposes.  
We assume we have a dataset (corpus)
\[
D = (x_1, x_2, \dots, x_N) \in \mathcal{X}^N,
\]
consisting of \(N\) texts, where each text is independently drawn from \(p_0\).

\subsection{Log-likelihood vector}\label{sec:loglikelihood-vector}

For a model \(p_i\), the probability of generating a text \(x\) is denoted \(p_i(x)\).  
Following the convention in statistical model selection, we refer to \(p_i(x)\) as the likelihood of model \(p_i\) given the text \(x\).  
The log-likelihood is then defined as
\[
\ell_i(x) = \sum_{t=1}^n \log p_i(y_t \mid y^{t-1}).
\]
In language model implementations, \(-\ell_i(x)\) corresponds to the cross-entropy loss for the text \(x\), and \(\exp(-\ell_i(x)/n)\) is known as the perplexity.

Our approach is straightforward. Given that the dataset \(D\) consists of \(N\) texts, we use the log-likelihood vector
\[
\bmell_i = (\ell_i(x_1), \dots, \ell_i(x_N))^\top \in \mathbb{R}^N
\]
as the feature vector for model \(p_i\).  
The first step in our model analysis is to construct the log-likelihood matrix
\[
\bm{L} = (\bmell_1, \dots, \bmell_K)^\top \in \mathbb{R}^{K \times N}
\]
by stacking the vectors \(\bmell_i\) for the \(K\) models.

\subsection{Double centering} \label{sec:double-centering}

As a preprocessing step for model analysis, we apply a technique called double centering~\cite{borg2005modern} to \(\bm{L}\).  
First, we perform row-wise centering.  
The mean of each row, referred to as the mean log-likelihood, is given by
\[
    \bar \ell_i = \sum_{s=1}^N \ell_i(x_s)/N.
\]
Subtracting this value from each component of \(\bmell_i\), we define the centered log-likelihood vector \(\bmxi_i = (\xi_{i1},\ldots,\xi_{iN})^\top \in \mathbb{R}^N\), where
\[
\xi_{is} := \ell_i(x_s) - \bar \ell_i, \quad s=1,\ldots,N.
\]
Next, we apply column-wise centering to the matrix of centered feature vectors \((\bmxi_1,\ldots,\bmxi_K)^\top\).  
The mean vector is
\(
\bar{\bmxi} = \frac{1}{K} \sum_{i=1}^K \bmxi_i,
\)
and by subtracting this vector from each \(\bmxi_i\), we define the double-centered log-likelihood vector
\[
\bmq_i = \bmxi_i - \bar{\bmxi}.
\]
For further details, see Appendices~\ref{sec:theory-double-centering} and \ref{sec:error-analysis}.

\subsection{Kullback-Leibler divergence}\label{sec:theorem-KL-text}

The Kullback-Leibler (KL) divergence is often used to measure how far apart two models \(p_i\) and \(p_j\) are in the space of probability distributions.\footnote{\(\mathbb{E}(\cdot)\) denotes expectation and \(\mathrm{Var}(\cdot)\) denotes variance.} It is defined as
\begin{align}
\mathrm{KL}(p_i,p_j) &= \sum_{x\in\mathcal{X}} p_i(x)\log\frac{p_i(x)}{p_j(x)} \nonumber\\
&= \mathbb{E}_{x\sim p_i}\bigl(\ell_i(x) - \ell_j(x)\bigr).
\label{eq:main-KL-def}
\end{align}
We assume the dataset \(D\) is generated from an unknown underlying model \(p_0\) and that the models \(p_i\) and \(p_j\) provide good approximations of \(p_0\). Under this assumption, the KL divergence can be approximated as follows:
\begin{align}
2 \,\mathrm{KL}(p_i,p_j)\approx
\mathrm{Var}_{x\sim p_0}\bigl(
\ell_i(x)-\ell_j(x)
\bigr).
\label{eq:main-KL-var-model}
\end{align}
While the definition of KL divergence in \eqref{eq:main-KL-def} involves the expectation of \(\ell_i(x) - \ell_j(x)\), the approximation in \eqref{eq:main-KL-var-model} takes the form of a variance. This result is somewhat surprising yet quite insightful.  
Notably, although KL divergence is not symmetric in the two models, the approximation in \eqref{eq:main-KL-var-model} is symmetric.  
We estimate \eqref{eq:main-KL-var-model} from the dataset \(D\) as
\begin{align}
2\,\mathrm{KL}(p_i,p_j) \approx \|\bmq_i - \bmq_j\|^2/N.
\label{eq:main-KL-xi}
\end{align}
Thus, if we regard the model coordinates of \(p_i\) as \(\bmq_i/\sqrt{N}\) by scaling with \(N\), then the squared Euclidean distance between two points approximates \(2\,\mathrm{KL}(p_i,p_j)\).  

The main results, namely \eqref{eq:main-KL-var-model} and \eqref{eq:main-KL-xi}, are proved in Appendix~\ref{sec:theory-text} using the theory of exponential family of distributions~\citep{barndorff2014information,efron1978geometry,efron2022exponential,amari1982-10.1214/aos/1176345779}, similar to the discussion on the relationship between the norm of embeddings and KL divergence~\citep{oyama-etal-2023-norm}.  
Although the concepts of model map and model coordinates have been discussed in statistics~\citep{Shimodaira1993modelmap,shimodaira1998graphical,shimodaira2001multiple}, and there have been a few applications of model maps~\citep{Shimodaira2005,shimodaira2019selective}, they have seldom been utilized in practice.

\subsection{Model coordinates}\label{sec:model-coordinates}

We primarily use \(\bmq_i\) as the feature vector of model \(p_i\) and refer to it as the model coordinates.  
As shown in \eqref{eq:main-KL-xi}, the squared Euclidean distance in the \(\bmq\)-coordinate system approximates the KL divergence between language models,\footnote{That is, \(\|\bmq_i-\bmq_j\|^2 \approx 2N\,\mathrm{KL}(p_i,p_j)\). For simplicity, we omit the constant scaling factor in expressions of this type.}  
indicating that \(\bmq_i\) represents the position of \(p_i\) in the space of probability distributions.  
Since \(\bmxi_i\) differs from \(\bmq_i\) only by an offset from the origin, \(\bmxi_i\) also serves as a model coordinate, and \(\|\bmq_i - \bmq_j\|^2 = \|\bmxi_i - \bmxi_j\|^2\).  
However, we prefer \(\bmq_i\) for its more interpretable components and thus adopt it throughout this paper.

For visualization purposes, we mainly use \(\bmell_i\) as the coordinates of the model map, as \(\bmell_i\) can be intuitively interpreted as encoding \(\sqrt{N}\,\bar{\ell}_i\) in the ``height'' dimension and \(\bmq_i\) in the ``horizontal'' dimensions.  
As shown in Appendix~\ref{sec:theory-height},
\begin{align}
    \|\bmell_i - \bmell_j\|^2
   =  \|\bmq_i - \bmq_j\|^2 + N(\bar \ell_i - \bar \ell_j)^2,
\label{eq:lik-decomposition}
\end{align}
which means the squared Euclidean distance in the \(\bmell\)-coordinate system can be decomposed into the sum of \(2N\,\mathrm{KL}(p_i,p_j)\) and \(N\,(\bar{\ell}_i - \bar{\ell}_j)^2\).

\section{Experimental Setup} \label{sec:experiment-settings}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/figure2_text-embeddings_v6.pdf}

    \caption{
    Text embeddings for 10,000 texts in dataset $D$, computed via \texttt{simcse-roberta-large}~\cite{gao2021simcse} and visualized with t-SNE.  
    Colors indicate 17 text categories.
    }

\label{fig:dataset-visualize}

\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/figure3_matrixQ_v7.pdf}

    \caption{
    The double-centered log-likelihood matrix \(\bm{Q}\), with rows and columns reordered by hierarchical clustering.  
    Each row corresponds to one of the 1,018 models, color-coded by model type.  
    Each column represents one of the 10,000 texts, color-coded by text category.
    }
\label{fig:matrix-Q}
\end{figure*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/figure4_hc100_v10.pdf}

    \caption{
    Hierarchical clustering of the top 100 most-downloaded models, based on their feature vectors \(\bm{q}_i\).  
    Model names are color-coded by model type.
    }
    \label{fig:cluster-100-models}
\end{figure}

We describe the key components of our experiment. 
In particular, Section~\ref{sec:text-set} explains the procedure for selecting the 10,000 texts used to compute the model coordinates, 
and Section~\ref{sec:models} discusses how we selected the 1,018 language models. 
Further details are given in Appendix~\ref{sec:experiment-details}.

\subsection{Selection of text data} \label{sec:text-set}

The texts used for computing the language models' coordinates were extracted from the Pile~\cite{arxiv:2101.00027},  
with five categories of copyrighted material removed.\footnote{\url{https://huggingface.co/datasets/monology/pile-uncopyrighted}}  
This yielded a dataset \(D\) consisting of 10,000 texts, each tagged with a category label from the Pile.  
Figure~\ref{fig:dataset-visualize} visualizes these texts.

To build the dataset, we began by dividing the first 1M texts from the Pile Uncopyrighted corpus into 1,024-byte chunks (UTF-8 encoded).  
In cases where decoding errors occurred, we truncated by one byte at a time.  
Chunks smaller than 256 bytes were discarded, resulting in about 5.7M valid chunks.  
From these, we randomly sampled 10,000 texts to create the final dataset used for computing model coordinates.
The average length of these 10,000 texts was 972.3188 bytes.

\subsection{Selection of language models} \label{sec:models}

We used \(K = 1{,}018\) language models in total. Of these, 1,000 were selected from models listed on Open LLM Leaderboard v1. Specifically, we considered CausalLM models ranging from 1B to 13B parameters and ranked them by their number of downloads over the 30 days preceding February 1, 2025.\footnote{Hugging Face's API provides the total number of downloads in the past 30 days.} We initially selected the top 1,100 models by download count and attempted log-likelihood calculations. Among these, 1,011 successfully produced valid log-likelihood values, and we chose the 1,000 most frequently downloaded from that set.  
In addition, we included 18 models from the DeepSeek language model series. We obtained information on model parameter sizes and architectures from the Leaderboard. Appendix~\ref{sec:experiment-details} provides basic information on the selected models and details on how model types were defined. A complete list of models used in this study is given in Appendix~\ref{app:model_list}.

\subsection{Computation of the log-likelihood} \label{sec:logp-computation}

The log-likelihood matrix \(\bm{L}\) was computed in float16 precision,
with the bottom 2\% of values clipped.  
This clipping mitigates the large impact of extremely low likelihoods on \eqref{eq:main-KL-xi}.  
After computing \(\bm{L}\), we applied row-wise and column-wise centering to obtain the double-centered log-likelihood matrix \(\bm{Q}\).
Figure~\ref{fig:matrix-Q} visualizes \(\bm{Q}\).  
Each value in the matrix can be interpreted in two ways: as the relative probability of a text for each model or as the relative likelihood of a model for each text.  
Both models and texts exhibit clustering patterns.  
Figure~\ref{fig:cluster-100-models} shows a dendrogram of the top 100 models.  
We examined the effective dimension from the perspective of feature vector dimensionality reduction and found that the cumulative contribution ratio, based on the sum of squared singular values of \(\bm{Q}\), reached 90\% at 42 dimensions and 95\% at 82 dimensions.

\subsection{Obtaining the leaderboard scores}\label{sec:leaderboard_scores}

We obtained benchmark scores for the language models used in our experiments from Open LLM Leaderboard v1.\footnote{\url{https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard}}  
Between April 2023 and June 2024, this leaderboard evaluated language models on six tasks:  
AI2 Reasoning Challenge (ARC)~\cite{arxiv:1803.05457}, HellaSwag~\cite{arxiv:1905.07830}, MMLU~\cite{arxiv:2009.03300}, TruthfulQA~\cite{arxiv:2109.07958}, Winogrande~\cite{arxiv:1907.10641}, and GSM8K~\cite{arxiv:2110.14168}.  
Along with individual task scores, we also use the average score across all six tasks, referred to as 6-TaskMean.

\subsection{Byte-normalized KL divergence for cross-experiment comparison} 
\label{sec:byte-normalized-KL}
The KL divergence of text generation, $\textrm{KL}(p_i, p_j)$, generally increases with the number of tokens in the text. As a result, it cannot be directly compared with the KL divergence from other experiments using different text data. For models that use the same tokenizer, one can normalize the KL divergence by the average number of tokens in the text to obtain the KL divergence per token. However, when comparing KL divergence between models with different tokenizers, as in this study, it is more appropriate to normalize by the average text length in bytes and use the KL divergence per byte. For instance, if $\textrm{KL}(p_i, p_j) = 1{,}000$, dividing by the average text length of 972.3188 bytes yields a KL divergence per byte of $1{,}000 / 972.3 = 1.028$ nats $= 1.484$ bits\footnote{To convert the code length unit from nats to bits, multiply by \( 1/\log 2 = 1.4427 \).}.

KL divergence can be understood from the perspective of coding theory as a measure of how much longer a message becomes when encoded using an incorrect probability distribution. $\textrm{KL}( p_i, p_j )$ represents the extra code length required when encoding text generated from probability distribution \( p_i \) using a different distribution \( p_j \). Dividing this value by the average text length in bytes gives the additional code length per byte. In the example above, this means that, due to differences between the models, an extra 1.484 bits are needed to encode each byte of text.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/figure5_performance_v6.pdf}

    \caption{
    Model maps illustrating model performance.  
    From left to right, the panels show each model's mean log-likelihood, 6-TaskMean score, and the ``primary task,'' meaning the task for which each model achieves its highest standardized score among the six tasks (Appendix~\ref{sec:standard-score}).  
    The color bar is clipped at the 10th percentile for mean log-likelihood and 6-TaskMean, with darker colors indicating better performance.  
    In the primary task panel, models with standardized scores below zero on all six tasks are labeled ``All Under 0.''
    }

    \label{fig:modelmap-logp-score-task}
\end{figure*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/modelmap-size-date_v5.pdf}
    \caption{A model map color-coded by (Left) model size and (Right) model creation date.}

    \label{fig:size-date}
\end{figure}


\section{Map of Language Models} \label{sec:model-map}

We applied t-SNE~\cite{vanderMaaten-2008-tsne} to the log-likelihood matrix $\bm{L}$ for dimensionality reduction\footnote{The perplexity value was set to 30, and we used the scikit-learn implementation~\citep{DBLP:journals/sigmobile/VaroquauxBLGPM15}.}.  
Using this visualization, we analyze the insights gained from the model map in this section.  
While this paper presents model maps using $\bm{L}$, alternative maps using the double-centered log-likelihood matrix $\bm{Q}$, as well as a model map with labels for all language models, are available in Appendix~\ref{sec:add-model-map}.

\subsection{Visualizing attributes on the model map} \label{sec:model-map-discussion}

\paragraph{Model type.}

The top panel of Fig.~\ref{fig:model-map-tsne-intro} visualizes the distribution of model types, with each model color-coded according to its type.  
We observe that models belonging to the same type tend to cluster together, forming distinct regions on the map (e.g., \texttt{llama-2}, \texttt{mistral}, and \texttt{gemma}).  
In particular, models optimized for coding tasks\footnote{For example, \texttt{starcoder2-7b}~\citep{arxiv:2402.19173}, \texttt{deepseek-coder-1.3b-base}~\citep{arxiv:2401.14196}, \texttt{codegemma-2b}~\citep{arxiv:2406.11409} and \texttt{CodeLlama-7b-Instruct-hf}~\citep{arxiv:2308.12950}.} appear in a relatively compact region, suggesting that these models share notable similarities in their probability distributions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/the-Pile_v5.pdf}

    \caption{(Left) Models tagged with ``the Pile.'' (Right) Difference between the standardized mean log-likelihood and the standardized 6-TaskMean score.}

    \label{fig:the-pile}
\end{figure}

\paragraph{Text category.}

The bottom panel of Fig.~\ref{fig:model-map-tsne-intro} shows the model map with each model color-coded according to the text category in which it achieves the highest
standardized log-likelihood score (Appendix~\ref{sec:standard-score}). 
From this figure, we see that models exhibiting high likelihoods for the same text category are grouped together.  
Notably, the cluster containing coding-specialized models in the top panel aligns with the GitHub/StackExchange region in the bottom panel, suggesting that these models have relatively high likelihoods for text originating from GitHub and StackExchange.

\paragraph{Model performance.}

Figure~\ref{fig:modelmap-logp-score-task} visualizes two evaluation metrics: mean log-likelihood and benchmark task performance.  
From the left and central panels, we see that both metrics exhibit similar trends on the map, where models that lie close together tend to show similar metric values.  
Additionally, in the right panel, the GSM8K/MMLU region corresponds to the ArXiv/PubMed Central region in the bottom panel of Fig.~\ref{fig:model-map-tsne-intro},  
suggesting that models with high likelihoods on academic and scientific texts also tend to perform well on mathematical reasoning and academic knowledge-intensive tasks.

\paragraph{Model size and creation date.}  

Figure~\ref{fig:size-date} shows the distribution of models by size and creation date.  
Compared to Fig.~\ref{fig:modelmap-logp-score-task}, newer models generally perform better, but model size does not always correlate with performance, as some smaller models perform comparably to larger ones.

\subsection{Detection of data leakage} \label{sec:the-pile}

Since the text data we used was extracted from the Pile corpus, models that were pre-trained on the Pile are likely to exhibit higher log-likelihood values than their actual capabilities measured by benchmarks.  
We analyze this effect using the model map in Fig.~\ref{fig:the-pile}.  
The left panel highlights models that used the Pile for pre-training.  
The right panel shows models with high mean log-likelihood relative to their 6-TaskMean score.  
The alignment between these two distributions suggests that models pre-trained on the Pile tend to achieve higher likelihoods on our text data, while their benchmark performance remains comparatively lower.

\begin{table*}[t!]
\scriptsize
    \centering
    \begin{tabular}{lrrrrrrrr}
    \toprule
        & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K & 6-TaskMean & mean log-likelihood\\
        \midrule
        Pearson's $r$ & 0.972 & 0.948 & 0.963 & 0.958 & 0.965 & 0.935 & 0.976 & 0.994 \\
        Spearman's $\rho$ & 0.976 & 0.974 & 0.969 & 0.937 & 0.973 & 0.899 & 0.978 & 0.990 \\
    \bottomrule
    \end{tabular}

\caption{Results of ridge regression for predicting benchmark scores from model coordinates.  
Predictions for 6-TaskMean and mean log-likelihood are also included.  
High correlation coefficients are observed across all settings.}

    \label{tab:result-regression}
\end{table*}

\section{Predicting Model Performance from Model Coordinates} \label{sec:predict-performance}

As shown in the central panel of Fig.~\ref{fig:modelmap-logp-score-task}, the positioning of models on the map suggests that a model's benchmark performance may be inferred from its coordinates.  
In this section, we conduct a regression analysis using the \(\bmq\)-coordinates to predict benchmark scores and evaluate predictive performance.

\subsection{Benchmark scores and models}  

We use six benchmark scores from Open LLM Leaderboard v1, as described in Section~\ref{sec:leaderboard_scores}.  
Our experiments are conducted on 996 models for which these benchmark scores are available\footnote{  
From the 1,018 models, we excluded four that lacked TruthfulQA scores and 18 from \texttt{deepseek-ai}, as their benchmark scores were incomplete due to the models being relatively new, leaving 996 for analysis. For simplicity, we denote the number of models as $K$.  
}.

\subsection{Setting for regression analysis}\label{sec:setting_for_ridge}

For each benchmark task, the dataset is given as $\{(\bmq_1, f_1), \dots, (\bmq_K, f_K)\}$,  
where $\bmq_i \in\mathbb{R}^N$ is the double-centered log-likelihood vector of the language model $p_i$,  
and $f_i \in [0, 100]$ is its corresponding benchmark score.
We use ridge regression to predict each benchmark score.  
Let $\bmQ \in\mathbb{R}^{K\times N}$ be the matrix of explanatory variables, and let $\bm{f} = (f_1,\ldots,f_K) \in \mathbb{R}^{K}$ be the vector for the target variable.  
The objective function with parameter $\bm{w} \in \mathbb{R}^N$ is given by:  
\begin{align*}
    L(\bm{w}) = \|\bm{f} - \bmQ\bm{w}\|^2 + \alpha \|\bm{w}\|^2,
\end{align*}
where $\alpha \in \mathbb{R}_{>0}$ is a hyperparameter that controls the strength of regularization.
Since the number of variables $N$ is much larger than the sample size $K$ ($N \gg K$), making this a high-dimensional regression setting, we carefully set $\alpha$ using cross-validation to avoid overfitting.

We split the set of models into five folds and perform parameter training\footnote{We used scikit-learn~\cite{DBLP:journals/sigmobile/VaroquauxBLGPM15} \texttt{RidgeCV}.} and benchmark score prediction.  
To mitigate the effect of randomness, we repeat the data splitting with five different seeds and take the average of the predictions as the final predicted score.  
As evaluation metrics, we compute Pearson's $r$ and Spearman's $\rho$ to measure the correlation between the predicted and benchmark scores.  
Additionally, we conduct experiments by replacing the target variable with 6-TaskMean and mean log-likelihood, leading to a total of eight experimental settings.  
See Appendix~\ref{app:model_pred_training_details} for details.

\subsection{Results}

Table~\ref{tab:result-regression} presents the results of the regression analysis.  
For all benchmark tasks, the correlation coefficients are high, demonstrating that ridge regression achieves strong predictive performance.
Similarly, for 6-TaskMean, the correlation remains high, as also illustrated by the scatter plot in Fig.~\ref{fig:pred_and_gt_by_mean_logp}. Even for mean log-likelihood, the regression model attains a high correlation, despite the explanatory variables being double-centered.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{images/5fold_split_average_task.png}
\caption{
Scatter plot of predicted scores versus 6-TaskMean scores for test sets.  
Pearson's $r$ is 0.976, and Spearman's $\rho$ is 0.978.  
Each point is color-coded by the mean log-likelihood, and the color bar is clipped at the 10th percentile. Higher mean log-likelihood values generally correspond to higher 6-TaskMean scores.
However, some models with extremely high mean log-likelihood due to data leakage (Section~\ref{sec:the-pile}) appear in the lower score region, deviating from the general trend. 
Despite this, the regression-based predictions remain highly accurate, as reflected in the strong correlation coefficients.  
Scatter plots for individual benchmark tasks are provided in Fig.~\ref{fig:all_pred_and_gt_by_mean_logp} in Appendix~\ref{app:model_pred_result_details}.
}

\label{fig:pred_and_gt_by_mean_logp}
\end{figure}

\section{Empirical Validation of Theory} \label{sec:confirmation}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{images/figure9_token-text_v8.pdf}
    \caption{Relationship between the squared Euclidean distance of model coordinates and KL divergence.  
(Left) In the token-level experiment (Section~\ref{sec:experiment-KL-token}), each point represents a text.  
(Right) In the text-level experiment (Section~\ref{sec:experiment-KL-text}), each point represents a pair of models.}

    \label{fig:token-text_KL}
\end{figure}

\begin{figure}
    \centering\includegraphics[width=\linewidth]{images/fig_horizontal_grid_Llama-2-7b_vicuna-7b-v1.5_Llama-2-7b-chat_pile_dataset1_20250118_1k_03.pdf}

    \caption{
    Visualization of 36 language models obtained by linearly interpolating pretrained model weights.  
    Each point is color-coded according to its mean log-likelihood.  
    (Left) Models in the weight parameter space.  
    (Right) Models in the probability distribution space, represented by the \(\bmq\)-coordinate system.
    }
    
    \label{fig:weight-interpolation-grids}
\end{figure}

In Section~\ref{sec:model-text}, we discussed model coordinates for text probability models.  
Appendix~\ref{sec:model-token} extends this framework to token-sequence probability models.  
These theoretical results state that the squared Euclidean distance in the model-coordinate space approximates the KL divergence between models.  
To validate this, we first evaluate token-level conditional probability models (Section~\ref{sec:experiment-KL-token}), since token-level experiments are generally easier to conduct than text-level experiments.  
We then extend our analysis to text probability models (Section~\ref{sec:experiment-KL-text}).  
Additionally, in Section~\ref{sec:weight-parameter}, we explore the relationship between model weight parameters and model coordinates.

\subsection{Validation of (\ref{eq:main-token-KL-xi}) for token-level models} \label{sec:experiment-KL-token}

\paragraph{Settings.}  
Two models with a shared tokenizer \texttt{Llama-2-7b-hf} and \texttt{Llama-2-7b-chat-hf}~\citep{arxiv:2307.09288}, denoted as $p_1$ and $p_2$, were used.
Following the method described in Appendix~\ref{sec:token-logp}, we computed the model coordinates $\bmzeta_{1}(x)$ and $\bmzeta_{2}(x)$ for each text $x = (y_1, \cdots, y_{n}) \in D$, where these coordinates are centered vectors with elements $\log p_i(y_t|y^{t-1})$ as defined in (\ref{eq:zeta-coordinate}).
We then calculated the squared Euclidean distance between these coordinates, $\|\bmzeta_1(x) - \bmzeta_2(x)\|^2$.  
To obtain the exact KL divergence between models, we used the outputs of the softmax function in the language models.  
We computed the sum of per-token KL divergences:
\(
\sum_{t=1}^{n}\mathrm{KL}(p_1(y_t|y^{t-1}),p_2(y_t|y^{t-1})),
\)
which is also used in \citet{lv-etal-2023-parameter}.

\paragraph{Results and discussion.}  
The left panel of Fig.~\ref{fig:token-text_KL} shows a scatter plot of the squared Euclidean distance and KL divergence for $x\in D$, with a Pearson's correlation coefficient of $r=0.893$.  
This result indicates that (\ref{eq:main-token-KL-xi}) provides a good approximation in actual language models.

\subsection{Validation of (\ref{eq:main-KL-xi}) for text-level models} \label{sec:experiment-KL-text}

\paragraph{Settings.}  
Among the 292 language models sharing the tokenizer with \texttt{Llama-2-7b-hf}~\citep{arxiv:2307.09288}, we excluded the five models with the largest values of \(\sum_{x\in D}\|\bmzeta_i(x)\|^2\), leaving 287 models for our experiment.  
Using the approach described in Section~\ref{sec:model-text}, we computed the model coordinates for each model and calculated the squared Euclidean distance \(\|\bmq_i - \bmq_j\|^2\) between every pair of models.  
Because it is extremely difficult to directly compute \(\mathrm{KL}(p_i, p_j)\), we instead used
\(
\frac{1}{N} \sum_{x \in D} \bigl\| \bmzeta_i(x) - \bmzeta_j(x) \bigr\|^2
\)
as a proxy.  
For a theoretical justification that this quantity approximates \(\mathrm{KL}(p_i, p_j)\), see \eqref{eq:main-token-KL-text-KL-xi} in Appendix~\ref{sec:model-token}.

\paragraph{Results and discussion.}  
As shown in the right panel of Fig.~\ref{fig:token-text_KL}, the scatter plot of squared Euclidean distance versus KL divergence exhibits a Pearson's correlation coefficient of $r=0.904$.  
This finding confirms that the relationship in \eqref{eq:main-KL-xi} holds approximately in practical language models.

\subsection{Relationship between model weights and model coordinates} \label{sec:weight-parameter}

For language models with the same architecture, comparison can also be conducted via weight parameters.  
We investigated how the structure of the model-coordinate space aligns with the structure of the weight-parameter space.  
We generated 36 new language models by linearly interpolating the weights of \texttt{Llama-2-7b-hf}, \texttt{Llama-2-7b-chat-hf}~\citep{arxiv:2307.09288}, and \texttt{vicuna-7b-v1.5}~\citep{arxiv:2306.05685}.  
For these 36 models, we computed text-generation log-likelihoods and visualized the resulting model coordinates in two dimensions using Principal Component Analysis (PCA).  
Figure~\ref{fig:weight-interpolation-grids} shows these 36 models in both weight space and model coordinate space.  
We observe that the two-dimensional grid structure in the weight space is mapped continuously into the model coordinate space.  
This finding supports the validity of (\ref{eq:exponential-family}) and (\ref{eq:KL-approx-exponential-family}) in Appendix~\ref{sec:theory-text}, especially for models that are close to one another.  
Further details on this experiment can be found in Appendix~\ref{app:weight-parameter}.

\section{Conclusion}

We propose a method to compare autoregressive language models using log-likelihoods from a predefined text set.  
By treating these as model coordinates, we show that the squared Euclidean distance approximates KL divergence, enabling efficient large-scale comparisons.  
Experiments with over 1,000 models confirm its effectiveness in structuring model relationships and predicting benchmark task performance, while also validating the theoretical foundations.

    

    

    
    
    

\section*{Limitations}
\begin{itemize}
    \item Changing the text data will alter the analysis results of the model map. This is both a limitation and an advantage of the proposed method, because it allows us to choose text data according to the analysis objective. For example, if we want to investigate code-focused language models in more detail, we can increase the proportion of code data from GitHub, thereby increasing the resolution of code-focused models on the model map.
    
    \item The proof (Section~\ref{sec:model-text} and Appendix~\ref{sec:theory-text}) that the squared Euclidean distance in the model coordinate system approximates the KL divergence assumes that the language model's text-generation probabilities closely match the distribution of the text data. When this assumption does not hold, the approximation accuracy decreases. However, even under such circumstances, the model coordinates should still function sufficiently as model features.

    \item If the text data used for the model coordinates is contained in a language model's pre-training corpus, that model's mean log-likelihood may be overestimated. This data leakage, or data contamination, is generally non-negligible, as shown in Fig.~\ref{fig:the-pile} in Section~\ref{sec:the-pile}, which illustrates the effect of using the Pile corpus. However, comparing it against benchmark scores makes it possible to detect such data leakage, and one can remove models that are affected.
Furthermore, the model map based on the \(\bmq\)-coordinate system is robust to contamination in the data,  
as the estimation of KL divergence using the squared Euclidean distance in the \(\bmq\)-coordinate system remains valid even if the true generative model \(p_0\) that produces the dataset \(D\) varies,  
as long as all compared models remain sufficiently close to \(p_0\).  
For instance, even if \(D\) is generated from one of the \(K\) models, such as \(p_i\), the estimation formula (\ref{eq:main-KL-xi}) remains correct (Appendix~\ref{sec:theory-note-modifyp0}).

    
    \item Beyond the data leakage mentioned above, other systematic errors introduced into the model coordinates can also affect the model map. As noted in Appendix~\ref{sec:error-analysis}, $\bmell_i$ and $\bar{\ell}_i$ are susceptible to systematic biases. However, thanks to double centering, $\bmq_i$ is less influenced by bias terms.

    \item Although the calculation of model coordinates is linear time $O(KN)$ in the number of models $K$ and the number of texts $N$, it still requires a non-negligible amount of computation. In our experiments, it took about 10 minutes on a single GPU (RTX 6000 Ada) to compute coordinates ($N=10^4$, float16) for a single 7B model.

    \item Computing the model map visualization from the model coordinates is generally not linear in $K$. For example, t-SNE requires a distance matrix, incurring $O(K^2 N)$ computational cost. However, in modern computing environments, as long as $K$ is not extremely large (e.g., in the millions), the cost of visualization is negligible compared to the cost of calculating the model coordinates.

    \item A sufficiently large number of text samples, $N$, is desirable for the model coordinates. We used $N=10^4$. Since the error in the KL divergence estimate due to randomness decreases proportionally to $N^{-1/2}$, $N$ must be increased according to the desired resolution of the model map.

    \item When using model coordinates as feature vectors, $N=10^4$ can be unwieldy. According to the experiment in Section~\ref{sec:logp-computation}, applying PCA to reduce the dimensionality of $\bmq$-coordinates to 82 dimensions still retains 95\% of the information. However, the predictive performance of such dimension-reduced features has not yet been tested.

    \item Since the language models used in our experiments were obtained from the Open LLM Leaderboard v1 (which ran from April 2023 to June 2024), our discussion of models released after June 2024 is limited.

    \item The results of the task-performance prediction in Section~\ref{sec:predict-performance} should be interpreted conservatively. While we use a proper cross-validation setup to prevent data leakage (splitting the training set, validation set, and test set), if very similar models appear in both training and test sets (for example, models that share a base model and differ only slightly by fine-tuning), it might become easier to predict task performance. Also, we have not evaluated models that are not listed on the leaderboard.

    \item The theoretical validation experiment in Section~\ref{sec:confirmation} is limited. Currently, it is difficult to directly compute exact KL divergence values among text-generation probability models, so conducting more precise validation experiments remains a future challenge.

    \item In the method of computing model coordinates from token-sequence conditional probabilities (Appendix~\ref{sec:model-token} and Appendix~\ref{sec:theory-token}), the proof that the squared Euclidean distance in the model coordinate system approximates the KL divergence requires additional assumptions (Appendix~\ref{sec:token-theory-assumptions}). In practice, Assumption~2 does not hold, and due to the variations in (\ref{eq:token-assumtion2}), $\|\bmzeta_i - \bmzeta_j\|^2$ in (\ref{eq:main-token-KL-xi}) tends to overestimate the KL divergence. Nonetheless, even in such situations, token-level model coordinates will still likely function sufficiently as features.
\end{itemize}

\section*{Acknowledgments}
This study was partially supported by JSPS KAKENHI 22H05106, 23H03355, JST CREST JPMJCR21N3, JST SPRING JPMJSP2110, JST BOOST JPMJBS2407.

\bibliography{custom, modelcard}

\appendix

\section{Related Work}\label{sec:related-work}
In recent years, research on comparing large language models (LLMs) has gained attention.  
This section provides an overview of existing studies from three perspectives: model parameters, activations\footnote{In general, ``activations'' refer to the intermediate outputs of Transformer models (e.g., the residual stream or neurons)~\cite{bereska2024mechanistic}.}, and probability distributions.

\paragraph{Comparison of model parameters.}
One approach to comparing LLMs is to analyze their parameters.
\citet{zhu2025independence} proposed a statistical framework for evaluating parameter similarity between different models and introduced a method for determining whether these models were trained independently. 
Additionally, \citet{NEURIPS2023_1644c9af} focused on parameter changes due to task adaptation, specifically analyzing task vectors\footnote{The difference in parameters before and after fine-tuning is referred to as a task vector, and arithmetic operations on these vectors are effective~\cite{ilharco2023editing}.}. They proposed a method to mitigate interference when integrating task vectors from different models. Specifically, by reducing redundant numerical components and adjusting for conflicting signs, their approach enables effective model merging.

\paragraph{Comparison of activations.}
Comparisons of LLMs based on activations have also been studied.  
\citet{zhou-etal-2025-linguistic} quantified the similarity between LLMs by measuring the cosine similarity of activation differences for linguistic minimal pairs.
In particular, they used datasets such as BLiMP~\cite{10.1162/tacl_a_00321} and showed that model similarity is significantly influenced by the pre-training dataset.

\paragraph{Comparison of probability distributions.}
Several approaches compare LLMs using probability distributions.
\citet{lv-etal-2023-parameter} proposed a method for computing coefficients in parameter ensembling by providing the same input text to two models and comparing the softmax probability distributions at each token. 
Specifically, they used KL divergence and summed the results to derive appropriate coefficients.
Furthermore, \citet{yax2025phylolm} proposed a similarity metric based on the conditional probabilities of LLMs and introduced a method for calculating the phylogenetic distance between different models. 
 Additionally, a method has been proposed for measuring differences in conditional probabilities based on prompts using KL divergence~\cite{melamed-etal-2024-prompts}.

\section{Double Centering} \label{sec:theory-double-centering}

We confirm the notation and computational operations.  
The matrices $\bm{L}$, $\bm{\Xi}$, and $\bmQ$ are all of size $K \times N$, and their elements are denoted by $\ell_{is}, \xi_{is}, q_{is}$, respectively. In particular, we have $\ell_{is} = \ell_i(x_s)$.  
First, row-wise centering of $\bm{L}$ is performed by subtracting the mean log-likelihood $\bar\ell_i$ of each model from each row $(\ell_{i1},\ldots,\ell_{iN})$, resulting in $\bm{\Xi}=(\bmxi_1,\ldots,\bmxi_K)^\top$.  
Next, column-wise centering of $\bm{\Xi}$ is performed by subtracting the coordinate component $\bar \xi_s$ of the mean vector $\bar\bmxi$ from each column $(\xi_{1s},\ldots,\xi_{Ks})^\top$, yielding $\bmQ=(\bmq_1,\ldots,\bmq_K)^\top$.  
Thus, this process involves \emph{double centering}, where column-wise centering follows row-wise centering.  
Notably, even after column-wise centering, the row-wise mean of $\bmQ$ remains zero:
\begin{align}
\frac{1}{N}\sum_{s=1}^N q_{is} &= \frac{1}{N}\sum_{s=1}^{N} (\xi_{is} - \bar \xi_{s}) \nonumber \\
&=\frac{1}{N}\sum_{s=1}^{N} \xi_{is} - \frac{1}{NK}\sum_{i=1}^{K}\sum_{s=1}^{N} \xi_{is}  \nonumber \\
&=0-0=0. \label{eq:q-zero-mean}
\end{align}

The column-wise centering can be interpreted as follows.  
In the $\bmxi$-coordinate system, the mean vector $\bar\bmxi$ of the $K$ model coordinates $\bmxi_1,\ldots,\bmxi_K$ can be regarded as representing an ``average model.''  
By redefining this average model as the new origin, we obtain the $\bmq$-coordinate system.  
From the definition $\bmq_i = \bmxi_i - \bar\bmxi$, its mean satisfies  
\[
\sum_{i=1}^K \bmq_i /K= \bmzero.
\]

The row-wise centering can be interpreted as follows.  
Let $\bmone_N = (1,\ldots,1)^\top \in \bbR^N$. Since $\bar \ell_i = \bmone_N^\top \bmell_i/N$, we have $\bmxi_i = \bmell_i - \bar\ell_i \bmone_N$.  
Thus,  
\[
\bmone_N^\top \bmxi_i = \bmone_N^\top \bmell_i -\bar\ell_i N = 0,
\]
and furthermore, $\bmone_N^\top \bar\bmxi = 0$.  
From this, equation (\ref{eq:q-zero-mean}) is actually trivial, as  
\[
\bmone_N^\top \bmq_i = \bmone^\top (\bmxi_i - \bar\bmxi) = 0-0=0.
\]
The row-wise centering implies that $\bmxi_1,\ldots,\bmxi_K$ and $\bmq_1,\ldots,\bmq_K$ lie in the subspace orthogonal to $\bmone_N$.

\section{Effect of Errors in Model Coordinates} \label{sec:error-analysis}

We analyze the impact of additive errors $\epsilon_{is}$ in the log-likelihood vector components $\ell_{is}$ for $i=1,\ldots,K$ and $s=1,\ldots,N$.  
Denoting the true values with an asterisk as $\ell_{is}^*$, the observed values can be expressed as  
\[
\ell_{is} = \ell_{is}^* + \epsilon_{is}.
\]
We decompose the error as follows:
\[
\epsilon_{is} = a + b_i + c_s + d_{is},
\]
where we assume, without loss of generality, the constraints  
\[
\sum_{i=1}^K b_i =\sum_{s=1}^N c_s = \sum_{i=1}^K d_{is} = \sum_{s=1}^N d_{is} = 0.
\]
Here, $a$, $b_i$, and $c_s$ are bias terms, while $d_{is}$ represents interaction terms.  
Using simple calculations, we obtain:
\[
\bar \ell_i = \frac{1}{N}\sum_{s=1}^N \ell_{is} = \bar \ell_i^* + a + b_i,
\]
\[
\xi_{is} = \ell_{is} - \bar \ell_i = \xi_{is}^* + c_s + d_{is},
\]
\[
\bar \xi_s = \frac{1}{K} \sum_{i=1}^K \xi_{is} = \bar\xi_s^* + c_s,
\]
\[
q_{is} = \xi_{is} - \bar\xi_s = q_{is}^* + d_{is}.
\]
Additionally, for the differences between two models, which are crucial for the model map, we obtain:
\[
\ell_{is} - \ell_{js} = \ell_{is}^* - \ell_{js}^* + b_i - b_j + d_{is} - d_{js},
\]
\[
\xi_{is} - \xi_{js} = \xi_{is}^* - \xi_{js}^* + d_{is} - d_{js},
\]
\[
q_{is} - q_{js} = q_{is}^* - q_{js}^* + d_{is} - d_{js}.
\]
Thus, the terms affected by the error are $\bar\ell_i$, which is influenced by $a + b_i$, and $\ell_{is} - \ell_{js}$, which is affected by $b_i + d_{is}$, meaning it is influenced by the bias terms. However, in the centered values $\xi_{is} - \xi_{js}$ and $q_{is} - q_{js}$, only the interaction term $d_{is}$ contributes to the error.

\section{Theory of Model Coordinates for Text Probability Distributions}\label{sec:theory-text}

In this section, we prove the main results of Section~\ref{sec:model-text}, namely (\ref{eq:main-KL-var-model}) and (\ref{eq:main-KL-xi}).  
Our discussion applies not only to text probability distributions but also more generally to any setting where i.i.d.\ observations \( x_1, \dots, x_N \sim p_i(x) \) are available.  
Compared to the previous study that proposed model maps~\citep{Shimodaira1993modelmap,shimodaira1998graphical}, we conduct a more precise analysis in this paper.  
Specifically, while the previous study provides only a brief evaluation of the approximation, we present a more transparent discussion based on the properties of the exponential family of distributions.
In the next section, as the starting point of our discussion, we construct a \emph{super model} that includes the \( K \) models \( p_i \), \( i=1,\dots,K \), as submodels.  
This model is introduced as a mathematical tool to rigorously prove the main theorem of this paper, and we do not compute it numerically in practice.

\subsection{Exponential family of distributions} \label{sec:theory-exponential-family-setup}

We first consider a model in the exponential family of distributions parameterized by a $K$-dimensional parameter $\bmt \in \bbR^K$:
\begin{align}\label{eq:exponential-family}
    p(x;\bmt) = p_0(x)\exp(\bmt^\top \bmb(x) - \psi(\bmt)).
\end{align}
Here, the function $\bmb(x) = (b_1(x), \dots, b_K(x))^\top$ will be defined later using the $K$ models.  
The normalization constant is given by  
\begin{align*}
    Z(\bmt) &= \sum_{x\in \cX} p_0(x)\exp(\bmt^\top \bmb(x)), \\
    \psi(\bmt) &= \log Z(\bmt),
\end{align*}
which ensures that $\sum_{x\in\cX} p(x;\bmt) = 1$.  
For $\bmt = \bmzero$, where $\bmzero = (0, \dots, 0)^\top$, we obtain  
\[
p(x;\bmzero) = p_0(x).
\]

To associate the $K$ models with (\ref{eq:exponential-family}), we define $\bmb(x)$. For a constant $\lambda > 0$, we set  
\begin{align} \label{eq:bix}
    \lambda b_i(x) := \ell_i(x) - \ell_0(x).
\end{align}
The constant $\lambda$ is an order parameter introduced for theoretical convenience, and in our theoretical framework, we assume that $b_i(x)$ is of constant order and that $\lambda$ is sufficiently small\footnote{If numerical computation were to be performed, $\lambda$ could be set to any arbitrary value (e.g., $\lambda=1$).}.  
Thus, we essentially assume that $|\ell_i(x) - \ell_0(x)| = O_p(\lambda)$ is sufficiently small, implying that each model $p_i$ provides a good approximation of the true generative model $p_0$.  
In the proof of the main theorem, we consider the asymptotic theory as $\lambda \to 0$, retaining terms up to $O(\lambda^2)$ while ignoring those of $O(\lambda^3)$.

A one-hot vector is defined as $\bme_i = (0,\ldots,0,1,0,\ldots,0)^\top \in \bbR^K$ for $i=1,\ldots,K$, where only the $i$-th element is 1.  
Then, setting $\bmt = \lambda \bme_i$ gives  
\begin{align} \label{eq:pix}
  p(x;\lambda \bme_i) = p_i(x).
\end{align}
Indeed, substituting (\ref{eq:bix}) into (\ref{eq:exponential-family}) yields  
\begin{align*}
&    p(x;\lambda \bme_i) \\
=&p_0(x)\exp(\lambda \bme_i^\top \bmb(x)-\psi(\lambda \bme_i))\\
=&p_0(x)\exp(\ell_i(x)-\ell_0(x) - \psi(\lambda \bme_i))\\
=&p_0(x) (p_i(x)/p_0(x)) \exp(- \psi(\lambda \bme_i))\\
=&p_i(x)\exp(- \psi(\lambda \bme_i))\\
=&p_i(x),
\end{align*}
where $\psi(\lambda \bme_i) = 0$.

\subsection{Properties of the exponential family of distributions}

This section outlines some well-known basic properties of the exponential family of distributions, which have been established in the literature~\citep{barndorff2014information, efron1978geometry, efron2022exponential, amari1982-10.1214/aos/1176345779}.  
We define the expectation and covariance matrix of $\bmb(x)$ as follows:
\begin{align*}
    \bmet(\bmt):=& \bbE_{x\sim p(\bmt)}\bigl(\bmb(x)\bigr)
    =\sum_{x\in\cX} \bmb(x) p(x;\bmt),\\
    G(\bmt):=& \bbE_{x\sim p(\bmt)}
    \bigl\{ (\bmb(x)- \bmet(\bmt)) (\bmb(x)- \bmet(\bmt))^\top\bigr\}\\
    =&\Var_{x\sim p(\bmt)}\bigl(\bmb(x)\bigr).
\end{align*}
Here, the elements of $\bmet(\bmt)$ are expectations given by $\bbE_{x\sim p(\bmt)}(b_i(x))$, and the elements of $G(\bmt)$ are covariances given by $G_{ij}(\bmt) = \Cov_{x\sim p(\bmt)}(b_i(x), b_j(x))$.
These quantities can be expressed in terms of $\psi(\bmt)$ as follows:
\begin{align}
 \bmet(\bmt) =     \frac{\partial\psi(\bmt)}{\partial \bmt}, \label{eq:psi-eta}\\
 G(\bmt) =    \frac{\partial^2\psi(\bmt)}{\partial \bmt \partial \bmt^\top}.  \label{eq:psi-G}
\end{align}
We now derive these two equations. First, from
\[
\frac{\partial Z(\bmt)}{\partial \bmt}
= \sum_{x\in\cX} \bmb(x)p_0(x)e^{\bmt^\top \bmb(x)},
\]
we obtain
\begin{align*}
\frac{\partial\psi(\bmt)}{\partial \bmt}
=&\frac{\partial \log Z}{\partial \bmt}
=\frac{1}{Z(\bmt)}\frac{\partial Z}{\partial \bmt}\\
=& \frac{1}{Z(\bmt)} \sum_{x\in\cX}
\bmb(x) p_0(x) e^{\bmt^\top \bmb(x)}\\
=&\sum_{x\in\cX} \bmb(x)p(x;\bmt) = \bmet(\bmt).
\end{align*}
Thus, we have established (\ref{eq:psi-eta}).  
Next, using
\begin{align*}
\frac{\partial p(x;\bmt)}{\partial \bmt}
=&\Bigl(\bmb(x)-\frac{\partial \psi}{\partial \bmt}
\Bigr) p(x;\bmt)\\
=&(\bmb(x)-\bmet(\bmt)) p(x;\bmt),
\end{align*}
we obtain
\begin{align*}
&\frac{\partial^2 \psi(\bmt)}
{\partial \bmt \partial \bmt^\top}
=\frac{\partial \bmet(\bmt)^\top}{\partial \bmt}\\
=&\frac{\partial}{\partial\bmt}
\sum_{x\in\cX} \bmb(x)^\top p(x;\bmt)\\
=&\sum_{x\in\cX}(\bmb(x)-\bmet(\bmt)) \bmb(x)^\top p(x;\bmt)\\
=&\sum_{x\in\cX}(\bmb(x)-\bmet(\bmt)) (\bmb(x)-\bmet(\bmt))^\top p(x;\bmt)\\
=&G(\bmt).
\end{align*}
Thus, we have established (\ref{eq:psi-G}).

\subsection{Approximation of the KL divergence} \label{sec:theory-approximate-KL}

The Kullback-Leibler (KL) divergence between the models $p(\bmt)$ and $p(\bmt')$ at parameter values $\bmt, \bmt' \in \bbR^K$ is given by  
\begin{align}
 &  \mathrm{KL}(p(\bmt), p(\bmt')) \nonumber\\
=& \sum_{x\in\cX}p(x;\bmt) \log\frac{p(x;\bmt)}{p(x;\bmt')} \nonumber\\
=& \sum_{x\in\cX} p(x;\bmt) \bigl\{
(\bmt-\bmt')^\top \bmb(x) - \psi(\bmt) + \psi(\bmt')
\bigr\}\nonumber\\
=&(\bmt-\bmt')^\top \bmet(\bmt) - \psi(\bmt) + \psi(\bmt').
\label{eq:KL-exponential-family}
\end{align}
Here, we assume that the parameter values $\bmt$ and $\bmt'$ are sufficiently close to $\bmzero$.  
In particular, we assume $\|\bmt\| = O(\lambda)$ and $\|\bmt'\| = O(\lambda)$.  
Substituting (\ref{eq:psi-eta}) and (\ref{eq:psi-G}) into the Taylor expansion of $\psi(\bmt)$ gives  
\begin{align}
 &\psi(\bmt') \nonumber \\
=&\psi(\bmt)
+\frac{\partial \psi}{\partial \bmt^\top}
(\bmt'-\bmt)\nonumber \\
&+\frac{1}{2}(\bmt'-\bmt)^\top
\frac{\partial^2 \psi(\bmt)}
{\partial \bmt \partial \bmt^\top}
(\bmt'-\bmt)\nonumber \\
& +O(\|\bmt'-\bmt\|^3)\nonumber\\
=&\psi(\bmt) + \bmet(\bmt)^\top (\bmt'-\bmt)\nonumber\\
&+\frac{1}{2}(\bmt'-\bmt)^\top G(\bmt)(\bmt'-\bmt)+O(\lambda^3).
\label{eq:psi-taylor}
\end{align}
Substituting (\ref{eq:psi-taylor}) into (\ref{eq:KL-exponential-family}) gives  
\begin{align}
&\mathrm{KL}(p(\bmt),p(\bmt'))\nonumber \\
&=\frac{1}{2}(\bmt'-\bmt)^\top G(\bmt)(\bmt'-\bmt)+O(\lambda^3).
\label{eq:KL-approx-exponential-family-theta}
\end{align}
This corresponds to eq.~(9) of \citet{oyama-etal-2023-norm}.  
Here, the equation holds approximately by ignoring higher-order terms of $O(\lambda^3)$.  
For more details, refer to \citet[p.~369]{amari1982-10.1214/aos/1176345779} and \citet[p.~35]{efron2022exponential}.  
More generally, $G(\bmt)$ represents the Fisher information metric, and \eqref{eq:KL-approx-exponential-family-theta} holds for a wide class of probability models~\cite{amari1998natural} with $\|\bmt' - \bmt\|=O(\lambda)$.  
Furthermore, since $G(\bmt) = G(\bmzero) + O(\|\bmt\|) = G(\bmzero) + O(\lambda)$, we obtain  
\begin{align}
&\mathrm{KL}(p(\bmt),p(\bmt'))\nonumber\\
&=\frac{1}{2}(\bmt'-\bmt)^\top G(\bmzero)(\bmt'-\bmt)+O(\lambda^3).
\label{eq:KL-approx-exponential-family}
\end{align}

\subsection{The variance representation of the KL divergence} \label{sec:theory-variance-KL}

Substituting $p_i = p(\lambda\bme_i)$ into (\ref{eq:KL-approx-exponential-family}) gives  
\begin{align}
&2\mathrm{KL}(p_i,p_j)
=2\mathrm{KL}(p(\lambda\bme_i), p(\lambda\bme_j))
\nonumber\\
&=\lambda^2(\bme_i-\bme_j)^\top G(\bmzero)(\bme_i-\bme_j)+O(\lambda^3).
\label{eq:KL-approx-model}
\end{align}
Here, we have  
\begin{align}
&\bme_i^\top G(\bmzero) \bme_j = G_{ij}(\bmzero)\nonumber\\
&=\bbE_{x\sim p_0}\Bigl\{
(b_i(x)-\eta_i(\bmzero))(b_j(x) - \eta_j(\bmzero))
\Bigr\}.
\label{eq:eiGej}
\end{align}
Next, we substitute (\ref{eq:eiGej}) and (\ref{eq:bix}) into the right-hand side of (\ref{eq:KL-approx-model}) to derive an alternative expression for the KL divergence:
\begin{align*}
&\lambda^2 (\bme_i-\bme_j)^\top G(\bmzero)(\bme_i-\bme_j)\\
=&\lambda^2\bbE_{x\sim p_0}
\Bigl[
\Bigl\{(b_i(x)-\eta_i(\bmzero))\\
&\qquad -(b_j(x) - \eta_j(\bmzero))\Bigr\}^2
\Bigr]\\
=&\bbE_{x\sim p_0}
\Bigl[\Bigl\{
(\ell_i(x)-\ell_0(x))-(\ell_j(x)-\ell_0(x)) \\
&\qquad - \bbE_{x'\sim p_0}\Bigl(
(\ell_i(x')-\ell_0(x'))\\
&\qquad\qquad -(\ell_j(x')-\ell_0(x'))\Bigr)
\Bigr\}^2\Bigr]\\
=&\bbE_{x\sim p_0}
\Bigl[\Bigl\{
\ell_i(x)-\ell_j(x) \\
&\qquad - \bbE_{x'\sim p_0}\Bigl(
\ell_i(x')-\ell_j(x')\Bigr)
\Bigr\}^2\Bigr]\\
=&\Var_{x\sim p_0} \Bigl(
\ell_i(x)-\ell_j(x)
\Bigr).
\end{align*}
Finally, substituting this result into (\ref{eq:KL-approx-model}) yields  
\begin{align}
2\mathrm{KL}(p_i,p_j)=
\Var_{x\sim p_0} \Bigl(
\ell_i(x)-\ell_j(x)
\Bigr)+O(\lambda^3).
\label{eq:KL-var-model}
\end{align}
This establishes (\ref{eq:main-KL-var-model}).  
Furthermore, since $|\ell_i(x)-\ell_j(x)|=O_p(\lambda)$, the magnitude of (\ref{eq:KL-var-model}) is $O(\lambda^2)$.

\subsection{Estimation of the KL divergence} \label{sec:theory-estimate-KL}

If the expected value $\bbE_{x\sim p_0}(f(x))$ of a function $f(x)$ exists and is bounded, then by the law of large numbers, the sample mean\footnote{$\bbE_{x\sim D}(f(x)) = \frac{1}{N} \sum_{x\in D} f(x) = \frac{1}{N}\sum_{s=1}^N f(x_s)$ represents the sample mean, and $\Var_{x\sim D}(\cdot)$ represents the sample variance.} converges to the expected value as $N \to \infty$, and we have  
\[
\bbE_{x\sim D}(f(x)) = \bbE_{x\sim p_0}(f(x)) + O_p(N^{-1/2}).
\]
Applying this to (\ref{eq:KL-var-model}) and ignoring the terms of order $O_p(\lambda^3 + \lambda^2 N^{-1/2})$, we obtain the following approximation:
\begin{align}
2\mathrm{KL}(p_i,p_j)\approx
\Var_{x\sim D} \Bigl(
\ell_i(x)-\ell_j(x)
\Bigr).
\label{eq:KL-data-var-model}
\end{align}
We define the coordinates $\bmxi_i\in \bbR^N$ of model $p_i$ as  
\[
\bmxi_i=(\xi_{i1},\ldots,\xi_{iN})^\top
\]
with  
\begin{align*}
\xi_{is}:= \ell_i(x_s) - \bbE_{x\sim D} ( \ell_i(x))
\end{align*}
for $s=1,\ldots,N$.  
From (\ref{eq:KL-data-var-model}), we obtain  
\begin{align}
2\mathrm{KL}(p_i,p_j) \approx&
\frac{1}{N}\sum_{s=1}^N (\xi_{is}-\xi_{js})^2 \nonumber\\
=&
\frac{1}{N}\|\bmxi_i - \bmxi_j\|^2.
\label{eq:KL-xi}
\end{align}
Since  
\[
\|\bmxi_i - \bmxi_j\|^2=\|(\bmq_i + \bar\bmxi) - (\bmq_j + \bar\bmxi)\|^2 = \|\bmq_i - \bmq_j\|^2,
\]
this establishes (\ref{eq:main-KL-xi}).

\subsection{Relationships among the three types of model coordinates}\label{sec:theory-height}

Let $\bmone_N = (1,\ldots,1)^\top \in \bbR^N$.  
From the definitions of the $\bmxi$-coordinate system and the $\bmq$-coordinate system, we have  
\begin{align}
    \bmxi_i &= \bmq_i + \bar\bmxi, \nonumber\\
    \bmell_i &= \bmxi_i + \bar\ell_i \bmone_N \nonumber\\
    &= \bmq_i + \bar\ell_i \bmone_N + \bar\bmxi.
    \label{eq:ell-q-transform}
\end{align}
Additionally, equation (\ref{eq:q-zero-mean}) in Appendix~\ref{sec:theory-double-centering} can be rewritten as  
\begin{align}
    \bmone_N^\top \bmq_i = 0.
    \label{eq:q-centered}
\end{align}
Thus, we obtain  
\begin{align*}
 &    \|\bmell_i - \bmell_j\|^2\\
=& \| (\bmq_i - \bmq_j ) + (\bar\ell_i - \bar\ell_j)\bmone_N \|^2\\
=& \|\bmq_i - \bmq_j\|^2 + N (\bar\ell_i - \bar\ell_j)^2\\
& \quad +2(\bar\ell_i - \bar\ell_j)\bmone_N^\top (\bmq_i - \bmq_j)\\
=& \|\bmq_i - \bmq_j\|^2 + N (\bar\ell_i - \bar\ell_j)^2,
\end{align*}
where (\ref{eq:ell-q-transform}) and (\ref{eq:q-centered}) are used in the first and last equations, respectively.  
This establishes (\ref{eq:lik-decomposition}).  

Moreover, since $\bar\ell_i = \bmone_N^\top \bmell_i/N$, it is straightforward that the component of the $\bmell$-coordinate system in the $\bmone_N$ direction is given by  
\[
(\bmone_N/\sqrt{N})^\top \bmell_i = \sqrt{N} \bar \ell_i.
\]

\subsection{Additional notes on modifying the underlying generative model}\label{sec:theory-note-modifyp0}

We examine the effect of changing the true generative model \(p_0 = p(\bmzero)\) that produces the data \(D\).  
For clarity, we continue to use the same \(p_0\) as before in constructing \(p(x;\bmt)\) as described in Section~\ref{sec:theory-exponential-family-setup}.  
We then introduce a new parameter value \(\bmt^*\).  
We assume that each element \(x_s\) of the dataset \(D\) is generated independently from  
\begin{align}
    x_1,\ldots,x_N \sim p(x;\bmt^*),
    \label{eq:data-generation-star}
\end{align}
and that \(\|\bmt^*\| = O(\lambda)\).  
In other words, the true generative model remains sufficiently close to the \(K\) models, satisfying \(\|\bmt_i - \bmt^*\| = O(\lambda)\) for \(i = 1,\ldots,K\).  

First, consider replacing \(G(\bmt)\) in \eqref{eq:KL-approx-exponential-family-theta} of Section~\ref{sec:theory-approximate-KL} with \(G(\bmt^*)\).  
Because \(G(\bmt^*) = G(\bmt) + O(\lambda)\), we obtain  
\begin{align}
&\mathrm{KL}(p(\bmt),p(\bmt'))\nonumber\\
&=\frac{1}{2}(\bmt' - \bmt)^\top G(\bmt^*)(\bmt' - \bmt) + O(\lambda^3).
\label{eq:KL-approx-exponential-family-star}
\end{align}
We are generalizing the discussion in the previous sections, and indeed, if we set \(\bmt^* = \bmzero\) in \eqref{eq:KL-approx-exponential-family-star}, then \eqref{eq:KL-approx-exponential-family} is recovered.  

Now substitute \(p_i = p(\lambda \bme_i)\) into \eqref{eq:KL-approx-exponential-family-star}, yielding  
\begin{align}
&2\,\mathrm{KL}(p_i,p_j)
=2\,\mathrm{KL}\bigl(p(\lambda\bme_i), p(\lambda\bme_j)\bigr)
\nonumber\\
&=\lambda^2(\bme_i - \bme_j)^\top G(\bmt^*)(\bme_i - \bme_j) + O(\lambda^3),
\label{eq:KL-approx-model-star}
\end{align}
which is a generalization of \eqref{eq:KL-approx-model} in Section~\ref{sec:theory-variance-KL}.  
Using the definition of \(\bm{G}(\bmt)\), we have  
\begin{align}
\bme_i^\top G(\bmt^*) \bme_j 
= G_{ij}(\bmt^*) 
= \Cov_{x\sim p(\bmt^*)}\bigl(b_i(x), b_j(x)\bigr).
\label{eq:eiGej-star}
\end{align}
Substituting \eqref{eq:eiGej-star} and \eqref{eq:bix} into the right-hand side of \eqref{eq:KL-approx-model-star} yields  
\begin{align*}
&\lambda^2(\bme_i - \bme_j)^\top G(\bmt^*)(\bme_i - \bme_j)\\
=&\,\lambda^2\Bigl\{
\Var_{x\sim p(\bmt^*)}\bigl(b_i(x)\bigr)
+\Var_{x\sim p(\bmt^*)}\bigl(b_j(x)\bigr)
\\
&\qquad -2\,\Cov_{x\sim p(\bmt^*)}\bigl(b_i(x), b_j(x)\bigr)
\Bigr\}
\\
=&\,\lambda^2 \Var_{x\sim p(\bmt^*)}\bigl(b_i(x) - b_j(x)\bigr)
\\
=&\,\Var_{x\sim p(\bmt^*)}\bigl(\ell_i(x) - \ell_j(x)\bigr).
\end{align*}
Finally, substituting this back into \eqref{eq:KL-approx-model-star} gives  
\begin{align}
2\,\mathrm{KL}(p_i,p_j)=
\Var_{x\sim p(\bmt^*)}\bigl(\ell_i(x) - \ell_j(x)\bigr)+O(\lambda^3),
\label{eq:KL-var-model-star}
\end{align}
which is a generalization of \eqref{eq:main-KL-var-model}.  

Hence, the estimators for KL divergence in Section~\ref{sec:theory-estimate-KL}, specifically \eqref{eq:KL-data-var-model} and \eqref{eq:KL-xi}, and also \eqref{eq:main-KL-xi} in Section~\ref{sec:model-text}, remain valid even when the texts are generated by \eqref{eq:data-generation-star}.  
Since \eqref{eq:KL-var-model-star} holds for any \(\bmt^*\) with \(\|\bmt^*\| = O(\lambda)\), this result also applies when the true data-generating model is \(p(\bmt^*) = p_0\), or, for instance, one of the models \(p(\bmt^*) = p_i\), or a mixture of the \(K\) models, \(\bmt^* = \sum_{i=1}^K \alpha_i \lambda \bme_i\) with \(\alpha_i = O(1)\).  
Therefore, this method is robust to contamination in the dataset \(D\) (e.g., when the text corpus used for pre-training a model is included in \(D\)), as the estimation of KL divergence via the squared Euclidean distance in the \(\bmxi\)-coordinate system or the \(\bmq\)-coordinate system remains relatively unaffected.

\section{Mapping Language Models into the Space of Token Probability Distributions} \label{sec:model-token}

In Section~\ref{sec:model-text}, we discussed model maps based on the probability distributions $p_i(x)$ of texts generated by language models.  
This approach requires computing probabilities for a large number of texts in the dataset $D = (x_1,\ldots,x_N)$, leading to high computational costs.  
To mitigate this issue, we focus on the fact that a text $x = (y_1,\ldots,y_n)$ is a sequence of tokens.  
Instead of using text probabilities, we discuss model maps based on the conditional probability distributions of token generation, $p_i(y_t|y^{t-1})$.  
In this approach, model coordinates are computed using only a single text $x$.  
A limitation of this approach is that it can only be used for comparing models that share the same tokenizer.  
Furthermore, the current estimation method ignores the variance of the expected log-likelihood ratio of conditional probabilities, resulting in a rough approximation.  
Thus, the estimated values should be regarded only as reference values rather than precise measurements.

\subsection{Model coordinates} \label{sec:token-logp}

For a text $x = (y_1,\ldots,y_n)$, the coordinates of model $p_i$  
\[
\bmzeta_i = (\zeta_{i1},\ldots,\zeta_{in})^\top \in \bbR^n
\]
are defined as  
\begin{align}
\zeta_{it} := \log p_i(y_t|y^{t-1}) - \ell_i(x)/n
\label{eq:zeta-coordinate}
\end{align}
for $t=1,\ldots,n$.  
This is centered for each $i$ and for each text, satisfying $\sum_{t=1}^n \zeta_{it} = 0$.

\subsection{Kullback-Leibler divergence}

The KL divergence for next-token generation in language models, where $y_t \sim p_i(y_t|y^{t-1})$, is given by  
\begin{align}
&\mathrm{KL}(p_i(y_t|y^{t-1}) ,p_j(y_t|y^{t-1}) ) =\\
&\qquad \sum_{y_t\in\cV} p_i(y_t|y^{t-1})\log\frac{p_i(y_t|y^{t-1})}{p_j(y_t|y^{t-1})}.
\label{eq:token-KL-def}
\end{align}
We apply the results for text probability distributions from Section~\ref{sec:model-text} and Appendix~\ref{sec:theory-text} to the conditional probability distributions of token generation.  
The equation corresponding to (\ref{eq:main-KL-var-model}) is  
\begin{align}
&2\mathrm{KL}(p_i(y_t|y^{t-1}) ,p_j(y_t|y^{t-1}) )\approx \nonumber \\
&\Var_{y_t\sim p_0(y_t|y^{t-1})} \biggl\{
\log\frac{p_i(y_t|y^{t-1})}{p_j(y_t|y^{t-1})}  
\biggr\}.
\label{eq:main-token-KL-var-model}
\end{align}
The squared Euclidean distance in the $\bmzeta$-coordinate system provides an estimate of the sum of (\ref{eq:main-token-KL-var-model}) over all tokens in the text $x$:  
\begin{align}
&\|\bmzeta_i - \bmzeta_j\|^2 \nonumber\\
\approx& 2\sum_{t=1}^n\mathrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1})) 
\label{eq:main-token-KL-xi}\\
\approx&2\mathrm{KL}(p_i,p_j).
\label{eq:main-token-KL-text-KL-xi}
\end{align}
The proof is provided in Appendix~\ref{sec:theory-token}.  
To justify the estimation in (\ref{eq:main-token-KL-xi}), we assume the following:  
\begin{align}
\bbE_{y_t\sim p_0(y_t|y^{t-1})}\biggl\{ \log \frac{p_i(y_t|y^{t-1})}{p_j(y_t|y^{t-1})}\biggr\}
\label{eq:main-token-assumtion2}
\end{align}
takes a constant value independent of $t$.
In reality, this assumption is not entirely correct, and the degree of variation affects the accuracy of the approximation in (\ref{eq:main-token-KL-xi})\footnote{Since this assumption does not affect (\ref{eq:main-KL-xi}), there is no concern regarding the use of model maps based on text probabilities.}.  
On the other hand, the approximation in (\ref{eq:main-token-KL-text-KL-xi}) holds more generally and is demonstrated in Appendix~\ref{sec:token-text-KL-relation}.

\section{Theory of Model Coordinates for Token Probability Distributions} \label{sec:theory-token}

In this section, we provide a more detailed explanation of the content discussed in Appendix~\ref{sec:model-token}.  
We extend the discussion of text probability distributions in Appendix~\ref{sec:theory-text} to the case of conditional probability distributions for token generation.

 \subsection{Exponential family of distributions}

We apply the same setting as for $p_i(x)$ in Section~\ref{sec:theory-text} to the conditional probability distributions of tokens:  
\[
y_t \sim p_i(y_t|y^{t-1}),\quad t=1,\ldots,n.
\]
The exponential family of distributions incorporating $K$ models, corresponding to (\ref{eq:exponential-family}), is given here as  
\begin{align}
\label{eq:token-exponential-family}
&p(y_t|y^{t-1};\bmt) := p_0(y_t|y^{t-1})\nonumber\\
&\qquad\exp(\bmt^\top \bmb(y_t|y^{t-1}) - \psi(\bmt |y^{t-1})).
\end{align}

The setting (\ref{eq:bix}), which associates the $K$ models with (\ref{eq:token-exponential-family}), is given here as  
\begin{align} \label{eq:token-bix}
    &\lambda b_i(y_t|y^{t-1}) :=\nonumber \\
    &\quad \log p_i(y_t|y^{t-1}) - \log 
 p_0(y_t|y^{t-1}).
\end{align}
Thus, we have
\[
p_i(y_t|y^{t-1}) = p(y_t|y^{t-1}; \lambda \bme_i)
\]
for $i=1,\ldots,K$.

\subsection{The variance representation of the KL divergence}

The KL divergence is given by (\ref{eq:token-KL-def}).  
Applying the result for the model $p(x;\bmt)$ in (\ref{eq:KL-var-model}) to the token-level conditional distribution model $p(y_t|y^{t-1};\bmt)$, we obtain  
\begin{align}
&2\mathrm{KL}(p_i(y_t|y^{t-1}) ,p_j(y_t|y^{t-1}) )=\nonumber \\
&\Var_{y_t\sim p_0(y_t|y^{t-1})} \biggl\{
\log\frac{p_i(y_t|y^{t-1})}{p_j(y_t|y^{t-1})}  
\biggr\}+O(\lambda^3).
\label{eq:token-KL-var-model}
\end{align}

\subsection{Two additional assumptions}\label{sec:token-theory-assumptions}

To estimate the KL divergence from a single text $x$, two additional assumptions are required, as described below.  
Such assumptions were not necessary when estimating the KL divergence from the dataset $D$ in Appendix~\ref{sec:theory-text}.  
In reality, these two assumptions are not strictly satisfied, and the discrepancy between these assumptions and reality affects the accuracy of the KL divergence approximation.

\paragraph{Assumption 1:}
We assume that the probability distribution of $y_t$ depends only on the past $k$ tokens, denoted as $y^{t-1}_{t-k} = (y_{t-k},y_{t-k+1},\ldots,y_{t-1})$.  
That is,  
\[
p_i(y_t|y^{t-1}) = p_i(y_t|y^{t-1}_{t-k}),
\]
which allows us to regard $y^t_{t-k}$ as the state of a Markov chain. More generally, we use the notation $y^k$ to represent a state variable.
We consider a function $f$ of the state variable $y^k$.  
Furthermore, we assume that this Markov chain is positive Harris recurrent, has a stationary distribution $\pi$, and that $f$ is absolutely integrable, i.e.,  
\[
\bbE_{y^k\sim \pi}(|f(y^k)|)<\infty.
\]
Then, by the strong law of large numbers for Markov chains~\cite[Theorem~17.0.1~(i)]{Meyn_Tweedie_Glynn_2009}, in the limit $n\to\infty$,  
\[
\frac{1}{n}\sum_{t=1}^n f(y^t_{t-k})\to \bbE_{y^k\sim \pi}(f(y^k))\quad\text{a.s.}
\]
For simplicity in notation and discussion, we assume that $y_{-k+1},\ldots,y_0$ are appropriately defined.
Since the Markov chain converges to $\pi$, we also have  
\begin{align}\label{eq:lln-markov}
\frac{1}{n}\sum_{t=1}^n f(y^t_{t-k}) \to
\frac{1}{n}\sum_{t=1}^n \bbE_{y^t \sim p_0}(f(y^t_{t-k}))
\end{align}
almost surely as $n\to\infty$.

\paragraph{Assumption 2:}
\begin{align}
\bbE_{y_t\sim p_0(y_t|y^{t-1})} \log \frac{p_i(y_t|y^{t-1})}{p_j(y_t|y^{t-1})} = c
\label{eq:token-assumtion2}
\end{align}
for some $c\in\bbR$ that can depend on the indices $i$ and $j$ but not on $t$.  
In other words, (\ref{eq:token-assumtion2}) takes a constant value independent of $t$.

\subsection{Estimation of the KL divergence}

Define  
\[
h(y^t) := \log\frac{p_i(y_t|y^{t-1})}{p_j(y_t|y^{t-1})}.
\]
From Assumption 1, $h(y^t)$ can be written in the form $h(y^t) = f_1(y^t_{t-k})$ for some $f_1$, so applying (\ref{eq:lln-markov}), for sufficiently large $n$, we obtain  
\[
\frac{1}{n}\sum_{t=1}^n h(y^t) \approx \frac{1}{n}\sum_{t=1}^n \bbE_{y^t\sim p_0}(h(y^t)).
\]
Applying (\ref{eq:token-assumtion2}) to the right-hand side gives  
\[
\frac{1}{n}\sum_{t=1}^n h(y^t) \approx c.
\]
Next, since $(h(y^t) - c )^2$ can be written in the form of $f_2(y^t_{t-k})$ for some function $f_2$, applying (\ref{eq:lln-markov}) again yields  
\begin{align*}
&  \frac{1}{n} \sum_{t=1}^n (h(y^t) - c)^2\\
\approx &  \frac{1}{n} \sum_{t=1}^n
\bbE_{y^{t-1}\sim p_0} \Bigl\{
\Var_{y_t \sim p_0(y_t|y^{t-1})} ( h(y^t))  \Bigr\}\\
\approx &\frac{1}{n} \sum_{t=1}^n
\Var_{y_t \sim p_0(y_t|y^{t-1})} ( h(y^t)).
\end{align*}
In the final equation, we applied (\ref{eq:lln-markov}) using the fact that $\Var_{y_t \sim p_0(y_t|y^{t-1})} ( h(y^t)) = f_3(y^t_{t-k})$ for some $f_3$.  
Using (\ref{eq:token-KL-var-model}), we obtain  
\begin{align}
&\sum_{t=1}^n (h(y^t) - c)^2 \approx\nonumber\\
&\qquad
2\sum_{t=1}^n\mathrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1})).
\label{eq:token-var-KL}
\end{align}
Meanwhile, the components of the model coordinate $\bmzeta_i$ are given by  
\[
\zeta_{it}= \log p_i(y_t|y^{t-1}) - c_i
\]
where  
\[
c_i = \frac{1}{n} \sum_{t=1}^n \log p_i(y_t|y^{t-1}).
\]
Since  
\[
\zeta_{it}-\zeta_{jt} = h(y^t) -(c_i-c_j)
\]
with $c_i-c_j \approx c$, equation (\ref{eq:token-var-KL}) can be rewritten as  
\begin{align*}
&    \|\bmzeta_i - \bmzeta_j\|^2 \approx\\
&\qquad 2\sum_{t=1}^n\mathrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1})).
\end{align*}
Thus, (\ref{eq:main-token-KL-xi}) is established.

\subsection{Connecting the KL divergence of token and text probability distributions}
\label{sec:token-text-KL-relation}

Here, we fix the sequence length of the text $x = (y_1,\ldots,y_n)$ as $n$, i.e., we set $\cX = \cV^n$.  
For notational simplicity, we define  
\[
g_i(y^t) = \log p_i(y_t|y^{t-1}).
\]
Noting that  
\[
p_i(x) = \prod_{t=1}^n p_i(y_t|y^{t-1})=\prod_{t=1}^n e^{g_i(y^t)},
\]
we obtain  
\begin{align*}
& \textrm{KL}(p_i, p_j)\\
=&\sum_{x\in \cX}\prod_{t'=1}^n e^{g_i(y^{t'})} \sum_{t=1}^n ( g_i(y^t) - g_j(y^t))\\
=& \sum_{t=1}^n \sum_{y^t\in \cV^t} \prod_{t'=1}^t e^{g_i(y^{t'})}  ( g_i(y^t) - g_j(y^t))\\
=& \sum_{t=1}^n \sum_{y^{t-1}\in \cV^{t-1}} \prod_{t'=1}^{t-1} e^{g_i(y^{t'})}\\
&\qquad \sum_{y_t\in \cV} e^{g_i(y^t)}  ( g_i(y^t) - g_j(y^t))\\
=& \sum_{t=1}^n \sum_{y^{t-1}\in \cV^{t-1}} p_i(y^{t-1}) \\
&\qquad \textrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1}))\\
=& \sum_{t=1}^n \bbE_{y^{t-1} \sim p_i}
\Bigl\{ \textrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1})) \Bigr\}\\
=&  \bbE_{x \sim p_i}\Bigl\{ \sum_{t=1}^n\textrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1})) \Bigr\}.
\end{align*}
Thus, for sufficiently large $n$, by the strong law of large numbers for Markov chains, we obtain  
\begin{align}
\textrm{KL}(p_i, p_j)\approx
 \sum_{t=1}^n\textrm{KL}(p_i(y_t|y^{t-1}),p_j(y_t|y^{t-1})).
 \label{eq:KL-text-KL-token}
\end{align}
This corresponds to (\ref{eq:main-token-KL-text-KL-xi}).  
Assumption 1 from Appendix~\ref{sec:token-theory-assumptions} is used in (\ref{eq:KL-text-KL-token}), but Assumption 2 is not needed in the discussion of this subsection.

\section{Details of Experiments} \label{sec:experiment-details}

\subsection{Information obtained via the Hugging Face Hub API}
We used the Hugging Face Hub API to retrieve information about each language model's tags, the date the model was created, the number of downloads over the past 30 days, and the model's configuration details. All of this information is current as of February 1, 2025.

Among the model tags, we specifically used \texttt{llama2}, \texttt{llama-2}, \texttt{license:llama2}, \texttt{llama3}, \texttt{llama-3}, and \texttt{license:llama3} to determine the model type (llama-1, llama-2, or llama-3). Furthermore, to identify language models that were pre-trained on the Pile in Section~\ref{sec:model-map}, we employed tags such as \texttt{dataset:eleutherai/pile}, \texttt{dataset:eleutherai/the\_pile}, \texttt{dataset:eleutherai/the\_pile\_deduplicated}, and \texttt{arxiv:2101.00027}.

\subsection{How the model type was determined}

\begin{table}[t!]
    \centering
    \begin{tabular}{lr}
    \toprule
Model type & Models \\
\midrule
\texttt{llama-1} & 69\\
\texttt{llama-2} & 223\\
\texttt{llama-3} & 62\\
\texttt{llama} & 217\\
\texttt{mistral} & 232\\
\texttt{gpt\_neox} & 54 \\
\texttt{deepseek} & 26 \\
\texttt{gptj} & 19 \\
\texttt{gemma} & 18 \\
\texttt{opt} & 15 \\
\texttt{bloom} & 12 \\
\texttt{falcon} & 11 \\
\texttt{qwen2} & 10 \\
\texttt{mixtral} & 9 \\
\texttt{mpt} & 6 \\
\texttt{stablelm} & 6 \\
\texttt{gpt\_neo} & 3 \\
\texttt{phi} & 3 \\
\texttt{gpt\_bigcode} & 3 \\
\texttt{phi3} & 3 \\
\texttt{xglm} & 3 \\
\texttt{rwkv} & 3 \\
\texttt{starcoder2} & 2 \\
\texttt{olmo} & 2 \\
\texttt{camelidae} & 2 \\
\texttt{codegen} & 2 \\
\texttt{deci} & 1 \\
\texttt{recurrent\_gemma} & 1 \\
\texttt{stablelm\_alpha} & 1 \\
\midrule
Total & 1,018\\
    \bottomrule
    \end{tabular}
    \caption{Number of models by model type.}
    \label{tab:model-type}
\end{table}

In principle, we used the value of \texttt{model\_type} in the config retrieved from the Hugging Face Hub API as the model type. However, out of the 1,018 language models we examined, there were 587 whose config \texttt{model\_type} was listed as \texttt{llama}. For these, we used the following procedure to determine whether they were the original Llama (llama-1), Llama-2, or Llama-3; if we were able to identify which version they were, we reclassified them as \texttt{llama-1}, \texttt{llama-2}, or \texttt{llama-3} accordingly.

\begin{enumerate}
    \item We checked the tags assigned to each model. Of these, 136 models that included any of \texttt{llama2}, \texttt{llama-2}, or \texttt{license:llama2} were classified as \texttt{llama-2}. Similarly, 39 models that included any of \texttt{llama3}, \texttt{llama-3}, or \texttt{license:llama3} were classified as \texttt{llama-3}.
    \item For the remaining 412 models whose classification was not determined by tags alone, we used the creation date and the model name (converted to lowercase) to make a decision. First, 69 models that were created prior to July 18, 2023 (the Llama-2 release date) were classified as \texttt{llama-1}. Next, 88 models whose lowercase model name contained either \texttt{llama2} or \texttt{llama-2} were classified as \texttt{llama-2}. Among those whose lowercase model name contained \texttt{llama3} or \texttt{llama-3}, 22 models whose creation date was after April 18, 2024 (the Llama-3 release date) were classified as \texttt{llama-3}.
    \item After following the steps above, the 217 models that could not be classified were left as \texttt{llama}.
\end{enumerate}

Furthermore, any model whose name prior to the slash (\texttt{/}) was \texttt{deepseek-ai} was defined as \texttt{deepseek}. In addition, even though \texttt{abacusai/Llama-3-Smaug-8B} was tagged with \texttt{license:llama2}, we manually reclassified it as \texttt{llama-3}.

Table~\ref{tab:model-type} shows the number of models classified into each model type.

\subsection{Basic information on the dataset}

\begin{table}
    \centering
    \begin{tabular}{lr}
    \toprule
    Text category & Texts \\
    \midrule
Pile-CC & 2,353 \\
PubMed Central & 1,763 \\
ArXiv & 1,172 \\
Github & 925 \\
FreeLaw & 837 \\
StackExchange & 712 \\
Wikipedia (en) & 567 \\
USPTO Backgrounds & 487 \\
PubMed Abstracts & 464 \\
Gutenberg (PG-19) & 251 \\
DM Mathematics & 151 \\
EuroParl & 83 \\
HackerNews & 67 \\
Ubuntu IRC & 54 \\
PhilPapers & 51 \\
NIH ExPorter & 41 \\
Enron Emails & 22 \\
\midrule
Total & 10,000\\
    \bottomrule
    \end{tabular}
    \caption{Number of texts in each text category.}
    \label{tab:text-category}
\end{table}

The dataset used in our experiments consists of a total of 10,000 texts, which are divided into 17 text categories. Table~\ref{tab:text-category} shows the number of texts in each category.

To assign colors to the text categories, we first compute the average text embedding for each category in the Pile using \texttt{simcse-roberta-large}~\cite{gao2021simcse}. Next, we calculate a tour over the 17 average embedding vectors by solving the traveling salesman problem\footnote{Inspired by~\citet{DBLP:conf/naacl/Sato22,DBLP:conf/emnlp/YamagiwaTS24}.}. The TSP is solved using the nearest neighbor method to generate an initial tour, which is then refined using a 2-opt improvement procedure, and Euclidean distance is used as the metric. Based on the adjacency relationships along this tour, we segment the hue circle at equal intervals and color each category accordingly.

\subsection{Standard scores} \label{sec:standard-score}
In the three experiments described below, we use standardized values\footnote{By subtracting the mean from each value and dividing by the standard deviation, the data is transformed to have a mean of 0 and a variance of 1.}, or $Z$-score normalization, of both the log-likelihood and the benchmark scores calculated for the $K$ language models.
\begin{itemize}
    \item In Figures~\ref{fig:model-map-tsne-intro} and \ref{fig:Q-fig1}, where we define each language model's primary text category, we use the average log-likelihood for each category, standardized across all models.
    \item In Section~\ref{sec:model-map}, to determine each language model's primary task, we standardize each task's score across the $K$ models.
    \item Furthermore, in the data leakage detection described in Section~\ref{sec:model-map}, we use the difference between the standardized mean log-likelihood and the standardized 6-TaskMean score as the indicator.
\end{itemize}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/app_Q_figure1_v3.pdf}
    \caption{Model maps obtained by double centered log-likelihood matrix $\bmQ$. These maps correspond to Figure~\ref{fig:model-map-tsne-intro}. (Top) Colors indicate model types.  
(Bottom) Colors indicate the text category in which each model attains the highest 
standardized log-likelihood score among 17 categories.}
    \label{fig:Q-fig1}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/app_Q_figure5_v2.pdf}
    \caption{Model maps obtained by double centered log-likelihood matrix $\bmQ$. These maps correspond to Figure~\ref{fig:modelmap-logp-score-task}. These maps are illustrating model performance.  
From left to right, the panels show each model's mean log-likelihood, 6-TaskMean score, and the ``primary task,'' which refers to the task where each model achieves the highest standardized score among the six tasks, color-coded accordingly.  
The color bar is clipped at the 10th percentile for mean log-likelihood and 6-TaskMean, with darker colors indicating better performance.  
In the primary task panel, models with standardized scores below zero across all six tasks are labeled as ``All Under 0.''}
    \label{fig:Q-fig5}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/app_Q_modelmap-size-date_v1.pdf}
    \caption{Model maps obtained by double centered log-likelihood matrix $\bmQ$ color-coded by (Left) model size and (Right) model creation date.}
    \label{fig:Q-size-date}
\end{figure}

\subsection{Hierarchical clustering settings}
Figure~\ref{fig:matrix-Q} displays the double-centered log-likelihood matrix $\bmQ$, with hierarchical clustering applied to both its rows and columns.
We implemented the clustering using SciPy~\citep{2020SciPy-NMeth}.
Distance matrices were computed using \texttt{scipy.spatial.distance.pdist}, and clustering was performed using \texttt{scipy.cluster.hierarchy.linkage}.
For clustering models, we used \texttt{sqeuclidean} as the metric and \texttt{median} as the linkage method.
For clustering texts, we used \texttt{correlation} as the metric and \texttt{average} as the linkage method.
For the hierarchical clustering shown in Fig.~\ref{fig:cluster-100-models}, which presents a dendrogram of 100 language models, we used \texttt{sqeuclidean} as the metric and \texttt{median} as the linkage method.
The vertical axis of the dendrogram uses the symmetric logarithmic scale (with a linear threshold of 250) implemented in \texttt{matplotlib}. To ensure that the values on the vertical axis correspond to the Kullback-Leibler divergence, we used the $\bmq$-coordinates divided by $\sqrt{2N}$.

\section{Additional Model Maps} \label{sec:add-model-map}

In this section, we present additional model maps, including a figure that lists all the model names and a map obtained through dimensionality reduction of the double-centered log-likelihood matrix $\bmQ$.

\subsection{Model map via the double centered log-likelihood matrix} \label{sec:model-map-Q}

In the main text, we use a model map generated by dimensionality reduction of the log-likelihood matrix $\bm{L}$.
Here, in Figs.~\ref{fig:Q-fig1}, \ref{fig:Q-fig5} and \ref{fig:Q-size-date}, we present model maps obtained by dimensionality reduction of the double-centered log-likelihood matrix $\bmQ$.

\subsection{Model map with model names} \label{sec:model-map-with-names}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/modelmap-labeled-L_v3.pdf}
    \caption{Model map obtained by dimensionality reduction of the log-likelihood matrix $\bm{L}$. Each point on the model map is labeled with the corresponding model name.}
    \label{fig:large-map}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/modelmap-textcolor-labeled-L_v1.pdf}
    \caption{Model map obtained by dimensionality reduction of the log-likelihood matrix $\bm{L}$. Each point on the model map is labeled with the corresponding model name. Colors indicate the model's ``primary text category,'' the text category where the model achieves the highest standardized log-likelihood score among 17 categories.}
    \label{fig:large-map-text}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/modelmap-labeled-Q_v2.pdf}
    \caption{Model map obtained by dimensionality reduction of the double-centered log-likelihood matrix $\bmQ$. Each point on the model map is labeled with the corresponding model name.}
    \label{fig:large-map-Q}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{images/modelmap-textcolor-labeled-Q_v1.pdf}
    \caption{Model map obtained by dimensionality reduction of the double-centered log-likelihood matrix $\bmQ$. Each point on the model map is labeled with the corresponding model name. Colors indicate the model's ``primary text category,'' the text category where the model achieves the highest 
    standardized log-likelihood score among 17 categories.}
    \label{fig:large-map-Q-text}
\end{figure*}

We present figures that display the model names corresponding to each point on the model maps defined in Section~\ref{sec:model-map}. For both the log-likelihood matrix $\bm{L}$ and the double-centered log-likelihood matrix $\bm{Q}$, we provide two types of maps: one colored by model type and another colored by primary text category. Figures~\ref{fig:large-map} and \ref{fig:large-map-text} show the maps obtained by applying dimensionality reduction to $\bm{L}$, while Figures~\ref{fig:large-map-Q} and \ref{fig:large-map-Q-text} show the maps obtained using $\bm{Q}$.

\section{Table of Nearest Neighbor Models}\label{sec:top10-tables}

Table~\ref{tab:nearest-neighbor-16} presents the top 10 nearest neighbors among the 1,018 language models for each of the models highlighted in the top panel of Figure~\ref{fig:model-map-tsne-intro}. Each sub-table corresponds to a specific model of interest, listing its nearest neighbors in descending order of the KL divergence. From this table, we can see that similar models tend to cluster together, exhibiting relatively small KL divergence values. Additionally, the values in parentheses denote the KL divergence measured in bits per byte of text. This conversion makes it easier to interpret the information cost per byte when comparing predictive distributions of different models.

\begin{table*}[p]
\centering
\begin{adjustbox}{width=0.9\linewidth}
\begin{tabular}{lrlrlrlrlrlrlrlrlrlrlrlr}
\toprule
bigcode/starcoder2-7b & KL (/byte) & codellama/CodeLlama-7b-Instruct-hf & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
bigcode/starcoder2-3b & 769.780 (1.142) & NousResearch/CodeLlama-7b-hf & 48.192 (0.072) \\
stabilityai/stable-code-3b & 1606.483 (2.384) & codellama/CodeLlama-7b-hf & 48.192 (0.072) \\
deepseek-ai/deepseek-coder-6.7b-base & 1687.186 (2.503) & codellama/CodeLlama-13b-Instruct-hf & 138.606 (0.206) \\
EleutherAI/llemma\_7b & 1892.812 (2.808) & TheBloke/CodeLlama-13B-Instruct-fp16 & 138.627 (0.206) \\
deepseek-ai/deepseek-coder-7b-base-v1.5 & 2031.482 (3.014) & Nexusflow/NexusRaven-V2-13B & 147.114 (0.218) \\
deepseek-ai/DeepSeek-Coder-V2-Lite-Base & 2042.158 (3.030) & codellama/CodeLlama-13b-hf & 158.786 (0.236) \\
deepseek-ai/deepseek-coder-7b-instruct-v1.5 & 2045.330 (3.035) & NousResearch/CodeLlama-13b-hf & 158.949 (0.236) \\
deepseek-ai/deepseek-coder-6.7b-instruct & 2140.788 (3.176) & OpenAssistant/codellama-13b-oasst-sft-v10 & 219.381 (0.326) \\
deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct & 2149.008 (3.189) & HiTZ/GoLLIE-7B & 225.804 (0.335) \\
meta-math/MetaMath-Llemma-7B & 2184.274 (3.241) & WhiteRabbitNeo/WhiteRabbitNeo-13B-v1 & 260.047 (0.386) \\
\midrule
deepseek-ai/deepseek-coder-1.3b-base & KL (/byte) & deepseek-ai/deepseek-llm-7b-base & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
deepseek-ai/deepseek-coder-1.3b-instruct & 410.897 (0.610) & deepseek-ai/deepseek-moe-16b-base & 130.715 (0.194) \\
deepseek-ai/deepseek-coder-6.7b-instruct & 512.997 (0.761) & deepseek-ai/deepseek-llm-7b-chat & 245.408 (0.364) \\
deepseek-ai/deepseek-coder-6.7b-base & 601.002 (0.892) & deepseek-ai/deepseek-moe-16b-chat & 247.889 (0.368) \\
bigcode/starcoderbase-1b & 798.322 (1.185) & deepseek-ai/DeepSeek-V2-Lite & 306.530 (0.455) \\
bigcode/starcoderbase-7b & 1249.899 (1.855) & deepseek-ai/ESFT-vanilla-lite & 307.487 (0.456) \\
NTQAI/Nxcode-CQ-7B-orpo & 1307.618 (1.940) & deepseek-ai/DeepSeek-V2-Lite-Chat & 585.090 (0.868) \\
Qwen/CodeQwen1.5-7B-Chat & 1317.166 (1.954) & mistralai/Mistral-7B-Instruct-v0.1 & 913.708 (1.356) \\
bigcode/starcoder2-3b & 1858.329 (2.757) & statking/zephyr-7b-sft-full-orpo & 1089.127 (1.616) \\
Salesforce/codegen-6B-multi & 1974.095 (2.929) & Severian/ANIMA-Phi-Neptune-Mistral-7B & 1140.161 (1.692) \\
google/codegemma-2b & 2115.462 (3.139) & sethuiyer/Medichat-Llama3-8B & 1157.599 (1.718) \\
\midrule
EleutherAI/gpt-neo-1.3B & KL (/byte) & EleutherAI/pythia-12b & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
EleutherAI/gpt-neo-2.7B & 218.723 (0.325) & matsuo-lab/weblab-10b & 139.270 (0.207) \\
EleutherAI/pythia-1.4b & 289.377 (0.429) & h2oai/h2ogpt-oig-oasst1-256-6\_9b & 159.560 (0.237) \\
EleutherAI/pythia-1b-deduped & 413.204 (0.613) & Salesforce/codegen-6B-nl & 175.546 (0.260) \\
HWERI/pythia-1.4b-deduped-sharegpt & 421.551 (0.625) & EleutherAI/gpt-j-6b & 186.613 (0.277) \\
beaugogh/pythia-1.4b-deduped-sharegpt & 421.551 (0.625) & TehVenom/Dolly\_Malion-6b & 231.701 (0.344) \\
EleutherAI/pythia-1.4b-deduped & 447.122 (0.663) & TehVenom/PPO\_Shygmalion-6b & 233.159 (0.346) \\
PygmalionAI/metharme-1.3b & 448.970 (0.666) & TehVenom/Dolly\_Shygmalion-6b-Dev\_V8P2 & 236.753 (0.351) \\
RWKV/rwkv-raven-1b5 & 512.789 (0.761) & TehVenom/PPO\_Pygway-V8p4\_Dev-6b & 237.270 (0.352) \\
databricks/dolly-v2-3b & 525.574 (0.780) & TehVenom/GPT-J-Pyg\_PPO-6B-Dev-V8p4 & 245.015 (0.364) \\
EleutherAI/pythia-2.8b-deduped & 587.699 (0.872) & TehVenom/PPO\_Shygmalion-V8p4\_Dev-6b & 245.364 (0.364) \\
\midrule
facebook/opt-6.7b & KL (/byte) & google/codegemma-2b & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
KoboldAI/OPT-6.7B-Erebus & 0.685 (0.001) & deepseek-ai/deepseek-coder-1.3b-instruct & 1654.767 (2.455) \\
KoboldAI/OPT-6.7B-Nerybus-Mix & 78.604 (0.117) & bigcode/starcoderbase-1b & 1812.768 (2.690) \\
KoboldAI/OPT-6B-nerys-v2 & 124.520 (0.185) & deepseek-ai/deepseek-coder-1.3b-base & 2115.462 (3.139) \\
facebook/opt-13b & 196.634 (0.292) & bigcode/gpt\_bigcode-santacoder & 2644.348 (3.924) \\
KoboldAI/OPT-13B-Nerybus-Mix & 230.046 (0.341) & deepseek-ai/deepseek-coder-6.7b-instruct & 2675.004 (3.969) \\
KoboldAI/OPT-13B-Erebus & 237.845 (0.353) & Qwen/CodeQwen1.5-7B-Chat & 2759.243 (4.094) \\
KoboldAI/OPT-13B-Nerys-v2 & 273.410 (0.406) & NTQAI/Nxcode-CQ-7B-orpo & 2767.578 (4.106) \\
facebook/opt-2.7b & 327.851 (0.486) & Salesforce/codegen-6B-multi & 2859.701 (4.243) \\
KoboldAI/OPT-2.7B-Nerybus-Mix & 427.279 (0.634) & bigcode/starcoderbase-7b & 2990.326 (4.437) \\
KoboldAI/OPT-2.7B-Erebus & 446.712 (0.663) & deepseek-ai/deepseek-coder-6.7b-base & 3282.352 (4.870) \\
\midrule
google/gemma-7b & KL (/byte) & lmsys/vicuna-13b-v1.3 & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
SeaLLMs/SeaLLM-7B-v2.5 & 247.533 (0.367) & TheBloke/stable-vicuna-13B-HF & 195.152 (0.290) \\
VAGOsolutions/SauerkrautLM-Gemma-7b & 257.986 (0.383) & Yhyu13/chimera-inst-chat-13b-hf & 197.130 (0.292) \\
lemon-mint/gemma-ko-7b-instruct-v0.62 & 344.582 (0.511) & junelee/wizard-vicuna-13b & 219.148 (0.325) \\
google/gemma-2b & 503.918 (0.748) & TheBloke/wizard-vicuna-13B-HF & 219.148 (0.325) \\
google/codegemma-7b & 596.383 (0.885) & TheBloke/UltraLM-13B-fp16 & 219.689 (0.326) \\
PathFinderKR/Waktaverse-Llama-3-KO-8B-Instruct & 814.680 (1.209) & NousResearch/Nous-Hermes-13b & 221.324 (0.328) \\
FlagAlpha/Llama3-Chinese-8B-Instruct & 817.954 (1.214) & project-baize/baize-v2-13b & 233.813 (0.347) \\
FairMind/Llama-3-8B-4bit-UltraChat-Ita & 832.782 (1.236) & TheBloke/guanaco-13B-HF & 234.090 (0.347) \\
migtissera/Llama-3-8B-Synthia-v3.5 & 842.861 (1.251) & openaccess-ai-collective/minotaur-13b-fixed & 235.470 (0.349) \\
Orenguteng/Llama-3-8B-Lexi-Uncensored & 844.748 (1.253) & openaccess-ai-collective/wizard-mega-13b & 238.073 (0.353) \\
\midrule
medalpaca/medalpaca-7b & KL (/byte) & meta-llama/Llama-2-13b-hf & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
TheBloke/guanaco-7B-HF & 285.806 (0.424) & TaylorAI/Flash-Llama-13B & 0.000 (0.000) \\
eachadea/vicuna-7b-1.1 & 336.518 (0.499) & TheBloke/Llama-2-13B-fp16 & 0.002 (0.000) \\
TehVenom/Pygmalion-Vicuna-1.1-7b & 369.003 (0.548) & StudentLLM/Alpagasus-2-13b-QLoRA-merged & 4.502 (0.007) \\
lmsys/vicuna-7b-v1.3 & 374.469 (0.556) & CHIH-HUNG/llama-2-13b-FINETUNE2\_TEST\_2.2w & 7.610 (0.011) \\
jphme/orca\_mini\_v2\_ger\_7b & 383.382 (0.569) & garage-bAInd/Platypus2-13B & 10.373 (0.015) \\
ajibawa-2023/Uncensored-Jordan-7B & 384.396 (0.570) & CHIH-HUNG/llama-2-13b-dolphin\_5w & 14.386 (0.021) \\
bofenghuang/vigogne-7b-instruct & 390.797 (0.580) & CHIH-HUNG/llama-2-13b-OpenOrca\_5w & 14.982 (0.022) \\
TheBloke/airoboros-7b-gpt4-fp16 & 391.991 (0.582) & CHIH-HUNG/llama-2-13b-FINETUNE4\_3.8w-r16-gate\_up\_down-test1 & 17.256 (0.026) \\
TheBloke/tulu-7B-fp16 & 423.476 (0.628) & CHIH-HUNG/llama-2-13b-FINETUNE4\_addto15k\_4.5w-r16-gate\_up\_down & 17.405 (0.026) \\
TehVenom/Pygmalion\_AlpacaLora-7b & 431.540 (0.640) & CHIH-HUNG/llama-2-13b-FINETUNE4\_compare15k\_4.5w-r16-gate\_up\_down & 19.427 (0.029) \\
\midrule
meta-llama/Llama-2-7b-hf & KL (/byte) & meta-llama/Meta-Llama-3-8B & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
meta-llama/Llama-2-7b-hf & 0.000 (0.000) & Undi95/Meta-Llama-3-8B-hf & 0.003 (0.000) \\
TheTravellingEngineer/llama2-7b-chat-hf-v4 & 0.000 (0.000) & dfurman/Llama-3-8B-Orpo-v0.1 & 6.994 (0.010) \\
ibranze/araproje-llama2-7b-hf & 0.000 (0.000) & migtissera/Tess-2.0-Llama-3-8B & 28.829 (0.043) \\
yeen214/test\_llama2\_7b & 0.000 (0.000) & freewheelin/free-llama3-dpo-v0.2 & 67.783 (0.101) \\
TheTravellingEngineer/llama2-7b-chat-hf-v2 & 0.000 (0.000) & jondurbin/bagel-8b-v1.0 & 110.365 (0.164) \\
NewstaR/Starlight-7B & 0.003 (0.000) & migtissera/Llama-3-8B-Synthia-v3.5 & 128.141 (0.190) \\
Delcos/Mistral-Pygmalion-7b & 14.809 (0.022) & nvidia/Llama3-ChatQA-1.5-8B & 128.776 (0.191) \\
elliotthwang/elliott\_Llama-2-7b-hf & 16.612 (0.025) & ruslanmv/Medical-Llama3-8B & 138.566 (0.206) \\
garage-bAInd/Platypus2-7B & 22.811 (0.034) & FairMind/Llama-3-8B-4bit-UltraChat-Ita & 199.748 (0.296) \\
Lazycuber/L2-7b-Base-Guanaco-Uncensored & 23.345 (0.035) & NousResearch/Hermes-2-Theta-Llama-3-8B & 222.357 (0.330) \\
\midrule
meta-math/MetaMath-Mistral-7B & KL (/byte) & mistralai/Mistral-7B-v0.3 & KL (/byte) \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4}
Weyaxi/MetaMath-NeuralHermes-2.5-Mistral-7B-Linear & 43.095 (0.064) & MaziyarPanahi/Mistral-7B-v0.3 & 0.000 (0.000) \\
Weyaxi/MetaMath-Tulpar-7b-v2-Slerp & 81.882 (0.121) & mistral-community/Mistral-7B-v0.2 & 7.221 (0.011) \\
Weyaxi/MetaMath-OpenHermes-2.5-neural-chat-v3-3-Slerp & 101.097 (0.150) & unsloth/mistral-7b-v0.2 & 7.222 (0.011) \\
Q-bert/Bumblebee-7B & 106.376 (0.158) & mistralai/Mistral-7B-v0.1 & 22.038 (0.033) \\
OpenPipe/mistral-ft-optimized-1227 & 110.089 (0.163) & Cartinoe5930/Llama2\_init\_Mistral & 29.819 (0.044) \\
Toten5/Marcoroni-neural-chat-7B-v2 & 114.004 (0.169) & Locutusque/Hercules-3.1-Mistral-7B & 32.099 (0.048) \\
ignos/Mistral-T5-7B-v1 & 114.320 (0.170) & migtissera/Synthia-7B-v3.0 & 33.418 (0.050) \\
Weyaxi/MetaMath-Chupacabra-7B-v2.01-Slerp & 114.414 (0.170) & uukuguy/speechless-zephyr-code-functionary-7b & 35.723 (0.053) \\
Q-bert/Optimus-7B & 117.829 (0.175) & uukuguy/zephyr-7b-alpha-dare-0.85 & 35.745 (0.053) \\
Weyaxi/MetaMath-NeuralHermes-2.5-Mistral-7B-Ties & 127.457 (0.189) & crumb/apricot-wildflower-20 & 40.907 (0.061) \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{
Top 10 nearest neighbors among the 1,018 language models for the models labeled in the top panel of Fig.~\ref{fig:model-map-tsne-intro}.
The KL divergence is computed using formula~(\ref{eq:main-KL-xi}) in Section~\ref{sec:model-text}.
The values in parentheses represent the KL divergence measured in bits per byte of text (Section~\ref{sec:byte-normalized-KL}), obtained by multiplying the KL divergence by 0.001484.
}
\label{tab:nearest-neighbor-16}
\end{table*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{images/5fold_split_all_tasks.png}
    \caption{
Scatter plots of predicted scores and benchmark scores for six benchmark tasks. 
Additionally, results for predicting 6-TaskMean (identical to Fig.~\ref{fig:pred_and_gt_by_mean_logp}) and the mean log-likelihood are also shown. 
Each point is color-coded by the mean log-likelihood, with higher mean log-likelihood values generally corresponding to higher task scores. 
For better visualization, the color bar range is clipped to the 10th--100th percentile. 
}
    \label{fig:all_pred_and_gt_by_mean_logp}
\end{figure*}

\section{Details of Weight Interpolation}

\label{app:weight-parameter}

In this section, we describe the experimental details concerning the relationship between model coordinates and model weights, as introduced in Section \ref{sec:weight-parameter}.

\paragraph{Constructing the weight grid.}
Let $p_0$ denote the base model, and $p_1$ and $p_2$ denote the fine-tuned models derived from $p_0$. We denote the weight parameter vectors of these models as $W_0$, $W_1$, and $W_2$, respectively. To construct the weight grid, we merged the model weights using the following linear operation:
\begin{equation}
W_{\alpha,\beta} = W_0 + \alpha(W_1 - W_0) + \beta(W_2 - W_0),
\end{equation}
where the merge ratios $\alpha, \beta \in \mathbb{R}$ were chosen from 36 evenly spaced combinations within the interval $[0,1]$: $\{0.0, 0.2, 0.6, 0.8, 1.0\}$. The original models $W_0$, $W_1$, and $W_2$ correspond to $W_{0,0}$, $W_{1,0}$, and $W_{0,1}$, respectively. When $\alpha + \beta \leq 1$, the operation corresponds to linear interpolation between models.
Even among models with the same architecture, the sizes of the embedding/unembedding matrices may differ. In such cases, we truncated or reshaped the weight parameters to match the base model.

\paragraph{Computing model coordinates.}
For each composed model $p_{\alpha, \beta}$ with weight $W_{\alpha, \beta}$, we computed the model coordinates $\bm{q}_{\alpha,\beta}$ following the method described in Section \ref{sec:model-text}. The tokenizer of the base model was used to ensure consistency. The text data consists of 1,000 randomly sampled sentences from the dataset of 10,000 sentences created in Section \ref{sec:text-set}.

\paragraph{Selection of models.}
We set the base model $p_0$ as \texttt{Llama-2-7b-hf}. Among the fine-tuned models based on \texttt{Llama-2-7b-hf}, we selected the two most downloaded models: \texttt{vicuna-7b-v1.5} and \texttt{Llama-2-7b-chat-hf}, denoted as $p_1$ and $p_2$, respectively.

\paragraph{Visualization.}
Figure \ref{fig:weight-interpolation-grids} shows the linearly merged models, visualized in both weight space and log-likelihood space. The corners of the grid are labeled with their corresponding $(\alpha, \beta)$ values.

In the left panel, we visualized $W_{\alpha, \beta}$ in the weight space. Since the dimensionality of $W_{\alpha, \beta}$, i.e., the number of model parameters, is extremely high, we employed a 2D projection method using the norms of the difference vectors $r_1 = \left\|W_1 - W_0\right\|_2$, $r_2 = \left\|W_2 - W_0\right\|_2$, and the angle between them, $\phi = \arccos((W_1 - W_0)^\top (W_2 - W_0) / r_1 r_2)$. Each point was placed at $(\alpha r_1 + \beta r_2\cos\phi, \beta r_2\sin\phi)$.

In the right panel, we visualized the model coordinates $\bmq_{\alpha, \beta}$ by Principal Component Analysis (PCA). Each model $p_{\alpha, \beta}$ was mapped onto the $\bmq$-coordinate system to analyze the structure of the interpolated models.



\begin{table*}[t!]
\scriptsize
    \centering
    \begin{tabular}{@{\hspace{0.95em}}lr@{\hspace{0.95em}}r@{\hspace{0.95em}}r@{\hspace{0.95em}}r@{\hspace{0.95em}}r@{\hspace{0.95em}}r@{\hspace{0.95em}}r@{\hspace{0.95em}}r@{\hspace{0.95em}}}
    \toprule
        & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K & Average & mean log-likelihood\\
        \midrule
        Pearson's $r$ & 0.969$\pm$0.002 & 0.943$\pm$0.004 & 0.959$\pm$0.003 & 0.954$\pm$0.001 & 0.961$\pm$0.002 & 0.930$\pm$0.002 & 0.973$\pm$0.001 & 0.993$\pm$0.001 \\
        Spearman's $\rho$ & 0.975$\pm$0.001 & 0.971$\pm$0.002 & 0.966$\pm$0.003 & 0.933$\pm$0.001 & 0.971$\pm$0.002 & 0.892$\pm$0.001 & 0.976$\pm$0.001 & 0.989$\pm$0.001 \\
    \bottomrule
    \end{tabular}
    \caption{
This table presents the mean and standard deviation of correlation coefficients between predicted and actual benchmark scores.  
These coefficients were computed using ridge regression across five different data splits.  
Results for predicting 6-TaskMean and the mean log-likelihood are also included.
}
    \label{tab:result-regression_mean_and_std}
\end{table*}

\section{Details of Model Performance Prediction} \label{app:model_pred}

This section provides additional details on the prediction of benchmark scores using model coordinates, as discussed in Section~\ref{sec:predict-performance}.

\subsection{Details of ridge regression}\label{app:model_pred_training_details}
As described in Section~\ref{sec:setting_for_ridge}, ridge regression requires setting a regularization strength parameter, $\alpha$. 
To determine $\alpha$ from $\{10^1,\ldots,10^9\}$, we performed a five-fold cross-validation within each training dataset\footnote{The training dataset consisted of four out of the five folds obtained by splitting the dataset for each benchmark task.}.
As a post-processing step, we clipped the predicted scores to the range $[0, 100]$.

For the setting where the target variable $\mathbf{f}$ was replaced with the mean log-likelihood $(\bar \ell_1,\ldots,\bar \ell_K)\in\mathbb{R}^K$, we searched for $\alpha$ within $\{10^{-4},\ldots,10^4\}$ and did not apply clipping as a post-processing step.

\subsection{Details of prediction results}\label{app:model_pred_result_details}
Figure~\ref{fig:all_pred_and_gt_by_mean_logp} shows scatter plots of predicted scores and actual benchmark scores for each benchmark task, as well as for 6-TaskMean and the mean log-likelihood. 
As in Fig.~\ref{fig:pred_and_gt_by_mean_logp}, the scatter plots show strong correlations for all six benchmark tasks and for the mean log-likelihood.

To account for randomness, we ran five different data splits when predicting each benchmark score. 
The final predicted score was the average of these five runs. 
For each split, we computed the correlation coefficients between the predicted scores and the actual benchmark scores. 
Table~\ref{tab:result-regression_mean_and_std} presents their mean and standard deviation, and shows a similar trend as Table~\ref{tab:result-regression}.

\section{Model List} \label{app:model_list}
Table~\ref{tab:model_table} lists the 1,018 models used in this study, sorted alphabetically by their names.
The BibTeX entries cited for each model were determined through the following procedure. 

First, we extracted the BibTeX entries available in each model's Hugging Face model card\footnote{\url{https://github.com/huggingface/huggingface_hub}.}.
If the BibTeX entry was missing a year of publication, we filled it in with the model's creation date\footnote{\url{https://github.com/sciunto-org/python-bibtexparser}.}. 
Additionally, we generated BibTeX entries using the arXiv IDs found in the model card tags by querying the arXiv API\footnote{\url{https://github.com/lukasschwab/arxiv.py}.}. This process resulted in a set of BibTeX entries for each model. 

Next, we manually checked pairs of different BibTeX entries where the title similarity\footnote{We used Python's \texttt{difflib.SequenceMatcher}.} was high, or the authors matched, to determine whether they corresponded to the same source.
This step allowed us to create groups of BibTeX entries that were considered identical.

Then, for each BibTeX group, we selected a representative entry as follows. 
Within each group, the entry most frequently cited by the models was chosen as the representative. 
If multiple candidates met this criterion, we prioritized BibTeX entries generated from arXiv IDs when available. 
If no such entry existed, we selected the one with the longest string. 

Finally, we replaced each model's BibTeX entry with the representative entry from its corresponding group. 
Any selected BibTeX entry that contained typos or formatting errors was manually corrected based on compilation errors. 
If the author information was incomplete, we corrected it manually by checking the source.

Note that for \texttt{google/codegemma-2b} and \texttt{deepseek-ai/deepseek-llm-7b-base} in Table~\ref{tab:nearest-neighbor-models}, as well as for \texttt{deepseek-ai/deepseek-coder-1.3b-base} and \texttt{mistralai/Mistral-7B-v0.3} in Table~\ref{tab:nearest-neighbor-16}, we manually prepared the BibTeX entries for citation based on their respective sources. We also used the same BibTeX entries for all other models that were considered to be of the same type.

\onecolumn
\input{model_table}

\end{document}
