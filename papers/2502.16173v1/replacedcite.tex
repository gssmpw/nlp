\section{Related Work}
\label{sec:related-work}
In recent years, research on comparing large language models (LLMs) has gained attention.  
This section provides an overview of existing studies from three perspectives: model parameters, activations\footnote{In general, ``activations'' refer to the intermediate outputs of Transformer models (e.g., the residual stream or neurons)____.}, and probability distributions.

\paragraph{Comparison of model parameters.}
One approach to comparing LLMs is to analyze their parameters.
____ proposed a statistical framework for evaluating parameter similarity between different models and introduced a method for determining whether these models were trained independently. 
Additionally, ____ focused on parameter changes due to task adaptation, specifically analyzing task vectors\footnote{The difference in parameters before and after fine-tuning is referred to as a task vector, and arithmetic operations on these vectors are effective____.}. They proposed a method to mitigate interference when integrating task vectors from different models. Specifically, by reducing redundant numerical components and adjusting for conflicting signs, their approach enables effective model merging.

\paragraph{Comparison of activations.}
Comparisons of LLMs based on activations have also been studied.  
____ quantified the similarity between LLMs by measuring the cosine similarity of activation differences for linguistic minimal pairs.
In particular, they used datasets such as BLiMP____ and showed that model similarity is significantly influenced by the pre-training dataset.

\paragraph{Comparison of probability distributions.}
Several approaches compare LLMs using probability distributions.
____ proposed a method for computing coefficients in parameter ensembling by providing the same input text to two models and comparing the softmax probability distributions at each token. 
Specifically, they used KL divergence and summed the results to derive appropriate coefficients.
Furthermore, ____ proposed a similarity metric based on the conditional probabilities of LLMs and introduced a method for calculating the phylogenetic distance between different models. 
 Additionally, a method has been proposed for measuring differences in conditional probabilities based on prompts using KL divergence____.