\section{MLLM Comparative Study}
\subsection{Experimental Setup and Evaluation Metrics}
% Recent advances in MLLMs have demonstrated significant potential in general-purpose video understanding. However, as noted in~\cite{liu2024bench}, state-of-the-art models designed for video-level understanding face challenges in handling fine-grained tasks, such as temporal video grounding.

To test the performance of MLLMs on our parent-child joint-attention detection task, a fine-grained temporal video grounding challenge, we selected three state-of-the-art MLLMs to evaluate: GPT-4o-2024-08-06\footnote{\url{https://openai.com/index/hello-gpt-4o/}}, Gemini 1.5 Flash\footnote{\url{https://deepmind.google/technologies/gemini/flash/}}, and Video-ChatGPT~\cite{maaz2023video}.

The Gemini 1.5 model directly supports audio and video processing, allowing us to input the entire video seamlessly. In contrast, Video-ChatGPT can only process videos without audio, so we provided the full video without additional preprocessing. GPT-4o, however, does not natively support direct video processing. To address this, we extracted video frames and converted them into a Base64-encoded array of images. 
We also used WhisperX~\cite{bain2023whisperx} to transcribe the videos' audio and manually enriched these transcriptions with notes for greater informativeness. 
For instance, we included annotations such as \dquote{the child is laughing} and descriptions of background sounds, which helped compensate for the unclear speech often observed in younger children. These transcriptions were used as text input, serving as a substitute for audio input for both Video-ChatGPT and GPT-4o.

We designed Instruction Templates (see Table~\ref{tab:instructions}) inspired by Liu et al.~\cite{liu2024bench} and based on our interviews with SLPs. 
Our interviews identified key criteria for determining joint attention, such as consistent eye contact and full engagement between the child and parent.

\begin{table*}[!htb]
\small
  \caption{Instruction templates in our task. \textcolor{orange}{<time>} denotes the timestamp representation, e.g., "23.6s" \textcolor{NavyBlue}{<description>} denotes the brief summary of the segment focused on the child's joint attention behaviour. \textcolor{Plum}{<label>} denotes the child joint attention quality of the segment.}
  \label{tab:instructions}
  \Description{}
  \begin{tabular}{p{.5\linewidth} p{.4\linewidth}}
    \toprule
    \textbf{Instruction} &  \textbf{Example Response}\\
    \midrule
    You are given a video about parent-child interactions. Watch the video carefully and your task is to:

    1. Identify the key moments where the child's joint attention occurs. Child joint attention is defined as the child making eye contact and being fully engaged with the parent through interaction.
    
    2. Specify the timestamps for when each moment starts and ends.

    3. Classify the quality of the child's joint attention into "Strong" and "Poor".

    For example:
    
    - Timestamp: 23.6s - 26.8s
    
    - Description: The child looks at his mother as she gives him instructions.
    
    - Quality: Strong
    &
    Timestamp: \textcolor{orange}{<time>} - \textcolor{orange}{<time>} and \textcolor{orange}{<time>} - \textcolor{orange}{<time>}
    
    Description: \textcolor{NavyBlue}{<description>}
    
    Quality: \textcolor{Plum}{<label>}
    \\
  \bottomrule
\end{tabular}
\end{table*}

\subsubsection{Evaluation Method of MLLMs Output Quality and Content Objectivity}

To assess the objectivity and quality of the models' responses, we define four metrics building on key aspects emphasized by SLPs in their annotations:

\begin{itemize}
    \item \textbf{Time Sensitivity}: Proportion of responses that include \textcolor{orange}{<time>} - \textcolor{orange}{<time>}.
    \item \textbf{Description Accuracy}: Proportion of accurate \textcolor{NavyBlue}{<description>}s within the specified \textcolor{orange}{<time>} - \textcolor{orange}{<time>} in the responses.
    \item \textbf{Eye-Contact Sensitivity}: Proportion of accurate \textcolor{NavyBlue}{<description>} that include eye contact information.
    \item \textbf{Eye-Contact Accuracy}: Proportion of accurate \textcolor{NavyBlue}{<description>} that include accurate eye contact information.
\end{itemize}

We reviewed the responses against the original videos. For each response,  we verified the inclusion of time information, descriptions' accuracy, and eye-contact events' presence and correctness. Annotations were assigned based on these criteria, ensuring a reliable benchmark for comparing the models' performance across the four evaluation metrics.

\subsubsection{Evaluation Method of MLLMs Temporal Grounding in Joint Attention Segments}

To evaluate the temporal understanding capabilities of MLLMs in detecting strong and poor joint attention segments, we used mean Intersection over Union (mIoU) and Recall at IoU thresholds (R@m). These metrics assess the alignment quality between predicted and ground truth time segments. IoU measures the overlap between the predicted and ground truth time segments, defined as the ratio of the intersection to the union of these two time intervals. A higher IoU indicates better alignment, and mIoU represents the average IoU across all segments. R@m evaluates the proportion of ground truth segments with at least one predicted segment with sufficient overlap based on predefined IoU thresholds.

The evaluation process used the intersection of annotations from the two SLPs as the ground truth. Each model outputted its predicted timestamps for strong and poor joint attention segments. Using these predictions, we computed the metrics as follows:
\begin{itemize}
    \item For \textbf{mIoU}, we calculated the average overlap between the predicted and ground truth timestamps across all segments, ensuring that both temporal precision and alignment quality were considered.
    \item For \textbf{R@m}, we assessed how many ground truth segments had at least one predicted segment with sufficient overlap (defined by IoU thresholds of 0.3, 0.5, and 0.7).
\end{itemize}

\subsection{Performance Analysis and Findings}
We conducted experiments on three models. For GPT-4o and Gemini-1.5-Flash, we utilized their respective APIs. The model was deployed and executed locally on a Ubuntu 22.04 LTS server equipped with two NVIDIA RTX 3090 GPUs for Video-ChatGPT. 

\subsubsection{Evaluation Result of MLLMs response Quality and Content Objectivity}

Table~\ref{tab:model-evaluation} presents the evaluation of three models—GPT-4o, Gemini-1.5-Flash, and Video-ChatGPT—across four metrics.
For \textit{Time Sensitivity}, both GPT-4o and Gemini-1.5-Flash achieved perfect scores (\textit{100\%}), indicating their consistent inclusion of time information in the responses. 
In contrast, Video-ChatGPT performed poorly, with only a small proportion of responses with time information (\textit{36.36\%}).
GPT-4o demonstrated the strongest performance (\textit{90.17\%}) in \textit{Description Accuracy}, accurately describing tasks and scenes within the specified time ranges. 
Gemini-1.5-Flash and Video-ChatGPT performed worse, scoring (\textit{50.53\%}) and (\textit{35.71\%}), respectively. 
Gemini-1.5-Flash frequently misinterpreted interactions, often assuming that the child and parent had eye contact. At the same time, Video-ChatGPT exhibited significant scene misrecognition, such as mistaking a boy for both a boy and a girl.
For \textit{Eye-Contact Sensitivity}, GPT-4o showed limited inclusion of eye-contact information (\textit{23.64\%}), while Gemini-1.5-Flash demonstrated a much higher sensitivity (\textit{92.12\%}), actively identifying eye-contact cues. Video-ChatGPT failed to provide any eye-contact information in its responses.
For \textit{Eye-Contact Accuracy}, GPT-4o achieves the best performance (\textit{62.82\%}), avoiding overestimations of eye contact. Gemini-1.5-Flash has a lower accuracy (\textit{43.92\%}) due to frequent overestimation, assuming eye contact occurs more often than it does. Video-ChatGPT does not output eye-contact information, making it unsuitable for tasks requiring such data.

\begin{table*}[htb!]
\centering
\caption{Evaluation of MLLMs Output Quality and Content Objectivity Across Four Metrics: Time Sensitivity (proportion of responses including time information), Description Accuracy (accuracy of descriptions within specified time ranges), Eye-Contact Sensitivity (proportion of responses including eye contact information), and Eye-Contact Accuracy (accuracy of identified eye-contact information).}
\label{tab:model-evaluation}
\begin{tabular}{p{0.15\textwidth} p{0.18\textwidth} p{0.18\textwidth} p{0.18\textwidth} p{0.18\textwidth}}     
    \toprule
    \textbf{Model} & \textbf{Time Sens.} & \textbf{Description Acc.} & \textbf{Eye-Contact Sens.} & \textbf{Eye-Contact Acc.} \\
    \midrule
    GPT-4o & 100\% & 90.17\% & 23.64\% & 62.82\% \\
    Gemini-1.5-Flash & 100\% & 50.53\% & 92.12\% & 43.92\% \\
    Video-ChatGPT & 36.36\% & 35.71\% & 0\% & 0\% \\
    \bottomrule
\end{tabular}
\end{table*}

While GPT-4o and Gemini-1.5-Flash perform well in Time Sensitivity and Description Accuracy, all models show weaknesses in Eye-Contact Sensitivity and Accuracy. Since Video-ChatGPT performed poorly in time sensitivity, only GPT-4o and Gemini-1.5-Flash were considered for the temporal grounding evaluation in the following section.

\begin{table*}[htb!]
\centering
\caption{Comparison of Temporal Grounding Metrics between Gemini-1.5-Flash and GPT-4o for Strong, Poor, and Overall Joint Attention Segments. The metrics include Recall at IoU thresholds (R@0.3, R@0.5, and R@0.7) and mean Intersection over Union (mIoU). R@0.3, R@0.5, and R@0.7 represent the proportion of ground truth segments with at least one predicted segment with sufficient overlap at IoU thresholds of 0.3, 0.5, and 0.7, respectively. mIoU measures the average alignment quality between predicted and ground truth segments. "Strong" refers to the model's performance on segments with strong joint attention, "Poor" refers to segments with poor joint attention, and "Overall" reflects performance across all segments.}
\begin{tabular}{p{0.25\textwidth} p{0.15\textwidth} p{0.15\textwidth} p{0.15\textwidth} p{0.15\textwidth}}     \toprule
\textbf{Model}               & \textbf{R@0.3 (\%)} & \textbf{R@0.5 (\%)} & \textbf{R@0.7 (\%)} & \textbf{mIoU (\%)} \\ \midrule
GPT-4o (Strong)              & 2.01                 & 0.57                 & 0.57                 & 2.19               \\
Gemini-1.5-Flash (Strong)    & 1.72                 & 0.69                 & 0.52                 & 1.39               \\
GPT-4o (Poor)                & 0.00                 & 0.00                 & 0.00                 & 3.30               \\
Gemini-1.5-Flash (Poor)      & 0.00                 & 0.00                 & 0.00                 & 2.28               \\
\midrule
GPT-4o (Overall)             & 1.77                 & 0.51                 & 0.51                 & 2.29               \\
Gemini-1.5-Flash (Overall)   & 1.17                 & 0.47                 & 0.35                 & 1.78               \\
\bottomrule
\end{tabular}
\label{tab:metrics_comparison}
\end{table*}


\subsubsection{Evaluation Result of MLLMs Temporal Grounding in Joint Attention Segments}

Table \ref{tab:metrics_comparison} highlights that GPT-4o outperforms Gemini-1.5-Flash on strong joint attention segments, achieving a higher mIoU (2.19\% vs. 1.39\%) and a slightly better R@0.3 (2.01\% vs. 1.72\%). This suggests GPT-4o aligns more effectively with ground truth for strong segments, likely due to its broader contextual understanding and reduced reliance on explicit eye-contact information.

% \begin{table*}[htb!]
% \centering
% \caption{Comparison of Temporal Grounding Metrics between Gemini-1.5-Flash and GPT-4o for Strong, Poor, and Overall Joint Attention Segments. The metrics include Recall at IoU thresholds (R@0.3, R@0.5, and R@0.7) and mean Intersection over Union (mIoU). R@0.3, R@0.5, and R@0.7 represent the proportion of ground truth segments that have at least one predicted segment with sufficient overlap at IoU thresholds of 0.3, 0.5, and 0.7, respectively. mIoU measures the average alignment quality between predicted and ground truth segments. "Strong" refers to the model's performance on segments with strong joint attention, "Poor" refers to segments with poor joint attention, and "Overall" reflects performance across all segments.}
% \begin{tabular}{p{0.15\textwidth} p{0.18\textwidth} p{0.18\textwidth} p{0.18\textwidth} p{0.18\textwidth}}     \toprule
% \textbf{Model}               & \textbf{R@0.3 (\%)} & \textbf{R@0.5 (\%)} & \textbf{R@0.7 (\%)} & \textbf{mIoU (\%)} \\ \midrule
% GPT-4o (Strong)              & 2.01                 & 0.57                 & 0.57                 & 2.19               \\
% Gemini-1.5-Flash (Strong)    & 1.72                 & 0.69                 & 0.52                 & 1.39               \\
% GPT-4o (Poor)                & 0.00                 & 0.00                 & 0.00                 & 3.30               \\
% Gemini-1.5-Flash (Poor)      & 0.00                 & 0.00                 & 0.00                 & 2.28               \\
% \midrule
% GPT-4o (Overall)             & 1.77                 & 0.51                 & 0.51                 & 2.29               \\
% Gemini-1.5-Flash (Overall)   & 1.17                 & 0.47                 & 0.35                 & 1.78               \\
% \bottomrule
% \end{tabular}
% \label{tab:metrics_comparison}
% \end{table*}

For poor joint attention segments, GPT-4o also achieves a higher mIoU (3.30\% vs. 2.28\%), despite both models reporting zero Recall across all IoU thresholds (R@0.3, R@0.5, R@0.7). This discrepancy occurs because mIoU captures partial overlaps between predicted and ground truth regions, even if the overlaps are insufficient to meet recall thresholds. In the poor category, models may predict larger or misaligned regions, contributing to mIoU but failing to exceed the recall thresholds.

Overall, GPT-4o demonstrates superior performance, with a higher mIoU (2.29\% vs. 1.78\%) and slightly better recall values across all categories. However, this low value indicates limited alignment between predicted and ground truth regions. The result reflects the challenges of temporal grounding in joint attention tasks, where precise boundary prediction is difficult.

Despite these results, the findings highlight limitations in current MLLMs for direct understanding of joint attention. As noted earlier, deficiencies in processing eye-contact information significantly reduce accuracy, underscoring the need for improved multimodal integration.