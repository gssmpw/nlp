\begin{table*}[htb!]
\centering
\caption{Comparison of annotation between SLPs: \textbf{SLP} (annotator ID), \textbf{Total} (number of segments), \textbf{Strong (\%)} and \textbf{Poor (\%)} (counts and percentages of strong/poor joint attention), \textbf{Avg. (s)} (average segment duration), \textbf{Avg. Strong (s)}, and \textbf{Avg. Poor (s)} (average durations of strong and poor segments). The \textbf{Intersection} row shows metrics for mutually agreed segments.}
\begin{tabular}{p{0.12\textwidth} p{0.12\textwidth} p{0.12\textwidth} p{0.12\textwidth} p{0.12\textwidth} p{0.12\textwidth} p{0.12\textwidth}} 
\toprule
\textbf{SLP} & \textbf{Total} & \textbf{Strong (\%)} & \textbf{Poor (\%)} & \textbf{Avg. (s)} & \textbf{Avg. Strong (s)} & \textbf{Avg. Poor (s)} \\ 
\midrule
S1           & 96             & 89 (92.71)           & 7 (7.29)           & 5.61               & 5.24                      & 10.30                   \\
S2           & 72             & 56 (77.78)           & 16 (22.22)         & 14.51              & 14.85                     & 13.33                   \\
\midrule
Intersection & 62             & 58 (93.55)           & 4 (6.45)           & 3.88               & 3.65                      & 10.73                   \\
\bottomrule
\end{tabular}
\label{tab:segment_metrics}
\end{table*}
\section{Video Data Collection and Preparation}
\subsection{Video Selection Method}
We collected videos from YouTube using targeted keywords \dquote{parent-child interaction} to create a dataset for analysing parent-child interactions, carefully selecting videos that met quality and relevance criteria. Each video had to feature one child and one adult as the primary subjects. While most videos required active interactions between the adult and the child, we also included cases where the adult's role was limited to accompanying a very young child for support without directly engaging in the interaction. Videos involving multiple children or adults actively participating were excluded to maintain the focus on dyadic dynamics.

We applied stringent selection criteria to ensure the videos were suitably focused towards analysing multimodal signals. The scenes needed to be static with minimal camera movement, and the videos had to provide clear views of both the child's and adult's faces, enabling precise analysis of gaze direction and expressions. High-quality audio and visual clarity were also essential for accurate verbal and non-verbal communication observations.

Additionally, the dataset was curated to include children across a broad age range to capture a variety of developmental stages. We prioritised videos that showcased diverse and meaningful interactions, such as language learning activities, skill-building tasks, or natural daily exchanges. All textual elements, including subtitles and transitions, were removed to preserve the raw multimodal content.

\subsection{Dataset Composition and Analysis}
We curated a final dataset comprising 26 parent-child interaction videos that satisfied our quality and relevance criteria. These videos are of dialogue and interaction in fixed settings, covering age ranges between 0-8 years (age information obtained from the descriptions provided on the video websites). The majority of videos focus on children aged 4–6 years (n=16), with fewer videos in the 2–4 (n=5) and 0–2 (n=4) age groups. Only one video includes children aged 6–8, concentrating on younger age groups where parent-child interactions are more actively studied.

The duration of the videos ranges from brief clips of around 0.5 to 12 minutes. Most videos are relatively short: 13 videos are less than 1 minute, and 11 fall between 1 and 5 minutes. Only two videos extend beyond 5 minutes, one categorised as 5–10 minutes and the other over 10 minutes. This distribution highlights the concise nature of the interactions and tasks typically observed in parent-child settings.

The video content in the dataset can be categorised into three main categories. The first category is behaviour guidance and skill modelling (n=10), where parents demonstrate specific techniques (e.g., PRIDE skills, \dquote{Big Ignore} techniques) to guide children's behaviour and improve interaction quality. The second category focuses on language and cognitive development (n=10), showcasing how parent-child interactions foster language learning and cognitive skills through tasks or experiments, such as discussing specific topics or engaging in Piaget's cognitive experiments. The third category involves daily life skills and interaction (n=6), presenting natural exchanges in structured settings, such as reviewing reward charts or ending special playtime, emphasising practical life skills and building strong parent-child relationships.

% \begin{figure}[htbp]
%     \begin{minipage}[t]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/age_distribution.png}
%         \caption{Age Distribution}
%         \label{fig:age_distribution}
%     \end{minipage}%
%     \begin{minipage}[t]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/duration_distribution.png}
%         \caption{Duration Distribution}
%         \label{fig:duration_distribution}
%     \end{minipage}
%     \begin{minipage}[t]{0.32\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/classification_distribution.png}
%         \caption{Classification Distribution}
%         \label{fig:classification_distribution}
%     \end{minipage}
% \end{figure}
