\section{RELATED WORK}

\subsection{Parent-Child Interaction Challenges in Early Speech-Language Development}

Parent-child interactions, especially joint attention activities, are essential for speech-language development. When parents lack proper guidance or inadequately follow the advice of SLPs, children's speech-language development can be affected--leading to poorer outcomes for children who already have communication developmental delays. High costs and a shortage of SLPs, particularly in rural or underserved areas, often limit access to professional speech therapy~\cite{o2005barriers}. Programs like DIR Floortime~\cite{dir_floortime}, It Takes Two to Talk~\cite{pepper2004talk}, and More Than Words~\cite{sussman1999words} have been developed to help parents promote joint attention and language development. DIR Floortime emphasises emotional connections and following the child's lead~\cite{dir_floortime}. The Hanen Centre's It Takes Two to Talk~\cite{pepper2004talk} and More Than Words~\cite{sussman1999words} programs provide practical strategies to improve communication. Building on these efforts, integrating technology to detect and analyse joint attention could further enhance parent-child interactions and support speech-language development.

\subsection{Supporting Parent-Child Interaction}
Recent technological advancements have introduced systems aimed at enhancing parent-child interactions.
Chan et al. developed WAKEY, a system designed to improve parent-child communication during morning routines, enabling parents and preschool children to start their day more smoothly~\cite{chan2017wakey}. However, WAKEY relies entirely on manual input and logging by parents for tasks such as scheduling and tracking phrase usage frequency, lacking the ability to automatically gather contextual data, such as audio or video interaction details.
Hwang et al. proposed TalkBetter, a system that analyses turn-taking and provides real-time feedback and tailored recommendations to help parents foster their children's language development~\cite{hwang2014talkbetter}. Song et al. introduced TalkLIME, a mobile system that enhances parent-child interactions by providing real-time feedback on metrics like utterance count, turn-taking, and initiation ratio~\cite{song2016talklime}. 
However, both TalkBetter and TalkLIME rely solely on voice-based information, lacking conversational or visual context, which restricts their ability to capture the nuances of parent-child interactions fully.
Jeong et al. developed Captivate!, a system that uses multimodal sensing, including gaze estimation and speech recognition, to detect joint attention and recommend phrases during parent-child interactions~\cite{kwon2022captivate}. While it can help determine joint attention on a specific object, it cannot provide a detailed contextual understanding of interactions beyond this.
Existing technologies in this domain often lack either multimodal information or the ability to fully understand the context of interactions, highlighting the need for systems that integrate richer multimodal data and contextual awareness to support parent-child interactions better.

% Efforts to detect joint attention remain limited. Cazzato et al. used a ceiling-mounted depth camera to analyse head orientation for detecting joint attention in interactions involving robots, therapists, and children ~\cite{cazzato2015automatic}. However, their approach specifically lacks conversational context and modern multimodal integration.

\subsection{Potential of MLLMs in Parent-Child Interaction}

Large Language Models (LLMs) have been increasingly applied to enhance human-human interactions, showcasing their potential to facilitate seamless, context-aware communication. 
Tanneberg et al.~\cite{tanneberg2024attentivesupport} proposed a robot-based system that leverages LLMs for unobtrusive group support. 
By analysing spoken dialogue and contextual cues, the system determines when to intervene and provides targeted assistance, such as correcting wrong information or supporting task completion. This work highlights the potential of LLMs to foster collaborative efficiency in group settings.
Nihei et al.~\cite{nihei2024chatbots} developed CMBot, an LLM-powered system to mediate communication between older adults and their families.
The system dynamically adjusts self-disclosure levels and generates personalised conversation prompts based on themes like hobbies or finances. 
By referencing prior interactions, CMBot avoids repetition and provides contextually relevant recommendations, enhancing intergenerational communication.
Building on these advancements, MLLMs further extend the capabilities of traditional LLMs by integrating text, visual, and auditory modalities. This multimodal integration enables a deeper understanding of complex interactions across diverse environments.
For example, models like GPT-4o excel in tasks like video temporal grounding, dense captioning, and summarization~\cite{lu2024gpt}.
Given these advancements, applying MLLMs to detect and analyse joint attention in parent-child interactions represents a promising frontier.
By leveraging their ability to process multimodal inputs, MLLMs could offer innovative solutions for capturing and understanding the nuances of shared attention dynamics and more in real-world contexts.