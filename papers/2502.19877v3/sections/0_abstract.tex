\begin{abstract}
Joint attention is a critical component of early speech-language development and a key indicator of effective parent-child interaction.
However, research on detecting and analysing joint attention remains limited, particularly for Multimodal Large Language Models (MLLMs).
This study evaluates MLLMs' ability to comprehend joint attention by analysing 26 parent-child interaction videos annotated by two speech-language pathologists. 
These annotations identify strong and poor joint attention segments, serving as benchmarks for evaluating the models' interpretive capabilities. 
Our findings reveal that current MLLMs struggle to accurately interpret joint attention due to a lack of nuanced understanding of child-initiated eye contact, a crucial component of joint attention dynamics.
This study highlights the importance of incorporating detailed eye contact to enhance MLLMs' multimodal reasoning.
Addressing these gaps is essential for future research to advance the use of MLLMs in analysing and supporting parent-child interactions.
\end{abstract}