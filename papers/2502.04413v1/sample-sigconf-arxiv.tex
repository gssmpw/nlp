%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf,nonacm]{acmart}
% \documentclass[sigconf,review]{acmart}
\usepackage{amsmath}
\usepackage{float} 
\usepackage{multirow}
\usepackage{balance}
\usepackage{longtable}
\usepackage{placeins} % 在导言区加入这个包
\usepackage{float} % 导言区加入
\usepackage{tabularx} % 需要导入 tabularx 包
\usepackage{longtable} % 在导言区添加 longtable 宏包
\usepackage{xtab}
\usepackage{array}    % 需要导入 array 包
\settopmatter{printccs=false}
\settopmatter{printacmref=false}
\settopmatter{printfolios=false}
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2025}
% \acmYear{2025}
% \setcopyright{cc}
% \setcctype{by-nc-nd}
% \acmDOI{XXXXXXX.XXXTEST}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


% \copyrightyear{2025}
% \acmYear{2025}
% \setcopyright{cc}
% \setcctype{by-nc-nd}
% \acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April
% 28-May 2, 2025}{Sydney, NSW, Australia}
% \acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April
% 28-May 2, 2025, Sydney, NSW, Australia}
% \acmDOI{10.1145/3696410.3714782}
% \acmISBN{979-8-4007-1274-6/25/04}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{algorithm}
\usepackage{algorithmic}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{MedRAG: Enhancing Retrieval-augmented Generation with \\Knowledge Graph-Elicited Reasoning for Healthcare Copilot}



%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Xuejiao Zhao}
\authornote{Both authors contributed equally to the paper}
% \orcid{1234-4564-1234-4565}
% \author{Siyan Liu}

\affiliation{%
\institution{LILY Research Centre}
\department{Nanyang Technological Uniersity}
% \department{College of Computing and Data Science}
\country{Singapore}}
\email{xjzhao@ntu.edu.sg}
% \email{siyan.liu@ntu.edu.sg}
% \authornotemark[1]

\author{Siyan Liu}
\affiliation{%
\institution{LILY Research Centre}
\department{Nanyang Technological Uniersity}
\country{Singapore}}
\email{siyan.liu@ntu.edu.sg}
\authornotemark[1]

\author{Su-Yin Yang}
% \orcid{1234-4565-4564-1234}
\affiliation{%
\institution{Tan Tock Seng Hospital}
\institution{Woodlands Health}
\country{Singapore}}
\email{su_yin_yang@wh.com.sg}

\author{Chunyan Miao}
% \orcid{1234-4564-1234-4565}
\affiliation{%
\institution{LILY Research Centre}
\department{Nanyang Technological Uniersity}% \department{College of Computing and Data Science}
\country{Singapore}}
\email{ascymiao@ntu.edu.sg}
\authornote{Corresponding author}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Zhao et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Retrieval-augmented generation~(RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records~(EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph~(KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations.
MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset~(CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates.
Our code will be available at \textit{\url{https://github.com/SNOWTEAM2023/MedRAG}}

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010405.10010444.10010447</concept_id>
       <concept_desc>Applied computing~Health care information systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003341</concept_id>
       <concept_desc>Information systems~Language models</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Health care information systems}
\ccsdesc[500]{Information systems~Language models}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Healthcare Copilot, Retrieval-augmented Generation, Knowledge Graph, Large Language Models, Decision Support}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

\maketitle

\section{Introduction}
% P1: Misdiagnosis caused an enormous loss; Healthcare copilot function; decision-making support is one of the most challenge function for the healthcare colipot. RAG offers a good approach to access the doctor to the EHR.
\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{LaTeX/pics/motivation_5pm.pdf} \caption{(a) The existing RAG and LLMs rely on heuristic-based approaches, leading to incorrect or vague outputs, particularly when diseases share similar manifestations~(high lighted by green colour). (b) MedRAG is a RAG framework with KG-elicited reasoning ability that can make accurate diagnostic decisions and generate highly specific diagnoses, along with proactively providing follow-up questions when necessary.}
\label{fig: motivation}
\end{figure*}

Diagnostic errors cause significant harm to healthcare systems worldwide. In the United States, approximately 795,000 individuals each year suffer permanent disability or death due to misdiagnosis of dangerous diseases. These errors are predominantly attributed to cognitive biases and judgmental mistakes~\cite{newman2024burden,dixitelectronic,wei2018comprehensive}.
``Healthcare Copilot'' is a medical AI assistant designed to provide diagnostic decision support, mitigating biases and increasing efficiency for healthcare practitioners, while also empowering patients and improving overall decision-making~\cite{ren2024healthcare,lee2021survey,amotion_copilot_2024,avanade_health_ai_copilot_2024,rao2024survey}. We conducted interviews to gather requirements and suggestions from users of the healthcare copilot. The results showed that one of the most important and challenging tasks for a healthcare copilot is to provide an accurate diagnosis based on patient manifestations~\footnote{``Manifestations'' typically include all observable signs and symptoms of a patient's condition, such as physical indicators (e.g., rash, fever), patient-reported symptoms (e.g., pain, dizziness), and measurable clinical data (e.g., blood pressure, lab results).}, followed by offering appropriate treatment plans and medication recommendations based on the diagnosis. In addition, when patient information is insufficient or the diagnosis is ambiguous, the healthcare copilot should proactively offer precise follow-up questions to enhance the decision-making process~\cite{wsj_openai_healthcare_2023,openai_color_health_2023,ren2024healthcare,amballa2023ai,zakka2024almanac}. 

% P2: Existing RAG's effort, but they have shortcomings
Retrieval-augmented generation (RAG) offers an advanced approach by utilizing domain-specific, private datasets to address user queries without the need for additional model training~\cite{lewis2020retrieval,guu2020retrieval,edge2024local}. This approach is well-suited for retrieving information from privacy-sensitive Electronic Health Records~(EHRs), and helps healthcare professionals to reduce the risk of misdiagnosis as a healthcare copilot~\cite{wu2024medical,jiang2024tc}. The existing medical RAG and LLMs fine-tuned on medical datasets often rely on heuristic-based approaches, leading to incorrect or vague outputs, particularly when diseases share similar manifestations, making differentiation difficult~\cite{jiang2023active, yang2024talk2care, zelin2024rare, li2023chatdoctor, han2023medalpaca, wu2024pmc} as shown in Figure~\ref{fig: motivation}(a). To address this, we introduce MedRAG, a framework that combines RAG with a comprehensive diagnostic knowledge graph, enabling more accurate reasoning and tailored treatment recommendations by grounding predictions in structured, inferable medical data~\cite{varshney2023knowledge, luo2023reasoning, kang2023knowledge, jiang2023reasoninglm}. This approach significantly enhances the reasoning ability of RAG, enabling it not only to identify subtle diagnostic differences but also to proactively infer relevant follow-up questions, further clarifying ambiguous patient information, as shown in Figure~\ref{fig: motivation}(b).

% P3: Our method MedRAG and the main contributions
Specifically, a diagnostic knowledge graph~(KG) with a four-tier hierarchical structure is constructed systematically through advanced techniques, including disease clustering, hierarchical aggregation and large language model augmentation. While medical ontologies like UMLS could be considered, their ambiguous class definitions and low granularity make them less suitable for direct use. To address these limitations, we construct this KG tailored to each database to eliminate redundancies and enhance manifestations for improved granularity. The diagnostic differences KG searching module then identifies all critical diagnostic differences KG related to the input patient by performing multi-level manifestations matching within the diagnostic KG. Finally, a KG-augmented RAG module synthesizes the retrieved EHRs and the critical diagnostic differences KG to elicit the reasoning within a large language model. This integration enhances the system’s ability to make precise and highly specific diagnostic decisions, while also providing personalized treatment recommendations, medication guidance, and, when necessary, proactive follow-up questions.

% P4
We evaluate the general applicability of MedRAG by a public dataset DDXPlus~\cite{fansi2022ddxplus} and real-world clinical applicability by a private chronic pain diagnostic dataset~(CPDD). Performance is quantitatively compared against several popular state-of-the-art (SOTA) RAG models, including FL-RAG~\cite{ram2023context} and DRAGIN~\cite{su2024dragin}.
We further validate the generalization of MedRAG on widely-used open-source LLMs, including Mixtral-8x7B~\cite{jiang2024mixtral} and Llama-3.1-Instruct~\cite{dubey2024llama}, as well as on some closed-source LLMs such as GPT-3.5-turbo~\cite{openai_chatgpt}, GPT-4o~\cite{openai_gpt4o}.
Experimental results demonstrate that our model outperforms existing RAG approaches in terms of diagnostic accuracy and specificity. Additionally, MedRAG demonstrates robust generalization across various LLMs, and proves highly effective in generating reasoning-based follow-up diagnostic questions. These capabilities are particularly valuable for distinguishing between diseases with similar manifestations.
Based on extensive experiments, our key contributions can be summarized as follows:
\begin{itemize}
    \item We deliver two diagnostic knowledge graphs: one focused on chronic pain and the other based on DDXPlus~\cite{fansi2022ddxplus}, a large-scale synthesized dataset. These knowledge graphs contain a rich hierarchical structure of diseases, along with their key diagnostic differences. This comprehensive organization allows for enhanced precision in disease differentiation and diagnosis, enabling better decision-making support across various medical systems.
    \item We proposed a novel RAG approach enhanced by KG-elicited reasoning, which significantly improves RAG's ability to make accurate and highly specific diagnostic decisions. In addition to supporting personalized treatment recommendations and medication guidance, it proactively generates follow-up questions when necessary. These enhancements greatly optimize the decision-making process in complex medical scenarios.
    \item Comprehensive experiments conducted on two datasets demonstrate the superiority of our model over existing RAG and LLM approaches. Additionally, the results highlight its applicability across various backbone LLMs and its effectiveness in proactively generating reasoning-based diagnostic questions for medical consultation.
\end{itemize}


\section{Related Works}
\subsection{LLMs and RAG in Healthcare} 
Large Language Models (LLMs) have been increasingly applied to healthcare tasks such as EHR analysis, clinical note generation, virtual medical assistant, and clinical decision support~\cite{jiang2024tc,han2023medalpaca,yang2024talk2care,zhang2024llm,wangretcare}. Although LLMs fine-tuned on medical datasets can handle large amounts of unstructured clinical information, most of these models are heuristic-based, with limitations such as generating incorrect or vague information and struggling to handle complex patient cases~\cite{jiang2023active,yang2024talk2care}. To address this, integrating external information sources becomes essential to improve their contextual accuracy. We adopt a Retrieval-Augmented Generation (RAG) approach~\cite{lewis2020retrieval}. 
% 
RAG enhances LLMs by incorporating retrieved text passages from external sources such as electronic health records, medical papers, textbooks, and databases into their input, resulting in significant improvements in knowledge-intensive tasks~\cite{asai2023self}. In the field of healthcare, integrating retrieved information grounds the predictions in current, verifiable medical data, resulting in more accurate, specificity and context-aware outputs such as diagnostic assessments and treatment recommendations.
% 
RAG typically employs a retrieve-and-read approach to retrieve information based on the initial user query and an answer is generated using that content~\cite{khandelwal2019generalization, soman2023biomedical, sen2023knowledge,fan2024survey,gao2024two,zhao2024retrieval}. 
However, this simplicity restricts their ability to adapt to complex and evolving medical cases.
Enhanced RAG models aim to improve retrieval and generation quality by integrating more sophisticated components such as retrievers, re-rankers, filters, and readers~\cite{sarthi2024raptor,ma2023chain,cheng2021unitedqa, jiang2024hykge, yoran2023making,lewis2020retrieval}.
Despite these advancements, delivering accurate clinical decision support remains challenging. The models often struggle to provide precise diagnoses, particularly when diseases share similar manifestations, making differentiation difficult.
Our proposed MedRAG addresses these challenges by systematically constructing a four-tier hierarchical diagnostic knowledge graph to elicit reasoning for the generation module of RAG. This approach enables the model to make accurate diagnostic decisions and generate highly specific diagnoses along with personalized treatment recommendations.

\subsection{Knowledge Graph-enhanced LLMs and RAG}
Recent studies have focused on creating strategies that integrate knowledge graphs to enhance LLMs and RAG, enabling them to generate accurate and reliable medical responses. Compared to knowledge contained in document repositories~\cite{izacard2021unsupervised}, knowledge graphs offer structured and inferable information, making them more suitable for augmenting LLMs and RAG~\cite{li2018improving,varshney2023knowledge,luo2023reasoning, kang2023knowledge, jiang2023reasoninglm,zhong2023comprehensive,yu2021kg}. 
Several works~\cite{zhang2019grounded, wu2020diverse, tuan2019dykgchat, zhou2021earl,jiang2023reasoninglm,liu2021kg} propose training sequence-to-sequence models from scratch, focusing on dialogue generation by conditioning the output on entities extracted from knowledge graphs.
% 
However, existing medical knowledge graphs~\cite{zhao2021brain,gao2023leveraging,varshney2023knowledge,chandak2023building} often fall short because they lack the detailed and structured information necessary for accurate diagnostic assistance, especially when distinguishing between diseases with similar manifestations. To overcome this limitation, we introduce MedRAG, a framework that combines RAG with a comprehensive diagnostic knowledge graph to enhance the reasoning ability of RAG in identifying subtle differences in diagnoses. MedRAG allows physicians to input patients' medical records or manifestations.
Our knowledge graph is constructed based on patterns extracted from Electronic Health Record (EHR) databases and augmented by LLMs, making it highly scalable and adaptable to various medical specialties. It supports customization with local databases, ensuring relevance to specific clinical settings. We employ LLMs to enrich the knowledge graph by providing detailed descriptions of the manifestations of each disease at the leaf nodes, including symptoms, affected areas, activity limitations, and other pertinent features. 

\section{Preliminaries}\label{Preliminaries}
\paragraph{\textbf{Definition 3.1~(Diagnostic Knowledge Graph)}}
Given an EHR database $D$ and an LLM $\mathcal{M}_a$, our target is to construct a four-tier hierarchical diagnostic knowledge graph $\mathcal{G}$. A multi-hop path, from the top level to the bottom level of $\mathcal{G}$ is represented as $(E_{L1} \xleftarrow{r_s} E_{L2} \xleftarrow{r_s} E_{L3} \xrightarrow{r_m} E_{L_4})$. $E_{L3}$ is the set of all diseases~(i.e. potential diagnoses) names extracted from $D$, $E_{L2}$ represents the set of subcategories of $E_{L3}$, and $E_{L1}$ is the set of broader categories of $E_{L2}$. Each $e_{Lij}$ is a disease name or a category name and $e_{Lij}\in E_{Li}$. $E_{L1}$ and $E_{L2}$ are generated by hierarchical aggregation in Section \ref{sec: 4.1}, they indicate the diseases with similar manifestations. $r_s$ is an ``\texttt{is\_a}'' relation, indicating a hierarchical or subordinate relationship. $r_m$ is a  ``\texttt{has\_manifestation\_of}'' relation between diseases and their manifestations. $E_{L4}$ contains two subtypes: $E_{L4a}$, representing disease-specific features augmented by the LLM $\mathcal{M}a$, and $E{L4d}$, representing features decomposed from the manifestations extracted from the EHR database $D$.

\paragraph{\textbf{Definition 3.2~(Diagnostic Differences KG Searching)}}
Given a $\mathcal{G}$ and the input patient’s manifestations $q$, let $e_{L2_s} \in E_{L2}$ denote a certain subcategory identified through the method described in Section \ref{sec: upward} determined from $q$. The target is to extract the diagnostic differences KG $K$, related to $e_{L2_s}$, from $\mathcal{G}$.

\paragraph{\textbf{Definition 3.3~(RAG)}}
We define a typical retrieval-augmented generation approach for generating diagnostic reports in two phases: algorithm $\mathcal{R}$ for the retrieval phase and LLM $\mathcal{M}_g$ for the generative phase. A prompt $p_{naive}$ is used to guide $\mathcal{M}_g$ to generate the final report. Given a $q$, $D$ and embedding model $\mathcal{E}$, $\mathcal{R}$ retrieves top-$k$ relevant documents $d_r$, and then $\mathcal{M}_g$ generates answer $A$ with $q$, $d_r$ and prompt $p_{naive}$ as shown in Equation~\ref{eq: retriever} and \ref{eq: llm}:
\begin{equation}\label{eq: retriever}
    d_r=\mathcal{R}(q,D,\mathcal{E}),
\end{equation}
\begin{equation}\label{eq: llm}
    A=\mathcal{M}_g(q, d_r, p_{naive}).
\end{equation}

\section{Methods}

\begin{figure*}[htb]
\centering
\includegraphics[width=2\columnwidth]{LaTeX/pics/method_5pm.pdf} 
\caption{The overall framework of MedRAG. MedRAG first extracts patient~(red node) manifestations from structured or unstructured input, and decomposes different clinical features. These features are embedded and matched with a diagnostic KG to identify critical diagnostic differences KG. MedRAG’s KG-elicited reasoning RAG module retrieves relevant EHRs and integrates them with these diagnostic differences KG to trigger reasoning in an LLM. This reasoning generates precise diagnoses, treatment recommendations, and follow-up questions.}
\label{fig: methods}
\end{figure*}


In this section, we elaborate on the details of our proposed MedRAG, and the overall framework is illustrated in Figure~\ref{fig: methods}. MedRAG includes five modules:
\begin{itemize}
    \item \textbf{Input}: The input to MedRAG is the description of patient manifestations, which can be either structured EHR or unstructured text descriptions.
    \item \textbf{Output}: The output of MedRAG includes the diagnoses, treatment recommendations, medication guidance and follow-up questions when necessary.
    \item \textbf{Diagnostic Knowledge Graph Construction}: This module constructs a four-tier hierarchical diagnostic knowledge graph systematically. First, potential diagnoses and corresponding manifestations are extracted from an EHR database to form a four-tier disease KG through clustering and hierarchical aggregation. Then, an LLM is used to augment the graph with critical diagnostic differences, transforming it into a diagnostic KG.
    \item \textbf{Diagnostic Differences KG Searching}: This module identifies key diagnostic differences by decomposing patient manifestations into clinical features, such as symptoms and locations, through medical chunking. Then, the extracted features are embedded and matched with relevant diagnostic differences via multi-level matching and upward traversal within the diagnostic KG.
    \item\textbf{KG-elicited Reasoning RAG}: This module comprises a document retriever and a KG-elicited reasoning LLM engine. The retriever selects relevant top-k EHRs based on patient embeddings and integrates them with critical diagnostic differences KG to trigger reasoning in the LLM, generating final diagnoses, treatment and medical recommendations and follow-up questions for medical consultation.
\end{itemize}

% Next, we will provide a detailed explanation of each phase in the following subsections.

\subsection{Diagnostic Knowledge Graph Construction}

To enhance the reasoning capabilities and fill the knowledge gaps of the RAG, we propose constructing a diagnostic knowledge graph $\mathcal{G}$ tailored to the medical domain of a specific EHR database.  The construction of the diagnostic knowledge graph draws inspiration from the hierarchical structure of the World Health Organization's International Classification of Diseases, 11th Edition (ICD-11)~\cite{world1992icd}~\footnote{The specific classification principles of our diagnostic KG and ICD-11 are different. Our approach classifies and organizes diseases based on the similarity of their manifestations, rather than the traditional classification of ICD-11 based on diagnostic categories. As a result, while the hierarchical concept is similar, the ICD-11 structure cannot be directly applied to our model.}.
% This four-tier hierarchical diagnostic knowledge graph $\mathcal{G}$ is designed to improve diagnostic accuracy and specificity by addressing the challenge of distinguishing nuanced differences between diseases with similar manifestations.

\subsubsection{Disease Knowledge Graph Construction}\label{sec: 4.1}
% For example, ``left neck pain'' and ``right neck pain'' will be unified to the most common topic ``neck pain''
The forms and representations of the diseases in an EHR database are diverse, we first unify the set of original disease descriptions $E_{L3_{raw}}$ by disease clustering to $E_{L3}$. The most common disease name within each cluster is regarded as the final disease name and is assigns to all other diseases in the cluster, as shown in Equation~\ref{eq: KG}:
\begin{equation}\label{eq: KG}
    E_{L3}=\mathcal{C}(E_{L3_{raw}}, \mathcal{E}),
\end{equation}
where $\mathcal{C}$ represents the clustering model applied to $E_{L3}$, $\mathcal{E}$ is an embedding model.

Then we use the unified $E_{L3}$ to construct a four-tier hierarchical disease knowledge graph through hierarchical aggregation. This graph integrates the relationships between diseases and their potential categories, with each disease aggregated into a subcategory and category~\cite{zhao2017hdskg,zhao2021explainable}. We define the disease knowledge graph as $\mathcal{G}_D$ where $\mathcal{G}_D \subset \mathcal{G}$ aggregated by $\Theta$ and LLM $\mathcal{M}_h$, as shown in Equation~\ref{eq: agg}:
\begin{equation}\label{eq: agg}
    \mathcal{G}_D= \Theta(E_{Li},\mathcal{M}_h,\mathcal{E}), i=3,2.
\end{equation}
In the first phase, we apply LLM-based topic aggregation using $\mathcal{M}_h$, which extracts the most relevant topics from $E{L3}$ to aggregate subcategories. These subcategory topics are then further aggregated into higher-level categories, forming the hierarchical structure from subcategories to broader categories. Next, hierarchical clustering is applied to assign diseases in $E_{L3}$ into aggregated subcategory topics and then subtopics to topics. 

This approach leverages LLM's powerful semantic understanding and topic extraction capability, allowing for a more nuanced categorization of diseases in topic aggregation. By applying hierarchical clustering to the LLM-based topics, diseases in $E_{L3}$ are aggregated into a hierarchical structure. Hierarchical aggregation introduces multiple layers of granularity to $E_{L3}$, ensuring that diseases with different manifestations are properly categorized.

To effectively utilize historical diagnoses from $D$ as accurate representations of disease manifestations, we decompose their manifestations of the diseases in $E_{L3}$, parsing them into discrete features $E_{L4d}$. Every single feature like symptom, location, or activity limitation from each $e_{L3_i}$ is created as a node $e_{L4d_i} \in E_{L4}$. This final decomposition results in the comprehensive disease knowledge graph $\mathcal{G}_D$, capturing both disease category information derived from hierarchical aggregation and their associated features.
% The hierarchical semi-supervised clustering model $\mathcal{C}$ is implemented after $e_{L3_i} \in E_{L3}$ for all $i$ are embedded by $\mathcal{E}$, where $\mathcal{E}$ is an open-source model, allowing for a comprehensive understanding of the disease semantics. Initially, predefined labels $S$ serve as the initial cluster centers, which are derived from domain experts or large language models based on manifestations of $E_{L3}$. After the first round of clustering, $E_{L2}$ is obtained and further clustered into broader categories $e_{L3_j} \in E_{L3}$ for all $j$. This results in a four-tier hierarchical structure where each tier corresponds to a different granularity, from specific diseases to broader categories.
% For the three-tier knowledge graph for disease structure, we carry out a hierarchical semi-supervised clustering $\mathcal{C}$ on $D$. First, we treat all disease names in EHRs as elements and cluster them into different categories with closer symptom representation after embedding. The supervised labels $S$ are set to be initial cluster centers in natural language predefined by an expert or LLM. 
% Then the categories' names are set to be new elements to be clustered.Finally, we obtain a three-tiered hierarchical structure denoted as $L_c$, encompassing all diseases from the EHRs, $c$ represents the c-th level. 
% The entities in Triple chain $<e_{L1}, r_s, e_{L2}, r_s, e_{L3}>$ represents the nodes in different level, relation $r_s$ here is fixed to $is\_a$.  From an $L3$ disease entity to an $L2$ subcategory and further to an $L_1$ broader category, a hierarchical chain is established: $e_{L1} \xleftarrow{r_s} e_{L2} \xleftarrow {r_s} e_{L3}$. This hierarchical structure allows for more precise localization for any given disease within its corresponding categories at different granularity in the specific medical domain.

\subsubsection{Knowledge Graph Manifestation Augmentation}

The knowledge in $\mathcal{G}_D$ only contains information from $D$, which is insufficient to accurately diagnose all diseases, particularly when distinguishing between diseases with similar clinical manifestations. Therefore, the integration of external knowledge is essential. To complement the diagnostic knowledge graph with essential knowledge that is not present in $D$, we augment external knowledge $E_{L4}$ to $\mathcal{G}_D$ that aids in distinguishing diseases with similar manifestations.
We traverse all disease $e_{L3_i}$ and employ a prompt $p_a$ specially tailored for searching and generating the nuances of the diseases on an LLM denoted by $\mathcal{M}_a$. As shown in Equation~\ref{eq: L4}, each generated diagnostic key difference node $e_{L4a_{ij}}$ is then connected to its corresponding $e_{L3_i}$ with relationship $r_m$. Thus we obtain a chain $E_{L3} \xrightarrow{r_m} E_{L4a}$. For example, we generate a manifestation and relation to disease node $lumbar spondylosis$ and form a chain: $<lumbar\_spondylosis, has\_symptom, 		stiffness\_or\_pain\_in\_the\_l- ower\_back>$.
% 太长了:等文本编辑完手动调整换行
\begin{equation}\label{eq: L4}
    \{ e_{L3_i} \}_{i=1}^n \xrightarrow{\mathcal{M}_a(p_a, e_{L3_i})} \{ e_{L4a_{ij}} \}_{i=1,j=1}^{n,m_i},
\end{equation}
\begin{equation}\label{eq: KG1}
    E_{L4} = E_{L4d} \cup E_{L4a},
\end{equation}
\begin{equation}\label{eq: KG2}
    \mathcal{G}=\mathcal{G}_D \cup_{E_{L3}} \{ E_{L3} \cup E_{L4} \}_{i=1}^n, 
\end{equation}
where $\mathcal{M}_a$ and $p_a$ represent the large language model for disease manifestation augmentation and its prompt respectively.

The finalized four-tier hierarchical diagnostic knowledge graph $\mathcal{G}$ is formed by integrating the disease knowledge graph $\mathcal{G}_D$ with $E_{L4}$ combined with $E_{L4_a}$ and $E_{L4_d}$ , as shown in Equation~\ref{eq: KG1} and \ref{eq: KG2}. 
% The advantage of this highly customized approach is that in subsequent diagnostic differences searching, instead of attempting an n accurate diagnosis on the knowledge graph, we only need to locate the patients' disease subcategory $e_{L2}$. This enables knowledge augmentation within a confined scope rather than the whole knowledge graph with higher computing costs which is more efficient for the final generation model.

\subsection{Diagnostic Differences KG Searching}
% This streamlined process ensures that all critical diagnostic variations related to the input manifestations are efficiently identified.
%这里开始讲怎么匹配
\subsubsection{Decomposition of Manifestations}
%这个地方得斟酌一下，输入应该可以是ehr等格式化数据，或者普通大段文本。处理方法的统一描述
Given $q$ as a query, which is a description of the patient’s manifestations, we perform sentence trunking on $q$ to decompose the manifestation into more detailed features, denoted as ${f_1, f_2, \dots, f_n} \in q$. We define a mapping function to describe the process, shown in Equation~\ref{eq: decomposition}:
\begin{equation}\label{eq: decomposition}
q \xrightarrow{\phi} \{f_1, f_2, \dots, f_n\}.
\end{equation}


\subsubsection{Clinical Features Matching}
Given a $q$, we compute the semantic similarity score $sim$ between $f_i$ and $e_{L4d_i}$, shown in Equation~\ref{eq: similarity}:
\begin{equation}\label{eq: similarity}
    sim_{ij}= \mathcal{S}(f_i,e_{L4d_j},\mathcal{E}),
\end{equation}
where $\mathcal{S}$ is similarity model and $\mathcal{E}$ is embedding model applied to $f_i$ and $e_{L4d_j}$ before similarity calculation.

For each patient feature $f_i$, we retrieve the top-$m$ most similar $e_{L4d_j}$, where $m$ denotes the number of closest matches selected. Totally, the system retrieves $n \times m$ matching nodes in the $\mathcal{G}$. To address the scenario where a $f_i$ has no closely matching counterpart in $E_{L4d}$, we introduce an indicator function $\delta(sim_{ij}, t_{matching})$ to filter irrelevant matches:
\begin{equation}
    \delta(sim_{ij}, t_{matching}) =
    \begin{cases}
        sim_{ij} & \text{if } sim_{ij} > t_{matching} \\
        0 & \text{otherwise},
\end{cases}
\end{equation}
\begin{equation}
    T = \bigcup_{i=1}^{n} \left\{ e_{L4d_j} \mid j \in \arg\max_{j{\prime} \in \{1, \dots, |E_{L4d}|\}}^{m} \delta(sim_{ij}, t_{matching}) \right\}
\end{equation}
where $T$ represents the set of nodes $e_{L4d_j}$ that satisfy the condition $sim_{ij} > t_{matching}$. The indicator function $\delta$ ensures that only $e_{L4d_j}$ with a similarity score above the threshold are selected into $T$. Through clinical features matching, we successfully matched $q$ to the most relevant clinical feature nodes in $\mathcal{G}$.

% \subsubsection{Disease Subcategory Matching}\label{sec: upward}
\subsubsection{Upward Traversal}\label{sec: upward}

%图中的叫upward traversal

%如何称呼匹配中的eL2
To precisely match the patient’s most relevant $e_{L2_s}$, we employ upward traversal which determines the closest disease subcategory by aggregating votes based on the shortest path distances between $t_i \in T$ and $e_{L2_j}$ in the graph.  

For $t_i$, we calculate the shortest path to each disease subcategory $e_{L2_j}$ by upward traversing through the graph. Denote the shortest path distance between $t_i$ to $e_{L2_j}$ as $P(t_i, e_{L2_j})$. If $e_{L2_{ik}}$ represents the closest disease subcategory node for the current $t_i$, the vote count for $e_{L2_{ik}}$ is incremented by one. We then accumulate the votes for each $e_{L2_{ik}}$ during the reversal and identify the node with the highest vote count as the $e_{L2_s}$. This voting process is formalized through the indicator function $\chi$, defined as follows:
\begin{equation}
\chi(t_i, e_{L2_{ik}}) =
\begin{cases}
1 & \text{if } e_{L2_{ik}} = \arg\min_{e_{L2_j}} P(t_i, e_{L2_j}), \\
0 & \text{otherwise}
\end{cases}
\end{equation}
\begin{equation}
e_{L2_s} = \arg\max_{e_{L2_{ik}}} \sum_{t_i \in T} \chi(t_i, e_{L2_{ik}}),
\end{equation}

Taking this $e_{L2_s}$ as the parent node, we traverse downward towards $E_{L4}$, retrieving all $e_{L3_i}$ that are adjacent to $e_{L2_s}$ and their adjacent $e_{L4a_i}$. Given $e_{L2_s}$, let $E_{L3_s} = \{e_{L3_i} \mid e_{L3_i} \in \text{Adj}(e_{L2_s})\}$ denote the set of disease nodes that belong to $e_{L2_s}$. Similarly, define $E_{L4a_s} = \{e_{L4a_j} \mid e_{L4a_j} \in \text{Adj}(e_{L3_i}), e_{L3_i} \in E_{L3_s}\}$ to denote the set of feature nodes linked to the disease nodes in $E_{L3_s}$.

We concatenate all triples $( e_{L3_s}, r_m, e_{L4a_i} )$, where $e_{L3_s} \in \text{Adj}(e_{L2_s})$ and $e_{L4a_i} \in \text{Adj}(e_{L3_s})$, to form the set of diagnostic differences KG:

\begin{equation}
    K(e_{L2_s}) = \bigcup_{e_{L3_s} \in \text{Adj}(e_{L2_s})} \{( e_{L3_s}, r_m, e_{L4a_s} ) \},
\end{equation}
where $K$ represents the diagnostic differences KG used for the reasoning in the LLM next.

\subsubsection{Proactive Diagnostic Questioning Mechanism}
Inaccurate diagnoses often stem from insufficient or incomplete patient descriptions. To address this issue, we propose a Proactive Diagnostic Questioning Mechanism. When the initial input $q$ lacks some crucial information required for doctors or LLMs to make more precise diagnostic decisions, this mechanism acts as a copilot to cast targeted follow-up questions.

In the diagnostic knowledge graph $\mathcal{G}$, a feature $e_{L4d_i}$ may be connected to multiple disease nodes $e_{L3}$, with each $e_{L4d_i}$ varying in its discriminability. For instance, certain features are more prevalent, such as ``pain located in the lumbar region'', while others represent more distinctive characteristics, like ``pain worsens while walking''. Here we define the discriminability score of $e_{L4d_i}$ as the reciprocal of the degree centrality in $\mathcal{G}$:
\begin{equation}
    \sigma(e_{L4d_i})=\frac{n-1}{deg(e_{L4d_i} )},
\end{equation}
where $n$ represents the total number of $e_{L4d_i}\mathcal{G}$.

We calculate the discriminability score $\sigma(e_{L4d_j})$ for each feature node $e_{L4d_j} \in E_{L4d_s}$ and select those with the highest discriminability scores as follows:
\begin{equation}
    \{e_{L4d_{s_1}}, e_{L4d_{s_2}}, \dots, e_{L4d_{s_k}}\} = \arg\max_{\{e_{L4d_j} \mid e_{L4d_j} \in E_{L4d_s}\}} \sigma(e_{L4d_j}),
\end{equation}
where $\{e_{L4d_{s_1}}, e_{L4d_{s_2}}, \dots, e_{L4d_{s_k}}\}$ represents the selected features with the highest discriminability scores, which are used to proactively guide follow-up questions for clarifying the diagnosis.

\subsection{KG-elicited Reasoning RAG}\label{RAG}

KG-elicited Reasoning RAG is the core component of MedRAG, we use an LLM to generate diagnoses, personalized treatment plans, and medication suggestions. Additionally, the system proactively suggests follow-up questions for doctors to clarify missing or ambiguous patient information. As shown in Equation~\ref{eq: medrag}, MedRAG utilizes diagnostic differences KG augmented by LLM and a tailored prompt $p_s$ to elicit the reasoning capabilities of LLM.
\begin{equation}\label{eq: medrag}
    A=\mathcal{M}_g(q, d_r, K, p_s)
\end{equation}

Unlike most RAG systems that focus on answering short factual questions, our system is tailored for complex tasks in clinical scenarios. The prompts are designed explicitly to optimize the reasoning capabilities of the LLM, particularly in distinguishing between diseases with similar manifestations. The system conducts thorough reasoning by using both the retrieved documents and the diagnostic differences KG extracted from $G$.

We use the EHR database as a document repository to retrieve the most relevant documents $d_r$ corresponding to the patient's manifestations $q$. We then perform a similarity search over the database to identify the most relevant k records. For this, we employ Facebook AI Similarity Search (FAISS) \cite{douze2024faiss}, a library optimized for efficient approximate nearest neighbor searches. FAISS allows rapid retrieval of similar records in large-scale EHR datasets, enabling adjustable trade-offs between speed and search accuracy. After obtaining all inputs, we designed a tailored prompt $p_s$ for guiding the LLM to reason through $K$, generating answers to assist doctors in distinguishing between similar diseases and proactively generating follow-up questions.
\section{Experiments}

\subsection{Datasets}
We evaluate MedRAG framework using two distinct datasets: one public and one private. The public dataset demonstrates the model’s general applicability, while the private dataset, focused on chronic pain patients, enables a more thorough evaluation of MedRAG's diagnostic capabilities in real-world clinical settings.

The public dataset, DDXPlus~\cite{fansi2022ddxplus}, is a large-scale synthesized EHR dataset, recognized for its complex and diverse medical cases. It includes comprehensive patient data such as socio-demographic information, underlying diseases, symptoms, and antecedents, addressing the symptom-related data gap in common EHR datasets like MIMIC~\cite{fansi2022ddxplus}. Many studies have employed DDXPlus to benchmark models in medical reasoning and diagnosis~\cite{xie2024preliminary,li2024knowledge,chen2023llm,tam2024let}. 
DDXPlus contains 49 different diagnoses with over 1.3 million patients, each of whom has approximately 10 symptoms and 3 antecedents on average. We ultimately utilized a maximum balanced sub-dataset comprising 13,230 patients' EHRs.

The private dataset is the Chronic Pain Diagnostic Dataset~(CPDD), a specialized EHR dataset focused on chronic pain patients. This dataset is collected from Tan Tock Seng Hospital, it comprises 551 patients with 33 distinct diagnoses. CPDD offers manifestations-specific chronic pain patient data, making it an invaluable resource for testing MedRAG's diagnostic capabilities in clinical settings. 

For more details on the partitioning, preprocessing, and experimental setup, please refer to the Appendix.

\subsection{Baselines}
In order to explore the performance of the MedRAG, we compare the MedRAG results against six other models, including Naive RAG with COT~\cite{wei2022chain}, FL-RAG~\cite{ram2023context}, FS-RAG~\cite{trivedi2022interleaving}, FLARE~\cite{jiang2023active}, DRAGIN~\cite{su2024dragin} and SR-RAG~\cite{wang2024bioinformatics}. More detailed introduction to each baseline model is provided in the appendix.
% \begin{itemize}
%     \item \textbf{Naive RAG + COT}~\cite{wei2022chain} 
%     We apply the chain-of-thought (COT) prompting with a naive RAG model, which only retrieves documents without additional enhancements.

%     \item \textbf{FL-RAG}~\cite{ram2023context}
%     %   title={In-context retrieval-augmented language models},
%     FL-RAG is a multi-round retrieval method that triggers the retrieval module every n tokens.
 
%     \item \textbf{FS-RAG}~\cite{trivedi2022interleaving}   
%     %   title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},
%     FS-RAG is an interleaving retrieval method that improves multi-round question answering by alternating between COT reasoning and document retrieval. 
    
%     \item \textbf{FLARE}~\cite{jiang2023active}
%     %   title={Active retrieval augmented generation},
%     FLARE is an active RAG method that improves knowledge-intensive tasks by retrieving relevant documents when the model encounters uncertain tokens.
 
%     \item \textbf{DRAGIN}~\cite{su2024dragin}
%     %   title={Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models},
%     DRAGIN is a dynamic retrieval method that enhances language models by retrieving relevant documents based on real-time information needs during generation, triggered by token uncertainty.

%     \item \textbf{SR-RAG}~\cite{wang2024bioinformatics}
%   % title={Bioinformatics and biomedical informatics with ChatGPT: Year one review},
%     In SR-RAG, relevant passages are retrieved from an external corpus based on the initial query and then incorporated into the input of the language model

% \end{itemize}

% \subsection{Experimental Setup}
% Throughout the experiment, we use text-embedding-3-large \cite{openai_embeddings} from OpenAI as the embedding model $\mathcal{E}$, use IndexFlatIP method after standardization in FAISS index search which is equivalent to cosine similarity in retriever $\mathcal{R}$. In diagnostic differences KG searching, we use K-means algorithm~\cite{macqueen1967some} for clustering model $\mathcal{C}$ and GPT-4o as LLM $\mathcal{M}_a$ for external diagnostic difference augmentation.

% For the quantitative performance comparison, we use Llama-3.1-Instruct 8B \cite{dubey2024llama} as the generative model $\mathcal{M}_g$, as some RAG algorithms only support open-source LLMs. For the generalizability evaluation, we utilize both open-source LLMs, such as Llama-3.1-Instruct 70B \cite{dubey2024llama}, Mixtral-8x7B \cite{jiang2024mixtral}, and Qwen2 \cite{yang2024qwen2}, as well as closed-source models, including GPT-3.5-turbo \cite{openai_chatgpt}, GPT-4o-mini \cite{openai_gpt4omini}, and GPT-4o \cite{openai_gpt4o}, as the backbone LLMs. For the hyperparameters of MedRAG, we use $k=3$ in Retriever and $m=5$ in diagnostic differences KG searching, set threshold $t$ and $t_{matching}$ to $0.5$.

% We mask certain existing $e_{L4_s}$ of a patient to simulate scenarios where they are missing. MedRAG then makes diagnoses and generates follow-up questions based on the remaining information. If MedRAG identifies the removed manifestations during questioning, the masked manifestations are added back to the patient's record as the answer of the patient, and diagnostic reasoning is repeated to evaluate the improvement in diagnostic accuracy. More detail is shown in the Appendix.

% Here we compare MedRAG to the SOTA baseline models across different levels of granularity (i.e. $L1$, $L2$, $L3$)

\section{Experimental Results}
In this section, we present the results of the experiments to answer the following research questions:
\begin{itemize}
    \item\textbf{RQ1}: Does MedRAG outperform the SOTA RAG methods using the same datasets?
    \item \textbf{RQ2}: Does MedRAG demonstrate compatibility, generalizability and adaptability across different backbone LLMs?
    \item \textbf{RQ3}: Does MedRAG's proactive diagnostic questioning mechanism provide users with impactful, relevant follow-up questions to enhance diagnostic performance?
    \item \textbf{RQ4}: Is the MedRAG system we designed effective? What is the impact of each module on its overall performance, and how do specific KG components contribute to MedRAG?
\end{itemize}


\begin{table*}[htb]
\centering
\captionsetup{justification=centering}
\begin{tabular}{lccccccc}
\toprule
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{\centering Model} & 
\multicolumn{3}{c}{CPDD} & \multicolumn{3}{c}{DDXPlus} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
                       &      &  $L1$ &  $L2$ & $L3$ & $L1$ & $L2$ & $L3$ \\
\midrule
\multirow{6}{*}{Baselines} 
\multirow{2}{*}{}  
   & Naive RAG + COT &        75.47 & 54.72 &  43.40 &  79.28 & 71.89 & 56.84 \\
   & FS-RAG &     64.71 &    49.02 &      45.10 & 78.18 & 68.20 & 51.40 \\
   & FLARE &    54.84 &    48.39 &      45.16 & 71.09 & 56.70 & 31.02 \\
   & FL-RAG &    65.45 &    50.91 &      49.09 & \textbf{90.12} & 83.32 & \underline{66.78}\\
   & DRAGIN &  \underline{78.72}  &   59.57  &  40.42    & 80.51 & 70.83& 50.24 \\
   & SR-RAG &    73.58 &    \underline{60.38} &      \underline{54.72} & 78.65 &70.28 & 52.16\\
\midrule
 Ours  & MedRAG &  \textbf{79.25} & \textbf{75.47} & \textbf{66.04} & \underline{88.65} & \textbf{83.46} & \textbf{68.01}\\

\bottomrule
\bottomrule
\end{tabular}
% \caption{Experiment Results of MedRAG}
\caption{Results of quantitative performance comparison}
\label{table: mainresult}
\end{table*}




\begin{table*}[ht]
\centering
\captionsetup{justification=centering}
\begin{tabular}{cccccccccc}
\toprule
\toprule

\multirow{2}{*}{} & \multirow{2}{*}{\centering Backbone LLMs} & \multirow{2}{*}{Size} & 
\multicolumn{3}{c}{w/o KG-elicited Reasoning} & \multicolumn{3}{c}{w/ KG-elicited Reasoning} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9}
                       &       &     &  $L1$ &  $L2$ &  $L3$ & $L1$ & $L2$ & $L3$ \\
\midrule

\multirow{4}{*}{Open-source Models} 
   & Mixtral-8x7B &  13B &  60.38 & 32.08 & 22.34 & 84.62 & \underline{82.69} & 63.46 \\
   & Qwen-2.5 &  72B & 66.04 & 41.51 & 39.62 & 80.36 & 73.21 & 64.29 \\
   & Llama-3.1-Instruct &   8B &  75.47 & 54.72 & 43.40 & 79.25 & 75.47 & 66.04 \\

   & Llama-3.1-Instruct &  70B & 86.79 & \underline{67.92} & \underline{56.60} & \underline{86.79} & \textbf{83.02} & \underline{71.70} \\
\midrule
\multirow{3}{*}{Closed-source Models} 
   & GPT-3.5-turbo & - & 83.02 & 56.60 & 45.28 & 70.56 & 68.68 & 50.57 \\
   & GPT-4o-mini & - & \underline{88.68} & \underline{67.92} & \underline{56.60} & 85.85 & 75.00 & 60.38 \\
   & GPT-4o & - & \textbf{90.57} & \textbf{71.70} & \textbf{60.38} & \textbf{91.87} & 81.78 & \textbf{73.23} \\
\bottomrule
\bottomrule
\end{tabular}
\caption{Performance of MedRAG on different LLM backbones with and without KG-elicited reasoning}
\label{table: general}
\end{table*}

\begin{table}[ht]
    \centering
    \begin{tabular}{c|ccc}
        \toprule
        Manifestation Masking Ratio & $L1$ & $L2$ & $L3$ \\
        \midrule
        100\% & 60.38 & 56.60 & 52.83 \\
        66.6\% & 69.39 & 67.35 & 55.10 \\
        33.3\% & 71.43 & 67.35 & 61.22 \\
        0\% & 79.25 & 75.47 & 66.04 \\
        %100\% & $53.1 \pm 1.7\%$ & $57.9 \pm 1.4$ & $48.6 \pm 1.2$ \\
        \bottomrule
    \end{tabular}
        
    \caption{Result of proactive diagnostic questioning}
\label{table: question}
\end{table}


\subsection{Quantitative Comparison (RQ1)}\label{RQ1}
Our experiments evaluate MedRAG against six different SOTA RAG models on 2 two datasets. We report the results using: 1) \textbf{Accuracy}, defined as the number of correct diagnoses out of the total diagnoses; 2) \textbf{Specificity}, which uses $L1$, $L2$, and $L3$ to represent different diagnostic granularity levels. As outlined in Section~\ref{Preliminaries} (Definition 3.1), $L_i$ refers to the MedRAG select potential diagnoses from $E_{Li}$. This metric evaluates the model's specificity and its ability to differentiate between similar diseases across varying levels of diagnostic granularity; 3) \textbf{Text Generation Metrics}, which uses BERTScore, BLEU, ROUGE, METEOR and subjective evaluatio from doctor to evaluate generated reports.

The result is shown in Table~\ref{table: mainresult}, MedRAG achieved the best or second-best~(with only one exception) performance across multiple metrics in all datasets. Accuracy on the $L3$ metric is the best indicator of MedRAG's performance, as higher specificity increases diagnostic difficulty. MedRAG outperformed the second-best scores on the CPDD and DDXPlus datasets by $11.32\%$ and $1.23\%$.

Additionally, most RAG models designed for simpler QA tasks do not perform as well in the more complex medical domain, leading to longer contextual and prompt. These models are often optimized for generating short and straightforward answers, which limits their effectiveness in handling intricate medical queries. We observe models that have a simpler mechanism in the query-organizing phase perform better than part of more sophisticated ones. Except for our MedRAG, models like SR-RAG and FL-RAG also secured several second-best performances. Even the Chain-of-Thought model, which lacks improvements in the retriever or generator components, outperformed some of the other SOTA models in complex medical tasks.

For report generation, we conducted both objective evaluation using BERTScore, BLEU, ROUGE and METEOR, as well as subjective evaluation based on Mini-CEX \cite{norcini1995mini} criteria by LLM\cite{shi2023llm} with validation by doctors. Performance results are shown in the Appendix.



%普遍性
\subsection{Compatibility, Generalizability and Adaptability (RQ2)}\label{RQ2}

The results in Table~\ref{table: general} demonstrate the performance of incorporating KG-elicited reasoning to various backbone LLMs, including both open-source and closed-source models. The results demonstrate that the inclusion of KG-elicited reasoning significantly enhances diagnostic accuracy across $L1$, $L2$, and $L3$ for all backbone LLMs, compared to models without its use. For example, Mixtral-8x7B shows a significant $L3$ improvement from $22.34\%$ to $63.46\%$, demonstrating the effectiveness of our proposed KG-elicited reasoning, particularly in smaller models.

Comparing open-source and closed-source models, the RAG with the GPT-4o as the backbone LLM outperforms all others, showing its superior adaptability with knowledge graph integration. In addition, MedRAG performs best on closed-source models, showcasing our framework's compatibility, generalizability and adaptability. In contrast, token-level RAG models like DRAGIN and FLARE face challenges in adapting to closed-source models due to their inherent frameworks, limiting their potential to achieve better performance across various LLMs.

We also observe an interesting result that L1 performance decreased when KG knowledge is incorporated into small-parameter closed-source models. We deduce that introducing similar disease difference complicates the reasoning. GPT-3.5 and GPT-4o-mini struggle with incorporating highly granular information due to parameter limitations, leading to knowledge conflicts and blurred classification boundaries, which impact L1 performance. However, GPT-4o's larger parameter scale and stronger reasoning capacity result in higher L1 accuracy.


\subsection{Proactive Diagnostic Questioning (RQ3)}\label{RQ3}

The results in Table~\ref{table: question} show the impact of following MedRAG’s optimized instructive questions and obtaining corresponding patient responses on diagnostic accuracy. 

As more detailed information is gathered through these targeted questions, the $L3$ accuracy progressively improves. Initially, with no specific patient information obtained through this questioning process, the $L3$ accuracy is $52.83\%$, representing MedRAG making a diagnosis with other information with very few manifestations. As the doctor collects more critical details about disease representation, covering from $33.3\%$ to $100\%$ of the key manifestations, the $L3$ score rises from $55.10\%$ to $66.04\%$ and other levels' metrics follow the same trend. This demonstrates the significant effectiveness of MedRAG’s proactive diagnostic questioning mechanism, validating its capability to provide doctors with impactful questions that not only enhance diagnostic performance but also improve the efficiency of the medical consultation process.

\subsection{Ablation Study (RQ4)}\label{RQ4}
\begin{figure}[h]
\centering
\includegraphics[width=1\columnwidth]{LaTeX/pics/ablation2.png} 
\caption{Ablation result on Llama-3.1-Instruct 8B backbone using the CPDD dataset}
\label{fig: ablation}
\end{figure}
We perform ablation studies to evaluate the effectiveness of different components in MedRAG and present the result in Figure~\ref{fig: ablation}. Specifically, we assess the retriever $\mathcal{R}$ and KG-elicited reasoning module $\mathcal{G}$ under three configurations: ``random'', ``with'' and ``without''. In the ``random'' setting for $\mathcal{R}$, we choose documents from the entire EHR database randomly. The ``without'' of the retriever refers to the scenario where no documents are passed to $\mathcal{M}_g$. The ``with'' setting of the retriever means to pass the top-$k$ relevant documents to $\mathcal{M}_g$. For the KG-elicited reasoning module, the ``random'' configuration denotes randomly selecting subcategory $e_{L2_s}$ and collecting corresponding $K$ accordingly. The ``without'' is the scenario where no diagnostic differences KG are passed to $\mathcal{M}_g$. Configuration ``with'' means to pass correct $K$ by the $e_{L2_s}$ to $\mathcal{M}_g$.

%消融实验图待之后调整!testtest i 
As shown in Figure~\ref{fig: ablation}, both the retriever and KG-elicited reasoning module significantly enhance performance across all specificity levels. the best outcomes are achieved when RAG and KG components are combined and aligned, especially for granular diagnosis tasks that demand high specificity. Notably, randomly selected documents performed better than no documents at all, this phenomenon was explored in detail by \cite{cuconasu2024power}. We also observed a performance decline in the lower-granularity levels of $L1$ and $L2$ when transitioning from random to no knowledge from KG when random documents are retrieved. Once correct KG-augmented knowledge was added, this noise effect was mitigated, leading to accuracy improvements across all metrics: an average accuracy increase of $18.88\%$ for $L1$, $26.92\%$ for $L2$, and $18.89\%$ for $L3$, compared to the baseline with random or without KG-elicited reasoning module. The ablation study of KG components is shown in the Appendix.

\section{Conclusion}
In conclusion, MedRAG significantly improves diagnostic accuracy and specificity in the medical domain by integrating KG-elicited reasoning with RAG models. By systematically retrieving and reasoning over EHRs and dynamically incorporating critical diagnostic differences KG, MedRAG offers more precise diagnosis and personalized treatment recommendations. Additionally, MedRAG’s proactive diagnostic questioning mechanism proves highly effective, and shows potential capacity to provide doctors and patients with impactful questions that enhance diagnostic performance and improve consultation efficiency. The evaluation of public and private datasets demonstrates that MedRAG outperforms state-of-the-art RAG models, particularly in reducing misdiagnosis rates for diseases with similar manifestations, showcasing its potential as a key module in healthcare copilot.

For future work, we aim to further enhance MedRAG's capabilities by incorporating multimodal data, such as medical imaging (e.g., MRI), physiological signal data (e.g., ECG), and blood test data to improve diagnostic accuracy and broaden its applicability to a wider range of medical conditions. Additionally, we plan to deploy MedRAG within our healthcare copilot systems (The user interface is shown in the Appendix) for real-world hospital testing, ensuring its effectiveness in clinical settings.
Furthermore, to improve usability for doctors, we will integrate a speech recognition module into the system. This feature will passively listen to conversations between doctors and patients during consultations without causing disruptions. Based on the dialogue content, it will provide real-time suggestions for follow-up questions and relevant explanations, assisting doctors in conducting more comprehensive and efficient patient assessments.

% \section{Acknowledgement}
% \begin{acks}
%     % This research is supported by the Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY) and the College of Computing and Data Science (CCDS) at NTU Singapore. It is also partially supported by the Singapore Ministry of Education Academic Research Fund Tier 1 (Grant No. 2017-T1-001-270). This work is partially supported by the Wallenberg Al, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.
%     This research is supported by the Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY) and the College of Computing and Data Science (CCDS) at NTU Singapore. It is also partially supported by the Singapore Ministry of Education Academic Research Fund Tier 1 (Grant No. 2017-T1-001-270). This research is also supported, in part, by the National Research Foundation, Prime Minister’s Office, Singapore under its NRF Investigatorship Programme (NRFI Award No. NRF-NRFI05-2019-0002). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not reflect the views of National Research Foundation, Singapore. This research is supported, in part, by the Singapore Ministry of Health under its National Innovation Challenge on Active and Confident Ageing (NIC Project No. MOH/NIC/HAIG03/2017). This research is supported, in part, by the RIE2025 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) (Award I2301E0026), administered by A*STAR, as well as partially supported by Alibaba Group and NTU Singapore through Alibaba-NTU Global e-Sustainability CorpLab (ANGEL). This work is partially supported by the Wallenberg Al, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.
% \end{acks}


% \newpage

%\begin{table*}[ht]
%\centering
%\captionsetup{justification=centering}
%\begin{tabular}{clccccccc}
%\toprule
%\toprule

%\multirow{2}{*}{Model} & \multirow{2}{*}{Size}  & 
%\multicolumn{3}{c}{Base} & \multicolumn{3}{c}{MedRAG} \\
%\cmidrule(lr){3-5} \cmidrule(lr){6-8}
%                       &      &  L1 &  L2 &  L3 & L1 & L2 & L3 \\
%\midrule
%LLAMA-3.1-INSTRUCT &   8B &  75.47&54.72&43.40 &     79.25&75.47& 66.04 \\
%    Mixtral-8x7B &  13B &  60.38&32.08&22.34 &    84.62&\underline{82.69}&63.46        \\
%

%    Qwen-72B &  72B & 66.04&41.51&39.62 &    80.36&73.21&64.29        \\
%    LLAMA-3-INSTRUCT &  70B & 86.79&\underline{67.92}&\underline{56.60}   &     \underline{86.79}&\textbf{83.02}&\underline{71.70}  \\
%\midrule
%    GP-3.5-turbo & - & 83.02&56.60&45.28  &    70.56&68.68&50.57 &       \\
%    GPT-4o-mini & -& \underline{88.68}&\underline{67.92}&\underline{56.60}  &    85.85&75.00&60.38  &   \\
%    GPT-4o & - & \textbf{90.57}&\textbf{71.70}&\textbf{60.38}  &   \textbf{ 91.87}&81.78&\textbf{73.23} &   \\
%\bottomrule
%\bottomrule
%\end{tabular}
%\caption{MedRAG Performances on Different LLM}
%\label{tab:label3}
%\end{table*}
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

% 附录部分
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}} % 将表格编号设置为 A1, A2, 等等
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

% 使用 \clearpage 以确保标题和表格在同一页面显示
\clearpage
\section*{Appendix}

This appendix is organized as follows:

\begin{itemize}
    \item \textbf{Section~\ref{sec:vd}} includes variables and definitions in the paper.
    \item \textbf{Section~\ref{sec:sd}} demonstrates the detailed data preprocessing steps and experimental setup, ensuring transparency and reproducibility. %（prompt here）
    \item \textbf{Section~\ref{sec:baseline}} describes the details of the baseline models in the experiments.
    \item \textbf{Section~\ref{sec:3}} presents intermediate results from experiments. %（including examples from the diagnostic knowledge graph）
    \item  \textbf{Section~\ref{sec: report}} shows the evaluation of report generation.
    \item  \textbf{Section~\ref{sec: ablation}} shows the ablation study on KG components.
    \item \textbf{Section~\ref{sec:4}} shows the user interface of the healthcare copilot.
\end{itemize}

\section{Variables and Definitions}
The variables used throughout this paper and their definitions are provided in Table~\ref{tab:variables}.


\label{sec:vd}
\FloatBarrier % 

\section{Data Preprocessing and Experimental Setup}
\label{sec:sd}

\subsection{Settings for Datasets}
\begin{itemize}
    \item \textbf{CPDD} We split the data set into a 9:1 ratio for the training set (to be retrieved) and test set. Since the dataset was collected from multiple doctors, the diagnosis descriptions are not standardized. Part of the diagnosis is presented as a type of pain instead of a specific disease. When calculating the accuracy of these pain-type diagnoses, if the predicted result is a disease associated with that type of pain, it will be considered a correct prediction.
    \item \textbf{DDXPlus} We directly use the training set and test set in a split dataset in the ratio of 8:1:1(validation set). Due to the massive size of the dataset with over a million synthesized patients' records, which is too large for the scale of our task, we first fixed the number of samples in the test set to 30, which corresponds to the fewest pathology. For the other pathology with more samples, we randomly select 30 samples to form the whole test set. In the training set, we randomly pick 240 samples for each pathology to retrieve. This approach can ensure we get a maximum balanced sub-dataset containing 13230 patients' EHR in total. The random seed is set to 42.
\end{itemize}

\subsection{Setup for Proactive Diagnostic Questioning Mechanism}\label{sec: question}

 We mask certain existing manifestations of a patient to simulate scenarios where they are missing. MedRAG then generates follow-up questions based on the remaining information. If MedRAG identifies the removed manifestations during questioning, they are added back to the patient's record, and diagnostic reasoning is repeated to evaluate the improvement in diagnostic accuracy. 

%捋一下思路
%假设少了一个医生的问题->输入中少几个来自病人的关键信息->影响两部分
% 1. q中病人本身自带的信息变少了
% 2. KG matching过程中，用来匹配KG中L4特征的节点少了-> 匹配到的特征节点少了
% 模拟思路： 从匹配到的KG中的特征节点出发，通过特异性分数排序，删除最高的a%的节点，只用剩下的节点进行匹配，完成对影响2的模拟
% 删除的节点，匹配到q中的信息（按句拆分）。按照t相似度阈值，超过就删除这个句子,完成对影响1的模拟。
%
%实际实验：每次删除1/3的信息，t=50%，获取到4个范围的指标，结果非常完全符合预期。

We begin by selecting all matching manifestation nodes $E_{L4d_s}$ and ranking them according to their discriminability scores. A proportion $r$ of the nodes with the highest discriminability scores is then removed, simulating the scenario where certain key patient features are missing or unclear, shown in Equation~\ref{eq: deleted_nodes_r}. After removing, we match the removed nodes $E_{L4d_s}^{\text{del}}$ with each $f_i$,  if the similarity score, the corresponding sentence $f_i$ is also removed, as formalized in Equation~\ref{eq: deleted_f}, which simulates the loss of relevant patient information from the input.
\begin{equation}\label{eq: deleted_nodes_r}
    e_{L4d_s}^{\text{del}} = \text{Top-r}\left( e_{L4d_s}, \sigma(e_{L4d_s}) \right),
\end{equation}
\begin{equation}\label{eq: deleted_f}
        f_i^{\text{del}} = \bigcup_{f_i} \{ f_i \mid \mathcal{S}(f_i, e_{L4d_s}^{\text{del}}, \mathcal{E}) > t \},
\end{equation}
where $e_{L4d_s}^{\text{del}}$ represents the nodes removed from $e_{L4d_s}$ based on the similarity score threshold $t$ and $f_i^{\text{del}}$ is removed $f_i$.



\subsection{Prompt Engineering}
The prompt configuration for disease clustering is shown below:

\par \textit{ Cluster the following diseases into multiple categories based on the similarity of their manifestations, affected locations, and other characteristics. Diseases: \{\}}.

The prompt configuration for the generative model in MedRAG is illustrated in Figure~\ref{fig: prompt}. The first block provides instructions as the system prompt. The second block displays the answer template. In the final block, relevant information including the patient’s manifestations $q$, retrieved documents $d_r$, and diagnostic differences $K$, is populated in this field.

\section{Baseline Details}\label{sec:baseline}
We conducted experiments on six baseline models and compared them with MedRAG.
\begin{itemize}
    \item \textbf{Naive RAG + COT}~\cite{wei2022chain} 
    We apply the chain-of-thought (COT) prompting with a naive RAG model, which only retrieves documents without additional enhancements.

    \item \textbf{FL-RAG}~\cite{ram2023context}
    %   title={In-context retrieval-augmented language models},
    FL-RAG is a multi-round retrieval method that triggers the retrieval module every n tokens.
 
    \item \textbf{FS-RAG}~\cite{trivedi2022interleaving}   
    %   title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},
    FS-RAG is an interleaving retrieval method that improves multi-round question answering by alternating between COT reasoning and document retrieval. 
    
    \item \textbf{FLARE}~\cite{jiang2023active}
    %   title={Active retrieval augmented generation},
    FLARE is an active RAG method that improves knowledge-intensive tasks by retrieving relevant documents when the model encounters uncertain tokens.
 
    \item \textbf{DRAGIN}~\cite{su2024dragin}
    %   title={Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models},
    DRAGIN is a dynamic retrieval method that enhances language models by retrieving relevant documents based on real-time information needs during generation, triggered by token uncertainty.

    \item \textbf{SR-RAG}~\cite{wang2024bioinformatics}
  % title={Bioinformatics and biomedical informatics with ChatGPT: Year one review},
    In SR-RAG, relevant passages are retrieved from an external corpus based on the initial query and then incorporated into the input of the language model

\end{itemize}
\section{Intermediate Results}\label{sec:3}

\subsection{Disease Clustering Result}

% \label{sec:5}
The result of disease clustering in CPDD is Shown in Figure~\ref{fig: clustering}. Through the disease clustering operation, we group different forms and representations of the same disease in the EHR database together, assigning a topic to each cluster. This process unifies the representation of diseases, ensuring consistency and comparability. Additionally, it provides a unified foundation for subsequent disease knowledge graph construction and augmentation.
% , while also facilitating more accurate diagnosis and data analysis by the system.

% 平衡最后一页的两栏

\subsection{Example of Diagnostic Differences Knowledge Graph}
While lumbar canal stenosis and sciatica share some similar features, the critical distinguishing factor lies in the response to sitting. In lumbar canal stenosis, features are typically alleviated when sitting, whereas in sciatica, sitting tends to exacerbate the discomfort. The augmented disease features are shown in Figure~\ref{fig: diagnosticdiff}.

\section{Report Generation Evaluation}
\label{sec: report}
To evaluate the report generation of MedRAG, we conducted both objective and subjective evaluations on the generated reports of CCPD, since the DDXPlus dataset does not contain report data.
\begin{itemize}
    \item \textbf{Objectice Evaluation:} We use BERTScore, BLEU, ROUGE, METEOR as metrics. The result is shown in Table~\ref{table:objective_results}.
    \item \textbf{Subjective Evaluation:} Reports were generated for 10 randomly selected patients using SRRAG and MedRAG. The results were scored on 4 Mini-CEX criteria (Scale 1-9) \cite{norcini1995mini}, and assessed by GPT-4o \cite{shi2023llm}, with validation by doctors. The results were: SRRAG 277, MedRAG 290 out of 360.
\end{itemize}

\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textbf{Models} & \textbf{BERTScore} & \textbf{BLEU} & \textbf{ROUGE} & \textbf{METEOR} \\
        \midrule
        FSRAG & 0.7853 & 0.0963 & 0.1459 & 0.1490 \\
        FLARE & 0.8328 & 0.1637 & 0.1011 & 0.1923 \\
        FL-RAG & 0.8130 & 0.1551 & 0.2171 & 0.2054 \\
        Dragin & 0.8259 & 0.2036 & 0.2053 & 0.2081 \\
        SRRAG & 0.8346 & 0.2013 & 0.2722 & 0.2756 \\
        MedRAG & 0.8359 & 0.2189 & 0.2863 & 0.2822 \\
        \bottomrule
    \end{tabular}
    \caption{Results of objective performance on CCPD}
    \label{table:objective_results}
\end{table}


\section{Ablation Study on KG Components}
In order to evaluate how different components in diagnostic differences KG, we conducted extra ablation study focusing on key components. Specifically, we examined the effects of diagnostic key difference nodes, augmented feature nodes, patient clinical feature matching, and the augmentation of diagnostic differences.
\label{sec: ablation}
\begin{table}[t]
    \centering
    \begin{tabular}{l|ccc}
        \toprule
        \textbf{Configuration} & \textbf{L1} & \textbf{L2} & \textbf{L3} \\
        \midrule
        w/ augmented feature node   & 79.25 & 75.47 & 67.92 \\
        w/o augmented feature node  & 66.04 & 60.38 & 49.06 \\
        w/ diagnostic key difference node  & 79.25 & 75.47 & 67.92 \\
        w/o diagnostic key difference node & 77.36 & 71.70 & 64.15 \\
        w/ L2 \& L3                 & 79.25 & 75.47 & 67.92 \\
        w/o L2 \& L3                & 63.64 & 57.58 & 51.52 \\
        \bottomrule
    \end{tabular}
    \caption{Impact of KG components on the performance of MedRAG}
    \label{table:kg_ablation}
\end{table}


Results in Table~\ref{table:kg_ablation} show that KG components like diagnostic key difference nodes, augmented feature nodes, the patient clinical feature matching and the augmentation of diagnostic differences contribute to MedRAG’s overall effectiveness significantly. Moreover, the hierarchical structure of the constructed diagnostic differences KG directly impacts the experimental results as well.

\section{User Interface (UI)}
\label{sec:4}
This section introduces how our MedRAG can be integrated into the user interface design of the healthcare copilot system. The healthcare copilot offers three modes of interaction, as shown in Figure~\ref{fig: 1}.

\begin{itemize}
    \item \textbf{Consultation Mode}: By monitoring the consultation dialogue between the doctor and patient, the system extracts patient manifestations in real-time and provides diagnostic suggestions along with proactive questioning recommendations to guide the consultation.
    \item \textbf{EHR Mode}: By uploading the patient's EHR to the healthcare copilot system, this system automatically extracts the relevant patient manifestations for diagnostic purposes.
    \item \textbf{Typewritting Mode}: The user can manually input the patient's manifestations into the system.
\end{itemize}

On the results page shown in Figure~\ref{fig: 2}, the output of the healthcare copilot system include diagnoses, instructive follow-up questions, physiotherapy treatments, and medication treatments.
This UI integrates the most essential functions derived from extensive interviews we conducted with numerous healthcare practitioners. It ensures that the healthcare copilot system meets the practical needs of healthcare professionals, ultimately enhancing the overall quality of care.


\clearpage  

\begin{table*}[htbp] % 使用table*环境来跨越双栏
\centering
\resizebox{\textwidth}{!}{  % 适应页面宽度
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}p{2cm}|X|}
\hline
\textbf{Variable} & \textbf{Definition} \\ \hline
$\mathcal{C}$ & The clustering model \\ \hline
$d_r$ & Retrieved relevant documents \\ \hline
$D$ & Electronic Health Record (EHR) database \\ \hline
$E_{L1}$ & The set of broad disease categories \\ \hline
$E_{L2}$ & The set of disease subcategories \\ \hline
$e_{L2_s}$ & The matched subcategory \\ \hline
$E_{L3}$ & The set of specific disease names \\ \hline
$E_{L3_{raw}}$& the set of original disease descriptions in $D$ \\ \hline
$E_{L3_s}$ & The set of disease nodes connected with $E_{L2_s}$ \\ \hline
$E_{L4}$ & The set of disease-specific features \\ \hline
$E_{L4_s}$ & The set of features nodes connected with node in $E_{L3_s}$ \\ \hline
$E_{L4d_s}^{\text{del}}$ & Deleted features in proactive diagnostic questioning\\ \hline
$E_{L4a}$ & Disease-specific features augmented by the LLM \\ \hline
$E_{L4d}$ & Features decomposed from the EHR database \\ \hline
$e_{L2_{ik}}$ & The closest disease subcategory node \\ \hline
$e_{Lij}$ & A disease or category name in the graph where $e_{Lij} \in E_{Li}$ \\ \hline
$f_i$ & A specific feature of the patient’s manifestation \\ \hline
$K$ & The set of diagnostic differences KG identified in the knowledge graph \\ \hline
$p_a$ & Prompt used by $\mathcal{M}_a$ for disease manifestation augmentation \\ \hline
$p_{naive}$ & Simple prompt used by $\mathcal{M}_g$ \\ \hline
$p_s$ & Prompt designed for reasoning and generating diagnostic reports \\ \hline
$P$ & The shortest path function \\ \hline
$q$ & A input patient’s manifestations \\ \hline
$r$ & $E_{L4_s}$ removing proportion in proactive diagnostic questioning mechanism \\ \hline
$r_m$ & Relation type "\texttt{has\_manifestation\_of}" between diseases and their manifestations \\ \hline
$r_s$ & Relation type "\texttt{is\_a}" for hierarchical relationships \\ \hline
$sim$ & The similarity score between patient features and nodes in the knowledge graph \\ \hline
$T$ & Set of relevant matching nodes in the knowledge graph \\ \hline
$t$ & Similarity score threshold in proactive diagnostic questioning mechanism  \\ \hline
$\chi$ & The voting indicator function \\ \hline
$\delta$ & The matching filtering indicator function \\ \hline
$\mathcal{E}$ & Embedding model used to compute similarity between features \\ \hline
$\mathcal{G}$ & The four-tier hierarchical diagnostic knowledge graph \\ \hline
$\mathcal{G}_D$ & The four-tier disease knowledge graph \\ \hline
$\mathcal{M}_a$ & LLM used for disease manifestation augmentation \\ \hline
$\mathcal{M}_g$ & LLM used for generating diagnostic reports \\ \hline
$\mathcal{M}_h$ & LLM used for topic aggregation \\ \hline
$\mathcal{S}$ & The similarity model \\ \hline
$\phi$ & The decomposition function for $q$ \\ \hline
$\sigma(e_{L4d_i})$ & Discriminability score of a feature in the knowledge graph \\ \hline
$\Theta$ & The hierarchical aggregation operator \\ \hline

\end{tabularx}
}
\caption{List of variables and their definitions} % 表格标题
\label{tab:variables}
\end{table*}



\begin{figure*}[htb]
\centering
\includegraphics[width=2\columnwidth]{LaTeX/pics/prompt3.pdf} 
\caption{Prompt for the generative model of MedRAG}
\label{fig: prompt}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=1\columnwidth]{LaTeX/pics/clustering.png} 
\caption{The result of disease clustering in CPDD}
\label{fig: clustering}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=2\columnwidth]{LaTeX/pics/diagnosticdiff.pdf} 
\caption{Diagnostic differences knowledge graph between lumbar canal stenosis and sciatica. (Similar manifestations but opposite responses to sitting (Alleviation vs. Exacerbation))}
\label{fig: diagnosticdiff}
\end{figure*}



\begin{figure*}[ht]
\centering
\includegraphics[width=2\columnwidth]{LaTeX/pics/1.pdf} 
\caption{The interactive interface of healthcare copilot allows multi-turn medical Q\&A through voice, files, and text.}
\label{fig: 1}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=2\columnwidth]{LaTeX/pics/2.pdf} 
\caption{A specific example of how healthcare copilot could handle the diagnosis of lumbar canal stenosis using a JSON format medical record input, and output relevant treatments.}
\label{fig: 2}
\end{figure*}

% \clearpage


% \begin{table}[h]
% \centering
% \resizebox{\columnwidth}{!}{  % 适应页面宽度
% \begin{tabularx}{\columnwidth}{|>{\centering\arraybackslash}p{1.5cm}|X|}
% \hline
% \textbf{Variable} & \textbf{Definition} \\ \hline
% $\mathcal{C}$ & The clustering model \\ \hline
% $d_r$ & Retrieved relevant documents \\ \hline
% $D$ & Electronic Health Record (EHR) database \\ \hline
% $E_{L1}$ & The set of broad disease categories \\ \hline
% $E_{L2}$ & The set of disease subcategories \\ \hline
% $e_{L2_s}$ & The matched subcategory \\ \hline
% $E_{L3}$ & The set of specific disease names \\ \hline
% $E_{L3_s}$ & The set of disease nodes connected with $E_{L2_s}$ \\ \hline
% $E_{L4}$ & The set of disease-specific features \\ \hline
% $E_{L4_s}$ & The set of features nodes connected with node in $E_{L3_s}$ \\ \hline
% $E_{L4a}$ & Disease-specific features augmented by the LLM \\ \hline
% $E_{L4d}$ & Features decomposed from the EHR database \\ \hline
% $e_{L2_{ik}}$ & The closest disease subcategory node \\ \hline
% $e_{Lij}$ & A disease or category name in the graph where $e_{Lij} \in E_{Li}$ \\ \hline
% $f_i$ & A specific feature of the patient’s manifestation \\ \hline
% $K$ & The set of diagnostic differences KG identified in the knowledge graph \\ \hline
% $p_a$ & Prompt used by $\mathcal{M}_a$ for disease manifestation augmentation \\ \hline
% $p_s$ & Prompt designed for reasoning and generating diagnostic reports \\ \hline
% $P$ & The shortest path function \\ \hline
% $q$ & A input patient’s manifestations \\ \hline
% $r$ & $E_{L4_s}$ removing proportion in proactive diagnostic questioning mechanism \\ \hline
% $r_m$ & Relation type "\texttt{has\_manifestation\_of}" between diseases and their manifestations \\ \hline
% $r_s$ & Relation type "\texttt{is\_a}" for hierarchical relationships \\ \hline
% $sim$ & The similarity score between patient features and nodes in the knowledge graph \\ \hline
% $T$ & Set of relevant matching nodes in the knowledge graph \\ \hline
% $t$ & Similarity score threshold in proactive diagnostic questioning mechanism  \\ \hline
% $\chi$ & The voting indicator function \\ \hline
% $\delta$ & The matching filtering indicator function \\ \hline
% $\mathcal{E}$ & Embedding model used to compute similarity between features \\ \hline
% $\mathcal{G}$ & The four-tier hierarchical diagnostic knowledge graph \\ \hline
% $\mathcal{G}_D$ & The four-tier disease knowledge graph \\ \hline
% $\mathcal{M}_a$ & LLM used for disease manifestation augmentation \\ \hline
% $\mathcal{M}_g$ & LLM used for generating diagnostic reports \\ \hline
% $\mathcal{S}$ & The similarity model \\ \hline
% $\phi$ & The decomposition function for $q$ \\ \hline
% $\sigma(e_{L4d_i})$ & Discriminability score of a feature in the knowledge graph \\ \hline
% \end{tabularx}
% }
% \caption{List of Variables and Their Definitions}
% \label{tab:variables}
% \end{table}
% \section{A}
% \balance

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
