@article{yang2022chain,
  title={Chain of thought imitation with procedure cloning},
  author={Yang, Mengjiao Sherry and Schuurmans, Dale and Abbeel, Pieter and Nachum, Ofir},
  journal={Advances in Neural Information Processing Systems},
  pages={36366--36381},
  year={2022}
}

@article{searchformer,
  title={Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping},
  author={Lucas Lehnert and Sainbayar Sukhbaatar and DiJia Su and Qinqing Zheng and Paul McVay and Michael Rabbat and Yuandong Tian},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{dualformer,
  title={Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces},
  author={Su, DiJia and Sukhbaatar, Sainbayar and Rabbat, Michael and Tian, Yuandong and Zheng, Qinqing},
  journal={arXiv preprint arXiv:2410.09918},
  year={2024}
}

@article{sos,
  title={Stream of Search (SoS): Learning to Search in Language},
  author={Gandhi, Kanishk and Lee, Denise and Grand, Gabriel and Liu, Muxin and Cheng, Winson and Sharma, Archit and Goodman, Noah D},
  journal={First Conference on Language Modeling},
  year={2024}
}

@article{gsos,
  title={Guided Stream of Search: Learning to Better Search with Language Models via Optimal Path Guidance},
  author={Moon, Seungyong and Park, Bumsoo and Song, Hyun Oh},
  journal={arXiv preprint arXiv:2410.02992},
  year={2024}
}

@article{bonw,
  title={Making large language models better reasoners with step-aware verifier},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  year={2022}
}

@article{tot,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{feng2023alphazero,
  title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},
  author={Ziyu Wan and Xidong Feng and Muning Wen and Stephen Marcus McAleer and Ying Wen and Weinan Zhang and Jun Wang},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{xie2024self,
  title={Self-evaluation guided beam search for reasoning},
  author={Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, James Xu and Kan, Min-Yen and He, Junxian and Xie, Michael},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{zhang2024rest,
  title={Rest-mcts*: Llm self-training via process reward guided tree search},
  author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2406.03816},
  year={2024}
}

@article{xie2024monte,
  title={Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning},
  author={Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael},
  booktitle={The First Workshop on System-2 Reasoning at Scale, Conference and Workshop on Neural Information Processing Systems},
  year={2024}
}

@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{wang2024math,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics},
  pages={9426--9439},
  year={2024}
}

@article{wu2024inference,
  title={Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@misc{teller2000speech,
  title={Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
  author={Teller, Virginia},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@misc{graves2012sequence,
      title={Sequence Transduction with Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2012},
      eprint={1211.3711},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@inproceedings{choi2023kcts,
      title={KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection}, 
      author={Sehyun Choi and Tianqing Fang and Zhaowei Wang and Yangqiu Song},
      booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
      year={2023}
}

@article{wang2022selfconsistency,
  title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal = {International Conference on Learning Representations},
  year = {2022},
}

@article{an2023learning,
  title={Learning from mistakes makes llm better reasoner},
  author={An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2310.20689},
  year={2023}
}

@article{tong2024can,
  title={Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning},
  author={Tong, Yongqi and Li, Dawei and Wang, Sizhe and Wang, Yujia and Teng, Fei and Shang, Jingbo},
  journal={arXiv preprint arXiv:2403.20046},
  year={2024}
}
@article{dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{cpo,
  title={Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs},
  author={Zhang, Xuan and Du, Chao and Pang, Tianyu and Liu, Qian and Gao, Wei and Lin, Min},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}
@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}

@article{zhang2024backtracking,
  title={Backtracking improves generation safety},
  author={Zhang, Yiming and Chi, Jianfeng and Nguyen, Hailey and Upasani, Kartikeya and Bikel, Daniel M and Weston, Jason and Smith, Eric Michael},
  journal={arXiv preprint arXiv:2409.14586},
  year={2024}
}

@article{ye2024physics,
  title={Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems},
  author={Ye, Tian and Xu, Zicheng and Li, Yuanzhi and Allen-Zhu, Zeyuan},
  journal={arXiv preprint arXiv:2408.16293},
  year={2024}
}

@article{cundy2023sequencematch,
  title={Sequencematch: Imitation learning for autoregressive sequence modelling with backtracking},
  author={Cundy, Chris and Ermon, Stefano},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@misc{o1,
  author       = {OpenAI},
  title        = {Learning to Reason with Large Language Models},
  year         = {2024},
    month      = {September},
}


@misc{r1,
  author       = {DeepSeek},
  title        = {Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!},
  year         = {2024},
  month        = {November},
}
@misc{qwq,
  author       = {Qwen},
  title        = {Qwq: Reflect deeply on the boundaries of the unknown},
  year         = {2024},
  month        = {November},
}

@article{gptf,
  title={Generative Language Modeling for Automated Theorem Proving},
  author={Polu, Stanislas and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2009.03393},
  year={2020}
}
@article{kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{overthink,
  title={Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@article{qin2024o1,
  title={O1 Replication Journey: A Strategic Progress Report--Part 1},
  author={Qin, Yiwei and Li, Xuefeng and Zou, Haoyang and Liu, Yixiu and Xia, Shijie and Huang, Zhen and Ye, Yixin and Yuan, Weizhe and Liu, Hector and Li, Yuanzhi and others},
  journal={arXiv preprint arXiv:2410.18982},
  year={2024}
}

@incollection{van2006backtracking,
  title={Backtracking search algorithms},
  author={Van Beek, Peter},
  booktitle={Foundations of artificial intelligence},
  pages={85--134},
  year={2006},
}

@article{chen2024odin,
  title={Odin: Disentangled reward mitigates hacking in rlhf},
  author={Chen, Lichang and Zhu, Chen and Soselia, Davit and Chen, Jiuhai and Zhou, Tianyi and Goldstein, Tom and Huang, Heng and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={International Conferenceon Machine Learning},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}