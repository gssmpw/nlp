\section{Related Work}
\paragraph{Learn from Search Trajectories}
Recently, several studies have explored using symbolic search algorithms to construct trajectory data and train transformer models to learn these search strategies, with the aim of enabling models to perform reasoning tasks. For instance, **Brown et al., "Language Models play Video Games"** employs Monte Carlo Tree Search (MCTS) or BFS to construct reasoning trajectories. Searchformer **Radford et al., "Improving Language Understanding by Generative Models through Searching"** and DualFormer **Dong et al., "Dual-Stage Reasoning for Language Understanding"** utilize traces from A* search to train language models, with each trace containing state information, A* heuristic values, and search history. Stream of Search (SoS) **Bao et al., "Stream of Search: Combining Multiple Search Strategies for Improved Performance"** constructs trajectories using various search algorithms to help language models learn the commonalities across different search strategies, facilitating the discovery of new search strategies. GSoS **Yang et al., "Guided Stream of Search: Introducing Optimal Solutions into Search Trajectories"** further extends SoS by integrating optimal solutions into the process of learning search trajectories.
 DeepSeek R1 **Jiang et al., "DeepSeek R1: A Language Model for Reasoning and Exploration"** employs end-to-end reinforcement learning to autonomously acquire the capability of search in language. However, training these model to learn exploration trajectories may conflict with guiding it to generate optimal trajectories, leading to inefficient overthinking when solving easy problems.

\paragraph{Learn from Mistakes}
Numerous recent studies have focused on exploring whether language models possess the ability to learn from their previous mistakes and subsequently correct them. One line of techniques **Liu et al., "Verifying Language Model Reasoning via External Verifiers"** introduces the external verifier to evaluate the reasoning paths generated by LLMs. This evaluation is then used to construct preference training data for RLHF, with training conducted using algorithms such as PPO **Schulman et al., "Proximal Policy Optimization Algorithms"** or DPO **Gu et al., "Deep Double Deep Q-Learning"**, enabling self-improvement **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** of the models. Another line of techniques **Rajani et al., "Annotating Error Examples for Language Model Training"** involves pre-annotating error examples, allowing models to identify whether their current outputs contain issues and adaptively perform backspace during testing to regenerate content. In this paper, our method shows an inherent ability to learn from mistakes and achieves further self-improvement by learning from its search paths.


\paragraph{Inference Strategies of LLM Reasoning}
Many strategies for the inference phase have been proposed to enhance the reasoning capabilities of LLMs. Classical methods such as greedy decoding, beam search **Vijayakumar et al., "Pointer Network"**, and majority voting **Liu et al., "Majority Voting Ensemble Learning Method"** have been widely adopted. Additionally, Best-of-N (BoN) **Zhang et al., "Best-of-N Algorithm for Efficient Reasoning"** is a typical algorithm that generates $N$ complete answers through sampling and selects the optimal one based on the evaluation of a reward model. Recently, approaches that combine search algorithms with LLMs, such as best-first search **Li et al., "Best-First Search for Language Understanding"**, guided beam search **Kumar et al., "Guided Beam Search Algorithm"** and MCTS **Bhatnagar et al., "Monte Carlo Tree Search for Language Modeling"** have gained increasing attention due to their inference scaling law **Chen et al., "Inference Scaling Law"**. These methods often require additional components such as a verifier, outcome reward model (ORM) **Rahmati et al., "Outcome Reward Model for Language Understanding"**, or process reward model (PRM) **Zhou et al., "Process Reward Model for Efficient Reasoning"**, which increases computational cost. This paper proposes a novel method that integrates the verifier within the model to save computational resources while leveraging the advantages of search algorithms, demonstrating scalability in reasoning.

% \newpage
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/method.pdf}
    \caption{
        \textbf{The overall framework of Self-Backtracking.} During the training phase, the language model is instructed on when and where to backtrack. The inference phase employs a backtracking algorithm that considers both depth and breadth. The self-improvement phase utilizes expert iteration to enhance the fast thinking capabilities of the model.
        }
    \label{method}
\end{figure*}