\section{Related Work}
\paragraph{Learn from Search Trajectories}
Recently, several studies have explored using symbolic search algorithms to construct trajectory data and train transformer models to learn these search strategies, with the aim of enabling models to perform reasoning tasks. For instance, \citet{yang2022chain} employs Monte Carlo Tree Search (MCTS) or BFS to construct reasoning trajectories. Searchformer \cite{searchformer} and DualFormer \cite{dualformer} utilize traces from A* search to train language models, with each trace containing state information, A* heuristic values, and search history. Stream of Search (SoS) \cite{sos} constructs trajectories using various search algorithms to help language models learn the commonalities across different search strategies, facilitating the discovery of new search strategies. GSoS \cite{gsos} further extends SoS by integrating optimal solutions into the process of learning search trajectories.
 DeepSeek R1 \cite{guo2025deepseek} employs end-to-end reinforcement learning to autonomously acquire the capability of search in language. However, training these model to learn exploration trajectories may conflict with guiding it to generate optimal trajectories, leading to inefficient overthinking when solving easy problems.

\paragraph{Learn from Mistakes}
Numerous recent studies have focused on exploring whether language models possess the ability to learn from their previous mistakes and subsequently correct them. One line of techniques \cite{an2023learning,tong2024can,cpo} introduces the external verifier to evaluate the reasoning paths generated by LLMs. This evaluation is then used to construct preference training data for RLHF, with training conducted using algorithms such as PPO \cite{ppo} or DPO \cite{dpo}, enabling self-improvement \cite{yuan2024self} of the models. Another line of techniques \cite{cundy2023sequencematch,zhang2024backtracking,ye2024physics} involves pre-annotating error examples, allowing models to identify whether their current outputs contain issues and adaptively perform backspace during testing to regenerate content. In this paper, our method shows an inherent ability to learn from mistakes and achieves further self-improvement by learning from its search paths.


\paragraph{Inference Strategies of LLM Reasoning}
Many strategies for the inference phase have been proposed to enhance the reasoning capabilities of LLMs. Classical methods such as greedy decoding, beam search \citep{teller2000speech,graves2012sequence}, and majority voting \cite{wang2022selfconsistency} have been widely adopted. Additionally, Best-of-N (BoN) \cite{bonw} is a typical algorithm that generates $N$ complete answers through sampling and selects the optimal one based on the evaluation of a reward model. Recently, approaches that combine search algorithms with LLMs, such as best-first search \cite{tot}, guided beam search \cite{xie2024self} and MCTS \cite{choi2023kcts,feng2023alphazero,zhang2024rest,xie2024monte} have gained increasing attention due to their inference scaling law \cite{wu2024inference,snell2024scaling}. These methods often require additional components such as a verifier, outcome reward model (ORM) \cite{lightman2023let}, or process reward model (PRM) \cite{lightman2023let,wang2024math}, which increases computational cost. This paper proposes a novel method that integrates the verifier within the model to save computational resources while leveraging the advantages of search algorithms, demonstrating scalability in reasoning.

% \newpage
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/method.pdf}
    \caption{
        \textbf{The overall framework of Self-Backtracking.} During the training phase, the language model is instructed on when and where to backtrack. The inference phase employs a backtracking algorithm that considers both depth and breadth. The self-improvement phase utilizes expert iteration to enhance the fast thinking capabilities of the model.
        }
    \label{method}
\end{figure*}