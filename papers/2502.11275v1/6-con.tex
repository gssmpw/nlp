\section{Conclusion and Future Work}

This paper proposes a large-scale IE pre-training paradigm with the LLM's pre-training and post-training resources. The massive nutrition incubates a versatile \our model, which outperforms the pre-training with previous IE resources. \our can evolve with the data preparation for LLMs.
Further work on \our will focus on variants in learning paradigms, datasets, and backbones.

