\section{Our \our}

\subsection{Next Tokens Extraction}

The learning paradigm for LLMs is next token prediction (NTP), which calculates the representation of a context $[x_1, x_2, \cdots, x_t]$ to output a probability distribution $p_{t+1}$ of the next token $x_{t+1}$ over all potential tokens in the LLM's vocabulary. The prediction $p_{t+1}$ is optimized by the cross entropy loss to maximize its value on $x_{t+1}$.

We modify NTP into next tokens extraction (NTE) for cases that the span of next $n$ tokens $[x_{t+1}, \cdots, x_{t+n}]$ already exist in the context $[x_1, x_2, \cdots, x_t]$, such that $[x_{k+1}, \cdots, x_{k+n}] = [x_{t+1}, \cdots, x_{t+n}] (1 \leq k \leq t-n)$. When we detect such $(t,k,n)$, we annotate IE tags for the context as $[l_1, l_2, \cdots, l_t]$ following a BIO scheme. We first set all tags $l$ to \textit{O}. As there can be multiple $k$ for $t$, for each $k$, we set $l_k$ to \textit{B} and $[l_{k+1}, \cdots, l_{k+n}]$ to \textit{I}. The high-level idea of NTE is to replace prediction by extraction for duplicative spans that appear multiple times in the context.

NTE thus allows IE pre-training to directly exploit NTP datasets for LLM training, which significantly broadens the potential training data. During the inference, one can adjust the prompts of an NTE-based tagger to instruct it to perform different kinds of extractive tasks. Recall the strengths mentioned for NTE in the introduction, NTE specialized for IE has advantages in parameter efficiency, inference efficiency, and adaptability over NTP. 

\subsection{Massive Nutrition for \our}

\paragraph{Pre-training and Post-Training} With NTP-to-NTE conversion, we can simply copy the two training stages for LLMs, to perform pre-training and post-training for NTE-based IE taggers. Pre-training learns raw texts while post-training learns instruction-following dialogues between the user and the IE assistant. During pre-training, we annotate BIO tag sequences based on all $(t,k,n)$ triplets, assuming the multiple appearances of the same span of tokens indicate a certain level of extractive relation~\citep{UCPhrase}. For post-training, we suppose the extraction should focus on the texts provided by users so we only keep $(t,k,n)$ triplets that $k$ falls in the user's request and $t$ falls in the assistant's response.

Then, we select the resources for pre-training and post-training. While the NTE framework allows us to exhaust all kinds of resources, we use only one dataset for each stage for experiment efficiency. For pre-training, we select the popular C4 (CommonCrawl) dataset~\citep{t5}, which contains $4$B passages and is commonly used to pre-train LLMs. For post-training, we use the most advanced TuluV3~\cite{tulu3} dataset with $939$K instruction-following interactions between the user and the assistant.

To further boost the experiment efficiency, we first collect noun phrases parsed by SpaCy\footnote{\href{https://spacy.io/}{https://spacy.io/}}, filtering stop words or punctuations. Then we collect $5\%$ of the rest spans (no overlapping) that are duplicative to produce NTE instances. On C4, we keep the first $100$M NTE instances transformed from the raw texts. On TuluV3, we transform all post-training interactions into the NTE format, resulting in $2.6$M instances. We also sample $5\%$ spans not existing in their previous contexts, whose NTE labels are annotated by all \textit{O} as negative cases.

With the $102.6$M instances, we continually pre-train a \texttt{roberta-large} model~\citep{roberta} as the BIO tagger for NTE, optimized by AdamW~\citep{AdamW} with learning rate initialized to $10^{-5}$. The batch size is set to $64$, taking about $1.6$M steps for the optimization.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/length_stats.pdf}
%     \caption{The length distributions of the NTE instances.}
%     \label{fig:length_stats}
%     \vspace{-5mm}
% \end{figure}

\subsection{Statistics}

Besides the huge scale, we analyze other key statistics of our massive NTE dataset to investigate its efficiency in learning various IE targets. Our investigation is respectively done on the two pre-training and post-training data splits. 

\paragraph{How ``extractive'' are the data?} An obvious concern on the NTE dataset is whether the automated annotations reflect real extractive relations. We prompt the advanced LLM, \texttt{gpt-4o}~\citep{achiam2023gpt4}, to identify whether NTE data establish real extractive relations. The responses on $20$K sampled data show $93.39\%$ pre-training data and $96.20\%$ post-training data contain extractive relations, which shows the high data efficiency of the annotation strategy.

\paragraph{How diverse are the data?} The data is extremely diverse by containing any duplicative spans in a broad domain. We find around $28$M unique spans in C4 and $0.4$M in TuluV3, which is combined with highly diverse contexts in C4 and TuluV3. Our dataset covers various span lengths (maximally $40$ words) and context lengths (maximally $512$ words). The proportion of span with $\geq 4$ tokens is $4.52\%$, which seems small but still contains $4.6$M spans because of the large scale of our dataset. Our context length is also more diverse than previous IE pre-training resources~\citep{multinerd,NuNER,metaie} where data only have one or two sentences as context.

\paragraph{What is the conversion rate?} The conversion rate from a sentence to an NTE instance is $332\%$ for C4 and $235\%$ for TuluV3. This is highly efficient in comparison with traditional IE pre-training datasets relying on scarce links or expensive synthesis. The full C4 dataset can be transformed into $5$B NTE instances. However, the efficiency is still relatively lower than NTP. Only $4.06\%$ tokens in pre-training and $4.14\%$ tokens in post-training are used for NTE tagger learning, which indicates the supervision from LLM resources can be further augmented.