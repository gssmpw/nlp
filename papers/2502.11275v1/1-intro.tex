\section{Introduction}

The biggest lesson researchers have learned from training large language models (LLMs)~\citep{tulu,llama-2,achiam2023gpt4,olmo,dubey2024llama3,team2024gemma} is the power of massive and high-quality data~\citep{scaling_law,scaling_law_transfer}. Although pre-training information extraction (IE) models~\citep{pretrain_ner,multinerd,UIE,TadNER,NuNER,metaie} has once been a popular topic before the rise of general LLMs, the relative scarcity of automated annotations has limited the further development of this domain. Consequently, more and more researchers have accepted LLMs as backbone models for IE tasks~\citep{llm4clinicalie,gpt-ner,llm4ie}.

The primary reason for the temporary lag in IE pre-training is the stricter format requirements for data collection compared to those for LLMs.
The paradigm for learning LLMs, the next token prediction (NTP), can utilize every token in the sentence as an annotation. In contrast, IE pre-training always requires spans annotated with label names. 
While certain platforms provide massive annotations, such as Page Links in Wikipedia~\citep{ner_wiki,fewnerd,fewrel,multinerd}, they are still much less efficient than NTP. 
To illustrate the gap, Multinerd~\citep{multinerd} takes multiple processing efforts to collect $164$K English named entity recognition (NER) instances from Wikipedia and Wikinews, while NTP can easily gather trillions of tokens from raw texts as supervision. 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/paradigm-v2-cropped.pdf}
    \caption{\our takes a free ride on LLM resources (e.g., C4 and TuluV3~\citep{tulu3}) by formalizing next token prediction for duplicative spans as extraction in the BIO paradigm. During the inference, the prompts can be adjusted to different extractive tasks, making \our a versatile IE model.
     % \jingbo{can we say something like ``repeated'' (in the figure too)? duplicative reads weird to me}
    }
    \label{fig:paradigm}
    \vspace{-5mm}
\end{figure*}

This paper proposes a frustratingly simple yet effective way to scale up IE pre-training. We suggest that IE pre-training can simply be a free rider on the LLM's training resources by learning on exactly the same pre-training and post-training datasets. We modify NTP to next tokens extraction (NTE), using BIO tags for next tokens that can be extracted from the input context as shown in Figure~\ref{fig:paradigm}. With the instruction-following ability learned in post-training, one can adjust the prompt to instruct NTE-based taggers to perform different IE tasks.

Specialized for IE, NTE has three advantages over NTP. 1) Parameter Efficiency, NTP requires extra parameters to store knowledge to generate tokens not in the input context, while NTE concentrates only on tagging input tokens. 
Thus, NTE-based IE taggers can have better parameter efficiency than NTP-based LLMs, fitting it to smaller models like RoBERTa~\citep{roberta}. 2) Inference Efficiency, NTE taggers are not only smaller because of the parameter efficiency but can also extract multiple tokens with the BIO scheme in one forward pass. 3) Transferability, NTE taggers can easily adapt to IE tasks, which are typically annotated in the same BIO scheme.

With NTE, we easily collect $100$M pre-training instances from C4\footnote{We estimate the English part of C4 can be transformed into $5$B instances, we only take $100$M $(2\%)$ for experiment efficiency.}~\citep{t5}, a popular pre-training dataset, and $2.6$M chat-formatted instances from TuluV3 post-training dataset~\citep{tulu3} to endow the model with instruction-following ability.
 % \jingbo{are these data already in chat format? or it's transformed by us specifically? mention that chat is good for instruction-following IE?}
We continually train a RoBERTa tagger on massive NTE data, which results in our \emph{\our} model, a free rider with a training paradigm similar to NTP on training resources for LLMs. We present the comparison of scale, cost and diversity with other IE pre-training datasets in Figure~\ref{fig:cuckoo_data_scale}.
% \zilong{I'm confused that why NTP is mentioned so heaviliy when training a Roberta model. Since we are using NTE to construct data for Roberta, is there an alternative/baseline where NTP is used instead?}
% \jingbo{I agree with Zilong that we should stop mentioning NTP after a certain point. I would suggest that we just mention NTP when we talk about more researchers accepted LLMs as IE foundation models. After we defined NTE, we will just focus on it from there.}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/cuckoo_data_scale.pdf}
    \caption{Comparison of scale, cost, and diversity among different IE pre-training datasets. Our data collection for \our is free by converting LLM's learning resources, which forces the tagger to learn from diverse contexts. \our can also evolve with the data collection for LLM's post-training.}
    \label{fig:cuckoo_data_scale}
    \vspace{-5mm}
\end{figure}

We follow the few-shot adaptation evaluation in previous works~\citep{multinerd,NuNER} to benchmark \our, which shows that \our is as versatile as LLMs in extractive tasks. Training with few-shot data, \our can quickly understand different kinds of NER labels, free text questions in machine reading comprehension, and complex instructions, to perform precise extraction. With overwhelming advantages in data scale, \our outperforms models pre-trained on massive human-annotated or LLM-synthesized datasets by a large margin.

Finally, we analyze to show 1) \our can evolve with the data collection for LLM's post-training data; 2) in-context tagging ability emerges in \our just like in-context learning in LLMs; and 3) \our scales up by the increasing number of our constructed NTE data.
% \footnote{Open \our: Repo link hidden during reviewing.}
% \jingbo{mention the reproducibility things? like we will release the dataset, we will share the cuckoo checkpoints, etc.?}