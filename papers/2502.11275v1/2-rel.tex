\section{Background}

\paragraph{Information Extraction} Information extraction (IE) is one of the most fundamental applications in natural language processing. IE systems take the user's requirement (e.g., defined by a label text, a question, or an instruction) and extract spans of several tokens from input texts. The two most frequent categories of IE targets are entity and relation, which structure many IE tasks, such as named entity recognition~\cite{conll2003}, relation extraction~\cite{conll2004}, event extraction~\citep{ace2005multilingual}, and others~\citep{srl-task,DBLP:conf/semeval/PontikiGPPAM14,aste-task}. A crucial challenge to modern IE systems is the growing number of IE targets (e.g., various label names) in the open world, which are scarce in annotation and require IE systems for quick transfer learning. Thus, many works have collected massive automated IE annotations to pre-train IE models~\cite{fewnerd,multinerd,TadNER,NuNER,metaie}, which shows benefits in transferring to low-resource IE targets.

\paragraph{Large Language Model} The biggest game-changer for natural language processing in all domains is the large language model (LLM)~\citep{tulu,llama-2,achiam2023gpt4,olmo,dubey2024llama3,team2024gemma}. Learning on trillions of tokens for pre-training and post-training, LLMs have shown surprisingly strong performance on all kinds of tasks~\citep{achiam2023gpt4}. Next token prediction, the paradigm behind the success of LLMs, supports exploiting every token in raw texts as the annotation to strengthen the model's capability. Consequently, many IE researchers have turned toward LLMs~\citep{llm4clinicalie,gpt-ner,llm4ie} to use them as strategic information extractors with planning~\citep{LLM_Plan,LLM_NestNER} and chain-of-thoughts~\citep{chain_of_thoughts,cot_re}.

\paragraph{Pre-training Paradigm: IE v.s. LLM} The rise of LLMs has challenged the meaningfulness of IE pre-training with an overwhelmingly larger number of annotations. The lagging of IE pre-training can be attributed to the relatively high format requirement for IE annotation like labels in Wikipedia links. This paper shows IE pre-training can take a free ride on LLM's NTP paradigm to unleash the power of massive pre-training.