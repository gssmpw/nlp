\section{Experiments}

\begin{table}
\centering
\small
\scalebox{1.0}{
\begin{tabular}{p{1.2cm}p{5.5cm}}
\toprule
Level & Example\\
\midrule
Basic &  Organization\\
\midrule
Query & Which organization launched the campaign?\\
\midrule
\multirow{3}*{Instruction} & Organization (Disambiguation: The organization entity must be a subject of any active action in the context.)\\
\bottomrule
\end{tabular}
}
\caption{IE targets of different understanding levels.} 
\vspace{-5mm}
\label{tab:level}
\end{table}

Different from previous evaluation procedures that enumerate IE tasks~\citep{UIE,tanl,metaie}, our evaluation splits IE tasks into different levels of understanding the IE target. Specifically, the three levels are 1) Basic IE, understanding a single label text for an entity or a relation, such as named entity recognition. 2) Query-based IE, understanding a sentence-level query, such as machine reading comprehension (MRC). 3) Instruction-following IE, understanding complex extractive instructions like LLMs.

Examples of different understanding level are enumerated in Table~\ref{tab:level}. We expect that \our will be comparable to traditional IE pre-training on Basic IE as most popular label texts have been enumerated by LLM synthesis~\citep{NuNER,metaie}. \our's advantage over traditional IE pre-training is on query-based and instruction-following IE, which requires understanding more complex IE targets.

\subsection{Benchmark and Evaluation}

Following the high-level evaluation objective, we use several traditional benchmarks for each level of IE ability. Method and benchmark details are included in Appendices~\ref{apdx:detail} and~\ref{apdx:itie}.

\paragraph{Basic IE} benchmarks the understanding of simple labels for entity and relation. We include $4$ named entity recognition datasets (CoNLL03~\citep{conll2003}, BioNLP2004~\citep{bionlp2004}, MIT-Restaurant/Moive~\citep{tner}) and $2$ relation extraction datasets (CoNLL04~\citep{conll2004} and ADE~\citep{ADE}).

\paragraph{Query-based IE} requires the understanding of more complex sentence-level semantics of the IE target. We thus include $3$ machine reading comprehension datasets (SQuAD~\citep{SQuAD}, SQuAD-V2~\citep{SQuAD-V2}, DROP~\citep{DROP}). We filter out non-extractive questions in DROP.

\paragraph{Instruction-following IE} is a feature of LLMs when they are applied for IE. Users can include detailed requirements for the IE target in the prompt, which is hard for traditional IE systems that only understand simple label texts. However, instruction-following IE currently lacks of benchmarks\footnote{Existing InstructIE benchmarks~\citep{InstructIE_UIUC,InstructIE_ZJU} concentrate more on using instruction for traditional IE than instruction-awareness.}. Based on the real role of instruction in IE, we apply rules and a strong LLM, \texttt{GPT-4o}, to synthesize $3$ instruction-following IE by modifying traditional benchmarks. 1) \textbf{Disambiguation}, we write a definition instruction for $3$ ambiguous types, (\textit{``Organization''} in CoNLL2003, \textit{``Protein''} in BioNLP2004, \textit{``Location''} in MIT-Restaurant), such as \textit{``Disambiguation: The organization entity must be a subject of any active action in the context.''}. We use \texttt{GPT-4o} to filter out entities that no longer meet the IE target, resulting in a new instruction-following IE benchmark. 2) \textbf{Preference}, there are different ground truth answers in machine reading comprehension like ``Bruno Mars'', ``Mars''. However, one might prefer the longer or the shorter answer. Thus, we modify the SQuAD dataset with $3$ instructions with a preference for ``Longer answer'', ``Shorter answer'', ``Concise answer (Answer with no extra words)''\footnote{This means when ``Los Angeles'', ``the US'' and ``US'' all exist in the answer candidates, ``the US'' will be removed but ``Los Angeles'' will be kept.}. This filtering modification is automated by functions with no LLM involved. 3) \textbf{Miscellaneous}, we write $3$ instructions to define the \textit{``Miscellaneous''} entity type in CoNLL2003, MIT-Restaurant, and MIT-Movie. In practice, we clarify the existing miscellaneous type for CoNLL2003 and combine $3$ minority types as miscellaneous for MIT-Restaurant and MIT-Movie. We calculate metrics only on miscellaneous entities to evaluate whether the model can understand the scope definitions.

The evaluation continues with the model's few-shot adaptability. The model will be fine-tuned on a few examples in the training set and then evaluated on the test set. For basic IE, we will have $5$ shots for each entity/relation category. For query-based IE, we will have $32$ training examples. For instruction-following IE, the definition of few-shot follows the original dataset. We include more details for the construction of instruction-following IE benchmark in Appendix~\ref{apdx:itie}.
% The model learns different instructions together in each instruction-following IE tasks, resulting in $3\times 2\times 10 = 60$ training examples for Disambiguity, $3\times 32 = 96$ for Preference, and $3\times 10=30$ for Miscellaneous.

We benchmark IE performance with the traditional F1 score. For Basic IE, it refers to the Micro F1 for labeled entity spans. In Query-based IE, the F1 score refers to the maximal word-level F1 between the answer and one of the ground truths. Instruction-following IE benchmarks follow the metric of the original datasets.

% We use the traditional F1 score\footnote{For Basic IE, it refers to the Micro F1 for labeled entity spans. For Query-based IE, it refers to the maximal word-level F1 between the answer and one of the ground truths. Instruction-following IE benchmarks follow the metric of the original datasets.} for benchmarking. To further emphasize the instruction-awareness, we propose $\Delta_I$F1 score as the metric on instruction-following IE. $\Delta_I$F1 penalizes the ignorance of instructions and thus subtracts the F1 score with ground-truth by the F1 score with the answer candidates eliminated by the instructions. 

\subsection{Baselines and Variants}

\input{table_basic_ie}

We incorporate baselines into our experiments to validate our two main claims. 1) NTE is a paradigm that can scale up the data resources for IE pre-training, which learns taggers with better few-shot adaptability, especially in instruction-following. 2) NTE is a more efficient paradigm than NTP for IE, which results in significantly stronger extractive ability of NTE-based taggers than NTP-based LMs.

For 1), we include previous IE pre-training resources to compare their pre-training effects with our NTE-based dataset. These resources include,

\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{MultiNERD}~\citep{multinerd} is a NER pre-training dataset based on Wikipedia and Wikinews, which contains $164$K instances in the English split with $17$ label names. The annotations are from community contributors.
    \item \textbf{NuNER}~\citep{NuNER} is a massive NER pre-training dataset synthesized by ChatGPT-$3.5$~\citep{chatgpt} on massive raw texts. NuNER has $4.38$M instances with $273$K unique label names.
    \item \textbf{MetaIE}~\citep{metaie} is a massive IE pre-training dataset synthesized by ChatGPT-$3.5$ and $4$ with a broader coverage than simple NER. The LLMs are prompted to enumerate possible important information for entities and relations. MetaIE includes $237$K IE instances with $31$K unique label names.
\end{itemize}

In addition to resources using annotations for label names, we also consider machine reading comprehension as a pre-training task for IE, as it can be viewed as query-based IE. We thus include,

\begin{itemize}[nosep,leftmargin=*]
    \item \textbf{MRQA}~\citep{MRQA} is a collection of machine reading comprehension data that extracts an answer from a passage for a question in each instance. We exclude SQuAD as it is used for benchmarking, which remains $488$K instances.
\end{itemize}

For 2), we use the same resources for \our (C4+TuluV3) to continually pre-train an OPT model~\citep{OPT} in the same parameter scale ($\sim300$M) as the base model RoBERTa of \our. We select OPT because its NTP pre-training resource has covered the one for RoBERTa~\citep{roberta,OPT}, which eliminates the attribution of \our's advantage to a better base model (RoBERTa).

For the ablation study, we include the variants of \our, which only use the LLM's pre-training (C4) or post-training (TuluV3) resource for IE pre-training. These two variants aim to demonstrate the contributions of both stages to justify the imitation of the LLM's training pipeline. 

% \textbf{\textcolor{red}{R}\textcolor{orange}{a}\textcolor{yellow}{i}\textcolor{green}{n}\textcolor{blue}{b}\textcolor{cyan}{o}\textcolor{purple}{w}} Cuckoo

\paragraph{Rainbow \our} Finally, we incorporate a strong variant combining more post-training resources, \emph{Rainbow \our}. Rainbow \our extends the post-training resource from only TuluV3 to merging multiple datasets including samples from MultiNERD, NuNER, MetaIE, and MRQA, which aims to exploit all possible resources to further boost the IE pre-training.

\paragraph{Zero-shot Performance} is also evaluated on our \our and its variant Rainbow \our to demonstrate the direct performance after the IE pre-training on LLM's resources.

\paragraph{Comparison with LLMs} is discussed in Appendix~\ref{apdx:vs_llm} to expand the comparison scope.

\subsection{Basic IE}

The performance on basic IE tasks is presented in Table~\ref{tab:basic_ie}. Our two main claims are supported by the experiment results,

1) \our outperforms all baselines using different IE pre-training resources on both entity and relation extraction. Among the baselines, the best-performing ones are NuNER for entity and MRQA for relation, which they specialize in. \our overwhelms the baselines with a much larger pre-training data scale. As \our with only the raw texts from C4 (pre-training) has already achieved comparable or better performance than baselines, the conversion to NTE shows strong data efficiency on raw texts. 

2) The NTE pre-trained RoBERTa (\our) outperforms the NTP pre-trained OPT, which validates our intuition that language models can be more parameter efficient by focusing on extraction.

Besides the validation of our main claims, we also have more discoveries from the performance of variants. The first observation is that both pre-training and post-training datasets contribute to adaptability. In basic IE tasks, the massive raw texts in C4 contribute more than the curated post-training data in TuluV3, which indicates the basic IE tasks are simple enough to be well transferred by learning without annotations. The Rainbow \our shows \our can be further enhanced with merging more post-training resources, demonstrating significantly strong IE ability. 

\subsection{Query-based IE}

\input{table_query_ie}

We present the performance of models on query-based IE (MRC) in Table~\ref{tab:query_ie}. Among out-of-domain models, \our significantly outperforms other models pre-trained on basic IE tasks, rivaling the model pre-trained on the in-domain MRQA dataset. The result exhibits the benefit of NTE to pre-train in a wild and diverse raw text distribution, contrasting the fixed templates in basic IE pre-training. Post-training resources show a more significant contribution to query-based than basic IE tasks as queries in MRC require higher instruction awareness. Merging MRQA into the pre-training, Rainbow \our shows a significant advantage over using only MRQA via unifying all kinds of pre-training resources by the NTE paradigm. 
% As MRQA shares the same task objective with the benchmark datasets, it has the most influence on improvement. 

\subsection{Instruction-following IE}

\input{table_instruct_ie}

Table~\ref{tab:instruct_ie} demonstrates the instruction-following ability of different IE models. The zero-shot performance implies that the task requires a higher-level understanding of IE instructions. \our once again significantly outperforms other models except for an in-domain case (MRQA on MRC-based preference instruction testing) and widens the gap, showing its strong adaption to new instructions with the following ability learned from LLM pre-training resources. Post-training data contribute the most to the ability to follow instructions, playing the same role as for LLMs. Occasionally, learning only post-training data outperforms the full \our. Rainbow \our, with a large amount of post-training supervision, once again significantly boosts the performance.

\begin{table}
\centering
\small
\resizebox{\linewidth}{!}
{
\begin{tabular}{lcccc}
\toprule
Method & {Long} & {Short} & {AnsSim $\downarrow$} & {DualEM} \\
\midrule
\our & $57.84$ & $51.39$ & $\textbf{40.48}$ & $11.67$ \\
MRQA & $62.61$ & $61.05$ & $48.17$ & $12.32$ \\
Rainbow \our & $\textbf{67.20}$ & $\textbf{63.67}$ & $44.58$ & $\textbf{18.95}$ \\
\bottomrule
\end{tabular}
}
\caption{Detailed analysis on the instruction-following ability of IE models with preference as an example.}
\vspace{-3mm}
\label{tab:instruct_ability}
\end{table}

\paragraph{\our reacts to instruction.} We provide a deeper investigation of \our's reactions to instructions. Specifically, we test the preference instructions for the longest and shortest answers, which will lead to different answers. We fine-tune pre-trained IE models with few shots for both the longest and the shortest answers and then test their instruction-following ability. For evaluation, we use answer similarity (AnsSim) between outputs from two instructions, where higher similarity indicates less instruction-awareness. We also use dual exact matching (DualEM) as a strict metric to evaluate whether the model correctly reacts to both instructions. AnsSim calculates the word-level F1 score between answers from two instructions and DualEM refers to the model accuracy to produce both answers correctly. Table~\ref{tab:instruct_ability} shows that the MRQA model is no longer significantly better than \our on DualEM. AnsSim also indicates MRQA model to have less instruction-awareness, restraining its strong MRC ability to be applied with specific instructions. In comparison, the Rainbow \our shows a much higher advantage over the MRQA model according to the DualEM metric, demonstrating a better efficiency in applying the MRC ability to the instruction-following scenario. 

% \subsection{\our+ X}