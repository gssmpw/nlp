\section{Analyses}

\subsection{Evolution with LLMs}

A feature of our \our is its evolution with LLM's training resources, especially for post-training data which are progressively curated by researchers~\citep{olmo,wizard_lm,tulu3}. In Figure~\ref{fig:coevolution_radar}, we plot the performance of \our post-trained by different versions of Tulu post-training datasets from V1 to V3~\citep{tulu,tulu2,tulu3} after pre-training on C4. All performances are normalized by a linear mapping from $[\mu-2\sigma, \mu+2\sigma]$\footnote{$\mu, \sigma$ are based on the performance of $4$ \our models (before post-training, after post-training with TuluV1 to V3)} to $[0, 10]$ for demonstration. The result illustrates a evolution between \our and the LLMs. With each evolution in post-training data collection for LLMs, \our's performance can also be expanded in most dimensions. In the future, \our can be further improved together with the quality of LLM's training data with the free-riding feature of our NTE paradigm. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/coevolution_sft_radar.pdf}
    \caption{The evolution of Cuckoo with LLM's post-training resources. Domain $[\mu-2\sigma, \mu+2\sigma]$ is annotated under each evaluation dimension.}
    \label{fig:coevolution_radar}
\end{figure}

\subsection{Emergence of In-context Tagging}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/in_context_tagging.pdf}
    \caption{In-context tagging ability emerges in Cuckoo but not in IE models pre-trained by other resources.}
    \label{fig:ict}
\end{figure}

In-context learning is an emerging skill in LLMs that adapts LLMs to new tasks with examples in the given context. We investigate whether in-context learning appears in \our, which uses a similar learning paradigm and resource as LLMs. We append $5$ examples for CoNLL2003 and $1$ example for SQuAD (due to context window limitation) to the context and test the in-context tagging performance of different models. In Figure~\ref{fig:ict}, we find only \our able to improve (at least retain) its IE ability while other models (even pre-trained on similar tasks) show a significant drop. Thus, NTE on LLM's resources is verified to enable in-context tagging for \our. As suggested in~\citet{emergence_of_icl}, the occasional burstiness in raw texts contributes to the emergence of in-context tagging in \our. While NuNER and MRQA are well formalized, they fail to learn models with in-context learning ability because of the lack of burstiness.

\subsection{Data Scaling Trend}

Data is an important factor in the scaling law~\citep{scaling_law}. Thus, we test the transfer learning ability of checkpoints pre-trained with different data scales to downstream tasks. We focus on the scaling law of raw texts in C4 as they are cheaper to scale up and we have discussed the evolution of \our with post-training data collection. Our investigation covers both early pre-training stages to $4.1$M instances and the scaling-up to $100$M.

\begin{figure}
    \centering
    \includegraphics[width=0.99\linewidth]{figures/cuckoo_data_scaling.pdf}
    \caption{The data scaling trend of \our on the early $4.1$M C4 instances and the massive $100$M instances.}
    \label{fig:scale}
\end{figure}

In the two subfigures of Figure~\ref{fig:scale}, we plot the data scaling trend in pre-training \our.  The upper figure shows a clear performance rising trend together with the increasing data amount, indicating all dimensions of IE ability are scaled-up in the early pre-training stage. In the scaling-up to $100$M stage, the macroscopic trend retains its steady increase but turbulence emerges. Some intermediate checkpoints like at $50\%\sim60\%$ data scale show a competitive performance with the fully pre-trained model. This implicates that the capacity of the small RoBERTa might meet its bound, and further improvement requires more parameters.