\section*{Limitations}

While \our validates the strength of NTE to take a free ride with LLM resources, our scope can be extended to several topics out of the main claims.

\paragraph{Label Embedding} Some IE paradigms (e.g., original NuNER) learns label embeddings to efficiently label the extracted spans. As \our imitates NTP to perform NTE, its IE process requires enumerating the label names similar as the generative IE using LLMs. Matching label embedding has its efficiency advantage while generative IE allows the label texts to interact with the context, resulting in potentially better performance. \our follows the generative IE paradigm to pursue better performance based on the established success of LLMs. However, future efforted can be devoted into a label embedding version of \our, which takes the context as the label text to boost the IE efficiency.

\paragraph{Data Source} The C4 corpus for raw text features broad coverage. However, recent progress in LLMs shows that specific sources of pre-training data (e.g., textbooks) benefit certain skills of LLMs, such as math. This paper only discusses C4 to avoid the IE performance improvement attributed to a specific data source. Future works can extend our scope to compare the effect of all kinds of resources in pre-training, which might find certain resources are superior in IE pre-training using NTE.

\paragraph{Backbone Variants} The current scopes is designed to justify the benefit of NTE in gathering massive IE pre-training data. Thus, the comparison is biased to data quality rather than backbone models. Further exploration in backbone models include the scaling law in model size, multilingual backbone, and model architectures.