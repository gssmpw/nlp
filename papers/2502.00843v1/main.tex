% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{xcolor,colortbl}
\usepackage{color}
\usepackage{comment}
\usepackage[misc]{ifsym}
\renewcommand{\baselinestretch}{0.985}
\definecolor{minetable1colorx}{rgb}{0.75, 0.75, 0.75}
\newcommand{\mineyes}{{\scriptsize \CheckmarkBold}}
\newcommand{\mineno}{{\scriptsize \textcolor{minetable1colorx}{\XSolidBrush}}}
\definecolor{lightgrey}{gray}{0.9}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{algorithm}
\usepackage{algorithmic}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{VLM-Assisted Continual learning for Visual Question Answering in Self-Driving}


%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{%
\textbf{Yuxin Lin, Mengshi Qi, Liang Liu, Huadong Ma} \\ 
State Key Laboratory of Networking and Switching Technology~~~\\ Beijing University of Posts and Telecommunications, China\\
\{linyuxin2019, qms, liangliu, mhd\}@bupt.edu.cn
}



\begin{document}
\maketitle

\begin{abstract}
In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a "teacher" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from $21.40\%$ to $32.28\%$ across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code.
\end{abstract}
% Uncomment the following to link to your code, datasets, an extended version or similar.
%
% \begin{links}
%     \link{Code}{https://aaai.org/example/code}
%     \link{Datasets}{https://aaai.org/example/datasets}
%     \link{Extended version}{https://aaai.org/example/extended-version}
% \end{links}
\section{Introduction}




With the rapid advancement of technology, autonomous driving~\cite{aitech,autonomoustechnology}  has become a landmark achievement in the development of modern transportation systems. Self-driving cars utilize various advanced sensors, cameras, and artificial intelligence algorithms to interpret the traffic environment, promising a safer and more efficient driving experience. 
%%Developments in this field have not only contributed to the modernization of traffic management systems, but also heralded significant changes in traffic safety and urban planning. 
Nowadays, Vision-language models~\cite{EM-VLM4AD,Drivevlm,clip,blip,llava}, play a crucial role as the core of autonomous driving systems. These models understand and predict road conditions and their complexity by parsing visual images and video data collected from cameras and other sensors, combined with simultaneously processed verbal commands. 

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{./picture/New_structure.png}
    \caption{This illustration presents a framework for a Vision-Language Model (VLM) designed for multi-task autonomous driving. (a) The upper section shows four essential tasks: Perception, Prediction, Planning, and Behavior, each represented with a specific question-answer example to demonstrate the model's response to various driving scenarios. (b) The lower section outlines the model's simplified pipeline, incorporating memory replay with knowledge distillation and projection layer regularization to enhance continual learning capabilities across tasks. (c) The diagram in the bottom-right corner illustrates how continual learning methods enable the model to add new knowledge and functionality without overwriting or erasing prior knowledge.}
    \label{structure}
\end{figure*}


% 主要的挑战是什么+其他领域的VLMCL
Although vision-language models (VLMs)~\cite{EM-VLM4AD,Drivevlm} show great potential in handling complex driving tasks, these models still face multiple challenges in autonomous driving applications. With the diversity of task types including perception, prediction, and planning, VLMs in autonomous driving increasingly need to be equipped with strong continual learning capabilities. Continual learning~\cite{DER,EWC,EWC++,MER,vqacl} allows models to accumulate new knowledge based on existing knowledge without having to go back to initial training data to relearn. Currently, continual learning is also introduced in a number of VLM models and VQA models in mitigating catastrophic forgetting~\cite{vqacl,lei2023symbolic} in different domains, e.g., medical domain~\cite{yu2024boosting,bai2023revisiting,chen2024llm,yi2023towards} and so on. 
%%Although in other domains VLMs with continual learning approaches have demonstrated effectiveness in mitigating catastrophic forgetting~\cite{Catastrophic}. 
However, in the context of autonomous driving, the application of these methods requires special adaptation and optimization. Any learning inaccuracies may lead to serious safety consequences. Addressing catastrophic forgetting within the VLM framework for autonomous driving tasks—such as perception, prediction, and planning—requires innovative strategies beyond standard sequential training. As illustrated in Figure~\ref{structure}, VLMs trained on new tasks may experience catastrophic forgetting, where prior task knowledge is overwritten, leading to loss of critical information. Therefore, ensuring that VLMs are able to adapt to new driving environments and tasks without losing prior knowledge is crucial for improving the reliability and safety of autonomous driving systems.



To address above-mentioned issues, in this work, we introduce a novel hybrid continual learning strategy with VLMs, which combines selective memory replay~\cite{ER} with knowledge distillation~\cite{KD}, enhanced by projection layer regularization. Specially, selective memory replay periodically reinforces previously learned information to mitigate forgetting, while knowledge distillation enables the model trained on earlier tasks to act as a teacher, guiding subsequent tasks and preserving complex task knowledge. This strategy balances learning new tasks and retaining essential prior knowledge, crucial in a continually changing autonomous driving environment. By replaying real driving data, the system retains critical safety-related information across diverse driving conditions, thus enhancing the model's reliability and generalization capability.

To further strengthen our model against the challenge of catastrophic forgetting, we introduce a new task-specific regularization methods of projection layers. In models with a relatively large number of model parameters and experimental data, both the classical continual learning parameter regularization approach~\cite{EWC} and the gradient projection approach~\cite{GEM} face challenges in terms of computational resource requirements and operational efficiency. Therefore, following the idea of regularization, we regularize the learning process by carefully designing the model feature projection layer and calculating the loss based on the differences in feature representations in successive tasks. This regularization not only reduces additional computational and memory requirements, but also prevents the dilution of previously learned features and ensures that the model's learning trajectory closely follows the continuum, thus mitigating the effects of sudden knowledge transfer. 



Our main contributions can be summarized as follows:  
\begin{enumerate}
    \item We introduce a new framework for tackling the Visual Question Answering (VQA) task in autonomous driving by leveraging Vision-Language Models (VLMs) in combination with continual learning strategies, enabling the model to adapt seamlessly across different driving tasks.
    \item We present an innovative approach combining memory replay and knowledge distillation, and enhance the continual learning by introducing a projection layer to achieve regularization in the feature embedding layer.
    \item We benchmark our model on the DriveLM dataset, achieving significant performance improvements. Compared to the baseline model, our method demonstrated improvements across various metrics, ranging from a minimum of $21.40\%$ to a maximum of $32.28\%$, highlighting its superior capacity to handle complex multimodal VQA tasks in autonomous driving environments.
\end{enumerate}




\section{Related Work}

\noindent\textbf{Vision Language Models for VQA.} In the field of autonomous driving, the application of Vision-Language Models (VLMs) is rapidly expanding~\cite{you2024v2x,Drivevlm}, enhancing the system's understanding of and decision-making capabilities in complex driving environments. VLMs, through extensive image-text pre-training, have already provided the ability for zero-shot learning in autonomous driving~\cite{vlmsurvey}. For example, the DriveVLM~\cite{Drivevlm} system explores the integration of VLMs within traditional autonomous driving technologies, enhancing the vehicle's spatial reasoning and planning capabilities. Additionally, the DriVLMe~\cite{drivlme} and Co-driver~\cite{Co-driver} projects explore how LLMs mimic human understanding and behavior in handling complex road conditions from the perspectives of experiential learning and social interaction, respectively. In recent years, Vision-Question-Answering (VQA) datasets for autonomous driving, such as the Nuscenes-QA~\cite{nuscenesqa}dataset, DriveLM dataset~\cite{drivelm}, have further facilitated advancements in the multimodal understanding of driving scenarios. In our model, we delve deeper into the field of continual learning for autonomous driving, building upon the EM-VLM4AD~\cite{EM-VLM4AD} framework.  

\noindent\textbf{Continual Learning.} Continual learning (CL) techniques like memory replay~\cite{DER}, regularization~\cite{EWC,MAS}, and optimization~\cite{GEM,OGD} mitigate catastrophic forgetting by balancing new and old tasks. Memory replay stores key data or features in a buffer, using methods such as random selection~\cite{chaudhry2019tiny} or feature averaging~\cite{iCaRL}. Regularization approaches add terms that selectively preserve crucial parameters based on their importance, assessed using tools like the Fisher Information Matrix (FIM)~\cite{EWC,EWC++}. Optimization methods, such as OGD~\cite{OGD}, maintain previous gradient directions while adjusting current gradients for orthogonality. However, in our approach, we employ some techniques for text vectorization and clustering of different tasks in the dataset to optimize data selection in the memory replay mechanism. In addition to this, we naturally combine memory replay with knowledge distillation (KD), which additionally integrates the past information of the old model. Due to the relatively large number of parameters in the VLM, regularization with gradient projection via parametric aspects is not very practical. Therefore, we emulated the idea of regularization and projection. We did this by comparing the projection layer gaps between different tasks after the model's feature embedding was introduced. Additionally, we added extra terms to the loss function to further enhance the continual learning effect at the model's feature level.

\section{Proposed Approach}


\begin{figure*}[htbp]
  \centering
  \begin{subfigure}{.472\textwidth}
    \centering
    \includegraphics[width=\linewidth]{./picture/ERKD.png}  % 第一张图的路径
    \caption{Memory replay with knowledge distillation}
    \label{ERKD}
  \end{subfigure}\hfill
  \begin{subfigure}{.528\textwidth}
    \centering
    \includegraphics[width=\linewidth]{./picture/Pro.png}  % 第二张图的路径
    \caption{Task-specific Embedding projection regularization}
    \label{Pro}
  \end{subfigure}\hfill
  \caption{(a)~\textbf{Memory replay with knowledge distillation} shows memory replay and knowledge distillation when Memory data coming in. Optimizing data selection using TF-IDF and K-means clustering to maintain a diverse and representative memory set. (b)~\textbf{Task-specific embedding projection regularization} demonstrates task-specific projection layers within an autonomous driving model to maintain feature continuity and mitigate catastrophic forgetting by transforming multimodal embedding into unique task-specific spaces and regulating the model with a specialized loss function.}
\end{figure*}

\subsection{Overview}
% figure
Targeting the autonomous driving visual question answering (VQA) dataset, we propose a new continual learning approach for Vision-Language Model (VLMs). This method alternates continual learning in two phases: one part is carried out during the data replay phase, and the other part during the current task data training phase. We construct a simple autonomous driving VLM using a straightforward image embedding network (ViT)~\cite{ViT} combined with a pre-trained T5 language model, applying the designed continual learning method to this VLM. The visualization of our continual learning model is shown in Figure~\ref{structure}, which includes memory reply with knowledge distillation and embedding projection. Next, we will delve into the details of each continual learning phase.


\subsection{Memory Reply with Knowledge Distillation}
In multitask data training for autonomous driving, models often suffer from catastrophic forgetting when confronted with different tasks. To address this challenge, we introduce an new memory replay with knowledge distillation approach(Figure~\ref{ERKD}), to retain important historical data to help the model learn new tasks while efficiently maintaining the memory of old tasks and avoiding knowledge loss.    
 
Specially, our approach divides the data into $D = \{D_1, D_2, \ldots, D_n\}$ based on tasks \(T_n\), where each task is a different type of VQA dataset, and each task has a corresponding multi-class problem. Different from randomly selecting the past task data to compose the memory, in our approach, we choose the combination of TF-IDF and K-means to record the memory as the following. 

\textbf{Memory Reply.} Firstly, we perform text vectorization on the data in the task and use TF-IDF to extract features from the problem set, which helps us understand the main semantic content of the data. 
Then, by applying the K-means algorithm, we cluster the questions into several categories, which ensures that the data stored in the memory is representative and diverse in terms of contents.
For each clustering, we refine the selection process: the number of samples to be selected for each category, denoted by \(S_{c,n}\), is determined based on the proportion of data in each category, ensuring that the importance of each category is balanced. Specifically, we calculate the ratio of the total data volume \(D_n\) to the data volume \(D_{c,n}\) in each category \(c\) within task \(D_n\) , and use this ratio to determine \(S_{c,n}\). This approach not only improves the coverage of the samples but also increases the adaptability and robustness of the model to different types of problems. \(S_{c,n}\) can be formulated as the following:
\begin{equation}
S_{c,n} = \left\lfloor \frac{|D_{c,n}|}{\sum_{i=1}^{k_n} |D_{i,n}|} \times S_n \right\rfloor,
\end{equation}
where \( D_{c,n} \) represents the volume of data for category \( c \) in task \( n \), \( k_n \) represents the total number of categories in that task, and \( S_n \) represents the total number of samples chosen.



Eventually, a specified number of data samples are randomly selected from each category. These samples are then combined with data from other tasks to form a new memory \(R_n\). As training progresses, the memory is continuously updated by adding data from the most recent task and replacing some of the existing data from other tasks at a certain ratio. The new memory \(R_n\) is defined as:

\begin{equation}
R_n = \bigcup_{i=1}^{n} \left\{ \text{sample}(D_{c,i}, S_{c,i}) \mid c \in C_i \right\},
\end{equation}
where \( C_i \) is the set of categories in task \( i \), and the function \( \text{sample}(D_{c,i}, S_{c,i}) \) refers to randomly selecting \( S_{c,i} \) samples from category \( c \) in task \( i \)'s dataset \( D_{c,i} \).


    

 
When dealing with complex data for autonomous driving, memory replay, while mitigating catastrophic model forgetting, still has limitations in ensuring robust generalization across diverse scenarios. Considering the importance of detailed information in autonomous driving models, the introduction of knowledge distillation via distillation can more effectively convey essential information and promote smoother transitions between tasks. This enhances the model's ability to generalize across varying conditions and improves its overall performance. Along with the memory replay strategy, we also integrate a knowledge distillation approach to improve the retention of prior knowledge, ensuring a more stable and consistent learning process.


\textbf{Knowledge Distillation.} In our approach, knowledge distillation guides the current model \(M_s\) by utilizing the model trained in the previous task as the teacher model \(M_t\). The teacher model provides insight into the specific details of the previous task and outputs the corresponding output probabilities encoded as \(K_t\). These probabilities were adjusted by temperature parameters to enhance the smoothness of the distribution, making subtle decision boundaries more easily captured by the student model.
The ability of the student model to replicate the teacher's decision-making process was effectively quantified by calculating the loss of the teacher model from the current model's predictions probabilities \(K_s\) through KL scatter during training, which can be formulated as the following:
\begin{equation}
\mathcal L_{KD}(K_t \parallel K_s) = \sum_i K_t(i) \log\left(\frac{K_t(i)}{K_s(i)}\right),
\end{equation}
where \( K_t(i) \) and \( K_s(i) \) represent output probabilities of the teacher and student models for category \(i\), respectively.


By adjusting the balance between the distillation loss $\mathcal{L}_{KD}$ and the traditional task-specific loss $\mathcal{L}_{GT}$ in the autonomous driving task, we filtered the replay data according to the data characteristics of the dataset to ensure as much comprehensiveness of the data from past tasks as possible. At the same time, knowledge distillation was introduced to optimize the benefits between direct task learning and the detailed guidance provided by the teacher model. Performance can be improved by mixing losses to ensure that the model is not only capable of basic continual learning properties, but can also be adapted to new tasks in terms of understanding complex autonomous driving interactions. The loss function for the network in memory replay is formulated as
\begin{equation}
\mathcal Loss = \alpha \mathcal L_{KD} + (1 - \alpha) \mathcal L_{GT},
\end{equation}
where \( \alpha \) is the parameter controlling the weight between knowledge distillation loss and task-specific loss.


\subsection{Embedding Projection}
In the continual learning task of autonomous driving, while memory replay and knowledge distillation can guide and assist the model in learning more abstract features and decision logic, this approach relies on the accuracy and generalization ability of the teacher's model with the replayed data. Furthermore, it primarily targets the similarity of the model outputs rather than directly addressing the feature continuity across tasks. Accordingly, the considerable bias of the model resulting from training on new tasks can be constrained and mitigated by regularizing the model during the phase when the model is trained on the current data for each task. In the context of the autonomous driving VLM model, the parameter regularization and gradient projection techniques that are typically applicable to small-scale networks cannot be directly applied. Consequently, our approach involves introducing a projection layer into the feature embedding block of the model(Figure~\ref{Pro}). By updating the feature projection layer and regularizing the model during the training of each task, it is possible to maintain the overall structure of the model's features without distorting them by new tasks.

In our work, each task is associated with a dedicated projection layer \(P_n\), designed to transform the merged feature embeddings from multimodal inputs (text and image) into a task-specific feature space \(F_n\). Each projection layer \(P_n\) consists of a weight matrix \(\mathbf{W}_n\), which linearly transforms the feature vector from the preceding layer into a new task-specific feature space:
\begin{equation}
    \mathbf{F}_n = \mathbf{W}_n \mathbf{e},
\end{equation}
where \(\mathbf{e}\) represents the feature embeddings obtained from the multimodal input processing. The transformation is designed to ensure that each task’s feature representation is both unique and aligned with the task-specific requirements.



During training, we employ a regularization approach that involves computing the mean squared error (MSE) between the feature outputs of the projection layers for the current and previous tasks. This MSE is used to formulate a regularization loss $\mathcal{L}_{pro}$, which serves to minimize the drift in feature space across tasks, thereby mitigating catastrophic forgetting, as the following:
\begin{equation}
    \mathcal L_{pro}(n, m) = \frac{1}{d} \sum_{i=1}^d \left(\mathbf{F}_n[i] - \mathbf{F}_m[i]\right)^2,
\end{equation}
where \(d\) is the dimensionality of the feature vectors, and \(i\) indexes the elements of these vectors, \(n\) represents the current task, \(m\) represents a previous task.
%%\qms{what is n and m?}




As the model encounters new tasks, we dynamically adapt the network by adding new projection layers while freezing the parameters of previous layers. This ensures that the newly learned tasks do not disrupt the knowledge encoded in the weights of earlier tasks. The overall regularization loss for the network, accumulated over all tasks up to the current one, is given by:
\begin{equation}
    \mathcal L_{pro}(n) = \sum_{m=1}^{n-1} \mathcal L_{pro}(n, m),
\end{equation}
thus maintaining a continuity of learning without catastrophic interference.

The training procedure optimizes the combined loss, which includes task-specific losses and the projection layer regularization loss. The loss function for the network is formulated as:
\begin{equation}
    \mathcal Loss = \mathcal{L}_{GT} + \lambda \mathcal{L}_{pro},
\end{equation}

\noindent where \(\mathcal{L}_{pro}\) represents the projection layer regularization loss and \(\lambda\) is a hyperparameter that balances the importance of the regularization term relative to the task-specific loss.

This approach ensures that our model adapts to new tasks while retaining critical information from previous tasks, thereby facilitating effective continual learning in a complex, multi-task environment. The algorithm for this approach will be presented in the supplementary materials. The total loss function can be formulated as the following:
\begin{equation}
\mathcal Loss_{total} = 
\begin{cases} 
\alpha \mathcal  L_{\text{KD}} + (1 - \alpha) \mathcal L_{\text{GT}} & \text{if replay data}, \\
\lambda \mathcal L_{{pro}} + \mathcal L_{\text{GT}} & \text{otherwise}.
\end{cases}
\end{equation}




\section{Experiments}
\subsection{Experimental Settings}
\textbf{Dataset.}
We use the DriveLM \cite{drivelm} dataset, which is a multimodal dataset designed for autonomous driving research, containing a large amount of image and text data to support the development of vision-language models. In our experiments, we divided the DriveLM dataset into 4 tasks: Perception, Prediction, Planning, and Behavior, and each task contains a corresponding training/validation/testing set. More details please refer to supplementary documents.

\noindent\textbf{Metrics.}
In this experiment, we use a variety of evaluation metrics to comprehensively assess the performance of the model. Specifically, BLEU~\cite{Bleu}, which is used to evaluate the fluency and accuracy of the generated text; METEOR~\cite{Meteor}, which takes into account synonyms and sentence structure which is used to assess translation quality; ROUGE~\cite{Rouge}, which is mainly used to assess the degree of overlap between automatic digest or machine translation outputs and human referencing; and CIDEr~\cite{Cider}, which is specifically used to assess the quality of image description. These metrics assess the model's capabilities in language generation and comprehension for critical autonomous driving tasks: Perception, Prediction, Planning, and Behavior. The dataset includes task-specific questions in a Visual Question Answering (VQA) format to evaluate the performance of the model, ensuring alignment with real-world autonomous driving needs.





\begin{table*}[!t]
\centering
\footnotesize
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\textbf{Method} &
  \multicolumn{1}{c}{BLUE-1} &
  \multicolumn{1}{c}{BLUE-2} &
  \multicolumn{1}{c}{BLUE-3} &
  \multicolumn{1}{c}{BLUE-4} &
  \multicolumn{1}{c}{METEOR} &
  \multicolumn{1}{c}{ROUGE\_L} &
  \multicolumn{1}{c}{CIDEr}\\ 
    \cmidrule{1-8}

Joint   & 66.82 & 60.34 & 54.74 & 49.58 & 36.07 & 72.46 & 3.2 \\
Vanilla  & 36.67 & 31.10 & 28.06 & 25.75 & 21.13 & 53.29 & 1.8 \\
DriveLM-Agent  & - & - & - & 53.09  & 36.19   & 65.58 & 2.8 \\
    % \midrule
    \cmidrule{2-8}
LwF   & 44.45 & 39.75 & 36.63 & 34.08 & 24.75 & 56.02 & 2.1 \\
EWC   & 46.51 & 42.47 & 39.54 & 39.67 & 27.40 & 63.10 & 2.4 \\
DER    & 65.27 & 60.74 & 56.94 & 53.29 & 38.83 & 71.44 & 3.1 \\
\textbf{Ours}  &\textbf{68.95} & \textbf{63.80} & \textbf{59.49} & \textbf{55.33} & \textbf{40.17} & \textbf{74.69} & \textbf{3.4} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of different methods on various metrics, demonstrating the effectiveness of our proposed continual learning with VLM model for autonomous driving across multiple performance indicators. }
\label{Experiment Result}
\end{table*}


\begin{table}
\renewcommand{\arraystretch}{1.1}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccc|cccc}
\toprule
\multicolumn{1}{c}{ER} &
\multicolumn{1}{c}{KD} &
\multicolumn{1}{c|}{PRO} &
  \multicolumn{1}{c}{BLUE-4} &
  \multicolumn{1}{c}{METEOR} &
  \multicolumn{1}{c}{ROUGE\_L} &
  \multicolumn{1}{c}{CIDEr}\\ 
    \cmidrule{1-7}
\checkmark  &  &  & 55.80 & 39.89 & 75.03 & 3.20 \\
\checkmark  & \checkmark  &  & 50.83 & 39.91 & 73.21 & 3.22 \\
 &  & \checkmark &31.80 & 23.94 & 53.19 & 1.85 \\
\checkmark& \checkmark & \checkmark &  \textbf{55.33} & \textbf{40.17} & \textbf{74.69} & \textbf{3.41} \\
\bottomrule
\end{tabular}
} 
\caption{Ablation studies on the effects of individual components and their combinations in methods.}
\label{ablation study}
\end{table}






\noindent\textbf{Compared Methods.}
We compare our method with the following methods on the DriveLM dataset~\cite{drivelm}: the weight regularization method EWC~\cite{EWC}, the function regularization method LwF~\cite{LwF}, and the memory replay method DER~\cite{DER} to validate the effectiveness of our approach. In addition, we provide a lower bound (Vanilla), which simply performs gradient updating without any countermeasures against forgetting, and a relative upper bound (Joint), which trains all tasks jointly. Moreover, we compare another method that uses a joint training approach but is based on a different model, referred to as DriveLM-Agent~\cite{drivelm}, to demonstrate the performance differences in handling multitask learning across various model architectures.


\noindent\textbf{Implementation Details.}Our implementation is based on the EM-VLM4AD framework~\cite{EM-VLM4AD}, utilizing a Vision Transformer (ViT)~\cite{ViT} for multi-view image embedding and a pre-trained T5 model for text processing, enabling Visual Question Answering (VQA) in autonomous driving. We enhance this with a projection layer for better embedding integration and employ continual learning strategies like memory replay and knowledge distillation to mitigate catastrophic forgetting and retain knowledge across tasks. We implement our model based on Pytorch~\cite{pytorch} on a single NVIDIA L20 GPU. The training phase of each task is performed for 4 epochs, and each model takes about 1 day to complete. For hyper-parameters, we use a learning rate of 1e-4, a weight decay of 0.05, a batch size of 4, a temperature parameter set to 2 and $\alpha$ value is set to 0.7 for the knowledge distillation part. A dynamic weight parameter to adjust the projection regularization loss, where the $ \lambda $ parameter is initialized to 0.05 and halved progressively with each task. More details of experimental hyperparameter settings and experiments refer to supplementary materials.



\subsection{Results}
 



\noindent\textbf{Quantitative Analysis.}
As shown in Table~\ref{Experiment Result}, our model significantly outperforms existing continual learning methods in key performance indicators for language and vision tasks. Our approach yielded the highest scores in BLEU, METEOR, ROUGE, and CIDEr metrics. Specifically, our method achieved a BLEU-1 score of $69.85\%$, surpassing the next best method (Joint training) by $2.13\%$. In more complex metrics, such as BLEU-4 and METEOR, our model demonstrated improvements of $5.75\%$ and $4.10\%$ points over the Joint method, respectively, indicating a better ability to handle nuanced language generation in the context of autonomous driving and highlight our model's enhanced ability to generate context-aware descriptions of the driving environment, crucial for interpreting dynamic situations like traffic signals, pedestrians, and obstacles. The improvement in CIDEr scores from 3.2 to 3.4 also suggests enhanced capability in generating relevant, human-aligned descriptions of images, improving the model's effectiveness in interpreting and reacting to dynamic driving environments. Our experiments demonstrate the efficacy of our proposed continual learning VLM model for autonomous driving across various critical metrics.



Furthermore, as illustrated in Table~\ref{Experiment Result}, our model outperforms traditional continual learning methods like EWC, LwF, and DER across several metrics. Specifically, it surpasses EWC by $15.66\%$ and LwF by $21.25\%$  in BLEU-4 scores, highlighting the limitations of methods relying solely on regularization. Unlike these approaches, our model incorporates memory replay and advanced knowledge distillation, improving task retention and prediction refinement. Additionally, while DER includes memory replay, it lacks projection layer regularization and knowledge distillation, key components in our method for handling the complexities of multimodal data in autonomous driving. Our model also improves METEOR and ROUGE\_L scores by $1.34\%$ and $3.25\%$ over DER.
Moreover, our CIDEr score of 3.4 is significantly better than all methods we compare. This highlights the effectiveness of our dynamic projection regularization, which constrains model over-migration through regularization. These comparative results suggest that persistence is crucial in situations where models encounter diverse and unpredictable environments, ensuring high accuracy and robustness under different driving conditions.


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{./picture/Cases.png}
    \caption{Example of correct and failure answer generations from our model.}
    \label{cases}
\end{figure*}

% 加一个case analysis

\subsection{Ablation Studies}

In order to analyze and study the effects of the components of our method, we designed several ablation experiments, with results shown in Table~\ref{ablation study}.

\noindent\textbf{Memory Reply and Knowledge Distillation.} Using memory replay/experience replay (ER) alone significantly enhances the model's continual learning performance. While knowledge distillation (KD) has a limited effect on traditional metrics, it effectively improves the CIDEr score by smoothing probability distributions, thereby reducing overconfident predictions and enhancing prediction stability. 

\noindent\textbf{Projection regularization (Pro).} Our proposed Pro plays a vital role by constraining the feature space, helping the model retain essential task-specific information and improve adaptability across tasks. The last line in the table demonstrates that combining all components yields strong performance across all variants, proving the overall effectiveness of our continual learning method. Further experimental validation of KD’s role in maintaining task-specific focus and Pro’s contribution to enhanced model adaptability and performance is provided in the supplementary materials.



 
 
 

\begin{table}
\renewcommand{\arraystretch}{1.0}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c|}{Method} &
\multicolumn{1}{c}{Exist} &
  \multicolumn{1}{c|}{Comparison} &
  \multicolumn{1}{c}{Average $\uparrow$}&
  \multicolumn{1}{c}{Forget $\downarrow$ } \\ 
    \cmidrule{1-5}
w/o CL  & 70.47  & 62.56 & 40.78 & 14.40\% \\
w/ CL  & \textbf{77.49}  & \textbf{67.54} & \textbf{49.27}  & \textbf{4.5\%} \\
\bottomrule
\end{tabular}
}
\caption{Performance of our method on Nuscenes-QA Dataset.}
\label{NuscenesQA}
\end{table}
 
\subsection{Analysis}



\noindent\textbf{Performance of Our Method on Other VQA Datasets.}
The table~\ref{NuscenesQA} illustrates the effectiveness of our proposed method on the Nuscenes-QA~\cite{nuscenesqa} dataset, demonstrating significant improvements across critical evaluation metrics. With the incorporation of continual learning (CL), our approach achieves notably higher accuracy in both the 'Exist' and 'Comparison' tasks, accompanied by a substantial increase in the overall 'Average' score. Additionally, the marked reduction in the 'Forget' rate underscores the model's enhanced capability for knowledge retention and adaptability in evolving environments.

\noindent\textbf{Adaptability of CL method with Other VLM model.}We replaced the original ViT and T5 backbones in our VLM framework with LLaMA-Adapter-v2\cite{llama} to evaluate the adaptability of our continual learning (CL) approach. Table~\ref{llama_backbone} compares models without CL ("w/o CL") and with CL ("w/ CL") using LLaMA-Adapter-v2, showing significant improvements in BLEU, METEOR, ROUGE\_L, and CIDEr scores. This comparison aims to evaluate the robustness and adaptability of our continual learning (CL) approach when applied to alternative model architectures. From the results, the BLEU, METEOR, ROUGE\_L, and CIDEr scores improved significantly, demonstrating enhanced fluency, contextual accuracy, and alignment with human references, particularly for image-related tasks.
This comparison further underscores the importance of continual learning in maintaining performance in dynamic and multi-tasks autonomous driving scenarios. It also highlights that our method is not limited to specific backbones, offering a flexible solution for integrating continual learning into different vision-language frameworks.

\begin{table}
\renewcommand{\arraystretch}{1.0}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc}
\toprule

\multicolumn{1}{c|}{Method} &
  \multicolumn{1}{c}{BLUE-1} &
  \multicolumn{1}{c}{BLUE-2} &
  \multicolumn{1}{c}{BLUE-3} &
  \multicolumn{1}{c}{BLUE-4} \\
    \cmidrule{1-5}

  
w/o CL  & 49.29 & 44.55 & 41.36 & 38.74 \\
w/CL  & \textbf{67.41} & \textbf{62.41} & \textbf{58.08} & \textbf{54.05} \\
\midrule
\midrule

\multicolumn{1}{c|}{Method} &
  \multicolumn{1}{c}{METEOR} &
  \multicolumn{1}{c}{ROUGE\_L} &
  \multicolumn{1}{c}{CIDEr}\\
  \cmidrule{1-5}
w/o CL   & 28.72 & 61.40 & 2.37 \\
w/CL   & \textbf{38.87} & \textbf{73.28} & \textbf{3.17} \\
\bottomrule
\end{tabular}
}
\caption{Performance Comparison of LLaMA-Adapter-v2 Backbone with and without Continual Learning (CL). }
\label{llama_backbone}
\end{table}



\subsection{Case Analysis}

Figure~\ref{cases} illustrates our approach to multitask learning with the autonomous driving framework, dividing the outcomes into correct and failure cases. In correct cases, for example, after the model is trained across multiple tasks, the application of continual learning methods enables it to retain its ability to accurately perform the Perception task. The model correctly answers queries about the status of a motorcycle, identifying it as without a rider. However, when continual learning methods are not employed, the model's responses are dominated by more recent tasks, causing it to lose crucial information from earlier tasks, highlighting the adverse effects of catastrophic forgetting in multitask scenarios.



Even with continual learning, the model struggles in failure cases, likely due to dataset distribution imbalance, such as in the Behavior task where the training examples of parked cars are very limited. Further case analyses are provided in the supplementary materials..


\section{Conclusion}
In this paper, we presented a continual learning approach applied to vision language models (VLMs) for autonomous driving with VQA task. The method combines memory replay and knowledge distillation, and demonstrates its strong performance on the DriveLM dataset by introducing a projection layer for regularization in feature embedding. Extensive experimental results demonstrate the effectiveness and generalizability of the proposed model.



{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

% WARNING: do not forget to delete the supplementary pages from your submission 
\input{sec/X_suppl}

\end{document}

