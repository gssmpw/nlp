\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Hyperparameters}
\label{sec:hyperparameters}


\subsection{Memory Size} Table \ref{Memory Size table} shows how the model's performance varies with different sizes of replay data. Overall, memory replay improves the overall continual learning performance of the model regardless of the size of the replay data. Based on the data in the table, it can be seen that the model improves with larger replay data in the two metrics BLUE and METEOR, because BLUE and METEOR focus more on text matching. However, although a large amount of replay data can reduce forgetting, it poses a significant challenge to the model's memory as the replay data becomes larger. Also, according to the ROUGE\_L and CIDEr metrics, it can also be seen that an increase in the amount of model replay data has a positive effect on the model's text-image fusion understanding, but too much replay data can also limit the model's learning of new tasks, which reduces the overall continual learning performance.




\subsection{Temperature Setting}
Table \ref{Temperature Setting table} shows the role of the knowledge distillation model for model Memory replay performance for different temperature and parameter settings. For the same temperature parameter T setting, different $\alpha$ values indicate different compositions of model loss when memory replay is subjected to knowledge distillation. As $\alpha$ increases, increasing the weight of knowledge distillation loss means that the model relies more on the knowledge of the teacher model, which may help the student model learn some intrinsic patterns and relationships that are not easily observed at the simple labeling level. On the other hand, for different temperature coefficients, higher T values may result in a weaker gradient signal being passed from the teacher model to the student model. In this case, the backpropagated signals received by the student model during training may not be sufficient to effectively tune its parameters, resulting in a model that is not as effective at learning past knowledge. Therefore, in our model, we chose T = 2 and $\alpha$ = 0.7, which is the parameter of function 4 in the main body, as the model parameters for knowledge distillation.






\subsection{Lambda Setting}
Table~\ref{lambda setting table} demonstrates the usefulness of the projection regularization approach for continual learning of the model for different settings of the projective regularization coefficients $\lambda$, which is the parameter of function 8 in the main body. The table shows that setting larger or smaller fixed regularization coefficients does not work for the overall architecture of the model, which changes as tasks increase. In our approach, the model projection layers are incremented as tasks increase, and each task projection layer constrains the model by calculating the distance to the previous task projection layer. With fixed $\lambda$ coefficients, the model regularization data increases as the model task increases, which has a large impact on model migration and stability, and thus model performance is poor. The dynamically decreasing $\lambda$ with the increase of model tasks can make the multi-projection layer gap in different tasks are maintained at a relatively stable amount, so that the model can remain stable during the migration process between tasks.


\begin{table}
\renewcommand{\arraystretch}{1.1}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc}
\toprule

\multicolumn{1}{c|}{Memory Size} &
  \multicolumn{1}{c}{BLUE-4} &
  \multicolumn{1}{c}{METEOR} &
  \multicolumn{1}{c}{ROUGE\_L} &
  \multicolumn{1}{c}{CIDEr}\\ 
    \cmidrule{1-5}
1000  & 52.00 & 37.11 & 71.32 & 3.06 \\
3000  & 53.07 & 37.79 & 72.05 & 3.12 \\
5000  & \textbf{55.33} & \textbf{40.17} & \textbf{74.69} & \textbf{3.41} \\
8000  &56.52 & 41.10 & 73.56 & 3.35 \\
\bottomrule
\end{tabular}
}
\caption{The comparison of different memory sizes in the model on various metrics demonstrates the effect of different memory sizes on the continual learning approach.}
\label{Memory Size table}
\end{table}


\begin{table}
\renewcommand{\arraystretch}{1.1}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc}
\toprule

\multicolumn{1}{c|}{Temperature Setting} &
  \multicolumn{1}{c}{BLUE-4} &
  \multicolumn{1}{c}{METEOR} &
  \multicolumn{1}{c}{ROUGE\_L} &
  \multicolumn{1}{c}{CIDEr}\\ 
    \cmidrule{1-5}

T=2/$\alpha$=0.6  & 34.08 & 24.74 & 56.02 & 2.04 \\
T=2/$\alpha$=0.7  & \textbf{55.33} & \textbf{40.17} & \textbf{74.69} & \textbf{3.41} \\
T=3/$\alpha$=0.7  & 41.37 & 31.20 & 54.47 & 2.64 \\
\bottomrule
\end{tabular}
}
\caption{The comparison of different knowledge distillation parameters in the model on various metrics demonstrates the effect of different temperature coefficients and parameters for the continual learning approach.}
\label{Temperature Setting table}
\end{table}


\begin{table}
\renewcommand{\arraystretch}{1.1}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cccc}
\toprule

\multicolumn{1}{c|}{lambda setting} &
  \multicolumn{1}{c}{BLUE-4} &
  \multicolumn{1}{c}{METEOR} &
  \multicolumn{1}{c}{ROUGE\_L} &
  \multicolumn{1}{c}{CIDEr}\\ 
    \cmidrule{1-5}

$\lambda$=0.01  & 53.41 & 38.42 & 72.43 & 3.01 \\
$\lambda$ change  & \textbf{55.33} & \textbf{40.17} & \textbf{74.69} & \textbf{3.41} \\
$\lambda$=0.1  &55.98 & 40.16 & 74.2 & 3.11 \\
\bottomrule
\end{tabular}
}
\caption{The comparison of different projection regularization coefficients in the model on various metrics demonstrates the effect of different regularization coefficients in for the continual learning approach.}
\label{lambda setting table}
\end{table}





\begin{figure*}[htbp]
  \centering
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{./picture/memo.png}  % 第一张图的路径
    \caption{Impact of Memory Size Settings}
    \label{fig Memory size}
  \end{subfigure}\hfill
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{./picture/temp.png}  % 第二张图的路径
    \caption{Impact of Temperature Parameter Settings}
    \label{fig temperature setting}
  \end{subfigure}\hfill
  \begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{./picture/lambda.png}  % 第三张图的路径
    \caption{Impact of Lambda Values Settings}
    \label{fig lambda setting}
  \end{subfigure}\hfill
  \caption{Illustration of a detailed visual analysis about how different configuration settings impact the performance metrics of our continual learning model for autonomous driving. Each subfigure presents a unique aspect of model tuning and its direct correlation with the performance in language and vision tasks. }
\end{figure*}







\section{Knowledge Distillation and Projection Regularization}

\subsection{Knowledge Distillation}
Knowledge Distillation (KD) is introduced in the model to encourage smoother and more balanced output probability distributions. This is particularly critical in autonomous driving tasks, where stability and reliability of predictions directly influence performance in real-world scenarios. To analyze this effect, we examine the entropy of the predicted probability distributions across different tasks at the final checkpoint, which corresponds to the completion of training on all tasks. The entropy $H(p)$ of a probability distribution $p$ is defined as:
\begin{equation}
H(p) = - \sum_{i=1}^C p_i \log(p_i),
\end{equation}
where $C$ is the number of classes, and $p_i$ represents the predicted probability for class $i$. Higher entropy indicates smoother predictions, avoiding extreme confidence in a single class.


\begin{table}
\renewcommand{\arraystretch}{1.2}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|cc|ccc}
\toprule

\multicolumn{1}{c|}{Task} &
\multicolumn{1}{c}{ER} &
\multicolumn{1}{c|}{ER+KD} &
\multicolumn{1}{c}{Test loss w/Pro} &
\multicolumn{1}{c}{Test loss w/o Pro} &
\multicolumn{1}{c}{Difference} \\
\cmidrule{1-6}

Perception  & 0.6153	& 0.6369 &0.1849	&0.1848	&0.0001\\
Prediction & 0.3515	    & 0.2964 & 0.1212	&0.1999	&-0.0787\\
Planning  &0.6120	    & 0.6301 &0.1084	&0.1111	&-0.0027 \\
Behavior  &0.7019	    &0.7856 & 0.1024	&0.1052	&-0.0028\\
\bottomrule
\end{tabular}
}
\caption{Comparison of average entropy and test loss across tasks. The table shows entropy with Experience Replay (ER) alone and ER with Knowledge Distillation (KD), as well as test loss with and without Projection Regularization (Pro).}
\label{kdpro}
\end{table}




Table~\ref{kdpro} shows the average entropy for tasks under two settings: using Experience Replay (ER) alone and combining ER with KD. 
For tasks Perception, Planning, and Behavior, the addition of KD increases entropy values, indicating a smoother distribution of predicted probabilities. In Behavior, for example, entropy rises from 0.7019 to 0.7856. This suggests that KD mitigates overconfident predictions, enhancing robustness in complex environments where the model must avoid excessive certainty in a single outcome.

In contrast, for Prediction, entropy slightly decreases from 0.3515 to 0.2964. This reflects a beneficial property of KD in tasks with limited plausible outcomes, where the model is guided to focus on relevant classes, improving prediction certainty while maintaining reliability. These findings underline the versatility of KD in balancing prediction smoothness and confidence depending on task-specific requirements.

\subsection{Projection Regularization}
Projection Regularization (Pro) imposes constraints on the feature space, which helps prevent overfitting and enhances the model's generalization to unseen data. To evaluate its effectiveness, we analyze the test loss across various tasks with and without Pro. The test loss $\mathcal L_{\text{test}}$ is calculated as:
\begin{equation}
\mathcal L_{\text{test}} = - \frac{1}{N} \sum_{j=1}^N \sum_{i=1}^C y_{j,i} \log \hat{y}_{j,i},
\end{equation}
where $y_{j,i}$ and $\hat{y}_{j,i}$ denote the ground truth and predicted probabilities for sample $j$ and class $i$, respectively.

Table~\ref{kdpro} summarizes the test loss values under both settings. The results demonstrate that Pro consistently reduces test loss across most tasks, highlighting its role in improving generalization. For the Prediction task, test loss significantly decreases from 0.1999 to 0.1212, indicating that Pro effectively constrains feature dependencies, enabling the model to adapt better to unseen scenarios. While the reductions in Planning and Behavior tasks are smaller, they still reflect the robustness that Pro introduces. 

For the Perception task, the change in test loss is negligible, suggesting that Pro neither harms nor substantially impacts performance in simpler scenarios. Overall, these results validate the effectiveness of Pro in promoting adaptability and robustness in real-world applications.

Both KD and Pro contribute critical enhancements to the model's performance in autonomous driving tasks. KD ensures smoother and more stable predictions, reducing overconfidence in complex environments while maintaining appropriate certainty in constrained tasks. Pro, on the other hand, strengthens generalization by constraining the feature space, improving test performance across diverse tasks. These complementary mechanisms play a vital role in ensuring the reliability and robustness of the model in safety-critical applications.









\section{Algorithm}
\begin{algorithm}
\caption{continual learning for AD VLM}


\textbf{Input}\\
\noindent \hangindent=3em \hangafter=1
Multi-task datasets \(\{D_1, \ldots, D_n\}\) with tasks \(T_n\), VLM model \(\phi\) with pre-trained components (ViT, T5), memory buffer \(R\)

\textbf{Output}\\
\noindent \hangindent=3em \hangafter=1
Trained VLM model \(\phi\) 

\begin{algorithmic}[1]
\STATE Initialize \(\phi\) and \(R\)
\FOR{\(n = 1\) to \(N\)}
    \STATE Load task-specific dataset \(D_n\) and task \(T_n\)
    \IF{\(n == 1\)}
        \STATE Train \(\phi\) on \(D_1\)
        \STATE Populate \(R\) using TF-IDF and K-means on \(D_1\)
    \ELSE
        \FOR{each iteration}
            \STATE Sample \(x\) from \(D_n \cup R\)
            \IF{\(x \in D_n\)}
                \STATE Update \(\phi\) using projection layer \(P_n\)
            \ELSE
                \STATE Replay using model \(M_t\) from \(T_{n-1}\)
            \ENDIF
        \ENDFOR
        \STATE Refresh \(R\) with samples from \(D_n\)
    \ENDIF
    \STATE Evaluate \(\phi\) on \(T_n\)
\ENDFOR
\end{algorithmic}
\end{algorithm}

The proposed algorithm integrates continual learning strategies for Vision-Language Models (VLMs) in autonomous driving, aiming to mitigate catastrophic forgetting while ensuring adaptability to new tasks. The algorithm operates iteratively across multiple tasks \( T_n \), leveraging pre-trained components 7and a dynamically updated memory buffer \( R \). The key steps are described below:

\begin{itemize}
    \item \textbf{Initialization:} The VLM model \( \phi \) and memory buffer \( R \) are initialized to prepare for training across tasks.
    \item \textbf{Task-Specific Training:}
For each task \( T_n \), the corresponding dataset \( D_n \) is loaded.
For the first task \( T_1 \), the model is trained directly on \( D_1 \), and the memory buffer \( R \) is populated using TF-IDF and K-means clustering to ensure representativeness and diversity of stored samples.
    \item \textbf{Memory Replay and Regularization:}
For tasks \( T_n \) (\( n > 1 \)), the model alternates between training on current task data \( D_n \) and replaying samples from the memory \( R \).
During training, task-specific projection layers \( P_n \) are updated to transform feature embeddings into task-specific spaces. This regularization helps maintain feature alignment across tasks, mitigating interference.
    \item \textbf{Dynamic Memory Update:}
After training on each task, the memory buffer \( R \) is refreshed by sampling representative data from both the current task and past tasks. This ensures the memory remains balanced and relevant for continual learning.
    \item \textbf{Evaluation:} Once trained on \( T_n \), the model is evaluated to measure its performance on the current task while retaining knowledge from previous tasks.
\end{itemize}

By combining memory replay, knowledge distillation, and embedding projection, the algorithm effectively balances learning new tasks and preserving prior knowledge, making it suitable for complex and dynamic autonomous driving scenarios. 










\section{Dataset distribution}

To optimize the selection of representative and diverse data samples for memory replay, we employ a combination of TF-IDF feature extraction and K-means clustering. The process begins with vectorizing the textual content (e.g., questions) from the dataset using TF-IDF, which extracts key semantic patterns in the data. Following this, K-means clustering is applied to categorize the textual data into distinct clusters, ensuring diverse representation across varying data contexts. This pipeline helps maintain semantic richness and diversity in the selected memory samples.

To further refine the memory replay strategy, we dynamically calculate the number of samples to be selected from each cluster based on the proportional size of the cluster in the dataset. This ensures a balanced contribution of each cluster to the memory while reflecting their significance in the data. The sample calculation for each cluster is determined using the formula:
\begin{equation}
S_{c,n} = \left\lfloor \frac{|D_{c,n}|}{\sum_{i=1}^{k_n} |D_{i,n}|} \times S_n \right\rfloor,
\end{equation}
where \( D_{c,n} \) represents the volume of data for category \( c \) in task \( n \), \( k_n \) represents the total number of categories in that task, and \( S_n \) represents the total number of samples chosen. This approach ensures that the sampling process is proportional, improving the adaptability and robustness of the memory replay.

\subsection{Perception}
For the perception task, the TF-IDF and K-means pipeline generated five clusters with distinguishable thematic distributions. Each cluster encapsulates a specific aspect of the perception data. Key semantic themes were extracted for each cluster to provide a clear understanding of their focus areas. Some clusters focuses on observed objects and their statuses (e.g., "status object" "observed status"), some clusters predominantly describes side objects and their interactions and some highlight scene-level reasoning and positioning(e.g., "current scene" "relative positioning").

% The analysis revealed key high-priority semantic indicators within each cluster. For instance, phrases such as "current scene" (5349.58), "status object" (2511.60), and "traffic element" (2120.61) were among the most representative. These indicators reflect critical aspects of autonomous perception, such as understanding the current driving scene, assessing object statuses, and identifying relevant traffic elements. This clustering approach ensures the memory retains representative and meaningful data points, enabling the model to generalize across various driving scenarios effectively.

The bubble chart visualizes the clusters and their respective phrase frequencies. Each bubble represents a cluster, with its size proportional to the TF-IDF weight of the most representative phrases. In figure~\ref{Perception_bubble}, Cluster 3 ("relative positioning" 1316.03) captures crucial reasoning about positioning, which is essential for understanding the spatial relationships between objects in the driving environment. The visualization demonstrates how the clustering process preserves diversity and ensures that key semantic patterns are retained.

\subsection{Prediction}
For the Prediction task, the TF-IDF and K-means pipeline generated five clusters, each with distinct thematic distributions. These clusters capture different semantic aspects of the Prediction task.Some clusters focus on the ego vehicle state and related actions, some Highlight objects that could influence future states (e.g., "future state") and some emphasize decision-making semantics (e.g., "relevant decision").

% Key high-priority semantic phrases were extracted from each cluster. For instance, “object consider” (2099.46) and “relevant decision” (2099.46) had the highest weights in Cluster 2, highlighting their importance in decision-making. “Moving direction” (2049.08) in Cluster 3 reflected critical reasoning about spatial relationships, while “future state” (1910.76) in Cluster 1 revealed the task's focus on predicting future scenarios. These high-weight phrases ensure that the memory retains essential and meaningful data points critical for addressing the Prediction task.
In figure~\ref{Prediction_bubble}, the phrase “future state” (1910.76) in Cluster 1 signifies the core semantic importance of predicting future scenarios. 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./picture/Perception_bubble.png}
    \caption{Bubble chart visualization of the perception task clusters generated using the TF-IDF and K-means pipeline.}
    \label{Perception_bubble}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./picture/Prediction_bubble.png}
    \caption{Bubble chart visualization of the prediction task clusters generated using the TF-IDF and K-means pipeline.}
    \label{Prediction_bubble}
\end{figure}

\subsection{Planning}
For the Planning task, the TF-IDF and K-means pipeline generated five clusters, each representing distinct semantic aspects of planning-related data.Some clusters highlight safe and dangerous actions in various scenarios (e.g., "safe actions" "dangerous actions"), some focus on vehicle considerations and actions taken to avoid collisions (e.g., "vehicle consider" "lead collision").


In figure~\ref{Planning_bubble}, the phrase “traffic signal” (1301.36) in Cluster 2 highlights the importance of considering external traffic signals in the planning process. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./picture/Planning_bubble.png}
    \caption{Bubble chart visualization of the planning task clusters generated using the TF-IDF and K-means pipeline.}
    \label{Planning_bubble}
\end{figure}













\section{Case Analysis}
\label{sec: Case Analysis}
We illustrate our approach to multitask learning within an autonomous driving framework through multiple cases, categorizing results into correct and failure cases, with separate illustrations provided for each. The model is evaluated on previously trained tasks to assess the efficacy of continual learning methodologies.

\subsection{Correct Cases} 
The "Correct Cases" illustrate the effectiveness of our continual learning approach in retaining task-specific knowledge across multiple tasks. Each case includes corresponding images, a question related to task, the ground truth (GT) answer, and test results obtained after training on various tasks. The results are compared between models with and without continual learning to highlight the differences.

% Case 1: Pedestrian Detection
For example, in case 1 (Figure~\ref{combine_1}) the perception task focuses on identifying the status of pedestrians in front of the ego vehicle. The ground truth specifies that "two pedestrians are moving." The model equipped with continual learning methods successfully retains the ability to provide correct answers across all subsequent task tests, including perception, prediction, planning, and behavior tasks. For example, the output consistently states, "Two pedestrians are moving," regardless of the task being tested. This demonstrates the model's ability to preserve its perception capabilities even after multitask training.
In contrast, the model without continual learning exhibits catastrophic forgetting. While it correctly answers the perception task in isolation, multitask training leads to the gradual degradation of its perception capabilities. For instance, the outputs become task-dependent and deviate from the original ground truth. The prediction task output adds irrelevant reasoning about safety, the planning task shifts focus to spatial orientation, and the behavior task entirely disregards pedestrian status, instead describing the vehicle's movement. This underscores the challenge of knowledge retention in multitask learning without continual learning strategies.

In correct case 5 (Figure~\ref{combine_2}) examines the model's ability to determine whether two specified objects in the back and front cameras are traffic signs. The ground truth states that ``only one of the boxes is a traffic sign.'' Results, including images, the question, and the ground truth (GT), are compared for models with and without continual learning.
The model with continual learning retains task-specific knowledge effectively. Across all tasks, including prediction, planning, and behavior, it consistently outputs the correct answer: ``Only one of the boxes is a traffic sign'' This demonstrates the success of continual learning in preserving knowledge across tasks, even as the model undergoes additional training.
In contrast, the model without continual learning suffers from catastrophic forgetting. While it correctly identifies the traffic sign in the prediction task, its outputs for subsequent tasks deviate significantly, focusing on irrelevant observations (e.g., "The road is relatively empty") or providing unrelated descriptions (e.g., "The ego vehicle is driving slowly"). These inconsistencies underscore the necessity of continual learning to maintain knowledge across tasks and prevent forgetting.
% Case 2: Truck Detection
% This case evaluates the perception task’s ability to recognize the status of a truck located at the back-left of the ego vehicle. The ground truth specifies that "one truck is parked." Similar to the first case, the model with continual learning consistently provides correct answers across subsequent tasks, retaining the perception task's ground truth answer, "One truck is parked," in all evaluations.

% In contrast, the model without continual learning demonstrates significant performance degradation after multitask training. While it initially recognizes the truck's status in the perception task, its outputs for subsequent tasks diverge. For instance, the prediction task misinterprets the situation, suggesting "keep going straight," while the planning task incorrectly describes the position of a truck in the right rear. The behavior task completely overlooks the truck’s status and instead describes the vehicle's actions inaccurately. These results further highlight the catastrophic forgetting issue when continual learning techniques are absent.

\subsection{Failure Cases}
The "Failure Cases" highlight scenarios where the model's outputs deviate from the ground truth despite undergoing training with continual learning methods. Each case includes corresponding images, a question related to the task, the ground truth (GT), and test results after training on multiple tasks. 

In failure case 1 (Figure~\ref{fcase_1}), the model is tasked with identifying the moving status of an object observed through the rear camera. The ground truth specifies "Going ahead." While the model correctly identifies the object's movement in the perception task, the outputs for subsequent tasks deviate. For example, the planning task incorrectly identifies the object's status as "Brake suddenly," likely due to the ambiguity in the dataset where the object's movement may appear inconsistent across frames. Such variations can introduce noise, leading to misinterpretation in planning tasks.

Failure case 2 (Figure~\ref{fcase_2}) evaluates the model’s ability to describe the visual characteristics of an object observed through the rear camera. The ground truth specifies "Black sedan." While the model correctly identifies the object as a "Black sedan" in the perception task, its outputs in subsequent tasks deviate. For example, the planning task concludes "Changing to the left lane," and the behavior task outputs "No entry." These inconsistencies could stem from the complexity of the scene, such as the presence of multiple objects or occlusions affecting the model's global understanding of the environment. Additionally, adverse weather conditions (e.g., low light or rain) in the images might increase the difficulty of the task and interfere with subsequent outputs.

Failure case 3 (Figure~\ref{fcase_3})  focuses on the model's ability to predict the future state of an object located in the back-left camera view. The ground truth specifies "Stationary." While the model correctly identifies the object as "Stationary" in the prediction task, its outputs in subsequent tasks describe the object as "Keep going straight." This inconsistency could be influenced by the background context of the scene, where the object might appear near a dynamic environment (e.g., close to a main road), making it more likely to be perceived as moving. Additionally, the stationary state of the object might not be visually obvious in the images, particularly if it is positioned at an intersection or surrounded by other moving objects, leading to misinterpretation of its future state.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth, height=20cm]{./picture/combine1.png}
    \caption{Correct Cases 1-3}
    \label{combine_1}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth, height=20cm]{./picture/combine2.png}
    \caption{Correct Cases 4-6}    
    \label{combine_2}

\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth, ]{./picture/combine3.png}
    \caption{Correct Cases 7-8}
    \label{combine_3}
\end{figure*}










\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./picture/fcase_1.png}
    \caption{Failure Case 1}
    \label{fcase_1}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./picture/fcase_2.png}
    \caption{Failure Case 2}
    \label{fcase_2}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./picture/fcase_3.png}
    \caption{Failure Case 3}
    \label{fcase_3}
\end{figure}

