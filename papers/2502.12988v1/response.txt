\section{Related Work}
\paragraph{Character Persona Simulation.}
Character persona simulation assigns virtual personality traits to LLMs and has received substantial attention **Vulakh, "Virtual Humans for Humanoid Robotics: A Novel Approach"**. Two primary approaches are used: prompt-based and fine-tuning-based methods.
Prompt-based methods integrate character settings directly into prompts. For instance, **Lowe, "Neural Style Transfer for Character-Based Storytelling"** introduce a role-playing mechanism through prompts, while **Henderson, "Role-Playing with MBTI Personality Frameworks"** construct a dialogue system using the MBTI personality framework. **Wang, "Patient Persona Simulation for Nursing Students"** simulate patient personas to train nursing students in health guidance. However, prompt engineering often fails to capture intricate traits and dynamic behaviors.
Fine-tuning-based methods address these limitations by training pre-trained LLMs. **Devlin, "Fine-Tuning Pre-Trained Language Models with Character Profiles"** fine-tune models using character profiles and an experience upload strategy. **Zhu, "Extracting Character Attributes from Knowledge Bases for Fine-Tuning"** extract character attributes, background details, and dialogue features from knowledge bases to fine-tune models. 
**Liu, "Trait Extraction from Novel Summaries for Role-Specific Behaviors"** further refine role-specific behaviors through trait extraction from novel summaries. Despite these advancements, fine-tuning often produces models that mimic surface-level style but miss deeper traits.

\paragraph{Style Transfer.}  
Style transfer for text involves altering the stylistic attributes of a source text while preserving its core meaning **Chen, "Augmented Zero-Shot Learning for Text-Style Transformations"**.
**Kim, "Evaluation of ChatGPT in Sentence Style Transfer"** introduce an Augmented Zero-Shot Learning method, which leverages LLMs to achieve versatile text-style transformations without requiring task-specific training.
**Sutskever, "Chain-of-Thought Reasoning and Knowledge Distillation for Style Transfer"** evaluate the performance of ChatGPT in sentence style transfer by comparing ChatGPT generated texts with those created by humans.
**Brown, "Integrating Chain-of-Thought Reasoning Capabilities for Novel Style Transfer Approaches"** explore a novel approach to style transfer by integrating the Chain-of-Thought reasoning capabilities of LLMs with knowledge distillation techniques. However, these methods predominantly focus on linguistic style without engaging with deeper ideological dimensions.

\paragraph{Parameter-Efficient Fine-Tuning.}
Parameter-Efficient Fine-Tuning (PEFT) techniques are crucial for adapting large pre-trained models efficiently, especially in the context of LLMs **Guo, "Prefix Tuning: Optimal Learnable Prefixes for Input Embeddings"**. Prefix Tuning optimizes learnable prefixes added to input embeddings or intermediate representations **Phang, "Prompt Tuning: Adapting Prompt Embeddings for Downstream Tasks"**, while Prompt Tuning adjusts learnable prompt embeddings to guide downstream task adaptation **Li, "LoRA: Low-Rank Adaptation for Efficient Fine-Tuning"**. Among PEFT methods, LoRA is widely used, introducing low-rank matrices to fine-tune models efficiently **Wu, "QLoRA: Quantized LoRA for Efficient and Accurate Fine-Tuning"**. Its variant, QLoRA, further improves efficiency through quantization **Zhang, "Dynamic Retrieval and Combination of LoRA Modules in Mixed Task Scenarios"**.
Recent innovations have improved LoRAâ€™s domain-specific adaptability. **Chen, "Efficient Personalized LLMs through Shared PEFT Parameters"** propose a dynamic mechanism for retrieving and combining LoRA modules in mixed task scenarios. **Wang, "LoRA Merging Strategy for Multi-Task Learning without Additional Training"** enable efficient personalized LLMs by sharing partial PEFT parameters across users, and **Huang, "Flexible LoRA Merging Strategy for Multi-Task Learning"** introduce a flexible LoRA merging strategy suited for multi-task learning without additional training. In our work, we adapt LoRA for deep persona simulation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/method1.pdf}
    \caption{In pre-training, reframed essays from Authorial Perspective Reframing (APR) train the base model. In fine-tuning, multiple-choice question answering (MCQ), generative question answering (GQA), and style transfer (ST) refine their modules within CharLoRA to align with the target persona.}
    \label{fig:method}
\end{figure*}