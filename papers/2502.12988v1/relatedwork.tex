\section{Related Work}
\paragraph{Character Persona Simulation.}
Character persona simulation assigns virtual personality traits to LLMs and has received substantial attention \cite{cheng2023towards, tseng2024two, samuel2024personagym}. Two primary approaches are used: prompt-based and fine-tuning-based methods.
Prompt-based methods integrate character settings directly into prompts. For instance, \citet{shanahan2023role} introduce a role-playing mechanism through prompts, while \citet{tu2023characterchat} construct a dialogue system using the MBTI personality framework. \citet{agatsuma2024building} simulate patient personas to train nursing students in health guidance. However, prompt engineering often fails to capture intricate traits and dynamic behaviors.
Fine-tuning-based methods address these limitations by training pre-trained LLMs. \citet{shao2023character} fine-tune models using character profiles and an experience upload strategy. \citet{lu2024large} extract character attributes, background details, and dialogue features from knowledge bases to fine-tune models. 
\citet{park2024enhancing} further refine role-specific behaviors through trait extraction from novel summaries. Despite these advancements, fine-tuning often produces models that mimic surface-level style but miss deeper traits.

\paragraph{Style Transfer.}  
Style transfer for text involves altering the stylistic attributes of a source text while preserving its core meaning \cite{li2023stylized}.
\citet{reif2022recipe} introduce an Augmented Zero-Shot Learning method, which leverages LLMs to achieve versatile text-style transformations without requiring task-specific training.
\citet{pu2023chatgpt} evaluate the performance of ChatGPT in sentence style transfer by comparing ChatGPT generated texts with those created by humans.
\citet{zhang2024distilling} explore a novel approach to style transfer by integrating the Chain-of-Thought reasoning capabilities of LLMs with knowledge distillation techniques.
However, these methods predominantly focus on linguistic style without engaging with deeper ideological dimensions.

\paragraph{Parameter-Efficient Fine-Tuning.}
Parameter-Efficient Fine-Tuning (PEFT) techniques are crucial for adapting large pre-trained models efficiently, especially in the context of LLMs \cite{xu2023parameter, ding2023parameter, chen2024flexible}. Prefix Tuning optimizes learnable prefixes added to input embeddings or intermediate representations \cite{li2021prefix}, while Prompt Tuning adjusts learnable prompt embeddings to guide downstream task adaptation \cite{lester2021power}. Among PEFT methods, LoRA is widely used, introducing low-rank matrices to fine-tune models efficiently \cite{hulora}. Its variant, QLoRA, further improves efficiency through quantization \cite{dettmers2024qlora}.
Recent innovations have improved LoRAâ€™s domain-specific adaptability. \citet{zhao2024loraretriever} propose a dynamic mechanism for retrieving and combining LoRA modules in mixed task scenarios. \citet{tan2024personalized} enable efficient personalized LLMs by sharing partial PEFT parameters across users, and \citet{zhao2024merging} introduce a flexible LoRA merging strategy suited for multi-task learning without additional training. In our work, we adapt LoRA for deep persona simulation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/method1.pdf}
    \caption{In pre-training, reframed essays from Authorial Perspective Reframing (APR) train the base model. In fine-tuning, multiple-choice question answering (MCQ), generative question answering (GQA), and style transfer (ST) refine their modules within CharLoRA to align with the target persona.}
    \label{fig:method}
\end{figure*}