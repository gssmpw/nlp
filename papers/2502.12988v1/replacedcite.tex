\section{Related Work}
\paragraph{Character Persona Simulation.}
Character persona simulation assigns virtual personality traits to LLMs and has received substantial attention ____. Two primary approaches are used: prompt-based and fine-tuning-based methods.
Prompt-based methods integrate character settings directly into prompts. For instance, ____ introduce a role-playing mechanism through prompts, while ____ construct a dialogue system using the MBTI personality framework. ____ simulate patient personas to train nursing students in health guidance. However, prompt engineering often fails to capture intricate traits and dynamic behaviors.
Fine-tuning-based methods address these limitations by training pre-trained LLMs. ____ fine-tune models using character profiles and an experience upload strategy. ____ extract character attributes, background details, and dialogue features from knowledge bases to fine-tune models. 
____ further refine role-specific behaviors through trait extraction from novel summaries. Despite these advancements, fine-tuning often produces models that mimic surface-level style but miss deeper traits.

\paragraph{Style Transfer.}  
Style transfer for text involves altering the stylistic attributes of a source text while preserving its core meaning ____.
____ introduce an Augmented Zero-Shot Learning method, which leverages LLMs to achieve versatile text-style transformations without requiring task-specific training.
____ evaluate the performance of ChatGPT in sentence style transfer by comparing ChatGPT generated texts with those created by humans.
____ explore a novel approach to style transfer by integrating the Chain-of-Thought reasoning capabilities of LLMs with knowledge distillation techniques.
However, these methods predominantly focus on linguistic style without engaging with deeper ideological dimensions.

\paragraph{Parameter-Efficient Fine-Tuning.}
Parameter-Efficient Fine-Tuning (PEFT) techniques are crucial for adapting large pre-trained models efficiently, especially in the context of LLMs ____. Prefix Tuning optimizes learnable prefixes added to input embeddings or intermediate representations ____, while Prompt Tuning adjusts learnable prompt embeddings to guide downstream task adaptation ____. Among PEFT methods, LoRA is widely used, introducing low-rank matrices to fine-tune models efficiently ____. Its variant, QLoRA, further improves efficiency through quantization ____.
Recent innovations have improved LoRAâ€™s domain-specific adaptability. ____ propose a dynamic mechanism for retrieving and combining LoRA modules in mixed task scenarios. ____ enable efficient personalized LLMs by sharing partial PEFT parameters across users, and ____ introduce a flexible LoRA merging strategy suited for multi-task learning without additional training. In our work, we adapt LoRA for deep persona simulation.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/method1.pdf}
    \caption{In pre-training, reframed essays from Authorial Perspective Reframing (APR) train the base model. In fine-tuning, multiple-choice question answering (MCQ), generative question answering (GQA), and style transfer (ST) refine their modules within CharLoRA to align with the target persona.}
    \label{fig:method}
\end{figure*}