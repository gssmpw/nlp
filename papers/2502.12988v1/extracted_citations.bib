@inproceedings{agatsuma2024building,
 author = {Agatsuma, Shinjitsu and Ohashi, Reon and Tsubokura, Kazuya and Nishio, Yua and Ishikawa, Mai and Ito, Niina and Ito, Fukuka and Minami, Shiori and Takegawa, Nao and Nakamura, Riko and others},
 booktitle = {2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCISandISIS)},
 pages = {1--3},
 title = {Building a Role-Play Interactive System using LLM for Health Guidance Education},
 year = {2024}
}

@inproceedings{chen2024flexible,
  title={Flexible and Adaptable Summarization via Expertise Separation},
  author={Chen, Xiuying and Li, Mingzhe and Gao, Shen and Cheng, Xin and Zhu, Qingqing and Yan, Rui and Gao, Xin and Zhang, Xiangliang},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2018--2027},
  year={2024}
}

@article{cheng2023towards,
  title={Towards personalized review summarization by modeling historical reviews from customer and product separately},
  author={Cheng, Xin and Gao, Shen and Zhang, Yuchi and Wang, Yongliang and Chen, Xiuying and Li, Mingzhe and Zhao, Dongyan and Yan, Rui},
  journal={arXiv preprint arXiv:2301.11682},
  year={2023}
}

@article{dettmers2024qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {Proc. of NeurIPS},
 title = {Qlora: Efficient finetuning of quantized llms},
 year = {2024}
}

@article{ding2023parameter,
 author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
 journal = {Nature Machine Intelligence},
 pages = {220--235},
 title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
 year = {2023}
}

@inproceedings{hulora,
 author = {Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
 booktitle = {Proc. of ICLR},
 title = {LoRA: Low-Rank Adaptation of Large Language Models},
 year = {2021}
}

@article{lester2021power,
 author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
 journal = {Proc. of EMNLP},
 title = {The power of scale for parameter-efficient prompt tuning},
 year = {2021}
}

@article{li2021prefix,
 author = {Li, Xiang Lisa and Liang, Percy},
 journal = {arXiv preprint arXiv:2101.00190},
 title = {Prefix-tuning: Optimizing continuous prompts for generation},
 year = {2021}
}

@inproceedings{li2023stylized,
  title={Stylized dialogue generation with feature-guided knowledge augmentation},
  author={Li, Jinpeng and Zhang, Zekai and Chen, Xiuying and Zhao, Dongyan and Yan, Rui},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={7144--7157},
  year={2023}
}

@article{lu2024large,
 author = {Lu, Keming and Yu, Bowen and Zhou, Chang and Zhou, Jingren},
 journal = {Proc. of ACL},
 title = {Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment},
 year = {2024}
}

@article{park2024enhancing,
 author = {Park, Jeiyoon and Park, Chanjun and Lim, Heuiseok},
 journal = {arXiv preprint arXiv:2405.19778},
 title = {Enhancing Consistency and Role-Specific Knowledge Capturing by Rebuilding Fictional Character's Persona},
 year = {2024}
}

@article{pu2023chatgpt,
 author = {Pu, Dongqi and Demberg, Vera},
 journal = {arXiv preprint arXiv:2306.07799},
 title = {ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer},
 year = {2023}
}

@inproceedings{reif2022recipe,
 author = {Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Callison-Burch, Chris and Wei, Jason},
 booktitle = {Proc. of ACL},
 pages = {837--848},
 title = {A Recipe for Arbitrary Text Style Transfer with Large Language Models},
 year = {2022}
}

@article{samuel2024personagym,
 author = {Samuel, Vinay and Zou, Henry Peng and Zhou, Yue and Chaudhari, Shreyas and Kalyan, Ashwin and Rajpurohit, Tanmay and Deshpande, Ameet and Narasimhan, Karthik and Murahari, Vishvak},
 journal = {arXiv preprint arXiv:2407.18416},
 title = {Personagym: Evaluating persona agents and llms},
 year = {2024}
}

@article{shanahan2023role,
 author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
 journal = {Nature},
 pages = {493--498},
 title = {Role play with large language models},
 year = {2023}
}

@inproceedings{shao2023character,
 author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
 booktitle = {Proc. of EMNLP},
 pages = {13153--13187},
 title = {Character-LLM: A Trainable Agent for Role-Playing},
 year = {2023}
}

@article{tan2024personalized,
 author = {Tan, Zhaoxuan and Liu, Zheyuan and Jiang, Meng},
 journal = {Proc. of EMNLP},
 title = {Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts},
 year = {2024}
}

@article{tseng2024two,
 author = {Tseng, Yu-Min and Huang, Yu-Chao and Hsiao, Teng-Yun and Hsu, Yu-Ching and Foo, Jia-Yin and Huang, Chao-Wei and Chen, Yun-Nung},
 journal = {Proc. of EMNLP Findings},
 title = {Two tales of persona in llms: A survey of role-playing and personalization},
 year = {2024}
}

@article{tu2023characterchat,
 author = {Tu, Quan and Chen, Chuanqi and Li, Jinpeng and Li, Yanran and Shang, Shuo and Zhao, Dongyan and Wang, Ran and Yan, Rui},
 journal = {arXiv preprint arXiv:2308.10278},
 title = {Characterchat: Learning towards conversational ai with personalized social support},
 year = {2023}
}

@article{xu2023parameter,
 author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
 journal = {arXiv preprint arXiv:2312.12148},
 title = {Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
 year = {2023}
}

@inproceedings{zhang2024distilling,
 author = {Zhang, Chiyu and Cai, Honglong and Li, Yuezhang and Wu, Yuexin and Hou, Le and Abdul-Mageed, Muhammad},
 booktitle = {Proc. of NAACL},
 pages = {200--211},
 title = {Distilling Text Style Transfer With Self-Explanation From LLMs},
 year = {2024}
}

@article{zhao2024loraretriever,
 author = {Zhao, Ziyu and Gan, Leilei and Wang, Guoyin and Zhou, Wangchunshu and Yang, Hongxia and Kuang, Kun and Wu, Fei},
 journal = {Proc. of ACL Findings},
 title = {Loraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild},
 year = {2024}
}

@article{zhao2024merging,
 author = {Zhao, Ziyu and Shen, Tao and Zhu, Didi and Li, Zexi and Su, Jing and Wang, Xuwu and Kuang, Kun and Wu, Fei},
 journal = {arXiv preprint arXiv:2409.16167},
 title = {Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering},
 year = {2024}
}

