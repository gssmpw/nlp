@article{achiam2023gpt,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {arXiv preprint arXiv:2303.08774},
 title = {Gpt-4 technical report},
 year = {2023}
}

@inproceedings{agatsuma2024building,
 author = {Agatsuma, Shinjitsu and Ohashi, Reon and Tsubokura, Kazuya and Nishio, Yua and Ishikawa, Mai and Ito, Niina and Ito, Fukuka and Minami, Shiori and Takegawa, Nao and Nakamura, Riko and others},
 booktitle = {2024 Joint 13th International Conference on Soft Computing and Intelligent Systems and 25th International Symposium on Advanced Intelligent Systems (SCISandISIS)},
 pages = {1--3},
 title = {Building a Role-Play Interactive System using LLM for Health Guidance Education},
 year = {2024}
}

@article{bai2024baijia,
 author = {Bai, Ting and Kang, Jiazheng and Fan, Jiayang},
 journal = {arXiv preprint arXiv:2412.20024},
 title = {BaiJia: A Large Scale Role-Playing Agent Corpus of Chinese Historical Charcaters},
 year = {2024}
}

@article{chang2024survey,
 author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 pages = {1--45},
 title = {A survey on evaluation of large language models},
 year = {2024}
}

@article{chen2019rpm,
 author = {Chen, Xiuying and Xiao, Daorui and Gao, Shen and Liu, Guojun and Lin, Wei and Zheng, Bo and Zhao, Dongyan and Yan, Rui},
 journal = {Proc. of AAAI},
 title = {Rpm-oriented query rewriting framework for e-commerce keyword-based sponsored search},
 year = {2019}
}

@article{chen2023robust,
 author = {Chen, Xuanting and Ye, Junjie and Zu, Can and Xu, Nuo and Zheng, Rui and Peng, Minlong and Zhou, Jie and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
 journal = {arXiv preprint arXiv:2303.00293},
 title = {How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks},
 year = {2023}
}

@article{chen2024improving,
 author = {Chen, Zhipeng and Zhou, Kun and Zhao, Wayne Xin and Wan, Junchen and Zhang, Fuzheng and Zhang, Di and Wen, Ji-Rong},
 journal = {arXiv preprint arXiv:2401.06081},
 title = {Improving large language models via fine-grained reinforcement learning with minimum editing constraint},
 year = {2024}
}

@article{chen2024persona,
 author = {Chen, Jiangjie and Wang, Xintao and Xu, Rui and Yuan, Siyu and Zhang, Yikai and Shi, Wei and Xie, Jian and Li, Shuang and Yang, Ruihan and Zhu, Tinghui and others},
 journal = {arXiv preprint arXiv:2404.18231},
 title = {From persona to personalization: A survey on role-playing language agents},
 year = {2024}
}

@article{chiang2023vicuna,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
 journal = {See https://vicuna. lmsys. org (accessed 14 April 2023)},
 pages = {6},
 title = {Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
 year = {2023}
}

@inproceedings{christophe2024med42,
 author = {Christophe, Clement and Kanithi, Praveenkumar and Munjal, Prateek and Raha, Tathagata and Hayat, Nasir and Rajan, Ronnie and Al Mahrooqi, Ahmed and Gupta, Avani and Salman, Muhammad Umar and Pimentel, Marco AF and others},
 booktitle = {Proc. of AAAI},
 title = {Med42-Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches}
}

@article{dettmers2024qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {Proc. of NeurIPS},
 title = {Qlora: Efficient finetuning of quantized llms},
 year = {2024}
}

@inproceedings{diao2023mixture,
 author = {Diao, Shizhe and Xu, Tianyang and Xu, Ruijia and Wang, Jiawei and Zhang, Tong},
 booktitle = {Proc. of ACL},
 pages = {5113--5129},
 title = {Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Modelsâ€™ Memories},
 year = {2023}
}

@article{ding2023parameter,
 author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
 journal = {Nature Machine Intelligence},
 pages = {220--235},
 title = {Parameter-efficient fine-tuning of large-scale pre-trained language models},
 year = {2023}
}

@article{dubey2024llama,
 author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
 journal = {arXiv preprint arXiv:2407.21783},
 title = {The llama 3 herd of models},
 year = {2024}
}

@inproceedings{fan2022modeling,
 author = {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and Jin, Taiwei and Yang, Keping},
 booktitle = {Proceedings of the ACM Web Conference 2022},
 pages = {203--212},
 title = {Modeling user behavior with graph convolution for personalized product search},
 year = {2022}
}

@article{ge2024openagi,
 author = {Ge, Yingqiang and Hua, Wenyue and Mei, Kai and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng and others},
 journal = {Proc. of NeurIPS},
 title = {Openagi: When llm meets domain experts},
 year = {2024}
}

@article{gu2023knowledge,
 author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
 journal = {arXiv preprint arXiv:2306.08543},
 title = {Knowledge distillation of large language models},
 year = {2023}
}

@inproceedings{gu2024minillm,
 author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
 booktitle = {Proc. of ICLR},
 title = {MiniLLM: Knowledge distillation of large language models},
 year = {2024}
}

@inproceedings{gu2024minillm,
 author = {Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
 booktitle = {Proc. of ICLR},
 title = {MiniLLM: Knowledge distillation of large language models},
 year = {2024}
}

@article{han2022meet,
 author = {Han, Seungju and Kim, Beomsu and Yoo, Jin Yong and Seo, Seokjun and Kim, Sangbum and Erdenee, Enkhbayar and Chang, Buru},
 journal = {arXiv preprint arXiv:2204.10825},
 title = {Meet your favorite character: Open-domain chatbot mimicking fictional characters with only a few utterances},
 year = {2022}
}

@article{han2024parameter,
 author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
 journal = {arXiv preprint arXiv:2403.14608},
 title = {Parameter-efficient fine-tuning for large models: A comprehensive survey},
 year = {2024}
}

@inproceedings{hu2024bad,
 author = {Hu, Beizhe and Sheng, Qiang and Cao, Juan and Shi, Yuhui and Li, Yang and Wang, Danding and Qi, Peng},
 booktitle = {Proc. of AAAI},
 pages = {22105--22113},
 title = {Bad actor, good advisor: Exploring the role of large language models in fake news detection},
 year = {2024}
}

@inproceedings{hulora,
 author = {Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
 booktitle = {Proc. of ICLR},
 title = {LoRA: Low-Rank Adaptation of Large Language Models},
 year = {2021}
}

@article{jin2022deep,
 author = {Jin, Di and Jin, Zhijing and Hu, Zhiting and Vechtomova, Olga and Mihalcea, Rada},
 journal = {Computational Linguistics},
 pages = {155--205},
 title = {Deep learning for text style transfer: A survey},
 year = {2022}
}

@article{lester2021power,
 author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
 journal = {Proc. of EMNLP},
 title = {The power of scale for parameter-efficient prompt tuning},
 year = {2021}
}

@article{li2021prefix,
 author = {Li, Xiang Lisa and Liang, Percy},
 journal = {arXiv preprint arXiv:2101.00190},
 title = {Prefix-tuning: Optimizing continuous prompts for generation},
 year = {2021}
}

@article{li2023chatharuhi,
 author = {Li, Cheng and Leng, Ziang and Yan, Chenxi and Shen, Junyi and Wang, Hao and Mi, Weishi and Fei, Yaying and Feng, Xiaoyang and Yan, Song and Wang, HaoSheng and others},
 journal = {arXiv preprint arXiv:2308.09597},
 title = {Chatharuhi: Reviving anime character in reality via large language model},
 year = {2023}
}

@inproceedings{li2023distilling,
 author = {Li, Xuanlin and Fang, Yunhao and Liu, Minghua and Ling, Zhan and Tu, Zhuowen and Su, Hao},
 booktitle = {Proc. of ICCV},
 pages = {2492--2503},
 title = {Distilling large vision-language model with out-of-distribution generalizability},
 year = {2023}
}

@inproceedings{li2024multi,
 author = {Li, Mingzhe and Chen, Xiuying and Xiang, Jing and Zhang, Qishen and Ma, Changsheng and Dai, Chenchen and Chang, Jinxiong and Liu, Zhongyi and Zhang, Guannan},
 booktitle = {Proc. of WSDM},
 pages = {360--368},
 title = {Multi-Intent Attribute-Aware Text Matching in Searching},
 year = {2024}
}

@inproceedings{liang2023less,
 author = {Liang, Chen and Zuo, Simiao and Zhang, Qingru and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
 booktitle = {Proc. of ICML},
 pages = {20852--20867},
 title = {Less is more: Task-aware layer-wise distillation for language model compression},
 year = {2023}
}

@inproceedings{lin2004rouge,
 author = {Lin, Chin-Yew},
 booktitle = {Text summarization branches out},
 pages = {74--81},
 title = {Rouge: A package for automatic evaluation of summaries},
 year = {2004}
}

@article{liu2024deepseek,
 author = {Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
 journal = {arXiv preprint arXiv:2412.19437},
 title = {Deepseek-v3 technical report},
 year = {2024}
}

@article{lu2024large,
 author = {Lu, Keming and Yu, Bowen and Zhou, Chang and Zhou, Jingren},
 journal = {Proc. of ACL},
 title = {Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment},
 year = {2024}
}

@inproceedings{ma2023large,
 author = {Ma, Yubo and Cao, Yixin and Hong, Yong and Sun, Aixin},
 booktitle = {Proc. of EMNLP Findings},
 pages = {10572--10601},
 title = {Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!},
 year = {2023}
}

@inproceedings{papineni2002bleu,
 author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
 booktitle = {Proc. of ACL},
 pages = {311--318},
 title = {Bleu: a method for automatic evaluation of machine translation},
 year = {2002}
}

@article{park2024enhancing,
 author = {Park, Jeiyoon and Park, Chanjun and Lim, Heuiseok},
 journal = {arXiv preprint arXiv:2405.19778},
 title = {Enhancing Consistency and Role-Specific Knowledge Capturing by Rebuilding Fictional Character's Persona},
 year = {2024}
}

@article{pu2023chatgpt,
 author = {Pu, Dongqi and Demberg, Vera},
 journal = {arXiv preprint arXiv:2306.07799},
 title = {ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer},
 year = {2023}
}

@article{reif2021recipe,
 author = {Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Callison-Burch, Chris and Wei, Jason},
 journal = {arXiv preprint arXiv:2109.03910},
 title = {A recipe for arbitrary text style transfer with large language models},
 year = {2021}
}

@inproceedings{reif2022recipe,
 author = {Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Callison-Burch, Chris and Wei, Jason},
 booktitle = {Proc. of ACL},
 pages = {837--848},
 title = {A Recipe for Arbitrary Text Style Transfer with Large Language Models},
 year = {2022}
}

@article{samuel2024personagym,
 author = {Samuel, Vinay and Zou, Henry Peng and Zhou, Yue and Chaudhari, Shreyas and Kalyan, Ashwin and Rajpurohit, Tanmay and Deshpande, Ameet and Narasimhan, Karthik and Murahari, Vishvak},
 journal = {arXiv preprint arXiv:2407.18416},
 title = {Personagym: Evaluating persona agents and llms},
 year = {2024}
}

@article{shanahan2023role,
 author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
 journal = {Nature},
 pages = {493--498},
 title = {Role play with large language models},
 year = {2023}
}

@inproceedings{shankar2024context,
 author = {Shankar, Bhavani and Jyothi, Preethi and Bhattacharyya, Pushpak},
 booktitle = {Proc. of ACL},
 pages = {4162--4176},
 title = {In-context Mixing (ICM): Code-mixed Prompts for Multilingual LLMs},
 year = {2024}
}

@inproceedings{shao2023character,
 author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
 booktitle = {Proc. of EMNLP},
 pages = {13153--13187},
 title = {Character-LLM: A Trainable Agent for Role-Playing},
 year = {2023}
}

@article{stewart2023large,
 author = {Stewart, Michael and Hodkiewicz, Melinda and Li, Sirui},
 journal = {arXiv preprint arXiv:2309.08181},
 title = {Large language models for failure mode classification: an investigation},
 year = {2023}
}

@article{tan2024personalized,
 author = {Tan, Zhaoxuan and Liu, Zheyuan and Jiang, Meng},
 journal = {Proc. of EMNLP},
 title = {Personalized Pieces: Efficient Personalized Large Language Models through Collaborative Efforts},
 year = {2024}
}

@misc{taori2023stanford,
 author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
 title = {Stanford alpaca: An instruction-following llama model},
 year = {2023}
}

@article{timiryasov2023baby,
 author = {Timiryasov, Inar and Tastet, Jean-Loup},
 journal = {arXiv preprint arXiv:2308.02019},
 title = {Baby llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty},
 year = {2023}
}

@article{tseng2024two,
 author = {Tseng, Yu-Min and Huang, Yu-Chao and Hsiao, Teng-Yun and Hsu, Yu-Ching and Foo, Jia-Yin and Huang, Chao-Wei and Chen, Yun-Nung},
 journal = {Proc. of EMNLP Findings},
 title = {Two tales of persona in llms: A survey of role-playing and personalization},
 year = {2024}
}

@article{tu2023characterchat,
 author = {Tu, Quan and Chen, Chuanqi and Li, Jinpeng and Li, Yanran and Shang, Shuo and Zhao, Dongyan and Wang, Ran and Yan, Rui},
 journal = {arXiv preprint arXiv:2308.10278},
 title = {Characterchat: Learning towards conversational ai with personalized social support},
 year = {2023}
}

@article{tu2024charactereval,
 author = {Tu, Quan and Fan, Shilong and Tian, Zihang and Yan, Rui},
 journal = {Proc. of ACL},
 title = {Charactereval: A chinese benchmark for role-playing conversational agent evaluation},
 year = {2024}
}

@article{wang2023rolellm,
 author = {Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Yang, Jian and others},
 journal = {arXiv preprint arXiv:2310.00746},
 title = {Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models},
 year = {2023}
}

@article{wei2022emergent,
 author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
 journal = {arXiv preprint arXiv:2206.07682},
 title = {Emergent abilities of large language models},
 year = {2022}
}

@article{xi2023rise,
 author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
 journal = {arXiv preprint arXiv:2309.07864},
 title = {The rise and potential of large language model based agents: A survey},
 year = {2023}
}

@article{xu2023parameter,
 author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
 journal = {arXiv preprint arXiv:2312.12148},
 title = {Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment},
 year = {2023}
}

@article{xu2023wizardlm,
 author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
 journal = {arXiv preprint arXiv:2304.12244},
 title = {Wizardlm: Empowering large language models to follow complex instructions},
 year = {2023}
}

@article{xu2024survey,
 author = {Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
 journal = {arXiv preprint arXiv:2402.13116},
 title = {A survey on knowledge distillation of large language models},
 year = {2024}
}

@article{yang2023fingpt,
 author = {Yang, Hongyang and Liu, Xiao-Yang and Dan Wang, Christina},
 journal = {FinLLM at IJCAI},
 title = {FinGPT: Open-Source Financial Large Language Models},
 year = {2023}
}

@article{yang2024qwen2,
 author = {Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
 journal = {arXiv preprint arXiv:2407.10671},
 title = {Qwen2 technical report},
 year = {2024}
}

@article{ye2023comprehensive,
 author = {Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang and Gong, Chao and Shen, Yang and others},
 journal = {arXiv preprint arXiv:2303.10420},
 title = {A comprehensive capability analysis of gpt-3 and gpt-3.5 series models},
 year = {2023}
}

@article{yuan2023rrhf,
 author = {Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
 journal = {arXiv preprint arXiv:2304.05302},
 title = {Rrhf: Rank responses to align language models with human feedback without tears},
 year = {2023}
}
@inproceedings{chen2024flexible,
  title={Flexible and Adaptable Summarization via Expertise Separation},
  author={Chen, Xiuying and Li, Mingzhe and Gao, Shen and Cheng, Xin and Zhu, Qingqing and Yan, Rui and Gao, Xin and Zhang, Xiangliang},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2018--2027},
  year={2024}
}
@inproceedings{li2023stylized,
  title={Stylized dialogue generation with feature-guided knowledge augmentation},
  author={Li, Jinpeng and Zhang, Zekai and Chen, Xiuying and Zhao, Dongyan and Yan, Rui},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={7144--7157},
  year={2023}
}

@article{cheng2023towards,
  title={Towards personalized review summarization by modeling historical reviews from customer and product separately},
  author={Cheng, Xin and Gao, Shen and Zhang, Yuchi and Wang, Yongliang and Chen, Xiuying and Li, Mingzhe and Zhao, Dongyan and Yan, Rui},
  journal={arXiv preprint arXiv:2301.11682},
  year={2023}
}
@article{yuan2024evaluating,
 author = {Yuan, Xinfeng and Yuan, Siyu and Cui, Yuhan and Lin, Tianhe and Wang, Xintao and Xu, Rui and Chen, Jiangjie and Yang, Deqing},
 journal = {Proc. of EMNLP},
 title = {Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works},
 year = {2024}
}

@article{zakka2024almanac,
 author = {Zakka, Cyril and Shad, Rohan and Chaurasia, Akash and Dalal, Alex R and Kim, Jennifer L and Moor, Michael and Fong, Robyn and Phillips, Curran and Alexander, Kevin and Ashley, Euan and others},
 journal = {NEJM AI},
 pages = {AIoa2300068},
 title = {Almanacâ€”retrieval-augmented language models for clinical medicine},
 year = {2024}
}

@inproceedings{zhang2023plug,
 author = {Zhang, Zhengyan and Zeng, Zhiyuan and Lin, Yankai and Wang, Huadong and Ye, Deming and Xiao, Chaojun and Han, Xu and Liu, Zhiyuan and Li, Peng and Sun, Maosong and others},
 booktitle = {Proc. of ACL},
 pages = {10641--10658},
 title = {Plug-and-Play Knowledge Injection for Pre-trained Language Models},
 year = {2023}
}

@inproceedings{zhang2024distilling,
 author = {Zhang, Chiyu and Cai, Honglong and Li, Yuezhang and Wu, Yuexin and Hou, Le and Abdul-Mageed, Muhammad},
 booktitle = {Proc. of NAACL},
 pages = {200--211},
 title = {Distilling Text Style Transfer With Self-Explanation From LLMs},
 year = {2024}
}

@article{zhang2024thinking,
 author = {Zhang, Baohua and Huang, Yongyi and Cui, Wenyao and Zhang, Huaping},
 journal = {arXiv preprint arXiv:2409.13752},
 title = {Thinking Before Speaking: A Role-playing Model with Mindset},
 year = {2024}
}

@article{zhao2023survey,
 author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
 journal = {arXiv preprint arXiv:2303.18223},
 title = {A survey of large language models},
 year = {2023}
}

@article{zhao2024loraretriever,
 author = {Zhao, Ziyu and Gan, Leilei and Wang, Guoyin and Zhou, Wangchunshu and Yang, Hongxia and Kuang, Kun and Wu, Fei},
 journal = {Proc. of ACL Findings},
 title = {Loraretriever: Input-aware lora retrieval and composition for mixed tasks in the wild},
 year = {2024}
}

@article{zhao2024merging,
 author = {Zhao, Ziyu and Shen, Tao and Zhu, Didi and Li, Zexi and Su, Jing and Wang, Xuwu and Kuang, Kun and Wu, Fei},
 journal = {arXiv preprint arXiv:2409.16167},
 title = {Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering},
 year = {2024}
}

@article{zhou2023characterglm,
 author = {Zhou, Jinfeng and Chen, Zhuang and Wan, Dazhen and Wen, Bosi and Song, Yi and Yu, Jifan and Huang, Yongkang and Peng, Libiao and Yang, Jiaming and Xiao, Xiyao and others},
 journal = {arXiv preprint arXiv:2311.16832},
 title = {Characterglm: Customizing chinese conversational ai characters with large language models},
 year = {2023}
}

@inproceedings{zhou2024enhancing,
 author = {Zhou, Xiaoling and Ye, Wei and Wang, Yidong and Jiang, Chaoya and Lee, Zhemg and Xie, Rui and Zhang, Shikun},
 booktitle = {Proc. of ACL},
 pages = {2810--2828},
 title = {Enhancing In-Context Learning via Implicit Demonstration Augmentation},
 year = {2024}
}

@inproceedings{zou2022divide,
 author = {Zou, Yicheng and Liu, Hongwei and Gui, Tao and Wang, Junzhe and Zhang, Qi and Tang, Meng and Li, Haixiang and Wang, Daniell},
 booktitle = {Proc. of ACL Findings},
 pages = {3622--3632},
 title = {Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents},
 year = {2022}
}
