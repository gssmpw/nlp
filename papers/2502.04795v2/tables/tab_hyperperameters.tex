
\begin{table}[h!]
\centering
\small
\tabcolsep=2pt % 列間のスペースを縮小
\begin{tabular}{|l|l|}
\hline
\textbf{Hyperparameter}       & \textbf{Value}                  \\ \hline
Model Architecture            & GPT-2     \\ \hline
Number of Layers              & 4                               \\ \hline
Number of Attention Heads     & 4                               \\ \hline
Embedding Dimension           & 256                             \\ \hline
Dropout Rate                  & 0.1                             \\ \hline
Learning Rate (\(\eta\))      & \(5 \times 10^{-6}\)            \\ \hline
Weight Decay                  & 0.01                            \\ \hline
Batch Size                    & 512                             \\ \hline
Gradient Accumulation Steps   & 2                               \\ \hline
Total Epochs                  & 20                              \\ \hline
Maximum Sequence Length       & 32                          \\ \hline
Learning Rate Scheduler       & Cosine with Restarts         \\ \hline
Warm-up Steps                 & 10\% of Total Steps             \\ \hline
Optimizer                     & AdamW                            \\ \hline
Optimizer Parameters          & $\beta = (0.9, 0.999)$, $\epsilon = 1e−08$ \\ \hline
Tokenizer                     & Trained on CHILDES \\ \hline
Early Stopping Tolerance      & 1 Epoch                         \\ \hline
Evaluation Metric             & Perplexity                      \\ \hline
\end{tabular}
\caption{Training Configuration (Hyperparameters) for the GPT-2 Model.}
\label{table:hyperparameters}
\end{table}
