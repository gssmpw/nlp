@inproceedings{oba2023second,
  author       = {Miyu Oba and
                  Tatsuki Kuribayashi and
                  Hiroki Ouchi and
                  Taro Watanabe},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL} 2023},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  pages        = {13557--13572},
  
  title        = {{S}econd Language Acquisition of Neural Language Models},
  url          = {https://doi.org/10.18653/v1/2023.findings-acl.856},
  year         = {2023}
}

@article{ellis2000age,
  author       = {Ellis, Andrew W. and Lambon Ralph, Matthew A.},
  journal      = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  number       = {5},
  pages        = {1103--1123},
  title        = {{A}ge of acquisition effects in adult lexical processing reflect loss of plasticity in maturing systems: {Insights} from connectionist networks.},
  volume       = {26},
  year         = {2000},
  url={https://pubmed.ncbi.nlm.nih.gov/11009247/}
}

@incollection{seidenberg2006connectionist,
  author       = {Seidenberg, Mark S. and Zevin, Jason D.},
  booktitle    = {Processes of {Change} in {Brain} and {Cognitive} {Development}},
  editor       = {Munakata, Yuko and Johnson, Mark H},
  pages        = {585--612},
  shorttitle   = {Connectionist models in developmental cognitive neuroscience},
  title        = {{C}onnectionist models in developmental cognitive neuroscience: {Critical} periods and the paradox of success},
  url          = {https://academic.oup.com/book/54488/chapter/422571818},
  year         = {2006},
  publisher = {Oxford University Press}
}


@article{hartshorne2018critical,
  author       = {Hartshorne, Joshua K. and Tenenbaum, Joshua B. and Pinker, Steven},
  journal      = {Cognition},
  pages        = {263--277},
  title        = {{A} critical period for second language acquisition: {Evidence} from 2/3 million {English} speakers},
  url          = {https://www.sciencedirect.com/science/article/pii/S0010027718300994},
  volume       = {177},
  year         = {2018}
}





@article{mayberry1989looking,
  author       = {Mayberry, Rachel I. and Fischer, Susan D.},
  doi          = {10.3758/BF03202635},
  issn         = {0090-502X, 1532-5946},
  journal      = {Memory \& Cognition},
  language     = {en},
  month        = nov,
  number       = {6},
  pages        = {740--754},
  shorttitle   = {Looking through phonological shape to lexical meaning},
  title        = {{L}ooking through phonological shape to lexical meaning: {The} bottleneck of non-native sign language processing},
  url          = {http://link.springer.com/10.3758/BF03202635},
  urldate      = {2023-10-26},
  volume       = {17},
  year         = {1989}
}

@article{Constantinescu-tacl2025,
    author = {Constantinescu, Ionut and Pimentel, Tiago and Cotterell, Ryan and Warstadt, Alex},
    title = {Investigating Critical Period Effects in Language Acquisition through Neural Language Models},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {13},
    pages = {96-120},
    year = {2025},
    month = {01},
    abstract = {Humans appear to have a critical period (CP) for language acquisition: Second language (L2) acquisition becomes harder after early childhood, and ceasing exposure to a first language (L1) after this period (but not before) typically does not lead to substantial loss of L1 proficiency. It is unknown whether these CP effects result from innately determined brain maturation or as a stabilization of neural connections naturally induced by experience. In this study, we use language models (LMs) to test the extent to which these phenomena are peculiar to humans, or shared by a broader class of language learners. We vary the age of exposure by training LMs on language pairs in various experimental conditions, and find that LMs, which lack any direct analog to innate maturational stages, do not show CP effects when the age of exposure of L2 is delayed. Our results contradict the claim that CP effects are an inevitable result of statistical learning, and they are consistent with an innate mechanism for CP effects. We show that we can reverse-engineer the CP by introducing a regularizer partway through training to simulate a maturational decrease in plasticity. All in all, our results suggest that L1 learning on its own may not be enough to induce a CP, and additional engineering is necessary to make language models more cognitively plausible.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00725},
    url = {https://doi.org/10.1162/tacl\_a\_00725},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00725/2499749/tacl\_a\_00725.pdf},
}



@article{penfield1965conditioning,
  author       = {Penfield, Wilder},
  doi          = {10.1093/brain/88.4.787},
  issn         = {0006-8950},
  journal      = {Brain},
  number       = {4},
  pages        = {787--798},
  title        = {{C}onditioning the uncommitted cortex for language learning},
  url          = {https://doi.org/10.1093/brain/88.4.787},
  volume       = {88},
  year         = {1965}
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@inproceedings{xiang-etal-2021-climp,
    title = "{CL}i{MP}: A Benchmark for {C}hinese Language Model Evaluation",
    author = "Xiang, Beilei  and
      Yang, Changbing  and
      Li, Yu  and
      Warstadt, Alex  and
      Kann, Katharina",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.242/",
    doi = "10.18653/v1/2021.eacl-main.242",
    pages = "2784--2790",
    abstract = "Linguistically informed analyses of language models (LMs) contribute to the understanding and improvement of such models. Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire. CLiMP consists of sets of 1000 minimal pairs (MPs) for 16 syntactic contrasts in Chinese, covering 9 major Chinese linguistic phenomena. The MPs are semi-automatically generated, and human agreement with the labels in CLiMP is 95.8{\%}. We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT. We find that classifier{--}noun agreement and verb complement selection are the phenomena that models generally perform best at. However, models struggle the most with the ba construction, binding, and filler-gap dependencies. Overall, Chinese BERT achieves an 81.8{\%} average accuracy, while the performances of LSTMs and 5-grams are only moderately above chance level."
}

@inproceedings{someya-oseki-2023-jblimp,
    title = "{JBL}i{MP}: {J}apanese Benchmark of Linguistic Minimal Pairs",
    author = "Someya, Taiga  and
      Oseki, Yohei",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.117/",
    doi = "10.18653/v1/2023.findings-eacl.117",
    pages = "1581--1594",
    abstract = "In this paper, we introduce JBLiMP (Japanese Benchmark of Linguistic Minimal Pairs), a novel dataset for targeted syntactic evaluations of language models in Japanese. JBLiMP consists of 331 minimal pairs, which are created based on acceptability judgments extracted from journal articles in theoretical linguistics. These minimal pairs are grouped into 11 categories, each covering a different linguistic phenomenon. JBLiMP is unique in that it successfully combines two important features independently observed in existing datasets: (i) coverage of complex linguistic phenomena (cf. CoLA) and (ii) presentation of sentences as minimal pairs (cf. BLiMP). In addition, JBLiMP is the first dataset for targeted syntactic evaluations of language models in Japanese, thus allowing the comparison of syntactic knowledge of language models across different languages. We then evaluate the syntactic knowledge of several language models on JBLiMP: GPT-2, LSTM, and n-gram language models. The results demonstrated that all the architectures achieved comparable overall accuracies around 75{\%}. Error analyses by linguistic phenomenon further revealed that these language models successfully captured local dependencies like nominal structures, but not long-distance dependencies such as verbal agreement and binding."
}



@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{JMLR:v9:vandermaaten08a,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}


@inproceedings{feng-etal-2024-child,
    title = "Is Child-Directed Speech Effective Training Data for Language Models?",
    author = "Feng, Steven Y.  and
      Goodman, Noah  and
      Frank, Michael",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1231/",
    doi = "10.18653/v1/2024.emnlp-main.1231",
    pages = "22055--22071",
    abstract = "While high-performing language models are typically trained on hundreds of billions of words, human children become fluent language users with a much smaller amount of data. What are the features of the data they receive, and how do these features support language modeling objectives? To investigate this question, we train GPT-2 and RoBERTa models on 29M words of English child-directed speech and a new matched, synthetic dataset (TinyDialogues), comparing to OpenSubtitles, Wikipedia, and a heterogeneous blend of datasets from the BabyLM challenge. We evaluate the syntactic and semantic knowledge of these models using developmentally-inspired evaluations. Through pretraining experiments, we test whether the global developmental ordering or the local discourse ordering of children`s training data supports high performance relative to other datasets. The local properties of the data affect model results, but surprisingly, global properties do not. Further, child language input is not uniquely valuable for training language models. These findings support the hypothesis that, rather than proceeding from better data, the child`s learning algorithm is substantially more data-efficient than current language modeling techniques."
}


@inproceedings{diehl-martinez-etal-2024-mitigating,
    title = "Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing",
    author = "Diehl Martinez, Richard  and
      Goriely, Z{\'e}bulon  and
      Caines, Andrew  and
      Buttery, Paula  and
      Beinborn, Lisa",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.344/",
    doi = "10.18653/v1/2024.emnlp-main.344",
    pages = "5999--6011",
    abstract = "Language models strongly rely on frequency information because they maximize the likelihood of tokens during pre-training. As a consequence, language models tend to not generalize well to tokens that are seldom seen during training. Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity.Our work introduces a method for quantifying the frequency bias of a language model by assessing sentence-level perplexity with respect to token-level frequency. We then present a method for reducing the frequency bias of a language model by inducing a syntactic prior over token representations during pre-training. Our Syntactic Smoothing method adjusts the maximum likelihood objective function to distribute the learning signal to syntactically similar tokens. This approach results in better performance on infrequent English tokens and a decrease in anisotropy. We empirically show that the degree of anisotropy in a model correlates with its frequency bias."
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@book{curtiss1977genie,
  title={Genie: A Psycholinguistic Study of a Modern-day "wild Child"},
  author={Curtiss, S.},
  isbn={9780121963507},
  lccn={lc76055968},
  series={Mathematics in Science and Engineering},
  url={https://books.google.co.jp/books?id=PDRsAAAAIAAJ},
  year={1977},
  publisher={Academic Press}
}



@article{FROMKIN197481,
title = {The development of language in genie: a case of language acquisition beyond the “critical period”},
journal = {Brain and Language},
volume = {1},
number = {1},
pages = {81-107},
year = {1974},
issn = {0093-934X},
doi = {https://doi.org/10.1016/0093-934X(74)90027-3},
url = {https://www.sciencedirect.com/science/article/pii/0093934X74900273},
author = {Victoria Fromkin and Stephen Krashen and Susan Curtiss and David Rigler and Marilyn Rigler},
abstract = {The present paper reports on a case of a now-16-year-old girl who for most of her life suffered an extreme degree of social isolation and experiential deprivation. It summarizes her language acquisition which is occurring past the hypothesized “critical period” and the implications of this language development as related to hemispheric maturation and the development of lateralization. The results of a series of dichotic listening tests administered to her are included.}
}



@inproceedings{haga-etal-2024-modeling,
    title = "Modeling Overregularization in Children with Small Language Models",
    author = "Haga, Akari  and
      Sugawara, Saku  and
      Fukatsu, Akiyo  and
      Oba, Miyu  and
      Ouchi, Hiroki  and
      Watanabe, Taro  and
      Oseki, Yohei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.865/",
    doi = "10.18653/v1/2024.findings-acl.865",
    pages = "14532--14550",
    abstract = "The imitation of the children`s language acquisition process has been explored to make language models (LMs) more efficient.In particular, errors caused by children`s regularization (so-called overregularization, e.g., using wroted for the past tense of write) have been widely studied to reveal the mechanisms of language acquisition. Existing research has analyzed regularization in language acquisition only by modeling word inflection directly, which is unnatural in light of human language acquisition. In this paper, we hypothesize that language models that imitate the errors children make during language acquisition have a learning process more similar to humans. To verify this hypothesis, we analyzed the learning curve and error preferences of verb inflections in small-scale LMs using acceptability judgments. We analyze the differences in results by model architecture, data, and tokenization. Our model shows child-like U-shaped learning curves clearly for certain verbs, but the preferences for types of overgeneralization did not fully match the observations in children."
}



@article{Macwhinney2000,
author = {Macwhinney, Brian},
year = {2000},
month = {01},
pages = {},
title = {The CHILDES project: tools for analyzing talk},
volume = {8},
journal = {Child Language Teaching and Therapy},
doi = {10.1177/026565909200800211}
}


@article{Patkowski1980,
author = {Patkowski, Mark S.},
title = {THE SENSITIVE PERIOD FOR THE ACQUISITION OF SYNTAX IN A SECOND LANGUAGE},
journal = {Language Learning},
volume = {30},
number = {2},
pages = {449-468},
doi = {https://doi.org/10.1111/j.1467-1770.1980.tb00328.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-1770.1980.tb00328.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-1770.1980.tb00328.x},
abstract = {This research tested the hypothesis that learners whose exposure to a second language begins before the age of 15 years achieve higher syntactic proficiency in the target language than adult learners. Sixty-seven immigrants who had come to the United States at various ages and who had resided in this country for various periods of time were tested for syntactic proficiency in English and were also administered a questionnaire to gather information concerning practice and instructional variables. Age at arrival was found to be a strong predictor of syntactic proficiency while other independent variables had very little effect. The results were interpreted as providing support for the hypothesis of an age related limitation on the ability to acquire full command of syntax in a second language.},
year = {1980}
}

@article{Tahta-etal-1981,
author = {Sonia Tahta and Margaret Wood and Kate Loewenthal},
title ={Foreign Accents: Factors Relating to Transfer of Accent from the First Language to a Second Language},

journal = {Language and Speech},
volume = {24},
number = {3},
pages = {265-272},
year = {1981},
doi = {10.1177/002383098102400306},
URL = {https://doi.org/10.1177/002383098102400306},
eprint = {https://doi.org/10.1177/002383098102400306},
abstract = { This study examined predictors of transfer of accent from the first language (L1) to a second language (L2), in a group of people whose acquisition of English as an L2 had begun at ages ranging from 6 to 15+. The effect of age of L2 acquisition is very marked. If L2 acquisition had begun by 6, there is no transfer of accent. If L2 acquisition began after 12—13, there is invariably accent transfer, usually very marked. When accent transfer occurs between 7 and 11, it is usually very slight. These findings agree quite well with those of other studies, but there are discrepancies and these are indicated and discussed. Between 7 and 11, accent transfer may be affected by factors other than biological maturation. In our study, the only such factor to emerge strongly was whether L2 was used in the home, suggesting a shift of identification from the L1 to the L2 culture. }
}



@article{JOHNSON198960,
title = {Critical period effects in second language learning: The influence of maturational state on the acquisition of English as a second language},
journal = {Cognitive Psychology},
volume = {21},
number = {1},
pages = {60-99},
year = {1989},
issn = {0010-0285},
doi = {https://doi.org/10.1016/0010-0285(89)90003-0},
url = {https://www.sciencedirect.com/science/article/pii/0010028589900030},
author = {Jacqueline S Johnson and Elissa L Newport},
abstract = {Lenneberg (1967) hypothesized that language could be acquired only within a critical period, extending from early infancy until puberty. In its basic form, the critical period hypothesis need only have consequences for first language acquisition. Nevertheless, it is essential to our understanding of the nature of the hypothesized critical period to determine whether or not it extends as well to second language acquisition. If so, it should be the case that young children are better second language learners than adults and should consequently reach higher levels of final proficiency in the second language. This prediction was tested by comparing the English proficiency attained by 46 native Korean or Chinese speakers who had arrived in the United States between the ages of 3 and 39, and who had lived in the United States between 3 and 26 years by the time of testing. These subjects were tested on a wide variety of structures of English grammar, using a grammatically judgment task. Both correlational and t-test analyses demonstrated a clear and strong advantage for earlier arrivals over the later arrivals. Test performance was linearly related to age of arrival up to puberty; after puberty, performance was low but highly variable and unrelated to age of arrival. This age effect was shown not to be an inadvertent result of differences in amount of experience with English, motivation, self-consciousness, or American identification. The effect also appeared on every grammatical structure tested, although the structures varied markedly in the degree to which they were well mastered by later learners. The results support the conclusion that a critical period for language acquisition extends its effects to second language acquisition.}
}

@article{Kirkpatrick-etal-2017,
author = {James Kirkpatrick  and Razvan Pascanu  and Neil Rabinowitz  and Joel Veness  and Guillaume Desjardins  and Andrei A. Rusu  and Kieran Milan  and John Quan  and Tiago Ramalho  and Agnieszka Grabska-Barwinska  and Demis Hassabis  and Claudia Clopath  and Dharshan Kumaran  and Raia Hadsell },
title = {Overcoming catastrophic forgetting in neural networks},
journal = {Proceedings of the National Academy of Sciences},
volume = {114},
number = {13},
pages = {3521-3526},
year = {2017},
doi = {10.1073/pnas.1611835114},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1611835114},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114},
abstract = {Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.}}

@InProceedings{Vaswani:17:NIPS,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  title = {{Attention Is All You Need}},
  booktitle = {Advances in Neural Information Processing Systems 31 (NIPS 2017)},
  pages = {5998--6008},
  year = {2017},
}


@article{warstadt-etal-2020-blimp-benchmark,
    title = "{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish",
    author = "Warstadt, Alex  and
      Parrish, Alicia  and
      Liu, Haokun  and
      Mohananey, Anhad  and
      Peng, Wei  and
      Wang, Sheng-Fu  and
      Bowman, Samuel R.",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.25/",
    doi = "10.1162/tacl_a_00321",
    pages = "377--392",
    abstract = "We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands."
}


@book{Clark-etal-2011,
	author = {Clark, Alexander  and Lappin, Shalom},
	title = {Linguistic Nativism and the Poverty of the Stimulus},
	publisher = {Wiley-Blackwell},
	year = {2011},
}


@article{Wilcox-etal-2024,
    author = {Wilcox, Ethan Gotlieb and Futrell, Richard and Levy, Roger},
    title = {Using Computational Models to Test Syntactic Learnability},
    journal = {Linguistic Inquiry},
    volume = {55},
    number = {4},
    pages = {805-848},
    year = {2024},
    month = {10},
    abstract = {We studied the learnability of English filler-gap dependencies and the “island” constraints on them by assessing the generalizations made by autoregressive (incremental) language models that use deep learning to predict the next word given preceding context. Using factorial tests inspired by experimental psycholinguistics, we found that models acquire not only the basic contingency between fillers and gaps, but also the unboundedness and hierarchical constraints implicated in the dependency. We evaluated a model’s acquisition of island constraints by demonstrating that its expectation for a filler-gap contingency is attenuated within an island environment. Our results provide empirical evidence against the argument from the poverty of the stimulus for this particular structure.},
    issn = {0024-3892},
    doi = {10.1162/ling_a_00491},
    url = {https://doi.org/10.1162/ling\_a\_00491},
    eprint = {https://direct.mit.edu/ling/article-pdf/55/4/805/2473564/ling\_a\_00491.pdf},
}





@article{McCoy-etal-2020,
    author = {McCoy, R. Thomas and Frank, Robert and Linzen, Tal},
    title = {Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {125-140},
    year = {2020},
    month = {01},
    abstract = {Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00304},
    url = {https://doi.org/10.1162/tacl\_a\_00304},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00304/1923427/tacl\_a\_00304.pdf},
}


@misc{constantinescu2024investigatingcriticalperiodeffects,
      title={Investigating Critical Period Effects in Language Acquisition through Neural Language Models}, 
      author={Ionut Constantinescu and Tiago Pimentel and Ryan Cotterell and Alex Warstadt},
      year={2024},
      eprint={2407.19325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.19325}, 
}

@article{Sowell2002DevelopmentOC,
  title={Development of cortical and subcortical brain structures in childhood and adolescence: a structural MRI study},
  author={Elizabeth R. Sowell and Doris A. Trauner and Anthony Collins Gamst and Terry L. Jernigan},
  journal={Developmental Medicine \& Child Neurology},
  year={2002},
  volume={44},
  url={https://api.semanticscholar.org/CorpusID:19241957}
}


@article{Luna-etal-2004,
author = {Luna, Beatriz and Garver, Krista E. and Urban, Trinity A. and Lazar, Nicole A. and Sweeney, John A.},
title = {Maturation of Cognitive Processes From Late Childhood to Adulthood},
journal = {Child Development},
volume = {75},
number = {5},
pages = {1357-1372},
doi = {https://doi.org/10.1111/j.1467-8624.2004.00745.x},
url = {https://srcd.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8624.2004.00745.x},
eprint = {https://srcd.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8624.2004.00745.x},
abstract = {To characterize cognitive maturation through adolescence, processing speed, voluntary response suppression, and spatial working memory were measured in 8- to 30-year-old (N=245) healthy participants using oculomotor tasks. Development progressed with a steep initial improvement in performance followed by stabilization in adolescence. Adult-level mature performance began at approximately 15, 14, and 19 years of age for processing speed, response inhibition, and working memory, respectively. Although processes developed independently, processing speed influenced the development of working memory whereas the development of response suppression and working memory were interdependent. These results indicate that processing speed, voluntary response suppression, and working memory mature through late childhood and into adolescence. How brain maturation specific to adolescence may support cognitive maturation is discussed.},
year = {2004}
}


@article{López-Vallejo-et-al-2024,
    doi = {10.1371/journal.pone.0299394},
    author = {López-Vallejo, Sofía AND Burneo-Garcés, Carlos AND Pérez-García, Miguel},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Development of working memory and inhibitory control in early childhood: Cross-sectional analysis by age intervals and gender in Ecuadorian preschoolers},
    year = {2024},
    month = {05},
    volume = {19},
    url = {https://doi.org/10.1371/journal.pone.0299394},
    pages = {1-19},
    abstract = {Working memory (WM) and inhibitory control (IC) play a crucial role in learning during early childhood. The literature suggests a non-linear developmental trajectory of executive functions (EFs) with varied results according to gender, usually attributed to environmental factors. However, there is insufficient and inconclusive data on whether this pattern is reproduced in the Latin American preschool population since most studies have been conducted in English-speaking, European, and Asian environments. Thus, objectively comparing children’s executive performance across diverse international geographical contexts becomes challenging. This study aimed to conduct a cross-sectional analysis of the performance in WM and IC of 982 Ecuadorian preschoolers aged between 42 and 65 months (M = 53.71; SD = 5.714) and belonging to medium-high, medium, and low-medium socioeconomic strata. The participants consisted of 496 boys (M = 53.77; SD = 5.598) and 486 girls (M = 53.65; SD = 5.834), representing nine cities in Ecuador. To assess the effect of age and gender on performance in these two domains, the sample was divided into four 6-month age intervals. Two tests were administered to the participants, and a survey was conducted with 799 of their usual caregivers. Viewing the cross-sectional mean scores of the WM and IC tests as a temporal continuum reveals an upward trend in each age interval studied. Girls outperformed boys on the IC test, showing statistically significant differences in the earliest age interval. The gender differences in executive performance reported in the literature emphasize the need to explore the modulating effect of environmental variables on early childhood development. This information could offer valuable insights for adapting and optimizing cognitive and didactic strategies in early childhood tailored to the characteristics and needs of the preschool population.},
    number = {5},

}

@article{Gathercole-etal-2004,
title = "The structure of working memory from 4 to 15 years of age",
abstract = "The structure of working memory and its development across the childhood years were investigated in children 4-15 years of age. The children were given multiple assessments of each component of the A. D. Baddeley and G. Hitch (1974) working memory model. Broadly similar linear functions characterized performance on all measures as a function of age. From 6 years onward, a model consisting of 3 distinct but correlated factors corresponding to the working memory model provided a good fit to the data. The results indicate that the basic modular structure of working memory is present from 6 years of age and possibly earlier, with each component undergoing sizable expansion in functional capacity throughout the early and middle school years to adolescence.",
keywords = "Adolescent Association Learning Child Child Development Child, Preschool Female Humans Male Memory, Short-Term Mental Recall Models, Psychological Multivariate Analysis Neuropsychological Tests Orientation Pattern Recognition, Visual Phonetics Psychometrics Serial Learning Verbal Learning",
author = "Gathercole, {S. E.} and Pickering, {S. J.} and B. Ambridge and H. Wearing",
note = "Gathercole, Susan E Pickering, Susan J Ambridge, Benjamin Wearing, Hannah 2004/2/26",
year = "2004",
doi = "10.1037/0012-1649.40.2.177",
language = "English",
volume = "40",
pages = "177--190",
journal = "Developmental psychology",
issn = "0012-1649",
publisher = "American Psychological Association",
number = "2",
}

@article{Cowan1999TheRO,
  title={The role of attention in the development of short-term memory: age differences in the verbal span of apprehension.},
  author={Nelson Cowan and Lara Nugent and Emily M. Elliott and Igor Ponomarev and John Scott Saults},
  journal={Child development},
  year={1999},
  volume={70 5},
  pages={
          1082-97
        },
  url={https://api.semanticscholar.org/CorpusID:18553089}
}


@article{krashen1973,
author = {Krashen, Stephen D.},
title = {LATERALIZATION, LANGUAGE LEARNING, AND THE CRITICAL PERIOD: SOME NEW EVIDENCE},
journal = {Language Learning},
volume = {23},
number = {1},
pages = {63-74},
doi = {https://doi.org/10.1111/j.1467-1770.1973.tb00097.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-1770.1973.tb00097.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-1770.1973.tb00097.x},
abstract = {New evidence is presented that modifies Lenneberg's (1967) proposed critical period of language acquisition. The development of lateralization is complete much earlier than puberty and is thus not a barrier to accent free second language learning by adults. Rather, the development of lateralization may correspond to normal first language acquisition. Also, the case of Genie, a girl who endured 11 years of enforced isolation, shows that some first language acquisition is possible after the critical period, although mechanisms outside of the left hemisphere may be involved. Genie's slow but steady progress also implies that adult achievement in learning second languages should not be pre-judged.},
year = {1973}
}



@inbook{aochildes2021,
title = "Using lexical context to discover the noun category: Younger children have it easier",
abstract = "Prior work has demonstrated that distributional dependencies between word or morpheme-like entities in artificial and naturalistic language can detect clusters of words which broadly conform to the categories of the adult language (Brent & Siskind, 2001; Mintz, 2002; Redington & Chater, 1998). In this work, we examine the hypothesis that the distributional statistics useful for the discovery of the noun category are more useful in speech to younger children compared to older children (approximately 1–3 vs 3–6 years of age). First, using a novel method for quantifying the extent that nouns occur in mutually shared contexts, we demonstrate an advantage for speech to younger compared to older children. Second, we develop a theoretical framework for understanding why caregiver speech might be scaffolded in this way, and test its predictions against information theoretic patterns computed on child-directed speech. Our account, based on entropy maximization, and anchoring originally proposed by Cameron-Faulkner, Lieven, and Tomasello (2003), clarifies issues in incremental learning from nonstationary input—the problem faced by language learners—and paves the way toward integrating the scaffolded organization of children's early language environment into computational models of acquisition.",
keywords = "Child-directed Speech, Corpus analysis, Distributional learning, Language acquisition, Lexical context, Noun category",
author = "Huebner, {Philip A.} and Willits, {Jon A.}",
year = "2021",
month = jan,
doi = "10.1016/bs.plm.2021.08.002",
language = "English (US)",
isbn = "9780323901352",
series = "Psychology of Learning and Motivation - Advances in Research and Theory",
publisher = "Academic Press Inc.",
pages = "279--331",
booktitle = "The Context of Cognition",
}



@article{radford2019language,
  added-at = {2024-11-15T12:44:17.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/233e4b003b64b1060334660fbf6db1f3f/albinzehe},
  interhash = {b926ece39c03cdf5499f6540cf63babd},
  intrahash = {33e4b003b64b1060334660fbf6db1f3f},
  journal = {OpenAI},
  keywords = {gpt gpt2 languagemodelling transferlearning transformer},
  note = {Accessed: 2024-11-15},
  timestamp = {2024-11-15T12:44:17.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year = 2019
}



@inproceedings{huebner-etal-2021-babyberta,
    title = "{B}aby{BERT}a: Learning More Grammar With Small-Scale Child-Directed Language",
    author = "Huebner, Philip A.  and
      Sulem, Elior  and
      Cynthia, Fisher  and
      Roth, Dan",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning",
    month = nov,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.conll-1.49/",
    doi = "10.18653/v1/2021.conll-1.49",
    pages = "624--646",
    abstract = "Transformer-based language models have taken the NLP world by storm. However, their potential for addressing important questions in language acquisition research has been largely ignored. In this work, we examined the grammatical knowledge of RoBERTa (Liu et al., 2019) when trained on a 5M word corpus of language acquisition data to simulate the input available to children between the ages 1 and 6. Using the behavioral probing paradigm, we found that a smaller version of RoBERTa-base that never predicts unmasked tokens, which we term BabyBERTa, acquires grammatical knowledge comparable to that of pre-trained RoBERTa-base - and does so with approximately 15X fewer parameters and 6,000X fewer words. We discuss implications for building more efficient models and the learnability of grammar from input available to children. Lastly, to support research on this front, we release our novel grammar test suite that is compatible with the small vocabulary of child-directed input."
}


@book{lenneberg1967biological,
  title={Biological Foundations of Language},
  author={Lenneberg, E.H.},
  isbn={9780471526261},
  lccn={66028746},
  url={https://books.google.co.jp/books?id=7UZiAAAAMAAJ},
  year={1967},
  publisher={Wiley}
}



@article{Seidenberg2006,
author = {Seidenberg, Mark and Plaut, David},
year = {2006},
month = {01},
pages = {},
title = {2 Progress in understanding word reading: Data fitting versus theory building},
journal = {From Inkmarks to Ideas: Current Issues in Lexical Processing}
}


@book{elman1996rethinking,
  author    = {Jeffrey L. Elman and Elizabeth A. Bates and Mark H. Johnson and Annette Karmiloff-Smith and Domenico Parisi and Kim Plunkett},
  title     = {Rethinking Innateness: A Connectionist Perspective on Development},
  year      = {1996},
  publisher = {MIT Press},
}



@book{pinker1994language,
  author    = {Steven Pinker},
  title     = {The Language Instinct: How the Mind Creates Language},
  year      = {1994},
  publisher = {William Morrow and Company},
}




@book{chomsky1965,
  abstract = {Standard-TG},
  added-at = {2007-02-16T16:53:53.000+0100},
  address = {Cambridge},
  author = {Chomsky, Noam},
  biburl = {https://www.bibsonomy.org/bibtex/2634ef0838fe6ad268105e0be37a3be8c/vittorio.loreto},
  citeulike-article-id = {867124},
  interhash = {10dff7148609d28159a0602fcd6c7cbb},
  intrahash = {634ef0838fe6ad268105e0be37a3be8c},
  keywords = {RMP_CFL generative-grammar linguistics syntax chomsky 1965},
  priority = {1},
  publisher = {The MIT Press},
  timestamp = {2007-02-16T16:53:53.000+0100},
  title = {Aspects of the Theory of Syntax},
  url = {http://www.amazon.com/Aspects-Theory-Syntax-Noam-Chomsky/dp/0262530074},
  year = 1965
}


@inproceedings{
press2022train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@misc{clark2024linearrecencybiastraining,
      title={Linear Recency Bias During Training Improves Transformers' Fit to Reading Times}, 
      author={Christian Clark and Byung-Doh Oh and William Schuler},
      year={2024},
      eprint={2409.11250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.11250}, 
}

@inproceedings{clark-etal-2025-linear,
    title = "Linear Recency Bias During Training Improves Transformers' Fit to Reading Times",
    author = "Clark, Christian  and
      Oh, Byung-Doh  and
      Schuler, William",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.517/",
    pages = "7735--7747",
    abstract = "Recent psycholinguistic research has compared human reading times to surprisal estimates from language models to study the factors shaping human sentence processing difficulty. Previous studies have shown a strong fit between surprisal values from Transformers and reading times. However, standard Transformers work with a lossless representation of the entire previous linguistic context, unlike models of human language processing that include memory decay. To bridge this gap, this paper evaluates a modification of the Transformer model that uses ALiBi (Press et al., 2022), a recency bias added to attention scores. Surprisal estimates from a Transformer that includes ALiBi during training and inference show an improved fit to human reading times compared to a standard Transformer baseline. A subsequent analysis of attention heads suggests that ALiBi`s mixture of slopes{---}which determine the rate of memory decay in each attention head{---}may play a role in the improvement by helping models with ALiBi to track different kinds of linguistic dependencies."
}


@inproceedings{warstadt-etal-2023-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

@article{NEWPORT199011,
title = {Maturational constraints on language learning},
journal = {Cognitive Science},
volume = {14},
number = {1},
year = {1990},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(90)90024-Q},
url = {https://www.sciencedirect.com/science/article/pii/036402139090024Q},
author = {Elissa L. Newport},
abstract = {This paper suggests that there are constraints on learning required to explain the acquisition of language, in particular, mului ultonol constraints. First, empirical evidence for this daim is reviewed. The evidence from several studies of both first and second languoge acquisition suggests that normal language learning occurs only when exposure to the languoge begins early in life. With exposure beginning later in life, asymptotic performance in the language declines: the effects over oge of first exposure are approximately linear through childhood, with a flattening of the function in adulthood. These outcomes argue that some type of constraints ensuring successful languoge learning exist early in life, and weaken with increasing maturation. Second, two hypotheses are considered as to the nature of these maturational changes. One hypothesis is that constraints on learning particular to languoge acquisition undergo maturational decay. A second hypothesis, which is considered in more detail, suggests that language learning abilities decline because of the expansion of nonlinguisftc cognitive abilities.}
}