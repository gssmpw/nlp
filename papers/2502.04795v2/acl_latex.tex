% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

%%%%%%%%%%% My Packages %%%%%%%%%%%%%%%%%%%


\usepackage{graphicx,xcolor}  % グラフィックス関連
\usepackage{url}

\usepackage{multirow}
\usepackage{comment}
\usepackage{booktabs}
\usepackage[normalem]{ulem}
\usepackage[xcolor,framemethod=tikz]{mdframed}
\usepackage{collcell}
\usepackage{hhline}
\usepackage{colortbl}
\usepackage{adjustbox}
\usepackage{lingmacros}
\usepackage{subcaption}
\usepackage{mathtools}

\usepackage{amsmath} 
\usepackage{amssymb} 

\usepackage{tikz}
\usepackage{xspace}


% For Japanese characters
\usepackage[whole]{bxcjkjatype}


%%%%%%%%%%% My Macro %%%%%%%%%%%%%%%%%%%




% % Global color model
\def\colorModel{hsb}

% Define ColCell for cyan
\newcommand\ColCell[1]{
  \color{white} 
  \pgfmathsetmacro\compA{180/360}  % Cyan hue
  \pgfmathsetmacro\compB{1}       
  \pgfmathsetmacro\compC{max(0, min(1, (#1-10)/90))}  % Adjusted brightness scale
  \edef\x{\noexpand\centering\noexpand\cellcolor[\colorModel]{\compA,\compB,\compC}}\x #1
}

% Define ColCell for orange
\newcommand\ColCellOrange[1]{
  \color{white} 
  \pgfmathsetmacro\compA{30/360}  % Orange hue
  \pgfmathsetmacro\compB{1}       
  \pgfmathsetmacro\compC{max(0, min(1, (#1-10)/90))}  % Adjusted brightness scale
  \edef\x{\noexpand\centering\noexpand\cellcolor[\colorModel]{\compA,\compB,\compC}}\x #1
}

% Define E column type
\newcolumntype{E}{>{\collectcell\ColCell}m{3.5ex}<{\endcollectcell}}

% Define O column type for orange
\newcolumntype{O}{>{\collectcell\ColCellOrange}m{3.5ex}<{\endcollectcell}}



\newcommand\items{14}   % Number of classes
\newcommand\rotation{30} % Amount to rotate top row text
% \arrayrulecolor{white} % Table line colors

\definecolor{backgray}{rgb}{0.9,0.9,0.9}





%% linguistically ungrammatical/unwellformed 
\newcommand{\un}{*}
\newcommand{\exc}{!}
%% linguistically acceptable 
%% (indented by width of ungrammatical asterisk)
\newlength{\astspace}
\newlength{\excspace}
\settowidth{\astspace}{\mbox{*}}
\settowidth{\excspace}{\mbox{!}}
\newcommand{\ac}{\hspace*{\astspace}}
\newcommand{\acexc}{\hspace*{\excspace}}

\newcommand{\nextline}[1]{\\\hspace*{\astspace}#1}

\newcommand{\lone}{\ensuremath{\mathrm{L}_1}\xspace}
\newcommand{\ltwo}{\ensuremath{\mathrm{L}_2}\xspace}



% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Developmentally-plausible Working Memory Shapes \\ a Critical Period for Language Acquisition}




\author{
 \textbf{Masato Mita\textsuperscript{\scriptsize $\spadesuit$$\diamondsuit$}},
 \textbf{Ryo Yoshida\textsuperscript{\scriptsize $\spadesuit$}},
 \textbf{Yohei Oseki\textsuperscript{\scriptsize $\spadesuit$}}
\\[0.1cm]
 \textsuperscript{\scriptsize $\spadesuit$}The University of Tokyo,
 \textsuperscript{\scriptsize $\diamondsuit$}CyberAgent
\\
 \texttt{mita\_masato@cyberagent.co.jp} \\
 \texttt{\{yoshiryo0617, oseki\}@g.ecc.u-tokyo.ac.jp}
}





\begin{document}
\maketitle
\begin{abstract}
Large language models possess general linguistic abilities but acquire language less efficiently than humans.
This study proposes a method for integrating the developmental characteristics of working memory during the critical period, a stage when human language acquisition is particularly efficient, into the training process of language models.
The proposed method introduces a mechanism that initially constrains~\textit{working memory} during the early stages of training and gradually relaxes this constraint in an exponential manner as learning progresses.
Targeted syntactic evaluation shows that the proposed method outperforms conventional methods without memory constraints or with static memory constraints.
These findings not only provide new directions for designing data-efficient language models but also offer indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the critical period in language acquisition.
\end{abstract}

\section{Introduction}
Large language models (LLMs) exhibit general linguistic abilities comparable to those of humans; however, their efficiency in language acquisition remains far inferior. 
It has been noted that LLMs require data quantities that are three to four orders of magnitude larger than those needed for humans to achieve comparable performance across many evaluation metrics~\cite{warstadt-etal-2023-findings}.
This disparity in data efficiency reflects the current reliance of LLMs on scaling and suggests not only a significant potential for improving learning efficiency but also the possibility of drawing \textit{insights} from human language processing and acquisition.


An important theoretical framework for understanding the efficiency of human language acquisition is the \textbf{Critical Period Hypothesis (CPH)}~\cite{lenneberg1967biological}. 
The CPH posits that there is a specific period during which language can be acquired efficiently, and that this ability diminishes thereafter. 
Various studies, including cases of limited first language (\lone) exposure during childhood and age-related effects on second language (\ltwo) acquisition, support the existence of a critical period (CP)~\cite{FROMKIN197481,curtiss1977genie,JOHNSON198960}. 
However, the reasons why children acquire language more efficiently than adults remain partially unresolved. 
One compelling explanation for the CP in \lone acquisition is the \textbf{Less-is-More Hypothesis}~\cite{NEWPORT199011}, which argues that children's cognitive limitations (e.g., working memory capacity and attentional scope) are advantageous for language learning. 
According to this hypothesis, children's limited processing capacities enable them to efficiently extract fundamental patterns and structures (e.g., grammatical rules) from linguistic input, whereas adults, with their greater cognitive capacities, are more likely to be distracted by complex information, thereby hindering rule acquisition.


\input{figures/fig_trajectory_wm}


Inspired by the ``Less-is-More'' hypothesis, we use language models (LMs) to study the CP for language acquisition, focusing on \lone acquisition and investigating whether integrating human cognitive developmental characteristics, particularly the developmental properties of \textit{working memory} (Figure~\ref{fig:trajectory}), into LMs can facilitate efficient language acquisition. 
Specifically, we propose a method for incorporating the exponential increase in working memory capacity that corresponds to the CP into LMs and analyze its impact on learning efficiency. 
Using a GPT-2 model~\cite{radford2019language} trained on a Child-Directed Speech (CDS) dataset~\cite{aochildes2021}, we conduct evaluation experiments with Zorro~\cite{huebner-etal-2021-babyberta}, a targeted syntactic evaluation benchmark specialized for CDS. 
The results demonstrate that a cognitively plausible model, which initially restricts working memory and gradually relaxes this constraint exponentially as training progresses, outperforms models without memory constraints or with static memory constraints.
These findings provide new insights into designing data-efficient LMs, contributing to the field of \textbf{natural language processing}, while also offering indirect evidence supporting the role of the developmental characteristics of working memory as the underlying mechanism of the CPH in human language acquisition, contributing to the field of \textbf{cognitive science}.

\section{Related Work}
\subsection{Critical Period for Language Acquisition}
\label{subsec:cp}

The CPH posits that language acquisition is most efficient within a specific developmental window, after which it declines. 
CP effects are observed in both \lone and \ltwo acquisition, suggesting a shared underlying mechanism. 


\paragraph{Critical Period for \lone Acquisition}
Research in neurolinguistics and cognitive science suggests that there is a biologically determined CP for acquiring an \lone, beyond which full native-like proficiency is unattainable if exposure to language is delayed. 
Studies on late \lone learners, such as deaf individuals who acquire sign language after early childhood, indicate severe deficits in grammatical proficiency compared to those exposed to language from birth~\cite{mayberry1989looking, NEWPORT199011}. 
These findings suggest that neural plasticity, essential for \lone acquisition, diminishes with age, limiting the ability to develop full linguistic competence.
From a theoretical perspective, the existence of the CP for \lone acquisition is often attributed to biological constraints. 
Nativist theories propose that \lone acquisition relies on an innate language faculty that operates most effectively during the CP~\cite{penfield1965conditioning,chomsky1965,pinker1994language}. 
On the other hand, empiricist perspectives argue that the decline in \lone learning ability may result from environmental factors, such as a reduced need for language learning mechanisms once fundamental linguistic structures have been internalized~\cite{elman1996rethinking,seidenberg2006connectionist}. 
Despite extensive research, the precise boundary and mechanisms of the CP for \lone remain a subject of debate.




\paragraph{Critical Period for \ltwo Acquisition}
CP effects are also observed in \ltwo acquisition, where late learners struggle with pronunciation, morphology, and syntax~\cite{JOHNSON198960,hartshorne2018critical}. 
While biological constraints play a role, entrenchment—where prior exposure to \lone limits flexibility in learning new linguistic structures—is also a factor~\cite{ellis2000age, seidenberg2006connectionist}. 
Although the CP for \ltwo acquisition is an important topic, this study focuses on the CP for \lone acquisition, since our goal is to design data-efficient LMs by exploring the mechanisms of CP in \lone acquisition.






\subsection{The Role of Language Models in Acquisition Theories}


In recent years, computational models have played a crucial role in elucidating the mechanisms of language acquisition. 
These models enable controlled investigations of learning mechanisms and environments, which are difficult to achieve with human participants, and they are used to test theoretical claims such as the ``poverty of the stimulus''~\cite{Clark-etal-2011}. 
For instance, \citet{McCoy-etal-2020}, \citet{Wilcox-etal-2024}, and~\citet{warstadt-etal-2023-findings} have employed LMs to directly test hypotheses about language acquisition, demonstrating that such models can provide proof-of-concept evidence for \textit{learnability}. 
These studies have attracted attention as efforts to deepen theoretical discussions on language acquisition through computational modeling, including research on the CP.


\citet{Constantinescu-tacl2025} investigated CP phenomena in \ltwo acquisition and \lone attrition,\footnote{The phenomenon in which earlier cessation of \lone exposure increases the likelihood of \lone forgetting.} assuming a shared underlying mechanism for CP effects across \lone and \ltwo. 
They simulated \ltwo exposure at varying ages to examine how LMs differ from human learners, finding that LMs do not naturally exhibit CP effects. 
To artificially induce such effects, they employed Elastic Weight Consolidation~\cite{Kirkpatrick-etal-2017}, a regularization method for mitigating catastrophic forgetting, thereby mimicking a maturational decline in plasticity. 
Their findings suggest that CP effects are not an inevitable outcome of statistical learning but may instead involve innate mechanisms.


While this study shares the broader objective of enhancing the cognitive plausibility of LMs as models of human language acquisition, it differs from \citet{Constantinescu-tacl2025} in both \textit{focus} and \textit{methodology}. 
Rather than modeling CP effects through dataset manipulation or post-CP plasticity constraints, this study explicitly addresses the \textbf{developmental processes unfolding during the CP itself}. 
Specifically, we integrate a mechanism to simulate the progressive growth of working memory capacity throughout the CP, a factor considered crucial for \lone acquisition but previously unmodeled in LM-based research. 
By incorporating developmental constraints, this study aims to provide a more fine-grained computational model of early \lone acquisition and its cognitive underpinnings, advancing the developmental plausibility of LMs.




\section{Language Model with Developmentally-plausible Working Memory}
\label{sec:proposed_method}


\subsection{Modeling Developmental Trajectory of Human Working Memory}
\label{subsec:wm_trajectory}
Human working memory undergoes substantial developmental changes, progressing through three distinct stages: early childhood to early school age (2–7 years), middle childhood to early adolescence (8–14 years), and post-adolescence (15 years and older). 
During early childhood, both information retention capacity and processing ability improve rapidly, reflecting a significant expansion of cognitive resources~\cite{Cowan1999TheRO, Gathercole-etal-2004}. 
This rapid growth begins to decelerate during middle childhood and early adolescence as the brain approaches maturation~\cite{Luna-etal-2004, Gathercole-etal-2004}. 
By post-adolescence, working memory capacity plateaus, reaching adult-level performance~\cite{Sowell2002DevelopmentOC, Luna-etal-2004}.





Based on these observations, we characterized the growth trajectory of working memory, as illustrated in Figure~\ref{fig:trajectory}, using an exponential model of the form \( y = b - a^x \) (\( 0 < a < 1 \)).
In this model, \( b \) represents the asymptotic upper limit of working memory capacity, corresponding to adult-level performance, while \( a \) determines the rate of growth. 
Specifically, smaller values of \( a \) result in steeper early growth, reflecting the rapid cognitive development observed during early childhood, whereas larger values of \( a \) indicate a slower rate of change. 

This modeling approach is justified for several reasons. 
First, the horizontal asymptote inherent in the exponential function accurately represents the biological ceiling of adult working memory capacity.
Second, the rapid initial increase observed during early childhood is consistent with the steep growth predicted by this exponential form. 
Finally, alternative models, such as logarithmic or linear growth, fail to account for both the early rapid development and the eventual plateau: logarithmic models imply unbounded growth, while linear models oversimplify the deceleration phase.
Thus, the exponential model \( y = b - a^x \) offers a concise and biologically plausible representation of the developmental trajectory of human working memory, aligning well with observed patterns and theoretical considerations.







\subsection{Integrating Human Working Memory into Language Models}
 
In this study, Attention with Linear Biases (ALiBi)~\cite{press2022train} is employed to model the constraints of human working memory.   
ALiBi is a method for Transformer~\cite{NIPS2017_3f5ee243} models that does not use positional embeddings but instead applies a distance-dependent linear penalty to attention scores.  
Specifically, the attention score for an input sequence of length \(L\) is calculated as follows:

\begin{equation}
\begin{aligned}
\text{Attention Score} &= \text{softmax}\left(q_i K^\top + m \cdot B \right), \\
B &= \begin{bmatrix}
-(i-1) & -(i-2) & \cdots & 0
\end{bmatrix}.
\end{aligned}
\label{eq:attention}
\end{equation}



Here, \( q_i \in \mathbb{R}^{1 \times d} \), \( K \in \mathbb{R}^{L \times d} \), \( m \in \mathbb{R}_{[0,1]} \), and \( B \in \mathbb{R}^{1 \times L} \) represent the query, the key, a scalar slope specific to each attention head, and a bias matrix encoding the relative distances between queries and keys, respectively, where \( B_{i} \) is defined as the negative absolute difference between the query position \( i \) and each key position.
The values of \(m\) are set geometrically for each head. 
For example, in an 8-head model, the values of \(m\) are assigned as follows: \(m = 1, \frac{1}{2}, \frac{1}{4}, \ldots, \frac{1}{128}\). 
The slope \(m\) takes values in the range \([0, 1]\), ensuring a consistent interpretation of its influence on attention scores.
By penalizing attention scores for query-key pairs with greater distances, ALiBi introduces a \textit{recency bias} to the model. 
Originally, ALiBi was proposed to enhance the extrapolation capability of Transformer models.
More recently, \citet{clark-etal-2025-linear} has shown that incorporating it into attention score computation during training allows for the estimation of surprisal patterns resembling human reading times.  
This suggests its potential for modeling human-like memory decay and cognitive limitations.   




However, since the slope \(m\) in ALiBi is fixed for each attention head, the approach does not inherently reflect the developmental increase in working memory capacity (i.e., reduced decay) over time (Figure~\ref{fig:trajectory}). 
Therefore, this study proposes a method, \textsc{DynamicLimit-Exp}, which replicates the developmental characteristics of working memory during the CP, specifically its exponential growth. 
This is achieved by exponentially decreasing the slope \( m \) in ALiBi as training epochs progress.  
In this method, the slope \(m\) in the ALiBi mechanism is updated at each epoch \(t\) as follows:

\begin{equation}
m_t = m_0 \cdot r^t,
\end{equation}
where \(m_0\) represents the initial slope, \(r \in (0, 1)\) is the decay rate, and \(t\) denotes the current epoch. 
In this study, the model's working memory capacity \(w_t\) is formulated as follows:

\begin{equation}
w_t \coloneqq 1 - m_t.
\end{equation}

This definition establishes a direct relationship between the dynamically decaying slope \(m_t\) and the model's working memory capacity \(w_t\). As \(m_t\) decreases exponentially over time, \(w_t\), representing working memory, grows correspondingly, allowing the model to retain broader contextual information as training progresses. 
By mimicking this developmentally plausible growth of working memory, the model prioritizes attention to short-range dependencies during the early stages of training, gradually shifting its focus to long-range dependencies as training progresses.

Furthermore, a key distinction between ALiBi and \textsc{DynamicLimit-Exp} lies in how the slope \( m \) is assigned across attention heads. 
While ALiBi applies a fixed per-head bias, enforcing a predetermined recency bias throughout training, \textsc{DynamicLimit-Exp} instead shares the slope \( m \) across all heads. 
This ensures that the model maintains a globally coherent bias that evolves dynamically over the course of training. 
In other words, ALiBi imposes a head-specific static recency bias, whereas \textsc{DynamicLimit-Exp} introduces a dynamically changing proximity bias that governs the entire learning schedule. 
This shift enables the model to more accurately simulate the adaptive nature of human working memory development, potentially capturing the CP of cognitive maturation.







\section{Experiments}
\label{sec:experiment}
This study explores whether LMs trained from scratch can achieve more efficient \lone acquisition by incorporating the developmental characteristics of human working memory.
Specifically, we aim to determine whether this approach can replicate the increased efficiency of \lone acquisition observed during the CP in \lone acquisition, focusing on the developmental advantages before the end of this period.



\subsection{Configurations}
\paragraph{Models}
\input{tables/tab_main_result}
\input{tables/tab_wikipedia_exp}  
We used the \texttt{transformers}~\cite{wolf-etal-2020-transformers} implementation of the GPT-2~\cite{radford2019language} as the base LM. 
While some studies utilize RoBERTa~\cite{liu2019robertarobustlyoptimizedbert} as a base model~\cite{huebner-etal-2021-babyberta, warstadt-etal-2023-findings}, we selected GPT-2 for two primary reasons: (1) its unidirectional (left-to-right) predictions more effectively capture human working memory constraints, and (2) GPT-based architectures dominate modern LLMs~\cite{openai2023gpt4,touvron2023llama}.


\input{figures/fig_wm_curves}


\paragraph{Dataset}

  

We used AO-CHILDES~\cite{aochildes2021}\footnote{\url{https://github.com/UIUCLearningLanguageLab/AOCHILDES}} as the training dataset, which is derived from the CHILDES dataset~\cite{Macwhinney2000} and records CDS from conversations between children and adults.
AO-CHILDES contains 5 million words of speech directed at English-speaking children aged 1–6 years and controls for external factors such as age group, speaker variation, and situational context.
As a preprocessing step, following \citet{haga-etal-2024-modeling}, all sentences were converted to lowercase, and sentences shorter than three words were excluded.
Since the AO-CHILDES dataset contains only about 5 million words, training a standard GPT-2 model would likely result in overfitting.  
To mitigate this, we followed existing studies on small language models (SLMs) trained with CDS datasets~\cite{huebner-etal-2021-babyberta,haga-etal-2024-modeling} and constructed an SLM with 4 layers, 4 attention heads, and 256 embedding dimensions for the base model.  
Details of the training configuration for the base model are provided in Appendix~\ref{appendix:detailed_settings}.

Furthermore, to determine whether the CP effect stems from exposure to specific linguistic stimuli, such as CDS, or from the model's cognitive developmental properties independent of input, we conducted a complementary experiment using Wikipedia (written language, adult-oriented) as training data.  
Following~\citet{huebner-etal-2021-babyberta}, 500,000 sentences were randomly sampled from the English Wikipedia corpus.
We used the latest version of Wikipedia, as of January 2025, \footnote{\url{https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}} and preprocessed it using  \texttt{WikiExtractor}.\footnote{\url{https://github.com/attardi/wikiextractor}}







\paragraph{Evaluation}

We evaluate the grammatical abilities of these models using a developmentally inspired targeted syntactic evaluation benchmark, Zorro~\cite{huebner-etal-2021-babyberta}.
Zorro is designed for assessing the syntactic and grammatical knowledge of LMs in child-directed language and consists of 13 mid-level categories and 23 subcategories.  
Each subcategory contains 2,000 sentence pairs, with one grammatically acceptable and one unacceptable sentence per pair.  
Below is an example of a minimal pair from the ``Subject-verb agreement (\textsc{S-v agr})'' category:\footnote{See Appendix~\ref{appendix:zorro_items} for the full list of grammatical categories.}


\eenumsentence[1]{
    \item \ac The \textbf{lie} on the foot is flat.
    \item \un The \textbf{lies} on the foot is flat.
}

By inputting both the acceptable and unacceptable sentence into the model and calculating the proportion of pairs where the model assigns a higher probability to the acceptable sentence, we obtain the grammaticality judgment score (Accuracy).
In this study, we report scores for each mid-level category (henceforth, \textit{grammatical items}) as well as their macro-average.

\input{tables/tab_vs_reverse}


\subsection{Baselines}
We prepared the following three baseline models to precisely analyze the learning effects of different working memory limitation strategies:

\begin{itemize}
    \item \textbf{\textsc{NoLimit}}: A model with no memory constraints. Working memory remains constant from the early stages of training, simulating the mature working memory observed post-adolescence. This configuration is equivalent to a vanilla GPT-2~\cite{radford2019language}.
    \item \textbf{\textsc{StaticLimit}}: A model applying standard ALiBi~\cite{press2022train} during attention score calculation, where memory constraints remain fixed throughout training.
    \item \textbf{\textsc{DynamicLimit-Linear}}: A model in which the ALiBi slope \(m\) decreases linearly over the course of training.
\end{itemize}

To ensure a fair comparison between the linear and exponential growth curves of working memory, we controlled the initial and final values of working memory capacity \(w_t\) in \textsc{DynamicLimit-Linear} and \textsc{DynamicLimit-Exp} to be as similar as possible.  
Specifically, we set the number of training epochs to 10 and configured both models with an initial slope of \(m = 1.0\) and a final slope of \(m = 0.0\).  
Figure~\ref{fig:wm_curves} illustrates the trajectory of working memory capacity for each model.  
All models were trained using three different seeds, and we report the average results across these runs.


\subsection{Results}
\label{subsec:main_results}
\paragraph{Developmentally-plausible working memory shapes the CP for \lone acquisition}
Table~\ref{tab:main_result} presents the accuracy of each model trained on the AO-CHILDES.  
Compared to \textsc{NoLimit} and \textsc{StaticLimit}, which do not account for developmental changes in working memory, \textsc{DynamicLimit-Linear} and \textsc{DynamicLimit-Exp}, which simulate its gradual growth, achieve significantly higher overall performance.  
Among them, \textsc{DynamicLimit-Exp} attains the highest overall accuracy, supporting the effectiveness of a cognitively plausible mechanism.  
The comparable performance of \textsc{StaticLimit} to \textsc{NoLimit} suggests that the gradual introduction of working memory constraints throughout training is crucial, rather than their static application.  
These results indicate that \textsc{DynamicLimit-Exp} effectively replicates the CP effect observed in human \lone acquisition.



\paragraph{The CP depends on the child's learning algorithm, not the input stimulus}
Table~\ref{tab:wiki_result} presents the accuracy of models trained on Wikipedia, showing trends similar to those observed in Table~\ref{tab:main_result}, where the models were trained on AO-CHILDES. 
Specifically, \textsc{DynamicLimit-Linear} and \textsc{DynamicLimit-Exp} outperform \textsc{NoLimit} and \textsc{StaticLimit} in overall accuracy, with \textsc{DynamicLimit-Exp} achieving the highest performance, further supporting the efficacy of incorporating developmental working memory constraints.
These findings suggest that the CP effect does not depend solely on exposure to specific linguistic stimuli (e.g., CDS) but rather on the learning algorithm itself, which mirrors human cognitive development. 


This result aligns with existing research~\cite{feng-etal-2024-child}, which has reported that child language input is not uniquely valuable for training LMs.
This finding suggests that our method is applicable to LLM pretraining, as they typically use non-CDS datasets such as Common Crawl and Wikipedia~\cite{touvron2023llamaopenefficientfoundation}.





\input{figures/fig_feature-extraction}



\section{Analysis}
\subsection{Testing the ``Less-is-more'' Hypothesis with Reversed Cognitive Constraints}
\label{subsec:test_reversed}
A key question arising from the results (\S\ref{sec:experiment}) is whether \textsc{DynamicLimit-Exp}'s superior performance stems from the ``Less-is-more'' hypothesis~\cite{NEWPORT199011}—i.e., the gradual growth of working memory—or from unintended side effects. 
In other words, does the gradual \textit{change} in working memory enhance information capacity, dynamically shifting the model's focus across epochs and ultimately aiding rule generalization?
To test this, we introduce a cognitively \textit{implausible} language model, referred to as~``\textsc{DynamicLimit-Exp} (\textdownarrow)'', which shares the same slope trajectory as our proposed~\textsc{DynamicLimit-Exp} (\textuparrow)~\footnote{This section adopts this notation for simplicity.} but with its direction reversed, such that working memory capacity decreases over time.
Specifically, DynamicLimit-Exp (\textuparrow) is set to \( m_0 = 1.0, r = 0.6 \) (the same setting as in \S\ref{sec:experiment}), while DynamicLimit-Exp~(\textdownarrow) is set to \( m_0 = 0.01, r = 1.668 \) to achieve a nearly symmetrical curve.\footnote{Since setting the initial slope \( m_0 = 0.0 \) prevents \( w_t \) from being updated in Equation (2), we set it this way for computational reasons.
}


Table~\ref{tab:vs_reverse} provides evidence supporting the Less-is-more hypothesis, as~\textsc{DynamicLimit-Exp} (\textuparrow) consistently outperformed the cognitively implausible~\textsc{DynamicLimit-Exp} (\textdownarrow).  
The observed performance gap, particularly in grammatical items requiring both local and non-local dependencies (e.g., CASE, ARG. STR, and FILLER-GAP), suggests that the gradual growth of working memory is crucial for grammatical learning and generalization, as it enables the early extraction of basic patterns followed by the progressive acquisition of complex rules.  
These findings indicate that the superior performance of~\textsc{DynamicLimit-Exp} (\textuparrow) is primarily driven by the developmental trajectory of working memory growth rather than unintended side effects of dynamic shifts in memory focus.

\input{tables/tab_cluster_analysis}


Incidentally, from the series of experimental results, along with those in \S\ref{sec:experiment} (Table~\ref{tab:main_result} and \ref{tab:wiki_result}), \textsc{NoLimit} and \textsc{DynamicLimit-Exp} (\textdownarrow) consistently outperform \textsc{DynamicLimit-Exp} (\textuparrow) in~\textsc{ELLIPSIS}, as exemplified by the following cases:
\eenumsentence[2]{ \item \ac Mark fixed one \textbf{worn} canal, and Roger \nextline{fixed more.} \item \un Mark fixed one canal, and Roger fixed \nextline{more \textbf{worn}.} }
Since resolving \textsc{ELLIPSIS} involves maintaining long-range dependencies, \textsc{DynamicLimit-Exp} (\textuparrow) may struggle due to its initial memory constraints. 
This suggests that grammatical items like \textsc{ELLIPSIS} require substantial memory from the early stages of training, and thus, our proposed method may not be optimal for learning such structures. 
Alternative workarounds, such as dynamically adjusting memory allocation or hybrid approaches, may be necessary to address this limitation.


\subsection{Development of Feature Extraction Capabilities}
Figure~\ref{fig:embedded_space} visualizes the clustering structure of final-layer embeddings using t-SNE~\cite{JMLR:v9:vandermaaten08a} for \textsc{FILLER.GAP}, a grammatical items where gradual memory expansion yielded significant performance improvements in both AO-CHILDES and Wikipedia datasets, as highlighted in the previous results  (\S\ref{subsec:main_results} and \S\ref{subsec:test_reversed}).
In \textsc{NoLimit} (Figure~\ref{fig:embedded_space}a), the embedding clusters initially expand between Epoch 1 and Epoch 5, but by Epoch 10, they appear to contract and overlap more, suggesting a stagnation in representation learning. The clusters become less distinguishable, which may indicate a loss of diversity in the learned representations.
In contrast, \textsc{DynamicLimit-Exp} (Figure~\ref{fig:embedded_space}b) maintains a more structured and progressive evolution of embeddings. 
The clusters remain well-separated throughout training, with clear distinctions between different epochs. This suggests that the model continuously refines its representations without excessive compression, preserving the diversity necessary for robust generalization.




To quantitatively analyze these differences, Table~\ref{tab:cluster_analysis} reports key statistical measures, including \textit{entropy} (distribution diversity) and \textit{mean Euclidean distance} (inter-cluster separation).\footnote{The appendix~\ref{appendix:statistical_measures} shows how to calculate each measure.} 
Regarding \textbf{entropy}, \textsc{NoLimit} shows a decreasing trend, reflecting reduced distribution diversity and potential over-clustering. In contrast, \textsc{DynamicLimit-Exp} preserves consistently higher entropy, indicating a balanced representation that avoids excessive compression. 
For \textbf{mean Euclidean distance}, \textsc{NoLimit} undergoes substantial change between Epoch 1 and Epoch 5 but stagnates thereafter, suggesting limited refinement. \textsc{DynamicLimit-Exp}, however, maintains large distances across epochs, reflecting continuous structural reorganization.

The differences also highlight the role of \textbf{isotropy}.
\textsc{NoLimit} exhibits increasing anisotropy, with embedding clusters becoming overly compact by Epoch 10, which may hinder generalization. 
In contrast, \textsc{DynamicLimit-Exp} maintains a more isotropic distribution, as indicated by stable entropy, allowing for more flexible and structured representation learning.
These findings align with recent work on syntactic smoothing, which suggests that reducing anisotropy enhances the ability to generalize across linguistic contexts~\cite{diehl-martinez-etal-2024-mitigating}.  
Thus, the increased isotropy observed in \textsc{DynamicLimit-Exp} provides strong evidence that gradual memory expansion facilitates structured representation learning and syntactic generalization.\footnote{We also analyzed \textsc{CASE}, which exhibited the same trend as \textsc{FILLER.GAP} (as shown in Appendix~\ref{appendix:cluster_analysis}).}






\subsection{Influence of Input Stimulus Length}
\input{tables/tab_length_analysis}

We analyze how sentence length affects the performance of \textsc{NoLimit} and \textsc{DynamicLimit-Exp}. 
To assess their adaptability, we created four Wikipedia-based datasets, each with 500,000 sentences in length ranges: [5,10], [11,50], [51,100], and [101,150].

The results in Table~\ref{tab:length} reveal notable differences in model performance. 
For shorter sentences in the [5,10] range, \textsc{NoLimit} achieves slightly higher accuracy compared to \textsc{DynamicLimit-Exp}. 
However, in the [11,50] range, \textsc{DynamicLimit-Exp} significantly outperforms \textsc{NoLimit}, achieving 58.7 compared to 47.0. 
This suggests that \textsc{DynamicLimit-Exp} excels at handling moderately long sentences, likely due to its ability to dynamically adjust working memory.
For longer sentences in the [51,100] and [101,150] ranges, \textsc{DynamicLimit-Exp} consistently outperforms \textsc{NoLimit}.



These findings highlight the benefits of dynamic working memory expansion in facilitating rule generalization and contextual adaptation across diverse sentence lengths. 
While \textsc{NoLimit} exhibits competitive performance on short sentences, its stagnation on longer sentences underscores its limited ability to generalize complex patterns. 
Conversely, \textsc{DynamicLimit-Exp}'s consistent performance across varying sentence lengths supports its suitability for grammatical items requiring the processing of both short and long contexts.











\section{Conclusion}
This study proposed a method for integrating the developmental trajectory of human working memory into the training process of LMs, inspired by the \textit{Less-is-More} hypothesis. 
The proposed method, \textsc{DynamicLimit-Exp}, initially restricts working memory and gradually relaxes it exponentially during training.
Experiments on both AO-CHILDES and Wikipedia showed that \textsc{DynamicLimit-Exp} improves grammatical learning efficiency compared to conventional methods without memory constraints or with static memory constraints.
These findings suggest not only provide new approaches for developing data-efficient LMs but also offer indirect evidence supporting the CPH in human language acquisition.



\section*{Acknowledgments}
We are grateful to Akiyo Fukatsu for her valuable comments and suggestions.
This work was supported by JSPS KAKENHI Grant Number 24H00087, JST PRESTO Grant Number JPMJPR21C2.


\section*{Limitations}
\paragraph{Scalability.}
One limitation of this study is the constrained scale of the experimental setup.  
The primary goal of this study is to computationally replicate the CP in \lone acquisition, as discussed in cognitive science~\cite{lenneberg1967biological,FROMKIN197481,curtiss1977genie,JOHNSON198960}.  
Following previous studies~\cite{huebner-etal-2021-babyberta,haga-etal-2024-modeling}, we designed the experiment to be as ecologically valid as possible by training an SLM using CDS.  
While this controlled setting allows for a more precise analysis and simulation of the Less-is-More hypothesis, it remains unclear how our findings contribute to the data efficiency of LLMs.  
The experimental results with Wikipedia (Table~\ref{tab:wiki_result}, \ref{tab:vs_reverse}, \ref{tab:length}) provide a promising outlook in this direction, but further investigation with larger models and datasets is necessary to determine the effectiveness and limitations of the proposed approach.  

\paragraph{Language.}
In this experiment, we investigated the replication of the CP effect in \lone acquisition using English.  
However, since the CP effect is observed across various languages~\cite{Patkowski1980,JOHNSON198960}, it remains to be tested whether the proposed approach is effective in multilingual environments.  
To our knowledge, there is currently no targeted syntactic evaluation specifically designed for CDS across different languages, such as Zorro.  
Zorro was developed based on BLiMP~\cite{warstadt-etal-2020-blimp-benchmark}, an adult-oriented targeted syntactic evaluation for English, and recent studies have proposed multilingual versions of BLiMP (e.g., JBLiMP~\cite{someya-oseki-2023-jblimp} for Japanese and CLiMP~\cite{xiang-etal-2021-climp} for Chinese).  
Therefore, developing CDS-specific versions based on these multilingual BLiMPs could help address this limitation.







% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\clearpage
\appendix




\section{Details of the Training Configuration for the Base Models}
\label{appendix:detailed_settings}
Table~\ref{table:hyperparameters} shows the training settings of the base model.
For the experiment, a single NVIDIA RTX A5000 (24GB) GPU was used, and the training time for each run was approximately one hour.
\input{tables/tab_hyperperameters}



\input{tables/tab_cluster_analysis_case}

\section{Details of Grammatical Items in Zorro}
\label{appendix:zorro_items}
\input{tables/tab_zorro_minimal-pairs}
Table~\ref{tab:zorro_minimal-pair} shows the full list of grammatical categories in Zorro. 
Examples are taken from Table 5 in the original paper~\cite{huebner-etal-2021-babyberta}.


\section{Analysis of Distributional Changes in t-SNE Space Across Training Epochs}
\label{appendix:statistical_measures}
This section explains in detail the analysis of the entropy and average distance of embeddings projected into the t-SNE space for different learning epochs.

\subsection{Entropy Calculation}
To quantify the distribution of embeddings, a 2D histogram is constructed using a fixed grid (\( 50 \times 50 \) bins). 
The probability distribution \( P \) is obtained by normalizing the histogram. The entropy is then computed as:

\begin{equation}
H(P) = - \sum_{i} P_i \log P_i,
\end{equation}
where \( P_i \) is the probability of each bin. 
Higher entropy suggests a more uniform distribution, whereas lower entropy indicates clustering.

\subsection{Mean Distance Between Epochs}
To analyze shifts in embedding distributions across epochs, we compute the Euclidean distance between the mean embedding vectors of different epochs:

\begin{equation}
D(X, Y) = \| \mu_X - \mu_Y \|,
\end{equation}
where \( \mu_X \) and \( \mu_Y \) are the mean vectors at different epochs. 
Larger distances imply greater shifts in the learned representation.



\section{Development of Feature Extraction Capabilities in \textsc{CASE}}
\label{appendix:cluster_analysis}
\input{figures/fig_feature-extraction_case}


Figure~\ref{fig:embedded_space_case} visualizes the clustering structure of final layer embeddings using t-SNE for \textsc{CASE}.
The embedding space visualizations reveal distinct patterns between~\textsc{NoLimit} and~\textsc{DynamicLimit-Exp} across training epochs. 
In~\textsc{NoLimit}, the embedding clusters expand between Epoch 1 and Epoch 5 but contract significantly by Epoch 10, suggesting stagnation in representation learning. 
In contrast,~\textsc{DynamicLimit-Exp} maintains structured evolution throughout training, with well-separated clusters that reflect progressive refinement.

Regarding \textbf{entropy}, \textsc{NoLimit} shows a slight decrease over time, reflecting reduced distribution diversity as training progresses. 
In contrast, \textsc{DynamicLimit-Exp} maintains or slightly increases entropy, suggesting a balanced emphasis on both basic patterns and diverse features, even in later training stages.
For \textbf{mean Euclidean distances} between clusters,~\textsc{NoLimit} exhibits large distances between Epoch 1 and Epoch 5 but demonstrates minimal evolution between Epoch 5 and Epoch 10. 
This stagnation may highlight the model's failure to effectively generalize new rules. 
\textsc{DynamicLimit-Exp}, on the other hand, maintains substantial distances across epochs, indicating continuous embedding evolution and refinement throughout training.



\end{document}
