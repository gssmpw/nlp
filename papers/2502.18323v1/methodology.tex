\section{Power-Aware Training}
\label{sec:methodology}

\begin{figure*}[!t]
    \centering
     \resizebox{0.9\textwidth}{0.46\textwidth}
    {

    \includegraphics{Framework.drawio-4.pdf}
    }
    
    \caption{Overview of our proposed cross-layer approach that accelerate training under power constraint though the joint selection of batch size $b$ and GPU frequency $f$.} 
    \label{fig:overview_figure}
\end{figure*}

We propose an optimization approach that co-selects~$b$ and~$f$ to minimize~$TT_\text{acc}$ on an edge device under~$P_\text{max}$. 
Following the problem split that we propose in \Cref{eq:time_to_acc2}, our solution consists of two main parts. The first one considers device specifics, and involves measuring time and power for training a given model~$\nn$, i.e., $T_{s}(b, f, \nn)$ and $P(b, f, \nn)$ for every $b\in\mathcal{B}$ and~$f\in\mathcal{F}$.
The second part is responsible for estimating the efficiency of batch sizes, i.e., their impact on~$N_{s_{\text{acc}}}$. This part does not depend on device characteristics and it is computationally expensive, therefore we consider it to be performed on the server that is responsible for pre-training and sending of the model. 

The workflow for our approach is as follows: The device first sends a request to the server for a pre-trained neural network model ($\nn$) with a specific architecture and input size. The server responds with the requested model and its pre-trained weights. Our proposed power-aware training then begins. The device profiles the model in terms of time and power as will be described in \cref{subsec:profiling}, while the server estimates~$N_{s_{\text{acc}}}$ for various batch sizes, as will be discussed in \Cref{subsec:proxy}, and sends these estimations to the device. Based on the profiling and estimations, the device selects the best combination of batch size and frequency to minimize~$TT_\text{acc}$. An overview is presented in \Cref{fig:overview_figure}. 






\subsection{Profiling} \label{subsec:profiling}


The power and time for training on a device also depend on the \ac{NN}'s architecture~$\nn$, in addition to the frequency and batch size. Besides, changing the input shape, such as the image size in vision tasks, can lead to different time and power requirements even when using the same \ac{NN} architecture. Furthermore, the time and power are independent of the actual training data. Therefore, profiling~${T_{s}(b, f, \nn)}$ and~$P(b, f, \nn)$ given the task's input shape can be performed before full data acquisition, as long as the input dimensions of the data samples are known. 


Processing a few mini-batches is adequate to obtain accurate profiling. For each batch size and frequency combination, we set the GPU frequency to~$f$ and process a few mini-batches~$m$ of size~$b$. The power sensor values are monitored to extract the peak power. Furthermore, the processing time is recorded, and then the average processing time for a mini-batch is calculated. This average processing time is then scaled to the processing time for~$s$ samples, denoted as~$T_{s}$.~$T_{s}(b, f, \nn)$ and~$P(b, f, \nn)$ are then stored in two lookup tables, denoted as~$\text{LUT}^{\nn}_{\text{Time}}$ and~$\text{LUT}^{\nn}_{\text{Power}}$.


The proposed profiling strategy considers hardware and \ac{NN} structures. Also, the time and power values are not affected by the network weight updates; thus, profiling is applicable before receiving pre-trained weights if~$\nn$ is available at the device and is needed to be perdomed only once. In a future and more practical setting, we could assume such profiling to be provided by the device manufacturer and for a pre-defined set of tasks and models.

%(unless a compression technique is applied)

Furthermore, if ~$P_{max}$ is known at the device, the profiling can be performed more efficiently as follows. Given that $\mathcal{B}$ and $\mathcal{F}$ are sorted, and power consumption increases with both $b$ and $f$, we begin by profiling the largest $b$ with the minimum $f$, incrementing $f$ until the maximum feasible value under ~$P_{max}$ is reached. Next, we move to the second largest $b$, starting from the highest feasible frequency found for the previous batch size, and repeat the process until the smallest batch size is profiled. This profiling is equivalent at most to 1.9\% and 6\% of the training times performed on ResNet18 and MobileNetV2 in \Cref{subsec:training_time_evaluation}.


\subsection{Estimation of~$N_{s_{\text{acc}}}$ for Batch Sizes} \label{subsec:proxy}


As discussed in \cref{sec:problem_statement}, batch sizes have different efficiency in terms of~$ N_{{s}_{\text{acc}}}(b_{i}, \nn, D)$. 
Estimating~$ N_{{s}_\text{{acc}}}(b_{i}, \nn, D)$ for every~$b_{i}\in\mathcal{B}$ depends also on the training data that is only available at the device and $\nn$'s pre-trained weights. To solve \Cref{eq:optimization}, the exact number of samples processed to reach target accuracy for each batch size can be replaced with the relative ratio between batch size~$r_{b_{i}}$ (i.e., normalized to the maximum~$N_{s_{\text{acc}}}$) as follows:
\begin{equation}
    r_{b_{i}}= \frac{N_{{s}_{\text{acc}}}(b_{i}, \nn, D)} {\underset{b \in \mathcal{B}}{\max} N_{{s}_{\text{acc}}}(b, \nn, D)}
\end{equation}
With this simplification, we adjust our focus to estimate the relation vector between batch sizes such that $\mathbf{r} = (r_{b_{1}},r_{b_{2}}, \ldots ,r_{b_\text{{max}}})$, where~$r_{b_{i}} \in (0, 1]$. However, this is still a complex task to solve given the non-linear training dynamics of deep learning; especially as the convergence speed of every batch size changes across training, making it impossible to estimate with few probes of multiple batch sizes given the training state. Obviously, training till convergence for multiple batch sizes is computationally-expensive task and cannot be conducted on the device. These all make on-device estimation of $\mathbf{r}$ inaccurate, if not infeasible.




We thus propose to estimate~$\mathbf{r}$ on a powerful GPU server. Particularly, we train~$\nn$ (with the same weights and optimizer) with multiple batch sizes until convergence (reaching the target accuracy) for a proxy dataset~$D_S$, since the server does not have an access to~$D$. This systematic exploration allows us to comprehensively assess the long-horizon impact of the batch size on the model's convergence while leveraging the computational capabilities of the server,  along with the datasets available on it and augmentation techniques.  Since~$\nn$, which would be used on the edge device, is already designed for a specific task type and pre-trained on a public dataset, a proxy dataset~$D_{S}$ should share the same task type (e.g., image object classification) and similar input shapes. Training network~$\nn$ on~$D_{S}$, despite having different objectives, allows us to estimate the relationship between batch sizes and their relative examples to accuracy on~$D$. 
Thus, we can finally have a mapping such that $\mathbf{r}_{D_{S}} \approx \mathbf{r}_{D}$.  

By estimating the batch size relation vector~$\mathbf{r}$ on the server, any edge device aiming to train~$\nn$ to utilize this vector. In \Cref{subsec:training_time_evaluation}, we provide evaluation for two different devices, namely Nvidia Jetson Nano and Nvidia Jetson TX2NX, utilizing~$\mathbf{r}$. 



\begin{algorithm}[tb!]
    \caption{Batch size and GPU frequency selection}
    \label{alg:configuration_selection}
        \input{configuration_selection}
\end{algorithm}
\subsection{Batch Size and Frequency Selection}
\label{subsec:selection}
The device profiling and estimation of batch size efficiently are performed in an offline manner and at the design time. In contrast, the configuration selection is performed at runtime, as described below.
%The following describes our configuration selection process. 
Given a power constraint~$P_{\text{max}}$, and the power measurements stored for training a specific~$\nn$ in~$\text{LUT}^{\nn}_{\text{Power}}$, we construct a set of feasible combinations~$C$ consisting of every feasible batch size with its corresponding highest (and fastest) frequency satisfying~$P_\text{max}$ as follows: 
\begin{equation}
    \begin{aligned}
    &C \gets \{(i, j_{i})) | i \in [1, \ldots, |\mathcal{B}|] \}, \\
    &j_{i} \gets \max \{j| j \in [1, \ldots, |\mathcal{F}|], \text{LUT}^{\nn}_{\text{Power}}(i, j) < P_{\text{max}}\} 
    \end{aligned}
\label{eq:feasible_combination}
\end{equation}

The processing time for the feasible combinations is then extracted from~$\text{LUT}^{\nn}_{\text{Time}}$. Following this, an approximate training time is computed by multiplying the time for every $b_{i}$ (and frequency) by the corresponding $\mathbf{r_{i}}$ element from the relation vector~$\mathbf{r}$ (i.e., estimated at design time). The configuration ($b^{*}$, $f^{*}$) that minimizes the approximate total training time is selected. Finally, we set the GPU frequency to~$f^{*}$ and then start the training using batch size~$b^{*}$. 
 

    
We provide the configuration selection in \cref{alg:configuration_selection}. The selection part is $\mathcal{O}(n^{2})$ in the worse case scenario ($\mathcal{O}(|\mathcal{B}| \times |\mathcal{F}|))$. Since $\mathcal{B}$ and $\mathcal{F}$ are sorted and the same for the power and time for $|\mathcal{F}|$ for each $b$, the selection can easily be transformed to $\mathcal{O}(n\log n)$ by a binary search and is negligible to training time.


