
\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\usepackage{caption}
\usepackage{amsmath}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{pgfplots.statistics, pgfplots.colorbrewer}  % provides \pgfplotstabletranspose
\usepgfplotslibrary{groupplots}
\usetikzlibrary{shapes.multipart}
\usetikzlibrary{patterns}
\usepackage{placeins}
\usepackage{svg}

%\usetikzlibrary{external}
%\tikzexternalize[prefix=figures/]



\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{acro}



\usepackage[capitalise]{cleveref}
\usepackage{adjustbox}
\usepackage{booktabs}


% Batch sizes colors
%\definecolor{b4}{RGB}{}
\definecolor{b8}{RGB}{0,100,100} % Teal
\definecolor{b16}{RGB}{30, 50, 100}
\definecolor{b32}{RGB}{100, 100, 100}
\definecolor{b64}{RGB}{100, 50, 30}
\definecolor{b128}{RGB}{200, 50, 30}


\newcommand\nn{M}
\newcommand{\INDSTATE}{\State \hspace{6mm}}

\definecolor{baseline1}{RGB}{180, 40, 200}
\definecolor{baseline2}{RGB}{0, 0, 140}
\definecolor{fastest}{RGB}{200, 20, 20}
\definecolor{ours}{RGB}{100, 200, 100}


\DeclareAcronym{VFS}{short=VFS, long=Voltage Frequency Scaling}
\DeclareAcronym{SOC}{short=SOC, long=System On Chip}
\DeclareAcronym{NN}{short=NN, long=Neural Network}
\DeclareAcronym{DVFS}{short=DVFS, long=Dynamic Voltage and Frequency Scaling}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\usepackage{comment}




\title{Accelerated Training on Low-Power Edge Devices}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Mohamed Aboelenien Ahmed \email mohamed.ahmed3@kit.edu \\
      \addr Karlsruhe Institute of Technology
      \AND
      \name Kilian Pfeiffer \email kilian.pfeiffer@kit.edu \\
      \addr Karlsruhe Institute of Technology
      \AND
      \name Heba Khdr \email heba.khdr@kit.edu \\
      \addr Karlsruhe Institute of Technology \\
      \AND
      \name Osama Abboud \email osama.abboud@huawei.com \\
      \addr Huawei Research Center Munich \\
      \AND
      \name Ramin Khalili \email ramin.khalili@huawei.com \\
      \addr Huawei Research Center Munich \\
      \AND
      \name JÃ¶rg Henkel \email henkel@kit.edu \\
      \addr Karlsruhe Institute of Technology }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Training on edge devices poses several challenges as these devices are generally resource-constrained, especially in terms of power.  
State-of-the-art techniques at the device level reduce the GPU frequency to enforce power constraints, leading to a significant increase in training time. To accelerate training, we propose to jointly adjust the system and application parameters (in our case, the GPU frequency and the batch size of the training task) while adhering to the power constraints on devices. 
We introduce a novel cross-layer methodology that combines predictions of batch size efficiency and device profiling to achieve the desired optimization. 
Our evaluation on real hardware shows that our method outperforms the current baselines that depend on state of the art techniques, reducing the training time by $2.4\times$ with results very close to optimal. Our measurements also indicate a substantial reduction in the overall energy used for the training process. These gains are achieved without reduction in the performance of the trained model. 
\end{abstract}

\input{introduction}
\input{background}
\input{problem_statement}
\input{methodology}
\input{results}
\input{conclusion}

\bibliography{bibliography}
\bibliographystyle{tmlr}

\input{appendix}


\end{document}
\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}