\clearpage
\appendix


\section{Appendix}

\subsection{Training on edge CPU}
\label{appendix:CPU_training}
In this section, we briefly discuss training on an edge device's CPUs (i.e., Jetson Nano) and the reasons of why to focus only on training on GPUs on such devices. In \cref{fig:cpu_gpu_latency}, we show the processing time and power for training over 50K samples with batch size 32, where both CPU and GPU are using the maximum frequency. We observe that the GPU is an order of magnitude faster compared to the CPU, while nearly consuming similar average power. Similarly, the GPU would be nearly an order of magnitude more energy efficient on these devices compared to the CPU. Thus, we focus in our work on edge GPUs as training is more time and energy efficient.  

\begin{figure}[htb!]
    \centering
    \includegraphics[]{gpu_cpu.pdf}
    \label{fig:cpu_gpu_latency}
    \caption{ResNet18 time for training 50K samples ($T_s$) and average power (red triangles) of Cifar10 on Jetson Nano over CPU and GPU.}
 
\end{figure}



\subsection{Proxy Dataset Analysis}
\label{appendix:proxy_dataset_extension}

\begin{figure*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[]{resnet_p1.pdf}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[]{resnet_p2.pdf}
    \end{subfigure}
    \hspace{1mm}
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[]{resnet_na.pdf}
    \end{subfigure}
    }
    \caption{Confusion Matrix of time increase percentages to the fastest configuration for image classification datasets across three power constraints for ResNet18 training.}
    \label{fig:confusion_matrix_resnet18}
\end{figure*}




We provide a continuation for the sensitivity analysis for the proxy dataset selection. In \cref{fig:confusion_matrix_resnet18}, provide the confusion matrix for selecting different proxy datasets to train ResNet18. The difference for ResNet18 compared to MobileNetV2 (in \Cref{fig:confusion_matrix_mobilenet})is that the suboptimal configurations are selected for the lowest power constraint, while for other cases the selected configurations match the optimal. 




\subsection{Text datasets details}
\label{appendix:text_datasets}
For Jane Austen's dataset part, we extracted it from the nltk package (gutenberg corpus) where the text consists of 3 novel namely Emma, Persuasion, and Sense and Sensibility.  For Dickens, we used eight novels namely The Pickwick Papers, Pictures from Italy,  A Tale of Two Cities, A Story of the French Revolution, The Chimes, Mugby Junction, The Haunted Man and the Ghost's Bargain, and The Mystery of Edwin Drood.
We filter the licensing text, author history, etc. from the training and testing text.
    
\subsection{Power measurement details}

We read the power sensor values during training with a sample rate of 1\si{\second}. For the Nvidia Jetson Nano, the power measurements are available to read from using sysfs nodes. For example, on the jetson nano, the sysfs nodes are available in the path '/sys/bus/i2c/drivers/ina3221x/6-0040/iio:device0/'. 










