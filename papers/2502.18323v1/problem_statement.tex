\section{Problem Statement}
\label{sec:problem_statement}


\begin{figure*}[tb!]
    %\input{figures/motivation_example}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[]{motivation_legend.pdf}
        \includegraphics[]{b8.pdf}
        \includegraphics[]{b32.pdf}
        
    \end{subfigure}
    \hspace{6mm}
    \begin{subfigure}[b]{0.46\textwidth}
        \includegraphics[]{TT_acc_fig.pdf}
    \end{subfigure}

    \caption{The training time on fixed number of samples~$T_s$ and the total training time~$TT_\text{acc}$ to reach an accuracy threshold of~$78\%$ using two batch sizes 8 and 32, while considering the maximum feasible GPU frequencies under three power constraints; $P_1=\SI{4.5}{\watt}$, $P_2=\SI{5}{\watt}$, and~$P_3=\SI{7}{\watt}$. We observe that for~$P_1$ and~$P_2$, selecting $b=8$ will lead to lower~$TT_\text{acc}$, while for~$P_3$ selecting $b=32$ is better. This is in contrast with our observation for~$T_s$, where selecting $b=32$ is the best option in all cases.}
    \label{fig:b8_b32_power_constraint}
\end{figure*}

We consider the following scenario: for a specific training task, an edge device requests a pre-trained \ac{NN} model~$\nn$ with its weights~$\theta$ from a server in order to fine-tune it on local data~$D$ till reaching a given accuracy threshold. 
Importantly, the edge device has a power constraint~$P_{\text{max}}$, which should not be exceeded during the training process. 
Our goal in this paper is to \textit{minimize the training (fine-tuning) time at edge devices under their given power constraints.} 



 We introduce~$T_s$ as the training time required to apply training using a fixed number of samples~$s$. As shown in \cref{fig:3d_profiling}, the joint selection of $b$ and $f$ will help reduce $T_s$  under a power constraint. However, the ultimate optimization goal is to minimize the total training time required to reach a target accuracy, which we label~$TT_{\text{acc}}$.
A set of parameters, i.e., frequency and batch size~$(f,b)$, that are optimal for training a fixed number of samples ($T_s$) might not be necessarily optimal for the training to accuracy ($TT_{\text{acc}})$.

We display in~\cref{fig:b8_b32_power_constraint} the training time to reach an accuracy  of~$78\%$ (~$TT_{\text{acc}}$) for ResNet18 using two batch sizes of~$b_1=8$ and~$b_2=32$ under three different power constraints (i.e., $P_1=\SI{4.5}{\watt}$, $P_2=\SI{5}{\watt}$, and $P_3=\SI{7}{\watt}$). 
We notice that for the three power constraints, selecting~$b_1$ allows to utilize a higher frequency than~$b_2$.
%For the feasible operating points (frequency, batch size) that satisfy the power constraints, we compare $T_S$ and $TT_{\text{acc}}$.
For each batch size, we select the highest frequency that satisfies the power constraint, and measure~$T_s$ and~$TT_{\text{acc}}$.
We observe that using~$b_2$ (the higher batch size) always leads to a lower~$T_s$. %\textcolor{red}{Furthermore, the ratio of ~$T_s$ for ~$b_1$ to ~$T_s$ for ~$b_2$ increases as the power limit value increases, indicating that the larger batch size ~$b_2$ becomes more efficient relative to ~$b_1$ under higher power limits. }
However, selecting the same batch size over~$b_1$ leads to a longer~$TT_{\text{acc}}$ for~$P_1$ and~$P_2$, and shorter~$TT_{\text{acc}}$ for~$P_3$. 
%This shows the complexity of the targeted problem, as the effect of the power constraint on the feasible frequency and the different number of training iterations to reach accuracy for batch sizes highly influences the optimal batch size to minimize~$TT_{\text{acc}}$.

This shows the complexity of the targeted problem. In particular, $TT_{\text{acc}}$ does not only depend on $T_s$, but it also depends on the number of times of processing~$s$ to reach target accuracy ($N_{{s}_\text{acc}}$). In this example, ~$N_{{s}_\text{acc}}$ for $b_2$ is equal to $15$ while ~$N_{{s}_\text{acc}}$ for $b_1$ is equal to $10$. These values and the effect of power constraint on the feasible frequency highly influence the optimal batch size to minimize~$TT_{\text{acc}}$. 
In summary, there is no clear indication on how to select the optimal operating points $(f,b)$ to achieve the target goal. 
%These values have led to higher $TT_{\text{acc}}$ at $b_2$ for $P_1$ and $P_2$, but still lower value for $P_3$, due to the impact of the selected frequencies on $T_s$. 

We formulate our optimization problem as follows:
\begin{equation}
    \begin{aligned}
        & \underset{b\in\mathcal{B}, f\in\mathcal{F}}{\text{min}}
        & & TT_{\text{acc}}(b, f, \nn, D) \\
        & \text{subject to}
        & & P(b, f, \nn) \leq P_{\text{max}}
    \end{aligned}
\label{eq:optimization}
\end{equation}
where $\mathcal{B}$ is the set of feasible batch sizes, $\mathcal{F}$ is the set of available GPU's frequencies, and $P(b, f, \nn)$ is the required power to training~$M$ using~$b$ and~$f$. We rewrite ~$TT_{\text{acc}}$ as the multiplication of~$T_{s}$ and ~$N_{{s}_{\text{acc}}}$, we thus have:
\begin{equation}
    TT_{\text{acc}}(b, f, \nn, D) = T_{s}(b, f, \nn) \times  N_{{s}_{\text{acc}}}(b, \nn, D)
    \label{eq:time_to_acc2}
\end{equation}
$s$ is selected, s.t.~$b_{\text{max}} \leq s \leq |D|$, where~$b_{\text{max}}$ is the largest batch size that can fit into the memory of the devices. 
This detached formulation enables our proposed optimization method, presented in \cref{sec:methodology}.
In particular, the first factor~$T_{s}$ does not depend on the training data~$D$, nor on the accuracy threshold. The second factor~$N_{{s}_\text{acc}}$ is independent of the GPU frequency of the device.

%the number of times of processing~$s$ to reach target accuracy ($N_{{s}_{\text{acc}}}$)
%, and therefore, we can perform profiling on the device prior to the training to obtain feasible operating points. The second factor~$N_{{s}_\text{acc}}$ is independent of the GPU frequency of the device.% and hence it can be estimated on the server. %by performing predictions on the server on a proxy data set. 
%The details of our method are discussed in the following section. 

