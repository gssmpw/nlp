\section{Background and Related Work}
\label{sec:background}
This section discusses the control parameters (i.e., GPU frequency and batch size)  used in this work and the state-of-the-art techniques.
We do not consider training on the CPU for the GPU-equipped devices, as the GPU is an order of magnitude faster (and energy efficient) while consuming nearly the same average power (see \cref{appendix:CPU_training} for some related discussion).

\subsection{\ac{VFS}} 
\label{sec:vfs}
\ac{VFS} is a system-level technique used for power management in processors~\cite{dvfs_thread_powercap, power_constrainted_gpus}. 
It adjusts the voltage and frequency at runtime, providing a trade-off between performance and energy efficiency \cite{dvfs_energy, dvfs_gpu_energy}. Reducing each of the processor's frequency~$f$ and voltage~$V$ values can result in a cubic reduction in the dynamic power, i.e., 
$
    P_{\text{dynamic}} \approx fV^{2}.
$
Within a device, the chip manufacturer provides a discrete set of frequencies to operate with, where~$f_{i}$,  $i \in \{1, 2, \ldots , \text{max}\}$. 
The voltage is a function of the operating frequency~$f$, and will be updated automatically for any given frequency. 
To enforce power constraints on processors, the industry standard technique is to have an upper bound $f$ (and $V$) for the device to use under a power limit. \ac{VFS} is also applied to the CPU's frequency, but this is beyond the scope of our work as we train on the GPUs.
%These values are \textit{decided independently} from the application which would be running on these GPUs. 


\ac{VFS} has been studied in \cite{dvfs_precision_control_nn, dvfs_energy_efficiency_hw_nn} for improving performance and energy efficiency for inference only. Tang et al. \cite{gpu_dvfs_impact_study} conducted a study on the impact of GPU \ac{VFS} on performance and energy for both training and inference. However, the study mainly focused on the computational perspective without addressing accuracy and training speed till convergence. In contrast, we show how changing the frequency and batch size due to power limits, accompanied by the difference in iterations to reach accuracy, can lead to different optimal configurations in training. 





\subsection{Batch Size} 
One of the state-of-the-art techniques for training \acp{NN} is the minibatch gradient descent, where gradients are computed using the samples drawn in mini-batch to approximate the overall gradient, enabling iterative updates of the model parameters. 
The latency for processing a batch is influenced by the GPU's ability to parallelize computations and its memory bandwidth. 
Increasing the batch size generally decreases the latency of processing data samples by parallelizing more computations on multiple processing units. 
Nevertheless, this increases the number of computational operations on the GPU, and hence the power consumption. 

The impact of batch size on accuracy and convergence speed is explored from multiple aspects. \cite{goyal2017accurate, krizhevsky2014one, batch_size_and_optimizers} studied the interdependencies between batch size and other hyperparameters such as learning rate, weight decay, and optimizers. 
\cite{incraese_bs_training}  proposed to increase the batch size during training rather than decreasing the learning rate to leverage the regularization effect employed by larger batch sizes.  \cite{large_batch_size_training} proposed a gradient noise scale metric to predict the most efficient batch size that maximizes the throughput in the next training step. This approach is tailored for distributed learning, where a batch of data is split across multiple devices.
These aforementioned works have not considered the power efficiency of batching on the device. 
%
In contrast, the works in \cite{batch_sizer_inference,batch_dvfs_inference} consider the impact of the batch size on power consumption of inference operations. In particular, a binary search approach is employed in \cite{batch_dvfs_inference} to find an appropriate batch size that accelerates inference and then the GPU frequency is adjusted accordingly to satisfy power constraints on GPU servers. \textit{Most importantly, this approach only considers inference where the statistical impact of the batch size does not exist}.
\cite{zeus} aimed at optimizing the energy efficiency of periodic training jobs (continuously re-executed for incoming data flow) on powerful GPU clusters. To achieve their optimization (which is different from the one we target in this paper), they propose to adjust the batch size, set a power limit on the server, and depend on circuit-level control to select the frequency that satisfies that power limit.
In particular, they obtain Pareto-optimal combinations of batch sizes and power limits that optimize for energy and performance through profiling the whole training job on the server, since this job will be repeated for new data flows. However, this solution can be applied on powerful GPU clusters and not on edge devices with limited resources. 

\textit{
In summary, none of the state of the art has addressed the joint selection of the batch size and GPU frequency to accelerate training at edge devices under power constraints.}