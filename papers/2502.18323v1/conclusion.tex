\section{Conclusion}
In this work, we propose a power-aware training method aimed at accelerating training on power-constrained GPU devices. Our results show that great savings can be achieved in terms of training time and energy consumption, when carefully and jointly selecting the system and application parameters for training. This is achieved without scarifying the training model quality. 
The proposed solution is applicable to a wide range of models (including, but not limited to, CNNs and transformers). 


Sustainability is one of the major issues when it comes to large scale adoption of AI services. Our solution can be employed to make training over such systems more energy efficient, reducing its carbon footprint. It can also be used to integrate the renewable energy resources deployed near the edge devices, making the training process even greener, i.e., by adapting the GPU frequency and batch size at the devices to the level of available green energy. 

A limitation of this work is that we assume that the power constraint on a device is constant and does not change through the training period. For longer training jobs that can take multiple days, this assumption will not hold. For future work, we want to extend our study to evaluate varying power constraints and distributed learning settings where heterogeneous devices with different constraints contribute to the training.