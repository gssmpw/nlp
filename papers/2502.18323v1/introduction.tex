\section{Introduction}

Recent trends are moving toward the deployment of deep learning models on edge devices due to the need for real-time decision-making, reduced communication cost, and privacy compared to offloading computation to cloud-based servers \cite{object_detection_on_mobile, dl_on_mobile, dl_on_edge_survey, object_detection_edge}. Low-latency inference is essential in applications such as autonomous vehicles, robotics, surveillance, and smart agriculture. To meet these requirements, edge devices are equipped with compact GPUs optimized for computer vision and deep learning inference tasks \cite{embedded_gpus, pedestrian_gpus, agriculture_gpus, object_tracking, lstm_mobile_gpus, object_detection_edge}. Such devices are often power constrained, running on batteries with limited power outputs. 


Many applications on edge devices involve sensitive or private information that cannot be shared due to regulatory compliance or ethical concerns, making it challenging to maintain accurate models without access to this data. %Therefore, it is hard to train an accurate specialized model in such applications. 
Training on the edge offers adaptability to the data available on these devices while reducing the risk of exposure during transmission. Transfer learning is thereby applied for on-device training, where a model is pre-trained with a large (public) dataset and fine-tuned on-device on the target domain \cite{tinyTL, MobileTL, under_256k_memory, on_device_gradient_filtering}. While this offers less training time compared to training from scratch, transfer learning is still a computationally demanding task utilizing higher power compared to inference.





To accelerate training, usually large batch sizes for mini-batch gradient descent are used. However, increasing the batch size increases the power consumption which is limited at edge devices. To enforce the power constraint, the circuit-level control will select the frequency level that satisfies this constraint considering the worst-case computation scenario. Intuitively, this will decelerate training: in the state of the art, \emph{the system-level and application-level (training) parameters are set independently from each other}, and \emph{this is sub-optimal} as will be demonstrated in the following example. 

\begin{figure*}[tb!]
    \centering
    \includegraphics[]{3dLegend.pdf}
    \begin{subfigure}[b]{0.49\textwidth}
    \centering
    % Define custom legend image for rectangle
        \includegraphics[]{Power3d.pdf}
        \caption{Power}
        \label{fig:3d_power_freq_constraint}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[]{Time3d.pdf}
        \caption{Time}
        \label{fig:3d_time_freq_constraint}    
    \end{subfigure}
    \caption{Peak power and time for training a set of samples across different batch size and GPU frequency combinations. The gray plane represents the power limit in the left figure, and the black dots in both figures are the feasible combinations that can be utilized under that constraint. The black circle represents the operating point with maximum feasible frequencies for the batch sizes of 128, which will be selected by the state-of-the-art techniques. The green circle represents an operating point at batch size 64, that could be selected when the frequency and the batch size are jointly selected to accelerate training under the power constraint. Selecting this operating point accelerates training by $31.9\%$ (see Time curves).}
    \label{fig:3d_profiling}
    
\end{figure*}

\Cref{fig:3d_profiling} shows the power and time required to train a ResNet18 model on Nvidia Jetson Nano with~$4096$ of CIFAR10's training samples, while using different combinations of the GPU's frequency and batch size.
%We observe that the power and the training time depends on both the processing frequency ($f$) of the GPU and the training batch size ($b$). 
Following the state of the art, and for a power constraint of~$\SI{5}{\watt}$, the system would select the GPU's frequency~$f=\SI{307}{\mega\hertz}$ and the batch size $b=128$\footnote{We select the maximum batch size which is allowed by the memory at the device.}. 
When we look at the training time, this selection is sub-optimal. In particular, we could choose a smaller batch size ($b=64$), allowing the GPU to operate at a higher frequency ($f=\SI{460}{\mega\hertz}$), and thus reducing the training time by~$31.9\%$.  
\emph{This shows that to accelerate the training on low-power edge devices, both the GPU frequency (a system parameter) and the batch size (an application parameter) should be jointly selected}. Further, a training process is more complex: a model is trained over multiple iterations until a target accuracy is reached. This adds a new dimension (accuracy) to the problem which is not included in this example. We will explore this in detail in Section~\ref{sec:problem_statement}. 

We propose a cross-layer methodology that enables joint optimization of the system and the application parameters to accelerate training under device power constraints. Our solution involves offline profiling of the \ac{NN} training on the device, measuring power and time requirements for various batch size and GPU frequency combinations. Additionally, we estimate the relationship between the batch size and the required number of training samples to reach a target accuracy through training on a proxy dataset on a server. By integrating both aspects, we select combinations that minimize the total training time while adhering to the given power constraints. Our cross-layer control of power consumption can exploit operating points from the design space which were not explored by the state of the art, significantly reducing the training time to accuracy. 
 In summary, we make the following contributions:
\begin{itemize}
    \item We demonstrate that the joint adjustment of the GPU frequency and the batch size given a power constraint is essential for accelerating on-device training.
    \item We propose a novel cross-layer methodology that aims at accelerating on-device training while adhering to the given power constraint of the device.
    \item Our evaluation on real hardware using different models (Convolution Neural Networks (CNNs) and transformers) and datasets shows a significant improvement in the training speed by over~$2.4\times$ compared to baselines that employ state-of-the-art techniques. We also observe a significant reduction in the total energy used for the training at the device, decreasing the carbon footprint of the process.  
    \item We provide a comprehensive sensitivity analysis that demonstrates the robustness of our solution towards the proxy dataset selection, validating the practicality of our approach and ensuring its effectiveness.
\end{itemize}