\section{Results}
\label{sec:results}
In this section, we evaluate the training time and energy consumption for finetuning vision and text tasks on Nvidia Jetson Nano and TX2NX. Additionally, we assess the effectiveness of our approach by conducting a sensitivity analysis on the selection of the proxy dataset.

\subsection{Experiment Setup}

\paragraph{Datasets and models:}
We evaluate our approach for the image object classification and next-word prediction tasks. For image classification, a model is pre-trained with the full CIFAR100 \cite{cifars_cite} and to be trained on subsets (i.e., quarter) of SVHN \cite{SVHN_cite} and CINIC \cite{cinic_cite} datasets on the device. We use a subset of CIFAR10 \cite{cifars_cite} as the proxy dataset on the server. For all datasets%\footnote{SVHN is licensed under CC. CIFAR10, CIFAR100, and CINIC are under MIT license.}
, the input images are of size 3 $\times$ 32 $\times$ 32. Each dataset (subset) is divided into training and testing sets with an 80/20 split, with target accuracy evaluated on the test split. We evaluate ResNet18 \cite{resnet_cite} and MobileNetV2 \cite{mobilenet_cite} models (widely adopted on edge devices), where both of them are trained with Adam optimizer \cite{adam_cite}. A summary of the experimental settings is provided in \Cref{table:experimet_setting}.

For the next character prediction task, we evaluate a 6 layer transformers with 6 attention heads per attention block, 256 embedding dimensions, and a sequence length of 64. We use AdamW \cite{Adamw_cite} as an optimizer. We pre-train the model on WikiText-2 dataset \cite{wiki_text_cite}, utilize tiny shakespeare \cite{tiny_shakespeare} as a proxy dataset, and train it on some Jane Austin and Charles dickens novels\footnote{We used the text for works of Jane Austin's from nltk package \cite{nltk_library_cite} and downloaded Dickens works from project Gutenburg. More details are provided in \cref{appendix:text_datasets}}. We use $90\%$ of a dataset for training and the rest for testing.  For all datasets, we fix the vocabulary to include only words with English letters, digits, punctuation, spaces, and new lines. We set the target character level accuracy for Austin and Dickens at 62\% and 61\%, respectively. 

\input{experiments_setting_table}
\input{power_limit_cifar_proxy_std_2}

\paragraph{Batch size and learning rate:} The choice of appropriate learning rate and batch size are often intertwined, as they impact each otherâ€™s effectiveness. Larger batch sizes provide more stable gradient estimates, potentially permitting the use of higher learning rates. 
Therefore, to preserve the performance of deep models with different batch sizes, we apply learning rate scaling (i.e., square root scaling \cite{krizhevsky2014one} for Adam and AdamW). For ResNet18, the batch sizes ranged from~4 to~128, consisting exclusively of powers of two. The initial learning rate of~$5 \times 10^{-4}$ is used for the largest batch size of~128 (with learning rates scaled for other batch sizes). The same setup was also applied to MobileNetV2; however, the batch size of~128 was omitted due to memory constraints.
For transformers, we similarly consider batch sizes of 4 to 128 with a learning rate of $1 \times 10^{-3}$ for the batch size of 128. 


\paragraph{Hardware and power limits:} 

We evaluate our method on Nvidia Jetson Nano with \qty{4}{\giga\byte} memory on three scenarios. In the first and second scenarios, the power limits at the device are set to $P_{\text{max}}^{1} = \SI{4.5}{\watt}$ and $P_{\text{max}}^{2} = \SI{7}{\watt}$. In the third, the device operates without any power limits, denoted as N/A.  To show that our solution and our results are not device specific, we also provide evaluation on another device (i.e., Nvidia Jetson TX2NX). We use PyTorch 1.10 \cite{pytorch_cite} for Jetson Nano and TX2NX. For pretraining and proxy datasets' training we use NVIDIA A6000 GPU with Pytorch 2.1.

\paragraph{Comparison baselines:} We compare our approach to the following baselines that depend on state of the art techniques:
\begin{itemize}


    \item \textbf{Baseline 1:} The state of practice is to use the largest~$b$  that can fit into memory \cite{goyal2017accurate, ecsa-10-16202}, where the latter use edge GPUs. For the three power limits, we use $\SI{307}{\mega\hertz}$,  $\SI{614}{\mega\hertz}$, and $\SI{921}{\mega\hertz}$ as upper-bound operating GPU frequencies for the device. These are determined based on profiling training of different models and selecting the frequencies that assure a power limit is satisfied irrespective of what model or $b$ is used for the training.
    
    
        %We select the recommended frequency on the device for the power limit.
    %We select the value of $f$ that minimizes $T_s$ for this $b$. 
    \item \textbf{Baseline 2:} We select the value of~$b$ that minimizes~$N_{{s}_{\text{acc}}}$ on the proxy dataset, but we use the same GPU frequencies as in Baseline 1, so no joint optimization is applied. 
    %with the recommended frequency on the device for the power limit. 
    %This baseline solely reflects knowledge of the learning process without considering device-specific characteristics.
    \item \textbf{Fastest configuration:} This baseline serves as an \textit{upper bound} where optimal~$f$ and~$b$ are selected. To determine this, we train the given model on the target dataset on device with all batch sizes
to get~$N_{s_{\text{acc}}}$, substitute in \cref{eq:time_to_acc2} and finally select the best $b$ and $f$ configuration on the device. 
\end{itemize}
We repeat every experiment with different five seeds for image classification tasks and three seeds for next character prediction and record the mean and standard deviation.

%\input{power_limit_cifar_proxy_std_2}
\input{power_limit_transformers_pretrained}

\subsection{Training Time Evaluation}
\label{subsec:training_time_evaluation}


\Cref{table:comparison_with_baseline_nano} and \Cref{table:comparison_with_baseline_nano_transformers} report the training time comparison of our approach and the three baselines on Nvidia Jetson Nano. \Cref{table:comparison_with_baseline_tx2nx} provides additional comparison for image classification task on Nvidia Jetson TX2NX.

Firstly, we start with the evaluation of image classification tasks reported in \Cref{table:comparison_with_baseline_nano}. For ResNet18 training, our method consistently outperforms baseline~1, achieving~$1.5-2.4\times$ reduction in training time. It also outperforms baseline~2 with~$1.4-1.7\times$ less training time. These two baselines are only able to explore a small subset of the design space, potentially missing the optimal configuration. 

Finally, compared to the fastest configuration, our method selects the same configurations in most of the evaluations, while in the case of a different selection, a minor performance decline (of $3\%$) is observed. 

Training of MobileNetV2 has three distinct characteristics: lower peak power, better parallelization for larger batch sizes, and different ~$\mathbf{r}$. %For the N/A scenario, our method performs better than baseline~1 with minor differences in training time since both select large batch sizes and maximum $f$ utilizing the power.
Our method adapts to that by selecting larger batch sizes (and frequencies) than those selected for ResNet18. Compared with baseline~1, our method performs better with minor differences in training time since both select large batch sizes and maximum $f$ utilizing the power.
In low-power, the gain from our approach increases as the selected batch size enables higher frequencies, reducing training time by up to~$1.4\times$. Baseline~2 misses the opportunity to use higher frequencies, especially as MobileNetV2 consumes less power. Ultimately, our method selects the fastest configurations when training MobileNetV2 in all power-limits scenarios.

Next, we examine the next character prediction training. As shown in \Cref{table:comparison_with_baseline_nano_transformers}, our method improves the training time by $1.15 \times$ and $2.14\times$ compared to baseline $1$ and $2$, while choosing the same configurations as the fastest configuration. 


In \Cref{table:comparison_with_baseline_tx2nx}, we compare our solution with the baselines for ResNet18 training on Nvidia Jetson TX2NX, where the device receives $\mathbf{r}$ for ResNet18 training from the server. For training on SVHN, our method outperforms both the baselines $1$ and $2$, reducing the training time by $1.7\times$ and $1.8\times$, respectively. Furthermore, it selects the same configuration as the optimal one. Similarly, when training on the CINIC dataset, baselines $1$ and $2$ require up to $1.94\times$ and $2.04\times$ more training time compared to ours. Our method did not select the exact optimal configuration in this scenario; however, it selects a near-optimal configuration that results in only $2\%-8.2\%$ more training time compared to optimal.  

In summary, our method outperforms existing baselines, reducing training time to accuracy across various model architectures, tasks, and hardware.
%\input{power_limit_cifar_proxy_std}
\input{power_limit_tx2_resnet18}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{\textwidth}
    \hspace{16mm}
    \includegraphics[]{energy1.pdf}
    \end{subfigure}
    \vskip \baselineskip \hspace{1mm} 
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[]{energy_2_1.pdf}
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[]{energy_2_2.pdf}
    \end{subfigure}
    
    \vskip \baselineskip \hspace{1mm} 
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[]{energy_3_1.pdf}
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.40\textwidth}
        \includegraphics[]{energy_3_2.pdf}
    \end{subfigure}

    \caption{Energy consumption comparison between our approach and baseline methods during training under three power constraint scenarios. The recorded data includes training ResNet18 and MobileNetV2 on the SVHN and CINIC datasets, as well as a transformers network on the Austen and Dickens dataset, all performed on an Nvidia Jetson Nano.}
    \label{fig:energy_comparison}
\end{figure*}

\subsection{Energy Evaluation}
We investigate the energy efficiency of our proposed approach for the image object classification and next character prediction tasks in \Cref{fig:energy_comparison}. 

For the image classification task, it is noticeable that our method always outperforms baseline~1 since it takes less training time given the power limits. In addition, selections made by baselines~1 and~2 can lead to up to~$2\times$ and~$1.4\times$ more energy usage compared to our method, respectively. It is important to state that minimizing the training time does not always mean minimizing energy consumption. An example is comparing the energy consumption for~$P_{\text{max}}^{2}$ on ResNet18 with the no power limit N/A. Despite having lower training time in N/A (as shown in \Cref{table:comparison_with_baseline_nano}), training under~$P_{\text{max}}^{2}$ is more energy efficient since the used frequencies for~$P_{\text{max}}^{2}$ provide a better tradeoff between performance and power. The same applies for Baseline~2 training MobileNetV2 under ~$P_{\text{max}}^{2}$, that uses a lower frequency with the same batch size of our approach. 

For the next character prediction task, our approach uses $9\%$ less energy for $P_\text{{max}}^{1}$ and nearly the same energy consumption for $P_\text{{max}}^{2}$ while training in less time (the same configuration selection for $P_\text{{max}}^{3}$) compared to the baseline $1$. In addition, our method records $1.25\times$ less training energy for the baseline $2$.




\begin{figure*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{subfigure}[b]{0.6\textwidth}
        \includegraphics[]{mobile_net_p1.pdf}
        \end{subfigure}
        \hspace{0.3mm}
        \begin{subfigure}[b]{0.6\textwidth}
            \includegraphics[]{mobile_net_p2.pdf}
        \end{subfigure}
        \hspace{0.3mm}
        \begin{subfigure}[b]{0.6\textwidth}
            \includegraphics[]{mobile_net_na.pdf}
        \end{subfigure}
    }
    \caption{Confusion Matrix of time increase percentages to the fastest configuration for image classification datasets on Nvidia Jetson Nano across three power constraints for MobileNetV2 training. The rows represent the selected proxy dataset while the columns represent the target datasets where training on edge is conducted on. The results indicate that the proposed method is not sensitive to the selection of proxy dataset. }
    \label{fig:confusion_matrix_mobilenet}
\end{figure*}
\begin{figure*}
    \centering
    \resizebox{0.85\textwidth}{!}{
        \begin{subfigure}{0.29\textwidth}
        \includegraphics[]{transformer_p1.pdf}
        \end{subfigure}
        \hspace{2mm}
        \begin{subfigure}{0.29\textwidth}
            \includegraphics[]{transformer_p1.pdf}
        \end{subfigure}
        \hspace{2mm}
        \begin{subfigure}{0.29\textwidth}
           \includegraphics[]{transofrmer_na.pdf}
        \end{subfigure}
    }
    \caption{ Confusion Matrix of time increase percentages to the fastest configuration for next character prediction on Nvidia Jetson Nano across three power constraints for transformers training. The rows represent the selected proxy dataset while the columns represent the target datasets where training on edge is conducted on. The results indicate that the proposed method is not sensitive to the selection of proxy dataset.}
    \label{fig:confusion_matrix_transformers1}
\end{figure*}

\subsection{Proxy Dataset Analysis}

We conduct a sensitivity analysis for the selection of the proxy dataset for image classification and next character prediction tasks by testing all possible proxy and target dataset combinations. 

For the image classification task, we add two additional datasets namely, Fashion MNIST \cite{fashion_mnist_cite} (FMNIST)  and STL \cite{STL_cite}. The tasks include object, digit, and fashion classification, with dataset sizes ranging from $5K$ to $22.5K$ samples. We provide in \cref{fig:confusion_matrix_mobilenet} a confusion matrix showing the percentage of time increase for the choice of proxy dataset selection compared to the fastest configuration for different datasets for training MobileNetV2 on Nvidia Jetson Nano. The results show that our approach selects the correct configurations in most of the cases with only a few instances where the optimal configuration is not selected in no power constraint scenario. The same observation is also applied to ResNet18 as shown in \Cref{fig:confusion_matrix_resnet18} (\cref{appendix:proxy_dataset_extension}). For the next character prediction, we provide the confusion matrix in \Cref{fig:confusion_matrix_transformers1}. The optimal configuration is selected regardless of the select proxy dataset.


To sum up, the results show that our approach is not sensitive to the selected proxy dataset, demonstrating the practicality of the proposed solution.

