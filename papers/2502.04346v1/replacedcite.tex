\section{Literature Review}
\label{literature}

This section reviews the current state-of-the-art (SOTA) research on multi-lingual threat detection on Twitter/X. Although ML, DL, and LLMs have widely reached the depths of multiple disciplines and achieved commendable results, multi-lingual threat detection ceases to remain one of them. This gap is particularly concerning given the current rising prevalence of cross-country cyber threats and social media based cyber attacks.

Rehan et al. ____ claimed to be one of the first to offer multi-lingual threatening text detection on Twitter with LLMs. They achieved this by first translating English text into their Urdu corpus and then working on the Urdu language with their AI implementation. The authors fine-tuned RoBERTa with 1,313 English and 2,400 Urdu samples. The sample of English threats is highly skewed with only 128 non-threat messages out of the given 1,313 samples. The authors also chose standard ML algorithms like Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), Convolutional Neural Networks (CNN), Bi-directional Long Short-Term Memory (Bi-LSTM) with RoBERTa and Word2Vec approaches to test their approach. Although they have shown exceptional results with over 91.89\% accuracy, the definition of multi-lingual classification for the papers includes just English and Urdu. Our paper overcomes this shortcoming by providing a more quantifiable and diverse approach that can work in English, Arabic, Russian, and Chinese; all of which, according to the CIA's The World Factbook ____, are in the top 10 most-spoken first languages around the world. This limitation in detecting multiple language to detect cyber threat can lead to catastrophic cyber attack vectors where the exploiters simply make use of the standing language and its essence barriers.

Apart from this, there exists very little literature on multi-language threat detection on Twitter tweets specifically. However, there are some related research works in the field which indirectly address the theme of this paper. Tundis et al. ____ provide a multi-language approach towards identification of suspicious users on social network platforms. Although their approach also relies on using platforms like Google Translate, Yandex, and Bing Translate, the authors deduce a similarity score. The authors don't mention a general formula to calculate the similarity score, but we can deduce it to the following from their examples.


\begin{equation}
\begin{aligned}
    S_{N_{max}} = maxSimilarity(&\sum_{i=1}^{n} (R_{1i} \times S_{1i}); \\
    &\sum_{i=1}^{n} (R_{2i} \times S_{2i}); \\
    &\sum_{i=1}^{n} (R_{3i} \times S_{3i}))
\end{aligned}
\label{eq:max_similarity}
\end{equation}

where $S_{ij} \in [0,1]$, $i$ and $j$ $\in \{1,2,3\}$, is the similarity score of all tweets between the 3 service providers, (i.e, $S_{12}$ denotes similarity between service 1 and service 2) and $ R_{ij} \in {0,1}$ is the binary indication that gives if the similarity score of $S_{ij}$ is associated with the service or not. 


From evaluation of Equation \ref{eq:max_similarity}, it can be evident that the function grows exponentially as i and j grow, i.e., adding more service translators for languages that may not be best performed by the given three translators. We again argue for the need of a language-dependent corpus system because some essence of the original message could be lost within translation. Furthermore, the authors only apply naive approaches like Bag of Words (BoW) alongside bi-gram and tri-gram versions of N-gram technique. This provides a more statistical analysis of whether the profile of some users could be dangerous or not rather than a prediction with a finely labeled dataset and advanced ML and DL algorithms. We overcome this drawback by using a finely labeled dataset which gives robust prediction against tweets that contain cyber threats.


\begin{table*}[ht]
\renewcommand{\arraystretch}{1.3}
\caption{Summary of Multi-lingual Threat Detection/Analysis Studies}
\scriptsize
\label{tab:literature-summary}
\centering
\begin{tabular}{l ccc c ccc cccc c}
\hline
\multirow{2}{*}{\textbf{Study}} & 
\multicolumn{3}{c}{\textbf{Data Source}} & 
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Number of\\Classes\end{tabular}}} & 
\multicolumn{3}{c}{\textbf{Models Used}} & 
\multicolumn{4}{c}{\textbf{Metrics}} & 
\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Content Language\end{tabular}}} \\
\cline{2-4} \cline{6-12}
& \textbf{Tweet} & \textbf{FB} & \textbf{IG} & 
& \textbf{ML} & \textbf{DL} & \textbf{LLM} & 
\textbf{Acc.} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \\
\hline
____ & \checkmark & \ding{55} & \ding{55} & 2 & SVM, LR, RF & CNN, BiLSTM & RoBERTa & \checkmark & \checkmark & \checkmark & \checkmark & English, Urdu \\
____ & \checkmark & \checkmark & \checkmark & 2 & BoW, NGram & \ding{55} & \ding{55} & \checkmark & \ding{55} & \ding{55} & \ding{55} & Multi-lingual \\
____ & \checkmark & \checkmark & \ding{55} & 2 & SVD & BERT & \ding{55} & \checkmark & \checkmark & \checkmark & \ding{55} & English, French \\
____ & \checkmark & \ding{55} & \ding{55} & 4 & \begin{tabular}[c]{@{}c@{}}SVM, RF\\LR, DT, NB\end{tabular} & \ding{55} & \ding{55} & \checkmark & \checkmark & \ding{55} & \ding{55} & English \\
____ & \checkmark & \checkmark & \ding{55} & Multi & \ding{55} & \ding{55} & LLaMA & \checkmark & \ding{55} & \ding{55} & \ding{55} & English, Hindi, Arabic \\
____ & \checkmark & \ding{55} & \ding{55} & 3 & \ding{55} &  BERT & RoBERTa, GPT-3 & \checkmark & \ding{55} & \ding{55} & \ding{55} & Multi-lingual \\
____ & \checkmark & \checkmark & \ding{55} & 5 & \ding{55}& \ding{55} & XLM-RoBERTa  & \checkmark & \ding{55} & \ding{55} & \ding{55} & Bengali \\
Our work & \checkmark & \ding{55} & \ding{55} & Multi & LR, DT, RF & RNN, LSTM, GRU & XLM-RoBERTa  & \checkmark & \checkmark & \checkmark & \ding{55} & Eng., Russian, Chinese, Arabic \\
\hline
\end{tabular}
\begin{tablenotes}


\item[a] \textbf{Abbreviations:} X = Twitter, FB = Facebook, IG = Instagram, ML = Machine Learning (SVM = Support Vector Machine, LR = Logistic Regression, RF = Random Forest, DT = Decision Tree, NB = Naive Bayes, BoW = Bag of Words, NGram = N-gram, SVD = Singular Value Decomposition), DL = Deep Learning (CNN = Convolutional Neural Network, BiLSTM = Bidirectional Long Short-Term Memory, BERT = Bidirectional Encoder Representations from Transformers, XLM-RoBERTa = Cross-lingual RoBERTa), LLM = Large Language Model (GPT-3 = Generative Pre-trained Transformer 3).
\end{tablenotes}
\end{table*}

In other similar works, Chiril et al. ____ comparatively studied and experimented with several methods and models to detect multi-lingual hate speech towards multiple targets. Their dataset contains 13,071 English and 3,085 French tweets classified into hate and non-hate tweets towards immigrants and women. They achieve precision of 0.78 and 0.66 in the two tasks respectively. The authors, however, do not train on more variety or advanced techniques like LLMs or GRUs. The benefit of not implementing a translation layer for multi-language system can be observed in this study as even with simple techniques like dimensionality reduction and singular value decomposition the authors were able to get good results in their dataset. Hussein et al. ____ did multi-class classification of tweets on Threat (8,280), Business (2,331), Irrelevant (6,598) or Unknown (4,159); number of samples in brackets. They experimented with SVM, Random Forrest (RF), Logistic Regression (LR), Decision Tree (DT), Naive Bayes (NB), and K-Nearest-Neighbor (KNN) algorithms and got the best result with RF classification with a precision of 74 and accuracy of 67. Similar to the previous literature, we conclude we could use more advanced methods and models to bridge the research gap presented by the findings of this article as well. 


In the context of analyzing text for cybersecurity threats, cyberbullying or hate speech by using SoTA LLMs, Kmainasi et al. ____ fine tuned Llama to LlamaLens which is a multilingual LLM for analyzing news and social media content. Their dataset comtains ~2.7 million samples and over 222 labels, all of which is an amalgamation of 103 dataset consisting multiple social media post, news article, political debates and transcripts. Out of the three language tested: English, Hindi and Arabic, cyberbullying was the only common category of interest in all three languages where the authors gained accuracy of 90.07, 60.90 and 86.30 respectively. Hindi and Arabic had two common hate speech categories. Offensive speech was yet another common category in all three of them, but the definition of the term itself could be ambiguous.  Miah et al. ____ worked with ensemble learning of transformer and LLM for multi-lingual sentiment analysis. Although they work on five languages: Arabic, Chinese, English, French and Italian, their method depend upon a translation layer as that of ____. The author's ensemble consisted of Twitter‑Roberta‑Base‑Sentiment‑Latest, bert‑base‑multilingual‑uncased‑sentiment, and GPT‑3 and produced an accuracy of 86\% on all languages. Although the authors argue that sentiment analysis is possible through translation to English given their ensemble method achieved the accuracy, but it's not the effective way to do so. We show that by using a language specific corpus and model we can handle nuanced information within the language like slang, irony, and sarcasm away from the actual threats present in the dataset.


Hirdi et al. ____ utilized different BERT based method to detect offensive behaviour on low resource language Bengali. They trained XLM-RoBERTa-base with 44,000 comments from social media platforms and gained an accuracy of 83.54\%. This accuracy is lower than ____, but the authors worked with a different language with not as many resource for accurate translation and also accounted for Banglish which is a combination of Bengali and English; either Bengali written in romanized English form or a mix of both language into a single sentence. This practice of writing is common in many foreign language system which don't use the roman alphabet as English alphabets can be used to produce similar sounds as the other characters in Arabic, Chinese, Devanagari or Bengali scripts ____________. The authors divided the remarks in the 44,000 comments into one of five categories: Sexual, Not bully, Troll, Religious, Threat. The authors employed technique to translate English words in a Bengali text to Bengali to fit for this corpus which is a better approach than translating the entire text from one language into another. The author's work in the Bengali language is commendable. Their work highlights the importance of language understanding by constructing methodology for Bengali which is not written in the roman alphabet and use the finer details observed to determine the level of threat present in the given tweet/post. We do argue that their approach could be expanded to cover bases for multiple languages which our research touches upon. 


Table \ref{tab:literature-summary} shows a details comparison between our work and previous research. Our work adds contribution to existing literature and offers a deeper research into classifying tweets for safer online interaction by incorporating a comprehensive analysis of all ML, DL and LLM algorithms in 4 of the top 5 most commonly spoken first language.