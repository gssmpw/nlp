
%Our goal is to demonstrate that our model can extract more enhanced sequential features compared to an RNN-based model. To make a Transformer model more effective, we use self-supervised learning on the model.
%
%Within the multi-\textcolor{red}{modality integration}, InfoNCE loss is employed to enhance the features by learning the features from the other view of a data sample in epoch-level Transformer and CNN backbones.
%
%In addition, to leverage a more generalized model, a self-supervised learning technique is also adopted at the sequence level training. The detailed performance comparison with the model architectures and training schemes is summarized in Table \ref{table5}. \textcolor{red}{Additionally, we can see the comparative analysis of overall sleep stage classification performance against other model's details in Table \ref{table2}.}


% At the epoch level, which aligns paired samples closely in the representation space while keeping unpaired samples apart in a single sequence. In doing so, we freeze the backbone layers and reconcile this by employing a masking technique at the sequence level that predicts corrupted sequence tokens, drawing on the strengths of cross-view information without the conventional clash. This pioneering integration allows our model to capture proficient representations with equal effectiveness in both smaller and larger datasets, demonstrating the synergy of these once-disparate techniques. 

\subsection{Datasets and training setup}  %%%%%%%%%%%%% Dataset 4-1

\input{Table/table1}

\subsubsection{Datasets:} \label{section4_dataset}
Two publicly available datasets are employed to assess the performance of the proposed model: SHHS and SleepEDF-78.
%
The SHHS database contains massive PSG records from 5,793 subjects aged 39-90. 
%
We use the preprocessing guidelines of the previous study and we utilize data from 5,791 patients in the SHHS dataset (it is named `SHHS 5791'). Additionally, we excluded the recordings that do not include all five sleep stages while following the approach detailed in \cite{SORS2018107} and the dataset is named `SHHS 5463'. 
%
%\textcolor{red}{Moreover, to ensure uniformity in experimental conditions, we exclusively preserved the 30-minute Wake stages before sleep onset and after sleep conclusion, specifically around the transitions into and out of the NREM/REM sleep stages.}
%
We validate the dataset under two conditions: one with 5,791 patients and another with 5,463 patients. Detailed information on the two datasets is presented in Table \ref{table1}.

We use the SHHS dataset to test if our model works well with a large-size dataset. The PSG records of 100 patients are excluded from the dataset for validation and then the remaining dataset is divided into training data and testing data with a partitioning ratio of 70:30. 
%
On the other hand, the SleepEDF-78 contains 78 healthy Caucasian subjects aged 25-101, and it has smaller PSG records than those of the SHHS.
We use the SleepEDF-78 as our small dataset to test if our model works well on a small dataset. We validate our model using the SleepEDF-78 through 10-fold cross-validation. % after dividing the dataset by 90\% for \textcolor{red}{cross-validation} and 10\% for validation while applying held-out validation results, which excludes 7 random patients for each round. 
%Finally, to avoid the bias of the majority class, we clip the front and back 30 Wake signals.

%\subsection{Experimental setup} %%%%%%%%%%%%%%% 4-2

%Our preprocess mechanism follows the conventional rules to make a more concise experiment result. Even though \cite{lee2024sleepyco} shows superior results than other works, there was a difference between \cite{lee2024sleepyco} and \cite{phan2021xsleepnet} training samples such as clipping 30 minutes of wake signals or filtering noise signals. Our purpose is to improve the representation of the model, so we adopted\cite{phan2021xsleepnet} preprocessing method. Our specific training samples are listed in Table \ref{table1}.

\subsubsection{Training setup:} In all experiments, the networks were trained using an Adam optimizer \cite{kingma2017adam} with the learning rate ($lr$) of $5 \times 10^{-4}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$ and the weight decay of $1 \times 10^{-5}$ to prevent over-fitting problems. We adopt $B=32$ as mini-batch sizes in the training. Early-stopping is used in our training.
% Early stopping을 얼마나 적용했는지 넣어야 할 것 같아서 파란색 처리했습니다.
We validate the model every 100-th iteration of the SleepEDF-78 and 500-th in SHHS datasets. We also used the early-stopping when the model didn't update during 1000 periods.% Using 10 times early-stopping methods.

When conducting contrastive learning, it is important to apply augmentation techniques properly.  As augmentations for our contrastive learning, we adopt one of \emph{Amplitude Scaling}, \emph{Amplitude Shift}, \emph{Add Gaussian Noise}, \emph{Band Stop Filter}, \emph{Time Shift} and \emph{Zero-Masking} schemes for raw signals. 
% selecting only one with 0.5 probability. 
We also employ an augmentation for the spectrogram data but only a simple \emph{Random Noise} augmentation is used for the spectrogram since improperly used augmentations can introduce erroneous modifications that may degrade the quality of features.


\subsection{Experiment result analysis} %%%%%%%%%%%%%%%%%%%%%%%% 4-3 
% 우선, 우리 모델의 선택의 근거를 보여주기 위해, 우리는 XsleepNet과 우리 모델의 from-scratch로 학습하였을 때의 모델의 성능을 비교하였습니다. \ref{table3}. CNN + RNN으로 구성된 XsleepNet의 Sequence level을 Transformer로 변환 후 성능 및 모델의 크기를 비교해보면, The modified XSleepNet (XSleepNet+TF) has 0.8\% accuracy improvement but the model size increases by 8\%. 입니다. 비록 Transformer의 사용으로 XsleepNet 보다 모델 파라미터가 커졌더라도, 비슷한 크기의 사이즈를 맞추기 위해 Epoch level의 파라미터터 수를 줄이는 작업을 진행하였습니다. 우리가 제안하는 모델을 바탕으로 Epoch level 수정 후, 성능 및 모델 파라미터의 차이를 조사하였을 떄, resulting in a slightly larger model size (0.3\% size increase) compared to the XSleepNet. On the other hand, the proposed model achieves a 1.2\% improvement in accuracy compared to the SOTA performance.
%
% To demonstrate the effectiveness of our MC$^2$SleepNet, we compared the performance of our model with the XSleepNet that is a {\it state-of-the-art} (SOTA) model with SleepEDF-78 as shown in Table \ref{table3}. 
%
%\textcolor{red}{
%The XSleepNet which is composed of a CNN and an RNN, was modified by replacing its sequence-level RNN with a Transformer, leading to an analysis of both performance and model size differences. This modification (XSleepNet+TF) resulted in a 0.8\% improvement in accuracy, though it also led to an 8\% increase in the model size. Despite the increased model parameters due to the use of a Transformer, we managed to adjust the epoch-level parameters to align the model sizes closely. Our epoch-level architectures are detailed in Section \ref{3-1_epoch_level_training}. Upon revising the epoch level and examining the differences in performance and model parameters, our proposed model exhibited a negligible increase in size (by only 0.3\%) compared to XSleepNet. Despite this minor size augmentation, it achieved a notable 1.2\% improvement in accuracy over the SOTA performance.
%}
%XSleepNet is composed of a CNN and an RNN, while the proposed model consists of a CNN and a Transformer, resulting in a slightly larger model size (0.3\% size increase) compared to the XSleepNet. On the other hand, the proposed model achieves a 1.2\% improvement in accuracy compared to the SOTA performance. To further explore the scenario where RNN in XSleepNet is replaced by a Transformer, we modified the XSleepNet model and conducted \textcolor{red}{performance} evaluation. The modified XSleepNet (XSleepNet+TF) has 0.8\% accuracy improvement even though the model size increases by 8\%.

The XSleepNet is composed of a CNN and an RNN network while the proposed model consists of CNN and Transformer model resulting in a slightly larger model size (0.3\% size increase) compared to the XSleepNet. On the other hand, the proposed model achieves a 1.2\% improvement in accuracy compared to the SOTA performance. To further explore the scenario where RNN in XSleepNet is replaced by a Transformer, we modify the XSleepNet model and conduct a performance comparison between the two models. Although the model size increases by 8\%, the modified XSleepNet (XSleepNet+TF) has 0.8\% of accuracy improvement.

%\input{Table/table3}

To evaluate the training efficiency of our proposed model, we compare the training time of three models: SleepTransformer, XSleepNet, and ours.
%
Table \ref{table4} shows that the training time of the proposed model is 2.02 (1,005/496) times slower than that of SleepTransformer, whereas XSleepNet demonstrates 2.68 (828/308) times slower than SleepTransformer.
% 오타가 존재하는 것 같아서 여쭤봅니다. 661이 어떤 의미로 쓰였는지 알 수 있을까요?
We predict the runtime of our model on a V100 GPU based on the performance ratio observed when the SleepTransformer is executed on two GPUs (it is 496/308 and {\bf 1.61} speedup can be roughly approximated by changing a GPU from RTX3090 to V100). Our model is expected to run on a V100 GPU with an approximate runtime of 624 (1,005/1.61) seconds, which means 1.32 (828/624) times faster training speed than XSleepNet (828 seconds) while showing better accuracy in both the SleepEDF-78 and the SHHS datasets.

\input{Table/table4}

To investigate the detailed performance characteristics of our proposed methods, we conducted an ablation study and evaluated the performance of the different variants of our model. 
%
In Table \ref{table5}, the `TF only' model refers to a Transformer model, while the `CNN only' model denotes a pure CNN model. These models lack a sequence-level training network and predict a stage class for each epoch solely based on the features extracted from that single epoch, without considering the sequential features between neighboring epochs.
%
Since they only consider a single epoch for their predictions, their accuracy performances are relatively lower. The `TF+CNN(multi)' model incorporates Transformer and CNN backbones with an additional Transformer-based sequence-level training network, allowing it to extract sequential features from multiple neighboring epochs.
%
In this model, multi-modal training is utilized with both backbones and sequential features exploited to predict stages, resulting in higher accuracies. When compared to the single-epoch model, performance improvements are observed (83.5\% for SleepEDF-78 and 87.8\% for SHHS 5463).

`TF+CNN+CL+FT' involves applying contrastive learning and fine-tuning. However, employing contrastive learning alone without the masking strategy results in only a 0.2\% performance improvement (83.5\% $\rightarrow$ 83.7\%) compared to that of `TF+CNN(multi)'. Nevertheless, on larger datasets like the SHHS dataset, it demonstrates a 0.6\% (87.8\% $\rightarrow$ 88.4\%) improvement.
%
`TF+CNN+M+FT' applies a masking strategy without contrastive learning. In this scenario, the performance increases by 0.7\% and 0.6\% on both datasets compared to the `TF+CNN (multi)'.
%
In the case of `TF+CNN+PT (CL+M)' both contrastive learning and masking are utilized during the pre-training step, where the entire model, including the epoch-level backbones and sequence-level networks, is trained simultaneously, without fine-tuning.
%
In this model, exploiting the synergy between contrastive learning and masking leads to performance improvements, as shown in Table \ref{table5}. 
%
Our proposed dual self-supervised learning strategies can achieve, or even surpass, the performance levels obtained by using just a single training method, without the need for a fine-tuning process.

Finally, with the addition of further fine-tuning, the highest accuracy (88.6\%) is observed in `TF+CNN+PT(CL+M)+FT'. It is noteworthy that the separate applications of contrastive learning and masking do not result in performance improvement.


\input{Table/table5}


Controlling the masking ratio is crucial for model accuracy. Increasing the masking ratio to a certain point enhances model accuracy as shown in Table \ref{table6}. Our model shows the highest accuracy (88.495) during the pre-training step when employing a masking ratio (70\%). This suggests that a higher degree of masking plays a crucial role in enhancing the model's ability to learn from incomplete data. % pivotal --> crucial 
%
Overall, the optimal performance (88.593) was observed at the masking ratio of 50\% after fine-tuning. This can be attributed to the fact that after equipping a foundational level of trained knowledge, the model benefits more from a reduced masking ratio. Thus reducing the masking ratio prevents the over-elimination of the extracted feature information and effectively leverages the previously learned patterns. Also, in Fig \ref{fig3}, we see the confusion matrices with the various masking rates. 


% During the pre-training phase, our model demonstrated an increased propensity to compensate for information scarcity by reinforcing the interactions between various modalities, reaching peak accuracy at a masking ratio of 70%.
%
% 정근: 아래 말이 무슨말일까?? Reference가 잘못 달려있습니다. 저 부분이 원래는 간단한 Masking 비율에 따른 F1 score혹은 ACC조절이 가능하다라는걸 보여주고 싶었습니다. 실제로 50%일 때는 Major class인 Wake/N2가, 15%일 때는 N1/N3가 높은 F1 score를 기록하고 있습니당. (table 4) 참조.
%Interestingly, our simple adjusting ratio step changes the importance class tailored to the needs of the user. 

%%%%%%%%%%%%%%%

\begin{figure*}[h]
\centerline{\includegraphics[width = \textwidth]{Figure/Confusionmatrixonvariousdataset_v5.png}}
\caption{\textbf{Confusion matrices for two datasets and masking ratio (M).}
}
\label{fig3}
\end{figure*}

%%%%%%%%%%%%%%%

\input{Table/table6}


% Moreover, the contrastive learning technique alone also shows lower performance when compared to simple pre-training techniques. However, upon combining contrastive learning with masking, we achieved significantly improved performance compared to using the naive combination and masking methods separately.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In Table \ref{table2}, we compare our model with various models to demonstrate the efficacy of our approach. Our model outperforms all the previous works in all sleep stages except for the `Wake' stage in both datasets.  
% 
The ambiguity and similarity between the NREM1 and NREM2 classes pose challenges in sleep staging. It is even challenging to classify them accurately in our model.
%
In general, the NREM2 class occupies a larger proportion of data samples compared to the NREM1 class and it makes the prediction of the NREM1 class highly challenging.
%
Our method outperforms other similar models such as XSleepNet by 0.4\%. More precisely, we exceed performance by 0.7\%, 0.5\%, 1.9\%, and 2.7\% in the NREM1, NREM2, NREM3, and REM classes, respectively, on the SleepEDF-78 Dataset.
%
The `Input' sub-column of the `Method' column of Table \ref{table2} shows what types of data are used. `RS' and 'SP' mean raw signal and spectrogram data, respectively.


\input{Table/table2}

The proposed MC$^2$SleepNet demonstrates improved performance on a larger dataset as well. While L-SeqSleepNet \cite{phan2023seqsleepnet} achieves SOTA performance using a relatively longer sequence length (L=200), Our MC$^2$SleepNet surpasses it while employing a relatively shorter sequence length.
%
Our MC$^2$SleepNet achieves the highest ACC (88.59\%) and MFI (82.3\%) scores when compared to the previous works as presented in Table \ref{table2}.
%
It is noteworthy that the F1 score for the minor classes, NREM1 (3.8\% of the SHHS dataset) and NREM3 (6.7\% of the SleepEDF-78 dataset), are enhanced when a 15\% masking ratio is used. Conversely, the major classes, NREM2 and REM,  achieve best results with a 50\% masking ratio, consistently as indicated by red color in Table \ref{table2}.
% 
%Our approach can improve the accuracy for minor classes in the datasets. By adjusting the masking ratio, 
%The F1 scores are increased for the NREM3 class, which comprises 6.7\% of the SleepEDF-78 dataset, and for the NREM1 class, which constitutes just 3.8\% of the SHHS dataset.
%
By adjusting the masking ratio readily, the proposed model can be tailored partly to prioritize some classes


In Table \ref{table2}, the `+' sign indicates that the models are pre-trained on the SHHS dataset with 5463 patients and a fine-tuning is conducted with SleepEDF-78. On the other hand, to prevent the over-fitting problem in the small size dataset, we conduct only the pre-training step symboled ${\Gamma}$ in the table.