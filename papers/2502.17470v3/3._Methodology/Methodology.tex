Given a training dataset $DS = \{SS_i\}_{i=1}^{N}$ has $N$ samples and each sample, $SS_i$ is an ordered tuple of sequential $L$ epochs.
Then $SS_i$ can be denoted by $(S_{i, j})_{j=1}^{L}$, in which $S_{i,j}$ corresponds to the $j$-th epoch of $SS_i$ and $L$ is set to 21 in this work.
%
The $S_{i,j}$ is a 3-element tuple and it is denoted by $(\text{Sg}_{i,j}, \text{Sp}_{i,j}, {y}_{i,j})$.
%
%\textcolor{red}{These three elements of a tuple correspond to multi-viewed data and ground-truth label for an epoch: a raw signal data $(\text{Sg}_{i,j})$, a spectrogram data $(\text{Sp}_{i,j})$ and ${y}_{i,j}$.}
% 원본: 
The first two elements of a tuple correspond to multi-modality data of an epoch: a raw signal data $(\text{Sg}_{i,j})$ and a spectrogram data $(\text{Sp}_{i,j})$. The last element, ${y}_{i,j}$, is a ground-truth label for the epoch.
%
Specifically, each sample of the training datasets consists of
%
$SS_i = ((\text{Sg}_{i,1}, {\text{Sp}_{i,1}}, {y}_{i,1}), \cdots, (\text{Sg}_{i,L}, {\text{Sp}_{i,L}}, {y}_{i,L})). $
%
The label ${y}_{i,j}$ $\in$ $\{$Wake, NREM1, NREM2, NREM3, REM$\}$ represents a stage class for $S_{i,j}$, $\text{Sg}_{i,j} \in \mathbb{R}^{C \times 3000}$ represents 30-second information across $C$ channels sampled at 100Hz frequency. 
In this work, we consider a single channel EEG so that $C=1$.

The overall model architecture and its layer components are presented in Fig \ref{fig1}. We use a CNN backbone network denoted by $\textbf{F}_{Sg}$ to draw out features from the raw signal data. Another backbone, $\textbf{F}_{Sp}$, for the spectrogram data, is based on a Transformer network.
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Epoch level training}\label{3-1_epoch_level_training} %%%%%%%%%%%%%%%%%%% Epoch level training scheme

In the backbone for raw signal data, following the \cite{perslev2019utime,SEO2020102037,lee2024sleepyco}, we adopt 5 CNN blocks with the following channel configurations: Each of the first two blocks has two convolutional layers, while the subsequent blocks each have three convolutional layers. Maxpool layers with a width of `5' are appended after all the blocks except the first block.
%
In the whole CNN blocks, we adopt a kernel size of `3' and the same padding size with the stride of `1'. 
%
Finally, we implemented a maxpool layer with a width size of `2' to adjust the CNN output dimensions that match the Transformer output dimensions.
%
% The backbone for the raw signal data produces the feature, $\mathbf{Z}^{Sg}_{i, j}$, as $\mathbf{F}_{Sg}(\text{Sg}_{i, j}) \in \mathbb{R}^{\text{batch} \times 5 \times 128}$.

The backbone model function $\mathbf{F}_{Sg}$ produces the feature having the raw signal $\mathbf{Z}^{Sg}_{i, j}$ as its domain, making dimension of the feature to $\mathbf{F}_{Sg}(\text{Sg}_{i, j}) \in \mathbb{R}^{\text{batch} \times 5 \times 128}$.
%
On the other hand, as described in Section \ref{Preliminary}, a spectrogram data sample, $\text{Sp}_{i,j} \in \mathbb{R}^{C \times 129 \times 29}$, undergoes a simple CNN-based resizing layer and then the output of the resizing layer is further augmented with positional encoding vector (PE) to yield $\text{Sp}^*_{i,j} \in \mathbb{R}^{C \times 29 \times 128}$ as defined in Eq. \ref{eq1}.
% 
This resizing aims to exploit the potential benefits of dimensionality being a power of two and provide additional non-linearity.

\vspace{-10pt}

\begin{align}
\label{eq1}
\text{Sp}^*_{i, j} = \text{proj}_{CNN}(\text{Sp}_{i, j}) + \text{PE}
\end{align}

In the backbone of the spectrogram, $\textbf{F}_{Sp}$, a Transformer layer performs a sequence of the computations defined from Eq. \ref{2. attn-1} to Eq. \ref{2. attn-7}.
%%
Specifically, $\text{Attn}(x)$ in Eq. \ref{2. attn-1} uses a set of typical learnable parameters for a Transformer: $\mathbf{W}^K_h$, $\mathbf{W}^Q_h$, $\mathbf{W}^V_h$ $\in$ $\mathbb{R}^{d_{model} \times \frac{d_{mat}}{d_{head}}}$, $\mathbf{W}^{F}_{1}$ $\in$ $\mathbb{R}^{d_{model} \times d_{FF}}$, $\mathbf{W}^F_{2} \in \mathbb{R}^{d_{FF} \times d_{model}}$, $\mathbf{b_1} \in \mathbb{R}^{d_{FF}}$, and $\mathbf{b_2} \in \mathbb{R}^{d_{model}}$. We set $d_k = 16$ and $d_{FF}$ is set to 1024 with a dropout rate of 0.1. 

$\text{Sp}^{*}_{i,j}$ is given as an input, $\mathbf{Z}^{1}_{i, j}$, for the first Transformer layer as presented in Eq. \ref{2. attn-7} with $l=1$.
%
After `$iter$' (4 is used as $iter$) iterations of the Transformer layer, the output, $\mathbf{Z}^{iter+1}_{i, j}$, is produced as a shape of $\mathbb{R}^{\text{batch} \times 5 \times 128}$ with $d_{head} = 8$. The final output of Eq. \ref{2. attn-7} can be re-written as $\mathbf{Z}^{Sp}_{i, j}$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 수식 2 label 명: 2. attn 

\vspace{-15pt}
\begin{align}
\label{2. attn-1}
\text{Attn}(x) = \sigma\left(\frac{\mathbf{W}^K_h x \cdot (\mathbf{W}^Q_h x)^ \top}{\sqrt{d_k}} \mathbf{W}^V_h x\right) 
\end{align}
\vspace{-20pt}

\begin{align}
\label{2. attn-2}
\text{SA}(x) = \text{concat}[\text{Attn}_1(\ldots), \ldots, \text{Attn}_H(x)]
\end{align}
\vspace{-29pt}

\begin{align}
\label{2. attn-3}
\text{FF}(x) = \max(0, x \mathbf{W}^F_1 + \mathbf{b}^F_1) \mathbf{W}^F_2 + \mathbf{b}^F_2 
\end{align}
\vspace{-29pt}
 
\begin{align}
\label{2. attn-4}
\text{LN}(\text{F}(x)) = \text{layernorm}(x + \text{F}(x))
\end{align}
\vspace{-29pt}

\begin{align}
\label{2. attn-5}
\text{SA Block}(x) = \text{LN}(\text{SA}(x))
\end{align}
\vspace{-29pt}

\begin{align}
\label{2. attn-6}
\text{TF}(x) = \text{SA Block}(x) + \text{LN}(\text{FF}(\text{SA Block}(x)))
\end{align}
\vspace{-29pt}

\begin{align}
\label{2. attn-7}
\text{for } l = 1 \text{ to } iter: \mathbf{Z}^{1+1}_{i,j} = \text{TF}(\mathbf{Z}^{l}_{i,j}) 
\end{align}
\vspace{-10pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 수식 2 END %%%%%%%%%%%%%%%%%%%%%%

Both the raw signal and spectrogram inputs are fed into their corresponding backbone networks independently as presented in Fig. \ref{fig1}. After getting $\textbf{Z}^{Sp}_{i, j}$, an epoch-wise attention ($\textbf{EW}$) defined in Eq. \ref{3. EW-attn-3} is applied to enhance features further.  
%
Attention, $\alpha_t$, for {\bf EW} is defined by Eq. \ref{3. EW-attn-1} and Eq. \ref{3. EW-attn-2}.
%
In Eq. \ref{3. EW-attn-1}, $\mathbf{W} \in \mathbb{R}^{A \times F}$ and $\mathbf{b}_a \in \mathbb{R}^{A}$ are learnable parameters and $A$ is an attention size. The parameters, $A$ and $F$, are set to 128.

The epoch-wise attention operation is also applied to the feature, $\textbf{Z}^{Sg}_{i, j}$, extracted from raw signal data to accentuate the important channel information in the feature.
%
In Eq. \ref{3. EW-attn-1}, $\mathbf{z}_t$ can be $\textbf{Z}^{Sg}_{i, j}$ or $\textbf{Z}^{Sp}_{i,j}$ according to the input. The index, $t$, of $z_t$ has it range, $1 \le t \le T$ where $T$ is `5' for $\textbf{Z}^{Sg}_{i, j}$ and `29' for $\textbf{Z}^{Sp}_{i, j}$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EW attention 수식 시작
\vspace{-10pt}
\begin{align}
\label{3. EW-attn-1}
\mathbf{a}_t = \tanh(\mathbf{W} \cdot \mathbf{z}_t + \mathbf{b}_a) 
\end{align}
\vspace{-27pt}

\begin{align}
\label{3. EW-attn-2}
\alpha_t = \frac{\exp(\mathbf{a}_t^ \top \mathbf{a}_e)}{\sum_{t=1}^{T} \exp(\mathbf{a}_t^ \top \mathbf{a}_e)}
\end{align}
% \vspace{-10pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EW attention 분리

Then, we employ global average pooling (GAP) to effectively condense the high-dimensional feature maps. This application of GAP results in compact representations of the feature vectors and finally we have $\textbf{O}^{Sg}_{i, j} \text{ and } \textbf{O}^{Sp}_{i, j} \in \mathbb{R}^{\text{batch} \times d_{model}}$ as depicted in Eq. \ref{3. EW-attn-4}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EW attention 다시 시작 %%%%%%%%%%%%%
\vspace{-15pt}
\begin{align}
\label{3. EW-attn-3}
\textbf{EW}(\mathbf{Z}_{i, j}) = \sum_{t=1}^{T} \alpha_{t} \mathbf{z}_t 
\end{align}
\vspace{-15pt}

\begin{align}
\label{3. EW-attn-4}
\textbf{O}^{Sg}_{i, j} &= \text{GAP}(\textbf{EW}(\textbf{Z}^{Sg}_{i, j})) \nonumber \\ 
\textbf{O}^{Sp}_{i, j} &= \text{GAP}(\textbf{EW}(\textbf{Z}^{Sp}_{i, j}))
\end{align}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EW attention 끝  %%%%%%%%%%%%%

After passing through the global average pooling layer,  InfoNCE loss is calculated for training while inducing an embedding feature space of similar semantic meanings. Before directly applying the InfoNCE loss, a projection layer is added to help a model be optimized by decoupling the layers before and after the projection layer.
%
The projection layer takes $\mathbf{O}^{Sg}_{i, j}$ and $\mathbf{O}^{Sp}_{i, j}$, and it generates $\mathbf{zz}^{Sg}_{i,j}$ and $\mathbf{zz}^{Sp}_{i,j}$, respectively as presented in Eq. \ref{eq12} 
%
($\mathbf{zz}^{Sg}_{i,j}$ and $\mathbf{zz}^{Sp}_{i,j}$ $\in \mathbb{R}^{d_{model} \times d_{proj}}$, $d_{proj} = 128$).
%
%$\mathbf{zz}^{Sg}_{i,j}$ and $\mathbf{zz}^{Sp}_{i,j}$ help each other to improve their features with InfoNCE loss between them.

The loss terms for the InfoNCE are described in Eq. \ref{eq13}, Eq. \ref{eq14}, and Eq. \ref{InfoNCE_all}. We employ these loss equations to assess similarities between the features by contrasting the raw signal features with those of the spectrogram.
For effective contrast, negative samples are collected from a batch with a batch size of $B$.
Additionally, $\mathcal{L}_{sig2spc}$ and $\mathcal{L}_{spc2sig}$ are evaluated as the averages of the similarity ratios over the $L$ epochs of a sequence sample. Finally, our epoch-level loss function can be defined in Eq. \ref{InfoNCE_all}.

\begin{align}
\label{eq12}
\mathbf{zz}^{Sg}_{i,j} &= \text{Proj}(\textbf{O}^{Sg}_{i, j}) \quad  \mathbf{zz}^{Sp}_{i,j} = \text{Proj}(\textbf{O}^{Sp}_{i, j})
\end{align}
\vspace{-15pt}

\begin{align}
\label{eq13}
\mathcal{L}_{sig2spc} &= -\log \frac{1}{L}\sum_{j=1}^{L}\frac{\exp((\mathbf{zz}^{Sg}_{i,j})^ \top \cdot \mathbf{zz}^{Sp}_{i,j} / \tau)}{\sum_{k=1}^{B} \exp((\mathbf{zz}^{Sg}_{i,j})^ \top \cdot \mathbf{zz}^{Sp}_{k, j} / \tau)} \\
\label{eq14} \mathcal{L}_{spc2sig} &= -\log \frac{1}{L} \sum_{j=1}^{L}\frac{\exp((\mathbf{zz}^{Sp}_{i,j})^ \top \cdot \mathbf{zz}^{Sg}_{i,j} / \tau)}{\sum_{k=1}^{B} \exp((\mathbf{zz}^{Sp}_{i,j})^ \top \cdot \mathbf{zz}^{Sg}_{k, j} / \tau)}
\end{align}

% Additionally, to tackle the foremost challenge of demanding substantial training data in CLIP, we incorporate label guidance into our model by optimizing a cross-entropy loss with ground-truth labels.

\vspace{-10pt}
\begin{align}
\label{InfoNCE_all}
\mathcal{L}_{epoch-level} &= (\mathcal{L}_{sig2spc} + \mathcal{L}_{spc2sig})/2 \;\;\; 
% &\nonumber + \mathcal{L}_{CE}(\text{MLP}(\textbf{O}^{Sg}_{i, j}), y_{i,j}) + \mathcal{L}_{CE}(\text{MLP}(\textbf{O}^{Sp}_{i, j}), y_{i,j})
\end{align}


Also, to mitigate unstable optimization and encourage learning useful features during the initial training steps, we employ pre-trained backbone networks for epoch-level training. 
% 이 부분이,,,, 증명이 되려면 Table 3에서 TF+CNN 없이 PT+FT가 수행되어야 하는데 결과가 없어서,, 이를 증명할 방법이 없습니다...,, 지울까요.? 
%\textcolor{blue}{
%With the pre-trained backbones, the training efficiency is improved even when trained with larger datasets such as SHHS.
%}


%Our contrastive learning method can be expected not only to train a model easier even with smaller datasets, but also to make the feature space more profound when we train a large amount of data like an SHHS dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sequence level training}\label{3-2_sequence_level_training} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 3-2

In our sequence-level training, we use a sequence of the features for each modality: 
% Original: View -> Modality
$(\textbf{O}^{Sg}_{i, j})_{j=1}^L$ and $(\textbf{O}^{Sp}_{i, j})_{j=1}^L$ obtained from $SS_i$ with the epoch-level backbones, epoch-wise attention and GAP.
%
The masking strategy is integrated into the sequence-level training process to leverage the complementary nature of multi-modal characteristic, % Original: view data-> modality characteristic
enabling features from one modality
% Original: view of a data source -> modality
to aid in restoring masked features of the other modality.
%view.
%
For random masking, we use a masking probability of 50\%. By selectively hiding half of the input, the training encourages a more robust learning process, intending to recover the masked features.

Our proposed method, named ``Cross-Masking'', allows our model to extract the sequential features from the neighboring tokens for each modality %Original: view
while extensively referencing the feature information from the different modalities % Original: views
through cross-attention layers.
%
For the Cross-Masking, two Transformer blocks are used for each modality %view 
and the Transformers exchange the feature information using a cross-attention mechanism.
Each Transformer block is composed of four Transformer layers. In the Transformer block for the Cross-Masking, the primary distinction from the Transformer backbone given in Eq. \ref{2. attn-7} is that a pair of cross-attention layer and a layer normalization is added after a self-attention layer and a layer normalization. %, as outlined in Eq. \ref{2. attn-2} through Eq. \ref{2. attn-7} and proceeding the Feed-Forward layer. %Apart from this, their architectures mirror those of epoch-level Transformers, each elucidating distinct types of information.
%
The cross-attention in the Transformer block for raw signal features can be described in Eq. \ref{eq16} where $\textbf{O}^{Sg}_{i, j}$ is used as a query while $\textbf{O}^{Sp}_{i, j}$ is used as key and a value. Note that the order of arguments in the function of Eq. \ref{eq16} is important.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 수식 14 
\input{Equation/CA_short}

For the cross-attention for the spectrogram features, the roles of $\textbf{O}^{Sg}_{i, j}$ and $\textbf{O}^{Sp}_{i, j}$ are interchanged so that $\textbf{O}^{Sp}_{i, j}$ is used as a query while $\textbf{O}^{Sg}_{i, j}$ is used as a key and a value.

The critical hyperparameter for the ``Cross-Masking'' is the masking ratio within the sequence of the features. Our random masking strategy is expected to enhance the model generalization by training a recovery process from incomplete or unseen data distributions as used in MAEEG \cite{chien2022maeeg} and BERT \cite{devlin2019bert}.
%
Even though the higher masking ratio is applied in paired samples, the model could compel to reference the features from different modalities % Original:  views -> modalities
through cross-attention layers to predict the label information. This results in more accurate predictions and facilitates the harmonization of different modalities.
%
Our masked tokens are shared and learnable within an expanded batch.

% 아래가 원본 위가 수정본
% When an even higher masking ratio is used with the paired samples, the model is more compelled to reference different views through cross-attention layers to predict the label information. This results in more accurate predictions and facilitates the harmonization of different modalities.
% Masked tokens as the learnable masked tokens denoted $M^{Sg}_{i, j}$ or $M^{Sp}_{i, j} \in \mathbb{R}^{1 \times 1 \times d_{model}}$ within the expanded batch size to create shareable parameters. % in order to vs to





To further improve our model, a fine-tuning step is conducted after obtaining the pre-trained model with the masking strategy. By freezing the epoch-level backbone models, we focus on refining the sequence-level predictions by training a sequence model part with the pairs of the unmasked feature sequences, fostering a more cohesive and effective dual encoder architecture. 
Note that the masking is not used for the fine-tuning step. 
%
This strategic approach can eliminate the need for redundant adjustments at the epoch-level, streamlining the model's optimization for peak performance. % Table 3를 봤을 때, 이 작업은 SHHS에서만 효과가 존재하는데, 이렇게 써도 괜찮겠죠?

A loss function for the recovering process is presented in Eq. \ref{eq17}. In the equation, $y_{i, j}$ in a set, $Y$, are the true labels in the $j$-th epoch data in the $i$-th samples across a sequence length of $L$ and a total of $N$ samples.
%
%To denote the masking status for $\textbf{O}^{Sg}_{i, j}$ and $\textbf{O}^{Sp}_{i, j}$, we use two binary indicators, $x^{Sg}_{i, j}$ and $x^{Sp}_{i, j}$, respectively. If $\textbf{O}^{Sg}_{i, j}$ is masked then $x^{Sg}_{i, j}$ is set to 1.
Similarly, $\hat{y}_{i, j}$ in a set, $\hat{Y}$, indicates the predicted label. The ground true label, $y_{i, j}$, and the predicted label, $\hat{y}_{i, j}$, are paired with the indices, $i$ and $j$.

\vspace{-1pt}
\begin{align}
\label{eq17}
\mathcal{L}_{recover}(\hat{Y}, Y) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{L} \log(\hat{y}_{i, j}) \cdot y_{i,j} \\ \nonumber
\text{where } y_{i, j} \in Y \text{and } \hat{y}_{i, j} \in \hat{Y}
\end{align}
\vspace{-10pt}

%\begin{align}
%\label{eq18}
%\mathcal{L}_{unmask}(\hat{y}_{i, k}, y_{i, k}) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{L - P} \log(\hat{y}_{i, k}) \cdot y_{i,k} 
%\end{align}
%\vspace{-5pt}

The loss function for the sequence-level training can be expressed in Eq. \ref{eq21} and it consists of three cross-entropy loss functions. The detailed model structure for the sequence-level training is presented in the right part of the proposed model in Fig. \ref{fig1}.
%
%Each loss function consists of the sum of two losses ($\mathcal{L}_{mask}$ and $\mathcal{L}_{unmask}$) for predicting the classes of masked and unmasked tokens as depicted in Eq. \ref{eq20-1}, Eq. \ref{eq20-2} and Eq. \ref{eq20-3}.
%
The recovered features by our model are represented by $t^{Sg}_{i,j}$ or $t^{Sp}_{i,j}$.
%
A cross-entropy loss function is used for evaluating the quality of the recovered features as shown in Eq. \ref{eq20-1} and Eq. \ref{eq20-2}. In addition, we apply a cross-entropy function to the features obtained by concatenating the $t^{Sg}_{i,j}$ and $t^{Sp}_{i,j}$. The concatenate operator is denoted by $\oplus$ in Eq. \ref{eq20-3}. 
%
Then, those features are fed forward through a multi-layer perceptron (MLP) to generate the class predictions. 
%
%All of the loss elements consist of masking and unmasking loss functions to prevent our model from solely considering the original information, which can help refer to the other information.

\vspace{-5pt}
\begin{align}
\label{eq21}
\mathcal{L}_{sequence-level} = w_{1} \cdot \mathcal{L}^{Sg}_{CE} + w_{2} \cdot \mathcal{L}^{Sp}_{CE} + w_{3} \cdot  \mathcal{L}^{Sg \oplus Sp}_{CE}
\end{align}

\vspace{-10pt}
\begin{align}
\label{eq20-1}
\mathcal{L}^{Sg}_{CE} &= \mathcal{L}_{recover}(\hat{Y}^{Sg}, Y) \text{, where } 
\\ \nonumber
\text{MLP}(t^{Sg}_{i, j}) \in \hat{Y}&^{Sg}, \,\forall\, t^{Sg}_{i, j} : i \in \{1, \ldots, N\}, j \in \{1, \ldots, L\}
\end{align}

\vspace{-15pt}
\begin{align}
\label{eq20-2}
\mathcal{L}^{Sp}_{CE} &= \mathcal{L}_{recover}(\hat{Y}^{Sp}, Y) \text{, where } 
\\ \nonumber
\text{MLP}(t^{Sp}_{i, j}) \in \hat{Y}&^{Sp}, \,\forall\, t^{Sp}_{i, j} : i \in \{1, \ldots, N\}, j \in \{1, \ldots, L\}
%\mathcal{L}^{Sp}_{CE} = \mathcal{L}_{recover}(\text{MLP}(t^{Sp}_{i, j}), Y)  + \mathcal{L}_{unmask}(\text{MLP}(t^{Sp}_{i, k}), y_{i, k}) 
\end{align}

\vspace{-15pt}
\begin{align}
\label{eq20-3}
\mathcal{L}^{Sg \oplus Sp}_{CE} &= \mathcal{L}_{recover}(\hat{Y}^{SS}, Y) \text{, where } 
\\ \nonumber
\text{MLP}(t^{SS}_{i, j}) \in \hat{Y}^{SS}, \,\forall\, t^{SS}_{i, j}&= t^{Sg}_{i, j} \oplus t^{Sp}_{i, j} \nonumber : i \in \{1, \ldots, N\} \text{ and } j \in \{1, \ldots, L\}
%\mathcal{L}^{Sg \bigoplus Sp}_{CE} = \mathcal{L}_{recover}(\text{MLP}(t^{Sg \bigoplus Sp}_{i, j}), Y)  \\ \nonumber
%+ \mathcal{L}_{unmask}(\text{MLP}(t^{Sg \bigoplus Sp}_{i, k}), y_{i, k}) 
\end{align}
\vspace{-5pt}

In Eq. \ref{eq21}, $w_{1}$ is set to 1 while $w_{2}$ and $w_{3}$ are set to 0.1 for a pre-training step.
For a fine-tuning step, the sequence loss, denoted $\mathcal{L}_{sequence-level}$, given in Eq. \ref{eq21} is used but $w_{1}$, $w_{2}$ and $w_{3}$ are set to 1.
%adopt this combination to emphasize the main distinctive feature map to make a more productive backbone model. 
For an inference, we use the predictions generated by the MLP using the concatenated features.
%
Finally, our total loss functions for the pre-training step can be expressed as the sum of an epoch-level loss and a sequence-level loss as given in Eq. \ref{total_pre_train_loss_function}.

\vspace{-10pt}
\begin{gather}
\label{total_pre_train_loss_function}
\mathcal{L}_{pretrain} = \mathcal{L}_{epoch-level} + \mathcal{L}_{sequence-level}
\end{gather}