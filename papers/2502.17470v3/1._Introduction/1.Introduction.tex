Sleep is crucial for human life, as it influences physical and mental health. Sleep disorders and deficiencies can profoundly affect an individual's overall well-being \cite{wulff2010sleep}.
%
Automating sleep scoring commonly relies on Polysomnography (PSG), a widely implemented key diagnostic tool, enabling the analysis of sleep disorders through the recording of physiological signals such as the electroencephalogram (EEG) for brain activity, Electrooculogram (EOG) for eye movement, and Electromyogram (EMG) for muscle activity, etc. 
%
These signals are broken down into 30-second intervals, known as epochs, to categorize sleep stages.

Every epoch is systematically classified using criteria from the Rechtschaffen and Kales (R\&K)~\cite{rechtschaffen1968manual} manual, and the more recent American Academy of Sleep Medicine (AASM)~\cite{berry2012aasm} standards. Both offer detailed guidelines for sleep stage classification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Despite the success of PSG-based manual sleep diagnostics, due to extensive recording time, a labor-intensive labeling process, and the inherent inconsistency in labeling \cite{chapotot2010automated}, automating sleep scoring becomes essential to address these challenges effectively. \cite{10.5665/sleep.2548} work highlights potential problems and demonstrates that automatic sleep stage classification achieves performance comparable to human experts, requiring only a few seconds for labeling. 

%%%%%%%%%%%%%%% Figure 1 %%%%%%%%%%%%%%%

\begin{figure*}[htb]
\centerline{\includegraphics[width = \textwidth]{Figure/Figure_1_v18.png}
}

\caption{ 
%
The MC$^2$SleepNet processes both raw signals and spectrograms as input. The raw signals are passed through the CNN-based backbone, while the spectrograms are fed into a Transformer-based backbone.
%
We carry out the pre-training steps concurrently across the granularity of epochs and sequences. To mitigate potential discrepancies between the features obtained from the data of each modality, our MC$^2$SleepNet employs InfoNCE loss.
%
Then, a random masking strategy with 50\% probability forces the model to refer to other features from other modality data through the cross-attention layers.
} 

\label{fig1}
\vspace{-5pt}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recently, a series of studies, including DeepSleepNet \cite{supratak2017deepsleepnet}, SeqSleepNet \cite{phan2019seqsleepnet}, XSleepNet \cite{phan2021xsleepnet} introduced deep learning models that demonstrate state-of-the-art single channel performance in PSG datasets.
%%
DeepSleepNet employs dual CNN models that capture temporal and frequency domain features with raw signal samples. 
%%
SeqSleepNet introduces an RNN-based sequence-to-sequence network for processing spectrogram data.
%%
In contrast to the DeepSleepNet architecture, to implement multi-modality training (raw signal and the corresponding spectrogram data), XSleepNet adopts hierarchical CNNs as a backbone network for raw signals and SeqSleepNet as a backbone network for spectrogram.
%
XSleepNet simultaneously optimizes two sub-models, leveraging a technique known as gradient blending \cite{what_makes_training_muli-modal_hard} for updating their gradients individually. 
%
Even though XSleepNet pioneers tried to optimize multi-modal tasks for sleep stage classification,
%Original: use multi-view training
%
its effectiveness is constrained to specific deep-learning environments.

%%%%%%%%%%%%%%%%%%%%%%%%%


Due to their ease of optimization, there are various single-modal
% original: multi-view
%
training methods whose performance is on par with that of human experts. 
Within the context of multi-modal training, only XSleepNet holds a competitive status, still retaining its reliance on old-fashioned RNN-based architecture. This underscores inherent challenges in optimizing multi-modal tasks.
%
Recently, START \cite{10385393} and CoReSleepNet \cite{kontras2023coresleep} have incorporated multi-channel inputs using distinct networks, highlighting the potential for integrating diverse information and reducing the modality gap.
%
However, their contributions only display combined homogeneous information like same shape vectors consisting of different channels that use different models even though they show prominent results. They have yet to fully address the challenge of effectively combining heterogeneous information like signals and spectrograms.


In our approach, instead of utilizing gradient blending and old-fashioned RNN-based architectures, we develop a multi-modal model
combining the features extracted from multi-modal data.
%Original: by combining multiple features extracted from multi-view data that originate from a single source. by를 굳이 쓸 이유가 없어서 by지우고 that originate또한 originating으로 변경하였습니다.
% Single source 말고 Raw/Original signal 혹은 single modality로 바꾸는게 어떤가 싶어서 여쭤봅니당
%
Our network is designed with dual backbones, for the dual modalities of data samples: (1) ``Raw Signal View" and (2) ``Spectrogram View". 
A CNN-based backbone and a Transformer-based backbone are used as feature extractors for raw signal data and spectrograms, respectively. 
%
%Initially, we set both the epoch-level spectrogram model and sequence-level model to the transformer model instead of old-fashioned RNN which can cause a bottleneck for performance and efficiency.

The proposed model architecture, {\bf M}ulti-modal {\bf C}ross-masking with {\bf C}ontrastive learning for {\bf Sleep} stage classification {\bf Net}work (MC$^2$SleepNet), is presented in Fig. \ref{fig1}. 
%% 
Training the MC$^2$SleepNet model is conducted in two steps: (1) Pre-training both ``{\bf Epoch- Level}" and  ``{\bf Sequence-Level}'' simultaneously with the use of both the supervised and self-supervised learning and (2) Fine-tuning the model while freezing the CNN and Transformer backbones trained during the pre-training step. 

%%%%%%%%%%%%%%%%%%%%%%%%%

During the pre-training step at an epoch-level, {\bf InfoNCE} loss \cite{oord2019representation} is employed to align and fine-tune the embedding from different network architectures, potentially improving the performance on downstream tasks. % \sout{ with attached label guidance (cross-entropy loss with ground truth labels).}
In the sequence-level, We introduce a novel concept, ``Cross-Masking", for implementing cross-attention to unmasked and masked pairs referring to \cite{NIPS2014_ImprovedMM} work.
%
It is noteworthy that two self-supervised learning techniques, ``Contrastive Learning" and "Masking Prediction", are integrated within a pre-training process. 
%
This integration encourages two different CNN and Transformer backbones to cross-examine the features extracted from another modality. It enables the inference of information that cannot be perceived by one view alone. %Ensure that each representation is relevant to a specific performance aspect by considering the different views of the data samples.

In the fine-tuning stage, we freeze the backbone and update only the part of the model for sequence-level training. 
%
This process effectively leverages precise estimation by training inferring capability of the ground information from masked information \cite{NIPS2013_bengio}, while maintaining the integrity of the core representations.

Our proposed model shows state-of-the-art accuracy on the SleepEDF-78 (84.6\%) and SHHS (88.6\%) datasets, demonstrating improved classification performance compared to recently proposed deep learning models.
