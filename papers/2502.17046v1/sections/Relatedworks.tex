\section{Related work}
\label{Related work}
\subsection{Generalization in MARL}
There are two significant obstacles that limit the transferability and generalization capability in MARL: (a)varying state/observation/action spaces across tasks and (b)overfitting task-specific information. In response to the obstacle (a), ASN~\cite{wang2019action} decomposes an agent's observation into a composite of $n$ entity-observations. Subsequently, by aligning the entity-observations with entity-based actions, a model structure that is generalizable across tasks can be formulated. After that, UPDeT~\cite{hu2021updet} combines ASN with transformer blocks to improve the model's generalization. To convert obstacle (b), some previous works focused on knowledge transfer by learning presentations that capture the task-specific information~\cite{xu2023improving,qin2022multi,schafer2022learning,liu2019value}. Additionally, to accomplish interpretable cross-task decision-making, some works~\cite{zhang2022discovering,Decompose_Tian} turn to the concepts like skills/options/roles/subtasks in MARL for assistance. For example, DT2GS~\cite{Decompose_Tian} utilizes a scalable subtask encoder and an adaptive subtask semantic module to maintain consistent and scalable semantics across tasks. ODIS~\cite{zhang2022discovering} discovers task-invariant skills from multi-task offline data and improves generalization by coordinating discovered skills in unseen tasks. Overall, existing skill-based methods ~\cite{yang2022ldsa,chen2022multi,yang2024hierarchical,yang2019hierarchical,wang2020roma,wang2020rode,liu2022heterogeneous} in MARL suffer from deficiencies caused by partial observation, such as neglect of team awareness, or relaxation of the centralized training with decentralized execution (CTDE) constraint, which also impede the generalization and asymptotic performance in MARL. 
\subsection{Self-Supervised Learning in RL \& MARL}
Self-Supervised Learning (SSL) has made tremendous success in CV and NLP. Consequently, the idea of SSL is natural and applicable in RL to accomplish effective representation learning, particularly in vision-based RL environments. Substantial works~\cite{laskin2020curl,zhu2022masked,yu2022mask,yu2021playvirtual,yarats2021reinforcement,liu2024enhancing} construct auxiliary SSL objectives by considering the correlations among vision states or predicting the reward model and dynamic model in MDP. As a simple and effective technique, the paradigm of pretraining is also investigated to assist RL, such as generalizing to multiple downstream tasks~\cite{liu2022masked,schwarzer2021pretraining}, promoting exploration~\cite{liu2021behavior} and discovering skills\cite{liu2021aps}.To the best of our knowledge, the primary efforts on this direction have been paid on a single-agent setting, making MARL lags thus far. In ~\cite{shang2021agent,feng2022joint}, they focus on representation learning by predicting the future properties of all agents at the team level, such as location and observation. Besides, MA2CL~\cite{song2023ma2cl} encourages agents to take full advantage of temporal and agent-level information. And ACORM~\cite{hu2023attention} derive a contrastive learning objective to promote role representation learning. It appears that recent works in MARL focous solely on contrastive self-supervised learning, neglecting the potential of generative self-supervised learning.
