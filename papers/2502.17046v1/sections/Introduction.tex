\section{Introduction}

% MPOWERING generalist robots through reinforcement learning is one of the essential targets of robotic learning.
\IEEEPARstart{I}{N} recent years, cooperative multi-agent reinforcement learning (MARL) has made significant progress in solving complex real-world tasks, such as autonomous driving, multi-robot systems, and sensor networks~\cite{xiao2023stochastic,wu2022deep,feng2024efficient,yun2022cooperative,okine2024multi,galvan2021neuroevolution}. Despite the success of MARL, most of the improvements are limited to single tasks. To improve the generalization of models across tasks, previous studies have primarily focused on solving varying agent numbers across tasks~\cite{hu2021updet,wang2022multi,xu2023improving,liu2023masked} and maintaining consistent semantics across tasks through the introduction of concepts like Skills/Roles/Subtasks~\cite{Decompose_Tian,zhang2022discovering}. 
While these approaches exhibit promising performance, they struggle with sample efficiency and generalization capabilities, particularly in partially observed settings.
% 部分可观带来的问题，引入自建督的表征学习方法,现有自建督方法的问题

Learning presentations that contain global state information can be an effective way to alleviate the aforementioned limitations. To facilitate the acquisition of informative representations, self-supervised learning (SSL) is commonly integrated with reinforcement learning (RL), especially in vision-based RL~\cite{laskin2020curl,zhu2022masked,yu2022mask,liu2022masked,kang2023sample,zhao2023learning,nguyen2021csnas}. As a critical technique in SSL, masked autoencoders (MAE) have made tremendous successes in computer vision (CV) and natural language processing (NLP)~\cite{devlin2018bert,he2022masked,brown2020language} for their ability to reconstruct masked tokens. Thus, recent works have explored the application of MAE in MAS \textit{e.g.,} MaskMA~\cite{liu2023masked}, MA2CL~\cite{song2023ma2cl} and MAJOR~\cite{feng2022joint}. While promising, they neglect entity-level information, limiting their applicability to a generalizable model structure. A detailed introduction to these methods is provided in Appendix \ref{Appendix:Related work} in the form of a related work section.
% masked autoencoder在不同领域应用的差别
% \yf{we explore the generality of MARL algorithms through masked autoencoders.} \yf{Is MAE generalizable learner in the context of MARL?}

In this work, we extend masked autoencoders (MAE)~\cite{he2022masked} in MARL to improve sample efficiency and generalization capabilities. As illustrated in ASN~\cite{wang2019action}, an agent's observation can be conducted as a concatenation of numerous entity-observations. Therefore, if treating each entity-observation as a token, the idea of MAE can be naturally applied in a generalizable model in MARL. Following the success of MAE in CV and NLP, we ask: \textit{how to extend MAE in the generalizable model in MARL?} As shown in Fig.\ref{fig:illustration_MAE}, we attempt to answer this question from the following insights:
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Images/illustration_MAE.pdf}
    \caption{Illustration of MAE in different domains. (a) Masked autoencoding in NLP. (b) Masked autoencoding in CV. (c) Masked autoencoding in the context of generalization in MARL.
}\label{fig:illustration_MAE}
\end{figure*}
\begin{figure*}[]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Images/compare_with_othermae.pdf}
    \caption{Simplified schematic diagram of typical methods for applying MAE in multi-agent systems (MA2CL, MaskMA and MA2RL). The figure shows a comparison of masking between MA2CL, MaskMA, and MA2RL. (a) random masking in MA2CL. (b) random masking in MaskMA. (c) dynamic masking in MA2RL}
\label{fig:simple_compare_mae}
\end{figure*}
\begin{enumerate}
  \item[(1)] \textbf{Inference at distinct scales}: In CV and NLP, a spatial contextual correlation exists between tokens. Therefore, it is necessary to incorporate position information to infer masked tokens or patches. Accordingly, MARL necessitates historical information for inference. In contrast to CV and NLP, MARL is commonly modeled as a partially observed Markov decision process (POMDP). Thus, masked entity-observations need to be inferred from distinct scales—"\textbf{temporal scale inference}". Additionally, the permutation invariance of entity-observations enables input embeddings to be independent of positional information.
  \item[(2)] \textbf{Dynamic masked ratio}: In CV and NLP, the masked ratio is fixed, which depends on information density. For example, BERT~\cite{devlin2018bert} predicts approximately 15\% of mask tokens, and MAE~\cite{he2022masked} masks a high portion of patches (around 75\%). Accordingly, in MARL, each agent's local entity-observation serves as a mask for the global entity-state. In contrast to CV and NLP, \textbf{dynamic masked ratio} in MARL calls for a generalizable model structure.
  \item[(3)] \textbf{Different reconstruction contents}: In CV and NLP, the optimization objective is to reconstruct the tokens which contain practical semantic information. Accordingly, MAE in MARL are designed to reconstruct the information of dynamically unobserved entities. In contrast to CV and NLP, MAE in MARL has \textbf{different reconstruction contents}, which aims at reconstructing the latent representation of dynamically unobserved entities and maximizing the cumulative return of policies. Policies rely solely on the latent representation of observations without requiring a decoder to explicitly reconstruct entity observations.
\end{enumerate}
% (1)嵌入的构成：在CV和NLP中，token和patch之间具有空间上的上下文相关性， 所以为了使模型能够利用序列的顺序，我们必须向序列中的表征注入一些关于相对或绝对位置的信息。而MARL通常被建模为马尔可夫决策过程，具有时间上的相关性，在对mask token的预测需要考虑历史轨迹的信息。此外，观测的置换不变性，使得嵌入表征无需位置信息。where temporal correlation needs to be considered for infering masked entity-observations
% (2)masked ratio:以bert和mae为例，mask的比例是固定的，具体的值取决于语义和信息的密度，例如，bert对单词mask的比例大概是15%，MAE对图片patch mask了75%。而在MARL中，智能体局部观测的entity-observation可以看作全局entity-state的mask，其中mask的ratio是随着与环境的交互动态变化的，取决于智能体观测的信息。
% (3)在CV和NLP中，通过decoder重建的pixel或者tokens有实际的语义信息，所以decoder在学习的潜在表示的语义级别方面起着关键作用。而在MARL中，策略学习一种观测和动作间的映射函数，所以无需使用decoder来重建实体观测信息。
% 我们的论文贡献。

Driven by this analysis, we propose a novel framework called \textbf{M}asked \textbf{A}utoencoders for \textbf{M}ulti-\textbf{A}gent \textbf{R}einforcement \textbf{L}earning (MA2RL), which implicitly reconstructs latent representation of dynamically masked entities for more comprehensive task-independent skills assignment and learning skill semantics in a global perspective. In MA2RL, entity-observations and entity-states are first mapped into a latent space using variational autoencoders (VAE). Subsequently, the MAE infers masked entities by reconstructing global entity-states. These reconstructed latent representations are then leveraged to select an appropriate task-independent skill. Finally, by reusing the decoder in VAE, the attentive action decoder learns skill semantics based on the latent representation of the masked entity-observations, the observed entity-observations, and the task-independent skill. Experimental results demonstrate the effective utilization of masked encoding in MA2RL, leading to improved asymptotic performance and generalization across a wide range of tasks. To the best of our knowledge, our work is the first attempt to employ MAE in the context of generalization in MARL. The contributions of our work are summarized as follows:
\begin{enumerate}
  \item[1)] We propose a novel framework called MA2RL, the first attempt to extend MAE in the context of generalization in MARL, which implicitly reconstructs the latent representation of dynamically masked entities for more comprehensive task-independent skills assignment and learning skill semantics in a global perspective.
  \item[2)] We explore MAE from the entity perspective, positioning it as orthogonal research to previous SSL methods. Surprisingly, we find that MAE are generalizable for MARL due to its remarkable ability to infer unobserved entities and learn high-capacity models.
  \item[3)] Through extensive experiments, we demonstrate that MA2RL achieves SOTA performance in various experimental settings, exhibiting remarkable generalization and transfer capabilities.
\end{enumerate}

To highlight the novelty of our work, we add a comparison between MA2RL and recent works that explore the application of MAE in MARL, such as MA2CL~\cite{song2023ma2cl} and MaskMA~\cite{liu2023masked}. As illustrated in Fig.~\ref{fig:simple_compare_mae}, MA2CL randomly masks the observations and actions of some agents at timestep $t$, and replaces them with the observations and actions from timestep $t-1$. This design idea aims to encourage learning representations to be both temporal and agent-level predictive. MA2CL mainly focused on the model’s asymptotic performance , neglecting its generalization across tasks. MaskMA is dedicated to building a single generalist agent with strong zero-shot capability. However, it merely applies MAE to the attention layer of the transformer, without considering the natural masking relationship between entity states and entity observations in MARL. Finally, MA2RL treats individual entity-observations as masked contexts of entity-states, naturally integrating MAE with a generalizable model in MARL. In this case, masking ratio is dynamically changing and determined by the environment, which hinders the direct application of MAE. To tackle this challenge, we develop a novel MAE in MA2RL. The main novelty of MAE in MA2RL is that, through utilizing Gaussian distribution and dividing agents' observation into two streams (observed-entities stream and masked-entities stream), MAE in MA2RL is endowed with generalization capability to infer the dynamically changing masked entities.



% We treat local entity-observations as masked context of global entity-states. And we extend MAE to MARL for reconstructing latent representation of dynamically masked entities, which enables agents to infer the global entity-states.