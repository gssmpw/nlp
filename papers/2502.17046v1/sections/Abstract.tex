\begin{abstract}

To develop generalizable models in multi-agent reinforcement learning, recent approaches have been devoted to discovering task-independent skills for each agent, which generalize across tasks and facilitate agents' cooperation. However, particularly in partially observed settings, such approaches struggle with sample efficiency and generalization capabilities due to two primary challenges: (a) How to incorporate global states into coordinating the skills of different agents? (b) How to learn generalizable and consistent skill semantics when each agent only receives partial observations? To address these challenges, we propose a framework called \textbf{M}asked \textbf{A}utoencoders for \textbf{M}ulti-\textbf{A}gent \textbf{R}einforcement \textbf{L}earning (MA2RL), which encourages agents to infer unobserved entities by reconstructing entity-states from the entity perspective. The entity perspective helps MA2RL generalize to diverse tasks with varying agent numbers and action spaces. Specifically, we treat local entity-observations as masked contexts of the global entity-states, and MA2RL can infer the latent representation of dynamically masked entities, facilitating the assignment of task-independent skills and the learning of skill semantics. Extensive experiments demonstrate that MA2RL achieves significant improvements relative to state-of-the-art approaches, demonstrating extraordinary performance, remarkable zero-shot generalization capabilities and advantageous transferability.

 % Additional rewards transform the original MTRL problem into a multi-objective MTRL problem, and the coupling relationship between the outputs of SP and ACP further complicates the optimization process. To solve this challenge, TSAC assigns a virtual expected budget to convert the multi-objective MTRL into a constrained single-objective formulation and then employs the Lagrangian method to transform a constrained single-objective optimization into an unconstrained one. The multiplier in the Lagrangian method automatically adjusts the weights during the training process, promoting cooperation between SP and ACP.
\end{abstract}
\begin{IEEEImpStatement}
The Current policies trained by Multi-Agent Reinforcement Learning (MARL) predominantly rely on meticulously designed structured environments, which considerably constrain the agents' generalization capabilities across multitasking and cross-task skill reuse. In this paper, we design a novel masked autoencoders for MARL to coordinate the skills of different agents and learn generalizable and consistent skill semantics when each agent only receives partial observations. Experimental results demonstrate that our proposed MA2RL framework significantly enhances both the asymptotic performance and generalization capabilities of the generalizable models. Specifically, MA2RL introduces masked autoencoders tailored for MARL, aimed at enhancing generalizable models. The framework holds promise for inspiring further explorations into the generalization of multi-agent reinforcement learning.
\end{IEEEImpStatement}


% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Multi-Agent reinforcement learning, generalization, self-supervised learning.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle