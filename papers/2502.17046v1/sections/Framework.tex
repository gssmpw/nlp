\section{Method}
\label{sec:framework}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/Images/MA2RL_framework_2.pdf}
    \caption{The network structure of MA2RL. (a) The overall architecture. (b) The stucture of variational autoencoder (VAE). (c) The details of masked autoencoder for MARL, where entity-observations can be regarded as a mask of the entity-states. (d) The attentive Action decoder that reuses the decoder in VAE to infer masked entity-observations for better action execution.
}
\label{fig:framework}
\end{figure*}

In this section, we will introduce our proposed framework MA2RL. Fig.\ref{fig:framework} illustrates the MA2RL framework, which consists of two components: masked autoencoders for MARL and attentive action decoder. The masked autoencoders for MARL enable agents to infer the global entity-states. And the attentive action decoder learns skill semantics from a global perspective. We outline the respective roles played by the two components in MA2RL:
\begin{itemize}
    \item masked autoencoders for MARL: it is utilized to incorporate global states into coordinating the skills of different agents, which plays a key role in asymptotic performance.
    \item action decoder: it is utilized to encompass the impact of inferred unobserved entities for achieving generalizable and comprehensive skill semantics, which play a key role in generalization.
\end{itemize}

Each component is introduced in detail in the following subsections.

\subsection{Masked Autoencoders for MARL}\label{subsec:MAE}
For agent $i$, its global entity-states at timestep $t$ can be conducted as : $s_i^t = [s_{i,1}^t,s_{i,2}^t,...,s_{i,m}^t]$, where $m$ denotes the total number of entities in the environment. Due to the partial observability of MAS, agent $i$ at timestep $t$ can only access local entity-observations $o_i^t = [o_{i,1}^t,o_{i,2}^t,...,o_{i,m}^t]$, where unobserved entities are zero padding. The connection between the global entity-states $s_i^t$ and the local entity-observation $o_i^t$ is established through a mask function $\bm{M}_i^t$, expressed as follows:
\begin{equation}
    \begin{split}
        {\bm{M}_i^t}&=\{M_{i1}^t,M_{i2}^t,\ldots,M_{ij}^t,\ldots,M_{im}^t\},M_{ij}^t=1 ~\mathrm{or}~ 0.\\
        o_{i}^t&={\bm{M}_i^t}\cdot s_i^t
    \end{split}
\end{equation}
If $M_{ij}^t=0$, the $j$-th entity at timestep $t$ is unobserved for agent $i$. We reconstruct the entity-states $s_i^t$ by leveraging information from the historical trajectory and currently observed entity-observations.

As illustrated in Fig.\ref{fig:framework}(b), we employ two variational autoencoders (VAEs) to derive latent representations from the local entity-observations and the global entity-states. In the VAEs, the encoder transforms entity-observations and entity-states into a Gaussian distribution in the latent space, while the decoder reconstructs entity-observations and entity-states by decoding samples from the Gaussian distributions. There are two key advantages to selecting a normal distribution as the encoded distribution: 1) Utilizing Gaussian Product with Gaussian distribution enables us to obtain fixed-dimensional latent representations of entity-observations. 2) The Gaussian Product naturally satisfies the permutation invariance property of entity-observations. Specifically, we employ two VAEs, which encoders and decoders parameterized by $(\theta_{\mathrm{oe}} , \theta_{\mathrm{od}}),(\theta_{\mathrm{se}},\theta_{\mathrm{sd}})$ to encode the entity-observations and entity-states into Gaussian distributions respectively. The VAE models are trained using the $loss_{\mathrm{VAE}}$ objective function:
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figures/Images/attentive_action_decoder.pdf}
    \caption{attentive action decoder. The attentive action decoder utilizes the latent representations of all masked entity-observations to infer the information of masked entities and then applies a skill attention module to obtain actions.
}
\label{fig:attention_decoder}
\end{figure*}
\begin{equation}
\begin{split}
(\mu_{\mathrm{o},ij}^t,\sigma_{\mathrm{o},ij}^t)&=f_{\theta_{\mathrm{oe}}}(o_{ij}^t),j=1,2,...,m, o_{ij}^t \in o_{\mathrm{obs},i}^t\\
\mathcal{L}_{\mathrm{VAE_o}}&=\|o_{ij}^t-f_{\theta_{\mathrm{od}}}(x \sim \mathcal{N}(\mu_{\mathrm{o},ij}^t, \sigma_{\mathrm{o},ij}^t))\|^2\\
(\mu_{\mathrm{s},ij}^t,\sigma_{\mathrm{s},ij}^t)&=f_{\theta_{\mathrm{se}}}(o_{ij}^t),j=1,...,m\\
\mathcal{L}_{\mathrm{VAE_s}}&=\|s_{ij}^t-f_{\theta_{\mathrm{sd}}}(x \sim \mathcal{N}(\mu_{\mathrm{s},ij}^t, \sigma_{\mathrm{s},ij}^t))\|^2
\label{eq:(2)}
\end{split}
\end{equation}
$(f_{\theta_{\mathrm{od}}},f_{\theta_{\mathrm{od}}}),(f_{\theta_{\mathrm{se}}},f_{\theta_{\mathrm{sd}}})$ respectively denotes the encoders and decoder of two VAEs. Thus, the latent representation of all observed entity-observations and entity-states, respectively denoted as Gaussian distributions $\mathcal{N}_{\mathrm{observed}}(\mu_{\mathrm{o}i}^t, \sigma_{\mathrm{o}i}^t)$, $\mathcal{N}_{\mathrm{states}}(\mu_{\mathrm{s}i}^t,\sigma_{\mathrm{s}i}^t)$, are constructed using Gaussian Product:
\begin{equation}
\begin{split}
\mathcal{N}_{\mathrm{observed}}(\mu_{\mathrm{o}i}^t,\sigma_{\mathrm{o}i}^t)&=\prod_{j=1}^{n_{\mathrm{obs}}}\mathcal{N}(\mu_{\mathrm{o},ij}^t,\sigma_{\mathrm{o},ij}^t)\\
\mathcal{N}_{\mathrm{states}}(\mu_{\mathrm{s}i}^t,\sigma_{\mathrm{s}i}^t)&=\prod_{j=1}^m\mathcal{N}(\mu_{\mathrm{s},ij}^t,\sigma_{\mathrm{s},ij}^t)
\label{eq:(3)}
\end{split}
\end{equation}

\subsection{Attentive action decoder}
Subsequently, as depicted in Fig.\ref{fig:framework}(b), we employ a GRU, parameterized by $f_{\theta_h}$, to infer the latent representations of all masked entity-observations $\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)$. The GRU takes the latent representation of all observed entity-observations $\mathcal{N}_{\mathrm{observed}}(\mu_{oi}^t, \sigma_{oi}^t)$ and the previous trajectory information $h_i^{t-1}$ as inputs. This process can be formulated as follows:
\begin{equation}
    \mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t), h_i^t=f_{\theta_h}(x \sim \mathcal{N}_{\mathrm{observed}}(\mu_{oi}^t,\sigma_{oi}^t),h_i^{t-1})
    \label{eq:(4)}
\end{equation}
To merge the encoded and inferred results, we employ Gaussian Product to merge the two Gaussian distributions and obtain an integrated representation of the entity-observations:
\begin{equation}
        \mathcal{N}_{\mathrm{integration}}(\mu_{i}^t,\sigma_{i}^t)=\mathcal{N}_{\mathrm{observed}}(\mu_{oi}^t,\sigma_{oi}^t)\cdot\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)
    \label{eq:(5)}
\end{equation}
By utilizing the integrated representation of the entity-observations and the latent representation of entity-states, we construct a reconstruction loss function to optimize the parameters of the GRU, enabling it to infer the latent representations of the masked entity-observations. The reconstruction loss is formulated as follows:
\begin{equation}
\begin{split}
\mathcal{L}_{\mathrm{reconstruct}}=\|(y \sim \mathcal{N}_{\mathrm{states}}(\mu_{si}^t,\sigma_{si}^t))-\\
(x \sim \mathcal{N}_{\mathrm{integration}}(\mu_{i}^t,\sigma_{i}^t))\|^2
\end{split}
\label{eq:(6)}
\end{equation}
$x$ and $y$ are sampled from Gaussian distributions $\mathcal{N}_{\mathrm{states}}$ and $\mathcal{N}_{\mathrm{integration}}$, respectively. By minimizing the reconstruction loss $\mathcal{L}_{\mathrm{reconstruct}}$, we can train both the encoder and the GRU to effectively extract informative representations and recover the latent representation of masked entity-observations.

By introducing historical information, MAE in MARL can attempt to figure out the enemy's representation from the following two aspects: (1) Historical opponent information observed (e.g., if an agent observes the enemy's information at time steps 5 to 10, then after step 11, the opponent's representation can be predicted based on historical information). (2)Currently observed information about allies (e.g., predicting the opponent's representation through the historical trajectory or current behavior of teammates).

According to the masked autoencoders for MARL, we obtain the integrated representation of the entity-observations $\mathcal{N}_{\mathrm{integration}}(\mu_{i}^t,\sigma_{i}^t)$, which contains the observed entities' information and the inferred masked entities' information. Based on the integrated representation of the entity-observations, we utilize the Gumbel-Softmax trick to assign an individual skill $z_i^t$:
\begin{equation}
z_i^t=\mathrm{Gambel-Softmax}(x \sim \mathcal{N}_{\mathrm{integration}}(\mu_{i}^t,\sigma_{i}^t))
\label{eq:skill}
\end{equation}
Inspired by DT2GS~\cite{Decompose_Tian} and ACORM~\cite{hu2023attention}, which utilize the attention mechanism~\cite{vaswani2017attention} to incorporate subtasks/roles into the action decoder, we introduce the concept of task-independent skills to facilitate the training process of the attentive action decoder. In DT2GS~\cite{Decompose_Tian}, subtask semantics refer to the effects of an agent on observed entities. While in MA2RL, task-independent skill semantics not only refer to the impact of observed entities but also encompass the impact of inferred unobserved entities for achieving generalizable and comprehensive skill semantics.

\begin{algorithm}[]
\caption{MA2RL}
\begin{algorithmic}[1]
\Statex \textbf{Input:} The VAE parameters $(\theta_{\mathrm{oe}},\theta_{\mathrm{od}})$ for the masked autoencoder of actor $\pi$; The VAE parameters $(\theta_{\mathrm{se}},\theta_{\mathrm{sd}})$ for the masked autoencoder of critic $V$; the parameters $\theta_{\mathrm{mae}}$ for mask encoder in $\pi$; the parameters $\theta_{\mathrm{de}}$ for attentive action decoder in $\pi$; the parameters $\theta^V$ for critic $V$.
\State{Initialize $(\theta_{\mathrm{oe}},\theta_{\mathrm{od}})$,$(\theta_{\mathrm{se}},\theta_{\mathrm{sd}})$, $V$} % Discriminator parameters 
\State{Initialize the total timesteps $T$ of an episode; the replay buffer $D$}
\State{Initialize $step=0$}
\For{$episode =0,1,\dots,K$}
\State{initialize actor RNN states $h_{1,\pi}^0,\dots,h_{n,\pi}^0$}
\State{initialize critic RNN states $h_{1,V}^0,\dots,h_{n,V}^0$}
\State{$\tau=\left[\right]$}
    \For{$timestep=1$ to $T$}
        \For{$i=0$ to $n$, agent $i$}
        \State{$\mathcal{N}_{\mathrm{masked}}(\mu_{i}^t,\sigma_{i}^t)$,$\mathcal{L}_{\mathrm{reconstruct}},h_i^{t}=\pi_{\mathrm{mae}}(o_i^t,h_i^t;\theta_{\mathrm{mae}})$,$ \triangleright$Call for Algorithm \ref{mae4marl}}
        \State{Choose an individual skill $z_i^t$ by by Formula(\ref{eq:skill})}
        \State{$a_i^t=\pi_{de}(o_i^t,\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t),z_i^t;\theta_{\mathrm{de}})$,$\triangleright$Call for Algorithm \ref{action_decoder}}
        \EndFor 
        \State{Execute actions $a^t$, then acquire $r^t,s^{t+1},o^{t+1}$}
        \State{$\tau+=[s^t,\boldsymbol{o^t},\boldsymbol{h_\pi^t},\boldsymbol{h_V^t},\boldsymbol{k^t},\boldsymbol{a^t},r^t,s^{t+1},\boldsymbol{o^{t+1}}]$}
    \EndFor
    \State{$D=D\cup\left(\tau\right)$}
    \For{mini-batch $k=1,...K$}
    \State{Sample data from $D$ to update $\pi$ by minimizing actor loss}
    \State{Sample data from $D$ to update $V$ by minimizing critic loss}
    \State{Sample data from $D$ to update $(\theta_{\mathrm{oe}},\theta_{\mathrm{od}});(\theta_{\mathrm{se}},\theta_{\mathrm{sd}})$ by minimizing $\mathcal{L}_{\mathrm{reconstruct}}$ }
    \EndFor
\EndFor
\end{algorithmic}
\label{alg}
\end{algorithm}


\begin{algorithm}[]
\caption{Masked autoencoders for MARL}
    \begin{algorithmic}[1]
\Statex \textbf{Input:} The VAE parameters $(\theta_{\mathrm{oe}},\theta_{\mathrm{od}})$ for masked autoencoder of actor $\pi$, The VAE parameters $(\theta_{\mathrm{se}},\theta_{\mathrm{sd}})$ for masked autoencoder of critic $V$, agent $i$'s observation $o_i^t$, agent $i$'s state $s_i^t$ and previous trajectory information $h_i^{t-1}$
\State{Encode the entity-observations and entity-states into $(\mu_{\mathrm{o},ij}^t,\sigma_{\mathrm{o},ij}^t);(\mu_{\mathrm{s},ij}^t,\sigma_{\mathrm{s},ij}^t),j=1,...,m$ by Formula(\ref{eq:(2)})}
\State{Compute $\mathcal{N}_{\mathrm{observed}}(\mu_{oi}^t,\sigma_{oi}^t);\mathcal{N}_{\mathrm{states}}(\mu_{si}^t,\sigma_{si}^t)$ by Formula(\ref{eq:(3)})}
\State{Infer the latent representation of all masked entity-observations $\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)$ and $h_i^{t}$ by Formula(\ref{eq:(4)})}
\State{Obtain an integrated representation of the entity-observations $\mathcal{N}_{\mathrm{integration}}(\mu_{i}^t,\sigma_{i}^t)$ by Formula(\ref{eq:(5)})}
\State{Compute the reconstruction loss $\mathcal{L}_{\mathrm{reconstruct}}$ by Formula(\ref{eq:(6)})}
\State{\textbf{Return} $\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)$,$\mathcal{L}_{\mathrm{reconstruct}},h_i^{t}$}
\end{algorithmic}
\label{mae4marl}
\end{algorithm}

\begin{algorithm}[]
\label{algorithm2}
\caption{Attentive action decoder}
    \begin{algorithmic}[1]
\Statex {\textbf{Input:} The VAE parameters $(\theta_{\mathrm{oe}},\theta_{\mathrm{od}})$ for masked autoencoder of actor $\pi$, agent $i$'s observation $o_i^t$ and skill $z_i^t$ at timestep $t$}
\State{Obtain the enhanced entity-observation $o_{\mathrm{enh},i}^t$ by Formula(\ref{eq:(7)})}
\State{Compute the embedding of the self-attention part $\tau_{i,\mathrm{self}}^t$ by Formula(\ref{eq:(8)})}
\State{Compute the embedding of the skill-based attention part $\tau_{i,skill}^t$ by Formula(\ref{eq:(9)})(\ref{eq:(10)})}
\State{Sample action $a_i^t$ from actions' probability distribution $P$ by Formula(\ref{eq:(11)}}
\State{\textbf{Return} action $a_i^t$}
\end{algorithmic}
\label{action_decoder}
\end{algorithm}
% 具体讲一下skill的概念 参考别的文章怎么讲的
As illustrated in Fig.\ref{fig:framework}(d), the attentive action decoder takes the local entity-observation $o_i = [o_{i1},o_{i2},...,o_{im}]$, the individual skill $z_i^t$ and the latent representations of all the masked entity-observations $\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)$ as inputs and generates actions. We also provide a detailed description of the structure in Fig.\ref{fig:attention_decoder}. The latent representation of all masked entity-observations $\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)$ are trained by MAE in Section\ref{subsec:MAE} and encompass speculative information about all unobserved entities. It is worth noting that the latent representation of all masked entity-observations is used to train a policy, thereby eliminating the need to explicitly reconstruct the observations of masked entities individually. Specifically, we utilize the inferred masked entity-observations by reusing the decoder parameterized by $f_{\theta_{\mathrm{od}}}$ in VAE:
\begin{equation}
\begin{split}
o_{\mathrm{enh},ij}^t&=\left\{\begin{array}{cc}
f_{\theta_{\mathrm{od}}}(m_{ij}^t \sim \mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)) & \text { if } o_{ij}^t \in o_{\mathrm{mask},i}^t \\
o_{ij}^t & \text { otherwise }
\end{array}\right.\\
o_{\mathrm{enh},i}^t&=[o_{\mathrm{enh},i1}^t,o_{\mathrm{enh},i2}^t,...,o_{\mathrm{enh},im}^t]
\end{split}
% o_{i1}, o_{i2},\ldots,o_{ij} \sim \mathcal{N}_{masked}(\mu_{mi}^t, \sigma_{mi}^t) ,j=1,2,...,n_{mask}, o_{ij} \in o_{mask,i}
\label{eq:(7)}
\end{equation}
$m_{ij}^t$ is a hidden embedding sampled from $\mathcal{N}_{\mathrm{masked}}(\mu_{mi}^t, \sigma_{mi}^t)$. After obtaining the enhanced entity-observation $o_{\mathrm{enh},i}^t$ by integrating the observed entity-observation and the inferred masked entity-observation, we introduce a skill attention module to learn skill semantics and enhance representational capacity and generalization. The skill attention module consists of two parts: self-attention to observed entities and skill-based attention. Specifically, in the self-attention part, we use the enhanced entity-observations as queries, keys and values:$Q_i^t=W_Qo_{\mathrm{enh},i}^t,K_i^t=W_Ko_{\mathrm{enh},i}^t,V_i^t=W_Vo_{\mathrm{enh},i}^t$, where $W_Q, W_K, W_V$ are parameter matrices for linear transformation. Formally, we calculate the embeddings of the self-attention part of agent $i$ at timestep $t$ as $\tau_{i,\mathrm{self}}^t$:
\begin{equation}
\tau_{i,\mathrm{self}}^t=\mathrm{softmax}(\frac{Q_i^tK_i^t}{\sqrt{d_k}})V_i^t
\label{eq:(8)}
\end{equation}
where $d_k$ represents the feature embedding of $K_i^t$ and $1/\sqrt{d_k}$ is a factor that scales the dot-product attention. Analogously, we compute the embedding of the skill-based attention part, denoted as $\tau_{i,\mathrm{skill}}^t$, by introducing the  individual skill $z_i^t$ as follows:
\begin{equation}
\tau_{i,\mathrm{skill}}^t=\sum_{j=1}^m\alpha_jv_{ij}^t=\sum_{j=1}^m\alpha_j\cdot W_Vo_{\mathrm{enh},ij}^t,
\label{eq:(9)}
\end{equation}
The attention weight $\alpha_j$ quantifies the relevance between the $j$-th entity-observation in $o_{\mathrm{enh},i}$ and the individual skill $z_i^t$. We apply a MLP and a softmax function to obtain the weight as:
\begin{equation}
    \alpha_j=\frac{\exp\left(\frac1{\sqrt{d_k}}\cdot W^Q\mathrm{MLP}(z_i^t) \cdot\left(W^Ko_{\mathrm{enh},ij}^t\right)^\top\right)}{\sum_{k=1}^m\exp\left(\frac1{\sqrt{d_k}}\cdot W^Q\mathrm{MLP}(z_i^t) \cdot\left(W^Ko_{\mathrm{enh},ik}^t\right)^\top\right)},
    \label{eq:(10)}
\end{equation}
In practice, we employ multi-head attention (MHA) to stabilize the learning process and collectively focus on information from different representation subspaces. Subsequently, we calculate the probability of each action/value by concatenating the embedding of the self-attention part $\tau_{i,\mathrm{self}}^t$ and the embedding of the skill-attention part $\tau_{i,\mathrm{skill}}^t$:
\begin{equation}
    P(a|o_i^t)= W[\tau_{i,\mathrm{self}}^t,\tau_{i,\mathrm{skill}}^t]
    \label{eq:(11)}
\end{equation}
$W$ is parameter matrices for linear transformation. The attentive action decoder enables us to flexibly leverage the latent representation of all masked entity-observations, thus alleviating performance degradation and improving generalization caused by partial observability. The skill attention module enhances the generalization of policies by introducing individual skills that can be generalized across tasks.


\subsection{Objective}
In this section, we present the training process of  MA2RL. The details of MA2RL is as shown in Algorithm 1, with the details of the two main modules, MAE and the action decoder, presented in Algorithms 2 and 3 respectively.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Images/single_task_v2.pdf}
    \caption{The performance of MA2RL and baselines, including DT2GS, UPDeT, ASN\_G, and MAPPO, are compared in the Single-Task settings. The evaluation is conducted on 2 hard tasks (5m\_vs\_6m, 3s\_vs\_5z) and 2 superhard tasks (3s5z\_vs\_3s6z, 6h\_vs\_8z).}
\label{fig:single_task}
\end{figure*}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Images/mpe_performance.pdf}
    \caption{The asymptotic performance of MA2RL and baselines in MPE.}
\label{fig:asymptotic_mpe}
\end{figure}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Images/Multi-task.pdf}
    \caption{The performance of MA2RL and baselines, including DT2GS, UPDeT, ASN\_G, are compared in the Multi-Task settings. The evaluation is conducted on 2 multi-task problems with different distributions of difficulty: (2s3z, 3s5z, 3s5z\_vs\_3s6z),(5m\_vs\_6m, 8m\_vs\_9m, 10m\_vs\_11m) }.
\label{fig:multi_task}
\end{figure*}
