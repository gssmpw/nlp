\section{Preliminaries}
\label{Preliminaries}
\subsection{Multi-Agent Reinforcement Learning}
A MARL problem can be formulated as a decentralized partially observed Markov decision process (Dec-POMDP)~\cite{oliehoek2016concise}, which is described as a tuple $\langle n,\boldsymbol{S},\boldsymbol{A},P,R,\boldsymbol{O},\boldsymbol{\Omega},\gamma\rangle $, where $n$ represents the number of agents, $\boldsymbol{S}$ is the global state space. $\boldsymbol{A}$ is the action space. $\boldsymbol{O}=\{O_{i}\}_{i=1,\cdots,n}$ is the observation space. At timestep $t$, each agent $i$ receives an observation $o_{i}^t\in O_{i}$ according to the observation function $\boldsymbol{\Omega}(s^t,i):\boldsymbol{S}\to O_i$ and then selects an action $a_i^t\in\boldsymbol{A}$. The joint action $\boldsymbol{a}^t=(a_1^t,\ldots,a_n^t)$ is then applied to the environment, resulting in a transition to the next state $s^{t+1}$ and a global reward signal $r^{t}$ according to the transition function $P(s^{t+1}\mid s^{t},\boldsymbol{a}^t)$ and the reward function $R(s^t,\boldsymbol{a}^t)$. $\gamma\in[0,1]$ is the discount factor. The objective is to learn a joint policy $\pi$ that maximizes the expected cumulative reward $\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r^{t}\right|\pi]$.

\subsection{Centralized Training With Decentralized Execution}
Centralized Training with Decentralized Execution (CTDE) is a commonly employed architecture in MARL~\cite{lowe2017multi,rashid2020monotonic}. In CTDE, each agent utilizes an actor network to make decisions based on local observations. Additionally, the training process incorporates global information to train a centralized value function. The centralized value function provides a centralized gradient to update the actor network based on its outputs.

\subsection{Generalizable Model Structure in MARL}
To handle varying state/observation/action spaces, previous works like UPDeT~\cite{hu2021updet} and ASN~\cite{wang2019action} propose a generalizable model that treats all agents as entities. In such models, observation $o_i$ can be conducted as entity-observations: $[o_{i,1},o_{i,2},...,o_{i,m}]$, where $m$ denotes the number of all entities in the environment. Based on the criterion of whether entites can be observed, entity-observations can be splited into two subsets: observed entity-observations $o_{\mathrm{obs},i}$ and unobserved entity-observations $o_{\mathrm{mask},i}$. We denote the number of observed entities and masked entities as $n_{\mathrm{obs}}$ and $n_{\mathrm{mask}}$, respectively, and it holds that $m=n_{\mathrm{obs}}+n_{\mathrm{mask}}$.
Additionally, action space $\boldsymbol{A}$ can be decomposed into two subsets:$\boldsymbol{A}^{\mathrm{self}}$ containing actions that affect the environment or itself  and $\boldsymbol{A}^{\mathrm{out}}$ representing actions that directly interact with other entities.