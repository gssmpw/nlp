

\documentclass[letterpaper, 10 pt, conference]{ieeeconf} 

\IEEEoverridecommandlockouts                               

\overrideIEEEmargins 
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}

\usepackage{tikz}
\usepackage{subcaption}  

\usepackage{multirow} 
\usetikzlibrary{arrows.meta,automata,positioning}
\usepackage{xparse}
\usepackage{xcolor}
\usepackage[ruled,vlined]{algorithm2e}
% \usepackage{algorithm}
\newcommand{\lnm}{\mathcal{L}(\mathcal{M}_L)}
\usepackage{algpseudocode}
\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{problem}{\textbf{Problem}}
\newtheorem{definition}{\textbf{Definition}}
\newcommand{\ptp}{\pi_{\mathrm{PTP}}}
\newcommand{\plnm}{\mathrm{Pref}(\lnm)}
\newcommand{\tptp}[1]{\pi_\mathrm{PTP}^\mathrm{#1}}
\newtheorem{proposition}{\textbf{Proposition}}
\newtheorem{corollary}{\textbf{Corollary}}
\newtheorem{lemma}{\textbf{Lemma}}
\newtheorem{remark}{\textbf{Remark}}
\newtheorem{assumption}{\textbf{Assumption}}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{amssymb}
\newcommand{\ML}[1]{\textcolor{red}{ML: #1}}
\renewcommand{\AA}[1]{{\color{cyan}AA: #1}}
\newcommand{\NO}[1]{{\color{blue}NO: #1}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\Pp}{\mathbf{P}}
\bibliographystyle{IEEEtran}
\newcommand{\figureref}[1]{Figure~\ref{#1}}
\newcommand{\sectionref}[1]{Section~\ref{#1}}
\newcommand{\appendixref}[1]{Appendix~\ref{#1}}
\newcommand{\equationref}[1]{Equation~\ref{#1}}



\title{\LARGE \bf
Learning Reward Machines from Partially Observed Optimal Policies
}

\author{Mohamad Louai Shehab$^{1}$, Antoine Aspeel$^{2}$, and Necmiye Ozay$^{1,2}$
\thanks{$^{1}$ M.L. Shehab and N. Ozay are with the Robotics Department, University of Michigan, Ann Arbor, MI 48109, USA {\tt\small \{mlshehab, necmiye\}@umich.edu }}
\thanks{$^{2}$ A. Aspeel and N. Ozay are with EECS, University of Michigan, Ann Arbor, MI 48109, USA {\tt\small antoinas@umich.edu }}
}
\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}%
Inverse reinforcement learning is the problem of inferring a reward function from an optimal policy. In this work, it is assumed that the reward is expressed as a reward machine whose transitions depend on atomic propositions associated with the state of a Markov Decision Process (MDP). Our goal is to identify the true reward machine using finite information. To this end, we first introduce the notion of a prefix tree policy which associates a distribution of actions to each state of the MDP and each attainable finite sequence of atomic propositions. Then, we characterize an equivalence class of reward machines that can be identified given the prefix tree policy. Finally, we propose a SAT-based algorithm that uses information extracted from the prefix tree policy to solve for a reward machine. It is proved that if the prefix tree policy is known up to a sufficient (but finite) depth, our algorithm recovers the exact reward machine up to the equivalence class. This sufficient depth is derived as a function of the number of MDP states and (an upper bound on) the number of states of the reward machine. Several examples are used to demonstrate the effectiveness of the approach.
\end{abstract}

\section{Introduction} 
Several frameworks exist for solving complex multi-staged tasks, including hierarchical reinforcement learning (HRL) \cite{pateria2021hierarchical}, reward machines (RMs) \cite{icarte2018using} and linear temporal logic (LTL) specifications \cite{chou2020explaining,vaezipoor2021ltl2action}. HRL leverages a decomposition of tasks into subtasks, enabling agents to focus on solving smaller, manageable problems before integrating solutions into a higher-level policy \cite{sutton1999between}. On the other hand, RM and its generalizations \cite{corazza2022reinforcement} encode task-specific knowledge as finite-state machines, capturing temporal dependencies and logical constraints in a concise and interpretable manner, similar to LTL. This structure simplifies policy learning and improves efficiency, especially in environments with long horizons or sparse rewards. 

As an extension to inverse reinforcement learning (IRL) \cite{ng2000algorithms}, one could ask the question of learning RMs, which enables agents to autonomously extract structured representations of tasks, significantly enhancing their ability to solve complex, temporally extended problems. By learning reward machines directly from demonstrations, agents can adapt to tasks without requiring manually specified task representations, making this approach scalable and practical for real-world applications, such as robotic manipulation and autonomous vehicle navigation \cite{camacho2019ltl,icarte2023learning, xu2020joint,baert2024reward,camacho2021reward}. This capability is especially valuable in environments where only high-level task-relevant features (aka, atomic propositions) are observable, underscoring the importance of learning RMs in advancing autonomous decision-making systems. Beyond autonomy applications, IRL and RM learning can also be used to infer the agent's (e.g., humans') intentions to design incentives or better decision making environments \cite{nitschke2024amber}.

Some of previous work on learning reward machines from data either assumes that the machine's states are observed \cite{araki2019learning} or the reward is observed \cite{xu2020joint, icarte2023learning, hu2024reinforcement}. In the latter case the problem becomes finding a reward machine consistent with the observed input-output traces. Other work \cite{hasanbeig2024symbolic,hasanbeig2021deepsynth,furelos2020induction} infers reward machines by combining automata synthesis with reinforcement learning and querying the environment for experiences. Others \cite{xu2021active,memarian2020active} use the standard $L^*$ algorithm for automata learning \cite{angluin1987learning} to learn a consistent reward machine. This assumes access to an oracle that can answer membership and conjectures queries. There are also works that only use observations of atomic propositions \cite{camacho2021reward}, similar to us, however they are limited to single-stage goal reaching tasks, where the RM has a simple structure that is merely used to obtain dense rewards. In parallel, several works aim to infer an LTL specification from demonstrations satisfying and/or violating the specification \cite{neider2018learning, vazquez2020maximum}, requiring a potentially large, labeled data set. Since LTL learning problem is inherently ill-posed, several regularization techniques are used such as formula templates or concept classes.

To the best of the authors' knowledge, no prior work has formalized and solved the problem of learning reward machines from partially observed optimal policies directly without the need to observe the rewards or the machine's state. The two main challenges of this setting are 1) partial observability (the reward is not observed, only the atomic propositions are observed), 2) partial reachability (not all transitions of the reward machine are visited in a given environment). In this work, we address these challenges by first characterizing what can be learned in this setting (i.e., an equivalence class of reward machines) and then proposing a SAT-based algorithm, which provably learns a reward machine equivalent to the underlying true one. The key insight of our algorithm is to identify pairs of atomic proposition prefixes, namely \emph{negative examples}, that lead to different nodes of the underlying reward machine from the observable optimal prefix-based policy, and encoding these examples as constraints in the SAT problem. We demonstrate the efficacy of our algorithm with several examples. 

{\bf Notation:} Given a set $X$, we denote by $\Delta(X)$ and $|X|$ the set of all valid probability distributions on $X$ and the cardinality of $X$, respectively. $\mathbf{1}(X)$ denotes the indicator function of $X$. $X^*, X^\omega$ denote the set of all finite/infinite sequences of elements in $X$. For a sequence $\tau$ and non-negative integers $i,j$, $\tau_i$ denotes the $i^{th}$ element of $\tau$; $|\tau|$ denotes the length of $\tau$; $\tau_{end}$ denotes the last element of $\tau$ when $\tau$ is finite; $\tau_{i:j}$ denotes the subsequence starting with the $i^{th}$ element and ending with the $j^{th}$; and $\tau_{:i}$ denotes the subsequence ending with the $i^{th}$ element.


\section{Preliminaries and Problem Statement}

\subsection{Markov Decision Processes and Reward Machines}\label{sec:mdp}
A Markov Decision Process (MDP) is a tuple $\mathcal{M} = (\mathcal{S},\mathcal{A}, \mathcal{P},\mu_0, \gamma, r)$, where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of actions, $\mathcal{P}:\mathcal{S}\times \mathcal{A} \to \Delta(\mathcal{S})$ is the Markovian transition kernel, $\mu_0 \in \Delta(\mathcal{S})$ is the initial state distribution, $\gamma \in [0,1)$ is the discount factor and $r:\mathcal{S}\times \mathcal{A}\times \mathcal{S}\to \mathbb{R}$ is the reward function. The set of feasible state trajectories for an MDP $\mathcal{M}$, denoted by $\mathcal{T}_s(\mathcal{M})$, is defined as:
\begin{align*}
    \mathcal{T}_s(\mathcal{M})
 = \{ (s_0, \ldots, s_t, \ldots) \in &(\mathcal{S})^\omega \mid \exists a_0, \ldots, a_{t-1},\ldots, \\
 &\mathcal{P}(s_{t+1} \mid s_t, a_t) > 0, \forall t \}.
\end{align*}

When we want to refer to finite prefixes of $\mathcal{T}_s(\mathcal{M})$, we simply use $\mathcal{T}_s^{\mathrm{fin}}(\mathcal{M})$, and we omit $\mathcal{M}$ when it is clear from the context.

An MDP without the reward is referred to as an \emph{MDP model}, and is denoted by $\mathcal{M}/r$. MDP models can be decorated with labels. We denote such labeled MDP models as $\mathcal{M}_L = (\mathcal{S},\mathcal{A}, \mathcal{P},\mu_0, \gamma, L, \mathrm{AP})$, where $L:\mathcal{S}\to \mathrm{AP}$ is a labeling function that assigns to each state an atomic proposition, representing high-level conditions satisfied at that state, from the set $\mathrm{AP}$. A labeled MDP has a corresponding language $\mathcal{L}(\mathcal{M}_L)\subseteq (\mathrm{AP})^\omega$, with $\mathcal{L}(\mathcal{M}_L) \doteq \{\sigma \in (\mathrm{AP})^\omega \mid \sigma = L(\tau), \text{ where } \tau  \in \mathcal{T}_s(\mathcal{M}_L)\}$, where we overload $L$ to take in sequences. We also define the prefixes of a language as:
\begin{equation}
\mathrm{Pref}(\mathcal{L}) = \{ w \in (\mathrm{AP})^* \mid \exists x \in \mathcal{L}, \, w \text{ is a prefix of } x \}.
\end{equation}
The set of reachable states for a proposition sequence $\sigma$ is:
\begin{align*}
    \mathrm{Reach}(\sigma) &= \{ s \in \mathcal{S} \mid  \tau \in \mathcal{T}_s^{\mathrm{fin}} \text{ s.t. } L(\tau) = \sigma, \tau_{end}=s  \}. 
\end{align*}
A Reward Machine (RM) is a tuple $\mathcal{R}=(\mathcal{U}, u_I, \mathrm{AP}, \delta_{\mathbf{u}}, \delta_{\mathbf{r}})$ which consists of a finite set of states $\mathcal{U}$, an initial state $u_I \in \mathcal{U}$, an input alphabet $\mathrm{AP}$, a (deterministic) transition function $\delta_{\mathbf{u}}: \mathcal{U} \times \mathrm{AP}\to \mathcal{U}$, and an output function $\delta_{\mathbf{r}}: \mathcal{U}\times \mathrm{AP} \to \mathbb{R}$. To avoid ambiguity between MDP states and RM states, the later will be referred to as \emph{nodes}. The reward machine without the reward is denoted as $\mathcal{G} \triangleq \mathcal{R}/\delta_\mathbf{r}$, and we refer to it as a \emph{reward machine model}. We extend the definition of the transition function to define $\delta_{\mathbf{u}}^*: \mathcal{U} \times (\mathrm{AP})^* \to \mathcal{U}$ as $\delta_{\mathbf{u}}^*(u, l_0 , \cdots, l_k) = \delta_{\mathbf{u}}(\cdots(\delta_{\mathbf{u}}(\delta_{\mathbf{u}}(u, l_0),l_1), \cdots, l_k)$.  Given a state \( u \in \mathcal{U} \), we define \emph{the paths of \( u \)} as the input words which can be used to reach \( u \):
$$
\mathrm{Paths}(u) = \{w \in (\mathrm{AP})^* \mid \delta_{\mathbf{u}}^*(u_I, w) = u\}.
$$
We overload the operator $\mathrm{Reach}$ to include the set of MDP states reachable at $u$. It is given by:
\begin{equation*}
     \mathrm{Reach}(u) = \{s\in \mathrm{Reach}(\sigma) \mid \sigma \in \mathrm{Paths}(u)\}.
\end{equation*}

As a running example, we borrow the patrol task from \cite{icarte2018using}. Consider the room grid world shown in \figureref{fig:gridworldroom}. It is a 4 by 4 grid where the agent can move in the four cardinal directions, with a small probability of slipping to neighboring cells. We color-code different cells to denote the proposition label associated with the corresponding cell. For example, all cells colored green have the high level proposition $\mathrm{A}$. The agent is tasked to patrol the rooms in the order $\mathrm{A}\to\mathrm{B}\to\mathrm{C}\to\mathrm{D}$. This is captured by the reward machine shown in \figureref{fig:patrol_rm}. Assume the agent's state trajectory starts with $\tau = (\mathbf{a}_1,\mathbf{a}_2,\mathbf{a}_3,\mathbf{b}_4,\mathbf{c}_1)$. The proposition sequence associated with $\tau$ is $\sigma = \mathrm{AAABC}$. The RM nodes traversed by following $\tau$ are $(u_0, u_1,u_1,u_1, u_2,u_3)$. Since $\mathbf{c}_1$ is the only state than can be reached with $\sigma$, we have that $\mathrm{Reach}(\sigma) = \{\mathbf{c}_1\}$. Similarly, $\sigma \in \mathrm{Paths}(u_3)$.
 
\begin{figure}[h!]
    \centering
    % First subfigure
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rooms_gridworld.pdf}
        \caption{}
        \label{fig:gridworldroom}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}[t]{0.27\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ABCD_patrol_rm.pdf}
        \caption{}
        \label{fig:patrol_rm}
    \end{subfigure}
    \caption{(a) The Room Grid World and (b) the Patrol Reward Machine.}
    \label{fig:overall}
\end{figure}
\vspace{-0.4cm}
 
\vspace{-0.2cm}
\subsection{Markov Decision Process with a Reward Machine}
A Markov decision process with a reward machine (MDP-RM) is a tuple $\mathcal{R}_{\mathcal{M}} = (\mathcal{M}/r,\mathcal{R},L)$ where $\mathcal{M}$ and $\mathcal{R}$ are defined as in Section \ref{sec:mdp}, and $L$ is a labeling function $L:\mathcal{S}\to \mathrm{AP}$. 
 
An MDP-RM can be equivalently seen as a product MDP $\mathcal{M}_\mathrm{Prod} = (\mathcal{S}', \mathcal{A}', \mathcal{P}', \mu_0^\prime, \gamma^\prime, r')$ where $\mathcal{S}' = \mathcal{S}\times \mathcal{U}$, $\mathcal{A}' = \mathcal{A}$, $\mathcal{P}'(s',u'| s,u,a) = \mathcal{P}(s'|s,a) \mathbf{1}(u' = \delta_\textbf{u}(u,L(s')))$, $\gamma^\prime = \gamma$, $\mu_0^\prime \in \Delta(\mathcal{S}\times \mathcal{U})$ with $\mu_0^\prime(s,u) = \mu_0(s)\mathbf{1}(u = u_I)$ and $r'(s,u,a,s',u') = \delta_\mathbf{r}(u,L(s^\prime))$. To make the notation compact, we denote the product state by $\bar s$, where $\bar s = (s,u)$. The product of an MDP model with a RM model is a product MDP model $\mathcal{G} \times \mathcal{M}_L = (\mathcal{S}', \mathcal{A}', \mathcal{T}', \mu_0^\prime, \gamma^\prime)$ defined similarly.

{A \emph{trajectory} of the product MDP $\mathcal{M}_\mathrm{Prod}$ is a sequence $(\bar s_{\emptyset}, a_{\emptyset}, \bar s_0, a_0, \bar s_1, a_1,\cdots)$, where $\bar s_{\emptyset} = (\emptyset, u_I)$ and $a_{\emptyset}= \emptyset$. An initial state $s_0$ is sampled from $\mu_0$. The augmentation of $\bar s_{\emptyset}$ and $a_\emptyset$ at the start of the trajectory is to ensure that $s_0$ induces a transition in the reward machine. The reward machine thus transitions to $u_0 = \delta_\textbf{u}(u_I, L(s_0))$. The agent then takes action $a_0$ and transitions to $s_1$. Similarly, the reward machine transitions to $u_1 = \delta_\textbf{u}(u_0, L(s_1))$. The same procedure continues infinitely. }We consider the product policy $\pi_{\mathrm{Prod}}:\mathrm{Dom_{Prod}} \to \Delta(\mathcal{A})$ where $\mathrm{Dom_{Prod}}\subseteq \mathcal{S}\times\mathcal{U}$ is the set of accessible $(s,u)$ pairs in the product MDP. This policy is a function that describes an agent’s behavior by specifying an action distribution at each state.  We consider the Maximum Entropy Reinforcement Learning (MaxEntRL) objective given by:
\begin{equation}\label{eq:max_ent_obj}
    J_{\mathrm{MaxEnt}}(\pi;r') = \mathbb{E}^{\pi}_{\mu_0}[\sum\limits_{t=0}^{+\infty} \gamma^t \biggl(  r^\prime (\bar s_t,a_t, \bar s_{t+1}) + \lambda \mathcal{H}(\pi(.|\bar s_t)) \biggr)],
\end{equation}
where $\lambda > 0$ is a regularization parameter, and $\mathcal{H}(\pi(.|\bar{s})) = -\sum\limits_{a\in \mathcal{A}} \pi(a|\bar{s})\log(\pi(a|\bar{s}))$ is the entropy of the policy $\pi$. The expectation is with respect to the probability distribution $\mathbb{P}^\pi_{\mu_0}$, the induced distribution over infinite trajectories following $\pi$, $\mu_0$, and the Markovian transition kernel $\mathcal{P}^\prime$ \cite{ziebart2008maximum}. The optimal policy $\pi_{\mathrm{Prod}}^*$, corresponding to a reward function $r'$, is the maximizer of (\ref{eq:max_ent_obj}), i.e.,
\begin{equation}\label{eq:opt_prob}
    \pi_{\mathrm{Prod}}^* = \arg \max\limits_{\pi} J_{\mathrm{MaxEnt}}(\pi;r').
\end{equation}
\emph{Optimal} product MDP trajectories are trajectories of the product MDP generated using $\pi_{\mathrm{Prod}}^*$. We overload this definition to \emph{optimal trajectories} of the MDP, which is generated from the optimal product MDP trajectories by simply removing the $u$ states. For the rest of the paper, \emph{optimal trajectories} refer to the optimal trajectories of the MDP.

\subsection{Prefix Tree Policy}
Since the RM is unknown and the state of the RM is unobserved, we need a representation of the agent's policy that is independent of the RM state $u$. We accomplish this by defining a prefix tree policy (PTP) as the function that associates a distribution over the actions to each state and each finite sequence of atomic propositions that can be generated by the MDP. It is denoted as $\pi_{\mathrm{PTP}}: \mathrm{Dom_{PTP}} \to \Delta(\mathcal{A})$, with $\mathrm{Dom_{PTP}}=\{(s,\sigma)\mid \sigma\in \mathrm{Pref}(\mathcal{L}(\mathcal{M}_L)) \text{ and } s\in\mathrm{Reach}(\sigma) \}$. An important remark here is that the agent is acting according to a policy $\pi_{\mathrm{Prod}}(a|s,u)$, since the agent has access to $u$. The PTP in turn encodes the information of the agent's product policy in terms of the variables that we have access to, namely the MDP states only. The relation between the two policies is governed by:
\begin{equation}\label{eq:induced}
    \pi_{\mathrm{PTP}} (a|s,\sigma) = \pi_{\mathrm{Prod}}(a|s, \delta_\textbf{u}^*(u_I, \sigma)),
\end{equation}
where $\sigma \in \plnm$. In particular, we say that the product policy $\pi_{\mathrm{Prod}}$ \emph{induces}  $\pi_{\mathrm{PTP}}$. We define the \emph{depth-$l$ restriction} of a PTP as its restriction to the set $\left(\cup_{j=1}^{k}\mathcal{S} \times (\mathrm{AP})^j\right)\cap\mathrm{Dom_{PTP}}$. That is the policy associated to words $\sigma$ of length up to $l$. It is denoted by $\ptp^{l}$.

\begin{remark}
In this work, we assume that we have access to the depth-$l$ restriction of the PTP for a sufficiently large $l$ (the value of $l$ is discussed in \sectionref{sec:proof}). This is a reasonable assumption because this quantity can be approximated arbitrary well by a finite set of optimal trajectory prefixes. Note that this is because the optimal trajectories of the MDP for the max entropy objective cover $\mathrm{Dom_{PTP}}$. However, the impact of such an approximation and related sample complexity questions are left for future work.
\end{remark}


The induced PTP captures both what is observable about a product policy and what is reachable on the product. Therefore, we can only learn a reward machine up to whatever information is available in its induced PTP. We formalize this with the following definition.

\begin{definition}
   Two reward machines are \textbf{policy-equivalent} with respect to a labeled MDP if the product policies obtained by solving problem~\eqref{eq:opt_prob} for each of the reward machines induce the same prefix tree policy defined as in Eq. \eqref{eq:induced}. Among all the reward machines that are policy equivalent with respect to a labeled MDP, we define a \textbf{minimal} reward machine as one with the fewest number of nodes.
\end{definition}

Several equivalence relations among reward machines in the literature are special cases of policy equivalence. For instance, when learning finite state machines from observed rewards \cite{xu2020joint, giantamidis2021learning}, two reward machines are said to be input-output equivalent if they produce the same reward sequence for the same atomic proposition sequence. Such input-output equivalent reward machines are clearly policy-equivalent. Furthermore, for a trivial, i.e., one state, reward machine, our definition reduces to the policy-equivalence definition in the standard inverse reinforcement learning problem \cite{shehab2024learning, cao2021identifiability}.

\subsection{Problem Statement}\label{sec:prob_stat}

Consider a labeled MDP model $\mathcal{M}_L$ and a prefix tree policy $\ptp^{\mathrm{true}}$ induced by an optimal solution of problem~\eqref{eq:opt_prob}. We are interested in the following two problems in this paper:
\begin{enumerate}
\item[(P1)] Does there always exist a depth $l^*$ such that, given the labeled MDP model $\mathcal{M}_L$, a bound $u_{\mathrm{max}}$ on the number of nodes of the underlying reward machine, and the  depth-$l^*$ restriction $\ptp^{\mathrm{true},l^*}$ of the true prefix tree policy, it is possible to learn a reward machine that is policy-equivalent to the underlying one?
\item[(P2)] If $l^*$ in problem (P1) exists, find a minimal reward machine that is policy-equivalent to the underlying one.
\end{enumerate}
In what follows, we first provide an algorithm that takes the labeled MDP model $\mathcal{M}_L$, the depth-$l$ restriction $\ptp^{\mathrm{true},l}$ of the prefix tree policy $\ptp^{\mathrm{true}}$ for some arbitrary $l$, and the bound $u_{\mathrm{max}}$, and computes a reward machine that induces a prefix tree policy $\ptp^{\mathrm{learned}}$ with the same depth-$l$ restriction, i.e., $\ptp^{\mathrm{learned},l}=\ptp^{\mathrm{true},l}$. Then, we show the existence of a sufficient depth $l^*$ in (P1), for which this algorithm solves problem (P2).


\section{Methodology}
\subsection{SAT Encoding}\label{sec:sat_enc}

We encode the RM learning problem into a SAT problem. Specifically, we use SAT to encode a graph with $n \leq u_{\mathrm{max}}$ nodes and associate a Boolean variable with each edge in the graph. Each node has $|\mathrm{AP}|$ outgoing edges. 
We define the Boolean variables $\{b_{ikj} \mid 1 \leq i ,j \leq n, 1 \leq k \leq |\mathrm{AP}|\}$ as:
\begin{equation}
    b_{ikj} = \begin{cases} 1 \quad \text{if } i\overset{k}{\to} j, \\ 0 \quad \text{Otherwise,} \end{cases}
\end{equation}
where we use the shorthand $i\overset{k}{\to} j$ to denote that proposition $k$ transitions node $i$ to node $j$, i.e., $\delta_{\mathbf{u}}(u_i,k)=u_j$. We can encode several properties of the RM into Boolean constraints. WLOG, we set node $1$ of the graph to be $u_I$. To make the derivation easier, we define for each atomic proposition $k$ an adjacency matrix $B_k$ with $(B_k)_{ij} = b_{ikj}$. The Boolean constraints we add are due to \emph{determinism}, \emph{full-specification}, \emph{negative examples} and \emph{non-stuttering} of the learned reward machine. We expand on each of them below.

\subsubsection{Determinism}\label{sec:det}
Due to the RM being a deterministic machine, each label can only transition to one node. The corresponding Boolean constraints are:
\begin{equation}\label{eq:determinism}
    \forall i,k,j, \forall j' \neq j \quad b_{ikj} = 1 \implies b_{ikj'} = 0 .
\end{equation}

\subsubsection{Full Specification}\label{sec:fs}
This constraint, also known as being input-enabled \cite{hungar2003domain}, ensures that all labels generate valid transitions at all states. The corresponding Boolean constraints are: 
\begin{equation}\label{eq:full}
    \forall i, \forall k, \exists j \text{ such that } b_{ijk} = 1.
\end{equation}
We can combine the conditions of Sections \ref{sec:det} and \ref{sec:fs} into one condition on each $B_k$ enforcing that each row has exactly one entry with value $1$.

\subsubsection{Negative Examples}\label{sec:opt}
Our Boolean constraint here depends on the following result.
\begin{lemma} \label{lem:neg}
    Let $\sigma, \sigma' \in (\mathrm{AP})^*$ be two finite label sequences. If $\ptp^{\mathrm{true}}(a|s, \sigma) \neq \ptp^{\mathrm{true}}(a|s, \sigma')$, then  $\delta_\textbf{u}^*(u_I,\sigma) \neq \delta_\textbf{u}^*(u_I,\sigma')$. 
\end{lemma}
\begin{proof}
    It follows from Eq.~\eqref{eq:induced}.
\end{proof}

Based on this result, given the depth-$l$ restriction $\pi_{\mathrm{PTP}}^l$ of a PTP $\ptp$, we construct the set of negative examples as:
$$\mathcal{E}^-_l = \{(\sigma, \sigma') \mid \pi_{\mathrm{PTP}}^l(a|s, \sigma) \neq \pi_{\mathrm{PTP}}^l(a|s, \sigma') \text{ for some } s,a \}$$

Let $ \sigma = k_1 k_2 \cdots k_l$ and $\sigma' = k_1'k_2' \cdots k_m'$ be two propositional prefixes that lead to different policies in the same state, therefore $(\sigma,\sigma')\in \mathcal{E}^-_l$. We encode the condition given by Lemma~\ref{lem:neg} into Boolean constraints as:
\begin{align}\label{eq:ce_bool}
    (B_{k_l}^\intercal B_{k_{l-1}}^\intercal \cdots B_{k_1}^\intercal e_1)\bigwedge &(B_{k_m'}^\intercal B_{k_{m-1}'}^\intercal \cdots B_{k_1'}^\intercal e_1) \notag \\ = &\begin{bmatrix}0 &0 & \cdots & 0\end{bmatrix}^\intercal,
\end{align}
where $\bigwedge$ is the element-wise \textbf{AND} of the matrices and $e_1 \triangleq [1,0,\cdots,0]^\intercal$ indicates that the paths start from the initial node. Our algorithm adds the Boolean constraint given in Eq.~\eqref{eq:ce_bool} for each element of $\mathcal{E}^- $. The significance of encoding negative examples is that it eliminates the learning of trivial reward machines. In particular, our method never learns a trivial one-state reward machine with all self-transitions \cite{icarte2023learning} as long as there is at least one negative example in our prefix tree policy. 

\subsubsection{Non-Stuttering}\label{sec:conn}
A reward machine is said to be non-stuttering if when a proposition transitions into a node, that same proposition can not transition out of the node, i.e., for all $(u,a,u')\in\mathcal{U}\times\mathrm{AP}\times\mathcal{U}$: $ \delta_{\mathbf{u}}(u, a) = u' \implies \delta_{\mathbf{u}}(u',a) = u'$. This is related to multi-stage tasks where the particular duration spent on a subtask (i.e., satisfying a given atomic proposition) is not important \cite{baier2008principles}. When the reward machine is \textit{a priori} known to be non-stuttering, this extra condition can be included in the SAT problem. The main significance of this condition is trace-compression \cite{icarte2023learning}, by which we can reduce the number of negative examples in $\mathcal{E}^-$ by only keeping the shortest negative examples among the equivalent ones. In this case, two negative examples are equivalent if between two pairs, the corresponding label sequences differ only by the same proposition repeated consecutively more than one time. 
We encode non-stuttering into boolean constraints as follows:
\begin{equation} \label{eq:non_stutter}
    \forall i,j,k:\ b_{ikj} = 1 \implies b_{jkj} = 1.
\end{equation}


\subsection{Algorithm}\label{sec:algo}

To learn a minimal reward machine from the depth-$l$ restriction of a prefix tree policy, we proceed as follows. 

We start with one node and increases the number $n$ of nodes until the following SAT problem is feasible when instantiated for a graph with $n$ nodes:
\begin{center}
$\texttt{SAT}_n$(\eqref{eq:determinism}, \eqref{eq:full}, \eqref{eq:non_stutter}, for all $(\sigma,\sigma')\in \mathcal{E}^-$  \eqref{eq:ce_bool}).
\end{center}
By construction, this is guaranteed to be satisfiable for some $n\leq u_{\mathrm{max}}$, upon which a reward machine model $\mathcal{G}^{\mathrm{learned}}$ can be constructed from the satisfying assignment's $B_k$'s. Then, we compute the product MDP model $(\mathcal{M}/r)^{\mathrm{learned}}=\mathcal{G}^{\mathrm{learned}} \times \mathcal{M}_L$. The learned product policy is constructed as follows. For each length $l$ word $\sigma\in(\mathsf{AP})^l$ and for all $s \in \mathrm{Reach}(\sigma)$, we define
\begin{equation}\label{eq:const}
\pi_{\mathrm{Prod}}^{\mathrm{learned}}(a|s, \delta_\mathbf{u}^{\mathrm{learned},*}(u_I, \sigma))=\pi_{\mathrm{PTP}}^{\mathrm{true},l}(a|s,\sigma),
\end{equation}
where the transition function $\delta_\mathbf{u}^{\mathrm{learned},*}$ is the transition function of $\mathcal{G}^{\mathrm{learned}}$. 

The last step is finding the rewards that render the product policy $\pi_{\mathrm{Prod}}^{\mathrm{learned}}$ optimal for the product MDP model $(\mathcal{M}/r)^{\mathsf{learned}}$. This is a standard IRL problem without reward machines where the special structure of the rewards on the product can be represented as features. We solve this step using the method developed in \cite{shehab2024learning}. Featurization (see, \cite[Section~4]{shehab2024learning}) is used to enforce that the reward function of the product can be written as $r((s,u),a,(s',u'))=\delta_\mathbf{r}(u,L(s'))$, with $u'=\delta_\mathbf{u}(u,L(s'))$. This IRL method gives us the corresponding reward function $\delta_\mathbf{r}$. 

The overall procedure is summarized in Algorithm~\ref{alg:main}.


\begin{algorithm}
\caption{Learning a Minimal Reward Machine from Depth-$l$ Restriction of a Prefix Tree Policy}\label{alg:main}
\KwIn{Depth-$l$ prefix tree policy $\pi_{\mathrm{PTP}}^{\mathrm{true},l}$, labeled MDP $\mathcal{M}_L$.}
\KwOut{Learned reward machine $\mathcal{R}^{\mathrm{learned}}$}

$n \gets 1$\;

\While{\texttt{SAT}$_n$ is infeasible}{
    $n \gets n+1$\;
}
$\{B_k\}_{k=1}^{|\mathrm{AP}|} \gets$ \texttt{SAT}$_n$ solution \;

$\mathcal{G}^{\mathrm{learned}} \gets$ Construct\_RM\_model($\{B_k\}_{k=1}^{|\mathrm{AP}|}$)\;

$(\mathcal{M}/r)^{\mathrm{learned}} \gets \mathcal{G}^{\mathrm{learned}} \times \mathcal{M}_L$\;

\ForEach{$\sigma \in (\mathsf{AP})^l$}{
    \ForEach{$s \in \mathrm{Reach}(\sigma)$}{
        Define product policy:
        \[
        \pi_{\mathrm{Prod}}^{\mathrm{learned}}(a|s, \delta_\mathbf{u}^{\mathrm{learned},*}(u_I, \sigma)) \gets \pi_{\mathrm{PTP}}^{\mathrm{true},l}(a|s,\sigma)
        \]
    }
}

$\delta_\mathbf{r} \gets$ IRL\_to\_extract\_reward($\pi_{\mathrm{Prod}}^{\mathrm{learned}},(\mathcal{M}/r)^{\mathrm{learned}}$)

\Return{$\mathcal{R}^{\mathrm{learned}} = (\mathcal{G}^{\mathrm{learned}}, \delta_\mathbf{r})$}\;
\end{algorithm}


\subsection{Proof of Correctness}\label{sec:proof}
Let $\mathcal{G}^{\mathrm{learned}}$ be the reward machine model extracted from the SAT solution, with $\delta_\mathbf{u}^{\mathrm{learned}}$ being the associated transition function. The first property of our SAT solution is that it is consistent with any fixed depth of the prefix tree policy. We formalize this in the result below.
\begin{proposition}\label{prop:const_k}
    Given $\mathcal{M}_L$, the depth-$l$ restriction $\ptp^{\mathrm{true},l}$ of the true prefix policy $\ptp^{\mathrm{true}}$, and an upper bound $u_{\mathrm{max}}$ on the number  of nodes of the underlying reward machine, let $\mathcal{G}^{\mathrm{learned}}$ be the output of our SAT algorithm, and define $\ptp^{\mathrm{learned}}$ to be the (infinite depth) prefix tree policy induced by $\mathcal{G}^{\mathrm{learned}}$. Then, the learned and the true prefix tree policies have the same depth-$l$ restriction, i.e., $\ptp^{\mathrm{learned},l}=\ptp^{\mathrm{true},l}$. 
\end{proposition}
\begin{proof}
    See \appendixref{app:proof_const_k}.
\end{proof}

While Proposition~\ref{prop:const_k} represents a desirable property of our algorithm, being consistent with the first $l$ layers of the true prefix tree policy is not sufficient in general, given that our problem in \sectionref{sec:prob_stat} requires consistency with the infinite depth true prefix tree policy. This is potentially problematic if the agent demonstrates unseen changes in its policy beyond the first $l$ layers of the prefix tree policy. At the same time, it is not possible to run our algorithm with an infinite depth prefix tree policy, as the algorithm will never terminate. Fortunately, we can show that if the $l$ is large enough, then additional negative examples will not change the satisfying assignments of the SAT problem beyond $l$ layers.


\begin{proposition} \label{prop:prop6} 
Given $\mathcal{M}_L$,  an upper bound $u_{\mathrm{max}}$ on the number of nodes of the underlying reward machine, and the depth-$l$ restriction $\ptp^l$ of some prefix tree policy $\ptp$, where $l = |\mathcal{S}|u_{\mathrm{max}}^2$. Then, $\{B_k\}_{k=1}^{|\mathrm{AP}|}$ is a satisfying assignment for $$\texttt{SAT}_{u_{\mathrm{max}}}(\eqref{eq:determinism}, \eqref{eq:full}, \eqref{eq:non_stutter}, \text{ for all } (\sigma,\sigma')\in \mathcal{E}^{-}_l \eqref{eq:ce_bool})$$ 
if and only if it is a satisfying assignment for all $j\geq l$ for $$\texttt{SAT}_{u_{\mathrm{max}}}(\eqref{eq:determinism}, \eqref{eq:full}, \eqref{eq:non_stutter}, \text{ for all } (\sigma,\sigma')\in \mathcal{E}^{-}_j \eqref{eq:ce_bool}).$$
\end{proposition}
\begin{proof} See \appendixref{app:proof_prop6}.
\end{proof}

Now, we present the main result of this section. Our result guarantees that given a sufficiently deep restriction of the true prefix tree policy, our recovered reward machine will be consistent with true infinite depth prefix tree policy. That is, our algorithm is guaranteed to find a reward machine that is policy-equivalent to the true reward machine.
\begin{theorem}\label{thm:thm_major}
{
    Given a labeled MDP model $\mathcal{M}_L$ and the depth-$l$ restriction $\ptp^{\mathrm{true},l}$ of a prefix tree policy induced by a reward machine $\mathcal{R}^{\mathrm{true}}$ with at most $u_{\mathrm{max}}$ nodes, if $l\geq|\mathcal{S}|u_{\mathrm{max}}^2$, then the reward machine $\mathcal{R}^{\mathrm{learned}}$ returned by Algorithm~\ref{alg:main} is policy-equivalent to $\mathcal{R}^{\mathrm{true}}$ with respect to $\mathcal{M}_L$.
    }
\end{theorem}
\begin{proof}
   Follows immediately from Propositions~\ref{prop:const_k} and ~\ref{prop:prop6}. In particular, we know that $\mathcal{G}^{\mathrm{learned}}$ is a solution of $\texttt{SAT}_{u_\mathrm{max},j}$, for all $j \geq l$, due to Proposition~\ref{prop:prop6}. Combined with Proposition~\ref{prop:const_k}, this means that $\ptp^{\mathrm{learned,j}} =\ptp^{\mathrm{true,j}}$, for all $j \geq l$. 
\end{proof}
\begin{remark}
Note that, in practice, a depth-$l$ restriction where  $l\ll|\mathcal{S}|u_{\mathrm{max}}^2$ can be sufficient to find a reward machine that is policy equivalent to the true one if all the solutions of the corresponding SAT problem are policy-equivalent to each other (e.g., they correspond to the same reward machine up to renaming of nodes). This will be further illustrated in the examples section.
\end{remark}


\section{Examples}
For all our examples, we use $\gamma = 0.9$ and $\lambda = 1.0$ when solving Problem~\eqref{eq:opt_prob} to generate the data for our learning problem. The implementation is done in Python, and the Z3 library \cite{de2008z3} is used for solving the SAT problems. Table \ref{tab:stat_exp1} summarizes the key parameters related to the problem setup and solution performance for the examples considered in this section. The code is available at \href{https://2ly.link/23uDo}{github.com/mlshehab/learning\_reward\_machines}

Our first example is on the patrol task of Figure~\ref{fig:overall}. We are given a depth-$6$ restriction of a prefix tree policy $\ptp$, induced by the solution of Problem~\eqref{eq:opt_prob}. To enumerate all the satisfying assignments of the SAT problem, we add a constraint that the next solution should be different every time our SAT algorithm finds a solution. We end up with a total of $6$ solutions, which are all the possible renamings of the true reward machine, meaning that the true reward machine is learned up-to-renaming.  We also show how the non-stuttering condition of \sectionref{sec:conn} helps reduce the size of the negative example set, yet still recovering the true reward machine model. While some reduction in the SAT solver time is achieved, the drastic gain is in the time required to encode all the negative examples into the SAT solver, making the overall procedure orders of magnitude faster. For the remaining examples, non-stuttering is assumed.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \hline
        & Depth & $|\mathcal{S}|$& $|\mathcal{A}|$ & $|\mathcal{E}^-|$ & SAT time (sec) \\ \hline 
        Ex.1 w/ \eqref{eq:non_stutter} &  6 & 16 & 4& 7264 & 0.81  \\ \hline  
        Ex.1 w/o \eqref{eq:non_stutter} &  6 & 16 & 4& 168341 & 6.97  \\ \hline  
        Ex.2 &  9 & 9 & 4&241435 & 2.859  \\ \hline
        Ex.3 &  9 & 60 & 9&31656 & 0.558 \\ \hline
        Ex.4 &  8 & 60 & 9&24763 & 0.483  \\ \hline
    \end{tabular}
    \caption{Solution statistics for the examples.}
    \label{tab:stat_exp1}
    \vspace{-0.4cm}
\end{table}



For our second example, we add a hallway between the rooms, as shown in Figure~\ref{fig:rooms_with_hallway}. This is a $3\times3$ grid world, where the corresponding label of each room is shown. The reward machine is kept the same, and the atomic proposition $\mathrm{H}$ is added as a self-loop to all the nodes. With this added hallway, longer atomic proposition prefixes are required to reach all nodes of the reward machine, showing how the underlying MDP affects the required depth for learning a reward machine. For example, the shortest atomic proposition that can reach $u_3$ now is $\sigma = \mathrm{AHBHC}$ instead of $\mathrm{ABC}$ as in the previous example. With a depth $9$ prefix tree policy, the reward machine is again learned up-to-renaming. 
\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{images/rooms_gridworld_w_hallway.pdf}
    \caption{Hallway Grid World.}
    \label{fig:rooms_with_hallway}
\end{figure}

For our third example, we implement a modified block world problem \cite{khodeir2023learning,wolfe2006decision}. There are three blocks colored green, yellow and red, as well as 3 piles. Each stacking configuration of the blocks is a state of the MDP, and the action space consists of selecting a pile to pick from and a pile to place onto. We can only grab the top block of any stack. %The total number of states is 60, and the total number of actions is 9. 
Action outcomes are assumed deterministic. The goal is to stack the blocks in the ordered configurations $\mathrm{a,b,c}$ shown in Figure~\ref{fig:stacking_mdp}. All other states have the label $\mathrm{I}$, denoting intermediate states. The corresponding reward machine is shown in Figure~\ref{fig:stacking_rm}. If the robot stacks the blocks in the order $\mathrm{a} \to \mathrm{b} \to \mathrm{c}$, it gets a reward of $1$. With a depth $9$ prefix tree policy and $u_{\mathrm{max}} = 3$, our algorithm recovers $2$ consistent reward machines, which are the true reward machine up-to-renaming.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/stacking__1_.png}
    \caption{Block World Problem. The left-most stacking configuration has label $a$, where all blocks are stacked on the first pile with green being under yellow and yellow being under red. Similarly, the middle configuration has label $b$ and the right-most configuration has label $c$.}
    \label{fig:stacking_mdp}
    \vspace{-0.2cm}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\linewidth]{images/block_stacking_a.pdf}
    \caption{Stacking Reward Machine.}
    \label{fig:stacking_rm}
    \vspace{-0.3cm}
\end{figure}

For our last example, we introduce a ``bad'' state, shown in Figure~\ref{fig:badstate}. The true reward machine is shown in Figure~\ref{fig:bad_state_rm}.  The robot's task is to stack the blocks in the order $\mathrm{a} \to \mathrm{b}$ without going through $\mathrm{d}$. If it does so, it reaches $u_2$ and gets a reward of $1$ forever. If during execution it passes through $\mathrm{d}$, it will get a smaller (yet more immediate) reward of $0.2$, but it will get stuck at $u_3$ with $0$ reward forever. We note that the product policy is uniformly random in both $u_2$ and $u_3$. This means that traces such as  $\mathrm{a,I,b}$ and $\mathrm{a,I,d,b}$ look identical from a policy perspective, as both reach nodes with uniformly random policies, while the first being more desirable than the second. With a depth $8$ policy and $u_{\mathrm{max}} = 4$, our algorithm recovers more than $1000$ consistent reward machine models, reflecting the ambiguity in this specification. However, when setting $u_{\mathrm{max}} = 3$, we recover the reward machine shown in Figure~\ref{fig:minimal} up-to-renaming, that is, we find a smaller reward machine consistent for this task. 

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.33\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/bad_state__1_.png}
        \caption{}
        \label{fig:badstate}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.54\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{images/bad_state_rm.pdf}
        \caption{}
        \label{fig:bad_state_rm}
    \end{subfigure}
    \caption{(a): the block stacking configuration with label $d$ that we want our robot to avoid. (b): the ground truth reward machine capturing that behavior.}
    \label{fig:mainfigure}
    \vspace{-0.3cm}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{images/minimal_consis_rm.pdf}
    \caption{Minimal consistent reward machine with the task of example 4.}
    \label{fig:minimal}
    \vspace{-0.3cm}
\end{figure}

\vspace{-2mm}
\section{Conclusion}
In this work, we present a framework for learning reward machines from partially-observed optimal policies, where neither the rewards nor the reward machine states are available to the learner. Our method, instead, uses a prefix tree policy of sufficient depth in constructing a SAT problem instance from which a reward machine equivalent to the true underlying one can be obtained. Several examples demonstrate the success of our approach. In the future, we plan to extend the approach to the finite-sample settings, where the prefix tree policy is constructed from a finite demonstration set. 

\bibliography{references}

\appendix
\section{Proofs}

\subsection{Proof of Proposition~\ref{prop:const_k}}\label{app:proof_const_k}

\begin{proof}
    We proceed by contradiction. Assume that there exists some $\sigma' \in \mathrm{Pref}(\lnm)$, with $|\sigma'| \leq l$, a state $s \in \mathrm{Reach}(\sigma')$ and an action $a \in \mathcal{A}$ such that:
    \begin{equation}\label{eq:ineq}
        \ptp^{\mathrm{learned}}(a|s,\sigma') \neq \ptp^{\mathrm{true}}(a|s,\sigma').
    \end{equation}
    Let $u \triangleq  \delta_{\textbf{u}}^{*,\mathrm{learned}}(u_I, \sigma')$. The left-hand-side of Eq.~\eqref{eq:ineq} can be written as:
    \begin{align*}
         &\ptp^{\mathrm{learned}}(a|s,\sigma') = \pi_{\mathrm{Prod}}^{\mathrm{learned}}(a|s,u) \\
         &= \ptp^{\mathrm{true}}(a|s,\sigma), \text{ for some } \sigma \in \mathrm{Path}(u)\cap \plnm, \\
         & \hspace{4cm} \text{ with }|\sigma| \leq l,
    \end{align*}
    where the second equality is due to Eq.~\eqref{eq:const}. We get:
    \begin{equation*}
        \ptp^{\mathrm{true}}(a|s,\sigma) \neq \ptp^{\mathrm{true}}(a|s,\sigma'), \text{ where both } \sigma, \sigma' \in \mathrm{Path}(u). 
    \end{equation*}
    More precisely, $\delta_{\mathbf{u}}(u_I,\sigma) = \delta_{\mathbf{u}}(u_I,\sigma')$. Due to the contrapositive of Lemma~\ref{lem:neg}, we have a contradiction. Similarly, due to the full specification condition of \sectionref{sec:fs} and Eq.~\eqref{eq:const}, the support of the two prefix policies will be the same by construction. 
\end{proof}

\subsection{Proof of Proposition~\ref{prop:prop6}}\label{app:proof_prop6}
We start by formally defining some important concepts that will be central for proving our result. Our proof idea requires reasoning about joint (i.e. synchronized) paths over two distinct reward machine models, and being able to shrink these joint paths by removing cycles (i.e. loops). To start, we define \emph{cycles} in a product MDP model $\mathcal{G}\times \mathcal{M}_L$ as follows: 
\begin{definition}
    Given a product MDP model $\mathcal{G}\times \mathcal{M}_L$ and a proposition sequence $\sigma = l_1 \cdots l_k$, generated from a state sequence $\tau = (s_1,s_2,\cdots, s_k)$ (i.e., $\sigma \in \plnm$), we say that a subsequence $\sigma_{i:j}$ of $\sigma$ is a \textbf{cycle} in $\mathcal{G}\times \mathcal{M}_L$ if $s_i = s_j$ and $\delta_{\textbf{u}}^*(u_I,\sigma_{:i}) = \delta_{\textbf{u}}^*(u_I,\sigma_{:j})$.
\end{definition}
  


We will use the above definition to construct shorter label sequences with no cycles given a long label sequence. In particular, let $ l_c \triangleq |\mathcal{S}||\mathcal{U}|$ be the number of states in $\mathcal{G} \times \mathcal{M}_L$. By the pigeonhole principle, we know that any state trajectory of length more than $l_c$ has to visit some product state more than once, meaning that it has at least one cycle. In particular, given any proposition sequence $\sigma$, with $|\sigma| > l_c$, let $\bar \sigma$ be the subsequence of $\sigma$ obtained by removing all the cycles in $\sigma$. Then, we know that $|\bar\sigma|\leq l_c$, since $\bar \sigma$ has at most all the unique states from $\sigma$, which cannot exceed $l_c$. Note that removing cycles preserves the last product state reached from following $\sigma$.

Next, we define \emph{synchronized reward machine models}.
\begin{definition}\label{def:dsync}
    Let $\mathcal{G}_1 = (\mathcal{U}_1, u_I^1, \mathrm{AP}, \delta_{\mathbf{u}}^1),\  \mathcal{G}_2 = (\mathcal{U}_2, u_I^2, \mathrm{AP}, \delta_{\mathbf{u}}^2)$ be two reward machine models, with $|\mathcal{U}_1|,|\mathcal{U}_2|\leq u_{\mathrm{max}}$. The \textbf{synchronized reward machine model} is a reward machine model defined as follows:
    \begin{align*}
    \mathcal{G}^{\mathrm{sync}} &= (\mathcal{U}^{\mathrm{sync}}, u_I^{\mathrm{sync}}, \mathrm{AP}, \delta_{\mathbf{u}}^{\mathrm{sync}}) \\
    \mathcal{U}^{\mathrm{sync}} &= \mathcal{U}_1\times \mathcal{U}_2, \\
    u_I^{\mathrm{sync}} &= (u_I^1,u_I^2), \\
    \delta_{\mathbf{u}}^{\mathrm{sync}}((u_i^1,u_j^2), l) &= (\delta_{\mathbf{u}}^1(u_i^1,l), \delta_{\mathbf{u}}^2(u_j^2,l)), \quad l \in \mathrm{AP}. 
\end{align*}
\end{definition}

Similarly to a regular reward machine model,  the product $\mathcal{G}^{\mathrm{sync}} \times \mathcal{M}_L$ is well defined. The total number of states in $\mathcal{G}^{\mathrm{sync}} \times \mathcal{M}_L$ is upper bounded by $l = |\mathcal{S}|u_{\mathrm{max}}^2$. In particular, consider an arbitrary label sequence $\sigma \in \mathrm{Pref}(\lnm)$, generated from a state sequence $\tau = (s_1,s_2,\cdots, s^*)$, with $|\sigma| > |\mathcal{M}|u_{\mathrm{max}}^2$, and let $(u,u') = \delta_{\mathbf{u}}^{\mathrm{sync}, *}(u_I^{\mathrm{sync}}, \sigma)$. This means that the synchronized product state $(u,u',s^*)$ is reachable in $\mathcal{G}^{\mathrm{sync}} \times \mathcal{M}_L$. Thus, by removing cycles of $\sigma$ in  $\mathcal{G}^{\mathrm{sync}} \times \mathcal{M}_L$, we can construct a shorter prefix $\bar \sigma$, with $|\bar \sigma| \leq |\mathcal{M}|u_{\mathrm{max}}^2$, such that $(u,u') = \delta_{\mathbf{u}}^{\mathrm{sync}, *}(u_I^{\mathrm{sync}}, \bar \sigma)$, and $s^*$ is the MDP state reached.


 We are now ready for the proof of Proposition~\ref{prop:prop6}. We provide Figure~\ref{fig:proof_illustration} as an illustration of the proof. 

\begin{proof}
    For a shorthand notation, denote the first SAT instance in the proposition statement as $\texttt{SAT}_{{u_{\mathrm{max}}},l}$ and the second SAT instance as $\texttt{SAT}_{{u_{\mathrm{max}}},j}$. Also note that $\mathcal{E}^{-}_l \subseteq \mathcal{E}^{-}_j$ when $j\geq l$ as the negative examples can only grow as the depth increases. Throughout the proof, we interchange $\ptp^j$ and $\ptp$ as both are equal up to depth $j$. 

We need to show that the additional negative examples in $\mathcal{E}^{-}_j \setminus \mathcal{E}^{-}_l $ do not change the set of satisfying assignments. The $\impliedby$ direction is straightforward, since a satisfying assignment cannot become unsatisfying by removing constraints. 

For the $\implies$ direction, take a satisfying assignment $\{B_k\}_{k=1}^{|\mathrm{AP}|}$ for
$\texttt{SAT}_{{u_{\mathrm{max}}},l}$ and assume by contradiction that $\{B_k\}_{k=1}^{|\mathrm{AP}|}$ is not satisfying for $\texttt{SAT}_{{u_{\mathrm{max}}},j}$ with $j>l$. Consider the reward machine model $\mathcal{G}^{\mathrm{learned}}$, with the transition function $\delta_{\textbf{u}}^{\mathrm{learned}}$, corresponding to $\{B_k\}_{k=1}^{|\mathrm{AP}|}$. Since $\{B_k\}_{k=1}^{|\mathrm{AP}|}$ is unsatisfying for $\texttt{SAT}_{{u_{\mathrm{max}}},j}$, then there exists a negative example $(\sigma, \sigma')\in \mathcal{E}^{-}_j \setminus \mathcal{E}^{-}_l$ such that
\begin{equation}\label{eq:contra_assump}
 \delta_{\textbf{u}}^{\mathrm{learned},*}(u_I,\sigma) = \delta_{\textbf{u}}^{\mathrm{learned},*}(u_I,\sigma'),   
\end{equation}
while $\ptp^j(a^*|s^*,\sigma) \neq \ptp^j(a^*|s^*,\sigma')$ for some $(s^*,a^*) \in \mathcal{S}\times \mathcal{A}$. These two facts are shown as ${\color{green} \neq_{\ptp}}$ and ${\color{red}=_u}$ connecting $\sigma$ and $\sigma'$ in Figure~\ref{fig:proof_illustration}. 

Now, let $\mathcal{G}^{\mathrm{true}}= (\mathcal{U}, u_I, \mathrm{AP}, \delta_{\textbf{u}}^{\mathrm{true}})$ be a reward machine model consistent with $\ptp$. We define the following nodes, along with the associated product states:
\begin{align}
    u^{\mathrm{true}} &\triangleq  \delta_{\textbf{u}}^{\mathrm{true}} (u_I, \sigma), \quad  (u^{\mathrm{true}},s^*) \in \mathcal{G}^{\mathrm{true}}\times \mathcal{M}_L, \notag \\
    u^{\mathrm{learned}} &\triangleq  \delta_{\textbf{u}}^{\mathrm{learned}} (u_I, \sigma),  (u^{\mathrm{learned}},s^*) \in \mathcal{G}^{\mathrm{learned}}\times \mathcal{M}_L, \notag\\
    u^{\mathrm{true},\prime} &\triangleq  \delta_{\textbf{u}}^{\mathrm{true}} (u_I, \sigma'), \quad  (u^{\mathrm{true},\prime},s^*) \in \mathcal{G}^{\mathrm{true}}\times \mathcal{M}_L,\notag \\
    u^{\mathrm{learned},\prime} &\triangleq  \delta_{\textbf{u}}^{\mathrm{learned}} (u_I, \sigma'),  (u^{\mathrm{learned},\prime},s^*) \in \mathcal{G}^{\mathrm{learned}}\times \mathcal{M}_L. \notag
\end{align}
Hence, by Eq.~\eqref{eq:contra_assump},  we have that $u^{\mathrm{learned}} = u^{\mathrm{learned},\prime}$.
Let $\mathcal{G}^{\mathrm{sync}}$ be the synchronized reward machine model between $\mathcal{G}^{\mathrm{learned}}$ and $\mathcal{G}^{\mathrm{true}}$ according to Definition~\ref{def:dsync}. We observe the following:
\begin{align*}
    (u^{\mathrm{true}}, u^{\mathrm{learned}}) &= \delta_{\mathbf{u}}^{\mathrm{sync}, *}(u_I^{\mathrm{sync}}, \sigma), \\
    &(u^{\mathrm{true}}, u^{\textsf{learned}},s^*) \in \mathcal{G}^{\textsf{sync}}\times \mathcal{M}_L,\\
    (u^{\textsf{true},\prime}, u^{\textsf{learned},\prime}) &= \delta_{\mathbf{u}}^{\textsf{sync}, *}(u_I^{\textsf{sync}}, \sigma'), \\
    &(u^{\textsf{true},\prime}, u^{\textsf{learned},\prime},s^*) \in \mathcal{G}^{\textsf{sync}}\times \mathcal{M}_L.
\end{align*}
This means that the synchronized product states $ (u^{\textsf{true}}, u^{\textsf{learned}},s^*)$ and $(u^{\textsf{true},\prime}, u^{\textsf{learned},\prime},s^*)$ are both reachable in $\mathcal{G}^{\textsf{sync}} \times \mathcal{M}_L$. Thus, by removing cycles, there must exist shorter sequences, $\bar \sigma, \bar \sigma'$, with $|\bar \sigma|,|\bar \sigma'|\leq |\mathcal{S}|u_{\textsf{max}}^2$, such that: 
\begin{align}\label{eq:shorter_seq}
    (u^{\textsf{true}}, u^{\textsf{learned}}) &= \delta_{\mathbf{u}}^{\textsf{sync}, *}(u_I^{\textsf{sync}}, \bar \sigma), \notag \\
    (u^{\textsf{true},\prime}, u^{\textsf{learned},\prime}) &= \delta_{\mathbf{u}}^{\textsf{sync}, *}(u_I^{\textsf{sync}}, \bar \sigma').
\end{align}
Note that $s^*$ is still the reached MDP state in both synchronized product nodes above. By the definition of $\delta_{\mathbf{u}}^{\textsf{sync},*}$, we can decompose Eq.~\eqref{eq:shorter_seq} into:
\begin{align}\label{eq:shrink}
     u^{\mathrm{true}} &=                    \delta_{\textbf{u}}^{\mathrm{true}} (u_I, \bar \sigma),\ 
    u^{\mathrm{learned}} =                 \delta_{\textbf{u}}^{\mathrm{learned}} (u_I, \bar \sigma), \notag\\
    u^{\mathrm{true},\prime} &=  \delta_{\textbf{u}}^{\mathrm{true}} (u_I, \bar \sigma'),\ 
    u^{\mathrm{learned},\prime} =  \delta_{\textbf{u}}^{\mathrm{learned}} (u_I, \bar \sigma').
\end{align}
This means that $\sigma$ and $\bar \sigma$ lead to the same node $u^{\mathrm{true}}$ in $\mathcal{G}^{\mathrm{true}}$. Similarly, $\sigma'$ and $\bar \sigma'$ both lead to the same node $u^{\mathrm{true},\prime}$.
Since $\mathcal{G}^{\mathrm{true}}$ is consistent with $\ptp$, the following holds:
\begin{align}\label{eq:equalities}
    \ptp(a|s^*,\bar \sigma) &= \ptp(a|s^*,\sigma) ,\quad  \forall a \in \mathcal{A}, \notag \\
    \ptp(a|s^*,\bar \sigma') &= \ptp(a|s^*,\sigma') ,\quad \forall a \in \mathcal{A}.
\end{align}
Since $\ptp(a^*|s^*, \sigma) \neq \ptp(a^*|s^*, \sigma')$ due to our contradiction assumption, we conclude from Eq.~\eqref{eq:equalities} that $\ptp(a^*|s^*,\bar \sigma) \neq \ptp(a^*|s^*,\bar \sigma')$. However, $u^{\mathrm{learned}} = u^{\mathrm{learned},\prime}$ combined with Eq.~\eqref{eq:shrink} implies that $\delta_{\textbf{u}}^{\mathrm{learned}} (u_I, \bar \sigma) = \delta_{\textbf{u}}^{\mathrm{learned}} (u_I, \bar \sigma')$, contradicting that  $\{B_k\}_{k=1}^{|\mathrm{AP}|}$ is a SAT assignment for the depth $l$. This concludes the proof.
\end{proof}


\begin{figure}[htb!]
    \centering
    \includegraphics[width = 0.5\textwidth]{images/proof_diag_u.pdf}  % Adjust width as needed
\caption{Proof Illustration of Proposition~\ref{prop:prop6}. ${\color{green}=_{\ptp}},{\color{green}\neq_{\ptp}}$ means that the prefix tree policy $\ptp$ is equal/different for the corresponding sequences. ${\color{red}=_u}$ means that the corresponding sequences arrive at the same node in $\mathcal{G}^{\mathrm{learned}}$.}
    \label{fig:proof_illustration}
\end{figure}



 
\end{document}
