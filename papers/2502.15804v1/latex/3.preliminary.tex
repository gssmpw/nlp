\begin{table*}[ht]
    \centering
    \caption{Cosine Similarity of Retained KV Cache in Per-Head KV Compression across LongBench Sub-datasets}
    \fontsize{15}{14}\selectfont
    \renewcommand{\arraystretch}{1.2} % 调整行间距
    \setlength{\tabcolsep}{4pt} % 调整列间距
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{ccccccccccccccccccc}
        \toprule
        \multirow{2}{*}{KV Budget} & 
        \multicolumn{3}{c}{Single-Doc QA} & 
        \multicolumn{3}{c}{Multi-Doc QA} & 
        \multicolumn{3}{c}{Summarization} & 
        \multicolumn{3}{c}{Few-shot Learning} & 
        \multicolumn{2}{c}{Coding} & 
        \multirow{2}{*}{Avg} &
        \multirow{2}{*}{Max} &
        \multirow{2}{*}{Min} &
        \multirow{2}{*}{Std}\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-15}
        & NtrQA & Qasper & MF-en & HpQA & 2WMQA & Musiq & GovRp & QMSum & MNews & TREC & TriQA & SAMSum & LCC & RB-P \\
        \midrule
        \multicolumn{19}{c}{Llama-3.3-70B-Instruct } \\
        128 & 0.974 & 0.977 & 0.977 & 0.979 & 0.980 & 0.979 & 0.971 & 0.979 & 0.974 & 0.974 & 0.978 & 0.969 & 0.975 & 0.977 & 0.976 &
        0.980 & 0.969 & 0.003\\
        256 & 0.961 & 0.965 & 0.967 & 0.970 & 0.971 & 0.969 & 0.958 & 0.966 & 0.962 & 0.962 & 0.968 & 0.949 & 0.964 & 0.968 & 0.964 &
        0.971 & 0.949 & 0.006\\
        512 & 0.955 & 0.962 & 0.962 & 0.965 & 0.968 & 0.964 & 0.954 & 0.960 & 0.955 & 0.957 & 0.964 & 0.944 & 0.959 & 0.965 & 0.959 &
        0.968 & 0.944 & 0.006\\
        1024 & 0.950 & 0.961 & 0.959 & 0.961 & 0.966 & 0.959 & 0.953 & 0.956 & 0.943 & 0.956 & 0.962 & 0.947 & 0.949 & 0.962 & 0.956 &
        0.966 & 0.943 & 0.006\\
        \midrule
        \multicolumn{19}{c}{Meta-Llama-3-8B } \\
        128 & 0.904 & 0.935 & 0.932 & 0.919 & 0.933 & 0.914 & 0.929 & 0.919 & 0.938 & 0.919 & 0.928 & 0.934 & 0.933 & 0.932 & 0.926 &
        0.938 & 0.904 & 0.009\\
        256 & 0.873 & 0.912 & 0.913 & 0.897 & 0.917 & 0.887 & 0.912 & 0.891 & 0.923 & 0.896 & 0.909 & 0.911 & 0.915 & 0.909 & 0.905 &
        0.923 & 0.873 & 0.013\\
        512 & 0.886 & 0.916 & 0.917 & 0.909 & 0.924 & 0.902 & 0.917 & 0.904 & 0.933 & 0.909 & 0.917 & 0.908 & 0.925 & 0.915 & 0.913 &
        0.933 & 0.886 & 0.011\\
        1024 & 0.912 & 0.932 & 0.934 & 0.930 & 0.937 & 0.930 & 0.931 & 0.927 & 0.949 & 0.929 & 0.935 & 0.922 & 0.944 & 0.932 & 0.932 &
        0.949 & 0.912 & 0.009\\
        \midrule
        \multicolumn{19}{c}{
Mistral-Small-24B-Instruct-2501 } \\
        128 & 0.970 & 0.973 & 0.974 & 0.974 & 0.975 & 0.973 & 0.971 & 0.975 & 0.970 & 0.959 & 0.975 & 0.971 & 0.971 & 0.973 & 0.972 &
        0.975 & 0.959 & 0.004\\
        256 & 0.960 & 0.963 & 0.965 & 0.965 & 0.967 & 0.965 & 0.959 & 0.965 & 0.960 & 0.944 & 0.967 & 0.951 & 0.960 & 0.965 & 0.961 &
        0.967 & 0.944 & 0.006\\
        512 & 0.954 & 0.958 & 0.960 & 0.958 & 0.964 & 0.958 & 0.951 & 0.959 & 0.959 & 0.945 & 0.963 & 0.936 & 0.955 & 0.962 & 0.956 &
        0.964 & 0.936 & 0.007\\
        1024 & 0.954 & 0.961 & 0.961 & 0.959 & 0.965 & 0.957 & 0.951 & 0.959 & 0.963 & 0.954 & 0.962 & 0.937 & 0.959 & 0.962 & 0.957 &
        0.965 & 0.937 & 0.007\\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \label{tab:longbench}
\end{table*}

\begin{table}[ht]
\centering
\caption{GPU Utilization of Different Models. We applied Ada-SnapKV to three models, setting the KV cache budget to 128, 256, 512, and 1024, and measured its GPU utilization under tensor parallel sizes of 2, 4, and 8.}
\fontsize{24}{22}\selectfont
\renewcommand{\arraystretch}{1.2} % 调整行间距
\setlength{\tabcolsep}{4pt} % 调整列间距
\label{tab:SHA GPUutilization}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c c c c c}
\toprule
Model & KV Cache Budget & TP = 2 & TP = 4 & TP = 8 \\
\midrule
\multirow{4}{*}{LLaMA-3.3-70B-Instruct} 
    & 128  & 92.5 & 81.6 & 64.7 \\
    & 256  & 87.5 & 74.1 & 57.2 \\
    & 512  & 86.4 & 70.5 & 55.7 \\
    & 1024 & 87.2 & 71.2 & 58.2 \\
\midrule
\multirow{4}{*}{Meta-LLaMA-3-8B} 
    & 128  & 92.1 & 84.4 & 70.8 \\
    & 256  & 91.8 & 82.0 & 68.5 \\
    & 512  & 90.6 & 81.6 & 68.8 \\
    & 1024 & 90.9 & 82.1 & 69.8 \\
\midrule
\multirow{4}{*}{Mistral-Small-24B-Instruct} 
    & 128  & 93.2 & 86.3 & 75.2 \\
    & 256  & 91.7 & 82.9 & 71.9 \\
    & 512  & 91.2 & 82.0 & 70.8 \\
    & 1024 & 91.2 & 82.1 & 70.8 \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.40\linewidth}    % Slightly increased width
        \includegraphics[width=\linewidth]{latex/Batch_vs_Latency.pdf}
        \caption{Latency vs. Batch Size under Different Budgets}
        \label{fig:image-a}
    \end{subfigure}
    % \hfill
    \hspace{0.1cm}
    \begin{subfigure}[b]{0.40\linewidth}    % Slightly increased width
        \includegraphics[width=\linewidth]{latex/Budget_vs_Latency.pdf}
        \caption{Latency vs. Budget under Different Batch Sizes}
        \label{fig:image-b}
    \end{subfigure}
    \caption{Impact of Batchsize and KV Cache Budget on Inference Latency}
    \label{fig:bs_bud_lat}
\end{figure*}
\section{Preliminary}

\subsection{Observation of KV Cache Selection}

We examined per-head KV cache eviction patterns across multiple datasets using different models (LLaMA-3.3-70B-Instruct, Meta-LLaMA-3-8B, and Mistral-Small-24B-Instruct) and several subsets from LongBench v1 for our experiments and set the KV budget for the attention heads to 128, 256, 512, and 1024 as our basic experimental setup.

The per-head KV cache compression algorithm results in varying KV cache budgets across different attention heads, leading to imbalanced workloads when combined with tensor parallelism. The results, summarized in Table~\ref{tab:SHA GPUutilization}, show a clear trend: 

Decreasing GPU Utilization with Larger TP Sizes:  For instance, in LLaMA-3.3-70B-Instruct with KV cache budget 128, GPU utilization drops from 92.5\% (TP=2) to 81.6\% (TP=4) and further to 64.7\% (TP=8). A similar trend is observed in Meta-LLaMA-3-8B, where GPU utilization declines from 92.1\% (TP=2) to 84.4\% (TP=4) and 70.8\% (TP=8) under the same KV cache budget.

Impact of KV cache Budget: Lower KV cache budgets generally yield better GPU utilization. For example, in Mistral-Small-24B-Instruct with TP=4, GPU utilization is 86.3\% at KV=128 but decreases to 82.9\% at KV=256 and further to 82.1\% at KV=1024. Similarly, in LLaMA-3.3-70B-Instruct with TP=4, GPU utilization drops from 81.6\% (KV=128) to 71.2\% (KV=1024). This suggests that larger KV cache budgets lead to greater workload imbalance among attention heads, which negatively impacts GPU efficiency.  

Additionally, as shown in Table \ref{tab:longbench}, we use cosine similarity to quantify the difference in KV cache allocation between a subset and the remaining datasets. Finally, we averaged the metrics across all subsets to obtain an overall indicator. Our analysis indicates that eviction patterns remain largely consistent across datasets, confirming that the statistics for retained KV cache can guide optimization. Furthermore, the allocation pattern of the KV cache budget is influenced by the specific model used. Given this dataset-invariant nature, optimization strategies can be designed based on these profiles without requiring per-dataset recalibration.

\subsection{Empirical Performance Analysis}

To optimize the distribution of GPU workload, we built an empirical model mapping batch size, retained KV cache count, and inference latency. Measurements were taken across various configurations on a multi-GPU setup.

We conducted experiments on the LongBench v1 dataset by systematically varying both batch size and KV cache budget parameters. Specifically, we controlled these variables independently to evaluate their individual and combined effects. We simulated the inference scenario of LLaMA 70b on a single layer and obtained the latency during decoding one token. The experimental results, as illustrated in Figure \ref{fig:bs_bud_lat}, demonstrate the performance characteristics under different configurations.

For the batch size latency model (Figure a), we observe that latency ($L$) increases approximately linearly with batch size ($B$) across different budget configurations. The relationship can be expressed as $L \approx \alpha B + \beta$, where $\alpha$ represents the slope and $\beta$ the initial offset. The graph shows four distinct budget levels (128, 256, 512, and 1024), with higher budget values corresponding to steeper slopes in the linear relationship.
Similarly, the KV cache latency model (Figure b) demonstrates a comparable linear pattern, where latency ($L$) increases proportionally with the KV cache budget ($C$). This can be represented as $L \approx \gamma C + \delta$, where $\gamma$ and $\delta$ are the slope and offset parameters respectively. The data presents five different batch sizes (32, 64, 128, 256, and 512), with larger batch sizes showing more pronounced slopes in their linear relationships.
In both cases, the approximately linear nature of these relationships is particularly noteworthy, as it suggests predictable scaling behavior in the system's performance characteristics. 