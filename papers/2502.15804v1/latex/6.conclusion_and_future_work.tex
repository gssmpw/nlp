\section{Conclusion}

In this paper, we propose \textit{\AlgName{}}, a novel optimization technique designed to improve inference efficiency by dynamically adjusting the allocation of attention heads in Tensor Parallelism. \AlgName{} leverages the statistics of the retained Key-Value (KV) cache to partition attention heads based on their computational load, ensuring a balanced workload distribution across GPUs.

Our method has been extensively evaluated across various configurations, demonstrating consistent performance improvements. Specifically, \AlgName{} significantly reduces inference latency and improves GPU utilization, regardless of the number of GPUs used, the model size (including different versions of LLaMa and Mistral), or the KV cache budget. This versatility highlights the robustness of our approach, making it effective across different settings and resource constraints. Experimental results show that \AlgName{} achieves up to a 66\% increase in inference throughput, with minimal impact on model accuracy. These findings confirm that \AlgName{} is a promising solution for optimizing large-scale model inference in real-world, real-time applications.

Future work will explore the adaptability of \AlgName{} to dynamic KV cache budgets and investigate its potential for scaling to models such as Mixture of Experts (MoE), where the computational load can vary significantly across different tasks. These enhancements aim to further extend the applicability of \AlgName{} to a broader range of models and dynamic environments.


\section{Limitations and Future works}

Although \textit{\AlgName{}} provides significant improvements in inference efficiency, there are a few limitations that need to be addressed. First, \AlgName{} is designed for a single machine with multiple GPUs and does not account for scenarios involving distributed systems across multiple machines. Second, the current method primarily focuses on the parallelization of the inference process without considering the separation of the prefill and decode stages, which is critical in certain real-time applications where the inference process is more complex and involves sequential decoding. 
Additionally, \AlgName{} relies on the accuracy of the KV cache statistics for optimal head allocation. This means that the effectiveness of KV cache compression depends on the statistical properties of the cache, and any deviations from these properties could reduce the methodâ€™s performance. We will address these limitations in future work.
