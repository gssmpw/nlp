\subsection{Mathematical Model for Hybrid Parallelism in \AlgName{}}
This section presents a comprehensive mathematical formulation of the \AlgName{} system for optimizing multi-GPU inference in large language models with KV cache compression.

\subsubsection{System Parameters}
The system parameters are defined to model the components involved in the \AlgName{} strategy, for each transformer layer $l \in L$:
\begin{itemize}
    \item $H_l = \{h_1, ..., h_n\}$: attention heads
    \item $G = \{g_1, ..., g_m\}$: available GPUs
    \item $w_i$: workload for head $h_i$
    \item $r_{ij}$: replication factor of head $h_i$ on GPU $g_j$
\end{itemize}

\subsubsection{Decision Variables}
The decision variables are crucial for determining the optimal assignment of attention heads to GPUs:

\begin{equation}
    x_{ij} = \begin{cases} 
        r_{ij} & \text{if head } h_i \text{ is assigned to GPU } g_j \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

\subsubsection{Parallelism Constraints}

The hybrid parallelism approach in \AlgName{} combines Tensor Parallelism and Data Parallelism to address the challenges posed by head-wise KV Cache compression. Tensor Parallelism ensures that the computational workload is distributed across multiple GPUs by partitioning the attention heads, while Data Parallelism introduces redundancy by allowing selective replication of attention heads across GPUs. This hybrid approach aims to balance the computational load, ensuring that no single GPU becomes a bottleneck due to uneven KV Cache compression rates.


\textbf{Head Distribution}
Each head must have at least one GPU assignment to ensure that all computations are accounted for.
\begin{equation}
    \sum_{j \in G} \mathbb{1}(x_{ij} > 0) \geq 1 \quad \forall i \in H_l, \forall l \in L
\end{equation}

\textbf{Data Parallelism}
Total replication factor for each head:
\begin{equation}
    \sum_{j \in G} r_{ij} \leq R_{max} \quad \forall i \in H_l, \forall l \in L
\end{equation}
The total replication factor for each head across all GPUs must not exceed a predefined maximum, $R_{max}$. This prevents over-replication, which could lead to increased communication overhead and reduced efficiency.

\subsubsection{Optimization Objective}
The primary goal of the \AlgName{} strategy is to minimize the maximum processing time across all GPUs. This is formulated as:
\begin{equation}
    \min \max_{j \in G} \sum_{l \in L} \sum_{i \in H_l} \frac{x_{ij} w_i}{r_{ij}}
\end{equation}

\subsubsection{System Efficiency}
System efficiency is calculated to evaluate how well the GPUs are utilized. The formula for efficiency is:
\begin{equation}
    E = \frac{1}{|G|} \sum_{j \in G} \frac{\sum_{l \in L} \sum_{i \in H_l} \frac{x_{ij} w_i}{r_{ij}}}{\max_{k \in G} \sum_{l \in L} \sum_{i \in H_l} \frac{x_{ik} w_i}{r_{ik}}}
\end{equation}

