
\section{Evaluation}
In this section, we primarily conduct a detailed evaluation of the \AlgName{} method proposed in the previous sections, assessing whether \AlgName{} can effectively improve inference efficiency in hybrid parallelism approaches  during Per-Head KV cache Compression.

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{latex/TP_llama70b.pdf}
        \caption{Rate on LLaMA-3.3-70B-Instruct}
        \label{fig:aha_tp16}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{latex/TP_llama8b.pdf}
        \caption{Rate on Meta-LLaMA-3-8B}
        \label{fig:aha_nodp}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{latex/TP_mistral.pdf}
        \caption{Rate on Mistral-Small-24B-Instruct}
        \label{fig:aha_dp}
    \end{subfigure}
    \caption{Throughput Gain Rates of \AlgName{} on different models, where the throughput of the baseline model is regarded as 1.0.}
    \label{fig:gainacrossmodel}
\end{figure*}

\subsection{Experimental Setup}
We used Python 3.10.16 as the programming language and implemented \AlgName{} on PyTorch. We leveraged AdaKV from KVPress as the per-head KV cache compression algorithm and tested our approach using Llama models \cite{llama3} of different sizes as well as the Mistral model \cite{jiang2023mistral7b}.

\textbf{Model and Hardware Configurations}  We used the LLaMA-3.3-70B-Instruct model, the Meta-LLaMA-3-8B model, and the Mistral-Small-24B-Instruct-2501 model as our experimental models. Our hardware environment consists of four Nvidia A100-80G GPUs and 960GB of CPU memory.

\textbf{Dataset Configurations}
We used LongBench v1 \cite{bai2024longbenchbilingualmultitaskbenchmark} as the evaluation dataset. LongBench v1 is a bilingual, multi-task benchmark for long-text understanding, enabling a more rigorous evaluation of long-text comprehension. 


\textbf{Baseline} To the best of our knowledge, this study is the first to highlight the issue of GPU workload imbalance caused by per-head KV cache compression algorithms in tensor parallelism. Therefore, we chose the situation of conducting tensor parallel inference with Per-Head KV Compression but without \AlgName{} as the baseline. By comparing the cases with and without \AlgName{}, we can determine whether \AlgName{} can improve GPU utilization and reduce inference latency.


\textbf{Evaluation Metrics}
The purpose of the evaluation experiments is to verify the following two questions: First, whether \AlgName{} can effectively reduce inference latency, and second, whether \AlgName{} can effectively increase GPU utilization. Therefore, our evaluation metrics are the following two aspects: inference time and GPU utilization.

\subsection{Performance Evaluation}
Performance evaluation is a critical component in assessing the effectiveness of the \AlgName{} method in improving multi-GPU inference efficiency for large language models. This subsection outlines the key metrics and methodologies used to evaluate the performance of \AlgName{}, ensuring a comprehensive understanding of its impact on system efficiency.

\subsubsection{Throughput Improvement}
We evaluated the performance of \AlgName{} across different models. Specifically, we selected a diverse set of models, including LLaMA-3.3-70B-Instruct, Meta-LLaMA-3-8B, and Mistral-Small-24B-Instruct, and set the tensor parallel size to either 4 or 8 with RC fixed at 4. Throughput gains relative to SHA were measured under KV cache budgets of 128, 256, 512, and 1024. As shown in Figure \ref{fig:gainacrossmodel}, \AlgName{} accelerates various models, and due to the inherent characteristics of each model, its throughput under SHA conditions differs. As a result, the acceleration effect of \AlgName{} may vary. Among them, \AlgName{} achieves the highest benefit of up to 1.66 on the Llama-3.3-70B-Instruct model.

We also evaluated the performance of \AlgName{} under different tensor parallel sizes. Firstly, we observed that \AlgName{} provides acceleration benefits under all tensor parallel size conditions. By comparing subfigures a, b, and c, we can see that as the tensor parallel size increases, the acceleration effect of \AlgName{} significantly improves under the same model and KV cache budget conditions. This suggests that \AlgName{} is more likely to achieve better acceleration performance when the tensor parallel size is larger.

As the KV cache budget increases, the acceleration effect of \AlgName{} shows a slight improvement for Meta-LLaMA-3-8B and Mistral-Small-24B-Instruct, while for LLaMA-3.3-70B-Instruct, the acceleration effect initially improves significantly and then slightly decreases. Overall, across these three models, the acceleration effect of \AlgName{} tends to improve as the KV cache budget increases.

In general, \AlgName{} demonstrates acceleration benefits across different models, tensor parallel sizes, and KV cache budgets. At the same time, the effectiveness of \AlgName{} is influenced by these factors, and the results may vary accordingly.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{latex/ablation.png} % 图片宽度设置为单列的适当比例
  \caption{Ablation Test Among Standard Model, \AlgName{} w/o Fair-copying and \AlgName{} with Fair-copying}
  \label{fig:ablation}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{latex/GPU_util.pdf} % 图片宽度设置为单列的适当比例
  \caption{GPU Utilization with different Data Parallel Size on LLaMA-3.3-70B.}
  \label{fig:GPUutilization}
\end{figure}

\subsubsection{GPU Utilization Improvement}

We used LLaMA-3.3-70B-Instruct to evaluate the impact of \AlgName{} on GPU utilization. In Figure \ref{fig:ablation}, we conducted ablation tests among standard model, \AlgName{} without Fair-copying and \AlgName{} with Fair-copying. The result indicate that both \AlgName{} with or without Fair-copying significantly improves GPU utilization compared to standard model, demonstrating that the \AlgName{} method can effectively balance GPU loads. Moreover, \AlgName{} with Fair-copying shows further improvement over \AlgName{} without Fair-copying. Then, to measure the impact of the parameter on the \AlgName{} with Fair-copying group, we set the size of parallel size, which is also the count of copied heads (CH), to 1, 2, 3, and 4. We measured the GPU utilization for these groups under KV cache budgets of 128, 256, 512, and 1024, with the results shown in Figure \ref{fig:GPUutilization}.
The results suggesting that incorporating only a small number of copied heads via Data Parallel can enhance the performance of the \AlgName{} strategy greatly. We also observed a positive correlation between the performance of \AlgName{} and CH; as CH increases, the GPU utilization curve becomes less steep, indicating that while increases in CH yield significant benefits when CH is small, the incremental gains diminish when CH is larger.