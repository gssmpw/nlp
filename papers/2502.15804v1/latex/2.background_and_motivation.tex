\section{Background and Motivation}






\subsection{Parallel Inference}

In the realm of large language models (LLMs), efficient inference is crucial for practical applications. To achieve this, various parallelization techniques have been developed to leverage the power of multiple GPUs. According to different ways of splitting the model, these parallelization techniques can be categorized into data parallelism, tensor parallelism, and pipeline parallelism.


Data parallelism\cite{zinkevich2010parallelized,  dean2012large, krizhevsky2014weirdtrickparallelizingconvolutional, goyal2018accuratelargeminibatchsgd, ben2019demystifying} is a method that was proposed early on, which focuses on data-level parallelism. In this method, the input data are split across multiple devices, and the model is replicated on each device to process sub-batches concurrently. 

Tensor parallelism \cite{lu2017flexflow, shazeer2018mesh, shoeybi2019megatron, rajbhandari2020zero} splits individual tensor operations across multiple devices for parallel computation, resulting in less and more concentrated communication, which is suitable for high-bandwidth GPU clusters internally (such as NVLink interconnects), thus reducing latency. 

Following the discussion on tensor parallelism, the last method is pipeline parallelism\cite{petrowski1993performance, harlap2018pipedreamfastefficientpipeline, huang2019gpipe, kim2023bpipe, qi2023zerobubblepipelineparallelism, qi2024pipelineparallelismcontrollablememory}. Pipeline parallelism reduces the memory consumption of a single computing device by distributing different layers of the model across multiple devices. TeraPipe\cite{li2021terapipe} introduced an optimized pipeline scheme tailored for inference, but its practical adoption remains limited due to several challenges. Specifically, during the forward inference process in pipeline parallelism, data must sequentially pass through each stage, which introduces dependencies between stages and can lead to inefficiencies. This sequential dependency often results in increased latency, which is particularly problematic in inference scenarios where low latency is critical. Additionally, pipeline parallelism requires sophisticated scheduling and mini-batch management, further complicating its implementation and limiting its widespread use.

Tensor parallelism has become the preferred method for inference in large language models (LLMs) due to its efficient communication, natural fit with the Transformer architecture, compatibility with data parallelism, reduced latency, scalability, and implementation simplicity. For example, vLLM \cite{kwon2023efficient} prioritizes support for tensor parallelism. These advantages make it particularly well-suited for multi-GPU inference in mainstream scenarios, where low latency and high throughput are critical.

\subsection{KV Cache Compression Techniques}
Inference efficiency for large language models is critically limited by both memory bandwidth and computational power. A widely adopted optimization technique to mitigate these limitations is KV Cache compression \cite{ge2024modeltellsdiscardadaptive, zhang2023h2o, yang2024pyramidinfer}, which reduces the memory required for storing previous tokens’ key and value states in the self-attention layers. KV cache compression methods can generally be classified into two categories: \textit{Balanced Head-wise Compression} and \textit{Imbalanced Head-wise Compression}.

The \textit{Balanced Head-wise Compression} methods, such as StreamingLLM \cite{xiao2024efficientstreaminglanguagemodels}, SnapKV \cite{li2024snapkvllmknowslooking}, and PyramidKV \cite{cai2024pyramidkvdynamickvcache}, allocate an equal KV cache budget across all attention heads. These methods aim to maintain a uniform distribution of memory resources, ensuring that each attention head receives the same cache allocation, which is simple and efficient in many scenarios.

In contrast, \textit{Imbalanced Head-wise Compression} algorithms, such as Ada-KV \cite{feng2024adakv}, dynamically adjust the KV cache budget for each attention head based on the specific computational and memory requirements of the current layer. This approach offers greater flexibility and optimization potential by tailoring the cache allocation to the individual characteristics of attention heads, resulting in improved performance in scenarios with varying resource demands.

Both types of KV cache compression methods contribute to the optimization of inference efficiency, but they differ in their approach to resource distribution. The \textit{Balanced Head-wise Compression} methods are straightforward and work well in general cases, while \textit{Imbalanced Head-wise Compression} methods, with their adaptive allocation, can provide significant performance improvements for more complex models and workloads.The Figure~\ref{fig:KV_methods_with_TP} shows common KV cache compression methods and illustrates the combination of different KV cache compression methods with tensor parallelism.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{latex/KV_methods_with_TP.png}
    \caption{Common KV Cache compression methods with Tensor Parallelism}
    \label{fig:KV_methods_with_TP}
\end{figure*}

Table~\ref{tab:KV_Methods} presents the performance of common KV cache compression methods on the LongBench v1 \cite{bai2024longbenchbilingualmultitaskbenchmark} dataset. We tested different KV cache compression methods on Llama-3.1-8B-Instruct with KV Cache budgets set to 128, 256, 512, 1024, and 2048. As described in Table~\ref{tab:KV_Methods}, across various KV cache budget settings, Ada-SnapKV (the optimized version of SnapKV based on Ada-KV) achieved higher average scores than other methods on multiple tasks in the Longbench v1 dataset, demonstrating the effectiveness of \textit{Imbalanced Head-wise Compression} methods.


\subsection{Challenges in Parallel Inference with Head-wise Compression}

Head-wise Tensor Parallelism is widely adopted in inference for large language models due to two main reasons: (1) the model size is often too large to fit into a single GPU, requiring multiple GPUs for storage and computation; (2) distributing the KV cache across different GPUs allows for increased batch size and throughput. Additionally, Tensor Parallelism provides greater inference performance on each GPU compared to other parallel strategies. 

However, a significant challenge arises when applying head-wise KV cache compression in Tensor Parallelism. The non-uniform distribution of KV cache across different attention heads leads to computational load imbalance. The imbalance in KV cache distribution across attention heads causes certain GPUs to become overloaded while others remain underutilized. In each inference layer, the slowest device, which handles the heaviest computational load, becomes the bottleneck, forcing all other devices to wait for it to complete before proceeding with the AllReduce operation. This synchronization requirement leads to significant idle time on multiple GPUs, which severely degrades overall inference efficiency.

Our approach addresses these challenges by optimizing the combination of tensor parallelism and fine-grained head-wise data parallelism, ensuring more balanced workload distribution and reducing the inference latency caused by synchronization bottlenecks.

% \section{Related Work}

% Inference efficiency for large language models is critically limited by both memory bandwidth and computational power. A widely adopted optimization technique is KV Cache compression \cite{ge2024modeltellsdiscardadaptive, zhang2023h2o, yang2024pyramidinfer}, which reduces the memory required for storing previous tokens’ key and value states in the self-attention layers. KV cache compression methods can generally be classified into two categories: \textit{Balanced Head-wise Compression} and \textit{Imbalanced Head-wise Compression}.

% The \textit{Balanced Head-wise Compression} methods implement the same compression strategy for all attention heads, aiming for a balanced cache allocation across the heads. For instance, StreamingLLM \cite{xiao2024efficientstreaminglanguagemodels} retains the previous $k$ sink tokens in addition to the recent window. H2O \cite{zhang2024h2o} further prioritizes important cache entries based on accumulated attention scores, while SnapKV \cite{li2024snapkvllmknowslooking} selects entries using attention scores from the observation window. Recently， PyramidInfer \cite{yang2024pyramidinferpyramidkvcache} and PyramidKV \cite{cai2024pyramidkvdynamickvcache} recognize distinct attention distribution patterns across different layers, allocating more cache to lower layers. However, these methods disregard the varying importance of different heads in actual computations. 

% In contrast, \textit{Imbalanced Head-wise Compression} algorithms, such as Ada-KV \cite{feng2024adakv} and HeadKV\cite{????}, dynamically adjust the KV cache budget for each attention head based on the specific computational and memory requirements of the current layer. AdaKV determines the KV cache budget for each attention head during inference, with a fully dynamic allocation. In contrast, HeadKV pre-allocates a fixed base budget for each head based on its importance and then assigns an additional dynamic budget on top of it. This approach offers greater flexibility and optimization potential by tailoring the cache allocation to the individual characteristics of attention heads, resulting in improved performance in scenarios with varying resource demands. Table~\ref{tab:KV_Methods} in the appendix show that Ada-SnapKV outperforms other methods on multiple tasks in the LongBench v1 \cite{bai2024longbenchbilingualmultitaskbenchmark} dataset, demonstrating the effectiveness of \textit{Imbalanced Head-wise Compression} approach.

% However, a significant challenge arises when applying \textit{Imbalanced Head-wise Compression} in Tensor Parallelism. The non-uniform distribution of KV cache across different attention heads leads to computational load imbalance, causing certain GPUs to become overloaded while others remain underutilized. The Figure~\ref{fig:KV_methods_with_TP} illustrates the combination of common KV cache compression methods with tensor parallelism. This highlights the urgent need for balancing per-head KV cache imbalance.

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{latex/KV_methods_with_TP.png}
%     \caption{Common KV Cache compression methods with Tensor Parallelism}
%     \label{fig:KV_methods_with_TP}
% \end{figure*}