
To analyze the reduction in inference latency achieved by the Adaptive Head Allocation (AHA) method, we examine the following sources of latency reduction:

\paragraph{Reduction in GPU Idle Time:}
The reduction in GPU idle time is quantified by the difference in idle time between tensor parallelism and AHA. Let \( T_{\text{idle}}^{\text{TP}} \) be the idle time in tensor parallelism and \( T_{\text{idle}}^{\text{AHA}} \) be the idle time in AHA. The reduction is given by:
\[
\Delta T_{\text{idle}} = T_{\text{idle}}^{\text{TP}} - T_{\text{idle}}^{\text{AHA}}
\]

\paragraph{Reduction in KV Cache Access Latency:}
The reduction in KV cache access latency is measured by the difference in cache access times. Let \( T_{\text{cache}}^{\text{TP}} \) be the cache access latency in tensor parallelism and \( T_{\text{cache}}^{\text{AHA}} \) be the cache access latency in AHA. The reduction is given by:
\[
\Delta T_{\text{cache}} = T_{\text{cache}}^{\text{TP}} - T_{\text{cache}}^{\text{AHA}}
\]

\paragraph{Communication Time Overhead:}
The communication time overhead is reduced by optimizing synchronization between GPUs. Let \( T_{\text{comm}}^{\text{TP}} \) be the communication time in tensor parallelism and \( T_{\text{comm}}^{\text{AHA}} \) be the communication time in AHA. The reduction is given by:
\[
\Delta T_{\text{comm}} = T_{\text{comm}}^{\text{TP}} - T_{\text{comm}}^{\text{AHA}}
\]

\subsubsubsection{Total Latency Reduction}
The total reduction in inference latency \( \Delta T_{\text{total}} \) is the sum of the reductions in GPU idle time, KV cache access latency, and communication time overhead:
\[
\Delta T_{\text{total}} = \Delta T_{\text{idle}} + \Delta T_{\text{cache}} + \Delta T_{\text{comm}}
\]
This formula captures the cumulative effect of AHA's optimizations, leading to improved inference efficiency.


\begin{figure*}[ht] % 使用 figure* 让图片横跨两列
    \centering
    \begin{subfigure}[b]{0.24\textwidth} % 每张图片占 23% 宽度
        \includegraphics[width=\textwidth]{latex/barcomp-budget128.png}
        \caption{budget = 128}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{latex/barcomp-budget256.png}
        \caption{budget = 256}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{latex/barcomp-budget512.png}
        \caption{budget = 512}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{latex/barcomp-budget1024.png}
        \caption{budget = 1024} 
        \label{fig:image4}
    \end{subfigure}
    \caption{Throughput Gains of AHA-NoDP and AHA-DP RC=4 Across Different Tensor Parallel Sizes on LLaMa-3.3-70B.}
    \label{fig:gainsacrossgpu}
\end{figure*}


\appendix

\section{Appendix}
\label{sec:appendix}

\begin{table*}
  \centering
  \begin{tabular}{llll}
    \hline
    \textbf{Dataset} & \textbf{Source} & \textbf{Avg len} & \textbf{Language} \\
    \hline
    \textbf{\textit{Single-Document QA}} & & & \\
    NarrativeQA & Literature, Film & 18,409 & English \\
    Qasper & Science & 3,619 & English \\
    MultiFieldQA-en & Multi-field & 4,559 & English \\
    MultiFieldQA-zh & Multi-field & 6,701 & Chinese \\
    \hline
    \textbf{\textit {Multi-Document QA}} & & & \\
    HotpotQA & Wikipedia & 9,151 & English \\
    2WikiMultihopQA & Wikipedia & 4,887 & English \\
    MuSiQue & Wikipedia & 11,214 & English \\
    DuReader & Baidu Search & 15,768 & Chinese \\
    \hline
    \textbf{\textit{Summarization}} & & & \\
    GovReport & Government report & 8,734 & English \\
    QMSum & Meeting & 10,614 & English \\
    MultiNews & News & 2,113 & English \\
    VCSUM & Meeting & 15,380 & Chinese \\
    \hline
    \textbf{\textit{Few-shot Learning}} & & & \\
    TREC & Web question & 5,177 & English \\
    TriviaQA & Wikipedia, Web & 8,209 & English \\
    SAMSum & Dialogue & 6,258 & English \\
    LSHT & News & 22,337 & Chinese \\
    \hline
    \textbf{\textit{Synthetic Task}} & & & \\
    PassageCount & Wikipedia & 11,141 & English \\
    PassageRetrieval-en & Wikipedia & 9,289 & English \\
    PassageRetrieval-zh & C4 Dataset & 6,745 & Chinese \\
    \hline
    \textbf{\textit{Code Completion}} & & & \\
    LCC & Github & 1,235 & Python/C\#/Java \\
    RepoBench-P & Github repository & 4,206 & Python/Java \\
    \hline
  \end{tabular}
  \caption{Dataset Configurations}
  \label{tab:dataset}
\end{table*}