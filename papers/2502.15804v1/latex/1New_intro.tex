\section{Introduction}

\subsection{Background and Motivation}

Large-scale Transformer-based models are at the core of modern artificial intelligence. To support fast inference, these models rely on a key-value (KV) cache\cite{Vaswani2017AttentionIA,Dai2019TransformerXLAL,Rae2019CompressiveTF} that stores key and value embeddings from previously generated tokens, trading memory usage for reduced redundant computation. This cache prevents redundant computations and is crucial for efficient sequence generation. 
%
%However, the huge memory usage of KV cache becomes a bottleneck of inference system, and extensive works study how to compress KV cache\cite{引用7篇左右论文}.
%
However, the huge memory usage of the KV cache becomes a bottleneck for inference systems, and a great number works have investigated KV cache compression\cite{ge2024modeltellsdiscardadaptive,Liu2023CacheGenKC,xiao2024efficientstreaminglanguagemodels,li2024snapkvllmknowslooking,cai2024pyramidkvdynamickvcache,feng2024adakv,fu2024headsmatterheadlevelkv,Adnan2024KeyformerKC}.




%Traditional KV cache compression methods, such as StreamingLLM\cite{xiao2024efficientstreaminglanguagemodels}, SnapKV\cite{li2024snapkvllmknowslooking}, PyramidKV\cite{cai2024pyramidkvdynamickvcache}, allocate an equal/fair KV cache budget by default to all attention heads. 
%
Traditional KV cache compression methods (StreamingLLM\cite{xiao2024efficientstreaminglanguagemodels}, SnapKV\cite{li2024snapkvllmknowslooking} and PyramidKV\cite{cai2024pyramidkvdynamickvcache}) naturally allocate an equal (fair) KV cache budget to all attention heads.
%
This fair allocation simplifies the design and ensures uniform memory usage across heads. 
%
%Recently, SOTA works of KV cache such as AdaSnapKV\cite{feng2024adakv} and HeadKV\cite{fu2024headsmatterheadlevelkv} introduced imbalanced (unfair) per-head KV cache compression algorithms that dynamically adjust the KV cache budget for each attention head. 
%
Recently, SOTA works of KV cache compression methods (AdaSnapKV\cite{feng2024adakv} and HeadKV\cite{fu2024headsmatterheadlevelkv}) introduce \textit{imbalanced per-head KV cache compression algorithms} that dynamically adjust the KV cache budget for each attention head.
%These methods compress the KV cache more effectively and achieve the SOTA performance for scenarios only considering one GPU.
%
These methods compress the KV cache more effectively and achieve SOTA performance in single-GPU scenarios.


%\textbf{Problem:} Our investigation reveals that the imbalanced per-head KV cache compression techniques, though beneficial for reducing overall memory usage, lead to \textbf{unfair load distribution} when deploying multi-GPU inference. 
%
% \noindent\textbf{Unfair head load Problem:} Our investigation reveals that although imbalanced per-head KV cache compression techniques reduce overall memory usage, they result in an \textit{Unfair per-Head KV cache Load distribution} during multi-GPU inference, and we named it \textbf{unfair head load} for convenience.
% %
% In systems employing tensor parallelism (one of the most common parallel strategy), this non-uniform KV cache usage forces some GPUs to handle a disproportionate share of memory-intensive attention heads.
% This imbalance (unfairness) increases GPU idle time, elevates inference latency, and ultimately degrades system throughput. 
% %
% Despite considerable research on load balancing, no prior work has identified or addressed this specific head-level imbalance caused by imbalanced KV cache compression. 
% %
% Tackling this problem is critical for achieving efficient, low-latency inference in large-scale Transformer models.

\noindent\textbf{Unfair head load Problem:} 
Our investigation reveals that although imbalanced per-head KV cache compression techniques reduce overall memory usage, they result in an \textit{uneven per-head KV cache load distribution} during multi-GPU inference, which we refer to as the \textbf{unfair head load} problem. In systems using tensor parallelism, one of the most common parallel strategies, this non-uniform KV cache usage forces some GPUs to handle a disproportionate share of memory-intensive attention heads. This imbalance increases GPU idle time, elevates inference latency, and ultimately degrades system throughput.
Despite extensive research on load balancing in parallel inference systems, no prior work has identified or addressed this specific imbalance caused by imbalanced KV cache compression. Addressing this imbalance is crucial for achieving efficient, low-latency inference in large-scale Transformer models, particularly in multi-GPU settings.



\subsection{Our solution}

%\subsubsection{Overview}
To address the unfair head load problem, we propose \AlgName{}. Our approach targets the fair memory usage among attention heads in imbalanced KV cache compression systems. \AlgName{} mainly employs two techniques to address this issue: best-effort assignment and fair-copying.
 \textit{Best-effort Assignment} is responsible for distributing attention heads across GPUs to achieve a relatively balanced workload.
%An \textbf{advanced version}, termed \emph{Fair-Copying}, which replicates a small subset of memory-intensive attention heads across GPUs using data parallelism, further mitigating load imbalance. 
 \textit{Fair-Copying} involves replicating certain attention heads to participate in the assignment, thereby reducing the workload of the replicated heads.
%
%An \textbf{advanced version}, namely \emph{Fair-Copying}, which selectively replicates a small subset of memory-intensive attention heads across GPUs using data parallelism, further mitigating load imbalance.
%


%\subsubsection{Basic Version: Best-effort Assignment}
%\noindent\textbf{Basic Version: Best-effort Assignment}
\noindent\textbf{Technique I: Best-effort Assignment.}
%
Best-effort Assignment works as follows.
We first analyze the KV cache consumption of each attention head under imbalanced compression. Based on this analysis, our allocation algorithm assigns attention heads to GPUs such that the aggregate memory and computational load is balanced across GPUs in a best-effort manner. This method requires no additional overhead and can be integrated into existing multi-GPU inference systems easily, offering a practical improvement in load balancing without modifying the underlying KV cache compression scheme.
%
%Best-effort Assignment works as follows. We analyze the KV cache consumption of each attention head under imbalanced compression and assign heads to GPUs using a simple heuristic. This best-effort assignment minimizes load variance without adding extra overhead, providing an immediate but limited improvement in load balancing.


%\noindent\textbf{Advanced Version: Fair-Copying}
\noindent\textbf{Technique II: Fair-Copying.}
While best-effort assignment offers a straightforward solution, some attention heads remain significantly more memory-intensive, leading to persistent bottlenecks. 
%
To further improve load balance, we propose another technique called \emph{Fair-Copying}, which utilizes Data Parallel techniques to enhance the effectiveness of the assignment.
%
In this technique, we set a replication budget that allows the algorithm to attempt replicating attention heads. By utilizing data parallelism to reduce the load on the replicated heads, these replicated heads participate in the assignment alongside the original ones, thereby expanding the search space and enabling finer-grained partitioning. This replication method minimizes additional overhead while substantially reducing GPU idle time and inference latency.

\noindent\textbf{Key Contributions.}
This paper makes the following key contributions: 
\begin{enumerate} [label=\arabic*)]
\vspace{-0.1in}
\item To the best of our knowledge, we are the first to reveal the unfair head load problem: imbalanced per-head KV cache compression causes significant load imbalance in multi-GPU inference environments. 


%\item We propose \AlgName{}, which includes two mechanisms: best-effort assignment and fair-copying, to balance the workload across GPUs for imbalanced per-head KV cache compression systems.
\vspace{-0.1in}
\item To address the unfair head load problem,
we propose \AlgName{}, which includes two mechanisms—best-effort assignment and fair-copying.

%\vspace{-0.1in}
%\item We develop a mathematical model and an optimization algorithm to guide the assignment and replication of attention heads. 

\vspace{-0.1in}
\item Theoretical analysis provides insights, while experiments on popular models, including LLaMA 70b and Mistral 24b model, demonstrate that \AlgName{} increases throughput by 1.66x compared to standard tensor parallelism inference. Our code will be released as open source upon acceptance.
\end{enumerate}