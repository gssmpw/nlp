\begin{abstract}
As deep learning models continue to scale in size and complexity, efficient GPU utilization and minimal inference latency have become paramount for real-time applications. Transformer-based language models rely on KV Cache compression to reduce memory usage and Tensor Parallelism for multi-GPU inference. However, head-wise KV cache compression creates workload imbalances across GPUs, leading to suboptimal utilization and increased latency.  

We propose a novel optimization technique Adaptive Head Allocation (AHA), which selectively partition attention heads in transformer models, leading to improved GPU utilization and reduced inference latency. Our experiments on popular models, including LLaMA(70B) and Mistral(24B), show AHA increases throughput by 1.66x over standard TP inference.
\end{abstract}