\section{Related Work}

Inference efficiency for large language models is critically limited by both memory bandwidth and computational power. KV cache compression \cite{ge2024modeltellsdiscardadaptive, zhang2024h2o, yang2024pyramidinfer}, a widely adopted optimization technique, reduces memory for storing previous key-value states in attention layers. It can generally be classified into two categories: \textit{Balanced (Fair) Per-Head Compression} and \textit{Imbalanced (Unfair) Per-Head Compression}.

\textit{Balanced (Fair) Per-Head Compression} methods applies the same strategy to all attention heads. For instance, StreamingLLM \cite{xiao2024efficientstreaminglanguagemodels} retains the initial $k$ sink tokens along with the recent window. H2O \cite{zhang2024h2o} further prioritizes important cache entries based on accumulated attention scores, while SnapKV \cite{li2024snapkvllmknowslooking} selects entries using attention scores from the observation window. Recently, Pyramid \cite{yang2024pyramidinfer, cai2024pyramidkvdynamickvcache} recognize distinct attention distribution patterns across different layers. However, these methods disregard the varying importance of different heads in actual computations. 

In contrast, \textit{Imbalanced (Unfair) Per-Head Compression} algorithms, like Ada-SnapKV \cite{feng2024adakv} and HeadKV\cite{fu2024headsmatterheadlevelkv}, dynamically adjust the KV cache budget per attention head based on the current layer's computational and memory requirements. Ada-SnapKV determines the budget for each head during inference, with a fully dynamic allocation. HeadKV, however, pre-allocates a fixed base budget for each head according to its importance and then adds a dynamic budget. This tailored cache allocation offers more flexibility and optimization potential. Table~\ref{tab:KV_Methods} in the appendix show that Ada-SnapKV outperforms other methods on multiple tasks in the LongBench v1 \cite{bai2024longbenchbilingualmultitaskbenchmark}, demonstrating the effectiveness of \textit{Imbalanced (Unfair) Per-Head Compression} approach.

However, a significant challenge arises when applying \textit{Imbalanced (Unfair) Per-Head Compression} in tensor parallelism, which has become the preferred method for inference in large language models \cite{lu2017flexflow, shazeer2018mesh, shoeybi2019megatron, rajbhandari2020zero}. The non-uniform distribution of KV cache across heads causes computational load imbalance, degrading overall inference efficiency. This highlights the urgent need for balancing per-head KV cache.

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{latex/KV_methods_with_TP.png}
%     \caption{Common KV Cache compression methods with Tensor Parallelism}
%     \label{fig:KV_methods_with_TP}
% \end{figure*}