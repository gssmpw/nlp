\begin{abstract}   
%KV cache techniques in Transformer models trade increased memory usage for reduced redundant computation. However, this approach leads to huge GPU memory usage, making KV cache compression an important and popular research topic. 
%
KV cache techniques in Transformer models aim to reduce redundant computations at the expense of substantially increased memory usage, making KV cache compression an important and popular research topic.
%
Recently, state-of-the-art KV cache compression methods implement imbalanced, per-head allocation algorithms that dynamically adjust the KV cache budget for each attention head, achieving excellent performance in single-GPU scenarios. However, we observe that such imbalanced compression leads to significant load imbalance when deploying multi-GPU inference, as some GPUs become overburdened while others remain underutilized.
%
In this paper, we propose \AlgName{}, a method designed to ensure fair memory usage among attention heads in systems employing imbalanced KV cache compression. 
%
The core technique of \AlgName{} is \emph{Fair-Copying}, which replicates a small subset of memory-intensive attention heads across GPUs using data parallelism to mitigate load imbalance. 
%
Theoretical analysis provides insights, while experiments on popular models, including LLaMA 70b and Mistral 24b model, demonstrate that \AlgName{} increases throughput by 1.66x compared to standard tensor parallelism inference. Our code will be released as open source upon acceptance.
\end{abstract}