\section{Design of \AlgName{}}
\subsection{Overview}


During inference with tensor parallelism, per-head KV cache compression algorithms can lead to imbalanced GPU workloads, which in turn reduces inference efficiency.To address this issue, we designed \AlgName{}, a load-aware static approach that uses a search algorithm to reassemble and rearrange attention heads across layers, thereby balancing the load among GPUs. Moreover, \AlgName{} leverages fair-copying mechanism to expand the search space via replication of attention heads and DataParallel techniques, enabling a more fine-grained balancing of GPU loads and enhancing GPU utilization during inference. Figure \ref{fig:aha_overview} visually illustrates the core concept of \AlgName{}.

The \AlgName{} algorithm requires predefining the model and the KV cache budget to be used. It then samples a dataset to analyze the proportion of KV cache budget allocated to attention heads across different layers for that model, summarizing the findings into a statistical profile. Based on this data, we first replicate attention heads to expand the search space. Next, we perform a constrained search to determine the optimal attention head arrangement. Finally, the model weights are loaded according to this arrangement for inference.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{latex/overall_new.png} 
  \caption{Illustration of different head allocation strategies for multi-GPU inference in large-scale transformer models. The figure shows the following strategies: (1) Static Head Allocation (SHA), where attention heads are evenly distributed across GPUs without considering computational load; (2) Load-Aware Head Allocation (\AlgName{}-NoDP), where attention heads are allocated based on their computational load, ensuring a balanced GPU utilization; (3) Load-Aware Head Allocation with DataParallel (\AlgName{}-DP), where heads are replicated across GPUs for improved load distribution and efficiency; }
  \label{fig:aha_overview}
\end{figure*}

\subsection{Search Space}
Fair-copying leverages Data Parallel techniques to expand the search space by replicating some redundant heads, with the goal of achieving more effective search outcomes. Experiments were conducted with selective head replication, and the results demonstrated improved latency distribution across GPUs. Allowing for limited redundancy provides an effective way to enhance parallel efficiency without excessive computational overhead.

\input{latex/math_model}

\subsection{Optimizer Design}

We introduce a recursive backtracking algorithm that systematically explores possible head distributions while ensuring workload balance.

\begin{algorithm}
\caption{Backtracking-Based Partitioning Algorithm}
\begin{algorithmic}
\State \textbf{Input:} List of heads $L$, max GPU load $m$
\State \textbf{Output:} Optimal partitioning of $L$
\State Initialize result set $R$
\Procedure{Backtrack}{$index, current$}
    \If{$index == |L|$ and $|current| \leq m$}
        \State Add $current$ to $R$
        \Return
    \EndIf
    
    \State Add $L[index]$ to $current$ and recurse
    \Call{Backtrack}{$index+1, current$}
    \State Remove last element from $current$
    
    \For{$n = 2$ to $m - |current| + 1$} 
        \State Split $L[index]$ into $n$ parts and add to $current$
        \Call{Backtrack}{$index+1, current$}
        \State Remove last $n$ elements from $current$
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
This approach systematically searches for the best head distribution by iterating over all valid partitions. By ensuring balanced allocation, the algorithm minimizes inference latency across GPUs, leading to improved system efficiency.

\subsection{Key Innovations}

\noindent\textbf{Workload-aware Redistribution} A static yet optimized head allocation strategy balances GPU workloads efficiently.

\noindent\textbf{Hybrid Parallelism} Selective head replication combines tensor and data parallelism.

\noindent\textbf{Latency-driven Optimization} Backtracking minimizes inference latency, improving GPU utilization.

