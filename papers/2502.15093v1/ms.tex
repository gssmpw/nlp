\documentclass[a4paper,11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{authblk}
\usepackage{subcaption}

%% ADDED BY AUTHORS:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[table,xcdraw]{xcolor}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{bm} %bold math
\usepackage{url} %this package should fix any errors with URLs in refs.
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{lineno}
\usepackage{soul}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{float}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{mathtools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Theorem}
\newtheorem{dfn}{Definition}
\newtheorem{lem}{Lemma}

\newcommand{\cf}{{\it cf.}~}

\title{Forecasting Local Ionospheric Parameters Using Transformers}
\author[1,2,3]{Daniel J.  Alford-Lago\thanks{daniel.j.alford-lago.civ@us.navy.mil}}
\author[1]{Christopher W. Curtis}
\author[3]{Alexander T. Ihler}
\author[4]{Katherine A. Zawdie}
\author[4]{Douglas P. Drob}
\affil[1]{Atmospheric Propagation Branch, Naval Information Warfare Center Pacific, San Diego, California, USA}
\affil[2]{Department of Mathematics and Statistics, San Diego State University, San Diego, California, USA}
\affil[3]{Department of Computer Science, UC Irvine, Irvine, California, USA}
\affil[4]{Space Science Division, Naval Research Laboratory, Washington, District of Columbia, USA}
\renewcommand\Affilfont{\itshape\small}
\date{}
\begin{document}
\maketitle

\begin{abstract}
We present a novel method for forecasting key ionospheric parameters using transformer-based neural networks. The model provides accurate forecasts and uncertainty quantification of the F2-layer peak plasma frequency (foF2), the F2-layer peak density height (hmF2), and total electron content (TEC) for a given geographic location. It supports a number of exogenous variables, including F10.7cm solar flux and disturbance storm time (Dst). We demonstrate how transformers can be trained in a data assimilation-like fashion that use these exogenous variables along with na\"ive predictions from climatology to generate 24-hour forecasts with nonparametric uncertainty bounds. We call this method the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that the trained model can generalize to new geographic locations and time periods not seen during training, and we compare its performance to that of the International Reference Ionosphere (IRI).
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Accurate and efficient modeling of Earth's ionosphere has a significant impact on research and operational communities due to its effects on radio communications, radar performance, \cite{budden, davies_1990, ratcliffe_1959} and satellite drag \cite{berger_2023}. Success in forecasting key parameters such as the F2 layer critical frequency (foF2) and height (hmF2) and the total electron content (TEC) allows one to anticipate and mitigate the impacts of ionospheric variability on such systems. Over the past decades, many modeling approaches have been developed to predict these ionospheric parameters with increasing accuracy and skill. These models may be broadly categorized as empirical, physics-based, and, more recently, machine learning methods.

Empirical models often rely on extensive historical datasets to establish statistical relationships between ionospheric parameters and geophysical variables. The International Reference Ionosphere (IRI) model \cite{bilitza_2022} is a widely used standard that provides monthly averages of various ionospheric parameters based on many decades of past observations. IRI has seen continual development and improvements over the years, adding a host of submodels used to capture specific aspects of the ionosphere such as the CCIR \cite{jones_1962, jones_1965} and URSI \cite{fox_1988} foF2 models for representing the diurnal variations of the peak plasma density across the globe, the AMTB \cite{altadill_2013} and SHU-2015 \cite{shubin_2015} models for even more harmonic expansions of hmF2, and NeQuick 2 \cite{nava_2008} for improved topside electron density accuracy and thus better estimates of TEC \cite{bilitza_2009, kashcheyev_2019}. So, while large empirical models like IRI continue to improve, the number of these available options needed to address each domain and source of variance in the ionosphere also grows, and choosing the appropriate settings may be prohibitive without expert knowledge of each submodel.

The Physics-based models instead aim to incorporate the fundamental processes governing the dynamics of the ionosphere and its drivers by solving the highly complex sets of equations related to the plasma magnetohydrodynamics, electrodynamics, and chemistry. These often seek to simulate the ionospheric response to solar and geomagnetic inputs. The Thermosphere-Ionosphere-Mesosphere-Electrodynamics General Circulation Model (TIME-GCM) \cite{roble_1994}, the Coupled Thermosphere Ionosphere Plasmasphere Electrodynamics (CTIPe) model \cite{millward_2001}, and SAMI3 \cite{huba_2013} are examples of such models, and while they offer detailed insights through highly accurate nowcast and hindcast, their ability to forecast is often limited, and forecast skill can drop precipitously over relatively short time windows, even with careful treatment of driving parameters such as solar and geomagnetic activity. Additionally, the simulations themselves can be computationally expensive, requiring high-performance computing to generate high-resolution or large numbers of simulations.

Both empirical and physics-based methods require special treatment of drivers and parameterizations in order to maintain accuracy over a forecast period. Although there are efforts to develop better indices for these drivers that can be included in models \cite{nishioka_2017}, the impact on forecast skill is still largely determined by how well these drivers can inform future values of the parameters and not just how correlated they are in the past. This challenge is further complicated by the fact that these drivers, which often take the form of global indices, may interact with the forecasted parameters with unknown or varying time delays \cite{schmolter_2024, chen_2018, chen_2015, coley_2012, jakowski_1991}. Thus, determining the complex interactions between multivariate time series of drivers and observations motivates the consideration of machine learning as a component in the next generation of forecast models.

Learned latent representations can greatly simplify a time series through linearizations of the latent representation \cite{lago_2022} or time delay embeddings \cite{curtis_2024}. However, the burden of choosing the correct latent dimension or time delay can make these methods cumbersome when applied to real-world datasets, and there is no straightforward means of validating that a given embedding will continue to work for future time series or, in the case of ionospheric forecasting, for new geographic locations. Nevertheless, recent advances in transformer-based neural networks have produced promising results \cite{lim_2021, zhou_2023}.

The parameters we wish to forecast are readily available through sounding measurements and subsequent inverse processing, yet the multiscale and noisy nature of these data makes them difficult to predict using conventional spectral decompositions \cite{lago_2023}. We find that transformers allow us to overcome all of these challenges at once. Namely, we analyze the use of transformer networks and demonstrate their ability to ingest a complex mix of inputs, including geomagnetic and solar activity, to produce a probabilistic forecast of key ionospheric measures. Moreover, it is simple to also feed the transformer a naive prediction from an existing model such as IRI as well. Naive in this context, implying that the settings and parameters for IRI are not tuned by the user. Thus, the transformer learns to dynamically assimilate those data into the naive prediction. Another major drawback of current empirical and physics-based models is the lack of or difficulty in generating uncertainty quantification. Our use of transformers also allows us to take steps towards addressing this by way of multi-quantile regression. So, instead of the model producing point forecasts over the prediction horizon, we produce a set of quantiles that are learned from the data that capture empirical distributions of uncertainty in each target parameter. We call this model the Local Ionospheric Forecast Transformer (LIFT). 

The following sections will describe the data used to train this model, Section \ref{chp4:sec:data}, the model architecture itself in Section \ref{chp4:sec:methods}, the results of these forecasts, Section \ref{chp4:sec:results}, and conclusions for future directions, Section \ref{chp4:sec:conlcusions}. The results of this model suggest that transformers may play an essential role in the next generation of data-driven ionospheric models, and while they lack the interpretability of the full-physics systems, their ability to transform new data into forecasts makes them a power tool for operational communities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data}\label{chp4:sec:data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our model is trained using data from several openly available sources. The observational data for the target parameters, foF2, hmF2, and TEC are obtained from the Lowell GIRO Data Center digital ionogram database (DIDBase) \cite{reinisch_2011}. These parameters are determined from raw sounder data by way of an autoscaling and inversion algorithm, ARTIST5 \cite{galkin_2008}. The sounder data acquisition and subsequent parameter computation has a number of potential sources of error, which ARTIST5 attempts to quantify using a confidence score; we filter out any poorly inverted parameters using a heuristically chosen threshold, retaining only values with confidence $\ge 70$. Higher confidence scores do not necessarily correspond to better autoscalings. However, this threshold has been assessed to cover autoscalings that are accurate enough for the foF2 and hmF2 parameters; see \cite{themens_2022} for a more detailed analysis of the confidence scores of ARTIST5. 

Although DIDBase provides access to data from 128 ionosonde stations, many have only sparse (infrequent) observations or poorly autoscaled data. Figure \ref{chp4:fig:giro_map} shows the geographic locations from which data were obtained, where the relative sizes of the markers indicate the amount of data from each station. Data were collected from each GIRO station for the period 1 Jan 2000 to 1 Jan 2023. The complete time series for foF2, hmF2, and TEC for one of these stations (located in Boulder, Colorado, USA) is shown in Figure \ref{chp4:fig:raw_params}. Although Boulder has comparatively dense observations, many stations have extreme gaps or very small amounts of data for much of this time period. So, in Figure \ref{chp4:fig:giro_map}, many stations simply do not show up as their relative density of data is too low.
 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_station_map_7day.png}
  \caption[Geographic map of GIRO sounders]{Geographic map of GIRO stations used. The size of each marker indicates roughly the relative amount of data obtained from each location. Larger marker sizes represent more valid data obtained between the years 2000 to 2023. Some stations only had a few segments of usable data and thus are not visible in this figure.}
  \label{chp4:fig:giro_map}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_raw_data_one_station.png}
  \caption[foF2, hmF2, and TEC parameter time series]{All data for the parameters foF2, hmF2, and TEC obtained from a Digisonde sounder in Boulder, Colorado, USA from 1 January 2000 to 1 January 2023.}
  \label{chp4:fig:raw_params}
\end{figure}

Often, forecast models are tested by evaluating them on a held out data set consisting of measurements from future time steps that the model has not seen.  Unfortunately, for a localized ionospheric forecast, this tells us little about how the model will perform data measured at a new location. Instead, we elect to perform our testing on data from the same time periods as the training data, but from completely new geographic regions, i.e., we hold out entire stations' data for testing. Figure 
\ref{chp4:fig:giro_map} indicates which station locations were used for training and validation versus test evaluation in our experiments. Although most available sounding measurements are located in the Northern hemisphere, a few test stations are located near the equatorial region and the lower hemisphere; we reserve these held out stations entirely for testing, which allows us to demonstrate our model's ability to generalize to novel geographic locations, and new parts of the ionosphere, after training. This makes our LIFT model more useful in practice, since a user is not required to retrain or tune the model in order to forecast at a new station.
Because we are training a local forecaster, all locations are kept in geographic latitude and longitude, and no conversions to magnetic coordinates were needed.

Our model ingests windows of data that are 72 hours in length, and generates a forecast for the subsequent 24 hours;  these must be contiguous periods of observations, since we wish to learn sequential temporal relationships between past measurements and covariates and future observations. After removing data with poor ARTIST5 confidence autoscalings, we further downselect to only contiguous time series from each station that contain more than the minimum number of points needed by the model, i.e., 96 hours. Each of these contiguous time series is given a unique segment identifier and this process is performed separately for the training, validation, and test sets. Figure \ref{chp4:fig:data_years} provides a summary of the number of unique sounder observations over time for the training, validation, and test sets. The total number of points and contiguous segments are reported in Table \ref{chp4:tab:num_points}.0
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_segment_plot_4day_counts.pdf}
  \caption[Scatterplot showing geographic data density]{Number of observations obtained for training (blue), validation (green), and testing (red) grouped by year.}
  \label{chp4:fig:data_years}
\end{figure}
\begin{table}[htbp]
    \centering
    \resizebox{0.75\textwidth}{!}{%
    \begin{tabular}{ccc}
        \hline
        & {\ul \textbf{Number of points}} & {\ul \textbf{Number of 96-hour segments}} \\ \hline
        \textbf{Training} & 9,336,446 & 528,413 \\ \hline
        \textbf{Validation} & 642,821 & 38,494 \\ \hline
        \textbf{Testing} & 1,481,978 & 76,220 \\ \hline
\end{tabular}
}
\caption[Summary of number of points used to train LIFT]{Total number of data points and 96-hour segments used for training, validation, and testing.}
\label{chp4:tab:num_points}
\end{table}

Each data segment is then paired with a corresponding set of drivers that are used as exogenous inputs to the model. These drivers include sunspot number (SSN), the Kp index (Kp), and F10.7, which were obtained from the GFZ German Research Center for Geosciences \cite{matzka_2021}, and the Dst index from the WDC for Geomagnetism, Kyoto \cite{nose_2015}. Together, these indices represent a set of additional covariates that can be measured or obtained alongside new sounding data, but whose interactions with the ionospheric parameters may be too complex, nonlinear, or lagged to be expressed as closed analytic expressions. Figure \ref{chp4:fig:data_indices} illustrates the affiliated time series of these indices for the Boulder, CO, USA ionosonde station.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_indices.png}
  \caption[Exogenous inputs for LIFT]{Indices used as exogenous inputs to the model for the time period 2004-2023.}
  \label{chp4:fig:data_indices}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}\label{chp4:sec:methods}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The complexities and multiscale nature of the ionospheric data require an architecture that is flexible enough to deal with the varying space weather conditions across different latitudes and seasons. In this section, we introduce our transformer-based model, which is designed to leverage these complex datasets for probabilistic forecasting. First, we establish some basic notation and expressions, denoting a time series of a single variable from time index $t$ to $t+k$ as a vector $\mathbf{x}_{t:t+k} \in \mathbb{R}^{k+1}$, and a time series of $m$ variables as a matrix $\mathbf{X}_{t:t+k} \in \mathbb{R}^{(k+1) \times m}$. We will refer to scalars, such as the $j^{th}$ variable at time $t+k$ as $x_{t+k;j}$.

We then define the data used in our model as follows:
\begin{align}
    &\mathbf{X}_{t-c:t} \in \mathbb{R}^{(c+1) \times d_1}: &&\text{Past observations of the target variables}\\
    &\mathbf{Y}_{t-c:t} \in \mathbb{R}^{(c+1) \times d_2}: &&\text{Past observations of the covariates}\\
    &\mathbf{Z}_{t-c:t+p} \in \mathbb{R}^{(c+p+1) \times d_3}: &&\text{Past and future covariates computable up to $t+p$}
\end{align}
where $c$ and $p$ are the number of \emph{context} and \emph{prediction} points, respectively. The matrix $\mathbf{X}$ contains the foF2, hmF2, and TEC parameters observed from an ionosonde, thus $d_1 = 3$, $\mathbf{Y}$ corresponds to the data in Figure \ref{chp4:fig:data_indices} and consists of the exogenous or forcing variables that we expect to influence the forecast, so $d_2=4$, and $\mathbf{Z}$ contains data for computable covariates, including a time vector and the na\"ive IRI prediction, which we describe in more detail in Sections \ref{chp4:sec:pos_encoding} and \ref{chp4:sec:model_covariates}, resulting in $d_3=13$.

We seek a model for the conditional distribution of each target variable at each point in the forecast horizon from which we may estimate their expected values and provide uncertainty quantification. A common method is to assume a fixed parametric form for the distributions, e.g.
\begin{equation}\label{chp4:eqn:pdf}
    f_{t+k;j}(x_{t+k;j} \mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}) \sim \mathcal{N}(\mu_{t+k;j}, \sigma_{t+k;j}),
\end{equation}
where $j\in \{1,...,d_1\}$ indicates the target variable, and $\mu_{t+k;j}$ and $\sigma_{t+k;j}$ are predicted parameters of a normal distribution $k$ points into the future. 
However, such a representation is sensitive to the selected parametric form and may perform poorly due to model mismatch.
Instead, we elect to implement a fully nonparametric approach using quantiles. To this end, let the conditional cumulative distribution function (CDF) of Equation \ref{chp4:eqn:pdf} be
\begin{equation}
    F_{t+k;j}(x_{t+k;j} \mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}) \coloneqq \text{Pr}(x_{t+k;j} \le x \mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}) = \tau
\end{equation}
where $\tau \in [0, 1]$. Then, the \emph{quantile function} is the inverse of our conditional CDF
\begin{align}\label{chp4:eqn:quantilefunction}
\begin{split}
    Q_{x_{t+k;j} \mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}}(\tau) &= F_{t+k;j}^{-1}\left(x_{t+k;j} \mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}\right) \\
     &= \inf \left\{ x \in \mathbb{R} : F_{t+k;j}(x \mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}) \geq \tau \right\} \\
     &= x_{t+k;j}^{\tau}.
\end{split}
\end{align} % A simple definition used in Theorem 2.1.10 in Casella and Berger
The infimum in this definition is necessary only if the CDF happens to be constant on some interval, in which case $Q_{x_{t+k;j}\mid \mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}}(\tau)$ would not be well defined \cite{casella_2002, gilchrist_2000, gneiting_2016}. 

To estimate these functions, we construct a model with two main components: a linear predictor, which is designed to capture linear trends and the diurnal pattern from observations of the target variable, and a transformer neural network, which learns nonlinear relationships across all past and future data. The output of the transformer is a set of values that together with the linear component allows us to predict $Q_{\cdot \mid \cdot}(\tau)$ for many different values of $\tau$. So, we have,
\begin{equation}\label{chp4:eqn:themodel}
    \hat{\mathbf{x}}_{t+1:t+p;j}^{\tau} = 
    \underbrace{L\left(\mathbf{x}_{t-c:t;j}\right)}_{\text{Linear}} 
    + 
    \underbrace{T\left(\mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}; \tau, j \right)}_{\text{Transformer}},
\end{equation}
where $\hat{\mathbf{x}}_{t+1:t+p;j}^{\tau}$ is the forecasted time series of either foF2, hmF2, or TEC, depending on $j$ for the specified quantile $\tau$ over the prediction horizon $t+1$ to $t+p$. The function $L(\cdot)$ is straightforward and is defined in the remainder of this section, while $T(\cdot)$ will be defined in more detail in Section \ref{chp4:sec:transformercomp}. The loss function used to train this model will be provided in Section \ref{chp4:sec:loss_fun}.

The linear component is used to predict the simple linear trends that we expect to see in each of the target parameters by generating predictions for the next $p$ time steps based solely on the previous $c$ time steps. Furthermore, it predicts all $p$ time steps simultaneously, similar to sequence-to-sequence models \cite{sutskever_2014}, and so we refer to it as a \emph{direct multistep} linear model. We fit independent linear models for each of the $j$ target variables (foF2, hmF2, TEC),
\begin{align}\label{chp4:eqn:linearmodel}
\begin{split}
    &\hat{\mathbf{x}}_{t+1:t+p;j}^{L} =
    \mathbf{W}_{j}\mathbf{x}_{t-c:t;j} + \mathbf{b}_{j},
\end{split}
\end{align}
with the superscript $L$ denoting this prediction comes from the linear model. The weight matrices $\mathbf{W}_{j} \in \mathbb{R}^{p \times (c+1)}$ and bias vectors $\mathbf{b}_{j} \in \mathbb{R}^{p}$ are fit using the Adam optimizer \cite{kingma_2014} alongside the transformer network. So, this model only generates a linear approximation for a vector of future values from a vector of past values. This forecast will have residual errors,
\begin{equation}\label{chp4:eqn:linerrors}
    \boldsymbol{\epsilon}_{t+1:t+p;j} = \mathbf{x}_{t+1:t+p;j} - \hat{\mathbf{x}}_{t+1:t+p;j}^{L},
\end{equation}
each of which follow some unknown distribution that we may reasonably expect to depend on a set of drivers of the system. Instead of approximating the distributions for these errors, we apply a transformer model to learn quantiles for these residuals directly,
\begin{equation}\label{chp4:eqn:residualquantiles}
    \hat{\boldsymbol{\epsilon}}_{t+1:t+p;j}^{\tau} = T\left(\mathbf{X}_{t-c:t},\mathbf{Y}_{t-c:t},\mathbf{Z}_{t-c:t+p}; \tau, j \right).
\end{equation}
In our model, we fit seven different quantile levels,  $\tau \in \{0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95\}$. Then, a point forecast from our model is obtained by adding the linear component to the predicted median, $\tau=0.5$, from this transformer,
\begin{align}\label{chp4:eqn:medpred}
    \hat{\mathbf{x}}_{t+1:t+p;j}^{0.5} = \hat{\mathbf{x}}_{t+1:t+p;j}^{L} + \hat{\boldsymbol{\epsilon}}_{t+1:t+p;j}^{0.5} = L\left(\mathbf{x}_{t-c:t;j}\right) + T\left(\mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}; 0.5, j\right).
\end{align}
Moreover, we are able to construct confidence intervals (CI) using different levels of $\tau$. For example, the upper and lower bounds of a $90\%$-CI are generated using
\begin{align}\label{chp4:eqn:cipred}
    \hat{\mathbf{x}}_{t+1:t+p;j}^{\text{0.95}} &= \hat{\mathbf{x}}_{t+1:t+p;j}^{L} + \hat{\boldsymbol{\epsilon}}_{t+1:t+p;j}^{0.95} = L\left(\mathbf{x}_{t-c:t;j}\right) + T\left(\mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}; 0.95, j\right)\\
    \hat{\mathbf{x}}_{t+1:t+p;j}^{\text{0.05}} &= \hat{\mathbf{x}}_{t+1:t+p;j}^{L} + \hat{\boldsymbol{\epsilon}}_{t+1:t+p;j}^{0.05} = L\left(\mathbf{x}_{t-c:t;j}\right) + T\left(\mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t+p}; 0.05, j\right).
\end{align}
Conceptually, the transformer acts as a nonlinear correction to the simple linear model that incorporates all available data, including drivers and na\"ive IRI-based climatology, but without the need to specify explicit functional relationships to the target parameters foF2, hmF2, and TEC. Figure \ref{chp4:fig:model_components} provides a concrete example of the output of each of the LIFT model components and Figure \ref{chp4:fig:model_diagram} illustrates the complete end-to-end architecture of our LIFT model architecture.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\textwidth]{fig_model-components.pdf}
  \caption[LIFT model components]{An illustration of how the LIFT model generates point forecasts and uncertainty quantification. Each row provides results for a different target variable (foF2, hmF2, TEC). The linear (first column) and nonlinear (second column) components add together to generate an estimate of the quantile distribution for each target (third column). The final median prediction for each target variable is represented by the bold weighted lines in the third column.}
  \label{chp4:fig:model_components}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{fig_model_diagram.pdf}
  \caption[LIFT model diagram]{Complete LIFT model diagram. All components of the model related to the transformer are shaded green, while the linear model is indicated by red, the inputs in yellow, and the outputs in blue. The stacks each correspond to an individual forecasted parameter (foF2, hmF2, and TEC), so there are three sets of outputs from the linear models and the transformer. The final forecast data at the top is a matrix containing all aggregated outputs.}
  \label{chp4:fig:model_diagram}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Transformer Component}\label{chp4:sec:transformercomp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our transformer is based on the original encoder-decoder architecture introduced by \cite{vaswani_2017} which uses multi-head self-attention networks followed by fully connected feed-forward layers. The attention mechanism allows the transformer to discover long-range temporal relationships very efficiently and to incorporate information from the covariate time series into its forecast of each quantile in Equation \ref{chp4:eqn:residualquantiles}. This is a desirable property for ionospheric forecasting, since there may be unknown time lags between changes in global driver indices and their effects on the target parameters \cite{schmolter_2024, chen_2018, chen_2015, coley_2012, jakowski_1991}.

In our adaptation for time series forecasting, we introduce 1-dimensional convolutions that embed each set of variables in a $d_m$-dimensional latent space, allowing the model to learn richer, high-dimensional feature representations for both the encoder and the decoder inputs. For our model, we use $d_m=128$, and a kernel size of 1 time step, so the embedding is simply mapping the data from each time step into latent coordinates,
\begin{align}
\begin{split}
    &E_{\text{enc}}: \mathbb{R}^{d_1+d_2+d_3} \rightarrow \mathbb{R}^{d_m},\\
    &E_{\text{dec}}: \mathbb{R}^{d_3} \rightarrow \mathbb{R}^{d_m},
\end{split}
\end{align}
or, when applied to our time series,
\begin{align}
    &E_{\text{enc}}(\mathbf{X}_{t-c:t}, \mathbf{Y}_{t-c:t}, \mathbf{Z}_{t-c:t}) = \tilde{\mathbf{X}}_{t-c:t} \in \mathbb{R}^{(c+1) \times d_m}, \label{chp4:eqn:encembed}\\
    &E_{\text{dec}}(\mathbf{Z}_{t+1:t+p}) = \tilde{\mathbf{X}}_{t+1:t+p} \in \mathbb{R}^{p \times d_m}.\label{chp4:eqn:decembed}
\end{align}

The transformer is composed of three layers of attention which we will call the encoder self-attention, decoder self-attention, and decoder cross-attention. Each layer consists of $H$ \emph{attention heads}. We will define the process for computing the encoder and decoder self-attention first, since the cross attention uses output from the previous two. These first two layers begin with a set of learned projections,
\begin{align}
    &\mathbf{W}_{Q,h,enc}, \mathbf{W}_{K,h,enc}, \mathbf{W}_{V,h,enc} \in \mathbb{R}^{(d_m \times d_k)},\label{chp4:eqn:encprojections}\\
    &\mathbf{W}_{Q,h,dec}, \mathbf{W}_{K,h,dec}, \mathbf{W}_{V,h,dec}\in \mathbb{R}^{(d_m \times d_k)},\label{chp4:eqn:decprojections}
\end{align}
where $d_k=d_m/H$ is the latent dimension for each head and $h$ is the head index. These project each latent vector onto what are known as \emph{query}, \emph{key}, and \emph{value} vectors. We represent time series of these query, key, and value vectors as matrices by stacking the vectors row-wise. So, when the projections in Equations \ref{chp4:eqn:encprojections} and \ref{chp4:eqn:decprojections} are applied to our embedded time series in Equations \ref{chp4:eqn:encembed} and \ref{chp4:eqn:decembed}, respectively, we have
\begin{align}
\begin{split}
    &\mathbf{Q}_{h,\text{enc}} = \tilde{\mathbf{X}}_{t-c:t} \mathbf{W}_{Q,h,\text{enc}}, \\
    &\mathbf{K}_{h,\text{enc}} = \tilde{\mathbf{X}}_{t-c:t} \mathbf{W}_{K,h,\text{enc}}, \\
    &\mathbf{V}_{h,\text{enc}} = \tilde{\mathbf{X}}_{t-c:t} \mathbf{W}_{V,h,\text{enc}},
\end{split}
\end{align}
where $\mathbf{Q}_{h,\text{enc}}, \mathbf{K}_{h,\text{enc}}, \mathbf{V}_{h,\text{enc}} \in \mathbb{R}^{(c+1) \times d_k}$, and
\begin{align}
\begin{split}
    &\mathbf{Q}_{h,\text{dec}} = \tilde{\mathbf{X}}_{t+1:t+p} \mathbf{W}_{Q,h,\text{dec}}, \\
    &\mathbf{K}_{h,\text{dec}} = \tilde{\mathbf{X}}_{t+1:t+p} \mathbf{W}_{K,h,\text{dec}}, \\
    &\mathbf{V}_{h,\text{dec}} = \tilde{\mathbf{X}}_{t+1:t+p} \mathbf{W}_{V,h,\text{dec}},
\end{split}
\end{align}
where $\mathbf{Q}_{h,\text{dec}}, \mathbf{K}_{h,\text{dec}}, \mathbf{V}_{h,\text{dec}} \in \mathbb{R}^{p \times d_k}$. The queries and keys are then used to compute the attention scores,
\begin{align}\label{chp4:eqn:scores}
    &\mathbf{S}_{h,\text{enc}} = \frac{\mathbf{Q}_{h, \text{enc}}\mathbf{K}_{h,\text{enc}}^{T}}{\sqrt{d_k}} \in \mathbb{R}^{(c+1) \times (c+1)}\\
    &\mathbf{S}_{h,\text{dec}} = \frac{\mathbf{Q}_{h, \text{dec}}\mathbf{K}_{h,\text{dec}}^{T}}{\sqrt{d_k}} \in \mathbb{R}^{p \times p}.
\end{align}
In principle, these scores determine how much weight, or attention, the model should place on each time step of its input data when producing each time step of its output, and the division by $\sqrt{d_k}$ simply prevents the scores from growing too large if the embedding dimension is increased. 

We then pass these scores through Softmax functions which convert each score vector into probabilities and multiply by the affiliated value matrices,
\begin{align}\label{chp4:eqn:attentions}
    &\mathbf{A}_{h,\text{enc}} = \text{Softmax}(\mathbf{S}_{h,\text{enc}})\mathbf{V}_{h,\text{enc}} \in \mathbb{R}^{(c+1)\times d_k}, \\
    &\mathbf{A}_{h,\text{dec}} = \text{Softmax}(\mathbf{S}_{h,\text{dec}})\mathbf{V}_{h,\text{dec}} \in \mathbb{R}^{p \times d_k}.
\end{align}
Each of these matrices, $\mathbf{A}_{\cdot, \cdot}$, use full self-attention, which means that the attention score for any time step can incorporate values from anywhere in its input time series. Each set of attention heads is then concatenated and projected back to the original embedding dimension,
\begin{align}
    &\bar{\mathbf{X}}_{t-c:t} = [\mathbf{A}_{1,\text{enc}} \mathbf{A}_{2,\text{enc}} \dots \mathbf{A}_{H,\text{enc}}] \mathbf{W}_{O,\text{enc}}, &&\mathbf{W}_{O,\text{enc}}\in \mathbb{R}^{Hd_k \times d_m} \label{chp4:eqn:transconcat1}\\
    &\bar{\mathbf{X}}_{t+1:t+p} = [\mathbf{A}_{1,\text{dec}} \mathbf{A}_{2,\text{dec}} \dots \mathbf{A}_{H,\text{dec}}] \mathbf{W}_{O,\text{dec}}, &&\mathbf{W}_{O,\text{dec}}\in \mathbb{R}^{Hd_k \times d_m},\label{chp4:eqn:transconcat2}
\end{align}
which allows the model to share information across the heads and produce an output that is a time series in the $d_m-$dimensional model space. 

After each attention layer, we apply a fully connected feed-forward network with a nonlinear activation. For our model, we use GELU activation functions \cite{hendrycks_2023}. Although the original transformer architecture used different hidden dimensions in the feed-forward layer, $d_{ff}$, than in the attention mechanisms, we found that our model performed well with $d_{ff}=d_{m}=128$. These feed-forward networks introduce additional nonlinearity to the transformer's processing of the data through their activation functions,
\begin{align}
    &\bar{\mathbf{X}}'_{t-c:t} = \sigma(\bar{\mathbf{X}}_{t-c:t}\mathbf{W}_{ff1,\text{enc}} + \mathbf{b}_{ff1,\text{enc}})\mathbf{W}_{ff2,\text{enc}} + \mathbf{b}_{ff2, \text{enc}}, \label{chp4:eqn:encout}\\
    &\bar{\mathbf{X}}'_{t+1:t+p} = \sigma(\bar{\mathbf{X}}_{t+1:t+p}\mathbf{W}_{ff1,\text{dec}} + \mathbf{b}_{ff1,\text{dec}})\mathbf{W}_{ff2,\text{dec}} + \mathbf{b}_{ff2, \text{dec}},\label{chp4:eqn:decout}
\end{align}
where $\sigma(\cdot)$ is the nonlinear activation. We also incorporate normalization layers \cite{ba_2016} after each attention and feed-forward step to improve the training process. 

The output from Equations \ref{chp4:eqn:encout} and \ref{chp4:eqn:decout} are now ready for processing in the decoder's cross-attention. Here, a new set of queries, keys, and values are created. The queries use output from the decoder's self-attention, and the keys and values use output the encoder's self-attention,
\begin{align}\label{chp4:eqn:crossprojections}
\begin{split}
    &\mathbf{Q}_{h,\text{cross}} = \bar{\mathbf{X}}^{'}_{t+1:t+p} \mathbf{W}_{Q,h,\text{cross}},\\
    &\mathbf{K}_{h,\text{cross}} = \bar{\mathbf{X}}^{'}_{t-c:t} \mathbf{W}_{K,h,\text{cross}},\\
    &\mathbf{V}_{h,\text{cross}} = \bar{\mathbf{X}}^{'}_{t-c:t} \mathbf{W}_{V,h,\text{cross}},
\end{split}
\end{align}
where $\mathbf{Q}_{h,\text{cross}} \in \mathbb{R}^{p \times d_k}$ and $\mathbf{K}_{h,\text{cross}}, \mathbf{V}_{h,\text{cross}} \in \mathbb{R}^{(c+1) \times d_k}$. The cross-attention scores are given by
\begin{align}
    &\mathbf{S}_{h,cross} = \frac{\mathbf{Q}_{h,\text{cross}}\mathbf{K}^{T}_{h,\text{cross}}}{\sqrt{d_k}} \in \mathbb{R}^{p \times (c+1)}\\
    &\mathbf{A}_{h,\text{cross}} = \text{Softmax}\left(\mathbf{S}_{h,cross}\right)\mathbf{V}_{h,\text{cross}} \in \mathbb{R}^{p \times d_k},
\end{align}
and the concatenated heads are projected back into the model latent dimension,
\begin{align}\label{chp4:eqn:transconcat3}
    &\bar{\bar{\mathbf{X}}}_{t+1:t+p} = [\mathbf{A}_{1,\text{cross}} \mathbf{A}_{2,\text{cross}} \dots \mathbf{A}_{H,\text{cross}}]\mathbf{W}_{O,\text{cross}}, &&\mathbf{W}_{O,\text{cross}}\in \mathbb{R}^{Hd_k \times d_m},
\end{align}
where the double bar accent indicates this output is distinct from Equation \ref{chp4:eqn:transconcat2}. As with the self-attention layers, the cross-attention layer includes a final feed-forward network,
\begin{equation}
     \bar{\bar{\mathbf{X}}}^{'}_{t+1:t+p} = \sigma(\bar{\bar{\mathbf{X}}}_{t+1:t+p}\mathbf{W}_{ff1,\text{cross}} + \mathbf{b}_{ff1,\text{cross}})\mathbf{W}_{ff2,\text{cross}} + \mathbf{b}_{ff2, \text{cross}}.\label{chp4:eqn:deccrossout}
\end{equation}
The LIFT model requires one final set of maps to transform the decoder's cross-attention output into the predicted quantiles for foF2, hmF2, and TEC at each point from $t+1$ to $t+p$, 
\begin{align}
    &\hat{\mathbf{E}}_{t+1:t+p}^{\text{foF2}} = \bar{\bar{\mathbf{X}}}^{'}_{t+1:t+p}\mathbf{W}_{\text{foF2}} + \mathbf{b}_{\text{foF2}}, &&\hat{\mathbf{E}}_{t+1:t+p}^{\text{foF2}} \in \mathbb{R}^{p \times 7} \label{chp4:eqn:quantileoutfof2}\\
    &\hat{\mathbf{E}}_{t+1:t+p}^{\text{hmF2}} = \bar{\bar{\mathbf{X}}}^{'}_{t+1:t+p}\mathbf{W}_{\text{hmF2}} + \mathbf{b}_{\text{hmF2}}, &&\hat{\mathbf{E}}_{t+1:t+p}^{\text{hmF2}} \in \mathbb{R}^{p \times 7} \label{chp4:eqn:quantileouthmf2}\\
    &\hat{\mathbf{E}}_{t+1:t+p}^{\text{TEC}} = \bar{\bar{\mathbf{X}}}^{'}_{t+1:t+p}\mathbf{W}_{\text{TEC}} + \mathbf{b}_{\text{TEC}}, &&\hat{\mathbf{E}}_{t+1:t+p}^{\text{TEC}} \in \mathbb{R}^{p \times 7} \label{chp4:eqn:quantileoutTEC}.
\end{align}
Each column in $\hat{\mathbf{E}}_{t+1:t+p}^{foF2}, \hat{\mathbf{E}}_{t+1:t+p}^{hmF2}$, and $\hat{\mathbf{E}}_{t+1:t+p}^{TEC}$ forms a forecast for one of the $7$ different residual quantile levels for each parameter, respectively, i.e. we complete our model for Equation \ref{chp4:eqn:residualquantiles}.

The decoder's cross-attention layer performs the critical function of fusing the information from the encoder with that of the decoder. Larger transformer models may include several \emph{blocks} of encoders and decoders, each of which computes a new set of attention heads and repeats the entire process outlined above. These models will have many more parameters to train and thus require very large amounts of data, however, they offer more flexibility by allowing each new set of attention heads in each layer to attend to different information. For our model, we found that a single block of attention heads in both our encoder and decoder performed well and kept the total number of trainable parameters much lower than other transformer implementations for time series forecasting \cite{zhou_2021, lim_2021}. Figure \ref{chp4:fig:attention_plots} provides a visualization of the attention scores for each of the model's layers for an encoder input length of $c=288$ and a decoder input length of $p=96$. This corresponds to a context length of 3 days and a prediction length of 1 day.
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.95\textwidth]{fig_attn_enc.pdf}\\
        \includegraphics[width=0.95\textwidth]{fig_attn_dec_self.pdf} \\
        \includegraphics[width=0.95\textwidth]{fig_attn_dec_cross.pdf}
    \end{tabular}
    \caption[Transformer attention plots]{A visualization of the attention scores for the encoder self-attention (top), decoder self-attention (middle), and decoder cross-attention (bottom) for a randomly selected test sequence.}
    \label{chp4:fig:attention_plots}
\end{figure}

A final note is that, in practice, transformers often utilize residual connections, as introduced by \cite{he_2016}, which simply add the input and output of a given layer together before being passed into the following layer, e.g. Equation \ref{chp4:eqn:encout} would be implemented as
\begin{equation}
    \bar{\mathbf{X}}'_{t-c:t} = \sigma((\bar{\mathbf{X}}_{t-c:t} + \tilde{\mathbf{X}}_{t-c:t})\mathbf{W}_{ff1,\text{enc}} + \mathbf{b}_{ff1,\text{enc}})\mathbf{W}_{ff2,\text{enc}} + \mathbf{b}_{ff2, \text{enc}}.
\end{equation}
%\ati{Notation makes this confusing -- no indicator of layer? Reads like 2x.}\JL{Added layer index, also added a sentence in Line 170 addressing the numbers of layers throughout, which is only 1}
This is now common practice in machine learning models that involve many layers of neural networks, as residual connections have been known to help alleviate vanishing gradients and improve overall stability during training \cite{he_2020}. However, because our model consists of a single block of attention layers, i.e. one pass through the encoder and one pass through the decoder, these skip connections are not required. Nevertheless, we include them here since future efforts in the space weather community may require more expressive models, and one way of achieving this is by including additional blocks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Loss Function}\label{chp4:sec:loss_fun}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The loss function uses a combination of the mean squared error (MSE) of the linear predictors and the multi-quantile loss on the combined linear and residual predictions. The quantile loss, or pinball loss, for a single quantile $\tau \in (0,1)$ is
% \begin{equation}
%     \mathcal{L}_{\tau}(x,\hat{x}) =
%     \begin{cases} 
%         \tau(x-\hat{x}) & \text{if } x \ge \hat{x}\\ 
%         (\tau-1)(x-\hat{x}), & \text{if } x < \hat{x}
%     \end{cases}.
% \end{equation}
\begin{equation}\label{chp4:eqn:quantilefunc}
    \mathcal{L}_\tau(\mathbf{X}_{t+1:t+p}, \hat{\mathbf{X}}_{t+1:t+p}) = \sum_{i=1}^{d_1} \sum_{k=1}^{p} \max\left(\tau (\mathbf{X}_{t+k,i} - \hat{\mathbf{X}}_{t+k,i}), (1-\tau)(\hat{\mathbf{X}}_{t+k,i} - \mathbf{X}_{t+k,i})\right),
\end{equation}
where $\mathbf{X}_{t+k}$ are the true target values, and $\hat{\mathbf{X}}_{t+k}$ are the predicted targets from Equation \ref{chp4:eqn:themodel}. By minimizing this loss, the model learns to align the predicted quantiles with the empirical distribution of the target variables.

This loss function penalizes overestimation and underestimation asymmetrically based on the level of $\tau$, and when $\tau=0.5$ it is equivalent to the mean absolute error, or L1 loss function. 
% \ati{"it"? loss is not equivalent to the median? Loss at 0.5 is equiv. to L1 loss, whose minimizer is the median.} \JL{good catch, fixed}
This loss has been used as a standard forecasting metric for weather models where observations can only be obtained once \cite{james_2000, gneiting_2007, koenker_2017, chung_2021}. Figure \ref{chp4:fig:quantile_loss} plots this loss function for several quantile levels, which serves to illustrate the asymmetric penalty induced by quantile levels above or below $\tau=0.5$.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{fig_pinball_loss.pdf}
  \caption[Illustration of the quantile loss function]{An illustration of the quantile loss function for different levels of the desired predicted quantile, $\tau$, 
  given a true target value of $x=0$.
  Each quantile level corresponds to an asymmetric absolute error that penalizes over- or under-prediction differently depending on the quantile $\tau$. When $\tau<0.5$, overestimates incur a larger penalty, encouraging our prediction to be, on average, smaller than the true value; conversely, when $\tau>0.5$ underestimates incur a larger loss penalty. At $\tau=0.5$, the loss reduces to the mean absolute error.}
  \label{chp4:fig:quantile_loss}
\end{figure} 

Applying multiple quantile levels to Equation \ref{chp4:eqn:quantilefunc}, say, $\mathcal{T}=\{0.05,0.1,0.25,0.5,0.75,0.9,0.95\}$, and averaging, we have
\begin{equation}
    \mathcal{L}_{residual} = \frac{1}{|\mathcal{T}| \ d_{1} \ p} \ \sum_{\tau \in \mathcal{T}}\mathcal{L}_\tau(\mathbf{X}_{t+1:t+p}, \hat{\mathbf{X}}_{t+1:t+p}).
\end{equation}

Finally, incorporating the linear component of the model, the total linear plus residual multi-quantile loss for a given test segment from times $t+1$ to $t+p$, is
\begin{align}
    \mathcal{L}_{\text{total}}\left(\mathbf{X}_{t+1:t+p}, \hat{\mathbf{X}}_{t+1:t+p}^{L}, \hat{\boldsymbol{\epsilon}}_{t+1:t+p}\right) = \mathcal{L}_{\text{linear}} + \mathcal{L}_{\text{residual}}.
\end{align}
Using our seven selected quantile levels, we may generate empirical estimates of the $90^{th}-$percent CI as well as the interquartile range (IQR), defined by the region between the $25^{th}$ and $75^{th}$ percentiles. Conceptually, our transformer network is adding a nonlinear corrective term to the linear predictor by learning to predict the median residual, while also quantifying uncertainty in the forecast in a manner that incorporates all available data from both past and future and without relying on any assumptions of the underlying distributions of the residuals. 

In practice, we also dynamically weight each component of the loss function, as the loss can become dominated by the errors of the na\"ive linear predictor $\mathcal{L}_{\text{linear}}$. We balance the loss function using learnable weights from each model component, $w_{\text{linear}}$ and $w_{\text{residual}}$, so that
\begin{equation}
    \mathcal{L}_{\text{total}} = e^{-w_{\text{linear}}} \cdot \mathcal{L}_{\text{linear}} + e^{-w_{\text{residual}}} \cdot \mathcal{L}_{\text{residual}} + (w_{\text{linear}} + w_{\text{residual}}).
\end{equation}
These weights adjust the contribution of each loss component during training, and the exponentiation ensures that they are always positive and inversely related to the magnitude of the respective component \cite{kendall_2018}. We found that this weighting reduced the number of epochs needed to reach our early stopping criterion, which was set to halt training when the validation loss was not reduced by at least $1\text{e}^{-4}$ for more than 8 epochs. Additionally, we include dropout layers \cite{hinton_2012} in each of the transformer's multi-head attention blocks to further control overfitting. 

The linear error term only depends on the parameters of each linear model component and thus can be pre-trained using multiple least squares fits. However, this could introduce difficulties if the LIFT model is trained on larger datasets, as the least squares equations would require significant memory. We also found that including the linear components as additional linear layers and training them with Adam did not significantly increase training time.

% \ati{Seems like the linear error term only involves its own parameters and loss, i.e., doesn't depend on the transformer at all, yes?  So it could be pre-trained (just multivariate linear regression?) and then held fixed, and the transformer correction learned after?}\JL{Yes, added some sentences above to address this.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Positional Encoding}\label{chp4:sec:pos_encoding}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The basic transformer architecture does not have any inherent mechanism to preserve the temporal or sequential structure of the data. Therefore, it is often necessary to provide a positional encoding feature. The now conventional positional encoding from \cite{vaswani_2017} only gives relative position information for each data segment, so, we also equip our transformer with a time vector. The time vector provides the model with absolute position information, which we split into quadrature components,
\begin{align}
    \mathbf{t}(t) &= 
    \begin{bmatrix}
        \cos\left(\frac{2\pi\text{YOS}_{t}}{11\times365.25}\right) \\
        \cos\left(\frac{2\pi \text{DOY}_{t}}{365.25}\right) \\
        \cos\left(\frac{2\pi \text{HOD}_{t}}{24}\right) \\
        \cos\left(\frac{2\pi \text{MOD}_{t}}{60}\right) \\
        \sin\left(\frac{2\pi \text{YOS}_{t}}{11\times365.25}\right) \\
        \sin\left(\frac{2\pi \text{DOY}_{t}}{365.25}\right) \\
        \sin\left(\frac{2\pi \text{HOD}_{t}}{24}\right) \\
        \sin\left(\frac{2\pi \text{MOD}_{t}}{60}\right)
    \end{bmatrix}^{T},
\end{align}

where $\text{YOS}_{t}$, $\text{DOY}_{t}$, $\text{HOD}_{t}$, and $\text{MOH}_{t}$ are components of the time variable $t$ that represent the year-of-solar-cycle, day-of-year, hour-of-day, and minute-of-day respectively \cite{poole_2000, athieno_2017}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model covariates}\label{chp4:sec:model_covariates}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Incorporating additional covariates into the transformer model is as easy as concatenating another time series to the matrix $\mathbf{X}$. Moreover, what sets the transformer apart from most other empirical and assimilative models is its natural ability to incorporate what may be considered future-known covariates through the decoder block (see Figure \ref{chp4:fig:model_diagram}). Since IRI is a quick and efficient means of generating climatological predictions of the F2-layer parameters, it is an obvious choice for a decoder input. We use a convenient, pure Python version of IRI called PyIRI \cite{forsythe_2024} to generate foF2, hmF2, and the thickness parameters $B_{bot}^{\text{F2}}$ and $B_{top}^{\text{F2}}$ for every contiguous data segment. Together, these IRI covariates allow the transformer to include climatology for the F2-region of the ionosphere that uses the latest parameterization of the observed F10.7cm solar flux. Finally, the solar zenith angle (SZA) is calculated for each geographic location and time \cite{reda_2008}. Figure \ref{chp4:fig:data_covariates} illustrates these additional covariates for the Boulder, CO, USA ionosonde station.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{fig_covariates.png}
  \caption[LIFT model covariates]{Model covariates that form $\mathbf{Z}_{t-c:t+p}$, which are computable and provide the encoder and decoder networks with climatology (IRI) and solar position data (SZA) for a given sounding station.}
  \label{chp4:fig:data_covariates}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}\label{chp4:sec:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
A challenge with any time series model is determining the appropriate length of data to provide as input. In classic Box-Jenkins methods, a simple partial autocorrelation can inform the maximum input sequence length, and while previous work \cite{schmolter_2024} suggests that delayed responses of certain ionospheric parameters to solar activity can exceed 48 hours, giving a rough estimate for a desired time lag, our transformer is learning to embed all data in high-dimensional latent variables, thus potentially altering any correlations one may find through the raw data alone. To address this, we adopt a brute-force method and train multiple models with different sequence lengths for the encoder and decoder networks. This was only viable given the relatively reasonable training times of roughly 1-5 hours per model, which we carried out in parallel on a computer with 4 NVIDIA A100 GPUs. Figure \ref{chp4:fig:sequence_length_grid} shows the result of this model search in which we find a clear relationship between longer encoder lengths and a reduction in the overall root mean squared error (RMSE).
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.65\textwidth]{fig_final_encoder-decoder-lengths-grid.pdf}
  \caption[LIFT RMSE for varying encoder and decoder lengths]{Sum of RMSE for foF2, hmF2, and TEC for varying encoder and decoder sequence lengths. The RMSE reported here is the cumulative RMSE over the three forecast parameters for the median prediction.}
  \label{chp4:fig:sequence_length_grid}
\end{figure}

% \ati{This is test loss, right? Did you evaluate training loss? I'm guessing everything on the right of the image is due to overfitting? What about zero decoder hours?}

We are primarily interested in the model's ability to produce a 24-hour forecast; Figure \ref{chp4:fig:sequence_length_lines} shows the same RMSE values, restricted to a decoder length of 24 hours.
In light of Figures \ref{chp4:fig:sequence_length_grid} and \ref{chp4:fig:sequence_length_lines}, we chose an input sequence length of 3 days: beyond 3 days, we find that the total RMSE for a 24-hour forecast is only marginally improved with longer input sequences.  This input length balances the accuracy of the forecast against the need for very long sequential segments from a given sounder, which makes the model more practical in an operational setting. In practice, of course, one may retrain this model to fit their specific needs for forecast horizon.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_final_encoder-decoder-lengths-lines.pdf}
  \caption[LIFT RMSE for varying encoder lengths]{Sum of RMSE for foF2, hmF2, and TEC for varying encoder lengths for a fixed output length of 24-hours. }
  \label{chp4:fig:sequence_length_lines}
\end{figure}

The test stations, which were held out entirely during the training process, allow us to examine the model's ability to generalize to new geographic locations. Randomly selected samples from the test set are provided in the following figures to illustrate the prediction of foF2, hmF2, and TEC in practice along with their respective $90\%$ CI and IQR. Figure \ref{chp4:fig:test_mid_lat} is from a mid-latitude sounder located at $(41.90 \text{N}, 12.50 \text{E})$, and additional forecasts for high-latitude $(64.66 \text{N}, 212.93 \text{E})$ and low-latitude $(-2.60 \text{N}, 315.80 \text{E})$ stations are given in Figures \ref{chp4:fig:test_high_lat} and \ref{chp4:fig:test_low_lat}, respectively. These forecasts show, qualitatively, the model's ability to produce uncertainty quantification that is dynamic and dependent on the given segment of data.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig_test_segment_RO041.pdf}
  \caption[LIFT forecasts for Rome, Italy]{Model forecasts for foF2, hmF2, and TEC parameters from a mid-latitude test holdout station RO041 in Rome, Italy.}
  \label{chp4:fig:test_mid_lat}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig_test_segment_EI764.pdf}
  \caption[LIFT forecasts for Fairbanks, AK, USA]{Model forecasts for foF2, hmF2, and TEC parameters from a high-latitude test station EI764 near Fairbanks, Alaska, USA.}
  \label{chp4:fig:test_high_lat}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig_test_segment_SAA0K.pdf}
  \caption[LIFT forecasts for Sao Luis, Brazil]{Model forecasts for foF2, hmF2, and TEC parameters from a low-latitude test station SAA0K in Sao Luis, Brazil.}
  \label{chp4:fig:test_low_lat}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Forecast accuracy}\label{sec:accuracy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We quantify the overall forecast accuracy of the combined linear model plus median predictions from the transformer using Figures \ref{chp4:fig:test_fof2}, \ref{chp4:fig:test_hmf2}, and \ref{chp4:fig:test_tec}. These figures report the median absolute deviation (MAD) and mean absolute percent error (MAPE) in addition to RMSE. We achieve an overall RMSE of 0.617 MHz for foF2 across all test segments, including high- and low- solar activity and different latitudes. The hmF2 parameter, which is a challenging parameter to predict but has a high impact on many operational systems \cite{bilitza_2022}, has an average RMSE of 21.6 km, and the TEC parameter averages 2.426 TECU over the entire test period. Note that the IQRs for each are represented in the box-plots underneath each error distribution and show that the forecasts, in general, are capturing the point predictions well.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig_test_metrics_mse-mad-mape_foF2.pdf}
  \caption[Basic LIFT statistics for foF2 forecasts]{Model forecast statistics for the foF2 parameter averaged across all test stations.}
  \label{chp4:fig:test_fof2}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig_test_metrics_mse-mad-mape_hmF2.pdf}
  \caption[Basic LIFT statistics for hmF2 forecasts]{Model forecast statistics for the hmF2 parameter averaged across all test stations.}
  \label{chp4:fig:test_hmf2}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig_test_metrics_mse-mad-mape_TEC.pdf}
  \caption[Basic LIFT statistics for TEC forecasts]{Model forecast statistics for the TEC parameter averaged across all test stations.}
  \label{chp4:fig:test_tec}
\end{figure}

Comparisons with IRI are straightforward, since PyIRI has provided the peak height and frequency of the F2-layer as additional covariates to the LIFT encoder and decoder. Figure \ref{chp4:fig:model_vs_iri} shows the RMSE error distribution for foF2 and hmF2 using LIFT versus IRI. The LIFT model provides an appreciable increase in skill over IRI parameterized with the F10.7cm solar flux value from the last observation before the forecast period begins. Despite the best available drivers, IRI is still a climatological model, and without computationally expensive data assimilation schemes \cite{bilitza_2017}, the short-term forecast accuracy is difficult to improve.
\begin{figure}[htbp]
  \centering
  % \includegraphics[width=\textwidth]{fig_final_model_vs_IRI.pdf}
  \includegraphics[width=\textwidth]{fig_final_model_vs_IRI_only-error.pdf}
  \caption[LIFT median forecast performance versus IRI]{Model median predictions versus IRI parameterized with the F10.7 value from at the final context variable time step.}
  \label{chp4:fig:model_vs_iri}
\end{figure}

The LIFT model provides uncertainty quantification with each forecast; we evaluate how well calibrated the predicted quantiles are through Figure \ref{chp4:fig:test_quantile_statistics}, in which each distribution represents the percentage of points that fall within each confidence region. The statistics for these confidence regions are determined over the full 24-hour duration of each forecast individually,
\begin{align*}
    \text{90\% CI Calibration}(\hat{\mathbf{x}}_{t+1:t+p;j}) &= 100*\frac{1}{p}\sum_{i=1}^{p}\mathbbm{1}(\hat{x}^{0.05}_{t+i;j} < x_{t+i;j}<\hat{x}^{0.95}_{t+i;j}) \\
    \text{IQR Calibration}(\hat{\mathbf{x}}_{t+1:t+p;j}) &= 100*\frac{1}{p}\sum_{i=1}^{p}\mathbbm{1}(\hat{x}^{0.25}_{t+i;j} < x_{t+i;j}<\hat{x}^{0.75}_{t+i;j}),
\end{align*}
where $\mathbbm{1}(condition)$ is the indicator function which equals 1 when \textit{condition} is satisfied and $0$ otherwise. So, for each 24-hour forecast from LIFT, we obtain a single percentage for the number of points correctly within each CI range. Perfectly calibrated quantiles over the entire test dataset would result in the distributions in Figure \ref{chp4:fig:test_quantile_statistics} being delta functions at 90 and 50, for the 90\% CI and IQR, respectively. Here we see that both the 90\% CI and IQR are conservative, often estimating uncertainty bounds that are slightly too wide. Future iterations of this approach may explore additional means of calibrating the uncertainty bounds, such as incorporating conformalized quantiles \cite{ramano_2019}, however, our results for the 90\% CI for TEC are comparable to those of \cite{natras_2023} who explored the use of super ensembles, quantile gradient boosting, and Bayesian neural network methods for uncertainty quantification.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_quantile_statistics.pdf}
  \caption[Calibration of the LIFT forecast model]{Model quantile calibration for the foF2, hmF2, and TEC parameters across all test stations for the $90\%$-CI, $\{\tau=0.05, \tau=0.95\}$, and the IQR, $\{\tau=0.25, \tau=0.75\}$.}
  \label{chp4:fig:test_quantile_statistics}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.97\textwidth]{fig_quantile_boxplots.pdf}
  \caption[Boxplots for LIFT model bias]{Boxplots of the daily percentile values for (top) foF2 (MHz), (middle) hmF2 (km), and (bottom) total electron content (TEC, in TECU). For each day in the test dataset, the specified percentile (e.g., 10\%, 20\%, , 90\%) is computed from the observed data (black), the forecast model predictions (colored boxes, as labeled), and the IRI climatological predictions (colored boxes, as labeled). Each box therefore represents the distribution across all days of that daily p-th percentile; the box edges show the interquartile range, the horizontal line (or triangle) indicates the median (or mean), and whiskers extend to capture the spread of values. Comparison of the model and IRI distributions against the observed distribution provides an overall indication of forecast bias and variability at different percentile levels.}
  \label{chp4:fig:test_quantile_boxplots}
\end{figure}
We may also examine our overall model bias as compared to the IRI climatological predictions. Figure \ref{chp4:fig:test_quantile_boxplots} demonstrates an improvement in the total model bias using LIFT over IRI. Each boxplot corresponds to the distribution across all 24hour forecasted periods in the test dataset of the p-th percentile for observed data, the forecast model, and the climatological model (IRI). That is, for each day we compute, say, the $10^{th}$ percentile of the observed data, and then gather those $10^{th}$percentile values across all days into one boxplot---repeating the same procedure for the model and for IRI at each percentile. From the figure, we show how the median, interquartile range, and whiskers for that daily percentile varies over the entire test dataset.
% \ati{Not sure I understand what's being plotted? The model quantiles are a function of the past context; how are they averaged over the test data set?}\JL{I've completely changed this plot. Hopefully it is more clear now.}
While this demonstrates a generally well-calibrated model over the entirety of each 24-hour forecast, calibration of the model's uncertainty as a function of time-of-day will be a focus for future efforts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model Forecast Impacts}\label{chp4:sec:impacts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Numerical ray tracing offers additional insight into the performance of our ionospheric forecast model over simply driving down the MSE as compared to climatology. One of the primary use cases for ionospheric forecasting is to simulate radio propagation for long-range radar and communications systems. The ray tracing model we use is based on the Jones Stephenson 3D magnetoionic model that is prevalent throughout the space weather community \cite{jones_1975}. We numerically solve for the ray paths using EDPs constructed from our predicted parameters, specifically, foF2 and hmF2. The ray paths allow us to determine the distances a radio wave at each frequency will propagate down range. In doing so, we may illustrate how an increase in the prediction accuracy of these parameters and their uncertainty quantification translates directly into measurable improvements for operational users.

Figure \ref{chp4:fig:wsbis_low_guam} shows frequency-range plots for a hypothetical radio transmitter for which the sounding station GU513 (Guam, USA) is roughly at the midpoint of its propagation path. We randomly selected a time from our test period and reconstructed EDPs using parameters from the sounder measurements, our LIFT model, and the IRI climatology and then traced rays at frequencies from 3-30MHz through each EDP to give full coverage of the HF radio band. We see in Figure \ref{chp4:fig:wsbis_low_guam} that the forecasted frequency-range plot more closely matches that of the true profile. Similar plots are regularly used to help inform the operational space weather community as to what frequencies should be used to propagate out certain distances, wherein the leading edge of these heatmaps shows the maximum usable frequency for a given range. Regarding the leading edge curve generated by the LIFT model's median prediction, it represents a significant improvement over IRI, even though IRI is being parameterized with the F10.7cm solar flux index from the timestamp of the last input to the encoder module. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_GU513_wsbi_low_201911011245.pdf}
  \caption[Frequency-range plots derived from LIFT output for low solar activity]{Simulated frequency-range plots using vertical electron density profiles generated from the observed foF2 and hmF2 (True), the LIFT model (Forecasted), and IRI predicted (IRI). The date for this simulation was randomly selected but constrained to a period in 2019 of low solar activity.}
  \label{chp4:fig:wsbis_low_guam}
\end{figure}

However, the leading edge curve from the median prediction is only the point forecast. Using the LIFT model CIs for foF2 and hmF2, we may generate additional leading edge curves and produce uncertainty quantification in these downstream propagation tools as well. While Equation \ref{chp4:eqn:cipred} represents the $90\%$-CI for both foF2 and hmF2 simultaneously, we may also produce different permutations of the quantile outputs from LIFT for each parameter. Figure \ref{chp4:fig:leading_edges_low_guam} illustrates the leading edge curves for these permutations of the estimated LIFT quantiles. These curves reveal the inverse relationship between foF2 and hmF2, where the true leading edge will almost always be bracketed on the left by the combination ($5^{th}$-percentile foF2, $95^{th}$-percentile hmF2) and on the right by ($95^{th}$-percentile foF2, $5^{th}$-percentile hmF2). These represent the extremes in our forecast, and while their purpose may remain illustrative in this work, they further motivate the need for multivariate uncertainty quantification in future ionospheric forecasting models.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_GU513_leading_edge_low_201911011245.pdf}
  \caption[Leading edge plots derived from LIFT output for low solar activity]{Simulated leading edges of frequency-range plots using vertical electron density profiles generated from the observed foF2 and hmF2 (True), IRI predicted (IRI), and the upper and lower 5th percentiles from the LIFT model (Forecasted). The date for this simulation was randomly selected but constrained to a period in 2019 of low solar activity.}
  \label{chp4:fig:leading_edges_low_guam}
\end{figure}

Figures \ref{chp4:fig:wsbis_low_guam} and \ref{chp4:fig:leading_edges_low_guam} only illustrate a single time stamp from the 24-hour LIFT model forecast. By generating the leading edge curves in Figure \ref{chp4:fig:leading_edges_low_guam} for every time step in the forecast, we show which combination of quantiles most accurately matched the leading edge derived from the sounder profile as a function of time and range. Figure \ref{chp4:fig:le_best_q_guam_low} shows this as a heatmap, and Figure \ref{chp4:fig:le_hist_guam_low} provides the histogram for how many range-time bins each combination was the best match of the true leading edge. The histogram indicates that this forecast period was particularly well matched by the median LIFT output ($50^{th}, 50^{th}$). See Appendices \ref{appendix1} and \ref{appendix2} for additional frequency-range and leading edge plots for periods of low and high solar activity, respectively.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{fig_le_best_q_GU513_low_201911011245.pdf}
    \caption[LIFT leading edge performance during low solar activity]{Performance of LIFT quantile forecast in Guam, USA during a period of low solar activity.}
    \label{chp4:fig:le_best_q_guam_low}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{fig_le_hist_q_GU513_low_201911011245.pdf}
  \caption[Histogram summary of LIFT leading edge performance for low solar activity]{Histogram reporting which quantile combination of foF2 and hmF2 parameters from the LIFT forecast produced the most accurate HF propagation in Guam, USA during a periods of low solar activity.}
  \label{chp4:fig:le_hist_guam_low}
\end{figure}
Although the purpose of these plots in this work is purely qualitative, future efforts to improve LIFT models may seek to incorporate these types of propagation results into the calibration of the model uncertainty. While numerical ray tracing is a computationally expensive process to incorporate into the training process, it is also possible that substituting more efficient approximations could also be useful in performing model calibration. %\ati{edited; check}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Conclusions}\label{chp4:sec:conlcusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We have demonstrated that transformer models can be used in a pseudo-data assimilation fashion that fuses multiple space weather data sources with naive predictors. In this method, we treat both the simple linear autoregression and the IRI climatology as naive forecasts. While the transformer makes including exogenous covariates very straightforward, it is also a natural choice for quantile forecasting and uncertainty quantification due to its ability to translate long-range influences in the data into confidence regions that are tailored to each data segment rather than relying on assumptions about the underlying distributions of the errors. The other advantage of transformers for space weather forecasting is their efficiency in processing long time series with relatively high cadence in comparison to conventional recurrent neural networks (RNNs) such as long short-term memory (LSTM) networks. We also establish how the predicted confidence regions for these parameters translate directly into real-world applications for more robust radio propagation assessments and planning through ray traced frequency-range plots.

Although the LIFT model approach addresses several challenges in modern space weather forecasting, it is nevertheless limited to local forecasts and may not provide the most robust predictions under severely disturbed conditions. Future iterations of this approach will therefore focus on extensions to global grids and better calibration of uncertainty, especially in the tails of the distributions and during perturbed conditions. It is important to reiterate that the data used in this study were minimally processed from the original GIRO dataset, only using the ARTIST5 confidence scores to remove poorly autoscaled data. This was intentional and meant to demonstrate that we could train a reasonably well-calibrated system using data as one would expect it to find it in real-time from the GIRO repository. This is in contrast to many other data-driven models built from only exquisite, manually scaled EDPs. Finally, transformers require large amounts of data to train on, and time series prediction in particular is much easier when data are contiguous. This is not the case for the other common parameters for the ionospheric profile that specify the F1- and E- regions, but future work is required to include these types of data in the output of this model as they only intermittently available, i.e. only measurable during certain hours of the day or during certain space weather conditions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Availability Statement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Availability Statement}\label{sec:availability}
The model code is openly available at \url{https://github.com/JayLago/LIFT}. The ionosonde data may be found at \url{https://giro.uml.edu/didbase/}. The SSN, Kp, and F10.7 indices were obtain from the GFZ German Research Centre for Geosciences, \url{ftp://datapub.gfz-potsdam.de/download/10.5880.Kp.0001}. Storm index, Dst, was obtained through the WDC for Geomagnetism, Kyoto \url{http://wdc.kugi.kyoto-u.ac.jp/wdc/Sec3.html}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgments
\section*{Acknowledgments} \label{sec:acknowledgements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The authors would like to thank Dr. Victoriya Forsythe for her assistance and input with running the PyIRI model, the code for which can be found at \cite{forsythe_2024}.The authors would like to also thank Dr. Ivan Galkin from the LGDC for the data they continue to collect and make available. We would also like to acknowledge the authors of \cite{zhou_2021, zhou_2023} for the code they made available and which initially formed a basis for constructing our model.

\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Frequency Versus Range Plots from the LIFT Model During Low Solar Activity}\label{appendix1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following presents visualizations of the performance of the LIFT model when used with numerical ray tracing to estimate the frequency versus range curves within the HF radio band during low solar activity. These results were generated for a randomly selected sounding station and time from the test set. Presented are the predicted frequency-range heatmaps and leading edge curves at $+0, +3, +6, +9, +12, +15, +18$ and $+21$ hours. The station in Learmonth, Australia, was selected from the test set and a date was randomly chosen from the period 2019-7-1 00:00Z to 2019-12-31 00:00Z. This occurred roughly during the last solar minimum. Figures \ref{appendix:wsbi-low1} to \ref{appendix:wsbi-low8} illustrate the frequency-range plots and affiliated leading edges. Figure \ref{appendix:bestqs-low} summarizes which forecasted uncertainty ranges for different levels of CI for each parameter (foF2, hmF2) produced the most accurate estimates of the leading edge as a function of time and range.
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910170000.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910170000.pdf}
    \end{tabular}
    \caption{Forecast +0 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low1}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910170300.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910170300.pdf}
    \end{tabular}
    \caption{Forecast +3 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low2}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910170600.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910170600.pdf}
    \end{tabular}
    \caption{Forecast +6 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low3}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910170900.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910170900.pdf}
    \end{tabular}
    \caption{Forecast +9 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low4}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910171200.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910171200.pdf}
    \end{tabular}
    \caption{Forecast +12 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low5}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910171500.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910171500.pdf}
    \end{tabular}
    \caption{Forecast +15 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low6}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910171800.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910171800.pdf}
    \end{tabular}
    \caption{Forecast +18 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low7}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201910172100.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201910172100.pdf}
    \end{tabular}
    \caption{Forecast +21 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-low8}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.95\textwidth]{fig_le_best_q_LM42B_low_201910170000.pdf} \\[2ex]
        \includegraphics[width=0.95\textwidth]{fig_le_hist_q_LM42B_low_201910170000.pdf}
    \end{tabular}
    \caption{Best forecasted quantile pairings (foF2, hmF2) at the given timestamp for station LM42B in Learmonth, Australia located at -21.80 latitude, 114.10 longitude.}\label{appendix:bestqs-low}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Frequency Versus Range Plots from the LIFT Model During High Solar Activity}\label{appendix2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The following presents visualizations of the performance of the LIFT model when used with numerical ray tracing to estimate the frequency versus range curves within the HF radio band during high solar activity. These results were generated for a randomly selected sounding station and time from the test set. Presented are the predicted frequency-range heatmaps and leading edge curves at $+0, +3, +6, +9, +12, +15, +18$ and $+21$ hours. The station in Learmonth, Australia, was selected from the test set and a date was randomly chosen from the period 2013-12-1 00:00Z to 2014-5-30 00:00Z. This occurred roughly during the last solar maximum. Figures \ref{appendix:wsbi-low1} to \ref{appendix:wsbi-low8} illustrate the frequency-range plots and affiliated leading edges. Figure \ref{appendix:bestqs-low} summarizes which forecasted uncertainty ranges for different levels of CI for each parameter (foF2, hmF2) produced the most accurate estimates of the leading edge as a function of time and range.
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402191130.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402191130.pdf}
    \end{tabular}
    \caption{Forecast +0 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high1}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402191430.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402191430.pdf}
    \end{tabular}
    \caption{Forecast +3 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high2}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402191730.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402191730.pdf}
    \end{tabular}
    \caption{Forecast +6 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high3}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402192030.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402192030.pdf}
    \end{tabular}
    \caption{Forecast +9 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high4}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402192330.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402192330.pdf}
    \end{tabular}
    \caption{Forecast +12 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high5}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402200230.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402200230.pdf}
    \end{tabular}
    \caption{Forecast +15 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high6}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402200530.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402200530.pdf}
    \end{tabular}
    \caption{Forecast +18 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high7}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.65\textwidth]{fig_wsbi_LM42B_201402200830.pdf} \\[1ex]
        \includegraphics[width=0.75\textwidth]{fig_leading_edge_LM42B_201402200830.pdf}
    \end{tabular}
    \caption{Forecast +21 hours. (Top) Frequency vs. range plot using EDPs derived from foF2 and hmF2 parametres from: the sounder measurement (Ionosonde), LIFT model median, and IRI. (Bottom) The leading edge of the frequency vs. range plots using: the sounder measurement (sounder), IRI climatology, and all combinations of the LIFT predicted quantiles.}\label{appendix:wsbi-high8}
\end{figure}
\begin{figure}[htbp]
    \centering
    \begin{tabular}{c}
        \includegraphics[width=0.95\textwidth]{fig_le_best_q_LM42B_high_201402191130.pdf} \\[2ex]
        \includegraphics[width=0.95\textwidth]{fig_le_hist_q_LM42B_high_201402191130.pdf}
    \end{tabular}
    \caption{Best forecasted quantile pairings (foF2, hmF2) at the given timestamp for station LM42B in Learmonth, Australia located at -21.80 latitude, 114.10 longitude.}\label{appendix:bestqs-high}
\end{figure}


\clearpage
\bibliography{ms}
\bibliographystyle{unsrt}
\end{document}



