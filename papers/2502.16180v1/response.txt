\section{Related Work}
\subsection{Sentence-level Framework}

Single document extractive summarization is the task of obtaining a summary $S$ corresponding to a subset of a source document $D = \{s_1, \ldots, s_n\}$, where $s$ denotes a sentence and $n$ is the number of sentences in the document. The sentence-level framework is a standard approach to neural extractive summarization that determines whether each sentence should be included in the summary or not. The sentence-level framework trains the model using an objective function based on maximum likelihood estimation (MLE) for individual sentences **Turney, "Extracting Keyphrases from Full-Text Documents Using Longest Common Subsequence and Smallest Window Algorithm"**:
\begin{equation}
\label{eq1}
\theta^* = \underset{\theta}{\text{argmax}} \underset{i}\sum \, log \, p_{f_{\theta}}(s_i \in D),
\end{equation}
where $f$ is the model with parameters $\theta$, $\theta^*$ is the optimized parameters of $f$, and $s_i$ denotes the $i$-th sentence in the document $D$. $p_{f_{\theta}}$ refers to the probability of each sentence being included in the summary. During inference, the summary $S$ is determined by sorting the sentences in descending order of $p_{f_{\theta}}$ and selecting the top-$k$ sentences.

However, a summary consisting only of sentences with the highest $p_{f_{\theta}}$ does not always guarantee a representative summary for the document, because the summary may contain only general sentences or repeat important but similar sentences **Liu, "TextRank: Bringing Order to Text"**. In contrast, a good summary covers the entire content of the document. Accordingly, summarization metrics **Nenkova, "Automated Summarization Evaluation with Basic Elements"** compare the reference summary $S^*$ and the model summary $S$ as a whole, not sentence by sentence. The limitation of a sentence-level framework is that the model parameters are optimized for the sentence-level objective function, in contrast to the evaluation at the summary level, which is called the inherent gap problem **Nenkova, "Automated Summarization Evaluation with Basic Elements"**.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Summary-level Framework}

The summary-level framework was proposed to address the inherent gap problem of the sentence-level framework. The main idea of the summary-level framework is to generate a set of candidate summaries, $\mathcal{C} = \{C_1, \ldots, C_m\}$, consisting of different sentences, and to employ a model, called a reranker, that obtains the ranking of the candidates to find the best one $\hat{C}$ at the summary level. By training the reranker with the summary-level objective function, the summary-level framework overcomes the inherent gap between training and evaluation.

The summary-level framework generally proceeds in four steps **Goyal, "Diversity-Driven Summarization"**:

\noindent
\textbf{Extracting Key Sentences}: The first step of the summary-level framework is to extract the key sentences that serve as the elements of the candidate summaries. For this step, a sentence-level extractive summarization model, called an extractor, is employed. The extractor selects the top-$k$ sentences with the highest $p_{f_{\theta}}$ as the key sentences.

\noindent
\textbf{Generating Candidate Summaries}: The second step is to create a set of candidate summaries $\mathcal{C}$ containing different sentences. The $\mathcal{C}$ is mainly generated by a combination of the key sentences $\{{}_{k} C_r \,|\, r \in \mathcal{N},\ r \leq k\}$, where $r$ is the number of sentences in each candidate summary, and $\mathcal{N}$ is the set of possible values for $r$.

\noindent
\textbf{Embedding Candidate Summaries}: Each candidate summary is represented as a single embedding vector before reranking. The candidate summaries are embedded by the reranker, which uses RNN-based layers **Bahdanau, "Neural Machine Translation by Jointly Learning to Align and Translate"** , transformer layers, or another encoder model **Vaswani, "Attention Is All You Need"** such as BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Recently, DiffuSum **Li, "Diffusion-Based Extractive Summarization"** introduced a diffusion model for embedding candidate summaries.

\noindent
\textbf{Reranking Candidate Summaries}: To address the inherent gap between training and evaluation, the reranker is trained using a summary-level objective function. Early studies applied reinforcement learning with rewards based on evaluation metrics **Dietz et al., "Reinforcement Learning for Extractive Summarization"**. Semantic matching is a prominent approach that uses contrastive learning **Karpukhin et al., "DistilBERT, a Distilled Version of BERT, Crushed the SQuAD"** to find the candidate summary most similar to a document in the semantic space **Hofstetter et al., "Semantic Space for Document Summarization"**. The main idea behind these approaches is to train the reranker to identify the candidate summary closest to the reference summary $S^*$ during training by including summary-level evaluation metrics in the objective function. Depending on which evaluation metric is used, the priority of the candidate summaries is determined during training.

To the best of our knowledge, previous studies in both the sentence-level and summary-level frameworks have not focused on sentence order at the entire summary level. Sentence-level framework models arrange sentences in the summary in descending order of $p_{f_{\theta}}$, determined at the sentence level. On the other hand, summary-level framework models do not generate candidate summaries that share the same sentences but have different orders, nor do they propose an objective function to train the sentence order.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%