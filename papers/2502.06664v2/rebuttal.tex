\documentclass[10pt,twocolumn,letterpaper]{article}

% ---------------------------------------------------------------
% Include basic CVPR package in rebuttal mode

\usepackage[rebuttal]{template/cvpr}

% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{template/eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage{xcolor}
\definecolor{seabornBlue}{RGB}{76,114,176}
\definecolor{seabornGreen}{RGB}{85,168,104}
\definecolor{seabornRed}{RGB}{196,78,82}

\newcommand{\RThree}{\textcolor{seabornRed}{R3}}
\newcommand{\RFour}{\textcolor{seabornBlue}{R4}}
\newcommand{\RFive}{\textcolor{seabornGreen}{R5}}
% ---------------------------------------------------------------
% Hyperref package
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).

\definecolor{eccvblue}{rgb}{0.12,0.49,0.85}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}


% ---------------------------------------------------------------
% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
\setcounter{figure}{1}
\setcounter{table}{1}
\setcounter{equation}{1}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


% ---------------------------------------------------------------
% TODO REBUTTAL: Please enter paper ID
\def\paperID{5240} % *** Enter the paper ID here
\def\confName{ICASSP}
\def\confYear{2025}

\begin{document}

% ---------------------------------------------------------------
% TODO REBUTTAL: Please enter paper title
\title{Evaluation of Deep Audio Representations for Hearables}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix
\frenchspacing

We sincerely thank all reviewers for their detailed and thorough assessments of our work.
We particularly appreciate the recognition from \RThree, \RFour, and \RFive{} that our paper makes a significant contribution to the field by addressing current gaps in the evaluation of foundation models for hearables (\RFour).
\RThree{} highlights the rapid growth of the hearable space and emphasizes the importance of introducing a specific benchmark tailored to this domain.
Furthermore, \RFour{} recognizes the DEAR benchmark as innovative and necessary for advancing audio processing capabilities tailored to assistive hearing.
\RFour{} also notes the paper's potential to generate widespread interest among both industry and academic researchers in audio and AI.

We address the reviewers' comments in detail below and are committed to incorporating all constructive feedback to strengthen the paper further.

\textbf{\RThree{} - Dataset splitting:}
The dataset to be released is generated using two disjoint sets for all three components---monologues, background environment recordings, and impulse responses---for the development and test splits.
Monologues are all from distinct speakers, which prevents leakage, and each impulse response is associated with a specific background environment recording.
We indeed missed to specify dataset splitting details in Section 2 sufficiently, and we are grateful to the reviewer for prompting us to document them properly.

\textbf{\RFour{} - Future directions:}
Future directions may include designing audio model pre-training to cover different acoustic properties, and extending it to multimodal learning where acoustic properties can be used to align with other modalities such as text. 
It should be noted that real-world implementations require much smaller models, which can be trained from scratch or derived from larger ones.
Running such implementations in real-time will soon be feasible thanks to recent advances like the DEEPSONIC chip~\citep{seitzleveraging}.

\textbf{\RFive{} - Mixing of background recordings:}
Both the 25-channel backgrounds and the speech signals convolved with impulse responses are spatially rendered and then sampled from a virtual mono microphone.

\textbf{\RFive{} - Generation speech signals:}
The three speech signals are independent monologue recordings. 
The dataset is generated by first deciding how many speech signals to generate and then randomly selecting the corresponding number of recordings.
A 30s segment is then mixed in the final audio with a target SNR.

\textbf{\RFive{} - Consistency of the generated scene:}
Consistency foresees that impulse responses used in the same recording correspond to the same environment and more precisely to the background (e.g., outdoor reverberation for outdoor environments). 
Moreover, the loudness of the speakers with respect to each other and the background is roughly matched, as is the vocal effort of the speakers with respect to the SNR of the environment.

\textbf{\RFive{} - Exclusive labels:}
The parameters and the resulting labels are not mutually exclusive unless explicitly stated.
For instance, in some outdoor environments DRR could be very high and RT60 very low.

\textbf{\RFive{} - Estimation number of speakers:}
We chose 1s for segment durations in order to balance between segments being too short, which would not enable the task, and ones that are too long, which would make the task very easy. 
However, since the information provided in the datasets is rather granular, it could be easily aggregated to longer (e.g., 5s) segments.

\textbf{\RFive{} - Classification metrics:}
The Matthews Correlation Coefficient (MCC) is often a more informative statistical score compared to other metrics such as Accuracy and F1.
MCC incorporates a correction for randomness such that random models obtain a score of zero in expectation value.
Moreover, MCC produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset \citep{chicco2020advantages}.
Finally, it extends to the multi-class case without requiring macro-, micro-, or weighted averaging, which is useful to naturally treat environment classification on the same footing as binary tasks.

\textbf{\RFive{} - Relation to hearable applications:}
For instance, reverberation estimation is related to source localization, which is, in turn, useful for beam forming \citep{giurda2018reverberation}, and speaker count may be useful to understand when to switch between intimate conversation and situational awareness \citep{pontoppidanpredicting,zapatareducing}.

\textbf{\RFive{} - Figure layout:}
We extensively experimented with horizontal and vertical layouts for Figure 1 and concluded that horizontal best fits the 4-page format of the publication.

\textbf{\RThree{} - Typo on linear and kNN:}
Corrected, thanks for reading carefully.

%-------------------------------------------------------------------------
% References

{
    \small
    \bibliographystyle{splncs04}
    \bibliography{scholar}
}

\end{document}