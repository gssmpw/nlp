\section{Analysis}
\label{sec:analysis}

The collected traffic metrics and demographics reveal a global community that is not only aware of but also deeply invested in tracking review outcomes and statistics. In this section, we delve into the collected data to evaluate how different review models align with the community’s demand for transparency and how they shape community's behaviors. We first clarify the primary modes of review disclosure, then assess the \emph{community engagement} and validate \emph{community interest}.
\input{fig/google_analytics}

\subsection{Review Disclosure}
\label{subsec:review_disclosure}

Many AI / ML venues have migrated from traditional closed platforms (e.g., Microsoft CMT) to more transparent platforms (e.g., OpenReview). However, as illustrated in Figure~\ref{fig:adoption_of_review_platforms} and Figure~\ref{fig:adoption_of_review_closure}, not all venues that move to OpenReview adopt a fully open process. We categorize venues into three disclosure modes:

\begin{itemize}
    \item \textbf{Fully Open:} All reviews, discussions, and are publicly visible in real-time (e.g., ICLR).
    \item \textbf{Partially Open:} Reviews and discussions become public only after the decision phase concludes (e.g., NeurIPS, CoRL).
    \item \textbf{Fully Closed:} Reviews and discussions remain private indefinitely (e.g., ICML, CVPR).
\end{itemize}

Figure~\ref{fig:adoption_of_review_closure} shows that the \emph{actual level of transparency} has remained mostly unchanged over the past decade, despite migrations to more flexible review platforms. Thus, while platform shifts suggest a trend toward openness, the community has not fully embraced complete real-time visibility.

\subsection{Community Engagement}

Before diving into the effective community interest, we first elaborate and understand who forms the community and how they engaged with open statistics. By analyzing key demographic markers—such as age, gender, and geographic distribution—we can better account for variations in usage patterns and guard against potential biases.
% Before jumping into the community's actual interest. It's necessary to understand the community from the perspective of ages, genders and countries to minimize the bias.


\paragraph{Ages and Genders}
Figure~\ref{fig:active_user_and_engagement_time_by_age_and_gender} details user demographics by age and gender, revealing that the 18--24 age group accounts for the largest number of active users. Notably, younger males not only represent a substantial user base but also have the longest average engagement time (4 minutes 15 seconds), whereas older age brackets show a smaller user base and slightly shorter engagement durations (around 2.5 minutes). For females, engagement time remains relatively consistent across age groups, with a slight increase observed in the 65+ category (3 minutes 8 seconds). These findings suggest that early-career researchers—likely graduate students—are highly active and eager to follow review processes closely, making them potential drivers of future norms favoring transparency and standardization.

\paragraph{Top 10 Countries}
Figure~\ref{fig:active_user_and_engagement_time_by_country} displays both the number of active users and their average engagement time across ten countries. The United States and China lead with the largest user bases (60,648 and 59,269 users, respectively). However, locations with fewer total users, such as Singapore and Australia, exhibit notably high engagement times, exceeding 3 minutes on average. By contrast, the United Kingdom and Germany show comparatively shorter engagement (under 2 minutes), indicating distinct usage patterns. Taken together, these data not only confirm a global appetite for tracking AI / ML conference trends but also highlight the necessity for formal, widespread adoption of open-review principles that can address the diverse needs of researchers worldwide.

\subsection{Community Interests Validation}
\label{subsec:community_interests}

We quantize and validate community's activity and interests via various metrics including site visits, Google Organize Search Rankings and user activity on Openreview platform.

\paragraph{Page Views and CTR}
Google Click-Through Rage (CTR) is the rate when an arbitrary user saw the site page via searching and made a click to it. As shown in Figure \ref{fig:venues_engagement}, the CTR remains consistently high across venues, with values ranging from 66.08\% to 86.49\%. This consistency suggests that researchers are equally curious about review statistics, irrespective of the conference's transparency level.

% is uniform (over 66\%) across all venues, suggesting broad curiosity. However, deeper metrics (page views and session duration) show that transparent conferences foster more sustained user involvement.

Based on this, Figure \ref{fig:venues_engagement} demonstrates a significant disparity in engagement across conferences, largely influenced by their review modes. Notably, except for EMNLP, ACL, and KDD, submission numbers for most venues fall within a similar range of 11,000 to 15,000, providing a comparable baseline for analysis. Conferences adopting Fully Open or Partially Open review processes, such as ICLR and NeurIPS, exhibit substantially higher levels of community interaction compared to their Fully Closed counterparts. For example, ICLR, with its Fully Open review model, leads with 414,096 views, 88,220 active users, and an average engagement time of 3 minutes and 50 second—attracting nearly four times more views and six times more active users than NeurIPS (Partially Open) and far surpassing Fully Closed venues. In contrast, Fully Closed venues such as CVPR and ECCV lag significantly behind, with views under 35,000 and average engagement times of less than 1.5 minutes. This deeper metrics (page views and session duration) show that transparent conferences foster more sustained user involvement. 
% \textcolor{red}{needs the number of submission in the chart}

\input{fig/openreview_iclr_nips}
\paragraph{Organic Search Engine Rankings}
Figure~\ref{fig:google_rank} demonstrates a clear relationship between Google search clicks and the average position of pages for AI / ML-related queries, such as "ICLR 2025 statistics" and "NeurIPS 2024 accepted papers." Similar patterns are observed for Bing search metrics. These pages rank highly in search engine results, driven by algorithms like Google’s PageRank~\cite{page1999pagerank}, which evaluates the quantity and quality of links a page receives from authoritative sources. High natural rankings for community-driven queries indicate that these pages effectively address the informational needs of users.

The prominence of pages related to open reviews and conference statistics underscores the AI / ML community’s strong demand for transparency and accessibility in the research review process. The natural alignment between top-ranked content and community queries reflects a collective preference for systems that prioritize openness and accountability. This trend highlights the importance of the open review model as a mechanism to democratize access to research insights and foster trust in the peer review process, positioning it as a key expectation for the future of scientific publishing in AI / ML.

Additionally, the organic visibility of these resources highlights that many researchers—especially early-career individuals—actively seek centralized and transparent platforms. The consistent alignment between top-ranked content and user engagement demonstrates a grassroots push within the community for more accessible and open reviewing data, rewarding platforms that prioritize transparency with sustained attention and trust.

\paragraph{OpenReview Dynamics}
\input{fig/openreview_replies}
\input{fig/confidence_table}

Over recent years, confidence levels across review processes—fully open (e.g., ICLR), partially open (e.g., NeurIPS), and closed—have remained consistent, averaging between 3.5 and 3.6, as shown in Table~\ref{tab:review_confidence}. However, a closer look at the discussion data in 2024 comparing ICLR and NeurIPS in Figure~\ref{fig:active_confidence_histogram} reveals a noteworthy distinction: ICLR exhibits a slightly lower concentration of high-confidence ratings among accepted papers. This may reflect the nature of open reviewing~\cite{bharti2022confident}, where public visibility fosters cautious, deliberate evaluations and mitigates overconfidence, contributing to a more thoughtful review process. 

Discussion activity further underscores the advantages of fully open reviews. As shown in Figure~\ref{fig:replies_histogram}, ICLR demonstrates a broader distribution of replies than NeurIPS in the same year, with a maximum count of 76 compared to 49 for NeurIPS, and significantly greater variance. The violin plot in Figure~\ref{fig:replies_boxplot} confirms increasing medians and a wider range of replies for ICLR across years, reflecting a more dynamic, iterative review environment. This vibrant engagement highlights the collaborative potential of open reviews, where authors, reviewers, and the community can engage in extended dialogues to refine research. By contrast, fully closed models restrict authors to a one-time rebuttal phase, limiting opportunities for clarification and broader community input.

These findings reinforce the value of fully open reviewing processes in fostering transparency, community engagement, and rigorous scrutiny. By enabling real-time, public discussions, open reviews systematically address ambiguities, encourage constructive feedback, and enhance reproducibility. 
% From a regulatory perspective, the evidence supports the establishment of guidelines or requirements for open review models. 
As the demand for transparency and accountability in academic publishing grows, fully open processes offer a promising pathway to align peer review with these evolving standards.


% Over the past few years, the confidence levels of the review processes for fully open reviews (e.g., ICLR), partially open reviews (e.g., NeurIPS), and even closed reviews have consistently stayed at an average level between 3.5 and 3.6 as summarized in Table~\ref{tab:review_confidence}. However, a closer comparison of the 2024 review confidence statistics between ICLR and NeurIPS in Figure~\ref{fig:active_confidence_histogram} reveals an interesting nuance. ICLR, with its fully open review process, shows a slightly lower concentration of higher confidence levels among accepted papers.
% \input{fig/confidence_table}

% Furthermore, a closer look at actual discussion threads reveals how \emph{fully open} processes amplify community and reviewer engagement. In Figure~\ref{fig:replies_histogram}, we compare the number of replies under submissions for ICLR 2025 (fully open) vs. NeurIPS 2024 (partially open). Note that review process of the two venues all happen in the year of 2024. The distribution of replies is broader for ICLR, with a higher maximum count (76 vs. 49 for NeurIPS) and significantly greater variance. Boxplots in Figure~\ref{fig:replies_boxplot} confirm that ICLR achieves both higher medians and a larger range in reply activity, indicating a more vibrant, iterative review environment. By contrast, fully closed models limit authors to a single rebuttal phase, stifling potential clarifications and broader peer input.

% % NOTE: Tie directly to your thesis: why does it matter for regulation?
% These observations reinforce the argument that fully open reviewing fosters transparency, community collaboration, and deeper scrutiny of research. By enabling real-time, public discussions, open-review venues can systematically address ambiguities, encourage constructive feedback, and enhance reproducibility. From a regulatory standpoint, these empirical results hint that conferences and professional bodies might consider guidelines or even requirements for open processes, particularly if the community is increasingly vocal about the benefits of transparency.

% NOTE: This paragraph ties the data to your thesis about benefits of real-time transparency.
% Comparing fully open vs. partially open processes, our analysis confirm that \emph{when reviews are disclosed in real-time, the community can better learn from the back-and-forth discussions between authors and reviewers}. Figure~\ref{fig:iclr2024_main_rebuttal} demonstrates that transparent dialogues help clarify points of confusion, improve rebuttal quality, and foster collective learning. This openness can be instrumental for early-career researchers, who gain insights into effective communication and argumentation in peer review.


% \textcolor{red}{minor argue}
% it's clear that in the year of 2024, the review confidence

% % \textcolor{red}{showcase the difference between review confidence distribution.}
% \input{fig/confidence_histogram}

% Regardless of disclosure mode, reviewers often report high levels of confidence. Ideally, confidence distributions might approximate a bell curve, but Paper Copilot’s data from ICLR, NeurIPS, and ICML (spanning multiple years) indicate a skew toward higher confidence scores.

% \begin{itemize}
%     \item \textbf{Evidence of Overconfidence:} When examining historical data from fully open conferences like ICLR, we find that mean confidence levels have trended consistently above the midpoint (i.e., above 3 on a typical 1–5 scale).
%     \item \textbf{Impact on Review Quality:} Overly high confidence may mask genuine uncertainties or knowledge gaps. Reviewers may feel pressured to appear well-informed, thus withholding nuanced doubts or questions that could lead to more constructive feedback.
% \end{itemize}

% This phenomenon points to a systemic issue, where inflated confidence levels could undermine the objective evaluation of novel research. **In the context of open vs. closed models**, it raises questions about how transparent reviewing environments might reduce—or exacerbate—this bias.

% \subsection{Community Interests and Engagement}
% \label{subsec:community_interests}

% \subsubsection{Site Page Views and CTR}
% \input{fig/google_analytics}

% Figure~\ref{fig:venues_engagement} demonstrates a clear relationship between review openness and engagement on Paper Copilot:
% \begin{itemize}
%     \item \textbf{Fully Open Venues} (e.g., ICLR) see the highest page views (over 400k), with average engagement times nearing 4 minutes. 
%     \item \textbf{Partially Open Venues} (e.g., NeurIPS) generate moderate activity, reflecting a spike in interest post-decision but less sustained dialogue compared to real-time access models.
%     \item \textbf{Fully Closed Venues} (e.g., CVPR, ECCV) lag with fewer than 35k views and significantly lower average engagement times.
% \end{itemize}
% Interestingly, the Google Click-Through Rate (CTR) is uniformly high (over 66\%) across all venues, suggesting broad curiosity. However, deeper metrics (page views and session duration) show that transparent conferences foster more sustained user involvement.

% \subsubsection{Google Organic Search Rankings}

% After a year of organic growth (no external marketing), Paper Copilot reached top rankings on search engines such as Google and Bing when users searched for venue-specific review information. This organic visibility confirms that many researchers—especially early-career individuals—actively seek centralized and transparent resources, illustrating a grassroots push for more accessible reviewing data.

% High search rankings for terms like “ICLR 2025 statistics” or “NeurIPS 2024 accepted papers” also reflect this demand for accessible information. As Paper Copilot ranks near the top of Google results—similar to how academic citations boost an article’s authority—these impressions confirm a persistent community appetite for open review data. The alignment between top-ranked content and user queries signals that researchers actively seek, and reward with clicks, platforms that prioritize openness and accountability.

% \subsubsection{OpenReview Discussion Dynamics}

% \input{fig/openreview_replies}
% \input{fig/openreview_replies_table}

% A closer look at actual discussion threads reveals how \emph{fully open} processes amplify community and reviewer engagement. In Figure~\ref{fig:openreview_replies}, we compare the number of replies under submissions for ICLR (fully open) vs. NeurIPS (partially open). The distribution of replies is broader for ICLR, with a higher maximum count (76 vs. 49 for NeurIPS) and significantly greater variance. Boxplots confirm that ICLR achieves both higher medians and a larger range in reply activity, indicating a more vibrant, iterative review environment. By contrast, fully closed models limit authors to a single rebuttal phase, stifling potential clarifications and broader peer input.

% % NOTE: Tie directly to your thesis: why does it matter for regulation?
% These observations reinforce the argument that fully open reviewing fosters transparency, community collaboration, and deeper scrutiny of research. By enabling real-time, public discussions, open-review venues can systematically address ambiguities, encourage constructive feedback, and enhance reproducibility. From a regulatory standpoint, these empirical results hint that conferences and professional bodies might consider guidelines or even requirements for open processes, particularly if the community is increasingly vocal about the benefits of transparency (Section~\ref{sec:paper_copilot}).

% \subsection{Summary of Key Findings and Implications}

% \begin{itemize}
%     \item \textbf{Shift Toward OpenReview, But Not Fully Open:} While more conferences now use OpenReview, many still opt for partially or fully closed review settings. 
%     \item \textbf{Inflated Reviewer Confidence:} Across open and closed venues, reviewers often rate their confidence highly, suggesting a need for standardized methods (e.g., calibration exercises or mandatory self-assessment) to ensure more accurate evaluations.
%     \item \textbf{Community Appetite for Transparency:} Engagement metrics—page views, search queries, discussion threads—underscore that researchers actively seek open access to reviews and discussions, particularly for real-time processes.
%     \item \textbf{Potential Policy Ramifications:} Given the consistent evidence that fully open environments spark higher engagement, stronger feedback loops, and learning opportunities, we posit that a regulated framework ensuring greater transparency could better serve the AI / ML community’s collective interests.
% \end{itemize}

% These findings bolster our overarching thesis: the community’s increasing reliance on open data—combined with the measurable benefits of real-time review disclosures—makes a strong case for more standardized and regulated transparency in peer review. In the next section, we propose specific guidelines and policy recommendations aimed at encouraging such openness while maintaining fairness and quality in AI / ML conferences.


%%%%%%%%%%%%%%%%%%%%%% v3 %%%%%%%%%%%%%%%%%%%%%%%
% \section{Analysis}
% \input{fig/change_of_review}

% Overall, AI / ML venues show a trend towards fostering a more transparent community by transitioning their review processes from closed platforms, such as Microsoft CMT, to open platforms like OpenReview, as shown in Figure \ref{fig:adoption_of_review_platforms}. However, our data indicates that these venues are not fully advancing toward complete openness. Despite this, there is significant interest within the community in adopting a fully open review process. We analyze and validate these findings in the following section.

% We track the interests across the community for each review process modes through multiple sources including the traffic on Paper Copilot after the reviews are released to the authors despite fully open or not, the corresponding discussion activity on Reddit and also the discussion metrics on Open Review to reveal the community interests.

% \subsection{Review Disclosure}

% OpenReview offers various modes for revealing the review process, including the following: \textbf{Fully Open}, where all reviews, identities, and discussions are disclosed to the public in real-time; \textbf{Partially Open}, where only the final status, along with reviews and discussions, is disclosed publicly after the paper decision is announced; and \textbf{Fully Closed}, where no reviews or discussions are disclosed publicly. With the increasing shift from Microsoft CMT to the OpenReview platform, it is evident that many venues are adopting a more open review process, as illustrated in Figure \ref{fig:adoption_of_review_platforms}. However, when classifying these conferences by their actual review process, as shown in Figure \ref{fig:adoption_of_review_closure}, it becomes apparent that the level of transparency in the review process has remained largely unchanged across venues over the past decade.

% While the difference between Fully Open and Partially Open review processes may seem to lie solely in whether reviews are disclosed in real-time, the Fully Open review process offers significant additional benefits. It allows the entire community to observe and learn from the most effective approaches to rebuttals and discussions, as well as to identify and clarify points of confusion between authors and reviewers, as illustrated in Figure \ref{fig:iclr2024_main_rebuttal}. This transparency can greatly benefit the community by improving the overall quality of rebuttals and fostering better communication in the review process.

% \subsection{Reviewers' Confidence Level}

% Whether in open or closed review processes, reviewers tend to report relatively high confidence in their reviews. Ideally, if confidence levels were distributed objectively, they would resemble a Gaussian bell curve. However, as illustrated in the following charts based on data from ICLR, NeurIPS, and ICML, this is not the case. This phenomenon is understandable, as reviewers often need to appear confident about their existing knowledge when evaluating papers.

% When examining confidence levels over the past years using Full OpenReview data, the mean confidence level is consistently skewed to the right, indicating an overall tendency toward higher confidence. This trend reflects a potential bias: reviewers may overestimate their familiarity with a topic or feel pressured to present a confident stance to justify their evaluations.

% This observation highlights a systemic issue in the peer review process: the inflated confidence levels could obscure genuine uncertainties in the evaluation process, potentially affecting the quality of reviews and the fairness of the outcomes.

% \subsection{Community Interests}
% \input{fig/google_analytics}
% % \textcolor{red}{how to reason that the community has interesting towards fully open review}
% \paragraph{Site Page Views} Figure \ref{fig:venues_engagement} demonstrates a significant disparity in engagement across conferences, largely influenced by their review modes. Conferences adopting Fully Open or Partially Open review processes, such as ICLR and NeurIPS, exhibit substantially higher levels of community interaction compared to their Fully Closed counterparts. For example, ICLR, with its Fully Open review model, leads with 414,096 views, 88,220 active users, and an average engagement time of 3 minutes and 50 seconds. In contrast, Fully Closed venues such as CVPR and ECCV lag significantly behind, with views under 35,000 and average engagement times of less than 1.5 minutes.

% Interestingly, the Google CTR (Click-Through Rate) remains consistently high across venues, with values ranging from 66.08\% to 86.49\%. This consistency suggests that researchers are equally curious about review statistics, irrespective of the conference's transparency level. However, the stark differences in views, active users, and engagement time highlight that transparency not only attracts initial interest but also sustains deeper engagement.

% These findings indicate that while the community is broadly interested in understanding review statistics, venues like ICLR and NeurIPS amplify this interest by providing accessible and open review processes. Transparency fosters a sense of trust and accountability, encouraging researchers to spend more time engaging with these platforms. Fully Closed conferences, despite their reputation, miss the opportunity to leverage this interest to enhance community interaction.

% This consistency in CTR, combined with the disparity in deeper engagement metrics, underscores the community's readiness for more open review practices, paving the way for other venues to adopt similar transparency models.

% \paragraph{Google Organic Search}
% The chart demonstrates a clear relationship between search clicks and the average position of pages for AI / ML-related queries, such as "ICLR 2025 statistics" and "NeurIPS 2024 accepted papers." These pages rank highly in Google's search results, which are determined by the PageRank algorithm. PageRank operates on principles similar to academic citations, where a page's rank is influenced by the quantity and quality of links it receives from other authoritative sources. A page that naturally ranks high for specific community-driven queries indicates that it effectively meets the informational needs of users.

% In this context, the high engagement with pages related to open review and conference statistics suggests a strong demand from the AI / ML community for transparency and accessibility in the research review process. The natural alignment between top-ranked content and community queries reflects a collective preference for systems that prioritize openness and accountability. This trend highlights the importance of open review as a mechanism to democratize access to research insights and foster trust in the peer review process, positioning it as a key expectation for the future of scientific publishing in AI / ML.

% \paragraph{Open Review}
% \input{fig/openreview_replies}
% \input{fig/openreview_replies_table}
% The Full Open Review Process, as implemented by ICLR, fosters greater engagement and transparency compared to partially open processes like NeurIPS and fully closed reviews. The histogram comparison of ICLR and NeurIPS in 2024 vividly demonstrates this disparity, with ICLR exhibiting a broader distribution of reply counts (bin indices). For example, the maximum number of replies for ICLR reaches 76, while NeurIPS only reaches 49. This broader spread highlights the iterative and inclusive nature of the open review process, which encourages active discussions and deeper scrutiny of submissions.

% The boxplot further reinforces this point, showing that ICLR consistently achieves a higher median and variance in reply counts. In 2024, the variance in reply counts for ICLR (73.77) is nearly three times that of NeurIPS (25.21), reflecting the more dynamic and interactive nature of the Full Open Review Process. The linked table complements these findings, revealing that ICLR not only achieves higher maximum replies but also a greater mean number of replies in both years analyzed. In 2024, ICLR averaged 21.96 replies per submission compared to NeurIPS' 17.86, underscoring the consistent willingness of reviewers and the community to engage in transparent and constructive discussions.

% By contrast, the Full Closed Review Process, typically conducted via a single rebuttal phase, inherently limits engagement. In this model, authors are restricted to addressing reviewer concerns in a one-time response, which curtails opportunities for meaningful dialogue or clarification. The relatively lower engagement in partially open processes like NeurIPS reflects the limitations of closed-review traits. The linked evidence collectively supports the argument that the Full Open Review Process provides a superior framework for promoting transparency, encouraging collaboration, and enhancing the quality and reproducibility of research.

% 1. iclr has longer engagement due to open, more data is available, therefore people, the interests drive that the average is longer than the other.

% 2. Another thing is, the rank of the statis goes to Google Search Top 1 with a fair amount of volume, this indicates that the content of review data is driving the page rank goes to top 1.

% \paragraph{Paper Copilot}
% On paper copilot, we try enable a comprehensive data visualization for venues that are fully open and setup corresponding google forms to collect review scores for venues that are partially open or fully closed.

% \paragraph{Reddit}

% \paragraph{Average Discussion on Open Review}

% \begin{table}
%     \centering
%     \begin{tabular}{ccccccc}
%          Venues&  &  &  &  &  & \\
%          Collected&  &  &  &  &  & \\
%          Total Submissions&  &  &  &  &  & \\
%         Rate& & & & & &\\
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}


% Google Forms Submissions / Total number of submissions chart
% Google Forms # Submission in dates from the data released.

% NeurIPS collected scores vs released scores 