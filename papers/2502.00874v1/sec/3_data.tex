%%%%%%%%%%%%%%%%%%%%%% v3 %%%%%%%%%%%%%%%%%%%%%%%
\input{fig/change_of_review}
\section{Open Statistics: Paper Copilot}
\label{sec:paper_copilot}

Moving toward a more transparent AI / ML community has become a prominent topic at various venues and within the broader research ecosystem. However, the push for more openness and regulation must be guided by concrete evidence of community needs and interests. Despite growing discussions, there is a lack of quantitative evidence reflecting the community’s true interests and practices around open reviewing. To address this gap, we created \textit{Paper Copilot}—a website designed to deliver research-related services and insights for the AI / ML community.

% NOTE: This paragraph focuses on the high-level introduction to Paper Copilot.
In this section, we explain how Paper Copilot collects, analyzes, and presents open statistics on review processes. We also discuss our preliminary observations regarding web traffic and user demographics via Google Analytics, setting the stage for the deeper analyses in Section~\ref{sec:analysis}, where we reinforce our position that standardized, open, and regulated review process are essential to meet the evolving demands of AI / ML researchers.

\subsection{Data Collection Methodology}

% NOTE: Consolidate the "how" of data collection here. Keep definitions of review models very brief or reference them in Section 4.
Paper Copilot provides research-related services by gathering and visualizing key metrics from AI / ML conferences. These venues vary in their reviewing models—ranging from choices that expose all review discussions publicly to those that remain fully private. To accommodate these variations, we employ two main strategies for obtaining data:

\begin{enumerate}
    \item \textbf{Automated Retrieval via Public APIs and Site Bots:} 
    When review data are publicly available (e.g., via the ~\citet{openreview_api} API for ICLR), our custom bots retrieve key metrics such as ratings, confidence levels, and reviewer comments. These bots run on a daily schedule, creating a temporal profile that documents how scores and discussions evolve throughout the review cycle.

    Additionally, we enhance our data collection by deploying bots on the official websites of the respective venues. This approach allows us to include descriptive details such as author identities and affiliations while also enabling us to identify and address inconsistencies across data sources.

    \item \textbf{Community Submissions via Google Forms:} 
    For partially open or closed-review venues where data are not shared publicly during the review process, we invite authors to voluntarily submit anonymized review information via Google Forms embedded on the Paper Copilot website. This community-driven approach underscores researchers’ appetite for transparency even when official policies restrict open peer review data.

\end{enumerate}

Table~\ref{tab:review_collection_methods} summarizes the applicability of each review collection method to conferences based on their review disclosure preferences. In total, we processed 10 years of available data from 24 venues across 9 subfields in the field of AI / ML. Over the past yea, we gathered 3,876 valid responses through Community Submissions.
\input{fig/review_collection_methods}

The collected data from multiple sources is processed using a standardized pipeline to clean, merge, and store it systematically. The resulting datasets are made open-source and are visualized through an interactive frontend. This interface provides insights into review distributions, temporal trends in scores, and basic analytics on authors and affiliations.

\subsection{Traffic and Engagement Overview}
\input{fig/active_user}

% NOTE: Keep this traffic overview high-level, deferring deeper "fully open vs. closed" comparisons to Section 4.
We use~\citet{googleanalytics} to track page views, session durations, referral sources, and basic demographic details (e.g., user location, device type) for Paper Copilot. Also, the collected data is validated via ~\citet{matomo}. \textbf{No personally identifying information is collected, ensuring user privacy.}

\input{fig/traffic_distribution}
Table~\ref{tab:traffic_distribution} illustrates that the majority of users arrive via organic search (e.g., Google, Bing, Baidu, Yahoo Search), suggesting that researchers actively seek information on review processes and publication statistics. Direct traffic and referrals also contribute significantly, indicating that many visitors either bookmark our site or navigate from discussion forums and social media platforms. The dominance of organic search indicates that users are actively seeking open statistics about review processes and decisions. Notably, we also see a growing number of users referred from AI language models, including ChatGPT, Perplexity AI, Google Gemini, and DeepSeek.
% , underscoring the increasing role of AI-driven tools in guiding users to access open data.

\paragraph{User Demographics} 
Since its launch, Paper Copilot has naturally (no ads and marketing) attracted over 6 million impressions and one million site views globally, generating 4 million user-triggered events (e.g., clicks, scrolls) across 177 countries. The geographic distribution of users is visualized in Figure~\ref{fig:active_user_distribution}. These numbers reflect approximately 200,000 active users, with a maximum daily peak of 15,000 unique visitors. Over the past 28 days alone, the platform recorded 50,000 organic clicks from Google Search, highlighting strong and sustained community interest. These metrics underscore the importance of transparency in fostering engagement and demonstrate the community’s enthusiasm for open and accessible systems.

% NOTE: End with a forward-looking statement bridging to Section 4.
% \noindent
% \textbf{Summary and Transition.} 
% Paper Copilot’s traffic metrics and demographics reveal a global community that is not only aware of but also deeply invested in tracking review outcomes and statistics. Having established the data sources and usage patterns, we now turn to a deeper analysis of these data in Section~\ref{sec:analysis}, examining how different review models (from fully open to fully closed) correlate with community engagement, reviewer confidence, and overall transparency. \textcolor{red}{to remove}

% END OF SECTION 3


%%%%%%%%%%%%%%%%%%%%%% v2 %%%%%%%%%%%%%%%%%%%%%%%
% \section{Open Statistics: Paper Copilot}

% Moving toward a more transparent AI / ML community has become a prominent topic at various conferences and within the broader research ecosystem. However, as we argue in this paper, the push for more openness and regulation must be guided by concrete evidence of community needs and interests. Despite growing discussions, there is a lack of quantitative evidence reflecting the community’s true interests and practices around open reviewing. To address this gap, we created \textit{Paper Copilot}—a website designed to deliver research-related services and insights for the AI / ML community.

% In this section, we explain how Paper Copilot collects, analyzes, and presents “open statistics” on review processes. We also describe the global traffic patterns, user demographics, and engagement metrics gathered through Google Analytics, demonstrating the community’s widespread enthusiasm for deeper transparency. By quantifying engagement and user demographics, we reinforce our position that standardized, open, and regulated review process are essential to meet the evolving demands of AI / ML researchers.

% \subsection{Open Statistics}

% Over the past two years, we have sequentially launched multiple data-collection and visualization services targeting conferences with different reviewing models—ranging from fully open-review (e.g., ICLR) to partially open (e.g., NeurIPS) and closed-review venues. These services include:
% \begin{itemize}
%     \item \textbf{Review Collection} (e.g., ratings, reviewer confidence, novelty criteria),
%     \item \textbf{Review Visualization} (e.g., time-series graphs of scores, distribution histograms),
%     \item \textbf{Author and Affiliation Analytics} (e.g., most-published authors, institution rankings).
% \end{itemize}

% \paragraph{Collection Methods}
% We employ two primary methods for gathering review data:
% \begin{enumerate}
%     \item \textbf{Automated Retrieval (Scraping Bots and APIs):} 
%     When review data are publicly available (e.g., via the OpenReview API for ICLR), our custom bots retrieve key metrics such as ratings, confidence levels, and reviewer comments. These bots run on a daily schedule, creating a temporal profile that documents how scores and discussions evolve throughout the review cycle.
    
%     \item \textbf{Community Submissions (Google Forms):} 
%     For partially open or closed-review venues where data are not shared publicly during the review process, we invite authors and reviewers to voluntarily submit anonymized review information via Google Forms embedded on the Paper Copilot website. This community-driven approach underscores researchers’ appetite for transparency even when official policies restrict open data.
% \end{enumerate}

% \input{fig/review_collection_methods}
% Table~\ref{tab:review_collection_methods} summarizes how each method applies to different venues based on their openness.  

% After a year of organic growth (no external marketing), Paper Copilot reached top rankings on search engines such as Google and Bing when users searched for venue-specific review information. This organic visibility confirms that many researchers—especially early-career individuals—actively seek centralized and transparent resources, illustrating a grassroots push for more accessible reviewing data. Building on that momentum, our Google Forms approach has yielded 3,876 valid responses over the past year, further validating the community’s desire for a more transparent insights.

% \paragraph{Analysis and Visualization}
% To provide meaningful insights, we developed a data processing pipeline that merges and cleans these datasets before generating various interactive charts and summaries. When venues offer fully open reviews, we visualize score distributions over time—covering the period from initial review release to final acceptance decisions. For partially open or closed venues, we rely more heavily on user-submitted data, presenting aggregated snapshots that highlight trends in scores, reviewer comments, and discussion topics.

% Alongside review scores, Paper Copilot also analyzes authors and their affiliations, reporting:
% \begin{itemize}
%     \item \textbf{Author Rankings:} Based on the number of accepted papers at a given venue,
%     \item \textbf{Institutional Rankings:} Based on the number of accepted papers, 
%     \item \textbf{Author Roles:} Current positions of corresponding authors (e.g., PhD student, postdoc, faculty).
% \end{itemize}
% These statistics are available on the website to promote broader transparency, providing stakeholders with a clearer view of how research and publishing dynamics function in the AI / ML community. By making such information publicly accessible, we underline the urgent need for standardized practices that ensure fair and transparent reviewing.

% \subsection{Traffic}

% We use Google Analytics to monitor Paper Copilot’s performance and user engagement. This includes tracking page views, session durations, and traffic sources, as well as anonymized attributes like user location and device type (we do not collect or store personally identifying information).

% \input{fig/traffic_distribution}
% \input{fig/active_user}
% Table~\ref{tab:traffic_distribution} shows the distribution of traffic sources:
% \begin{itemize}
%     \item \textbf{Organic Search:} Visitors who reach Paper Copilot through search engines like Google or Bing,
%     \item \textbf{Direct Traffic:} Users who type our URL directly or use a saved bookmark,
%     \item \textbf{Referral Traffic:} Visitors arriving via links on other websites,
%     \item \textbf{Organic Social:} Traffic originating from social media platforms without paid advertisements.
% \end{itemize}


% The dominance of organic search indicates that users are actively seeking information about review processes and decisions, reinforcing our central claim that transparency is not just an abstract value but a concrete need within the community.

% \subsection{Engaged Researcher Cohort}

% Since launching, Paper Copilot has garnered over 6 million impressions and 1 million site views worldwide, spanning 177 countries. These global numbers correspond to approximately 200,000 active users, with a maximum daily peak of 15,000 unique visitors. In the last 28 days alone, we recorded 50,000 organic clicks from Google Search, demonstrating sustained community interest and underscoring the potential impact that regulated, open review systems can achieve at a global scale.

% \paragraph{Ages and Genders}
% Figure~\ref{fig:active_user_and_engagement_time_by_age_and_gender} details user demographics by age and gender, revealing that the 18--24 age group accounts for the largest number of active users. Notably, younger males not only represent a substantial user base but also have the longest average engagement time (4 minutes 15 seconds), whereas older age brackets show a smaller user base and slightly shorter engagement durations (around 2.5 minutes). For females, engagement time remains relatively consistent across age groups, with a slight increase observed in the 65+ category (3 minutes 8 seconds). These findings suggest that early-career researchers—likely graduate students—are highly active and eager to follow review processes closely, making them potential drivers of future norms favoring transparency and standardization.

% \paragraph{Top 10 Countries}
% Figure~\ref{fig:active_user_and_engagement_time_by_country} displays both the number of active users and their average engagement time across ten countries. The United States and China lead with the largest user bases (60,648 and 59,269 users, respectively). However, locations with fewer total users, such as Singapore and Australia, exhibit notably high engagement times, exceeding 3 minutes on average. By contrast, the United Kingdom and Germany show comparatively shorter engagement (under 2 minutes), indicating distinct usage patterns. Taken together, these data not only confirm a global appetite for tracking AI / ML conference trends but also highlight the necessity for formal, widespread adoption of open-review principles that can address the diverse needs of researchers worldwide.

% \paragraph{Overall} 
% % Our data confirm a wide, global interest in obtaining transparent insights about AI / ML paper reviews, indicating that the research community—particularly younger scholars—desires more openness throughout the submission and review lifecycle.
% Collectively, the data from Paper Copilot’s open statistics, traffic analysis, and user demographics reveal a broad and growing constituency invested in transparent reviewing practices. These observations—particularly the enthusiastic engagement among younger researchers—reinforce our position that AI / ML conferences could benefit from adopting clear, standardized, and regulated review processes. The success of Paper Copilot serves as a microcosm of the community’s expectations, indicating that any efforts to formalize transparency and fairness in peer review are likely to find strong grassroots support. \textcolor{red}{overclaim? togo analytics?}


%%%%%%%%%%%%%%%%%%%%%% v1 %%%%%%%%%%%%%%%%%%%%%%%
% \section{Open Statistics: Paper Copilot}

% Moving towards a more transparent community is actively discussed at various venues and throughout the broader AI / ML community. However, there are currently no quantitative measurements to gauge the community's true interests. To better identify the actual needs and desires of the entire AI / ML community, we launched Paper Copilot — a website project designed to offer research-related services and insights tailored to the AI / ML domain. 

% In this section, we detail how to conduct the open statistics we achieve and showcase the global traffic and user cohoart we engaged with the validation via Google Analytics.

% \subsection{Open Statistics}

% Specifically, over the past two years, we have consecutively launched the open statistics services for both fully open review, partially open and close review venues including: review score collection, review visualization, and author and affiliations analysis.

% \paragraph{Collection Methods} 

% % split by open, partially open and close
% We implement two strategies for gathering reviews using scrawling bots via API and Google Forms hosted on the website that's publicly avaiable everywhere. With these two strategy, we apply them easily to the venues that have different review choise and resulting the following Table~\ref{tab:review_collection_methods} as our implementation.

% \input{fig/review_collection_methods}

% Specificially, when the review data is publicly available, we build a bot that comptable with multiple venues to fetch reviews in a convenienet manner collecting metrics incluing rating, levels, novelty, etc., which highly related to the venues. The bot fetches data via API in a daily manner to build a temporal profile for the post visualization. For Fully open venues, e.g. ICLR, this temporal provile will last from the first day of review released all the way to the paper acceptance decision is announcemented publicly. For those partially open venues, e.g., NeurIPS, this temporal porfile usually downgrades to static since no review data is publicly avaialble during the review process.

% After one year of operation without marketing, the site achieved top rankings on search engines like Google and Bing for searches involving keywords related to these AI / ML venues as illustrated in Table\ref{}. This achievement highlights the community's growing interest in tracking AI trends through such tools. 

% We successfully attracted users organically, ensuring that the website's audience consisted of individuals specifically interested in the respective venues. Leveraging this predictable user activity and to further reveal the community interest when review data is not avaiable during the review. We lunch the Google Forms to publicly sample reviews for each venues. In the past year, we collected in total of 3876 Google Form responses across the entire community.

% % Two years ago, we began building trust and engaging researchers by offering visualizations of review score distributions, which were made publicly available for venues such as ICLR and NeurIPS via the OpenReview API \cite{openreviewAPI}. After a year of organic growth with no marketing efforts, the site achieved top rankings on search engines like Google and Bing for searches involving keywords related to these AI / ML venues as illustrated in Table\ref{}. This achievement highlights the community's growing interest in tracking AI trends through such tools.

% \paragraph{Analysis and Visualization}

% % several charts, and need to highlight that when open review, we can also enable the differences across the time and visualize the rebuttal etc.

% With the collected data via various methods, we similar design a working pipelien to clean and merge the data for a easy to use visualization and analysis tool publicly avaiable across the community. This includes various interative charts including histogram, boxplots, and scatter charts, parallel coordinates of the current collected data, visual comparison of the changes before and after discussion. To simplify the analysis, we also grouping papers by it's acceptance status and their primary areas for a more comprehensive and downaward usage.

% Consecutively, we introduced the analysis and visualization of authors and affiliations across various venues, incorporating the following metrics: the ranking of authors by the number of accepted papers per venue, the ranking of affiliations by the number of accepted papers, the current positions of corresponding authors, and more.
% % \input{fig/iclr2024_main_rebuttal}



% \subsection{Traffic}
% \input{fig/traffic_distribution}
% \input{fig/active_user}
% We track website performance using Google Analytics, which enables the platform to monitor user interactions such as page views, session durations, and referral sources, while also analyzing user attributes like location and device type without explecitly collect the user attributes.

% As showcased in Table \ref{tab:traffic_distribution}, most users access the site through organic search, which refers to visitors finding the website through search engines by using relevant keywords. Direct traffic represents users who access the website by typing the URL directly into their browser or through bookmarks. Referral traffic includes visitors who are directed to the site from other websites, while organic social captures traffic from social media platforms without paid advertisements.

% This breakdown demonstrates that the majority of users engage with the website by searching for relevant keywords via Google Search. 

% \subsection{Engaged Researcher Cohort}

% By offering these visualizations, we have successfully garnered approximately 6 million impressions and 1 million site views worldwide, spanning 177 countries, as illustrated in Figure \ref{fig:active_user_distribution}, and engaging 200,000 active users with maximum daily user activity of 15k. To further describe this user cohort, we highlight the average engagement time across different age groups and countries. \textcolor{red}{50k within 28days}


% \paragraph{Ages and Genders}
% In Figure \ref{fig:active_user_and_engagement_time_by_age_and_gender} reveals notable trends in active user counts and average engagement time across age groups and genders. Younger age groups, particularly 18-24, show the highest active user counts for both males and females, with a significant drop as age increases. Male active users slightly outnumber females in the youngest group but see a sharper decline in older demographics. Interestingly, engagement time does not correlate directly with user count; the youngest males, despite having the highest user numbers, also have the highest engagement time (4m 15s), whereas engagement among older males remains relatively stable around 2.5 minutes. For females, engagement time is more consistent across age groups, peaking slightly in the oldest group (65+, 3m 8s). This suggests younger audiences are more numerous and engaged, while older demographics, though fewer, maintain steady engagement levels.

% \paragraph{Top Countries}
% Figure \ref{fig:active_user_and_engagement_time_by_country} illustrates the active user count and average engagement time per active user across ten countries. The United States and China lead significantly in active users, with 60,648 and 59,269 users, respectively, reflecting their large digital user bases. Despite having fewer users, Singapore and Australia demonstrate the highest average engagement times, exceeding 3 minutes, suggesting higher user retention or engagement quality. South Korea and Japan also show strong engagement, both at 2 minutes 57 seconds. In contrast, the United Kingdom and Germany report shorter engagement times, under 2 minutes, potentially indicating different usage patterns or preferences. This data highlights variations in user behavior, suggesting that countries with smaller user bases might focus on deeper engagement strategies.

% % \paragraph{Correlation?}