\section{Discussion: Close or Open}
In this section, we examine three key challenges affecting the integrity of the fully closed peer review process and then propose how moving toward more open or partially open models could address these issues effectively.


\subsection{Problems in Close Review}

\paragraph{Challenges for Younger Reviewers}
Demographic data indicate that a substantial portion of the AI research community now consists of younger individuals aged 18–24. As the field grows exponentially and the number of submissions soars, venues often face a shortage of qualified reviewers. In response, some venues~\cite{cvpr2025changes} require each submitting author to serve as a reviewer in order to manage the massive influx of papers.

While this policy helps alleviate reviewer shortages, it also compels younger, less-experienced researchers to evaluate work at the forefront of the field. Younger researchers are undoubtedly talented and growing in number, their limited familiarity with rigorous peer-review standards, combined with the pressure of large submission volumes, can lead to uneven or suboptimal feedback. This dynamic risks diluting the overall quality of the peer-review process.

A growing concern within the community form Paper Copilot highlights this issue: many authors report that reviewers struggle to fully understand the nuances of their submissions. While such claims are currently anecdotal and not yet quantifiable, future studies could analyze this trend systematically. As these reports continue to rise, they signal a potential systemic challenge that, if left unaddressed, could impose significant additional burdens on program committees, requiring extensive resources to mediate disputes and resolve misunderstandings stemming from insufficiently experienced reviewers.

% Such arguemetn becomes stronger as a lot of authors argue that the reviewer have difficulty understanding their submitted work. This is subjective, but as the number of such argues goes up. This becomes an noticeable systematic argument and could cost a lot of extra resources for PCs to resolve such problems.

% \textcolor{red}{potential solution? open review with feedbacks can further reinforce the overall system. a lot of authors argue that the reviewer have difficulty understanding their submitted work. This is subjective, but as the number of such argues goes up. This becomes an noticeable systematic argument and could cost a lot of extra resources for PCs to resolve such problems. }

\paragraph{Ethical Concerns and AI Usage in Closed Review}
Whether closed or open, reviewers typically perform their duties with minimal oversight and must balance these tasks alongside their own research. The rise of large language models (LLMs) adds further complexity~\cite{kuznetsov2024can, seghier2024ai, zhang2022investigating}. Although LLMs can assist in revising or evaluating manuscripts, their unregulated use in a closed review context raises concerns about consistency and accountability.

In response, some venues have introduced policies to regulate LLM usage. However, enforcement remains challenging in a closed review environment, where the reviewing process—and any potential misuse—occurs largely out of public view. Moreover, these issues disproportionately affect younger reviewers, who may lack both the resources and the confidence to navigate potential ethical dilemmas. Overreliance on LLMs risks homogenizing feedback, thus reducing the diversity of perspectives that is vital for thorough peer review.

\paragraph{Noticed Inconsistencies in Acceptance Records}

A notable concern emerging from closed-review venues is the discrepancy in author information between official conference records and final published versions. For instance, in 2024, some authors changed their names after paper acceptance, creating mismatches that are difficult to detect in a closed setting. While we refrain from revealing specific names or details to protect the authors’ identities, these inconsistencies can be traced through publicly available statistics. Such incidents underline gaps in accountability and underscore the need for more robust regulatory mechanisms.

By contrast, open review processes naturally invite broader oversight, making it easier to spot and address potential irregularities. Publicly visible reviews and commentary foster collective accountability and discourage misconduct. Taken together, these observations highlight the urgent need for a more transparent and well-regulated review framework in the AI / ML community to maintain trust, ensure high-quality feedback, and safeguard research integrity.

% \paragraph{Reviewers' Confidence Levels on different review mode} 
% % \input{fig/confidence_histogram}
% \input{fig/confidence_table}
% Over the past few years, the confidence levels of the review processes for fully open reviews (e.g., ICLR), partially open reviews (e.g., NeurIPS), and even closed reviews have consistently stayed at an average level between 3.5 and 3.6 as summarized in Table~\ref{tab:review_confidence}. This consistency suggests that, despite differences in the transparency of the review process, reviewers across venues generally exhibit similar confidence in their assessments.

% However, a closer comparison of the 2024 review confidence statistics between ICLR and NeurIPS in Figure~\ref{fig:active_confidence_histogram} reveals an interesting nuance. ICLR, with its fully open review process, shows a slightly lower concentration of higher confidence levels among accepted papers. This could indicates the potential regulation that Fully Open review brings to the system.

\subsection{Towards Open}

The challenges described in prior sections underscore the urgent need for a more transparent and accountable review framework—one that supports the influx of younger reviewers, regulates AI usage, and preserves the integrity of scholarly discourse. Although expanding participation can bring fresh perspectives, it also risks undermining quality if newer reviewers lack structured mentorship and formal training. At the same time, ethical concerns regarding AI-assisted reviewing—such as homogenized feedback—illustrate the fragility of closed systems, where limited oversight makes it difficult to enforce standards, detect biases, or reconcile inconsistencies in authorship records.

Moving toward open or partially open review processes offers a pragmatic solution to these issues. By making reviews publicly visible, community members can collectively scrutinize and address potential problems, from name-change discrepancies to excessive reliance on large language models. Such transparency fosters fairer evaluations, encourages ethical conduct, and cultivates a more collaborative environment for all participants. As AI research continues to evolve at a rapid pace, embracing open review mechanisms can help maintain a high standard of scholarly rigor while supporting the long-term credibility and vitality of the research community.

\paragraph{User Studies}
To assess the research community’s stance on open or partially open review processes, we conducted an interest survey prominently featured on Paper Copolit’s front page. So far, the survey received over 228 responses, reflecting swift and enthusiastic engagement. Respondents spanned more than 20 distinct subfields—ranging from traditional AI / ML and robotics to medical informatics—covering a total of more than 50 major research venues.

When asked whether review scores should be publicly accessible at fully closed-review conferences such as CVPR 2025, 57\% of respondents indicated they would be willing to share their scores with the community anonymously. This willingness points to growing support for more transparent peer-review practices. Equally notable was the speed with which respondents engaged, suggesting that the research community is eager to explore open or partially open review models that can address the challenges documented in this paper. 


%%%%%%%%%%%%%%%%%%%%%% v0 %%%%%%%%%%%%%%%%%%%%%%%
% \section{Discussion}

% \subsection{Problems in Close Review}

% \paragraph{Challenges for Younger Reviewers}
% Demographic data indicate that a substantial portion of the AI research community now consists of younger individuals aged 18–24. As the field grows exponentially and the number of submissions soars, venues often face a shortage of qualified reviewers. In response, some conferences require each submitting author to serve as a reviewer in order to manage the massive influx of papers.

% While this policy helps alleviate reviewer shortages, it also compels younger, less-experienced researchers to evaluate work at the forefront of the field. Younger researchers are undoubtedly talented and growing in number, their limited familiarity with rigorous peer-review standards, combined with the pressure of large submission volumes, can lead to uneven or suboptimal feedback. This dynamic risks diluting the overall quality of the peer-review process and underscores the urgent need for structured mentorship and training programs.


% % The user demographics indicates that a significant portion of the AI research community consists of younger individuals aged 18–24. As the AI field rapidly expands, the availability of experienced reviewers is struggling to keep pace. Most senior reviewers were trained before the exponential growth in AI submissions, leaving a gap in the supply of qualified reviewers.

% % While younger researchers are undoubtedly talented and growing in number, their limited experience and knowledge can pose challenges in maintaining review quality. To address this, some venues require authors to contribute as reviewers. However, this approach raises concerns: an author who may lack sufficient expertise to publish an accepted paper might also lack the capacity to provide high-quality peer reviews. This dynamic creates a bifurcated review system where the quality of reviews becomes uneven and potentially less reliable.

% % As the average age of the community declines and the demand for reviewers increases, it becomes critical to ensure adequate mentorship and training for younger reviewers to maintain the integrity of the process.

% % As the community grows, the short of qualified reviewers will definitly be a bottleneck for the current community, since usually the qualified reviewers where trained before the fast grown of AI submissions. There are absotely talented yonger generation is gorwing more and more. However, the growing speed is not matching.

% % Therefore, some venues introduce that the authors need to take duty of reviewering papers. However, when the age goes down, this situation becomes challenging, since their knowledge and reviewing experience is limited due to the age upper bounds.

% % This cause the reviews bipartite.

% % as the age of the entire community goes down, and authors need to be reviewer, this means that an author don't have the capability of being a accepted author can also peer review others work.

% \paragraph{Ethical Concerns and AI Usage in Closed Review}
% In closed review processes, reviewers typically perform their duties with minimal oversight and must balance these tasks alongside their own research. The surge of large language models (LLMs) further complicates matters. Although LLMs can assist in revising or evaluating manuscripts, their unregulated use in a closed review context raises concerns about consistency and accountability.

% In response, some venues have introduced policies to regulate LLM usage. However, enforcement remains challenging in a closed review environment, where the reviewing process—and any potential misuse—occurs largely out of public view. These issues disproportionately affect younger reviewers, who may lack both the resources and the confidence to navigate potential ethical dilemmas. Overreliance on LLMs risks homogenizing feedback, thus reducing the diversity of perspectives that is vital for thorough peer review.



% % The rapid growth of AI / ML submissions has created significant pressure on reviewers, who often perform their duties voluntarily and with minimal oversight. Many reviewers must balance their reviewing responsibilities with their own research, leading to potential conflicts of interest and uneven attention to detail.

% % The advent of large language models (LLMs) has introduced another layer of complexity. While LLMs can assist reviewers, their limited regulared use in a closed review process raises concerns about consistency and accountability. Some venues have introduced policies regarding LLM usage, but these policies are difficult to enforce in a closed review setting. 

% % This lack of transparency disproportionately affects younger reviewers, who may lack the tools or resources to navigate these challenges effectively. Unchecked use of LLMs could homogenize reviews and diminish the nuanced insights that come from diverse perspectives, ultimately harming the community.


% % Based on the fact of the rapid growing AI field, most of the reviewers are responsible for their duty for free and with very limited regulation. It's really depends on the ethical level of the reveiwers. The common knowledge in the field is that the majority reviewers are also in charge of their own work, making it's really hard to balance the reviewing process and their own works. 

% % As LLM becomes an important and convenient tool, it's really hard for the reviewers to balance the level of using LLM during the review. There are majority of venues announceed their LLM policy since the LLM introduce the homogeneous across various reviewers. Since all review process is closed, there's no regulation to limit the usage of the LLM leaving only the yonger talents to suffer the consequences. This is absoltely harmful to the entire community.

% % The rapid change of the number of the submissions bring pressure to all AI / ML venues, and such imbalance exists in almost all venues, which is widely discussed across the community. However, expanding the number of chairs can only make the regulation slighly better but not a fix since the number of the submissions in the upcoming year is not predictable accurately.

% % usage of ai is not able to be find out due to close review.

% \paragraph{Noticed Inconsistencies in Acceptance Records}

% A notable concern emerging from closed-review venues is the discrepancy in author information between official conference records and final published versions. For instance, in 2024, some authors changed their names after paper acceptance, creating mismatches that are difficult to detect in a closed setting. While we refrain from revealing specific names or details to protect the authors’ identities, these inconsistencies can be traced through publicly available statistics. Such incidents underline gaps in accountability and underscore the need for more robust regulatory mechanisms.

% By contrast, open review processes naturally invite broader oversight, making it easier to spot and address potential irregularities. Publicly visible reviews and commentary foster collective accountability and discourage misconduct. Taken together, these observations highlight the urgent need for a more transparent and well-regulated review framework in the AI / ML community to maintain trust, ensure high-quality feedback, and safeguard research integrity.

% % An intriguing observation from closed review venues is the discrepancy between the list of authors on official paper records and their later appearances in the published versions. For example, in 2024, some authors changed their names after their papers were accepted at a closed venue. While the specifics cannot be disclosed due to privacy concerns, this raises questions about the integrity and transparency of the closed review process.

% % In open review settings, such issues would be more easily identified and addressed, as the process is publicly visible. The occurrence of such anomalies in closed venues underscores the need for stronger regulations and greater accountability in the peer review process.

% % Another interesting fact is that, when we conduct the statistics from the list of actually accounce paperlist and those records on the venue websiste. We find that the number of authors are actually not matched. After the closer comparison, we find that for closed venues, some authors even change their names after the paper's acceptance. This happended for a closed venues in the year of 2024. Due to the privacy issue, it's not approprate to provide more details in this section. 

% % But this is an interesting fact that why it's found in the close reviewd conferences. And if this could happend to also the Fully Opend venues. This will leave as a future topic to discuss with.

% % ECCV 2024

% \subsection{Towards Open}

% The challenges described in prior sections underscore the urgent need for a more transparent and accountable review framework—one that supports the influx of younger reviewers, regulates AI usage, and preserves the integrity of scholarly discourse. Although expanding participation can bring fresh perspectives, it also risks undermining quality if newer reviewers lack structured mentorship and formal training. At the same time, ethical concerns regarding AI-assisted reviewing—such as homogenized feedback—illustrate the fragility of closed systems, where limited oversight makes it difficult to enforce standards, detect biases, or reconcile inconsistencies in authorship records.

% Moving toward open or partially open review processes offers a pragmatic solution to these issues. By making reviews publicly visible, community members can collectively scrutinize and address potential problems, from name-change discrepancies to excessive reliance on large language models. Such transparency fosters fairer evaluations, encourages ethical conduct, and cultivates a more collaborative environment for all participants. As AI research continues to evolve at a rapid pace, embracing open review mechanisms can help maintain a high standard of scholarly rigor while supporting the long-term credibility and vitality of the research community.

% % The challenges highlighted in the current closed review system emphasize the need for a more transparent and accountable framework to address critical gaps in mentorship, ethical usage of AI tools, and the integrity of the peer review process. The influx of younger reviewers, while indicative of a growing and vibrant research community, also reveals a pressing need for robust training and support systems to ensure review quality. Without adequate oversight and structured guidance, the burden placed on these less experienced reviewers could undermine the fairness and reliability of peer reviews, creating uneven standards across submissions.

% % Furthermore, the ethical concerns surrounding AI-assisted reviewing, combined with limited transparency in author attribution and review practices, expose vulnerabilities in closed systems. Open review processes could serve as a countermeasure, fostering greater trust and accountability by allowing community oversight and collaboration. By embracing open review models, the research community has an opportunity to mitigate these systemic issues, promoting fairness, equity, and innovation in the peer review process.

% \paragraph{User Studies}
% To assess the research community’s stance on open or partially open review processes, we conducted an interest survey prominently featured on our platform’s front page. So far, the survey received over 228 responses, reflecting swift and enthusiastic engagement. Respondents spanned more than 20 distinct subfields—ranging from traditional AI / ML and robotics to medical informatics—covering a total of more than 50 major research venues.

% When asked whether review scores should be publicly accessible at fully closed-review conferences such as CVPR 2025, 57\% of respondents indicated they would be willing to share their scores with the community anonymously. This willingness points to growing support for more transparent peer-review practices. Equally notable was the speed with which respondents engaged, suggesting that the research community is eager to explore open or partially open review models that can address the challenges documented in this paper. 
% % Table~\ref{tab:fields_venues} provides additional details on the diverse array of fields and venues represented, further underscoring the broad applicability of these findings.
