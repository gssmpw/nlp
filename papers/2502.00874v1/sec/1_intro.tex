%%%%%%%%%%%%%%%%%%%%%% v3 %%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\input{fig/user_distribution}

The exponential growth in submissions to top-tier Artificial Intelligence and Machine Learning (AI / ML) conferences has created unprecedented challenges for the academic review process. With submission numbers exceeding 10,000 for AI / ML venues \cite{weissburg2024position}, traditional review practices are under immense pressure to maintain fairness, efficiency, and quality. In response, many conferences have adopted open review platforms, as illustrated in Figure~\ref{fig:adoption_of_review_platforms}. However, the implementation of open peer reviews varies significantly, reflecting diverse decisions by conference organizers. These models—\textbf{fully open}, \textbf{partially open}, and \textbf{closed}—share a common double-blind review framework, where neither authors nor reviewers know each other’s identity during the review phase. The key differences lie in the timing and extent of public disclosure of reviews and discussions. Fully open reviews~\cite{ross2017open} make all content public from the start, partially open reviews disclose reviews after final decisions, and closed reviews do not disclose reviews at all. These differing approaches have sparked debates about their implications for transparency, accountability, and community engagement.

Fully open reviews promote transparency by making review content and discussions accessible to the broader community~\cite{tran2021an, cortes2021inconsistency, Lawrence2022NeurIPSExperiment, beygelzimer2023has, wang2023have}, fostering collaboration and accountability. However, even with double-blind protocols in place, the public nature of fully open reviews can introduce subtle biases or discourage candid feedback from reviewers wary of visibility or potential backlash. In contrast, partially open and closed reviews provide a more private environment, encouraging frank critique but limiting transparency and broader engagement. These trade-offs raise critical questions about the best practices for academic review processes in rapidly evolving fields like AI and ML, where robust systems are vital to fostering innovation and collaboration.

To explore these dynamics, we publicly launched \textbf{Paper Copilot} two years ago—a platform designed to aggregate and analyze data from AI / ML conferences. By sourcing information from official conference websites, review platforms, and community inputs, Paper Copilot tracks engagement throughout the review and decision-making process. Figure~\ref{fig:active_user_distribution} presents a global user distribution map derived from~\citet{googleanalytics}, showcasing the diverse geographic reach of Paper Copilot users. This global participation underscores the community’s interest in transparency and collaboration within the review process. Through its data aggregation and analysis capabilities, we highlight trends and patterns in review practices, providing valuable insights into how transparency impacts engagement in AI / ML reviewing process.

In this work, we contribute to the ongoing discussion on review transparency in the AI / ML community by: \begin{itemize} \item Providing open statistics via Paper Copilot, including visualizations of review score distributions, review timelines, and author/affiliation analyses across conferences over the past 3–5 years. \item Presenting quantitative evidence of the community's increasing interest in review transparency, based on two years of engagement data. \item Critically examining the strengths and weaknesses of various review models while advocating for the adoption of a more transparent, open, and regulated peer review process. \end{itemize}

\textbf{Based on our findings, this position paper advocates for a more transparent, open, and regulated peer review process to enhance community involvement, foster collaboration, and drive progress in the field.}

%%%%%%%%%%%%%%%%%%%%%% v2 %%%%%%%%%%%%%%%%%%%%%%%
% \section{Introduction}
% \input{fig/user_distribution}

% The exponential growth of submissions to top-tier AI and ML conferences has placed unprecedented pressure on the academic review process. With submission numbers exceeding 10,000 for some conferences \cite{weissburg2024position}, traditional review practices are struggling to ensure fairness, efficiency, and quality. To address these challenges, many conferences have begun transitioning to open review platforms, as shown in Figure~\ref{fig:adoption_of_review_platforms}. However, the implementation of open peer reviews varies significantly, reflecting different choices made by conference organizers. These choices—\textbf{fully open}, \textbf{partially open}, and \textbf{closed}—all operate under a double-blind review system, ensuring that neither authors nor reviewers know each other’s identity throughout the review phase. The primary differences lie in whether or not reviews and discussions are disclosed to the public, and if so, when this disclosure occurs. Fully open reviews make all reviews and discussions public from the outset, partially open reviews disclose them only after final decisions, and closed reviews do not disclose them at all. These varying choices have sparked debates about transparency, accountability, and their impact on community engagement.

% Fully open reviews can promote transparency by making the content of the reviews and discussions publicly accessible, thereby fostering broader community participation. However, even with authors’ and reviewers’ identities concealed through double-blind protocols, the public nature of these reviews may introduce subtle biases or discourage candid feedback from reviewers concerned about potential backlash or visibility. By contrast, partially open or closed reviews restrict the public availability of review content, offering a more private setting that can encourage unfettered critique. The trade-off, however, is reduced transparency and less opportunity for community-wide engagement. Understanding the implications of these different choices is essential in fast-evolving fields like AI and ML, where robust and adaptive review processes are critical to fostering innovation.

% To explore these dynamics, we publicly launched \textbf{Paper Copilot} two years ago — a website designed to aggregate and analyze data from AI / ML conferences. By sourcing data from official conference sites,  review platforms, and community, we track community engagement throughout the review and decision-making process. In addition, Figure~\ref{fig:active_user_distribution} shows a global user distribution map derived from Google Analytics~\cite{googleanalytics}, highlighting the diverse geographical regions from which users access Paper Copilot. This analysis highlights trends and patterns in review practices, shedding light on the community’s preferences and the impact of transparency on collaboration and innovation. Based on these findings, this position paper advocates for more transparent, open, and regulated review practices to enhance community involvement and propel advancements in AI / ML research.

% \textbf{Contributions:} To this end, we provide the following contributions to the discussion on review transparency in the AI / ML community:
% \begin{itemize}
%     \item We provide open statistics via Paper Copilot, including visualizations of review score distributions, review collection data, and author/affiliation analysis across conferences over the past 3–5 years.
%     \item We present quantitative validation of the community's growing interest in review transparency, based on engagement data collected over two years.
%     \item we critically analyze the advantages and disadvantages of different review choices and advocate for adopting more transparent, open, and regulated review practices.
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%% v1 %%%%%%%%%%%%%%%%%%%%%%%
% \section{Introduction}
% \input{fig/user_distribution}

% The rapid growth of submissions to top-tier AI and ML conferences has transformed the landscape of academic publishing. With submission numbers exceeding 10,000 for some conferences\cite{weissburg2024position}, the review process has become a critical bottleneck in ensuring the quality and fairness of accepted work. To address the challenges associated with traditional closed review systems, many conferences have begun adopting open review platforms, as showcased in Figure~\ref{fig:adoption_of_review_platforms}. However, the implementation of the open review process varies significantly between venues—ranging from \textbf{fully open} procedure where reviews and discussions between the reviewers and authors are public from the outset to \textbf{partially open} systems where reviews/discussions are disclosed only after final decisions while keeping the discussion phase closed. Meanwhile, some venues maintain \textbf{fully close}, using close/open review platforms without making reviews publicly available. These varying practices have sparked debates about transparency, accountability, and their impact on community engagement.


% Understanding the implications of these review models requires a careful examination of their advantages and drawbacks. Fully open reviews encourage transparency and allow for broader community participation in discussions around submissions. However, they may also introduce biases and discourage candid feedback from reviewers. Conversely, delayed or restricted review disclosure can protect reviewer anonymity but limits the broader benefits of transparency. To better understand these trade-offs, it is essential to analyze how these review practices impact the academic community, particularly in fields like AI and ML, where the pace of innovation is accelerating rapidly.

% To explore these dynamics, we launched Paper Copilot publicly two years ago — a website designed to aggregate and analyze data from AI / ML conferences from multiple sources including official conference sites and review platforms while tracking community engagement throughout the review and decision-making process. By examining trends and patterns in review practices, we aim to shed light on the community's preferences and needs regarding review transparency. This position paper draws on our findings to advocate for a more transparent, open, and regulated review process, which we believe can foster stronger community engagement and drive further progress in AI / ML research.

% \textbf{Contributions}: To this end, we provide the following main contributions to the discussion about the increasing community needs for fully open review in the AI / ML
% domain:
% \begin{itemize}
%     \item We provide open statistics including review score visualization, review score collection, and author/affiliation analysis across various venues for the past 3 - 5 years.
%     \item We validate the community's interest for review disclosure by providing quantization and analysis based on the collected data in the past two years.
%     \item we discuss the pros / cons for different review process and we position to advocate for a more transparent, open, and regulated review process.
% \end{itemize}



%%%%%%%%%%%%%%%%%%%%%% v0 %%%%%%%%%%%%%%%%%%%%%%%

% Observation 1: more and more venues turn into Openreview. However, the review is not released after the Review Process. Chart, venues that fully opened review, before and after the rebuttal.

% Observation 2: More and more venues introduce the rebuttal period to 

% Observation 3: Paper released from Openreview and venues site remain inconsistency.

% Observation 4: The community has a need to learn from the review and also supervise the review process.
% figure of age here, as the community age goes down, meaning we probably need more regulation to balance the review quality.


% Figure 1. effectiveness and review changes to reveal the effectiveness of fully open-review process e.g. ICLR
