\section{Conclusion, limitation, and future work}
\label{sec:future}
We introduce ATOP, the first method for adding realistic part articulation to existing 3D assets using text prompts and motion personalization. Unlike works such as \cite{liu2024cage, lei2023nap}, which focus on generating articulated assets, our goal is to enhance static 3D models by inferring motion attributes. While ATOP achieves high accuracy in 3D part articulation, it has some limitations. First, our method currently supports only revolute and prismatic motions and does not handle multiple moving parts simultaneously, though it can infer different part motions in separate inference steps. Second, our model requires a segmented mesh as input, which can sometimes be a constraint. However, with recent advances in vision-language models (VLMs) \cite{ravi2024sam2segmentimages, liu2023partslip, kim2024partstad}, obtaining segmented meshes has become more accessible. By rendering multiple views of a mesh, VLMs can generate part segmentations, which can then be mapped back to the original mesh. In future, we will focus on expanding scalability across object categories and enabling cross-category motion transferâ€”e.g., opening vehicle and room doors using few-shot learning from cabinet-opening videos. More broadly, our approach is a step toward animating vast existing 3D assets, but challenges like the "missing interior" problem remain, as most models lack interiors.