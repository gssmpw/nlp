%\vspace{-7mm}
\section{Introduction}
\label{sec:intro}

Everyday objects in the 3D world we live in undergo various movements and articulations. The ability to model and reason about object articulations in 3D plays an important role in simulation, design, autonomous systems, as well as robotic vision and manipulation.
%
Over the past ten years, there has been a steady build-up of digital 3D assets, from ShapeNet~\cite{chang2015shapenet} (3M models)  in the early days, to efforts on improving the assets' structure (e.g., PartNet~\cite{mo2019partnet} with 27K finely segmented models) and quality (e.g., Amazon Berkeley Objects (ABO)~\cite{collins2022abo} with $\sim$8K models), and to the latest and largest open-source repository Objaverse-XL~\cite{deitke2023objaversexluniverse10m3d} (10M+ models).
However, very few, if any, of these models come with part motions. Predominantly, they were all constructed in their rest (i.e., unarticulated) states. To our knowledge, the largest 3D datasets with part articulations, PartNet-Mobility~\cite{xiang2020sapien} and 
Shape2Motion~\cite{wang2019shape2motion}, only contain 2,346 and 2,440 {\em manually\/} annotated synthetic models, respectively. Clearly, human annotations are expensive in terms of time, cost, and expertise --- they are not scalable to endow large volumes of 3D assets with articulations. 

In this paper, we wish to develop a learning method to generate {\em plausible\/} and {\em accurate\/} articulations for {\em any\/} given 3D object, {\em without\/} relying on any human-annotated 3D part motions as most existing methods~\cite{hu2017learning,wang2019shape2motion,abbatematteo2019learning,mo2021where2act,jain2021screwnet,yan2020rpm,jiang2022opd} do. In addition, we would also like to solve this problem in a {\em few-shot\/} setting, making the task even more challenging. Recently, there has been a surge of attempts to train foundational generative models for this task \cite{li2024dragapart, li2024puppet}. However, the scarcity of annotated datasets significantly limits their generalization ability.
%
With the recent success of {\em video diffusion models\/}~\cite{blattmann2023stable,wang2024vc,xing2024dynamicrafter,guo2023animatediff}, it is natural to leverage their zero shot and open vocabulary capabilities to first generate motion videos for our target object and then perform a 2D-to-3D motion transfer. This approach is (3D) annotation-free, hence more scalable, but faces three key challenges to obtain accurate 3D part articulations:

\begin{itemize}[leftmargin=*]
\item First, while SOTA video diffusion models 
can attain unprecedented visual quality for open-domain text-to-video generation, they are not yet well trained or sufficiently adaptive to generating articulations of everyday objects (e.g., even something as simple as a laptop), especially when typical {\em piecewise rigidity\/} is expected; see Fig. \ref{fig:diff_articulation}. 
\item Second, while current text-to-video models can generate plausible motions for {\em some\/} referenced object, to our knowledge, none of them can ensure that the generated motion is exactly of a {\em specific\/} (the target) {\em 3D object\/}.
\item Third, current diffusion models still lack fine-grained control over which specific part should move.
\end{itemize}

\input{figures/diff_articulation}
\input{figures/t2i_articulation}

To address these challenges, we present a novel method based on {\em motion personalization\/} to articulate a 3D object in the form of a mesh, with respect to a part prescribed in a {\em text prompt\/}; see Fig.~\ref{fig:teaser}.
Our method is coined ATOP, for Articulate That Object Part, and it consists of three steps:
\begin{enumerate}[itemsep=2pt, parsep=0pt, topsep=2pt, leftmargin=*]
\item {\em Finetuning for category-specific motion video generation.} This is a key first step to compensate for the lack of articulation awareness by current video diffusion models. We finetune a pre-trained multi-view \textit{image} generator, ImageDream~\cite{wang2023imagedream}, for \textit{controllable multi-view video} generation, using {\em few-shot\/} video samples obtained for the target object category, that are of different objects from the target. We start from the pre-trained weights of ImageDream and inflate them for video processing, with motion region-of-interest and multi-view controls enabled through text and camera pose conditioning.

\item {\em Motion video personalization.}
We render multi-view images and masks of the part which is to be articulated, for the target 3D mesh. These images, rendered masks, camera poses, and text prompt embeddings are fed to our diffusion model, which \textit{hallucinates} the {\em desired part motion} from multi-views, personalized to the target object.
\item {\em Video-to-mesh motion transfer.}
Using the personalized output from above, we transfer the plausible video motion to the target 3D object via differentiable rendering, which optimizes the motion axis parameters associated with the prescribed part by a score distillation sample (SDS) loss \cite{poole2022dreamfusion}; see Fig.~\ref{fig:pipeline} for an overview.
\end{enumerate}

To our knowledge, ATOP presents the first annotation-free learning framework for generating accurate part articulations of existing 3D objects from text prompts. We present both qualitative and quantitative experiments to show that our method is not only capable of generating realistic personalized motion videos on unseen shapes, but also transferring the motion to 3D meshes, resulting in more accurate articulations in 3D than prior works in a few-shot setting.


