\section{Experiments}
\label{experiments}

In this section, we present the qualitative and quantitative results of our method's ability to infer 3D motion parameters given only a static segmented mesh as input. Along with it we also compare our multi-view video outputs with other baselines.
\subsection{Dataset and Metrics Details}
We evaluate our approach using the PartNet-Mobility dataset \cite{xiang2020sapien}, following the train-test split of \cite{kim2024partstad, liu2023partslip}. Each training shape is rendered into four view videos, capturing articulations like a folding chairâ€™s seat movement. Views are set at azimuth angles of $[45\degree, 135\degree, 225\degree, 315\degree]$, with elevation at $10\degree$ for some categories and $30\degree$ for others. This views are fixed throughout our experiments. Each view consists of $10$ frames at $256\times256$ resolution, along with first frame binary mask of the articulated part. To assess generalization, we perform \emph{zero-shot} experiments on $135$ objects from the ACD dataset \cite{iliash2024s2o} curated by \cite{liu2024singapo}, which includes challenging objects from ABO \cite{collins2022abo}, 3D-Future \cite{fu20213d}, and HSSD \cite{khanna2024habitat}. This dataset provides textured segmented meshes with part annotations. We render multi-view images and segmentation masks using the same camera viewpoints. 

We assess our approach in two steps: 1) Multi-View Video Generation and 2) 3D Motion Axis Parameter estimation. For temporal coherence, we use Frechet Video Distance (FVD) \cite{unterthiner2019fvd}, while text alignment is measured with the CLIP score \cite{CLIP1}. Spatial and temporal consistency are evaluated using PSNR and LPIPS \cite{zhang2018perceptual}. To assess 3D motion axis parameter estimation, we follow \cite{wang2019shape2motion, jiang2022opd} and compute Mean Angular Error (MAE) and Mean Position Error (MPE) of the joint axis. MAE is the dot product between predicted and ground truth motion axes, while MPE is the euclidean distance between their joint origins.

\subsection{Baselines \& Result Discussions}
% To the best of our knowledge, this is the first work to address the challenge of 3D mesh articulation via few-shot motion personalization. Consequently, no existing baseline directly takes the same input and produces the same output as ours. Instead, we evaluate individual components of our pipeline by comparing them against suitable baselines for each step. For the multi-view video evaluation step, our model takes a input image with a part mask as input and generates multi-view videos for the part. For this step, we adapt PuppetMaster \cite{li2024puppet} and ImageDream \cite{wang2023imagedream} as our baselines. PuppetMaster \cite{li2024puppet} is an open-vocabulary articulation method that generates a motion articulated video from a input image and a sparse set of motion trajectories (i.e., drags) as user input. Since there is no drag based input to our model, we use the ground-truth part trajectory from the training data to estimate the drags. Imagedream \cite{wang2023imagedream} is a multi-view generation model which generates multi-view images given a single image as input. For the 3D motion transfer step, our pipeline takes a static segmented 3D mesh as input and estimates the 3D motion axis parameters based on the motion output from the multi-view diffusion model. To evaluate this step, we compare with Shape2Motion \cite{wang2019shape2motion} (S2M) and Openable Part Detection \cite{jiang2022opd} (OPD). S2M takes 3D object point cloud as input and estimates the motion parameters, whereas OPD takes a image of the object as input and estimates the motion parameters. We evaluate both the steps of our method along with the baselines on PartNet-Mobility dataset.  
Given the modules used in our approach, we assess the individual components of our pipeline by benchmarking them against appropriate baselines tailored to each step. 
\\
\noindent\textbf{Multi-View Video Generation Baselines.} Since no prior work directly predicts multi-view videos from \emph{static multi-view images} in a {\em controllable manner}, we establish two baselines. The first baseline uses DynamiCrafter \cite{xing2024dynamicrafter}, where a pre-trained video diffusion model generates videos from multi-view images in a zero-shot manner. We denote this as MV+DC. The second baseline finetunes the ImageDream (ID$^{\dagger}$) \cite{wang2023imagedream} model without extra temporal attention.
\\
\noindent\textbf{3D Motion Axis Parameter Estimation Baselines.} Our pipeline estimates 3D motion axis parameters from a static, segmented 3D mesh using motion outputs from the multi-view diffusion model. We benchmark this step against Shape2Motion (S2M) \cite{wang2019shape2motion} and Openable Part Detection (OPD) \cite{jiang2022opd}. Shape2Motion predicts motion parameters from a point cloud and is trained end-to-end with 3D ground truth. OPD, on the other hand, estimates motion parameters from a single image using an object detection backbone that predicts segmentation masks, motion type, and motion parameters. We compare all these models in a few-shot setting by retraining them with our train and test split.
\input{tables/motion_parameter_results}
\input{tables/motion_parameter_generalization_results}
\input{tables/mv_results}
\input{tables/ablations}
\input{figures/qual_mv_results_comparison}
\input{figures/mask_effect}
\noindent \textbf{Discussions.} As shown in Table \ref{tab: partnet_sapian_quant}, Table \ref{tab:motion_parameter_estimation_gen}, and Fig. \ref{fig:results_acd}, our method predicts plausible motion parameters without using 3D annotations, beating the prior state-of-the-art methods by a large margin especially in angular error metrics. Shape2Motion outperforms OPD (Table \ref{tab:motion_numbers}) since it leverages point clouds, whereas OPD relies on single-image inputs. However, Shape2Motion struggles on the generalization dataset (Table \ref{tab:motion_parameter_estimation_gen}) due to limited generalization capability of the 3D backbone network in a few-shot setting. In contrast, our method generalizes well showcasing the effectiveness of our approach.
% for two reasons: (1) Diffusion models, trained on large datasets, generalize better than feed-forward networks trained from scratch. (2) We train a separate personalized diffusion model for each motion type, enhancing generalization while preventing overfitting.
Beyond 3D motion parameter estimation, we also compare our multi-view video generation results with the baselines. As shown in Table \ref{tab:video_generation_comparison} and Fig. \ref{fig:results_mv}, our method outperforms the baselines in LPIPS, PSNR, and FVD, while achieving comparable CLIP scores. Fig.~\ref{fig:results_mv} highlights that MV+DC preserves the appearance of multi-view image inputs across all frames. However, it struggles with geometric consistency across views and has no articulation motion, reinforcing our claims in Sec.~\ref{sec:intro} and Fig.~\ref{fig:diff_articulation}. In contrast, as can be seen in Fig. \ref{fig:results_mv} and Table \ref{tab:video_generation_comparison} our results achieves best $\text{PSNR}$, $\text{LPIPS}$ and $\text{FVD}$ scores which shows that our method is able to generate 3D consistent output along with successful articulation. 
\section{Ablations} 
In Table \ref{tab:dir_vs_algo.} and Table \ref{tab:num_vids} and Fig. \ref{fig:mask_effect} we demonstrate the efficacy of several design choices. First, in Table \ref{tab:dir_vs_algo.} we show how our algorithm based optimization yields much better results compared to optimizing the motion axis parameters directly with chamfer distance loss. In Table \ref{tab:num_vids} we show how the generation quality of multi-view videos is affected as we increase the number of videos for finetuning. As can be seen in Table \ref{tab:num_vids} temporal consistency significantly falls as we reduce the number of videos. Finally in Fig. \ref{fig:mask_effect} we show the motion controllability of our method. As can be seen, by providing the appropriate part mask, we can control the spatial location of the part motion.
% In addition, our method is also controllable using a binary mask using which we can articulate all the parts of interest for a particular shape by just changing the mask location. This can be seen in Fig. \ref{fig:mask_effect}.


