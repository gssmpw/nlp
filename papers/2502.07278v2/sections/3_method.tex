\section{Method}
\label{method}
\input{figures/pipeline_overview}  

\subsection{Problem Setting}
\label{sec:problem setting}
Given a small collection $\mathcal{V}$ of multi-view (4 views) videos of articulated objects belonging to the same category (e.g., trashcans), along with a motion prompt $\tau$ (e.g., ``The lid of the trash can is getting opened."), our goal is to learn category-specific part motion and adapt and transfer the part motion into 3D space of a target mesh from the same shape category. To address this challenge, we decompose the problem into two main stages: \textit{a) Motion Personalization} (Sec.\ref{sec:ft_for_mv_video-gen} and Sec.\ref{sec: personalized_motion_inference}) and \textit{b) 3D Motion Axis Optimization} (Sec.\ref{sec: 3D motion inference}). In the first stage, the multi-view videos $\mathcal{V}$ and the motion prompt $\tau$ are utilized to capture the dynamics of the object parts. This is achieved by fine-tuning a pre-trained image-to-multi-view diffusion (I2MV) model \cite{wang2023imagedream}, enabling the synthesis of multi-view video data (Sec.\ref{sec:ft_for_mv_video-gen}). Once trained, the fine-tuned model is then used to synthesize motion from multi-views for any target object from the same shape category (Sec.\ref{sec: personalized_motion_inference}). In the second stage (Sec.\ref{sec: 3D motion inference}), the synthesized multi-view videos are employed to extract the motion axis within 3D space, which subsequently guides the articulation of the 3D mesh $\mathcal{M}$. An overview of the problem formulation and the proposed methodology is depicted in Fig. \ref{fig:pipeline}. 

\subsection{Generating Multi-view Videos} 
\label{sec:ft_for_mv_video-gen}
As shown in Fig. \ref{fig:diff_articulation}, state-of-the-art diffusion models struggle with articulated motion. To address this, we introduce a finetuning step that efficiently incorporates multi-view articulation awareness into a diffusion model while preserving data efficiency and generalizability. This task is challenging because: (1) Existing finetuning methods for video generation \cite{tuneavideo, qi2023fatezero, zhao2024motiondirector} cannot generate 3D-aware, multi-view videos with generalizability. (2) Recent advances in 4D generation \cite{zhang20244diffusion, li2024vivid} lack spatially controllable motion, which is essential for articulation. To overcome these limitations, we propose a finetuning strategy that adapts a multi-view image diffusion model \cite{wang2023imagedream} for controllable multi-view video generation. Since these models are image-based, we extend them to videos using a network inflation approach \citep{FLATTEN, tuneavideo}, enabling multi-view video synthesis.

\noindent \textbf{Correspondence-Aware Spatial Attention.} We want the generated multi-view videos to be geometrically consistent to ensure faithful reconstruction of coarse articulated shape during the 3D motion axis inference step. 
\input{figures/self_attention}
\noindent For this, we inflate the correspondence aware self-attention module of Image-Dream to capture the geometric consistency among the generated multi-view videos. Given a multi-view video as input during finetuning, we first encode these video frames into a $6D$ latent $\text{Z}$ using a VAE encoder $\mathcal{E}$ \cite{vae}, and tokenize the latents to size $(\text{B}\times \text{N}_{\text{v}}\times\text{N}_{\text{f}}) \times (\text{H}\times\text{W})\times\text{F}$ before applying self-attention using ImageDream’s pre-trained weights. Fig. \ref{fig:corr_self_attention} demonstrates how information is aggregated across views in the self-attention process. Because self-attention layers are trained to capture geometrical consistency across views, we reshape the tensors accordingly to apply attention across views at each frame index independently. This reshaping operation is given as:

\begin{equation}
\small
    \text{Z}_{\text{out}} = \texttt{reshape}(\text{Z}, (\text{B} \text{N}_{\text{v}} \text{N}_{\text{f}})(\text{H}\text{W})\text{F}\rightarrow(\text{B} \text{N}_{\text{f}})(\text{N}_{\text{v}}\text{H}\text{W})\text{F}) 
\end{equation}
Here,  $\text{B}$ is the batch size, $\text{N}_\text{v}$ the number of views, $\text{N}_\text{f}$ the number of frames, $\text{H}$ and $\text{W}$ the spatial dimensions, and $\text{F}$ the feature dimension. The reshaped tensor  $\text{Z}_{\text{out}}$ is then used in subsequent layers. After the correspondence-aware spatial attention block, object appearance is integrated into the model via a cross-attention mechanism.

\noindent\textbf{Cross Attention for Appearance Injection.} Since our goal is to personalize the motion to the target shape, in addition to ensuring that the generated multi-view videos have geometric consistency, we also need to preserve the overall structure of the target shape across multiple views in the generated videos. This structural alignment helps produce pixel-accurate outputs that closely match the target mesh, ultimately improving the motion axis optimization during the motion inference step. For this, we leverage the pre-trained CLIP image encoder weights, which are trained along-with ImageDream diffusion model for incorporating images as an additional modality for 3D generation. To make sure that the object appearance is preserved across all the viewpoints, we provide multi-view conditioning through this CLIP Image encoder and fuse the information with the latents from the diffusion model. For this, \emph{multiple views (e.g., 4) of the object's rest state} are encoded into fixed-size embeddings using a pre-trained CLIP image encoder. These embeddings, along with CLIP text embeddings, are fused into the latent space via cross-attention, ensuring each view's features align with its corresponding latent representation. This is achieved as follows: 

\begin{equation}
\small
{\text{F}}_{\text{out}} = \texttt{Attn}(\text{Q}, \text{K}_{\text{txt}}, \text{V}_{\text{txt}}) + \lambda \cdot \texttt{Attn}(\text{Q}, \text{K}_{\text{img}}, \text{V}_{\text{img}})
\end{equation}
In this setup, $\text{Q}$ represents the query tokens from the latent $\text{Z}$, while $\text{K}_{\text{txt}}$ and $\text{V}_{\text{txt}}$ are the keys and values for the text embeddings. $\text{K}_{\text{img}}$ and $\text{V}_{\text{img}}$ are keys and values of image tokens. $\texttt{Attn(.)}$ denotes the attention mechanism \cite{vaswani2017attention}.
\noindent\textbf{Controllable Motion Module.} Besides geometric consistency and appearance injection, it is crucial for the diffusion model to learn the temporal motion patterns in the articulation videos. Improper temporal constraints would break the synchronization among different views and introduce geometric inconsistency. However, simply inserting a temporal attention block within the diffusion model might not serve our purpose because our main goal is to achieve controllability in the motion generation so that the generated motion output can be spatially controlled using the part masks of the target object. To achieve this, we first add a new sparse spatio-temporal attention block that interpolates temporal features between the first and $n^{th}$ frame using attention \cite{vaswani2017attention}. The latents $\texttt{Z}$ from previous layers are then rearranged accordingly in order to apply temporal attention. This is given as:
\begin{equation}
\small
\text{Z}_{\text{out}} = \texttt{reshape}(\text{Z}, (\text{B}\text{N}_{\text{f}}\text{N}_{\text{v}})(\text{H}\text{W})\text{F}  \rightarrow ((\text{B}\text{N}_{\text{v}}\text{H}\text{W})\text{N}_{\text{f}}\text{F})
\end{equation}
Next, in order to control the spatial location of the motion, we apply an \emph{adaptive affine transformation} to the model's intermediate features. For this, given a mask input $\mathcal{B}$ and the mapping functions $f(.)$ and $g(.)$ we map the mask to scale $\gamma$ and shift $\beta$ and use them to modulate the latents as $\text{Z}^{'} = \gamma \odot \text{Z} + \beta$. These functions are implemented using two-layer small convolutional blocks. A high-level overview of the controllable motion module is shown in Fig. \ref{fig:pipeline-temporal}. Temporal attention is then applied between the first and the last frames:

\begin{equation}
\small
    \text{Z} = \texttt{Attn}(\text{Q}_{\text{i}}, \text{K}_{1}, \text{V}_{1})
    \label{eq: temp_attention}
\end{equation}
Here, $\text{Q}_{\text{i}}$ represents the query tokens of the $\text{i}^{th}$ frame, while $\text{K}_{1}$ and $\text{V}_{1}$ are the keys and values from the first frame. The final output, $\text{Z}$, is then fed into the decoder VAE to generate the multi-view video.

\input{figures/pipeline_temporal}
\noindent \textbf{Camera Controllability.} For multi-view video generation, camera poses are incorporated into the diffusion model as a sequential input. They are first encoded using MLP layers, transforming them into pose embeddings. These embeddings are then fused with the timestep embeddings and added to the latents produced by the ResBlocks.

\subsection{Personalized Motion Generation}
\label{sec: personalized_motion_inference}

In the previous section, we made design changes to the pre-trained multi-view image diffusion model with network inflation to adapt this model for multi-view video generation. We finetune this model using the few-shot training samples and update the newly added motion module. Post finetuning, we obtain a controllable multi-view video diffusion model, $\psi^{*}$. In order to generate personalized motion to a target mesh, we first render the 3D mesh in its rest state using the same fixed camera poses from finetuning. Then, given a text prompt ($\tau$) and multi-view images $\mathcal{I}_{r}$, we infer the object's motion from different views:
\begin{equation}
\small
    \hat{\mathcal{V}} = \psi^{*}(\mathcal{I}_{r},\tau,\mathcal{B})
    \label{eq: personalization}
\end{equation}
Here, $\mathcal{I}_{r}$ is the rendered image of the target mesh, and $\mathcal{B}$ is the corresponding part segmentation mask. The generated personalized multi-view motion, $\hat{\mathcal{V}}$, is then used to estimate the 3D motion axis for the target mesh.



\subsection{3D Motion Axis Optimization} 
\label{sec: 3D motion inference}
We leverage the personalized motion as a prior to robustly estimate the 3D motion parameters on a static target 3D mesh. Fig.~\ref{fig:pipeline-3d-motion-transfer} provides an overview of the 3D motion parameter optimization stage, which we detail below. 

\noindent\textbf{Estimating intermediate states:} Given the input shape which is represented as a mesh $\mathcal{M}$, with vertices $\mathcal{V} \in \mathbb{R}^{n\times3}$ and faces $\mathcal{F} \in \{1, 2, \dots, n\}^{m\times3}$, we deform the mesh using the guidance from the personalized diffusion model. For this, we first parameterize the 3D representation for each frame of the video using $N_{g}$ gaussians which are initialized with the points sampled from the input mesh. This representation is then optimized by distilling the category specific motion priors encapsulated in the personalized frozen diffusion model $(\psi^{*})$ using the score distillation loss Eq. \ref{eq: sds_actual}. At each optimization step illustrated in Fig. \ref{fig:pipeline-3d-motion-transfer} we use a differentiable renderer $\mathcal{R(.)}$ \cite{keselman2022approximate, keselman2023flexible} to render multiple views of per-frame gaussians in 3D space to its corresponding frame in latent space of the diffusion model. The motion video rendered by these gaussians is then denoted as $\mathcal{Z}_{g}$. Next, we sample a diffusion timestep $t$ and noise $\epsilon \thicksim \mathcal{N}(0,1)$. We use this to add noise to the rendered latents and the noisy latents are then denoised using the personalized frozen diffusion model $(\psi^{*})$, where the diffusion model is conditioned using the text prompt, $\tau$, rendered multi-view images $\mathcal{I}_{r}$ and rendered segmentation masks $\mathcal{B}$ of the target mesh. This optimization results in a sparse point cloud of the articulated shape for each time step. 

\noindent\textbf{Motion Axis Estimation:} Given this sparse point cloud of intermediate articulated shape, our goal is to then estimate the motion attributes: \textit{motion axis} and \textit{motion origin}. We focus on piecewise rigid motions, specifically \textit{revolute} and \textit{prismatic} which are the most widely appearing motion types in nature \cite{liu2023paris, qiu2025articulate}. Having obtained a sparse point cloud of articulated shape for each time step, we can directly optimize the motion axis parameters using chamfer distance loss. However, to further improve the accuracy we propose an algorithmic approach for the same. In this step, given the static mesh $\mathcal{M}$, we sample points from the segmented part to be articulated and approximate it with an oriented bounding box (OBB). From the OBB parameters, we determine 7 possible axis origins—one centroid and the centers of its six side faces—and 6 possible axis directions, derived from its three principal components and their negations. The dynamic part is then transformed under a piecewise rigid assumption, and we select the best axis and origin by minimizing the chamfer distance with the articulated point cloud. See the supplementary for algorithmic details.
\input{figures/pipeline_3d_transfer}

\input{figures/qualitative_3d} 

\subsection{Training Objectives}
\label{sec: training_objectives}
We train our model in a two stage paradigm: (a) Finetuning for Motion Personalization and (b) 3D Motion Axis Optimization respectively.\\
\noindent\textbf{Finetuning Stage.} During finetuning, we keep the diffusion model frozen and train only the controllable motion module (Eq. \ref{eq: temp_attention}). The network is optimized using the multi-view diffusion objective $\mathcal{L}_{LDM}$, defined as:
\begin{equation}
\small
\mathcal{L}_{\text{\textit{LDM}}} = \mathbb{E}_{\text{z}, \tau, \mathcal{I}_{\text{r}}, \mathcal{C}, \epsilon, \text{t}} [\|\epsilon - \epsilon_{\theta}(\text{z}_\text{t}; \tau, \mathcal{I}_\text{r}, \mathcal{B}, \mathcal{C}, \text{t})\|^2].
\label{eq:l_ldm}
\end{equation}
where, $\text{z}_{t}$ represents the latents at time step $t$, $\tau$ is the text prompt, $\mathcal{I}_{\text{r}}$ is the reference image, $\mathcal{B}$ is the binary mask, and $\mathcal{C}$ denotes the camera poses.

\noindent\textbf{3D Motion Parameter Optimization.} During 3D motion parameter optimization, we optimize the gaussians using SDS loss \cite{poole2022dreamfusion}. Since our generation is conditioned on multi-view images $\mathcal{I}_{r}$ and masks $\mathcal{B}$, the loss is defined as:
\begin{equation}
\small
    \nabla_{\theta} \mathcal{L}_{SDS} = \mathbb{E}_{t, \epsilon} \left [ \omega(t)(\epsilon_\phi(\boldsymbol{z}_{g_t}; t, \tau, \mathcal{I}_{r}, \mathcal{B}) - \epsilon) \frac{\partial \boldsymbol{z}_{g_t}}{\partial \theta}\right ]
    \label{eq: sds_actual}
\end{equation}
Here, $\theta$ represents the parameters of the differentiable renderer $\mathcal{R}(.)$, $\epsilon_{\phi}$ is the UNet of the personalized diffusion model $\psi^{*}$, and $z_{g_{t}}$ is the latent rendered by the renderer with noise for timestep $t$. $\tau$, $\mathcal{I}_{r}$, and $\mathcal{B}$ refer to the text prompt, rendered image, and segmentation mask of the target mesh, respectively.
