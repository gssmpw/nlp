\section{Related Work}
\label{sec:related work}

\noindent\textbf{Mobility analysis.} Understanding part mobility is essential for kinematics \cite{abbatematteo2019learning, mo2021where2act}. Recent data-driven approaches leverage 3D datasets with articulation annotations to predict motion attributes. ScrewNet \cite{jain2021screwnet} infers articulations directly from depth image sequences without requiring part segmentation, while Hu et al. \cite{hu2017learning} classify motion types from point clouds. Similarly, Shape2Motion \cite{wang2019shape2motion}, RPM-Net \cite{yan2020rpm}, and OPD \cite{jiang2022opd} jointly predict motion-oriented part segmentation and attributes from point clouds and images. While these approaches enable fast inference, their reliance on extensive human annotations limits scalability and generalizability. In contrast, our method piggybacks on the knowledge of video foundational models to generalize under a few-shot setting. 

Another line of works estimates motion attributes of an object as a by-product in the process of reconstructing digital twins of a scanned object \cite{jiang2022ditto, liu2023paris, wei2022nasam, song2024reacto, mandi2024real2code}. However, different from our work, these methods focus on high fidelity reconstruction of the scanned object from multi-view images, monocular videos or depth maps where the scan is captured in different articulation states assuming the object is already articulated. In contrast to these approaches, our work focuses on estimating plausible motion axis parameters for a {\em static mesh}, hence enabling dynamic attributes.
% An alternative line of work estimates motion attributes as a by-product of reconstructing digital twins in a self-supervised manner \cite{liu2023paris, wei2022nasam, song2024reacto}. However, these methods require articulated inputs—such as multi-view images or videos capturing different articulation states—making them impractical for our setting, where the 3D assests do not have any motion attributes. In contrast, we first learn category-specific motion patterns by finetuning a diffusion model. We then use the finetuned model to hallucinate motion for any new target \emph{static} mesh and subsequently transfer the motion back to 3D. This process requires no human annotation of motion attributes and no articulated inputs.

\vspace{3pt}

\noindent\textbf{Motion personalization.} Diffusion models~\citep{ho2020denoising, song2020denoising, song2020score} have garnered significant interest for their training stability and remarkable performance in text-to-image (T2I) generation~\citep{ramesh2021zero, balaji2022ediffi}. Video generation~\citep{le2021ccvs, yu2023magvit, luo2023videofusion} extends image generation to sequential frames, incorporating temporal dynamics. Recent advances~\citep{singer2022make, zhou2022magicvideo, ge2023preserve, nag2023difftad, alimohammadi2024smite} adapt T2I diffusion to spatio-temporal domains via architectural modifications. Personalizing motions using reference videos has gained traction, with methods such as Tune-a-Video~\citep{tuneavideo} enabling one-shot video editing with structural control. Current approaches~\citep{materzynska2023customizing, jeong2023vmc} refine pre-trained text-to-video (T2V) models using regularization or frame residual losses, reducing dependency on training video appearances. Subject-driven video generation~\citep{dreamix, wu2023lamp, zhao2024motiondirector} fine-tunes video diffusion models but often struggles with overfitting and limited dynamics, failing to accurately capture 3D articulations. In contrast, our method proposes a motion personalization approach that enables controllable multi-view video generation, enabling part motions at desired spatial locations, which is crucial for articulation.

\vspace{3pt}

\noindent\textbf{Generative articulation.} With the rapid advancements in generative AI in recent years, the task of learning to represent, reconstruct, and generate object articulations from various input sources has become a prominent research area in visual computing~\cite{li2024puppet,uzolas2024motiondreamer,le2024articulate}. NAP~\cite{lei2023nap} employed diffusion models to synthesize articulated 3D objects by introducing a novel articulation graph parameterization. However, it faced limitations in scalability and controllability for articulations. CAGE~\cite{liu2024cage} addressed these challenges by jointly modeling object parts and their motions within a graph-based structure. Recently, \cite{liu2024singapo} proposed the seminal task of generating articulated objects from a single image. In contrast to these existing approaches that are focused on generating articulated assets, our goal is to enhance existing 3D assets by incorporating precise articulations.

Concurrent to our work, Articulate Any Mesh (AAM) \cite{qiu2025articulate} constructs a ``digital twin" of an input 3D mesh with articulation, instead of directly inferring motion parameters on the input mesh as our work. For articulation generation, AAM leverages GPTo’s foundational knowledge on {\em joint classification} only, over the types of prismatic or revolute joints, while the joint locations and articulation axes are both estimated through heuristic shape analysis. %While GPTo can effectively classify joints, it struggles to determine correct motion axis direction which is crucial for predicting in which direction the part will move. In contrast, 
With ATOP, we optimize motion axis directions by aligning them with plausible video outputs generated during the motion personalization step. This ensures that the target part is articulated correctly with proper motion axis orientations.
