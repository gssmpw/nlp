\noindent{\textbf{Prosthetic control}.} While the SSC paradigm is a well-established control method, it prevents the direct control of multiple DoFs simultaneously.
%Most commercial prostheses use a proportional myoelectric control, relating muscle activations to the velocity of the prosthesis motors~\cite{chen2023}. When more than one degrees of freedom (DoFs) are available, the Sequential Switching and Control (SSC) paradigm is used. In this case, only one joint at a time is driven and the muscle co-contraction is used to switch between DoFs, resulting in a cumbersome control~\cite{amsuess2014}. 
A viable alternative and active field of research is the EMG \textit{pattern recognition}. This aims to recognize repeatable and distinct features from the EMG input signals and associate them to the hand movements. However, bringing these techniques into the real world is still a challenge due to robustness issues (e.g., electrode shift, muscle fatigue, etc.)~\cite{chen2023}. Another possibility is exploiting additional input sources. For instance, in~\cite{castro2022semi}, the user aims at the target object with the prosthesis and four laser scanner lines are used to estimate the object shape and size. Then, the wrist orientation and grasp size are automatically computed. In~\cite{shi2023hand}, instead, the information of the hand-object pose during the approach-to-grasp action is leveraged to predict the final wrist pose and pre-shape type. However, the model needs the hand-object pose as input, requiring a preliminary pose estimation. 
Other approaches exploit the visual input from an \textit{eye-in-hand} camera (i.e., a camera embedded into the prosthesis palm) as additional information, showing promising results~\cite{vasile2022, starke2022, robotics12060152, shi2022target}. 
%For instance, in~\cite{starke2022}, the embedded camera is used to take a picture of the target object, classify it, retrieve the associated grasp from a database and suggest it to the user. The database is also exploited to select the hand and wrist pre-grasp trajectories to execute during the approach. Similarly, in~\cite{shi2022target} an object detector is used to identify the target object in the clutter and predict the right grasp type. Thus, most of the systems for transradial amputees, either do not consider controlling the hand during the approach or the control is based on a one-shot decision (i.e., no closed-loop control). This might be a problem for \textit{eye-in-hand} configurations, since the camera may lose the view on the object. Moreover, even if the object is visible, it is not possible to ensure an optimal view on it during the approach, resulting in a possibly wrong grasp prediction. A possibility would be to request the user to point the prosthesis at the object and take a picture before grasping~\cite{starke2022}, but this would prevent a smooth, human-like, approach. Ideally, they should seamlessly approach the object while the hand points at it throughout the movement.
For instance, in~\cite{shi2022target}, an object detector is used to identify the target object in the clutter and predict the grasp type. Similarly, in~\cite{starke2022}, the embedded camera is used to take a picture of the target object, classify it, retrieve the associated grasp from a database and suggest it to the user. The database is also exploited to select the best hand and wrist pre-grasp trajectories for the approach. However, taking a picture before grasping may hinder a smooth and natural approach to grasp. Thus, in most of the systems for transradial amputees, either the hand is not controlled during the movement or the control is non-adaptiveâ€”meaning it relies on a one-shot decision before grasping rather than continuous control during the approach. 
%Instead, we propose a continuous control architecture for transradial amputees. This exploits a visual servoing~\cite{chaumette2006} for continuous wrist control during the approach, obtaining a natural movement. In addition, we segment the target object into parts and exploit this finer information to predict the final wrist configuration for grasping.
Instead, we propose a control architecture for transradial amputees that integrates visual servoing~\cite{chaumette2006} for continuous wrist control during the approach, enabling more natural movement. Additionally, we segment the target object into parts and utilize this finer information to predict the final wrist configuration for grasping.

\noindent{\textbf{Object parts segmentation}.} Segmenting the object into parts for prosthetic grasping might resemble the affordance detection problem as both computer vision tasks aim at identifying the different parts of an object and the human actions (or grasp types) associated to them. However, they are substantially different and this limits the adoption of affordance detection systems for prosthetic grasping. Specifically, the affordance detection task is typically framed with a coarse labeling~\cite{nguyen2017,myers2015} using, e.g., only the class \textit{grasp}, while, in prosthetics, specific grasp types are required. Furthermore, available image datasets are mostly taken from egocentric or external cameras while the \textit{eye-in-hand} configuration is preferable in prosthetics~\cite{vasile2022}. Finally, the affordance recognition is generally framed as an object detection followed by a semantic segmentation of the affordances within the bounding box~\cite{do2018affordancenet}. 
%Instead, in prosthetic grasping, identifying the different affordance instances is fundamental (e.g., the opposing sides of a bottle).
Instead, in prosthetic grasping, it is fundamental to identify the different parts of an object sharing the same affordance label as distinct instances.
%In this work, instead, we propose an \textit{eye-in-hand} object parts segmentation. We consider the possibility of having multiple grasp types per object, as in~\cite{vasile2022}, but differently from~\cite{vasile2022} and~\cite{do2018affordancenet}, 
Therefore, in this work, we frame the task as an instance segmentation since we need to consider all object parts as separate instances and select one of them to obtain the final wrist configuration. Finally, to overcome the aforementioned limitations of affordance detection datasets, we devise a semi-automatic procedure to obtain ground truth masks for a video dataset from prior work~\cite{vasile2022} and we propose a synthetic dataset generation tool. We use this data for training and testing the models.


% In order to train an object parts segmentation network, a dataset of objects with parts-level labels is required. Despite few affordance-related datasets are available in the robotics literature~\cite{nguyen2017,myers2015,guo2023}, their adoption in the prosthetic scenario is limited for several reasons. These include a coarse labeling (e.g., only a \textit{grasp} label is available instead of the specific type of grasp); missing camera-object configurations (i.e., the \textit{eye-in-hand} setup implies closer object images as the hand approaches it) and multiple grasps per object are barely considered, restricting the prosthesis usability. In addition, the affordance recognition is generally framed as object detection followed by semantic segmentation of the affordances within the bounding box~\cite{do2018affordancenet}. Instead, in our scenario, every object part has to be treated as a separate instance since selecting one of them is necessary to predict the final wirst configuration (see \textbf{Rotation} in Sec.~\ref{sec:application}). Therefore, we address the object parts segmentation as an instance segmentation task.


%Thereby, we build our work upon the prosthetic grasping dataset introduced in~\cite{vasile2022}, termed the iHannes dataset