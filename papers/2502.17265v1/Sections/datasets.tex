

%In order to train the object parts segmentation network, a dataset with parts-level labels is required. 
% An object parts segmentation network requires a dataset of objects with parts-level labels.
% Moreover, the latter should meet the characteristics of a \textit{eye-in-hand} prosthestic grasping scenario. For instance, the camera can be very close to the object and all of its viewpoints have to be considered. Thereby, our work is based on the prosthetic grasping dataset introduced in~\cite{vasile2022}, termed the iHannes dataset. However, this defines the grasp type labels in terms of fingers configuration during the grasping movement, while in this work we are interested in the wrist configuration. Hence, a preliminary step is to convert the labels. We consider two wrist configurations: \textit{top grasp} and \textit{side grasp}. Then, we convert every object part labels from the fingers to the wrist configuration (e.g., the \textit{power grasp} of the \texttt{006\_mustard\_bottle} is converted to \textit{side grasp}). 
% Finally, since the iHannes dataset has no segmentation ground truth, in the remainder of this section we present the methodology that we used to obtain them. Then, a synthetic dataset to train the models is proposed.



In the considered grasping task, the prosthetic hand might approach the object from any direction. Adopting an \textit{eye-in-hand} configuration, this means that the object parts segmentation model should work well from all the object viewpoints. 
However, collecting and labelling a dataset with such characteristic is a tedious procedure. Therefore, motivated by recent works on bridging the sim-to-real gap~\cite{vasile2022}, we aim to train the models on a synthetic dataset and evaluate them on a real dataset. 
%Thus, a proper dataset should be used for training and testing.
Given the similarity with the task and setting, in this work, we adopt the real dataset from~\cite{vasile2022} but with significant modifications. Firstly, while in~\cite{vasile2022} the grasp type labels are defined in terms of the fingers configuration for grasping (e.g., \textit{pinch} grasp), in this work, we consider the wrist configurations (i.e., \textit{top grasp} and \textit{side grasp}). Therefore, we convert every object part label considered in~\cite{vasile2022} from the fingers to the wrist configuration (e.g., the \textit{power grasps} of the \texttt{006\_mustard\_bottle} are converted to \textit{side grasps}). Secondly, in~\cite{vasile2022} the label is assigned to an entire video, while for this study we consider a per-frame instance segmentation task. Thus, masks labels are required. In the following sections, we describe how we adapt the prosthetic dataset, namely the \textit{iHannes} dataset, collected in~\cite{vasile2022}, to the task at hand and the generation process of the synthetic dataset used for training the vision models.


\begin{figure}
    \centering
    \vspace{+0.30cm}
    \includegraphics[width=1.0\linewidth]{Figures/obj_parts.png}
    \caption{The 15 YCB objects with labeled object parts. The red and green masks encode the \textit{top} and \textit{side grasps}, respectively. 
    The non-highlighted object parts are labeled as \textit{no grasp}.  
    }
    \vspace{-0.60cm}
    \label{fig:obj_parts}
\end{figure}

\subsection{Real dataset annotation}
\label{sec:real_dataset_annotation}
The \textit{iHannes} dataset consists of RGB-D videos of approaching-to-grasp actions for 15 YCB objects~\cite{calli2015}. Since no ground-truth masks are available, in this work, we devise an efficient way to obtain them. Despite the emerging availability of semi-automatic mask annotation tools, the masks of the considered object parts may not have clear boundaries (e.g., a mask boundary is not necessarily aligned with the edges of the object's texture), resulting in error-prone manual labeling (i.e., incoherent parts boundaries between images). Therefore, we conceived a two step approach: (i) we manually partition the 3D mesh of all the 15 considered objects into graspable parts and assign the correspondent grasp type to each of them (see Fig.~\ref{fig:obj_parts}); then, (ii) for every image, the pose of the object ($o$) with respect to the camera ($c$), i.e. $\mathbf{T}_{c,o} \in SE(3)$, is used to project the object mesh onto the image plane in order to obtain the exact masks of the object parts. However, $\mathbf{T}_{c,o}$ is unknown and needs to be computed for every image.

In the \textit{iHannes} RGB-D videos, the target object position is fixed and the camera is moving toward it. Consequently, while the camera-to-object pose needs to be directly estimated for the first frame ($\mathbf{T}_{c^1,o}$), for any subsequent frame $k$, the relative camera displacement from frame $1$ to $k$ ($\mathbf{T}_{c^1,c^k}$) is sufficient to obtain $\mathbf{T}_{c^k,o}$. For instance, let $\mathbf{T}_{c^1,c^2}$ denote the camera pose from the first to the second frame (i.e., camera displacement) and $\mathbf{T}_{c^2,o}$ the camera-to-object pose for the second frame. $\mathbf{T}_{c^2,o}$ can be computed as $\mathbf{T}_{c^2,o} = \mathbf{T}_{c^1,c^2}^{-1} \mathbf{T}_{c^1,o}$. In general, being $\mathbf{T}_{c^{k-1},c^k}$ the camera displacement from frame $k-1$ to $k$, we first derive the camera displacement from frame $1$ to frame $k$ as $\mathbf{T}_{c^1,c^k} = \prod_{i=1}^{k-1}\mathbf{T}_{c^i, c^{i+1}}$ , then $\mathbf{T}_{c^k,o} = \mathbf{T}_{c^1,c^k}^{-1} \mathbf{T}_{c^1,o}$ is obtained. Hence, $\mathbf{T}_{c^1,o}$ and $\mathbf{T}_{c^i,c^{i+1}}$ are needed for every video.

In practice, we run a state-of-the-art object pose estimator~\cite{wang2021} on the first frame of the video to obtain a coarse estimate of $\mathbf{T}_{c^1,o}$ and we manually refine it to be used as ground truth. Then, the camera displacement for subsequent frames ($[\mathbf{T}_{c^i,c^{i+1}}]_{i=1}^{n-1}$) is estimated using~\cite{park2017colored}. We applied this two-step procedure rather than directly estimating $\mathbf{T}_{c^i,o}$ for every frame $i$ using~\cite{wang2021} since its predictions can be noisy (e.g., if the object is scarcely visible). Instead, estimation of the camera displacement~\cite{park2017colored} allows to use features from the surrounding environment. We used this procedure on the \textit{Same person} subset~\cite{vasile2022} of the \textit{iHannes} dataset, resulting in 311 videos labeled with the camera-to-object pose for every frame ($\mathbf{T}_{c^i,o}$). However, some frames were discarded because the blur caused a too noisy estimate of $\mathbf{T}_{c^i,c^{i+1}}$. Overall, 14692 frames were labeled with the $\mathbf{T}_{c^i,o}$ pose. Finally, note that in some cases it was not possible to initialize $\mathbf{T}_{c^1,o}$ using~\cite{wang2021}, since the pre-trained weights do not include a few objects used in the iHannes dataset. For those cases, the initialization of $\mathbf{T}_{c^1,o}$ was done by manually aligning the object mesh with the object in the point cloud.

\subsection{Synthetic dataset generation}
\label{sec:synthetic_dataset_generation}
While the \textit{iHannes} set is composed of real grasping videos and is used for testing the proposed prosthetic vision system, a training set comprised of images and mask labels is also required. Morever, it should meet the main requirement of the \textit{eye-in-hand} configuration, i.e., the variability of object viewpoints. Thus, we design a dataset generation tool.
%that allows to meet the main requirement of the \textit{eye-in-hand} configuration, i.e., the extreme variability of object viewpoints.

We import the partitioned meshes of the 15 YCB objects, introduced in the previous section, in the \href{https://unity.com/}{Unity engine}. The data generation pipeline works as follows: (i) one object at a time is considered in the indoor tabletop scene; (ii) we uniformly sample 400 points on the surface of a upper hemisphere centered on the 3D centroid of the object; (iii) the simulated camera is placed on each of these points, look at the object and capture an image. We use the Unity High Definition Render Pipeline (HDRP) to obtain photo-realistic images and the Perception package to collect the ground-truth masks of the object parts. In addition, we apply domain randomization to various extent: (i) we randomize the background, light condition, table and position of the object; (ii) we use hemispheres of radius in the range from 0.2 to 1 meter by applying stratified sampling, i.e., the range is divided into 6 bins and then uniformly sample values within each bin; (iii) we apply random rotations (between 0 and 90 degrees) about the camera optical axis and make the camera look at a random point on the object. In summary, 400 points are sampled for each radius bin, resulting in 2400 images per object. This dataset is used for training the vision model. Moreover, one further dataset with slight modifications is generated for model validation. Specifically, for this, 100 points are sampled on the surface of the same hemisphere. In this case, the background and light condition are not randomized but we use a completely different scene, representing an outdoor environment with sunlight.


