In this work, we addressed the challenge of controlling the wrist of a prosthetic arm during a reach-to-grasp task. Leveraging the shared-autonomy principle, we introduced a novel computer vision-based framework focused on a continuous wrist control followed by predicting the final wrist configuration for grasping. The system has the potential to reduce both the compensatory body movements and the cognitive burden on the user. Such characteristics can be quantified using a motion capture system and fatigue measures such as the pupil dilation. We leave this for future work by testing our system with amputees.

