    Latest advancements in prosthetic technologies have made significant steps toward restoring motor functions of amputees. However, achieving dexterous and intuitive control of prosthetic hands is yet a challenging task, requiring to meet the user intent with the complicated joints motion needed for object grasping. Most commercial prostheses are based on \textit{electromyography} (EMG) or \textit{mechanomyography} (MMG), relating these input signals to the velocity of the prosthesis motors~\cite{chen2023}. When more than one degree-of-freedom (DoFs) is available, the Sequential Switching and Control (SSC) paradigm is used. In this case, only one joint at a time is driven and the user gives an explicit input signal to switch between the different DoFs, resulting in a cumbersome control~\cite{amsuess2014}. Therefore, relieving the user from complex control input modalities is of high interest in prosthetics. 
Exploiting the know-how from grasp synthesis techniques in robotics~\cite{newbury2023deep} might be a possibility. However, they generally predict a set of grasp poses, then naively select one (e.g., highest score) to be executed by the robot. Instead, in prosthetics, the prediction must conform to the user's intent, i.e., how the user is approaching the object.
In view of this, the \textit{shared-autonomy} (or \textit{shared-control}) principle has been introduced in the literature~\cite{gardner2020}, relying on the collaboration between the user and a semi-autonomous system, generally exploiting additional sources of input such as images or inertial measurements. However, previously presented semi-autonomous systems~\cite{vasile2022,starke2022} do not consider a continuous control for the prosthesis during the approach-to-grasp action. Instead, we believe that in order to foster a more natural grasping approach, the automatic system should continuously drive the joints in compliance with the user motion.

\begin{figure}
    \vspace{+0.3cm}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/phases_cut_v4.png}
    \caption{The phases of the prosthetic grasping pipeline.}
    \label{fig:phases}
    \vspace{-0.6cm}
\end{figure}

In this work, we introduce a novel \textit{eye-in-hand} vision-based shared autonomy system designed to continuously control the wrist DoFs of a prosthetic arm.
\begin{comment}
\begin{itemize}
    \item We present a shared autonomy system for prosthetic grasping. This is based on an eye-in-hand computer vision-based approach that \textcolor{blue}{firstly allows to control the prosthetic wrist to follow the target object using visual servoing and then prepare it to conclude the grasp seamlessly through a prediction based on object parts}.
    \item We propose an object parts segmentation approach, called DINOv2Det, that exploits the powerful feature descriptors of DINOv2~\cite{oquab2023} together with the well-established Mask R-CNN~\cite{he2017} structure for instance segmentation.
    \item We devised a tool to generate a synthetic dataset for objects parts segmentation and a semi-automatic pipeline to annotate existing prosthetic grasping video sets. We used these for training and testing, our vision system.
    \item We tested each component of the proposed eye-in-hand shared autonomy control pipeline with datasets and simulation and, finally, we deployed it on the Hannes arm to verify its effectiveness on a real prosthesis.
\end{itemize}
\end{comment}
We present a prosthetic grasping pipeline based on three phases (see Fig.~\ref{fig:phases}): (i) first, while the user approaches the object, an automatic system continuously control the prosthetic wrist to follow the object in order to achieve a natural motion (\textit{transport} phase); (ii) then, as soon as the user triggers a signal, the system predicts the target object part and prepares the wrist accordingly for grasping (\textit{rotation} phase); (iii) finally, the control is left to the user who will use the EMG signals to drive the fingers opening-closing (\textit{grasping} phase). Moreover, we propose an object parts segmentation network, called DINOv2Det, that exploits the powerful feature descriptors of DINOv2~\cite{oquab2023}, together with the well-established Mask R-CNN~\cite{he2017} structure for instance segmentation. We devised a tool to generate a synthetic dataset for objects parts segmentation and a semi-automatic pipeline to annotate existing prosthetic grasping video sets. We used these for training and testing our vision system. We tested each component of the proposed \textit{eye-in-hand} shared autonomy control pipeline with datasets and simulation and, finally, we deployed it on the Hannes~\cite{laffranchi2020hannes} arm to verify its effectiveness on a real prosthesis.



%In the remaining of this paper we firstly overview the related literature (Sec.~\ref{sec:related_work}). We describe the proposed shared autonomy framework and the presented synthetic and real datasets (Sec.~\ref{sec:method} and~\ref{sec:datasets}). Then, we present the empirical evaluation of the method and its application on the Hannes prosthesis (Sec.~\ref{sec:experiments} and~\ref{sec:application}). Finally, we conclude identifying future directions (Sec.~\ref{sec:conclusions}).
