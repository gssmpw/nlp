\subsection{The Hannes prosthetic device}
\label{sec:the_hannes_prosthetic_device}
We test the proposed methods on the Hannes arm~\cite{laffranchi2020hannes}, considering the setup for a right arm transradial amputation~\cite{boccardowrist}. It has three DoFs: wrist flexion-extension (WFE), wrist pronation-supination (WPS) and fingers opening-closing (FOC). The WFE and WPS are revolute joints that are orthogonal and intersect at a common point (see Fig.~\ref{fig:trajectories}c). The FOC is a single DoF being the fingers actuated all together using one motor. Moreover, a tiny RGB camera is embedded into the prosthesis palm. It points downward with an angle $\theta = 16\degree $ with respect to the WPS rotational axis.

\subsection{Visual servoing simulation}
\label{sec:visual_servoing_simulation}

\noindent\textbf{Setup}. The virtual Hannes~\cite{di2021hannes} is imported into Unity to evaluate the visual servoing schemes in simulation. The aim is to control the WFE and WPS to bring the object at the image center. The IBVS represents the features as image plane coordinates, thus, we use the ground truth object mask from simulation and select the mask centroid as the current feature $\mathbf{s}(t)$, while the image center is the target $\mathbf{s}^*$. 
%According to~\cite{chaumette2007potential}, the depth Z of the current visual feature is required. However, since the real Hannes has no depth sensor, we use a fixed value for $Z$. Given that we consider a tabletop grasping scenario, we set $Z=1$ meters. This approximation is acceptable since the IBVS is well-known for its robustness under imprecise values~\cite{chaumette2006, corke2023}. 
Hereafter, we firstly present an example highlighting the limitation of the s-IBVS and, then, how the pp-IBVS overcomes it. Finally, we provide a quantitative comparison.

\begin{comment}
\begin{table}
    \vspace{+0.2cm}
    \centering
    \renewcommand{\arraystretch}{1.2} % Increase row height
    \setlength{\tabcolsep}{5pt} % Increase space between columns
    \caption{\textcolor{blue}{Quantitative results for s-IBVS and pp-IBVS}} 
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c?c|c?c|c} % Adjust the width of the "Method" column
        \cline{1-5}
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c?}{\textbf{Condition 1}} & \multicolumn{2}{c}{\textbf{Condition 2}} \\
        \cline{2-5}
         & Iteration & Error & Iteration & Error  \\ \hline
        s-IBVS           & $\mathbf{129.2 \pm 20.9}$  & $22.5 \pm 19.8$ & $\mathbf{382.0 \pm 57.5}$  & $187.4 \pm 18.5$ \\ 
        pp-IBVS          & $334.5 \pm 55.7$  & $22.5 \pm 19.8$  & $416.0 \pm 65.3$  & $\mathbf{17.6 \pm 9.4}$  \\ \hline
        
    \end{tabular}
    }
    \label{table:vservo_quantitative}
    \vspace{-0.4cm}
\end{table}
\end{comment}




\begin{figure}
    \vspace{+0.3cm}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/trajectories_v2.png}
    \caption{The trajectories generated by the visual servo schemes for two different WFE initial configurations (a-b). The Hannes arm (c).
    }
    \label{fig:trajectories}
    \vspace{-0.6cm}
\end{figure}


\noindent\textbf{Standard IBVS (s-IBVS)}. This controls both the WFE and WPS using Eq.~\eqref{eqn:control}. Our objective is to analyze the travelled trajectories in two scenarios having the same hand-object pose (hence same initial input image) but different initial WFE configurations (i.e., $10\degree$ flexion and $20\degree$ extesion). We run the control scheme while keeping the arm fixed in space and analyze the wrist motion. As shown in Fig.~\ref{fig:trajectories}a, the s-IBVS demands two completely distinct trajectories. The green-line trajectory is considered natural since the wrist rotates inward, i.e., the normal to the palm point toward the object (as in Fig.~\ref{fig:ps_fe_angles}a). Instead, the red-line trajectory is non-natural as the wrist rotates outward (similarly to Fig.~\ref{fig:ps_fe_angles}c). This is due to the different WFE initial configuration. Indeed, as the WFE rotates, the angle $\theta$ between the WPS rotational axis and the camera optical axis varies. In addition, when the WPS rotates, any point on the image plane translates both vertically and horizontally following a circular path governed by $\theta$. Therefore, there exist initial values of $\theta$ such that, in the first optimization step, a WPS outward rotation has the lowest error toward the target, eventually leading to a non-natural trajectory.

\noindent \textbf{Proportional and Partitioned IBVS (pp-IBVS)}. The s-IBVS controls both the WFE and WPS using Eq.~\eqref{eqn:control}, however, the WPS joint causes non-natural trajectories. Consequently, we control only the WFE using Eq.~\eqref{eqn:control} and apply a proportional control law for the WPS: $\dot{q}_{wps} = sign \: \lambda_{wps} \: | \mathbf{e}_{wps} |$, being $\lambda_{wps}$ the proportional gain. Then, at each timestep, we set $sign$ to either +1 or -1 depending on whether the object mask centroid is to the left or right of the image center and $\mathbf{e}_{wps} = x_c - x_o$ (i.e., the horizontal offset between the object mask centroid and the image center). This way, the direction of the WPS rotation only depends on the hand-object relative positioning. As a result, a WPS inward rotation is always executed (see Fig.~\ref{fig:trajectories}b), resulting in a natural wrist motion (as in Fig.~\ref{fig:ps_fe_angles}a).



\noindent\textbf{s-IBVS vs. pp-IBVS}. 
We aim to quantitatively assess the convergence time and naturalness of the final configuration through experiments in simulation. We sample 20 points on the surface of a upper hemisphere centered on the \texttt{006\_mustard\_bottle}. We place the hand on each point and randomly orient the wrist (i.e., the WFE and WPS) such that the object is in view. We run the control schemes and analyze both the convergence time (i.e, iterations required to converge) and the naturalness of final configuration (i.e., successful if the normal to the palm points toward the object). As shown in Tab.~\ref{table:vservo_quantitative}, the convergence time for pp-IBVS is slightly higher, this is expected since two different control laws are used for WPS and WFE, though the naturalness of WPS is ensured by design thanks to the ad hoc control law. Instead, since s-IBVS controls both WPS and WFE using Eq.~\ref{eqn:control}, the shortest path toward the target is ensured. However, this may result in non-natural wrist configurations.


\begin{table}
    \vspace{+0.2cm}
    \centering
    \renewcommand{\arraystretch}{1.2} % Increase row height
    %\setlength{\tabcolsep}{8pt} % Increase space between columns
    \caption{Quantitative Results for s-IBVS and pp-IBVS}
    \begin{tabular}{ccc} % Adjust the width of the "Method" column
        \cline{1-3}
        Method & Convergence time (iter.) & Natural configuration (succ.) \\ \hline
        s-IBVS           & $\mathbf{213.5 \pm 124.9}$  & $13/20$ \\ 
        pp-IBVS          & $361.7 \pm 70.5$  & $\mathbf{20/20}$    \\ \hline
       
    \end{tabular}
    \label{table:vservo_quantitative}
    \vspace{-0.4cm}
\end{table}
    
\begin{table*}[ht]
    \centering
    \vspace{+0.3cm}
    \renewcommand{\arraystretch}{1.2} % Increase row height
    \setlength{\tabcolsep}{5pt} % Increase space between columns
    \captionsetup{justification=centering}
    \caption{Bounding Box and Segmentation Results on the Synthetic and Real Sets} 
    \begin{tabular}{p{3.35cm}?ccc|ccc?ccc|ccc?c} % Adjust the width of the "Method" column
        \cline{1-14}
        \multirow{3}{*}{\textbf{Method}} & \multicolumn{6}{c?}{\textbf{Bounding box}} & \multicolumn{6}{c?}{\textbf{Mask}} & \multirow{3}{*}{\textbf{Inf. time (ms)}} \\
        \cline{2-13}
         & \multicolumn{3}{c|}{\textbf{Hemisphere val. set}} & \multicolumn{3}{c?}{\textbf{iHannes test set}} & \multicolumn{3}{c|}{\textbf{Hemisphere val. set}} & \multicolumn{3}{c?}{\textbf{iHannes test set}} \\ 
        \cline{2-13}
        & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ \\ \hline

        Mask R-CNN~\cite{he2017} & \textbf{76.2} & 94.9 & 85.1 & 37.7 & 68.8 & 38.0 & \textbf{67.0} & 90.4 & \textbf{75.8} & 23.1 & 50.6 & 18.0 & \textbf{41.0} \\       % maskrcnn iter 1799
        ViTDet~\cite{li2022} & 73.3 & \textbf{96.0} & 82.9 & 43.7 & 78.7 & 43.6 & 64.1 & 91.3 & 72.6 & 29.1 & 62.0 & 24.0 & 128.2 \\ \hline        % vitdet iter 1349

        \hline
        
        DINOv2Det (Ours) & 76.1 & 95.6 & \textbf{85.2} & \textbf{55.7} & \textbf{91.4} & \textbf{60.7} & 66.9 & \textbf{91.7} & 75.2 & \textbf{36.9} & \textbf{73.8} & \textbf{32.9} & 54.6 \\  \hline     % dinov2p16_scale1_freeze_unfreezeall_fromckpt 2137
        
    \end{tabular}
    \label{table:res_origtext}
\end{table*}
\begin{comment}
\begin{table*}[ht]
    \centering
    \renewcommand{\arraystretch}{1.2} % Increase row height
    \setlength{\tabcolsep}{5pt} % Increase space between columns
    \caption{Ablation Study on Different Training Strategies for DINOv2Det} 
    \begin{tabular}{p{2.5cm}|c?ccc|ccc?ccc|ccc?c} % Adjust the width of the "Train. strategy" column
        \cline{1-15}
        \multirow{3}{*}{\textbf{Train. strategy}} & \multirow{3}{*}{\textbf{SFP}} & \multicolumn{6}{c?}{\textbf{Bounding box}} & \multicolumn{6}{c?}{\textbf{Mask}} & \multirow{3}{*}{\textbf{Inf. time (ms)}} \\
        \cline{3-14}
         & & \multicolumn{3}{c|}{\textbf{Hemisphere val. set}} & \multicolumn{3}{c?}{\textbf{iHannes test set}} & \multicolumn{3}{c|}{\textbf{Hemisphere val. set}} & \multicolumn{3}{c?}{\textbf{iHannes test set}} \\ 
        \cline{3-14}
        & & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ \\ \hline

        Heads & \multirow{3}{*}{Yes} & 60.7 & 90.6 & 66.9 & 48.0 & 87.1 & 48.0 & 49.9 & 82.8 & 52.0 & 30.2 & 66.8 & 22.5 &  \multirow{3}{*}{86.9} \\       % dinov2p16_freeze_v3 iter 1687
        Heads + 4 last layers &  & 73.4 & 95.3 & 83.6 & 55.1 & 91.6 & 59.2 & 62.7 & 90.2 & 71.2 & 35.7 & 73.5 & 30.5 &  \\         % dinov2p16_freeze_v3_unfreeze4_fromckpt iter 1574
        Heads + all layers & & \textbf{76.1} & \textbf{96.3} & \textbf{85.8} & 54.2 & 91.0 & 57.9 & 66.2 & \textbf{92.0} & \textbf{75.2} & 35.5 & 72.8 & 31.0 &  \\ \hline    % dinov2p16_freeze_v3_unfreezeall_fromckpt iter 1574

        Heads & \multirow{3}{*}{No} & 61.0 & 89.6 & 67.9 & 48.7 & 86.3 & 50.0 & 51.9 & 82.9 & 56.0 & 31.5 & 67.1 & 25.7 & \multirow{3}{*}{\textbf{54.6}} \\         % dinov2p16_scale1_freeze iter 1687
        Heads + 4 last layers &  & 72.5 & 94.1 & 81.9 & \textbf{55.8} & \textbf{91.7} & 60.5 & 63.0 & 89.6 & 70.8 & \textbf{36.9} & \textbf{73.8} & 32.2 &  \\         % dinov2p16_scale1_freeze_unfreeze4_fromckpt iter 2250
        Heads + all layers & & \textbf{76.1} & 95.6 & 85.2 & 55.7 & 91.4 & \textbf{60.7} & \textbf{66.9} & 91.7 & \textbf{75.2} & \textbf{36.9} & \textbf{73.8} & \textbf{32.9} & \\   \hline      % dinov2p16_scale1_freeze_unfreezeall_fromckpt 2137

        
    \end{tabular}
    \label{table:res_abl}
    \vspace{-0.5cm}
\end{table*}
\end{comment}
%Finally, note that while we focus on the WFE and WPS, the proposed control system can be easily expanded to different scenarios, e.g., by including the elbow in the kinematic chain, or by controlling the fingers during the \textit{grasping} phase to assist the user. We will release our code to promote future research in the field.

\begin{table*}[ht]
    \centering
    \vspace{+0.1cm}
    \renewcommand{\arraystretch}{1.2} % Increase row height
    \setlength{\tabcolsep}{5pt} % Increase space between columns
    \captionsetup{justification=centering}
    \caption{Texture Generalization Results When Training on Random Textures and Testing on Unseen Textures}
    \begin{tabular}{p{3.35cm}?ccc|ccc?ccc|ccc?c} % Adjust the width of the "Method" column
        \cline{1-14}
        \multirow{3}{*}{\textbf{Method}} & \multicolumn{6}{c?}{\textbf{Bounding box}} & \multicolumn{6}{c?}{\textbf{Mask}} & \multirow{3}{*}{\textbf{Inf. time (ms)}} \\
        \cline{2-13}
         & \multicolumn{3}{c|}{\textbf{Hemisphere val. set}} & \multicolumn{3}{c?}{\textbf{iHannes test set}} & \multicolumn{3}{c|}{\textbf{Hemisphere val. set}} & \multicolumn{3}{c?}{\textbf{iHannes test set}} \\ 
        \cline{2-13}
        & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ & AP & AP$_{50}$ & AP$_{75}$ \\ \hline

        Mask R-CNN~\cite{he2017} & 62.0 & 87.0 & 69.6 & 19.1 & 38.1 & 17.0 & 50.9 & 78.3 & 55.2 & 11.7 & 28.5 & 7.5 & \textbf{41.0} \\     % maskrcnn_randtext iter 1199  
        ViTDet~\cite{li2022} & 65.0 & 92.3 & 72.8 & 34.4 & 69.3 & 30.2 & 54.9 & 86.1 & 59.2 & 20.0 & 49.0 & 13.1 & 128.2 \\ \hline      % vitdet_randtext iter 3000 

        \hline
        
        DINOv2Det (Ours) & \textbf{70.1} & \textbf{93.2} & \textbf{79.2} & \textbf{53.1} & \textbf{89.0} & \textbf{57.3} & \textbf{60.5} & \textbf{88.7} & \textbf{67.4} & \textbf{34.9} & \textbf{70.7} & \textbf{29.9} & 54.6 \\  \hline     % dinov2p16_scale1_freeze_fromckpt_randtext iter 2250 
        
    \end{tabular}
    \label{table:res_randtext}
    \vspace{-0.3cm}
\end{table*}

\subsection{Object parts segmentation}
\label{sec:exp_object_parts_segmentation}
The proposed DINOv2Det network is compared with Mask R-CNN~\cite{he2017} and ViTDet~\cite{li2022}. For the backbone, we use the smallest DINOv2 model size available, for a fast inference. Instead, Mask R-CNN and ViTDet are based on a ResNet-50 FPN~\cite{lin2017} and a ViT-Base, respectively. The input image size is 640x480px but this is not suitable for DINOv2 since it uses a patch size of 14. Hence, we interpolate the pre-trained weights of the patch embedding filters from 14x14x3 to 16x16x3. We rely on the Detectron2 framework to implement DINOv2Det and to train our models. Mask R-CNN and ViTDet are initialized with the weights provided within the framework. ViTDet is fine-tuned end-to-end while in Mask R-CNN the first two stages of the ResNet-50 are freezed. Regarding DINOv2Det, we first trained from scratch the RPN and RoI heads while keeping DINOv2 (i.e., the backbone) freezed, then fine-tuned the whole network. 
%both in terms of training strategy and backbone structure. Meanwhile, we use here the best performing DINOv2Det variant.
%Regarding DINOv2Det, the weights of the backbone are loaded from the official DINOv2 repository while the Region Proposal Network (RPN) and the detection heads are initialized from scratch. 
Each model is trained on the synthetic dataset for 22500 iterations 
%using AdamW~\cite{loshchilov2018} and a batch size of 16. 
and we validate it every 1125 iterations on the synthetic validation set presented in Sec.~\ref{sec:synthetic_dataset_generation}. When the training ends, the best performing checkpoint is evaluated on the \textit{iHannes} test set. To assess models performance, we consider the Average Precision (AP) of both the masks and bounding boxes, computed at different Intersection over Union (IoU) of the ground truth with the prediction. Specifically, we use the AP$_{50}$ and AP$_{75}$ with IoU set to 50 and 75, respectively, and the standard COCO AP obtained by averaging the AP over multiple IoUs.
% the standard COCO AP, i.e., the average over multiple AP$_{IoU}$ having different Intersection over Union (IoU) thresholds; (ii) AP$_{50}$ (IoU=50) and (iii) AP$_{75}$ (IoU=75). 
% When the training ends, the best performing checkpoint on the mask AP of the validation set is evaluated on the iHannes test set. 
The results and the inference time are shown in Tab.~\ref{table:res_origtext}. Interestingly, Mask R-CNN and DINOv2Det obtain comparable performance on the synthetic validation set. However, DINOv2Det achieves higher results on the \textit{iHannes} test set (+18.0 on the bounding box AP and +13.8 on the Mask AP), proving the effectiveness of adopting DINOv2 features for instance segmentation in zero-shot sim-to-real transfer. The cross-dataset generalization capability of self-supervised features is further confirmed by comparing Mask R-CNN with ViTDet. Indeed, while the former can better fit the training data, the latter retains more general features, resulting in higher performance when moving from the synthetic to the real domain. Finally, we show the AP$_{50}$ and AP$_{75}$ to highlight that the models are generally able to coarsely identify the object parts in the image (e.g., 73.8 for the Mask AP$_{50}$ of DINOv2Det). The main challenge is to precisely draw the mask of the object part, which for some objects is ambiguous since no clear object part boundaries exist, resulting in a performance drop (e.g, from 73.8 for the Mask AP$_{50}$ to 32.9 for the Mask AP$_{75}$ of DINOv2Det).
\begin{comment}
\noindent\textbf{Ablation on DINOv2Det} Since DINOv2 is a recently proposed method, it is not well-known how to fine-tune it, especially when applied to instance segmentation as downstream task. In this matter, we investigate both different fine-tuning strategies and whether the Simple Feature Pyramid (SFP) introduced in~\cite{li2022} is still useful or not. We consider three training strategies: (i) \textit{Heads}, the backbone is freezed while the RPN and the RoI heads are trained; (ii) \textit{Heads + 4 last layers}, also the 4 last layers (out of 12) of the backbone are trained; (iii) \textit{Heads + all layers}, the DINOv2Det is trained end-to-end. For the first training strategy, the weights of the backbone are loaded from the official DINOv2 repository while the RPN and the RoI heads are randomly initialized. 
The weights of the best performing checkpoint on the validation set are used as initialization for the other two training strategies, since it led to slightly better results than randomly initializing the heads. As shown in Table~\ref{table:res_abl}, differently from~\cite{li2022}, there is no improvement in using the SFP, this may be due to DINOv2 being already powerful enough to capture the object parts at the given scales. Moreover, the last two training strategies clearly outperforms the first one, both on the synthetic and real data. Finally, qualitative results are available in the supplementary video.
\end{comment}

\begin{comment}
\begin{figure}
    %\vspace{-0.2cm}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/randtext-removebg.png}
    \caption{The 15 random textures on \texttt{006\_mustard\_bottle}.
    }
    \label{fig:randtext}
    \vspace{-0.6cm}
\end{figure}
\end{comment}

\begin{comment}
\begin{figure*}
    \centering
    \vspace{+0.3cm}
    \includegraphics[width=1.0\linewidth]{Figures/phases.png}
    \caption{The phases of the prosthetic grasping pipeline along with the components involved and output visualizations.}
    \label{fig:phases}
    \vspace{-0.60cm}
\end{figure*}
\end{comment}

\noindent\textbf{Texture generalization}. 
We conduct an exploratory study on the generalization to unseen textures for each vision model. For this, we generate a synthetic training set following the procedure in Sec.~\ref{sec:synthetic_dataset_generation}, but applying random textures to objects. Prior to each capture, a texture is uniformly sampled from a set of 15 pre-defined textures and applied to the current object. We trained the models on this dataset and evaluate on the same synthetic validation set and \textit{iHannes} test set as the previous experiment (i.e., where the objects have their original texture). As shown in Tab.~\ref{table:res_randtext}, DINOv2Det exhibits strong generalization to the original (unseen) textures, reporting only a slight drop, i.e., from 36.9 to 34.9 for the Mask AP. This suggests that our model uses the object's shape instead of the texture to classify the parts. We believe this characteristic to be crucial for future works targeting generalization to unseen objects.
