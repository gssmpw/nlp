\begin{comment}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/obj_parts.png}
    \caption{The 15 YCB objects with labeled object parts. The red and green masks encode the \textit{top grasp} and \textit{side grasp}, respectively. The non-highlighted object parts are labeled as \textit{no grasp}.  }
    \vspace{-0.50cm}
    \label{fig:obj_parts}
\end{figure}
\end{comment}
%Recent works~\cite{vasile2022, starke2022} have shown promising results in using a vision-based \textit{eye-in-hand} prosthetic arm. The system predicts a one-shot grasp configuration for the target object to grab. 
%However, \cite{vasile2022} does not consider that the prosthesis, driven by the user, may approach the object with unpredictable trajectories. This might be a problem for \textit{eye-in-hand} configurations, since the camera may lose the view on the object.
%Conversely, even if the object is visible, there is no ad-hoc mechanism to ensure an optimal view on it during the approach, resulting in a possibly wrong grasp prediction. On the other hand, in~\cite{starke2022}, the user is requested to point the prosthesis at the object and take a picture before grasping, thus preventing a human-like approach. Ideally, they should naturally approach the object while the hand points at it throughout the movement.

We propose an \textit{eye-in-hand} vision-based pipeline to control the wrist of a prosthetic hand using a camera embedded into the palm. The framework is presented in Sec.~\ref{sec:shared_autonomy_prosthetic_grasping}, and its components are detailed in Sec.~\ref{sec:visual_servoing_control} and Sec.~\ref{sec:object_parts_segmentation}.

%We propose an \textit{eye-in-hand} vision-based prosthetic wrist control pipeline. Our system uses the input image from the camera embedded into the prosthesis palm for a twofold purpose. Firstly, we continuously adapt the wrist during the approach, with visual servoing, to keep the object at the image center. Then, we predict the final wrist configuration based on the object part that the user is about to grasp, right before the hand closure. This allows for a more natural approach to grasp, since the palm is always directed toward the object. Moreover, keeping the object in the image center ensures an optimal view, resulting in better grasp predictions. 
%We propose a vision-based wrist control pipeline for an \textit{eye-in-hand} prosthetic arm. Even though this configuration has achieved promising results compared to an \textit{egocentric} camera placement~\cite{vasile2022}, when considering an unconstrained scenario, it comes with the drawback of a narrower field of view on the scene. Indeed, since the prosthetic hand is driven by the user, they may approach the object with unpredictable trajectories. Thus, the camera embedded into the prosthesis' palm may lose the view on the object. 
%To overcome this issue, we introduce a visual servoing to keep the object in the image center by continuously controlling the wrist. The benefit of such approach is twofold. Firstly, keeping the object in the image center ensures an optimal view on it, resulting in better grasp prediction. Secondly, a continuous wrist adaptation would result in a more human-like approach to grasp.
%Since the user moves the prosthesis while the pipeline controls the wrist, the proposed system falls in the \textit{shared-autonomy} framework.
%, there is a tight coupling between the parts. Conseque.ntly, we formalize such user-machine collaboration in Sec.~\ref{sec:shared_autonomy_prosthetic_grasping}. Then, the components of the pipeline are presented in Sec.~\ref{sec:visual_servoing_control} and Sec.~\ref{sec:object_parts_segmentation}
 

\subsection{Shared-autonomy prosthetic grasping}
\label{sec:shared_autonomy_prosthetic_grasping}
%Since the user moves the prosthesis while the pipeline controls the wrist, the proposed system falls in the \textit{shared-autonomy} framework.

A reach-to-grasp task can be split into three different phases: \textit{transport}, \textit{rotation} and \textit{grasping} (or \textit{manipulation})~\cite{gentilucci1991}. During the \textit{transport} stage, the user moves the hand toward the target object. Right before the hand closure, the wrist should be aligned according to the object part to grasp (\textit{rotation} step). Finally, in the \textit{grasping} phase, the fingers close around the object. 
%Commercial prostheses rely solely on EMG signals as source of input and the SSC is a widely adopted solution~\cite{chen2023, geethanjali2016} to control the DoFs. However, the switching paradigm of SSC inhibits a human-like approach-to-grasp action, since no smooth transitioning between the \textit{transport}-\textit{rotation}-\textit{grasping} phases is possible. Therefore, we leverage the shared-autonomy framework~\cite{gardner2020} to mitigate this issue and to reduce the cognitive burden on the user.
%Commercial prostheses use the SSC paradigm to execute these three phases. However, as explained in Sec.~\ref{sec:related_work}, this leads to a cumbersome control. 

The proposed work aims to closely follow the above phases in order to enable a natural grasping approach. We delegate the \textit{transport} and \textit{rotation} phases to an automatic vision system while leaving only the final action (i.e., \textit{grasping} phase) to the user, thus reducing the cognitive load. 
Specifically, throughout the \textit{transport} phase, given an input image, an object parts segmentation network locates the object, then a visual servoing control scheme drives the wrist to keep the object in the image center. As a result, the wrist is continuously adjusted in accordance with the user's movements around the object. However, it might end up in a suboptimal configuration for grasping. Therefore, when the user triggers the \textit{rotation} phase through an explicit EMG signal, the visual servoing stops running and the wrist is instantaneously configured for grasping using a prediction based on object parts. Moreover, note that since the visual servoing kept the object in the field of vision, an optimal view is ensured, resulting in better grasp prediction. Finally, the control is left to the user who uses the EMG signals to drive the fingers opening and closing to grasp the object (i.e., \textit{grasping} phase). We refer to Fig.~\ref{fig:phases} for a detailed illustration.
%Specifically, during the \textit{transport} phase, the user drives the prosthetic hand while a visual servoing system continuously makes the palm pointing at the target object. In the \textit{rotation} phase, the user explicitly stops the visual servoing and the system predicts the final wrist configuration that is immediately executed by the joints. Finally, the control is left to the user who uses the EMG signals to adjust the fingers opening and closing to grasp the object (i.e., \textit{grasping} phase). 
The presented framework, based on a \textit{shared-autonomy} paradigm, relieves the user from complex mode switching and control of multiple DoFs. 
%Moreover, as the user is solely requested for one switch signal and a single DoF control, only two surface EMG (sEMG) electrodes are needed. 

\subsection{Visual servoing control}
\label{sec:visual_servoing_control}
In this section, we first revise the traditional visual servoing control. 
%Then, we present our ad-hoc solution specifically devised for the prosthetic setting.
Then, we discuss how the prosthetic setting differs from the standard robotic setup. We leverage such argument in Sec.~\ref{sec:visual_servoing_simulation} to design an ad hoc control method for the Hannes~\cite{laffranchi2020hannes} prosthetic arm.

\noindent{\textbf{Visual servoing background}.} A classical visual servoing control system minimizes the error function $\mathbf{e}(t) = \mathbf{s}(t) - \mathbf{s}^*$,
%\begin{equation}  \label{eqn:error}
%\mathbf{e}(t) = \mathbf{s}(t) - \mathbf{s}^*
%\end{equation}
being $\mathbf{s}(t)$ and $\mathbf{s}^*$ the current and target visual features, respectively. We design a velocity controller in the joint space~\cite{chaumette2007} for an \textit{eye-in-hand} system. 
Hence, the control law is:

\vspace{-0.2cm}
\begin{equation}  \label{eqn:control}
\dot{\mathbf{q}} = - \lambda \: (\mathbf{L}_s \: ^c\mathbf{V}_e \: ^e\mathbf{J}_e(\mathbf{q}))^+ \: (\mathbf{s}(t) - \mathbf{s}^*)
\end{equation}

where $\mathbf{q} \in \mathbb{R}^n$ encodes the position for $n$ robot joints, $^e\mathbf{J}_e(\mathbf{q}) \in \mathbb{R}^{6\times n}$ is the robot \textit{Jacobian} expressed in the end-effector frame, $^c\mathbf{V}_e \in \mathbb{R}^{6\times6}$ is the \textit{spatial motion transform matrix}~\cite{chaumette2006} to transform velocities from the end effector to the camera frame, $\mathbf{L}_s \in \mathbb{R}^{k\times6}$ is the \textit{interaction matrix}~\cite{chaumette2006}, $\mathbf{L}^+$ is the Moore-Penrose pseudo-inverse of a matrix $\mathbf{L}$ and $\lambda$ is the visual servoing gain. The features $\mathbf{s}(t)$, $\mathbf{s}^*$ and the \textit{interaction matrix} $\mathbf{L}_s$ depend on the chosen visual servo scheme.
We adopt the Image Based Visual Servoing (IBVS) due to its robustness under imprecise measurements~\cite{chaumette2006, corke2023}. %Specifically, since our objective is to keep the object in the image center, we select the object centroid and the image center as current and target coordinates, respectively. Consequently, the IBVS will drive the robot joints $\mathbf{q}$ in order to center the object in the image. We refer to this scheme as \textit{standard} IBVS (s-IBVS).
Finally, given an input image, the IBVS will iteratively drive the robot joints $\mathbf{q}$ to align the current feature to the target. We refer to this scheme as \textit{standard} IBVS (s-IBVS).

\begin{figure}
    \centering
    \vspace{+0.30cm}
    \includegraphics[width=1.0\linewidth]{Figures/ps_fe_angles.png}
    \caption{The natural wrist motion (a-b) and a non-natural motion (c) as the user drives the arm around the object.}
    \vspace{-0.60cm}
    \label{fig:ps_fe_angles}
\end{figure}
\newcolumntype{?}{!{\vrule width 1.5pt}}

\noindent{\textbf{Visual servoing for prosthesis control}.} While in a classical scenario the robot base is fixed and the control system drives the joints to bring the end-effector to the target, 
%here the robot base (i.e., the prosthetic hand) is driven by the user 
here the user moves the prosthetic hand while the robot joints (i.e., the prosthesis wrist) should perform compensatory motions to keep the object in the field of view. In such condition, given the strict coupling between the user movements and the control scheme, it is crucial to generate joint motions that are compliant with the user intentions (see Fig.~\ref{fig:ps_fe_angles}a-b). 
%As discussed in Sec.~\ref{sec:visual_servoing_simulation}, the s-IBVS might generate trajectories that are uncomfortable for the user. 
In this regard, as we will discuss in Sec.~\ref{sec:visual_servoing_simulation}, the s-IBVS might generate non-natural trajectories (see Fig.~\ref{fig:ps_fe_angles}c). This is mitigated by applying a similar idea to Partitioned IBVS~\cite{corke2023}: being $j$ the robot joint causing these trajectories, we remove it from s-IBVS and apply a separate control law for it. The objective is to obtain a natural motion while ensuring convergence. Thus, for this joint, a simple proportional control can be applied. The details are specified in Sec.~\ref{sec:visual_servoing_simulation} as they depend on the desired motion. Instead, the other joints are still controlled using the s-IBVS, thus $\mathbf{q} \in \mathbb{R}^{n-1}$ and $^e\mathbf{J}_e(\mathbf{q}) \in \mathbb{R}^{6\times {(n-1)}}$ in Eq.~\eqref{eqn:control}. We call this method \textit{proportional} and \textit{partitioned} IBVS (pp-IBVS).

\begin{comment}
Thus, being $j$ the robot joint causing this trajectory, we follow a similar idea to Partitioned IBVS~\cite{corke2023}, i.e., remove $j$ from the IBVS and apply a separate control law for it. Our objective is to mitigate the trajectory issue by forcing the sign of motion for $j$. Hence, for this joint, a simple proportional control law is applied:
%force the sign of motion for $j$ such that 
%depending on the position of the object with respect to the image center (i.e., object being in the left or right side of the image). Hence, for this joint, a simple proportional control law is applied:
\begin{equation}   \label{eqn:ps}
%\dot{q_j} = sign \: \lambda_j \: | \mathbf{s} - \mathbf{s}^* |
%\dot{q_j} = sign \: \lambda_j \: | \mathbf{s_x} - \mathbf{s_x}^* |
%\dot{q_j} = sign \: \lambda_j \: | x_c - x_o |
\dot{q_j} = sign \: \lambda_j \: | \mathbf{e}_j |
\end{equation}
where $\lambda_j$ is the proportional gain, $sign$ is either -1 or +1 depending on the current value of a certain image feature and $\mathbf{e}_j$ is the error between the current and target image feature (these will be chosen depending on the problem, see Sec.~\ref{sec:visual_servoing_simulation}). In summary, an IBVS is used for all the robot joints except $j$. Consequently, $\mathbf{q} \in \mathbb{R}^{n-1}$ and $^e\mathbf{J}_e(\mathbf{q}) \in \mathbb{R}^{6\times {(n-1)}}$ in Eq.~\eqref{eqn:control}. Meanwhile, the proportional control of Eq.~\eqref{eqn:ps} is applied to $j$. We call this method \textit{proportional} and \textit{partitioned} IBVS (pp-IBVS).
\end{comment}

%In summary, thanks to the separate control for the robot joint $j$ and forcing its sign of motion, the IBVS is no more affected by the joints configurations causing unpleasant trajectories for the user. We refer the reader to Sec.~\ref{sec:visual_servoing_simulation} and the accompanying video for a detailed analysis.

\subsection{Object parts segmentation}
\label{sec:object_parts_segmentation}
We use the information of the object location in the image in two parts of the pipeline presented in Sec.~\ref{sec:application}. Firstly, we use the object mask centroid as input for the visual servoing during the \textit{transport} phase. In addition, the segmented object parts are used to predict the final wrist configuration during the \textit{rotation} phase.
%As discussed in Eq.~\eqref{eqn:ibvs}, a pixel belonging to the object (e.g., the centroid of the object mask) is needed to run the visual servoing control during the \textit{transport} phase. In addition, the information of the object's appearance should be used to predict the final wrist configuration for the \textit{rotation} phase.
%Even though segmenting the object in the image seems to be the most straightforward solution, reasoning at object-level granularity may be a limitation. 
%Indeed, once the object is detected, either one grasp is assigned to it or a post-processing step is required to discriminate between the different grasp types for that object. 
Each object part is considered as a different instance and labeled with a grasp type (or \textit{no grasp} for non-graspable parts). Thereby, we propose to use an instance segmentation network. We remark that, differently from related work~\cite{starke2022}, the object parts are segmented based on the grasp types. We do not inject any object-specific information into our model as we strive for an object-agnostic pipeline by design. Even though the generalization to novel objects is not the focus of this work, we believe that an object-agnostic model would serve as a basis for such generalization, facilitating future research. 

\noindent{\textbf{DINOv2Det}.} For the proposed vision system, we leverage the recently introduced DINOv2~\cite{oquab2023} foundation model as a general-purpose backbone. Since DINOv2 is a Vision Transformer (ViT)~\cite{dosovitskiy2020}, it is not directly applicable for the instance segmentation task. Nevertheless,~\cite{li2022} proposed ViTDet, a network based on Mask R-CNN~\cite{he2017} structure but with a ViT as backbone. In addition, as opposed to the well-established Feature Pyramid solution, ViTDet only uses the last feature map of ViT to produce multi-scale feature maps. Then, the pipeline follows the standard Mask R-CNN: the multi-scale feature maps are used as input for the Region Proposal Network (RPN) and the Region of Interest (RoI) heads. The architecture obtains remarkable results when ViT is pre-trained in a self-supervised fashion. 
These findings support the strategy of using pre-trained ViTs as general purpose backbones, with minimal adaptation for the downstream tasks. Therefore, we push this idea forward by replacing the original ViT backbone of ViTDet with DINOv2. 
We refer to this novel architecture as DINOv2Det.
%To the best of our knowledge, our model DINOv2Det is the first work adapting DINOv2 features for the instance segmentation task. 
Notice that despite DINOv2 features have shown great capabilities in clustering semantic parts (e.g., differentiate between the legs and the body of an animal)~\cite{oquab2023}, our object parts do not necessarily have such distinction. 
%For instance, while the handle of the \texttt{019\_pitcher\_base} clearly has a semantic meaning, the same does not apply for the \textit{side grasps} of the \texttt{006\_mustard\_bottle} or the \textit{top grasps} of the \texttt{010\_banana}. 
Indeed, in some cases, no exact boundaries in terms of object shape or texture can be drawn (e.g., see the \texttt{010\_banana} in Fig.~\ref{fig:obj_parts}). We discuss it in details in Sec.~\ref{sec:exp_object_parts_segmentation}. 

%Finally, the segmented objects parts can be used to guide the prosthetic grasping pipeline. We discuss further details about this in Sec.~\ref{sec:application}.
%In conclusion, the object parts segmentation network is responsible for segmenting the object into parts. We will discuss in Sec.~\ref{sec:application_hannes_prosthesis} how this information is used in the proposed prosthetic grasping pipeline.

