\section{Ablation Study}

In this section, we analyze the impact of various components on the performance of our model.

\subsection{Effect of the Loss Function}

We conducted experiments to evaluate the contribution of different loss components in our model. As shown in Table~\ref{tab:loss}, we analyzed three configurations: using only reconstruction loss, only classification loss, and combining both losses. The results demonstrate that while reconstruction loss alone achieves reasonable performance (81.30\%), classification loss by itself underperforms (69.80\%). However, combining both losses yields the best results (82.40\%), suggesting that the joint optimization of reconstruction and classification objectives leads to more robust feature learning and better overall performance.

\begin{table}[htbp]
    \centering
    \caption{Accuracy for different loss configurations}
    \begin{tabular}{ccc}
        \toprule
        Reconstruction Loss & Classification Loss & Accuracy (\%) \\
        \midrule
        \ding{51} &     & 81.30 \\
                 & \ding{51} & 69.80 \\
        \ding{51} & \ding{51} & 82.40 \\
        \bottomrule
    \end{tabular}
    \label{tab:loss}
\end{table}

\subsection{Effect of Auxiliary Loss Weights}

We conducted experiments to analyze the impact of auxiliary loss weights on model performance, as shown in Table~\ref{tab:weights_accuracy}. The results demonstrate that the model's accuracy varies with different weight values, with 0.10 achieving the optimal performance at 82.4\%. We observed a clear trend where very high weights (1.0) significantly degraded performance to 79.60\%, while moderate weights (0.25, 0.50) maintained consistent performance at 82.0\%. Lower weights (0.01, 0.05) showed slightly reduced performance at 81.3\% and 81.5\% respectively, indicating that the auxiliary loss weight needs to be carefully tuned to achieve the best balance between learning objectives.

\begin{table}[htbp]
    \centering
    \caption{Accuracy for different auxiliary loss weights}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccccc}
        \toprule
        Aux. loss weights & 0.01 & 0.05 & 0.10 & 0.25 & 0.50 & 1.0 \\
        \midrule
        Accuracy (\%) & 81.3 & 81.5 & 82.4 & 82.0 & 82.0 & 79.60 \\
        \bottomrule
    \end{tabular}%
    }
    \label{tab:weights_accuracy}
\end{table}

\subsection{Combination of Losses}

We investigated the effect of different weight combinations between reconstruction and auxiliary losses, as shown in Table~\ref{tab:weights_comb}. The results reveal that balancing these two losses is crucial for optimal performance. Equal weighting (0.5, 0.5) yields suboptimal performance at 78.80\%, while gradually increasing the reconstruction loss weight and decreasing the auxiliary loss weight shows steady improvement. The optimal combination is achieved with reconstruction loss weight of 1.0 and auxiliary loss weight of 0.10, resulting in 82.40\% accuracy. Further increasing the reconstruction loss weight to 1.2 while reducing auxiliary loss weight to 0.05 leads to a slight performance degradation (82.0\%), suggesting that maintaining a proper balance between these losses is essential for optimal model performance.

\begin{table}[htbp]
    \centering
    \caption{Accuracy for different combination of reconstruction and auxiliary loss weights}
    \label{tab:weights_comb}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccccc}
        \toprule
        Rec. loss Weights & 0.5 & 0.75 & 0.80 & 0.90 & 1.0 & 1.2 \\
        \midrule
        Aux. loss weights & 0.5 & 0.25 & 0.10 & 0.10 & 0.10 & 0.05 \\
        \midrule
        Accuracy (\%) & 78.80 & 80.50 & 81.60 & 82.0 & 82.40 & 82.0 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Effect of Number of MLP Hidden Dimensions}

We examined the impact of varying MLP hidden dimensions on model performance, with results presented in Table~\ref{tab:mlp_accuracy}. The experiments demonstrate that model performance improves as the hidden dimension size increases from 256 to 768, reaching peak accuracy at 768 dimensions (82.40\%). Further increases to 1024 maintain this performance level, with a marginal improvement to 82.50\% at 1280 dimensions. This suggests that while larger hidden dimensions can capture more complex features, the benefits plateau around 768-1024 dimensions, indicating this range as the optimal choice for balancing model capacity and computational efficiency.

\begin{table}[htbp]
    \centering
    \caption{Accuracy results for different MLP hidden dimensions}
    \label{tab:mlp_accuracy}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccccc}
        \toprule
        MLP dimensions &  256 & 512 & 768 & 1024 & 1280 \\
        \midrule
        Accuracy (\%) & 81.00 & 81.60 & 82.40 & 82.40 & 82.50 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Effect of number of decoders}

We investigated the impact of varying the number of decoders on model performance, as shown in Table~\ref{tab:decoder_accuracy}. The results reveal a clear pattern where using a single decoder yields the lowest accuracy of 78.60\%. A substantial improvement is observed when increasing to 3-4 decoders, both achieving 81.60\%. Further enhancement is achieved with 8 decoders, reaching 82.40\%, which plateaus through 12 decoders. Adding more decoders up to 16 provides only marginal improvement (82.50\%). These results indicate that while multiple decoders are crucial for better feature learning, the performance gains saturate beyond 8 decoders, suggesting this as an optimal choice considering the computational efficiency trade-off.

\begin{table}[htbp]
    \centering
    \caption{Accuracy results for different decoder sizes}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccccc}
        \toprule
        No. of decoders & 1 & 3 & 4 & 8 & 12 & 16 \\
        \midrule
        Accuracy (\%) & 78.60 & 81.60 & 81.60 & 82.40 & 82.40 & 82.50 \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:decoder_accuracy}
\end{table}