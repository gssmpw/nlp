\section{Related Work}

\subsection{AMC in Wireless Communication}

Early AMC methods primarily relied on expert-crafted features extracted from constellation diagrams and signal statistics, utilizing likelihood-based approaches and higher-order statistics to classify modulation schemes \cite{dobre2007survey}. Constellation diagrams, which visually represent symbol distributions in the complex plane, have been a fundamental tool for feature extraction in classical AMC. These diagrams illustrate the effects of noise, interference, and channel distortions, making them valuable for differentiating modulation schemes based on clustering patterns, symbol dispersion, and trajectory structures \cite{o2018over}. 

The advent of deep learning revolutionized AMC by enabling models such as CNNs to operate directly on I/Q samples and constellation diagrams, bypassing the need for manual feature engineering \cite{zhang2021efficient}. Deep learning approaches leverage spatial patterns in constellation diagrams, learning robust representations that improve classification accuracy over conventional methods \cite{doan2020learning}. More recent advancements incorporate attention mechanisms and transformer architectures to capture long-range dependencies in signal patterns \cite{hamidi2021mcformer}. However, these deep learning-based AMC methods often require large labeled datasets and exhibit limited generalization across varying channel conditions, posing challenges for real-world deployment \cite{ya2022large}.

\subsection{Self-Supervised Learning Approaches}

The evolution of modern self-supervised learning strategies in deep learning and wireless communication has progressed through distinct phases, starting with contrastive learning, followed by representation learning, and more recently, the adoption of masking techniques.  Contrastive learning exemplified by SimCLR~\cite{chen2020simple} and MoCo \cite{he2020momentumcontrastunsupervisedvisual}, relied on distinguishing positive and negative sample pairs to learn robust feature representations, proving effective in image and signal processing tasks. This was followed by representation learning methods \cite{grill2020bootstraplatentnewapproach, berthelot2019mixmatch}, which eliminated the need for negative samples by aligning representations of augmented views, enhancing efficiency and scalability. More recently, masking-based techniques \cite{devlin2019bertpretrainingdeepbidirectional, he2021maskedautoencodersscalablevision} have gained prominence by reconstructing masked portions of inputs, offering a powerful framework for learning from incomplete data \cite{faysal2025denomae}.

In wireless communication, semi-supervised learning are particularly useful for modulation classification, where labeled data is often scarce due to the complexity of signal environments. Ma et al. \cite{ma2024refined} developed a semi-supervised framework leveraging pseudo-labeling, where a model trained on limited labeled modulation samples generates labels for unlabeled data, iteratively refining its performance across diverse modulation schemes. Another approach by Sifaou and Simeone \cite{sifaou2024semisupervisedlearningcrosspredictionpoweredinference} exploits cross-prediction-powered inference (CPPI) to enhance semi-supervised learning by mitigating the bias of synthetic labels. Their method refines ML-based predictions through tuned CPPI and meta-CPPI, achieving improved performance in tasks like beam alignment and indoor localization, particularly when labeled data is scarce. More related SSL methods in wireless communication includes convolution \cite{zhang2024sswsrnetsemisupervisedfewshotlearning, ermis2022cnn, pmlr-v77-longi17a} and transformer \cite{kong2023transformer, ren2022sigtefficientendtoendmimoofdm, kunde2023transformers} based SSLs.

