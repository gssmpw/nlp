\section{Methodology: DenoMAE2.0}

The overall framework of the proposed DenoMAE2.0 is illustrated in Figure \ref{fig:denomae2}. DenoMAE2.0 consists of three components: encoder, reconstruction denoising decoder, and local patch classification head. There are two complementary objectives: the denoising reconstruction objective and the local patch classification classification objective. Details are provided in Algorithm~\ref{lst:DenoMAE2.0} and introduced as follows.

\begin{lstlisting}[language=Python, caption={DenoMAE2.0, PyTorch-like pseudo code}, label={lst:DenoMAE2.0}]
# f_enc , f_dec: encoder , decoder
# f_mlp: patch classification head
# mask_r: mask ratio
# lambda_rec, lambda_class: loss function weights

for x, tgt in loader: # load a minibatch
    x = patch_emb(x) # embed patches
    x_v, x_m, mask_ids = masking(x, mask_r) # random split visible and masked patches
    q_v = f_enc(x_v) # local patch features
    logits = f_mlp(q_v) # predicted labels
    k_m = f_dec(q_v) # reconstructed pixels
    loss = lambda_rec * lambda_cls(k_m, x_m) + lambda_cls * cls_loss(logits, tgt)
    loss.backward()
    update() # optimizer update

def class_labels(mask_ids):
    classes = ((img_size//patch_size)**2) * (1 - mask_r)
    labels = classes[mask, where mask == 0] # 0: visible patches, 1: masked patches
    return labels

def rec_loss(k, x): # reconstruction
    x = norm_pix(x) # normalize every patch
    loss = (k - x) ** 2 # compute MSE loss over masked patches
    return loss

def cls_loss(logits, tgt): # classification
    loss = CrossEntropyLoss(logits, tgt)
    return loss
\end{lstlisting}

\subsection{Image Masking and Encoding}

The preprocessing stage transforms an input image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ into a sequence of non-overlapping patches $\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$, where $(H, W)$ defines the spatial dimensions, $C$ denotes the number of channels, and $(P, P)$ specifies the patch size. The total number of patches is $N = HW/P^2$. 

These patches undergo linear projection through a PatchEmbed layer to obtain initial embeddings. We adopt a masking strategy that randomly removes a substantial portion (75\%) of the patches. Let $\mathbf{x}_v$ and $\mathbf{x}_m$ represent the visible and masked patches respectively. The visible patches $\mathbf{x}_v$ are combined with learned positional embeddings and processed by a Vision Transformer (ViT) encoder to generate patch-level features $\mathbf{q}_v$. These features form the basis for downstream reconstruction and classification tasks.

\subsection{Decoder Architecture and Reconstruction}

Since the length of visible patch features $\mathbf{q}_v$ is smaller than the total number of image patches $N$, we augment $\mathbf{q}_v$ with mask tokens to construct a complete feature set. Each mask token is a shared, learnable vector that indicates positions requiring reconstruction. Following the encoding process, we enhance this complete feature set with positional embeddings before processing it through a transformer-based decoder. The decoder's output undergoes transformation through a linear projection layer (not depicted in Figure 1) to map features back to pixel space. 

The reconstruction objective $\mathcal{L}_{\text{rec}}$ employs mean squared error (MSE) between the reconstructed and original images. Consistent with established approaches, we compute this loss exclusively on the masked patches:

\begin{equation}
    \mathcal{L}_{\text{rec}} = \text{MSE}(\hat{\mathbf{x}}_m, \mathbf{x}_m)
\end{equation}

where $\hat{\mathbf{x}}_m$ represents the reconstructed patches and $\mathbf{x}_m$ denotes the original masked patches.


\subsection{Classification Branch}

The visible patch features $\mathbf{q}_v$ are additionally leveraged for classification. Diverging from traditional classification, where one input contains a single class, we assign visible patches to classes based on their spatial positions. The classification branch takes only the class tokens from the encoder to classify them. The total number of classes is the same as the number of visible patches which is calculated as:

\begin{equation}
    N_{\text{classes}} = \left(\frac{\text{img\_size}}{\text{patch\_size}}\right)^2 \cdot (1 - r_{\text{mask}})
\end{equation}

Since we obtain several classes from a single image, the classification branch is a multi-class classification problem. The classification head consists of a simple linear layer that projects the patch features directly to the number of classes. The classification loss $\mathcal{L}_{\text{cls}}$ employs cross-entropy (CE) between predicted and ground truth labels:

\begin{equation}
    \mathcal{L}_{\text{cls}} = \text{CE}(\mathbf{p}, \mathbf{y})
\end{equation}

where $\mathbf{p}$ denotes the predicted probabilities and $\mathbf{y}$ represents the ground truth labels.

\subsection{Overall Objective}

Our DenoMAE2.0 optimizes a combination of reconstruction and classification losses, enabling simultaneous learning of fine-grained local and global features. The overall loss function is formulated as a weighted sum:

\begin{equation}
    \mathcal{L} = \lambda_{\text{rec}}\mathcal{L}_{\text{rec}} + \lambda_{\text{cls}}\mathcal{L}_{\text{cls}}
\end{equation}

where $\lambda_{\text{rec}}$ and $\lambda_{\text{cls}}$ are balancing weights for the reconstruction and classification objectives, respectively. Through empirical validation (see Table~\ref{tab:weights_accuracy}), we set $\lambda_{\text{rec}} = 1.0$ and $\lambda_{\text{cls}} = 0.1$.

\subsection{Fine-Tuning on Downstream Tasks}

Following pre-training, the DenoMAE2.0 encoder is fine-tuned on specific recognition tasks to enhance performance. During this stage, the decoder and classification head are excluded, retaining only the encoder. For fine-tuning, the model utilizes the complete set of patches, corresponding to the uncorrupted input images, to optimize for downstream recognition tasks.
