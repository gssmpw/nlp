\section{Datasets}

\subsection{Constellation Diagram Generation}

Constellation diagrams offer a more informative approach to modulation classification compared to raw time-series signals, capturing richer detail essential for accurate interpretation \cite{doan2020learning}. The process of generating constellation diagrams for modulation signal classification involves multiple stages. We begin by mapping modulated signals onto a $7 \times 7$ complex plane, which provides sufficient space to capture signal samples while maintaining computational efficiency for SNR ranges of -10 dB to 10 dB. The basic constellation diagram is then enhanced through a multi-step process: first, as a gray image that handles varying pixel densities where multiple samples may occupy single pixels; second, as an enhanced grayscale image that employs an exponential decay model ($B_{i,j}$) to account for both the precise position of samples within pixels and their influence on neighboring pixels. This model considers the sample point's power ($P$), the distance between sample points and pixel centroids ($d_{i,j}$), and an exponential decay rate ($\alpha$) \cite{peng2018modulation}. To make the representation compatible with DenoMAE2.0, which expects RGB input, we generate a three-channel image by creating three distinct enhanced grayscale images from the same data samples, each utilizing different exponential decay rates. 

The pretraining dataset comprises 10,000 samples uniformly distributed across ten modulation types, with randomly assigned SNR values in the range of -10 dB to 10 dB. For the downstream classification task, we use 1000 samples for training and 100 samples for testing, evenly distributed among the 10 classes, evaluated at SNR values from -10 dB to 10 dB in 1 dB increments.

\subsection{Parameters in Sample Generation} %Dataset description add table

The dataset comprises signals from ten modulation formats (see appendix, Table~\ref{tab:cons_classes}, for detailed modulation types), each originally of length $L_0 = 1024$. To conform to the transformer's input dimensions, signals undergo a two-step preprocessing: (1) reshaping to $\mathbf{S}_1 \in \mathbb{R}^{32 \times 32}$, and (2) interpolation to $\mathbf{S}2 \in \mathbb{R}^{224 \times 224}$. For formats with $L_0 > 1024$ (e.g., $L_\text{GMSK} = 8196$), signals are initially downsampled to $L_0$. To match the three-channel structure of constellation images $\mathbf{I} \in \mathbb{R}^{3 \times 224 \times 224}$, $\mathbf{S}_2$ is replicated across three channels, yielding $\mathbf{S}_3 \in \mathbb{R}^{3 \times 224 \times 224}$. All signals are sampled at $f_s = 200$ kHz, ensuring consistency across modulation formats. 

\section{Experimental Results}

\subsection{Denoising Performance}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{images/recon.png}
    \caption{Reconstruction performance comparison between DenoMAE and DenoMAE2.0}
    \label{fig:recon}
\end{figure*}

During the pretraing DenoMAE2.0 learns to denoise and reconstruct the missing patches and thereby learn their inherent representation. Therefore, we evaluate this denoisng and reconstruction performance during and after the pretraining phase. The effectiveness of DenoMAE2.0 is evident in the reconstructed constellation diagrams shown in Figure \ref{fig:recon}. Compared to DenoMAE, DenoMAE2.0 achieves superior noise reduction while preserving key signal features crucial for modulation classification. Table \ref{tab:ssim_psnr_comparison} quantifies this improvement using two standard metrics: structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR). Interestingly, for the first image, DenoMAE achieved higher SNR and PSNR despite DenoMAE2.0 producing a visually superior reconstruction. However, for the remaining two samples, DenoMAE2.0 outperformed DenoMAE in both SSIM and PSNR.

\begin{table}[htbp]
    \centering
    \caption{Comparison of SSIM and PSNR for DenoMAE and DenoMAE2.0}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|c|c|c|c|}
        \hline
        Method & Image & DenoMAE & DenoMAE2.0 \\ \hline
        \multirow{3}{*}{SSIM} 
        & Image1 & 0.9660 & 0.9634 \\ 
        & Image2 & 0.9407 & 0.9607 \\ 
        & Image3 & 0.9422 & 0.9466 \\ \hline
        \multirow{3}{*}{PSNR (dB)} 
        & Image1 &  42.9054 & 42.3228 \\ 
        & Image2 & 42.0005 & 42.1061 \\ 
        & Image3 & 40.3067 & 40.7508 \\ \hline
    \end{tabular}
    \label{tab:ssim_psnr_comparison}
\end{table}

\subsection{Latent Space Visualization with TSNE}

In Figure~\ref{fig:tsne}, we visualize the latent representations of DenoMAE and DenoMAE2.0 using t-distributed stochastic neighbor embedding (t-SNE)~\cite{van2008visualizing}. The plots (left two) reveal that DenoMAE2.0 produces more accurate and distinct clusters than DenoMAE, indicating superior feature separation and representation learning. Specifically, in DenoMAE, classes 0, 3, and 9 contain more separated clusters within the same class. In contrast, DenoMAE2.0 exhibits more distinct clusters for these classes.

This enhanced clustering is particularly evident in the masked samples (right two plots), where we mask out a significant portion (75\%) of the images. Masking typically makes distinct cluster formation more challenging. However, DenoMAE2.0 achieves clearer boundaries between classes, suggesting improved robustness and generalization capabilities. In DenoMAE, classes 0, 3, and 9 achieve somewhat distinct clusters, with a few samples overlapping in other clusters. In DenoMAE2.0, classes 0, 3, 6, 7, 8, and 9 show better-separated clusters, indicating enhanced feature learning and representation.

However, in all the plots, classes 1 and 2 entirely overlap with each other, indicating that the model struggles to distinguish between these two classes. This is a common issue in modulation classification, as some classes are very similar to each other.

\begin{figure*}
    \centering
    \subfloat{%
       \includegraphics[trim=1cm 1cm 1cm 1cm,clip,
       width=0.25\linewidth]{images/denoMAE_no_mask.png}}
    \hfill
    \subfloat{%
        \includegraphics[trim=1cm 1cm 1cm 1cm,clip,
        width=0.25\linewidth]{images/denoMAE2_no_mask.png}}
    \hfill
    \subfloat{%
        \includegraphics[trim=1cm 1cm 1cm 1cm,clip,
        width=0.25\linewidth]{images/denoMAE_mask.png}}
    \hfill
    \subfloat{%
        \includegraphics[trim=1cm 1cm 1cm 1cm,clip,
        width=0.25\linewidth]{images/denoMAE2_mask.png}}
    \caption{Latent representation visualization using t-SNE. From left to right: (1) DenoMAE without masking, (2) DenoMAE2.0 without masking, (3) DenoMAE with 0.75\% masking, and (4) DenoMAE2.0 with 0.75\% masking.}
    \label{fig:tsne} 
\end{figure*}

\subsection{Finetuning Accuracy}

After pretraining, we transfer the learned representations to the downstream classification task and compare DenoMAE2.0 with state-of-the-art models. As shown in Table \ref{tab:example}, when finetuned on the downstream task, DenoMAE2.0 achieves the highest test accuracy of 82.40\% among all compared methods. This represents a significant improvement over the baseline ViT (79.90\%) and other representation learning approaches including DEiT (81.20\%), MoCov3 (81.00\%), BEiT (80.40\%), MAE (80.10\%), and the original DenoMAE (81.30\%). The consistent performance advantage demonstrates that DenoMAE2.0's enhanced denoising strategy learns more effective representations during pretraining that transfers well to the downstream classification task.

\begin{table}[htbp]
    \centering
    \caption{Downstream classification accuracy for different models}
    \resizebox{0.8\linewidth}{!}{%
    \begin{tabular}{|c|c|}
        \hline
        Method & Test accuracy (\%) \\ \hline
        ViT & 79.90 \\ \hline
        DEiT & 81.20 \\ \hline
        MoCov3 & 81.00 \\ \hline
        BEiT & 80.40 \\ \hline
        MAE &  80.10 \\ \hline
        DenoMAE &  81.30 \\ \hline
        DenoMAE2.0 &  \textbf{82.40} \\ \hline
    \end{tabular}%
    }
    \label{tab:example}
\end{table}

Figure~\ref{fig:confusion} presents a confusion matrix visualizing the per-class performance of DenoMAE2.0. The diagonal elements indicate correct classifications, while off-diagonal elements show misclassifications. The matrix reveals strong performance across most classes, though notable confusion exists between classes 1 and 2, aligning with observations from the t-SNE analysis. This confusion pattern suggests inherent similarities in the signal characteristics of these two modulation types that challenge even our enhanced model architecture.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{images/cm.png}
    \caption{Confusion matrix for DenoMAE2.0 downstream classification} 
    \label{fig:confusion}
\end{figure}

\subsection{Performance on Number of Epochs}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{images/finetune_epoch.png}
    \caption{Epoch finetuning.}
    \label{fig:epoch}
\end{figure}

Figure~\ref{fig:epoch} illustrates the finetuning performance across different epochs. The plot shows a steady improvement in model accuracy as the number of epochs increases, with DenoMAE2.0 consistently outperforming baseline methods. DenoMAE achieves higher accuracy than ViT continuously. On the other hand, although the gap between DenoMAE and ViT is larger than DenoMAE and DenoMAE2.0, DenoMAE2.0 always obtains better accuracy than DenoMAE.

\subsection{Performance Across Different SNRs}

The SNR performance plot demonstrates robust classification accuracy across various SNRs ranging from -10 dB to 10 dB. As the SNR values increases all the methods perform better and when the SNR is low the performance degrades. ViT, which has no pertaining phase, presumably obntains the lowest performance in all cases. MAE, which pretrains on modulation signals but has no denoising capability, obtains a slight higher accuracy than ViT. DenoMAE obtains higher accuracy than MAE in most cases however for SNE of 2 and 3 we observe that MAE obtains a higher accuracy. On the other hand, DenoMAE2.0 outperforms all the methods for all the SNRs.  Noteably, all the methods lose significant accuracy for snr lower than -2 dB. However, DenoMAE2.0 maintains comparatively robust performance down to -7 dB, demonstrating its enhanced resilience to noise.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/SNR_Accuracy_Plot.png}
    \caption{Accuracy comparison across different SNRs}
    \label{fig:snr_acc}
\end{figure}

\section{Comparison with Other AMC Methods}

Table~\ref{tab:compare} presents a comprehensive comparison between DenoMAE2.0 and other state-of-the-art modulation classification methods. AlexNet achieved 82.8\% accuracy with 8 classes at 2dB SNR using a large dataset of 800,000 samples. NMformer reported 71.6\% accuracy across 10 classes with SNR ranging from 0.5 to 4.5dB using 106,800 samples. CNN-AMC and DL-GRF showed relatively lower performance with 50.40\% and 59\% accuracy respectively, though they were tested under different conditions. The original DenoMAE achieved 81.3\% accuracy across 10 classes with SNR ranging from 0 to 10dB. Our proposed DenoMAE2.0 demonstrates superior performance with 82.4\% accuracy under similar conditions, using only 10,000 samples for pretraining and 1,000 samples for fine-tuning, indicating improved efficiency in both performance and data utilization.

\begin{table*}[htbp]
    \centering
    \caption{Comparison of DenoMAE2.0 with other modulation classification methods}
    \label{tab:compare}
    \begin{tabular}{cccccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{\textbf{Number of samples}} & \multirow{2}{*}{\textbf{SNR dB}} & \multirow{2}{*}{\textbf{Number of classes}} & \multirow{2}{*}{\textbf{Test accuracy (\%)}} \\ 
    \cmidrule(lr){2-3}
     & \textbf{Pretraining} & \textbf{Fine-tuning} & & \\ \midrule
    
    AlexNet \cite{peng2018modulation}  & 800,000 (train) & 8,000 (test)  & 2 & 8 & 82.8  \\
    NMformer \cite{faysal2024nmformer} & 106,800 & 3,000 & 0.5 - 4.5 & 10 & 71.6 \\
    CNN-AMC \cite{meng2018automatic} & 79,200 & 228,060 & -6 & 4 & 50.40 \\
    DL-GRF \cite{sun2022automatic} &  2,000 (train) & 200 (test) & 0 & 4 & 59 \\
    DenoMAE \cite{faysal2025denomae} & 10,000 & 1,000 & 0 - 10 & 10 & 81.3 \\
    DenoMAE2.0 (\textbf{Ours}) & 10,000 & 1,000 & 0 - 10 & 10 & 82.4 \\
    
    \bottomrule
    \end{tabular}
\end{table*}


\subsection{Transferability}

To show the transferability of DenoMAE2.0 on other similar task, we performed experiments on a similar benchmark dataset named RadioML. A brief description on the RadioML dataset is provided in the following subsection.

\subsubsection{RadioML Dataset}

RadioML 2018.01A~\cite{o2018over} is a large-scale dataset designed for AMC and machine learning-based RF signal processing. It consists of 2,555,904 IQ (in-phase and quadrature) signal samples, where each sample is represented as a 1024Ã—2 array, capturing both in-phase and quadrature components. The dataset spans 24 modulation types, including digital and analog schemes such as BPSK, QPSK, 8-PSK, 16-QAM, AM, and FM, making it a comprehensive resource for signal classification tasks. Signals are generated under 26 distinct signal-to-noise ratio (SNR) levels, ranging from -20 dB to +30 dB in 2 dB increments, simulating real-world conditions affected by fading, interference, and channel noise. Each modulation type and SNR combination contains 4096 samples, ensuring a balanced distribution for model training and evaluation. The dataset, widely used as a benchmark in wireless communication~\cite{ccamlibel2024automatic} and spectrum sensing research~\cite{elyousseph2021deep}, facilitates advancements in deep learning-based modulation recognition, interference mitigation, and cognitive radio applications~\cite{cheng2024automatic, jagatheesaperumal2024deep, 10014805}.

To keep a consistent same scale implementation, we used 100 samples in each class for the 24 classes in RadioML for training and 10 samples in each class for testing. Therefore, we used only 2400 samples for training and 240 samples for testing. As a result we only used 0.103\% of the total dataset. We only obtain SNRs of -20, -10, 0, 10, 20 dB to show performance across low and high SNRs, as well as interpolaribility. 

\subsubsection{Performance}

The transfer learning results are shown in Figure~\ref{fig:trans}. Performance improves with increasing SNR, with DenoMAE2.0 consistently outperforming baseline methods across all SNR values. At 20 dB SNR, DenoMAE2.0 achieves 33.75\% accuracy compared to 21.92\% for DenoMAE, representing an 11.83\% improvement. Similarly at 10 dB SNR, DenoMAE2.0 obtains 29.88\% accuracy versus 13.33\% for DenoMAE, a 16.55\% gain. For out-of-distribution SNRs between -10 dB and -20 dB, ViT performs at chance level (4.27\%) while DenoMAE shows minimal improvement. However, DenoMAE2.0 maintains significant accuracy of 7.08\% at -10 dB and 6.67\% at -20 dB, demonstrating enhanced transferability and robustness.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/trans.png}
    \caption{Transfer learning performance on RadioML dataset}
    \label{fig:trans}
\end{figure}

