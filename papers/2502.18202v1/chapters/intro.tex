\section{Introduction}

Semi-supervised models have emerged as a transformative paradigm in machine learning, addressing the high costs associated with data labeling \cite{zhang2024maskmatch, pmlr-v182-poulain22a}. A prevalent approach in semi-supervised learning involves pretraining models on large amounts of unlabeled data, which are abundant and readily available \cite{chen2020bigselfsupervisedmodelsstrong, xu2022revisitingpretrainingsemisupervisedlearning}. These models leverage unlabeled data to capture underlying patterns and structures, enabling effective representation learning. Consequently, the pretrained models exhibit improved performance and faster adaptation during downstream tasks, making semi-supervised learning a cost-efficient and scalable solution for real-world applications \cite{cai2022semisupervisedvisiontransformersscale}.

Recent advancements in self-supervised learning have achieved remarkable success through representation learning \cite{grill2020bootstraplatentnewapproach, chen2020exploringsimplesiameserepresentation}, contrastive learning \cite{chen2020simple, khosla2020supervised}, and masking strategies \cite{devlin2019bertpretrainingdeepbidirectional, yin2024stablemask}. Among these, masking strategies leveraging Vision Transformer (ViT) \cite{dosovitskiy2020image} architectures have set new benchmarks in tasks such as classification , reconstruction, and image analysis. Masked Autoencoders (MAE) \cite{he2021maskedautoencodersscalablevision}, in particular, have demonstrated significant potential by randomly masking portions of input data and training models to reconstruct the missing information. Despite their success, traditional MAE methods predominantly emphasize global reconstruction objectives, often neglecting fine-grained local patterns that are critical for enhancing representation learning \cite{yue2023understanding}.

Masking strategies are particularly beneficial for pretraining in wireless communication for autometic modulation classification (AMC), where acquiring labeled data is challenging due to privacy and copyright constraints \cite{zhao2024vit, zayat2023transformer}. Moreover, communication signals are often corrupted by environmental noise and channel losses, making robust representation learning essential. DenoMAE \cite{faysal2025denomae} addresses these challenges by integrating masking and denoising within a unified framework, leveraging masked modeling to enhance performance. While DenoMAE has demonstrated exceptional results in denoising and achieving strong classification accuracy, it inherits the fundamental principles of MAE, which focus primarily on global representation learning through masking. Consequently, the model remains unaware of the specific locations of the masked inputs, limiting its ability to fully exploit fine-grained local patterns and unlock the complete potential of representation learning.

We present DenoMAE2.0, an enhanced framework that introduces a novel local patch classification objective alongside the traditional reconstruction task. Our approach differs from existing methods by treating visible patches as distinct classes based on their spatial positions, enabling the model to learn position-aware local features while maintaining global coherence. This dual-objective strategy encourages the model to capture both structural and semantic information, leading to more robust and informative representations.
The key contributions of our work are threefold:

\begin{itemize}
    \item We propose a novel architecture that combines denoising reconstruction with local patch classification, enabling simultaneous learning of global and local features.
    \item We introduce a position-based classification strategy that leverages spatial information to enhance representation learning without requiring additional labels.
    \item Through extensive experiments, we demonstrate that DenoMAE2.0 achieves superior performance on various downstream and transfer learning tasks compared to traditional MAE and DenoMAE approaches, particularly in scenarios with limited labeled data.
\end{itemize}

Our results indicate that the integration of local patch classification significantly improves the quality of learned representations, leading to better denoising, generalization and faster convergence during fine-tuning. The proposed method not only advances the state-of-the-art in self-supervised learning but also provides insights into the importance of incorporating local structural information in representation learning frameworks.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/flow_con.png}
    \caption{An example figure.}
    \label{fig:denomae2}
\end{figure*}