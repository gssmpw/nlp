\section{Theoretical Analysis}

\subsection{Convergence Analysis}
We begin by examining the two-grid iteration under standard multigrid assumptions, emphasizing how our approach handles both high- and low-frequency error components. In particular, a smoothing operator \(S\) damps high-frequency errors, while coarse-grid correction addresses low-frequency modes. This two-part strategy yields a convergence rate that does not degrade with increasing problem size.

\begin{theorem}[Two-Grid Convergence]
\label{th:twogrid_convergence}
Let \(\mathbf{e}^{(k)}\) be the error at iteration \(k\) of a two-grid scheme for the SPD system \(A\mathbf{x}=\mathbf{b}\). Suppose the coarse correction satisfies the \emph{Approximation Property} and the smoothing step remains stable. Then there exists a constant \(\rho < 1\) such that
\[
    \|\mathbf{e}^{(k+1)}\|_{a}
    \;\le\;
    \rho\,\|\mathbf{e}^{(k)}\|_{a},
\]
where \(\|\cdot\|_{a}\) is the energy norm induced by \(A\). Consequently, the iteration converges at a rate independent of the system size \(n\).
\end{theorem}

\noindent
(See Appendix~\ref{appendix:proof_2} for proof.) A key ingredient is the coarse spaceâ€™s ability to capture smooth (low-frequency) errors. In classical multigrid, this is formalized by the \emph{Approximation Property}:

\begin{property}[Approximation Property]
\label{prop:approximation}
Let \(P\in\mathbb{R}^{n\times m}\) be the prolongation operator from the coarse space \(\mathbb{R}^m\) to the fine space \(\mathbb{R}^n\). For any error vector \(\mathbf{e}\in \mathbb{R}^n\), there exists a coarse representation \(\mathbf{z}^c \in \mathbb{R}^m\) such that
\[
    \min_{\mathbf{z}^c}
    \|\mathbf{e} - P\,\mathbf{z}^c\|_{a}
    \;\le\;
    \alpha\,\|\mathbf{e}\|_{a},
\]
with \(\alpha < 1\). (See Appendix~\ref{appendix:proof_4} for details.) 
\end{property}

\noindent
(See Appendix~\ref{appendix:proof_1} for proof.) Together, Theorem~\ref{th:twogrid_convergence} and the Approximation Property ensure uniform error reduction per iteration: smoothing removes high-frequency errors, while the coarse grid approximates low-frequency errors sufficiently well.

\subsection{Operator Properties}

Beyond two-grid convergence, another measure of preconditioner quality is its impact on the spectrum of \(MA\). If \(M\approx A^{-1}\), the eigenvalues of \(MA\) should lie near 1, yielding a small condition number and fast convergence for Krylov solvers like CG or GMRES.

\begin{theorem}[Preconditioned Spectrum Clustering]
\label{th:spectrum_clustering}
Let \(A \in \mathbb{R}^{n \times n}\) be SPD, and let \(M\) be a preconditioner satisfying the smoothing and coarse approximation assumptions of Theorem~\ref{th:twogrid_convergence}. Then there exist constants \(0 < \lambda_{\min} \le \lambda_{\max}\) close to 1 such that every eigenvalue \(\lambda\) of \(MA\) lies in the interval \([\lambda_{\min}, \lambda_{\max}]\). Hence, \(\kappa(MA)=\lambda_{\max}/\lambda_{\min} \) remains near 1, ensuring rapid convergence.
\end{theorem}

\noindent
(See Appendix~\ref{appendix:proof_3} for proof.) This spectral view complements the two-grid analysis by connecting eigenvalue clustering to reduced iteration counts. In a neural setting, the learned operators play a role analogous to restriction, prolongation, and smoothing, thereby preserving these spectral advantages.

Finally, we note that our Neural Algebraic Multigrid (NAMG) Operator can be interpreted as a learnable integral over the domain \(\Omega\), supporting adaptive feature extraction on coarse grids. Formally:

\begin{theorem}[NAMG Operator as a Learnable Integral]
\label{th:integral}
Given an input \(a:\Omega\to\mathbb{R}^d\) and a point \(\mathbf{x}\in\Omega\), the NAMG operator approximates
\[
    \mathcal{G}a(\mathbf{x}) 
    \;=\;
    \int_{\Omega} 
    \kappa(\mathbf{x}, \boldsymbol{\xi})\,a(\boldsymbol{\xi})
    \,d\boldsymbol{\xi},
\]
for some learnable kernel \(\kappa\). 
\end{theorem}

\noindent
(See Appendix~\ref{appendix:proof_4} for proof.) This perspective unifies the classical AMG principle of coarse-grid correction with a data-driven, integral-based formulation. It underscores the capacity of neural operators to adaptively handle diverse PDE structures, ultimately enhancing both convergence and generalization.
