\section{Preliminaries}
In this section, we introduce the essential concepts and notations for understanding the proposed neural precondition operator. We begin with an overview of Krylov subspace methods for large linear systems, followed by a discussion of preconditioning techniques.

\subsection{Krylov Subspace Methods}
Krylov subspace iterative methods solve large-scale systems of the form
\begin{equation}
    A\mathbf{x} = \mathbf{b},
\end{equation}
where \(A \in \mathbb{R}^{n \times n}\) is sparse, \(\mathbf{x}\in \mathbb{R}^n\) is the solution, and \(\mathbf{b}\in \mathbb{R}^n\) is the right-hand side (RHS). Starting from an initial guess \(\mathbf{x}_0\), the residual is 
\begin{equation}
    \mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0.
\end{equation}
The \(m\)-th Krylov subspace is
\begin{equation}
    \mathcal{K}_m(A, \mathbf{r}_0) = \text{span}\{\mathbf{r}_0,\, A\mathbf{r}_0,\, A^2\mathbf{r}_0,\, \dots\}.
\end{equation}

Krylov methods (e.g., Conjugate Gradient, GMRES) construct approximate solutions \(\mathbf{x}_m \in \mathcal{K}_m\) that minimize the residual \(\|\mathbf{b} - A\mathbf{x}_m\|\) in some norm. Further details on Krylov Subspace methods can be found in Appendix~\ref{sec:krylov}.

\subsection{Preconditioner}
Preconditioning improves the convergence of Krylov methods by transforming the system \(A\mathbf{x} = \mathbf{b}\) into one with more favorable spectral properties:
\begin{equation}
    M A \mathbf{x} = M \mathbf{b},
\end{equation}
where \(M \in \mathbb{R}^{n \times n}\) approximates the inverse operator \(A^{-1}\) and is easily invertible. An effective preconditioner \(M\) should closely approximate \(A^{-1}\), thereby clustering the eigenvalues of \(M A\) and reducing the system's condition number. Additionally, \(M\) must be computationally efficient to apply, as excessive overhead can negate the benefits of accelerated convergence. The preconditioner should also scale effectively to large problem sizes, maintaining its performance as the dimension of the system increases. Finally, it should remain robust across varying problem instances, consistently providing improvements even when the parameters or structure of the system change. A comprehensive introduction to preconditioning methods, both numerical and neural, is provided in Appendix~\ref{appendix:relate}.

\subsection{Discretization}
\label{subsec:discretization}

To numerically solve PDEs, we discretize the spatial domain \(\Omega\) using finite differences \cite{07:Finite}, finite volumes \cite{00:Finite}, or finite elements \cite{12:Numerical}, thereby converting the continuous system into $A\mathbf{x} = \mathbf{b}$, where \(A\in\mathbb{R}^{n\times n}\) is a sparse, symmetric positive-definite (SPD) stiffness matrix, \(\mathbf{b}\in\mathbb{R}^n\) encodes source terms and boundary conditions, and \(\mathbf{x}\in\mathbb{R}^n\) contains the nodal approximations of the solution. The SPD property \(\mathbf{v}^\top A\,\mathbf{v}>0\) for all nonzero \(\mathbf{v}\) ensures well-posedness and preserves the key operator characteristics needed for stable numerical solutions.

\subsection{Neural Operators}
We employ a neural operator approach based on the FNO framework \cite{21:fno,23:no} to learn mappings between function spaces. Concretely, let \(D \subset \mathbb{R}^d\) be the spatial domain, and consider an operator $\mathcal{G}: \mathcal{A}(D;\mathbb{R}^{d_a}) \to \mathcal{U}(D;\mathbb{R}^{d_u})$, where \(a \in \mathcal{A}(D;\mathbb{R}^{d_a})\) and \(u \in \mathcal{U}(D;\mathbb{R}^{d_u})\) are functions on \(D\). A neural operator \(\mathcal{N}\) approximates \(\mathcal{G}\) via the composition
\begin{equation}
    \mathcal{N}(a) = \mathcal{Q}\circ\mathcal{L}_L\circ\cdots\circ\mathcal{L}_1\circ\mathcal{R}(a),
\end{equation}
where \(\mathcal{R}\) lifts \(a\) to a higher-dimensional representation, \(\{\mathcal{L}_\ell\}\) are neural operator layers, and \(\mathcal{Q}\) projects the final output to the target dimension.
