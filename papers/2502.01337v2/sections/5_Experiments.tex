\input{tables/poisson}

\section{Experiments}

This section evaluates our Neural Preconditioning Operator (NPO) across various PDEs, mesh resolutions, and solver settings. We first detail the overall experimental setup and baselines, then present the main results for Poisson, Diffusion, and Linear Elasticity problems. Finally, we examine model generalization, ablation studies, and hyperparameter sensitivity to demonstrate how NPO balances data, residual, and condition losses for robust, scalable performance.

\subsection{Experiment Setup}
\label{subsec:setup}
\subsubsection{General Setting}
We aim to learn a Neural Preconditioning Operator (NPO), \(\mathcal{M}_{\theta}\), that expedites Krylov-based solvers for discretized PDE systems. Each instance in our dataset consists of a system matrix \(A_{i}\), derived from a finite element or finite difference discretization, and multiple recorded solution states \(\bigl(\mathbf{x}_{i,k}, \mathbf{b}_{i}, \mathbf{r}_{i,k}\bigr)\). Here, \(\mathbf{x}_{i,k}\) and \(\mathbf{r}_{i,k}\) represent the partial solution and residual at iteration \(k\) of a baseline solver (e.g., GMRES with AMG). 

The learning objective combines two components: \emph{Data Loss}, which leverages \(\mathbf{x}_{i,k}\) and \(\mathbf{b}_{i}\) to align the network’s output with observed solution patterns; \emph{Residual Loss}, inspired by \eqref{eq:residual_loss}, which nudges \(A_{i}\,\mathcal{M}_{\theta}(A_{i})\) toward the identity operator on residual vectors \(\mathbf{r}_{i,k}\). By balancing these two losses, \(\mathcal{M}_{\theta}\) learns to approximate \(A^{-1}\) while reflecting the intermediate dynamics of the iterative solution process.

\subsubsection{Dataset Generation}
To build our dataset, we discretize the governing PDEs (using finite elements or finite differences) to obtain matrices \(A \in \mathbb{R}^{n\times n}\). We then sample right-hand sides \(\mathbf{b}\) from a Gaussian Random Field (GRF) \(\phi(x)\), mapped onto the same mesh as \(A\). For each system \(A\mathbf{x} = \mathbf{b}\), we run a baseline Krylov solver (CG or GMRES) preconditioned by Algebraic Multigrid (AMG), with a convergence tolerance of \(10^{-10}\) and a cap of 100 iterations. At each step, we record the current partial solution \(\mathbf{x}_{k}\) and residual \(\mathbf{r}_{k} = \mathbf{b} - A\,\mathbf{x}_{k}\). These \(\{(A, \mathbf{b}, \mathbf{x}_{k}, \mathbf{r}_{k})\}\) samples form a comprehensive dataset capturing diverse solution states and residual behaviors, which we use to train \(\mathcal{M}_{\theta}\).

We consider three PDE systems: the Poisson equation, the diffusion equation, and the linear elasticity equation, each discretized to generate our dataset.

\textbf{Poisson Equation.} The Poisson equation is defined as:
\begin{equation}
    -\nabla \cdot (\nabla u) = f \quad \text{in} \, D,
\end{equation}
where \(u\) is the unknown solution and \(f\) is a given source term. It models potential fields such as electrostatics and steady-state heat conduction.

\textbf{Diffusion Equation.} The diffusion equation models the spread of substances over time:
\begin{equation}
    \frac{\partial u}{\partial t} = \nabla \cdot (D \nabla u),
\end{equation}
where \(D\) is the diffusion coefficient. 

\textbf{Linear Elasticity.} The linear elasticity equations describe the deformation of elastic materials under applied forces:
\begin{equation}
    \nabla \cdot \sigma = \mathbf{f}, \quad \sigma = \lambda (\nabla \cdot \mathbf{u}) I + \mu (\nabla \mathbf{u} + \nabla \mathbf{u}^\top),
\end{equation}
where \(\mathbf{u}\) is the displacement field, \(\sigma\) is the stress tensor, and \(\lambda, \mu\) are Lamé parameters. These equations are critical for structural mechanics simulations.

\subsubsection{Baselines}
\label{subsec:baselines}

Our evaluation framework encompasses three methodological categories: 
(i) \textbf{classical iterative methods} Jacobi, Gauss-Seidel, and SOR; 
(ii) \textbf{data-driven architectures} MLP \cite{86:mlp} and U-Net \cite{15:UNet}; (iii) \textbf{operator learning approaches} including Fourier neural operators (FNO \cite{21:fno}), 
transformer-based solvers (Transolver \cite{24:Transolver}), 
and multigrid-enhanced operators (M2NO \cite{24:M2NO}). 
This hierarchy spans from foundational numerical analysis to modern deep learning paradigms, ensuring rigorous comparison across methodological generations.

\input{tables/results}

\subsection{Main Results}
\label{subsec:results}
Table~\ref{table:poisson} compares the performance of our Neural Preconditioning Operator (NPO) against various classical and neural methods for Poisson problems on a one dimensional uniform \(512\) grid, a two dimensional uniform \(32\times32\) grid, and an irregular mesh. Each method is integrated into GMRES, and we report both runtime (in seconds) and iteration counts for different tolerances. Table~\ref{table:results} further evaluates NPO on Diffusion and Linear Elasticity, also at a \(32\times32\) resolution, showing similar trends.

\textbf{Classical Methods.}
Jacobi, Gauss--Seidel, and SOR serve as baselines. While they eventually converge, their iteration counts remain high across all tests. For instance, at \(\mathrm{tol}=10^{-10}\) on the \(512\) mesh (Poisson), SOR requires 502 iterations and 3.452\,s, with Gauss--Seidel close behind at around 500 iterations. On the irregular mesh, they still demand up to 130 iterations (0.379\,s). Similarly, Table~\ref{table:results} shows that even for the smaller \(32\times32\) Diffusion problem, SOR and Gauss--Seidel need 139 iterations or more.

\textbf{Neural-Based Approaches.}
MLP, U-Net, FNO, Transolver, and M2NO all improve upon classical smoothers by reducing iteration counts or runtime. For example, FNO achieves 403 iterations in 1.785\,s at \(10^{-10}\) on the uniform \(512\) Poisson problem, while Transolver lowers it to 472 iterations in roughly the same time. M2NO cuts both time and iterations further. Table~\ref{table:results} confirms this pattern on Diffusion and Linear Elasticity: FNO and Transolver surpass classical methods, although they may still require over 100 iterations in certain cases.

\textbf{Neural Preconditioning Operator (NPO).}
Our proposed NPO demonstrates consistently fewer iterations and shorter runtime across all test settings. For instance, at \(\mathrm{tol}=10^{-10}\) on the \(512\) Poisson grid, NPO converges in 184 iterations and 0.623\,s, providing more than a twofold speedup over SOR or Gauss--Seidel. At a looser tolerance of \(10^{-4}\), NPO completes in just 93 iterations, whereas classical methods still require 300 or more. On the irregular mesh, NPO requires only 82 iterations in 0.162\,s at \(\mathrm{tol}=10^{-10}\). For Diffusion and Linear Elasticity (Table~\ref{table:results}, \(32\times32\)), NPO similarly outperforms both classical and neural rivals, converging in as few as 38 iterations for Diffusion and 31 for Linear Elasticity, with minimal total runtime.

\input{figures/code/poisson_converge}

\input{figures/code/poisson_res}

\textbf{Detailed Convergence Patterns.}
Figure~\ref{fig:poisson_conv} depicts the residual reduction curves for Poisson on a \(512\times512\) grid. Classical methods (Jacobi, Gauss--Seidel, SOR) remain near the top, requiring hundreds of iterations for modest error reductions. Neural approaches like MLP, U-Net, and FNO descend more quickly but often plateau. Transolver and M2NO (blue circles and squares, respectively) push the residual several orders lower while maintaining moderate iteration counts. NPO (red dots) shows the steepest slope, surpassing other methods’ progress by iteration 100 and driving the residual below \(10^{-8}\) after about 200 iterations, ultimately achieving the lowest final residual.

\textbf{Summary.}
Across uniform and irregular meshes, as well as different PDE types (Poisson, Diffusion, and Linear Elasticity), NPO consistently outperforms classical and existing neural methods. Its ability to approximate \(A^{-1}\) more accurately significantly lowers GMRES’s iteration count and total runtime. As problem sizes and complexities grow, we anticipate NPO’s data-driven advantage to become even more pronounced.


\subsection{Generalization of Resolutions}

To investigate how iteration counts scale with increasing problem size, we trained all methods solely at a $128$ resolution, then tested them on meshes ranging from \(128\) up to \(4096\). Figure~\ref{fig:poisson_res} plots the number of iterations versus resolution, revealing two main trends. First, classical iterative solvers (Jacobi, Gauss--Seidel, and SOR) display a steep climb in iteration counts, exceeding 4000 iterations at the largest grid. While methods such as MLP, FNO, and Transolver exhibit somewhat better scaling, their iteration counts still grow appreciably as the resolution increases.

By contrast, our Neural Preconditioning Operator (NPO) maintains a comparatively moderate increase in iterations despite operating far beyond its original training resolution. This behavior indicates that learning on a relatively small grid can, in practice, yield robust performance on significantly larger domains. Consequently, the data-driven approach underlying NPO demonstrates both adaptability and scalability in real-world scenarios where mesh sizes can vary substantially.

\input{tables/ablation}

\subsection{Model Analysis}

\textbf{Ablation Study (Table~\ref{table:ablation}).}
We analyze the impact of removing key components from the Neural Preconditioning Operator (NPO). Omitting the residual or condition loss slightly increases the iteration count (to 189), while removing the data loss has a larger effect (206 iterations), indicating its importance for matching solution states. Disabling the entire NAMG operator raises the iteration count to 314, demonstrating the loss of preconditioning benefits. Similarly, removing the matrix \(A\) from the input increases iterations to 227, highlighting the importance of system information. Removing pre- or post-processing each adds about 50 iterations (233 vs.\ 184), and removing both degrades performance to 309 iterations. These findings underscore the importance of each component in improving convergence efficiency.

\textbf{Hyperparameter Study (Table~\ref{table:hp}).}
We evaluate how hyperparameters affect performance on the Poisson equation. A feature width of 32 yields the best result (184 iterations), while smaller widths (8, 16) perform slightly worse and larger widths (64, 128) increase iteration counts. A single pre/post-processing pass achieves the optimal result, with additional passes sometimes increasing complexity without improving performance. Using 128 coarse points balances representation and overhead, achieving 184 iterations, while extremes like 8 or 64 lead to over 200 iterations. Similarly, setting 4 attention heads achieves optimal performance; fewer heads underfit, while more heads (8, 16) increase complexity without significant gains.

These studies confirm that NPO’s effectiveness relies on a balance between data, residual, and condition losses, along with carefully tuned hyperparameters, enabling faster convergence across diverse problem instances. More details, including efficiency analysis and model configurations, can be found in Appendix~\ref{appendix:detail}.

\input{tables/hyperparameter}