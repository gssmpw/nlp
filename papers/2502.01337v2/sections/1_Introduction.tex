\section{Introduction}
Partial differential equations (PDEs) are widely used to model phenomena such as heat transfer, electrostatics, and quantum mechanics \cite{82:AnIntro,87:Levy,02:PDE,20:Turbulence}, and are typically solved by discretizing them into large, sparse linear systems \(A x = b\) \cite{77:Numerical}. Krylov subspace methods efficiently handle these systems by projecting onto lower-dimensional subspaces \cite{07:Computational}, yet parametric PDEs and high-resolution meshes often produce ill-conditioned matrices with entries spanning multiple scales, posing considerable computational challenges. Preconditioning is therefore critical for accelerating convergence by improving the system’s spectral properties. Classical techniques like Jacobi, Gauss--Seidel, or multigrid \cite{00:tutorial} can substantially reduce iteration counts, but they frequently require problem-specific tuning and may fail to generalize across changing PDE parameters or domain discretizations.

Neural operators \cite{21:deeponet,21:fno,21:Choose} have recently emerged as a powerful tool for mapping parametric inputs to PDE solutions, learning universal operators directly from data. Building on these developments, researchers have begun leveraging neural operator approaches to speed up classical numerical methods by training data-driven preconditioners \cite{19:LearningNeural,23:LearningPre,24:fcg-no}. While this paradigm offers promising adaptability and the potential to handle complex PDE parameters, it also presents new challenges: extensive training data may be required, large-scale systems often demand specialized architectures, and rigorous theoretical guarantees for convergence—especially under changing boundary conditions or mesh refinements—remain an open problem.

Motivated by these challenges, we introduce a new \emph{Neural Preconditioning Operator} (NPO) framework designed to address the difficulties of data collection, loss function design, and large-scale PDE solving. Our approach first generates datasets by discretizing PDEs, then collects solution and residual information under varying system matrices. We define two key losses—\emph{condition loss} and \emph{residual loss}—to guide the neural operator toward learning effective preconditioning strategies for Krylov subspace methods. Building on these foundations, we propose a \emph{Neural Algebraic Multigrid} operator that blends algebraic multigrid principles with a transformer-based architecture. By exploiting the hierarchical structure of multigrid, the NPO accelerates error reduction across multiple scales while retaining the adaptability inherent in data-driven models. As we show through comprehensive experiments, NPO outperforms both classical and existing neural preconditioners, achieving superior convergence rates and generalizing effectively across different PDE domains and resolutions. Our contributions are as follows:

\begin{itemize}
    \item \textbf{Neural Preconditioning Operator (NPO) Framework.} We propose a framework encompassing data generation, neural operator training (via condition and residual losses), and integration with Krylov subspace solvers, ultimately enabling more efficient and robust PDE solves across varying mesh types and dimensions. Additionally, we provide theoretical analysis to show that our method inherits strong convergence guarantees from classical multigrid, ensuring rapid and stable error reduction.

    \item \textbf{Neural Algebraic Multigrid (NAMG) Operator.} By embedding algebraic multigrid principles within a transformer-based network, we accelerate multiscale error elimination while preserving the adaptability of a data-driven model. The theoretical foundations further confirm that the learned operators cluster eigenvalues around 1, thereby improving the system’s spectral properties.

    \item \textbf{Comprehensive Evaluation.} We benchmark NPO on diverse PDEs and mesh resolutions, demonstrating faster convergence and broader generalization compared to classical and existing neural approaches. Our experiments validate that the learned preconditioners consistently reduce iteration counts and maintain robust performance across various problem settings.
\end{itemize}
