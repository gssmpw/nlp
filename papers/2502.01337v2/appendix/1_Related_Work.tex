\section{Related Work}
\label{appendix:relate}
\subsection{Numerical Preconditioner}

Preconditioning is a well-established numerical technique for accelerating the convergence of iterative solvers applied to large linear systems. It involves applying a matrix transformation to reduce the condition number of the system matrix. Here, we briefly review three key approaches in numerical preconditioning: matrix factorization, matrix reordering, and multilevel methods.

Matrix factorization-based preconditioners are among the most widely used. Simple methods like the Jacobi preconditioner only use the diagonal elements of the matrix, which are fast to compute but provide limited improvement in condition number. More sophisticated approaches, such as the incomplete Cholesky (IC) preconditioner~\cite{06:Numerical}, offer a balance between computational cost and accuracy by partially factorizing the matrix while limiting fill-in. Advances in this area, including dynamic fill-in strategies~\cite{12:Numerical}, improve accuracy but at a higher computational cost.

Matrix reordering techniques~\cite{Liu_book, davis1999modifying} aim to reduce the bandwidth of sparse matrices by reorganizing their structure into block-diagonal forms. This restructuring minimizes fill-in during factorization and enhances parallelization, making it particularly beneficial when combined with other preconditioning methods. Our approach leverages graph-based representations to naturally maintain order invariance and parallelizability.

Multilevel methods, such as the classical multigrid approach~\cite{00:tutorial}, improve scalability by addressing errors at different levels of discretization. These methods are particularly effective for elliptic PDEs, though they face challenges with hyperbolic and parabolic PDEs~\cite{TrotMult2001}. Our Neural Preconditioning Operator (NPO) differs by using data-driven neural operators to approximate the inverse system matrix without being tailored to specific PDE types. Research has also explored hybrid approaches combining multilevel and neural strategies to further enhance solver efficiency~\cite{chen_2021_icsiggraph}.

\subsection{Neural Preconditoner}

Recent advances have explored neural networks for preconditioning linear systems derived from PDEs. Unlike classical preconditioners, neural approaches adaptively improve solver performance by learning data-driven representations of the inverse operator.

Early methods aimed to guarantee convergence through neural approximations of PDE solvers \cite{19:LearningNeural}. Machine learning techniques have also been applied to geophysical fluid dynamics, demonstrating the effectiveness of neural preconditioners in large-scale simulations \cite{20:MachineLearned}. Further extensions hybridize operator learning with traditional relaxation methods for enhanced scalability and accuracy \cite{22:AHybrid}.

Neural networks have been used to accelerate solvers in physics domains, such as lattice gauge theory, by reducing the iteration count in large sparse systems \cite{22:NeuralNetwork}. Preconditioners specifically optimized for conjugate gradient (CG) solvers were introduced by \cite{23:LearningPre}, achieving faster convergence through learned adaptations to PDE structures.

Recent work leverages neural operator frameworks, such as DeepONet and Fourier Neural Operators (FNO), to enhance preconditioning strategies \cite{24:DeepOnet}. Transformer-based architectures have also been explored, incorporating multigrid principles to refine both fine- and coarse-grid error correction \cite{24:MultigridAugmented,24:fcg-no}.

Moreover, specialized applications in porous microstructures and Helmholtz equations highlight how machine-learned preconditioners can integrate compact implicit layers for improved error control and spectral properties \cite{24:Machine}. Collectively, these developments point to the growing importance of neural preconditioners in accelerating PDE solvers across diverse physical and computational settings.
