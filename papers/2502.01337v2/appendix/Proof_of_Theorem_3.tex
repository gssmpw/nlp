\subsection{Proof of Theorem \ref{th:integral}} \label{appendix:proof_4}
Recall that the NAMG operator applies a graph-based attention mechanism to approximate an integral over the spatial domain \(\Omega\). The theorem is established by demonstrating that the graph attention mechanism employed in the NAMG operator can be formalized as a Monte-Carlo approximation of an integral operator \cite{21:Choose, 23:no, 24:Transolver,24:AMG}.

\begin{proof}[Proof of Theorem~\ref{th:integral}]
Let \(a:\Omega \to \mathbb{R}^d\) be an input function, and let \(\mathbf{x}\in \Omega\subset\mathbb{R}^d\). Our goal is to show that
\[
    \mathcal{G}a(\mathbf{x})
    \;=\;
    \int_{\Omega} \kappa(\mathbf{x}, \boldsymbol{\xi})\,a(\boldsymbol{\xi})
    \,d\boldsymbol{\xi}
\]
can be approximated by the NAMG attention update.

Define the kernel \(\kappa(\mathbf{x},\boldsymbol{\xi})\) to measure similarity between \(\mathbf{x}\) and \(\boldsymbol{\xi}\). In a continuous setting, \(\mathcal{G}a(\mathbf{x})\) integrates over all \(\boldsymbol{\xi}\in\Omega\).

We discretize \(\Omega\) into a mesh or graph with nodes \(\{\mathbf{x}_i\}\). Each node \(\mathbf{x}_i\) represents a sample in \(\Omega\). The adjacency structure \(A\) (or neighborhood set \(\mathcal{N}(\mathbf{x})\)) reflects local connectivity.

Attention weights 
\[
    \alpha_{i} 
    \;\propto\; 
    \exp\!\Bigl(\mathbf{a}^\top [\mathbf{W}a(\mathbf{x}) \;\|\; \mathbf{W}a(\mathbf{x}_i)]\Bigr)
\]
approximate the continuous kernel \(\kappa(\mathbf{x}, \mathbf{x}_i)\). Normalizing by the softmax denominator
\[
    \sum_{j \in \mathcal{N}(\mathbf{x})}
    \exp\!\Bigl(\mathbf{a}^\top [\mathbf{W}a(\mathbf{x}) \;\|\; \mathbf{W}a(\mathbf{x}_j)]\Bigr)
\]
yields a discrete approximation to the integral \(\int_{\Omega} \kappa(\mathbf{x}, \boldsymbol{\xi})\,d\boldsymbol{\xi}\). Thus, summing over neighbors \(\mathbf{x}_i \in \mathcal{N}(\mathbf{x})\) corresponds to sampling from \(\Omega\).

Replacing \(\kappa(\mathbf{x}, \boldsymbol{\xi})\) with the above attention weights and summing over \(\mathcal{N}(\mathbf{x})\), we obtain
\[
    \mathcal{G}a(\mathbf{x})
    \;\approx\;
    \sum_{i \in \mathcal{N}(\mathbf{x})} 
    \alpha_{i} \,\mathbf{W}a(\mathbf{x}_i),
\]
which matches the NAMG attention update rule. Hence, the NAMG operator realizes a Monte Carlo approximation to \(\int_{\Omega} \kappa(\mathbf{x}, \boldsymbol{\xi})\,a(\boldsymbol{\xi})\,d\boldsymbol{\xi}\), where \(\alpha_i\) and \(\mathbf{W}\) are learned parameters.

Therefore, the attention-based aggregation in NAMG acts as a learnable integral operator over \(\Omega\), completing the proof.
\end{proof}
