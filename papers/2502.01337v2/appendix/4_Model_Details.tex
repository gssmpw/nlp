\section{Model Details}
\label{appendix:detail}
\input{tables/efficiency}
\subsection{Efficiency Analysis}
\label{subsec:efficiency}

Efficiency in computational models is crucial, especially when dealing with large-scale problems such as those encountered in solving PDEs. Here, we focus on the efficiency metrics for the Neural Preconditioning Operator (NPO), comparing it against other models based on parameter size, GPU memory usage, and execution time during the training phase. A detailed comparison is provided in Table~\ref{table:eff}.

NPO demonstrates exceptional efficiency across various matrix sizes, maintaining a low parameter size (0.14 MB) and consistent execution times (around 0.0121 to 0.0123 seconds), regardless of the input scale. Notably, despite the increasing matrix sizes from 512 to 4096, NPO's GPU memory usage grows predictably without disproportionate spikes, which is crucial for scalable applications.

In contrast, other models such as M2NO and U-Net show significant increases in GPU memory demands and slower execution times as matrix sizes grow. M2NO, for instance, uses up to 5.55 million MB of GPU memory for the largest matrix size, with longer running times that reach up to 0.0814 seconds. This reflects a substantial computational overhead compared to NPO.

FNO and MLP, while smaller in parameter size, do not match NPO in terms of balancing the execution speed and memory efficiency at higher matrix dimensions. FNO offers the fastest execution times among the competitors but does not provide the robustness in feature representation that NPO does.

Overall, NPO not only excels in handling larger matrices efficiently but also showcases a balanced profile in terms of memory usage and computational speed, making it particularly suitable for real-world applications where both accuracy and efficiency are paramount.

\subsection{Model Configurations}
The primary configurations for Nerual Preconditioning Operator (NPO) are detailed in Table \ref{table:config}. Except where specifically noted, model parameters remain consistent across different hyperparameters and resolutions within the same benchmark.

\input{tables/config}