\section{Krylov Subspace Methods}
\label{sec:krylov}
Krylov subspace methods are a class of iterative solvers designed for large-scale linear systems of the form
\begin{equation}
    A\mathbf{x} = \mathbf{b},
\end{equation}
where \(A \in \mathbb{R}^{n \times n}\) is a sparse matrix, \(\mathbf{x}\in \mathbb{R}^n\) is the solution vector, and \(\mathbf{b}\in \mathbb{R}^n\) is the right-hand side (RHS) vector. Starting with an initial guess \(\mathbf{x}_0\), the residual vector is defined as
\begin{equation}
    \mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0.
\end{equation}
At each iteration, Krylov methods construct a solution within the Krylov subspace, defined by:
\begin{equation}
    \mathcal{K}_m(A, \mathbf{r}_0) = \text{span}\{\mathbf{r}_0,\, A\mathbf{r}_0,\, A^2\mathbf{r}_0,\, \dots,\, A^{m-1}\mathbf{r}_0\}.
\end{equation}

These methods aim to find an approximate solution \(\mathbf{x}_m \in \mathcal{K}_m\) that minimizes the residual norm \(\|\mathbf{b} - A\mathbf{x}_m\|\). Two widely used Krylov methods are Conjugate Gradient (CG) and Generalized Minimal Residual (GMRES).

\paragraph{Conjugate Gradient (CG).} CG is specialized for symmetric positive definite (SPD) matrices. It constructs a series of orthogonal search directions \(\{\mathbf{p}_k\}\) such that each iterate \(\mathbf{x}_{k+1}\) minimizes the quadratic form
\begin{equation}
    \|\mathbf{b} - A\mathbf{x}_{k+1}\|_A^2 = (\mathbf{x}_{k+1} - \mathbf{x})^\top A (\mathbf{x}_{k+1} - \mathbf{x}),
\end{equation}
where \(\| \cdot \|_A\) denotes the \(A\)-norm. CG converges rapidly for well-conditioned systems, often within a number of iterations proportional to the square root of the condition number of \(A\).

\paragraph{Generalized Minimal Residual (GMRES).} GMRES is designed for general non-symmetric matrices. It iteratively constructs an orthonormal basis of the Krylov subspace using the Arnoldi process. At each step, GMRES finds \(\mathbf{x}_m\) that minimizes the residual norm in the Euclidean sense:
\begin{equation}
    \mathbf{x}_m = \arg\min_{\mathbf{x} \in \mathcal{K}_m} \|\mathbf{b} - A\mathbf{x}\|.
\end{equation}
Since the orthonormal basis grows with each iteration, GMRES requires restarts to control memory usage and computational cost. Despite this, it is effective for systems with complex eigenvalue structures.

Both methods benefit significantly from preconditioning, which transforms the system into one with a more favorable spectrum, thereby accelerating convergence.
