\vspace{5em}
\paragraph{Step 5:} We have $V^{\star}_1(s; \alpha^n/H)\le \tilde{V}^n_1(s)$, where $\tilde{V}^n_h$ is defined as follows. Start with $\tilde{V}^n_{H+1}=\bm{0}$. Define $\tilde{f}_h^n=[P_h\tilde{V}^{n}_{h+1}]$ for simplicity of notations. Let $\hat{\tilde{f}}_h^n$ be the kernel ridge predictor of $\tilde{f}_h^n$ from $n$ observations in the first $n$ episodes. In particular let, $\tilde{Y}^{n}_h=[\tilde{V}^n_{h+1}(s^1_{h+1}), \tilde{V}^n_{h+1}(s^2_{h+1}), \cdots, \tilde{V}^n_{h+1}(s^n_{h+1})]^{\top}$. We let
\begin{equation*}
    \hat{\tilde{f}}_h^n(z) = (k^n_h(z))^{\top}(\tau^2I+K^n_h)^{-1}\tilde{Y}^{n}_h
\end{equation*}
and its uncertainty estimate
\begin{equation*}
    (\sigma_h^n(z))^2 = k(z,z)- (k^n_h(z))^{\top}(\tau^2I+K^n_h)^{-1}k^n_h(z).
\end{equation*}

We define 
\begin{equation*}
    \tilde{Q}^h_n(z)= \alpha^n_h(z)/H +  \hat{\tilde{f}}_h^n(z) + \beta\sigma_h^n(z)
\end{equation*}
and 
\begin{equation*}
    \tilde{V}^n_h(s)= \tilde{Q}^n_h(s,\tilde{\pi}^n_h(s)),
\end{equation*}
where $\tilde{\pi}^n_h(s)=\argmax_{a\in\Ac}Q^n_h(s,a)$.
In the following lemma, we prove that $\tilde{V}^n_h$ is an upper on the optimal value function.

\begin{lemma}\label{lem:VstarVn}
    Under $\Ec$, $\forall s$, 
    \begin{equation*}
        V^{\star}_1(s; \alpha^n/H)\le \tilde{V}^n_1(s).
    \end{equation*}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:VstarVn}]
The proof is similar to that of Lemma~\ref{lem:VstarVplan}, and it is by induction, starting from $V^{\star}_{H+1}(\cdot;\alpha^n/H) = V^n_{H+1}=\bm{0}$. We have
\begin{align*}
    Q^{\star}_h(s,a;\alpha^n/H) - Q^n_{h}(s,a) &= [P_hV^{\star}_{h+1}](s,a;\alpha^n/H) - \hat{f}^n_h(s,a)-\beta\sigma^n_h(s,a)\\
    &\le [P_hV^{\star}_{h+1}](s,a;\alpha^n/H) -[P_hV^n_{h+1}](s,a)\\
    &=[P_h(V^{\star}_{h+1}-V^n_{h+1})](s,a;\alpha^n/H)\\
    &\le 0. 
\end{align*}
The first inequality holds by the confidence intervals and the second inequality holds bu the induction assumption. 
Then, we have
\begin{align*}
    V^{\star}_h(s;\alpha^n/H) - V^n_h(s) &=
    \max_{a\in\Ac} Q^{\star}_h(s,a;\alpha^n/H) - \max_{a\in\Ac}Q^n_{h}(s,a)\\
    &\le \max_{a\in\Ac} \{Q^{\star}_h(s,a;\alpha^n/H) - Q^n_{h}(s,a)\}\\
    &\le 0
\end{align*}
That proves the lemma. 
\end{proof}

\paragraph{Step 6:} We have $\tilde{V}^n_h(s)\le {V}^n_h(s)$, $\forall s\in\Sc$, where ${V}^h_n(s)$ is defined as follows. 
Start with ${V}^n_{H+1}=\bm{0}$. Define ${f}_h^n=[P_h{V}^{n}_{h+1}]$ for simplicity of notations. Let $\hat{{f}}_h^n$ be the kernel ridge predictor of ${f}_h^n$ from $n$ observations in the first $n$ episodes. In particular let, ${Y}^{n}_h=[{V}^n_{h+1}(s^1_{h+1}), {V}^n_{h+1}(s^2_{h+1}), \cdots, {V}^n_{h+1}(s^n_{h+1})]^{\top}$. We let
\begin{equation*}
    \hat{{f}}_h^n(z) = (k^n_h(z))^{\top}(\tau^2I+K^n_h)^{-1}{Y}^{n}_h
\end{equation*}
and its uncertainty estimate
\begin{equation*}
    (\sigma_h^n(z))^2 = k(z,z)- (k^n_h(z))^{\top}(\tau^2I+K^n_h)^{-1}k^n_h(z).
\end{equation*}

We define 
\begin{equation}\label{eq:Qtilden}
    {Q}^h_n(z)= \alpha^n_h(z)/H +  \hat{{f}}_h^n(z) + \sattar{3}\beta\sigma_h^n(z)
\end{equation}
and 
\begin{equation}\label{eq:Vtilden}
    {V}^n_h(s)= {Q}^n_h(s,\pi^n_h(s)), 
\end{equation}
where $\pi^n_h(s) = \argmax_{a\in\Ac}\sigma^n_h(s)$.

We highlight the two differences between ${V}^n_h$ and $\tilde{V}^n_h$. One is the factor $3$ in upper confidence bound in Equation~\eqref{eq:Qtilden}. The other is the selection of action based on $\pi^n_h$ is Equation~\eqref{eq:Vtilden}, in contrast to $\tilde{\pi}^n_h$

In the following lemma, we prove that ${V}^n_h$ is an upper on $\tilde{V}^n_h$.

\begin{lemma}\label{lem:VnVtilden}
Under $\Ec$, $\forall s\in\Sc$, we have
\begin{equation*}
    \tilde{V}^n_h(s)\le {V}^n_h(s).
\end{equation*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lem:VnVtilden}]
The proof is by induction, starting from $\tilde{V}^n_{H+1} = {V}^n_{H+1}=\bm{0}$. We then have


\begin{align*}
    \tilde{Q}^n_h(s,a) = 
\end{align*}


\begin{align*}
    \hat{\tilde{f}}_h^n(z) = (k^n_h(z))^{\top}(\tau^2I+K^n_h)^{-1}\tilde{Y}^{n}_h
\end{align*}










\begin{align*}
    \tilde{V}^n_h(\tilde{s}^n_h) - {V}^n_h({s}^n_h) &= \tilde{Q}^n_h(\tilde{s}^n_h,\tilde{\pi}^n_h(\tilde{s}^n_h)) - {Q}^n_h(s^n_h, \pi^n_h(s^n_h))\\
    &=\beta\sigma^n_h(\tilde{s}^n_h,\tilde{\pi}^n_h(\tilde{s}^n_h))/H +  \hat{\tilde{f}}_h^n(\tilde{s}^n_h,\tilde{\pi}^n_h(\tilde{s}^n_h)) + \beta\sigma_h^n(\tilde{s}^n_h,\tilde{\pi}^n_h(\tilde{s}^n_h)) \\
    & ~~~~~ - \beta\sigma^n_h(s^n_h,\pi^n_h(s^n_h))/H -  \hat{{f}}_h^n(s^n_h,\pi^n_h(s^n_h)) - \sattar{3}\beta\sigma_h^n(s^n_h,\pi^n_h(s^n_h))\\
    &\le  [P_h\tilde{V}^{n}_{h+1}](\tilde{s}^n_h,\tilde{\pi}^n_h(\tilde{s}^n_h)) + (2+\frac{1}{H})\beta\sigma^n_h(\tilde{s}^n_h,\tilde{\pi}^n_h(\tilde{s}^n_h)) \\
    & ~~~~~ - [P_h{V}^{n}_{h+1}](s^n_h,\pi^n_h(s^n_h)) - (2+\frac{1}{H})\beta\sigma^n_h(s^n_h,\pi^n_h(s^n_h)) 
\end{align*}



\begin{align*}
    \tilde{V}^n_h({s}^n_h) - {V}^n_h({s}^n_h) &\le [P_h\tilde{V}^{n}_{h+1}](s^n_h,\tilde{\pi}^n_h(s^n_h)) - [P_h{V}^{n}_{h+1}](s^n_h,\pi^n_h(s^n_h))\\
    &=[P_h\tilde{V}^{n}_{h+1}](s^n_h,\tilde{\pi}^n_h(s^n_h)) -
    [P_h\tilde{V}^{n}_{h+1}](s^n_h,{\pi}^n_h(s^n_h))+[P_h\tilde{V}^{n}_{h+1}](s^n_h,{\pi}^n_h(s^n_h))-[P_h{V}^{n}_{h+1}](s^n_h,\pi^n_h(s^n_h))\\
    &=\tilde{V}^n_{h+1}({s}^n_{h+1}) - {V}^n_{h+1}({s}^n_{h+1})\\
    & ~~~~~ + ({V}^n_{h+1}({s}^n_{h+1}) -[P_h{V}^{n}_{h+1}](s^n_h,\pi^n_h(s^n_h))) - (\tilde{V}^n_{h+1}({s}^n_{h+1}) - [P_h\tilde{V}^{n}_{h+1}](s^n_h,{\pi}^n_h(s^n_h)))\\
    & ~~~~~ + [P_h\tilde{V}^{n}_{h+1}](s^n_h,\tilde{\pi}^n_h(s^n_h)) -
    [P_h\tilde{V}^{n}_{h+1}](s^n_h,{\pi}^n_h(s^n_h))
\end{align*}
    
\end{proof}
















--- New analysis:


Two different policies $\pi$ with maximum variance and $\tilde{\pi}$ with maximum Q. Has its own Q and V. 

$V^*\le \tilde{V}$ and $V-V^{\pi}\le 2\beta\sigma$. Thus $V\le 3\beta\sigma$. 

We thus need to bound the error between $\tilde{V}$ and $V$

\begin{align*}
    &\tilde{V}_h(s) - V_h(s) = \tilde{Q}_h(s, \tilde{\pi}_h(s)) - Q_h(s,\pi(s))\\
    &\le [P_h\tilde{V}_{h+1}](s, \tilde{\pi}_h(s)) + (2+\frac{1}{H})\beta\sigma_h^n(s, \tilde{\pi}_h(s)) - [P_hV_{h+1}](s, \pi(s)) + \beta\sigma_h^n(s,\pi(s)) - (\bm{3}+\frac{1}{H})\sigma_h^n(s, \pi(s))\\
    & \le [P_h\tilde{V}_{h+1}](s, \tilde{\pi}_h(s)) - [P_hV_{h+1}](s, \pi(s))
    \\
    & \le 
    \tilde{V}_{h+1}(s) - V_{h+1}(s)+
    ([P_h\tilde{V}_{h+1}](s, \tilde{\pi}_h(s)) 
    -\tilde{V}_{h+1}(s))
    + (V_{h+1}(s)
    - [P_hV_{h+1}](s, \pi(s)))
\end{align*}

In the above we used that $\pi$ is the variance maximizer

3 in important here (Q = Qhat + 3 beta sigma), when summed up $\tilde{V}_1(s) - V_1(s)$ is the sum of martingale different terms. 

---





$\pi^n_h(s)=\arg\max_a Q^n_h(s,a)$

\begin{align}
    V_h^n(s; \sigma_h^n) - V^{\pi}_h (s; \sigma_h^n)
\end{align}


$a_h=\pi(s_h)$
\begin{align*}
    Q_h^n(s_h,a_h; \sigma_h^n) - Q^{\pi}_h (s_h,a_h; \sigma_h^n) &= r_h(s_h,a_h) + \text{UCBPV}^n_h(s_h,a_h) -r_h(s_h,a_h) - [P_hV^{\pi}_{h+1}](s_h,a_h) \\
    &= \text{UCBPV}^n_h(s_h,a_h) -[P_hV^n_{h+1}](s_h,a_h)  + [P_hV^n_{h+1}](s_h,a_h) - [P_hV^{\pi}_{h+1}](s_h,a_h)\\
    &\le 2\beta\sigma^n_h(s_h,a_h)
\end{align*}



