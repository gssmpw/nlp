\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MediaTek Research Proposal}
\author{Aya Kayal}
\date{February 2024}


\input{notation}

\begin{document}

\maketitle

\section*{MediaTek Research Internship}

This document lays out the research problems we aim at tackling during Aya's internship at MediaTek Research. We consider two general routes which have overlaps. The details of planning and a final proposal will be provided at a later stage. The two general routes are as follows. 

i) Aya's previous work has focused on intrinsic rewards for exploration in RL. We aim at developing her work to a framework for RL exploration that can serve as a reference to the community. 

ii) MR has previously published on RL in complex environments. We aim at extending on this line of work with a focus on model learning,  exploration and empirical studies. 

\paragraph{Tentative internship timeline:} Half time (20 hour/week) for 6 months starting in March 2024. 


\section{Intrinsic rewards for exploration}
\subsection{Background summary}
\label{section1}
One of the open challenges in Reinforcement Learning is the Hard Exploration problem \cite{sutton2018reinforcement}. Sparse rewards can cause the agent to suffer from poor sample efficiency; a large number of samples is needed to explore and stumble into a successful sequence
of actions leading to the desired outcome. Hard exploration has been addressed by intrinsic rewards \cite{chentanez2004intrinsically}, which are bonus signals added to the extrinsic rewards to encourage the agent to explore less visited states \cite{bellemare2016unifying,tang2017exploration,pathak2017curiosity,hazan2019provably,burda2018exploration, badia2020never}, or to favor diverse behaviors \cite{eysenbach2018diversity,zahavy2022discovering,eysenbach2021maximum,gregor2016variational}.
There are various types of intrinsic rewards in the literature. \cite{taiga2021bonus} evaluated selected intrinsic rewards that push towards visiting novel states (RND \cite{burda2018exploration}, Pseudocounts \cite{bellemare2016unifying} and ICM \cite{pathak2017curiosity}) 
within the Arcade Learning Environment (ALE), but there is no a common study that aim at comparing intrinsic rewards by looking at different levels of diversity that can be imposed (states/dynamics/policies/skills). 
Our study investigated whether the different levels of diversity imposed by the intrinsic reward lead to different exploration/sample efficiency. Specifically, we subdivided intrinsic rewards into two categories ``Where to explore'' and ``How to explore''. The former favors exploration by imposing diversity on the states or state transitions (state and state + dynamics levels). The latter (``How to explore'') rather pushes the agent to discover diverse policies that can elicit diverse behaviors (policy and skill levels). We compared the exploration behavior of selected intrinsic rewards (State Count, Intrinsic Curiosity Module \cite{pathak2017curiosity}, Max Entropy \cite{eysenbach2021maximum} and DIAYN \cite{eysenbach2018diversity}) on MiniGrid with grid encodings and RGB observations, in a procedurally generated partially observable framework.  


\subsection{General mathematical framework unifying intrinsic rewards}
As mentioned in Section \ref{section1}, a variety of intrinsic rewards were proposed in the RL literature. The first category pushes the agent to find new knowledge about its environment and discover new areas. This is done  by computing the novelty of states, the information gain, or the prediction error (curiosity) \cite{aubret2019survey}:
\begin{enumerate}
    \item State Novelty: $R_{int}(s_t)=1/\sqrt{N(s_t)}$ where $N$ is the state count.
    \item Information gain (reduction in uncertainty $U$): $R_{int}(s_t,s_{t+1})= U_{t+1}(\theta) - U_t(\theta)$
    \item Prediction error over a forward model: $R_{int}(s_t,s_{t+1})= ||g(s_{t+1}) - F(g(s_t),a_t)||$ where $g$ is an embedding function and $F$ is a model of the environment's dynamics. 
\end{enumerate}
The second category of intrinsic rewards pushes the agent to acquire a repertoires of diverse skills in order to achieve self-generated goals in sparse reward settings. For example: 
\begin{enumerate}
    \item: $R_{int}(s_t,g_t)= D(f(s_t),f(g_t))$ where $D$ is a distance function between the chosen goal $g_t$ and state $s_t$, and f is any embedding function.
    \item: $R_{int}(s_t,g_t)= I(g_t,f(\tau ))$ where $I$ is the mutual information between the goal $g_t$ and its associated trajectory $\tau$.
\end{enumerate}
Although these methods were studied empirically on different types of environments, and each one of them has its own strengths and limitations, a theoretical framework that unifies their different branches does not exit yet. The aim is to formalize these methods as a general objective function, which subsumes several of the aforementioned intrinsic rewards as special cases. A similar general framework is RL with general utility functions \cite{zhang2020variational,zahavy2021reward}; it generalizes cumulative reward, which can be viewed as a linear functional of the state-action occupancy measure, and it includes several known problems as special cases such as constrained MDPs, exploration and learning from demonstrations. \\
By having a general theoretical framework, we aim to analyze the different intrinsic rewards in terms of sample complexity, statistical efficiency and robustness to perturbations in the environment. 


\paragraph{Objective:} To design a general theoretical framework for various exploration strategies mentioned above to enable their comparison with respect to statistical and computational efficiency and perhaps other measures such as robustness. This framework will be complementary to the empirical studies previously conducted by Aya. 

\paragraph{Impact:} RL has shown great empirical success in various domains and is a widely studied setting. By providing a comprehensive survey and a technical argument on various exploration methods in this field, we can have an impactful publication that serves guidance for the community. 

\paragraph{Methods:} 
We find out some important performance metrics in RL such as statistical and computational efficiency. We develop a setting and a theoretical framework where various exploration methods can be compared. We analyze the performance of various exploration methods withing this framework and compare the results with the results obtained from empirical study.    


\section{Model-based Reinforcement learning with complex models}

\subsection{Background summary}

Reinforcement Learning has shown great empirical success in various domains such as robotics, gaming, autonomous driving, and recommendation systems. However, the theoretical analysis focused on the tabular setting with finite state-actions space \cite{jin2018q} and on the linear setting where the probability transition distribution can be linearized in the feature map of the state-action space \cite{jin2020provably}. Nevertheless, extending the analysis to complex models with infinite state-action space is still an open research problem. Recent studies have considered the kernelized setting which is a generalization of the linear setting; it allows for non linear function approximation, and assumes that the reward and transition probability distribution belong to the Reproducing Kernel Hilbert space (RKHS) \cite{yang2020provably}. This will pave the way to address the neural network setting in the infinite width. The idea of kernel regression is originally inspired from kernelized bandit algorithms \cite{scarlett2017lower} and there are ongoing efforts to extend it to the RL setting, where there is no control over the states due to the Markovian dynamics. \\
In \cite{vakili2024kernelized}, authors used a kernelized setting and leveraged a  domain partitioning approach to establish confidence intervals that apply to all functions in the state-action value function class. They proved a regret bound that is order optimal for a general class of kernels. 



\subsection{Model-based setting}


In the existing work, the non-linear function approximation machinery predicts the value function in the RL setting. See appendix for the details. This is often referred to,  in the literature, as model-free approach to algorithm design. The alternative approach 
is to directly estimate the MDP model (transition probability distribution) and then use it to estimate the value function. This approach requires new algorithmic design. Theoretically there seem to be certain benefits to this approach. One example is that the domain partitioning idea in the previous work seems inevitable within a model based approach. Given that in practice the domain partitioning idea is barely used, we hypothesise that taking a model-based approach is the key to provide a comprehensive analysis for RL algorithms under general kernel-based models. 


\paragraph{Objective:}
To develop a model based algorithm 
in the kernelized RL setting where the aim is to learn a model for the transition probability distribution of the MDP, which is a fixed function (unlike the value function). The open questions which need to be addressed are how to generate trajectories to estimate $P$, how to learn the model, and how many interactions with the environment are needed to have a good estimation of $P$ that can be used to design a good policy. Our initial ideas indicate that we may be able to develop algorithms which guarantee optimal performance without domain partitioning. 

We also aim at an empirical study on various model-free and model-based algorithms with or without domain partitioning. 

\begin{table}[h]
\centering

\label{tab:comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
 & \textbf{With Partitioning} & \textbf{Without Partitioning} \\ \midrule
\textbf{Model Based} & This project & This project \\
\textbf{Model Free} & \cite{vakili2024kernelized} & \cite{yang2020provably} \\
\bottomrule
\end{tabular}
\end{table}


\paragraph{Impact:}
This work lays the foundations towards a theoretical understanding of complex non-linear models, and eventually Neural Networks which are currently not understood in the literature. By considering a model-based approach and deriving tighter regret bounds in kernelized setting, we would be contributing towards the development of sample efficient RL algorithms with convergence guarantees.
\paragraph{Method:}
We adopt the same kernel ridge regression framework as the previous study \cite{vakili2024kernelized}, but we take a model-based approach to algorithm design. We propose a new algorithm to estimate the transition model first and we run experiments to compare it to model-free algorithms with or without domain partitioning.








\section*{Appendix}

Here we formalize the MDP setting.
\subsection*{Episodic MDP}
An episodic MDP can be described by the tuple $M=(\Sc,\Ac, H, P, r)$, where $\Sc$ is the state space, $\Ac$ is the action space, the integer $H$ is the length of each episode, $r=\{r_h\}_{h=1}^H$ are the reward functions and $P=\{P_h\}_{h=1}^H$ are the transition probability distributions.\footnote{We intentionally do not use the standard term transition kernel for $P_h$, to avoid confusion with the term kernel in kernel-based learning.}
We use the notation $\Zc=\Sc\times\Ac$ to denote the state-action space. For each $h\in[H]$, the reward $r_h: \Zc\rightarrow [0,1]$ is the reward function at step $h$, which is supposed to be deterministic for simplicity, and $P_h(\cdot|s,a)$ is the unknown transition probability distribution on $\Sc$ for the next state from state-action pair~$(s,a)$.


\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        % Nodes
        \node[state] (s1) {$s_1$};
        \node[state, right=of s1] (s2) {$s_2$};
        \node[right=of s2] (dots) {$\cdots$};
        \node[state, right=of dots] (sH) {$s_H$};

        % Edges
        \draw[->] (s1) to[bend left] node[above] {$a_1$} (s2);
        \draw[->] (s2) to[bend left] (dots);
        \draw[->] (dots) to[bend left] node[above] {$a_{H-1}$} (sH);
    \end{tikzpicture}
    \caption{Illustration of an Episodic MDP with episode of length~$H$.}
    \label{fig:episodic_mdp}
\end{figure}

A policy $\pi=\{\pi_h\}_{h=1}^H$, at each step $h$, determines the (possibly random) action $\pi_h:\Sc\rightarrow \Ac$ taken by the agent at state $s$.  
At the beginning of each episode, the environment picks an arbitrary state $s_1$. The agent determines a policy $\pi=\{\pi_h\}_{h=1}^H$. Then, at each step $h\in[H]$, the agent observes the state $s_h\in\Sc$, and picks an action $a_h=\pi_h(s_h)$. The new state $s_{h+1}$ then is drawn from the transition distribution $P_h(\cdot|s_h, a_h)$. The episode ends when the agent receives the final reward $r_H(s_H,a_H)$.  



We are interested in maximizing the expected total reward in the episode, starting at step $h$, i.e., the value function, defined as
\begin{equation}
V^{\pi}_h(s) = \E\left[\sum_{h'=h}^Hr_{h'}(s_{h'},a_{h'})\bigg|s_{h}=s\right],  ~~~\forall s\in\Sc, h\in[H],
\end{equation}
where the expectation is taken with respect to the randomness in the trajectory $\{(s_h,a_h)\}_{h=1}^H$ obtained by the policy~$\pi$. It can be shown that under mild assumptions (e.g., continuity of $P_h$, compactness of $\Zc$, and boundedness of $r$) there exists an optimal policy $\pi^{\star}$ which attains the maximum possible value of $V^{\pi}_{h}(s)$ at every step and at every state. We use the notation
$
V_{h}^{\star}(s) = \max_{\pi}V_h^{\pi}(s), ~\forall s\in\Sc, h\in[H]
$.
By definition $V^{\pi^{\star}}_h=V_{h}^{\star}$.
An $\epsilon$-optimal policy is defined as follows. 

For $\epsilon>0$, A policy $\pi$ is called $\epsilon$-optimal if it achieves near-optimal values from any initial state as follows:
\[
V_1^{\pi}(s) \geq V_1^{\star}(s) - \epsilon, \quad \forall s \in \Sc.
\]


For a value function $V$, we define the following notation
\begin{equation}
    [P_hV](s,a) := \E_{s'\sim P_h(\cdot|s,a)}[V(s')].  
\end{equation}

We also define the state-action value function $Q^{\pi}_h:\Zc\rightarrow [0,H]$ as follows.
\begin{equation}
Q_h^{\pi}(s,a) = \E_{\pi}\left[
\sum_{h'=h}^Hr_{h'}(s_{h'},a_{h'})\bigg|s_h=s, a_h=a
\right],
\end{equation}
where the expectation is taken with respect to the randomness in the trajectory $\{(s_h,a_h)\}_{h=1}^H$ obtained by the policy~$\pi$.
The Bellman equation associated with a policy $\pi$ then is represented as
\begin{align*}
Q_h^{\pi}(s,a) &= r_h(s,a) + [P_hV^{\pi}_{h+1}](s,a), \\
V_h^{\pi}(s) &= \max_{a}Q_h^{\pi}(s,a),~~
V_{H+1}^{\pi} = \bm{0}.
\end{align*}

where the expectation is taken with respect to the randomness in the policy $\pi$. 
We may specify the reward function in $V^{\pi}, Q^{\pi}, V^{\star}, Q^{\star}$ notations for clarity, for example, $V^{\pi}(s;r)$ and $Q^{\star}(z;r)$. 


We aim to learn $\epsilon$-optimal policies using as small as possible number of collected exploration episodes. 
In particular, we collect
$N$ exploration episodes $\{(s_1^n,a_1^n, s_2^n, a_2^n, \cdots, s_{H}^n)\}_{n=1}^N$. Then, we design a policy for reward $r$ using the trajectories collected in the exploration phase. We refer to~$N$ as the sample complexity of designing $\epsilon$-optimal policy. Under certain assumptions, the question is: \emph{How many exploration episodes are required to obtain $\epsilon$-optimal policies? How should we collect these trajectories.}


\paragraph{Regret setting:} In this setting, the performance of a policy $\pi$ is measured in terms of the loss in the value function, referred to as \emph{regret}, denoted by $\Rc(T)$ in the following definition
\begin{equation}
\Rc(T) = \sum_{t=1}^T(V^{\star}_1(s_1^t) - V^{\pi^t}_1(s_1^t)),
\end{equation}
where $\pi^t$ is the policy executed by the agent at episode $t$ and $s_1^t$ is the initial state in that episode determined by the environment. 

\paragraph{Model-Based Algorithm:} We design the algorithms by estimating $P_h$. For this purpose, we have the following assumption that $P_h(\cdot|\cdot, \cdot) \in \Hc_k$ where $\Hc_k$ is the RKHS of a known kernel $k$. This is different from~\cite{yang2020provably, vakili2024kernelized} that consider a model-free setting. Roughly, they directly estimate $[P_hV]$ without estimating $P_h$ that appears to cause some issues in algorithm design. 




\bibliographystyle{unsrt}
\bibliography{references}
\end{document}

