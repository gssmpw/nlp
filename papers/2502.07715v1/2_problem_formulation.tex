\section{Preliminaries and Problem Formulation}\label{sec:pf}

In this section, we introduce the episodic MDP setting, describe the reward-free RL framework, provide background on kernel methods, and outline our technical assumptions.

\subsection{Episodic MDP}
An episodic MDP can be described by the tuple $M=(\Sc,\Ac, H, P, r)$, where $\Sc$ denotes the state space, $\Ac$ the action space, and the integer $H$ the length of each episode. Here, $r=\{r_h\}_{h=1}^H$ represents the reward functions, and $P=\{P_h\}_{h=1}^H$ the transition probability distributions.\footnote{We deliberately do not use the standard term transition kernel for $P_h$, to avoid confusion with kernel in kernel-based learning.}
The state-action space is denoted by $\Zc=\Sc\times\Ac$. The notation $z=(s,a)$ is used throughout the paper for state-action pairs. For each step $h\in[H]$, the reward function $r_h: \Zc\rightarrow [0,1]$ is supposed to be deterministic for simplicity, and $P_h(\cdot|s,a)$ is the unknown transition probability distribution on $\Sc$ for the next state given the state-action pair~$(s,a)$.

%\begin{figure}[ht]
%    \centering
%    \begin{tikzpicture}
%
%    \tikzset{state/.style={circle, draw=blue, minimum size=1.1cm, align=center}}
%        % Nodes
%        \node[line width=0.3mm,  blue, state] (s1) {\tcd{$s_1$}};
%        \node[line width=0.3mm,  blue, state, right=of s1] (s2) {\tcd{$s_2$}};
%        \node[right=of s2] (dots) {$\cdots$};
%        \node[line width=0.3mm,  blue, state, right=of dots] (sH) {\tcd{$s_H$}};
%        \node[dashed, line width=0.3mm,  blue, state, right=of sH] (sH1) {\tcd{$s_{H+1}$}};
%
%        % Edges
%        \draw[line width=0.3mm,  blue, ->] (s1) to[bend left] node[above] {\tcd{$a_1$}} (s2);
%        \draw[line width=0.3mm,  blue,->] (s2) to[bend left] (dots);
%        \draw[line width=0.3mm,  blue,->] (dots) to[bend left] node[above] {\tcd{$a_{H-1}$}} (sH);
%        \draw[dashed, line width=0.3mm,  blue,->] (sH) to[bend left] node[above] {\tcd{$a_{H}$}} (sH1);
%    \end{tikzpicture}
%    \caption{Illustration of an Episodic MDP with an episode of length~$H$.}
%    \label{fig:episodic_mdp}
%\end{figure}

A policy $\pi = \{\pi_h: \Sc \rightarrow \Ac\}_{h=1}^H$ determines the action $\pi_h(s)$ ---possibly random--- taken by the agent at state $s$ during each step $h$.  
At the beginning of each episode, the environment picks an arbitrary initial state $s_1$. The agent adopts a policy $\pi=\{\pi_h\}_{h=1}^H$. For each step $h\in[H]$, the agent observes the current state $s_h\in\Sc$, and selects an action $a_h=\pi_h(s_h)$. The subsequent state, $s_{h+1}$, is then drawn from the transition probability distribution $P_h(\cdot|s_h, a_h)$. The episode ends when the agent receives the final reward $r_H(s_H,a_H)$.  



We are interested in maximizing the expected total reward in the episode, starting at step $h$. This is quantified by the value function, which is defined as follows:
\begin{equation}
V^{\pi}_h(s) = \E\left[\sum_{h'=h}^Hr_{h'}(s_{h'},a_{h'})\bigg|s_{h}=s\right], \forall s\in\Sc, h\in[H],
\end{equation}
where the expectation is taken with respect to the randomness in the trajectory $\{(s_h,a_h)\}_{h=1}^H$ obtained by the policy~$\pi$. It can be shown that under mild assumptions (e.g., continuity of $P_h$, compactness of $\Zc$, and boundedness of $r$), there exists an optimal policy $\pi^{\star}$ which attains the maximum possible value of $V^{\pi}_{h}(s)$ at every step and at every state~\citep[e.g., see,][]{puterman2014markov}. We use the notation
$
V_{h}^{\star}(s) = \max_{\pi}V_h^{\pi}(s), ~\forall s\in\Sc, h\in[H]
$.
By definition $V^{\pi^{\star}}_h=V_{h}^{\star}$.
An $\epsilon$-optimal policy is defined as follows. 

\begin{definition}\label{def:epsopt}
($\epsilon$-optimal policy) For $\epsilon>0$, a policy $\pi$ is called $\epsilon$-optimal if it achieves near-optimal values from any initial state as follows:
$
V_1^{\pi}(s) \geq V_1^{\star}(s) - \epsilon, ~~~ \forall s \in \Sc.
$
\end{definition}

Policy design often relies on the expected value of a value function with respect to the transition probability distribution, presented using the following notation:
\begin{equation}
    [P_hV](s,a) := \E_{s'\sim P_h(\cdot|s,a)}[V(s')].  
\end{equation}

We also define the state-action value function $Q^{\pi}_h:\Zc\rightarrow [0,H]$ as follows:
\begin{equation}
Q_h^{\pi}(s,a) = \E_{\pi}\left[
\sum_{h'=h}^Hr_{h'}(s_{h'},a_{h'})\bigg|s_h=s, a_h=a
\right],
\end{equation}
where the expectation is taken with respect to the randomness in the trajectory $\{(s_h,a_h)\}_{h=1}^H$ obtained by the policy~$\pi$.
The Bellman equation associated with a policy $\pi$ is then represented as
\begin{align*}
Q_h^{\pi}(s,a) = r_h(s,a) + [P_hV^{\pi}_{h+1}](s,a), \\
V_h^{\pi}(s) = \max_{a}Q_h^{\pi}(s,a),~~
V_{H+1}^{\pi} = \bm{0}.
\end{align*}
The notation $V=\bm{0}$ is used for $V(s)=0$, for all $s\in\Sc$.
We may specify the reward function in $V^{\pi}, Q^{\pi}, V^{\star}, Q^{\star}$ notations for clarity, for example, $V^{\pi}(s;r)$ and $Q^{\star}(z;r)$. 



\subsection{Reward-Free RL Framework}

We aim to learn $\epsilon$-optimal policies while minimizing the samples collected during exploration.
%the smallest possible number of interactions with the environment during exploration episodes. 
Specifically, we employ the reward-free RL framework, which consists of two phases: {exploration} and {planning}. In the exploration phase, we collect
a dataset $\Dc_{N}=\{\Dc_{h,N}\}_{h\in[H]}$, where each $\Dc_{h,N}=\left\{\left(s_{h,n}, a_{h,n}, s'_{h+1,n}\sim P_h(\cdot|s_{h,n},a_{h,n})\right)\right\}_{n\in[N]}$ consists of $N$ transition samples at step~$h$. Then, in the planning phase, once the reward $r$ is revealed, we design a policy specific to reward $r$ using the data collected during the exploration phase. The number $N$ denotes the sample complexity required to design an $\epsilon$-optimally performing policy. A critical question arises: \emph{How many exploration episodes are necessary to achieve $\epsilon$-optimal policies?} We provide an answer in this work. 

\subsection{Kernel Ridge Regression}

A main step in RL with function approximation, keeping the Bellman equation in mind, is to derive statistical predictions and bounds for the expected value function
$[PV]:\Zc\rightarrow \Rr$, for some given value function $V:\Sc\rightarrow \Rr$ and conditional probability distribution $P(\cdot|z)$. Let us use the notation $f=[PV]$. Suppose that we are given $n$ noisy observations of $f$, represented as $\left\{(z_i, y_i=f(z_i)+\varepsilon_i)\right\}_{i\in[n]}$, where $\varepsilon_i$ denotes zero-mean random noise. Provided a positive definite kernel $k: \Zc\times\Zc\rightarrow \Rr$ and employing kernel ridge regression, we can make the following prediction for $f$:
\begin{equation}
    \hat{f}_n(z) = k^{\top}_n(z)(K_n+\tau^2 I)^{-1}\bm{y}_n,
\end{equation}
where $k_n(z)=[k(z,z_1), k(z,z_2), \cdots, k(z,z_n)]^{\top}$ is the pairwise kernel values between $z$ and observation points, $K_n=[k(z_i, z_j)]_{i,j\in[n]}$ is the Gram matrix, $\bm{y}_n=[y_1, y_2, \cdots, y_n]^{\top}$ is the vector of observations, $\tau>0$ is a free parameter, and $I$ is the identity matrix, appropriately sized to match the dimensions of $K_n$. In addition, the following uncertainty estimate can be utilized to bound the prediction error:
\begin{equation}\label{eq:krrvar}
    \sigma_n^2(z)= k(z,z)-k^{\top}_n(z)(K_n+\tau^2 I)^{-1}k_n(z)
\end{equation}
In particular, various $1-\delta$ confidence intervals of the form $|f(z)-\hat{f}_n(z)|\le \beta(\delta) \sigma_n(z)$, under various assumptions, are proven, where $\beta(\delta)$ is a confidence interval width multiplier that depends on the setting and assumptions~\citep{abbasi2013online, chowdhury2017kernelized, vakili2021optimal,  whitehouse2024sublinear}.
One of our primary contributions is establishing a novel confidence intervals for $f=[PV]$, applicable to our RL setting.
Equipped with the confidence intervals, we are able to design policies using least squares value iteration or its \emph{optimistic} variant.


\paragraph{Reproducing Kernel Hilbert Spaces and Mercer Representation:} Mercer theorem provides a representation of a positive definite kernel function $k$ using an infinite-dimensional feature map: $k(z,z')=\sum_{m=1}^{\infty}\gamma_m\varphi_m(z)\varphi_m(z')$,
% \begin{eqnarray}\label{eq:Mercer}
% k(z,z')=\sum_{m=1}^{\infty}\gamma_m\varphi_m(z)\varphi_m(z'),
% \end{eqnarray}
where $\gamma_m > 0$ are referred to as Mercer eigenvalues, and $\varphi_m$ are the corresponding eigenfunctions. The Reproducing Kernel Hilbert Space (RKHS) associated with $k$, denoted $\Hc_k$, is defined as:
$
    \Hc_k =\{f: f =\sum_{m=1}^{\infty}w_m\phi_m,~~~ w_m\in\Rr,~ \|\bm{w}\|<\infty\},
$
where $\phi_m := \sqrt{\gamma_m} \varphi_m$ form an orthonormal basis of $\Hc_k$. Here, $\bm{w} = [w_1, w_2, \cdots]^{\top}$ represents a possibly infinite-dimensional weight vector. The RKHS norm of $f$ is defined as $\|f\|_{\Hc_k}=\|\bm{w}\|$, the $\ell^2$ norm of the weight vector. Formal statements and further details are provided in Appendix~\ref{appx:rkhs}.

To effectively use the confidence intervals established by the kernel-based models on $f$,
we require the following assumption.
\begin{assumption}\label{ass:rkhsnorm}
    We assume $P_h(s|\cdot, \cdot)\in \Hc_k$, for some positive definite kernel $k$, and $\|P_h(s|\cdot, \cdot)\|_{\Hc_k}\le 1$, for all $s\in \Sc$ and $h\in[H]$. 
\end{assumption}

Consequently, for all $V:\Sc\rightarrow [0,H]$, we have $\|[PV]\|_{\Hc_k}=\Oc(H)$. See~\cite{yeh2023sample}, Lemma~$3$,
for a proof.




\paragraph{Information Gain and Eigendecay:}
The analytical results in kernel-based RL and bandits are often given in terms of a kernel specific complexity term referred to as maximum information gain, defined as follows~\citep{srinivas2009gaussian, vakili2021information}:
%
\begin{equation}
    \Gamma(n)=\sup_{\{z_i\}_{i=1}^{n}\subset \Zc}\frac{1}{2}\log\det(I+\frac{K_n}{\tau^2}),
\end{equation}
%
Maximum information gain depends on the eigendecay defined as follows. 
\begin{definition}\label{def:eigendecay}
    A kernel $k$ is said to have a polynomial (exponential) eigendecay if $\gamma_m=\Oc(m^{-p})$ ($\gamma_m=\Oc(c^{m})$), for some $p>1$ ($c<1$), where $\gamma_m$ are the Mercer eigenvalues in decreasing order. 
\end{definition}
For kernels with polynomial and exponential eigendecays, $\Gamma(n) = \Oct(n^{\frac{1}{p}})$ and $\Gamma(n) = \Oc(\text{polylog}(n))$, respectively~\citep{vakili2021information}.

% We highlight our results for kernels with polynomial eigendecay since the existing work~\cite{qiu2021reward} does not apply to this case.

% In the model-based approach, we obtain this by estimating the mean embedding of the conditional distribution $P(.|s,a)$. For a kernel $k_{\psi}$ on state space $\Sc$, the mean embedding of the conditional distribution $P(\cdot|s,a)$ in $\Hc_{\psi}$ is an element $\mu^{(s,a)}\in \Hc_\phi$ such that
% \begin{equation*}
%     \forall V\in \Hc_{\psi}, ~~~ \E_{X\sim P(\cdot|s,a)}[V(X)] = \langle V, \mu^{(s,a)}\rangle_{\Hc_{\psi}}.
% \end{equation*}

% The mean embedding can be explicitly expressed as a function
% \begin{equation*}
%     \mu^{(s,a)}(\cdot) = \E_{X\sim P(\cdot|s,a) k_{\psi}(X,\cdot)} 
% \end{equation*}

% The mean embedding admits a linear representation in state-action features via conditional embedding operator $O_P$ such that
% \begin{equation*}
%     \forall(s,a)\in \Sc\times \Ac, ~~~ \mu^{s,a} = O_P\phi(s,a)
% \end{equation*}


% \paragraph{Sample estimate:} At the beginning of each episode $t$, given the observations $\Dc_t=(s_h^{\tau}, a_h^{\tau}, s_{h+1}^{\tau})_{\tau<t, h\le H}$, we consider sample based estimate of the conditional embedding operator.
% This is achieved by solving the following ridge-regression problem:
% \begin{equation}
% \min_{\Theta \in {HS}(\Hc_\phi, \Hc_\psi)} \sum_{\tau<t, h\leq H} \left\|\psi(s_{h+1}^{\tau}) - O\phi(s_{h}^{\tau}, a_{h}^{\tau})\right\|_{H\psi}^2 + \lambda \|\Theta\|_{\mathcal{HS}}^2, \quad (6)
% \end{equation}
% where $\lambda > 0$ is a regularizing constant. The solution of Equation (6) is given by:
% \begin{equation}
% \hat{O}^t = \left( \sum_{\tau<t, h\leq H} \psi(s_{h+1}^{\tau}) \otimes \phi(s_{h}^{\tau}, a_{h}^{\tau}) \right) \left( \hat{C}_{\phi,t} + \lambda I \right)^{-1}, \quad (7)
% \end{equation}
% where $\hat{C}_{\phi,t}:= \sum_{\tau<t, h\leq H} \phi(s_{h}^{\tau}, a_{h}^{\tau}) \otimes \phi(s_{h}^{\tau}, a_{h}^{\tau})$. To simplify notations, we now let $n = (t-1)H$ denote the total number of steps completed at the beginning of episode $t$. We denote a vector $k_{\phi,t}(s, a) \in \mathbb{R}^{n}$ and a matrix $K_{\phi,t} \in \mathbb{R}^{n \times n}$ by:
% \begin{align}
% k_{\phi,t}(s, a) &:= \left[k_{\phi} \left((s_{h}^{\tau}, a_{h}^{\tau}),(s, a)\right)\right]_{\tau<t, h\leq H}, \\
% K_{\phi,t} &:= \left[k_{\phi}\left((s_{h}^{\tau}, a_{h}^{\tau}),(s^{\tau'}_{h'}, a^{\tau'}_{h'})\right)\right]_{\tau, \tau'<t, h, h'\leq H}.
% \end{align}


% Then, the conditional mean embeddings can be estimated as
% \begin{equation}
% \muhat_t^{(s,a)} = \hat{O}^t\phi(s, a) = \sum_{\tau<t, h\leq H} [\alpha_{t}(s, a)]_{(\tau,h)} \psi(s^{\tau}_{h+1}), \quad (8)
% \end{equation}
% where we define the weight vector \(\alpha_{t}(s, a)\) as:
% \begin{equation}
% \alpha_{t}(s, a) := \left( K_{\phi,t} +\lambda I \right)^{-1}k_{\phi,t}(s, a).
% \end{equation}


% \begin{theorem}[Concentration of conditional embedding operator]





% \end{theorem}