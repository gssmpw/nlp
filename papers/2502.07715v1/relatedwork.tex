\section{Related Work}
\label{sec:relatedworks}
In this Section, we first provide a technical comparison between our work and that of \citet{qiu2021reward} and \citet{vakilireward}. Following this, we present a more comprehensive literature review, including related works that were not covered in the main paper. We also include a summary table of the sample complexity results in the reward-free RL setting, highlighting our key contributions.

\subsection{Comparison to the Existing Work} \label{appx:comp}

Here, we discuss the key differences between our approach and the closely related works of \citet{qiu2021reward} and \citet{vakilireward}. In ~\citet{qiu2021reward}, they conduct exploration by accumulating standard deviation over an episode, then they apply a planning phase-like algorithm to maximize a reward proportional to $\beta(\delta)\sigma_{h,n}$ at each step of an episode. However, this approach can inflate the confidence interval width multiplier $\beta(\delta)$ by a factor of $\sqrt{\Gamma(n)}$, potentially leading to suboptimal or even trivial sample complexities when $\sqrt{\Gamma(n)}$ is large, as seen in \cite{qiu2021reward}. Specifically, their results are applicable to very smooth kernels like SE, with exponentially decaying Mercer eigenvalues, for which $\Gamma(n)=\Oc(\text{polylog}(n))$. For kernels with polynomial eigendecay, where $\Gamma(n)=\Oc(n^{\frac{1}{p+1}})$ grows polynomially with $n$, this algorithm possibly leads to trivial (infinite) sample complexities. Intuitively, the inflation of $\beta(\delta)$ is due to the  adaptive sampling creating statistical dependencies among observations, specifically through next state transitions. When such dependencies exist, the best existing confidence intervals are based on a kernel adaptation of self-normalized vector values martingales \citep{abbasi2013online}. The $\sqrt{\Gamma(n)}$ term cannot be removed in general for adaptive samples that introduce bias, as was discussed in~\citet{vakili2024open} and~\citet{lattimore2023lower}.  

\cite{vakilireward} utilizes domain partitioning, relying on only a subset of samples to obtain confidence intervals. This approach achieves order-optimal sample complexity for kernels with polynomial eigendecay, offering an $H$-factor improvement compared to our work in the online setting. However, firstly, their results are limited by specific assumptions regarding the relationship between kernel eigenvalues and domain size, which reduces the generality of their findings. Secondly, their domain partitioning method is cumbersome to implement and lacks practical justification, as it requires dropping samples from other subdomains. In contrast, our algorithm achieves order-optimal results for general kernels with a simpler approach that leverages statistical independence. Moreover, our method is well-suited to the generative setting, where their approach offers no clear advantages.

\subsection{Literature Review} \label{appx:lit_review}
\begin{table}[h]
\caption{Existing sample complexities in reward-free RL. $\Sc$, $\Ac$, $H$, $d$ and $p$ represent the state space, action space, episode length, state-action space dimension and parameter of the kernel with polynomial eigendecay, respectively. Last two rows correspond to the performance guarantees for the algorithms proposed in this work.\\ }
\label{samplecomplexitytable}
\centering
\begin{tabular}{ll}
\hline
Setting      & Sample complexity                                                           \\ \hline
Tabular \citep{jin2020reward}     & $\Oc\left(\frac{|\Sc|^2|\Ac| H^5}{\epsilon^2}\right)$                       \\
Linear \citep{wang2020reward}       & $\Oct\left(\frac{d^3 H^6}{\epsilon^2}\right)$                                \\
Kernel-based (exponential eigendecay)  \citep{qiu2021reward} & $\Oc\left(\frac{H^6 \text{polylog} (\frac{1}{\epsilon})}{\epsilon^2}\right)$ \\
Kernel-based (polynomial eigendecay) \citep{vakilireward} 
&$\Oct\left((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$  \\ \hline
\textbf{Kernel-based (exponential eigendecay) (this work)} & $\Oct\left(\frac{H^7 \text{polylog} (\frac{1}{\epsilon})}{\epsilon^2}\right)$ \\ 
\textbf{Kernel-based (polynomial eigendecay) (this work) }
&$\Oct \left(H(\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$  \\ \hline
\end{tabular}
\end{table}

Numerous studies have addressed the sample complexity problem in the discounted MDP framework with an infinite horizon, where the agent has sampling access to a generative model, such as \citep{kearns1998finite,gheshlaghi2013minimax,agarwal2020model}. Alternatively, other research has focused on the episodic MDP framework, without reliance on a generative model or an exploratory policy. Both the tabular setting \citep{jin2018q,auer2008near,bartlett2012regal} and the linear setting \citep{jin2020provably,yao2014pseudo,russo2019worst,zanette2020frequentist,neu2020unifying} have been thoroughly examined. Recent literature has extended these techniques to the kernel setting \citep{yang2020provably,yang2020reinforcement,chowdhury2019online,domingues2021kernel,vakili2024kernelized}, although further improvements are needed in achieving better regret bounds. In contrast to these prior works which assume that the reward function is provided, we explore the episodic reward-free setting in this work, both with and without a generative model. This setting is significantly different from standard RL, rendering the existing sample complexity results inapplicable to our context. 

In the context of reward-free RL, numerous empirical studies have proposed various exploration methods from a practical perspective, as demonstrated by works such as \citep{bellemare2016unifying,pathak2017curiosity,hazan2019provably}. Theoretically, researchers have explored the reward-free RL framework across different levels of complexity, ranging from tabular to linear, kernel-based, and deep learning-based models \citep{jin2020reward,wang2020reward, qiu2021reward} (Table \ref{samplecomplexitytable}). %In the tabular case, \citep{jin2020reward} achieve a sample complexity of $\Oc\left(\frac{|\Sc|^2|\Ac| H^5}{\epsilon^2}\right)$, while in the linear function approximation case,  \citep{wang2020reward} prove a sample complexity of $\Oc\left(\frac{d^3 H^6}{\epsilon^2}\right)$. Here $\Sc$, $\Ac$, $H$ and $d$ represent the state and action spaces, the episode length, and state-action space dimension respectively.% 
Although the existing literature adequately covers the tabular and linear settings, it often provides only partial and incomplete findings when addressing the more intricate kernel-based and deep learning settings. The most relevant work in the kernel setting is \citet{qiu2021reward}, which provides a reward-free algorithm whose sample complexity is $\Oc\left(\frac{H^6 \text{polylog} (\frac{1}{\epsilon})}{\epsilon^2}\right)$. Their results however are only applicable to very smooth kernels with exponentially decaying eigenvalues. The recent work of \citet{vakilireward} proved a sample complexity of $\Oct\left((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$ for kernels with polynomial eigendecay. However, they employ a niche domain partitioning technique that, despite its theoretical appeal, is cumbersome to implement and raises practical concerns, as mentioned earlier.
% The recent work by \textcolor{red}{[cite new ICML paper Reward-Free Kernel-Based Reinforcement Learning]} improves over their work by leveraging a domain partitioning technique, inspired by \citep{vakili2024kernelized}, to achieve a better sample complexity applicable to kernels with polynomially decaying eigenvalues. They demonstrate that $\Oc\left((\frac{H^3}{\epsilon})^{2+\frac{2d}{\alpha}}\right)$ exploration episodes are sufficient to obtain $\epsilon$-optimal policies during planning. Here, $\alpha$ denotes the smoothness of the kernel. Our paper contributes by improving the sample complexity within this framework for any positive definite kernel, and by eliminating the need for impractical domain partitioning techniques.

Finally, it's important to mention that the planning phase of our proposed algorithm is similar to the problem of learning a good policy from predefined datasets, typically called batch or offline RL \citep{levine2020offline}. Many prior works on offline RL make the coverage assumption on the dataset, requiring it to sufficiently include any possible state-action pairs with a minimum probability \citep{precup2000eligibility,antos2008learning,chen2019information,munos2008finite}. These works do not address the exploration needed to achieve such good coverage, which is where our reward-free approach significantly differs. Our goal is to demonstrate how to collect sufficient exploration data without any reward information, enabling the design of a near-optimal policy for any reward function during the planning phase.
%