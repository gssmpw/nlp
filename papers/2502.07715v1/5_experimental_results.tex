\section{Experiments}\label{sec:exp}

%    \includegraphics[width=0.32\textwidth]{figures/Matern2.5_all_algos.png} % Use the same path as before
%    \caption{Average suboptimality gap against $N$ for the Mat{\'e}rn Kernel with $\nu=2.5$. The error bars indicate standard deviation.}
%    \label{fig:Matern2.5_all_algos_single}
%\end{figure}


We numerically validate our proposed algorithms and compare with the baseline algorithms. From the literature, we implement~\cite{qiu2021reward}, in which the exploration aims at maximizing a hypothetical reward of $\beta\sigma_n/H$ over each episode $n$. The planning phase is similar to Algorithm~\ref{alg:plan}, but with upper confidence bounds based on kernel ridge regression being used rather than the prediction~$\hat{g}_h$. 
%We numerically validate and compare the performance of several algorithms. We implement the existing approach of~\cite{qiu2021reward} where during the exploration phase at each episode $n$, a policy is designed to obtain high value with respect to a hypothetical reward of $\beta\sigma_n/H$. The planning phase is similar to Algorithm~\ref{alg:plan} with a minor difference that rather than kernel ridge predictions, upper confidence bounds based on kernel ridge regression are used. 
We also implement our exploration algorithms with and without a generative model: Algorithms~\ref{alg:exp_gen} and~\ref{alg:exp2} respectively. Additionally, we implement a heuristic variation of Algorithm~\ref{alg:exp2}, which collects the exploration samples in a greedy manner $a_{h,n}=\argmax_{a\in\Ac}\sigma_{h,n}(s_{h,n},a)$ while remaining on the Markovian trajectory by sampling $s_{h+1}\sim P(\cdot|s_h,a_h)$. We refer to this heuristic as \emph{Greedy Max Variance}. For all these  algorithms, we use Algorithm~\ref{alg:plan} to obtain a planning policy. In the experimental setting, we choose $H=10$ and $\Sc=\Ac=[0,1]$ consisting of $100$ evenly spaced points. We choose $r$ and $P$ from the RKHS of a fixed kernel. For the detailed framework and hyperparameters, please refer to Appendix~\ref{appx:exp}. We run the experiment for three different kernels across all $4$ algorithms for $80$ independent runs, and plot the average suboptimality gap $V_1^{\star}(s)-V^{\pi}(s)$ for $N=10,20, 40, 80, 160$, as shown in Figure~\ref{fig:overallresults}.
Our proposed Algorithm~\ref{alg:exp2}, without generative model, demonstrates better performance compared to prior work \citep{qiu2021reward} across all three kernels, validating the improved sample efficiency. Notably, \citet{qiu2021reward} performs poorly with nonsmooth kernels. Greedy Max Variance is a heuristic that in many of our experiments performs close to Algorithm~\ref{alg:exp2}.
Furthermore, with access to a generative model, Algorithm~\ref{alg:exp_gen} performs the best. This is anticipated, as the generative model provides the flexibility to select the most informative state-action pairs, unconstrained by Markovian transitions.


%We adopt an episodic MDP framework with episodes of fixed length, set to $H=10$ and we define the state and action spaces as consisting of 100 evenly spaced points within the interval [0,1]. The probability transition distribution and reward functions are chosen as general functions within the Reproducing Kernel Hilbert Space (RKHS). The process of generating $P(s,a,s')$ and $r(s,a)$ involves utilizing Gaussian Process (GP) regression with a chosen kernel. In this process, a subset of inputs $X=(s,a,s')$ is sampled from the state and action spaces for $P$, and $X=(s,a)$ is sampled for $r$. Their corresponding outputs $y$ are drawn from a Gaussian Process (GP) and evaluated at $X$. Subsequently, GP regression is performed between these inputs and outputs, and the resulting estimated mean is appropriately scaled and normalized to represent $P$ or $r$. Various kernels are explored in generating these functions, including Radial Basis Function (RBF) (length scale=$0.1$ and $\tau= 0.01$) and Mat{\'e}rn kernels (smoothness= $1.5$ / $2.5$, length scale=$0.1$ and $\tau=0.5)$ . Refer to the Appendix for plots of $r$ and $P$ corresponding to each kernel (Figures ~\ref{fig:R_P_RBF}, ~\ref{fig:R_P_Matern1.5}, ~\ref{fig:R_P_Matern2.5}).

%For each algorithm, we alternate between exploration and planning phases. After training for each of $N=10,20,40,80,160$ exploration episodes, we execute the planning phase to derive the policy from the state-action pairs collected so far during the exploration phase, and measure the regret as: $V_1^{\star}(s)-V_1^{\pi}(s)$. The optimal value function $V^{\star}$ is obtained by running the value iteration algorithm for $H$ steps, and $V^{\pi}$ is obtained by averaging the return after unrolling the learned policy for $20$ episodes during the planning phase. We evaluate the regret at the initial state which is fixed at the start of each episode for all algorithms in both the exploration and planning phases. We plot the regret performance for each algorithm averaged over $80$ independent runs. The simulations were executed on a cluster which has $376.2$ GiB of RAM, and an Intel(R) Xeon(R) Gold 5118 CPU running at $2.30$ GHz.

%It's important to note that we select the best values of the Upper Confidence Bound (UCB) coefficient: $\beta$, for both of our proposed algorithm without generative model and the algorithm introduced by \citep{qiu2021reward} by conducting a grid search across a limited set of values [0.1, 1, 10, 100] for each kernel. The plots of the regret for different values $\beta$ are included in the Appendix (Figures \ref{fig:Hyperparams_RBF}, \ref{fig:Hyperparams_Matern1.5},\ref{fig:Hyperparams_Matern2.5}). We have found that the existing work \citep{qiu2021reward} is more sensitive to the hyperparameter $\beta$ than our proposed algorithm, and a smaller value of $\beta=0.1$ leads to lower regret for both.


%Figure  \ref{fig:overallresults} illustrates the average regret during training for all algorithms. Our proposed algorithm without generative model demonstrates lower regret compared to prior work \citep{qiu2021reward} across all three kernels, validating the improved sample efficiency of our algorithm. Notably, \citep{qiu2021reward} fails to converge after 160 episodes when nonsmooth kernels are utilized (Mat{\'e}rn $1.5$ and $2.5$). Both Greedy Max Variance and our proposed algorithm without a generative model perform similarly, and both surpass the performance of the existing work \citep{qiu2021reward}.
%Furthermore, with access to a generative model, the regret is the lowest among all tested algorithms across all three kernels. This outcome is anticipated, as the generative model has the flexibility to select state-action pairs that optimize performance, unconstrained by Markovian transitions.








 