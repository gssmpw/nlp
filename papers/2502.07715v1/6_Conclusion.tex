\section{Conclusion}
\label{sec:conclusion}

We proposed novel algorithms for the kernel-based reward-free RL problem, both with and without generative models. We demonstrated that, with a generative model, a simple algorithm can achieve near-optimal sample complexities. Without the generative model requirement, we showed that an online algorithm achieves the same sample complexity up to a factor of $H$, indicating that the cost of online sampling is a factor of $H$. Our results apply to a general class of kernels, including those with polynomial eigendecay, where existing methods may either lead to vacuous sample complexities~\citep{qiu2021reward} or require additional assumptions and a sophisticated, difficult-to-implement domain partitioning method~\citep{vakilireward}. Our experimental results support these analytical findings. In comparison to the lower bounds proven for the degenerate case of bandits with $H=1$ for the Mat√©rn kernel, the order optimality of our results in terms of $\epsilon$ becomes clear.


% In summary, the analysis of reward-free RL has predominantly focused on tabular and linear settings. However, this paper delves into the kernel-based setting, where prior works have shown suboptimal sample complexity, particularly for non-smooth kernels, or have relied on intricate domain partitioning techniques to address this limitation. Our contribution lies in the introduction of algorithms tailored for both exploration and planning phases, with and without sampling access to a generative model. We demonstrate order-optimal sample complexities in both scenarios, applicable to a wider range of kernels, thus pushing the boundaries of current research. Unlike previous kernel-based methodologies, our algorithm is straightforward and applicable to all Mercer kernels without the need for domain partitioning. Furthermore, we support our theoretical findings with empirical studies that confirm the enhanced sample efficiency compared to other benchmark methods.

% \paragraph{Limitations:}

