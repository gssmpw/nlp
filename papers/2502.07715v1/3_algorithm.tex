\section{Algorithm Description}
\label{sec:alg}

We now present our algorithms for both the exploration and planning phases. We begin by presenting the algorithm for the planning phase, as it remains unchanged across various exploration algorithms.
\begin{algorithm}[ht]
   \caption{Planning Phase}
   \label{alg:plan}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\tau$, $\beta$, $\delta$, $k$, $M(\Sc,\Ac, H, P, r )$, and exploration dataset $\Dc_{N}$.
   \FOR{$h=H, H-1, \cdots, 1,$}
        \STATE Compute the prediction $\hat{g}_h$ according to~\eqref{eq:ghn};
        \STATE Let $Q_h(\cdot, \cdot) = \Pi_{[0,H]}[\hat{g}_h(\cdot, \cdot)+r_h(\cdot, \cdot)] $;
        \STATE $V_h(\cdot)= \max_{a\in\Ac}Q_h(\cdot, a)$;
        \STATE $\pi_h(\cdot) = \argmax_{a\in\Ac}Q_h(\cdot,a)$;
   \ENDFOR
   \STATE {\bfseries Output:} $\{\pi_h\}_{h\in[H]}$. 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Exploration Phase \textbf{with} Generative Model}\label{alg:exp_gen}
\begin{algorithmic}[1]
\REQUIRE $\tau$, $k$, $\Sc$, $\Ac$, $H$, $P$, $N$;
\STATE Initialize $\Dc_{h,0}=\{\}$, for all $h\in[H]$;
\FOR{$n=1,2,\cdots, N$}
    \FOR{$h=1,2,\cdots, H$}
    \STATE Let $s_{h,n}, a_{h,n} = \argmax_{s,a\in\Ac}\sigma_{h, n-1}(s,a)$;
    \STATE Observe $s'_{h+1,n}\sim P_h(\cdot|s_{h,n}, a_{h,n})$;
    \STATE Update $\Dc_h^n \bigcup\{s_{h,n}, a_{h,n}, s'_{h+1,n}\}$.
    \ENDFOR
\ENDFOR
\STATE {\bfseries Output:} $\Dc_{N}$. 
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{Exploration Phase \textbf{without} Generative Model}\label{alg:exp2}
\begin{algorithmic}
\REQUIRE $\tau$, $k$, $\beta$, $\delta$, $\Sc$, $\Ac$, $H$, $P$, $N$;
\FOR{$n=1,2,\cdots, N$}
    \FOR{$h_0=1, 2,\cdots, H$}
    \STATE Initialize $V_{h_0+1,n}=\bm{0}$
    \FOR{$h=h_0, h_0-1, \cdots, 1$}
        \STATE Obtain $\hat{f}_{h,(n,h_0)}$; $Q_{h, (n,h_0)}$, and $V_{h,(n,h_0)}(\cdot)$ according to~\eqref{eq:mean_predictor} and ~\eqref{eq:value_func}, respectively. 
    \ENDFOR
    \FOR{$h=1,2,\cdots, h_0$}
    \STATE Observe $s_{h,n}$; Take action $a_{h,n} = \argmax_{a\in\Ac}Q_{h, n}(s_{h,n},a)$; 
    \ENDFOR
    \STATE Update $\Dc_{h_0}^n \bigcup\{s_{{h_0},n}, a_{h_0,n}, s_{h_0+1,n}\}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}



\subsection{Planning Phase} In the planning phase, the reward function $r$ is revealed to the learner. In addition, a dataset $\Dc_N=\{\Dc_{h,N}\}_{h\in[H]}$ is available, with $\Dc_{h,N}=\{s_{h,n}, a_{h,n}, s'_{h+1,n}\sim P(\cdot|s_{h,n}, a_{h,n})\}_{ n\in[N]}$ for each step $h\in[H]$. The objective is to leverage the knowledge of the reward function and utilize the dataset to design a near-optimal policy.
As mentioned in the introduction, the planning phase comprises of an offline RL design without further interaction with the environment.  


In the planning phase of our algorithm, we derive a policy using least squares value iteration. Specifically, at step $h$, we compute a prediction, $\hat{g}_{h}$, for the expected value function in the next step $[P_hV_{h+1}]$. We then define
\begin{equation}\label{eq:Qplan}
    Q_h(\cdot, \cdot) = \Pi_{[0,H]}\left[r_h(\cdot, \cdot)+ \hat{g}_h(\cdot, \cdot)\right],
\end{equation}
where $\Pi_{[a,b]}$ denotes projection on $[a,b]$ interval. 
The policy $\pi$ is then obtained as a greedy policy with respect to $Q$. For each $h\in[H]$,
\begin{equation*}
    \pi_h(\cdot) = \argmax_{a\in\Ac}Q_h(\cdot, a).
\end{equation*}
We now detail the computation of $\hat{g}_h$. Keeping the Bellman equation in mind and starting with $V_{H+1}=\bm{0}$, $\hat{g}_h$ is the kernel ridge predictor for $[P_hV_{h+1}]$. This prediction uses $N$ observations $\bm{y}_{h}=[V_{h+1}(s'_{h+1, 1}),V_{h+1}(s'_{h+1, 2}), \cdots,V_{h+1}(s'_{h+1, N}) ]^{\top}$ at points $\{z_{h,n}\}_{n=1}^N$. 
Recall that 
$
\E_{s'\sim P(\cdot|z_{h, n})}\left[V
_{h+1}(s')  \right]  = [P_hV_{h+1}](z_{h,n})
$.
The observation noise $V_{h+1}(s'_{h+1,n})-[P_hV_{h+1}](z_{h,n})$ is due to random transitions and is bounded by $H-h\le H$. Specifically, 
\begin{equation}\label{eq:ghn}
    \hat{g}_h(z) = k^{\top}_{h,N}(z)(\tau^2 I + K_{h,N})^{-1} \bm{y}_{h},
\end{equation}
where $k_{h,N}(z)=[k(z,z_{h,1}), k(z,z_{h,2}), \cdots, k(z,z_{h,N})]^{\top}$ is the pairwise kernel values between $z$ and observation points and $K_{h,N}=[k(z_{h,i}, z_{h,j})]_{i,j\in[N]}$ is the Gram matrix.
We then define $Q_h$ according to~\eqref{eq:Qplan} and also set
\begin{equation*}
    V_h(s)=\max_{a\in\Ac} Q_h(s, a).
\end{equation*}
The values of $\hat{g}_h$, $Q_h$ and $V_h$ are obtained recursively for $h=H,H-1, \cdots, 1$.
For a pseudocode, see Algorithm~\ref{alg:plan}.


% \sattar{Comment on how this is slightly different from Qiu et al. }

%\begin{algorithm}[tb]
%   \caption{Planning Phase}
%   \label{alg:plan}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} $\tau$, $\beta$, $\delta$, $k$, $M(\Sc,\Ac, H, P, r )$, and exploration data set $\Dc_{N}$.
%   \FOR{$h=H, H-1, \cdots, 1,$}
%        \STATE Compute the prediction $\hat{g}_h$ according to~\eqref{eq:ghn};
%        \STATE Let $Q_h(\cdot, \cdot) = \Pi_{[0,H]}[\hat{g}_h(\cdot, \cdot)+r_h(\cdot, \cdot)] $;
%        \STATE $V_h(\cdot)= \max_{a\in\Ac}Q_h(\cdot, a)$;
%        \STATE $\pi_h(\cdot) = \argmax_{a\in\Ac}Q_h(\cdot,a)$;
%   \ENDFOR
%   \STATE {\bfseries Output:} $\{\pi_h\}_{h\in[H]}$. 
%\end{algorithmic}
%\end{algorithm}


\subsection{Exploration Phase} In the exploration phase, the algorithm collects a dataset
$\Dc_{N}=\{\Dc_{h,N}\}_{h\in[H]}$, where
$\Dc_{h,N}=\{s_{h,n},a_{h,n}, s'_{h+1,n}\}_{h\in[H], n\in[N]}$ for each $h\in[H]$, %. As we showed, these observations will be 
later used in the planning phase to design a near-optimal policy. The primary goal during this phase is to gather the most informative observations.

% As discussed in the introduction, existing work fails to achieve order-optimal or even finite sample complexities without imposing strong and restrictive assumptions %such as assuming a kernel with exponential eigendecay.
Initially, we consider a preliminary case where a \emph{generative model}~\citep{kakade2003sample} is present that can produce transitions for the state-actions selected by the algorithm. Under this setting, we demonstrate that a simple rule for data collection leads to improved and desirable sample complexities. Inspired by these results, we introduce a novel algorithm that completely relaxes the requirement for a generative model, at the price of increasing the number of exploration episodes by a factor of $H$. The key aspect of our algorithms is the \emph{unbiasedness}--statistical independence of the collected samples, which means that the observation points do not depend on previous transitions.

\subsubsection{Exploration with a Generative Model}

In this section, we outline the exploration phase when a generative model is present. At each step $h$ of the current exploration episode, uncertainties derived from kernel ridge regression are employed to guide exploration. Specifically, let
\begin{equation} \label{eq:std_dev}
   \sigma^2_{h,n}(z) = k(z,z) -k^{\top}_{h,n}(z)(\tau^2I+K_{h,n})^{-1}k_{h,n}(z) 
\end{equation}
where $k_{h,n}(z) = [k(z,z_{h,1}),k(z,z_{h,2}), \cdots, k(z,z_{h,n}) ]^{\top}$ is the vector of kernel values between the state-action of interest and past observations in $\Dc_{h,n}$, and $K_{h,n}=[k(z_{h,i}, z_{h,j})]_{i,j=1}^n$ is the Gram matrix of pairwise kernel values between past observations in $\Dc_{h,n}$. Equipped with $\sigma_{h,n}(z)$, at step $h$, we select
\begin{equation}\label{eq:sel_rule}
    s_{h,n}, a_{h,n}=\argmax_{s\in\Sc,a\in\Ac} \sigma_{h,n-1}(s,a),
\end{equation}
and observe the next state $s'_{h+1,n}\sim P(\cdot|s_{h,n}, a_{h,n})$. We then add this data point to the dataset and update $\Dc_{h,n}=\Dc_{h,n-1}\cup\{(s_{h,n}, a_{h,n}, s'_{h+1,n})\}$. For a pseudocode, see Algorithm~\ref{alg:exp_gen}.

%\begin{algorithm}[ht]
%\caption{Exploration Phase \textbf{with} Generative Model}\label{alg:exp_gen}
%\begin{algorithmic}[1]
%\REQUIRE $\tau$, $k$, $\Sc$, $\Ac$, $H$, $P$, $N$;
%\STATE Initialize $\Dc_{h,0}=\{\}$, for all $h\in[H]$;
%\FOR{$n=1,2,\cdots, N$}
%    \FOR{$h=1,2,\cdots, H$}
%    \STATE Let $s_{h,n}, a_{h,n} = \argmax_{s,a\in\Ac}\sigma_{h, n-1}(s,a)$;
%    \STATE Observe $s'_{h+1,n}\sim P_h(\cdot|s_{h,n}, a_{h,n})$;
%    \STATE Update $\Dc_h^n \bigcup\{s_{h,n}, a_{h,n}, s'_{h+1,n}\}$.
%    \ENDFOR
%\ENDFOR
%\STATE {\bfseries Output:} $\Dc_{N}$. 
%\end{algorithmic}
%\end{algorithm}
We highlight that the selection rule~\eqref{eq:sel_rule} relies on the generative model that allows the algorithm to deviate from the Markovian trajectory and move to a state of its choice. Since observations are selected based on maximizing $\sigma_{h,n-1}$, which by definition \eqref{eq:std_dev} does not depend on previous transitions $\{s'_{h+1,i}\}_{i=1}^{n-1}$,  the statistical independence conveniently holds. The generative model setting is feasible in contexts such as games, where the player can manually set the current state. However, this may not always be possible in other scenarios. Next, we introduce our online algorithm, which strictly stays on the Markovian trajectory.




\subsubsection{Exploration without Generative Models}

In this section, we show that a straightforward algorithm, in contrast to existing approaches, achieves near-optimal performance in an online setting without requiring a generative model. Compared to the scenario with a generative model, the sample complexity of this algorithm increases by a factor of $H$. For a detailed and technical comparison with existing work, please refer to Appendix~\ref{appx:comp}. %, highlighting how it may lead to trivial (infinite) sample complexities.

Our online algorithm operates as follows: in each exploration episode, only one data point specific to a step $h$ is collected ---this accounts for the $H$ scaling in sample complexity. This observation however is collected in an unbiased way, which eventually leads to tighter performance guarantees. Specifically, at episode $nH+h_0$, where $n\in[N]$ and $h_0\in[H]$, the algorithm collects an informative sample for the transition at step $h_0$. This results in a total of $N$ samples at each step over $NH$ episodes. The algorithm initializes $V_{h_0+1,(n,h_0)}=\bm{0}$.
Let $\hat{f}_{h,(n,h_0)}$ and $\sigma_{h,n}$ represent the predictor and uncertainty estimator for $[P_hV_{h+1,(n,h_0)}]$, respectively. These are derived from the historical data $\Dc_{h,n-1}$ of observations at step $h$. Specifically, 
\begin{align}\nn
    \hat{f}_{h,(n,h_0)}(z) &= k^{\top}_{h,n}(z)(K_{h,n}+\tau^2 I)^{-1}\bm{y}_{h,n},  \\
    \sigma^2_{h,n}(z) &= k(z,z) - k^{\top}_{h,n}(z)(K_{h,n}+\tau^2 I)^{-1}k_{h,n}(z), \label{eq:mean_predictor}
\end{align}
where $k_{h,n}(z) = [k(z,z_{h,1}),k(z,z_{h,2}), \cdots, k(z,z_{h,n}) ]^{\top}$ is the vector of kernel values between the state-action of interest and past observations in $\Dc_{h,n}$, $K_{h,n}=[k(z_{h,i}, z_{h,j})]_{i,j=1}^n$ is the Gram matrix of pairwise kernel values between past observations in $\Dc_{h,n}$, and 
\begin{align*}
\bm{y}_{h,(n,h_0)} &= [V_{h+1,(n,h_0)}(s_{h+1,1}), V_{h+1,(n,h_0)}(s_{h+1,2}), \\
&\quad \cdots, V_{h+1,(n,h_0)}(s_{h+1,n}) ]^{\top}
\end{align*} 
is the vector of observations. 
We then have
%\begin{align}\nn 
%Q_{h,(n,h_0)}=\Pi_{0,H}\left[\hat{f}_{h,(n,h_0)}+\beta(\delta)\sigma_{h,n}\right], 
%~~~~~ \text{and} ~~~ V_{h,(n,h_0)}(\cdot) = \max_{a\in\Ac}Q_{h,(n,h_0)}(\cdot, a). 
%\end{align}
\begin{align*}
Q_{h,(n,h_0)} = \Pi_{0,H} \left[ \hat{f}_{h,(n,h_0)} + \beta(\delta) \sigma_{h,n} \right], \\
V_{h,(n,h_0)}(\cdot) = \max_{a \in \Ac} Q_{h,(n,h_0)}(\cdot, a) \label{eq:value_func}.
\end{align*}













%\begin{algorithm}[ht]
%\caption{Exploration Phase \textbf{without} Generative Model}\label{alg:exp2}
%\begin{algorithmic}
%\REQUIRE $\tau$, $k$, $\beta$, $\delta$, $\Sc$, $\Ac$, $H$, $P$, $N$;
%
%\FOR{$n=1,2,\cdots, N$}
%    \FOR{$h_0=1, 2,\cdots, H$}
%    \STATE Initialize $V_{h_0+1,n}=\bm{0}$
%    \FOR{$h=h_0, h_0-1, \cdots, 1$}
%        \STATE Obtain $\hat{f}_{h,n}$; $Q_{h, n}$, and $V_{h,n}(\cdot)$ according to~\eqref{eq:mean_predictor} and %~\eqref{eq:value_func}, respectively. 
%    \ENDFOR
%    \FOR{$h=1,2,\cdots, h_0$}
%    \STATE Observe $s_{h,n}$; Take action $a_{h,n} = \argmax_{a\in\Ac}Q_{h, n}(s_{h,n},a)$; 
%    \ENDFOR
%    \STATE Update $\Dc_{h_0}^n \bigcup\{s_{{h_0},n}, a_{h_0,n}, s_{h_0+1,n}\}$
%    \ENDFOR
%\ENDFOR
%\end{algorithmic}
%\end{algorithm}

The values of $Q_{h,(n,h_0)}$ and $V_{h,(n,h_0)}$ are obtained recursively for all $h\in[h_0]$. 
The exploration policy at episode $nH+h_0$ is then the greedy policy with respect to $Q_{h,(n-1,h_0)}$. The dataset is updated by adding the new observation to the dataset for step $h_0$, such that $\Dc_{h_0,n} = \Dc_{h_0,n-1}\cup \{(s_{h_0,n}, a_{h_0,n}, s_{h_0+1,n})\}$, while datasets for all other steps remain unchanged: $\Dc_{h,n} = \Dc_{h,n-1}$ for all $h\neq h_0$. This specific update ensures that the collected samples are unbiased. More specifically, the sample collected at $h_0$ solely relies on the uncertainty $\sigma_{h_0,n}$, due to the initialization $V_{h_0+1,(n,h_0)}=\bm{0}$ which implies $\hat{f}_{h_0,(n,h_0)}=\bm{0}$. Since $\sigma_{h_0,n}$ does not depend on previous transitions $s_{(h_0+1,i)}$ for any $i\leq n$, the samples at $h=h_0$ are unbiased. However, for \( h < h_0 \), the samples depend on both the uncertainty \( \sigma_{h,n} \) and the prediction \( \hat{f}_{h,(n,h_0)} \)~\eqref{eq:mean_predictor}. Since the prediction depends on the transitions $s_{(h+1,i)}$ for $i\leq n$, these samples are biased. As a result, we discard them and only retain the unbiased samples at \( h = h_0 \). This approach improves the rates in our analysis, albeit at the cost of a factor of~\( H \).

% A detailed technical comparison with the closely related works of~\cite{qiu2021reward} and~\cite{vakilireward} is provided in Appendix~\ref{sec:relatedworks}.
 
 %However, their results rely on specific assumptions about the relationship between kernel eigenvalues and domain size, which limits the generality of their results. Also, the domain partitioning technique, despite its theoretical appeal, is cumbersome to implement and raises practical concerns, such as the justification of discarding samples and using only those within a subdomain. Our algorithm demonstrates order-optimal results for general kernels using simpler approach leveraging statistical independence, and it only requires a sublinear maximum information gain $\Gamma(n)$, which is always the case. }
 







% \begin{algorithm}\label{alg:exp}
% \caption{Exploration Phase}
% \begin{algorithmic}
% \REQUIRE $\tau$, $k$, $N$

% \FOR{$n=1,2,\cdots, N$}
%     \STATE Initialize 
%     \FOR{$h=H, H-1, \cdots, 1$}
%         \STATE DO NOTHING! : )
%     \ENDFOR
%     \FOR{$h=1, 2, \cdots, H$}
%         \STATE Observe $s_h$
%         \STATE Select $a_h=\pi_h(s_h):=\arg\max_{a}\sigma_h^n(s_h,a)$  \sattar{\# Note the significant change we are making here}
%     \ENDFOR
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}









% In the next section, we obtain a sufficient number of exploration episodes for designing $\epsilon$-optimal policies, under both scenarios: with and without generative models. 
