@inproceedings{yang2019sample,
  title={Sample-optimal parametric q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International conference on machine learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}


@inproceedings{vakili2023information,
  title={Information gain and uniform generalization bounds for neural kernel models},
  author={Vakili, Sattar and Bromberg, Michael and Garcia, Jezabel and Shiu, Da-shan and Bernacchia, Alberto},
  booktitle={2023 IEEE International Symposium on Information Theory (ISIT)},
  pages={555--560},
  year={2023},
  organization={IEEE}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@book{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  publisher={University of London, University College London (United Kingdom)}
}


@inproceedings{lee2023demystifying,
  title={Demystifying Linear MDPs and Novel Dynamics Aggregation Framework},
  author={Lee, Joongkyu and Oh, Min-hwan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@article{wang2019optimism,
  title={Optimism in reinforcement learning with generalized linear function approximation},
  author={Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
  journal={arXiv preprint arXiv:1912.04136},
  year={2019}
}


@inproceedings{srinivas2009gaussian,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  booktitle={International Conference on Machine Learning},
  year={2010}
}

@inproceedings{chowdhury2017kernelized,
  title={On kernelized multi-armed bandits},
  author={Chowdhury, Sayak Ray and Gopalan, Aditya},
  booktitle={International Conference on Machine Learning},
  pages={844--853},
  year={2017},
  organization={PMLR}
}


@article{vakili2021optimal,
  title={Optimal order simple regret for {G}aussian process bandits},
  author={Vakili, Sattar and Bouziani, Nacime and Jalali, Sepehr and Bernacchia, Alberto and Shiu, Da-shan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21202--21215},
  year={2021}
}



@article{whitehouse2024sublinear,
  title={On the sublinear regret of {GP-UCB}},
  author={Whitehouse, Justin and Ramdas, Aaditya and Wu, Steven Z},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{abbasi2013online,
  title={Online learning for linearly parametrized control problems},
  author={Abbasi-Yadkori, Yasin},
  year={2013}
}



@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}


@article{wang2020reward,
  title={On reward-free reinforcement learning with linear function approximation},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17816--17826},
  year={2020}
}


@inproceedings{chowdhury2023value,
  title={Value Function Approximations via Kernel Embeddings for No-Regret Reinforcement Learning},
  author={Chowdhury, Sayak Ray and Oliveira, Rafael},
  booktitle={Asian Conference on Machine Learning},
  pages={249--264},
  year={2023},
  organization={PMLR}
}

@inproceedings{qiu2021reward,
  title={On reward-free rl with kernel and neural function approximations: Single-agent {MDP} and {M}arkov {G}ame},
  author={Qiu, Shuang and Ye, Jieping and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={International Conference on Machine Learning},
  pages={8737--8747},
  year={2021},
  organization={PMLR}
}



@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@article{chentanez2004intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Chentanez, Nuttapong and Barto, Andrew and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={17},
  year={2004}
}
@article{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}
@article{tang2017exploration,
  title={\# exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Xi Chen, OpenAI and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International Conference on Machine Learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}
@inproceedings{hazan2019provably,
  title={Provably efficient maximum entropy exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019},
  organization={PMLR}
}
@article{eysenbach2018diversity,
  title={Diversity is all you need: Learning skills without a reward function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}
@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}
@article{eysenbach2021maximum,
  title={Maximum entropy {RL} (provably) solves some robust {RL} problems},
  author={Eysenbach, Benjamin and Levine, Sergey},
  journal={arXiv preprint arXiv:2103.06257},
  year={2021}
}
@article{gregor2016variational,
  title={Variational intrinsic control},
  author={Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint arXiv:1611.07507},
  year={2016}
}
@article{zahavy2022discovering,
  title={Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality},
  author={Zahavy, Tom and Schroecker, Yannick and Behbahani, Feryal and Baumli, Kate and Flennerhag, Sebastian and Hou, Shaobo and Singh, Satinder},
  journal={arXiv preprint arXiv:2205.13521},
  year={2022}
}
@article{badia2020never,
  title={Never give up: Learning directed exploration strategies},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Piot, Bilal and Kapturowski, Steven and Tieleman, Olivier and Arjovsky, Mart{\'\i}n and Pritzel, Alexander and Bolt, Andew and others},
  journal={arXiv preprint arXiv:2002.06038},
  year={2020}
}
@article{aubret2019survey,
  title={A survey on intrinsic motivation in reinforcement learning},
  author={Aubret, Arthur and Matignon, Laetitia and Hassas, Salima},
  journal={arXiv preprint arXiv:1908.06976},
  year={2019}
}
@article{zhang2020variational,
  title={Variational policy gradient method for reinforcement learning with general utilities},
  author={Zhang, Junyu and Koppel, Alec and Bedi, Amrit Singh and Szepesvari, Csaba and Wang, Mengdi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4572--4583},
  year={2020}
}
@article{zahavy2021reward,
  title={Reward is enough for convex mdps},
  author={Zahavy, Tom and O'Donoghue, Brendan and Desjardins, Guillaume and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25746--25759},
  year={2021}
}
@article{amin2021survey,
  title={A survey of exploration methods in reinforcement learning},
  author={Amin, Susan and Gomrokchi, Maziar and Satija, Harsh and van Hoof, Herke and Precup, Doina},
  journal={arXiv preprint arXiv:2109.00157},
  year={2021}
}
@inproceedings{bagot2022deep,
  title={Deep learning of intrinsically motivated options in the arcade learning environment},
  author={Bagot, Louis and Mets, Kevin and De Schepper, Tom and Latr{\'e}, Steven},
  booktitle={Deep Reinforcement Learning Workshop, NeurIPS 2022, 9 December, 2022},
  pages={1--14},
  year={2022}
}
@article{hao2023exploration,
  title={Exploration in deep reinforcement learning: From single-agent to multiagent domain},
  author={Hao, Jianye and Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Meng, Zhaopeng and Liu, Peng and Wang, Zhen},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}
@article{moos2022robust,
  title={Robust reinforcement learning: A review of foundations and recent advances},
  author={Moos, Janosch and Hansel, Kay and Abdulsamad, Hany and Stark, Svenja and Clever, Debora and Peters, Jan},
  journal={Machine Learning and Knowledge Extraction},
  volume={4},
  number={1},
  pages={276--315},
  year={2022},
  publisher={MDPI}
}
@article{taiga2021bonus,
  title={On bonus-based exploration methods in the arcade learning environment},
  author={Taiga, Adrien Ali and Fedus, William and Machado, Marlos C and Courville, Aaron and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2109.11052},
  year={2021}
}
@article{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{vakili2024kernelized,
  title={Kernelized Reinforcement Learning with Order Optimal Regret Bounds},
  author={Vakili, Sattar and Olkhovskaya, Julia},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}
@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}
@article{yang2020provably,
  title={Provably efficient reinforcement learning with kernel and neural function approximations},
  author={Yang, Zhuoran and Jin, Chi and Wang, Zhaoran and Wang, Mengdi and Jordan, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13903--13916},
  year={2020}
}



@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  pages={89--129},
  year={2008},
  publisher={Springer}
}
@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6683--6694},
  year={2021}
}
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.01643},

}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}
@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}
@article{munos2008finite,
  title={Finite-Time Bounds for Fitted Value Iteration.},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={5},
  year={2008}
}
@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  year={2008}
}
@article{bartlett2012regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating {MDPs}},
  author={Bartlett, Peter L and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1205.2661},
  year={2012}
}

@inproceedings{yao2014pseudo,
  title={Pseudo-{MDPs} and factored linear action models},
  author={Yao, Hengshuai and Szepesv{\'a}ri, Csaba and Pires, Bernardo Avila and Zhang, Xinhua},
  booktitle={2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={1--9},
  year={2014},
  organization={IEEE}
}
@article{russo2019worst,
  title={Worst-case regret bounds for exploration via randomized value functions},
  author={Russo, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020},
  organization={PMLR}
}
@article{neu2020unifying,
  title={A unifying view of optimism in episodic reinforcement learning},
  author={Neu, Gergely and Pike-Burke, Ciara},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1392--1403},
  year={2020}
}
@inproceedings{yang2020reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}
@inproceedings{chowdhury2019online,
  title={Online learning in kernelized {M}arkov decision processes},
  author={Chowdhury, Sayak Ray and Gopalan, Aditya},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3197--3205},
  year={2019},
  organization={PMLR}
}
@inproceedings{domingues2021kernel,
  title={Kernel-based reinforcement learning: A finite-time analysis},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={2783--2792},
  year={2021},
  organization={PMLR}
}
@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}
@article{kearns1998finite,
  title={Finite-sample convergence rates for {Q}-learning and indirect algorithms},
  author={Kearns, Michael and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={11},
  year={1998}
}
@article{gheshlaghi2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  pages={325--349},
  year={2013},
  publisher={Springer}
}


@article{Mercer1909,
 ISSN = {02643952},
 author = {J. Mercer},
 journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
 number = {},
 pages = {415--446},
 publisher = {The Royal Society},
 title = {Functions of Positive and Negative Type, and their Connection with the Theory of Integral Equations},
 volume = {209},
 year = {1909}
}

@book{Christmann2008,
  title={Support Vector Machines},
  author={Andreas Christmann and Ingo Steinwart},
  year={2008},
  publisher={Springer New York, NY}
}


@inproceedings{scarlett2017lower,
  title={Lower bounds on regret for noisy {G}aussian process bandit optimization},
  author={Scarlett, Jonathan and Bogunovic, Ilija and Cevher, Volkan},
  booktitle={Conference on Learning Theory},
  pages={1723--1742},
  year={2017},
  organization={PMLR}
}
@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}


@inproceedings{vakili2021information,
  title={On information gain and regret bounds in gaussian process bandits},
  author={Vakili, Sattar and Khezeli, Kia and Picheny, Victor},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={82--90},
  year={2021},
  organization={PMLR}
}
@inproceedings{vakilireward,
  title={Reward-Free Kernel-Based Reinforcement Learning},
  author={Vakili, Sattar and Nabiei, Farhang and Shiu, Da-shan and Bernacchia, Alberto},
  booktitle={Forty-first International Conference on Machine Learning},
  year = {2024}
}
@inproceedings{vakili2024open,
  title={Open Problem: Order Optimal Regret Bounds for Kernel-Based Reinforcement Learning},
  author={Vakili, Sattar},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={5340--5344},
  year={2024},
  organization={PMLR}
}
@inproceedings{lattimore2023lower,
  title={A lower bound for linear and kernel regression with adaptive covariates},
  author={Lattimore, Tor},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2095--2113},
  year={2023},
  organization={PMLR}
}



@inproceedings{kearns1998,
  title = {Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kearns, Michael and Singh, Satinder},
  year = {1998},
  volume = {11},
  publisher = {MIT Press}
}


@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving Markov decision processes with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin and Ye, Yinyu},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{sidford2018variance,
  title={Variance reduced value iteration and faster algorithms for solving markov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={770--787},
  year={2018},
  organization={SIAM}
}


@inproceedings{yeh2023sample,
  title={Sample complexity of kernel-based q-learning},
  author={Yeh, Sing-Yuan and Chang, Fu-Chieh and Yueh, Chang-Wei and Wu, Pei-Yuan and Bernacchia, Alberto and Vakili, Sattar},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={453--469},
  year={2023},
  organization={PMLR}
}