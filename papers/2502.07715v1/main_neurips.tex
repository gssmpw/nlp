\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023
% \PassOptionsToPackage{round}{natbib}
% ready for submission
% 
%\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[]{neurips_2023}

% ready for submission
\usepackage{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{subcaption} % for subfigures
\usepackage{afterpage}
\usepackage{placeins}
\usepackage{multirow}

\hypersetup{
    colorlinks=true,
    linkcolor=magenta,
    citecolor =blue,
    filecolor=magenta,      
    urlcolor=magenta,
}



\title{Reward-Free Kernel-Based Reinforcement Learning: Near-Optimal Sample Complexity}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Aya Kayal\\
  UCL\\
  \texttt{aya} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\input{notation}

\begin{document}


\maketitle


\begin{abstract}
  Reinforcement Learning (RL) problems are being considered under increasingly more complex structures in the literature. While tabular and linear models have been thoroughly explored, the analytical study of RL under non-linear function approximation has only recently begun to gain traction. This interest is particularly focused on kernel-based models, which offer significant representational capacity while still lending themselves to theoretical analysis. In this context, we examine the question of statistical efficiency in kernel-based RL within the reward-free RL framework, specifically asking: \emph{how many samples are required to design a near-optimal policy?}
    Existing work addresses this question under restrictive assumptions about the class of kernel functions. We initially explore this question, assuming the existence of a \emph{generative model}. Subsequently, we demonstrate that the requirement for a generative model can be relaxed, at the cost of an $H$ factor increase in sample complexity, where $H$ is the episode length. Thus, we address this fundamental problem in RL using a broad class of kernels and a relatively straightforward algorithm, compared to the existing works.
    To obtain these results, we derive novel confidence intervals for kernel ridge regression, specific to our RL setting, that may be of broader interest. We further validate our theoretical findings through simulations.
\end{abstract}


\input{1_introduction}
\input{2_problem_formulation}
\input{3_algorithm}
\input{4_analysis}
\input{5_experimental_results}
\input{6_Conclusion}

\newpage

\bibliographystyle{unsrt}
\bibliography{references}


\newpage
\appendix
\input{appendix}
\clearpage

\input{checklist}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}