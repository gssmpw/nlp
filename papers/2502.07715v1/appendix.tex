% \section{Episodic MDP diagram}

% \begin{figure}[ht]
%     \centering
%     \begin{tikzpicture}

%     \tikzset{state/.style={circle, draw=blue, minimum size=1.1cm, align=center}}
%         % Nodes
%         \node[line width=0.3mm,  blue, state] (s1) {\tcd{$s_1$}};
%         \node[line width=0.3mm,  blue, state, right=of s1] (s2) {\tcd{$s_2$}};
%         \node[right=of s2] (dots) {$\cdots$};
%         \node[line width=0.3mm,  blue, state, right=of dots] (sH) {\tcd{$s_H$}};
%         \node[dashed, line width=0.3mm,  blue, state, right=of sH] (sH1) {\tcd{$s_{H+1}$}};

%         % Edges
%         \draw[line width=0.3mm,  blue, ->] (s1) to[bend left] node[above] {\tcd{$a_1$}} (s2);
%         \draw[line width=0.3mm,  blue,->] (s2) to[bend left] (dots);
%         \draw[line width=0.3mm,  blue,->] (dots) to[bend left] node[above] {\tcd{$a_{H-1}$}} (sH);
%         \draw[dashed, line width=0.3mm,  blue,->] (sH) to[bend left] node[above] {\tcd{$a_{H}$}} (sH1);
%     \end{tikzpicture}
%     \caption{Illustration of an Episodic MDP with an episode of length~$H$.}
%     \label{fig:episodic_mdp}
% \end{figure}

\onecolumn

\section{Related Work}\label{sec:relatedworks}
In this Section, we first provide a technical comparison between our work and that of \citet{qiu2021reward} and \citet{vakilireward}. Following this, we present a more comprehensive literature review, including related works that were not covered in the main paper. We also include a summary table of the sample complexity results in the reward-free RL setting, highlighting our key contributions.

\subsection{Comparison to the Existing Work} \label{appx:comp}

Here, we discuss the key differences between our approach and the closely related works of \citet{qiu2021reward} and \citet{vakilireward}. In ~\citet{qiu2021reward}, they conduct exploration by accumulating standard deviation over an episode, then they apply a planning phase-like algorithm to maximize a reward proportional to $\beta(\delta)\sigma_{h,n}$ at each step of an episode. However, this approach can inflate the confidence interval width multiplier $\beta(\delta)$ by a factor of $\sqrt{\Gamma(n)}$, potentially leading to suboptimal or even trivial sample complexities when $\sqrt{\Gamma(n)}$ is large, as seen in \cite{qiu2021reward}. Specifically, their results are applicable to very smooth kernels like SE, with exponentially decaying Mercer eigenvalues, for which $\Gamma(n)=\Oc(\text{polylog}(n))$. For kernels with polynomial eigendecay, where $\Gamma(n)=\Oc(n^{\frac{1}{p+1}})$ grows polynomially with $n$, this algorithm possibly leads to trivial (infinite) sample complexities. Intuitively, the inflation of $\beta(\delta)$ is due to the  adaptive sampling creating statistical dependencies among observations, specifically through next state transitions. When such dependencies exist, the best existing confidence intervals are based on a kernel adaptation of self-normalized vector values martingales \citep{abbasi2013online}. The $\sqrt{\Gamma(n)}$ term cannot be removed in general for adaptive samples that introduce bias, as was discussed in~\citet{vakili2024open} and~\citet{lattimore2023lower}.  

\cite{vakilireward} utilizes domain partitioning, relying on only a subset of samples to obtain confidence intervals. This approach achieves order-optimal sample complexity for kernels with polynomial eigendecay, offering an $H$-factor improvement compared to our work in the online setting. However, firstly, their results are limited by specific assumptions regarding the relationship between kernel eigenvalues and domain size, which reduces the generality of their findings. Secondly, their domain partitioning method is cumbersome to implement and lacks practical justification, as it requires dropping samples from other subdomains. In contrast, our algorithm achieves order-optimal results for general kernels with a simpler approach that leverages statistical independence. Moreover, our method is well-suited to the generative setting, where their approach offers no clear advantages.

\subsection{Literature Review} \label{appx:lit_review}
\begin{table}[h]
\caption{Existing sample complexities in reward-free RL. $\Sc$, $\Ac$, $H$, $d$ and $p$ represent the state space, action space, episode length, state-action space dimension and parameter of the kernel with polynomial eigendecay, respectively. Last two rows correspond to the performance guarantees for the algorithms proposed in this work.\\ }
\label{samplecomplexitytable}
\centering
\begin{tabular}{ll}
\hline
Setting      & Sample complexity                                                           \\ \hline
Tabular \citep{jin2020reward}     & $\Oc\left(\frac{|\Sc|^2|\Ac| H^5}{\epsilon^2}\right)$                       \\
Linear \citep{wang2020reward}       & $\Oct\left(\frac{d^3 H^6}{\epsilon^2}\right)$                                \\
Kernel-based (exponential eigendecay)  \citep{qiu2021reward} & $\Oc\left(\frac{H^6 \text{polylog} (\frac{1}{\epsilon})}{\epsilon^2}\right)$ \\
Kernel-based (polynomial eigendecay) \citep{vakilireward} 
&$\Oct\left((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$  \\ \hline
\textbf{Kernel-based (exponential eigendecay) (this work)} & $\Oct\left(\frac{H^7 \text{polylog} (\frac{1}{\epsilon})}{\epsilon^2}\right)$ \\ 
\textbf{Kernel-based (polynomial eigendecay) (this work) }
&$\Oct \left(H(\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$  \\ \hline
\end{tabular}
\end{table}

Numerous studies have addressed the sample complexity problem in the discounted MDP framework with an infinite horizon, where the agent has sampling access to a generative model, such as \citep{kearns1998finite,gheshlaghi2013minimax,agarwal2020model}. Alternatively, other research has focused on the episodic MDP framework, without reliance on a generative model or an exploratory policy. Both the tabular setting \citep{jin2018q,auer2008near,bartlett2012regal} and the linear setting \citep{jin2020provably,yao2014pseudo,russo2019worst,zanette2020frequentist,neu2020unifying} have been thoroughly examined. Recent literature has extended these techniques to the kernel setting \citep{yang2020provably,yang2020reinforcement,chowdhury2019online,domingues2021kernel,vakili2024kernelized}, although further improvements are needed in achieving better regret bounds. In contrast to these prior works which assume that the reward function is provided, we explore the episodic reward-free setting in this work, both with and without a generative model. This setting is significantly different from standard RL, rendering the existing sample complexity results inapplicable to our context. 

In the context of reward-free RL, numerous empirical studies have proposed various exploration methods from a practical perspective, as demonstrated by works such as \citep{bellemare2016unifying,pathak2017curiosity,hazan2019provably}. Theoretically, researchers have explored the reward-free RL framework across different levels of complexity, ranging from tabular to linear, kernel-based, and deep learning-based models \citep{jin2020reward,wang2020reward, qiu2021reward} (Table \ref{samplecomplexitytable}). %In the tabular case, \citep{jin2020reward} achieve a sample complexity of $\Oc\left(\frac{|\Sc|^2|\Ac| H^5}{\epsilon^2}\right)$, while in the linear function approximation case,  \citep{wang2020reward} prove a sample complexity of $\Oc\left(\frac{d^3 H^6}{\epsilon^2}\right)$. Here $\Sc$, $\Ac$, $H$ and $d$ represent the state and action spaces, the episode length, and state-action space dimension respectively.% 
Although the existing literature adequately covers the tabular and linear settings, it often provides only partial and incomplete findings when addressing the more intricate kernel-based and deep learning settings. The most relevant work in the kernel setting is \citet{qiu2021reward}, which provides a reward-free algorithm whose sample complexity is $\Oc\left(\frac{H^6 \text{polylog} (\frac{1}{\epsilon})}{\epsilon^2}\right)$. Their results however are only applicable to very smooth kernels with exponentially decaying eigenvalues. The recent work of \citet{vakilireward} proved a sample complexity of $\Oct\left((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$ for kernels with polynomial eigendecay. However, they employ a niche domain partitioning technique that, despite its theoretical appeal, is cumbersome to implement and raises practical concerns, as mentioned earlier.
% The recent work by \textcolor{red}{[cite new ICML paper Reward-Free Kernel-Based Reinforcement Learning]} improves over their work by leveraging a domain partitioning technique, inspired by \citep{vakili2024kernelized}, to achieve a better sample complexity applicable to kernels with polynomially decaying eigenvalues. They demonstrate that $\Oc\left((\frac{H^3}{\epsilon})^{2+\frac{2d}{\alpha}}\right)$ exploration episodes are sufficient to obtain $\epsilon$-optimal policies during planning. Here, $\alpha$ denotes the smoothness of the kernel. Our paper contributes by improving the sample complexity within this framework for any positive definite kernel, and by eliminating the need for impractical domain partitioning techniques.

Finally, it's important to mention that the planning phase of our proposed algorithm is similar to the problem of learning a good policy from predefined datasets, typically called batch or offline RL \citep{levine2020offline}. Many prior works on offline RL make the coverage assumption on the dataset, requiring it to sufficiently include any possible state-action pairs with a minimum probability \citep{precup2000eligibility,antos2008learning,chen2019information,munos2008finite}. These works do not address the exploration needed to achieve such good coverage, which is where our reward-free approach significantly differs. Our goal is to demonstrate how to collect sufficient exploration data without any reward information, enabling the design of a near-optimal policy for any reward function during the planning phase.
% \section{Pseudocode for the Algorithms}\label{appx:algos}

% The pseudocodes for the algorithms proposed in Section~\ref{sec:alg}, are provided in this appendix. Algorithm~\ref{alg:plan} presents the planning phase and Algorithms~\ref{alg:exp_gen} and~\ref{alg:exp2} present the exploration phase with and without a generative model, respectively. 
\begin{comment}
\begin{figure}[ht]
\centering
\begin{minipage}[t]{0.42\textwidth}
\begin{algorithm}[H]
   \caption{Planning Phase}
   \label{alg:plan}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\tau$, $\beta$, $\delta$, $k$, $M(\Sc,\Ac, H, P, r )$, and exploration dataset $\Dc_{N}$.
   \FOR{$h=H, H-1, \cdots, 1,$}
        \STATE Compute the prediction $\hat{g}_h$ according to~\eqref{eq:ghn};
        \STATE Let $Q_h(\cdot, \cdot) = \Pi_{[0,H]}[\hat{g}_h(\cdot, \cdot)+r_h(\cdot, \cdot)] $;
        \STATE $V_h(\cdot)= \max_{a\in\Ac}Q_h(\cdot, a)$;
        \STATE $\pi_h(\cdot) = \argmax_{a\in\Ac}Q_h(\cdot,a)$;
   \ENDFOR
   \STATE {\bfseries Output:} $\{\pi_h\}_{h\in[H]}$. 
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.55\textwidth}
\begin{algorithm}[H]
\caption{Exploration Phase \textbf{with} Generative Model}\label{alg:exp_gen}
\begin{algorithmic}[1]
\REQUIRE $\tau$, $k$, $\Sc$, $\Ac$, $H$, $P$, $N$;
\STATE Initialize $\Dc_{h,0}=\{\}$, for all $h\in[H]$;
\FOR{$n=1,2,\cdots, N$}
    \FOR{$h=1,2,\cdots, H$}
    \STATE Let $s_{h,n}, a_{h,n} = \argmax_{s,a\in\Ac}\sigma_{h, n-1}(s,a)$;
    \STATE Observe $s'_{h+1,n}\sim P_h(\cdot|s_{h,n}, a_{h,n})$;
    \STATE Update $\Dc_h^n \bigcup\{s_{h,n}, a_{h,n}, s'_{h+1,n}\}$.
    \ENDFOR
\ENDFOR
\STATE {\bfseries Output:} $\Dc_{N}$. 
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{figure}
\begin{algorithm}[ht]
\caption{Exploration Phase \textbf{without} Generative Model}\label{alg:exp2}
\begin{algorithmic}
\REQUIRE $\tau$, $k$, $\beta$, $\delta$, $\Sc$, $\Ac$, $H$, $P$, $N$;

\FOR{$n=1,2,\cdots, N$}
    \FOR{$h_0=1, 2,\cdots, H$}
    \STATE Initialize $V_{h_0+1,n}=\bm{0}$
    \FOR{$h=h_0, h_0-1, \cdots, 1$}
        \STATE Obtain $\hat{f}_{h,(n,h_0)}$; $Q_{h, (n,h_0)}$, and $V_{h,(n,h_0)}(\cdot)$ according to~\eqref{eq:mean_predictor} and ~\eqref{eq:value_func}, respectively. 
    \ENDFOR
    \FOR{$h=1,2,\cdots, h_0$}
    \STATE Observe $s_{h,n}$; Take action $a_{h,n} = \argmax_{a\in\Ac}Q_{h, n}(s_{h,n},a)$; 
    \ENDFOR
    \STATE Update $\Dc_{h_0}^n \bigcup\{s_{{h_0},n}, a_{h_0,n}, s_{h_0+1,n}\}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
\end{comment}

%\newpage
\section{Proof of Theorem~\ref{the:conf} and Corollary~\ref{Cor:cont}}\label{appx:conf}
For the proof of Theorem~\ref{the:conf}, we leverage the fact that $V$ belongs to an RKHS. Specifically, we use the Mercer representation of $V$
\begin{equation}
    V(s) = \sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}}\psi_m(s).
\end{equation}

We can also rewrite the observations in the observation vector $\bm{y}_n$ as the sum of a noise term and the expected value of the observation (noise free part).

\begin{align}
    V(s'_i) = \underbrace{(V(s'_i) - f(z_i))}_{\text{Observation noise}} + \underbrace{f(z_i)}_{\text{Noise-free observation}}
\end{align}

Using the notation $\psibar_m(z)=\E_{s'\sim P(\cdot|z)}\psi(s')$, we can rewrite $f(z_i)$ as follows

\begin{align}\nn
    f(z_i) &= 
    \E_{s\sim P(\cdot|z_i)}[V(s)]\\\nn
    &= \E_{s\sim P(\cdot|z_i)}\left[\sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}}\psi_m(s)\right]\\\nn
    &= \sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}}\E_{s\sim P(\cdot|z_i)}[\psi_m(s)]\\
    &= \sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}}\psibar_m(z_i)
\end{align}

We then use the following notations, $\varepsilon_i=V(s'_i)-f(z_i)$, $\bm{\varepsilon}_n=[\varepsilon_1, \varepsilon_2, \cdots, \varepsilon_n]^{\top}$, $\bm{f}_n=[f(z_1), f(z_2), \cdots, f(z_n)]^{\top}$, to rewrite the prediction error

\begin{align}\nn
    f(z) -\hat{f}_n(z) &= f(z) - k^{\top}_n(z)(\tau^2I+K_n)^{-1}\bm{y}_n\\\nn
    &= f(z) - k^{\top}_n(z)(\tau^2I+K_n)^{-1}(\bm{\varepsilon}_n+\bm{f}_n)\\\nn
    &= \underbrace{f(z) - k^{\top}_n(z)(\tau^2I+K_n)^{-1}\bm{f}_n}_{\text{Prediction error from noise-free observations}}- \underbrace{k^{\top}_n(z)(\tau^2I+K_n)^{-1}\bm{\varepsilon}_n}_{\text{The error due to noise}}
\end{align}

The first term is deterministic (not random) and can be bounded following the standard approaches in kernel-based models, for example using the following result from \cite{vakili2021optimal}. Let us use the notations 
\begin{equation*}
    \bm{\zeta}_n(z) = k^{\top}_n(z)(\tau^2I+K_n)^{-1}
\end{equation*}
and $\zeta_i(z) =[\bm{\zeta}_n(z)]_i$. 
\begin{lemma}[Proposition~$1$ in \cite{vakili2021optimal}]\label{lem:vakili} We have
\begin{equation*}
    \sigma_n^2(z) = \sup_{f: \|f\|_{\Hc}\le 1} (f(z) - \bm{\zeta}_n^\top(z)\bm{f}_n )^2 + \tau^2\| \bm{\zeta}_n(z)\|^2_{\ell^2}. 
\end{equation*}
\end{lemma}
Based on this lemma, the first term can be deterministically bounded by $B_1\sigma_n(z)$ :

\begin{equation}
    |f(z) - k^{\top}_n(z)(\tau^2I+K_n)^{-1}\bm{f}_n| \le B_1\sigma_n(z)
\end{equation}


We next bound the second term, the error due to noise. 
\begin{align*}
    k^{\top}_n(z)(\tau^2I+K_n)^{-1}\bm{\varepsilon}_n &= \sum_{i=1}^n\zeta_i(z)\varepsilon_i\\
    &=\sum_{i=1}^n \zeta_i(z) (\sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}}\psi_m(s'_i) - \sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}} \psibar_m(z_i))\\
    &= \sum_{m=1}^\infty w_m\lambda_m^{\frac{1}{2}}\sum_{i=1}^n\zeta_i(z)(\psi_m(s'_i)-\psibar_m(z_i))
    \\
    &= \sum_{m=1}^M w_m\lambda_m^{\frac{1}{2}}\sum_{i=1}^n\zeta_i(z)(\psi_m(s'_i)-\psibar_m(z_i)) + \sum_{m=M+1}^\infty w_m\lambda_m^{\frac{1}{2}}\sum_{i=1}^n\zeta_i(z)(\psi_m(s'_i)-\psibar_m(z_i))
\end{align*}

We note that $\psi_m(s'_i)-\psibar_m(z_i)$ are bounded random variables with a range of $2\psi_{\max}$. Using Chernoff-Hoeffding inequality and the bound on the norm of $\bm{\zeta}_n$ provided in Lemma~\ref{lem:vakili}, we have that with probability at least $1-\delta/M$ 
\begin{equation*}
    \sum_{i=1}^n\zeta_i(z)(\psi_m(s'_i)-\psibar_m(z_i)) \le \frac{\psi_{\max}\sigma_n(z)}{\tau}\sqrt{2\log(\frac{M}{\delta})}.
\end{equation*}



Using a probability union bound, 
with probability $1-\delta$
\begin{align*}
    &\sum_{m=1}^M w_m\lambda_m^{\frac{1}{2}}\sum_{i=1}^n\zeta_i(z)(\psi_m(s'_i)-\psibar_m(z_i)) \\
    &\le \sum_{m=1}^M w_m\lambda_m^{\frac{1}{2}} \frac{\psi_{\max}\sigma_n(z)}{\tau}\sqrt{2\log\left(\frac{M}{\delta}\right)} \\
    &\le \left(\sum_{m=1}^M\lambda_m\right)^{\frac{1}{2}} \left(\sum_{m=1}^M w^2_m\right)^{\frac{1}{2}} \frac{\psi_{\max}\sigma_n(z)}{\tau}\sqrt{2\log\left(\frac{M}{\delta}\right)} \\
    &\le \left(\sum_{m=1}^M\lambda_m\right)^{\frac{1}{2}}B_2\frac{\psi_{\max}\sigma_n(z)}{\tau}\sqrt{2\log\left(\frac{M}{\delta}\right)} \\
    &\le C B_2 \frac{\psi_{\max}\sigma_n(z)}{\tau}\sqrt{2\log\left(\frac{M}{\delta}\right)}
\end{align*}


The second inequality is based on the Cauchy-Schwarz inequality. In the third inequality, we used that $B_2$ is the upper bound on the RKHS norm of $V$. In the last inequality, we used the observation that under both polynomial eigenvalue decay with $p>1$ and exponential eigendecay, the sum of the eigenvalues is bounded by an absolute constant $C$, independent of $M$.




Also, for the second term, we have
\begin{align*}
\sum_{m=M+1}^\infty w_m\lambda_m^{\frac{1}{2}}\sum_{i=1}^n\zeta_i(z)\left(\psi_m(s'_i)-\psibar_m(z_i)\right)
&\le 
2\psi_{\max}\sum_{m=M+1}^\infty w_m\lambda_m^{\frac{1}{2}}\sum_{i=1}^n\zeta_i(z)
\\
&\le 
2\psi_{\max}\sum_{m=M+1}^\infty w_m\lambda_m^{\frac{1}{2}}\left(n\sum_{i=1}^n\zeta^2_i(z)\right)^{\frac{1}{2}}
\\
&\le \frac{2\sigma_n(z)\psi_{\max}\sqrt{n}}{\tau}\sum_{m=M+1}^\infty w_m\lambda_m^{\frac{1}{2}}\\
&\le \frac{2\sigma_n(z)\psi_{\max}\sqrt{n}}{\tau}\left(\left(\sum_{m=M+1}^\infty w_m^2\right)\left(\sum_{m=M+1}^{\infty}\lambda_m\right)\right)^{\frac{1}{2}}\\
& \le \frac{2B_2\sigma_n(z)\psi_{\max}}{\tau}\left(n\sum_{m=M+1}^{\infty}\lambda_m\right)^{\frac{1}{2}}.
\end{align*}

The first inequality holds by definition of $\psi_{\max}$. The second inequality is based on the Cauchy-Schwarz inequality. The third inequality uses Lemma~\ref{lem:vakili}. The fourth inequality utilizes the Cauchy-Schwarz inequality again, and the last inequality results from the upper bound on the RKHS norm of~$V$.


Putting together, with probability $1-\delta$,
%
\begin{equation}
    k^{\top}_n(z)(\tau^2I+K_n)^{-1}\bm{\varepsilon}_n \le \frac{ C B_2\psi_{\max}\sigma_n(z)}{\tau}\sqrt{2\log(\frac{M}{\delta})}  + \frac{2B_2\sigma_n(z)\psi_{\max}}{\tau}\sqrt{n\sum_{m=M+1}^{\infty}\lambda_m}.
\end{equation}


\paragraph{Proof of Corollary~\ref{Cor:cont}}
To extend the confidence interval given in Theorem~\ref{the:conf} to hold uniformly on $\Zc$, we use a discretization argument. For this purpose, we apply Assumption~\ref{ass:disc} to $f$ and $\hat{f}_n$, and also use Assumption~\ref{ass:disc} to bound the discrimination error in $\sigma_n$. The following lemma provides a high probability bound on $\|\hat{f}_n\|_{k_{\varphi}}$.

\begin{lemma}\label{lem:rkhshatf}
    For function $f$ defined in Theorem~\ref{the:conf}, the RKHS norm of $\hat{f}_n$ satisfies the following with probability at least $1-\delta$:
    \begin{equation}
    \|\hat{f}_n\|_{\Hc_{k_\varphi}} \le B_1 +\frac{v_{\max}}{\tau}\sqrt{2\Gamma_{k_{\varphi}}(n)+1+\log(\frac{1}{\delta})}.
    \end{equation}
\end{lemma}

For a proof see Lemma~5 in~\citep{vakili2024kernelized}.

Let $B_3(\delta) = B_1 +\frac{v_{\max}}{\tau}\sqrt{2\Gamma_{k_{\varphi}}(n)+1+\log(\frac{1}{\delta})}$ denote the $1-\delta$ upper confidence bound on $\|\hat{f}_n\|_{H_{k_{\varphi}}}$. Let $\Zz$ be the discretization of $\Zc$ specified in Assumption~\ref{ass:disc} with RKHS norm bound $B_3(\frac{\delta}{2})$. That is for any $g\in \Hc_{k_{\varphi}}$ with $\|g\|_{\Hc_{k_{\varphi}}}\le B_3(\frac{\delta}{2})$, we have $g(z)-g([z])\le \frac{1}{n}$, where $[z]=\argmin_{z'\in\Zz}\|z'-z\|$ is the closest point in $\Zz$ to $z$, and $|\Zz|\le c_n$, where $c_n=c(B_3(\frac{\delta}{2}))^dn^d$. Applying Assumption~\ref{ass:disc} to $f$ and $\hat{f}_n$ with this discretization, it holds for all $z\in \Zc$ that
\begin{equation}\label{eq:mm}
    |f(z)-f([z])|\le \frac{1}{n}. 
\end{equation}
In addition, by Lemma~\ref{lem:rkhshatf}, with probability eat least $1-\delta$
\begin{equation}\label{eq:mm2}
    |\hat{f}_n(z)-\hat{f}_n([z])\le \frac{1}{n}. 
\end{equation}

Furthermore, we have the following lemma, which can roughly be viewed as a Lipschitz continuity property for $\sigma_n$. 

\begin{lemma}\label{lem:sigmadisc}
    Under Assumption~\ref{ass:disc}, with the discrimination $\Zz$ described above, it holds for all $z\in\Zc$ that
    \begin{equation*}
        \sigma_n(z) -\sigma_n([z]) \le \frac{2}{\sqrt{n}}.
    \end{equation*}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:sigmadisc}]

Using the reproducing property of RKHS, we have 
\begin{align}
    \|k_n^{\top}(z)(K_n+\tau^2I)^{-1}k_n(\cdot)\|_{\Hc_k} \le \frac{k_{\max}\sqrt{n}}{\tau},
\end{align}
where $k_{\max}$ is the maximum value of the kernel. 
Let us define $q(\cdot, \cdot') = k_n^{\top}(\cdot)(K_n+\tau^2I)^{-1}k_n(\cdot')$. We can write 
\begin{align*}\nn
&\vspace{-3em}|\sigma^2_n(z) - \sigma_n^2([z]) |\\
&=\big|(k(z,z)-q(z,z))-(k([z],[z])-q([z],[z]))\big|\\
&=\big|(k(z,z)-q(z,z)) -(k(z,[z])-q(z,[z]))
+(k(z,[z])-q(z,[z]))
-(k([z],[z])-q([z],[z]))\big|\\\nn
&\le |k(z,z)-k(z,[z])| + |k(z,[z])
-k([z],[z])| +|q(z,z)-q(z,[z])| - |q(z,[z])
-q([z],[z])|\\\nn
&\le\frac{4}{n}.
\end{align*}

To obtain a discretization error bound for the standard deviation from that of the variance, we write
\begin{align}\nn
(\sigma_n(z) - \sigma([z]))^2&\le
\left|\sigma_n(z) - \sigma([z])\right|(\sigma_n(z) + \sigma([z]))\\\nn
&=|\sigma^2_n(z) - \sigma^2([z])|\\\nn
&\le\frac{4}{n}.
\end{align}

Therefore,
\begin{equation*}
|\sigma_n(z) - \sigma([z])|\le\frac{2}{\sqrt{n}}.
\end{equation*}

\end{proof}

Applying a probability union bound on the discretization $\Zz$ to Theorem~\ref{the:conf}, and considering the error bounds in~\eqref{eq:mm},~\eqref{eq:mm2} and Lemma~\ref{lem:sigmadisc}, we arrive at Corollary~\ref{Cor:cont}.























\section{Proof of Theorem~\ref{the:gen}}\label{appx:gen}



First, we define the following high-probability event :
\begin{equation}\label{eq:event}
    \Ec = \left\{  \forall h\in[H], |\hat{g}_{h}(z)-[P_hV_{{h+1}}](z)|\le \beta(\delta)\left(\sigma_{h,N}(z)+\frac{2}{\sqrt{n}}\right)+\frac{2}{n}\right\},
\end{equation}
where $\beta(\delta)= \Oc\left(\frac{H}{\tau}\sqrt{d\log(\frac{NH}{\delta})}\right)$ as specified in Corollary~\ref{Cor:cont} with $B_1=\Oc(H)$ and $B_2=c_v$. 
Using Corollary~\ref{Cor:cont}, we have $\Pr[\Ec]\ge 1-\delta$.



We divide the rest of the analysis into several steps, as outlined next. 


\paragraph{Step 1:} Under $\Ec$, with reward $r$, we bound $V^{\star}_1(s)-V^{\pi}_1(s)$ using $ V_1(s)-V^{\pi}_1(s)$, based on the following lemma. Recall that $V^{\pi}_h$ and $V^\star_h$ are the value functions of policy $\pi$ and the optimal policy, respectively, and $V_h$ is the proxy value functions used in Algorithm~\ref{alg:plan}. 

\begin{lemma}\label{lem:VstarVplan}
    Under $\Ec$, we have
    \begin{equation}
        V^{\star}_h(s)- V_h(s) \le (H+1-h)(\frac{2\beta(\delta)}{\sqrt{N}}+\frac{2}{N}).
    \end{equation}
\end{lemma}    
\begin{proof}[Proof of Lemma~\ref{lem:VstarVplan}]

The lemma is proven by induction over $h$, starting from $V^{\star}_{H+1} = V_{H+1}=\bm{0}$. We have
\begin{align*}
    Q^{\star}_h(s,a) - Q_{h}(s,a)
    &= r_h(s,a)+ [P_hV^{\star}_{h+1}](s,a)- r_h(s,a) - \hat{g}_{h}(s,a)- \beta(\delta)\sigma_{h,N}(s,a)\\
    &\le [P_hV^{\star}_{h+1}](s,a) -[P_hV_{h+1}](s,a)+\frac{2\beta(\delta)}{\sqrt{N}}+\frac{2}{N}\\
    &=[P_h(V^{\star}_{h+1}-V_{h+1})](s,a)+\frac{2\beta(\delta)}{\sqrt{N}}+\frac{2}{N}\\
    &\le (H+1-h)(\frac{2\beta(\delta)}{\sqrt{N}}+\frac{2}{N})
\end{align*}
The first inequality holds by $\Ec$, and the second inequality by induction assumption.
Then, we have
\begin{align*}
    V^{\star}_h(s_h) - V_h(s_h) &=
    \max_{a\in\Ac} Q^{\star}_h(s,a) - \max_{a\in\Ac}Q_{h}(s,a)\\
    &\le \max_{a\in\Ac} \{Q^{\star}_h(s,a) - Q_{h}(s,a)\}\\
    &\le (H+1-h)(\frac{2\beta(\delta)}{\sqrt{N}}+\frac{2}{N}).
\end{align*}
That proves the lemma. 

\end{proof}






\paragraph{Step 2:} We also bound $V_1(s)-V^{\pi}_1(s) $ using the sum of standard deviations for the trajectory generated by the policy. 

\begin{lemma}\label{VHVpisum}
    Under $\Ec$, we have 
    \begin{equation*}
        V_1(s_1)-V^{\pi}_1(s_1)  \le \E\left[\sum_{h=1}^H\left(\beta(\delta)\left(\sigma_{h,N}(s_h,a_h)+\frac{2}{\sqrt{N}}\right)+\frac{2}{N}\right)\right],
    \end{equation*}
where the expectation is taken with respect to the trajectory generated by the policy.  
\end{lemma}




\begin{proof}[Proof of Lemma~\ref{VHVpisum}]
Note that $V_{H+1}=V^{\pi}_{H+1}=\bm{0}$. We next obtain a recursive relationship for the difference $V_h(s)-V^{\pi}_h(s)$. 
\begin{align*}
    V_h(s_h)-V^{\pi}_h(s_h) &= Q_h\left(s_h, \pi(s_h)\right) -  Q^{\pi}_h\left(s_h, \pi(s_h)\right) \\
    &=r\left(s_h, \pi(s_h)\right) + \hat{g}_h\left(s_h, \pi(s_h)\right) + \beta(\delta)\sigma_{h,N}\left(s_h, \pi(s_h)\right) - r\left(s_h, \pi(s_h)\right) - [P_hV^{\pi}_{h+1}]\left(s_h, \pi(s_h)\right)\\
    &\le [P_hV_{h+1}]\left(s_h, \pi(s_h)\right) +2\beta(\delta)\sigma_{h,N}\left(s_h, \pi(s_h)\right) + \frac{2\beta(\delta)}{\sqrt{N}}+\frac{2}{N}- [P_hV^{\pi}_{h+1}]\left(s_h, \pi(s_h)\right),\\
\end{align*}
where the inequality is due to $\Ec$. 
Recursive application of the above inequality over $h=H, H-1, \cdots, 1$, we obtain
\begin{align*}
    V_1(s_1) - V_1^{\pi}(s_1)& \le \E_{s_{h+1}\sim P(\cdot|s_h,\pi(s_h)), h < H}\left[\sum_{h=1}^H2\beta(\delta)\sigma_{h,N}\left(s_h, \pi(s_h)\right)\right]+ \frac{2H\beta(\delta)}{\sqrt{N}}+\frac{2H}{N}.
\end{align*}


\end{proof}




\paragraph{Step 3:} By definition, we have $V_1^{\pi}(s_1; \beta(\delta)\sigma_{N} )\le V_1^{\star}(s_1; \beta(\delta)\sigma_{N})$. Note that $V^{\pi}_1(s_1; \beta(\delta)\sigma_{N})= \beta (\delta)\sum_{h=1}^H\sigma_{h,N}(s_h, \pi(s_h))$.




\paragraph{Step 4:} We have $V^{\star}_1(s;\beta(\delta)\sigma_{N} ) \le V_1^{\star}(s; \beta(\delta)\sigma_{n} )$. This is due to the observation that $\sigma_{h,n}$ is decreasing in the number $n$ of observations. We note that conditioning on observations only reduces the variance. That is seen from the positive definiteness of the Gram matrix and the formula for kernel ridge uncertainty estimator given in~\eqref{eq:krrvar}. 

\paragraph{Step 5:} Recall the selection rule in Algorithm~\ref{alg:exp_gen}: $s_{h,n},a_{h,n} = \argmax_{s,a}\sigma_{h,n-1}(s,a)$.
When exploring with generative model, with this rule of selection, we have $V_1^{\star}(s_1; \beta(\delta)\sigma_{n-1} ) \le \beta(\delta)\sum_{h=1}^H \sigma_{h,n-1}(s_{h,n}, a_{h,n})$. 

\paragraph{Step 6:} Combining all previous steps, we conclude that, under $\Ec$,
\begin{equation}
    V_1^{\star}(s) - V_1^{\pi}(s) \le \frac{2\beta(\delta)}{N}\sum_{n=1}^N\sum_{h=1}^H\sigma_{h,n-1}(s_{h,n},a_{h,n})+\frac{4\beta(\delta)H}{\sqrt{N}} +\frac{4H}{N}.
\end{equation}


\paragraph{Step 7:}

We bound the sum of standard deviations according to the following lemma that is a kernel based version of elliptical potential lemma~\citep{abbasi2013online}. 
\begin{lemma}
    For each $h$, we have
    \begin{equation}
        \sum_{n=1}^N\sigma^2_{h,n-1}(s_{h,n},a_{h,n})\le \frac{2\Gamma(N)}{\log(1+1/\tau^2)}.
    \end{equation}
\end{lemma}
See, e.g., \cite{srinivas2009gaussian} for a proof. Using Cauchy–Schwarz inequality, we obtain
\begin{equation*}
    \sum_{n=1}^N\sigma_{h,n-1}(s_{h,n},a_{h,n}) \le \sqrt{\frac{2N\Gamma(N)}{\log(1+1/\tau^2)}}.
\end{equation*}


\paragraph{Step 8:} From Steps $6$ and $7$, we conclude that, $\pi$ is an $\epsilon$-optimal policy with $\epsilon$ no larger than 
\begin{equation*}
    V_1^{\star}(s) - V_1^{\pi}(s) \le 
    2H\beta(\delta)\sqrt{\frac{2\Gamma(N)}{N\log(1+1/\tau^2)}} +\frac{4\beta(\delta)H}{\sqrt{N}} +\frac{4H}{N}.
\end{equation*}

A simpler expression can be given as

\begin{equation*}
    V_1^{\star}(s) - V_1^{\pi}(s) =\Oc\left(  H^2\sqrt{\frac{\Gamma(N)\log(NH/\delta)}{N}}\right).
\end{equation*}

Now, let $N_0$ be the smallest integer suh that the right hand side less than $\epsilon$. For any $N\ge N_0$ the suboptimality gap of the policy is at most $\epsilon$.
This completes the proof of Theorem~\ref{the:gen}. 

\section{Proof of Theorem~\ref{the:main}}\label{appx:main_sample}

We define the event $\Ec$ similar to the proof of Theorem~\ref{the:gen}. The first $4$ steps related to the planning phase are exactly the same as in the proof of Theorem~\ref{the:gen}. The rest of the proof is different and we will present it here. 



In addition to $\Ec$, we define another high-probability event $\Ec'$ where all the confidence intervals utilized in the exploration hold true. Specifically, we define the following:
\begin{equation}\label{eq:event2}
    \Ec' = \{\forall n\in[N], \forall h\in[H], |\hat{f}_{h,n}(z)-f_{h,n}(z)|\le \beta(\delta)(\sigma_{h,n}(z)+\frac{2}{\sqrt{n}})+\frac{2}{n}\},
\end{equation}
where $\beta(\delta)= \Oc\left(\frac{1}{\tau}\sqrt{d\log(\frac{NH}{\delta})}\right)$. 
Using Corollary~\ref{Cor:cont}, we have $\Pr[\Ec']\ge 1-\delta$.





The following steps are specific to the exploration without the generative model. We define a reward sequence using $\tilde{\sigma}^{h_0}_{h, n}$ such that $\tilde{\sigma}^{h_0}_{h, n}=\sigma_{h,n}$ when $h=h_0$ and $\tilde{\sigma}^{h_0}_{h, n}=0$ for all $h\neq h_0$. 

\paragraph{Step 5b:}
We have
$V^{\star}_1(s; \beta(\delta)\sigma_{n})\le \sum_{h_0=1}^H V^{\star}_1(s; \beta(\delta)\tilde{\sigma}^{h_0}_{ n})$. Note that the optimal policy with rewards $\tilde{\sigma}^{h_0}_{h, n}$ optimizes $\sigma_{h,n}$ at step $h=h_0$, while the optimal policy with rewards $\sigma_{n}$ optimizes the sum of $\sigma_{h,n}$ over all steps. 

\paragraph{Step 6b:} We bound $V^{\star}_1(s; \beta(\delta)\tilde{\sigma}^{h_0}_{ n})$ using  $ V_{1,(n,h_0)}(s)$ where $V_{h,(n,h_0)}$ is the proxy for the value function used in Algorithm~\ref{alg:exp2}. 

\begin{lemma}\label{lem:vnstar_vn}
  Under event $\Ec'$, for all $s\in\Sc$, 
    \begin{equation*}
        V^{\star}_h(s; \beta(\delta)\tilde{\sigma}^{h_0}_{ n})\le V_{h,(n,h_0)}(s)+(h_0+1-h)(\frac{2\beta(\delta)}{\sqrt{n}}+\frac{2}{n}).
    \end{equation*}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:vnstar_vn}]

    
    The lemma is proven by induction, starting from $V^{\star}_{h_0+1}(\cdot; \beta(\delta) \tilde{\sigma}^{h_0}_{ n})=V_{h_0+1,(n,h_0)}=\bm{0}$. We have, for $h\le h_0$
    \begin{align}\nn
        V^{\star}_{h}(s;  \beta(\delta)\tilde{\sigma}^{h_0}_{ n})-V_{h,(n,h_0)}(s) &= \max_{a\in\Ac} Q^{\star}_{h}(s,a;  \beta(\delta)\tilde{\sigma}^{h_0}_{ n})-\max_{a\in\Ac}Q_{h,(n,h_0)}(s,a)\\\nn
        &\le \max_{a\in\Ac} \left\{Q^{\star}_{h}(s,a; \beta(\delta) \tilde{\sigma}^{h_0}_{ n})-Q_{h,(n,h_0)}(s,a)\right\}\\\nn
        & \le\max_{a\in\Ac} \left\{[P_hV^{\star}_{h+1}](s,a;  \beta(\delta)\tilde{\sigma}^{h_0}_{ n})-[P_hV_{h+1,n}](s,a)+\frac{2\beta(\delta)}{\sqrt{n}}+\frac{2}{n}\right\}\\\nn
        &\le (h_0+1-h)(\frac{2\beta(\delta)}{\sqrt{n}}+\frac{2}{n}).
    \end{align}
    The fist inequality is due to rearrangement of $\max$, the second inequality holds under $\Ec'$, and the third inequality is by the base of induction. We thus prove the lemma. 
\end{proof}



\paragraph{Step 7b:} Fix $n$ and $h_0$.
Let $\pi_{n,h_0}$ denote the exploration policy in episode $nH+h_0$.
We bound $V_{1,(n,h_0)}(s_1)- V^{\pi_{n,h_0}}_1(s_1; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})
$ using $
2\beta(\delta)\sum_{h=1}^{h_0}\sigma_{h,n-1}(s_h, a_h) $. Here for simplicity of notation, we use $s_h$ and $a_h$ for the state and action at step $h$ of epsiode corresponding to $n$ and $h_0$. In a richer notation, $n$ and $h_0$ should be specified. 
 
\begin{lemma}\label{lem:v_minus_vpi}
    Under event $\Ec'$, we have 
%
    \begin{align*}
        &V_{1,(n,h_0)}(s_1)- V^{\pi_{n,h_0}}_1(s_1; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})\le 2\sum_{h=1}^{h_0}((\beta(\delta)\sigma_{h,n-1}(s_h, a_h)+\frac{2}{\sqrt{n}}) +\frac{2}{n})\\\nn
        &+ \sum_{h=1}^{h_0}([P_hV_{h+1,n}](s_h, a_h) - V_{h+1, n}(s_{h+1}))
    + \sum_{h=1}^{h_0}(V^{\pi_{n,h_0}}_{h+1}(s_{h+1}; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1}) - [P_hV^{\pi_{n,h_0}}_{h+1}](s_h, a_h; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})).
    \end{align*}
\end{lemma}

The second and third terms are martingale sums which can be bounded using Azuma-Hoeffding inequality, we refer to them as
\begin{equation*}
    \zeta_{h,(n,h_0)} = [P_hV_{h+1,n}](s_h, a_h) - V_{h+1, n}(s_{h+1})
\end{equation*}

\begin{equation*}
    \xi_{h,(n,h_0)} = V^{\pi_{n,h_0}}_{h+1}(s_{h+1}; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1}) - [P_hV^{\pi_{n,h_0}}_{h+1}](s_h, a_h; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})
\end{equation*}


\begin{proof}[Proof of Lemma~\ref{lem:v_minus_vpi}]

We obtain a iterative relation over $h$. In particular
%
\begin{align*}
    V_{h,(n,h_0)}(s_h)- V^{\pi_{n,h_0}}_h(s_h; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})&=Q_{h,(n,h_0)}(s_h, a_h)- Q^{\pi_{n,h_0}}_h(s_h, a_h; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})\\
    & \hspace{-7em}\le  [P_hV_{h+1,n}](s_h, a_h) - [P_hV^{\pi_{n,h_0}}_{h+1}](s_h, a_h; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})
    + 2\beta(\delta)\sigma_{h,n-1}(s_h,a_h)+\frac{2\beta(\delta)}{\sqrt{n}}+\frac{2}{n}\\
    &\hspace{-7em} = V_{h+1,(n,h_0)}(s_h)- V^{\pi_{n,h_0}}_{h+1}(s_h; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1})+ 2\beta(\delta)\sigma_{h,n-1}(s_h,a_h)+\frac{2\beta(\delta)}{\sqrt{n}}+\frac{2}{n} + \zeta_{h,(n,h_0)} + \xi_{h,(n,h_0)}.
\end{align*}

Iterating over $h$ and noticing $V_{h_0+1,(n,h_0)}- V^{\pi_{n,h_0}}_{h_0+1}(\cdot; \beta(\delta)\tilde{\sigma}^{h_0}_{n-1}) =\bm{0}$ the lemma is proven. 


\end{proof}




\paragraph{Step 8b:}
We note that $V^{\pi_{n,h_0}}_{1,(n,h_0)}(s)=\beta(\delta)\sigma_{h,n-1}(s_{h_0, (n,h_0)}, a_{h_0,(n,h_0)})$.


\paragraph{Step 9b:}

Combining Steps 5b-8b we conclude that
\begin{equation}
    V_1^{\star}(s, \beta(\delta)\sigma_{n-1}) \le \sum_{h_0=1}^H\sum_{h=1}^{h_0}\left(3\beta(\delta)\left(\sigma_{h,n-1}(s_{h,(n,h_0)}, a_{h,(n,h_0)})\right)+\frac{4\beta(\delta)}{\sqrt{n}}+\frac{4}{n} +\zeta_{h,(n,h_0)}+\xi_{h,(n,h_0)}\right).
\end{equation}

\paragraph{Step 10b:} Combining with previous steps similar to the proof of Theorem~\ref{the:gen}, and using Azuma-Hoeffding inequality on $\zeta_{h,(n,h_0)}$ and $ \xi_{h,(n,h_0)}$, we get, with probability $1-\delta$
\begin{equation*}
    V_1^{\star}(s) - V_1^{\pi}(s) \le 3H^2\beta(\delta)\sqrt{\frac{2\Gamma(N)}{N\log(1+1/\tau^2)}} +\frac{8\beta(\delta)H^2}{\sqrt{N}} +\frac{4H^2(\log(N)+1)}{N}+2H^2\sqrt{2N\log({\frac{3N}{\delta})}}.
\end{equation*}

The expression can be simplified as

\begin{equation*}
    V_1^{\star}(s) - V_1^{\pi}(s) =\Oc\left(  H^3\sqrt{\frac{\Gamma(N)\log(N/\delta)}{N}}\right).
\end{equation*}

Now, let $N_0$ be the smallest integer suh that the right hand side less than $\epsilon$. For any $N\ge N_0$ the suboptimality gap of the policy is at most $\epsilon$.
This completes the proof of Theorem~\ref{the:main}.

%\newpage
\section{Experimental Details}
\label{appx:exp}

%For the experimental setting, we choose $H=10$, $\Sc=\Ac=[0,1]$ consisting of $100$ evenly spaced points. The reward function and the conditional probability distribution are chosen as arbitrary functions in the RKHS of a fixed kernel: Squared Exponential (SE), Mat{\'e}rn kernels with parameter $\nu=2.5$ and $\nu=1.5$. For all kernels we use lengthscale of $0.1$. With SE kernel we use $\tau=0.01$, and with Met{\'e}rn kernels we use $\tau=0.5$.
%For fixed reward and transition probability distribution, we repeat the experiment with all $4$ algorithms for $80$ independent runs and plot the average sub-optimality of the algorithms $V_1^{\star}(s)-V^{\pi}(s)$ for values $N=10,20, 40, 80, 160$. We also perform hyperparameter tuning for the confidence interval width multiplier. While the theoretical values for $\beta$, especially in~\cite{qiu2021reward}, tend to be high, we fine-tune  $\beta$ and select the best one ($\beta=0.1$). %run the experiments with various $\beta$ values and pick the one performing the best ($\beta=0.1$ for the experiments shown here).
%Further experimental details, including environment specifications, simulations with different $\beta$ values, and additional experiments, can be found in Appendix~\ref{appx:exp}.

Here, we outline the procedure for generating $r$ and $P$ test functions from the RKHS, the computational resources utilized for the simulations, and the fine-tuning process of the confidence interval width multiplier $\beta$. Additionally, we present further experimental results when various samples of $r$ and $P$ are drawn from the RKHS.
\subsection{Synthetic Test Functions from the RKHS}
Our reward function $r$ and transition probability $P$ are arbitrarily chosen functions from an RKHS. For the reward function $r$, we draw a Gaussian Process (GP) sample on a subset of the domain $\Zc$. This subset is generated by sampling a set of evenly spaced points on a $10 \times 10$ grid spanning the range $[0, 1]$ in both dimensions. We then fit kernel ridge regression to these samples and scale the resulting predictions to fit the $[0,1]$ range to obtain $r$. For $P(s'|s,a)$, we similarly draw a GP sample on a subset of the domain $\Zc\times \Sc$, fit kernel ridge regression to these samples, and then shift and rescale for each $z$ to obtain  $P(\cdot|z)$ as a conditional probability distribution. We use the same kernel as the one used in the algorithm. This is a common approach to create functions belonging to an RKHS ~\citep[e.g., see,][]{chowdhury2017kernelized}. Examples of $r$ and $P$ are visualized in Figures \ref{fig:R_P_RBF}, \ref{fig:R_P_Matern2.5} and \ref{fig:R_P_Matern1.5} using Squared Exponential (SE) and Mat{\'e}rn kernels with parameter $\nu=2.5$ and $\nu=1.5$, respectively. For all kernels we use lengthscale of $0.1$. With SE kernel we use $\tau=0.01$, and with Met{\'e}rn kernels we use $\tau=0.5$. %~\citep[see, e.g., Section 6]{chowdhury2017kernelized}.\\


\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/Reward_RBF.png}
        \caption{Reward function $r(s,a)$}
        \label{fig:R_RBF}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/P_RBF_1.png}
        \caption{$P(s'|(s=0,a=0)$}
        \label{fig:P_RBF_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/P_RBF_2.png}
        \caption{$P(s'|(s=0,a=0.5051)$}
        \label{fig:P_RBF_2}
    \end{subfigure}

    \vspace{\baselineskip} % Add vertical space between rows

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/P_RBF_3.png}
        \caption{$P(s'| (s=0,a=1)$}
        \label{fig:P_RBF_3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/P_RBF_4.png}
        \caption{$P(s'| (s=0.5051,a=0)$}
        \label{fig:R_RBF_4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_RBF_7.png}
        \caption{$P(s'|(s=1,a=1)$}
        \label{fig:P_RBF_7}
    \end{subfigure}  
  \caption{Reward and transition probability functions generated by kernel ridge regression using SE Kernel with lengthscale $=0.1$ and $\tau= 0.01$}
  \label{fig:R_P_RBF}
\end{figure}

%\subsection{Matern kernel with smoothness 2.5}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/Reward_Matern2.5.png}
        \caption{Reward function $r(s,a)$}
        \label{fig:R_Matern2.5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern2.5_1.png}
        \caption{$P(s'|(s=0,a=0)$}
        \label{fig:P_Matern2.5_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern2.5_2.png}
        \caption{$P(s'|(s=0,a=0.5051)$}
        \label{fig:P_Matern2.5_2}
    \end{subfigure}
\vspace{\baselineskip} % Add vertical space between rows
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern2.5_3.png}
        \caption{$P(s'|(s=0,a=1)$}
        \label{fig:P_Matern2.5_3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern2.5_4.png}
        \caption{$P(s'|(s=0.5051,a=0)$}
        \label{fig:R_Matern2.5_4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern2.5_7.png}
        \caption{$P(s'|(s=1,a=1)$}
        \label{fig:P_Matern2.5_7}
    \end{subfigure}
    \caption{Reward and transition probability functions generated by kernel ridge regression using Mat{\'e}rn kernel with $\nu=2.5$, lengthscale $=0.1$ and $\tau=0.5$}
    \label{fig:R_P_Matern2.5}
\end{figure}

%\subsection{Matern kernel with smoothness 1.5}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/Reward_Matern1.5.png}
        \caption{Reward function $r(s,a)$}
        \label{fig:R_Matern1.5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern1.5_1.png}
        \caption{$P(s'|(s=0,a=0)$}
        \label{fig:P_Matern1.5_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern1.5_2.png}
        \caption{$P(s'|(s=0,a=0.5051)$}
        \label{fig:P_Matern1.5_2}
    \end{subfigure}
\vspace{\baselineskip} % Add vertical space between rows
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern1.5_3.png}
        \caption{$P(s'|(s=0,a=1)$}
        \label{fig:P_Matern1.5_3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern1.5_4.png}
        \caption{$P(s'|(s=0.5051,a=0)$}
        \label{fig:R_Matern1.5_4}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/P_Matern1.5_7.png}
        \caption{$P(s'|(s=1,a=1)$}
        \label{fig:P_Matern1.5_7}
    \end{subfigure}
    \caption{Reward and transition probability functions generated by kernel ridge regression using Mat{\'e}rn kernel with $\nu=1.5$, lengthscale $=0.1$ and $\tau=0.5$}
    \label{fig:R_P_Matern1.5}
\end{figure}

\subsection{Implementation and Computational Resources}
%\vspace{-15pt}
For kernel ridge regression, we used Sickit-Learn library \citep{scikit-learn}, which offers robust and efficient tools for implementing and tuning kernel-based machine learning models.
The simulations were executed on a cluster which has $376.2$ GiB of RAM, and an Intel(R) Xeon(R) Gold 5118 CPU running at $2.30$ GHz. The algorithm by \citep{qiu2021reward}, our algorithm without a generative model, and the Greedy Max Variance algorithm typically require approximately 2 minutes of CPU time on average per run. However, our algorithm with a generative model requires around 7 minutes per run due to the cost of increasing the number of exploration episodes by a factor of $H$.
%\vspace{-15pt}
\subsection{Tuning the Confidence Interval Width Multiplier}
We perform hyperparameter tuning for the confidence interval width multiplier $\beta$. The theoretical analysis, especially in~\cite{qiu2021reward}, leads to  high values for $\beta$. To ensure a fair comparison between algorithms, we fine-tune  $\beta$ for~\cite{qiu2021reward}, our algorithm without a generative model, our algorithm with generative model and Greedy Max Variance, selecting the best value for each. Figures \ref{fig:Hyperparams_RBF}, \ref{fig:Hyperparams_Matern2.5}, \ref{fig:Hyperparams_Matern1.5}  show the simulation results for various values of $\beta \in [0.1,1,10,100]$ for several kernels. The value $\beta=0.1$ yields the best performance consistently.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_benchmark_differentbeta.png}
        \caption{(Qiu et al., 2021)}
        \label{fig:RBF_benchmark_differentbeta}
    \end{subfigure}% <-- No whitespace here
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_Without_generative_model_differentbeta.png}
        \caption{Without generative model}
        \label{fig:/RBF_Maxvariance2_differentbeta}
    \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_With_generative_model_differentbeta.png}
        \caption{With generative model}
       
    \end{subfigure}% <-- No whitespace here
     \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_Greedy_Max_Variance_differentbeta.png}
        \caption{Greedy Max Variance}
        
    \end{subfigure}% <-- No whitespace here
    \caption{Average suboptimality gap plotted against the number of episodes $N$ for different values of $\beta$ in the case of SE kernel.}
    \label{fig:Hyperparams_RBF}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_benchmark_differentbeta.png}
        \caption{(Qiu et al., 2021)}
        \label{fig:Matern2.5_benchmark_differentbeta}
    \end{subfigure}% <-- No whitespace here
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_Without_generative_model_differentbeta.png}
        \caption{Without generative model}
        \label{fig:/Matern2.5_Maxvariance2_differentbeta}
    \end{subfigure}
         \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_With_generative_model_differentbeta.png}
        \caption{With generative model}
       
    \end{subfigure}% <-- No whitespace here
     \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_Greedy_Max_Variance_differentbeta.png}
        \caption{Greedy Max Variance}
        
    \end{subfigure}% <-- No whitespace here
    \caption{Average suboptimality gap plotted against the number of episodes $N$ for different values of $\beta$ in the case of Mat{\'e}rn kernel with $\nu=2.5$.}
    \label{fig:Hyperparams_Matern2.5}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_benchmark_differentbeta.png}
        \caption{(Qiu et al., 2021)}
        \label{fig:Matern1.5_benchmark_differentbeta}
    \end{subfigure}% <-- No whitespace here
    \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_Without_generative_model_differentbeta.png}
        \caption{Without generative model}
        \label{fig:/Matern1.5_Maxvariance2_differentbeta}
    \end{subfigure}
         \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_With_generative_model_differentbeta.png}
        \caption{With generative model}
       
    \end{subfigure}% <-- No whitespace here
     \begin{subfigure}[b]{0.3\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_Greedy_Max_Variance_differentbeta.png}
        \caption{Greedy Max Variance}
        
    \end{subfigure}% <-- No whitespace here
    \caption{Average suboptimality gap plotted against the number of episodes $N$ for different values of $\beta$ in the case of Mat{\'e}rn kernel with $\nu=1.5$.}
    \label{fig:Hyperparams_Matern1.5}
\end{figure}
\FloatBarrier
\subsection{Repeated Experiments for Different Draws of $r$ and $P$}
To validate the robustness of the results against specific environment realizations, 
we  ran the experiments  three times, with each repetition using different reward and transition probability functions drawn from the RKHS. We kept the hyperparameters (lengthscale, $\tau$, and $\beta$) identical to the ones used in the main paper. The results remained consistent across all repetitions, as shown in Figures \ref{fig:overallresults_experiment1}, \ref{fig:overallresults_experiment3} and \ref{fig:overallresults_experiment4} for different kernels and algorithms.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_kernel_all_algos_experiment1.png}
        \caption{SE Kernel}
        \label{fig:RBF_all_algos_experiment1}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_all_algos_experiment1.png} % Replace with the path to your third figure
        \caption{Mat{\'e}rn kernel with $\nu=2.5$}
        \label{fig:Matern2.5_all_algos_experiment1}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_all_algos_experiment1.png} % Replace with the path to your second figure
        \caption{Mat{\'e}rn kernel with $\nu=1.5$}
        \label{fig:Matern1.5_all_algos_experiment1}
    \end{subfigure}
    \caption{Average suboptimality gap plotted against $N$ for repeated experiment 1}
    \label{fig:overallresults_experiment1}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/RBF_kernel_all_algos_experiment3.png}
        \caption{SE Kernel}
        \label{fig:RBF_all_algos_experiment3}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_all_algos_experiment3.png} % Replace with the path to your third figure
        \caption{Mat{\'e}rn kernel with $\nu=2.5$}
        \label{fig:Matern2.5_all_algos_experiment3}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_all_algos_experiment3.png} % Replace with the path to your second figure
        \caption{Mat{\'e}rn kernel with $\nu=1.5$}
        \label{fig:Matern1.5_all_algos_experiment3}
    \end{subfigure}
    \caption{Average suboptimality gap plotted against $N$ for repeated experiment 2}
    \label{fig:overallresults_experiment3}
\end{figure}
\vspace{-15pt} % Adjust the value as needed
\begin{figure}[ht]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_kernel_all_algos_experiment3.png}
        \caption{SE Kernel}
        \label{fig:RBF_all_algos_experiment4}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_all_algos_experiment3.png} % Replace with the path to your third figure
        \caption{Mat{\'e}rn kernel with $\nu=2.5$}
        \label{fig:Matern2.5_all_algos_experiment4}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_all_algos_experiment3.png} % Replace with the path to your second figure
        \caption{Mat{\'e}rn kernel with $\nu=1.5$}
        \label{fig:Matern1.5_all_algos_experiment4}
    \end{subfigure}
    \caption{Average suboptimality gap plotted against $N$ for repeated experiment 3}
    \label{fig:overallresults_experiment4}
\end{figure}
\newpage
\section{RKHS and Mercer Theorem}\label{appx:rkhs}

Mercer theorem \citep{Mercer1909} provides a representation of the kernel in terms of an infinite dimensional feature map~\citep[e.g., see,][Theorem~$4.49$]{Christmann2008}. Let $\mathcal{Z}$ be a compact metric space and $\mu$ be a finite Borel measure on $\mathcal{Z}$ (we consider Lebesgue measure in a Euclidean space). Let $L^2_\mu(\mathcal{Z})$ be the set of square-integrable functions on $\mathcal{Z}$ with respect to $\mu$. We further say a kernel is square-integrable if
\begin{equation*}
\int_{\mathcal{Z}} \int_{\mathcal{Z}} k^2(z, z') \,d \mu(z) d \mu(z')<\infty.
\end{equation*}

\begin{theorem}
(Mercer Theorem) Let $\mathcal{Z}$ be a compact metric space and $\mu$ be a finite Borel measure on~$\mathcal{Z}$. Let $k$ be a continuous and square-integrable kernel, inducing an integral operator $T_k:L^2_\mu(\mathcal{Z})\rightarrow L^2_\mu(\mathcal{Z})$ defined by
\begin{equation*}
\left(T_k f\right)(\cdot)=\int_{\mathcal{Z}} k(\cdot, z') f(z') \,d \mu(z')\,,
\end{equation*}
where $f\in L^2_\mu(\mathcal{Z})$. Then, there exists a sequence of eigenvalue-eigenfeature pairs $\left\{(\gamma_m, \varphi_m)\right\}_{m=1}^{\infty}$ such that $\gamma_m >0$, and $T_k \varphi_m=\gamma_m \varphi_m$, for $m \geq 1$. Moreover, the kernel function can be represented as
\begin{equation*}
k\left(z, z^{\prime}\right)=\sum_{m=1}^{\infty} \gamma_m \varphi_m(z) \varphi_m\left(z^{\prime}\right),
\end{equation*}
where the convergence of the series holds uniformly on $\mathcal{Z} \times \mathcal{Z}$.
\end{theorem}

According to the Mercer representation theorem~\citep[e.g., see,][Theorem $4.51$]{Christmann2008}, the RKHS induced by~$k$ can consequently be represented in terms of $\{(\gamma_m,\varphi_m)\}_{m=1}^\infty$.

\begin{theorem}(Mercer Representation Theorem) Let $\left\{\left(\gamma_m,\varphi_m\right)\right\}_{i=1}^{\infty}$ be the Mercer eigenvalue-eigenfeature pairs. Then, the RKHS of $k$ is given by
\begin{equation*}
\mathcal{H}_k=\left\{f(\cdot)=\sum_{m=1}^{\infty} w_m \gamma_m^{\frac{1}{2}} \varphi_m(\cdot): w_m \in \mathbb{R},\|f\|_{\mathcal{H}_k}^2:=\sum_{m=1}^{\infty} w_m^2<\infty\right\}.
\end{equation*}
\end{theorem}
Mercer representation theorem indicates that the scaled eigenfeatures $\{\sqrt{\gamma_m}\varphi_m\}_{m=1}^\infty$ form an orthonormal basis for~$\Hc_k$.















