\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Neurips rebuttal}

\date{July 2024}




\input{notation}

\begin{document}

\maketitle

\section*{Reviewer 1}

Questions: In the comparison section (Appendix A), I think the sample complexity in this work should be $H^7$
 because the total number of trajectories is indeed $NH$?

Answer: We thank the reviewer for the valuable feedback and for correctly identifying the $H-$scaling in sample complexity for the online setting. In the case of kernels with exponential eigendecay, where the maximum information gain $\Gamma(N) = \mathcal{O}(\text{polylog}(n))$, the sample complexity with a generative model is $\mathcal{O}\left(\frac{H^6 \text{polylog}\left(\frac{1}{\epsilon}\right)}{\epsilon^2}\right)$, which is the one we put in Table 1 of Appendix A. However, without a generative model, we need to interact with $H$ times more episodes to collect the necessary samples. Specifically, during the exploration phase, the number of episodes required is $NH = \mathcal{O}\left(\frac{H^7 \text{polylog}\left(\frac{1}{\epsilon}\right)}{\epsilon^2}\right)$. Despite this, the algorithm still actually uses $\mathcal{O}\left(\frac{H^6 \text{polylog}\left(\frac{1}{\epsilon}\right)}{\epsilon^2}\right)$ samples to converge to $\epsilon$-optimal policy because it keeps only one sample per episode collected in an unbiased way and drops the other biased samples. We hope this clarifies the point.

Throughout the paper, we acknowledge the additional cost of a factor of $H$ associated with collecting online samples. Nonetheless, even with this increase, we demonstrate improved sample complexities for kernels with polynomial eigendecay. This is in contrast to previous works, such as \cite{qiu2021reward}, which fail to provide finite sample complexities for these cases. 

\section*{Reviewer 2}
Question1: In the case of "Exploration without Generative Models" (Line 218 - 215), why collected transitions are unbiased? Is it because it collects $N$ samples then take the expectation?

In the case of exploration without a generative model, the $N$ samples collected at a specific step $h$ are unbiased because they are selected solely based on maximizing the uncertainty $\sigma_{h,n}$, independent of the predictor $\hat{f}_{h,(n,h_0)}$. This is because, by design, at episode $nH + h_0$, we retain only one sample collected at $h_0$: ${(s_{h_0,n}, a_{h_0,n}, s_{h_0+1,n})}$. Specifically, $(s_{h_0,n}, a_{h_0,n})$ is independent of $s_{h_0+1,n}$. This independence arises from setting $V_{h_0 + 1} = 0$, and when applying kernel ridge regression with $y_{h_0,(n,h_0)} = 0$, the criterion for selecting the action $a_{h_0,n}$ is based on maximizing the estimated $Q$ value, calculated as follows:

\begin{align}
Q_{h_0,(n,h_0)} &= \Pi_{0,H} \left[ \hat{f}_{h_0,(n,h_0)} + \beta(\delta) \sigma_{h_0,n} \right] \
&= \Pi_{0,H} \left[ 0 + \beta(\delta) \sigma_{h_0,n} \right].
\end{align}

However, samples collected from earlier steps ($h = h_0 - 1, \ldots, 1$) will be selected based on maximizing $\hat{f}_{h,(n,h_0)} \neq 0 + \beta(\delta) \sigma_{h,n}$. Consequently, ${(s_{h,n}, a_{h,n})}$ will depend on the estimated value function at the next state $s_{h+1,n}$, introducing a bias between step $h$ and step $h+1$. This effectively results in selecting actions maximizing the sum of uncertainties over multiple steps $h$ up to $h_0$. For this reason, we discard these biased samples, as they contribute to inflating the confidence intervals, and keep only the unbiased sample at $h_0$.

Question 2: Could you explain why \cite{qiu2021reward} inflate $\beta(\delta)$ by a factor of $\Gamma(N)$ (Section 3.3)?

%In \cite{qiu2021reward}, the confidence interval width $\beta$ has a $\sqrt{\Gamma(N)}$ factor in its terms, according to  Theorem $3.3$ of \cite{qiu2021reward}. The reason is that in the exploration phase, they maximize the Upper Confidence Bound (UCB) on the Q function given by $ Q_{h,n} = \Pi_{0,H} \left[ \hat{f}_{h,n} + \frac{\beta(\delta) \sigma_{h,n}}{H} + \beta(\delta) \sigma_{h,n}  \right]$, where $\frac{\beta(\delta) \sigma_{h,n}}{H}$ represents a hypothetical reward at each step, proportional to the uncertainty obtained from kernel ridge regression. This leads to maximizing the associated value function $V_{h,n$, which quantifies the expected sum of hypothetical rewards -- uncertainties along the episode, introducing bias between the samples because the value function at a particular step $h$ will depend on the uncertainties at the next steps. This approach is inspired by the construction of the UCB bonus for kernelized contextual bandit or RL with kernel function approximation, and always leads to a confidence interval width that depends on the maximum information gain.  While we use a similar formula for Q, we do the trick of fixing $h_0$ at each episode, and only keep one sample per episode, collected at $h_0$ in an unbiased way, ensuring that $\hat{f}_{h_0+1,n}$ is $0$. This allows us to save $\sqrt{\Gamma(N)}$ which appears usually in $\beta$, coming from the bias of adaptive samples. 
%is given by the sum of $f^{\hat}$ (which approximates the expected next Value) and $\beta(\delta) \sigma$
% which, hence, takes the role of a reward function that is proportional to the confidence interval of the kernel ridge regression
In \cite{qiu2021reward}, the confidence interval width $\beta$ includes a $\sqrt{\Gamma(N)}$ factor, as stated in Theorem 3.3 of \cite{qiu2021reward}. This factor arises because, during the exploration phase, they maximize the Upper Confidence Bound (UCB) on the Q function, expressed as $Q_{h,n} = \Pi_{0,H} \left[ \hat{f}_{h,n} + \frac{\beta(\delta) \sigma_{h,n}}{H} + \beta(\delta) \sigma_{h,n} \right]$. Here, $\frac{\beta(\delta) \sigma_{h,n}}{H}$ represents a hypothetical reward at each step, which is proportional to the uncertainty derived from kernel ridge regression. Hence, this method maximizes the associated value function $V_{h,n}$, which quantifies the expected sum of hypothetical rewardsâ€” namely, the uncertainties throughout the episode. Consequently, this introduces a bias between the samples, as the estimated value function at a particular step $h$ depends on the uncertainties at subsequent steps. The approach is inspired by the UCB bonus construction for kernelized contextual bandits or reinforcement learning with kernel function approximation and consistently results in a confidence interval width dependent on the maximum information gain.
While we use a similar formula for Q, we design an online algorithm fixing $h_0$ in each episode, retaining only one sample per episode collected at $h_0$ in an unbiased manner (ensuring that $\hat{f}_{h_0+1,n}$ is 0). This allows us to avoid the $\sqrt{\Gamma(N)}$ term typically present in $\beta$, which originates from the bias of adaptive samples.

Question 3: In experiments, why the performance of Greedy Max Variance is pretty similar to the algorithm without generative model?

Answer: Greedy max variance is a heuristic that shares the same fundamental idea as our online algorithm without a generative model: it collects unbiased samples adhering to the Markovian trajectory, by maximizing the kernel-based regression uncertainty at each step while remaining independent of the uncertainty values at subsequent steps. Consequently, since both algorithms retain unbiased samples, we expect them to perform similarly and outperform the baseline method in \cite{qiu2021reward}, which collects biased samples. Although we do not provide theoretical results for this heuristic, leaving that for future work, we included it to intuitively assess whether it would empirically perform better than the baseline. We did not intend to compare it with our method, as the theoretical proof for it is beyond the scope of this paper.

\section*{Reviewer 3}

Question 1: Can you elaborate on the differences between the proposed exploration policy and \cite{qiu2021reward}? Line 47-50 mentions that the sampling of \cite{qiu2021reward} is biased by optimizing the exploration policy with respect to a "hypothetical reward---proportional to the uncertainties of the kernel ridge regression", however, it seems to me that the proposed method is doing the same. Namely, in Eq. 12 the Q function (before clipping) is given by the sum of $\hat{f}$
 (which approximates the expected next Value) and $\beta(\delta) \sigma$
 which, hence, takes the role of a reward function that is proportional to the confidence interval of the kernel ridge regression. Section 3.3 unfortunately didn't clarify the differences for me. Please also elaborate why the approach of \cite {qiu2021reward} is biased. In what sense is the sampling procedure using your confidence interval unbiased?

 %Answer: We thank the reviewer for asking to point out the difference between our approach and \cite{qiu2021reward}. It's true that their way of estimating the Q function is very similar to our equation 12. However, they use the UCB-style $Q_{h,n} = \Pi_{0,H} \left[ \hat{f}_{h,n} + \frac{\beta(\delta) \sigma_{h,n}}{H} + \beta(\delta) \sigma_{h,n} \right]$, by considering $ \frac{\beta(\delta) \sigma_{h,n}}{H}$ as hypothetical reward that they are trying to learn a high value with respect to it, which in turn creates bias between the samples, because the value function estimates the sum of uncertainties along multiple steps. Nevertheless, we use $Q_{h,(n,h_0)} = \Pi_{0,H} \left[ \hat{f}_{h,(n,h_0)} + \beta(\delta) \sigma_{h,n} \right]$. $\beta(\delta) \sigma_{h,n}$ can still be interpreted as hypothetical reward, and we do collect biased samples, but the key difference is that we drop these biased samples and keep only 1 sample per episode, specific to step $h_0$, which is collected in an unbiased way. The way we do that is by following this approach: we do not collect H samples at each episode. Instead, we set  $h_0= n \mod H$ at each episode $n$, and we collect $h_0$ samples. We run LSVI starting from $h_0$ by setting $V_{h_0+1}=0$, and we estimate the Q function (equation 12) at each step by applying the Bellman equation recursively from $h=h_0$ till $h=1$. Therefore, the samples collected greedily according to $Q$ will be biased at steps $h_0-1 ... 1$ but unused, except at step $h_0$, because the estimated value at the next state is $0$. In the case of \cite{qiu2021reward}, they apply LSVI recursively starting from $V_H=0$, which makes only the samples at $h=H$ unbiased, and they add all the samples (including the biased ones) to the dataset. This is the reason of their inflated confidence interval width by a factor of $\Gamma(N)$. Hope this clarifies the confusion. 
 
We thank the reviewer for asking us to highlight the differences between our approach and that of \cite{qiu2021reward}. While it is true that their method of estimating the Q function is very similar to our equation 12, there are key differences in implementation and outcome. In \cite{qiu2021reward}, they use the UCB-style Q function $Q_{h,n} = \Pi_{0,H} \left[ \hat{f}_{h,n} + \frac{\beta(\delta) \sigma{h,n}}{H} + \beta(\delta) \sigma_{h,n} \right]$, where $\frac{\beta(\delta) \sigma_{h,n}}{H}$ is treated as a hypothetical reward. This method aims to learn high values for these rewards, which introduces bias between the samples because the value function estimates the sum of uncertainties over multiple steps.
In contrast, we use $Q_{h,(n,h_0)} = \Pi_{0,H} \left[ \hat{f}_{h,(n,h_0)} + \beta(\delta) \sigma_{h,(n,h_0)} \right]$. While $\beta(\delta) \sigma_{h,n}$ can still be interpreted as a hypothetical reward that leads to biased samples, the key difference is that we discard these biased samples and keep only one unbiased sample per episode, specifically at step $h_0$.
Our approach involves not collecting H samples at each episode. Instead, we set $h_0 = n \mod H$ at each episode $n$ and collect samples only at $h_0$. We run LSVI starting from $h_0$ by setting $V_{h_0+1} = 0$ and estimate the Q function (equation 12) at each step by recursively applying the Bellman equation from $h = h_0$ to $h = 1$.  We drop the biased samples collected at steps $h_0-1, ..., 1$ and keep only the unbiased sample collected at step $h_0$, since the estimated value at the next state $\hat{f}_{h_0+1} = 0$.
In \cite{qiu2021reward}, they apply LSVI recursively starting from $V_{H+1} = 0$, making the samples at $h = H$ unbiased, but they include all samples (biased and unbiased) in the dataset. This results in their confidence interval width being inflated by a factor of $\Gamma(N)$. We hope this clarifies the confusion.

Question 2: I wonder whether the approach should be better derived from a GP perspective. With a GP one would obtain the same mean prediction and confidence interval, but in my opinion the confidence interval occurs more naturally in the GP, whereas KRR often confines with point estimates. Also, the Q function in Eq. 12 somewhat relates to Bayesian optimization of a constant function with the UCB acquisition function. I wonder whether alternative acquisition functions could also be sensible for exploration in the reward-free RL setting, and (relating to the first question) in what sense such alternate approaches would be suboptimal/biased.

Answer:

While GP and KRR produce the same mean predictions and uncertainty estimations, their underlying assumptions are different ? In our scenario, we assume the target function $f=P_hV$ resides in a Reproducing Kernel Hilbert Space (RKHS) with a bounded norm, specifically:  $\|f\|_{\Hc_{k_\varphi}}\le B_1$. We apply the same assumption to the value function:  $\|V\|_{\Hc_{k_\psi}}\le B_2$. Additionally, we incorporate the optimistic closure assumption. However, a sample from a GP is typically not an element of the RKHS defined by its kernel. Therefore, reinterpreting the problem within a GP framework does not necessarily lead to the same derived theorems. [Aya: Not sure if we apply GP framework, does our theorems still hold?].

We thank the reviewer for pointing out that the Q function formula in equation 12 relates to BO with UCB as acquisition function. Other acquisition function can be used, leveraging the uncertainty in the Q function estimates to guide the exploration phase effectively. (like entropy search, Thompson Sampling, max information gain...). [Aya: These acquisition function might lead to sub-optimal sample complexities? why is UCB better? I havent come accross papers that compares diff acquisition functions theoretically ...]

Question 3: Theorem 1 assumes that each $z_t$ is independent of the set ${s'j}_{j=1}^{n}$. Is this assumption violated when exploring without a generative model since $s_{i+1}$ depends on $s_i$ and $a_i$:

Answer: This assumption holds for both our algorithms, whether a generative model is used or not. When using a generative model, the dataset is updated by adding the observation at step $h_0$, such that $\Dc_{h_0,n} = \Dc_{h_0,n-1}\cup \{(s_{h_0,n}, a_{h_0,n}, s_{h_0+1,n})\}$. Here, $s_{h_0,n}$ and $a_{h_0,n}$ are independent of the expected value at the next state $s_{h_0+1,n}$. This independence is ensured by the algorithm's design: we set $V_{h_0+1,n}=0$, making the estimated value at the next state  $\hat{f}_{h_0,(n,h_0)}=\bm{0}$. Consequently, the selection of $a_{h_0,n}$ 
focuses solely on maximizing $\sigma_{h_0,n}$,without considering the mean prediction of KRR. This approach ensures that only independent samples are collected, and therefore does not violate the assumption.

Question 4: The experiments use a reward function and transition function from the RKHS. How does violate these assumptions affect the experimental results?

The synthetic experiments are designed to validate our theoretical findings. Our theorems assume that both  $P_h V$ and $V$ are functions within the RKHS with bounded norms. Accordingly, in our experiments, we selected the reward function $r$ and transition probability $P$ to be within the RKHS to ensure that our derived confidence intervals and sample complexities translate into better empirical performance compared to the baseline \cite{qiu2021reward}. However, if these assumptions are violated - such as if the reward or transition functions exhibit abrupt changes, discontinuities, or unbounded norms, or if the kernel is not positive semi-definite - the model's predictions might become erratic or unrealistic, and the confidence intervals may no longer be reliable. Consequently, there would be no guarantee of good performance. Given that the main contribution of the paper is theoretical, and due to time constraints on conducting more extensive experiments within broader frameworks, we have left testing this for future work.

Question 5:
Can you elaborate on potential broader interest, e.g. of the derived confidence interval as hinted in line 74?

One of our key contributions is a confidence interval that improves upon existing ones, as outlined in Theorem 1. This theorem has wider applicability and can be used in various contexts beyond our specific case, as long as the assumptions of the target function with bounded norm in the RKHS and independent samples are met (ensuring no bias between collected samples). It is particularly  valuable in scenarios involving kernel methods and RL where the goal is to predict a function $f(z)$ and account for the uncertainty using kernel ridge regression. The theorem provides theoretical guarantees for confidence intervals of function estimates that are related to value functions in the context of RL, which are essential for reliable decision-making and analysis. 

Regarding the reviewer's question on the broader impact of this work: This paper establishes a theoretical foundation for understanding complex non-linear models, including neural networks, which remain poorly understood in current literature. By deriving tighter regret bounds in kernelized settings for polynomial-decayed kernel functions - where previous efforts have failed to provide optimal bounds - we contribute to the development of sample-efficient RL algorithms with convergence guarantees. This advancement can aid in creating frameworks for designing more reliable practical systems. Future work could focus on addressing the $H$-increase in sample complexity caused by online sampling, and conducting more extensive experiments on RL benchmarks to demonstrate the effectiveness of kernel methods,


\section*{Reviewer 4}

Question 1: What is the relationship with standard kernel-based RL and the tools that are used in those works? Indeed, there are some works which are used within the analysis, but that are not cited nor discussed within the main text. I would appreciate if the authors can expand the discussion with the related works in this direction.

Answer: Thank you for the insightful question. Indeed, there is a substantial body of work analyzing RL policies that do not rely on generative models or exploratory behavioral policies. Several studies have examined RL in kernelized settings, as explored in \cite{yang2020provably, yang2020reinforcement, chowdhury2019online, domingues2021kernel, vakili2024kernelized}. These works draw inspiration from methods originally designed for linear bandits (Abbasi-Yadkori et al., 2011; Agrawal and Goyal, 2013) and kernelized bandits (Srinivas et al., 2010b; Valko et al., 2013; Chowdhury and Gopalan, 2017).

However, the regret bounds derived in standard kernelized RL settings are not order-optimal for a general class of kernels, necessitating further improvements. Since our study focuses on the reward-free setting, the existing sample complexity results in standard RL do not directly apply. Nonetheless, we employ similar tools such as the optimistic Least Square Value Iteration (LSVI) algorithm \cite{vakili2024kernelized, yang2020provably} in the context of action-value functions represented by kernel functions. We also adopt similar structural assumptions for the kernel setting, solve a kernel ridge regression problem to derive confidence intervals for the expected value function, and bound the maximum information gain.

The novelty of our approach lies in adapting these techniques to the reward-free setting and providing tighter confidence intervals than existing methods. Our contribution includes designing an online algorithm that collects samples in an unbiased manner and eliminating the need for domain partitioning, which was required in \cite{vakili2024kernelized} to derive regret bounds.

Question 2: I wonder what are the relationship with "Reward-Free Kernel-Based Reinforcement Learning", Vakili et al., 2024, which is not discussed within this work.

Answer: 

they used adaptive domain partitioning 

same sample complexity for Matern kernels


\subsection{Additional questions:}

1) Regarding the confidence interval, could you elaborate on how it is tighter compared to the existing results. e.g. Lemma 4.4 in Vakili et al. ICML'24 / Theorem 1 of Vakili and Olkhovskaya, 2023? Which params have better scaling compared to existing confidence interval results? And where does the $\epsilon$-covering number come into play?

Our confidence interval is tighter as it does not depend on  $\sqrt{\Gamma}$ (the maximum information gain), due to the statistical independence of the samples, unlike the case in Lemma 4.4 Vakili et al. ICML'24 and Vakili and Olkhovskaya. Additionally, in contrast to both works, our confidence interval does not include the covering number of the state-action value class due eliminating the need for domain partitioning, and leveraging the optimistic closure assumption. 


\noindent 2)Regarding the final sample complexity by adopting the above confidence interval, it has an extra dependence of 
 $\sqrt{d}$ in Thm 3 compared to Theorem 4.5 in Vakili et al. ICML'24 to achieve the $\epsilon$-optimality. Is it possible to comment on this extra dependence? (I am struggling to understand when considering this extra dependence of 
, can we still safely conclude that the proposed confidence interval is a tighter result. Appreciate your further input.)

1. Scaling with the number $n$ of samples and the parameter $\epsilon$ of the covering number: Lemma 4.4 in Vakili et al., ICML'24 and Theorem 1 of Vakili & Olkhovskaya, 2023 present confidence intervals in a similar form to those in Theorem 1 but with a differnet confidence interval width multiplier $\beta(\delta) = O( B_1 + \sqrt{\Gamma(N)+\log(\frac{1}{\delta}) + \log(N_{\epsilon})})$, where $N_{\epsilon}$ represents the $\epsilon$-covering number of the space of functions defined in Equation (15). Typically, $\epsilon$ is selected relative to $n$, making $\log(N_{\epsilon}) > \Gamma(N)$ the dominant factor.s typically selected with repsect to $n$ leading to $\log(N_{\epsilon}) > \Gamma(N)$ being the dominating factor.






\bibliographystyle{unsrt}
\bibliography{references}
\end{document}