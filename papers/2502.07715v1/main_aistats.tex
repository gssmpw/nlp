%\documentclass[twoside]{article}

%\usepackage{aistats2025}
\documentclass[twoside]{article}
\usepackage[accepted]{aistats2025}




\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{subcaption} % for subfigures
\usepackage{afterpage}
\usepackage{placeins}
\usepackage{multirow}
\usepackage{float} % Add this in the preamble
\usepackage{placeins}

\usepackage{comment}
\raggedbottom

% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
%\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
\bibliographystyle{apalike}

\input{notation}

% Define a professional comment color
\definecolor{commentcolor}{RGB}{0, 102, 204} % A medium blue color

% Define the \sattar command
\newcommand{\aya}[1]{\textcolor{commentcolor}{#1}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement Learning}

\aistatsauthor{Aya Kayal$^{1}$ \And Sattar Vakili$^{2}$ \And  Laura Toni$^{1}$ \And Alberto Bernacchia$^{2}$ }


\aistatsaddress{ $^{1}$University College London \And $^{2}$MediaTek Research } ]


\begin{abstract}
  Reinforcement Learning (RL) problems are being considered under increasingly more complex structures. While tabular and linear models have been thoroughly explored, the analytical study of RL under nonlinear function approximation, especially kernel-based models, has recently gained traction for their strong representational capacity and theoretical tractability. In this context, we examine the question of statistical efficiency in kernel-based RL within the reward-free RL framework, specifically asking: \emph{how many samples are required to design a near-optimal policy?}
    Existing work addresses this question under restrictive assumptions about the class of kernel functions. We first explore this question by assuming a \emph{generative model}, then relax this assumption at the cost of increasing the sample complexity by a factor of $H$, the length of the episode.  We tackle this fundamental problem using a broad class of kernels and a simpler algorithm compared to prior work. Our approach derives new confidence intervals for kernel ridge regression, specific to our RL setting, which may be of broader applicability. We further validate our theoretical findings through simulations.
\end{abstract}

\input{1_introduction}
\input{2_problem_formulation}
\input{3_algorithm}
\input{4_analysis}
\input{5_experimental_results}
\input{6_Conclusion}
\clearpage
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%



 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes, we describe the mathematical setting, preliminaries, and problem formulation in Section \ref{sec:pf}. We present the proposed algorithms in Section~\ref{sec:alg} with their corresponding pseudocodes. All theorems and assumptions are stated in Section~\ref{sec:anal}. Theorem~\ref{the:conf} is self-contained, with all necessary assumptions explicitly mentioned in the body of the theorem. It is formulated to be broadly applicable to other problems. Theorems~\ref{the:gen} and~\ref{the:main} rely on Assumptions~\ref{ass:disc} and~\ref{closure_assumption}, which are clearly stated in Section~\ref{sec:anal}.
]
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes, detailed analysis about the confidence intervals and sample complexities of our proposed algorithms are provided in Theorems~\ref{the:conf},~\ref{the:gen} and~\ref{the:main} of Section \ref{sec:anal}. However, we do not provide analysis for time and space complexities.]
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes, the code used to conduct our experiments is included in a zip file as supplementary material. It also contains a README file and a requirements file to facilitate the installation of all necessary packages.]
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [Yes, we clearly state the full set of assumptions (Assumptions \ref{ass:disc} and \ref{closure_assumption}) in Section \ref{sec:anal}.] 
   \item Complete proofs of all theoretical results. [Yes, we provide the detailed proofs of Theorems~\ref{the:conf}, ~\ref{the:gen} and~\ref{the:main} in Appendices \ref{appx:conf}, \ref{appx:gen}, \ref{appx:main_sample}, respectively.]
   \item Clear explanations of any assumptions. [Yes, we explicitly state the assumptions of Theorem~\ref{the:conf} within the main body of the theorem, making it self-contained. Assumptions~\ref{ass:disc} and~\ref{closure_assumption}, which are necessary for Theorems~\ref{the:gen} and~\ref{the:main}, are clearly articulated and explained separately.]  
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes, we provide the code as an anonymized zip file in the supplementary material, along with a Readme file that instructs the user on how to run the code.]
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes, the main paper provides a core explanation of the results, including the
algorithms tested, kernels used, and the synthetic framework in Section \ref{sec:exp}. For comprehensive details (hyperparameter-tuning, visualizations of the reward and transition probability functions), please refer to Appendix \ref{appx:exp}.]
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes, our results are accompanied by error bars indicating the standard deviation across 80 independent runs of our experiments.]
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes, we include in Appendix \ref{appx:exp} the computational resources required for our experiments.]
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [Yes, we have used the scikit-learn library to implement our kernel-based algorithms, and we have properly cited it (See Appendix \ref{appx:exp}). ]
   \item The license information of the assets, if applicable. [Not applicable]
   \item New assets either in the supplemental material or as a URL, if applicable. [Yes, we submit the code generating our experimental results as a zip file in the supplementary material.]
   \item Information about consent from data providers/curators. [Not Applicable]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Not Applicable]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
 \end{enumerate}

 \end{enumerate}
\appendix
\input{appendix}


\end{document}
