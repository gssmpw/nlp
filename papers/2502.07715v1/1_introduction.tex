\section{Introduction}


Reinforcement Learning (RL) with nonlinear function approximation is a powerful method for learning general Markov Decision Processes (MDPs) through interactions with the environment. Kernel ridge regression for the prediction of the expected value function is perhaps one of the most versatile methods that has gained traction in recent years~\citep{yang2020provably, vakili2024kernelized, chowdhury2023value}, and lends itself to theoretical analysis. As a burgeoning research area, there are still numerous open problems and challenges in this topic.

We focus our work on statistical aspects of RL within the reward-free RL framework \citep{jin2020reward,wang2020reward,qiu2021reward}, which involves an exploration phase and a planning phase. In the exploration phase, the reward is unknown; the algorithm interacts with the environment to gather information about the underlying MDP, in the form of a dataset of transitions. In the planning phase, the reward is revealed; the algorithm uses the knowledge of the reward and the dataset gathered in the exploration phase to design a near-optimal policy. The planning phase is thus akin to offline RL~\citep{precup2000eligibility,antos2008learning,munos2008finite, levine2020offline,xie2021bellman,chen2019information}. In this paper, we answer the following fundamental question:  %The fundamental question we ask is: 
\emph{Under some reasonable assumptions on the underlying MDP, what is the minimum number of samples required to enable designing a near-optimal policy?}


We refer to the number of samples as \emph{sample complexity} and measure the optimality of the eventual policy in terms of error in the value function. In particular we refer to a policy as $\epsilon$-optimal if its value function is at most a small $\epsilon>0$ away from that of the optimal policy for all states. 

The reward-free RL framework has been studied in tabular \citep{jin2020reward} and linear~\citep{wang2020reward} settings.
% , with results shown in Table~\ref{samplecomplexitytable}.
Under the tabular setting, it has been shown that $\Oc(|\Sc|^2|\Ac|H^5/\epsilon^2)$ samples are sufficient to achieve an $\epsilon$-optimal policy, where $\Sc$ and $\Ac$ are the state and action spaces, respectively, and $H$ represents the length of episode. In the linear setting, a sample complexity of $\Oc(d^3H^6/\epsilon^2)$ has been established that does not scale with the size of the state-action space, but the ambient dimension $d$ of the linear model representing the transition structure of the MDP. With the limitations of the linear model~\citep[e.g., as shown in][]{lee2023demystifying}, recent works have considered non-linear function approximation in RL. The work
of~\citet{qiu2021reward} considered the reward-free RL framework with 
kernel-based function approximation. However, their results only apply to very smooth kernels with exponential eigendecay, such as Squared Exponential (SE), but fail to provide finite sample complexity applicable to a large class of kernels of interest with polynomial eigendecay (see Definition~\ref{def:eigendecay}), such as Mat{\'e}rn family or Neural Tangent (NT) kernels.
This shortcoming arises from the bias in the collected samples. Specifically in the exploration phase of~\citet{qiu2021reward}, the samples are adaptively collected to achieve a high value with respect to a hypothetical reward ---proportional to the uncertainties of the kernel ridge regression--- introducing bias to the samples and inflating confidence intervals. 
%\aya{Another closely related work addressing reward-free RL in the kernel setting is \citep{vakilireward}. Similar to \citep{qiu2021reward}, it uses a hypothetical reward proportional to the uncertainty of kernel ridge regression. However, it improves on \citep{qiu2021reward} by deriving order-optimal sample complexities for kernels with polynomially decaying eigenvalues, whereas the results of \citep{qiu2021reward} are unbounded for that case. This enhanced performance is achieved through an adaptive domain partitioning procedure inspired by \citep{vakili2024kernelized}. In this method, the state-action domain is adaptively divided into multiple subdomains as samples are collected, with kernel-based value function estimates constructed based on samples from the same subdomain, while discarding previous observations from other subdomains. More details about their method is given in Section~\ref{sec:comp}.
%They prove a regret bound of $\Oct\left((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$.
%Although their approach offers theoretical advantages, it is tedious to implement in practice due to complex domain partitioning structure. Moreover, discarding samples may degrade the empirical performance, a concern that is not addressed in~\cite{vakilireward}. Additionally, their theoretical results depend on specific assumptions about the relationship between kernel eigenvalues and domain size, which limits generality of their work.}

Another closely related work on reward-free RL in the kernel setting is \citet{vakilireward}, which, like \citet{qiu2021reward}, uses a hypothetical reward proportional to the uncertainty of kernel ridge regression. However, it improves upon \citet{qiu2021reward} by providing order-optimal sample complexities for kernels with polynomially decaying eigenvalues, where \citet{qiu2021reward}'s results are unbounded. This is achieved via an adaptive domain partitioning procedure inspired by \citet{vakili2024kernelized}. In this method, the state-action domain is adaptively divided into multiple subdomains as samples are collected, with kernel-based value function estimates constructed based on samples from the same subdomain, while discarding previous observations from other subdomains. Although their approach offers theoretical advantages, it is tedious to implement in practice due to complex domain partitioning structure. Moreover, discarding samples may degrade the empirical performance, a concern that is not addressed in~\cite{vakilireward}. Additionally, their theoretical results depend on specific assumptions about the relationship between kernel eigenvalues and domain size, which limits generality of their work. A detailed comparison between our work and the two closely related works of~\citet{qiu2021reward} and~\citet{vakilireward} is provided in Appendix~\ref{appx:comp} along with a more comprehensive literature review in Appendix~\ref{appx:lit_review}.



In contrast to the existing work, this paper establishes near-optimal sample complexities for the reward-free kernel-based RL framework over a general class of kernels, without relying on restrictive assumptions. This is accomplished via a simple algorithm and a novel confidence interval for unbiased samples, broadly applicable to other RL settings (offline RL, model-based, infinite horizon), and supported by empirical evidence.
%\aya{In contrast, in this work, we establish near-optimal sample complexities for the reward-free kernel-based RL framework across a general class of kernels, without relying on such specific assumptions. We achieve this through a straightforward algorithm and a novel confidence interval for unbiased samples, which is broadly applicable to other RL problems (offline RL, model-based, infinite horizon), and supported by empirical evidence.}
% This shortcoming arises from the bias inherent in the collected samples. %Specifically in the exploration phase of~\citep{qiu2021reward}, the samples are collected to achieve high value with respect to a hypothetical reward that is proportional to the uncertainties of the kernel ridge regression at each step. This adaptive sampling with respect to a certain reward introduces bias terms to kernel-based confidence intervals that eventually lead to poor analytical sample complexities.%
% Specifically, in the exploration phase of~\citep{qiu2021reward},  the samples are adaptively collected with respect to a hypothetical reward ---proportional to the  kernel ridge regression uncertainty---  introducing bias terms to kernel-based confidence intervals (More details are provide in Section~\ref{sec:comp}). In this case, the hypothetical learned value function accumulates the kernel ridge regression uncertainties across multiple steps, resulting in poor analytical sample complexities.
% %Our main contribution involves establishing near-optimal sample complexities in the reward-free kernel-based RL framework applicable to a broader class of kernels.%
% In this work, we also consider the sample complexity problem in the reward-free kernel-based RL framework. We propose addressing the bias issue of adaptive sampling by ensuring that, within each exploration episode, the transitions specific to each step are collected in a completely unbiased manner. This involves directly maximizing the kernel ridge regression uncertainty at each particular step, independently of other samples within the episode. This is different from using the uncertainty as hypothetical reward as in~\citep{qiu2021reward}, which implicitly selects samples based on maximizing the sum of uncertainties over the next states in the same episode. 
% Our approach also departs from domain partitioning, used in \cite{vakili2021optimal} and usually tailored for special kernels (where Mercer eigenvalues diminish at a specific rate with the domain size). This approach leads us to a broad and yet simple algorithm that has near-optimal sample complexities applicable to more general kernels.   
% Our approach eliminates the need for domain partitioning, a technique used in  [cite recent ICML paper] that involves decomposing the state-action space and constructing value function estimates based solely on observations within each partition.
%Our main contribution involves establishing near-optimal sample complexities in the reward-free kernel-based RL framework applicable to a broader class of kernels.
Specifically, we start with a case where a \emph{generative model}~\citep{kakade2003sample} is present and it permits the algorithm to sample state-actions of its choice during the exploration phase, not limiting the algorithm to stay on the Markovian trajectory. This setting has been extensively considered in previous work on statistical efficiency of RL~\citep[see, e.g.,][]{kearns1998,gheshlaghi2013minimax, sidford2018near, sidford2018variance, agarwal2020model,yang2019sample}. In the presence of a generative model, we propose a simple algorithm that collects \emph{unbiased} samples by choosing the state-actions with highest kernel-based regression uncertainty at each step. 
We derive order-optimal sample complexities for this algorithm in terms of $\frac{1}{\epsilon}$, while \citet{vakilireward} do not offer any particular advantages in the generative model case. 
Generative models are applicable in scenarios like games where the algorithm can manipulate the current state, offering insights into the statistical aspects of RL.  However, this may not be the case in other scenarios. Inspired by the analysis of the exploration algorithm with a generative model, we propose a second online exploration algorithm that collects samples adhering to the Markovian trajectory. We prove that this relaxing of generative model requirement incurs merely an $H$ factor increase in the sample complexity.%; implying that the price of online samples compared to generative samples is an $H$ factor. 

To highlight the significance of our results, we consider kernels with polynomial eigendecay %such as the Mat{\'e}rn family and NT kernels
that are of practical and theoretical interest~\citep{srinivas2009gaussian, jacot2018neural, vakili2023information}. When the eigenvalues of the kernel decay polynomially as $\Oc(m^{-p})$ ---see Definition~\ref{def:eigendecay}---the results of \citet{qiu2021reward} lead to possibly vacuous (infinite) sample complexities, while we prove an $\Oct((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}})$ sample complexity for the generative setting and $\Oct(H(\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}})$ for the online setting. Our sample complexity results are comparable to those of \citet{vakilireward}. In a technical comparison, their approach requires a specific assumption on the dependence between kernel eigenvalues and domain size~\citep[see,][Definition $4.1$]{vakilireward}, which we do not. Additionally, they employ a sophisticated domain partitioning algorithm that is more difficult to implement and possibly inefficient in practice, whereas our algorithm is simpler and more straightforward.
In the case of Mat{\'e}rn kennel with smoothness parameter $\nu$ on a $d$-dimensional domain, where $p=1+\frac{2\nu}{d}$, our results translate to a sample complexity of $\Oct(H(\frac{H^3}{\epsilon})^{2+\frac{d}{\nu}})$, that matches the $\Omega((\frac{1}{\epsilon})^{2+\frac{d}{\nu}})$ lower bound proven in~\cite{scarlett2017lower} for the degenerate case of bandits with $H=1$. Our sample complexities thus are not generally improvable in their scaling with $\frac{1}{\epsilon}$. 

To achieve these results, we establish a confidence interval applicable to kernel ridge regression in our RL setting that may be of broader interest. The key technical novelties of this confidence interval involves leveraging the structure of RKHS and the properties of unbiased, independent samples. The main results regarding the confidence interval and sample complexities of the two exploration algorithms, with and without the generative model, are presented in Theorems~\ref{the:conf}, \ref{the:gen} and~\ref{the:main}, respectively, in Section~\ref{sec:anal}. We empirically validate our analytical findings through numerical experiments comparing the performance of our proposed exploration algorithms with that of \citet{qiu2021reward}, as detailed in Section~\ref{sec:exp}. Section~\ref{sec:pf} provides an overview of episodic MDPs, the reward-free RL framework, and kernel-based models. Section~\ref{sec:alg} presents our algorithms for both the exploration and planning phases. Detailed proofs of theorems, along with the details of experiments and further experimental results, are included in the appendix due to space limitations.
%While we here presented the most closely related work,   
%given the extensive body of research in RL, and due to space constraints, a broader overview of the related work is provided in Appendix \ref{sec:relatedworks}.
%In the introduction, we presented the most closely related work. A more comprehensive literature review is provided in Appendix~\ref{sec:relatedworks}.