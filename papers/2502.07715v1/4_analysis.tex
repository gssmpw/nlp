\section{Analysis of the Sample Complexity}\label{sec:anal}

In this section, we present our main results on the sample complexity of the algorithms. We first establish a novel confidence interval that is applicable to the unbiased samples collected by our exploration algorithms. We then provide theorems detailing the performance of these algorithms. 


\subsection{Confidence Intervals}
We introduce a novel confidence interval that is tighter than existing ones in our RL setting and can also be applied to other RL problems such as offline RL and infinite-horizon settings.
%
\begin{theorem}[Confidence Bounds]\label{the:conf}

Consider compact sets $\Sc\subset\Rr^{d_s}, \Ac\subset\Rr^{d_a}$, and define $\Zc=\Sc\times \Ac$, $d=d_a+d_s$. Consider two Mercer kernels $k_{\varphi}:\Zc\times\Zc\rightarrow \Rr$ and  $k_{\psi}:\Sc\times\Sc\rightarrow \Rr$. Assume that functions $f:\Zc\rightarrow\Rr$ and $V:\Sc\rightarrow\Rr$, and for each $z\in\Zc$, a conditional probability distribution $P(\cdot|z)$ over $\Sc$, are given such that $f(z)=\E_{s\sim P(\cdot|z)}[V(s)]$, $\|f\|_{\Hc_{k_\varphi}}\le B_1$, $\|V\|_{\Hc_{k_\psi}}\le B_2$, and $\max_{s\in\Sc}V(s)\le v_{\max}$, for some $B_1,B_2, v_{\max}>0$.
Assume a dataset of $\{z_{i}, s'_i\}_{i=1}^n$ is provided, where each $z_i$ is independent of the set $\{s'_j\}_{j=1}^n$, and $s'_i\sim P(\cdot|z_i)$.
Let $\hat{f}^n$ and $\sigma^n$ be the kernel ridge predictor and uncertainty estimator of $f$ using the observations:
\begin{align}\nn
    \hat{f}_n(z) &= k^{\top}_{\varphi_n}(z)(\tau^2I+K_{\varphi_n})^{-1}\bm{y}_{n},\\
    \sigma_n^2(z) &= k_{\varphi}(z,z) - k^{\top}_{\varphi_n}(z)(\tau^2I+K_{\varphi_n})^{-1}k_{\varphi_n}(z),
\end{align}
where $\bm{y}_n=[V(s'_1), V(s'_2), \cdots, V(s'_n))]^{\top}$.
In addition, let $\lambda_m$, $m=1,2,\cdots$ represent the Mercer eigenvalues of $k_{\psi}$ in a decreasing order, and $\psi_m$ the corresponding Mercer eigenfunctions. Assume $\psi_m\le \psi_{\max}$ for some $\psi_{\max}>0$. Fix $M\in \Nn$, and let $C$ be a constant such that $C \geq \sum_{m=1}^{M} \lambda_m$. %that serves as an upper bound for the sum of the first $M$ eigenvalues.


Then, for a fixed $z\in\Zc$, and for all $V$, with $\|V\|_{\Hc_{k_\psi}}\le B_2$, we have, the following each hold, with probability at least $1-\delta$,
\begin{equation*}
|f(z) - \hat{f}_n(z)| \le \beta(\delta) \sigma_n(z)
\end{equation*}
%\begin{equation*}
%    f(z) - \hat{f}_n(z) \le \beta(\delta) \sigma_n(z)~~ \text{and} ~~ f(z) - \hat{f}_n(z) \ge -\beta(\delta) \sigma_n(z).
%\end{equation*}
% \begin{equation*}
% \left\{
% \begin{aligned}
% & f(z) - \hat{f}_n(z) \le \beta(\delta) \sigma_n(z),  \\
% & f(z) - \hat{f}_n(z) \ge -\beta(\delta) \sigma_n(z),
% \end{aligned}
% \right.
% \end{equation*}
with $\beta(\delta) =$
\small{\begin{align*}
     B_1+ \frac{C B_2 \psi_{\max} }{\tau}\sqrt{2\log(\frac{M}{\delta})} 
    + \frac{2B_2\psi_{\max}}{\tau}\sqrt{n\sum_{m=M+1}^{\infty}\lambda_m}~.
\end{align*}}

%\aya{, where $C$ is a constant} %that serves as an upper bound for the sum of the first $M$ eigenvalues.}
\end{theorem}



Theorem~\ref{the:conf} provides a confidence bound for kernel ridge regression that is applicable to our RL setting, and is a key result in deriving our sample complexities. 

\paragraph{Proof sketch}
To derive our confidence bounds, we use the Mercer representation of \( V \) and decompose the prediction error \( f(z) - \hat{f}_n(z) \) into error terms corresponding to each Mercer eigenfunction \( \psi_m \). We then divide these terms into two groups: the first \( M \) elements, corresponding to eigenfunctions with the largest eigenvalues, and the remainder. For the top \( M \) eigenfunctions, we establish high-probability bounds using standard kernel-based confidence intervals from \cite{vakili2021optimal}. The remaining terms are bounded based on eigendecay, and we sum over all \( m \) to obtain \( \beta(\delta) \).
%To derive our confidence bounds, we use a novel approach by leveraging the Mercer representation of \( V \) and decomposing the prediction error \( f(z) - \hat{f}_n(z) \) into error terms corresponding to each Mercer eigenfunction \( \psi_m \). We then divide these terms into two groups: the first \( M \) elements, corresponding to eigenfunctions with the largest eigenvalues, and the remainder. For the top \( M \) eigenfunctions, we establish high-probability bounds using standard kernel-based confidence intervals from \citep{vakili2021optimal}. The remaining terms are bounded based on eigendecay, and we sum over all \( m \) to obtain \( \beta(\delta) \).}
\begin{remark}
    Under some mild conditions, for example, the polynomial eigendecay given in Definition~\ref{def:eigendecay}, the following expression can be derived for $\beta$:
    \begin{equation}
        \beta(\delta)= \Oc\left(B_1+\frac{ B_2 \psi_{\max} }{\tau}\sqrt{\log(\frac{n}{\delta})}\right).
    \end{equation}
\end{remark}

With polynomial eigendecay, the remark follows from setting $M$ to $\lceil n^{\frac{1}{p-1}}\rceil 
$ in the expression of $\beta$ in Theorem~\ref{the:conf}.

The confidence interval presented in Theorem~\ref{the:conf} is applicable to a fixed $z\in\Zc$. Over a discrete domain this can be easily extended to all $z\in\Zc$ using a probability union bound and replacing $\delta$ with~$\frac{\delta}{|\Zc|}$ in the expression of $\beta(\delta)$. Using standard discretization techniques, we can also prove a variation of the confidence interval that holds true uniformly over continuous domains. In particular, under the following assumption, we present a variation of the theorem over continuous domains. 


\begin{assumption}\label{ass:disc}
For each $n\in\Nn$, there exists a discretization $\Zz$ of $\Zc$ such that, for any $f\in \Hc_k$ with $\|f\|_{\Hc_k}\le B_1$, we have $f(z) - f([z])\le \frac{1}{n}$, where $[z] = {\arg}{\min}_{ z'\in \Zz}||z'-z||_{l^2}$ is the closest point in $\Zz$ to $z$, and $|\Zz|\le cB_1^dn^{d}$, where $c$ is a constant independent of $n$ and $B_1$.
\end{assumption}
Assumption~\ref{ass:disc} is a mild technical assumption that holds for typical kernels~\citep{srinivas2009gaussian, chowdhury2017kernelized, vakili2021optimal}.

\begin{corollary}\label{Cor:cont}
Under the setting of Theorem~\ref{the:conf}, and under Assumption~\ref{ass:disc}, the following inequalities each hold uniformly in $z\in\Zc$ and $V: \|V\|_{\Hc_{k_\psi}}\le B_2$, with probability at least $1-\delta$
\begin{align*}
    f(z)  \le \hat{f}_n(z) + \frac{2}{n} +  \tilde{\beta}(\delta) (\sigma_n(z)+\frac{2}{\sqrt{n}}), \\
    f(z)  \ge\hat{f}_n(z) -\frac{2}{n} -\tilde{\beta}(\delta) (\sigma_n(z)+\frac{2}{\sqrt{n}}),
\end{align*}
% \begin{equation*}
% \left\{
% \begin{aligned}
% & f(z)  \le \hat{f}_n(z) + \frac{2}{n} +  \tilde{\beta}(\delta) (\sigma_n(z)+\frac{2}{\sqrt{n}}),  \\
% & f(z)  \ge\hat{f}_n(z) -\frac{2}{n} -\tilde{\beta}(\delta) (\sigma_n(z)+\frac{2}{\sqrt{n}}),
% \end{aligned}
% \right.
% \end{equation*}
with $\tilde{\beta}(\delta)=\beta(\frac{\delta}{2c_n})$, $c_n=c(u_n(\frac{\delta}{2}))^dn^d$, and $u_n(\delta) = \Oc(\sqrt{n+\log(\frac{1}{\delta}}))$.
%is a $1-\delta$ upper confidence bound on $\|\hat{f}_n\|_{\Hc_k}$.

\end{corollary}


\begin{remark}
    Under some mild conditions, for example, the polynomial eigendecay given in Definition~\ref{def:eigendecay}, the following expression can be derived for $\tilde{\beta}$:
    \begin{equation}
        \tilde{\beta}(\delta)= \Oc\left(B_1+\frac{C B_2 \psi_{\max}}{\tau}\sqrt{d\log(\frac{n}{\delta})}\right).
    \end{equation}
\end{remark}

 
\subsection{Sample Complexities}

We have the following theorem on the performance of Algorithm~\ref{alg:exp_gen}. 
The weakest assumption one can pose on the value functions is realizability, which posits that the optimal value functions \(V^{\star}_{h}\) for \(h \in [H]\) lie in the RKHS \(H_{k_\psi}\) for some kernel $k_{\psi}:\Sc\times\Sc\rightarrow \Rr$, or at least are well-approximated by \(H_{k_\psi}\). For stateless MDPs or multi-armed bandits where \(H = 1\), realizability alone suffices for provably efficient algorithms~\citep{srinivas2009gaussian, chowdhury2017kernelized, vakili2021optimal}. But it does not seem to be sufficient when \(H > 1\), and in these settings it is common to make stronger assumptions~\citep{jin2020provably, wang2019optimism, chowdhury2023value}.
Following these works, our main assumption is a closure property for all value functions in the following class:
\begin{align}
    \Vc = \left\{
    s\rightarrow \min\left\{
    H, \max_{a\in\Ac}\left\{
    r(s,a) + \varphi^{\top}(s,a)\bm{w} + 
    \right. \right. \right. \nonumber \\
    \left. \left. \left. \beta \sqrt{\varphi^{\top}(s,a)\Sigma^{-1}\varphi(s,a)}
    \right\}
    \right\}
    \right\},
\end{align}

where $0<\beta<\infty$, $\|\bm{w}\|\le\infty$ 

and $\Sigma$ is an $\infty\times \infty$ matrix with $\Sigma \succ \tau^2 I$. 

\begin{assumption}[Optimistic Closure]\label{closure_assumption}
For any $V\in\Vc$, for some positive constant $c_v$, we have $\|V\|_{H_{k_\psi}}\leqslant c_v$.
\end{assumption}

This is the same assumption as Assumption~1 in~\cite{chowdhury2023value} and can be relaxed to value functions $\epsilon$ away from this class as described in Section $4.3$ of~\cite{chowdhury2023value}.
We have the following theorem on the sample complexity of the exploration algorithm with a generative model.


\begin{theorem}\label{the:gen}
    Consider the reward-free RL framework described in Section~\ref{sec:pf}. Assume the existence of a generative model in the exploration phase that allows the algorithm to select state-action pairs of its choice at each step. Let $N_0$ be the smallest integer satisfying
    \begin{equation*}
2H\beta(\delta)\sqrt{\frac{2\Gamma(N_0)}{N_0\log(1+1/\tau^2)}} +\frac{4\beta(\delta)H}{\sqrt{N_0}} +\frac{4H}{N_0}\le \epsilon,
\end{equation*}
with $\beta(\delta) =\Oc(\frac{H}{\tau}\sqrt{d\log(\frac{NH}{\delta})})$ with a sufficiently large constant. 
Run Algorithm~\ref{alg:exp_gen} for $N\ge N_0$ episodes to obtain the dataset $\Dc_N$. Then, use the obtained samples to design a policy $\pi$ using Algorithm~\ref{alg:plan} with $\beta(\delta) =\Oc(\frac{H}{\tau}\sqrt{d\log(\frac{NH}{\delta})})$ with a sufficiently large constant. 
Then, under Assumptions~\ref{ass:rkhsnorm}, \ref{ass:disc} and~\ref{closure_assumption}, with probability at least $1-\delta$, $\pi$ is guaranteed to be an $\epsilon$-optimal policy. 
\end{theorem}

The following theorem presents the sample complexity for exploration without generative models. 


\begin{theorem}\label{the:main}
    Consider the reward free RL framework described in Section~\ref{sec:pf}. Let $N_0$ be the smallest integer satisfying
    \begin{align}\nn
    &3H^2\beta(\delta)\sqrt{\frac{2\Gamma(N_0)}{N_0\log(1+1/\tau^2)}} +\frac{8\beta(\delta)H^2}{\sqrt{N_0}} \\\label{eq:suboptgap}
    &+\frac{4H^2(\log(N_0)+1)}{N_0}+2H^2\sqrt{2N_0\log({\frac{3N_0}{\delta})}}\le \epsilon
\end{align}
with $\beta(\delta) =\Oc(\frac{H}{\tau}\sqrt{d\log(\frac{NH}{\delta})})$ with a sufficiently large constant.     Run Algorithm~\ref{alg:exp2} for $NH\ge N_0H$ episodes to obtain the dataset $\Dc_N$. Then, use the obtained samples to design a policy $\pi$ using Algorithm~\ref{alg:plan}. 
Then, under Assumptions~\ref{ass:rkhsnorm}, \ref{ass:disc} and~\ref{closure_assumption}, with probability at least $1-\delta$, $\pi$ is guaranteed to be an $\epsilon$-optimal policy. 
\end{theorem}
    

The proof of theorems are provided in Appendix~\ref{appx:gen} and~\ref{appx:main_sample}. 

The expression of suboptimality gap after $N$ samples, given in~\eqref{eq:suboptgap}, can be simplified as
\begin{equation*}
    \Oc\left(  H^3\sqrt{\frac{\Gamma(N)\log(NH/\delta)}{N}}\right).
\end{equation*} 
 
\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/RBF_kernel_all_algos.png}
        \caption{SE Kernel}
        \label{fig:RBF_all_algos}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern2.5_all_algos.png} % Replace with the path to your third figure
        \caption{Mat{\'e}rn Kernel with $\nu=2.5$}
        \label{fig:Matern2.5_all_algos}
    \end{subfigure}
    %\hspace{0.3em} 
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/new_figures/Matern1.5_all_algos.png} % Replace with the path to your second figure
        \caption{Mat{\'e}rn kernel with $\nu=1.5$}
        \label{fig:Matern1.5_all_algos}
    \end{subfigure}
    \caption{Average suboptimality gap against $N$. The error bars indicate standard deviation.}
    \label{fig:overallresults}
\end{figure*}
\begin{remark}
Replacing $\Gamma(N)=\Oct(N^{\frac{1}{p}})$ in the case of kernels with polynomial eigendecay, we obtain a sample complexity of 
$
    N = \Oct((\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}).
$
We also recall that without a generative model, we interact with $H$ times more episodes to collect these samples. Specifically, the number of episodes in the exploration phase is 
$NH = \Oct\left(H(\frac{H^3}{\epsilon})^{2+\frac{2}{p-1}}\right)$.

\end{remark}

When specialized for the case of Mat{\'e}rn kernels with $p=1+\frac{2d}{\nu}$, we obtain $NH=\Oct(H(\frac{H^3}{\epsilon})^{2+\frac{d}{\nu}})$ that matches the lower bound for the degenerate case of bandits with $H=1$ proven in~\citet{scarlett2017lower}. Our sample complexity is thus order optimal in terms of $\epsilon$ dependency. We also recall that the existing results lead to possibly vacuous (infinite) sample complexities for these kernels.
