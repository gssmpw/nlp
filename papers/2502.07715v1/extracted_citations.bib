@article{abbasi2013online,
  title={Online learning for linearly parametrized control problems},
  author={Abbasi-Yadkori, Yasin},
  year={2013}
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  year={2008}
}

@article{bartlett2012regal,
  title={REGAL: A regularization based algorithm for reinforcement learning in weakly communicating {MDPs}},
  author={Bartlett, Peter L and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1205.2661},
  year={2012}
}

@article{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@inproceedings{chowdhury2019online,
  title={Online learning in kernelized {M}arkov decision processes},
  author={Chowdhury, Sayak Ray and Gopalan, Aditya},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3197--3205},
  year={2019},
  organization={PMLR}
}

@inproceedings{domingues2021kernel,
  title={Kernel-based reinforcement learning: A finite-time analysis},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={2783--2792},
  year={2021},
  organization={PMLR}
}

@article{gheshlaghi2013minimax,
  title={Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
  author={Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@inproceedings{hazan2019provably,
  title={Provably efficient maximum entropy exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019},
  organization={PMLR}
}

@article{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={4870--4879},
  year={2020},
  organization={PMLR}
}

@article{kearns1998finite,
  title={Finite-sample convergence rates for {Q}-learning and indirect algorithms},
  author={Kearns, Michael and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={11},
  year={1998}
}

@inproceedings{lattimore2023lower,
  title={A lower bound for linear and kernel regression with adaptive covariates},
  author={Lattimore, Tor},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2095--2113},
  year={2023},
  organization={PMLR}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.01643},

}

@article{munos2008finite,
  title={Finite-Time Bounds for Fitted Value Iteration.},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={5},
  year={2008}
}

@article{neu2020unifying,
  title={A unifying view of optimism in episodic reinforcement learning},
  author={Neu, Gergely and Pike-Burke, Ciara},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1392--1403},
  year={2020}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International Conference on Machine Learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@inproceedings{qiu2021reward,
  title={On reward-free rl with kernel and neural function approximations: Single-agent {MDP} and {M}arkov {G}ame},
  author={Qiu, Shuang and Ye, Jieping and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={International Conference on Machine Learning},
  pages={8737--8747},
  year={2021},
  organization={PMLR}
}

@article{russo2019worst,
  title={Worst-case regret bounds for exploration via randomized value functions},
  author={Russo, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{vakili2024kernelized,
  title={Kernelized Reinforcement Learning with Order Optimal Regret Bounds},
  author={Vakili, Sattar and Olkhovskaya, Julia},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{vakili2024open,
  title={Open Problem: Order Optimal Regret Bounds for Kernel-Based Reinforcement Learning},
  author={Vakili, Sattar},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={5340--5344},
  year={2024},
  organization={PMLR}
}

@inproceedings{vakilireward,
  title={Reward-Free Kernel-Based Reinforcement Learning},
  author={Vakili, Sattar and Nabiei, Farhang and Shiu, Da-shan and Bernacchia, Alberto},
  booktitle={Forty-first International Conference on Machine Learning},
  year = {2024}
}

@article{wang2020reward,
  title={On reward-free reinforcement learning with linear function approximation},
  author={Wang, Ruosong and Du, Simon S and Yang, Lin and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17816--17826},
  year={2020}
}

@article{yang2020provably,
  title={Provably efficient reinforcement learning with kernel and neural function approximations},
  author={Yang, Zhuoran and Jin, Chi and Wang, Zhaoran and Wang, Mengdi and Jordan, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13903--13916},
  year={2020}
}

@inproceedings{yang2020reinforcement,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={10746--10756},
  year={2020},
  organization={PMLR}
}

@inproceedings{yao2014pseudo,
  title={Pseudo-{MDPs} and factored linear action models},
  author={Yao, Hengshuai and Szepesv{\'a}ri, Csaba and Pires, Bernardo Avila and Zhang, Xinhua},
  booktitle={2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={1--9},
  year={2014},
  organization={IEEE}
}

@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020},
  organization={PMLR}
}

