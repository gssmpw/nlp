\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\newtheorem{remark}{Remark}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

% updated with editorial comments 8/9/2021

\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}    %UseOutput in the format of Algorithm

\allowdisplaybreaks[3]
\begin{document}

\title{Qubit-Efficient Quantum Annealing for Stochastic Unit Commitment  }

\author{Wei Hong,~\IEEEmembership{Student Member,~IEEE}, Wangkun Xu,~\IEEEmembership{Member,~IEEE}, and Fei Teng,~\IEEEmembership{Senior Member,~IEEE}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}
}

% The paper headers
\markboth{Submitted to IEEE}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}


\maketitle

\begin{abstract}
Stochastic Unit Commitment (SUC) has been proposed to manage the uncertainties driven by the integration of renewable energy sources. When solved by Benders Decomposition (BD), the master problem becomes a binary integer programming which is NP-hard and computationally demanding for classical computational methods. Quantum Annealing (QA), known for efficiently solving Quadratic Unconstrained Binary Optimization (QUBO) problems, presents a potential solution. However, existing quantum algorithms rely on slack variables to handle linear binary inequality constraints, leading to increased qubit consumption and reduced computational efficiency. To solve the problem, this paper introduces the Powell-Hestenes-Rockafellar Augmented Lagrangian Multiplier (PHR-ALM) method to eliminate the need for slack variables so that the qubit consumption becomes independent of the increasing number of bender's cuts. To further reduce the qubit overhead, quantum ADMM is applied to break large-scale SUC into smaller blocks and enables a sequential solution. Consequently, the Quantum-based PHR-ADMM (QPHR-ADMM) can significantly reduce qubit requirements and enhancing the applicability of QA in SUC problem. The simulation results demonstrate the feasibility of the proposed QPHR-ADMM algorithm, indicating its superior time efficiency over classical approaches for large scale QUBO problems under the D-Wave QPU showcases.

\end{abstract}

\begin{IEEEkeywords}
Quantum computing, quantum annealing, two-stage stochastic unit commitment programming, benders decomposition, PHR-Augmented Lagrangian Multiplier, D-ADMM.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{A}{S} the penetration of renewable energy, such as wind and solar, continues to increase within power systems, how to efficiently handle the uncertainties associated with these resources in system operation remains an open question \cite{bellizio2023transition}.

\subsection{Unit Commitment Problem}

To ensure secure and reliable power system operation, the system operator implements unit commitment (UC) in day ahead to decide the online status of the generators that minimizes production costs while ensuring supply and demand balance, given energy forecasts and various operational constraints. The problem can be mathematically formulated as a mixed-integer programming (MIP) problem \cite{Nasri2016NetworkConstrainedAC}, which is NP-hard \cite{Colonetti2020CombiningLR}. The complexity and computational burden of UC poses significant challenges to classic computational methods. 

Depending on different treatments of uncertainties, UC can be classified into deterministic, robust, and stochastic UC. In deterministic UC models, parameters such as load demand are assumed to be precisely known, excluding the representation of uncertainty and resulting in a relatively smaller problem scale. Unlike deterministic models, the robust \cite{Velloso2020TwoStageRUC,Xiong2023MultistageRD,Lorca2017MultistageRUC} and stochastic \cite{Zhang2017ChanceConstrainedTUC,Huang2014TwostageSUC,Asensio2016StochasticUC} UC problems offer a more detailed and realistic representation of power system operations by explicitly accounting for uncertainties through the introduction of random variables \cite{teng2016stochastic}. In SUC, uncertainty is modeled using probabilistic methods by generating a representative set of scenarios through sampling. However, achieving a reliable and feasible solution may require a large number of scenarios, significantly increasing computational complexity. Multi-block Alternating Direction Method of Multipliers (ADMM) and Benders Decomposition (BD) are two algorithms that can efficiently reduce the problem size and allow parallel processing. However, dealing with large numbers of integer variables and scenarios is still challenging for a real-time application.

\subsection{Quantum Computing for Power System Operations}

Quantum computing (QC), an emerging computational paradigm grounded in quantum mechanics\cite{Nielsen2010QC}, provides a novel architecture with the potential to overcome current computational bottlenecks. Unlike classical computing, where the fundamental unit is the bit, QC uses quantum bits, or qubits, to store information. The essence of QC involves exploiting quantum entanglement and superposition to create and manipulate complex multi-eigenstate superpositions \cite{Nielsen2010QC}. This capability allows quantum computers to process multiple eigenstates in parallel. Currently, QC has been developed under two primary computational models: the gate-based model and the quantum annealing (QA) based model \cite{Morstyn2024OpportunitiesQC}. The gate-based model constructs quantum circuits using quantum gates to manipulate qubit states, thereby evolving the probabilities of all eigenstates to achieve specific computational goals. On the other hand, QA utilizes the phenomenon of quantum tunneling to identify the lowest energy state of the system, known as the ground state \cite{Pastorello2019QuantumAL}. In this ground state, the spin orientations of qubits can be interpreted as the global optimal solution to a Quadratic Unconstrained Binary Optimization (QUBO).

In power systems, the potential of QC algorithms has already been explored in initial applications. For example, the Harrow-Hassidim-Lloyd (HHL) algorithm provides exponential acceleration to solve linear problems, including DC optimal power flow (DCOPF) \cite{Amani2023QuantumenhancedDC} and fast decoupled load flow (FDLF) \cite{Feng2021QuantumPF}, compared to classical algorithms. \cite{Nikmehr2023QuantuminspiredPS} introduces a Quantum Monte Carlo Simulation (MCS) method based on Quantum Amplitude Estimation (QAE) for power system reliability assessment under uncertainty, demonstrating a quadratic speedup over classical MCS. Moreover, Quantum Approximate Optimization Algorithm (QAOA) is a promising method for realizing quantum speedup in solving combinatorial optimization problems. For instance, QAOA has been used to determine optimal planning strategies for efficient electric vehicle charging within the power grid \cite{Kimleang2023LeveragingKQAOA}. Furthermore, QA has been applied to solve combinatorial optimal power flow problems using the D-Wave quantum processing unit (QPU), as proposed in \cite{Morstyn2023AnnealingbasedQC}.

Beyond the applications mentioned above, QC offers emerging opportunities for UC. The UC problem can be reformulated into a QUBO format, enabling evolution through a quantum gate circuit designed by QAOA, leveraging Hamiltonian simulation \cite{Mahroo2023LearningIQ, Nikmehr2022QuantumDUC, Feng2023NovelRUC}. Due to the limited capability of QC in handling continuous variables, classical-quantum hybrid methods have been proposed. For example, \cite{Nikmehr2022QuantumDUC} introduces the Quantum ADMM (QADMM) algorithm to partition integer and continuous blocks. Similarly, \cite{Feng2023NovelRUC} employs Quantum Surrogate Lagrangian Relaxation (QSLR) to coordinate quantum and classical resources locally and across neighboring subproblems for computation.

Since the master problem in the BD framework encompasses all first-stage integer variables, QC is particularly suitable for SUC problems. A key challenge is how to efficiently manage the increasing number of Benders cut constraints during Bender's iteration. Slack variables are often used to convert the resulting inequality constraints into equalities \cite{Feng2023NovelRUC, Wang2023QuantumAIS, Gao2022HybridQC, Leenders2024IntegratingQCC,FU2023CoordinatedPR}. However, enabling QPUs to handle these slack variables requires a substantial allocation of qubits for binary encoding (a technique to convert continuous variables into binary variables). With each BD iteration building a new Benders cut, the inclusion of slack variables dramatically extends the qubit demand. This leads to significant qubit overhead, reduced computational efficiency, or even causes chain break, a catastrophic failure of QC. 

\subsection{Contributions}

This paper, for the first time, studies the efficient use of QA for solving SUC within the BD framework. Combining the Powell-Hestenes-Rockafellar Augmented Lagrangian Multiplier (PHR-ALM) and Direct-extended ADMM (D-ADMM), Quantum-based PHR ADMM (QPHR-ADMM) is proposed to significantly reduce the qubit overhead, improve the QC accuracy, and enhance computational efficiency. The contributions of this paper can be summarized as:

\begin{enumerate}
    \item{Compared with conventional QC algorithms that handle inequality constraints with binary-encoded slack variables, the proposed PHR-ALM method eliminates slack variables within the BD framework. We theoretically demonstrate that this method breaks down the linear dependency between the number of qubits and the number of BD iterations. }

    \item{By integrating with D-ADMM algorithm, QPHR-ADMM can break large-scale SUC into smaller blocks, which further reduces qubit numbers and enables a sequential solution. We theoretically prove that the proposed algorithm reduces the multiple linear space complexities into a constant which equals to the time horizon of the SUC or the pre-defined precision in binary encoding.}

    \item For both PHR-ALM and D-ADMM, we provide an analytical derivation of their QUBO and Hamiltonian representations, making them directly implementable with QA algorithms. Through a case study on SUC, we demonstrate that the proposed approach eliminates the need for auxiliary qubits on QPUs compared to basic QA while achieving superior qubit efficiency.

\end{enumerate}
With advancements in quantum technology, QA is expected to surpass classical computation for solving SUC. More broadly, we demonstrate that even with limited quantum resources, the potential of QC can be effectively harnessed through rigorous mathematical reformulations, paving the way for quantum-integrated optimization for power system applications.

The structure of this paper is organized as follows. Section \ref{section:II} describes the scenario-based SUC problem, providing its mathematical formulation within the BD framework, along with the overview of QC technology. Section \ref{section:III} details the design process of the QPHR-ADMM algorithm. Two specific case studies are presented in Section \ref{section:IV} to demonstrate the feasibility, accuracy, and efficiency of the proposed QPHR-ADMM algorithm. Finally, Section \ref{section:V} concludes the paper.

\section{Preliminaries}
\label{section:II}
This section provides the background of SUC and explains how QA can be applied to solve integer optimization problems.

\subsection{Stochastic Unit Commitment}\label{section:II A}

\subsubsection{SUC Problem Formulation}

To model the uncertainty of wind power generation on the supply side and the uncertainty of electricity consumption on the demand side, a two-stage scenario-based SUC model can be applied. The first stage involves making day-ahead UC decisions that satisfy all unit constraints, primarily determining the on/off status of thermal generators. The second stage involves the economic dispatch of thermal generators under each uncertain scenario, while ensuring compliance with operation constraints. 


In detail, during an operation horizon of $T$ periods, the decision variables are categorized into two groups. The first-stage decision variables are binary variables $u_{{g},{t}}$ determining the on/off states for the $g$-th unit in time period $t$. Since these first-stage decisions pertain to day-ahead scheduling, they are independent of realizations of uncertainties. Subsequently, when wait-and-see decisions are made, uncertainties in the power system are realized in terms of the output of wind farms on the supply side and the load on the demand side. The corresponding random variables are defined as $P_{{h},{t}}^{Wind}$ and $D_{{h},{t}}^{Load}$, respectively, for period $t$ for scenario $h$ in the scenario set $\Omega$ with probability $\pi \left ( \xi_{h} \right )$. Due to uncertainties in both supply and demand in the second stage, we introduce load shedding, modeled as a high-cost, fast-start, and unconstrained generator, to maintain the supply-demand balance, thereby enhancing feasibility. Therefore, the decision variables at this stage include not only the output of each unit in different time periods $P_{{h},{g},{t}}^{G}$ but also the load shedding amount $P_{{h},{t}}^{shed}$ during the corresponding periods. 

A complete formulation of the scenario-based SUC is considered as follows\cite{Pablo2009UncertaintyMUC,Ignacio2017AnEfficientRS,Canan2016AnImprovedSUC}.
\begin{subequations}\label{eq:1}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{align}
\min &\sum\limits_{g=1}^{N} \sum\limits_{t=1}^{T} C_{g}^{cons} u_{g,t}+\sum\limits_{h=1}^{K} \pi\left(\xi_{h}\right) \Bigg\{ \sum\limits_ { g = 1 } ^ { N } \sum\limits_ { t = 1 } ^ { T } \bigg[ C_{g}^{quad}\left(P_{h, g, t}^{G}\right)^{2} \nonumber\\ 
&+ C_{g}^{prim } P_{h, g, t}^{G} \bigg]+\sum_{t=1}^{T}C^{shed
 } P_{h, t}^{shed
 } \Bigg\} \label{eq:1a}
\end{align}
\begin{align}
&\text {subject to: } \nonumber\\
& u_{g, t} P_{g}^{\min } \leq P_{h, g, t}^{G} \leq u_{g, t} P_{g}^{\max } \quad \forall h, g, t \label{eq:1b}\\
&P_{g}^{r d} \leq P_{h, g, t+1}^{G}-P_{h, g, t}^{G} \leq P_{g}^{r u} \quad \forall h, g, t \label{eq:1c}\\
&\sum_{g=1}^{N} P_{h, g, t}^{G}+P_{h, t}^{shed
 }+P_{h, t}^{Wind}=D_{h, t}^{Load} \quad \forall h, g, t \label{eq:1d}\\
&u_{g, t} \in\{0,1\} \quad \forall g, t \label{eq:1e}\\
&P_{h, t}^{shed
 }\geq 0\quad \forall h, t \label{eq:1f}
\end{align}
\end{subequations}
where $C_{g}^{quad}$, $C_{g}^{prim}$ and $C_{g}^{cons}$ are cost coefficients. $C^{shed}$ is the penalty cost of load shedding. $P_{g}^{min}$ and $P_{g}^{max}$ donate the minimum and maximum power output. $P_{g}^{rd}$ and $P_{g}^{ru}$ are ramp-up and ramp-down rate limits.
With consideration of economic dispatches in SUC, the objective function aims to minimize the total cost, consisting of constant production costs and the quadratic fuel costs of thermal units in (\ref{eq:1a}).  (\ref{eq:1b}) and  (\ref{eq:1c}) are generator capacity and ramp limits, respectively. (\ref{eq:1d}) represents the power balance constraints for any given period in any scenario.

\subsubsection{Solution Algorithm Based on Benders Decomposition}
The BD method has been widely applied in large-scale power system optimization to reduce its computational burden\cite{Gao2022HybridQC,Leenders2024IntegratingQCC,Roald2023PowerSO}. This algorithm decouples complex, multi-constraint problems into small-scale subproblems, that can be potentially solved in parallel. In (\ref{eq:1b}), the first-stage variables are incorporated into the second-stage constraints, leading to a coupling between the two stages. To decouple the two stages, BD divides the SUC problem (\ref{eq:1a})-(\ref{eq:1f}) into two sections, i.e., a mixed-integer master problem and several scenario-based quadratic subproblems. The master problem represents the first-stage commitment problem, where the variables only include the binary on/off statuses of the units $u_{g, t}^{k}$. 

In detail, the master problem is written as,
\begin{subequations}\label{eq:2}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{align}
\min_{u_{g, t}^{k}, \Upsilon^{k}} \limits\sum_{g=1}^{N} \limits\sum_{t=1}^{T} C_{g}^{c o n s} u_{g,t}^{k}+\Upsilon^{k} \label{eq:2a}
\end{align}
\begin{align}
&\text { subject to: } \nonumber\\
&\Upsilon^{k} \geq \alpha^{lower }\label{eq:2b}\\
&\Upsilon^{k} \geq \sum_{h=1}^{K} \pi\left(\xi_{h}\right)\left\{\sum_{g=1}^{N} \sum_{t=1}^{T}\bigg[C_{g}^{quad}\left(P_{h, g, t}^{G, l}\right)^{2}\right. \nonumber\\
&\quad\!\!\left.+C_{g}^{prim } P_{h, g, t}^{G, l}\bigg]+\sum_{t=1}^{T}C^{shed} P_{h, t}^{shed, l}\right\} \nonumber\\
&\quad\!\!+\sum_{g=1}^{N}\sum_{t=1}^{T} \limits\sum_{h=1}^{K} \theta_{g, t}^{l}\left(\xi_{h}\right)\!\left[u_{g,t}^{k}-u_{g,t}^{l}\right] \!\quad \forall l=0, \ldots, k-1\label{eq:2c} \\
&u_{g, t}^{k} \in\{0,1\} \quad \forall g, t \label{eq:2d}
\end{align}
\end{subequations}
where $\theta_{g, t}^{l}\left(\xi_{h}\right)$s are the dual variables introduced by the subproblems. Note that, due to the introduction of a continuous Benders lower bound (LB), $\Upsilon $, the master problem of the SUC in the $k$-th iteration, given in (\ref{eq:2a})-(\ref{eq:2d}), is characterized as an MIP problem rather than an integer programming problem.

As each scenario is independent, the subproblem is decoupled and can be treated separately. For example, the subproblem for scenario $h$ is given as (\ref{eq:3a})-(\ref{eq:3f}) below. The main objective is to strategize the optimal power generation levels for each unit in different scenarios, along with the corresponding load shedding amount.
\begin{subequations}\label{eq:3}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{align}
\min _{u_{g, t}^{fixed }, P_{h, t}^{G, k}, P_{h, t}^{p, k}, P_{h, t}^{p_{t}, k}} & \pi\left(\xi_{h}\right)\Bigg\{\sum_{g=1}^{N} \sum_{t=1}^{T}\bigg[C_{g}^{q u a d}\left(P_{h, g, t}^{G, k}\right)^{2} \nonumber\\
 +C_{g}^{p r i m} P_{h, g, t}^{G, k} &\bigg]+\sum_{t=1}^{T}C^{shed} P_{h, t}^{shed, k}\Bigg\}\label{eq:3a}
\end{align}
\begin{align}
&\text { subject to: } \nonumber\\
&u_{g, t}^{fixed} P_{g}^{\min } \leq P_{h, g, t}^{G, k} \leq u_{g, t}^{fixed} P_{g}^{\max } \quad \forall h, g, t \label{eq:3b}\\
&P_{g}^{r d} \leq P_{h, g, t+1}^{G, k}-P_{h, g, t}^{G, k} \leq P_{g}^{r u} \quad \forall h, g, t \label{eq:3c}\\
&\sum_{g=1}^{N} P_{h, g, t}^{G, k}+P_{h, t}^{shed, k}+P_{h, t}^{W i n d}=D_{h, t}^{L o a d} \quad \forall h, g, t \label{eq:3d}\\
&u_{g, t}^{fixed }=u_{g, t}^{k}: \theta_{g, t}^{k}\left(\xi_{h}\right) \quad \forall h, g, t\label{eq:3e}\\
&P_{h, t}^{shed, k} \geq 0\quad \forall h, t \label{eq:3f}
\end{align}
\end{subequations}

In each subproblem, $u_{g,t}^{fixed}$ is a newly introduced auxiliary continuous variable and $u_{g,t}^{k}$ is the optimal decision variable of the master problem \cite{Nasri2016NetworkConstrainedAC}. As a result, \eqref{eq:3} results in an upper bound (UB) of \eqref{eq:1}. Meanwhile, due to the convexity of \eqref{eq:3}, the dual variable $\theta_{g, t}^{l}\left(\xi_{h}\right)$s in \eqref{eq:3e} defines the sensitivity of subproblem's objective. After collecting all sensitivities, a new Benders cut (\ref{eq:2c}) can be added in the next iteration and the master problem \eqref{eq:2} becomes a LB to \eqref{eq:1}. Moreover, as subproblem is a continuous quadratic programming, it can be efficiently solved on CPUs.

The BD method decouples the first-stage and second-stage operation, significantly reducing the problem size. 
It is noteworthy that all integer variables are now included in the master problem where QA holds the potential to further improve its computation efficiency.  

\subsection{Quantum Computing
} \label{section:II B}
With the maturity of quantum technology, the currently available quantum annealers have the potential to solve large-scale integer programming problems. This section provides conversion of classical QUBO problems into quantum forms, followed by the principles and challenges of QA algorithm. 

\subsubsection{QUBO Problem and Hamiltonian Representation}
QUBO, an NP-hard problem, is one of the most widely studied optimization problems in QC. In this model, the solution to the optimization problem is composed of a series of binary variables whose value is $0$ or $1$, and these variables correspond exactly to the off or on state of units in \eqref{eq:2}. The general expression of the QUBO problem is given as 

\begin{equation}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\label{eq:4}
\min \sum_{i \in \aleph } \sum_{j \in \aleph , i \neq j} B_{i j} \tau_{i} \tau_{j}+\sum_{i \in \aleph } c_{i} \tau_{i}
\end{equation}
where $\aleph =\{1, \ldots, \mathfrak{n} \}$  is a set of the binary variable indices. Then, $\tau_{i}, \tau_{j} \in\{0,1\}$ are binary variables, and $B_{i j}, c_{i} \in \mathbb{R}$ are the coefficients of quadratic and linear terms, respectively.
 To allow QUBO to be executable by quantum compter, it must be mapped into Ising model. The Ising-Lenz model is one of the prevalently used methods, which originally intended to explain the phase transition of ferromagnets in statistical mechanics \cite{Selinger2016IntroductionTSM}. In quantum mechanics, the binary variables in the Ising model represent the spin direction of particles. The spin-up state $\left | \uparrow  \right \rangle=\begin{bmatrix} 1&0 \end{bmatrix}^{\mathrm{T}}$ and spin-down state $\left | \downarrow  \right \rangle = \begin{bmatrix} 0&1 \end{bmatrix}^{\mathrm{T}}$ represent $\upsilon$   correspond to $+1$ and $-1$, respectively \cite{Nielsen2010QC}. In QC, we define $\left | 0  \right \rangle$ is spin up and $\left | 1  \right \rangle$ is spin down. Hence, the substitution relationship between QUBO and the Ising model is $\tau=(1-\upsilon) / 2$. The expression of the Ising model can then be derived from (\ref{eq:4}) as
\begin{equation}\label{eq:5}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{split}
   \min \sum_{i \in \aleph } \sum_{j \in \aleph , i \neq j} Q_{i j} \upsilon_{i} \upsilon_{j}+\sum_{i \in \aleph } p_{i} \upsilon_{i}+  \text{Const.}\quad \text{with} \\
   Q_{i j}=\frac{B_{i j}}{4}, \quad p_{i}=-\sum_{j \in \aleph , i \neq j}\left(\frac{B_{i j}+B_{j i}}{4}\right)-\frac{c_{i}}{2}
\end{split}
\end{equation}

In QC, all results are obtained through measurement, in contrast to classical computing. Therefore, two key aspects of measurement require precise definitions. 

The first is the representation. In a quantum system, the probability distribution characteristics of particles are clarified by wave functions, and the expansion of the wave functions varies depending on the chosen representation. Hence, useful measurement results only occurs when the representation is clearly defined. Generally, the spin direction along the z-axis is chosen as the representation in QC. 

The next necessary concept is the measurement operator. According to the definition in quantum mechanics, any measurable physical quantity can be characterized by a Hermitian operator, which is a square matrix acting on the state space. Additionally, once the measurement operator is determined, all possible measurement results must be eigenvalues of the observable. From this, it is evident that the key to selecting the measurement operator lies in matching the appropriate eigenstates and eigenvalues. As mentioned above, spin up $\left | \uparrow  \right \rangle=\begin{bmatrix} 1&0 \end{bmatrix}^{\mathrm{T}}$ and spin down $\left | \downarrow  \right \rangle = \begin{bmatrix} 0&1 \end{bmatrix}^{\mathrm{T}}$ are eigenstates in the Ising model, with corresponding eigenvalues of 1 and -1. Thus, we can use Pauli-z operator $\boldsymbol{\sigma}^{z}$ to explicitly delineate our measurement space. By replacing $\upsilon$ with $\boldsymbol{\sigma}^{z}$ in (\ref{eq:5}), the quantum expression of the QUBO objective function becomes a Hermitian operator called Hamiltonian, which is composed of Pauli-z operators as
\begin{equation}\label{eq:6}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{aligned}
\boldsymbol{H}_{Q U B O} & =\sum_{i \in \aleph } \sum_{j \in \aleph , i \neq j} Q_{i j} \boldsymbol{\sigma}_{i}^{z} \otimes \boldsymbol{\sigma}_{j}^{z}+\sum_{i \in \aleph } p_{i} \boldsymbol{\sigma}_{i}^{z},\; \text{with} \\
\boldsymbol{\sigma}^{z} & =\left[\begin{array}{cc}
1 & 0 \\
0 & -1
\end{array}\right]
\end{aligned}
\end{equation}

\subsubsection{Quantum Annealing}
QA utilizes the quantum tunneling effect, induced by quantum fluctuations, to overcome suboptimal local solutions, enabling the search for the global minimum of an objective function among a vast number of candidate local minimum solutions \cite{Pastorello2019QuantumAL}. Quantum tunneling effect can tunnel through barriers, transitioning directly from a local minimum to the global minimum. Therefore, the QA algorithm usually outperforms classical computing in solving QUBO problems.

QA design is grounded in the principle of adiabatic computation. This theory stresses that if the initial qubits are in the eigenstate of the initial Hamiltonian, the final quantum state will evolve into the ground state of the objective Hamiltonian when the evolution of quantum systems is sufficiently slow and no energy level crossings occur \cite{Morstyn2023AnnealingbasedQC,Albash2018AdiabaticQC}. When the objective Hamiltonian corresponds to the Ising model mapped from the objective function, the final ground state of QA can be regarded as the global optimum of the objective function. However, if the barriers in the problem are too wide, it is possible for the system to evolve into a local optimum. The mathematical expression of this process is as
\begin{equation}\label{eq:7}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\boldsymbol{H}_{mixed}(t)=[1-s(t)] \boldsymbol{H}_{I}+s(t) \boldsymbol{H}_{F}
\end{equation}
where $\boldsymbol{H}_{I}$ is an initial Hamiltonian, $\boldsymbol{H}_{F}$ is a final Hamiltonian that equals the objective Hamiltonian, and $s(t)$ is referred to as the normalized evolution time.

The described QA procedure can currently be executed on D-Wave QA processors. The processing units of the D-Wave 2000Q quantum computer consist of lattices made up of tiny metal loops, with each loop representing a quantum bit. The interconnection structure between qubits is known as the Chimera graph, which is composed of interconnected unit cells, with each unit cell containing 8 qubits. Once the QUBO expression of the objective function is formulated, the corresponding graph embedding can be constructed on the D-Wave QA-QPU, enabling the identification of the eigenstate with the lowest energy. 

\subsubsection{Challenges of QA}

Despite D-Wave QA-QPUs, such as the Advantage\_system 4.1, offering a substantial qubit count (up to 5,627 qubits) \cite{DWave2024QPUSpecificPP}, practical applications are still limited. First, the non-fully connected topology of QA-QPUs requires more qubits for embedding than the actual QUBO variables in theory, as additional qubits are needed to map the objective function correctly. For example, achieving a fully connected topology for 4 binary variables in a QUBO problem may require 6 qubits in the QA-QPU as Fig. \ref{fig_1} showing. Second, QA is affected by the chain-breaking issue \cite{Elijah2023ComparingTG}, where the embedding process fails to preserve the integrity of the QUBO representation, leading to erroneous annealing outcomes. These limitations significantly restrict the scalability and effectiveness of QA in directly solving large-scale optimization problems.

\begin{figure}[!t]
\centering
\includegraphics[width=2.4in]{Figures/QA.pdf}
\vspace{-0.6em}
\caption{4-qubit full-connected QUBO problem and the embedded structure in Chimera unit cell (Ignore the chain strength embedding).}
\label{fig_1}
\vspace{-1em}
\end{figure}


\section{Proposed QPHR-ADMM Algorithm} \label{section:III}

As highlighted in the previous section, designing a QA algorithm with minimal qubit overhead is crucial for addressing real-world problems. This section first introduces binary encoding and augmented Lagrangian function to transform \eqref{eq:2} into QUBO form and then demonstrates that a direct implementation of QA for solving \eqref{eq:2} inevitably requires an increasing number of additional qubits as the Benders' iterations progress, compounding the already significant qubit demand introduced by SUC itself. Two novel algorithms, namely PHR Augmented Lagrangian and quantum-based ADMM, are proposed to resolve the challenges with theoretical analysis on their space complexity.  

\subsection{Pre-processing for Quantumization}
\label{section:III A}
As the master problem in Section \ref{section:II A} is a constrained MIP problem rather than in a QUBO form, QA-QPUs cannot directly construct the corresponding quantum embedding. Therefore, preprocessing techniques are presented in this section. 

\subsubsection{Binary Encoding}

To start, the continuous variable $\Upsilon$, introduced by Benders LB \eqref{eq:2}, cannot be directly processed by the QPU. Binary encoding \cite{Zhao2021HybridQB}, expressed as $\Upsilon^{k}=\chi \sum_{j=0}^{J-1} 2^{j} u_{j}^{k}$, can be applied where $u_{j}^{k}$ are binary variables. By selecting the appropriate precision coefficients $\chi$ and precision levels $J$, the desired precision can be attained. Leveraging the $NT$ number of binary variables in the original SUC, the qubit occupation after binary encoding becomes $NT+J$.

\subsubsection{Augmented Lagrangian Function}

To eliminate the inequality Bender's cut \eqref{eq:2c}, augmented Lagrangian can be used by introducing extra slack variables into the objective. However, since slack variables are continuous, they also require binary encoding. According to (\ref{eq:2c}), the number of Benders cuts grows with the number of iterations $k$. Assuming the precision level of each slack variable is $\mathcal{F}$, the qubit overhead for the QPU grows linearly as iteration $k$ increase and can be denoted as $NT+J+k\mathcal{F}$. 

Considering the limited number of qubits in quantum computers, introducing slack variables is inefficient and unacceptable. This inefficiency serves as the primary motivation for the PHR Lagrangian algorithm proposed in this Section \ref{section:III B}. Moreover, the qubit occupation grows linearly with the number of generator $N$. This qubit overhead will be resolved by the quantum ADMM algorithm in \ref{section:III C}.

\subsection{Quantum PHR Augmented Lagrangian Algorithm}

\label{section:III B}

\subsubsection{Classic Formulation}
The Powell-Hestenes-Rockafellar Augmented Lagrangian Multiplier (PHR-ALM) method was originally designed for equality-constrained optimization problems. Rockfellar later extended it to handle inequality constraints, and this method eliminates the need for slack variables in optimization problems \cite{Rockafellar1974AugmentedLM}, which is an ideal property for QA.

To start, the master problem \eqref{eq:2}, after binary encoding, can be compactly formulated as a linear binary optimization (LBO) problem.
\begin{equation}\label{eq:8}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{aligned}
\min \ &f(\boldsymbol{x}) \\
\text { s.t. } &g_{i}(\boldsymbol{x}) \leq 0 \text { for } i=1, \ldots, w
\end{aligned}
\end{equation}
where $n$ represents the total number of unit states and binary-encoded variables. $\boldsymbol{x} \in\{0,1\}^{n}$ and $w$ is the total number of binary inequality constraints after $w$ BD iterations. After introducing slack variables $\mathcal{S} _i$ to relax each inequality constraints, the augmented Lagrangian function $\mathcal{L}$ of (\ref{eq:8}) can be given as
\begin{equation}\label{eq:9}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{aligned}
\mathcal{L} & =f(\boldsymbol{x})\!+\kappa(\boldsymbol{x}, \mathcal{S}, \lambda, \sigma) \\
& =f(\boldsymbol{x})\!+\lambda_{i} \sum_{i=1}^{w}\left[g_{i}(\boldsymbol{x})\!+\mathcal{S}_{i}{}^{2}\right]\!+\frac{\sigma}{2} \sum_{i=1}^{w}\left[g_{i}(\boldsymbol{x})\!+\mathcal{S}_{i}{}^{2}\right]^{2}
\end{aligned}
\end{equation}
where $\kappa$ is the augmented penalty function involving Lagrange multipliers $\lambda$ and the penalty coefficient $\sigma$.

In order to eliminate continuous slack variables $\mathcal{S}_i$s, it is necessary to derive their analytic expressions. Considering the minimization of $\mathcal{L}$ with respect to slack variables, the first-order necessary condition with respect to $\mathcal{S}$ for optimality can be expressed as
\begin{equation}\label{eq:10}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\nabla_{\mathcal{S}} \mathcal{L}=2 \lambda_{i} \mathcal{S}_{i}+2 \sigma \mathcal{S}_{i}\left[g_{i}(\boldsymbol{x})+\mathcal{S}_{i}{}^{2}\right]=0 \quad \forall i
\end{equation}
Solving (\ref{eq:10}), the square form of slack variables can be obtained as
\begin{equation}\label{eq:11}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
{\mathcal{S}_{i}{}^{2}} = \begin{cases}
\hfil{-\frac{\lambda_{i}}{\sigma}-g_{i}(\boldsymbol{x})},&\lambda_{i}+\sigma g_{i}(\boldsymbol{x}) \leq 0 \\ 
\hfil{0,}&{\lambda_{i}+\sigma g_{i}(\boldsymbol{x})>0} 
\end{cases}
\end{equation}

From (\ref{eq:11}), when $\lambda_{i}+\sigma g_{i}(\boldsymbol{x}) \leq 0$, $\mathcal{S}_{i}{}^{2}$ is non-negative, which means that the solution satisfies the $i$-th constraint $g_{i}(\boldsymbol{x}) \leq 0$. In contrast, it indicates that the solution is not within the feasible region. Next, substituting the above results into the augmented penalty function, the corresponding PHR-based expression is given as follows,
\begin{equation}\label{eq:12}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
{\kappa_{i}} = \begin{cases}
\hfil -\frac{\lambda_{i}{}^{2}}{2 \sigma},&\lambda_{i}+\sigma g_{i}(\boldsymbol{x}) \leq 0 \\ 
\hfil{\frac{1}{2 \sigma}\left[\left(\sigma g_{i}(\boldsymbol{x})+\lambda_{i}\right)^{2}-\lambda_{i}{}^{2}\right],}&{\lambda_{i}+\sigma g_{i}(\boldsymbol{x})>0} 
\end{cases}
\end{equation}
Combining the above two situations, the PHR-based augmented Lagrangian function can be concisely rewritten as
\begin{equation}\label{eq:13}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\mathcal{L}=f(\boldsymbol{x})+\frac{1}{2 \sigma} \sum_{i=1}^{w}\left(\left[\max \left\{\sigma g_{i}(\boldsymbol{x})+\lambda_{i}, 0\right\}\right]^{2}-\lambda_{i}{}^{2}\right)
\end{equation}
which can be solved by iterative algorithm as follows.

At iteration $\ell$, solving the unconstrained PHR augmented Lagrangian function \eqref{eq:13} yields the solution $\boldsymbol{x}^\ell$. If $\boldsymbol{x}^\ell$ satisfies the stop criterion (\ref{eq:14c}), the iteration can be terminated, and the output $\boldsymbol{x}^\ell$ is the approximate minimum solution of the original problem. Otherwise, update the next step Lagrange multipliers $\lambda_i^{\ell+1}$ and the penalty coefficient $\sigma^{\ell+1}$ according to the rules in (\ref{eq:14a}) and (\ref{eq:14b}) for the next iteration.
\begin{subequations}\label{eq:14}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\vspace{-0em}
\begin{align}
\lambda_{i}^{\ell+1}&=\max \!\left\{\lambda_{i}^{\ell}\!+\sigma^{\ell} g_{i}\left(\boldsymbol{x}^{\ell+1}\right)\!, 0\right\} \text { for } i=1, \ldots, w \label{eq:14a}\\
\sigma^{\ell+1}&=\eta \sigma^{\ell} \label{eq:14b}\\
\mathcal{R}^{\ell}&=\left[\sum_{i=1}^{w}\left(\max \left\{-\frac{\lambda_{i}^{\ell}}{\sigma^{\ell}}, g_{i}\left(\boldsymbol{x}^{\ell+1}\right)\right\}\right)^{2}\right]^{1 / 2} \leq \delta \label{eq:14c}
\end{align}
\end{subequations}

Compared to the general augmented Lagrangian multiplier method, the absence of introduced slack variables in PHR-ALM is a major advantage, making it particularly well suited for quantum algorithm reformulation to solve inequality-constrained optimization problems. 

\subsubsection{Equivalent QUBO Form and Hamiltonian}

In the QPHR-ALM framework, the new QUBO form of the objective function is obtained in the previous iteration, so using a QA processor, the $\ell$-th  iteration eigenstate $\left | \boldsymbol{x}^\ell  \right \rangle$, where $\boldsymbol{x}^\ell$ is a binary qubit string, can then be measured. It is worth highlighting that a bit string is not the same as an eigenstate. For example, in a two-qubit system, when the measured result $\left | \boldsymbol{x}^\ell  \right \rangle$ is $\left | 00  \right \rangle$, the bit string $\boldsymbol{x}^\ell$ is $00$, which means that the two binary variables $x_1^\ell$ and $x_2^\ell$ are $0$. However, the eigenstate is $\left | 00  \right \rangle=\left | 0  \right \rangle \otimes \left | 0  \right \rangle=\begin{bmatrix} 1&0&0&0 \end{bmatrix}^{\mathrm{T}}$, which is not equivalent to $\boldsymbol{x}^\ell$. Therefore, to obtain the quantum state solution in the next iteration, we must consider the structure of the Hamiltonian. Due to the addition of an augmented penalty term, the Hamiltonian consists of two parts, an objective Hamiltonian $\boldsymbol{H}_{O b j}$  representing an objective function $f(\boldsymbol{x})$, and the PHR-based augmented Lagrangian Hamiltonian $\boldsymbol{H}_{P H R}$  standing for the PHR augmented penalty function.
\begin{equation}\label{eq:15}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\boldsymbol{H}=\boldsymbol{H}_{O b j}+\boldsymbol{H}_{P H R}=\boldsymbol{H}_{O b j}+\sum_{i=1}^{w}\boldsymbol{H}_{P H R,i}
\end{equation}

It is evident that $\boldsymbol{H}$ is not constant in each iteration. When an optimization problem is well-defined, it results in a clear expression of  $f(\boldsymbol{x})$, ensuring the invariance of $\boldsymbol{H}_{O b j}$. However, $\boldsymbol{H}_{P H R}$ is not fixed, and it represents the aggregate of the Hamiltonians corresponding to all unsatisfied constraints. Additionally, since the $\max$ function in (\ref{eq:13}) cannot be directly embedded in QA-QPUs, $\boldsymbol{x}^\ell$ is employed to classify the PHR augmented penalty function from (\ref{eq:12}) into two cases, determining the expression for $\boldsymbol{H}_{P H R}$
for all inequality constraints.

\begin{itemize}{}{}
    \item \textit{Case 1}: Substituting the bit string $\boldsymbol{x}^\ell$ into $\lambda_{i}+\sigma g_{i}(\boldsymbol{x})$, if the result is less than or equal to 0, the bit string of the eigenstate $\left | \boldsymbol{x}^\ell  \right \rangle$ satisfies the $i$-th  inequality constraint. Then, according to (\ref{eq:12}), the relevant $\boldsymbol{H}_{P H R,i}$ is a diagonal matrix with component $-\left(\lambda_{i}{}^{2} / 2 \sigma\right)$. Actually, in this case, the PHR augmented Lagrangian Hamiltonian is equivalent to a constant term, so it does not affect the optimal solution but only influences the value of objective function. Therefore, it can be neglected directly.
    \item \textit{Case 2}: When ${\lambda_{i}+\sigma g_{i}(\boldsymbol{x}^\ell)>0}$, it implies that $\left | \boldsymbol{x}^\ell  \right \rangle$ violates the current constraint, and the eigenstate result lies outside the feasible region. In this case, it is necessary to apply a related augmented penalty Hamiltonian to guide the next eigenstate sequence $\left | \boldsymbol{x}^{\ell+1}  \right \rangle$ closer to the feasible region. Consequently, QUBO-formed expression of $\boldsymbol{H}_{P H R,i}$ can be obtained by (\ref{eq:12}).
\end{itemize}

Consequently, when $\left | \boldsymbol{x}^\ell  \right \rangle$ is known, the QUBO form of the PHR augmented Lagrangian function can be derived using (\ref{eq:12}) and (\ref{eq:13}). Next, convert the QUBO form into the Hamiltonian $\boldsymbol{H}_{P H R}$ by (\ref{eq:5}) and (\ref{eq:6}). After updating the new Hamiltonian, we can use it to evolve a new quantum state $\left | \boldsymbol{x}^{\ell+1}  \right \rangle$. If the acquired quantum state satisfies the stop criterion (\ref{eq:14c}) or reaches the maximum iteration limit, the iteration can be terminated. Otherwise, proceed to adjust Lagrange multipliers and penalty coefficients relying on (\ref{eq:14a}) and (\ref{eq:14b}).

To sum up, by eliminating the slack variables, the qubit overhead of the QPHR-ALM algorithm is reduced  from $NT+J+k\mathcal{F}$ to $NT+J$, which is independent to the number of Bender iterations.

\begin{algorithm}[H]
\caption{QPHR-ALM}\label{alg:alg1}
\begin{algorithmic}[1]
\REQUIRE{LBO-formed $f(\boldsymbol{x})$ and linear binary constraints}
\ENSURE{Optimal quantum state $\left|\boldsymbol{x}^{*}\right\rangle$}
\STATE $ \textbf{Initialization: } \ell \gets 1, \sigma_{0}>0, \epsilon>0, \lambda_{i}^{0} \gets 0 \ \forall i$
\WHILE {(\ref{eq:14c}) does not satisfy $\textbf{or}$ $\ell<\ell^{max }$}
\STATE Obtain  $\left|\boldsymbol{x}^{\ell}\right\rangle$  using QA algorithms
\FOR {each inequality constraint}
        \IF {$\lambda_{i}+\sigma g_{i}\left(\boldsymbol{x}^{\ell}\right)>0$}
        \STATE Add an augmented penalty Hamiltonian using the QUBO form based on (\ref{eq:5}), (\ref{eq:6}) and (\ref{eq:12})
        \ELSE
        \STATE Keep Hamiltonian unchanged
        \ENDIF
\ENDFOR
\STATE Renew the QUBO form of the objective function and the corresponding Hamiltonian 
\STATE Update coefficients using (\ref{eq:14a}) and (\ref{eq:14b})
\STATE $\ell \gets \ell+1$
\ENDWHILE
\end{algorithmic}
\label{alg1}
\end{algorithm}

\subsection{Quantum-based PHR-ADMM Algorithm}
\label{section:III C}
Although the QPHR-ALM method removes slack variables, reducing the qubit overhead to
$NT+J$, this is still insufficient to meet the requirements of the actual SUC problem. For example, when scheduling 24-hour operation of a power system with 12 generator units hourly, the master problem, after applying BD, would involve at least 288 integer variables. Considering the discretized LBs, it might require a total of over 300 qubits after binay encoding. For large-scale power systems, the qubit requirement could escalate to thousands or even tens of thousands, far exceeding the capacity of current QPUs.

Since it is impractical to rely on a single quantum computer for the entire computational workload, the ADMM algorithm can be implemented for the SUC master problem. This approach decomposes a large-scale LBO-formatted integer programming master problem into several smaller sub-problems that can be efficiently managed by smaller scale QPUs, and the solution to the large global problem is obtained by coordinating the solutions to all the sub-problems. 

For multi-unit, multi-period SUC problems, the D-ADMM approach can decompose the master problem by units, which means that the states for all time periods of each unit are grouped into the same block. Considering that the number of decomposed subproblems will exceed two, the multi-block D-ADMM algorithm for the master problem can be compactly expressed as
\begin{equation}\label{eq:16}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{aligned}
\min & \sum_{m=1}^{M} f_{m}\left(\boldsymbol{x}_{m}\right) \\
\text { s.t. } & \sum_{m=1}^{M} g_{i, m}\left(\boldsymbol{x}_{m}\right) \leq 0 \text { for } i=1, \ldots, w
\end{aligned}
\end{equation}

Based on a similar derivation process in Section \ref{section:III B}, we can easily obtain the PHR-based augmented Lagrangian function for the D-ADMM form after eliminating the slack variables. 
\begin{multline}\label{eq:17}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\!\!\!\!\!\!\mathcal{L}_{D}\!=  \frac{1}{2 \sigma} \!\sum_{i=1}^{w}\left(\!\left[\max \left\{\sigma \!\sum_{m=1}^{M} g_{i, m}\left(\boldsymbol{x}_{m}\right)\!+\lambda_{i}, 0\right\}\right]^{2}\!\!-\!\lambda_{i}{}^{2}\!\right)\\
 +\sum_{m=1}^{M} f_{m}\left(\boldsymbol{x}_{m}\right)
\end{multline}


Next, the integer variables within each block can be updated step-by-steps in (\ref{eq:18a}) using a QA-QPU. After updating all state variables, the Lagrange multipliers and penalty coefficients are adjusted according to (\ref{eq:18b}) and (\ref{eq:18c}), respectively. The error is evaluated to determine whether it satisfies the convergence criteria in (\ref{eq:18d}).
\begin{subequations}\label{eq:18}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{align}
\boldsymbol{x}_{m}^{\ell+1}=&\arg \min _{\boldsymbol{x}_{m}^{\ell}} \mathcal{L}_{D}(\boldsymbol{x}_{1}^{\ell+1}, \ldots, \boldsymbol{x}_{m-1}^{\ell+1}, \boldsymbol{x}_{m}^{\ell}, \boldsymbol{x}_{m+1}^{\ell}, \ldots, \boldsymbol{x}_{M}^{\ell}, \nonumber\\
&\lambda^{\ell}, \sigma^{\ell} ) \text { for } m=1, \ldots, M \label{eq:18a}\\
\lambda_{i}^{\ell+1}=&\max \left\{\lambda_{i}^{\ell}+\sigma^{\ell} \sum_{m=1}^{M} g_{i, m}\left(\boldsymbol{x}_{m}^{\ell+1}\right), 0\right\}\! \text { for } i=1, \ldots, w \label{eq:18b}\\
\sigma^{\ell+1}=&\eta \sigma^{\ell} \label{eq:18c}\\
\mathcal{R}^{\ell}\!=\!&\left[\sum_{i=1}^{w}\!\left(\max \left\{\!-\frac{\lambda_{i}^{\ell}}{\sigma^{\ell}}, \sum_{m=1}^{M} g_{i, m}\left(\boldsymbol{x}_{m}^{\ell+1}\right)\right\}\right)^{2}\right]^{1 / 2} \!\leq \delta \label{eq:18d}
\end{align}
\end{subequations}

The convergence of QPHR-ADMM is based on D-ADMM. Its convergence properties remain as an open problem \cite{Hong2017linearCADMM}. However, to achieve optimal convergence, the following criteria should ideally be met: 1) Strong convexity. 2) The penalty coefficient $\sigma $ should be selected within a suitable range. 3) The step size factor $\eta$ should be sufficiently small \cite{Hong2017linearCADMM,Han2012noteADMM}. 

Notably, as each block in \eqref{eq:18a} can be serially solved on QPUs, the total QPU qubit overhead further reduces to $\max\{T,J\}$ as a constant.

\begin{remark}
    Alternatively, the binary variables can be partitioned into $T$ blocks, resulting in qubits number $\max\{N,J\}$. As $T$ is conventionally fixed as 24 in power system operation, our setting aligns with the worst possible qubits number when $N\gg T$.
\end{remark}

To sum up, the overall saving on qubits number is summarized in Table \ref{tab:summary}, which is independent of the number of generators and the number of BD iterations. 

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{Summary on the qubit number and computational paradigm of the algorithm under BD framework.}
    \begin{tabular}{ccc}
        \toprule[1.2pt]
        \hline
        \textbf{Method} & \textbf{\makecell{Theoretical Qubit \\Number}}& \textbf{\makecell{Computation \\Paradigm}} \\\hline
        Basic QA & $NT+J+k\mathcal{F}$& Quantum\\
        % QADMM & $NT$& Quantum + Classcial \\
        QPHR-ALM & $NT+J$& Quantum\\
        QPHR-ADMM & $\max\{T, J\}$& Quantum\\
        \bottomrule[1pt]
    \end{tabular}
    \label{tab:summary}
    \vspace{-0.8em}
\end{table}

\subsection{Framework of BD and QPHR-ADMM for SUC}\label{section:III D}

 Our proposed QPHR-ADMM algorithm, powered by PHR Augmented Lagrangian and quantum ADMM, is summarized in Fig. \ref{fig_2}. The algorithm begins by decoupling the SUC problem using BD. This decoupling serves two primary purposes. First, it ensures that all scenarios within the second stage are independent and continuous in the sub-problems, which can be efficiently solved by allocating dedicated CPU to each scenario in parallel. Second, the first-stage master problem is decomposed by ADMM, allowing for sequential processing of each block on a single quantum processor. Each block is then solved by the PHR Augmented Lagrangian method without occupying extra qubits for slack variables introduced by inequality constraints. 

The binary unit decisions from QPHR-ADMM iterations are then integrated into subproblems to optimize power outputs and update dual variables. The calculation of subproblems contributes to tightening the UB of BD. The process repeats until the BD bounds converge within a small tolerance. 

\begin{figure}[!t]
\centering
\includegraphics[width=3.3 in]{Figures/Framework.pdf}
\caption{The hybrid quantum-classical algorithm for SUC (The purple region is decoupled using the proposed QPHR-ADMM algorithm).}
\label{fig_2}
\vspace{-1em}
\end{figure}

\section{Simulations} \label{section:IV}

 This section validates the reliability of the algorithm through two optimization simulation cases based on QPHR-ADMM, and then verifies its feasibility and effectiveness for SUC.

\subsection{Binary Integer Programming Example}

QPHR-ADMM obviates the need for continuous slack variables when handling integer inequality constraints, thereby enabling the optimization problem to achieve optimal solutions entirely within the QPU, without recourse to classical computation. The correctness of the algorithm will be validated by the following example. 
\begin{subequations}\label{eq:19}
\setlength\abovedisplayskip{1pt}%shrink space
\setlength\belowdisplayskip{5pt}
\begin{align}
\min_{\boldsymbol{x}\in\{0,1\}^6} &6 x_{1}+3 x_{2}-5 x_{3}-6 x_{4}+4 x_{5}-7 x_{6} \label{eq:19a}\\
\text { s.t. }&-2 x_{2}-2 x_{5}-x_{6}+3 \leq 0 \label{eq:19b}\\
&-x_{1}+x_{3}-x_{4}+2 x_{6} \leq 0 \label{eq:19c}\\
&-x_{1}+x_{3}+x_{4} \leq 0 \label{eq:19d}
\end{align}
\end{subequations}

In this case study, the expression of the objective function follows the structure of the master problem discussed in Section \ref{section:III B}, specifically focusing on integer programming without quadratic terms. Furthermore, all constraints are linear inequality constraints with binary variables. Because there are $6$ binary variables, the qubit string can be denoted as $\left | x_1 x_2 x_3 x_4 x_5 x_6  \right \rangle$. 

Theoretically, basic QA requires 6 qubits to solve the problem. However, as there is no continuous variable, each inequality constraint will introduce one slack variable that needs to be encoded by $\mathcal{F}$ binary variables. For the proposed QPHR-ADMM, the default parameters is set as $\eta=1.05$ and $\delta=0.01$, and the entire problem is divided into three ADMM blocks, each containing $2$ qubits. Each block will be iteratively updated in sequence, and upon completion of this process, the coefficients will be updated. The QPU solver, \textit{D-Wave Advantage\_system 4.1}, will independently execute the QA evolution process. In addition, we use the Gurobi solver on a classical computer with an  \textit{Intel Core i7-12700H} processor to obtain the correct target solutions as references.

To test the effectiveness of QPHR-ADMM, three cases are considered: \textit{unconstrained}, \textit{single inequality constraint}, and \textit{multiple inequality constraints} optimization, which can mimic the progressive introduction of Benders cuts. The test results are listed in Table \ref{tab1} and Fig. \ref{fig_3} presents the influence of parameters on algorithmic convergence under the activation of two constraints, (\ref{eq:19b}) and (\ref{eq:19c}). 

First, when no inequality constraints are added, the problem is already in QUBO form. Using a quantum annealer, the correct qubit string solution $\left | 001101  \right \rangle$ can be obtained through a single complete QPHR-ADMM iteration. Second, by incorporating the constraint (\ref{eq:19b}) into the original QUBO formulation, the previously obtained solution will no longer satisfy this constraint. Introducing slack variables would require additional qubits for discretization or the involvement of a classical computer to complete the computation. However, utilizing QPHR-ADMM allows us to solve the problem using only $2$ qubits, with the quantum result consistent with those produced by classical computation.

In addition, the multiple inequality constraints optimization emulates the master problem after several iterations of BD, as each iteration of BD creates a new inequality constraint. The results presented in Table \ref{tab1} demonstrate that QPHR-ADMM can achieve highly precise solutions for these types of problems. Notably, the ADMM algorithm can reduce the constant qubit occupation from $6$ to $2$, and unlike the basic QA, the qubit overhead will not increase as the number of constraints increases. This capability underscores its potential to effectively address the master problem of SUC.

At last, Fig. \ref{fig_3} confirms the influence of parameters on QPHR-ADMM convergence, as outlined in Section \ref{section:III C}. Smaller penalty coefficients and step size factors ensure stable convergence, while increasing these parameters significantly reduces iteration count, accelerating convergence. For instance, with $\sigma^0 = 0.8$  and $\eta = 1.14$, only $5$ iterations are needed. However, overly large coefficients may result in non-convergence. For example, when $\sigma^0 > 0.6$  and $\eta > 1.12$, convergence deteriorates sharply, with most cases failing to converge. Therefore, parameter selection requires a trade-off to convergence and speed.

\begin{figure}[!t]
\centering
\includegraphics[width=2.7in]{Figures/Figure_iter.pdf}
\vspace{-0.5em}
\caption{The impact of different coefficients on the number of iterations and convergence (The gray area indicates where the results either fail to converge or converge to incorrect values).}
\label{fig_3}
\vspace{-0.2em}
\end{figure}

\begin{table}
\setlength\tabcolsep{3.5pt}
\begin{center}
\caption{The Optimization Results of \eqref{eq:19} Based on QPHR-ADMM.}
\label{tab1}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{cccccc|c}
\toprule[1.2pt]
\hline
 Constraints& \makecell{Target \\Solutions} & $\sigma^0$ & \makecell{Quantum \\Results} & Iter. & \makecell{Qubit\\ QPHR\\ADMM} & \makecell{Qubit\\Basic\\ QA}  \\ \hline
 No Constraint &  $001101$ & 0.3 & $\left | 001101  \right \rangle$ & 1 & 2 & 6 \\
 (\ref{eq:19b})&  $011101$ & 0.3 & $\left | 011101  \right \rangle$ & 3 & 2 & 6+$\mathcal{F}$\\
 (\ref{eq:19b}) \& (\ref{eq:19c}) &  $011110$ & 0.5 & $\left | 011110  \right \rangle$ & 19 & 2 & 6+$2\mathcal{F}$\\
 (\ref{eq:19b}), (\ref{eq:19c}) \& (\ref{eq:19d})& $110101$ & 0.5 & $\left | 110101  \right \rangle$& 4 & 2 & 6+$3\mathcal{F}$\\ 
\bottomrule[1pt]
\end{tabular}
\end{center}
\vspace{-1.5em}
\end{table}

\subsection{Scenario-based SUC Example}

This section explores the feasibility of the QPHR-ADMM method in solving SUC. The power system consists of four generators, an equivalent wind generator, and a load. The output of the wind turbine is influenced by wind speed over a 24-hour period, assuming the wind speed follows a Weibull probability distribution. Additionally, the load demand is uncertain and is assumed to follow a Beta distribution\cite{Maneesh2020AnOptimalMVCNL}. 

\subsubsection{Theoretical Analysis on Qubit Overhead}

Considering that the time horizon of the SUC is $24$ hours, the master problem after BD involves $96$ decision variables for the total commitment statuses. Additionally, $J=12$ and $\mathcal{F}=13$ are set to allocate qubits for discretizing the LB and slack variables, with the corresponding precision coefficients specified as $0.004$ and $0.005$, respectively. Therefore, a total of $108 + 13k$ qubits are required in the basic QA where $k$ is the number of Bender's iterations, which is a substantial computational burden for current QPUs. Then, since no binary-encoded slack variables are introduced, the number of qubits required for QPHR-ALM can be determined as $108$. In contrast, based on the QPHR-ADMM algorithm, we decompose the master problem into five unit-based blocks. The first four blocks contain the on$/$off decision variables for each generator over the $24$-hour period, while the last block is used to compute the discretized LB value. As a result, these five blocks require $24$, $24$, $24$, $24$, and $12$ qubits, respectively, for computation. This implies that the QPHR-ADMM algorithm, based on the D-ADMM framework, requires only a QPU with a capacity of $24$ qubits to finish tasks that would otherwise demand $108 + 13k$ qubits. 

Fig. \ref{fig_4} illustrates how the qubit overhead of the three algorithms in this SUC case study varies with the number of BD iterations. The results indicate that as the number of iterations increases, QPHR-ADMM method demonstrates an increasingly significant advantage in qubit efficiency compared to basic QA.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5 in]{Figures/Figure_qubits.pdf}
\vspace{-1.7em}
\caption{Qubit overhead of different algorithms with increasing iterations under the BD framwork.}
\label{fig_4}
\vspace{-0.5em}
\end{figure}

\subsubsection{Convergence and Sensitivity Analysis}

To start, when we attempt to use basic QA to directly handle the large-scale first Benders iteration, the measured results often fail to converge to the correct ground-state solution, leading to erroneous LB values as purple curves showing in Fig. \ref{fig_5}. This failure is primarily attributed to chain break issues \cite{Elijah2023ComparingTG} in current D-Wave QA-QPUs. These issues arise from the excessive qubit requirements, which sometimes result in some longer chains in embedding. This weakens the chain strength of some qubits, ultimately causing broken chains. Thus, adopting QPHR-ADMM not only reduces qubit overhead for each annealing process, but is also crucial to ensure result accuracy even when the qubit number is efficient.

Table \ref{tab3} illustrates the convergence of upper and lower bounds in the BD method under different numbers of scenarios. The UB results are obtained by aggregating all the subproblems, which are processed exclusively by classical computers. The LB can be computed using either QPUs or CPUs. Using quantum methods, BD convergence was ensured after $2$ iterations for a sample of $10$ and $100$ scenarios, respectively. Note that there exits slight discrepancy between the LBs computed by Gurobi and the proposed QPHR-ADMM, due to the precision level of the discretized LB being set at $0.004$. 

Fig. \ref{fig_5} shows that the trends in the LB values and the corresponding errors obtained using the QPHR-ADMM method and Basic QA during the first iteration of BD under $100$ sampled scenarios. QPHR-ADMM demonstrates satisfactory convergence performance. For example, as shown in Fig. \ref{fig_5}, the LB value gradually narrows down to the optimal value, and the stopping criteria error eventually approaches zero during the QPHR-ADMM iterations. It indicates that the master problem will converge to the correct solution. Additionally, This figure also demonstrates the impact of the initial penalty parameters on convergence performance. For instance, a larger $\sigma^0$ can reduce iterations, thereby saving computational time. However, due to the uncertain convergence of QPHR-ADMM, setting a smaller $\sigma^0$ is necessary to ensure more reliable convergence, as discussed before.

\begin{table}

\setlength\tabcolsep{3pt} 
\begin{center}
\caption{The BD Results for SUC Problems.}
\label{tab3}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{cccccccc}
\toprule[1.2pt]
\hline
 & \multicolumn{3}{c}{$10$ scenarios} &  & \multicolumn{3}{c}{$100$ scenarios} \\ \cline{2-4} \cline{6-8} 
 \makecell{Iteration \\No.}&  \makecell{UB\\(Gurobi)}  &   \makecell{LB\\(Gurobi)}    &   \makecell{LB\\(Dwave)}   &  &    \makecell{UB\\(Gurobi)}   &  \makecell{LB\\(Gurobi)}     &   \makecell{LB\\(Dwave)}   \\ \hline
 $0$&    $27.856$   &    $0$   &    $0$  &  &    $27.644$   &    $0$   &    $0$  \\
 $1$&    $11.501$   &   $11.056$    &   $11.056$   &  &    $11.290$   &   $10.844$    &   $10.844$   \\
 $2$&    $11.501$   &   $11.501$    &   $11.500$   &  &   $11.290$    &    $11.290$   &   $11.288$   \\

\bottomrule[1pt]
\end{tabular}
\end{center}
\vspace{-1.5em}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5 in]{Figures/Figure_convergence.pdf}
\vspace{-1.7em}
\caption{Convergence of the error (top) and lower bounds (bottom) in the first BD iteration under different values of $\sigma^0$. }
\label{fig_5}
\vspace{-0.5em}
\end{figure}

\subsubsection{Promise on the Future Quantum Technique}

In the context of QC, its primary potential advantage lies in accelerating computations for power system scheduling. Although current quantum computers are incapable of solving real-scale problems, their potential to handle large-scale problems needs serious consideration. Fig. \ref{fig_6} demonstrates the time required by D-Wave QA processors to solve QUBO problems of varying sizes, focusing solely on direct QPU access time while disregarding queue wait times before and after QPU access. For smaller-scale QUBO problems, classical computers exhibit a clear advantage in computational speed over quantum processors. However, as the problem size increases, the computational time for classical methods escalates significantly due to the NP-hard nature of integer programming. In contrast, the time complexity for quantum computation does not vary a lot. Once the utilization of roughly $100$ equivalent qubits, QC begins to surpass classical methods in efficiency. Nevertheless, due to the risk of chain broken during the embedding process on D-Wave QPUs for larger problems, it is advisable to keep the block size within QPHR-ADMM relatively small.


\begin{figure}[!t]
\centering
\includegraphics[width=3.5 in]{Figures/Figure_time.pdf}
\vspace{-1.7em}
\caption{Average time cost for different size of QUBO problems under a single computation.}
\label{fig_6}
\vspace{-1em}
\end{figure}


\section{Conclusion}\label{section:V}

This paper explores the feasibility of solving scenario-based SUC using quantum computation. Within the BD framework, we propose a novel QPHR-ADMM algorithm that builds on QPHR-ALM and D-ADMM. This method enables the master problem to be independently accelerated by a quantum annealing processor, while the subproblems are solved in parallel on CPUs. Additionally, the proposed algorithm effectively mitigates quantum computing failures and significantly reduces qubit overhead, maintaining it as a constant rather than scaling linearly with the number of Benders iterations and quantum precision. By extending the versatility of QA to address a wider range of optimization problems, this study establishes a foundational framework for tackling more complex integer programming problems with inequality constraints in the future.


\bibliographystyle{IEEEtran}
\bibliography{Reference}


\newpage


\vfill

\end{document}


