\section{Related Work}
\paragraph{Long-context LLMs} The ability to process extensive contextual information is a crucial aspect of language models, with context length serving as a key determinant of their processing capacity. During the pre-training phase, methods to enhance long-context capabilities primarily involve increasing the training window through Adjusted Base Frequency (ABF) and then training with selected high-quality long-context data \citep{dubey2024llama,xiong-etal-2024-effective,chen2024long,lv2024longwanjuan}. In the post-training phase, there are still efforts dedicated to post-training data \citep{gao2024train,fu2024dataengineeringscalinglanguage,si2025gateauselectinginfluentialsamples,wang2024bootstrap,Chen2024WhatAT,longalign,wu2024long}. There are also efforts dedicated to making structural adjustments, such as modifying positional encoding \citep{Chen2023ExtendingCW,zhu2023pose,peng2023yarn,ding2024longrope,an2024training,an2024does} and attention mechanisms~\citep{an2024does,jin2024llm}, aiming to more efficiently enhance the model's ability to process long contexts. Accurately assessing a model's ability to process long contexts has also become increasingly important, and a series of comprehensive and complete evaluation schemes have subsequently been proposed \citep{hsieh2024ruler,bai2023longbench, bai2025longbenchv2,kuratov2024babilong,li2024long,zhu-etal-2024-longembed,levy2024same}. From the above, it is evident that data is always crucial. Below are related works on data.

\paragraph{Pre-training data} 
Training data that exhibits long-range dependency patterns is crucial for enhancing the model's ability to handle extended contextual information. For post-training data, numerous methodologies have been explored to generate synthetic long-context data \citep{wang2024bootstrap,Chen2024WhatAT,longalign,wu2024long}. Conversely, for pre-training data, the predominant approach involves the curation and selection of relevant text from existing corpora, which is exemplified by prominent models including Qwen \citep{bai2023qwen} and LLaMA \citep{touvron2023llama}. While scaling laws suggest that a model's capabilities improve with more data \citep{kaplan2020scaling}, large volumes of data bring about high resource demands. Therefore, optimizing data utilization more effectively should become a key area of research. ProLong \citep{chen2024long} proposes a framework for calculating \textbf{long-distance dependencies} of data at the sentence level. LongWanjuan \citep{lv2024longwanjuan} also designed metrics and filtered data based at the sentence level.
However, \citet{xiong-etal-2024-effective} assert that the key factor affecting the long-context ability of LLMs is the positional encoding's capacity to aggregate information from distant tokens. Our method focuses on token-level long-distance dependencies to select high-quality long-context data.
\begin{figure*}
  \includegraphics[width=1\linewidth]{figs/LongAttn.pdf}
  \caption{LongAttn Framework: After preprocessing the data, the long-distance dependency strength at the token-level is analyzed using the self-attention mechanism of an LLM. This analysis serves as the basis for filtering the data, which is then used for continual pre-training of a base model that initially lacks long-context capabilities, resulting in our LongAttn model}
  \label{fig:main}
\end{figure*}