\section{Related Work}
\paragraph{Long-context LLMs} The ability to process extensive contextual information is a crucial aspect of language models, with context length serving as a key determinant of their processing capacity. During the pre-training phase, methods to enhance long-context capabilities primarily involve increasing the training window through Adjusted Base Frequency (ABF) and then training with selected high-quality long-context data ____. In the post-training phase, there are still efforts dedicated to post-training data ____. There are also efforts dedicated to making structural adjustments, such as modifying positional encoding ____ and attention mechanisms____, aiming to more efficiently enhance the model's ability to process long contexts. Accurately assessing a model's ability to process long contexts has also become increasingly important, and a series of comprehensive and complete evaluation schemes have subsequently been proposed ____. From the above, it is evident that data is always crucial. Below are related works on data.

\paragraph{Pre-training data} 
Training data that exhibits long-range dependency patterns is crucial for enhancing the model's ability to handle extended contextual information. For post-training data, numerous methodologies have been explored to generate synthetic long-context data ____. Conversely, for pre-training data, the predominant approach involves the curation and selection of relevant text from existing corpora, which is exemplified by prominent models including Qwen ____ and LLaMA ____. While scaling laws suggest that a model's capabilities improve with more data ____, large volumes of data bring about high resource demands. Therefore, optimizing data utilization more effectively should become a key area of research. ProLong ____ proposes a framework for calculating \textbf{long-distance dependencies} of data at the sentence level. LongWanjuan ____ also designed metrics and filtered data based at the sentence level.
However, ____ assert that the key factor affecting the long-context ability of LLMs is the positional encoding's capacity to aggregate information from distant tokens. Our method focuses on token-level long-distance dependencies to select high-quality long-context data.
\begin{figure*}
  \includegraphics[width=1\linewidth]{figs/LongAttn.pdf}
  \caption{LongAttn Framework: After preprocessing the data, the long-distance dependency strength at the token-level is analyzed using the self-attention mechanism of an LLM. This analysis serves as the basis for filtering the data, which is then used for continual pre-training of a base model that initially lacks long-context capabilities, resulting in our LongAttn model}
  \label{fig:main}
\end{figure*}