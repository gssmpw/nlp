In this paper, we propose a comprehensive evaluation framework that incorporates multiple algorithms for evaluating consistency and accuracy, providing a holistic metric of how trustworthy an LLM is. In this paper, we measure LLM consistency in the context of cybersecurity applications.

The rest of the paper follows the formal model and definitions. However, the notations used may differ in order to provide more readability for the algorithms and discussion.

\subsection{Consistency}

To be trustworthy, an LLM has to return a similar answer every time it’s prompted with the same question, so different users don’t get different answers or explanations to answers when researching the same topic. Our consistency algorithm gives an LLM the same prompt $n$ times and evaluates the similarity between responses using multiple metrics such as Jaccard Index~\cite{article}, Cosine Similarity~\cite{cosine_similarity}, Sequence Matcher~\cite{python_doc}, and Levenshtein distance~\cite{levenshtein_distance}, all standardized to a scale of 0 to 100. 

The Consistency algorithm (Algorithm \ref{alg:consistency}) operates in three modes: low, medium, and high, where higher settings require progressively greater consistency in the metrics for the model to be considered consistent. For each question, the algorithm collects $k$ model responses and then calculates pairwise consistency scores using the four metrics for every possible pair of responses, including consecutive responses. If the metric score is higher than a certain threshold, that pair passes for that metric. Therefore, while consecutive comparisons are part of the pairwise evaluation, the algorithm ensures a comprehensive assessment by comparing all responses in the set. 

If the pair passes $x$ out of 4 consistency score metrics, it is considered to pass overall. If 80\% of pairs pass, the model is considered consistent for that question. If 80\% of questions pass, the model is considered consistent overall.

Instead of keeping the same percentages to pass each metric, we have implemented low, medium, and high settings to further bring out the differences between the models. Under these settings, the percentage required for a pair to "pass" a certain consistency metric changes. 

For the low threshold, Jaccard and Cosine have to be 70\%, and Sequence Matcher and Levenshtein have to be 20\%. For medium, Jaccard and Cosine have to be 80\%, and Sequence Matcher and Levenshtein have to be 40\%. For high, Jaccard and Cosine have to be 90\%, and Sequence Matcher and Levenshtein have to be 60\%. Sequence Matcher and Levenshtein Distance similarity take the order of characters into account as opposed to the two, so they tend to be more critical of responses that are roughly the same but worded differently. Due to this, their required percentages are significantly lower than the other two.

\begin{algorithm}[tb]
\caption{Consistency Analysis}
\label{alg:consistency}
\begin{algorithmic}[1]
\Statex \textbf{Input:} \textit{LLM} $L_i$ -  LLM to perform consistency analysis
\Statex \textbf{Input:} \textit{Prompts/Queries} -  list of queries to be validated
\Statex \textbf{Input:} \textit{k} - The number of repetitions for validation
\Statex \textbf{Input:} \textit{simthreshold} - The threshold checked to determine similarity: low, medium, high
\Statex \textbf{Input:} \textit{qthreshold} - The minimum fraction of questions for which the LLM's answers need to be consistent
\Statex
\Statex \textbf{Output:} True/False
\Statex
\Procedure{Consistency\_Analysis}{\textit{LLM, Queries, k, simthresh, qthreshold}}
\State $qcnt \gets 0$
\State $npt \gets 0.8 * k * (k-1) / 2$
\For{each $q \in Queries$}
  \State $Resp \gets [~]$
  \For{$i \in 1 \dots k$}
    \State $Resp_i \gets LLM\_Api(q)$
  \EndFor
  \State $SS\_cnt,LS\_cnt,JS\_cnt,CS\_cnt \gets 0$
  \For{$i \in 1 \dots k-1$}
    \For{$j \in i+1 \dots k$}
        \State $SS \gets SeqMatcher(Resp_i,Resp_j)$
        \State $LS \gets LevenDist(Resp_i,Resp_j)$
        \State $JS \gets JaccardCoef(Resp_i,Resp_j)$
        \State $CS \gets CosineSim(Resp_i,Resp_j)$
        \State $SS\_cnt$ += $SS \ge simthresh$ ? 1 : 0
        \State $LS\_cnt$ += $LS \ge simthresh$ ? 1 : 0
        \State $JS\_cnt$ += $JS \ge simthresh$ ? 1 : 0
        \State $CS\_cnt$ += $CS \ge simthresh$ ? 1 : 0
    \EndFor
  \EndFor
  \If{$SS\_cnt,LS\_cnt,JS\_cnt,CS\_cnt \ge npt$}
    %\If{$SS\_cnt \ge npt$ \&\& $LS\_cnt \ge npt$ \&\& $JS\_cnt \ge npt$ \&\& $CS\_cnt \ge npt$}
    \State $qcnt \gets qcnt + 1$
  \EndIf
\EndFor
\If{$qcnt / |Queries| \ge qthreshold$}
  \State \Return{$true$}
\Else
  \State \Return{$false$}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsection{Agreement}
To be trustworthy an LLM has to return the correct answer to a question. To determine if the LLMs agree on whether a certain answer is correct or not, our framework uses two algorithms. The first is Self-Validation, where an LLM checks it's own answer to a question. The second is Cross-Validation, where an LLM's answer to a question is checked by every other LLM. An LLM must be considered accurate by both these algorithms to be considered trustworthy in terms of information accuracy.

\subsubsection{Self-Validation}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Self-Valid.png}
    \caption{Self-Validation Architecure}
    \label{fig:Self-Valid-Alg-Fig}
\end{figure}

Figure~\ref{fig:Self-Valid-Alg-Fig} illustrates the Self-Validation framework for Large Language Models (LLMs). In this process, the LLM generates a "Response List" with repeated responses to the same question. That same LLM is then asked whether the generated responses are the correct answer to the original query. If it agrees with enough of its own responses, it is considered factually consistent by self-validation.

The Self-Validation Algorithm (Algorithm \ref{Self Validation}) has an LLM to evaluate the accuracy of its answers as shown in \ref{fig:Self-Valid-Alg-Fig} . It prompts the LLM with its answer to a question and asks if it is the correct answer to that question. This is done $k$ times for every question. If the LLM responds "yes" 80\% of the time, that question is considered correct by this metric. If the number of correct questions divided by the total number of questions is greater than $qthreshold$, the LLM is considered accurate overall by this metric.


\begin{algorithm}[tb]
\caption{Self Validation}
\label{Self Validation}
\begin{algorithmic}[1]
\Statex \textbf{Input:} \textit{LLM} $L_i$ - The LLM to validate
\Statex \textbf{Input:} \textit{Queries} - The list of queries to be validated
\Statex \textbf{Input:} \textit{k} - The number of repetitions for validation
\Statex \textbf{Input:} \textit{qthreshold} - The minimum fraction of questions for which the LLM needs to accept its own answer to be considered successful
\Statex
\Statex \textbf{Output:} True/False
\Statex
\Procedure{Self\_Validation}{\textit{LLM, Queries, k, qthreshold}}
\State $qcnt \gets 0$
\For{each $q \in Queries$}
  \State $Orig\_Resp \gets LLM\_Query\_Api(q)$
  %\Statex \Comment{Get a yes or no response from the LLM for whether the answer it provides is correct?}
  \State $svq \gets $q + Orig\_Resp + ``correct? yes or no''
  \State $valcnt \gets 0$
  \For{$i \in 1 \dots k$}
    \State $Resp_i \gets LLM\_Api(svp)$
    \If{$Resp_i$ = Yes}
        \State $valcnt \gets valcnt + 1$
    \EndIf
  \EndFor
  \If{$valcnt > 0.8 * k$}
    \State $qcnt \gets qcnt + 1$
  \EndIf
\EndFor
\If{$qcnt / |Queries| \ge qthreshold$}
  \State \Return{$true$}
\Else
  \State \Return{$false$}
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Cross-Validation}

\ref{fig:Cross-Valid-Alg-Fig} illustrates the cross-validation framework for evaluating the consistency of a Large Language Model (LLM). This framework fact checks LLM responses with other LLMs. Each LLM generates a response to a prompt, and then every other LLM is asked whether that LLM's response to the original prompt is correct. If there is enough agreement between LLMs, that LLM is considered factually consistent by cross-validation

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{Cross-Valid.png}
    \caption{Cross-Validation Architecture}
    \label{fig:Cross-Valid-Alg-Fig}
\end{figure}

If an LLM has unreliable information caused by biased training data, it may not be able to recognize that in the self-validation step. To remedy that we propose the Cross-Validation Algorithm (Algorithm \ref{fig:Cross-Valid-Alg-Fig}), which cross-validates an LLM's responses with the other LLMs as shown in \ref{fig:cross-valid}. To begin with, the algorithm is provided with a list of one LLMs responses to a set of questions. For each response, all the other LLMs are asked whether it is the correct response to the respective question. If 80\% of the other LLMs' responses are yes to a question, that question is considered correct by this metric. If the number of correct questions divided by the total number of questions is greater than $qthreshold$, the LLM is considered factually consistent overall by this metric.

\begin{algorithm}[tb]
\caption{Cross Validation}
\label{Cross Validation}
\begin{algorithmic}[1]
\Statex \textbf{Input:} \textit{LLMs} $\mathcal{L}$ - The list of LLMs to cross-validate
\Statex \textbf{Input:} \textit{Queries} - The list of queries to be validated
\Statex \textbf{Input:} \textit{k} - The number of repetitions for validation
\Statex \textbf{Input:} \textit{qthreshold} - The minimum fraction of questions for which the other LLMs needs to accept any LLM's answer for it to be considered successful
\Statex
\Statex \textbf{Output:} cv\_llm - a boolean list; $cv\_llm_i$ indicates whether $llm_i$ is cross validated
\Statex
\Procedure{Cross\_Validation}{\textit{LLMs, Queries, k, qthreshold}}
\State $cv\_llm \gets \phi$
\For{each $llm_i \in LLMs$}
    \State $qcnt \gets 0$
    \For{each $q \in Queries$}
      \State $Orig\_Resp \gets llm_i\_Query\_Api(q)$
      %\Statex \Comment{Get a yes or no response from the LLM for whether the answer it provides is correct?}
      \State $svq \gets $q + Orig\_Resp + ``correct? yes or no''
        \State $llmcnt \gets 0$
        \For{each $llm_j \in LLMs$, s.t. $llm_j \neq llm_i$}
          \State $valcnt \gets 0$
          \For{$i \in 1 \dots k$}
            \State $Resp_i \gets LLM_j\_Api(svp)$
            \If{$Resp_i$ = Yes}
                \State $valcnt \gets valcnt + 1$
            \EndIf
          \EndFor
          \If{$valcnt > 0.8 * k$}
              \State $llmcnt \gets llmcnt + 1$
          \EndIf
        \EndFor
        \If{$llmcnt > 0.66 * |LLMs|$}
            \State $qcnt \gets qcnt + 1$
        \EndIf
    \EndFor
    \If{$qcnt / |Queries| \ge qthreshold$}
      \State $cv\_llm_i \gets$ {$true$}
    \Else
      \State $cv\_llm_i \gets$ {$false$}
    \EndIf
\EndFor
\State \Return {cv\_llm}
\EndProcedure
\end{algorithmic}
\end{algorithm}
