Over the last couple of years, the rapid development of Generative AI, and wide adoption of Large Language Models (LLMs) have revolutionized the field of natural language processing, and automated  generation of content in several languages, in  multi modal manner such as text, audio, and video. These machine learning and AI models, related AI agents~\cite{talebirad2023multi} are also being widely used for automation of tasks across the industry and several sectors. One such important sector is cybersecurity. LLMs have already been used to assist with cybersecurity processes (SecOps)~\cite{gennari2024considerations}, in security operation centers (SoC)~\cite{saha2024llm}, in security analysis of code and configurations~\cite{toth2024llms, minna2024analyzing}, in generation of secure code~\cite{vaidya2023critical,saha2024empowering}, in security and penetration testing~\cite{song2024poster} among many others. 

Trustworthiness of such LLMs and generative AI is a critical factor in whether and how we use it in cybersecurity as well as other real-world applications and systems. LLMs suffer from security issues~\cite{wu2024new,qachfar2024all}, and there are risks of code generation using LLMs~\cite{vaidya2023critical}. LLMs also hallucinate~\cite{xu2024hallucination} and such hallucinations often increase the risk of reduction of utility of the models, or even risk of failure during the task execution with LLMs as judges~\cite{zheng2024judging}, or autonomous LLM agents~\cite{talebirad2023multi}. Consistency has a strong relation to hallucination. 

In order to be able to trust the large language models, one key factor is consistency.  Studies of LLM consistency lead to the following questions:  (1) are they reliably consistent, in that can we rely on the responses - are they semantically consistent? And that leads to the following questions further: (2) What is consistency, and (3) how can consistency be evaluated in an automated manner. In this paper, we study these research questions and attempt to address them. Our focus is on semantic consistency of responses by one LLM or a set of LLMs. There are other types of consistency requirements such as: syntactic consistency, structural consistency, which are often in the context of copyrights, and are out of the scope of this paper. 

{\bf Our Contributions};
We have studied the problem of consistency of responses by LLMs especially in the context of cybersecurity. That is because, in such a domain, consistency of responses is quite important for making correct and effective decisions, otherwise inconsistency may lead to security vulnerabilities, weaknesses and/or liabilities and harm to systems, users and to enterprises. We have formally defined what is consistency of responses and then develop a framework for consistency evaluation. The paper proposes two approaches to validate consistency: self-validation, and validation across multiple LLMs. We have carried out extensive experiments for several LLMs such as ChatGPT 4o Mini, GPT3.5, Gemini, Cohere, and Llama3, on a security benchmark consisting of both informational and situational cybersecurity questions. Our experiments corroborate the fact that even though these LLMs are being considered and/or already being used for several cybersecurity tasks today, they are often inconsistent in their responses. Thus unless the consistency of LLMs in their responses is improved to a reliable level, they cannot be trusted to an extent that they can be used in enterprise-level cybersecurity operations. 

{\bf Organization of the paper}: Section~\ref{sec:def} defines consistency of LLM responses formally and analyzes its relationship with accuracy and hallucination.  Section~\ref{sec:framework} describes the framework of self-validation and cross-validation of consistency and the algorithms. In Section~\ref{sec:experiments} we have presented the experimental benchmark, settings and analyzed the experimental results. Section~\ref{sec:related} discusses the related work, and Section~\ref{sec:conclusions} concludes the paper with future work. 