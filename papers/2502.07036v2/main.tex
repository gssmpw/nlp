\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
\usepackage{fontawesome}
\usepackage{tikz}
\usepackage{comment}
\usepackage{centernot}

\usepackage{subfig,hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automated Consistency Analysis of LLMs\\
}

\author{\IEEEauthorblockN{Aditya Patwardhan}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Stony Brook University}\\
Stony Brook, USA \\
aapatwardhan@cs.stonybrook.edu}
\and
\IEEEauthorblockN{Vivek Vaidya}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{Rutgers University}\\
New Brunswick, USA \\
vivek.vaidya@rutgers.edu}
\and
\IEEEauthorblockN{Ashish Kundu}
\IEEEauthorblockA{\textit{Cisco Research} \\
San Jose, USA \\
ashkundu@cisco.com}
}

\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\begin{IEEEkeywords}
 Cybersecurity,  Generative AI. Large Language Models, Agents, Consistency, Trustworthiness, Validity, Reliability, Hallucination
\end{IEEEkeywords}

\section{Introduction}
\input{intro}


\section{Consistency in the context of LLMs} \label{sec:def}
\input{background-motivation.tex}

\section{Consistency Validation Framework} \label{sec:framework}

\input{validation}


\section{Empirical Analysis on LLMs using this Framework} \label{sec:experiments}
\subsection{Benchmarks}
To test the LLMs, we developed a benchmark on 40 Cybersecurity interview questions from a popular list of cybersecurity interview questions and answers~\cite{Hiremath_2024}. In the benchmark, the questions are divided into two types as follows.

\subsubsection{Information Questions} 
The first 33 questions are basic information questions, with a well-known correct answer. For example, ``what is cryptography", or ``what is the CIA triad in Cybersecurity"? These should theoretically be the easiest for LLMs to answer and check the answers of. They are listed in Table \ref{tbl:infq}.

\subsubsection{Situation Question}
The last 7 questions are Cybersecurity situation questions. They place the reader in a situation and ask what they should do. For example, "You receive an email from your bank telling you there is a problem with your account. The email provides instructions and a link so you can log into your account and fix the problem. What should you do?" These questions are more open to interpretation and should be harder for LLMs to answer and check. They are listed in Table \ref{tbl:sitq}.

\subsection{Consistency}
Gemini~\cite{geminiteam2024geminifamilyhighlycapable} and Bloom~\cite{workshop2023bloom176bparameteropenaccessmultilingual} are both deterministic models, and return the exact same response when the same prompt is passed. Therefore, they are perfectly consistent for all thresholds. Since their behavior is deterministic, the consistency check does not provide any useful information and they aren't included in the plots. 

Figures \ref{fig:consistency-analysis-low}-\ref{fig:consistency-analysis-high} depict the results for the low, medium, and high thresholds respectively for the 33 information questions. Figures \ref{fig:consistency-analysis-low-sit}-\ref{fig:consistency-analysis-high-sit} provide the corresponding results for the 7 situational questions. In each figure, there are 4 sets of bars corresponding to each LLM, providing the results of the consistency analysis for the cases where 1 or more, 2 or more, 3 or more, or all 4 of the consistency metrics evaluate to true.

Looking at the results for Low threshold with information questions (Figure \ref{fig:consistency-analysis-low}), the majority of LLMs seem to pass when 1-3 similarity metrics are sufficient to pass for a response pair to be considered consistent. When all 4 similarity metrics are needed, the percentage of questions that pass drops drastically, and not a single model is above 80\%. Comparing this with the situational questions (Figure \ref{fig:consistency-analysis-low-sit}), all the results are noticeably lower, as expected. As opposed to all LLMs passing at least once under the Low threshold, none pass under the High threshold. With the information questions (Figure \ref{fig:consistency-analysis-high}), GPT 4o mini performs the best once again, with GPT 3.5 not far behind. 3.5 and 4o are the only ones to have any questions pass when questions have to pass 3/4 metrics, and 4o mini is the only one to have any questions pass when all 4 metrics are used. The difference between the situation questions (Figure \ref{fig:consistency-analysis-high}) and the information questions is also greatest under the High threshold. Interestingly, GPT 3.5 outperforms 4o mini here, and it and Gemini are the only models to have any questions pass when only 1/4 of metrics are used.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{ConsistencyAnalysisLow.png}
    \caption{Consistency Analysis for Low threshold}
    \label{fig:consistency-analysis-low}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{ConsistencyAnalysisMed.png}
    \caption{Consistency Analysis for Medium threshold}
    \label{fig:consistency-analysis-med}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{ConsistencyAnalysisHigh.png}
    \caption{Consistency Analysis for High threshold}
    \label{fig:consistency-analysis-high}
\end{figure}

The Medium threshold has the most interesting results. With the information questions (Figure \ref{fig:consistency-analysis-med}), GPT 4o Mini and GPT 3.5 are the only models to pass when 2/4 metrics are used. With 1 metric, Gemini passes as well. With the situation questions (Figure \ref{fig:consistency-analysis-med-sit}) GPT 4o Mini was the only model to pass when only 1/4 metrics are used.



\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{ConsistencyAnalysisLowSit.png}
    \caption{Low threshold for Situational Questions}
    \label{fig:consistency-analysis-low-sit}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{ConsistencyAnalysisMedSit.png}
    \caption{for Medium threshold for Situational Questions}
    \label{fig:consistency-analysis-med-sit}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{ConsistencyAnalysisHighSit.png}
    \caption{High threshold for Situational Questions}
    \label{fig:consistency-analysis-high-sit}
\end{figure}



Not counting Bloom and Meta OPT~\cite{zhang2022optopenpretrainedtransformer} since they're inherently 100\% consistent, the most consistent models are GPT 4o Mini, GPT 3.5, and Google Gemini, in that order. When putting the average metric scores into tables, in both the regular questions (Table \ref{tab:sim_tab}) and the situation questions (Table \ref{tab:sim_tab}) GPT 4o Mini consistently scores higher in Sequence Matcher and Levenshtein Distance, while 3.5 consistently scores higher on Jaccard Index and Cosine similarity. Sequence and Levenshtein take order into account, so it can be inferred that 4o mini has more variation in the way it words its responses as compared to the older 3.5. Along with that, there is very little variation in the Cosine Similarity and to a lesser degree Jaccard Index scores. This increases a bit on the situation questions but is still considerably lower than the other two. This shows that these two metrics are less useful for comparing LLMs to each other.





\begin{table}[tb]
    \centering
        \caption{Average Similarity Scores per LLM}
    \label{tab:sim_tab}
    \begin{tabular}{|p{1.5cm} | p{1.5cm} | p{1.5cm} | p{1.3cm} | p{1.1cm} |}
        \hline
         Model & Sequence Matcher &  Levenshtein Distance & Jaccard Index & Cosine Similarity \\
         \hline
         GPT 4o Mini	& 30.21 & 46.54 & 89.75 & 89.29 \\ 
         \hline
         GPT 3.5 & 30.82 & 50.56 & 89.49 & 84.63 \\
         \hline
         Gemini & 10.22 & 32.5 & 86.62 & 82.1 \\
         \hline
         Cohere & 13.33 & 33.35 & 79.45 & 81.03 \\
         \hline
         Llama3 & 14.2 & 33.88 & 84.97 & 81.24 \\
         \hline
    \end{tabular}

\end{table}

\begin{table}[tb]
    \centering
        \caption{Average Similarity Scores per LLM Situation Questions}
    \label{tab:sim_tab_sit}
    \begin{tabular}{|p{1.5cm} | p{1.5cm} | p{1.5cm} | p{1.3cm} | p{1.1cm} |}
        \hline
         Model & Sequence Matcher &  Levenshtein Distance & Jaccard Index & Cosine Similarity \\
         \hline
         GPT 4o Mini & 23.31 & 43.62 & 86.79 & 81.84 \\
         \hline
         GPT 3.5 & 33.02 & 46.68 & 84.22 & 81.51 \\
         \hline
         Gemini & 12.31 & 34.24 & 83.9 & 79.69 \\
         \hline
         Cohere & 10.6 & 31.9 & 73.87 & 72.57 \\
         \hline
         Llama3 & 13.86 & 32.09 & 80.63 & 73.39 \\
         \hline
    \end{tabular}

\end{table}

\begin{table}[!h]
\caption{Difference between the average Similarity Scores for Information vs Situation Questions}
    \label{tab:sim_tab_diff}
    \begin{tabular}{|p{1.5cm} | p{1.5cm} | p{1.5cm} | p{1.3cm} | p{1.1cm} |}
\hline
Model           & Sequence Matcher & Levenshtein Distance & Jaccard Index & Cosine Similarity \\ \hline
GPT 4o Mini & 6.9              & 2.92                 & 2.96          & 7.45              \\ \hline
GPT 3.5     & -2.2             & 3.88                 & 5.27          & 3.12              \\ \hline
Gemini   & -2.09            & -1.74                & 2.72          & 2.41              \\ \hline
Cohere          & 2.73             & 1.45                 & 5.58          & 8.46              \\ \hline
Llama3          & 0.34             & 1.79                 & 4.34          & 7.85              \\ \hline
\end{tabular}
\end{table}



\subsubsection{How Types of Questions affect LLM Consistency}
When looking at the raw similarity scores for each question for each LLM, we noticed some patterns. The question "What are the response codes that can be received from a Web Application?" yielded the highest similarity scores for 3 out of 4 metrics on GPT 4o Mini, 1 out of 4 metrics on GPT 3.5, and 3 out of 4 metrics on Gemini. Interestingly, this same question yielded the lowest similarity score on 2 out of 4 of the metrics for Cohere, specifically Sequence Matcher and Levenshtein Distance, the two metrics that take order into account. On the other two metrics, that question scored fairly high for Cohere. It scored a little less than Cohere's highest recorded Jaccard index and in the middle of its highest and lowest recorded Cosine similarity scores. Looking at the responses themselves, they provide the same explanations, but with varying degrees of elaboration. This shows Cohere is not factually inconsistent but still inconsistent in its responses. Overall, there has been very little variation in Cosine and Jaccard scores, which shows relative factual consistency, but a lot of variation in Sequence Matcher and Levenshtein scores which shows that all the LLMs studied display this to some degree.

If we use Sequence Matcher and Levenshtein Distance scores to represent consistency in explanations and Jaccard Index and Cosine Similarity scores to represent informational consistency, looking at a table of the differences between the average Similarity Scores
for Information vs Situation Questions (Table \ref{tab:sim_tab_diff}), informational consistency drops much more for the situation questions than consistency in explanation. In some cases, the average Sequence Matcher and Levenshtein Distance scores are even higher for situational questions, most notably with Gemini, where they are both higher. This shows that LLMs are less factually accurate when given more open to interpretation situational questions, but are sometimes more consistent in explaining their responses to those questions.


\subsection{Agreement}
For agreement, Meta OPT~\cite{metaopt}'s responses had to be left out, because it refuses to give a yes or no answer on whether a response is correct or not, usually responding with "I think it's a good question", or just answering the question again, no matter what prompt we used. This means it cannot be used for Self-Validation, and cannot Cross-Validate the other models. So it isn't included in the Self-Validation plots, and on the Cross-Validation plots every other model only has 5 other LLMs to check its answer instead of 6, Meta OPT is not included.


\subsubsection{Self-Validation}
For Self-Validation, whether it's informational (Figure \ref{fig:self-valid}) or situational questions (Figure \ref{fig:self-valid-sit}), most of the models agree with themselves more than 80\% of the time. Bloom is the only model that is critical of itself. Interestingly, it appears to agree with itself more on the situational questions than the informational questions.


\subsubsection{Cross-Validation}
For Cross-Validation, the plots are organized by the number of LLMs that agree with one LLM's response. The first bar depicts the case when 2 of the 5 LLMs agree with a response, and the last bar depicts the case when all 6 agree. The last bar is only applicable for Meta OPT since Meta OPT itself cannot be used to cross-validate the other models, they all have 5 models checking their responses, while Meta OPT has all 6.


Interestingly, despite being the only 100\% consistent models, Meta OPT and Bloom score the lowest in both the informational questions (Figure \ref{fig:cross-valid}) and the situational questions (Figure \ref{fig:cross-valid-sit}). They actually score higher on the situational questions than the informational ones. Along with Gemini, they're the only models to behave this way.


\subsubsection{Model Hallucination and Validation} \label{sec:hallucination}
When asked to "Explain CIA triad in Cybersecurity", Meta OPT provided an incorrect answer, talking about the Central Intelligence Agency instead of the CIA triad. In the Cross-Validation step, 4 of the 6 LLMs responded that the answer was incorrect, which marks the overall question as incorrect by our parameters. While that showcases the strengths of Cross-Validation with other LLMs, two LLMs still got the validation wrong, those being GPT 3.5 and Cohere. More interestingly, Cohere answered "Yes, you explained the CIA triad, which stands for confidentiality, integrity, and availability, in cybersecurity", generating the correct answer itself but wrongfully believing it was provided in the prompt. Future work on refining the effectiveness of the Cross-Validation algorithm could be on marking the LLMs which are most accurate when validating answers and removing those that are the least. 

Detecting a hallucination is more difficult with situational questions. When prompted with "A friend of yours sends an e-card to your mail. You have to click on the attachment to get the card. What do you do? Justify your answer", Meta OPT simply responded with "I have to click on the attachment to get the card", where the correct answer would be to confirm that it is not malicious first. In the cross-validation step, only GPT 4o Mini identified this as the incorrect answer. Despite Gemini, GPT 3.5, Cohere and Llama3 all giving the correct answer when directly responding to the question, they still believed the incorrect answer was correct despite correctly identifying the security risks in their own responses. This shows the weakness in using LLM agreement to double-check responses for more abstract questions, as an LLM's response to a situation can be a possible course of action, but not necessarily a correct one.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=\linewidth]{SelfValidation.png}
    \caption{Self Validation for Information Questions}
    \label{fig:self-valid}
\end{figure}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=\linewidth]{SelfValidationSit.png}
    \caption{Self Validation for Situational Questions}
    \label{fig:self-valid-sit}
\end{figure}
\begin{figure}[!tb]
    \centering
    \includegraphics[width=\linewidth]{CrossValidation.png}
    \caption{Cross Validation for Information Questions}
    \label{fig:cross-valid}
\end{figure}


\begin{figure}[!tb]
    \centering
    \includegraphics[width=\linewidth]{CrossValidationSit.png}
    \caption{Cross Validation for Situational Questions}
    \label{fig:cross-valid-sit}
\end{figure}

\begin{comment}
    
\end{comment}


\section{Related Work} \label{sec:related}
\input{related}

\section{Conclusions and Future Work} \label{sec:conclusions}
In this paper, we ask the following question -- "how consistent are LLM responses, both in the information provided and factually" especially in the context of their use in cybersecurity. LLMs are expected to be used for various security operations in the industry~\cite{gennari2024considerations}. Before we put significant reliance on the black-box LLMs (which the industry has already started), can we evaluate such models on how consistent they are in their responses, and can security stakeholders such as CISOs make decisions on whether and how to use which LLM for tasks as important as cybersecurity operations?

We have carried out an extensive set of experiments and analyzed the consistency of LLMs for their responses against a benchmark of cybersecurity questions. Our experiments demonstrate that LLMs have made significant strides in improving consistency and reducing hallucination in the past couple of years, as newer models like GPT 4o Mini and Meta Llama3 outperform older ones like Meta OPT and Bloom. Despite that, LLMs still have quite a way to go before they become usable for important cybersecurity operations. When confronted with more abstract situational questions, there is a clear drop in consistency and agreement between LLMs. While our self and cross-validation algorithms have been effective at detecting LLM hallucination, they become less reliable the more abstract a question is. In the future, we plan to conduct a more supervised analysis of how accurate LLMs are to specifically select the ones with the best track record for response validation.

We plan to explore the relations between consistency and hallucination in detail, as well as carry out further experiments on classifying LLMs to different cybersecurity tasks. Further understanding of how inconsistent are the responses and how they are generated based on an analysis of the internal states of the models and attention layers may help us fine-tune the models better. 


\bibliographystyle{IEEEtranS}
\bibliography{references}
\vspace{12pt}

\newpage

\section*{Appendix}

\begin{table}[!h]
\caption{List of Cybersecurity Information Questions}\label{tbl:infq}
\begin{tabular}{lp{3.2in}}
Q1:  & What is the difference between   VA(Vulnerability Assessment) and PT (Penetration Testing)? \\
Q2:  & What is a three-way handshake?                                                             \\
Q3:  & What are the response codes that can be received from a Web Application?                   \\
Q4:  & What is the difference between HIDS and NIDS?                                              \\
Q5:  & What are the steps to set up a firewall?                                                   \\
Q6:  & Explain SSL Encryption                                                                     \\
Q7:  & What steps will you take to secure a server?                                               \\
Q8:  & Explain Data Leakage                                                                       \\
Q9:  & What is Port Scanning?                                                                     \\
Q10: & What are the different layers of the OSI model?                                            \\
Q11: & What is a VPN?                                                                             \\
Q12: & What do you understand by Risk, Vulnerability \& Threat in a network?                      \\
Q13: & How can identity theft be prevented?                                                       \\
Q14: & What are black hat, white hat, and grey hat hackers?                                        \\ Q15: & How often should you perform Patch management?                                             \\
Q16: & How would you reset a password-protected BIOS configuration?                               \\
Q17: & What is an ARP and how does it work?                                                       \\
Q18: & What is port blocking within LAN?                                                          \\
Q19: & What are salted hashes?                                                                    \\
Q20: & Explain SSL and TLS                                                                        \\
Q21: & What is 2FA and how can it be implemented for public websites?                             \\
Q22: & What is Cognitive Cybersecurity?                                                           \\
Q23: & What is the difference between VPN and VLAN?                                               \\
Q24: & What is cryptography?                                                                      \\
Q25: & What is the difference between VPN and VLAN?                                               \\
Q26: & What is the difference between Symmetric and Asymmetric encryption?                        \\
Q27: & What is the difference between IDS and IPS?                                                \\
Q28: & Explain the CIA triad in cybersecurity.                                                        \\
Q29: & How is Encryption different from Hashing?                                                  \\
Q30: & What is a Firewall and why is it used?                                                     \\
Q31: & What are some of the common Cyberattacks?                                                  \\
Q32: & What protocols fall under the TCP/IP internet layer?                                           \\
Q33: & What is a Botnet?                                                                         
\end{tabular}

\vspace{0.3cm}
\caption{List of Cybersecurity Situation Questions}\label{tbl:sitq}

\begin{tabular}{lp{3.2in}}
Q1: & A friend of yours sends an e-card to your mail. You have to click on the attachment to get the card. What do you do?   Justify your answer                                                                                                                                                                                                                                                  \\
Q2: & In our computing labs, print billing is often tied to the user’s login.   Sometimes people call to complain about bills for printing they never did only to find out that the bills are, indeed, correct. What do you infer from this situation? Justify                                                                                                                                  \\
Q3: & Which of the following passwords meets UCSC’s password requirements?   a).@\#\$)*\&\textasciicircum{}\% b).akHGksmLN c).UcSc4Evr! d).Password1                                                                                                                                                                                                                                                \\
Q4: & You receive an email from your bank telling you there is a problem with your account. The email provides instructions and a link so you can log into your account and fix the problem. What should you do?                                                                                                                                                                                \\
Q5: & A while back, the IT folks got a number of complaints that one of our campus computers was sending out Viagra spam. They checked it out, and the reports were true: a hacker had installed a program on the computer that made it automatically send out tons of spam emails without the computer owner’s knowledge. How do you think the hacker got into the computer to set this up? \\
Q6: & There is this case that happened in my computer lab. A friend of mine used their Yahoo account at a computer lab on campus. She ensured that her account was not left open before she left the lab. Someone came after her and used the same browser to re-access her account. and they started sending emails from it. What do you think might be going on here?                     \\
Q7: & Two different offices on campus are working to straighten out an error in an employee’s bank account due to a direct deposit mistake. Office \#1 emails the correct account and deposit information to Office \#2, which promptly fixes the problem. The employee confirms with the bank that everything has,   indeed, been straightened out. What is wrong here?                     
\end{tabular}

\end{table}

\end{document}
