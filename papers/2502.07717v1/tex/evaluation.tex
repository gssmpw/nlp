\section{Evaluation Benchmarks}
\label{sec:evaluation}

We evaluate off-the-shelf LMs and versions further pre-trained with our strategies on benchmarks that require reasoning over negation.
We primarily evaluate on CondaQA~\cite{ravichander-etal-2022-condaqa} \hl{and provide results on eight other relevant benchmarks.}
All the corpora that we work with are in English.

\subsection{CondaQA}
\label{sec:condaqa}

CondaQA \cite{ravichander-etal-2022-condaqa} is a contrastive question-answering dataset.
It is designed to evaluate the ability of models to reason over negation.
The corpus consists of $14,182$ question-answer pairs. 
Each question is paired with a passage that contains the answer to the question.
Answers to questions are either Yes/No/Don't Know, a span in the question, or a span in the passage.
It was created by extracting $1,289$ passages from the English Wikipedia 
which contained negation phrases.
They started with a collection of negation cues 
\cite{Morante2011NegationCues, van-son-etal-2016-building} 
and expanded them. 
There are over 200 unique negation cues in CondaQA,
including 
single-word (e.g., not, never), 
\hl{affixal} (e.g., \emph{un}-lucky, \emph{in}-correct),
and multi-word negation cues (e.g., a lack of, instead of).
\hl{They also include} several parts of speech tags such as
verbs (e.g., refuse), 
nouns (e.g., absence), 
adjectives (e.g., uninterested), 
and adverbs (e.g., incorrectly).


\hl{The authors of CondaQA} instructed crowdworkers to make three types of edits to each passage:
\begin{compactitem}
    \item {Paraphrase}: Rewrite the negated sentence.
    \item {Scope}: Change the scope of the negation, i.e., the part of the sentence that is negated.
    \item {Affirmation}: Remove the negation from the sentence.
\end{compactitem}

The \emph{paraphrase} edit keeps the meaning unchanged but \hl{the} \emph{scope} and \emph{affirmation} edits change the meaning of the sentence.
The questions are written by crowdworkers to target the implication of the negated statement in the passage.
The questions are then answered for the original and edited passages. 
Appendix~\ref{sec:condaqaexample} shows examples.

Importantly, \citet{ravichander-etal-2022-condaqa} {demonstrated that CondaQA cannot be solved by models relying solely on questions, edit types, or cues. 
This ensures that the dataset is free from artifacts that models could exploit to solve the task.}


We evaluate our models on the CondaQA dataset using the accuracy and \emph{group consistency} metrics.
The term \emph{group} refers to the original passage and either all three or one of the edited passages.
\emph{Group consistency} measures the percentage of questions answered correctly for all the passages in a group.
Group consistency is arguably more important,
as being robust against negation means being able to answer the question correctly with all original and edited passages.

\begin{figure*}
    \centering
    \includegraphics[width=0.32\textwidth]{figs/nspp.pdf}
    \includegraphics[width=0.32\textwidth]{figs/nsp.pdf}
    \includegraphics[width=0.32\textwidth]{figs/joint.pdf}
    \caption{
        Trends in \hl{pre-training} transformers on NSPP, NSP, and both tasks jointly from left to right.
        Validation loss decreases as the model is trained on larger subsets of the corpus.
        We stop training when the validation loss plateaus.
        \label{fig:pretraining}
    }
\end{figure*}
\subsection{Other Corpora}
\label{sec:othercorpora}
\paragraph{NLI and NLU Corpora.}
\citet{hossain-etal-2020-analysis} and \citet{hossain-etal-2022-analysis} \hl{analyzed negation in existing NLI and NLU corpora.
They showed that there are few negation cues in these corpora,
and the cues are often \emph{unimportant} for the task.
That is, negation can be removed from the sentences without changing the label of the example.}

Furthermore,
they introduced three new negation benchmarks built from existing NLI corpora.
They worked with RTE~\cite{dagan2005pascal,BarHaim2006TheSP,giampiccolo-etal-2007-third,DBLP:conf/tac/BentivogliMDDG09},
SNLI~\cite{bowman-etal-2015-large}, and MNLI~\cite{williams-etal-2018-broad}.
\hl{To create their benchmarks, they randomly selected 500 text and hypothesis pairs from each corpus.
They then added negation cues to the main verb of the text and hypothesis sentences to create three new pairs from each original pair.
The new pairs were manually annotated using the labels of the original benchmarks.}
Appendix~\ref{sec:nlunlicorpora} contains more details and examples from the NLU and NLI corpora that we work with. 


\paragraph{LAMA and LAMA-Neg.}
LAMA probe~\cite{petroni-etal-2019-language} is a benchmark for evaluating the factual and commonsense knowledge of language models.
LAMA is composed of various datasets, namely
GoogleRE,\footnote{\footnotesize {code.google.com/archive/p/relation-extraction-corpus/}}~T-REx~\cite{elsahar-etal-2018-rex}, ConceptNet~\cite{speer-havasi-2012-representing}, and SQuAD~\cite{rajpurkar-etal-2016-squad}. 
The input to the model is a sentence with a masked token.
The goal is to correctly predict the masked token.
LAMA-Neg~\cite{kassner-schutze-2020-negated} is a variant of LAMA where negation cues are inserted into the sentences.
The model is supposed to predict any token other than the original token. 
Appendix~\ref{app:lama} provides more information about these corpora.