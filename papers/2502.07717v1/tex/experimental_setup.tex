\section{Experiments}

First, we further pre-train transformers on the tasks described in Section~\ref{sec:empowering}.
Then, evaluate the models on the benchmarks described in Section~\ref{sec:evaluation}.
    
\subsection{Pre-training LMs for Negation}
We use base and large versions of {BERT}~\cite{devlin-etal-2019-bert} and {RoBERTa}~\cite{liu2019roberta} as our baseline models.
We further pre-train the models on NSPP and NSP tasks individually and jointly.
Since \hl{{BERT} and {RoBERTa}} are already pre-trained on Wikipedia using masked language modeling, further masked language modeling during our pre-training is redundant.
We use Transformers ~\cite{wolf-etal-2020-transformers}
and PyTorch~\cite{paszke2019pytorch} libraries.
We use the Adam optimizer~\cite{kingma2017adam} with a learning rate of $1e-6$
and Cross-Entropy loss for both tasks. 
We use early stopping with a patience of 3 epochs on the validation set. 


For joint training on NSPP and NSP tasks, we use the same learning rate and optimizer. 
The output of the model's last layer is fed into two separate linear layers,
each predicting the label for its respective task. 
The total loss is computed as the sum of the individual task losses. 
The input to the model remains the same as in the NSP task, 
with two sentences concatenated using the special token [SEP].
When pre-training only on the NSPP task, the input is only the first sentence $S_1$.
However, 
the presence of the second sentence in the input when pre-training jointly does not make the task trivial.
Note that in the NSP task, we reverse the polarity of the second sentence half of the time.
Therefore, even if the model looks for negation cues in the second sentence,
it cannot rely on them to predict the polarity of the next sentence in the NSPP task.
\begin{table*}[t]
    \centering
    \input{tables/condaqaresults.tex}
    \caption{
        Results on CondaQA test set.
        The first group of rows shows the results from previous work.
        The second group of rows shows the results of \hl{further pre-training on our tasks}.
        An asterisk ($^{\ast}$) indicates a statistically significant improvement (McNemar's test \cite{mcnemar1947note}, $p < 0.05$)
        over \hl{off-the-shelf} version of LMs.
        Pre-training {BERT} and {RoBERTa} on any of our tasks statistically significantly outperforms \hl{off-the-shelf} LMs.
        {RoBERTa-large} pre-trained on any of the tasks achieves higher group consistency (and accuracy)
        than (1) using affirmative interpretations
        and (2) {UnifiedQA-v2-large}\hl{,} which has been pre-trained on $\approx$1M annotated question-answer pairs.
        \label{tab:condaqaresults}
    }
\end{table*}

\hl{Training} on the entire corpus
($\approx$12.8M sentence pairs for NSP and $\approx$6.4M for NSPP)
is computationally expensive and time-consuming. 
Instead, we train on smaller subsets of the corpus, 
gradually increasing their sizes as training progresses, 
until the validation loss plateaus.
Figure~\ref{fig:pretraining} \hl{plots losses as more pre-training data is used. }
We observe similar trends for {BERT} and {RoBERTa} models across both base and large configurations.
We stop pre-training after 1M sentence pairs, 
except for the large models trained on NSP and NSPP tasks, 
where we stop after 500K pairs. 
While the loss reduction on the NSPP task is not as substantial as on the NSP task, 
the model still demonstrates improved performance on downstream tasks.


\subsection{Downstream Tasks}
For the downstream tasks (other than LAMA that has no training set),
we further fine-tune the pre-trained models on the benchmarks described in Section~\ref{sec:evaluation}.
More details on the implementation and hyperparameters are provided in Appendix~\ref{app:hyperparameters}.
