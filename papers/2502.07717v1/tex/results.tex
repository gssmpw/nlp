\section{Results and Discussion}
\label{sec:results}

We first present the results on CondaQA and then on the \hl{eight} other corpora introduced in Section~\ref{sec:evaluation}.


\subsection{Results on CondaQA}
\label{sec:condaqaresults}

Table~\ref{tab:condaqaresults} presents the results with the CondaQA test set.
{BERT} and {RoBERTa} models further pre-trained on any of our tasks consistently achieve 1.8\%-9.3\% higher accuracy compared to the \hl{off-the-shelf} models. 
Importantly, \hl{all} improvements are statistically significant. 
\hl{Group consistency,
which according to} \citet{ravichander-etal-2022-condaqa} \hl{is a more reliable metric for evaluating the ability of models to reason over negation,
also improves for all models.}
The only exception is {BERT-base} pre-trained on NSPP or both tasks jointly where the difference is marginal (2.7\% and 2.9\% vs. 3.4\%).
Most notably, {RoBERTa-base} pre-trained on any of the tasks achieves between 15.6\% and 17.6\% group consistency compared to 2.4\% of off-the-shelf.

\begin{table*}[h!]
    \centering
    \input{tables/nliresults.tex}
    \caption{
        \hl{Results on the natural language inference tasks.
        We report accuracy on the development sets (dev) and the subset of the development sets that contain negation ($\text{dev}_{\text{neg}}$).
        Additionally, we evaluate the models on the new pairs created by }\citet{hossain-etal-2020-analysis}\hl{ that add negation to the main verb of the text and hypothesis sentences (Negated).}
        Further pre-training on NSP consistently outperforms the \hl{off-the-shelf} models on the new negated pairs (except for {RoBERTa-base} on SNLI and MNLI).
        It also outperforms the previous work other than {RoBERTa} on RTE.
        \label{tab:nliresults}
    }
\end{table*}
Recall that \hl{the} affirmation edit of the negated sentences is obtained by undoing negation (i.e., removing negation cues),
\hl{then the edited} sentence is affirmative.
\hl{We observe that the same improvements on accuracy and group consistency hold with the affirmation edits as well} (except for the {BERT-base} models).
This suggests that our proposed pre-training tasks enhance model robustness beyond just reasoning over negation.


{UnifiedQA-v2} \cite{khashabi2022unifiedqav2} models outperform \hl{off-the-shelf} {RoBERTa} models. 
However, they have been fine-tuned on $\approx$1M question-answering pairs. 
Our models, in contrast, have not been pre-trained on any question-answering data \hl{yet} outperform {UnifiedQA-v2} with\hl{ similar amount} of parameters.
All {RoBERTa-large} \hl{(355M)} models pre-trained on our tasks outperform the large (770M) {UnifiedQA-v2} model by 3.3\%-4.6\% in group consistency and 0.6\%-1.6\% in accuracy.

\begin{table*}[t]
    \centering
    \input{tables/nluresults.tex}
    \caption{
        Macro-averaged F1 scores on the validation sets of natural language understanding benchmarks.
        We report the performance on the original instances and the subset of instances that contain negation.
        All instances with negation in WiC and WSC are unimportant (i.e. can be answered without considering negation).
        The main takeaways are: (a) pre-training on NSP consistently improves the performance on instances with negation, and
        (b) pre-training on NSPP or joint pre-training is also either beneficial or does not substantially change the performance.
        \label{tab:nluresults}
        }
\end{table*}

Additionally,
we compare our models with the previous work by \citet{rezaei2024paraphrasing}.
They automatically generate affirmative interpretations of the negated sentences and add them to the input.
However, pre-training on any of our tasks is more effective than coupling the input with affirmative interpretations
(2.1\%--3.4\% in group consistency and 0.2\%--1.2\% improvement in accuracy). 
\hl{Importantly, }
our method bypasses the need to generate affirmative interpretations of the negated sentences.

\hl{
While pre-training on any of the tasks is beneficial,
we observe that pre-training on NSP consistently outperforms pre-training on NSPP.
Jointly training on both tasks improves the performance for the RoBERTa-large model by 0.3\% in accuracy and 0.8\% in group consistency.
However, it decreases the performance for the rest of the models. 
This is likely because the NSP task is easier to learn as it requires the model to predict the coherence of the two sentences in the input.
The NSPP task, on the other hand, is more challenging and requires the model to predict the polarity of a sentence solely based on the previous sentence.
We hypothesize that when trained jointly, the NSPP task introduces complexity that interferes with the model's ability to effectively learn tasks and reason over negation.
}

Additionally, 
we ran an ablation experiment where we only added or removed negation cues in the pre-training data.
Appendix~\ref{app:ablation} shows that reversing sentence polarity is
consistently more effective than only adding or removing negation cues.
Interestingly, 
pre-training on only one type of instances (add or remove negation) 
also statistically significantly outperforms the \hl{off-the-shelf} models for all models other than {BERT-large}.
We also observe that pre-training with the subset of the data that includes only adding negation
is more beneficial than pre-training with the subset of the data that includes only removing negation.


\subsection{Results on the Other Corpora}
\paragraph{NLI Corpora.}
Table~\ref{tab:nliresults} presents the results on RTE, SNLI, and MNLI.
Pre-training does not substantially change the accuracy on all the original instances. 
They are all within -1\% to +1.8\% of the \hl{off-the-shelf} models other than a 2.5\%-7.9\% improvement on RTE with {RoBERTa-base}.
This suggests that our models still perform as good as the \hl{off-the-shelf} models when negation is not present.
\hl{Interestingly, joint training on both tasks does not improve the performance over pre-training on NSP for any of the models other than RoBERTa-base.} \tbd{This sentence used to be in the last paragraph, now I moved it here.}

According to \citet{hossain-etal-2020-analysis}, 
the original \hl{development} sets contain too few negated instances to assess model performance on negation. 
Hence, we focus on their newly created negated pairs. 
See Appendix~\ref{app:nliresults} for detailed results on each pair type.

Further pre-training {BERT} models on any of the tasks consistently outperforms the \hl{off-the-shelf} models on the new negated pairs
(RTE: 5.1\%-25.5\%, SNLI: 1.9\%-6.1\%, MNLI: 0-1.6\%). 
Importantly, the models further pre-trained on NSP outperform the previous work of 
\citet{hosseini-etal-2021-understanding} and \citet{singh-etal-2023-nlms} 
by 0\%-4.2\% and 1\%-4.7\%, respectively.
{RoBERTa} shows limited improvements from further pre-training compared to {BERT} models. 
The base model benefits more from jointly pre-training, 
outperforming the \hl{off-the-shelf} model and the previous work. 
The large model, however, benefits more from pre-training on NSP but does not outperform the previous work on RTE.
Pre-training on NSPP consistently improves performance across all models, except for {RoBERTa-large}.
However, other than a 3.5\% improvement on SNLI with {BERT-base},
it is either less effective or within +1\% of the models pre-trained on NSP.

\paragraph{NLU Corpora.}

Table~\ref{tab:nluresults} presents the results on the validation sets of natural language understanding tasks. 
Following prior work, we report macro-averaged F1 scores on the validation sets as some test labels are not publicly available. 
The results demonstrate that further pre-training consistently improves performance on instances containing negation or, at worst, causes a negligible decline 
(a marginal 0.01\% decrease with {RoBERTa-base} on WSC.)
On average, pre-training yields a 3.11\% improvement across all tasks.
Notably, the most significant improvements are observed on WiC with base models
(achieving an average increase of 7.5\%) and on WSC with large models (performance improves by 6.0\% on average.) 
Importantly, all models pre-trained on NSP or NSPP outperform off-the-shelf versions on important instances in QNLI, 
with the only exceptions being {BERT-large} pre-trained on either NSPP or NSP. 


\paragraph{LAMA and LAMA-Neg.}
Table~\ref{tab:lama-neg} presents the results on LAMA-Neg.
It is important to note that 
\hl{achieving 0\% error rate on}
the LAMA-Neg task only requires a model to behave randomly in the presence of negation. 
Therefore, while low top-1 mean error rates can serve as a sanity check, 
they do not reliably indicate the ability of the models to reason over negation.
Nevertheless, 
our models outperform the \hl{off-the-shelf} models on LAMA-Neg, 
with error rates reduced by 0.09 to 19.96 points (6.19 on average). 
Table~\ref{tab:ori-lama-results} presents the mean precision at $k = 1$ on the original LAMA dataset. 
Crucially, except for {RoBERTa} models jointly pre-trained on both NSP and NSPP, 
all other models remain within $\pm$1.65\% of the vanilla models. 
Most notably, models pre-trained on NSP and NSPP consistently outperform the vanilla models on {SQuAD} by 0.33\%-1.32\%,
with the exception of {BERT-base} pre-trained on NSPP, which performs 0.32\% worse.
It is important to note that LAMA does not contain negated instances.
Therefore, improvements are not necessarily expected. 
However, the fact that the models remain within $\pm$1.65\% of the vanilla models, 
coupled with the substantial improvements on LAMA-neg (Table~\ref{tab:lama-neg}) 
and other corpora, 
demonstrates that the models achieve more robustness to negation while maintaining competitive performance on inputs without negation.
\begin{table}[t]
\centering
\input{tables/lama-neg.tex}
\caption{
    We report the mean top 1 error rate for negated LAMA queries. 
    The lower the error rate, the better the model.
    All our models outperform the \hl{off-the-shelf} models.
    \label{tab:lama-neg}
}
\end{table}