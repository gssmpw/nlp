\section{Conclusion}
\begin{table}
    \centering
    \input{tables/lama.tex}
    \caption{
      We report the mean precision at $k = 1$ on the original LAMA dataset.
      The higher the precision, the better the model.
      Other than {RoBERTa} models jointly pre-trained on both tasks, 
      all our models are within $\pm$1.65\% of the vanilla models. 
      \label{tab:ori-lama-results}
    }
  \end{table}
\hl{In this work,
we proposed a self-supervised method to make language models more robust against negation.
We introduced two tasks to further pre-train LMs:
(a) the novel task of Next Sentence Polarity Prediction (NSPP)
and (b) a variation of the Next Sentence Prediction (NSP) task that involves reversing the polarity of the second sentence instead of selecting a random sentence.
Pre-training data for these tasks can be generated from any text corpus,
and the tasks do not require any human annotations.
Our experiments showed that further pre-training BERT and RoBERTa models
on these tasks consistently improves their performance on a range of negation-related benchmarks,
particularly on the challenging CondaQA corpus.
Notably, while both NSPP and NSP tasks are beneficial,
the NSP task yields consistently stronger improvements than the challenging NSPP task.
Perhaps surprisingly, training on both tasks jointly does not provide further benefits.
Future research could explore additional model architectures and pre-training corpora beyond Wikipedia
or include more contextual information (more than two sentences) in the pre-training tasks.}