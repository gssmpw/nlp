\section{Introduction}
\label{sec:introduction}

Negation is a semantic phenomenon that alters an expression to convey the opposite meaning. 
It is a ubiquitous linguistic construct used in various forms across all languages.
Negation is present in approximately 25\% of English sentences \cite{hossain-etal-2020-analysis}.
Language Models (LMs) achieve remarkable performance in a wide range of natural language understanding tasks 
but have been shown to struggle when negation is involved 
\cite{ettinger-2020-bert,dobreva-keller-2021-investigating,hosseini-etal-2021-understanding,jang-etal-2022-beyond}.
\citet{truong-etal-2023-language} showed that Larger LMs 
such as GPT-3 
\cite{NEURIPS2020_1457c0d6}
and InstructGPT 
\cite{ouyang2022training} 
are also insensitive to the presence of negation and struggle to reason over it.
Although larger state-of-the-art models might be better at handling negation, 
it is still important to address this issue in smaller language models that are more computationally efficient and cheaper to deploy.

\begin{figure}
    \centering
    \input{tables/introexample.tex}
    \caption{
        An example of the training data for our self-supervised tasks.
        The tasks are: (a) given a sentence, predict whether the next sentence will contain negation~(NSPP)
        and (b) given two sentences, predict whether the second sentence is a coherent continuation of the first one~(NSP).
        \label{fig:introexample}
    }
\end{figure}

\citet{kassner-schutze-2020-negated} showed that {BERT}~\cite{devlin-etal-2019-bert}
often predicts the same token when negation is added to a sentence.
\hl{
For example, in the sentence \textit{A beagle is a type of [MASK]}, 
{BERT} predicts \textit{dog} as the masked token both in
the original sentence
and 
when the negation cue \textit{not} is added to the sentence,
\textit{A beagle is {not} a type of [MASK]}.}
We hypothesize that this behavior is due to the lack of negation modeling in pre-training.
Specifically, the model has not been exposed to instances where the addition (or removal) of negation \hlg{influences} meaning and coherence \hlg{within a discourse}.
We propose to further pre-train LMs on two novel tasks that involve negation. 
The first task is the Next Sentence Polarity Prediction (NSPP) task, 
where \hlg{given a sentence,} the model predicts whether the next sentence will contain negation. 
The second task is a variation of the well-known Next Sentence Prediction (NSP) task as introduced in {BERT}~\cite{devlin-etal-2019-bert}.
However, in our version, 
we \emph{reverse the polarity} of the second sentence \hlg{to form} negative samples rather than select random sentences from the document.
We define \emph{reversing polarity} as adding (or removing) negation to a sentence that contains (or does not contain) negation.
Figure~\ref{fig:introexample} shows \hlg{examples} of the self-supervised tasks.

The main contributions of this paper are:\footnote{All code and data available at \url{https://github.com/mhrezaei1/pretraining-negation} under the Apache 2.0 license.}
\begin{compactitem}
    \item Introducing \hl{two novel self-supervised tasks for pre-training LMs for negation.}
    \item Creating a large-scale dataset ($\approx$6.4M samples) for these tasks\hl{.}
    \item Showing that further pre-training {{BERT}}~\cite{devlin-etal-2019-bert} and {RoBERTa}~\cite{liu2019roberta} \hl{independently} on these tasks improves performance on CondaQA~\cite{ravichander-etal-2022-condaqa} and \hl{eight} other negation-related benchmarks. \hl{Joint pre-training on both tasks, however, does not always improve performance.}
\end{compactitem}

CondaQA, to the best of our knowledge, is the largest corpus that requires reasoning over negation. \hl{It consists of} $14,182$ question-answer pairs and over 200 unique negation cues.
Additionally, we also evaluate our models on three of the NLU corpora analyzed by~\citet{hossain-etal-2022-analysis} (QNLI~\cite{rajpurkar-etal-2016-squad},
WiC~\cite{pilehvar-camacho-collados-2019-wic},
and 
WSC~\cite{levesque_winograd_2012}),
and the new negation benchmarks introduced by
\citet{hossain-etal-2020-analysis}
for RTE~\cite{dagan2005pascal},
SNLI~\cite{bowman-etal-2015-large},
and MNLI~\cite{williams-etal-2018-broad}.
Finally, we evaluate on LAMA~\cite{petroni-etal-2019-language}
and LAMA-neg~\cite{kassner-schutze-2020-negated}.\tbd{No paper is being cited more than once here.}