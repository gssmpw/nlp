\section{Empowering Language Models Against Negation}
\label{sec:empowering}

We propose a self-supervised method to make LMs more robust against negation.
Our approach is to further pre-train LMs on two tasks that involve negation.
These tasks are the Next Sentence Polarity Prediction (NSPP) task 
and a variation of the well-known Next Sentence Prediction (NSP) task.
None of these tasks require labeled data\hl{;
any text corpora are suitable.}
Also, they are not specific to any domain or downstream task.



\subsection{Next Sentence Polarity Prediction (NSPP)}
\label{sec:nspp}
We introduce NSPP as the task of predicting the polarity of the next sentence given the current sentence.
\hlg{Given} a pair of consecutive sentences, $(S_1, S_2)$, 
the input to the model is only $S_1$, 
and the output is a binary label indicating whether $S_2$ \hlg{includes} any negation cues or not.
For example, consider the following pair of sentences:
\begin{compactitem}
  \item[] $S_1$: \hl{\textit{The weather report showed sunny skies.}}% \textit{``I like apples.''}
  \item[] $S_2$: \hl{\textit{But it didnâ€™t stay that way.}}% \textit{``I don't like oranges.''}
\end{compactitem}
Given only $S_1$,
the model should predict that the following sentence \hlg{includes} negation cues.



\subsection{Next Sentence Prediction (NSP)}
\label{sec:nsp}
NSP is a well-known task in LM pre-training as first introduced by BERT~\cite{devlin-etal-2019-bert}.
The NSP task is to predict whether two sentences are consecutive.
\citet{devlin-etal-2019-bert} (a) used consecutive sentences from Wikipedia as positive examples 
and (b) chose a random sentence from the same article to replace the second sentence \hl{and} create a negative example.

We propose a variation of the NSP task to improve negation understanding.
For a pair of consecutive sentences, $(S_1, S_2)$,
we create the negative pair $(S_1, S_2')$ 
where $S_2'$ is obtained by reversing the polarity of $S_2$. 
That is, if $S_2$ includes negation cues, we remove them, 
and vice versa. 