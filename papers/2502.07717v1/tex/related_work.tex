\section{Related Work}
\label{sec:related_work}

Negation understanding studies have focused on various aspects of negation, 
including negation detection~\cite{khandelwal-sawant-2020-negbert,truong-etal-2022-improving,morante-daelemans-2012-conandoyle},
scope detection~\cite{qian-etal-2016-speculation,jumelet-hupkes-2018-language,fancellu-etal-2017-detecting,morante-daelemans-2009-metalearning,li-lu-2018-learning,zhao-bethard-2020-berts}, and
focus detection~\cite{rosenberg-bergler-2012-uconcordia,shen-etal-2019-negative,zou-etal-2014-negation,zou-etal-2015-unsupervised}.

Recent studies have also focused on improving the performance of LMs when negation is present.
\citet{sarabi-etal-2019-corpus} introduced \emph{affirmative interpretations}, 
i.e., paraphrasing negated sentences without using negation cues.
\citet{hossain-blanco-2022-leveraging}
and 
\citet{rezaei2024paraphrasing}
showed that incorporating affirmative interpretations improves the performance of LMs
on negation-related benchmarks. 
In this paper, we show that our \hl{pre-training strategies} outperform affirmative interpretations on CondaQA. 
Unlike theirs, our \hl{pre-training} does not add any complexity at prediction time.


\citet{hosseini-etal-2021-understanding} propose to augment negation understanding with {BERT} by unlikelihood training and synthetic data generation.
\citet{singh-etal-2023-nlms} repeats the pre-training procedure of {BERT} (i.e., MLM and NSP) modified to improve negation understanding.
They add negation cues to the original next sentences rather than choosing a random sentence.
Unlike them, we do not always add negation; instead, we reverse the polarity of the second sentence for the NSP task.
That is, half the time we add negation to the next sentence and half the time we remove it.
We also introduce a novel task, 
Next Sentence Polarity Prediction (NSPP),
which is not explored by previous work.
As we shall see,
our pre-training outperforms previous work.