\section{Results on LAMA}
\label{app:lamaresults}

% LAMA - RoBERTa
\begin{table*}
    \centering
    \input{tables/lama.tex}
    \caption{
      We report the mean precision at $k = 1$ on the original LAMA dataset.
      The higher the precision, the better the model.
      Other than {RoBERTa} models jointly pre-trained on both tasks, 
      all our models are within $\pm$1.65\% of the vanilla models. 
      \label{tab:lama-roberta}
    }
  \end{table*}

Table~\ref{tab:lama-roberta} presents the mean precision at $k = 1$ on the original LAMA dataset. 
Except for {RoBERTa} models jointly pre-trained on both NSP and NSPP, 
all other models remain within $\pm$1.65\% of the vanilla models. 
Notably, models pre-trained on NSP and NSPP consistently outperform the vanilla models on {SQuAD} by 0.33\%-1.32\%,
with the sole exception of {BERT-base} pre-trained on NSPP, which performs 0.32\% worse.

On {GoogleRE}, the gains are less pronounced, with improvements ranging from 0.01\% to 0.65\%, 
except for {RoBERTa-base} pre-trained on NSPP, which performs 0.33\% worse. 
For ConceptNet and TREx, the models perform within $-1.65\%$ to $+0.21\%$ of the vanilla models.

It is important to note that LAMA does not contain negated instances, 
so improvements are not necessarily expected. 
However, the fact that the models remain within $\pm$1.65\% of the vanilla models, 
coupled with the substantial improvements on LAMA-neg (Table~\ref{tab:lama-neg}) 
and other corpora, 
demonstrates that the models achieve more robustness to negation while maintaining competitive performance on inputs without negation.