\section{LAMA and LAMA-Neg}
\label{app:lama}

The LAMA (Language Model Analysis) dataset, introduced by ~\citet{petroni-etal-2019-language},
serves as a probe for evaluating the factual and commonsense knowledge embedded within pre-trained language models. 
It consists of facts structured as subject-relation-object triples or question-answer pairs, 
which are transformed into cloze-style sentences to test whether language models can predict the correct masked token. 
LAMA encompasses various knowledge sources covering a range of factual and commonsense relations.

SQuAD~\cite{rajpurkar-etal-2016-squad} the ability of models to answer natural language questions by transforming them into cloze-style sentences.
Google-RE\footnote{\footnotesize {code.google.com/archive/p/relation-extraction-corpus/}} 
assesses the ability to retrieve facts about dates, places of birth, and places of death. 
T-REx~\cite{elsahar-etal-2018-rex}, 
which contains a larger set of relations derived from Wikidata, challenges the models on their ability to generalize across a broader range of facts, 
although some noise exists due to the automatic alignment to Wikipedia. 
ConceptNet~\cite{speer-havasi-2012-representing}, 
on the other hand, tests commonsense reasoning by providing facts about everyday concepts and relationships.

LAMA-Neg, introduced by \citet{kassner-schutze-2020-negated},
extends the LAMA dataset by incorporating negated instances to evaluate the ability of pre-trained language models to handle negation. 
The dataset transforms positive cloze statements from LAMA into their negated counterparts by inserting negation cues such as “not” 
(e.g., “Einstein was born in [MASK]” becomes “Einstein was not born in [MASK]”). 
This addition allows them to probe whether models can correctly differentiate between positive and negative factual assertions.
The model is expected to predict any token other than the original masked token in the negated variant.