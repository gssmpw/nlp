\section{Results on the NLU Tasks}
\label{app:nluresults}

\begin{table*}
    \centering
    \input{tables/nluresults.tex}
    \caption{
        Results on the validation sets of natural language understanding tasks.
        All numbers are macro-averaged F1 scores.
        \label{tab:nluresults}
        }
\end{table*}

Table~\ref{tab:nluresults} presents the results on the validation sets of natural language understanding tasks. 
Following prior work, we report macro-averaged F1 scores on the validation sets as some test labels are not publicly available. 
The results demonstrate that further pre-training consistently improves performance on instances containing negation or, at worst, causes a negligible decline 
(a marginal 0.01\% decrease with {RoBERTa-base} on WSC.)
On average, pre-training yields a 3.11\% improvement across all tasks.
Notably, the most significant improvements are observed on WiC with base models
(achieving an average increase of 7.5\%) and on WSC with large models (where performance improves by 6.0\% on average.) 
Importantly, all models pre-trained on NSP or NSPP outperform off-the-shelf versions on important instances in QNLI, 
with the only exceptions being {BERT-large} pre-trained on either NSPP or NSP. 
