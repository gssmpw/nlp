\section{Implementation Details}
\label{app:hyperparameters}


\begin{table}
\centering
\begin{tabular}{l r}
\toprule
Training Set Size & Batch Size \\
\midrule
1k & 32 \\
2k & 32 \\
5k & 32 \\
10k & 32 \\
20k & 32 \\
50k & 64 \\
100k & 128 \\
200k & 256 \\
500k & 512 \\
1M & 512 \\
\bottomrule
\end{tabular}
\caption{
    Training set sizes and batch sizes used for pre-training.
    The batch size only changes for the training set sizes, not the task we are pre-training on.
    The learning rate is set to $1e-6$ for all training set sizes.
    \label{tab:trainingsetsize}
}

\end{table}

\begin{table*}
\centering
\begin{tabular}{l rrrr}
\toprule
& \multicolumn{1}{c}{Vanilla} & \multicolumn{1}{c}{NSPP} & \multicolumn{1}{c}{NSP} & \multicolumn{1}{c}{NSPP+NSP} \\
\midrule
{RoBEBTa-base} & \\
~~~~CondaQA & $5e-6(16)$ & $1e-5(8)$ & $5e-6(8)$ & $1e-5(16)$ \\
~~~~QNLI & $1e-5(16)$ & $5e-6(16)$ & $1e-5(16)$ & $1e-5(16)$ \\
~~~~WiC & $5e-5(16)$ & $1e-5(16)$ & $1e-5(16)$ & $1e-5(16)$ \\
~~~~WSC & $1e-6(16)$ & $1e-4(16)$ & $1e-6(16)$ & $1e-4(16)$ \\
~~~~RTE & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ \\
~~~~SNLI & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ \\
~~~~MNLI & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ \\
\midrule
{RoBERTa-large} & \\
~~~~CondaQA & $1e-5(16)$ & $1e-5(16)$ & $5e-6(8)$ & $5e-6(16)$ \\
~~~~QNLI & $1e-5(16)$ & $5e-6(16)$ & $1e-5(16)$ & $5e-6(16)$ \\
~~~~WiC & $5e-5(16)$ & $5e-6(16)$ & $1e-5(16)$ & $1e-5(16)$ \\
~~~~WSC & $1e-6(16)$ & $5e-5(16)$ & $1e-6(16)$ & $1e-5(16)$ \\
~~~~RTE & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ \\
~~~~SNLI & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ \\
~~~~MNLI & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ \\
\midrule
\midrule
{BEBT-base} & \\
~~~~CondaQA & $1e-5(8)$ & $1e-5(8)$ & $1e-5(8)$ & $1e-5(8)$ \\
~~~~QNLI & $1e-4(16)$ & $1e-6(16)$ & $5e-5(16)$ & $1e-6(16)$ \\
~~~~WiC & $1e-5(16)$ & $5e-5(16)$ & $5e-5(16)$ & $1e-5(16)$ \\
~~~~WSC & $1e-4(16)$ & $5e-6(16)$ & $1e-5(16)$ & $5e-6(16)$ \\
~~~~RTE & $2e-5(8)$ & $2e-5(8)$ & $2e-5(8)$ & $2e-5(8)$ \\
~~~~SNLI & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ \\
~~~~MNLI & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ \\
\midrule
{BERT-large} & \\
~~~~CondaQA & $5e-6(8)$ & $1e-5(8)$ & $5e-6(8)$ & $1e-5(8)$ \\
~~~~QNLI & $1e-6(16)$ & $1e-5(16)$ & $1e-5(16)$ & $5e-6(16)$ \\
~~~~WiC & $1e-5(16)$ & $1e-5(16)$ & $5e-5(16)$ & $1e-5(16)$ \\
~~~~WSC & $1e-6(16)$ & $5e-6(16)$ & $5e-6(16)$ & $1e-5(16)$ \\
~~~~RTE & $2e-5(8)$ & $2e-5(8)$ & $2e-5(8)$ & $2e-5(8)$ \\
~~~~SNLI & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ & $1e-5(32)$ \\
~~~~MNLI & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ & $2e-5(32)$ \\
\bottomrule
\end{tabular}
\caption{
    The learning rates (and batch sizes) used for fine-tuning the pre-trained models on the training sets of the corpora we evaluate on.   
    \label{tab:finetuninghyperparameters} 
}
\end{table*}

Batch sizes used for pre-training the off-the-shelf {RoBERTa} and {BERT} models on different training set sizes 
are shown in Table~\ref{tab:trainingsetsize}.
We use a learning rate of $1e-6$ for all experiments.

To fine-tune the further pre-trained models on the CondaQA, NLI, and NLU tasks, we utilize the implementations 
provided by \citet{rezaei2024paraphrasing},
 \citet{hossain-etal-2020-analysis}, 
 and \citet{hossain-etal-2022-analysis}, respectively. 
For the NLU and NLI tasks, we use the formatted versions of the datasets available in the 
GLUE \cite{wang-etal-2018-glue} and SuperGLUE \cite{wang2020superglue} benchmarks.
Table~\ref{tab:finetuninghyperparameters} presents the hyperparameters used for fine-tuning the models on the downstream tasks.

For evaluation on LAMA and LAMA-Neg, we do not fine-tune the models, 
as these tasks lack dedicated training sets. 
Notably, since both LAMA and LAMA-Neg are masking tasks, 
it is crucial to prevent catastrophic forgetting during further pre-training. 
To address this, 
we apply Elastic Weight Consolidation (EWC) regularization \cite{ewcregularization} with a coefficient of $\lambda = 1e-3$. 

EWC helps retain the knowledge acquired from the original masked language modeling task 
by penalizing changes to weights deemed important for this task. 
This regularization ensures that while the model adapts to new tasks during pre-training, 
it preserves essential knowledge required for effective performance on the masking tasks. 
This strategy allows us to evaluate the models reliably on LAMA and LAMA-Neg without compromising their original capabilities.