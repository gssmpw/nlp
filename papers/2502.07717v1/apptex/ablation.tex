\section{Ablation Study on Reversing the Polarity}
\label{app:ablation}
\begin{table*}[h!]
    \centering
    \input{tables/condaqa-ablation.tex}
    \caption{
        Results on CondaQA test set for models pre-trained on NSP task with different subsets of the pre-training data.
        An asterisk ($^{\ast}$) indicates a statistically significant improvement (McNemar's test \cite{mcnemar1947note}, $p < 0.05$)
        over the vanilla transformer model.
        Pre-training on the original data consistently outperforms pre-training on subsets of the data.
        However, except for {BERT-large}, pre-training on only one type of instances (add or remove negation) still statistically significantly outperforms the vanilla models.
        \label{tab:condaqa-ablation}
    }
\end{table*}

Recall that our pre-training data includes reversing the polarity of sentences, 
i.e., half the time we add negation to the original data and half the time we remove negation from the original data.
In this section, we provide an ablation study to understand the effect of reversing the polarity of sentences on the performance of the models.
That is, 
we compare our models pre-trained on the original data (obtained by reversing the polarity of sentences) 
with models that have been pre-trained on subsets of the original data where we only add negation or only remove negation.

However,
we need to be careful about the tasks we are evaluating.
Recall that we have two tasks: Next Sentence Polarity Prediction (NSPP) and Next Sentence Prediction (NSP).
Note that we do not reverse the polarity of sentences for the NSPP task and only use original instances.
Additionally, when jointly pre-training on both tasks (NSPP + NSP), NSPP requires both types of instances to be present in the training data.
Therefore, the comparison is only meaningful for the NSP task. 

Table~\ref{tab:condaqa-ablation} presents the results of models pre-trained on different subsets of the data on the CondaQA test set.
Pre-training on the original data consistently outperforms pre-training on subsets of the data by 0.3\% to 7.8\% in terms of accuracy 
and 0.7\% to 12.9\% in terms of group consistency. 
Importantly, 
further pre-training even on a subset of the data still statistically significantly outperforms the vanilla models. 
The only exception is {BERT-large} where pre-training with removing negation leads to a marginally lower accuracy (0.6\%)
and pre-training with adding negation leads to a marginally higher accuracy (0.3\%) compared to the vanilla model.
This suggests that while reversing the polarity of sentences is the best strategy,
pre-training on only one type of instances (add or remove negation) still improves the performance of the models.

Moreover, pre-training with the subset of the data that includes only adding negation consistently outperforms pre-training with the subset of the data that includes only removing negation.
We suspect this is because adding negation is more likely to make the sentence an incoherent follow-up to the first sentence than removing negation is.
Therefore, the model learns more about negation when trained on instances with added negation than when trained on instances with removed negation.