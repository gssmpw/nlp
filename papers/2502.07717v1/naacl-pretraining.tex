% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
% \usepackage[table]{xcolor} % Include the xcolor package

% Remove the "review" option to generate the final version.
\usepackage[]{style/acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{paralist}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{soul}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{soul}
\usepackage{xcolor}

% \newcommand{\hlg}[1]{\sethlcolor{green}\hl{#1}\sethlcolor{yellow}}
\newcommand{\hlg}[1]{#1}
\renewcommand{\hl}[1]{#1}

\usepackage{soulpos} % Extended version of `soul` for nested commands
\usepackage{etoolbox}
\newcommand{\role}[1]{\textsc{#1}}
% \newcommand{\tbd}[1]{\marginpar{\footnotesize#1}}
\newcommand{\tbd}[1]{}

\usepackage{colortbl}
\usepackage{pgfplots}

\usepackage{tabularx}
\usepackage{icomma}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{
  Making Language Models Robust Against Negation
}

\author{MohammadHossein Rezaei \and Eduardo Blanco \\
         Department of Computer Science, University of Arizona \\ \texttt{\{mhrezaei,eduardoblanco\}@arizona.edu}}

% \pagecolor{yellow!20}
\begin{document}
\maketitle
\begin{abstract}
Negation has been a long-standing challenge for language models.
Previous studies have shown that they struggle with negation in many natural language understanding tasks.
In this work, we propose a self-supervised method to make language models more robust against negation.
We introduce a novel task, Next Sentence Polarity Prediction (NSPP), and a variation of the Next Sentence Prediction (NSP) task.
We show that BERT and RoBERTa further pre-trained on our tasks outperform the off-the-shelf versions on nine negation-related benchmarks.
Most notably, our pre-training tasks yield between 1.8\% and 9.1\% improvement on CondaQA, a large question-answering corpus requiring reasoning over negation.
\end{abstract}


\input{tex/introduction.tex}
\input{tex/related_work.tex}
\input{tex/empowering.tex}
\input{tex/reverse_polarity.tex}
\input{tex/dataset.tex}
\input{tex/evaluation.tex}
\input{tex/experimental_setup.tex}
\input{tex/results.tex}
\input{tex/conclusion.tex}



% \pagebreak
\section*{Limitations}
We experiment with two models, {RoBERTa} and {BERT}, and a single pre-training dataset, Wikipedia.
Future work may consider other models and pre-training datasets.
Our rules for reversing polarity only cover \hl{``not'', ``n't'', and ``never''}.
However, they are still effective in making models more robust against negation in general---recall that CondaQA has over 200 unique negation cues.
Future work may consider working with more sophisticated rules to reverse the polarity of the sentences.
We also only experiment with models pre-trained on 500K and 1M instances.
Future work may consider training on the whole corpus and evaluate the performance on downstream tasks.
Additionally, all the corpora we work with are in English.
We acknowledge that negation may be expressed differently in other languages and this work may not generalize to other languages.
We note, however, that the proposed tasks are language-agnostic and can be applied to other languages.



\section*{Ethics Statement}
The work in this paper does not involve human subjects.
We only use publicly available datasets and models.
We do not collect any personal information.
Therefore, this work does not raise any ethical concerns.

% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
% \bibliographystyle{acl_natbib}

%\clearpage
\appendix
\input{apptex/rules.tex}
\input{apptex/llm.tex}
\input{apptex/condaqa_example.tex}
\input{apptex/nlunli_example.tex}
\input{apptex/lama.tex}
\input{apptex/hyper.tex}
\input{apptex/ablation.tex}
\input{apptex/nli-details.tex}
% \input{apptex/nlu-results.tex}
% \input{apptex/lama-results.tex}


\end{document}