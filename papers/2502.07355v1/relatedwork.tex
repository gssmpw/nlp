\section{Related Works}
There are several existing studies on the finite-length performance of BP decoding for fountain codes, LDPC codes, and BATS codes. Karp \textit{et al.} developed a recursive formula to compute the error probability of LT codes based on a finite-state machine model~\cite{FL_analysis_LT}. Shokrollahi provided an analysis on the error probability of Raptor codes, utilizing the finite-length analyses of LT codes and a class of variable-regular LDPC codes~\cite{Raptor,shokrollahi2001finite}. His analysis is based on a cascade of an LT decoder and an LDPC decoder, which neglects the joint iteration between the two parts, and so becomes an upper bound on the overall error probability. Moreover, several methods for finite-length analysis of regular LDPC codes on the erasure channel presented in~\cite{FL_analysis_LDPC,Modern_Coding_Theory,Johnson2009finite} can also be directly applied to improve Shokrollahi's upper bound for Raptor codes with more general LDPC precodes. Yang \textit{et al.} provided two recursive formulae to compute the error probability of BATS codes for a given number of received batches~\cite{FL_analysis_BATS}. These two recursive formulae also follow the model of a finite-state machine, but in comparison to the recursive formula for LT codes, they are more computationally complex. Note that the precode of BATS codes are not taken into account in these two recursive formulae.

A few works have addressed the error probability of LT and Raptor codes under ML decoding~\cite{Rahnavard2007, Birgit2013LT, Wang2016performanceRaptorML, Zhang2017boundsRaptorQ, Lazaro2017inactivation, Lazaro2021bounds}. In~\cite{Rahnavard2007}, Rahnavard \textit{et al.} derived upper and lower bounds on the bit error probability for binary LT and Raptor codes, where all entries of the parity-check matrix of the precode are independent and identically distributed (i.i.d.) Bernoulli random variables. In~\cite{Birgit2013LT}, Schotsch \textit{et al.} provided the upper and lower bounds on the error probability of LT codes on word as well as on symbol level, where the upper bound on the symbol-wise error probability is a generalization of~\cite{Rahnavard2007}. Wang \textit{et al.} provided upper and lower bounds on error probability of binary Raptor codes with a systematic low-density
generator-matrix precode~\cite{Wang2016performanceRaptorML}. This work was elegantly extended in~\cite{Zhang2017boundsRaptorQ}, where $q$-ary Raptor codes are considered. Both~\cite{Wang2016performanceRaptorML} and~\cite{Zhang2017boundsRaptorQ} consider very short Raptor codes (the number of input symbols $\le 100$). Furthermore, upper and lower bounds for $q$-ary Raptor codes with generic $q$-ary linear precodes were derived in~\cite{Lazaro2021bounds}. 

\textit{Inactivation decoding} is an efficient ML decoding for sparse linear systems, and its complexity is typically measured by the number of inactive symbols (see~\cite{InactivationPatent,Raptor_codes_foundations_and_trends,Lazaro2017inactivation} for inactivation decoding and inactive symbols). In~\cite{FL_analysis_BATS}, Yang \textit{et al.} developed a recursive formula to compute the expected number of inactive symbols for BATS codes (the inactivation decoding of BATS codes is similar to that of fountain codes, which is introduced in Sec.~\ref{subsection:decoding_std_BATS}). This recursive formula becomes essentially the same as the recursive formula for fountain codes presented in~\cite{Lazaro2017inactivation} when the batch size is one. However, there has been no analysis on the error probability of BATS codes under ML decoding, even without a precode.