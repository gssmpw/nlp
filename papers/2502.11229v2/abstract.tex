 \begin{abstract}
 This paper investigates the convergence properties of the hypergradient descent method ({\hdm}), a 25-year-old heuristic originally proposed for adaptive stepsize selection in stochastic first-order methods \cite{almeida1999parameter, gunes2018online}.  
 We provide the first rigorous convergence analysis of {\hdm} using the online learning framework of \cite{gao2024gradient} and apply this analysis to develop new state-of-the-art adaptive gradient methods with empirical and theoretical support. 
 Notably, {\hdm} automatically identifies the optimal stepsize for the local optimization landscape and achieves local superlinear convergence. Our analysis explains the instability of {\hdm} reported in the literature and proposes efficient strategies to address it. We also develop two {\hdm} variants with heavy-ball and Nesterov momentum. Experiments on deterministic convex problems show {\hdm} with heavy-ball momentum (\hdmhb) exhibits robust performance and significantly outperforms other adaptive first-order methods. Moreover, {\hdmhb} often matches the performance of \texttt{L-BFGS}, an efficient and practical quasi-Newton method, using less memory and cheaper iterations.
\end{abstract}