\section{Proof of Results in Section \ref{sec:hdm-ol}} \label{app:proof-hdm-ol}

\subsection{Auxiliary Results}

\begin{lem}[Sublinear dynamic regret \cite{hazan2016introduction}] \label{lem:auxi-dynamic}
  Given a family of convex and $\gamma$-Lipschitz losses $\{ h_k \}$, online
  gradient descent $P_{k + 1} = \Pi_{\mathcal{P}} [P_k - \eta \nabla h_k
  (P_k)]$ with constant stepsize $\eta > 0$ generates a sequence of scaling
  matrices $\{ P_k \}$ such that
\begin{equation} \label{eqn:auxi-ogd-dynamic-regret}
	\textstyle \sum_{k = 1}^K h_k (P_k) - h_k (\hat{P}_k) \leq \tfrac{D^2}{2 \eta} +
     \tfrac{\eta}{2} \gamma^2 K + \tfrac{D}{2 \eta} \mathsf{PL} (\{ \hat{P}_k
     \}).
\end{equation}
  where $\{ \hat{P}_k \}, \hat{P}_k\in \Pcal$ are arbitrarily chosen competitors and $\mathsf{PL}
  (\{ \hat{P}_k \}) \assign \sum_{k = 1}^K \| \hat{P}_k - \hat{P}_{k + 1}
  \|_F$ is the path length of the competitors. In particular, if $\hat{P}_k \equiv P$, then $\mathsf{PL} (\{ \hat{P}_k
     \})=0$ and 
\begin{equation} \label{eqn:auxi-ogd-static-regret}
	\textstyle \sum_{k = 1}^K h_k (P_k) - h_k (\hat{P}_k) \leq \tfrac{D^2}{2 \eta} +
     \tfrac{\eta}{2} \gamma^2 K.
\end{equation}
\end{lem}
\begin{proof}
The result follows from a standard dynamic regret analysis from online convex
optimization literature, and we adapt the proof for our analysis. For any $P
\in \mathcal{P}$, we deduce
\begin{align}
  \| P_{k + 1} - P \|_F^2 ={} & \| \Pi_{\mathcal{P}} [P_k - \eta \nabla h_k
  (P_k)] - P \|_F^2 \nonumber\\
  \leq{} & \| P_k - P - \eta \nabla h_k (P_k) \|_F^2 \label{eqn:dynamic-regret-1} \\
  \leq{} & \| P_k - P \|_F^2 - 2 \eta \langle \nabla h_k (P_k), P_k - P \rangle
  + \eta^2 \| \nabla h_k (P_k) \|^2_F \nonumber \\
  \leq{} & \| P_k - P \|_F^2 - 2 \eta [h_k (P_k) - h_k (P)] + \eta^2 \gamma^2, \label{eqn:dynamic-regret-2} 
\end{align}
where \eqref{eqn:dynamic-regret-1} uses non-expansiveness of orthogonal projection; \eqref{eqn:dynamic-regret-2} applies convexity and $\gamma$-Lipschitz continuity of $h_k$.
Now, let $P = \hat{P}_k$ and we re-arrange to get
\begin{align}
  h_k (P_k) - h_k (\hat{P}_k) \leq{} & \tfrac{1}{2 \eta} [\| P_k - \hat{P}_k
  \|_F^2 - \| P_{k + 1} - \hat{P}_k \|_F^2] + \tfrac{\eta}{2} \gamma^2
  \nonumber\\
  ={} & \tfrac{1}{2 \eta} [\| P_k \|_F^2 - \| P_{k + 1} \|_F^2 + 2 \langle
  \hat{P}_k, P_{k + 1} - P_k \rangle] + \tfrac{\eta}{2} \gamma^2 \nonumber\\
  \leq{} & \tfrac{D^2}{2 \eta} + \tfrac{D}{2 \eta} \| P_{k + 1} - P_k \|_F +
  \tfrac{\eta}{2} \gamma^2,\label{eqn:dynamic-regret-3}
\end{align}
where \eqref{eqn:dynamic-regret-3} uses Cauchy's inequality $\langle
  \hat{P}_k, P_{k + 1} - P_k \rangle \leq \|\hat{P}_k\|\cdot\|P_{k + 1} - P_k\| \leq D\|P_{k + 1} - P_k\|$. 
   Telescoping,
\[ \textstyle \sum_{k = 1}^K h_k (P_k) - h_k (\hat{P}_k) \leq \tfrac{D^2}{2 \eta} +
   \tfrac{\eta}{2} \gamma^2 K + \tfrac{D}{2 \eta} \sum_{k = 1}^K \| \hat{P}_k
   - \hat{P}_{k + 1} \|_F \]
and this completes the proof.
\end{proof}
\begin{lem}[Logarithmic static regret \cite{orabona2019modern}] \label{lem:auxi-log-regret}
  Given a family of $\mu$-strongly convex and $\gamma$-Lipschitz losses $\{
  h_k \}$, online gradient descent $P_{k + 1} = \Pi_{\mathcal{P}} [P_k - \eta_k \nabla h_k
  (P_k)]$ with stepsize $\eta_k = 1 / (\mu k)$
  generates a sequence of scaling matrices $\{ P_k \}$ such that $\sum_{k =
  1}^K h_k (P_k) - h_k (P) \leq \tfrac{1}{2} \gamma^2 \log K.$
\end{lem}

\begin{proof}

Using strong convexity, we have $h_k (P) \geq h_k (P_k) + \langle \nabla h_k (P_k), P - P_k \rangle +
   \tfrac{\mu}{2} \| P - P_k \|_F^2  $ and 
\begin{align}
  \| P_{k + 1} - P \|_F^2 \leq{} & \| P_k - P \|_F^2 - 2 \eta_k \langle \nabla
  h_k (P_k), P_k - P \rangle + \eta_k^2 \gamma^2 \nonumber\\
  \leq{} & \| P_k - P \|_F^2 - 2 \eta_k [h_k (P_k) - h_k (P)] + \eta_k^2
  \gamma^2 - \mu \eta_k \| P - P_k \|_F^2 \nonumber\\
  ={} & \tfrac{k - 1}{k} \| P_k - P \|_F^2 - \tfrac{2}{k \mu} [h_k (P_k) - h_k
  (P)] + \tfrac{\gamma^2}{k^2 \mu^2} \label{eqn:dynamic-regret-4}, 
\end{align}
where \eqref{eqn:dynamic-regret-4} plugs in $\eta_k = 1/(\mu k)$. Re-arranging the terms, \[h_k (P_k) - h_k (P) \leq \tfrac{\mu}{2} [(k - 1) \|
P_k - P \|_F^2 - k \| P_{k + 1} - P \|_F^2] + \tfrac{\gamma^2}{2 k \mu}\] and
telescoping gives $\sum_{k = 1}^K h_k (P_k) - h_k (P) \leq
\sum_{k = 1}^K \tfrac{\gamma^2}{2 k \mu} \leq \tfrac{\gamma^2}{2 \mu} (\log K
+ 1)$, which completes the proof.
\end{proof}

\subsection{Proof of Lemma \ref{lem:hx-properties}}

Consider the first property. Convexity and smoothness follow directly from \cite{gao2024gradient}.
To verify strong convexity, note that for $h_x (\alpha) = \tfrac{f (x - \alpha
\nabla f (x)) - f (x^{\star})}{\| \nabla f (x) \|^2}$
\[ h_x'' (\alpha) = \tfrac{\mathd}{\mathd \alpha} \Big[ \tfrac{\langle \nabla
   f (x - \alpha \nabla f (x)), \nabla f (x) \rangle}{\| \nabla f (x) \|^2}
   \Big] = \big\langle \tfrac{\nabla f (x)}{\| \nabla f (x) \|}, \nabla^2 f
   (x) \tfrac{\nabla f (x)}{\| \nabla f (x) \|} \big\rangle \geq \mu \]
since $\nabla^2 f (x) \succeq \mu I$ and $x \nin \Xcal^\star$. This completes the proof of the first property.\\


Next, we consider the second property. Lipschitz continuity also follows from \cite{gao2024gradient}. To verify exp-concavity, recall that a twice-differentiable function
$h$ is $\beta$-exp-concave if $\nabla^2 h (x) \succeq \beta \nabla h (x)
\nabla h (x)^{\top}$ for some $\beta \geq 0$. By definition of $\Dcal$,
\[ \nabla h_x (P) = - \tfrac{\nabla f (x) \circ \nabla f (x - P \nabla f
   (x))}{\| \nabla f (x) \|^2} = - \tfrac{\diag (\nabla f (x)) \nabla f
   (x - P \nabla f (x))}{\| \nabla f (x) \|^2} \]
and $\nabla^2 h_x (P) = \frac{\diag (\nabla f (x)) \nabla^2 f (x - P
\nabla f (x)) \diag (\nabla f (x))}{\| \nabla f (x) \|^2}$. Using
$\nabla^2 f (x - P \nabla f (x)) \succeq \mu I$, we deduce that
\begin{align}
  & \nabla^2 h_x (P) - \beta\nabla h_x (P) \nabla h_x
  (P)^{\top} \nonumber\\
  ={} & \tfrac{\diag (\nabla f (x)) \nabla^2 f (x - P \nabla f (x))
  \diag (\nabla f (x))}{\| \nabla f (x) \|^2} - \beta \tfrac{\diag
  (\nabla f (x)) \nabla f (x - P \nabla f (x)) \nabla f (x - P \nabla f
  (x))^{\top} \diag (\nabla f (x))}{\| \nabla f (x) \|^4} \nonumber\\
  ={} & \diag \big( \tfrac{\nabla f (x)}{\| \nabla f (x) \|} \big)
  \Big[ \nabla^2 f (x - P \nabla f (x)) - \beta \tfrac{\nabla f (x - P \nabla
  f (x))}{\| \nabla f (x) \|} \tfrac{\nabla f (x - P \nabla f (x))^{\top}}{\| \nabla
  f (x) \|} \Big] \diag \big( \tfrac{\nabla f (x)}{\| \nabla f
  (x) \|} \big) \nonumber\\
  \succeq{} & \diag \big( \tfrac{\nabla f (x)}{\| \nabla f (x) \|}
  \big) \Big[ \mu I - \beta \tfrac{\nabla f (x - P \nabla f (x))}{\| \nabla
  f (x) \|} \tfrac{\nabla f (x - P \nabla f (x))^{\top}
}{\| \nabla f (x) \|}  \Big] \diag \big( \tfrac{\nabla f (x)}{\| \nabla f (x) \|} \big), \label{eqn:proof-1-1}
\end{align}
where \eqref{eqn:proof-1-1} uses $\mu$-strong convexity of $f(x)$. Now, it suffices to verify that
\begin{equation} \label{eqn:proof-1-2}
	\tfrac{\nabla f (x - P \nabla f (x))}{\| \nabla f (x) \|} \tfrac{\nabla f
   (x - P \nabla f (x))^{\top}}{\| \nabla f (x) \|} \preceq \tfrac{\mu}{\beta}  I
\end{equation}
for all $x \nin \Xcal^\star$.
Write $\tfrac{\nabla f (x - P \nabla f (x))}{\| \nabla f (x) \|} =
\tfrac{\nabla f (x)}{\| \nabla f (x) \|} + \tfrac{\nabla f (x - P \nabla f
(x)) - \nabla f (x)}{\| \nabla f (x) \|}$ and let $z \assign \nabla f (x -
P \nabla f (x)) - \nabla f (x)$, we have, by $L$-smoothness, that $\| z \| \leq L \|P \nabla f(x) \| \leq L D \| \nabla f (x) \|$
and
\begin{align}
  \Big\| \tfrac{\nabla f (x - P \nabla f (x))}{\| \nabla f (x) \|}
  \tfrac{\nabla f (x - P \nabla f (x))^{\top}}{\| \nabla f (x) \|} \Big\| ={}
  & \Big\| \Big( \tfrac{\nabla f (x)}{\| \nabla f (x) \|} + \tfrac{z}{\|
  \nabla f (x) \|} \Big) \Big( \tfrac{\nabla f (x)}{\| \nabla f (x) \|} +
  \tfrac{z}{\| \nabla f (x) \|} \Big)^{\top} \Big\| \nonumber\\
  ={} & \big\| \tfrac{\nabla f (x) \nabla f (x)^{\top}}{\| \nabla f (x) \|^2} +
  \tfrac{z \nabla f (x)^{\top}}{\| \nabla f (x) \|^2} + \tfrac{\nabla f (x)
  z^{\top}}{\| \nabla f (x) \|^2} + \tfrac{z z^{\top}}{\| \nabla f (x) \|^2}
  \big\| \nonumber\\
  \leq{} & 1 + \tfrac{2 \| z \|}{\| \nabla f (x) \|} + \tfrac{\| z \|^2}{\|
  \nabla f (x) \|^2} = ( 1 + \tfrac{\| z \|}{\| \nabla f (x) \|}
  )^2 \leq (1 + L D)^2 . \nonumber
\end{align}

Hence, for $\beta \leq \tfrac{\mu}{(1 + L D)^2}$ the relation \eqref{eqn:proof-1-2} holds. We conclude that $h_x(P) = h_x(d)$ is $\frac{\mu}{(1 + L D)^2}$-exponential concave.

\subsection{Proof of Lemma \ref{lem:regret-sublinear}}

We use Lipschitzness from \Cref{lem:hx-properties} and \eqref{eqn:auxi-ogd-static-regret} from \Cref{lem:auxi-dynamic} by taking $\gamma = 1+LD$ and $\eta = \frac{D}{(LD+1)\sqrt{K}}$.

\subsection{Proof of Lemma \ref{lem:regret-log}}
We use Lipschitzness and strong convexity from \Cref{lem:hx-properties} and invoke \Cref{lem:auxi-log-regret} by taking $\gamma = 1 + L D$.

\subsection{Proof of Lemma \ref{lem:hypergrad-to-online}}

The proof resembles \cite{gao2024gradient} and uses a tighter analysis.
Consider the optimality measure $f (x^{K + 1}) - f (x^{\star})$, and we deduce that
\begin{align}
  f (x^{K + 1}) - f (x^{\star}) ={} & \frac{1}{\frac{1}{f (x^{K + 1}) - f
  (x^{\star})}} \nonumber\\
  ={} & \frac{1}{\sum_{k = 1}^K \frac{1}{f (x^{k + 1}) - f (x^{\star})} -
  \frac{1}{f (x^k) - f (x^{\star})} + \frac{1}{f (x^1) - f (x^{\star})}}
  \nonumber\\
  ={} & \frac{1}{\sum_{k = 1}^K \frac{f (x^k) - f (x^{k + 1})}{[f (x^{k + 1}) -
  f (x^{\star})] [f (x^k) - f (x^{\star})]} + \frac{1}{f (x^1) - f
  (x^{\star})}} \nonumber\\
  ={} & \frac{1}{\sum_{k = 1}^K \frac{\max \{ - h_{x^k} (P_k), 0 \} \| \nabla f
  (x^k) \|^2}{[f (x^{k + 1}) - f (x^{\star})] [f (x^k) - f (x^{\star})]} +
  \frac{1}{f (x^1) - f (x^{\star})}} \nonumber
\end{align}

Next, using $f (x) - f (x^{\star}) \leq \| \nabla f (x) \| \cdot \| x -
x^{\star} \|$,
\[ \tfrac{\max \{ - h_{x^k} (P_k), 0 \} \| \nabla f (x^k) \|^2}{[f (x^{k + 1})
   - f (x^{\star})] [f (x^k) - f (x^{\star})]} \geq \tfrac{\max \{ - h_{x^k}
   (P_k), 0 \} \| \nabla f (x^k) \|^2}{[f (x^k) - f (x^{\star})]^2} \geq
   \tfrac{\max \{ - h_{x^k} (P_k), 0 \}}{\dist (x^k,
   \mathcal{X}^{\star})^2} \geq \tfrac{\max \{ - h_{x^k} (P_k), 0
   \}}{\Delta^2}. \]
Finally, we deduce that
\begin{align}
  f (x^{K + 1}) - f (x^{\star}) \leq{} & \tfrac{\Delta^2}{\sum_{k = 1}^K \max \{
  - h_{x^k} (P_k), 0 \} + \tfrac{\Delta^2}{f (x^1) - f (x^{\star})}}
  \nonumber\\
  \leq{} & \tfrac{\Delta^2}{\max \{ \sum_{k = 1}^K - h_{x^k} (P_k), 0
  \} + \frac{\Delta^2}{f (x^1) - f (x^{\star})}} \nonumber\\
  \leq{} & \min \Big\{ \tfrac{\Delta^2}{K \max \{ \frac{1}{K} \sum_{k =
  1}^K - h_{x^k} (P_k), 0 \}}, f (x^1) - f (x^{\star}) \Big\}
  \nonumber
\end{align}
and this completes the proof.

\subsection{Intermediate Iterate Convergence with Adaptive Online Algorithms} \label{sec:intermediate-iter}

One disadvantage of constant stepsize online gradient descent is 1) it requires the total iteration number $K$. 2) No regret guarantee for the intermediate iterates. One simple fix is let $\eta_k = \Ocal(1/\sqrt{k})$. It gives the same sublinear regret guarantee up to a constant multiplicative factor \cite{orabona2019modern}, but the regret guarantee holds for any $k$. Similar arguments hold for adaptive gradient methods \cite{duchi2011adaptive,mcmahan2010adaptive}.


