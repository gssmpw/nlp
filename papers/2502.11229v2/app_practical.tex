\section{{\hdm} in Practice} \label{app:hdmprac}

This section introduces {\hdmbest}, our recommended practical hypergradient descent method. This variant is adapted from {\hdmhb}, with simplifications to reduce the implementation complexity. The algorithm is given in \Cref{alg:practical}.

\begin{algorithm}[h]
{\textbf{input} starting point $x^0 = x^1$, $\Pcal = \mathbb{S}^n_{+} \cap \Dcal, \Bcal = [0,0.9995]$, initial diagonal preconditioner $P_1 \in \mathbb{S}^n_{+} \cap \Dcal$, \\initial scalar momentum parameter $\beta_1 = 0.95$, {\adagrad} stepsize $\eta_p, \eta_b > 0$, {\adagrad}  diagonal matrix $U_1 = 0$, {\adagrad} momentum scalar $v_1 = 0$, $\tau > 0$}\\
\For{k =\rm{ 1, 2,...}}{
$\begin{aligned}
x^{k + 1 / 2} & = x^k - P_k \nabla f (x^k) + \beta_k (x^k - x^{k - 1}) \\
\nabla_P h_{x^k, x^{k - 1}} (P_k, \beta_k) & = \tfrac{\diag(\nabla f (x^{k + 1 / 2}) \circ \nabla f (x^k))}{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1} \|^2} \texttt{ \# Element-wise product} \\
\nabla_{\beta} h_{x^k, x^{k - 1}} (P_k, \beta_k) & = \tfrac{\langle \nabla f (x^{k + 1 / 2}), x^k - x^{k - 1} \rangle}{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1} \|^2} \texttt{ \# Inner product} \\
U_{k + 1} & = U_k + \nabla_P h_{x^k, x^{k - 1}} (P_k, \beta_k) \circ \nabla_P h_{x^k, x^{k - 1}} (P_k, \beta_k) \texttt{ \# Diagonal matrix} \\
v_{k + 1} & = v_k + \nabla_{\beta} h_{x^k, x^{k - 1}} (P_k, \beta_k) \cdot \nabla_{\beta} h_{x^k, x^{k - 1}} (P_k, \beta_k) \texttt{ \# Scalar matrix} \\
P_{k + 1} & = \Pi_{\Rbb^n_{+} \cap \Dcal} [P_k - \eta_p U_{k + 1}^{- 1 / 2} \nabla_P h_{x^k, x^{k - 1}} (P_k, \beta_k)] \texttt{ \# Diagonal matrix} \\
\beta_{k + 1} & = \Pi_{[0, 0.9995]} [\beta_k - \eta_b v_{k + 1}^{- 1 / 2} \nabla_{\beta} h_{x^k, x^{k - 1}} (P_k, \beta_k)] \\
x^{k + 1} & = \argmin_{x \in \{ x^k, x^{k + 1 / 2} \}} f (x).
\end{aligned}
$
}
{\textbf{output} $x^{K+1}$}
\caption{{\hdmbest} \label{alg:practical}}
\end{algorithm}

We make several remarks about \Cref{alg:practical}. 

\begin{itemize}[leftmargin=10pt]
\item \textit{Choice of online learning algorithm.} Unless $f(x)$ is quadratic, adaptive online learning algorithms such as {\adagrad} often significantly outperform online gradient descent with constant stepsize. Note that {\adagrad} introduces additional memory of size $n$ to store the diagonal online learning preconditioner $U$.

\item \textit{Sensitivity of parameters.} The two stepsize parameters in {\adagrad} are the most important algorithm parameters: $\eta_p, \eta_b$. According to the experiments, $\eta_p$ should be set proportional to $1/L$, the smoothness constant, while an aggressive choice of $\eta_b \in \{1,10,100\}$ often yields fast convergence. A local estimator of the smoothness constant $L$ can significantly enhance algorithm performance.
\item \textit{Heavy-ball feedback and null step.} In practice, it is observed that dropping the $\frac{\omega}{2}\|x^+(P, B) - x\|^2$ in the numerator of heavy-ball feedback \eqref{eqn:heavyball-feedback} often does not affect algorithm performance. Therefore, in \Cref{alg:practical} the hypergradient with respect to $\frac{\omega}{2}\|x^+(P, B) - x\|^2$ is ignored. 
On the other hand, the $\frac{\tau} {2}\|x^+(P, B) - x\|^2$  term in the denominator smoothes the update of $\beta_k$ and can strongly affect convergence. The parameter $\tau$ should be taken to be proportional to $L^2$ according to the discussions in \Cref{app:heavy-ball}. The null step is taken with respect to the function value $f(x)$ instead of the heavy-ball potential function.
\item \textit{Memory usage.} The memory usage of {\hdmbest}, measured in terms of number of vectors of length $n$ is $7n$: 1) three vectors store primal iterates $x^{-}, x, x^{+}$. 2) Two vectors store past and buffer gradients $\nabla f(x), \nabla f(x^+)$. 3) A vector stores the diagonal preconditioner $P_k$. 4) A vector stores the {\adagrad} stepsize matrix $U$.
\item \textit{Computational cost. } The major additional computation cost arises from computing hypergradient $\nabla h$, which involves one element-wise product and one inner product for vectors of size $n$. In addition, {\hdmbest} needs to maintain a diagonal matrix for {\adagrad}. The overall additional computational cost is several $\Ocal({n})$ operations.
\end{itemize}
