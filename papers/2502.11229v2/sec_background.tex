\section{Background: {\hdm} and Online Learning} \label{sec:hdm-ol}

This section establishes the connection between {\hdm} and online learning through the framework in \cite{gao2024gradient}. 
We refer to the following assumptions in the paper.

\begin{enumerate}[leftmargin=30pt,label=\textbf{A\arabic*:},ref=\rm{\textbf{A\arabic*}},start=1]
  \item $f(x)$ is $L$-smooth and convex.  \label{A1}
  \item $f(x)$ is $\mu$-strongly convex with $\mu > 0$. \label{Ascvx}
  \item Closed convex set $\Pcal$ satisfies $0 \in \Pcal, L^{-1} I\in \Pcal$ and $\diam (\mathcal{P}) \leq D < \infty$. \label{A2}
\end{enumerate}

\subsection{Descent Lemma and Hypergradient Feedback}

Hypergradient feedback \eqref{eqn:hypergrad-feedback} is motivated by  descent lemma:
\[ f ( x - \tfrac{1}{L} \nabla f (x) ) - f (x) \leq - \tfrac{1}{2
   L} \| \nabla f (x) \|^2 . \]
The descent lemma states that, under the constant stepsize $P_k \equiv \tfrac{1}{L} I$,
the function value progress of a gradient step is proportional to $\| \nabla f
(x) \|^2$ with ratio $- 1/(2L)$. 
When an (effective) preconditioner $P_k$ is used, the \emph{effective} smoothness constant decreases, and thus the ratio $h_x (P) = \tfrac{f (x - P \nabla f (x)) - f (x)}{\| \nabla f (x) \|^2}$ is expected to become smaller than $- 1 / (2 L)$, yielding a faster convergence. 
Hence, the ratio $h_x (P)$ is a suitable feedback to measure the quality of a preconditioner. {\hdm} uses this
feedback to learn a good preconditioner using online gradient descent. The hypergradient feedback $h_x (P)$ has the following properties.

\begin{lem}[{Extension of Proposition 6.1 in \cite{gao2024gradient}}]\label{lem:hx-properties}
  For any $x \nin \mathcal{X}^{\star}$.
  \begin{itemize}[leftmargin=10pt]
    \item Under \ref{A1}, $h_x (P)$ is convex and $L$-smooth and $h_x(\frac{1}{L}I)\leq -\frac{1}{2L}$. Moreover, if \ref{Ascvx} holds and $\mathcal{P} \subseteq \mathcal{S}$,
    then $h_x (P) = h_x (\alpha)$ is $\mu$-strongly convex. 
    
    \item Under \ref{A1} and \ref{A2}, $h_x (P)$ is  $(L D + 1)$-Lipschitz. Moreover, if \ref{Ascvx} holds and $\mathcal{P} \subseteq \mathcal{D}$,
    then $h_x (P) = h_x (d)$ is $\frac{\mu}{(1 + L D)^2}$-exponential concave \cite{hazan2007logarithmic}.
  \end{itemize}
\end{lem}

\subsection{Online Learning Guarantees}

Using the convexity and Lipschitz continuity of $h_x(P)$, standard analysis in online learning literature  \cite{orabona2019modern,hazan2016introduction} guarantees sublinear regret for online gradient descent. 

\begin{lem}[{Sublinear regret \cite{gao2024gradient}}] \label{lem:regret-sublinear}
  Under \ref{A1} and \ref{A2}, online gradient descent
\begin{equation} \label{eqn:olalg-ogd}
	P_{k + 1} = \Pi_{\mathcal{P}} [P_k - \eta_k \nabla h_{x^k} (P_k)]
\end{equation}
with stepsize $\eta_k \equiv \tfrac{D}{ (L D + 1) \sqrt{K}}$ generates 
  % a  sequence of 
  $\{P_k\}$ such that
\begin{align}
\textstyle \sum_{k = 1}^K h_{x^k} (P_k) - \displaystyle \min_{P \in \mathcal{P}}  \textstyle \sum_{k = 1}^K h_{x^k} (P)
\leq \rho_{K} \assign D (L D + 1) \sqrt{K} \label{eqn:ogd-constant}. 
\end{align}
\end{lem}

If strong convexity \ref{Ascvx} is further assumed and $P_k \in \Scal$, a different choice of hypergradient stepsize $\eta_k$ in \eqref{eqn:olalg-ogd} improves the regret to $\log K$.

\begin{lem}[Logarithmic regret] \label{lem:regret-log}
Instate \ref{A1} to \ref{A2} and suppose $\mathcal{P} \subseteq
  \mathcal{S}$. Then online gradient descent \eqref{eqn:olalg-ogd}	with  $\eta_k = 1/(k \mu)$ generates a sequence of $\{
  P_k \}$ such that $\textstyle \sum_{k = 1}^K h_{x^k} (P_k) -  \min_{P \in \mathcal{P}}  \textstyle \sum_{k
  = 1}^K h_{x^k} (P) = \Ocal(\log K). $
\end{lem}

\begin{rem}
Given exponential-concavity of $h_x$ established in \Cref{lem:hx-properties}, it is possible to apply online learning algorithms such as online Newton method \cite{hazan2007logarithmic}.
\end{rem}

\subsection{Hypergradient Reduction and {\hdm}}
One major contribution of \cite{gao2024gradient} is an online-to-offline reduction that relates the minimization of cumulative hypergradient feedback $\textstyle \sum_{k=1}^K h_{x^k}(P_k)$ to the function value gap.
We provide a sharper version of this reduction.

\begin{lem}[{Sharper version of Lemma 6.1 in \cite{gao2024gradient}}] \label{lem:hypergrad-to-online}
Under \ref{A1}, the iterates generated by \Cref{alg:hdm} satisfy
\begin{equation*}
f(x^{K+1}) - f(x^\star)
\leq \min \Big\{ \tfrac{\Delta^2}{K \max \{ \frac{1}{K} \sum_{k = 1}^K - h_{x^k} (P_k), 0 \}}, f (x^1) - f (x^{\star}) \Big\},
\end{equation*}
where $\Delta = \max_{x \in \mathcal{L}_{f (x^1)}} \min_{x^{\star} \in \mathcal{X}^{\star}} \| x - x^{\star} \|$.
Further, under  \ref{A1} and \ref{Ascvx},
\begin{equation*}
f(x^{K+1}) - f(x^\star)
\leq (f (x^1) - f (x^{\star})) \big( 1 - 2 \mu \max \big\{ \tfrac{1}{K} \textstyle\sum_{k = 1}^K - h_{x^k} (P_k), 0 \big\} \big)^K.
\end{equation*}
\end{lem}

According to \Cref{lem:hypergrad-to-online}, the negative average feedback $\tfrac{1}{K} \textstyle\sum_{k = 1}^K - h_{x^k} (P_k)$ determines the rate for sublinear/linear convergence of \Cref{alg:hdm}: larger $\tfrac{1}{K} \textstyle\sum_{k = 1}^K - h_{x^k} (P_k)$ implies faster convergence. Given the objective $\tfrac{1}{K} \textstyle\sum_{k = 1}^K - h_{x^k} (P_k)$, {\hdm} applies online gradient descent to generate a sequence of preconditioners $\{P_k\}$ that guarantee the following lower bound:
\begin{equation} \label{eqn:avg-guarantee}
  \tfrac{1}{K} \textstyle\sum_{k = 1}^K - h_{x^k} (P_k) \geq \displaystyle \max_{P \in \mathcal{P}}  \tfrac{1}{K} \textstyle \sum_{k
  = 1}^K - h_{x^k} (P) + o(1),
\end{equation}
which follows from the sublinear regret $\rho_K = o(K)$ in \Cref{lem:regret-sublinear} and \Cref{lem:regret-log}, implying $\tfrac{\rho_K}{K} = o(1)$.