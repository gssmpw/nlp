\section{Introduction}\label{sec:intro}

We consider the smooth convex optimization problem
\begin{eqnarray*}
  \minf{x \in \mathbb{R}^n} & f (x), & 
\end{eqnarray*}
where $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and $L$-smooth with $f
(x^{\star}) \assign \min_x f (x) > - \infty$. Theoretically, gradient descent
\begin{equation*}%\label{eqn:gradient-descent}
	x^{k+1} = x^k - \alpha_k \nabla f(x^k)
\end{equation*}
 with constant stepsize $\alpha_k \equiv 1/L$ is guaranteed to converge. However, the choice of stepsize $\alpha_k$ strongly affects the performance of gradient descent in practice \cite{defazio2024road}, and various stepsize selection strategies have been proposed to improve the practical convergence of gradient descent. Examples include line-search \cite{armijo1966minimization}, Polyak stepsize \cite{polyak1987introduction}, stepsize scheduling  \cite{li2021second, wang2023convergence}, hypergradient descent \cite{almeida1999parameter,rubio2017convergence,gunes2018online} and  the well-known adaptive stepsizes \cite{orabona2016coin,duchi2011adaptive,kingma2014adam}.

Our paper focuses on the \emph{hypergradient descent method} (\hdm), which was initially proposed in \cite{almeida1999parameter} as a heuristic for stochastic optimization. It was later tested on modern machine learning problems and exhibited promising performance \cite{gunes2018online}. In {\hdm}, the stepsize $\alpha_k$ is adjusted by another gradient descent update:
\begin{align*}
\alpha_{k+1} &= \alpha_k - \tilde{\eta}_k \tfrac{\mathd}{\mathd \alpha}[f(x^k - \alpha \nabla f(x^k))]\big|_{\alpha = \alpha_k} = \alpha_k - \eta_k \tfrac{-\langle \nabla f(x^{k+1}), \nabla f(x^k) \rangle}{\| \nabla f(x^k) \|^2},
\end{align*}
where the hypergradient stepsize $\tilde{\eta}_k$ is often set to be $\tilde{\eta}_k = \tfrac{\eta_k}{\| \nabla f(x^k) \|^2}$ for $\eta_k > 0$ to be invariant of the scaling of $f$.
Gao et al. \cite{gao2024gradient} generalized {\hdm}
to support learning a \emph{preconditioned} gradient descent update with 
preconditioner (matrix stepsize) $P_k \in \Rbb^{n\times n}$
through the iteration
\begin{align}
    x^{k+1} ={} & x^k - P_k \nabla f(x^k), \label{eqn:pgd} \\
    P_{k+1} ={} & \Pi_{\Pcal} \big[P_k - \eta_k \tfrac{-\nabla f(x^{k+1}) \nabla f(x^k)^{\top}}{\| \nabla f(x^k) \|^2}\big], \label{eqn:hdm-Pk-update}
\end{align}
where \eqref{eqn:hdm-Pk-update} follows from $\nabla_P[ f(x^k - P \nabla f(x^k))] \big|_{P = P_k} = - \nabla f(x^{k+1}) \nabla f(x^k)^{\top}$ and $\Pi_{\Pcal}[\cdot]$ is orthogonal projection on to a compact set of candidate preconditioners $\Pcal$. Note that $P$ does not need to be positive definite \cite{gao2024gradient}, hence the projection is easy to compute in practice. We call the update \eqref{eqn:pgd}-\eqref{eqn:hdm-Pk-update} \emph{vanilla} {\hdm} throughout the paper. In practice, $P_k$ is often set to be diagonal and \eqref{eqn:hdm-Pk-update} simplifies to $P_{k+1} = P_k - \eta_k \tfrac{-\diag(\nabla f(x^{k+1}) \circ \nabla f(x^k))}{\| \nabla f(x^k) \|^2}$, where $\circ$ is entry-wise product.\\

\begin{figure}[t]
  \centering
  \begin{subfigure}{0.35\textwidth}
\includegraphics[height=0.2\textheight]{figs/demo_1.pdf}
    \caption{Two-phase behavior}
    \label{fig:demo:a}
  \end{subfigure}
  \begin{subfigure}{0.35\textwidth}~~
\includegraphics[height=0.2\textheight]{figs/demo_5.pdf}
    \caption{Addressing instability}
    \label{fig:demo:b}
  \end{subfigure}
\caption{The behavior of different {\hdm} variants on the toy quadratic optimization problem. \Cref{fig:demo:a}: two-phase convergence behavior of vanilla {\hdm}. \Cref{fig:demo:b}: effect of null step and our best variant {\hdmbest}. \label{fig-demos}}
  \label{fig:demo:instability}
\end{figure}

Vanilla {\hdm} is widely used in practice, but it can be unstable if the hypergradient stepsize $\eta_k$ is not carefully tuned \cite{kunstner2024searching,chandra2022gradient,rubio2017convergence}. 
\Cref{fig:demo:a} shows $f(x^k)$ can spike as high as $10^{30}$ in the early iterations of vanilla {\hdm}, which would lead experienced users to abandon the algorithm. 
Surprisingly, our analysis reveals that this behavior of {\hdm} is not true divergence; instead, it can be understood as the warm-up phase of an online learning procedure, and is followed by fast convergence (\Cref{fig:demo:a}).
Moreover, we show in both theory and practice that the explosion of $f(x^k)$ can be circumvented by taking a \emph{null step}, which skips the update whenever the new iterate fails to decrease the objective value, i.e., $f(x^k - P_k \nabla f(x^k)) \geq f(x^k)$.
The null steps flatten the objective value curve in the warm-up phase of {\hdm} but cannot shorten the warm-up (\Cref{fig:demo:b}).\\

Our analysis exploits the online learning framework in \cite{gao2024gradient}, in which the authors observe that the $P$-update \eqref{eqn:hdm-Pk-update} in vanilla {\hdm} can be viewed as online gradient descent with respect to the online surrogate loss
\begin{equation} \label{eqn:hypergrad-feedback}
h_x(P) \assign \tfrac{f ( x - P \nabla f (x) ) - f (x) }{\| \nabla f (x) \|^2}.
\end{equation}
The function $h_x(P)$, called \emph{hypergradient feedback} in this paper, is a function of preconditioner $P$ and is well-defined for all non-stationary $x$. 
To see that \eqref{eqn:hdm-Pk-update} aligns with the online gradient descent update, notice $\nabla h_{x^k}(P_k) = -\tfrac{\nabla f(x^{k+1}) \nabla f(x^k)^{\top}}{\| \nabla f(x^k) \|^2}$ so the update \eqref{eqn:hdm-Pk-update} sets $P_{k+1} = P_k - \eta_k \nabla h_{x^k} (P_k)$.
Using insights from our analysis, we develop a variant of {\hdm} 
based on {\adagrad} that improves convergence of {\hdm} (\Cref{fig:demo:b}) both in theory and practice. 

\begin{algorithm}[h]
  {\textbf{input} initial point $x^1, P_1 \in \Pcal$}\\
  \For{k =\rm{ 1, 2,...}}{
  $x^{k + 1} = \displaystyle \argmin_{x \in \{x^k, x^k - P_k \nabla f(x^k)\}} f(x) $\\
  $P_{k+1} = \Pi_{\mathcal{P}} [P_k - \eta_k \nabla h_{x^k} (P_k)]$\\
  }
  \caption{Hypergradient Descent Method (\hdm) \label{alg:hdm}}
\end{algorithm}

Vanilla {\hdm} + null steps (\Cref{alg:hdm}) was first considered by \cite{gao2024gradient} and guaranteed to converge globally.
However, their analysis is not sufficient to explain the practical behavior of {\hdm} and provides no advice for how to design a practically efficient {\hdm}.
In this paper, we dive deeper into the convergence behavior of {\hdm} (\Cref{alg:hdm}), establishing sharper global convergence guarantees and conducting a local convergence analysis. Our findings offer new insights into (vanilla) {\hdm} and serve as a foundation to design more efficient and practical variants of {\hdm}.
The contributions of this paper include:

\begin{itemize}[leftmargin=10pt]
    \item We provide the first rigorous convergence analysis for {\hdm}, including both global and local convergence guarantees (\Cref{sec:hdm}) that show {\hdm} can adapt to the local optimization landscape. Our analysis provides several new insights into how {\hdm} adapts to optimization landscapes (\Cref{sec:global-conv}), why vanilla {\hdm} is unstable in practice (\Cref{sec:instability}), and the connection between {\hdm} and quasi-Newton methods (\Cref{sec:local-conv}). 
    
    \item We develop and analyze two improved variants of {\hdm}: {\hdm} + heavy-ball momentum ({\hdmhb} in \Cref{sec:heavyball}), which has the same convergence rate as {\hdm} but is faster than {\hdm} in practice; and {\hdm} + Nesterov momentum ({\hdmagd} in \Cref{sec:nesterov}), which is faster in theory and intermediate between {\hdm} and {\hdmhb} in practice. 
    
    \item We develop a practically efficient variant {\hdmbest}, which updates $x^k$ by preconditioned gradient descent with heavy-ball momentum and jointly updates $P_k$ and momentum parameter by {\adagrad}.
    Our {\hdmbest} outperforms adaptive first-order methods and performs on par with \texttt{L-BFGS} (with memory size 5 or 10) using \emph{less memory} (memory size 1) (\Cref{sec:exp}).
\end{itemize}


\subsection{Related Literature}

\paragraph{Adaptive First-order Methods.}
Notable adaptive first-order methods include {\adagrad} \cite{duchi2011adaptive,mcmahan2010adaptive}, \texttt{Adam} \cite{kingma2014adam,zhang2024adam}, and parameter-free stepsizes \cite{orabona2016coin,defazio2024road}.
Most of these techniques originate in the online learning community, and they typically achieve both strong empirical convergence and online learning regret guarantees.

\paragraph{Hypergradient Descent.}
Hypergradient descent dates back to \cite{almeida1999parameter}, which was first proposed as a heuristic to accelerate stochastic gradient descent. 
Similar concepts were also explored in \cite{sutton1992adapting,schraudolph1999local,jacobs1988increased,mahmood2012tuning}, while those works employed slightly different algorithmic updates. 
Later, \cite{gunes2018online} rediscovered the {\hdm} and named it ``hypergradient descent''; \cite{gunes2018online} also extended {\hdm} to other first-order methods with extensive experimental validation of its practical efficacy.  Recent studies \cite{jie2022adaptive,chandra2022gradient,ozkara2024mada} further empirically enhanced {\hdm} for broader applicability, reporting promising numerical results.\\

Despite these empirical successes, a rigorous theoretical understanding of {\hdm} has emerged only recently. \cite{rubio2017convergence} showed that {\hdm} converges on convex quadratic functions and established several analytic properties. 
Subsequently, \cite{kunstner2024searching} demonstrated that when using a diagonal preconditioner, hypergradient can be employed to generate cutting planes in the preconditioner space, achieving an $\Ocal(\sqrt{n}\kappa^\star \log (1/\varepsilon))$ complexity result on smooth strongly convex functions. Here, $\kappa^\star$ is the condition number associated with the optimal diagonal preconditioner. 
More recently, \cite{gao2024gradient} showed that {\hdm} can be viewed as online gradient descent applied to some surrogate loss function and that {\hdm} has strong trajectory-based convergence guarantees.

\subsection{Notations}
We denote Euclidean norm by $\| \cdot \|$ and  Euclidean inner product by $\langle \cdot, \cdot \rangle$.
The upper and lower case letters $A, a$ respectively denote matrices and scalars. 
Denote the Frobenius norm by $\| A \|_F
\assign \sqrt{\sum_{i j} a_{i j}^2}$. Define $[\cdot]_+ := \max\{\cdot, 0\}$. We use
$\Pi_{\mathcal{C}} [\cdot]$ to denote the orthogonal projection onto a
closed convex set $\mathcal{C}$ and use $\tmop{dist}
(x, \mathcal{C}) \assign \| x - \Pi_{\mathcal{C}} [x] \|$ to denote the distance between a point $x$ and a closed convex set $\mathcal{C}$. 
Denote the optimal set of $f (x)$ by $\mathcal{X}^{\star} = \{ x : f (x) = f (x^{\star}) \}$; and the $\alpha$-sublevel set of $f$ by $\Lcal_\alpha := \{x: f(x) \leq \alpha\}$. 
For consistency of notation, a \textit{stepsize} $P$ in this paper always refers to a matrix applied in the gradient update.
Define $\mathcal{S} \assign \{ P = \alpha I : \alpha \in
\mathbb{R} \}$ and $\mathcal{D} \assign \{ P = \tmop{diag} (d) : d \in \mathbb{R}^n \}$. The condition number of an $L$-smooth and $\mu$-strongly convex function is $\kappa := L/\mu$.