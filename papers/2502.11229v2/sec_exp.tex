\section{Experiments} \label{sec:exp}

This section conducts numerical experiments to validate the empirical
performance of hypergradient descent. We compare {\hdmbest} (see \Cref{sec:hdmbest} below) with 
different adaptive optimization algorithms.

\subsection{Efficient and Practical Variant: {\hdmbest}} \label{sec:hdmbest}

This section highlights the major components of our most competitive variant {\hdmbest}. The algorithm and a more detailed explanation are available in \Cref{app:hdmprac}. The implementation is available at \url{https://github.com/udellgroup/hypergrad}. 

\paragraph{Diagonal Preconditioner and Heavy-ball Momentum.} {\hdmbest} updates $x$ by \eqref{eqn:heavyball-update} with diagonal preconditioner \cite{qu2024optimal,gao2023scalable} $\Pcal \subseteq \Dcal$ and scalar momentum $\Bcal = \{ \beta I : \beta \in \mathbb{R} \}$.
This choice balances practical efficiency and implementation complexity. Boundedness of $\Pcal$ does not greatly impact the performance, while the bound on $\Bcal$ can significantly change algorithm behavior. Two empirically robust ranges for $\Bcal$ are $[0,0.9995]$ and $[-0.9995,0.9995]$. 

\paragraph{$\adagrad$ for Online Learning. } {\hdmbest} uses {\adagrad} to shorten the warm-up phase for learning of $(P_k, \beta_k)$ (see \Cref{sec:instability}). {\adagrad} usually yields faster convergence of {\hdm} than online gradient descent at the cost of additional memory of size $n$.

\subsection{Dataset and Testing Problems}
We test {\hdmbest} on deterministic convex problems. We adopt two convex optimization tasks in machine learning: support vector machine \cite{lee2001ssvm} and logistic regression \cite{hastie2009elements}. The testing datasets are obtained from \texttt{LIBSVM} \cite{chang2011libsvm}.

\subsection{Experiment Setup}

\paragraph{Algorithm Benchmark.}
We benchmark the following algorithms.\\
\begin{itemize}[leftmargin=10pt,itemsep=2pt,topsep=0pt]
    \item \texttt{GD}. Vanilla gradient descent.
    \item \texttt{GD-HB}. Gradient descent with heavy-ball momentum. \cite{polyak1964some}
    \item \texttt{AGD-CVX}. The smooth convex version of accelerated gradient descent (Nesterov momentum). \cite{d2021acceleration}
    \item \texttt{AGD-SCVX}. The smooth strongly convex version of accelerated gradient descent. \cite{d2021acceleration}
    \item \texttt{Adam}. Adaptive momentum estimation. \cite{kingma2014adam}
    \item \texttt{AdaGrad}. Adaptive (sub)gradient method. \cite{duchi2011adaptive}
    \item \texttt{BFGS}. {\bfgs} from \texttt{scipy} \cite{nocedal1999numerical,virtanen2020scipy}.
    \item \texttt{L-BFGS-Mk}. {\lbfgs} with memory size \texttt{k} in \texttt{scipy}.
    \item Practical variant {\hdmbest} uses as memory $7$ vectors of size $n$, comparable to memory for \texttt{L-BFGS-M1}.
\end{itemize}

\paragraph{Algorithm Configuration.} See \Cref{app:hdmprac} for details.
\begin{itemize}[leftmargin=10pt]
  \item For {\hdmbest}, we search for the optimal $\eta_p$ within $\{ 0.1 / L, 1 / L, 10 / L, 100/L \}$
  and $\eta_b \in \{ 1, 3, 5, 10, 100 \}$. 
  
  \item Stepsize in \texttt{GD}, \texttt{GD-HB},
  \texttt{AGD-CVX}, and \texttt{AGD-SCVX} are all set to $1 / L$.
  
  \item The momentum parameter in \texttt{GD-HB} is chosen within the set $\{
  0.1, 0.5, 0.9, 0.99 \}$.
  
  \item The \texttt{Adam} stepsize is chosen within the set $\{ 1 / L, 10^{- 3},
  10^{- 2}, 10^{- 1}, 1, 10 \}$. $\beta_1 = 0.9, \beta_2 = 0.999$.
  
  \item  The \texttt{AdaGrad} stepsize is chosen within the set $\{ 1 / L, 10^{- 3},
  10^{- 2}, 10^{- 1}, 1, 10 \}$.
  
  \item {\bfgs}, \texttt{L-BFGS-Mk} use default parameters in
  \texttt{scipy}.
\end{itemize}

\paragraph{Testing Configurations.}

\begin{enumerate}[leftmargin=15pt,label=\textbf{\arabic*)}]
  \item {\textit{Maximum oracle access.}} We allow a maximum of 1000 gradient
  oracles for each algorithm.
  
  \item {\textit{Initial point}}. All the algorithms are initialized from the
  same starting point generated from normal distribution $\mathcal{N} (0,
  I_n)$ and normalized to have unit length.
  
  \item {\textit{Stopping criterion.}} Algorithms stop if $\| \nabla f 
  \|_\infty \leq 10^{- 4}$.
\end{enumerate}

\begin{table}[h]
\centering
\caption{Number of solved problems for each algorithm. \label{table:stats}}
\begin{tabular}{ccc}
\toprule
    Algorithm/Problem & SVM (33) $\uparrow$ & Logistic Regression (33) $\uparrow$\\
\midrule
    \texttt{GD} & 5 & 2\\
    \texttt{GD-HB} & 9 & 7\\
    \texttt{AGD-CVX} & 8 & 3\\
    \texttt{AGD-SCVX} & 7 & 6\\
    \texttt{Adam} & 26 & 11\\
    \texttt{AdaGrad} & 9 & 8\\
    \texttt{L-BFGS-M1} & 13 & 11\\
    \texttt{L-BFGS-M3} & 20 & 14\\
    \texttt{L-BFGS-M5} & 26 & 16\\
    \texttt{L-BFGS-M10} & \textcolor{orange}{31} & 18\\
    \texttt{BFGS} & \textcolor{red}{32} & \textcolor{red}{26}\\
    \texttt{HDM-Best} & \textcolor{red}{32} & \textcolor{red}{21}\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[!h]
\centering
\includegraphics[scale=0.2]{figs/a1a_objval_svm.pdf}
\includegraphics[scale=0.2]{figs/a9a_objval_svm.pdf}
\includegraphics[scale=0.2]{figs/w8a_objval_svm.pdf}
\includegraphics[scale=0.2]{figs/svmguide3_objval_svm.pdf}
\includegraphics[scale=0.2]{figs/a1a_gnorm_svm.pdf}
\includegraphics[scale=0.2]{figs/a9a_gnorm_svm.pdf}
\includegraphics[scale=0.2]{figs/w8a_gnorm_svm.pdf}
\includegraphics[scale=0.2]{figs/svmguide3_gnorm_svm.pdf}
\includegraphics[scale=0.4]{figs/legend.pdf}
\caption{Support vector-machine problems. First row: function value gap. Second row: gradient norm. \label{fig:svm}}
\end{figure*}

\begin{figure*}[!h]
\centering
\includegraphics[scale=0.2]{figs/a1a_objval_logistic.pdf}
\includegraphics[scale=0.2]{figs/w4a_objval_logistic.pdf}
\includegraphics[scale=0.2]{figs/ionosphere_scale_objval_logistic.pdf}
\includegraphics[scale=0.2]{figs/ijcnn1_objval_logistic.pdf}
\includegraphics[scale=0.2]{figs/a1a_gnorm_logistic.pdf}
\includegraphics[scale=0.2]{figs/w4a_gnorm_logistic.pdf}
\includegraphics[scale=0.2]{figs/ionosphere_scale_gnorm_logistic.pdf}
\includegraphics[scale=0.2]{figs/ijcnn1_gnorm_logistic.pdf}
\caption{Logistic regression problems. First row: function value gap. Second row: gradient norm.  \label{fig:logistic}}
\end{figure*}

For each algorithm, we record the number of successfully solved instances ($\|\nabla f\|_\infty \leq 10^{-4}$ within 1000 gradient oracles). \Cref{table:stats} summarizes the detailed statistics. The number of instances solved by {\hdmbest} is comparable to that of \texttt{L-BFGS-M10}.

\paragraph{Support Vector Machine.} \Cref{fig:svm} shows the function value gap and gradient norm plots on sample test instances on support vector machine problems. The optimal value for each instance is obtained by running {\bfgs} until $\|\nabla f \|_\infty \leq 10^{-4}$.  We see that the practical variant of {\hdmbest} achieves a significant speedup over other adaptive first-order methods. In particular, {\hdmbest} often matches \texttt{L-BFGS-M5} and \texttt{L-BFGS-M10}, while its memory usage is closer to \texttt{L-BFGS-M1}. Notably, {\adam} also achieves competitive performance in several instances.

\paragraph{Logistic Regression.} In logistic regression (\Cref{fig:logistic}), {\hdmbest} still compares well with \texttt{L-BFGS-M5} and is significantly faster than other adaptive first-order methods.\\

Overall, {\hdmbest} demonstrates superior performance on deterministic convex problems and is comparable with the mature \texttt{L-BFGS} family. We believe that further development of {\hdm} will fully unleash its potential for a broad range of optimization tasks.
