\subsection{{\hdm} + Heavy-ball Momentum ({\hdmhb})} \label{app:heavy-ball}

\Cref{alg:ospolyak} uses the following \emph{heavy-ball feedback function} to guide the online learning for $(P_k, B_k)$:
\begin{equation*}
  h_{x, x^-} (P, B) \assign 
  \tfrac{\psi(x^{+}, x) - \psi(x, x^{-})}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}
  = \tfrac{[f (x^+) + \frac{\omega}{2} \| x^+ - x \|^2] - [f (x) + \frac{\omega}{2} \| x - x^- \|^2]}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}, 
\end{equation*}
where $\omega > 0$, $ \tau > 0$, $x^{+} = x - P \nabla f (x) + B (x - x^{-})$, and $\psi(x, x^-) = f (x) + \tfrac{\omega}{2} \| x - x^- \|^2$.
To show that online learning can be applied to $h_{x, x^-} (P, B)$ with regret guarantees, we need to verify the convexity and Lipschitz continuity of $h_{x, x^-} (P, B)$ with respect to the norm defined by
\begin{equation} \label{eqn:PB-norm}
  \|(P, B)\| \assign \sqrt{\|P\|_F^2 + \|B\|_F^2}.
\end{equation}


\begin{lem} \label{lem:heavyball-feedback-property}
Under \ref{A1}, \ref{A2}, and \ref{ABcal}, the heavy-ball feedback function $h_{x, x^{-}}(P, B)$ is jointly convex in $(P, B)$ and $c$-Lipschitz with respect to the norm defined in \eqref{eqn:PB-norm}, where $c \assign \sqrt{2} (1+\tfrac{2}{\tau}) [1 + 2(1+\tfrac{2}{\tau}) D (L+\omega)]$.
\end{lem}
\begin{proof}
Denote $x^{+}(P, B) \assign x - P \nabla f(x) + B (x - x^{-})$. 
Recall that the feedback function is
\begin{equation*}
h_{x, x^{-}}(P, B) = \tfrac{[f (x^+(P, \beta)) + \frac{\omega}{2} \| x^+(P, \beta) - x \|^2] - [f (x) + \frac{\omega}{2} \| x - x^- \|^2]}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}.
\end{equation*}
Since $x^{+}(P, B)$ is affine in $(P, B)$ and $f$ is convex, the term $f (x^+(P, \beta)) + \frac{\omega}{2} \| x^+(P, \beta) - x \|^2$ is jointly convex as a function of $(P, B)$. The other terms in the feedback function $h_{x, x^{-}}(P, B)$ are constants, so $h_{x, x^{-}}(P, B)$ is also jointly convex in $(P, B)$.\\

To prove the Lipschitz continuity of $h_{x, x^{-}}(P, B)$, it suffices to show that the gradients of $h_{x, x^{-}}(P, B)$ are bounded. 
The gradients of $h_{x, x^{-}}(P, B)$ with respect to $P$ and $B$ are
\begin{align*}
\nabla_P h_{x, x^{-}}(P, B) & = \tfrac{[- \nabla f(x^{+}(P, B)) + \omega P \nabla f(x) - \omega B (x - x^{-})] \nabla f(x)^{\top}}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2} , \\
\nabla_B h_{x, x^{-}}(P, B) & = \tfrac{[\nabla f(x^{+}(P, B)) - \omega P \nabla f(x) + \omega B (x - x^{-})](x - x^{-})^{\top}}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}.
\end{align*}
Using the fact $\|a b^{\top}\|_F = \|a\| \cdot \|b\|$, the gradients have norms
\begin{align}
\|\nabla_P h_{x, x^{-}}(P, B)\|_F & = \tfrac{\| \nabla f(x^{+}(P, B)) - \omega P \nabla f(x) + \omega B (x - x^{-})\| \|\nabla f(x)\|}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}, \label{eqn:proof-4-1-1} \\
\|\nabla_B h_{x, x^{-}}(P, B)\|_F & = \tfrac{\| \nabla f(x^{+}(P, B)) - \omega P \nabla f(x) + \omega B (x - x^{-})\|\|x - x^{-}\|}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}. \label{eqn:proof-4-1-2}
\end{align}
Using \ref{A1}, we have the Lipschitz continuity of $\nabla f(x)$ and thus
\begin{align}
&\| \nabla f(x^{+}(P, B)) - \omega P \nabla f(x) + \omega B (x - x^{-})\| \nonumber \\
\leq{} & \|\nabla f(x^{+}(P, B)) - \nabla f(x)\| + \| (I - \omega P) \nabla f(x)\| + \omega \|B\| \|x - x^{-}\| \nonumber \\
\leq{} & L \| P \nabla f(x) - B (x - x^{-}) \| + (1 + \omega \|P\| ) \|\nabla f(x)\| + \omega \|B\| \|x - x^{-}\| \nonumber\\
\leq{} & L D (\| \nabla f(x) \| + \| x - x^{-} \|) + (1 + \omega D ) \|\nabla f(x)\| + \omega D \|x - x^{-}\| \nonumber \\
={} &(1 + LD + \omega D ) \|\nabla f(x)\| + (\omega + L) D \|x - x^{-}\|. \label{eqn:proof-4-1-3}
\end{align}
Now, we bound the norms in \eqref{eqn:proof-4-1-1}--\eqref{eqn:proof-4-1-2} by the case analysis.

\paragraph{Case 1.} If $\frac{\tau}{2} \| x - x^- \|^2 \leq \| \nabla f (x) \|^2$, then together with \eqref{eqn:proof-4-1-3}, we have
\begin{align*}
  \max \{ \|\nabla_P h_{x, x^{-}}(P, B)\|_F, \|\nabla_B h_{x, x^{-}}(P, B)\|_F \}  
  &\leq \tfrac{[(1 + LD + \omega D ) \|\nabla f(x)\| + (\omega + L) D \|x - x^{-}\|] \max\{ \sqrt{2\tau^{-1}}, 1\} \|\nabla f(x)\|}{\| \nabla f (x) \|^2} \\
  &\leq \max\{ \sqrt{2\tau^{-1}}, 1\} [(1 + LD + \omega D ) + \tfrac{\sqrt{2} D (\omega + L)}{\sqrt{\tau}}] \\
  &= \max\{ \sqrt{2\tau^{-1}}, 1\} (1 + D(L+\omega)(1+\sqrt{2\tau^{-1}})).
\end{align*}

\paragraph{Case 2.} If $\frac{\tau}{2} \| x - x^- \|^2 \geq \| \nabla f (x) \|^2$, then
\begin{align*}
  \max \{ \|\nabla_P h_{x, x^{-}}(P, B)\|_F, \|\nabla_B h_{x, x^{-}}(P, B)\|_F \}  
  &\leq \tfrac{[(1 + LD + \omega D ) \|\nabla f(x)\| + (\omega + L) D \|x - x^{-}\|] \max\{ \sqrt{\frac{\tau}{2}}, 1\} \|x - x^-\|}{\frac{\tau}{2}\|x - x^- \|^2} \\
  &\leq \max\{ \sqrt{\tau/2}, 1\}[\tfrac{ \sqrt{2} (1 + LD + \omega D )}{\sqrt{\tau}} + \tfrac{2 D (\omega + L)}{\tau}] \\
  &= \sqrt{2\tau^{-1}} \max\{ \sqrt{2\tau^{-1}}, 1\} (1 + D(L+\omega)(1+\sqrt{2\tau^{-1}})).
\end{align*}

Combining the two cases, we have
\begin{align*}
  \max \{ \|\nabla_P h_{x, x^{-}}(P, B)\|_F, \|\nabla_B h_{x, x^{-}}(P, B)\|_F \}  
  &\leq \max\{ \tfrac{2}{\tau}, 1\} (1 + D(L+\omega)(1+\sqrt{2\tau^{-1}})) \\
  &\leq (1+\tfrac{2}{\tau}) [1 + 2(1+\tfrac{2}{\tau}) D (L+\omega)].
\end{align*}
Then the gradient of $h_{x, x^{-}}(P, B)$ under the norm defined in \eqref{eqn:PB-norm} is bounded by the constant $c \assign \sqrt{2} (1+\tfrac{2}{\tau}) [1 + 2(1+\tfrac{2}{\tau}) D (L+\omega)]$.
\end{proof}

The next lemma bounds the potential at the last iterate $x^{K+1}$ from \Cref{alg:ospolyak} in terms of the sum of feedback functions $h_{x^k, x^{k-1}}(P_k, B_k)$.

\begin{lem} \label{lem:heavyball-conversion}
  The sequence $\{x^k\}$ generated from \Cref{alg:ospolyak} satisfies
  \begin{equation} \label{eqn:potential-bound}
    f(x^{K+1}) - f(x^{\star}) + \tfrac{\omega}{2} \|x^{K+1} - x^K\|^2 \leq \tfrac{f (x^1) - f (x^{\star})}{1 + \sum_{k = 1}^K
    \max \{ - h_{x^k, x^{k - 1}} (P_k, B_k), 0 \} V},
  \end{equation}
  where $V \assign \min \big\{\tfrac{f (x^1) - f (x^{\star})}{4 \Delta^2}, \tfrac{\tau}{4 \omega} \big\}$ and $\Delta \assign \max_{x \in \Lcal_{f(x^1)}} \min_{x^{\star} \in \mathcal{X}^{\star}} \| x - x^{\star} \|$.
\end{lem}

\begin{proof}
  The null step guarantees
\[ \tfrac{\psi (x^{k + 1}, x^k) - \psi (x^k, x^{k - 1})}{\| \nabla f
   (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1} \|^2} = \min \{ h_{x^k, x^{k
   - 1}} (P_k, B_k), 0 \}. \]
Using the initial condition $x^1 = x^0$, we have
\begin{align}
\psi (x^{K + 1}, x^K) - f (x^{\star})   ={} & \frac{1}{\frac{1}{\psi (x^{K + 1}, x^K) - f (x^{\star})}} \nonumber\\
  ={} & \frac{1}{\sum_{k = 1}^K \frac{1}{\psi (x^{k + 1}, x^k) - f
  (x^{\star})} - \frac{1}{\psi (x^k, x^{k - 1}) - f (x^{\star})} +
  \frac{1}{\psi (x^1, x^0) - f (x^{\star})}} \nonumber\\
  ={} & \frac{1}{\sum_{k = 1}^K \frac{\psi (x^k, x^{k - 1}) - \psi (x^{k +
  1}, x^k)}{[\psi (x^{k + 1}, x^k) - f (x^{\star})] [\psi (x^k, x^{k -
  1}) - f (x^{\star})]} + \frac{1}{\psi (x^1, x^0) - f (x^{\star})}}
  \nonumber\\
  ={} & \frac{1}{\sum_{k = 1}^K \frac{\max \{ - h_{x^k, x^{k - 1}} (P_k,
  B_k), 0 \} [ \| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k
  - 1} \|^2 ]}{[\psi (x^{k + 1}, x^k) - f (x^{\star})] [\psi (x^k,
  x^{k - 1}) - f (x^{\star})]} + \frac{1}{f (x^1) - f (x^{\star})}} \label{eqn:proof-4-2-1}.
\end{align}

Then, by monotonicity, $\tfrac{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k
- x^{k - 1} \|^2}{[\psi (x^{k + 1}, x^k) - f (x^{\star})] [\psi (x^k,
x^{k - 1}) - f (x^{\star})]} \geq \tfrac{\| \nabla f (x^k) \|^2 +
\frac{\tau}{2} \| x^k - x^{k - 1} \|^2}{[\psi (x^k, x^{k - 1}) - f
(x^{\star})]^2}$.\\

Now we do case analysis to bound
\[ \tfrac{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1}
   \|^2}{[\psi (x^k, x^{k - 1}) - f (x^{\star})]^2} = \tfrac{\| \nabla f
   (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1} \|^2}{[ f (x^k) +
   \frac{\omega}{2} \| x^k - x^{k - 1} \|^2 - f (x^{\star}) ]^2} \]
\paragraph{Case 1.} If $\frac{\omega}{2} \| x^k - x^{k - 1} \|^2 \leq f (x^k) -
f (x^{\star})$, then
\begin{equation*}
\tfrac{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1} \|^2}{[ f (x^k) + \frac{\omega}{2} \| x^k - x^{k - 1} \|^2 - f(x^{\star}) ]^2} \geq \tfrac{\| \nabla f (x^k) \|^2}{4 [f (x^k) - f (x^{\star})]^2} \geq \tfrac{1}{4 \Delta^2},
\end{equation*}
where $\Delta \assign \max_{x \in \Lcal_{f(x^1)}} \min_{x^{\star} \in \mathcal{X}^{\star}} \| x - x^{\star} \|$.

\paragraph{Case 2.} If $\frac{\omega}{2} \| x^k - x^{k - 1} \|^2 \geq f (x^k) -
f (x^{\star})$, then $\frac{\tau}{2} \| x^k - x^{k - 1} \|^2 \geq
\frac{\tau}{\omega} [f (x^k) - f (x^{\star})]$ and
\[ \tfrac{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k - x^{k - 1}
   \|^2}{[ f (x^k) + \frac{\omega}{2} \| x^k - x^{k - 1} \|^2 - f
   (x^{\star}) ]^2} \geq \tfrac{\frac{\tau}{2} \| x^k - x^{k - 1}
   \|^2}{\omega^2 \| x^k - x^{k - 1} \|^4} = \tfrac{\tau}{2 \omega^2}
   \tfrac{1}{\| x^k - x^{k - 1} \|^2} \geq \tfrac{\tau}{4 \omega} \tfrac{1}{f
   (x^1) - f (x^{\star})} . \]
since $\frac{\omega}{2} \| x^k - x^{k - 1} \|^2 \leq \psi (x^k, x^{k - 1})
- f (x^{\star}) \leq \psi (x^1, x^0) - f (x^{\star}) = f (x^1) - f
(x^{\star})$.\\

In both cases, we have $\tfrac{\| \nabla f (x^k) \|^2 + \frac{\tau}{2} \| x^k
- x^{k - 1} \|^2}{[\psi (x^k, x^{k - 1}) - f (x^{\star})]^2} \geq \min
\{ \tfrac{1}{4 \Delta^2}, \frac{\tau}{4 \omega} \tfrac{1}{f (x^1) - f (x^{\star})} \} = \tfrac{V}{f(x^1) - f(x^\star)}$, where the constant $V$ is defined in the lemma.
Finally, plugging in the definition of $\psi$, \eqref{eqn:proof-4-2-1} gives
\[f(x^{K+1}) - f(x^{\star}) + \tfrac{\omega}{2} \|x^{K+1} - x^K\|^2 \leq \tfrac{f (x^1) - f (x^{\star})}{1 + \sum_{k = 1}^K
\max \{ - h_{x^k, x^{k - 1}} (P_k, B_k), 0 \} V}.\]
\end{proof}
The next lemma shows that there exist hindsight $\bar{P}, \bar{B}$ such that $h_{x, x^-} (\bar{P}, \bar{B}) \leq - \theta < 0$ for some $\theta$.

\begin{lem} \label{lem:heavyball-hindsight}
Let $\omega = 3 L$ and $\tau = 16 L^2$. Then for any $x, x^- \nin  \mathcal{X}^{\star}$, we have $h_{x, x^-} ( \tfrac{1}{4 L} I, \tfrac{1}{2} I ) \leq - \tfrac{1}{8 L}$. 
In particular, if $\tfrac{1}{4L}I \in \Pcal$, $\tfrac{1}{2}I \in \Bcal$, and $\{x^k\}_{k=1}^K \cap \mathcal{X}^{\star} = \varnothing$, then 
\begin{equation*}
  \gamma_K^{\star} \assign - \min_{(P, B) \in \mathcal{P} \times \mathcal{B}} \tfrac{1}{K} \textstyle \sum_{k=1}^K h_{x^k, x^{k-1}}(P, B) \geq \tfrac{1}{8 L}.
\end{equation*}
\end{lem}
\begin{proof}
When $P = \alpha I$ and $B = \beta I$ for some $\alpha, \beta > 0$, the classical analysis for the heavy-ball momentum \cite{danilova2020non} gives
\begin{equation*}
f (x^+) + \tfrac{1 - \alpha L}{2 \alpha} \| x^+ - x \|^2 \leq f (x) + \tfrac{\beta^2}{2 \alpha} \| x - x^- \|^2 - \tfrac{\alpha}{2} \| \nabla f (x) \|^2 .
\end{equation*}
Let $\alpha = \tfrac{1}{4 L}$ and $\beta = \tfrac{1}{2}$, we have
\begin{align}
  f (x^+) + \tfrac{3 L}{2} \| x^+ - x \|^2 
  \leq{}& f (x) + \tfrac{L}{2} \| x -
  x^- \|^2 - \tfrac{1}{8 L} \| \nabla f (x) \|^2 \nonumber\\
  ={}& f (x) + \tfrac{3 L}{2} \| x - x^- \|^2 - \tfrac{1}{8 L} \| \nabla f (x)
  \|^2 - L \| x - x^- \|^2 \nonumber\\
  ={}& f (x) + \tfrac{3 L}{2} \| x - x^- \|^2 - \tfrac{1}{8 L} [\| \nabla f (x)
  \|^2 + 8 L^2 \| x - x^- \|^2] \nonumber
\end{align}

and re-arranging the terms, we get
\[ \tfrac{f (x^+) + \frac{3 L}{2} \| x^+ - x \|^2 - [ f (x) + \frac{3
   L}{2} \| x - x^- \|^2 ]}{\| \nabla f (x) \|^2 + 8 L^2 \| x - x^-
   \|^2} \leq - \tfrac{1}{8 L} \]
and this completes the proof.
\end{proof}


\subsubsection{Proof of Theorem \ref{thm:heavyball}}

By \Cref{lem:heavyball-feedback-property}, the heavy-ball feedback is convex and Lipschitz, and thus the same proof of \Cref{lem:regret-sublinear} guarantees that online gradient descent 
\begin{equation*}
  (P_{k + 1}, B_{k+1}) = \Pi_{\mathcal{P} \times \mathcal{B}} [(P_k, B_k) - \eta \nabla h_{x^k, x^{k-1}} (P_k, B_k)]
\end{equation*}
(with $\eta_p = \eta_b = \eta)$ gives the regret bound
\begin{equation*}
  \tfrac{1}{K} \textstyle \sum_{k=1}^K - h_{x^k, x^{k-1}}(P_k, B_k) \geq 
  \gamma_K^{\star} - \tfrac{\rho_K}{K}
\end{equation*}
for some sublinear regret $\rho_K = \mathcal{O}(\sqrt{K})$ and the constant $\gamma_K^{\star}$ as defined in \Cref{lem:heavyball-hindsight}. 
Using the inequality
\begin{equation*}
  \tfrac{1}{K} \textstyle \sum_{k=1}^K \max \{ - h_{x^k, x^{k-1}}(P_k, B_k), 0 \}
  \geq \max \big \{\tfrac{1}{K} \textstyle \sum_{k=1}^K - h_{x^k, x^{k-1}}(P_k, B_k), 0 \big\} \geq \max \{ \gamma_K^{\star} - \tfrac{\rho_K}{K}, 0 \},
\end{equation*}
the desired result follows directly from \eqref{eqn:potential-bound} in \Cref{lem:heavyball-conversion}.
