\section{Conclusion}
This paper addresses the long-standing challenge of establishing convergence of the hypergradient descent heuristic. We provide the first rigorous theoretical foundation for hypergradient descent and introduce a novel online learning perspective that extends to other first-order methods with adaptive hyperparameter updates. Our theoretical advances support effective and scalable enhancements that allow the (first-order) {\hdm} to achieve superlinear convergence with guarantees that resemble quasi-Newton methods. 
Building on these results, we propose {\hdmbest}, an efficient variant of {\hdm} that performs competitively with the widely used \texttt{L-BFGS} method on convex problems. This empirical success positions {\hdm} as a compelling alternative for modern machine learning. Extending the theory of {\hdm} to stochastic and nonconvex optimization is a crucial next step to understanding its potential to speed up the training of large-scale models.
