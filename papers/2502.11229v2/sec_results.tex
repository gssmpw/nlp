\section{The Convergence Behavior of {\hdm} \label{sec:hdm}}

This section presents our main convergence results on {\hdm} and consequent insights. All the analyses are based on the online learning framework established in \Cref{sec:hdm-ol}. Unless specified, we assume the online gradient descent in {\hdm} (\Cref{alg:hdm}) uses the constant stepsize $\eta_k \equiv \eta > 0$ throughout this section.

\subsection{{\hdm} Adapts to the Local  Landscape} \label{sec:global-conv}

Our first convergence result follows by combining \Cref{lem:hypergrad-to-online} and \Cref{lem:regret-sublinear}:
\begin{thm}[Static adaptivity]\label{thm:adaptivity}
Under \ref{A1} and \begin{rm}{(\ref{A1} + \ref{Ascvx})}\end{rm} respectively, \Cref{alg:hdm} satisfies
\begin{align}
f (x^{K + 1}) - f (x^{\star}) & \leq{} \min \{ \tfrac{\Delta^2}{K \max\{ \gamma_K^{\star} - \frac{\rho_K}{K}, 0 \} }, f (x^1) - f (x^{\star}) \} \tag{\ref{A1}} \\
    f (x^{K + 1}) - f (x^{\star}) & \leq{} [f (x^1) - f (x^{\star})] ( 1 - 2 \mu \max\{ \gamma_K^{\star} - \tfrac{\rho_K}{K}, 0 \}
    )^K, \tag{\ref{A1} + \ref{Ascvx}}
\end{align}
where $\Delta$ is the same as defined in \Cref{lem:hypergrad-to-online}, $\rho_K$ is defined in \eqref{eqn:ogd-constant}, and $\gamma_K^{\star} \assign - \min_{P \in \mathcal{P}}  \tfrac{1}{K} \textstyle\sum_{k = 1}^K h_{x^k}(P)$.
\end{thm}

\Cref{thm:adaptivity} has two implications: 
1) Since $\gamma_K^\star \geq - \frac{1}{K}\sum_{k = 1}^K  h_{x^k} (\frac{1}{L}I) \geq \frac{1}{2L}$ (by descent lemma) and $\tfrac{\rho_K}{K} = o(1)$, both upper bounds in \Cref{thm:adaptivity} converge to $0$ when $K$ goes to infinity, guaranteeing global convergence of {\hdm}.
2) More importantly, $\gamma_K^\star$ reflects the possibly improved convergence rate of {\hdm} through the adaptive $P$-update, which depends on the local optimization landscape.
To see this, when $K$ is large and $\tfrac{\rho_K}{K}$ is negligible, the convergence of {\hdm} is competitive with preconditioned gradient descent \eqref{eqn:pgd} with any \emph{static} preconditioner. In particular, the optimal $P_k \equiv P_K^{\star} \assign \argmin_{P \in \mathcal{P}}  \tfrac{1}{K} \textstyle\sum_{k = 1}^K h_{x^k}(P)$ achieves the rate $\tfrac{\Delta^2}{K \gamma_K^\star}$.
Note that $\gamma_K^\star$ (or $P_K^\star$) depends only on the past trajectory $\{x^k\}_{k \leq K}$; and thus if the algorithm visits a local region with a smaller smoothness constant than the global constant $L$, one can expect $\gamma_K^\star \gg \frac{1}{2L}$. Adaptivity leads to faster convergence than standard gradient descent.
In summary, {\hdm} \emph{adapts to the local optimization landscape}.\\

We borrow a standard dynamic regret argument in online convex optimization literature \cite{hazan2016introduction} to provide an even stronger notion of adaptivity of {\hdm}:
\begin{thm}[Dynamic adaptivity]\label{thm:dynamic-adaptivity}
Under \ref{A1} and \begin{rm}{(\ref{A1} + \ref{Ascvx})}\end{rm} respectively, \Cref{alg:hdm} satisfies
\begin{align}
f (x^{K + 1}) - f (x^{\star}) & \leq{} \min \big\{ \tfrac{\Delta^2}{K \max\{ \delta_K^{\star} - \frac{\rho_K}{K}, 0 \} }, f (x^1) - f (x^{\star}) \big\}; \tag{\ref{A1}} \\
f (x^{K + 1}) - f (x^{\star}) & \leq{} (f (x^1) - f (x^{\star})) ( 1 - 2 \mu \max\{ \delta_K^{\star} - \tfrac{\rho_K}{K}, 0 \} )^K, \tag{\ref{A1} + \ref{Ascvx}}
\end{align}
where $\Delta$ is the same as defined in \Cref{lem:hypergrad-to-online},
\begin{align}\label{eqn:dynamic-regret}
\delta_K^{\star} \assign -\min_{\{\hat{P}_k \in \Pcal\}}  \big[ \tfrac{1}{K} \textstyle\sum_{k = 1}^K h_{x^k} (\hat{P}_k) + \mathsf{PL}(\{\hat{P}_k\}) \big], 
\end{align}
and $\mathsf{PL}(\{\hat{P}_k\}):=\tfrac{(L D + 1)}{2 \sqrt{K}} \textstyle\sum_{k = 1}^K \| \hat{P}_{k + 1} - \hat{P}_k \|_F$.
\end{thm}

\Cref{thm:adaptivity} and \Cref{thm:dynamic-adaptivity} differ in the constants $\gamma_K^\star$ and $\delta_K^\star$, as the minimum in \eqref{eqn:dynamic-regret} searches over different optimal preconditioners for different $h_{x^k}$. \Cref{thm:dynamic-adaptivity} shows that, even if the sequence $\{ x^k \}$ traverses different regions of the landscape, 
{\hdm} automatically chooses $\hat{P}_k$
to adapt to the local region, at the price of an additional regret term $\mathsf{PL}(\{\hat{P}_k\})$. 
Adaptivity of {\hdm} undergirds its good empirical performance.

\subsection{Online Regret and Instability} \label{sec:instability}

Though adaptive $P$-update underpins the strong performance of {\hdm}, vanilla {\hdm} is observed unstable in practice. 
This section identifies the source of instabilty in vanilla {\hdm} (\Cref{fig:demo:instability}) based on our analysis. We also propose two simple yet effective strategies to address the instability.

 
\paragraph{Divergence Behavior due to Regret.} 
Recall from \Cref{thm:adaptivity} that the optimality gap at $x^{K + 1}$ is bounded by $\tfrac{\Delta^2}{K \max\{ \gamma_K^{\star} - \frac{\rho_K}{K}, 0 \} }$.
This rate can be better than that of gradient descent when $K$ is large and $\gamma_K^{\star} \gg \tfrac{1}{2L}$, but the analysis provides no guarantee on earlier iterates $\{ x^k \}_{k \leq K}$.
In particular, the convergence rate makes sense only if $\gamma_K^{\star} > \frac{\rho_K}{K}$. That is, the progress $\sum_k h_{x^k} (P_k)$ accumulated by the online gradient descent outweighs its regret $\rho_K$. 
In other words, online gradient descent takes time to learn a good preconditioner, and the regret accumulated during this warm-up phase causes {\hdm} to behave as if it is diverging until the progress $\sum_k h_{x^k} (P_k)$ outpaces the regret $\rho_K$. 
Since $\rho_K$ grows sublinearly with the iteration count $K$, {\hdm} will eventually converge. 
However, the objective value will usually explode (and be terminated by the user) before convergence begins.
Consequently, the two-phase convergence behavior (\Cref{fig:demo:a}) is rarely observed.

\paragraph{Addressing Instability.} While our analysis guarantees {\hdm}  eventually converges, an algorithm that diverges up to $10^{30}$ before converging is not practical. We propose two simple but effective fixes based on our analysis:

\begin{itemize}[leftmargin=10pt]
\item \emph{Null step.} 
The $x$-update is skipped if the new iterate increases the objective value:
 \begin{equation} \label{eqn:null-step}
 x^{k + 1} = \displaystyle \argmin_{x \in \{x^k, x^k - P_k \nabla f(x^k)\}} f(x).
 \end{equation}
The null step ensures a monotonic decrease as {\hdm} learns a good preconditioner, although it requires an additional function value oracle call at each iteration. 
Even on iterations when $x^k$ is not updated, 
the preconditioner $P_k$ is updated using online gradient descent, so the algorithm is still making progress.
In \Cref{fig:instablity}, the null steps  flatten the objective value curve in the divergence phase.

\item \emph{Advanced Learning Algorithms.} Better online learning algorithms with lower regret shorten the divergence phase. \Cref{fig:instablity} shows a significant speedup when the online gradient descent in \Cref{alg:hdm} is replaced by \texttt{AdaGrad}.
In our experiments, {\adagrad} often improves the robustness of {\hdm} since it does not require pre-specifying algorithm parameters that depend on the total iteration count $K$
and provides convergence guarantees for the earlier iterates $\{ x^k \}_{k \leq K}$ (\Cref{sec:intermediate-iter}).
\end{itemize}
\begin{figure}
\centering
\includegraphics[scale=0.3]{figs/demo_2.pdf}	
\caption{Addressing instability of {\hdm} \label{fig:instablity}}
\end{figure}
\subsection{Local Superlinear Convergence} \label{sec:local-conv}
\Cref{fig:demo:instability} shows {\hdm} converges faster than the (linearly convergent) first-order methods. In fact, {\hdm} exhibits local superlinear convergence on strongly convex objectives (\Cref{thm:superlin} below). Results in this subsection assume a strongly convex objective (\ref{Ascvx}) and Lipschitz Hessian (\ref{A3}):
\begin{enumerate}[leftmargin=30pt,label=\textbf{A\arabic*:},ref=\rm{\textbf{A\arabic*}},start=4]
  \item $f (x)$ has $H$-Lipschitz Hessian. \label{A3}
\end{enumerate}

\paragraph{Strongly Convex Quadratics.}  We develop intuition by considering a strongly convex quadratic. For $f (x) = \tfrac{1}{2} \langle x, A x \rangle$, we have $x^{\star} = x - [\nabla^2 f (x^{\star})]^{- 1} \nabla f (x)$ \footnote{since $\nabla f(x)$ equals to its first-order Taylor expansion at $x^{\star}$ for quadratics: $\nabla f(x) = \nabla f(x^{\star}) + \nabla^2 f(x^\star) (x - x^\star)$}.
In other words, $P^{\star} = [\nabla^2 f (x^{\star})]^{- 1}$ is a universal minimizer of $h_x(P)$ that drives any non-optimal point $x \nin \mathcal{X}^{\star}$ to the optimum $x^{\star}$ \emph{in one step}. When $[\nabla^2 f (x^{\star})]^{- 1} \in \mathcal{P}$, 
\Cref{thm:adaptivity} guarantees the performance of {\hdm} is competitive,
so we should expect the descent curve to decrease more and more sharply, giving superlinear convergence (\Cref{fig:demo:a}).

\paragraph{Local Superlinear Convergence.}
For general functions satisfying \ref{A3}, $f(x)$ locally behaves like a quadratic
 \begin{equation*}
   f (x) = f (x^{\star}) + \tfrac{1}{2} \langle x - x^{\star}, \nabla^2 f (x^{\star}) (x - x^{\star}) \rangle +\mathcal{O} (\| x - x^{\star} \|^3).
 \end{equation*}
Therefore, local superlinear convergence is expected for {\hdm} near $x^{\star}$. \Cref{thm:superlin} formalizes this intuition.

\begin{thm}[Local superlinear convergence]\label{thm:superlin}
Suppose $ [\nabla^2 f (x^{\star})]^{- 1} \in \mathcal{P}$ and assume \ref{A1} to \ref{A3}. Then \Cref{alg:hdm} has local superlinear convergence:
\begin{equation} \label{eqn:superlin}
f (x^{K + 1}) - f (x^{\star}) \leq{} (f (x^{1}) - f (x^{\star})) \big(\min\{\tfrac{H^2 \kappa^2}{4 \mu^2 K} \textstyle\sum_{k = 1}^K {\| x^k - x^{\star} \|^2} + \tfrac{2L \rho_K}{K}, 1 \} \big)^K.
\end{equation}
\end{thm}
\Cref{thm:superlin} justifies our observation of superlinear convergence in \Cref{fig:demo:a}: for strongly convex quadratics, the Hessian Lipschitz constant is zero ($H = 0$) and \eqref{eqn:superlin} guarantees the superlinear convergence at rate $\mathcal{O} ( ( \tfrac{\rho_K}{K} )^K )$. 
For general strongly convex objectives, global linear convergence (\Cref{thm:adaptivity}) implies 
$\lim_{K \rightarrow \infty} \frac{1}{K} \textstyle \sum_{k = 1}^K
\| x^k - x^{\star} \|^2 = 0$. 
So eventually, the first term in \eqref{eqn:superlin} vanishes, giving superlinear convergence.

\paragraph{{\hdm} Learns the Hessian at the Optimum.}
In fact, $\{P_k\}$ in {\hdm} will converge to $[\nabla^2 f (x^{\star})]^{-1}$ under an assumption similar to one studied in the quasi-Newton literature \cite{conn1991convergence, nocedal1999numerical}. \Cref{lem:scalmat-conv} quantifies the effect of learning the preconditioner through the distance $\|P_{k} - [\nabla^2 f (x^{\star})]^{-1}\|_F$.
\begin{lem}\label{lem:scalmat-conv}
  Under the same assumptions as \Cref{thm:superlin}, \Cref{alg:hdm} generates $\{P_k\}$ such that
\begin{align}
&\| P_{k + 1} - [\nabla^2 f (x^{\star})]^{- 1} \|_F^2 \nonumber \\
\leq{} & \| P_k - [\nabla^2 f (x^{\star})]^{- 1} \|_F^2  - \tfrac{\mu (\eta - L \eta^2)}{2} \big\| (P_k - [\nabla^2 f (x^{\star})]^{- 1}) \tfrac{\nabla f (x^k)}{\| \nabla f (x^k) \|} \big\|^2 
+ (2\eta - L \eta^2) \tfrac{H^2 \kappa}{4 \mu^3}\| x^k - x^{\star} \|^2.  \label{eqn:scalmat-conv}
\end{align}
\end{lem}
Relation \eqref{eqn:scalmat-conv} consists of three terms: the distance $\| P_k - [\nabla^2 f (x^{\star})]^{- 1} \|_F^2$; a decrement in the distance (second term); and an error term (last term) that converges to zero as $x^k \rightarrow x^{\star}$.
The decrement is determined by the magnitude of $\big\| (P_k - [\nabla^2 f (x^{\star})]^{- 1}) \tfrac{\nabla f (x^k)}{\| \nabla f (x^k) \|} \big\|^2$, which measures the difference between the operators $P_k$ and $[\nabla^2 f (x^{\star})]^{-1}$ in the (unit) gradient direction $\tfrac{\nabla f (x^k)}{\|\nabla f (x^k)\|}$.
To ensure fast convergence, it suffices for $P_k \nabla f (x^k)$ and $[\nabla^2 f (x^{\star})]^{-1} \nabla f (x^k)$ to remain sufficiently close. If the set $\big\{ \tfrac{\nabla f (x^k)}{\|\nabla f (x^k)\|} \big\}$ spans the entire space over the iterations, $P_k$ and $[\nabla^2 f (x^{\star})]^{-1}$ should align in all directions, leading to convergence of $\{P_k\}$.

\begin{thm}[Convergence of the preconditioner]\label{thm:scal-mat-conv}
Instate the same assumptions as in \Cref{lem:scalmat-conv} and let $\eta_k \equiv \eta \in (0, \tfrac{1}{2L(LD+1)^2 \kappa}]$ in online gradient descent \eqref{eqn:olalg-ogd}. Suppose the gradient directions
$\big\{ \tfrac{\nabla f (x^k)}{\| \nabla f (x^k) \|} \big\}$ are uniformly independent \footnote{The formal definition of a uniformly independent sequence is given in \Cref{app:thm-pf-scal-mat-conv}, which is adapted from quasi-Newton literature \cite{conn1991convergence, nocedal1999numerical}}. Then 
$\lim_{k \rightarrow \infty}  \| P_k - [\nabla^2 f (x^{\star})]^{- 1} \| = 0$.
\end{thm}

\begin{rem}
The convergence of parameters in {\hdm} was observed experimentally by \cite{gunes2018online} for a scalar stepsize ($\Pcal \subseteq \Scal$). Our result theoretically justifies this observation.
\end{rem}

\paragraph{{\hdm} and Quasi-Newton Methods.}
Our results identify a similarity between {\hdm} and quasi-Newton methods.
Both learn the inverse Hessian operator $g \mapsto [\nabla^2 f (x^{\star})]^{-1} g$ as the algorithm progresses, but through different properties of the operator.
The quasi-Newton methods use the secant equation 
$x - y \approx [\nabla^2 f(x^\star)]^{-1}(\nabla f(x)- \nabla f(y))$ for $x, y$ close to $x^\star$ and enforce this equation, replacing the inverse Hessian by $P_k$,
to guide learning \cite{jiang2023online, jiang2024online}.
In contrast, {\hdm} learns an optimal preconditioner for the function. 
Since the function is locally quadratic, this optimal preconditioner is the inverse Hessian.
{\hdm} uses the hypergradient feedback $h_x(P)$ to directly measure the quality of the preconditioner and can search for an optimal preconditioner in a given closed convex set $\mathcal{P}$, whereas quasi-Newton methods use the secant equation as an indirect proxy.
Both approaches require a safeguard to prevent divergence in the warm-up phase, which is achieved by line-search in quasi-Newton  and null step in {\hdm}. 
 In a word, both {\hdm} and quasi-Newton leverage complementary perspectives on $g \mapsto [\nabla^2 f (x^{\star})]^{-1} g$, so it is natural that they achieve similar convergence guarantees.

