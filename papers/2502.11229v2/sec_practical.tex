\section{{\hdm} with Momentum} \label{sec:momentum}

This section develops two variants of {\hdm}, 
with heavy-ball momentum \cite{polyak1964some} and with Nesterov momentum \cite{nesterov1983method}.

\subsection{Heavy-ball Momentum}
\label{sec:heavyball}

The heavy-ball method is a practical acceleration technique: 
\begin{equation} \label{eqn:heavyball-update}
x^{k + 1} = x^k - P_k \nabla f (x^k) + B_k (x^k - x^{k - 1}).
\end{equation}
The momentum parameter $B_k$ is typically chosen as a scalar $B_k = \beta_k I$ with $\beta_k > 0$.
{\hdm} can learn a matrix momentum
$B_k \in \mathcal{B} \subseteq \mathbb{R}^{n \times n}$
with convergence guarantees (\Cref{thm:heavyball}) 
when $\mathcal{B}$ satisfies this assumption:
\begin{enumerate}[leftmargin=30pt,label=\textbf{A\arabic*:},ref=\rm{\textbf{A\arabic*}},start=5]
  \item Closed convex set $\Bcal$ satisfies $\tfrac{1}{2} I\in \Bcal$, $\diam (\Bcal) \leq D$. \label{ABcal}
\end{enumerate}

{\hdm} can \emph{jointly} learn the pair $(P_k, B_k)$ using the modified feedback function
\begin{equation} \label{eqn:heavyball-feedback}
  h_{x, x^-} (P, B) \assign 
  \tfrac{\psi(x^{+}(P, B), x) - \psi(x, x^{-})}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}
  = \tfrac{[f (x^+(P, B)) + \frac{\omega}{2} \| x^+(P, B) - x \|^2] - [f (x) + \frac{\omega}{2} \| x - x^- \|^2]}{\| \nabla f (x) \|^2 + \frac{\tau}{2} \| x - x^- \|^2}, 
\end{equation}
where $\psi$ is the potential function for heavy-ball momentum defined by $\psi (x, x^-) \assign f (x) + \tfrac{\omega}{2} \| x - x^- \|^2$ \cite{danilova2020non}; \[x^{+}(P, B) \assign x - P \nabla f (x) + B (x - x^{-})\] updates $x$; and $\omega > 0$ and $ \tau > 0$ are constants. \Cref{alg:ospolyak} presents the resulting method, \hdmhb, 
which uses {\hdm}, heavy-ball momentum, and a null step to ensure decrease of the potential function $\psi$.
\Cref{fig:demo:c} compares non-adaptive heavy-ball ($P_k \equiv  \alpha I, B_k \equiv \beta I$) against {\hdmhb} with full-matrix/diagonal preconditioner and scalar momentum.
 \Cref{thm:heavyball} presents the convergence of {\hdmhb}. 

\begin{algorithm}[h]
{\textbf{input} initial point $x^0 = x^1, \eta_p, \eta_b > 0$, $P_1$, $B_1$}\\
\For{k =\rm{ 1, 2,...}}{
$\hspace{1.2pt}~~~~x^{k+1/2} = x^k - P_k \nabla f(x^k) + B_k (x^k - x^{k-1})$ \\
$\hspace{2pt}~~~~~~P_{k+1} = \Pi_{\Pcal}[P_k - \eta_p \nabla_{P} h_{x^k, x^{k-1}}(P_k, B_k)]$ \\
$\hspace{1pt}~~~~~~B_{k+1} = \Pi_{\Bcal}[B_k - \eta_b \nabla_{B} h_{x^k, x^{k-1}}(P_k, B_k)]$ \\
$(x^{k + 1}, x^k) = \displaystyle \argmin_{(x^+, x) \in \{(x^k, x^{k-1}), (x^{k+1/2}, x^k) \}} \psi(x^+, x)$
}
{\textbf{output} $x^{K+1}$}
\caption{{\hdm} with heavy-ball momentum (\hdmhb)\label{alg:ospolyak}}
\end{algorithm}

\begin{thm}[Convergence of {\hdmhb}]\label{thm:heavyball}
Under \ref{A1}, \ref{A2} and \ref{ABcal}, \Cref{alg:ospolyak} satisfies
\begin{equation*}
  f (x^{K + 1}) - f (x^{\star}) \leq \tfrac{f (x^{1}) - f (x^{\star})}{K V \max\{ \gamma_K^{\star} - \frac{\rho_K}{K}, 0 \} + 1},
\end{equation*}
where $\gamma_{K}^{\star} \assign - \min_{(P, B) \in \mathcal{P} \times \mathcal{B}} \tfrac{1}{K} \sum_{k=1}^K h_{x^k, x^{k-1}}(P, B)$ depends on the iteration trajectory $\{x^k\}_{k \leq K}$; $\rho_K = \mathcal{O}(\sqrt{K})$ is the regret with respect to feedback \eqref{eqn:heavyball-feedback}; $V \assign \min\big\{ \tfrac{f (x^{1}) - f (x^{\star})}{4 \Delta^2}, \tfrac{\tau}{4 \omega} \big\}$; $\Delta$ is defined in \Cref{lem:hypergrad-to-online}.
\end{thm}

\subsection{Nesterov Momentum} \label{sec:nesterov}
{\hdm} can also improve accelerated gradient descent {\agd}:\begin{align}
  y^k ={} & x^k + ( 1 - \tfrac{A_k}{A_{k + 1}} ) (z^k - x^k)
  \nonumber\\
  x^{k + 1} ={} & y^k - \tfrac{1}{L} \nabla f (y^k) \label{eqn:agd-descent-lemma-0}\\
  z^{k + 1} ={} & z^k + \tfrac{A_{k + 1} - A_k}{L} \nabla f (y^k), \nonumber
\end{align}
where is a pre-specified sequence. 
{\hdm} can learn a preconditioner $P_k$ that replaces $\frac 1 L$ to accelerate the gradient step \eqref{eqn:agd-descent-lemma-0} in {\agd}. We call the resulting algorithm {\hdmagd}. \Cref{alg:osnes} provides a
realization of the {\hdmagd} based on a monotone variant of {\agd} \cite{d2021acceleration}. The convergence of {\hdmagd} is established in \Cref{thm:osnes}, the proof of which is deferred to \Cref{app:proof-osnes}.
\begin{algorithm}[h]
{\textbf{input} starting point $x^1, z^1, \eta > 0$, $\theta \in [\tfrac{1}{2}, LD) $, $A_0 = 0$}\\
\For{k =\rm{ 1, 2,...}}{
$\begin{aligned}
 A_{k + 1} ={} & (A_{k + 1} - A_k)^2 \nonumber \\
  y^k ={} & x^k + ( 1 - \tfrac{A_k}{A_{k + 1}} ) (z^k - x^k)
  \nonumber\\
  x^{k + 1} ={} & \underset{x \in \{ y^k - \frac{1}{L} \nabla f (y^k), y^k
  - P_k \nabla f (y^k), x^k \}}{\argmin} f (x) \nonumber\\
  P_{k + 1} ={} & \Pi_{\mathcal{P}} [P_k - \eta \nabla h_{y^k} (P_k)]
  \nonumber\\
  v_k ={} & \max \{ \tfrac{1}{2 \max \{ -h_{y^k} (P_k), 1 / (2 L) \}},
   \tfrac{L}{2 \theta} \}  \\
  z^{k + 1} ={} & z^k + \tfrac{(A_{k + 1} - A_k)}{v_k} \nabla f (y^k)
  \nonumber
\end{aligned}$
}
{\textbf{output} $x^{K+1}$}
\caption{{\hdm} with Nesterov momentum  \label{alg:osnes}}
\end{algorithm}

\begin{thm}
\label{thm:osnes}
Assume \ref{A1} and \ref{A2}. Suppose {\agd} starts from $(x', z')$ and runs for $K$ iterations to output $\hat{x}$.
Then \Cref{alg:osnes} starting from $(x^1, z^1) = (\hat{x}, z')$ and $\theta \in [\tfrac{1}{2}, LD)$ satisfies
\begin{align}
f (x^{K + 1}) - f (x^{\star}) \leq \big[ \tfrac{1}{2 \theta} + ( 8 - \tfrac{4}{\theta} ) ( \tfrac{L D - \omega^\star_K}{L D - \theta} ) \big] \tfrac{2 L \| z' - x^{\star} \|^2}{K^2} +\mathcal{O} ( \tfrac{\rho_K}{K^3} ), \nonumber
\end{align}
where $\omega^\star_K = - \min_{P \in \mathcal{P}} \tfrac{L}{K} \sum_{k = 1}^K h_{y^k} (P)$ depends on the iteration trajectory $\{x^k\}_{k \leq K}$.
\end{thm}

The parameter $\theta$ serves as a smooth interpolation between {\hdm} and {\hdmagd}: when $\theta = 1 / 2$, \Cref{thm:osnes} recovers the convergence rate of
vanilla {\agd}; when $\theta > 1 / 2$ and $\omega_K^{\star}
\rightarrow L D$, we expect {\hdmagd} to yield faster convergence. As suggested by \Cref{fig:demo:d}, {\hdmagd} achieves faster convergence than {\agd}.
\begin{rem}
To mitigate the effect of regret, \Cref{alg:osnes} needs a warm start from vanilla {\agd}. However, experiments
  suggest that it is unnecessary in practice, and we leave an improved analysis to future work.
\end{rem}

\begin{rem}
For strongly convex problems, we can combine \Cref{thm:osnes} with a
  standard restart argument \cite{d2021acceleration,roulet2017sharpness} and achieve a similar trajectory-based linear
  convergence rate.
\end{rem}

\begin{figure}
  \centering
  \begin{subfigure}{0.4\textwidth}
    \centering
\includegraphics[height=0.18\textheight]{figs/demo_4.pdf}
    \caption{\hdmhb}
    \label{fig:demo:c}
  \end{subfigure}
  \begin{subfigure}{0.4\textwidth}
    \centering
\includegraphics[height=0.18\textheight]{figs/demo_3.pdf}
    \caption{\hdmagd}
    \label{fig:demo:d}
  \end{subfigure}
\caption{The convergence behavior of {\hdmhb} and {\hdmagd} on a toy quadratic problem. \Cref{fig:demo:c}: {\hdmhb}. \label{fig-demos}\Cref{fig:demo:d}: {\hdm} with Nesterov momentum.} 
  \label{fig:demo:all}
\end{figure}