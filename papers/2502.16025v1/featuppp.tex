%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{bm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\DeclareMathOperator{\jbu}{\textup{JBU}}
\DeclareMathOperator*{\softmax}{\textup{softmax}}
\DeclareMathOperator*{\bilinear}{\textup{BL}}
\DeclareMathOperator*{\tile}{\textup{Tile}}

\def\authornote#1#2#3{{\textcolor{#2}{\textsl{\small[#1: #3]}}}}
\newcommand{\ANY}[1]{\authornote{\textbf{Authors}}{red}{#1}}
\newcommand{\MR}[1]{\authornote{\textbf{Mike}}{blue}{#1}}
\newcommand{\PM}[1]{\authornote{\textbf{Pavlo}}{orange}{#1}}
\newcommand{\ul}[1]{\underline{#1}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{FeatSharp: Your Vision Model Features, Sharper}

\begin{document}

\twocolumn[
\icmltitle{FeatSharp: Your Vision Model Features, Sharper}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mike Ranzinger}{comp}
\icmlauthor{Greg Heinrich}{comp}
\icmlauthor{Pavlo Molchanov}{comp}
\icmlauthor{Jan Kautz}{comp}
\icmlauthor{Bryan Catanzaro}{comp}
\icmlauthor{Andrew Tao}{comp}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{comp}{NVIDIA}

\icmlcorrespondingauthor{Mike Ranzinger}{mranzinger@nvidia.com}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The feature maps of vision encoders are fundamental to myriad modern AI tasks, ranging from core perception algorithms (e.g. semantic segmentation, object detection, depth perception, etc.) to modern multimodal understanding in vision-language models (VLMs). Currently, in computer vision, the frontier of general purpose vision backbones are Vision Transformers (ViT), typically trained using contrastive loss (e.g. CLIP). A key problem with most off-the-shelf ViTs, particularly CLIP, is that these models are inflexibly low resolution. Most run at 224x224px, while the ``high resolution'' versions are around 378-448px, but still inflexible. We introduce a novel method to coherently and cheaply upsample the feature maps of low-res vision encoders while picking up on fine-grained details that would otherwise be lost due to resolution. We demonstrate the effectiveness of this approach on core perception tasks as well as within agglomerative model (RADIO) training as a way of providing richer targets for distillation.
\end{abstract}

\section{Introduction}\label{sec:intro}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{resources/featsharp_arch.pdf}
    \vspace{-7mm}
    \caption{Upsampling architecture diagram. We combine the upsampled features coming from FeatUp \cite{fu2024featup} with the tiled features and mix them with FeatSharp to produce a feature map with higher fidelity. The tiled features have more detail, but also representation issues such as the difference in upper and lower body at the tile boundary. ``Full-Res Reference'' is for display purposes, as for a model that doesn't exhibit stable resolution scaling (e.g. DFN CLIP, SigLIP, etc.) we don't have access to a target hi-res feature map. Our proposed learnable modules are in green, combined with a learnable FeatUp JBU stack.}
    \label{fig:featsharp_arch_diagram}
\end{figure*}

The use of vision foundation models (VFM) \cite{awais2023foundational} has seen widespread use since the beginning of the modern era of computer vision using deep learning \cite{krizhevsky2012alexnet}, primarily used to perform transfer learning \cite{plested2022deeptransferlearningimage} (e.g. finetuning a VFM on a downstream task), information retrieval \cite{babenko2014neuralcodes,zhang2024retrieval}, and most recently, to power visual capabilities for vision-language models (VLM) \cite{alayrac2022flamingovisuallanguagemodel,openai2024gpt4technicalreport,liu2023llava,lin2023vila}. The recent shift toward using Transformers~\cite{vaswani2017attention} for computer vision (ViT~\cite{dosovitskiy2021image}) has both tremendously moved the field forward, but has generally left the use of VFMs in a tricky spot: Transformers are computationally demanding and have poor algorithmic scaling properties ($O(n^2)$ for 1D sequences, or $O(\left(w \cdot h \right)^2$ for 2D inputs), leaving the majority of models to be relatively low-resolution. For example, perhaps the most popular family of VFMs to date, CLIP~\cite{radford2021clip}, typically runs at 224 or 336px input resolutions, and produces spatial features at a 14x downsample (e.g. $224^2 \rightarrow 16^2$). Owing to the nature of learned position embeddings, ViTs also tend to be relatively inflexible to changes of input resolution, allowing for changes, but requiring finetuning \cite{dosovitskiy2021image}. 

It's possible that the strict dependence on the training resolution is an artifact of the algorithm used for training, as DINOv2~\cite{oquab2023dinov2,darcet2023vision} is quite robust to interpolating its position embeddings, producing stable features at various resolutions \cite{ranzinger2023amradio}, ignoring for the moment that DINOv2, being a transformer, is expensive to use at high-resolution. A recent technique called AM-RADIO~\cite{ranzinger2024phisdistributionbalancinglabelfree}, borrowing ideas from ViTDet~\cite{li2022vitdet}, FlexiViT~\cite{beyer2023flexivit}, and RO-ViT~\cite{kim2023regionaware}, has attempted to create a resolution-flexible ViT, however it is still dependent on low-resolution ViTs as it distills from other seminal VFMs which are low-res-only: DFN CLIP~\cite{fang2023data} and SigLIP~\cite{zhai2023sigmoid}. 

Recently, FeatUp~\cite{fu2024featup} aims to directly address the problem of low-resolution vision features by using one of two learned upsampling algorithms: A model-specific generalized upsampler using Joint Bilateral Upsampling (JBU) \cite{kopf2007jbu}, or a model-specific-image-specific implicit network. While they demonstrate particularly compelling results with their implicit network, their results using the stack of JBU filters lack refined details (shown in figure \ref{fig:featup_jbu_vs_implicit} in the appendix). Along with lack of granular refinement, it's impossible for this approach to capture fine-grained details that are too small for the vision backbone to detect at its native resolution. To this end, we take inspiration from both FeatUp's JBU approach, as well as the recent trend in VLMs such as LLaVA 1.6~\cite{liu2024llavanext1p6}, InternVL-1.5~\cite{chen2024internvl1p5}, NVLM~\cite{dai2024nvlmopenfrontierclassmultimodal} and Eagle~\cite{shi2024eagleexploringdesignspace} to tile an image, aggregating local features from a fixed-low-resolution model, to build an upsampler that simultaneously leverages the raw pixel guidance, low-res feature guidance, and regional tile guidance, resulting in substantially more detailed feature maps which are also capable of capturing details too small for the original resolution. Specifically, we:

\begin{itemize}
    \item Build on top of FeatUp's JBU algorithm \cite{fu2024featup} by adding de-biasing and tiling fusion modules to incorporate detailed tile features, resulting in significantly higher levels of detail, with extensive experiments demonstrating effectiveness
    \item Study the relationship between FeatUp's feature consistency and ViT-Denoiser's~\cite{yang2024denoising} approach to cleaning the features of a ViT at its native resolution
    \item Introduce an improved training setting for AM-RADIO \cite{ranzinger2024phisdistributionbalancinglabelfree} demonstrating a +$0.6\%$ improvement across the entire benchmark suite, and better teacher adapter features
\end{itemize}

\section{Related Work}\label{sec:related}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{resources/featsharp_module.pdf}
    \caption{Diagram of the FeatSharp module. We first concatenate the bilinear upsampled and tiled mosaic feature maps along the channel dimension. We then apply a transformer block with sliding window attention followed by MLP (in this case, SwiGLU), and then slice off the first half of the channels, corresponding to the bilinear upsampled buffer. The role of FeatSharp thus is to refine the bilinear buffer by leveraging the tile buffer.}
    \label{fig:featsharp_module_diagram}
\end{figure}

\paragraph{Feature Upsampling}
The most obvious baseline for feature upsampling is to use traditional filtering approaches such as bilinear or bicubic upsampling. The alternative is to evaluate the network at higher resolution, however it comes with the dual drawback that computational cost increases (quadratically in the case of Vision Transformers), and also that many models (ViTs in particular) have trouble extrapolating from their trained resolution \citep{beyer2023flexivit,dehghani2023navit}. If we expand our view to include parametric approaches, then deconvolution \cite{noh2015deconv,shi2016deconv,dumoulin2016AGT} and resize-conv \cite{odena2016deconvcheck} are popular choices. There are also pixel-adaptive approaches such as CARAFE~\cite{Wang2019CARAFECR}, SAPA~\cite{lu2022sapa}, and FeatUp~\cite{fu2024featup}. 

We adopt FeatUp's formulation of multi-view consistency as a way to train an upsampler, however, we notice that instead of solely relying on raw RGB pixels as guidance for upsampling, we can also use a small, fixed budget of inferences (similar in spirit to their implicit model), and use a mosaic of tiles as guidance at the higher resolution. This choice gives us a richer, and semantically relevant, feature space to merge from. Additionally, it allows us to incorporate information from regions that were too small for the low-res view, but become visible within a tile. Small details are a limitation of every approach that doesn't acquire extra samples from the base model, as they rely on all relevant information already being encoded by the initial model evaluation.

\paragraph{Feature Denoising}
Related to multi-view consistency, ViT-Denoiser~\cite{yang2024denoising} noticed that ViT features are generally very noisy (although some are much cleaner than others), and also propose a multi-view consistency formulation to learn how to separate fixed noise, conditional noise, and semantic content. We notice the deep ties between ViT-Denoiser and FeatUp, in that multi-view consistency provides a way to eradicate fixed-pattern noise from the feature buffer. Drawing inspiration from this, we add a learnable bias buffer (similar to learned position embeddings) at the output of the base model. This simple change works because fixed patterns will degrade multi-view consistency, as the pattern is always local to the view, and lacks global coherence.

\paragraph{VLMs}
The use of tiling to increase information is currently very prominent in VLMs \cite{liu2024llavanext1p6,chen2024internvl1p5,dai2024nvlm}, albeit an alternative approach is to instead leverage the models at hi-res themselves \cite{beyer2024paligemmaversatile3bvlm,wang2024qwen2vlenhancingvisionlanguagemodels}. We also see RADIO-Amp\citep{heinrich2024radioamplifiedimprovedbaselines} being primarily useful at high-resolution within VLMs. In the increasingly VLM-centric approach to computer vision, we turn our focus to RADIO-Amp, as it has a training procedure that relies on matching a high-resolution student against a low-resolution teacher, an application area that is perfect for studying feature upsampling, as it would provide richer guidance to the distillation.

\paragraph{Agglomerative Models}
In the agglomerative model space, there are currently three major approaches: RADIO \citep{ranzinger2023amradio,ranzinger2024phisdistributionbalancinglabelfree,heinrich2024radioamplifiedimprovedbaselines}, Theia \citep{shang2024theia}, and UNIC \citep{sariyildiz2024unic}. We focus our attention on RADIO because it is the only approach that directly tries to tackle resolution flexibility as well as high-resolution.

\section{Method}\label{sec:method}

We leverage FeatUp's training algorithm of treating the upsampling problem as that of multi-view consistency between the upsampled and then downsampled features and different low-res views of the same image.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/tile_process.pdf}
    \vspace{-8mm}
    \caption{Visualization of the tiling process. An input image (left) is split into $2 \times 2$ tiles, each of which is resized to match the input resolution of the encoder, fed through the encoder independently, and then stitched back into a higher resolution feature map. Feature maps shown are from DFN CLIP, and they are resized to be larger than actual for demonstration purposes.}
    \label{fig:tile_process}
\end{figure*}

\subsection{Review - FeatUp: Joint Bilateral Upsampling (JBU)}\label{sec:method:featup}

Given a high-resolution signal $G$ (e.g. the raw pixels) as guidance, and a low-resolution signal $F_{lr}$ that we'd like to upsample, and let $\Omega$ be a neighborhood of each pixel in the guidance. Let $k(\cdot, \cdot)$ be a similarity kernel that measures how close two vectors are. Then

\begin{equation}
\begin{split}
\hat{F}_{hr}[i, j] = \frac{1}{Z} \sum_{(a, b) \in \Omega} \Bigl(&F_{lr}[a, b] \cdot \\
    & k_{range}\left(G[i, j], G[a, b]\right)\cdot \\
    & k_{spatial}\left([i, j], [a, b] \right)  \Bigr)
    \label{eq:jbu_orig}
\end{split}
\end{equation}

with $Z$ being a normalization to make the kernel sum to 1. $k_{spatial}$ is a Gaussian kernel with learnable $\sigma_{spatial}$ defined as

\begin{equation}
    k_{spatial}(x,y) = \exp\left(\frac{-\left\lVert x - y \right\rVert_2^2}{2\sigma_{spatial}^2}\right)
    \label{eq:k_spatial}
\end{equation}

and $k_{range}$ as

\begin{equation}
    k_{range}(x,y) = \softmax_{(a, b) \in \Omega} \left(\frac{1}{\sigma_{range}^2} h(G[x,y]) \cdot h(G[a,b]) \right)
    \label{eq:k_range}
\end{equation}

with $h(x)$ being a learned MLP projector. They define

\begin{equation}
    F_{hr} = \left(\jbu(\cdot, x) \circ \jbu(\cdot, x) \circ ... \right)(f(x), x)
\end{equation}

as a stack of $2\times$ upsamplers, thus enabling power-of-2 upsample factors. With $x$ being the original input image, and $f(x)$ being the low-resolution feature map. We note that $2$ isn't a necessary part of the architecture, and that their implementation supported arbitrary factors, so we simply propose to take a given upsample factor $z \in \mathbb{Z}_{+}$ and prime factorize $z$ to get a set of upsample factors, using a $\jbu_k$ for each prime factor. This decomposes to an identical operation as before when $\log_2 z \in \mathbb{Z}_{+}$, but allows for an easy guide for any other integer, e.g. for a $14\times$ upsample corresponding to a patch-size-14 backbone, we'd use a $\left(\jbu_{7\times} \circ \jbu_{2\times}\right)(f(x), x)$ stack.

As is typical with bilateral upsampling, this method is very sensitive to strong edges in the guidance buffer, however, it also tends to over-smooth features in regions of lower contrast. Particularly, it struggles with feature patterns such as SAM (figure \ref{fig:viz_basketball}) where there are interior edges in feature space, but not pixel space. This results in the features being blurred inside of objects.

We don't make any changes to their downsampler, instead opting to just use their Attention Downsampler without modification. We then focus on two changes, one to output normalization, and the other to how upsampling guidance is computed.

\subsection{Feature Normalization}\label{sec:method:feat_norm}
FeatUp supports either leaving the features coming from the backbone as-is (e.g. no normalization), or using a LayerNorm to better condition the outputs for feature learning. For a similar motivation as PHI-S~\cite{ranzinger2024phisdistributionbalancinglabelfree}, we want to avoid using the raw features as they have varying spreads, and we'd also like to avoid using LayerNorm as it makes the features incompatible with the original feature space. NaÃ¯vely learning the raw feature space across the suite of teachers without normalization often led to convergence issues, particularly given the wide variance of activations.

\paragraph{Tile-Guided Attentional Refinement}

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{resources/2x/2x_bilinear.png}
    \includegraphics[width=0.3\linewidth]{resources/2x/2x_tile.png}
    \caption{Visualization of $2\times$ upsampling using bilinear (\textit{left}) versus tiling (\textit{right}), using the DFN CLIP encoder.}
    \vspace{-5mm}
    \label{fig:ups2x}
\end{figure}

Joint-Bilateral Upsampling is able to retain object boundaries primarily in instances when there are noticeable changes in intensity in the RGB input image. This results in sharp contours, but within a region, we end up with vague and blurry feature representations. Owing to the reliance on raw pixel intensities, object contours that are less discriminative in color space often get blurred with the neighborhood. Finally, because the upsampling operation is only truly operating on the low resolution feature maps of the model, it's impossible for JBU to introduce new details into the feature map that are visible/encodable at higher input resolutions. FeatUp's implicit upsampler doesn't have this problem because it's constructing a unified view from numerous local views of the original image, enabling detailed refinements. We propose an intermediary method between JBU which leverages a single featurizer inference, and the implicit model, which relies on numerous inferences and is thus cost prohibitive\footnote{https://github.com/mhamilton723/FeatUp/issues/2}.

Inspired by the use of tiling in Vision-Language Models (VLMs) \citep{liu2024llavanext1p6,shi2024s2,dai2024nvlmopenfrontierclassmultimodal}, we develop an attentional refinement block that is able to integrate the information between a bilinear upsampled feature map, as well as a feature map composed of tiles. We show an overview of the algorithm in figures 
 \ref{fig:featsharp_arch_diagram}, \ref{fig:featsharp_module_diagram} and \ref{fig:tile_process}. The diagram shows actual results using RADIOv2.5-L, which is the most scale equivariant foundation model \citep{heinrich2024radioamplifiedimprovedbaselines}, and generally the strongest visual foundation model \citep{lu2024swissarmyknifesynergizing,drozdova2024semisupervised,guo2024videosamopenworldvideosegmentation}. Because the model has strong resolution scaling, it provides us with a good way to compare the results of the upsampling process against the feature maps of the same resolution attained by increasing the resolution of the input image. We also observe that even just at $4\times$ tiling, there are major discontinuities in the tiled feature map, which the FeatSharp module must overcome to produce a unified higher resolution image.


For the FeatSharp module, we leverage a single Attention+SwiGLU transformer block. In order to prevent the quadratic cost of global attention, we instead use local attention \cite{ramachandran2019localattn}. We concatenate the JBU upsampled buffer with the tiled feature map and feed it to the block. After the block is computed, we slice the first $C$ dimensions of the output, with $C$ being the model feature dimension, and treat that as the refined features. The slicing strategy takes advantage of the fact that a transformer block has a residual pathway, and thus a no-op from the transformer would be equivalent to returning the bilinear upsampled features. Through the attention mechanism, the model is able to consider the local neighborhood and refine its features to achieve better multi-view consistency. To this end, we train our model identically to FeatUp's multi-view consistency algorithm. We do not employ any special loss functions beyond the MSE loss on multi-view consistency, contrary to FeatUp's use of Total Variation and Conditional Random Field losses. We provide ablations wrt architecture choice in appendix \ref{sec:featsharp_arch_ablations}.

\subsection{Denoising}

Drawing inspiration from \cite{yang2024denoising}, we notice that the problem formulation has a very similar solution to FeatUp (and ours), owing to the fact that all methods are using multi-view consistency and thus learn to eliminate position-based artifacts. From their formulation:

\begin{equation}
    \textup{ViT}(x) = f(x) + g(\textup{\textbf{E}}_{pos}) + h(x, \textup{\textbf{E}}_{pos})
    \tag{\cite{yang2024denoising}, Eq 5}
    \label{eq:vit_denoise}
\end{equation}

We add a learnable $g$ buffer, such that

\begin{equation}
    \hat{f}(x) = f(x) + g
\end{equation}

with $f(x)$ being the frozen vision encoder. The learnable $g$ allows our model to learn and negate the fixed position artifacts that the encoder produces. Notably, given that we are also using the base model for the tiles, this learned buffer is applied to all of the generated tiles as well. We visualize these biases in figure \ref{fig:model-biases}. It's entirely possible for FeatSharp to remove the biases itself, but we found that having this learnable bias buffer consistently improves multi-view consistency, which we show in table \ref{tab:bias_fidelity} in the appendix.

\subsection{Complexity}
An important point about this method is that because of the tiling, it requires more evaluations of the base vision model to construct the high resolution feature map. However, due to the scaling properties of global self-attention, our proposed method always has better scaling properties than running the original model at higher resolution (assuming the model is capable of doing this in the first place):

\begin{equation}
\begin{split}
    f(x) &= c \left(1 + x^2 \right) \\
    g(x) &= c \left(x^2\right)^2 = c x^4  \\
    f(x) &\leq g(x) \quad \forall x > 1
\end{split}
\label{eq:cost_inequality}
\end{equation}

where $f(x)$ is the relative cost of computing FeatSharp with $x \in \mathbb{Z}_{+}$ upsample factor, $g(x)$ is the cost of the hi-res image based on quadratic scaling on the number of patches (and thus tiles), and $c$ being the cost of processing a single tile. We show the actual scaling cost in figure \ref{fig:vith_throughput} in the appendix.

\section{Upsampling Results}\label{sec:experiments}

We consider upsampling to be important in cases where one is given a fixed pretrained model, and the goal is to extract more information out of it, for a given image. We study our method in relation to FeatUp from a core multi-view consistency standpoint in this section, from a semantic segmentation linear probe standpoint, and also for training a new RADIO-like model with hi-res teacher targets.

\subsection{Fidelity}\label{sec:fidelity}

\paragraph{Multi-View Consistency} 
Following \cite{ranzinger2024phisdistributionbalancinglabelfree}, we use their definition of fidelity (equation 51) for multi-view consistency, where a higher fidelity value means that the upsampled-transformed-downsampled representations are closer to the raw transformed predictions from the model. 

\begin{equation}
    f(\mathbf{X},\mathbf{Y}) = \frac{\text{MSE}(\mathbf{Y}, \bm{\mu_Y})}{\text{MSE}(\mathbf{X}, \mathbf{Y})}
    % \tag{\cite{ranzinger2024phisdistributionbalancinglabelfree} Eq 51}
\end{equation}

with $\mathbf{X}$ being the warped predictions and $\mathbf{Y}$ the targets. This serves as a proxy measure for how well the upsampler is working, as arbitrarily warping and downsampling it results in representations closer to the real prediction at low resolution. We show these results in figure \ref{fig:consistency_fidelity}, where we observe that FeatSharp consistently achieves the highest fidelities, substantially so with the ``cleaner'' models such as DINOv2-L, RADIOv2.5-L, and SAM-H.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/multiview_consistency_plot.pdf}
    \vspace{-7mm}
    \caption{Fidelity plot for different models and upsampling methods. Higher values are better. We don't show SAM 4x because of OOM issues training these models.}
    \vspace{-10mm}
    \label{fig:consistency_fidelity}
\end{figure}

\subsection{Qualitative}

We run this upsampling method on seven different foundation models coming from diverse domains such as supervised (ViT, \citep{dosovitskiy2021image}), contrastive (DFN~CLIP~\cite{fang2023data}, SigLIP~\cite{zhai2023sigmoid}), Self-supervised (DINOv2-L-reg~\cite{darcet2023vision}), Segmentation (SAM~\cite{kirillov2023sam}), VLM (PaliGemma~\cite{beyer2024paligemmaversatile3bvlm}), and Agglomerative (RADIOv2.5-L~\cite{ranzinger2024phisdistributionbalancinglabelfree}). Results are in figure \ref{fig:viz_basketball}. The original feature maps run the spectrum from extremely noisy (SigLIP) to very clean (RADIOv2.5-L, SAM), which allows us to demonstrate the effectiveness of the approach on a diverse set of models. Taking SAM for an example, the way in which is has thick edge outlines cannot be reproduced in the shape interior by FeatUp, primarily because the bilateral upsampler is operating on the raw pixels, where the interior edge doesn't exist in the real image. For all of the featurizers, FeatSharp is able to achieve more legible representations, particularly it's more able to closely match the real hi-res features in the second column.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{resources/basketball_viz_results.pdf}
    \vspace{-5mm}
    \caption{PCA visualizations of features from a basketball scene. \textbf{Column~1}: Raw features produced by the model at normal resolution (e.g. 14x downsample for DFN CLIP, SigLIP, PaliGemma, and DINOv2, 16x downsample for SAM and RADIOv2.5-L. \textbf{Column~2}: Raw features at the 4x upsample resolution (we interpolate the position embeddings for those models that don't natively support resolution changes). \textbf{Column~3}: FeatUp 4x upsampling (\textit{prior work}). \textbf{Column~4}: FeatSharp 4x upsampling. 
    \\
    \textit{NOTE: ``Real 4x'' technically only makes sense for models with strong scale equivariance, such as DINOv2, RADIO, and SAM.}}
    \label{fig:viz_basketball}
\end{figure}

\subsection{Semantic Segmentation}\label{sec:experiments:semseg}

Semantic segmentation has the potential to benefit from increased resolution, as it allows for label contours to be more precise, and potentially for regions to be recovered that are otherwise too small. The first setting we evaluate on is we train both FeatUp and FeatSharp at $2\times$ and $4\times$ upsampling, both using PHI-S. We resize the input size to be the featurizer's native input resolution, which we call ``$1\times$ Input Size'', and we also consider ``$2\times$ Input Size'', where we double the input size, and feed directly to the featurizer in the case of ``Baseline'', or we allow the upsampler to have higher resolution guidance while keeping the featurizer input fixed at $1\times$ resolution. We show these results in figure \ref{fig:semseg}. In most cases, both upsampling algorithms produce higher quality segmentations than the baseline, however, FeatUp is worse than the ``Baseline $2\times$'' method for RADIOv2.5-L and ViT. In all cases, FeatSharp is superior to both FeatUp and also the baselines by significant margins. We even improve upon SOTA RADIO's published result of 51.47 with a $2\times$ upsampling combined with $2\times$ input size, producing a model that attains 53.13 mIoU, a $+1.66$ mIoU improvement. RADIO itself improves with the $2\times$ input size, but not to the same degree as with FeatSharp, with FeatSharp being $57\%$ faster. We also notice that $3\times$ upsampling is generally worse than $2\times$ or $4\times$ for both upsamplers.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{resources/semseg_plots.pdf}
    \vspace{-7mm}
    \caption{ADE20k \cite{zhou2017ade20k} Semantic segmentation results for different featurizers and upsamplers. We also vary the input size between Inpt-$1\times$ and Inpt-$2\times$ the featurizer's native resolution. $1\times$ Resolutions: DFN CLIP = 378px, DINOv2-L = 448px, PaliGemma = 448px, RADIOv2.5-L = 512px, SigLIP = 378px, ViT = 224px. The dark line represents the mean of 5 runs, with shaded areas showing the standard deviation. Because the x-axis is the upsample amount, the baselines should technically be single points on a ``1x'' x-coord, but we instead draw a line to make it easier to see the change in the upsamplers across the upsample amounts. E.g. for ``RADIO, Baseline Inpt-2x'', we can see that it's better than FeatUp $2\times$ upsampling, but worse than FeatSharp $2\times$ upsampling.}
    \label{fig:semseg}
\end{figure*}

\subsection{Agglomerative Models}\label{sec:experiments:agglom}

We build upon RADIOv2.5-L~\cite{heinrich2024radioamplifiedimprovedbaselines} as it learns directly from the spatial features of teacher models. In particular, we consider whether we can improve upon their multi-resolution training strategy by using FeatSharp to convert the low-res teachers into hi-res teachers. We convert the teachers in the bottom left quadrant ``Low Res Teacher / High Res Student'' in their Figure 6 into ``High Res Teacher / High Res Student'' by using the upsampler. We consider a few different comparative baselines in order to prove the efficacy of the technique. First, our baseline matches that of \cite{heinrich2024radioamplifiedimprovedbaselines}, which is to downsample the student to match the teacher. Then, we consider two techniques which are popular in the literature: Tiling~\cite{liu2024llavanext1p6}, and S2~\cite{shi2024s2}. Both of these rely on tiling, but S2 also considers the low-res version. Because we need the feature space to remain the same as the low-res partition of RADIO, we opt to upsample the low-res feature map, and then interpolate the upsampled-low-res against the tiled version, using $y = \beta \cdot \text{low-res} + (1 - \beta) \cdot \text{high-res}$. We set $\beta = 0.5$ as it's unclear what an optimal balance might be, and it's expensive to search this space. As a final baseline, we include FeatUp's JBU variant, as the implicit version would be prohibitive to use within a training loop\footnote{\hyperlink{https://github.com/mhamilton723/FeatUp/issues/2\#issuecomment-2005688054}{1 minute per image}}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/radio_inference/dfn_clip_main_body.jpg}
    \vspace{-5mm}
    \caption{Visualization of our trained RADIO's DFN CLIP adaptor when the high-res partition used various teacher upsample schemes.}
    \label{fig:radio_dfn_clip_adaptor}
\end{figure*}

In figure \ref{fig:radio_dfn_clip_adaptor} we qualitatively visualize the DFN CLIP adaptor features learned by the radio model. We can see that each upsampling method has a substantial impact on the resulting feature maps. The baseline method exhibits strong high-frequency artifacting starting at 768px. This is likely when RADIO ``mode switches'' to high-resolution, which is something that \cite{heinrich2024radioamplifiedimprovedbaselines} addressed for the backbone features, but apparently still exhibit for the adaptor features. We observe that Tiling and S2 exhibit not only high-frequency noise patterns like the baseline, but also obvious grid patterns, arising from the use of tiles. FeatUp appears to mode switch starting at 768px into a smooth, but low-detail feature space. FeatSharp remains smooth and highly detailed as resolution increases, however, visually, it's still possible that the features are mode switching. We further study this tiling issue in appendix \ref{sec:apdx:overtiling}.

Along with improvements in the adaptors, we also study the effects on the backbone features for the RADIO model. Following \cite{Maninis2019AttentiveSO,lu2024swissarmyknifesynergizing} we report the MTL Gain ($\Delta_m$) across a suite of tasks. Unlike the prior works, instead of leveraging a single-task baseline, we instead opt to report the change relative to the baseline training run.

\begin{align}
    \delta_m &= 100 \cdot (-1)^{l_t} \frac{M_t - M_{B,t}}{M_{B,t}} \\
    \Delta_m &= \frac{1}{T} \sum_{t=1}^{T} \delta_m
\end{align}

where $M_t$ is the metric for the current model on task $t$, and $M_{B,t}$ is the metric for the baseline model. $l_t$ is 0 when higher task values are better, and 1 when lower is better.

\begin{table*}[!h]
    \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|cccccccc|c}
        Upsampler & Classification & Dense      & Probe 3D  & Retrieval & Pascal Context & NYUDv2      & VILA      & $\Delta_m \%$ \\
        \hline
        RADIO-AMP-L & -0.47          & -0.09      & -1.05     & -0.45     & \bf{0.62}      & -2.26       & \bf{2.24} & -0.21     \\
        \hline
        Baseline    & \ul{0.00}      & 0.00       & 0.00      & 0.00      & 0.00           & 0.00        & 0.00      & 0.00      \\
        Tile        & -0.03          & \bf{0.30}  & -0.08     & -0.23     & -0.02          & \bf{1.33}   & -3.17     & -0.27     \\
        S2          & -0.05          & 0.15       & -0.03     & -0.44     & 0.13           & \bf{1.33}   & -0.89     & 0.03      \\
        FeatUp      & -0.07          & 0.14       & 0.23      & -0.07     & 0.14           & 0.32        & -1.58     & -0.13     \\
        FeatSharp   & \bf{0.06}      & \ul{0.16}  & \bf{0.83} & \bf{0.13} & \ul{0.17}      & \ul{0.93}   & \ul{0.43} & \bf{0.39}
    \end{tabular}
    }
    \caption{Relative changes (in \%) on a suite of aggregated benchmarks, with each column reporting $\delta_m \%$ and averaged into $\Delta_m \%$. All relative changes are against our baseline run. Raw metrics are in section \ref{sec:apdx:raw_radio_results}. \textit{NOTE: The upsamplers are only applied to the DFN CLIP and SigLIP teachers during RADIO training. Metrics are collected from trained RADIO without upsampling methods.}
    }
    \label{tab:radio_mtl_task_suite}
\end{table*}

We show the MTL Gain results in table \ref{tab:radio_mtl_task_suite}. Given that the results are relative to our baseline run, S2 and FeatSharp are the only two methods to improve, however, only FeatSharp was categorically better, leading to a +0.39\% improvement across all benchmarks on average. We also see that our version of RADIO with FeatSharp teachers generally does better than RADIO-AMP-L \cite{heinrich2024radioamplifiedimprovedbaselines}, which is the current state of the art, where we improve over it on everything except for the VILA task. We report all of the raw benchmark scores in tables \ref{tab:apdx:radio_cls_retrieval_metrics}, \ref{tab:apdx:radio_dense_probe3d_metrics}, \ref{tab:apdx:radio_pascal_nyud_metrics} and \ref{tab:apdx:radio_vila_metrics} in the appendix.

\section{Conclusion}

We have presented a novel feature upsampling technique named FeatSharp that achieves higher multi-view fidelity than the current best method, FeatUp. We achieve this by joining FeatUp's JBU upsampler with a mosaic of tiles, and then process with a single local attention block. We demonstrate its effectiveness on ADE20K semantic segmentation linear probing, where the use of FeatSharp improves over both baseline and FeatUp, even with the strongest segmenter, RADIO, which itself can handle hi-res inputs robustly. We then demonstrate the effectiveness of FeatSharp by employing it directly within RADIO training, enabling low-res-only teacher models to have hi-res distillation targets. In doing this, our FeatSharp-RADIO largely improves on dense vision task benchmarks, and yields an overall improvement over our reproduction baseline, which itself improves over RADIO-AMP-L, the current state of the art. We believe this work can be useful both as a drop-in extension of existing vision systems which rely on pretrained vision encoders, as well as the newly trained FeatSharp-RADIO model with hi-res teachers, which can emulate these same models. Owing to FeatSharp-RADIO's emulation abilities, it allows us to estimate these teacher models at arbitrary resolutions, not just integer upsampling factors as restricted in FeatSharp/FeatUp's core training algorithm. Further, combining RADIO's ``ViTDet'' \citep{li2022vitdet} mode with these hi-res teacher emulations allows us to achieve hi-res feature maps without fully paying the quadratic penalty in number of tokens as required by standard ViTs.

% \section{Impact Statement}
% This paper presents work whose goal is to advance the field of Computer Vision. By virtue of being a lightweight addition to existing vision models, the work aims to open up doors for higher-resolution perception tasks (e.g. segmentation, depth perception, etc.) while retaining the original model representations. As such, the ethical impacts are constrained to those of the model being upsampled. The FeatSharp training code, upsampler weights, and trained RADIO model using FeatSharp will be released to the community.

\bibliography{featuppp}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\include{supplemental}

\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
