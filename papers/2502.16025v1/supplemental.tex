\appendix
\onecolumn

\section{RADIO Results}

\subsection{Benchmark Results}\label{sec:apdx:raw_radio_results}
\begin{table}[h]
    \centering
    \begin{tabular}{r|cc|cccc}
        \multirow{3}{*}{\bf{Upsampler}} & \multicolumn{2}{c|}{\bf{Classification}}  & \multicolumn{4}{c}{\bf{Zero Shot Retrieval}} \\
                           & \multicolumn{2}{c|}{ImageNet-1k} & \multicolumn{2}{c|}{\bf{COCO}} & \multicolumn{2}{c}{\bf{Flickr30k}} \\
                           & \bf{Zero Shot} & \bf{kNN}   & \bf{Text2Im} & \bf{Im2Text} & \bf{Text2Im} & \bf{Im2Text} \\
        \hline
        RADIO-AMP-L        & 81.01          & 84.68      & 51.65        & \bf{69.06}   & 77.52        & 90.80        \\
        \hline
        Baseline           & 81.47          & 85.00      & \bf{52.25}   & 68.68        & \bf{78.64}   & 90.60        \\
        Tile               & 81.41          & \bf{85.01} & 51.90        & 68.30        & 78.46        & 91.10        \\
        S2                 & 81.44          & 84.95      & 51.94        & 67.98        & 78.34        & 90.80        \\
        FeatUp             & 81.39          & 84.96      & 51.93        & 68.40        & 78.26        & \bf{91.70}   \\
        FeatSharp          & \bf{81.56}     & \bf{85.01} & 52.13        & 68.80        & 78.50        & 91.30        \\
    \end{tabular}
    \caption{Classification and Zero Shot Retrieval Metrics. All zero shot methods use the DFN CLIP Text encoder, paired with RADIO's respective learned adaptor.}
    \label{tab:apdx:radio_cls_retrieval_metrics}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{r|ccc|cccc}
        \multirow{2}{*}{\bf{Upsampler}} & \multicolumn{3}{c|}{\bf{Dense}} & \multicolumn{4}{c}{\bf{Probe3d}} \\
            & \bf{ADE20k} & \bf{VOC} & \bf{SAM COCO} & \bf{Depth} & \bf{Surface Normals} & \bf{Correspondence} & \bf{SPair71k} \\
        \hline
        RADIO-AMP-L & 51.47*     & 85.49*     & 75.06      & 84.69      & 60.06      & 58.46      & 54.36      \\
        \hline
        Baseline    & 51.58      & 85.08      & 75.46      & 85.03      & \bf{61.42} & 59.27      & 54.49      \\
        Tile        & 51.62      & \bf{85.55} & \bf{75.67} & 85.14      & 60.85      & 59.65      & 54.41      \\
        S2          & 51.56      & 85.28      & 75.66      & 85.11      & 60.49      & \bf{59.84} & 54.68      \\
        FeatUp      & 51.67      & 85.20      & 74.54      & 85.39      & 61.20      & 59.63      & 54.63      \\
        FeatSharp   & \bf{51.75} & 85.13      & 75.54      & \bf{85.48} & 60.76      & 59.55      & \bf{56.33} \\
    \end{tabular}
    \caption{Dense and Probe3D~\citep{elbanani2024probing} metrics. *We report numbers for evaluation at 512px, which are found in Table A5 in RADIO-AMP \cite{heinrich2024radioamplifiedimprovedbaselines}.}
    \label{tab:apdx:radio_dense_probe3d_metrics}
\end{table}

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r|cccc|ccc}
        \multirow{2}{*}{\bf{Upsampler}} & \multicolumn{4}{c|}{\bf{Pascal Context}} & \multicolumn{3}{c}{\bf{NYUDv2}} \\
          & \bf{SemSeg mIOU $\uparrow$} & \bf{Parsing mIoU $\uparrow$} & \bf{Saliency maxF $\uparrow$} & \bf{Surface Normals $\downarrow$} & \bf{SemSeg mIoU $\uparrow$} & \bf{Depth rmse $\downarrow$} & \bf{Surface Normals $\downarrow$} \\
        \hline
        RADIO-AMP-L & 82.87      & 74.32      & \bf{81.65} & \bf{16.15} & 61.42      & 0.458      & 18.57      \\ 
        \hline
        Baseline    & 82.88      & 75.02      & 80.55      & 16.49      & 62.64      & 0.448      & 18.09      \\
        Tile        & 83.07      & 75.28      & 80.56      & 16.60      & \bf{62.91} & 0.437      & 17.90      \\
        S2          & 83.09      & \bf{75.45} & 80.63      & 16.56      & 62.64      & \bf{0.436} & \bf{17.86} \\
        FeatUp      & 83.11      & 75.21      & 80.68      & 16.51      & 62.74      & 0.449      & 17.93      \\
        FeatSharp   & \bf{83.17} & 75.28      & 80.64      & 16.51      & 62.60      & 0.439      & 17.95
    \end{tabular}
    }
    \caption{Pascal Context and NYUDv2 multitask learning metrics. Following the setup of MLoRE \cite{jiang2024mlore} and RADIO-AMP \cite{heinrich2024radioamplifiedimprovedbaselines} with a convolutional probe. NOTE: We're only using their harness with a conv probe, and not using their architecture.}
    \label{tab:apdx:radio_pascal_nyud_metrics}
\end{table}

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r|ccccccccccc}
        \multirow{3}{*}{\bf{Upsampler}} & \bf{AI2D} & \bf{ChartQA} & \bf{DocVQA} & \bf{GQA} & \bf{InfoVQA} & \bf{MME} & \bf{MMMU} & \bf{OCR Bench} & \bf{POPE} & \bf{SEED} & \bf{TextVQA} \\
         & \bf{No Mask} & \multirow{2}{*}{\bf{Overall}} & \bf{Val} & \multirow{2}{*}{\bf{Accuracy}} & \multirow{2}{*}{\bf{Val}} & \multirow{2}{*}{\bf{Perception}} & \multirow{2}{*}{\bf{Val}} & \multirow{2}{*}{\bf{Accuracy}} & \multirow{2}{*}{\bf{F1}} & \multirow{2}{*}{\bf{All}} & \multirow{2}{*}{\bf{Val}} \\
         & \bf{Accuracy} & & \bf{Accuracy} \\
        \hline
        RADIO-AMP-L & \bf{79.2}  & 56.4       & \bf{49.2}  & 63.4       & \bf{29.8}  & \bf{1592.4}  & \bf{43.3}  & \bf{441}  & 87.6       & \bf{69.27} & \bf{66.7} \\ 
        \hline
        Baseline    & 78.04      & 57.32      & 47.12      & 63.41      & 28.78      & 1568.11      & 40.00      & 422      & 87.51      & 69.08   & 65.33 \\
        Tile        & 75.71      & 54.32      & 42.44      & 63.60      & 26.80      & 1541.61      & 40.33      & 400      & 86.63      & 68.62      & 63.78 \\
        S2          & 77.07      & 55.28      & 44.89      & 63.73      & 28.75      & 1549.50      & 42.33      & 405      & 87.14      & 68.96      & 64.86 \\
        FeatUp      & 78.40      & 55.56      & 45.31      & 63.60      & 26.98      & 1563.57      & 40.33      & 407      & 86.83      & 68.57      & 65.05 \\
        FeatSharp   & \bf{79.15} & \bf{57.56} & 46.39      & \bf{63.75} & 28.25      & 1564.41      & 42.22      & 416      & \bf{88.06} & 68.77      & 66.41
    \end{tabular}
    }
    \caption{VILA metrics, using the same setup from [\cite{heinrich2024radioamplifiedimprovedbaselines}, Table 9].}
    \label{tab:apdx:radio_vila_metrics}
\end{table}

\subsection{Additional Qualitative Visualizations}

In figures \ref{fig:apdx:radio_dfn_clip_viz_2} and \ref{fig:apdx:radio_siglip_viz}, we show more PCA feature visualizations coming from our trained RADIO models. In figure \ref{fig:apdx:radio_dfn_clip_viz_2} we can see that RADIO learned to mimic how tiling lacks global context, as the background-only tiles use a different feature space than those with background+content.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/radio_inference/dfn_clip_appendix.jpg}
    \vspace{-7mm}
    \caption{Visualization of our trained RADIO's DFN CLIP adaptor, using different teacher upsampling techniques.}
    \label{fig:apdx:radio_dfn_clip_viz_2}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/radio_inference/siglip_appendix.jpg}
    \vspace{-7mm}
    \caption{Visualization of RADIO's SigLIP adaptor, using different teacher upsampling techniques.}
    \label{fig:apdx:radio_siglip_viz}
\end{figure}

\subsection{Difference Visualization}

In figure \ref{fig:apdx:error_viz}, we show the difference heatmaps between FeatSharp/FeatUp and Bilinear upsampling. For DFN CLIP and SigLIP, we actually see that a lot of the differences are with high frequency noise. More intuitively, for the cleaner RADIO and SAM models, the differences are largely concentrated at the edges. Because the PCA projection down to 3D can sometimes distract from the true differences between representations (e.g. color flipping), these difference maps help show where the information is truly different between methods.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/featsharp_error_viz.pdf}
    \vspace{-7mm}
    \caption{Visualization of the differences between the FeatSharp or FeatUp algorithm, and bilinear upsampling.}
    \label{fig:apdx:error_viz}
\end{figure}

\section{Architecture Ablations}

\subsection{De-bias Module}

Adding the de-bias module yields a positive improvement in fidelity across all featurizers studied. We show the changes in fidelity metrics for all featurizers in table \ref{tab:bias_fidelity}. We also demonstrate that this module helps for both FeatSharp and FeatUp, as it occurs prior to upsampling, and is thus generally applicable. In figure \ref{fig:model-biases}, we visualize the learned biases, which are unique to each featurizer, but also how these biases can sometimes be directly visible in the output features of these models. Most obvious is SAM, which has windowing artifacts stemming from their use of windowed attention.

\begin{table}[h]
    \centering
    \resizebox{0.5\linewidth}{!}{
    \begin{tabular}{c|cccc}
        Model & FeatSharp 2x & FeatSharp 4x & FeatUp 2x & FeatUp 4x \\
        \hline
        DFN CLIP & 0.020 & 0.022 & 0.033 & 0.025 \\
        DINOv2-L & 0.121 & 0.110 & 0.164 & 0.144 \\
        PaliGemma & 0.017 & 0.021 & 0.030 & 0.023 \\
        RADIOv2.5-L & 0.208 & 0.173 & 0.144 & 0.138 \\
        SAM-H & 0.067 &  & 0.076 &  \\
        SigLIP & 0.014 & 0.017 & 0.033 & 0.019 \\
        ViT & 0.014 & 0.009 & 0.096 & 0.038 \\
    \end{tabular}
    }
    \caption{The delta change in multi-view consistency fidelity when applying the learned de-bias buffer. Positive values mean that the fidelity has improved, which is true for every model and upsampler tested.}
    \label{tab:bias_fidelity}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{resources/featsharp_model_biases.pdf}
    \vspace{-7mm}
    \caption{Visualization of the learned position biases for different models. All models have a bias signature, however some have very noticeable artifacts, which we visualize for SigLIP, PaliGemma, and SAM, where it's possible to see the artifacts in multiple different images and scales. We display the biases of the less apparent models in the bottom row.}
    \label{fig:model-biases}
\end{figure}

\subsection{FeatSharp Architecture}\label{sec:featsharp_arch_ablations}

\paragraph{Input Feature Selection}\label{sec:featsharp_arch:inputs}
Based on figures \ref{fig:featsharp_arch_diagram} and \ref{fig:featsharp_module_diagram}, there are important degrees of freedom in the design of the system. We demonstrate in table \ref{tab:bias_fidelity} that including the de-bias module always improves the distribution matching fidelity. Here, we look at some of the other design choices:

\begin{itemize}
    \item Should we use bilinear upsampling, or FeatUp, for the low-res upsampler? (or both)
    \item Should bilinear, tiling, or FeatUp be the residual pathway to the output?
    \item Should we use all three upsampling methods?
\end{itemize}

We show the results of this ablation in table \ref{tab:abl:featsharp_arch} for both our noisiest featurizer (SigLIP), and our cleanest (RADIO) as a sanity check that we aren't overfitting to a particular featurizer.

\begin{table}[h]
    \centering
    \begin{tabular}{r|ccc|ccc}
        \multirow{2}{*}{\bf{Arch}} & \multicolumn{3}{c|}{\bf{SigLIP}} & \multicolumn{3}{c}{\bf{RADIO}} \\
                & \bf{Fidelity $\uparrow$} & \bf{TV Loss $\downarrow$} & \bf{CRF Loss $\downarrow$} & \bf{Fidelity $\uparrow$} & \bf{TV Loss $\downarrow$} & \bf{CRF Loss $\downarrow$} \\
        \hline
        \multicolumn{7}{c}{\bf{Single Input}} \\
        \hline
        Bilinear         & 1.423      & 0.141      & 0.114      & 3.952      & 0.076      & 0.064       \\
        FeatUp           & 1.380      & \bf{0.025} & \bf{0.046} & 3.324      & \bf{0.034} & \bf{0.042}  \\
        Tiles            & 1.245      & 0.938      & 0.211      & 2.636      & 0.483      & 0.096       \\
        \hline
        \multicolumn{7}{c}{\bf{Two Inputs}} \\
        \hline
        Bilinear + Tiles & \bf{1.451} & 0.229      & 0.127      & \bf{4.216} & 0.141      & 0.073       \\
        FeatUp + Tiles   & 1.424      & 0.172      & 0.071      & 3.821      & 0.184      & 0.061       \\
        Tiles + Bilinear & 1.396      & 0.683      & 0.188      & 3.744      & 0.383      & 0.096       \\
        Tiles + FeatUp   & 1.357      & 0.790      & 0.190      & 3.517      & 0.424      & 0.093       \\
        \hline
        \multicolumn{7}{c}{\bf{Three Inputs}} \\
        \hline
        Bilinear First   & 1.450      & 0.222      & 0.126      & 4.184      & 0.126      & 0.070       \\
        FeatUp First     & 1.438      & 0.142      & 0.074      & 3.950      & 0.161      & 0.063       \\
        Tiles First      & 1.395      & 0.680      & 0.183      & 3.756      & 0.396      & 0.095       \\
    \end{tabular}
    \caption{Ablation over different FeatSharp-2x configurations. Single Input means that we only supply the respective buffer to the FeatSharp  module. For ``Two Inputs``, we compare different low-res upsamplers in conjunction with tiling, and also the residual pathway, where the first value indicates the residual path. The FeatSharp module must integrate the information from the other value into the residual. ``Three Inputs'' is similar to Two, except that we only care about which buffer is the residual path, owing to the fact that there's no intrinsic order preference in the weights for the secondary buffer(s). ``TV Loss'' stands for Total Variation Loss \cite{rudin1992tvloss}. CRF is Condition Random Field, and is essentially measuring how similar the semantics of two nearby RGB pixel patches are based on how similar the RGB values are. TV and CRF losses were not included in the gradient during training.}
    \label{tab:abl:featsharp_arch}
\end{table}

We also visualize the resulting feature maps of the different input configurations in figure \ref{fig:abl:featsharp_arch_viz}, as it's hard to get a feel for what this multi-view fidelity is telling us. It's clear both in the metrics (table \ref{tab:abl:featsharp_arch}) and the visualization that just using one of the three different feature maps largely retains the biases of those views (e.g. the bilinear result is roughly regular bilinear upsampling, the FeatUp input looks like vanilla FeatUp, etc.). We can also see the profound impact on the resulting maps based on which input feature map is the residual pathway. For the single input case, we can observe how the tile-only input results in a distribution shift, apparent because the color space has largely shifted. Once we look at 2+ inputs, the color spaces become consistent. Even though the bilinear-first configurations always have the highest fidelity, they also are clearly the blurriest. This is perhaps not surprising given that FeatUp's JBU upsampler has a strong edge prior, so incorporating it into FeatSharp will also hone in on edge boundaries. Also, regardless of 2+ input configuration, we can see that FeatSharp is able to refine the text, clearly leveraging the tile features. The similarity is very close to the tile-only input in that region. We do notice that using ``Bilinear + \textit{other(s)}'' yields the highest fidelities, but also that the resulting feature maps are relatively blurry. 

In order to not make an entire argument to prefer the use of FeatUp's JBU as the low-res upsampler due to the prettiness of the PCA features, we also consider alternative measures of the produced features. The Total Variation (TV) loss gives us a sense of how much ``noise'' is present in the produced features, simply based on accumulating the differences between neighbors. On its own, this doesn't tell us much, but in conjunction with the multi-view-consistency fidelity, and when that fidelity is roughly equal, it might be reasonable to assume that less variation is better. We can see in table \ref{tab:abl:featsharp_arch} that the use of FeatUp does indeed reduce this for SigLIP, but has the opposite effect on RADIO. The other prior that we consider is the CRF loss, which approximately translates to the idea that nearby regions that have a similar RGB color should probably also have similar semantics. The JBU also does a good job of reducing this for our noisiest SigLIP model, as well as for RADIO. It stands to reason that spurious model noise is penalized by CRF because it breaks visual/semantic correspondence. For both TV and CRF losses, we capture the metrics, but they do not participate in the gradient. So, we're purely measuring the latent behaviors.

An alternative argument, which doesn't require hand waving about whether less variance is a good thing, or if spatio-semantic similarity is necessarily good, we turn to Maximum Mean Discrepancy (MMD, \citep{gretton12mmd}) which is precisely defined as a way to test whether two sets of observations $X := \{x_1, ..., x_m\}$ and $Y := \{y_1, ..., y_n\}$ are sampled from the same distribution. It has the clear advantage in our setup in that $m$ doesn't have to be equal to $n$, or rather, we can have a different number of samples in $X$ than that in $Y$. Because we're upsampling, if we let the low-res distribution be $X$, then the high-res distribution can be $Y$, and then $n = u^2m$ with $u$ being the upsampling factor. Given a radial basis function kernel (RBF)

\begin{equation}
    k(x,y) = e^{-\gamma \left\lVert\mathbf{x} - \mathbf{y}\right\rVert^2}
\end{equation}

then we have 

\begin{equation}
    \text{MMD}_u^2\left[X,Y\right] = \frac{1}{m(m-1)} \sum_{\underset{i \neq j}{i,j \in m}}^m k(x_i,x_j) + \frac{1}{n(n-1)} \sum_{\underset{i \neq j}{i,j \in n}}^n k(y_i,y_j) - \frac{2}{mn} \sum_i^m \sum_j^n k(x_i,y_j)
    \tag{\citep{gretton12mmd}, Eq 3}
\end{equation}

we select $\gamma = \text{med}(\left\lVert x_i - x_j \right\rVert^2) \quad i \neq j$. We then collect results for Fidelity, TV Loss, CRF Loss, and MMD, for $4\times$ upsampling, and display the results in table \ref{tab:abl:featsharp_4x_arch}. We collect these results for SigLIP, DFN CLIP, and RADIO. It is clear that FeatSharp achieves the highest upsampling fidelities across the board. FeatUp produces the lowest TV and CRF losses. It achieving the lowest TV loss is intuitive given how smooth it tends to make object interiors, seen in the pca visualizations. We can see that the lower TV and CRF losses extends to FeatSharp when we apply JBU upsampling, as it achieves lower values than using bilinear upsampling for the residual pathway. The ``JBU + Tiles'' FeatSharp variant also does better on MDD versus ``Bilinear + Tiles'' across the board. It's curious that JBU alone has the worst MMD (probably due to over-smoothing), but the best when incorporated into FeatSharp (probably owing to smoothing out the noise). We can also see that generally either ``X + Tiles'' FeatSharp method produces similar fidelities, aside from RADIO, where bilinear actually does do a bit better. Most likely, this is because RADIO features are themselves already fairly clean, and at some point, the structural priors of JBU actually hurt, because they're eliminating some of the raw signal that bilinear upsampling preserves. In this case, the model always has access to the raw low-res signal with bilinear upsampling because we use an integer multiple upsampling factor, and our local attention window size is larger than this multiple. Given the totality of evidence, we choose to select ``JBU + Tiles'' as the default upsampling mechanism, as it's either the best, or nearly so, across the board, and particularly, it does better with the vision models that are not able to natively change their resolution very well.

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r|cccc|cccc|cccc}
        \multirow{3}{*}{\bf{Upsampler}} & \multicolumn{12}{c}{\bf{Featurizer} (4$\times$ Upsample, Long Recipe)} \\
         & \multicolumn{4}{c|}{\bf{SigLIP}} & \multicolumn{4}{c|}{\bf{DFN CLIP}} & \multicolumn{4}{c}{\bf{RADIO}} \\
         & Fidelity $\uparrow$ & TV $\downarrow$ & CRF $\downarrow$ & MMD $\downarrow$ & Fidelity $\uparrow$ & TV $\downarrow$ & CRF $\downarrow$ & MMD $\downarrow$ & Fidelity $\uparrow$ & TV $\downarrow$ & CRF $\downarrow$ & MMD $\downarrow$ \\
        \hline           % SigLIP                             % DFN CLIP                                     % RADIO
        Bilinear         & 1.348      & 0.048      & 0.129      & \bf{0.016} & 1.284      & 0.051      & 0.088      & 0.015      & 3.796      & 0.023      & 0.071      & 0.003      \\
        FeatUp (JBU)  & 1.375         & \bf{0.009} & \bf{0.047} & 0.025      & 1.326      & \bf{0.012} & \bf{0.032} & 0.023      & 3.680      & \bf{0.015} & \bf{0.064} & 0.003      \\
        \hline                                                                  
        Bilinear + Tiles & 1.522      & 0.105      & 0.093      & 0.020      & 1.508      & 0.167      & 0.062      & 0.014      & \bf{5.481} & 0.112      & 0.073      & 0.001      \\
        JBU + Tiles      & \bf{1.540} & 0.103      & 0.077      & 0.017      & \bf{1.514} & 0.157      & 0.046      & \bf{0.013} & 5.361      & 0.095      & 0.065      & \bf{0.001} \\
    \end{tabular}
    }
    \caption{Metrics for $4\times$ upsampling across SigLIP, DFN CLIP, and RADIO. We primarily compare whether to use bilinear or JBU upsampling for the residual branch of the FeatSharp module, but also report the same values for our two baseline methods, bilinear upsampling itself, and FeatUp (aka JBU upsampling).}
    \label{tab:abl:featsharp_4x_arch}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{resources/featsharp_inputs_viz_siglip.pdf}
    \includegraphics[width=\linewidth]{resources/featsharp_inputs_viz_radio.pdf}
    \vspace{-7mm}
    \caption{Feature visualizations of different input configurations for 2x upsampling.}
    \label{fig:abl:featsharp_arch_viz}
\end{figure}

\paragraph{Local Attention Window Size}\label{sec:featsharp_arch:window_size}

In figure \ref{fig:abl:window_size} we run an ablation over local attention window sizes between 1 and 11. We notice that either 3 or 5 appear to be optimal.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{resources/featsharp_wndsize_plot.pdf}
    \vspace{-7mm}
    \caption{Ablation study over the choice of window size and upsampling factor for the FeatSharp module.}
    \label{fig:abl:window_size}
\end{figure}

\paragraph{Do We Even Need Attention/MLP?}

As can be seen in figure \ref{fig:abl:window_size}, the choice of window size has a very small impact on the resulting fidelity. However, we can also see that FeatSharp is achieving much higher fidelity scores than Bilinear and FeatUp. So, we also study what effect the attention block, and the MLP, are having on the resulting quality. We use the ``Bilinear + Tile'' input configuration from section \ref{sec:featsharp_arch:inputs}, and when applicable, use a window size of 5 from section \ref{sec:featsharp_arch:window_size}. We show these results in table \ref{tab:abl:featsharp_block_arch}. We can see that in all cases, ``Attention + MLP'' (e.g. the setting proposed in this work) produces the highest fidelity for both SigLIP and RADIO. However, when running the normal setup (``Short'' in the table), we notice that ``Attention + MLP'' is usually only barely better than ``MLP'', meaning that we weren't getting much improvement from the local attention window, and that goes a long way toward explaining the relative insensitivity to the window size in figure \ref{fig:abl:window_size}. Inspired by figure \ref{fig:abl:featsharp_arch_viz}, we notice that the longer training recipe results in much sharper images. In table \ref{tab:abl:featsharp_block_arch} we study the effect of running just the MLP for the ``Long'' recipe. We can see that while the fidelity continues to improve, it doesn't keep up with the ``Attention + MLP'' setting, demonstrating that the attention module is indeed helpful.

\begin{table}[]
    \centering
    \begin{tabular}{r|cc|cc|cc|cc}
        \multirow{4}{*}{\bf{Modules}} & \multicolumn{8}{c}{\bf{Fidelity}} \\
                        & \multicolumn{4}{c|}{\bf{SigLIP}} & \multicolumn{4}{c}{\bf{RADIO}} \\
                        & \multicolumn{2}{c|}{2$\times$ Upsample} & \multicolumn{2}{c|}{4$\times$ Upsample} & \multicolumn{2}{c|}{2$\times$ Upsample} & \multicolumn{2}{c}{4$\times$ Upsample} \\
                        & \bf{Short}  & \bf{Long}  & \bf{Short} & \bf{Long} & \bf{Short}  & \bf{Long}  & \bf{Short} & \bf{Long}\\
        \hline
        Linear          & 1.444      & -          & 1.434      & -          & 4.134      & -          & 4.601      & -          \\
        Attention       & 1.437      & -          & 1.426      & -          & 4.017      & -          & 4.527      & -          \\
        MLP             & 1.444      & 1.514      & 1.433      & 1.514      & 4.151      & 4.668      & 4.599      & 5.128      \\
        Attention + MLP & \bf{1.452} & \bf{1.544} & \bf{1.442} & \bf{1.525} & \bf{4.216} & \bf{4.957} & \bf{4.656} & \bf{5.361} \\
    \end{tabular}
    \caption{Fidelity metrics for different combinations of blocks in the FeatSharp module (\ref{fig:featsharp_module_diagram}). The ``Long'' recipe trains for 3$\times$ longer than the short recipe. We only study the ``MLP'' vs ``Attention + MLP'' configurations in the long recipe because those were the top two configurations in the short recipe.}
    \label{tab:abl:featsharp_block_arch}
\end{table}

\section{Implementation Details}\label{sec:implementation}

\paragraph{Upsampler Training}
We leverage the same training harness as in FeatUp \citep{fu2024featup}, including leveraging the same attention downsampler. We disable the use of the CRF loss that was present in the FeatUp config. Parameters in table \ref{tab:upsampler_hparams}.

\begin{table}[]
    \centering
    \begin{tabular}{r|c|cc}
        \bf{Hyperparameter}  & \bf{FeatUp JBU}        & \bf{Regular}           & \bf{Long} \\
        \hline
        Num GPUS             & 1                      & 8                      & 8                 \\
        Batch Size (per GPU) & 4                      & 4                      & 4                 \\
        Batch Size (total)   & 4                      & 32                     & 32                \\
        Num Steps            & 2,000                  & 3,000                  & 9,000             \\
        Optimizer            & NAdam                  & NAdam                  & NAdam             \\
        Learning Rate        & 0.001                  & 0.001                  & 0.001             \\
        Downsampler          & Attention (k=7)        & Attention (k=7)        & Attention (k=7)   \\
        Num Jitters          & 5                      & 5                      & 5                 \\
        CRF Weight           & 0.001                  & 0                      & 0                 \\
        TV Weight            & 0                      & 0                      & 0                 \\
        Feature Normalization& LayerNorm              & PHI-S                  & PHI-S             \\
        Dataset              & COCO                   & SA-1B                  & SA-1B             \\
        Multi-view Augs      & Scale, Shift           & \multicolumn{2}{c}{Scale, Shift, HFlip, Rotate, Perspective}
    \end{tabular}
    \caption{Training hyperparameters. ``FeatUp JBU'' refers to the settings in the official \href{https://github.com/mhamilton723/FeatUp}{https://github.com/mhamilton723/FeatUp}. Unless otherwise specified, we report numbers based on the ``Long'' schedule, which includes FeatUp reproduction values, to maintain fairness.}
    \label{tab:upsampler_hparams}
\end{table}

\paragraph{RADIO Training}
We follow the staged setup in \cite{heinrich2024radioamplifiedimprovedbaselines} section 4.2, with stages 1 and 2 being exactly identical. For stage 3, in the hi-res student branch, instead of bilerp downsampling the student features to match DFN CLIP and SigLIP (RADIO-AMP baseline), we use our various upsampling methods to create hi-res feature maps which the student matches. We use our trained $3\times$ upsamplers for the task. For FeatSharp, because we have the learned de-bias buffer which operates on the original model resolution, we also choose to apply this to the teachers in the low-res partition, as it represents the fixed bias of the teacher model, and is thus not particularly useful information.

\section{Additional Benchmarks}

\subsection{Probe3d}
In table \ref{tab:probe3d-depth-metrics} we show the result of various configurations in Probe3d's \cite{elbanani2024probing} depth probing for both DFN CLIP and RADIO. We can see that FeatUp produces the best results, however, we also demonstrate that this is likely due to the strong structural prior to the method, as the single best configuration was to use a FeatUp JBU stack with randomly initialized weights. Both FeatUp and FeatSharp are able to strongly improve over any configuration of regular DFN CLIP. For RADIO, we can see that both FeatUp and FeatSharp are still able to improve over baseline, albeit the margins are much smaller. While FeatSharp $4\times$ does achieve the highest scores, the margin is too small to be significant compared to $2\times$ and FeatUp, but still better than baseline. We observe essentially the same trend in table \ref{tab:probe3d-navi-metrics}, where the random JBU stack works the best for DFN CLIP, and then FeatUp/FeatSharp are comparable for RADIO.

\begin{table*}[!h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cccc|cccc}
        \multirow{2}{*}{\textbf{Vision Encoder}} & \multirow{2}{*}{\textbf{Input Res}} & \multirow{2}{*}{\textbf{Upsampling Method}} & \multirow{2}{*}{\textbf{Output Tokens}} & \multicolumn{4}{c|}{\textbf{Depth (Scale Aware)}} & \multicolumn{4}{c}{\textbf{Depth (Scale Invariant)}} \\
         & & & & \textbf{d1} $\uparrow$ & \textbf{d2} $\uparrow$ & \textbf{d3} $\uparrow$ & \textbf{RMSE} $\downarrow$ & \textbf{d1} $\uparrow$ & \textbf{d2} $\uparrow$ & \textbf{d3} $\uparrow$ & \textbf{RMSE} $\downarrow$\\
        \hline
        % DFN CLIP & Interpolate Position Embeddings $518 \times 518$ & 0.3798 & 0.6638 & 0.8240 & 0.1442 & 0.4860 & 0.7503 & 0.8684 & 0.1220 \\        
        % DFN CLIP & Upsample features $512 \times 512$ & 0.4035 & 0.6920 & 0.8435 & 0.1358 & 0.5088 & 0.7720 & 0.8824 & 0.1147  \\  
        \multirow{11}{*}{DFN CLIP} & $378^2$  & -          & $27^2$  & 0.303 & 0.575 & 0.772 & 0.168 & 0.440 & 0.710 & 0.842 & 0.134 \\
         & $756^2$  & -          & $54^2$  & 0.291 & 0.558 & 0.757 & 0.173 & 0.426 & 0.695 & 0.829 & 0.140 \\
         & $1512^2$ & -          & $108^2$ & 0.280 & 0.535 & 0.733 & 0.181 & 0.399 & 0.664 & 0.805 & 0.152 \\
        \cline{2-12}
         & $378^2$  & $2\times$ Upsample features & $54^2$ & 0.301 & 0.573 & 0.773 & 0.168 & 0.443 & 0.713 & 0.844 & 0.133 \\ 
        \cline{2-12}
         & $(2 \times 2) \times 378^2$  & Tiling & $54^2$  & 0.248 & 0.489 & 0.697 & 0.193 & 0.354 & 0.616 & 0.771 & 0.165 \\
         & $(4 \times 4) \times 378^2$  & Tiling & $108^2$ & 0.218 & 0.434 & 0.634 & 0.212 & 0.317 & 0.567 & 0.732 & 0.184 \\
        \cline{2-12}
         & $378^2$ & FeatUp $2\times$ & $54^2$  & 0.430 & 0.712 & 0.851 & 0.128 & 0.538 & 0.793 & 0.894 & 0.107 \\
         & $378^2$ & FeatUp $4\times$ & $108^2$ & 0.435 & 0.716 & 0.853 & 0.128 & 0.542 & 0.796 & 0.896 & 0.107 \\
         & $378^2$ & FeatUp $4\times$ (Random Weights) & $108^2$ & \textbf{0.440} & \textbf{0.723} & \textbf{0.858} & \textbf{0.126} & \textbf{0.554} & \textbf{0.805} & \textbf{0.900} & \textbf{0.105}\\
        \cline{2-12}
         & $378^2$ & FeatSharp $2\times$ & $54^2$ & 0.398 & 0.685 & 0.837 & 0.136 & 0.512 & 0.772 & 0.882 & 0.113 \\
         & $378^2$ & FeatSharp $4\times$ & $108^2$ & 0.419 & 0.705 & 0.847 & 0.131 & 0.527 & 0.785 & 0.890 & 0.109 \\
        \hline
        \hline
        \multirow{7}{*}{RADIO} & $512^2$  & -         & $32^2$  & 0.472 & 0.749 & 0.873 & 0.118 & 0.584 & 0.827 & 0.916 & 0.097 \\
         & $1024^2$ & -         & $64^2$  & 0.478 & 0.756 & 0.877 & 0.115 & 0.589 & 0.831 & 0.918 & 0.095 \\
         & $2048^2$ & -         & $128^2$ & 0.456 & 0.739 & 0.868 & 0.120 & 0.571 & 0.820 & 0.911 & 0.099  \\
        \cline{2-12}
         & $512^2$  & FeatUp $2\times$ & $64^2$  & 0.482 & 0.764 & 0.885 & 0.114 & 0.606 & 0.840 & 0.921 & 0.092 \\
         & $512^2$  & FeatUp $4\times$ & $128^2$ & 0.481 & 0.763 & 0.885 & 0.114 & 0.604 & 0.838 & 0.920 & 0.092 \\
        \cline{2-12}
         & $512^2$  & FeatSharp $2\times$ & $64^2$  & 0.480 & 0.766 & 0.887 & 0.113 & 0.604 & 0.840 & 0.923 & 0.091 \\
         & $512^2$  & FeatSharp $4\times$ & $128^2$ & \textbf{0.487} & \textbf{0.769} & \textbf{0.888} & \textbf{0.112} & \textbf{0.610} & \textbf{0.843} & \textbf{0.924} & \textbf{0.090} 
    \end{tabular}
    }
    \caption{Probe3D - Depth metrics. Linear probe over output features. ``Random Weights'' refers to a randomly initialized, untrained, model.}
    \label{tab:probe3d-depth-metrics}
\end{table*}

\begin{table}[!h]
    \centering
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{c|ccc|cccc}
        \multirow{2}{*}{\bf{Vision Encoder}} & \multirow{2}{*}{\bf{Input Res}} & \multirow{2}{*}{\bf{Upsampling Method}} & \multirow{2}{*}{\bf{Output Tokens}} & \multicolumn{4}{c}{\bf{Recall}} \\
         & & & & \bf{Avg} & \bf{0.01m} & \bf{0.02m} & \bf{0.05m} \\
        \hline
        \multirow{7}{*}{DFN CLIP} & $378^2$ & - & $27^2$ & 49.26 & 26.02 & 44.47 & 77.30 \\
         & $756^2$ & - & $54^2$ & 47.06 & 23.55 & 41.72 & 75.89 \\
         & $1512^2$ & - & $108^2$ & 41.59 & 18.08 & 35.30 & 71.41 \\
        \cline{2-8}
         & $378^2$ & FeatUp $2\times$ & $54^2$  & 54.40 & 30.99 & 51.20 & 81.02 \\
         & $378^2$ & FeatUp $4\times$ & $108^2$ & 54.62 & 31.05 & 51.45 & 81.36 \\
         & $378^2$ & FeatUp $4\times$ (Random Weights) & $108^2$ & \textbf{55.72} & \textbf{32.23} & \textbf{53.05} & \textbf{81.89} \\
        \cline{2-8}
         & $378^2$ & FeatSharp $2\times$ & $54^2$ & 53.00 & 31.21 & 49.05 & 78.73 \\
         & $378^2$ & FeatSharp $4\times$ & $108^2$ & 53.62 & 31.49 & 49.60 & 79.77 \\
        \hline
        \hline
        \multirow{7}{*}{RADIO} & $512^2$ & - & $32^2$ & 59.49 & 37.20 & 56.44 & 84.82 \\
        & $1024^2$ & - & $64^2$  & 58.23 & 37.21 & 54.70 & 82.77 \\
        & $2048^2$ & - & $128^2$ & 57.22 & 34.99 & 53.53 & 83.15 \\
        \cline{2-8}
         & $512^2$  & FeatUp $2\times$ & $64^2$  & 60.39 & 38.52 & 57.51 & 85.16  \\
         & $512^2$  & FeatUp $4\times$ & $128^2$ & \textbf{60.72} & 39.01 & 57.87 & \textbf{85.29}  \\
        \cline{2-8}
         & $512^2$  & FeatSharp $2\times$ & $64^2$  & 60.69 & \textbf{40.11} & \textbf{57.95} & 84.02 \\
         & $512^2$  & FeatSharp $4\times$ & $128^2$ & 60.46 & 39.95 & 57.61 & 83.81 \\
    \end{tabular}
    }
    \caption{Probe3D - NAVI Correspondence. ``Random Weights'' refers to a randomly initialized, untrained, model.}
    \label{tab:probe3d-navi-metrics}
\end{table}

\subsection{NYUDv2}

We also report metrics on NYUDv2 \cite{silberman2012nyud} in table \ref{tab:nyud-metrics} for both DFN CLIP and RADIO, similar to Probe3d configurations. We use the MLoRE~\cite{jiang2024mlore} harness and their conv probing for all configurations. We only use features from the final layer of the models. We can see here that unlike Probe3d, FeatSharp does a noticeably better job than FeatUp across the board, and with FeatSharp $2\times$, we get the strongest results for DFN CLIP. For RADIO, it's much tighter between FeatSharp and Baseline, however, FeatSharp is significantly better than FeatUp.

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc|cccc}
        \bf{Vision Encoder} & \bf{Input Res} & \bf{Upsampling Method} & \bf{Output Tokens} & \bf{SemSeg mIoU} $\uparrow$ & \bf{Depth RMSE} $\downarrow$ & \bf{Surf Normals} $\downarrow$ & \bf{Edge Loss} $\downarrow$ \\
        \hline
        \multirow{6}{*}{DFN CLIP} & $378^2$ & - & $27^2$ & 53.15 & \textbf{0.551} & 23.49 & 0.130 \\
         & $756^2$ & - & $54^2$ & 51.11 & 0.589 & 23.33 & 0.127 \\
        \cline{2-8}
         & $378^2$ & FeatUp $2\times$ & $54^2$     & 52.51 & 0.589 & 23.66 & 0.129 \\
         & $378^2$ & FeatUp $4\times$ & $108^2$    & 52.45 & 0.601 & 24.15 & 0.129 \\ 
        \cline{2-8}
         & $378^2$ & FeatSharp $2\times$ & $54^2$  & \textbf{54.29} & 0.579 & \textbf{23.14} & 0.126 \\
         & $378^2$ & FeatSharp $4\times$ & $108^2$ & 53.74 & 0.615 & 23.93 & \textbf{0.125}\\
        \hline
        \hline
        \multirow{6}{*}{RADIO} & $512^2$ & - & $32^2$ & 60.80 & 0.486 & 19.45 & 0.127 \\
         & $1024^2$ & - & $64^2$ & 62.15 & \textbf{0.479} & \textbf{18.55} & 0.123 \\
        \cline{2-8}
         & $512^2$ & FeatUp $2\times$ & $64^2$     & 60.64 & 0.490 & 19.32 & 0.124 \\
         & $512^2$ & FeatUp $4\times$ & $128^2$    & 60.55 & 0.493 & 19.57 & 0.125 \\
        \cline{2-8}
         & $512^2$ & FeatSharp $2\times$ & $64^2$  & \textbf{62.23} & 0.485 & 19.25 & 0.123 \\
         & $512^2$ & FeatSharp $4\times$ & $128^2$ & 61.71 & 0.511 & 19.82 & \textbf{0.122} \\
    \end{tabular}
    }
    \caption{Multitask metrics on NYUDv2 \cite{silberman2012nyud} using the MLoRE \cite{jiang2024mlore} convolutional probe harness.}
    \label{tab:nyud-metrics}
\end{table}

\section{Throughput Analysis}

In \eqref{eq:cost_inequality}, we predict that based on the quadratic scaling of attention, theoretically FeatSharp should always be cheaper than running the base model at the upsampled resolution. FeatSharp's cost is linear in the number of tokens, whereas a ViT is quadratic. In figure \ref{fig:vith_throughput}, we show the results of this prediction on actual hardware. As can be seen with the ``Actual'' curve, the picture is a bit more complex than pure quadratic scaling, as between 1x and 3x upsample factors, the scaling is actually sub-linear, which likely reflects the period where self-attention is memory bound, and not compute bound, thus adding extra tokens doesn't proportionally increase the cost. Specifically, at 1.85x upsampling, we achieve the lowest time per token, and from then on, the cost approximately linearly increases (note that time per token is the first derivative of the time per image, so linear growth implies quadratic scaling, as predicted). Because FeatUp only runs the featurizer once, and its upsampling operation is cheap, we can see that it achieves strong scaling regardless of resolution. FeatSharp requires $u^2 + 1$ inferences with $u$ being the upsample factor, so its cost is higher. Likely due to non-optimal kernels, we can see that FeatSharp does start operating faster than the base model until about 3.3x upsampling ($\approx 1260^2$px). However, also as predicted by \eqref{eq:cost_inequality}, FeatSharp's scaling is linear.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{resources/vith_throughput.pdf}
    \caption{Throughput of a ViT-H/14 model (e.g. DFN CLIP) achieved with an A100 GPU, BS=1. The blue ``Actual'' curve reflects the time per token spent at various resolutions by the base model. ``Linear Scale'' assumes a constant time per token, based on the cost of 1x upsample factor. Note that ``Time Per Token'' is effectively the first derivative of ``Time Per Image'', so a linear growth in per-token represents quadratic growth in per-image.}
    \label{fig:vith_throughput}
\end{figure}

\section{Effects of ``Over-Tiling''}\label{sec:apdx:overtiling}

In figure \ref{fig:radio_dfn_clip_adaptor}, we can see that RADIO had learned some idiosyncratic representations when using the Tile and S2 upsampling algorithms. The effects are even more apparent in figures \ref{fig:apdx:radio_dfn_clip_viz_2} and \ref{fig:apdx:radio_siglip_viz} where color spaces can entirely flip. To understand what's happening, we rely on the pretrained RADIOv2.5-L model, which has strong scale equivariance properties \cite{heinrich2024radioamplifiedimprovedbaselines}, and first see that as the number of tiles increases, the MSE error between the brute-force inference at a given resolution and the tiling of that resolution, increases. We show these results in figure \ref{fig:tile_level_vs_mse}. Visually, we argue that the major increases in MSE owes largely to regions that lack context, making it difficult for the encoder (in this case RADIO), to come up with a reasonable representation of the tile-crop. We visualize this in figure \ref{fig:overtiling_viz}. Notably, we can see that the $8\times8$ tiling difference images are generally whiter, indicating a general drift towards higher error. We can also see particular tiles that have more error, such as the notecard in row 4, which gets nearly forgotten due to context. We can also see that there are a lot of errors with the car on row 5. The bottom center of the floor on row 6 has the same issue. So, while there appears to be a general upward error drift, it's exacerbated in regions without much variation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{resources/tile_level_vs_mse.pdf}
    \caption{MSE error between brute-force evaluation of RADIOv2.5-L at a given resolution ($512\text{px}^2*(\text{tile-level})$ and the tiling at the same resolution.}
    \label{fig:tile_level_vs_mse}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{resources/overtiling_viz.pdf}
    \caption{Visualization of the errors between running RADIOv2.5-L at a given resolution, and the equivalent of tiling it at the same resolution. The difference images are black when there is no difference, and white where there are large differences. The difference is computed as the euclidean distance of the full features, not their PCA projections.}
    \label{fig:overtiling_viz}
\end{figure}

\section{FeatUp's Two Methods}

The FeatUp \citep{fu2024featup} paper presented two methods for feature upsampling: The JBU-Stack, and the Implicit network. The resulting quality of these two approaches are quite different, with the implicit network producing much finer detailed maps, but having the major drawback that it requires training a network per-image, and is thus computationally prohibitive (\~1 minute per image). The JBU stack is effective at preserving edges, but also has the effect of over-blurring object interiors. We show Figure 5 from \cite{fu2024featup} in our figure \ref{fig:featup_jbu_vs_implicit}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{resources/featup_jbu_vs_implicit.pdf}
    \vspace{-17mm}
    \caption{FeatUp's two upsampler algorithms. Taken directly from their \cite{fu2024featup} Figure 5.}
    \label{fig:featup_jbu_vs_implicit}
\end{figure}