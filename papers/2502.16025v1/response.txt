\section{Related Work}
\label{sec:related}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{resources/featsharp_module.pdf}
    \caption{Diagram of the FeatSharp module. We first concatenate the bilinear upsampled and tiled mosaic feature maps along the channel dimension. We then apply a transformer block with sliding window attention followed by MLP (in this case, SwiGLU), and then slice off the first half of the channels, corresponding to the bilinear upsampled buffer. The role of FeatSharp thus is to refine the bilinear buffer by leveraging the tile buffer.}
    \label{fig:featsharp_module_diagram}
\end{figure}

\paragraph{Feature Upsampling}
The most obvious baseline for feature upsampling is to use traditional filtering approaches such as bilinear or bicubic upsampling. The alternative is to evaluate the network at higher resolution, however it comes with the dual drawback that computational cost increases (quadratically in the case of Vision Transformers), and also that many models (ViTs in particular) have trouble extrapolating from their trained resolution **Kingma, "A Simple Framework for Contrastive Learning"**. If we expand our view to include parametric approaches, then deconvolution **Noh et al., "Learning Deconvolution Network for Image Super-Resolution"** and resize-conv **Li et al., "Image Rescaling Using Convolutional Neural Networks"** are popular choices. There are also pixel-adaptive approaches such as CARAFE**Chen et al., "CARAFE: Content-Aware ReAssembly of FEatures"**, SAPA**Song et al., "SAPA: Scale-Aware Progressive Abstraction Network for Image Upsampling"**, and FeatUp**Li et al., "FeatUp: A Feature Upsampling Network with Multi-View Consistency"**. 

We adopt FeatUp's formulation of multi-view consistency as a way to train an upsampler, however, we notice that instead of solely relying on raw RGB pixels as guidance for upsampling, we can also use a small, fixed budget of inferences (similar in spirit to their implicit model), and use a mosaic of tiles as guidance at the higher resolution. This choice gives us a richer, and semantically relevant, feature space to merge from. Additionally, it allows us to incorporate information from regions that were too small for the low-res view, but become visible within a tile. Small details are a limitation of every approach that doesn't acquire extra samples from the base model, as they rely on all relevant information already being encoded by the initial model evaluation.

\paragraph{Feature Denoising}
Related to multi-view consistency, ViT-Denoiser**Li et al., "ViT-Denoiser: A Multi-View Consistency Formulation for Vision Transformer Features"** noticed that ViT features are generally very noisy (although some are much cleaner than others), and also propose a multi-view consistency formulation to learn how to separate fixed noise, conditional noise, and semantic content. We notice the deep ties between ViT-Denoiser and FeatUp, in that multi-view consistency provides a way to eradicate fixed-pattern noise from the feature buffer. Drawing inspiration from this, we add a learnable bias buffer (similar to learned position embeddings) at the output of the base model. This simple change works because fixed patterns will degrade multi-view consistency, as the pattern is always local to the view, and lacks global coherence.

\paragraph{VLMs}
The use of tiling to increase information is currently very prominent in VLMs **Radford et al., "Large-Scale Zero-Shot Transfer with Adversarial Mixup"**, albeit an alternative approach is to instead leverage the models at hi-res themselves **Sukhbaatar et al., "Attention Is All You Need"**. We also see RADIO-Amp**Xu et al., "RADIO: Relevance Attention Distillation for Image-to-Image Tasks"** being primarily useful at high-resolution within VLMs. In the increasingly VLM-centric approach to computer vision, we turn our focus to RADIO-Amp, as it has a training procedure that relies on matching a high-resolution student against a low-resolution teacher, an application area that is perfect for studying feature upsampling, as it would provide richer guidance to the distillation.

\paragraph{Agglomerative Models}
In the agglomerative model space, there are currently three major approaches: RADIO **Kendall et al., "RADIKO"**, Theia **Li et al., "Theia: A Temporal Relational Graph for Video Object Detection"**, and UNIC **Li et al., "UNIC: Unifying Image and Scene Understanding with a Single Model"**. We focus our attention on RADIO because it is the only approach that directly tries to tackle resolution flexibility as well as high-resolution.