% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
%\usepackage[review]{acl}
\usepackage[]{acl}

% Standard package includes
\usepackage{times}
\usepackage{booktabs, multirow}
\usepackage{graphicx, subcaption}
\usepackage{algorithm, algorithmic}
\usepackage{latexsym, amsmath, amsthm, amssymb, amsfonts, bm} % amsthm: proof, amsfonts: mathbb, bm: bold.
\usepackage{xcolor, tcolorbox}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\newtheorem{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
%\newtheorem{proposition}{Proposition}
\newcommand{\llm}[0]{\textsc{llm}}
\newcommand{\llms}[0]{\textsc{llm}s}
\newcommand{\qnn}[0]{\textsc{qnn}}
\newcommand{\qnns}[0]{\textsc{qnn}s}
\newcommand{\vqcs}[0]{\textsc{vqc}s}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript, and will typically save some space.
\usepackage{microtype}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in the typewriter font.
\usepackage{inconsolata}
% If the title and author information does not fit in the area allocated, uncomment the following
%\setlength\titlebox{<dim>}
% and set <dim> to something 5cm or larger.

\title{Large Language Models Can Help Mitigate Barren Plateaus}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

%\author{anonymous}
\author{Jun Zhuang \\
  Department of Computer Science \\
  Boise State University \\
  \texttt{junzhuang@boisestate.edu} \\
\And
 Chaowen Guan \\
  Department of Computer Science \\  
  University of Cincinnati \\
 \texttt{guance@ucmail.uc.edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum Neural Networks (\qnns) have emerged as a promising approach for various applications, yet their training is often hindered by barren plateaus (BPs), where gradient variance vanishes exponentially as the model size increases. To address this challenge, we propose a new Large Language Model (\llm)-driven search framework, AdaInit, that iteratively searches for optimal initial parameters of \qnns\ to maximize gradient variance and therefore mitigate BPs. Unlike conventional one-time initialization methods, AdaInit dynamically refines \qnn's initialization using \llms\ with adaptive prompting. Theoretical analysis of the Expected Improvement (EI) proves a supremum for the search, ensuring this process can eventually identify the optimal initial parameter of the \qnn. Extensive experiments across four public datasets demonstrate that AdaInit significantly enhances \qnn's trainability compared to classic initialization methods, validating its effectiveness in mitigating BPs.
\end{abstract}
% TL;DR: We propose a new LLM-driven framework designed for mitigating barren plateaus.

\section{Introduction}
\label{sec:intro}
In recent years, there have been significant advancements in quantum computing, particularly with the advent of noisy intermediate-scale quantum (NISQ) devices~\cite{preskill2018quantum}. Within this research landscape, quantum neural networks (\qnns), which integrate quantum circuits with classical deep-learning layers, have been widely applied in various domains, such as quantum machine learning~\cite{stein2021qugan, liang2024napa}, quantum physics~\cite{chen2022quantum}, and quantum hardware architecture~\cite{liang2023unleashing}. However, recent studies reveal that the performance of \qnns\ may be hindered due to gradient issues, such as barren plateaus (BPs)~\cite{McClean2018landscapes}, referring to a kind of gradient issue that the initialization of \qnns\ might be trapped on a flattened landscape at the beginning of training. \citet{McClean2018landscapes} first systematically investigate BPs and affirm that the gradient variance will exponentially decrease as the model size increases when the \qnns\ satisfy the assumption of the 2-design Haar distribution. Under this circumstance, most gradient-based approaches would fail.
To better illustrate the BPs' mitigation process, we present an example in Fig.~\ref{fig:BPs_example}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/fig_bp.pdf}
  \caption{Example of BPs' mitigation process. A flattened loss landscape ($1^{st}$ image), a.k.a. BPs, could be gradually recovered to the normal case ($3^{rd}$ image) by applying mitigation methods.}
\label{fig:BPs_example}
\end{figure}

Numerous studies have been devoted to mitigating the barren plateau issues, whereas within these studies, initialization-based strategies have proven to be very effective by initializing \qnns' parameters with well-designed distributions~\cite{sack2022avoiding}. However, most initialization-based strategies aim to mitigate BPs by one-time initialization with a well-designed data distribution, which may not be generalized to common data distribution. Initializing \qnns' parameters using deep-learning generative models could be a feasible solution since they can adaptively model data distribution on various datasets~\cite{friedrich2022avoiding}. Within the category of generative models, large language models (\llms) have demonstrated their superior performance in recent years~\cite{achiam2023gpt, dubey2024llama, pham2024word, wang2024enhancing}. Nonetheless, until now, leveraging the superior generative performance of \llms\ to alleviate BPs is still under-explored.

To fill this research gap, we propose a new \llm-driven framework, namely {\bf AdaInit}, that can {\bf Ada}ptively generate {\bf Init}ial model parameters $\theta_0$ for \qnns. After iterative generation, our framework obtains such an initial model parameter that can maximize the gradient variance for \qnns' training.
Specifically, for each iteration, we estimate the posterior of $\theta_0$ using a generative model, such as a \llm, given an adaptively improved prompt and a prior distribution as inputs. After posterior estimation, we train a \qnn\ initialized with the generated $\theta_0$ and compute the gradient variance. We then evaluate the gradient variance by expected improvement (EI) and update the prompts if the EI is improved. Besides updating prompts, we store the corresponding $\theta_0$ and return the optimal one at the end.
In this study, we theoretically analyze the submartingale property of EI and rigorously prove that the iterative search can eventually reach a supremum, which indicates that our framework can ultimately identify the optimal initial model parameters that maximize the gradient variance.
Besides, we conduct extensive experiments to demonstrate the effectiveness of our proposed framework across four public datasets. The results reveal that our framework can maintain higher gradient variances against three classic initialization methods and two popular initialization-based strategies for mitigating BPs.
Overall, we summarized our main primary contributions as follows:
\begin{itemize}[itemsep=-2mm]
  \item We propose a new \llm-driven framework, AdaInit, for mitigating BPs. To the best of our knowledge, we first leverage \llms\ to model \qnns' initial parameters for adaptively mitigating BPs.
  \item We theoretically analyze the submartingale property of expected improvement (EI) and rigorously prove the supremum of iterative search, providing theoretical validation for our search framework.
  \item Extensive experiments across four public datasets demonstrate that as the model size of \qnns\ increases, our framework can maintain higher gradient variances against classic initialization methods.
\end{itemize}


\section{Methodology}
\label{sec:method}
In this section, we first introduce the preliminary background and formally state the problem we aim to address in this study. Besides, we present our proposed framework in detail and conduct a theoretical analysis of the expected improvement (EI).

\subsection{Preliminary Background}
{\bf Variational Quantum Circuits (\vqcs)} play a core role in quantum neural networks (\qnns). Typical \vqcs\ consist of a finite sequence of unitary gates $U(\bm{\theta})$ parameterized by $\bm{\theta} \in \mathbb{R}^{LNR}$, where $L$, $N$, and $R$ denote the number of layers, qubits, and rotation gates. $U(\bm{\theta})$ can be formulated as:
%\vspace{-1mm}
\begin{equation}
    U(\bm{\theta}) = U(\theta_1, ..., \theta_L) = \prod_{l=1}^{L} U_l(\theta_l),
\label{eqn:vqc}
\end{equation}
%\vspace{-1mm}
where $U_l(\theta_l) = e^{-i\theta_lV_l}$.
%$V_l$ is a Hermitian operator, and $W_l$ is unitary operator that doesn't depend on $\theta_l \in \mathbb{R}^{NR}$.

\qnns, which are built by wrapping neural network layers with \vqcs, can be optimized using gradient-based methods. To optimize \qnns, we first define the loss function $E(\bm{\theta})$ of $U(\bm{\theta})$ as the expectation over Hermitian operator $H$:
\begin{equation}
    E(\bm{\theta}) = \langle0| U(\bm{\theta})^{\dagger} H U(\bm{\theta}) |0\rangle.
\label{eqn:loss_fn}
\end{equation}

Given the loss function $E(\bm{\theta})$, we can further compute its gradient by the following formula:
\begin{equation}
    \partial_k E \equiv \frac{\partial E(\bm{\theta})}{\partial \theta_k} = i\langle0| U_{-}^{\dagger} \left[ V_k, U_{+}^{\dagger} H U_{+} \right] U_{-} |0\rangle,
\label{eqn:gradient}
\end{equation}
where we denote $U_{-} \equiv \prod_{l=0}^{k-1} U_l(\theta_l)W_l$ and $U_{+} \equiv \prod_{l=k}^{L} U_l(\theta_l)W_l$. Also, $U(\bm{\theta})$ is sufficiently random s.t. both $U_{-}$ and $U_{+}$ (or either one) are independent and match the Haar distribution up to the second moment.

\noindent
{\bf Barren Plateaus (BPs)} are first investigated by~\cite{McClean2018landscapes}, who demonstrate that the gradient variance $\mathrm{Var}[\partial E]$ of \qnns\ will exponentially decrease as the number of qubits $N$ increases when the random \qnns\ match 2-design Haar distribution. This exponential pattern can be approximated as:
\begin{equation}
    \mathrm{Var}[\partial E] \propto 2^{-2N}.
\label{eqn:gvar}
\end{equation}
The Eq.~\ref{eqn:gvar} indicates that $\mathrm{Var}[\partial E]$ will approximate zero when the number of qubits $N$ is very large, i.e., most gradient-based approaches will fail to train \qnns\ in this case.

Based on the above description, we formally state the problem that we aim to solve as follows:
\begin{problem}
By leveraging a generative AI (GenAI) model, such as an \llm, as a Bayesian posterior estimator with adaptive prompting, we aim to iteratively identify the optimal \qnn's parameter $\bm{\theta^*_0}$, where a given \qnn\ is initialized with $\bm{\theta^*_0}$, which can maximize gradient variance $\mathrm{Var}[\partial E]$ during training, thereby mitigating barren plateaus (BPs).
\label{prob:statement}
\end{problem}


\subsection{Our Proposed Framework}
In this study, we introduce a new framework, AdaInit, designed to mitigate BP issues in \qnns\ by leveraging generative AI (GenAI) models, particularly \llms. Our key innovations can be described as follows.
{\bf (i)} First, unlike conventional one-time initialization strategies, we propose a generative approach that iteratively searches the optimal initial model parameters $\bm{\theta^*_0} \in \mathbb{R}^{LNR}$ that maximize the gradient variance $\mathrm{Var}[\partial E]$ of \qnns, thereby mitigating BPs and improving \qnns' trainability. In each search iteration, we employ an \llm\ as a Bayesian estimator to refine the posterior (candidate initial model parameters $\bm{\theta_0}$) through adaptive prompting. After posterior estimation, we train the \qnn\ initialized with the generated $\bm{\theta_0}$ and further compute its $\mathrm{Var}[\partial E]$. The benefit of using \llm\ as a posterior estimator is that the \llm\ can incorporate diverse textual instructions via prompts and adaptively update the prompts based on feedback from the previous iteration. This adaptive refinement allows our framework to dynamically optimize the generation process.
{\bf (ii)} To validate the generation quality, we employ Expected Improvement (EI), $\Delta^{(t)}$, as a guiding metric for our search. Furthermore, we rigorously prove that the EI and its cumulative sum satisfy the properties of submartingale. Consequently, we theoretically establish their boundedness, thereby demonstrating that our proposed search framework will ultimately find the optimal initial model parameters for \qnns.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/fig_framework.pdf}
  \caption{Our proposed framework follows an iterative search process over $T$ iterations (gray area). In $t$-th iteration, we perform four sequential steps: (i) Generate $\bm{\theta}_{0}^{(t)}$ using a Gen AI model, $f(\cdot)$, (ii) Compute $\mathrm{Var}[\partial E^{(t)}]$ after \qnn's training, (iii) Calculate EI, $\Delta^{(t)}$, and (iv) Update prompts $x_{p}^{(t+1)}$, historical maximum gradient variance $S^{(t)}$, and optimal candidates $\bm{\theta}_{0}^*$ for next iteration. Dashed arrows indicate data flow and corresponding outputs in each step.}
\label{fig:framework}
\end{figure}

\begin{algorithm}
\small
\caption{Search for optimal initial model parameters for \qnns.}
\label{algo:srch_init_params}
\begin{algorithmic}[1]
\REQUIRE A GenAI model $f(\cdot)$, prompts $x_{p}$, a \qnn\ $g(\cdot)$, the number of search iterations $T$.
\STATE Initialize prompts $x_{p}$ and the GenAI model, $f(\cdot)$;
\STATE Create an empty list $\bm{\Theta}_{0}^* \gets \varnothing$ to collect optimal candidates of initial model parameters for the \qnn, $g(\cdot)$;
\FOR{$t = 1$ to $T$}
    \STATE $P(\bm{\theta}_{0}^{(t)}|x_{p}^{(t)}) \gets f(x_{p}^{(t)}|\bm{\theta}_{0}^{(t)})P(\bm{\theta}_{0}^{(t)})$;
    \STATE $\mathrm{Var}[\partial E^{(t)}] \gets g(\bm{\theta}_{0}^{(t)})$;
    \STATE $\Delta^{(t)} \gets \max(\mathrm{Var}[\partial E^{(t)}] - S^{(t-1)}, 0)$;
    \IF{$\Delta^{(t)} > \frac{1}{poly(N, L)T}$} % EI > 1/(poly(N, L)T). 才能说明每次的改进有意义
        \STATE $x_{p}^{(t+1)} \xleftarrow{\bm{\theta}_{0}^{(t)},\ \mathrm{Var}[\partial E^{(t)}],\ S^{(t-1)}} x_{p}^{(t)}$;
        \STATE $S^{(t)} \gets \mathrm{Var}[\partial E^{(t)}]$;
        \STATE $\bm{\Theta}_{0}^* \gets \bm{\Theta}_{0}^* \oplus [\bm{\theta}_{0}^{(t)}]$;
    \ENDIF
\ENDFOR
\RETURN $\bm{\Theta}_{0}^*$;
\end{algorithmic}
\end{algorithm}

We present our framework workflow in Fig.~\ref{fig:framework} and further introduce details in Algo.~\ref{algo:srch_init_params}. Given a GenAI model $f(\cdot)$, prompts $x_{p}$ for the $f(\cdot)$, a \qnn\ $g(\cdot)$, and the number of search iterations $T$, we first initializes $f(\cdot)$, $x_{p}$ ({\bf line 1}) and also creates an empty list $\varnothing$ for $\bm{\Theta}_0^*$ to collect optimal candidates of \qnn's initial model parameters ({\bf line 2}). After initialization, we conduct $T$ iterations for searching ({\bf line 3}). In each iteration, let's say in the $t$-th iteration, we first employ $f(\cdot)$ with prompts $x_{p}^{(t)}$ and a prior distribution $P(\bm{\theta}_{0}^{(t)})$ to estimate the posterior distribution $P(\bm{\theta}_{0}^{(t)}|x_{p}^{(t)})$, which is the generated initial model parameter $\bm{\theta}_{0}^{(t)}$ for the \qnn\ ({\bf line 4}). After generation, we train the \qnn\ $g(\bm{\theta}_{0}^{(t)})$ with certain training epochs and compute the gradient variance $\mathrm{Var}[\partial E^{(t)}]$, whose gradient is abbreviated from $\frac{\partial E(\bm{\theta}^{(t)})}{\partial \bm{\theta}^{(t)}}$, where $\bm{\theta}^{(t)}$ denotes the \qnn's model parameter in the $t$-th iteration ({\bf line 5}). After computing the variance, we evaluate the improvement using the Expected Improvement (EI) metric, comparing the current gradient variance $\mathrm{Var}[\partial E^{(t)}]$ to the historical maximum gradient variance, which is the cumulative sum of EI when EI meets the following conditions ({\bf line 6}). If the current EI, $\Delta^{(t)}$, is effectively improved, i.e., $\Delta^{(t)} > \frac{1}{poly(N, L)T}$, where $\frac{1}{poly(N, L)T}$ denotes a strictly positive lower bound on the gradient variance of an $N$-qubit, $L$-layer \qnn\ for each search, in the absence of BPs ({\bf line 7}), then we update the prompts for next iteration based on the current initial model parameters $\bm{\theta}_{0}^{(t)}$, the current gradient variance $\mathrm{Var}[\partial E^{(t)}]$, and the historical maximum gradient variance $S^{(t-1)}$ ({\bf line 8}). After updating prompts, we update the historical maximum $S^{(t)}$ for the next iteration, where $S^{(t)} = S^{(t-1)}+\Delta^{(t)} = \mathrm{Var}[\partial E^{(t)}]$ ({\bf line 9}) and further concatenate $\bm{\theta}_{0}^{(t)}$ to the optimal candidate list $\bm{\Theta}_{0}^*$ ({\bf line 10}), which will be returned at the end ({\bf line 13}). If so, the optimal initial model parameter $\bm{\theta}_{0}^*$ will be the last element in the candidate list.

\paragraph{Analysis of time and space complexity.}
The search runs $T$ iterations. In each iteration, posterior estimation, which is linearly related to the output size of $\bm{\theta}_{0}$, takes $\mathcal{O}(|\bm{\theta}_{0}|)$ for a fixed-size \qnn. Besides, training $g(\bm{\theta}_{0})$ with $T_{tr}$ epochs may take $\mathcal{O}(T_{tr} \cdot |\bm{\theta}_{0}|)$, where $T_{tr}$ denotes the number of training epochs for \qnn. Combining $\bm{\theta}_{0} \in \mathbb{R}^{LNR}$, the total {\bf time complexity} is $\mathcal{O}(T \cdot (L \cdot N \cdot R + T_{tr} \cdot L \cdot N \cdot R)) \approx \mathcal{O}(T \cdot T_{tr} \cdot L \cdot N \cdot R)$.
The space complexity primarily depends on the storage requirements. $\bm{\Theta}_{0}^*$ at most stores $T$ number of $\bm{\theta}_{0}$, which consumes $\mathcal{O}(T \cdot |\bm{\theta}_{0}|)$. The output of posterior estimation takes $\mathcal{O}(|\bm{\theta}_{0}|)$ space. Gradient variance and EI are scalars, which cost $\mathcal{O}(1)$ space. The prompts $x_{p}$ are iteratively updated and thus occupy $\mathcal{O}(|x_{p}|)$ space. Considering the size of $\bm{\theta}_{0}$, the total {\bf space complexity} is $\mathcal{O}(T \cdot L \cdot N \cdot R + L \cdot N \cdot R + |x_{p}|) \approx \mathcal{O}(T \cdot L \cdot N \cdot R + |x_{p}|)$.


\subsection{Theoretical Analysis of Expected Improvement (EI).}
Before presenting the details of all necessary theoretical analysis, we would like to discuss how we can interpret these results.
First, we formally define the Expected Improvement (EI) at each search iteration $t$ as $\Delta^{(t)}$ and its accumulative sum in the past iterations as $S^{(t-1)}$ in Def.~\ref{def:ei}. Besides, we assume that the maximum possible gradient $\partial E_{max}$ during \qnn's training is bounded by a positive constant $B_{\partial E}$, which is practical in real-world simulation. Next, we establish an upper bound for EI through Lem.~\ref{lem:gvar_bound} and Lem.~\ref{lem:ei_bound}. These results indicate that $S^{(t)}$ is $L^1$-bounded and integrable for each $t$.
Building upon these lemmas, we investigate the submartingale property of $\Delta^{(t)}$ and rigorously prove in Lem.~\ref{lem:submg} that $S^{(t)}$ is submartingale. This insight is crucial as it provides a theoretical basis to analyze the convergence of our proposed search framework.
Finally, leveraging the convergence of submartingales and the monotonicity of $S^{(t)}$, we establish in Lem.~\ref{lem:submg_bound} that $S^{(t)}$ has a supremum, which indicates that our proposed search framework can eventually identify the optimal initial model parameters that maximize the gradient variance of \qnns\ in optimization. Due to the page limit, we provide rigorous proof in the {\bf Appendix}.

\begin{definition}[Expected Improvement]
For $\forall$ $t \in \mathbb{Z}^+$, the Expected Improvement (EI) in the $t$-th search iteration is defined as:
 \[
   \Delta^{(t)} = \max(\mathrm{Var}[\partial E^{(t)}] - S^{(t-1)}, 0),
 \]
where $\mathrm{Var}[\partial E^{(t)}]$ denotes the gradient variance in the $t$-th search iteration, and $S^{(t-1)} = \sum_{t_i=1}^{t-1} \Delta^{(t_i)} \cdot I^{(t_i)}$ denotes the maximum observed gradient variance in the past iterations, where $I^{(t_i)}$ represents an indicator function $\mathbf{1} \bigl( \Delta^{(t_i)} > \frac{1}{poly(N, L)T} \bigr)$ given a condition inside.
\label{def:ei}
\end{definition}


\begin{assumption}[Bounded Maximum Gradient]
We assume there exists a positive constant $B_{\partial E} > 0$, s.t. the maximum possible gradient $\partial E_{max}$ during \qnn's training satisfies:
\[
  \bigl| \partial E_{max} \bigr| \leq B_{\partial E}.
\]
Without loss of generality, let's say $\partial E_{max} \in [-\frac{B_{\partial E}}{2}, \frac{B_{\partial E}}{2}]$.
\label{assms:gmax}
\end{assumption}

\begin{lemma}[Boundedness of Gradient Variance]
Given a certain-size quantum neural network (\qnn), the variance of its gradient during training, $\mathrm{Var}[\partial E]$, is bounded by:
\[
\mathrm{Var}[\partial E] \leq (\partial E_{\max} - \partial E_{\min})^2,
\]
where $\partial E_{\max}$ and $\partial E_{\min}$ denote the maximum and minimum values of the gradient $\partial E$, respectively.
\label{lem:gvar_bound}
\end{lemma}

\begin{lemma}[Boundedness of EI]
From Def.~\ref{def:ei} and Lem.~\ref{lem:gvar_bound}, during the search of initial model parameters $\bm{\theta}_{0}$ for a certain-size \qnn, for $\forall$ $t \in \mathbb{Z}^+$, there exist a bound for the expected improvement (EI) s.t.
\[
\Delta^{(t)} \leq (\partial E_{max} - \partial E_{min})^2.
\]
\label{lem:ei_bound}
\end{lemma}

\begin{lemma}[Submartingale Property of EI]
Let $\{\Delta^{(t)}\}_{t \ge 1}$ be an i.i.d.\ sequence of random variables on a probability space $(\Omega, \mathcal{F}, P)$ s.t.
\[
\begin{cases}
P\bigl(\Delta^{(t)} > \frac{1}{poly(N, L)T} \bigr) = p,\\
P\bigl(\Delta^{(t)} \leq \frac{1}{poly(N, L)T} \bigr) = 1 - p,
\end{cases}
\]
for a probability $p \in [0,1]$.
We define natural filtration $\mathcal{F}^{(t)} = \sigma\bigl(\Delta^{(1)}, \Delta^{(2)}, \dots, \Delta^{(t)} \bigr)$, and the selective accumulation of $\Delta^{(t)}$ for the past $t$ iteration as a stochastic process $\{ S^{(t)} \}_{t \ge 1}$ according to Def.~\ref{def:ei}.
Then, $\{ S^{(t)} \}_{t \ge 1}$ is a submartingale with respect to the filtration $\{\mathcal{F}^{(t)}\}_{t \ge 1}$.
\label{lem:submg}
\end{lemma}

\begin{figure*}[t]
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 1st and 2nd images
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_iris_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_iris_layer.pdf}
    \caption{Iris}
  \end{subfigure}
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 3rd and 4th images
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_wine_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_wine_layer.pdf}
    \caption{Wine}
  \end{subfigure}
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 5th and 6th images
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_titanic_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_titanic_layer.pdf}
    \caption{Titanic}
  \end{subfigure}
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 7th and 8th images
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_mnist_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_dist_mnist_layer.pdf}
    \caption{MNIST}
  \end{subfigure}
\caption{Analysis of gradient variance trends in the first element of \qnns' model parameters across varying qubit and layer settings for three classic initialization distributions, uniform, normal, and beta. ``Classic'' denotes that we initialize the model parameters with a classic distribution. ``Ours'' denotes that we use our framework to search the initial model parameters.}
\label{fig:dist}
\end{figure*}

\begin{lemma}[Boundedness of Submartingale]
Let $\{S^{(t)}\}_{t \geq 1}$ be a submartingale w.r.t. a $\{\mathcal{F}^{(t)}\}_{t \geq 1}$ s.t. $\sup_t \mathbb{E}[|S^{(t)}|] < \infty$. Then, $\{S^{(t)}\}_{t \geq 1}$ is almost surely bounded by a finite constant $B_{S}$ s.t.
\[
S^{(t)} \le B_{S}, \quad \text{a.s.,} \quad \forall t \in \mathbb{Z}^+.
\]
\label{lem:submg_bound}
\end{lemma}
\vspace{-0.5cm}

\begin{table}[h] % Dataset
\centering
%\footnotesize
%\scriptsize
%\setlength{\tabcolsep}{3.8pt}
\caption{Statistics of datasets. $\left| D \right|$, $\left| F \right|$, and $\left| C \right|$ denote the original number of instances, features, and classes, respectively. ``Split'' denotes the split instances for the train, validation, and test data.}
\label{tab:data}
\begin{tabular}{ccccc}
  \toprule
    \textbf{Dataset} & {$\left| D \right|$} & {$\left| F \right|$} & {$\left| C \right|$} & {Splits} \\
    \midrule
    {\bf Iris} & 150 & 4 & 3 & 60:20:20 \\ %(60%:20%:20%)
    {\bf Wine} & 178 & 13 & 3 & 80:20:30 \\ %(62%:15%:23%)
    {\bf Titanic} & 891 & 11 & 2 & 320:80:179 \\ %(55%:14%:31%)
    {\bf MNIST} & 60,000 & 784 & 10 & 320:80:400 \\ %(40%:10%:50%)
  \bottomrule
\end{tabular}
\end{table}
\section{Experiment}
\label{sec:exp}
In this section, we first introduce the experimental settings and further present our results in detail.

\paragraph{Dataset.}
We evaluate our proposed method across four public datasets. {\bf Iris} is a classic machine-learning benchmark that measures various attributes of three-species iris flowers. {\bf Wine} is a well-known dataset that includes 13 attributes of chemical composition in wines. {\bf Titanic} contains historical data about passengers aboard the Titanic and is typically used to predict the survival. {\bf MNIST} is a widely used small benchmark in computer vision. This benchmark consists of 28$\times$28 gray-scale images of handwritten digits from 0 to 9.
We follow the settings of BeInit~\cite{kulshrestha2022beinit} and conduct experiments in binary classification. Specifically, we sub-sample instances from the first two classes of each dataset to create a new subset. After sub-sampling, we adjust the feature dimensions to ensure they do not exceed the number of available qubits. The statistics of the original datasets, along with the data splits for training, validation, and testing, are presented in Table~\ref{tab:data}. Importantly, the total number of sub-sampled instances corresponds to the sum of the split datasets. For instance, in the Iris dataset, the total number of sub-sampled instances is 100.

\begin{figure*}[t]
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 1st and 2nd images
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_iris_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_iris_layer.pdf}
    \caption{Iris}
  \end{subfigure}
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 3rd and 4th images
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_wine_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_wine_layer.pdf}
    \caption{Wine}
  \end{subfigure}
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 5th and 6th images
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_titanic_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_titanic_layer.pdf}
    \caption{Titanic}
  \end{subfigure}
  %\hfill
  \begin{subfigure}{0.5\linewidth}
    \centering  % include the 7th and 8th images
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_mnist_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_prompts_mnist_layer.pdf}
    \caption{MNIST}
  \end{subfigure}
\caption{Analysis of prompts' impact, i.e., investigate whether data description (desc.) and gradient feedback (feedback) affect the gradient variance in the first element of \qnns' model parameters across different model structures, considering variations in the number of qubits and layers.}
\label{fig:prompts}
\end{figure*}
\paragraph{Experimental settings.}
In the experiment, we analyze the trend of gradient variance by varying the number of qubits ranging from 2 to 20 in increments of 2 (fixed 2 layers) and the number of layers spanning from 4 to 40 in steps of 4 (fixed 2 qubits). To obtain reliable results, we repeat the experiments five times and present them as curves (mean) with their bandwidth (standard deviation).
During the search, our framework can identify the optimal model parameters within 50 iterations. In each search iteration, we employ the Adam optimizer with a learning rate of 0.01 and a batch size of 20 to train a \qnn\ with 30 epochs and compute the gradient variance. After training, we compute the expected improvement (EI) and compare it with an assumed lower bound, $\frac{1}{poly(N, L)T}$, in each iteration. We compute the lower bound by $[2^{2N}T]^{-1}$, which is originally designed for uniform distribution. We empirically apply it for all cases as we observe in Fig.~\ref{fig:dist} that the magnitude of gradient variance is comparable across all datasets.

\paragraph{Evaluation.}
We measure the \qnn's training by its gradient variance. A higher gradient variance in training indicates a lower likelihood of being trapped on the barren plateau landscape.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_prior.pdf}
  \caption{Example of three classic distributions commonly for initialization. In the figure, the red dots represent the initial values of the model parameters.}
\label{fig:prior}
\end{figure}
\paragraph{Searching initial model parameters of \qnns\ via large language models can help alleviate barren plateaus.}
We analyze gradient variance trends in the first element of \qnns' model parameters across varying qubit and layer settings for three classic initialization distributions, uniform, normal, and beta distributions, which are presented in Fig.~\ref{fig:prior} as examples. For each initialization with classic distribution, we compare it (``Classic'') with our proposed methods (``Ours'').
As presented in Fig.~\ref{fig:dist}, we observe that in the case of using classic initialization, the gradient variance of \qnns\ will significantly decrease as the number of qubits or layers increases. Compared with it, our method can maintain higher variances, indicating that our framework can mitigate BPs better.
In the rest of the experiments, if there is no specific state, we adopt uniform distribution as prior knowledge for posterior estimation.

\paragraph{Investigation of prompts.} We further examine whether the content of prompts influences search performance. In the experiments, we tested four prompting scenarios: (i) Including both data description and gradient feedback in prompts (Both desc. and feedback), (ii) Including gradient feedback only (No desc.), (iii) Including data description only (No feedback), (iv) Including neither data description nor gradient feedback (Neither desc. nor feedback). As the results presented in Fig.~\ref{fig:prompts}, we observe that suppressing either dataset description or gradient feedback in the prompts leads to a reduction in the gradient variance of \qnns. Notably, the reduction is more significant in most cases when gradient feedback is muted compared to the dataset description, suggesting that both factors play a crucial role in mitigating BPs, with gradient feedback contributing significantly more.

\begin{table}[h]
\small
\centering
\begin{tabular}{ccc}
\toprule
 \llms & Acc. & Max i/o\\
\midrule
GPT 4o & 100\% & 128K/4K \\
GPT 4o mini & 85\% & 128K/16K \\
Gemini 1.5 flash & 75\% & 1M/8K \\
Gemini 1.5 pro & 90\% & 2M/8K \\
Claude 3.5 sonnet & 100\% & 200K/8K \\
\bottomrule
\end{tabular}
\caption{Comparison of initial model parameters' generation by accuracy (Acc.) using \llms, GPT~\cite{hurst2024gpt}, Gemini~\cite{team2024gemini}, and Claude~\cite{anthropic2024claude35sonnet}. We measure the generation under different numbers of qubits and layers (20 combinations in total). We also present the maximum number of input/output tokens in the third column.}
\label{tab:llm}
\end{table}

\paragraph{Comparison of generative performance using \llms.} In our proposed framework, the initial model parameters of \qnns\ are generated by \llms. In this experiment, we compare the generative performance under varying \qnn\ structures, such as different numbers of qubits or layers. Specifically, we primarily evaluate whether the correct size of model parameters can be generated by testing 20 combinations in accuracy, fixing either 2 layers while varying qubits from 2 to 20 or 2 qubits while varying layers from 4 to 40. As shown in Tab.~\ref{tab:llm}, the results indicate that both GPT-4o and Claude 3.5 Sonnet can achieve 100\% accuracy in generating the correct shapes of model parameters. Considering that 4K output tokens are sufficient for our settings, in this study, we mainly use GPT 4o as the backbone \llms. 

\begin{figure}[h]
  \begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=0.49\textwidth]{figures/fig_baselines_iris_qubit.pdf}
    \includegraphics[width=0.49\textwidth]{figures/fig_baselines_iris_layer.pdf}
  \end{subfigure}
\caption{Comparison between two initialization-based strategies, GaInit and BeInit, and our framework, which is initialized with corresponding data distribution for a fair comparison.}
\label{fig:baselines}
\end{figure}
\paragraph{Comparison with initialization-based strategies.}
We compare our framework with two popular initialization-based strategies, GaInit~\cite{zhang2022escaping} and BeInit~\cite{kulshrestha2022beinit}. Both of them leverage well-designed Normal and Beta distributions to initialize the quantum circuits, respectively. For a fair comparison, we initialize the \qnns\ with the corresponding distribution. We present the results on Iris in Fig.~\ref{fig:baselines} as an example. The results demonstrate that our framework can identify the initial model parameters of \qnns\ that achieve higher gradient variance during training as the model size increases, indicating better mitigation for BPs.


\begin{figure}[h]
  \begin{subfigure}{0.49\linewidth}
    \centering  % include the 1st and 2nd images
    \includegraphics[width=0.95\textwidth]{figures/fig_hypm_iris.pdf}
    \caption{Iris}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering  % include the 1st and 2nd images
    \includegraphics[width=0.95\textwidth]{figures/fig_hypm_wine.pdf}
    \caption{Wine}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering  % include the 1st and 2nd images
    \includegraphics[width=0.95\textwidth]{figures/fig_hypm_titanic.pdf}
    \caption{Titanic}
  \end{subfigure}
  \begin{subfigure}{0.49\linewidth}
    \centering  % include the 1st and 2nd images
    \includegraphics[width=0.95\textwidth]{figures/fig_hypm_mnist.pdf}
    \caption{MNIST}
  \end{subfigure}
\caption{Analysis of the sensitivity of hyperparameters, including Temperature and Top P. The grid with the darkest color indicates the optimal combination.}
\label{fig:hypm}
\end{figure}
\paragraph{Sensitivity analysis of hyperparameters.}
We analyze the sensitivity of hyperparameters, including Temperature and Top P, for \llms. Temperature controls the randomness of predictions, with higher values generating more diverse outputs, while Top P affects the probabilities of token selections, ensuring a more focused yet flexible generation. To identify the optimal settings, we first narrowed down the hyperparameter ranges through manual tuning and then applied grid search to determine the best combinations (Temperature, Top P) for each dataset: Iris (0.5, 0.9), Wine (0.1, 0.45), Titanic (0.8, 0.75), and MNIST (0.8, 0.8), as presented in Fig.~\ref{fig:hypm}. The combinations of the above hyperparameters were subsequently used in this study.

\begin{figure*}[t]
\centering
    % the left-side column
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb2.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb4.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb6.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb8.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb10.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb12.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb14.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb16.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb18.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_qb20.pdf}
        \caption{The number of qubits ranges from 2 to 20.}
    \end{subfigure}
    % the right-side column
    \begin{subfigure}{0.49\linewidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly4.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly8.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly12.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly16.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly20.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly24.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly28.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly32.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly36.pdf}
        \includegraphics[width=0.99\linewidth]{figures/fig_ei_gvar_ly40.pdf}
        \caption{The number of layers ranges from 4 to 40.}
    \end{subfigure}
\caption{We analyze the patterns of expected improvement and the corresponding gradient variance and present the results in two columns: the left column illustrates the trends w.r.t. the number of qubits, while the right column captures the effects of increasing the number of layers.}
\label{fig:ei_gvar}
\end{figure*}
\paragraph{Analysis of the expected improvement.}
We analyze the patterns on the expected improvement (EI) and the corresponding gradient variance across various \qnn\ structures as search iterations progress. Representative experiments conducted on Iris are illustrated in Fig.~\ref{fig:ei_gvar} as an example. Our findings show that as the model size grows, more search iterations are required to obtain optimal initial parameters that enable \qnns\ to maintain higher gradient variance in training. This is expected, as larger models expand the search space, demanding greater computational resources to explore effectively.


\section{Related Work}
\label{sec:rewk}
\citet{McClean2018landscapes} first investigated barren plateau (BP) phenomenons and demonstrated that under the assumption of the 2-design Haar distribution, gradient variance in \qnns\ will exponentially decrease to zero during training as the model size increases. In recent years, enormous studies have been devoted to mitigating BP issues in \qnns~\cite{qi2023barren}. \citet{cunningham2024investigating} categorize most existing studies into the following five groups.
(i) Initialization-based strategies initialize model parameters with various well-designed distributions in the initialization stage~\cite{grant2019initialization, sack2022avoiding, mele2022avoiding, grimsley2023adaptive, liu2023mitigating, park2024hamiltonian}.
(ii) Optimization-based strategies address BP issues and further enhance trainability during optimization~\cite{ostaszewski2021structure, suzuki2021normalized, heyraud2023estimation, liu2024mitigating, sannia2024engineered}. %wu2021mitigating, gharibyan2023hierarchical, sciorilli2024towards, falla2024graph
(iii) Model-based strategies attempt to mitigate BPs by proposing new model architectures~\cite{li2021vsql, bharti2021simulator, du2022quantum, selvarajan2023dimensionality, tuysuz2023classical, Kashif2024resQNets}. %zhang2022quark, shin2024layerwise
(iv) To address both BPs and saddle points, \citet{zhuang2024improving} regularize \qnns' model parameters via Bayesian approaches.
(v) \citet{rappaport2023measurement} measure BP phenomenon via various informative metrics.


\section{Conclusion}
In this study, we proposed a new \llm-driven framework, AdaInit, designed to mitigate barren plateaus (BPs) in \qnn's training. By iteratively refining \qnn's initialization through adaptive prompting and posterior estimation, AdaInit can maximize gradient variance, improving \qnn's trainability against BPs. Our theoretical analysis establishes the submartingale property of expected improvement (EI), ensuring the iterative search can eventually identify optimal initial model parameters for \qnn. Through extensive experiments across four public datasets, we demonstrated that AdaInit outperforms conventional classic initialization methods in maintaining higher gradient variance as \qnn's sizes increase. Overall, this study paves a new way to explore how \llms\ help mitigate BPs in \qnn's training.


\paragraph{Limitations \& future work.}
First, in our theoretical analysis, we assume that the maximum gradient of \qnns\ is bounded by a positive constant, i.e., the gradient doesn't explode during training. This assumption is practical in most cases. Besides, we rigorously prove that the submartingale has a supremum in our settings. In the future, we plan to prove that convergence of submartingale is guaranteed in a finite number of search iterations.

% Entries for the entire Anthology, followed by custom entries
\bibliography{ref}
\bibliographystyle{acl_natbib}

\appendix
\section{APPENDIX}
\label{sec:appendix}
In the appendix, we present the architecture of the quantum circuit and our hardware/software. Besides, we display the prompt designs in this study.

\paragraph{Model architecture of the quantum circuit.}
In this study, we examine our proposed framework using a backbone \qnn, which concatenates the following quantum circuit with a fully connected layer. The circuit architecture is described in Figure~\ref{fig:vqc}.
\begin{figure}[h]
\centering
  \includegraphics[width=\linewidth]{figures/fig_vqc.pdf}
  \caption{Architecture of our backbone quantum circuit. The number of rotation gates in this study is fixed as 3.}
\label{fig:vqc}
\end{figure}

\paragraph{Hardware and software.}
The experiment is conducted on a server with the following settings:
\begin{itemize}[itemsep=-1mm]
  \item Operating System: Ubuntu 22.04.3 LTS
  \item CPU: Intel Xeon w5-3433 @ 4.20 GHz
  \item GPU: NVIDIA RTX A6000 48GB
  \item Software: Python 3.11, PyTorch 2.1, Pennylane 0.31.1.
\end{itemize}

\begin{figure*}[t]
\centering
\begin{tcolorbox}[colback=gray!20, colframe=gray!50, title=Prompts]
\textbf{Role:} data generator.\\
\textbf{Goal:} Generate a dictionary iteratively with the following shape:
\begin{verbatim}
{
  'l0': a list, shape=(nlayers, nqubits, nrot),
  'l1': a list, shape=(out_dim, nqubits),
  'l2': a list, shape=(out_dim)
}
\end{verbatim}
\textbf{Requirements:}
\begin{itemize}
 \item Data shape: nlayers=\{\textcolor{red}{nlayers}\}, nqubits=\{\textcolor{red}{nqubits}\}, nrot=\{\textcolor{red}{nrot}\}, out\_dim=\{\textcolor{red}{nclasses}\}.
 \item Data type: float rounded to four decimals.
 \item Data distribution: numerical numbers in each list are sampled from standard \{\textcolor{brown}{init}\} distributions, which may be modeled from the following dataset.   
 \item Dataset description: \{\textcolor{blue}{data\_desc}\}
 \item Adjust the sampling based on feedback from the previous searches: \{\textcolor{teal}{feedback}\}
 \item Crucially, ensure that the length of `l0' = `nlayers' and the length of `l1' = `out\_dim'.
 \item Print out a dictionary [only] (Don't show Python code OR include `[```python\textbackslash n]', `[```json\textbackslash n]', `[```]').
\end{itemize}
\end{tcolorbox}
\end{figure*}
\paragraph{Prompt designs.}
Before presenting the prompts, we first introduce the notation for the hyperparameter in prompts. `\textcolor{red}{nlayers}', `\textcolor{red}{nqubits}', `\textcolor{red}{nrot}', `\textcolor{red}{nclasses}' denote the number of layers, qubits, rotation gates, and classes for the \qnn, respectively. `\textcolor{brown}{init}' denotes the initial data distribution for the \qnn. `\textcolor{blue}{data\_desc}' denotes the data description. `\textcolor{teal}{feedback}' denotes the gradient feedback from the previous iteration during the search.

\paragraph{Proof of Lemmas.}
We provide a rigorous proof of the following lemmas.
% \begin{lemma}[Boundedness of Gradient Variance]
% Given a certain-size quantum neural network (\qnn), the variance of its gradient during training, $\mathrm{Var}[\partial E]$, is bounded by:
% \[
% \mathrm{Var}[\partial E] \leq (\partial E_{\max} - \partial E_{\min})^2,
% \]
% where $\partial E_{\max}$ and $\partial E_{\min}$ denote the maximum and minimum values of the gradient $\partial E$, respectively.
% \label{lem:gvar_bound}
% \end{lemma}
\begin{proof}[Lem.~\ref{lem:gvar_bound}]
We denote a sequence of gradient $\bm{\partial E} = \{\partial E^{(t)}\}_{t=0}^{T_{tr}}$, where $T_{tr}$ represents the number of training epochs for a \qnn. Within this sequence, we denote $\partial E_{max}$, $\partial E_{min}$, and $\overline{\partial E}$ as the maximum, minimum, and mean values of the gradient. For $\forall$ $t \in \mathbb{Z}^+$, we have:
\[
    \partial E^{(t)}, \overline{\partial E} \in [\partial E_{min}, \partial E_{max}],
\]
then the gap between $\partial E^{(t)}$ and $\overline{\partial E}$ will not exceed the range of $[\partial E_{min}, \partial E_{max}]$:
\[
    |\partial E^{(t)} - \overline{\partial E}| \leq \partial E_{max} - \partial E_{min}.
\]
Thus, we have:
\[
\begin{aligned}
   \mathrm{Var}[\partial E]
   &= \frac{1}{T_{tr}} \sum_{t=1}^{T_{tr}} (\partial E^{(t)} - \overline{\partial E})^2 \\
   &\leq \frac{1}{T_{tr}} \sum_{t=1}^{T_{tr}} (\partial E_{max} - \partial E_{min})^2 \\
   &= (\partial E_{max} - \partial E_{min})^2. \\
\end{aligned}
\]
Thus, the gradient variance $\mathrm{Var}[\partial E]$ satisfies the bound $\mathrm{Var}[\partial E] \leq (\partial E_{max} - \partial E_{min})^2$.
\end{proof}

% \begin{lemma}[Boundedness of EI]
% From Def.~\ref{def:ei} and Lem.~\ref{lem:gvar_bound}, during the search of initial model parameters $\bm{\theta}_{0}$ for a certain-size \qnn, for $\forall$ $t \in \mathbb{Z}^+$, there exist a bound for the expected improvement (EI) s.t.
% \[
% \Delta^{(t)} \leq (\partial E_{max} - \partial E_{min})^2.
% \]
% \label{lem:ei_bound}
% \end{lemma}
\begin{proof}[Lem.~\ref{lem:ei_bound}]
From Def.~\ref{def:ei}, for $\forall$ $t \in \mathbb{Z}^+$, in the $t$-th search iteration, we have:
 \[
   \Delta^{(t)} = \max(\mathrm{Var}[\partial E^{(t)}] - S^{(t-1)}, 0).
 \]
Combining with Lem.~\ref{lem:gvar_bound}, for $\forall$ $t \in \mathbb{Z}^+$, we have:
 \[
   \mathrm{Var}[\partial E^{(t)}], S^{(t-1)} \leq (\partial E_{max} - \partial E_{min})^2.
 \]
The above equation holds true as $S^{(t-1)}$ denotes the historical maximum gradient variance in the past iterations. Thus, we have:
 \[
   \mathrm{Var}[\partial E^{(t)}] - S^{(t-1)} \leq (\partial E_{max} - \partial E_{min})^2,
 \]
which indicates that:
 \[
   \Delta^{(t)} \leq (\partial E_{max} - \partial E_{min})^2.
 \]
\end{proof}

% \begin{lemma}[Submartingale Property of EI]
% Let $\{\Delta^{(t)}\}_{t \ge 1}$ be an i.i.d.\ sequence of random variables on a probability space $(\Omega, \mathcal{F}, P)$ s.t.
% \[
% \begin{cases}
% P\bigl(\Delta^{(t)} > \frac{1}{poly(N, L)T} \bigr) = p,\\
% P\bigl(\Delta^{(t)} \leq \frac{1}{poly(N, L)T} \bigr) = 1 - p,
% \end{cases}
% \]
% for a probability $p \in [0,1]$.
% We define natural filtration $\mathcal{F}^{(t)} = \sigma\bigl(\Delta^{(1)}, \Delta^{(2)}, \dots, \Delta^{(t)} \bigr)$, and the selective accumulation of $\Delta^{(t)}$ for the past $t$ iteration as a stochastic process $\{ S^{(t)} \}_{t \ge 1}$ according to Def.~\ref{def:ei}.
% Then, $\{ S^{(t)} \}_{t \ge 1}$ is a submartingale with respect to the filtration $\{\mathcal{F}^{(t)}\}_{t \ge 1}$.
% \label{lem:submg}
% \end{lemma}
\begin{proof}[Lem.~\ref{lem:submg}]
A process $S^{(t)}$ is submartingale relative to $(\Omega, \mathcal{F}, P)$ if the following three conditions, Adaptedness, Integrability, and Submartingale condition, hold true~\cite{williams1991probability}.
\paragraph{Adaptedness.}
We first aim to verify that $S^{(t)}$ is determined based on the information available up to past $t$ iterations. By Def.~\ref{def:ei}, $S^{(t)}=\sum_{t_i=1}^{t} \Delta^{(t_i)} \cdot I^{(t_i)}$ is a finite sum of random variables that are measurable w.r.t. $\sigma\bigl(\Delta^{(1)}, \Delta^{(2)}, \dots, \Delta^{(t)} \bigr)$. Thus, $S^{(t)}$ is also measurable w.r.t. $\mathcal{F}^{(t)}$, ensuring the adaptedness.

\paragraph{Integrability.}
In Lem.~\ref{lem:ei_bound}, $\Delta^{(t)} \leq (\partial E_{max} - \partial E_{min})^2$ for $\forall$ $t \in \mathbb{Z}^+$. Thus,
\begin{equation}
\begin{aligned}
\mathbb{E}[|S^{(t)}|]
 &= \mathbb{E}\Bigl[\Bigl| \sum_{t_i=1}^{t} \Delta^{(t_i)} \cdot I^{(t_i)} \Bigr|\Bigr] \notag \\
 &\le \mathbb{E}\Bigl[\Bigl| \sum_{t_i=1}^{t} (\partial E_{max} - \partial E_{min})^2 \cdot I^{(t_i)} \Bigr|\Bigr] \notag \\
 &< \infty, \notag
\end{aligned}
\end{equation}
which ensures $\mathbb{E}[|S^{(t)}|]$ is integrable for each $t$.

\paragraph{Submartingale condition.}
We observe that
\[
S^{(t)} = S^{(t-1)} + \Delta^{(t)}.
\]
Thus, given $\mathcal{F}^{(t-1)}$, we have
\[
\mathbb{E}\bigl[S^{(t)} \big| \mathcal{F}^{(t-1)}\bigr] = \mathbb{E}\bigl[S^{(t-1)} + \Delta^{(t)} \big| \mathcal{F}^{(t-1)}\bigr],
\]
Since $S^{(t-1)}$ is $\mathcal{F}^{(t-1)}$-measurable and $\{\Delta^{(t)}\}_{t \ge 1}$ is i.i.d., thus,
\begin{equation}
 \begin{aligned}
    \mathbb{E}\bigl[S^{(t)} \big| \mathcal{F}^{(t-1)}\bigr]
     &= \mathbb{E}\bigl[S^{(t-1)} + \Delta^{(t)} \big| \mathcal{F}^{(t-1)}\bigr] \notag \\
    &= S^{(t-1)} + \mathbb{E}[\Delta^{(t)}] \notag \\
    &= S^{(t-1)} + \bigl(\delta p + 0 (1-p)\bigr) \notag \\
    &\ge S^{(t-1)}, \notag
 \end{aligned}
\end{equation}
where $\delta$ denotes a positive increment when $\Delta^{(t)} > \frac{1}{poly(N, L)T}$.

Thus, the submartingale condition holds true for $\forall$ $t \ge 1$ s.t.
\[
\mathbb{E}\bigl[S^{(t)} \big| \mathcal{F}^{(t-1)}\bigr] \ge S^{(t-1)}, \quad \forall t \ge 1,
\]
\end{proof}

% \begin{lemma}[Boundedness of Submartingale]
% Let $\{S^{(t)}\}_{t \geq 1}$ be a submartingale w.r.t. a $\{\mathcal{F}^{(t)}\}_{t \geq 1}$ s.t. $\sup_t \mathbb{E}[|S^{(t)}|] < \infty$. Then, $\{S^{(t)}\}_{t \geq 1}$ is almost surely bounded by a finite constant $B_{S}$ s.t.
% \[
% S^{(t)} \le B_{S}, \quad \text{a.s.,} \quad \forall t \in \mathbb{Z}^+.
% \]
% \label{lem:submg_bound}
% \end{lemma}
\begin{proof}[Lem.~\ref{lem:submg_bound}]
Since the process $\{S^{(t)}\}_{t \geq 1}$ is a $L^1$-bounded submartingale s.t. $\sup_t \mathbb{E}[|S^{(t)}|] < \infty$, we apply \textbf{Doob's Forward Convergence Theorem}~\cite{williams1991probability}, which guarantees the almost sure existence of a finite random variable $S^{(\infty)}$ s.t. $S^{(\infty)}$=$\lim_{t \to \infty}S^{(t)}$.
This implies that the process $\{S^{(t)}\}$ has a well-defined almost sure limit.

Furthermore, if $\{S^{(t)}\}$ is monotone increasing, i.e., $S^{(t)} \leq S^{(t+1)}$, a.s., $\forall$ $t \in \mathbb{Z}^+$, then the limit $S^{(\infty)}$ serves as a supremum for the entire process. By Defining $B_{S} := \sup_t S^{(t)} = S^{(\infty)}$, we obtain a desired bound $S^{(t)} \leq B_{S}$, a.s., $\forall$ $t \in \mathbb{Z}^+$.
\end{proof}

\paragraph{Computational budgets.}
Based on the above computing infrastructure and settings, computational budgets in our experiments are described as follows.
Our search framework can be reproduced within one hour given that the number of qubits is less than 18. The total experiments take around 600 hours to complete.

\paragraph{Ethical and broader impacts.}
We confirm that we fulfill the author's responsibilities and address the potential ethical issues. This work paves a novel way to explore how generative models, such as \llms, help improve the trainability of \qnns, which could benefit the community of natural language processing and quantum machine learning.

\paragraph{Statement of data privacy.}
The datasets used in this study were obtained from publicly available sources.

\paragraph{Potential risk.}
Reproducing the experiments in this study may take significant time and computational resources. 

\paragraph{Disclaimer regarding human subjects results.}
NA

\end{document}
