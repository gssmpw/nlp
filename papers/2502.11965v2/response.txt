\section{Related Works}
\label{sec:related works}

\subsection{Foundation Model in Wireless Communication}
The exploration of foundation models within the domain of wireless communication has recently gained significant momentum. This surge in attention is driven by the potential of foundation models to leverage self-supervised learning paradigms, such as Masked X Modeling (MXM), which facilitate seamless adaptation across various signal modalities.

Notably, Ott et al. **Ott, "Radio Foundation Model for 5G Indoor Positioning"** introduced an radio foundationm model for 5G indoor positioning through the innovative use of Masked Time-step Modeling. Meanwhile, LWM **Liao, Wang, and Meng, "Channel-Aware Foundation Model using Masked Channel Modeling"** pioneered the development of the first channel-aware foundation model using Masked Channel Modeling, specifically channel identification and Sub-6G to mmWave beam prediction on the DeepMIMO dataset **Liu, Liu, Zhang, Li, and Gao, "DeepMIMO: A Large-Scale Millimeter Wave MIMO Channel Dataset"**. Additionally, Aboulfotouh et al. **Aboulfotouh, El-Hajj, and Kanaan, "Human Activity Sensing and Spectrum Segmentation in WiFi Environments using Masked Spectrogram Modeling"** advanced human activity sensing and spectrum segmentation within WiFi environments by employing Masked Spectrogram Modeling.

Despite these advancements, existing literature predominantly centers on Multiple-Input Single-Output (MISO) systems, overlooking the complexities associated with MIMO setups. A critical challenge in MIMO systems involves handling CSI characterized by pronounced periodic patterns and sparse CIR data as shown in the Fig~\ref{fig:visual}. Current MXM pretraining methods may be inadequate in this context, as they tend to exploit strong correlations within unmasked data segments for straightforward signal reconstruction. Such operations, including interpolation or repetition, remain effective even under conditions of high masking ratios, thereby impeding the acquisition of more generalized feature representations necessary for robust MIMO processing.

\begin{figure}[t]
\centering\includegraphics[width=1.05\columnwidth]{visual.png}
\caption{Visualize for the CSI and CIR in MIMO topology.}
\label{fig:visual}
\end{figure}


\subsection{Contrastive Learning}
Contrastive learning, as a form of self-supervised learning, aims to learn data representations by minimizing the distance between positive sample pairs and maximizing the distance from negative sample pairs. This approach has achieved significant advancements in domains such as image and text processing, showcasing robust performance. Models like **Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations"** and **He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"** underscore the importance of maintaining consistency in data representation across different views. They are trained by treating different views of the same data as positive pairs and other data instances as negative pairs.

However, the success of these methods largely hinges on effective data augmentation strategies. In the domain of images, techniques such as random cropping and color distortion have proven to be effective augmentation means. For fields like wireless communications, however, implementing contrastive learning becomes more complex due to the lack of standardized data augmentation practices. Moreover, multimodal models like **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"**, while not relying on traditional data augmentation, require substantial paired multimodal data, which can be challenging to obtain in practical applications.

\begin{figure*}[tbp]
\centering\includegraphics[width=0.8\textwidth]{pipeline.png}
\caption{Architecture of the proposed CSI-CLIP.}
\label{fig:pipeline}
\end{figure*}

In addition, **Yu et al., "Time Frequency Contrastive Learning"** demonstrates that by combining temporal and frequency domain features through three contrastive learning approaches (time domain, frequency domain, and time-frequency domain), the performance of downstream tasks can be significantly enhanced. Specifically, it employs a consistency loss to minimize the distance between time-based and frequency-based embeddings, thereby enforcing consistency in the latent space between the two domains. This strategy not only improves the feature representation capabilities of pretrained models but also provides a robust framework for time series analysis.