\if 0

% \usepackage{tabularray}
\begin{table*}[!t]
\centering
\caption{Experimental results on Wikitext perplexity (PPL) and performance across 7 commonly used zero-shot natural language reasoning tasks compare \textbf{ProxSparse} to 3 other baselines on 4 widely used large language models. \textbf{Bold} indicates the best performance. SparseGPT updates weights to minimize reconstruction error, while the other methods keep retained weights frozen. ProxSparse consistently yields better results compared to all other baselines.}
\label{tab: main_exp_table}
\resizebox{1\textwidth}{!}{
\begin{tblr}{
  cells = {c},
  vline{3-4,11} = {-}{},
  hline{1-2,7,12,17,22} = {-}{},
}
                 & Weight Update & Wikitext PPL   & ARC-C          & ARC-E          & SIQA           & HellaSwag      & OBQA           & PIQA           & TruthfulQA     & Average        \\
Llama-2-7B       & -             & 5.12          & 0.433          & 0.763          & 0.461          & 0.571          & 0.314          & 0.781          & 0.321          & 0.521          \\
magnitude        & \textbf{\ding{55}}            & 54.74          & 0.301          & 0.618          & 0.411          & 0.454          & 0.216          & 0.701          & 0.322          & 0.432          \\
SparseGPT        & \textbf{\ding{51}}           & 10.29         & 0.326          & 0.655          & \textbf{0.412} & 0.435          & 0.246          & 0.713          & 0.304          & 0.441          \\
Wanda            & \textbf{\ding{55}}            & 11.42          & 0.311          & 0.623          & 0.403          & 0.413          & \textbf{0.248} & 0.706          & 0.305          & 0.430          \\
\textbf{ProxSparse} & \textbf{\ding{55}}            & \textbf{8.51}  & \textbf{0.331} & \textbf{0.656} & 0.407          & \textbf{0.478} & 0.242          & \textbf{0.716} & \textbf{0.328} & \textbf{0.452} \\
Llama-2-13B      & -             & 4.57          & 0.485          & 0.794          & 0.473          & 0.601          & 0.352          & 0.791          & 0.314          & 0.544          \\
magnitude        & \textbf{\ding{55}}            & 8.32           & 0.319          & 0.623          & 0.408          & 0.501          & 0.232          & 0.717          & 0.309          & 0.444          \\
SparseGPT        & \textbf{\ding{51}}           & 8.14         & 0.378          & 0.714          & \textbf{0.437} & 0.478          & 0.282          & 0.735          & 0.296          & 0.473          \\
Wanda            & \textbf{\ding{55}}            & 8.35         & 0.340          & 0.683          & 0.424          & 0.464          & 0.246          & \textbf{0.739} & 0.292          & 0.455          \\
\textbf{ProxSparse} & \textbf{\ding{55}}            & \textbf{6.61}  & \textbf{0.383} & \textbf{0.720} & 0.427          & \textbf{0.532} & \textbf{0.288} & 0.723          & \textbf{0.319} & \textbf{0.486} \\
Llama-3.1-8B     & -             & 5.84         & 0.515          & 0.814          & 0.470          & 0.600          & 0.334          & 0.801          & 0.368          & 0.557          \\
magnitude        & \textbf{\ding{55}}            & 766.91         & 0.257          & 0.454          & 0.365          & 0.335          & 0.154          & 0.634          & \textbf{0.319} & 0.360          \\
SparseGPT        & \textbf{\ding{51}}           & 14.611         & 0.316          & 0.647          & \textbf{0.426} & 0.435          & 0.222          & 0.705          & 0.301          & 0.434          \\
Wanda            & \textbf{\ding{55}}            & 20.91         & 0.269          & 0.573          & 0.400          & 0.380          & 0.192          & 0.686          & 0.309          & 0.401          \\
\textbf{ProxSparse} & \textbf{\ding{55}}            & \textbf{13.63} & \textbf{0.333} & \textbf{0.623} & 0.422          & \textbf{0.460} & \textbf{0.240} & \textbf{0.721} & 0.296          & \textbf{0.444} \\
Mistral-v0.3-7b  & -             & 4.95           & 0.490          & 0.797          & 0.458          & 0.609          & 0.336          & 0.803          & 0.353          & 0.549          \\
magnitude        & \textbf{\ding{55}}            & 13.52          & 0.332          & 0.665          & 0.413          & 0.488          & 0.226          & 0.738          & 0.309          & 0.453          \\
SparseGPT        & \textbf{\ding{51}}           & 9.23         & 0.353          & 0.687          & 0.421          & 0.470          & \textbf{0.248} & 0.733          & 0.308          & 0.458          \\
Wanda            & \textbf{\ding{55}}            & 10.97         & 0.311          & 0.648          & 0.408          & 0.442          & 0.206          & 0.716          & 0.300          & 0.433          \\
\textbf{ProxSparse} & \textbf{\ding{55}}            & \textbf{8.68}  & \textbf{0.362} & \textbf{0.697} & \textbf{0.429} & \textbf{0.525} & 0.242          & \textbf{0.751} & \textbf{0.321} & \textbf{0.476} 
\end{tblr}
}
\end{table*}

\fi

\if 0

% \usepackage{tabularray}
\begin{table*}[!t]
\centering
\caption{Experimental results on Wikitext perplexity (PPL) and performance across 7 commonly used zero-shot natural language reasoning tasks comparing \textbf{ProxSparse} to 3 other baselines on 6 widely used large language models. \textbf{Bold} indicates the best pruning performance, while \textit{italic} represents the original unpruned performance. SparseGPT updates weights to minimize reconstruction error, while the other methods keep retained weights frozen. ProxSparse consistently yields better results compared to all other baselines.}
\label{tab: main_exp_table}
\resizebox{1\textwidth}{!}{
\begin{tblr}{
  cells = {c},
  vline{2-3,11} = {-}{},
  hline{1-2,7,12,17,22,27,32,37} = {-}{},
}
                & Weight Update & Wikitext PPL   & ARC-C          & ARC-E          & SIQA           & HellaSwag      & OBQA           & PIQA           & TruthfulQA     & AVG            \\
Mistral-v0.1-7b & -             & \textit{4.91}           & \textit{0.503}          & \textit{0.809}          & \textit{0.467}          & \textit{0.612} & \textit{0.324} & \textit{0.806} & \textit{0.354} & \textit{0.554}
          \\
magnitude       & \ding{55}            & 14.18          & 0.310          & 0.666          & 0.417          & 0.488          & 0.204          & 0.732          & 0.314          & 0.447          \\
SparseGPT       & \ding{51}           & 9.43           & 0.345          & 0.684          & 0.418          & 0.469          & \textbf{0.240} & 0.730          & 0.316          & 0.501          \\
Wanda           & \ding{55}            & 11.49          & 0.336          & 0.665          & 0.408          & 0.444          & 0.214          & 0.716          & 0.307          & 0.441          \\
ProxSparse         & \ding{55}            & \textbf{8.92}  & \textbf{0.362} & \textbf{0.698} & \textbf{0.428} & \textbf{0.525} & 0.232          & \textbf{0.756} & \textbf{0.350} & \textbf{0.527} \\
Mistral-v0.3-7b & -             & \textit{4.95} & \textit{0.490} & \textit{0.797} & \textit{0.458} & \textit{0.609} & \textit{0.336} & \textit{0.803} & \textit{0.353} & \textit{0.549}
        \\
magnitude       & \ding{55}            & 13.52          & 0.332          & 0.665          & 0.413          & 0.488          & 0.226          & 0.738          & 0.309          & 0.453          \\
SparseGPT       & \ding{51}           & 9.23           & 0.353          & 0.687          & 0.421          & 0.470          & \textbf{0.248} & 0.733          & 0.308          & 0.458          \\
Wanda           & \ding{55}            & 10.97          & 0.311          & 0.648          & 0.408          & 0.442          & 0.206          & 0.716          & 0.300          & 0.433          \\
ProxSparse         & \ding{55}            & \textbf{8.68}  & \textbf{0.362} & \textbf{0.697} & \textbf{0.429} & \textbf{0.525} & 0.242          & \textbf{0.751} & \textbf{0.321} & \textbf{0.475} \\
Qwen2.5-14B    & -             & \textit{4.93}  & \textit{0.56}  & \textit{0.822} & \textit{0.554} & \textit{0.634} & \textit{0.342} & \textit{0.814}& \textit{0.493} & \textit{0.602}\\
magnitude       & \ding{55}    & 48.87          & 0.359          & 0.638          & 0.405          & 0.418          & 0.256          & 0.680         & 0.356          & 0.444          \\
SparseGPT       & \ding{51}    & \textbf{9.19}           & 0.405          & 0.750          & \textbf{0.476}          & 0.512          & \textbf{0.296}          & 0.753         & 0.367          & 0.507          \\
Wanda           & \ding{55}    & 11.69          & 0.389          & 0.729          & 0.440          & 0.491          & 0.286          & 0.740         & 0.331          & 0.485          \\
ProxLLM         & \ding{55}    & 9.28           & \textbf{0.456}          & \textbf{0.772}          & 0.456          & \textbf{0.535}          & 0.290          & \textbf{0.756}         & \textbf{0.406}          & \textbf{0.525}          \\
OpenLlama-7b-v2 & -             & \textit{6.48} & \textit{0.387} & \textit{0.725} & \textit{0.441} & \textit{0.557} & \textit{0.296} & \textit{0.789} & \textit{0.336} & \textit{0.504}
          \\
magnitude       & \ding{55}            & 36.15          & 0.230          & 0.498          & 0.380          & 0.360          & 0.162          & 0.683          & 0.306          & 0.374          \\
SparseGPT       & \ding{51}           & 11.35          & 0.278          & 0.602          & 0.412          & 0.428          & 0.214          & 0.713          & 0.301          & 0.420          \\
Wanda           & \ding{55}            & 13.81          & 0.261          & 0.575          & 0.409          & 0.409          & 0.196          & 0.703          & \textbf{0.310} & 0.409          \\
ProxSparse         & \ding{55}            & \textbf{9.91}  & \textbf{0.281} & \textbf{0.616} & \textbf{0.415} & \textbf{0.472} & \textbf{0.236} & \textbf{0.720} & 0.299          & \textbf{0.434} \\
Anon.Model-1    & -             & \textit{5.12} & \textit{0.433} & \textit{0.763} & \textit{0.461} & \textit{0.571} & \textit{0.314} & \textit{0.781} & \textit{0.321} & \textit{0.521}
        \\
magnitude       & \ding{55}            & 54.74          & 0.301          & 0.618          & 0.411          & 0.454          & 0.216          & 0.701          & 0.322          & 0.432          \\
SparseGPT       & \ding{51}           & 10.30          & 0.326          & 0.655          & \textbf{0.412} & 0.435          & 0.246          & 0.713          & 0.304          & 0.441          \\
Wanda           & \ding{55}            & 11.42          & 0.311          & 0.623          & 0.403          & 0.413          & \textbf{0.248} & 0.706          & 0.305          & 0.430          \\
ProxSparse         & \ding{55}            & \textbf{8.51}  & \textbf{0.331} & \textbf{0.656} & 0.407          & \textbf{0.478} & 0.242          & \textbf{0.716} & \textbf{0.328} & \textbf{0.452} \\
Anon.model-3    & -             & \textit{4.57} & \textit{0.485} & \textit{0.794} & \textit{0.473} & \textit{0.601} & \textit{0.352} & \textit{0.791} & \textit{0.314} & \textit{0.544}
         \\
magnitude       & \ding{55}            & 8.32           & 0.319          & 0.623          & 0.408          & 0.501          & 0.232          & 0.717          & 0.309          & 0.444          \\
SparseGPT       & \ding{51}           & 8.14           & 0.378          & 0.714          & \textbf{0.437} & 0.478          & 0.282          & 0.735          & 0.296          & 0.473          \\
Wanda           & \ding{55}            & 8.35           & 0.340          & 0.683          & 0.424          & 0.464          & 0.246          & \textbf{0.739} & 0.292          & 0.455          \\
ProxSparse         & \ding{55}            & \textbf{6.61}  & \textbf{0.383} & \textbf{0.720} & 0.427          & \textbf{0.532} & \textbf{0.288} & 0.723          & \textbf{0.319} & \textbf{0.486} \\
Anon.model-2    & -             & \textit{5.84} & \textit{0.515} & \textit{0.814} & \textit{0.470} & \textit{0.600} & \textit{0.334} & \textit{0.801} & \textit{0.368} & \textit{0.557}
         \\
magnitude       & \ding{55}            & 766.91         & 0.257          & 0.454          & 0.365          & 0.335          & 0.154          & 0.634          & \textbf{0.319} & 0.360          \\
SparseGPT       & \ding{51}           & 14.61          & 0.316          & \textbf{0.647} & \textbf{0.426} & 0.435          & 0.222          & 0.705          & 0.301          & 0.434          \\
Wanda           & \ding{55}            & 20.91          & 0.269          & 0.573          & 0.400          & 0.380          & 0.192          & 0.686          & 0.309          & 0.401          \\
ProxSparse         & \ding{55}            & \textbf{13.63} & \textbf{0.333} & 0.623          & 0.422          & \textbf{0.460} & \textbf{0.240} & \textbf{0.721} & 0.296          & \textbf{0.444} 
\end{tblr}
}
\end{table*}

\fi


\begin{table*}[!t]
%{-0.5em}
\centering
\caption{Experimental results on Wikitext perplexity (PPL) and 7 commonly used zero-shot natural language reasoning tasks comparing \textbf{ProxSparse} to 3 other baselines on 7 widely used LLMs (Anon.Model-2 results are deferred to Table~\ref{tab:anon_model_2}). \textbf{Bold} indicates the best pruning performance, while \textit{italic} represents the original unpruned performance. SparseGPT updates weights to minimize reconstruction error, while the other methods keep retained weights frozen. ProxSparse consistently yields better results compared to all other baselines.}
\label{tab: main_exp_table}
\resizebox{1\textwidth}{!}{
\begin{tblr}{
  cells = {c},
  vline{2-3,11} = {-}{},
  hline{1-2,7,12,17,22,27,32,37} = {-}{},
}
                & Weight Update & Wikitext PPL   & ARC-C          & ARC-E          & SIQA           & HellaSwag      & OBQA           & PIQA           & TruthfulQA     & AVG            \\
Mistral-v0.1-7b & -             & \textit{4.91}           & \textit{0.503}          & \textit{0.809}          & \textit{0.467}          & \textit{0.612} & \textit{0.324} & \textit{0.806} & \textit{0.354} & \textit{0.554}
          \\
magnitude       & \ding{55}            & 14.18          & 0.310          & 0.666          & 0.417          & 0.488          & 0.204          & 0.732          & 0.314          & 0.447          \\
SparseGPT       & \ding{51}           & 9.43           & 0.345          & 0.684          & 0.418          & 0.469          & \textbf{0.240} & 0.730          & 0.316          & 0.501          \\
Wanda           & \ding{55}            & 11.49          & 0.336          & 0.665          & 0.408          & 0.444          & 0.214          & 0.716          & 0.307          & 0.441          \\
ProxSparse         & \ding{55}            & \textbf{8.92}  & \textbf{0.362} & \textbf{0.698} & \textbf{0.428} & \textbf{0.525} & 0.232          & \textbf{0.756} & \textbf{0.350} & \textbf{0.527} \\
Mistral-v0.3-7b & -             & \textit{4.95} & \textit{0.490} & \textit{0.797} & \textit{0.458} & \textit{0.609} & \textit{0.336} & \textit{0.803} & \textit{0.353} & \textit{0.549}
        \\
magnitude       & \ding{55}            & 13.52          & 0.332          & 0.665          & 0.413          & 0.488          & 0.226          & 0.738          & 0.309          & 0.453          \\
SparseGPT       & \ding{51}           & 9.23           & 0.353          & 0.687          & 0.421          & 0.470          & \textbf{0.248} & 0.733          & 0.308          & 0.458          \\
Wanda           & \ding{55}            & 10.97          & 0.311          & 0.648          & 0.408          & 0.442          & 0.206          & 0.716          & 0.300          & 0.433          \\
ProxSparse         & \ding{55}            & \textbf{8.68}  & \textbf{0.362} & \textbf{0.697} & \textbf{0.429} & \textbf{0.525} & 0.242          & \textbf{0.751} & \textbf{0.321} & \textbf{0.475} \\
Qwen2.5-14B    & -             & \textit{4.93}  & \textit{0.56}  & \textit{0.822} & \textit{0.554} & \textit{0.634} & \textit{0.342} & \textit{0.814}& \textit{0.493} & \textit{0.602}\\
magnitude       & \ding{55}    & 48.87          & 0.359          & 0.638          & 0.405          & 0.418          & 0.256          & 0.680         & 0.356          & 0.444          \\
SparseGPT       & \ding{51}    & \textbf{9.19}           & 0.405          & 0.750          & \textbf{0.476}          & 0.512          & \textbf{0.296}          & 0.753         & 0.367          & 0.507          \\
Wanda           & \ding{55}    & 11.69          & 0.389          & 0.729          & 0.440          & 0.491          & 0.286          & 0.740         & 0.331          & 0.485          \\
ProxSparse         & \ding{55}    & 9.28           & \textbf{0.456}          & \textbf{0.772}          & 0.456          & \textbf{0.535}          & 0.290          & \textbf{0.756}         & \textbf{0.406}          & \textbf{0.525}          \\
OpenLlama-7b-v2 & -             & \textit{6.48} & \textit{0.387} & \textit{0.725} & \textit{0.441} & \textit{0.557} & \textit{0.296} & \textit{0.789} & \textit{0.336} & \textit{0.504}
          \\
magnitude       & \ding{55}            & 36.15          & 0.230          & 0.498          & 0.380          & 0.360          & 0.162          & 0.683          & 0.306          & 0.374          \\
SparseGPT       & \ding{51}           & 11.35          & 0.278          & 0.602          & 0.412          & 0.428          & 0.214          & 0.713          & 0.301          & 0.420          \\
Wanda           & \ding{55}            & 13.81          & 0.261          & 0.575          & 0.409          & 0.409          & 0.196          & 0.703          & \textbf{0.310} & 0.409          \\
ProxSparse         & \ding{55}            & \textbf{9.91}  & \textbf{0.281} & \textbf{0.616} & \textbf{0.415} & \textbf{0.472} & \textbf{0.236} & \textbf{0.720} & 0.299          & \textbf{0.434} \\
Anon.Model-1    & -             & \textit{5.12} & \textit{0.433} & \textit{0.763} & \textit{0.461} & \textit{0.571} & \textit{0.314} & \textit{0.781} & \textit{0.321} & \textit{0.521}
        \\
magnitude       & \ding{55}            & 54.74          & 0.301          & 0.618          & 0.411          & 0.454          & 0.216          & 0.701          & 0.322          & 0.432          \\
SparseGPT       & \ding{51}           & 10.30          & 0.326          & 0.655          & \textbf{0.412} & 0.435          & 0.246          & 0.713          & 0.304          & 0.441          \\
Wanda           & \ding{55}            & 11.42          & 0.311          & 0.623          & 0.403          & 0.413          & \textbf{0.248} & 0.706          & 0.305          & 0.430          \\
ProxSparse         & \ding{55}            & \textbf{8.51}  & \textbf{0.331} & \textbf{0.656} & 0.407          & \textbf{0.478} & 0.242          & \textbf{0.716} & \textbf{0.328} & \textbf{0.452} \\
Anon.model-3    & -             & \textit{4.57} & \textit{0.485} & \textit{0.794} & \textit{0.473} & \textit{0.601} & \textit{0.352} & \textit{0.791} & \textit{0.314} & \textit{0.544}
         \\
magnitude       & \ding{55}            & 8.32           & 0.319          & 0.623          & 0.408          & 0.501          & 0.232          & 0.717          & 0.309          & 0.444          \\
SparseGPT       & \ding{51}           & 8.14           & 0.378          & 0.714          & \textbf{0.437} & 0.478          & 0.282          & 0.735          & 0.296          & 0.473          \\
Wanda           & \ding{55}            & 8.35           & 0.340          & 0.683          & 0.424          & 0.464          & 0.246          & \textbf{0.739} & 0.292          & 0.455          \\
ProxSparse         & \ding{55}            & \textbf{6.61}  & \textbf{0.383} & \textbf{0.720} & 0.427          & \textbf{0.532} & \textbf{0.288} & 0.723          & \textbf{0.319} & \textbf{0.486} 
% Anon.model-2    & -             & \textit{5.84} & \textit{0.515} & \textit{0.814} & \textit{0.470} & \textit{0.600} & \textit{0.334} & \textit{0.801} & \textit{0.368} & \textit{0.557}
%          \\
% magnitude       & \ding{55}            & 766.91         & 0.257          & 0.454          & 0.365          & 0.335          & 0.154          & 0.634          & \textbf{0.319} & 0.360          \\
% SparseGPT       & \ding{51}           & 14.61          & 0.316          & \textbf{0.647} & \textbf{0.426} & 0.435          & 0.222          & 0.705          & 0.301          & 0.434          \\
% Wanda           & \ding{55}            & 20.91          & 0.269          & 0.573          & 0.400          & 0.380          & 0.192          & 0.686          & 0.309          & 0.401          \\
% ProxSparse         & \ding{55}            & \textbf{13.63} & \textbf{0.333} & 0.623          & 0.422          & \textbf{0.460} & \textbf{0.240} & \textbf{0.721} & 0.296          & \textbf{0.444} 
\end{tblr}
}
%{-2em}
\end{table*}