\begin{table*}[!t]
\centering
\caption{Experimental results on Wikitext perplexity (PPL) and performance across 7 commonly used zero-shot natural language reasoning tasks comparing \textbf{ProxSparse} to 3 other baselines on Anon.Model-2. \textbf{Bold} indicates the best pruning performance, while \textit{italic} represents the original unpruned performance. SparseGPT updates weights to minimize reconstruction error, while the other methods keep retained weights frozen. Similar to the results in Table~\ref{tab: main_exp_table}, ProxSparse consistently yields better results compared to all other baselines.}
\label{tab:anon_model_2}
\resizebox{1\textwidth}{!}{
\begin{tblr}{
  cells = {c},
  vline{2-3,11} = {-}{},
  hline{1-2,7} = {-}{},
}
                & Weight Update & Wikitext PPL   & ARC-C          & ARC-E          & SIQA           & HellaSwag      & OBQA           & PIQA           & TruthfulQA     & AVG            \\
Anon.model-2    & -             & \textit{5.84} & \textit{0.515} & \textit{0.814} & \textit{0.470} & \textit{0.600} & \textit{0.334} & \textit{0.801} & \textit{0.368} & \textit{0.557}
         \\
magnitude       & \ding{55}            & 766.91         & 0.257          & 0.454          & 0.365          & 0.335          & 0.154          & 0.634          & \textbf{0.319} & 0.360          \\
SparseGPT       & \ding{51}           & 14.61          & 0.316          & \textbf{0.647} & \textbf{0.426} & 0.435          & 0.222          & 0.705          & 0.301          & 0.434          \\
Wanda           & \ding{55}            & 20.91          & 0.269          & 0.573          & 0.400          & 0.380          & 0.192          & 0.686          & 0.309          & 0.401          \\
ProxSparse         & \ding{55}            & \textbf{13.63} & \textbf{0.333} & 0.623          & 0.422          & \textbf{0.460} & \textbf{0.240} & \textbf{0.721} & 0.296          & \textbf{0.444} \\
\end{tblr}
}
\end{table*}