%{-1.5em}
\section{Empirical evaluation}
%{-0.5em}

\label{sec:evaluation}

In this section, we provide comprehensive evaluations of ProxSparse by addressing the following research questions: \textbf{1.} \textbf{End-to-end performance:} how does ProxSparse compare to other state-of-the-art pruning methods? \textbf{2.} \textbf{The in-depth analysis of mask selection regularizer:} how does the regularizer contribute to finding the effective mask? 
\textbf{3. Efficiency benefit:} does sparsified models produced by ProxSparse improve efficiency?

\subsection{Models, tasks and baselines}
%{-0.5em}
\label{sec:model}
We evaluated ProxSparse on four most advanced and widely used open-source LLM families: Mistral~\cite{jiang2023mistral}, Qwen~\cite{yang2024qwen2}, OpenLlama~\cite{openlm2023openllama} and an anonymous model family (Anon.Model). The specific models used in our experiments include Mistral-v0.1-7b, Mistral-v0.3-7b, Qwen2.5-14b, OpenLlama-7b-v2 and three models from the Anon.Model family. 
% We elaborate more details about the Anon.Model family below.
% \paragraph{The Anon.Model Family} 
The three anonymous models used in our experiments belong to the same model family. We designate the three distinct LLMs in our study as Anon.Model-1, Anon.model-2, and Anon.model-3, arranged in increasing size and each with billions of parameters. The larger size of Anon.model-3 makes it comparatively easier to prune than the smaller models.

We assess the performance of pruned models from different pruning mechanisms on both zero-shot tasks and language modeling. For calibration, we followed Wanda~\cite{sun2023simple} and SparseGPT~\cite{frantar2023sparsegpt} to utilize the C4~\cite{raffel2020exploring} dataset for calibration. Zero-shot performance was evaluated with the EleutherAI LM-Eval-Harness~\cite{eval-harness} on seven widely used tasks~\cite{liu2024dora}, while Wikitext~\cite{merity2016pointer} perplexity (PPL) was used as the language modeling metric, consistent with previous evaluation protocol~\cite{sun2023simple, frantar2023sparsegpt}. The experiments use 400 data samples for calibration unless specified, with consistent counts across baselines for fair comparison. We discuss MaskLLM and present ablation studies on mask effectiveness with regards to calibration sample size in Section~\ref{sec:calib}. For hyperparameters and configurations, we detail them in Appendix~\ref{config}. Our experiments were done on Nvidia A100 GPUs.

%{-1em}
\subsection{End to end performance evaluation}
%{-0.5em}

\label{end-to-end-eval}
\input{table/main_exp_table}

We first present end-to-end performance comparison against other baselines that enforce 2:4 sparsity: magnitude pruning~\cite{han2015deep}, SparseGPT~\cite{frantar2023sparsegpt}, and Wanda~\cite{sun2023simple}. Table~\ref{tab: main_exp_table} presents Wikitext PPL and performance on seven widely used zero-shot reasoning tasks. Overall, ProxSparse consistently outperforms all baselines across tested models.
%{-1em}
\paragraph{Language modeling} We first evaluate language modeling. ProxSparse surpasses magnitude pruning and outperforms Wanda, the SOTA mechanism without weight updates at the same scale. More specifically, ProxSparse achieves a PPL of 9.91 vs. Wanda's 13.81 on OpenLlama-7b-v2 with 28\% improvement. Similarly, ProxSparse achieves a PPL of 8.51 on Anon.Model-1, compared to Wanda's 11.42, reflecting a 35\% improvement. In the Anon.model-2 experiments shown in Table~\ref{tab:anon_model_2}, ProxSparse reduces PPL from Wanda's 20.91 to 13.63. 
% Compare to the Anon.Model-1 model, Anon.model-2 have more information encoded in the model weights as much larger training corpus was used during pretraining. This significant performance improvement highlights the potential of ProxSparse's effectiveness in handling dense model pruning mask selection.
Even when compared to SparseGPT, which updates the weights to minimize error, ProxSparse still outperforms it by up to an 18\% margin, as demonstrated in the Anon.model-3 experiments. In summary, across different models, ProxSparse consistently achieves better PPL with a significant gap compared to other baselines.
\paragraph{Zero-shot Task Performance} We present the performance analysis on seven widely used zero-shot natural language reasoning tasks. 
Consistent with the language modeling results, ProxSparse significantly outperforms both magnitude pruning and Wanda. In the Mistral-v0.1-7B experiments, ProxSparse achieved an average accuracy of 52.7\%, compared to Wandaâ€™s 44.1\%, marking a 20\% improvement in performance. Even with weight updates in SparseGPT, ProxSparse consistently achieves higher accuracy. Similar trends hold for Qwen2.5-14b and other models as well. This highlights ProxSparse's effectiveness in finding an optimal semi-structured mask to maintain superior performance, even compared to pruning methods with weight reconstruction for error reduction.
%{-1em}
\paragraph{Analysis of Better Performance} ProxSparse consistently outperforms all baselines across evaluated models. Its advantage stems from the global feedback in mask exploration, which enables ProxSparse to overcome localized constraints. By optimizing in an end-to-end manner, ProxSparse achieves superior performance gains.

% ProxSparse outperforms all baselines by leveraging global feedback in mask exploration, overcoming localized constraints. Its end-to-end optimization ensures superior performance gains.
%{-1em}
\subsection{Deep dive into the regularizing mechanism}
%{-0.5em}

This section explores the core properties of the mask selection regularizer. The regularizer relaxes rigid mask selection constraints into differentiable optimization for end-to-end learning. In the meantime, its added flexibility with "wiggling room" enhances exploration for better convergence. We ask the question: how does this flexibility aid in exploring the optimal mask during optimization?

% This section examines the mask selection regularizer, which relaxes rigid constraints into a differentiable optimization problem for end-to-end training. This added flexibility enhances exploration, improving mask convergence. But how does it aid in finding the optimal semi-structured mask?
%{-0.5em}

\subsubsection{Hard constraint v.s. soft regularization}
%{-0.5em}

To showcase the effectiveness of soft regularization with flexibility, we compare it with strict constraints. Unlike the gradually sparse regularizer, projected gradient descent (PGD) imposes hard thresholding during optimization. We conducted four experiments to evaluate both regularizers for mask selection, testing each with both soft and hard constraints, as shown in Table~\ref{tab: pip}.
In proximal gradient descent, "hard sparsity constraints" in the table enforce zeroing two of every four weights after each update, ensuring rigid 2:4 structural sparsity. "Hard frozen weights" reset the two largest-magnitude weights to their original values, enforcing strict objectives for mask selection.
With the relaxed regularizer, weights gradually shrink towards the 2:4 pattern (shown in Figure~\ref{fig:llama-lambda}(a)), while the retained weights are encouraged to approximate their original values. This relaxation meets both objectives under more flexible constraints. Table~\ref{tab: pip} indicates that hard constraints performs worst, while relaxed constraints enhance performance. Fully regularizing both semi-structured and frozen weight constraints maximizes flexibility, achieving the best results.
% With the relaxed regularizer, weights gradually shrink to the 2:4 pattern (Figure~\ref{fig:llama-lambda}(a)), while retained weights approximate their original values. This balance improves performance under flexible constraints. Table~\ref{tab: pip} shows that hard constraints perform worst, while relaxed constraints enhance performance. Fully regularizing both semi-structured and frozen weight constraints maximizes flexibility, achieving the best results.
% \usepackage{tabularray}
% \usepackage{tabularray}
% %{-2em}
\begin{table}[!t]
\centering
%{-0.5em}
\caption{Wikitext PPL under hard/soft constraints. Relaxing mask selection constraints improves performance over hard thresholding. Bold indicates the best result.}
\vspace{0.5em}
\label{tab: pip}
\resizebox{1\linewidth}{!}{%

\begin{tabular}{c|cccc} 
\hline
                & \begin{tabular}[c]{@{}c@{}}Both with\\~relaxation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Fronzen weight\\relaxation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sparsity constraints\\relaxation\end{tabular} & \begin{tabular}[c]{@{}c@{}}Both with hard\\Constraints\end{tabular}  \\ 
\hline
Mistral-v0.3-7b & \textbf{8.68}                                                  & 13.23                                                              & 11.24                                                                    & 13.6                                                                 \\ 
\hline
OpenLlama-7b-v2 & \textbf{9.91}                                                  & 34                                                                 & 33.07                                                                    & 35.28                                                                \\ 
\hline
% Anon.Model-1    & \textbf{8.51}                                                  & 48.43                                                              & 45.3                                                                     & 52.85                                                                \\
% \hline
\end{tabular}
}
\vspace{-2em}
\end{table}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{fig/lambda_1_study/openllama.pdf} 
%{-2em}
\caption{Evolution of sparsity ratio on OpenLlama-7b-v2 based on the degree of regularization. (a) Evolution of the 2:4 sparsity ratio over learning, where an insufficient regularization degree leads to under-learning. (b) With a larger $\lambda_{1}$ parameters shrink more quickly towards 2:4 sparsity, resulting in early commitment to a suboptimal mask. (c) Comparison of the 2:4 sparse block ratios at early (0.1 epochs) and final stages of learning. (d) Mask similarity between the final mask and the early mask obtained after 10\% of learning. An excessively large $\lambda_{1}$ results in premature mask commitment, causing mask selection to stagnate and hindering optimal mask discovery.}
    \label{fig:openllama-lambda}
    % \vspace{-1.5em}
\end{figure*}
%{-0.5em}
\subsubsection{The sparsity pattern enforcer}
\label{sec:lambda1}
%{-0.5em}



In the following sections, we analyze the contribution of each regularizer individually, starting with the sparsity pattern regularizer, which encourages 2:4 sparsity.
The regularizer coefficient, $\lambda_{1}$, controls the strength of regularization: higher values enforce more aggressive parameter shrinkage, approaching a harder projection with less flexibility.
To isolate the effect of regularization, we only study the semi-structured regularizer in this analysis. We examine how varying its strength impacts mask learning. As shown in Table~\ref{tab: reg_1}, optimal mask selection occurs at a balance between gradual and aggressive regularizationâ€”smaller values lead to conservative mask evolution, while larger values impose stricter constraints, both reducing performance.



% %{1.5em}

%{-0.5em}
To better understand this phenomenon, we analyze the regularizer's impact in detail. We show the evolution of 2:4 sparsity across different $\lambda_{1}$ values for OpenLlama-7b-v2 (Figures~\ref{fig:openllama-lambda}) and Anon.Model-1(Figure~\ref{fig:llama-lambda} in Appendix~\ref{app:reg_anon}) with consistent trend. We use OpenLlama-7b-v2 as the example. if $\lambda_{1}$ is too low, the model remains largely dense, as shown in Figures~\ref{fig:llama-lambda}(a), (b) and (c). This suggests under-learning, where unimportant weights are not fully recognized by the end of learning, resulting in incomplete mask selection. 
% Conversely, a high $\lambda_{1}$ value leads to an early commitment to a specific mask. In Figure~\ref{fig:llama-lambda}(d), the yellow line represents the similarity to the "early mask," which is the mask obtained after just going through 10\% epochs of learning. The final mask retains 99.5\% similarity to this early mask, indicating that mask optimization has stalled. 
Conversely, a high $\lambda_{1}$ value leads to early commitment to a specific mask. In Figure~\ref{fig:llama-lambda}(d), the yellow line shows similarity to the "early mask" obtained after just 10\% of learning. The final mask retains $\sim$99.5\% similarity to the early one, indicating stalled optimization.
In between, a balanced strength allows flexible mask exploration that avoids premature commitment, while also enabling more effective learning than excessively low values.



% \usepackage{tabularray}
% \usepackage{tabularray}
\begin{table}[!t]
%{-0.5em}
\centering
\caption{
Wikitext PPL across $\lambda_{1}$.
% Higher $\lambda_{1}$ indicates more aggressive regularization. 
Optimal performance occurs at balanced regularization. Bold indicates the best performance.}
\label{tab: reg_1}
\vspace{0.5em}
\resizebox{1\linewidth}{!}{%

\begin{tblr}{
  cells = {c},
  vline{2} = {-}{},
  hline{1,3,5,7} = {-}{},
}
~$\lambda_{1}$         & 0.001  & 0.01  & 0.1   & 0.25          & 0.5            & 2     & 10    & 50    & inf   \\
Mistral-v0.3-7b & 14.19 & 9.66  & 9.04 & 8.69         & \textbf{8.68} & 8.82 & 9.32  & 10.94 & 11.33 \\
~$\lambda_{1}$         & 0.001  & 0.1   & 0.25  & 1             & 2              & 5     & 10    & 50    & inf   \\
OpenLlama-7b-v2 & 25.23 & 11.25 & 10.89 & \textbf{9.91} & 10.13         & 10.97 & 12.11 & 23.29 & 33.09 %\\
% ~$\lambda_{1}$         & 0.001  & 0.01  & 0.1   & 0.25          & 0.5            & 2     & 10    & 50    & inf   \\
% Anon.Model-1    & 34.173 & 22.9  & 9.68  & \textbf{8.51} & 8.55           & 9.03  & 11.31 & 26.16 & 46.18 
\end{tblr}
}
\vspace{-1.5em}
\end{table}


%{-1em}
\subsubsection{The frozen weight retention enforcer}
%{-0.5em}
\label{frozen}
% We further examine the behavior of the frozen weight regularizer. The strength of this regularization is controlled by the coefficient $\lambda_{2}$. We utilize the optimal value of $\lambda_{1}$ determined from previous analyses to enforce semi-structured sparsity, and vary $\lambda_{2}$ to observe its impact on model performance. Interestingly, we find that the frozen weight regularizer creates a broad optimal performance plateau, indicating that a robust range of $\lambda_{2}$ can be applied without significantly impacting performance, as shown in Table~\ref{tab: reg_2}. 
We analyze the impact of the frozen weight regularizer, with its strength controlled by $\lambda_{2}$. Using the optimal $\lambda_{1}$ from previous analyses, we vary $\lambda_{2}$ to assess its effect. Interestingly, Table~\ref{tab: reg_2} shows a broad optimal performance plateau, suggesting that a robust range of $\lambda_{2}$ values can be applied without significantly affecting performance.

We further plot the evolution of the relative norm gap across $\lambda_{2}$ in Figure~\ref{fig:lambda2}. 
% in Appendix~\ref{app:reg_w0}. 
This gap quantifies the difference in norm between the learned model and the original model, with the mask applied. It assesses how closely retained weights preserve their original values. We see that even without the regularizer, the relative norm gap stays $\sim$20\%. Adding the regularizer incurs small impact until the strength reaches a high extent.
% We see that even without applying the regularizer, the relative norm difference remains controlled at approximately 20\%. Adding the regularizer does not significantly alter this difference until the regularization strength reaches a high extent.
This may result from implicit frozen weight constraints imposed by the sparsity pattern regularizer, which we leave it to future investigation. As $\lambda_{2}$ increases toward infinity, strict projection enforcement degrades performance, aligns with previous finding and reinforces the need for gradual and flexible mask optimization.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]
%     {}
%     \caption{The norm difference between different $\lambda_{2}$
%     }
%     \label{fig:regularization_path}
% \end{figure}




% \usepackage{tabularray}
\begin{table}[!t]
\centering
\caption{Wikitext PPL across $\lambda_{2}$. A broad optimal plateau suggests that performance remains stable across a robust range of $\lambda_{2}$.}
\label{tab: reg_2}
\vspace{0.5em}
\resizebox{1\linewidth}{!}{%
\begin{tabular}{c|cccccccc} 
\hline
$\lambda_{2}$          & 0     & 0.5   & 2     & 5     & 20    & 100    & 2500  & inf     \\
Mistral-v0.3-7b & 8.68 & 8.88 & 8.9   & 8.82 & 8.99  & 8.85  & 9.11  & 13.23   \\ 
\hline
$\lambda_{2}$          & 0     & 0.5   & 2     & 20    & 100   & 500    & 2500  & inf     \\
OpenLlama-7b-v2 & 9.91  & 9.92 & 9.96 & 10.12 & 10.41 & 10.9 & 12.38  & 34  \\ 
\hline
% $\lambda_{2}$          & 0     & 0.5   & 2     & 5     & 20    & 100    & 500   & inf     \\
% Anon.Model-1    & 8.51  & 8.49 & 8.55 & 8.58 & 9.18  & 10.4   & 15.78 & 48.43  \\
% \hline
\end{tabular}
}
\vspace{-1em}
\end{table}

\begin{figure}[!t]
    \centering
    \vspace{1em}
    \includegraphics[width=\linewidth]{fig/lambda2/lambda2.pdf} % Adjust width if needed
    \caption{The relative norm difference over different $\lambda_{2}$. The relative norm gap measures how closely retained weights match their original values post-training, with the semi-structured mask applied. The relative norm remained low ($\sim$20\%) with minimal change until a high lambda value was applied.}
    \label{fig:lambda2}
\end{figure}
%{-0.5em}
\subsubsection{Performance evolution with varying numbers of calibration samples}
\label{sec:calib}
%{-0.5em}

Our method enables effective semi-structured mask selection with only hundreds of samples. Here we analyze performance based on the number of calibration samples with OpenLlama-7b-v2 and Anon.Model-1.
We compare our results with MaskLLM, SparseGPT, and Wanda using 100, 200, and 400 samples. MaskLLM struggles with small sample sizes, making it a complementary method to ours in large-scale learning. We use the statistics reported in the MaskLLM paper~\cite{fang2024maskllm}.
% As shown in Table~\ref{tab:cali}, MaskLLM performed the worst on Anon.Model-1 when sample sizes were low, likely due to its reliance on extensive training to produce an effective frozen weight mask. SparseGPT and Wanda showed minimal improvement as the number of calibration samples increased, consistent with previous observations~\cite{fang2024maskllm, sun2023simple}. 
As shown in Table~\ref{tab:cali}, MaskLLM performed worst on Anon.Model-1 with low sample sizes, likely due to its reliance on extensive training for effective masks. SparseGPT and Wanda showed minimal improvement with increased calibration samples, consistent with previous observations~\cite{fang2024maskllm, sun2023simple}. 
% In comparison, ProxSparse achieved the best performance across there sample sizes, and we observe the performance slightly improved as the number of calibration samples increased in our targeted regime. This confirms the effectiveness of our method in learning towards an optimal mask selection.
ProxSparse achieved the best across these sample sizes, with slight performance gains as samples increased within our target range. This confirms the effectiveness of our method in learning towards an optimal mask for semi-structured sparsity.

% \usepackage{tabularray}
\begin{table}[!t]
%{-0.5em}
\centering
\caption{Wikitext PPL across calibration sample sizes. ProxSparse outperformed all methods, with performance slightly improved as sample size increased, confirming its effectiveness in optimal mask learning. Bold indicates the best performance.}
\vspace{0.5em}
\label{tab:cali}
\resizebox{1\linewidth}{!}{%
\begin{tblr}{
  cells = {c},
  hlines,
  vline{2,5-6} = {-}{},
}
OpenLlama-7b-v2 & 100            & 200            & 400           & Anon.Model-1 & 100           & 200           & 400           \\
MaskLLM         & -              & -              & -             & MaskLLM      & $>13$            & $>13$            & $>11$            \\
SparseGPT       & 11.581         & 11.478         & 11.35         & SparseGPT    & 10.36         & 10.32         & 10.298        \\
Wanda           & 13.854         & 13.828         & 13.814        & Wanda        & 11.46         & 11.45         & 11.42         \\
ProxSparse         & \textbf{10.39} & \textbf{10.09} & \textbf{9.91} & ProxSparse      & \textbf{9.24} & \textbf{8.99} & \textbf{8.51} 
\end{tblr}
}
%{-2em}
\end{table}

%{-0.5em}
\subsection{Improved efficiency during inference}
% \section{Improved efficiency during inference}

\label{app:effi}
Finally, we evaluate the efficiency-related metrics of the sparsified model produced by ProxSparse. We present wall-clock inference speedup and memory footprint improvements for the 2:4 semi-structured sparsified model induced by ProxSparse. Our experiments are conducted on Nvidia A100 GPUs. We utilize the Nvidia CUTLASS library as the underlying implementation for 2:4 semi-structured sparse operations.

\begin{table}[!t]
    \centering
    % \vspace{-1em}
    \caption{Speedup and memory utilization improvements achieved by ProxSparse induced 2:4 sparsity models(left: speedup, right: memory reduction). ProxSparse delivers a 1.3xâ€“1.35x speedup for matrix multiplication and a 1.26x end-to-end inference speedup on the Mistral-v0.3-7b model. Additionally, ProxSparse reduces memory consumption by 29.5\%â€“37.3\% across different models, demonstrating its efficiency in both computation and memory utilization.}
    \vspace{0.5em}
    \label{memory}
    \resizebox{1\linewidth}{!}{%

    \begin{tabular}{l c || l c}
    \hline
    \textbf{Module name} & \textbf{Speedup ratio} 
    & \textbf{Model family} & \textbf{Memory gain} \\
    \hline\hline
    self\_attn q/k/v/o & 1.35x  & Openllama\_7b\_v2 & 70.50\% \\
    mlp up/down/gate   & 1.30x  & Qwen2.5-14b        & 67.50\% \\
    End-to-end inference                  & 1.26x     & Mistral-v0.3-7b     & 62.70\% \\
    \hline
    \end{tabular}
    }
\end{table}
\subsubsection{Inference speedup}
We follow the evaluation setup of previous work~\cite{frantar2023sparsegpt,sun2023simple} and measure the latency of matrix multiplication in linear layers. The results of Mistral-v0.3-7b (batch size of 1) are presented in Table~\ref{memory}. As shown in the table, 2:4 semi-structured sparsity induced by ProxSparse provides significant inference speedup for linear layers in LLMs, achieving an average speedup gains of 1.3 to 1.35. Additionally, we measured the end-to-end inference wall-clock speedup and observed a 1.26x speedup, consistent with other sparsification methods evaluated in our experiments. We emphasize that the inference speedup is not specific to our pruning method but rather a result of the inherent computational efficiency enabled by semi-structured sparsity.

\subsubsection{Memory footprint improvements}

Next, we evaluate the memory footprint reductions achieved by ProxSparse-sparsified models. The results of peak memory utilization during inference time (batch size = 1) for different models are presented in Table~\ref{memory}. ProxSparse reduces peak memory usage by 29.5\% to 37.3\%, demonstrating significant memory savings with 2:4 sparsification. The exact reduction varies across different model architectures due to differences in model weight sizes, which influence activation sizes and ultimately affect peak memory consumption. Overall, ProxSparse effectively reduces memory footprint during LLM inference, highlighting the system benefits of the 2:4 sparse operation.

\if 0
\subsubsection{Inference speedup}
We follow the evaluation setup of previous work~\cite{frantar2023sparsegpt,sun2023simple} and measure the latency of matrix multiplication in linear layers. The results for Mistral-v0.3-7b (batch size of 1) are presented in Table~\ref{memory}. As shown in the table, 2:4 sparsity provides significant inference speedup for linear layers in LLMs, achieving an average speedup gains of 1.3 to 1.35. Additionally, we measured the end-to-end inference wall-clock speedup and observed a 1.26x speedup, consistent with other sparsification methods evaluated in our experiments. We emphasize that this inference speedup is not specific to our pruning method but rather a result of the inherent computational efficiency enabled by semi-structured sparsity.

\subsubsection{Memory footprint improvements}

The peak memory utilization during inference (batch size = 1) for different models is shown in Table~\ref{memory}. ProxSparse reduces peak memory usage by 29.5\% to 37.3\%, demonstrating significant memory savings with 2:4 sparsification. The exact reduction varies across different model architectures due to differences in model weight sizes, which influence activation sizes and ultimately affect peak memory consumption. Overall, ProxSparse effectively reduces memory footprint during LLM inference, highlighting the system benefits of the 2:4 sparse operation.

\begin{table}[ht]
    \centering
    \caption{Speedup and memory utilization improvements achieved by ProxSparse with 2:4 sparsity (left: speedup, right: memory reduction). ProxSparse delivers a 1.3xâ€“1.35x speedup for matrix multiplication and a 1.26x end-to-end inference speedup on the Mistral-v0.3-7b model. Additionally, ProxSparse reduces memory consumption by 29.5\%â€“37.3\% across different models, demonstrating its efficiency in both computation and memory usage.}
    \label{memory}
    \resizebox{1\linewidth}{!}{%

    \begin{tabular}{l c || l c}
    \hline
    \textbf{Module name} & \textbf{Speedup ratio} 
    & \textbf{Model family} & \textbf{Memory gain} \\
    \hline\hline
    self\_attn q/k/v/o & 1.35  & open\_llama\_7b\_v2 & 70.50\% \\
    mlp up/down/gate   & 1.30  & Qwen-2.5-14b        & 67.50\% \\
    End-to-end inference                  & 1.26     & Mistral-v0.3-7b     & 62.70\% \\
    \hline
    \end{tabular}
    }
\end{table}
\fi