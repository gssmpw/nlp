\section{Conclusion}


Large language models (LLMs) excel in natural language processing tasks and other downstreaming tasks. However, LLMs suffer from high computational costs due to their enormous parameter sizes. Semi-structured sparsity can improve inference efficiency, though it remains challenging due to the structural constraints during pruning. We propose a learning-based method with regularized optimization, progressively explores optimal mask through end-to-end gradient feedback. Extensive evaluation shows that ProxSparse significantly outperforms previous methods, enabling better accuracy for LLM pruning, making model deployment more cost-effective.