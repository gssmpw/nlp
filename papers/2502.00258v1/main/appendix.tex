
\section{Proofs of Technical Results}\label{sec:proofs}
\begin{proof}[Proof of Proposition~\ref{prop:24}]
For the first statement, check that if at least two parameters are $0$, there is at least one $0$ in all $4\choose3$ subsets, making the whole regularizer $0$. If at least three parameters are non-zero, then there is at least one group that is non-zero. The second statement follows by symmetry.  The third statement is valid because it is a cubic function when in the strict interior of an orthant.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:locality}]
First observe that this regularizer applies pointwise to each coordinate of $W$. It suffices to prove the statements for one coordinate $w, w_0$. w.l.o.g. assume $w_0>0$, then the regularizer gives $\left|\frac{w(w-w_0)}{(1+\epsilon)w_0}\right|^2$. Observe that the nullspace is either $0$ or $w=w_0$, thus checking Statement 1. Statement 2 follows because all coordinates with $w = 0$ contributes $0$ to the total.  Statement 3 follows because this is a fourth order polynomial of $w$, thus continuously differentiable.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:innerloop_convergence}]
We start with $S=4$. Define $f:=g_4$ as a shorthand. 

It should be noted that for all $1 \leq i , j \leq 4$, the partial functions $f_{i}\left(w_{i}\right) = f\left(w\right)$, for fixed $w_{j}$ with $j \neq i$, are strongly convex and quadratic. Therefore, when $i = 1$ for instance, we have for any $w_{i}$ and $v_{i}$ that
    \begin{equation*}
        f_{i}\left(w_{i}\right) = f_{i}\left(v_{i}\right) +    f_{i}^\prime\left(v_{i}\right) \left(w_{i} - v_{i}\right) + \frac{1}{2}(w_{i} - v_{i})^{2}.
    \end{equation*}
    Therefore, for any fixed $w_{j}$ with $j \neq i$, let $w_{i}^{\ast} = \argmin_{w_i\geq 0} f_i(w)$.
    %the function $f_{i}$ has a unique minimizer $w_{i}^{\ast}$, 
    $w_{i}^{\ast}$ satisfies the following (when $i = 1$ for instance)
\resizebox{\columnwidth}{!}{
    \parbox{\columnwidth}{
        \begin{align}
            f\left(w\right) = f_{1}\left(w_{1}\right) & = f_{1}\left(w_{1}^{\ast}\right) + f_{1}^\prime\left(w_{1}^{\ast}\right) \left(w_{1} - w_{1}^{\ast}\right) + \frac{1}{2}(w_{1} - w_{1}^{\ast})^2 \nonumber \\
            & \geq f_{1}\left(w_{1}^{\ast}\right) + \frac{1}{2}(w_{1} - w_{1}^{\ast})^2 \nonumber \\
            & = f\left(w_{1}^{\ast} , w_{2} , w_{3} , w_{4}\right) + \frac{1}{2}(w_{1} - w_{1}^{\ast})^2. 
        \label{DescentPro}
        \end{align}
    }
}
The inequality is due to the first-order optimality condition.
    This means that we have a sufficient descent property with respect to each minimized variable.

    \begin{proposition}
        Let $\left\{ w^{k} \right\}_{k \in \mathbb{N}}$ be a sequence generated by the Alternating Minimization algorithm. Then, for all $k \in \mathbb{N}$, we have that 
        \begin{equation}\label{eq:sufficient_descent}
            f\left(w^{k}\right) \geq f\left(w^{k + 1}\right) + \frac{1}{2}\| w^{k + 1} - w^{k} \|^{2}.
        \end{equation}
    \end{proposition}
%\resizebox{\columnwidth}{!}{
    %\parbox{\columnwidth}{
        \begin{proof}
            Let $k \in \mathbb{N}$. Using \eqref{DescentPro} for all $1 \leq i \leq 4$ yields
            \begin{align*}
                f\left(w^{k}\right) &\geq f\left(w_{1}^{k + 1}, w_{2}^{k}, w_{3}^{k}, w_{4}^{k}\right) + \frac{1}{2}(w_{1}^{k + 1} - w_{1}^{k})^2 \\
                f\left(w_{1}^{k + 1}, w_{2}^{k}, w_{3}^{k}, w_{4}^{k}\right) &\geq f\left(w_{1}^{k + 1}, w_{2}^{k + 1}, w_{3}^{k}, w_{4}^{k}\right) + \frac{1}{2}(w_{2}^{k + 1} - w_{2}^{k})^2 \\
                f\left(w_{1}^{k + 1}, w_{2}^{k + 1}, w_{3}^{k}, w_{4}^{k}\right) &\geq f\left(w_{1}^{k + 1}, w_{2}^{k + 1}, w_{3}^{k + 1}, w_{4}^{k}\right) + \frac{1}{2}(w_{3}^{k + 1} - w_{3}^{k})^2 \\
                f\left(w_{1}^{k + 1}, w_{2}^{k + 1}, w_{3}^{k + 1}, w_{4}^{k}\right) &\geq f\left(w^{k + 1}\right) + \frac{1}{2}(w_{4}^{k + 1} - w_{4}^{k})^2.
            \end{align*}
            Adding all the inequalities yields the desired result.
        \end{proof}
    %}
%}

Telescope \eqref{eq:sufficient_descent}, we get that 
%$$\min_{k}\|w^{k+1}-w_k\|^2 \leq 
$$\min_{k\in[T]}\|w^{k+1} - w^k\|^2\leq \frac{1}{T} \sum_{k=1}^K \|w^{k+1} - w^k\|^2 \leq \frac{f(w^0) - f(w^{k+1})}{T} \leq \frac{4\lambda\|z\|^3}{T}.$$ 
The last inequality follows as we initialize at $w^0=z$, thus $f(w^0)\leq 4\lambda \|z\|^3$. Also, since $w^k\in\R^4$,  $f(w^{k+1})$ is non-negative. This completes the proof for the first statement. 

% \yw{@Shoham, the following is still a bit fuzzy to me. Could you check / fix?}
Now take $\epsilon\rightarrow 0$, as the position this algorithm halt, $\|w^{k+1} - w^k\|^2 \leq \epsilon^2 \rightarrow 0$. 

By Theorem~1 of \cite{BST2016}, $w^k$ at $k\rightarrow \infty$ is a critical point of the objective function with the non-negative constraints handled by adding an indicator function.



The argument for the $S=3$ case follows analogously (hence omitted).
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:outerloop_convergence}]

Under the assumption, the function $\text{Reg}_{W_0}$ has a locally Lipschitz continuous gradient, which implies that the function $f$ also has a locally Lipschitz continuous gradient. Therefore, the convergence of the sequence $\{x_{t} \}_{t \in \mathbb{N}}$ convergence to a critical point of the function $\psi \equiv f + h$ follows immediately from \cite{CHT2022} (since $\psi$ is a semi-algebraic function and $\{x_{t} \}_{t \in \mathbb{N}}$ is bounded). 

\end{proof}
\if 0
\section{Details on convex optimization for Theorem~\ref{thm:split}}
\label{theory_detail}
For completeness, we restate the result from Theorem~\ref{thm:split}~\cite{jonas_paper} here. To solve \eqref{eq:prox} for any $y\in\R^4$, it suffices to solve: 
\begin{equation}\label{eq:prox_reformulate}
\small
\begin{aligned}
    \min_{w \in \mathbb{R}_+^4} \frac{1}{2} \|w - z\|^2 + \lambda \mathrm{Reg}_{2:4}(w)
\end{aligned}
\end{equation}
where $z = \mathrm{sorted}(|y|)$ is non-negative and sorted in descending order, i.e., $z_1\geq z_2\geq z_3 \geq z_4\geq 0$. Moreover, the optimal solution to \eqref{eq:prox_reformulate} must be one of the following three candidates:
\begin{enumerate}
    \item ``2-sparse solution'' $[z_1,z_2, 0,0]$; 
    \item ``3-sparse solution'', $[\dot{w}_1,\dot{w}_2,\dot{w}_3,0]$
    \item ``dense solution'' $[\ddot{w}_1,\ddot{w}_2,\ddot{w}_3,\ddot{w}_4]$
\end{enumerate}
where $\dot{w} = \argmin_{w\in\mathbb{R}_+^3} \{g_3(w) \; \text{s.t.}\; \nabla^2 g_3(w) \succeq 0 \}$ with $$g_3(w) :=\frac{1}{2}\|w - z_{1:3}\|^2 + \lambda (w_1w_2 + w_2w_3 + w_3 w_1),$$
and $\ddot{w} = \argmin_{w\in\mathbb{R}_+^4} \{g_4(w) \; \text{s.t.}\; \nabla^2 g_4(w) \succeq 0 \}$ with $g_4(w)$ being the objective function of \eqref{eq:prox_reformulate}.

\citet{jonas_paper} further showed that $\{w | \nabla^2 g_3(w)\succeq 0 \}$ and  $\{ w | \nabla^2 g_4(w)\succeq 0 \}$ are \emph{convex sets}, which makes the corresponding optimization problems convex.

\fi

\section{Hyperparameters and configurations}
\label{config}
Table~\ref{tab: config} presents the configurations and hyperparameters used in our experiments. There are three key hyperparameters for learning an optimal semi-structured mask: sparsity regularization strength ($\lambda_{1}$), frozen weight regularization extent (
$\lambda_{2}$), and learning rate. As discussed in Section~\ref{frozen}, the frozen weight regularization is robust across a wide range of values. Our learning procedure follows standard settings, using \textit{AdamW} as the optimizer with a warmup ratio of 0.1.

% \usepackage{tabularray}
\begin{table}[H]
\centering
\caption{Configure of the parammeter used in the experiment}
\label{tab: config}
\resizebox{0.4\linewidth}{!}{
\begin{tblr}{
  cells = {c},
  hlines,
  vline{2-6} = {-}{},
}
                & $\lambda_{1}$~ & $\lambda_{2}$~ & Learning rate & Optimizer & Warmup-ratio \\
Mistral-v0.1-7b & 20         & 0          & 5.00E-05      & Adamw     & 0.1          \\
Mistral-v0.3-7b & 25         & 0          & 5.00E-05      & Adamw     & 0.1          \\
Qwen-2.5-14b    & 0.2             & 0             & 0.0001        & Adamw     & 0.1          \\
OpenLlama-7b-v2 & 1          & 0          & 0.0001        & Adamw     & 0.1          \\
Anon.Model-1     & 0.25       & 0          & 0.0001        & Adamw     & 0.1          \\
Anon.Model-2     & 0.5        & 0.5        & 0.0001        & Adamw     & 0.1          \\
Anon.Model-3     & 0.85       & 0          & 5.00E-05      & Adamw     & 0.1          
\end{tblr}
}
\end{table}


\section{Regularization trajectory of the optimization algorithm.}
\label{app:reg_path}
We illustrate the regularization path for an example initialization with different $\lambda$ value using different optimization algorithms (EnumIPM, EnumPGD, and EnumALM) in Figure~\ref{fig:regularization_path} as explained in Section~\ref{sec_effi_proximal}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]
    {reg_path.pdf}

    \caption{Illustration of the solution to \eqref{eq:prox} with an example input $y=[1.4, 1.1, 1.0, 0.7]$ as $\lambda$ increases. Observe that (1) the regularizer shrinks different coordinates differently according to their relative magnitude (2) all three algorithms return the same solution path. (3) the dashed lines indicate the two thresholds of $\lambda$ from KKT conditions above which the 3-sparse and 2-sparse solutions become critical points (a necessary condition for them to become global optimal). 
    }
    \label{fig:regularization_path}
\end{figure}



\section{End-to-end evaluation results on Anon.Model-2}
\input{table/model_2}

In this section, we further discuss the evaluation results from Table~\ref{tab: main_exp_table}, focusing on Anon.Model-2. We present results on Wikitext perplexity (PPL) and performance across seven commonly used zero-shot natural language reasoning tasks, comparing ProxSparse to three baselines in Table~\ref{tab:anon_model_2}. In the Anon.Model-2 experiments, ProxSparse significantly reduces perplexity from Wanda’s 20.91 to 13.63. The overall results, compared to Magnitude Pruning, Wanda, and SparseGPT, follow the same trends discussed in Section~\ref{end-to-end-eval}, with ProxSparse consistently outperforming all other baselines.


\section{Evolution of 2:4 sparsity across $\lambda_{1}$ on Anon.Model-1}
In this section, we expand on the discussion from Section~\ref{sec:lambda1} and present the evolution trajectory of 2:4 sparsity across different $\lambda_{1}$ values on Anon.Model-1. Our findings on Anon.Model-1 are consistent with the main paper's discussion: a balanced regularization strength enables flexible mask exploration, preventing premature commitment while also facilitating more effective learning compared to excessively low values.
\label{app:reg_anon}
\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\textwidth]{fig/lambda_1_study/llama2_lambda1.pdf} 
\caption{Evolution of sparsity ratio on Anon.Model-1 based on the degree of regularization. (a) Evolution of the 2:4 sparsity ratio over learning progress, where an insufficient regularization degree leads to under-learning. (b) With a larger $\lambda_{1}$ parameters shrink more quickly towards 2:4 sparsity, resulting in early commitment to a suboptimal mask. (c) Comparison of the 2:4 sparse block ratios at early (0.1 epochs) and final stages of learning. (d) Mask similarity between the final mask and the early mask obtained after 0.1 epochs of learning. An excessively large $\lambda_{1}$ results in premature mask commitment, causing mask selection to stagnate and hindering optimal mask discovery.}
    \label{fig:llama-lambda}
    \vspace{-1.5em}
\end{figure*}

\if 0
\section{Evolution of relative norm gap accross $\lambda_2$}
In this section, we show the evolution of the relative norm gap across $\lambda_{2}$ in Figure~\ref{fig:lambda2}. This gap quantifies the difference in norm between the learned model and the original model, with the mask applied. The detailed discussion of the trajectory can be found in Section~\ref{frozen}.
\label{app:reg_w0}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\columnwidth]{fig/lambda2/lambda2.pdf} % Adjust width if needed
    \vspace{-0.5em}
    \caption{The relative norm difference over different $\lambda_{2}$. The relative norm gap measures how closely retained weights match their original values post-training, with the semi-structured mask applied. The relative norm remained low ($\sim$20\%) with minimal change until a high lambda value was applied.}
    \label{fig:lambda2}
    \vspace{-1em}
\end{figure}
\fi 
\section{Practical scenario of 2:4 sparsity and its extensibility discussion}
\label{Discussion}
To the best of our knowledge, commercially available hardware such as Nvidia Ampere GPUs, only supports the 2:4 sparsity pattern\footnote{\href{https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf}{NVIDIA AMPERE GA102 GPU ARCHITECTURE Whitepaper}}
. Our method directly aligns with the hardware features, making it directly applicable to real-world use cases. Meanwhile, our regularizer is flexible; extending to a 1:4 sparsity pattern is straightforward, as the regularizer can be reformulated and solved with even greater efficiency. On the otherhand, semi-structured patterns like 4:8 increase regularization terms,
% 12x (C(8, 5) = 56 = 12 * C(4,3))
which could slow the solving process. Despite the longer but tolerable search time, 
inference gain remains unaffected once the optimal mask is found. A more efficient solver could further improve handling of such complex patterns, and we leave this for future exploration.

In the meantime, we note that increasing sparsity complexity (e.g., 2:4 to 4:8) will expand the search space, which is a common challenge for learning-based methods, including MaskLLM~\cite{fang2024maskllm}. Nevertheless, our regularizer supports extensibility and shows practical benefits in real-world scenarios.
\vspace{-0.5em}

