\vspace{-2em}
\section{Introduction}



Large Language Models (LLMs) have demonstrated strong performance across a wide range of natural language processing (NLP) tasks~\cite{brown2020language, achiam2023gpt, wei2022emergent}. However, deploying and serving these LLMs is not cost-efficient due to their massive size~\cite{frantar2023sparsegpt, yuan2024kv}, often containing billions of parameters. To address the high computational demands and improve accessibility, various techniques have been proposed to make LLMs more efficient, such as model compression~\cite{han2015deep, frantar2022gptq}. By reducing memory footprint and accelerating computation, model compression significantly improves the feasibility and cost-effectiveness of deploying LLMs at scale~\cite{liu2024kivi, frantar2022gptq, lin2024awq}.

Network pruning~\cite{lecun1989optimal} is commonly used to reduce model size by removing unimportant parameters, lowering computational cost and improving efficiency~\cite{bai2024sparsellmglobalpruningpretrained}. Among various pruning patterns, semi-structured pruning~\cite{mishra2021accelerating}, or block-wise N:M sparsification, has emerged as a practical and effective approach for LLM compression~\cite{sun2023simple, fang2024maskllm}.
In this approach, only N non-zero elements are retained out of M consecutive elements within each parameter block. This semi-structured sparsity strikes a balance between model accuracy and hardware efficiency, and is well-supported by many hardware accelerators~\cite{mishra2021accelerating}, enabling efficient serving for LLMs. 


Despite its advantages, finding an effective semi-structured mask for LLMs remains challenging. Pruning must follow a per-block structural restriction, making efforts on other patterns hard to adopt. Additionally, extensive retraining after pruning is impractical due to LLMs' massive parameter size.~\cite{ma2023llm}. Recent advances like Wanda~\cite{sun2023simple} and SparseGPT~\cite{frantar2023sparsegpt} have improved semi-structured pruning using minimal resources with only hundreds of calibration samples, but still struggle to maintain optimal performance after pruning. 
We identify two main challenges in finding effective semi-structured masks: \textbf{1.} The heuristic rules used for mask selection cannot fully take advantage of the calibration dataset during pruning. Methods like SparseGPT and Wanda rely on the Hessian matrix and importance scores to select elements to prune, but these lightweight criteria fail to effectively leverage or learn from the calibration data. \textbf{2.} Both methods focus on solving a ``local'' optimization problem associated with individual layer, without considering the broader, end-to-end optimization across the entire model. In those methods, pruning is based on localized information within each layer, without considering the connections across layers. Thus they cannot benefit from the global feedback, limiting the overall effectiveness of the pruning method.

We advocate a learning based solution for semi-structured mask selection that incorporates global feedback. We propose \textbf{ProxSparse}, which learns to discover semi-structured masks through an end-to-end optimization process, rather than solely relying on local, heuristic-based decisions. 
ProxSparse enables a finetuning-like procedure that learns the mask through only hundreds of calibration datasets with low resource utilization.
The core of ProxSparse is the mask selection regularizer applied during learning, which transforms the rigid, non-differentiable mask-selection problem into a gradual search process. ProxSparse progressively enforces semi-structured sparsity and frozen weight constraints during training, gradually shrinking unimportant weights to be pruned. ProxSparse does not involve additional weight updates after deterimining the mask. 
One challenge in regularized learning is the efficiency of the solver, as a slow solver makes end-to-end learning on LLMs impractical. To address this, we developed a fast solver using iterative soft-thresholding, enabling efficient end-to-end learning at LLM scale.



To comprehensively evaluate our method, we conducted extensive experiments on 7 widely used high-performance open-source models from four model families including Mistral~\cite{jiang2023mistral}, Qwen~\cite{yang2024qwen2}, OpenLlama~\cite{openlm2023openllama} and an anonymous model family (elaborated in Sec~\ref{sec:model}). The benchmarks cover language modeling and seven widely used natural language reasoning tasks. The results show that our regularized learning significantly outperforms baselines consistently accross all evaluated models, producing more effective pruning masks. Our contributions are summarized as follows:


\begin{itemize}
\vspace{-1em}
\item We propose to apply mask selection regularizer for end-to-end learning of semi-structured masks in LLMs. It allows gradual mask discovery with gradient feedback, enabling global optimization with flexibility, which leads to substantial improvements.

\vspace{-0.5em}

\item We developed an efficient proximal gradient descent solver for the semi-structured sparsity regularizer. This method is 10x faster than gradient descent-based solvers and 100x faster than Interior Point Method (IPM) solvers, enabling end-to-end regularized learning at LLMs scale efficiently.


\vspace{-0.5em}

\item Across all tested models, ProxSparse consistently improved perplexity (PPL) and accuracy on 7 common-sense reasoning tasks. It outperforms the previous SOTA pruning baselines at the same scale by up to 35\% in PPL and 20\% in zero-shot tasks, highlighting its effectiveness.




\end{itemize}