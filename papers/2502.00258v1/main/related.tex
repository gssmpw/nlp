\vspace{-1.5em}
\section{Preliminaries and Problem Setup}
\vspace{-0.5em}
\subsection{Large Language Model pruning}

The massive size of LLMs has drawn attention to model compression to reduce serving overhead. Network pruning effectively removes redundant parameters, improving efficiency. In LLMs, pruning has proven effective~\cite{bai2024sparsellmglobalpruningpretrained, frantar2023sparsegpt, huang2024pruning}, and can be categorized into three classes based on granularity.


Structured pruning~\cite{ma2023llm, xia2023sheared} removes entire substructures like neurons or attention heads, reducing computation without extra overhead. However, its rigidity and lack of flexibility often lead to significant performance loss, requiring additional retraining to recover accuracy~\cite{ma2023llm, xia2023sheared}.
Unstructured pruning~\cite{frankle2018lottery} effectively preserves model accuracy by selectively removing unimportant weights in a fine-grained, non-uniform manner. However, its irregular pruning pattern is hardware-unfriendly, causing inefficient memory access.
Semi-structured (block-wise N:M) sparsity~\cite{mishra2021accelerating} balances accuracy and efficiency by retaining N non-zero elements per M-sized block. Such patterns can be effectively leveraged by commercial hardwares for real speedup~\cite{fang2024maskllm, sun2023simple, mishra2021accelerating}, while maintaining flexibility to remove unimportant weights. This work focuses semi-structured pruning for LLMs, introducing an end-to-end regularized learning framework towards optimal mask selection.


\subsection{Semi-Structured masks Selection for LLMs}

Previous research has explored various mask-finding techniques for LLMs, with many showing success in semi-structured pruning. Here, we review the most advanced methods for semi-structured mask selection.

Magnitude pruning~\cite{han2015deep} is a standard technique that removes individual weights based on their magnitudes with certain thresholds. Wanda~\cite{sun2023simple} also avoids retraining or updating weights and introduces activation-aware pruning. The importance of each weight is evaluated using a per-output pruning criterion, where the weight magnitude is multiplied by its corresponding input activation using calibration data. SparseGPT~\cite{frantar2023sparsegpt} leverages the Hessian matrix to calculate the weight importance and reconstruction errors with the calibration data. These pruning methods typically solve a local optimization problems, providing efficient and low-resource compression techniques~\cite{ma2023llm, frantar2023sparsegpt, frantar2022gptq, sun2023simple}.


On the other hand, learning-based solutions for pruning have been explored in previous works, particularly in vision tasks. The main challenge is the non-differentiable nature of mask selection, and techniques like Straight-Through Estimators (STE)~\cite{bengio2013estimating} have been proposed to overcome this. However, these methods typically require large-scale retraining, which is difficult for LLMs due to their enormous size. In our work, we propose to use the mask selection regularizer and efficiently identify the optimal mask in a learned manner with only hundreds of calibration samples without extensive retraining. A recently proposed learning-based method, MaskLLM~\cite{fang2024maskllm}, introduces a large-scale learning-based approach ($\sim$100,000 samples) to learn pruning masks using Gumbel Softmax sampling. Our approach employs a different design and operates with $\sim$1000x smaller sample size ($\sim$100 samples). We consider MaskLLM complementary to our approach, as it focuses on the regime that learns with large-scale data samples. We provide more comparison and discussion in Sec.~\ref{sec:calib}.



\vspace{-0.5em}

\subsection{Problem setup}
Let $W_0 \in\R^d$ be the pre-trained weights of the model and $\Ls(W)$ be the (population) loss function for the model with weight $W$. We say a $W\in \R^d$ is $2:4$-sparse if for every block of 4 parameters in $W$
only 2 are non-zero.
 
Our goal is to solve the pruning problem by finding an appropriate semi-structured sparse masks while keeping the weights of the pretrained model frozen. 

We may express our task using the following stochastic optimization problem:
\begin{equation}\label{eq:orig_problem}
\begin{aligned}
\min_{M} &\quad \mathcal{L}(W_0 \odot M), \\
\text{s.t.} &\quad M \in \{0, 1\}^{d},  M \text{ is 2:4 sparse}, %\label{eq_mask}
\end{aligned}
\end{equation}
where $\Ls$ denotes the loss, mask $M\in\{0,1\}^d$ denotes a Boolean-valued with the same shape as the frozen model weights $W_{0}$, and $\odot$ denotes element-wise multiplication.
The problem is hard to solve because $\Ls$ is non-convex and the constraints are combinatorial. Moreover, we do not have access to $\Ls$ directly (since it's the \emph{expected} loss). Instead, we have a small calibration dataset that we can stream through that gives us \emph{stochastic} first-order (gradient) access, if we assume they are new data points drawn from the test-data distribution. 

Given these constraints, our goal is not to \emph{solve} \eqref{eq:orig_problem}, but rather to find efficient heuristics that work in practice. In Section~\ref{sec:method}, we propose our approach and highlight the interesting aspect of it. In Section~\ref{sec:evaluation}, we thoroughly evaluate our method in semi-structured sparse pruning in a number of open-source LLM models.
%

