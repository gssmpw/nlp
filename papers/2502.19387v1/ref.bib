% Self-Supervised Speech Embeddings & Paralinguistics

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{mohamed2022self,
  title={Self-supervised speech representation learning: A review},
  author={Mohamed, Abdelrahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob D and Edin, Joakim and Igel, Christian and Kirchhoff, Katrin and Li, Shang-Wen and Livescu, Karen and Maal{\o}e, Lars and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1179--1210},
  year={2022},
  publisher={IEEE}
}

% Speech-Text Alignment & Disentanglement

@article{tjandra2020unsupervised,
  title={Unsupervised learning of disentangled speech content and style representation},
  author={Tjandra, Andros and Pang, Ruoming and Zhang, Yu and Karita, Shigeki},
  journal={arXiv preprint arXiv:2010.12973},
  year={2020}
}

@inproceedings{morais2022speech,
  title={Speech emotion recognition using self-supervised features},
  author={Morais, Edmilson and Hoory, Ron and Zhu, Weizhong and Gat, Itai and Damasceno, Matheus and Aronowitz, Hagai},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6922--6926},
  year={2022},
  organization={IEEE}
}

@article{ma2023emotion2vec,
  title={emotion2vec: Self-supervised pre-training for speech emotion representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2023}
}




@article{Shah2021,
   author = {Jui Shah and Yaman Kumar Singla and Changyou Chen and Rajiv Ratn Shah},
   isbn = {2101.00387v2},
   keywords = {interpretability,pre-trained acoustic representations,probing},
   month = {1},
   title = {What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure},
   url = {https://arxiv.org/abs/2101.00387v2},
   year = {2021},
}
{the authors investigate the internal representations learned by audio transformer models, specifically Mockingjay and wav2vec 2.0. Their goal is to understand what these models capture regarding various aspects of speech and language.


Methodology:

The authors employed a probing approach, using a three-layer feed-forward neural network to predict specific features from the embeddings produced by the transformer models. This method helps determine the extent to which each feature is encoded within the model's layers.
Datasets Utilized:

Native Read Speech: LibriSpeech dataset.
Native Spontaneous Speech: Mozilla Common Voice dataset.
Non-Native Read Speech: L2-Arctic dataset.
Findings:

Main findings
- The paper introduces a comprehensive set of probing tasks to analyze the linguistic features captured by two audio transformer models (wav2vec2.0 and Mockingjay) across different speech types.
- The performance of the audio transformer models is more dependent on the type of speech (native read, native spontaneous, non-native) than the type of speakers.
- The audio transformer models are able to capture textual features surprisingly well, despite not being trained on text-specific tasks, and are comparable to BERT on several parameters.
}

@article{Sun2019,
   author = {Zhongkai Sun and Prathusha K. Sarma and William A. Sethares and Yingyu Liang},
   doi = {10.1609/aaai.v34i05.6431},
   isbn = {9781577358350},
   issn = {2159-5399},
   journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
   month = {11},
   pages = {8992-8999},
   publisher = {AAAI press},
   title = {Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis},
   url = {https://arxiv.org/abs/1911.05544v2},
   year = {2019},
}
{
Feature Extraction:

Text Modality: The model utilizes pre-trained language models to extract textual features, leveraging advanced embeddings trained on extensive datasets.
Audio and Video Modalities: Features from audio and video are extracted using methods that, while effective, are noted to be less advanced compared to textual feature extraction techniques.
Interaction Representation:

To capture the interplay between modalities, the model computes the outer product between text features and audio features, as well as between text features and video features. This results in interaction representations termed "text-based audio" and "text-based video."
Deep Canonical Correlation Analysis (DCCA):

ICCN employs DCCA to learn correlated representations across the three modalities. DCCA is a deep learning extension of traditional Canonical Correlation Analysis, enabling the model to capture complex, non-linear relationships between the different types of data.
The study highlights that textual features often outperform audio and video features in tasks like sentiment analysis and emotion recognition. This disparity is attributed to the advanced nature of textual embeddings derived from large-scale language models, in contrast to the comparatively underdeveloped audio and video features. By learning hidden correlations between the modalities through the proposed interaction representations and DCCA, ICCN aims to create more robust multimodal embeddings that leverage the strengths of each data type.

Empirical evaluations on benchmark datasets demonstrate that ICCN effectively captures useful information from text, audio, and video, leading to improved performance in multimodal language analysis tasks 
"GAP"}

@article{Dogan2022,
   author = {Duygu Dogan and Huang Xie and Toni Heittola and Tuomas Virtanen},
   keywords = {Index Terms-zero-shot learning,audio classification,image embeddings,semantic embeddings},
   month = {6},
   title = {Zero-Shot Audio Classification using Image Embeddings},
   url = {https://arxiv.org/abs/2206.04984v1},
   year = {2022},
}
{The paper explores using image embeddings as side information for zero-shot audio classification and evaluates its performance compared to using textual and acoustic embeddings.
- Image embeddings can be used as semantic information to perform zero-shot audio classification.
- Image and textual embeddings perform similarly well in zero-shot audio classification, both individually and when used together.
- Zero-shot audio classification performance is highly dependent on the semantic similarity between training and test classes, and textual/image embeddings can match semantic acoustic embeddings when the classes are semantically similar.}
@article{Chung2017,
   author = {Yu-An Chung and James Glass},
   month = {11},
   title = {Learning Word Embeddings from Speech},
   url = {https://arxiv.org/abs/1711.01515v1},
   year = {2017},
}

{The authors propose a novel deep neural network architecture called Sequence-to-Sequence Audio2Vec for unsupervised learning of fixed-length vector representations of audio segments from a speech corpus, where the vectors contain semantic information and are close to other vectors if their corresponding segments are semantically similar.
- The Sequence-to-Sequence Audio2Vec model learned semantic vector representations of audio segments that performed competitively with GloVe word embeddings on standard word similarity tasks.
- The proposed model can extract semantic information directly from raw speech data, without relying on other modalities like text or images which are challenging and expensive to collect and annotate.
}
@article{Chung2018,
   author = {Yu An Chung and James Glass},
   doi = {10.21437/Interspeech.2018-2341},
   issn = {19909772},
   journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
   keywords = {Bag-of-words,Recurrent neural networks,Sequence-to-sequence learning,Skipgrams,Word embeddings},
   month = {3},
   pages = {811-815},
   publisher = {International Speech Communication Association},
   title = {Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech},
   volume = {2018-September},
   url = {https://arxiv.org/abs/1803.08976v2},
   year = {2018},
}
{
The authors propose a novel deep neural network architecture called Speech2Vec that learns fixed-length vector representations of audio segments from a speech corpus, where the vectors contain semantic information about the underlying spoken words and are close to other vectors if their corresponding words are semantically similar, allowing Speech2Vec to leverage the semantic information in speech that is not present in plain text.
- Speech2Vec is a novel deep neural network architecture that learns fixed-length vector representations of audio segments from speech, capturing semantic information not present in text.
- The word embeddings learned by Speech2Vec outperform those learned by Word2Vec from text transcriptions on 13 word similarity benchmarks.}

@misc{Elizalde2019,
   author = {Benjamin Elizalde and Shuayb Zarar and Bhiksha Raj},
   keywords = {Audio Search Engine,Content-Based Audio Retrieval,Cross Modal Retrieval,Index Terms-Joint Audio-Text Embedding,Query by Example,Siamese Neural Network},
   title = {CROSS MODAL AUDIO SEARCH AND RETRIEVAL WITH JOINT EMBEDDINGS BASED ON TEXT AND AUDIO},
   year = {2019},
}

{The paper proposes a framework that learns joint embeddings from a shared lexico-acoustic space, where vectors from either text or audio modality can be mapped together and compared directly, in order to improve semantic knowledge and enable the use of either text or audio queries to search and retrieve audio.
- The proposed framework improves semantic knowledge and enables the use of either text or audio queries to search and retrieve audio.
- The results of this study break new ground for a cross-modal audio search engine and open up further exploration of lexico-acoustic spaces.}

@article{Kalinowski2020,
   author = {Alexander Kalinowski and Yuan An},
   keywords = {Alignment,Alignment ·,Cross-,Entity,Graph ·,Knowledge,Language,Lingual,Models ·},
   title = {A SURVEY OF EMBEDDING SPACE ALIGNMENT METHODS FOR LANGUAGE AND KNOWLEDGE GRAPHS},
   year = {2020},
}
{ linear mapping methods are discussed as a fundamental approach to aligning different embedding spaces. These methods aim to learn a transformation that projects one embedding space onto another, facilitating the integration of data from diverse sources.

Linear Mapping Methods:

Linear mapping methods operate under the assumption that there exists a linear transformation that can align one embedding space with another. The goal is to find a transformation matrix W that, when applied to the embeddings from the source space, maps them as closely as possible to the corresponding embeddings in the target space.}
@article{Labbe2023,
   author = {Etienne Labbé and Julien Pinquier and Thomas Pellegrini},
   isbn = {2305.01482v1},
   keywords = {Index Terms-sound event description,audio language task,multitask learning,overfitting,semantic loss,sentence embedding regression loss},
   title = {Multitask learning in Audio Captioning: a sentence embedding regression loss acts as a regularizer},
   url = {https://www.sbert.net/docs/pretrained},
   year = {2023},
}
{Methodology:

Model Architecture:

Encoder: Utilized a pre-trained CNN14 model from the PANNs framework to extract audio features.
Decoder: Implemented a Transformer-based decoder to generate textual descriptions from the audio features.
Loss Functions:

Cross-Entropy Loss (CE): Used to train the model to predict the next word in a sequence based on the previous words.
Sentence Embedding Regression Loss (SER): Employed to minimize the distance between the embeddings of the generated caption and the reference caption. The Sentence-BERT (SBERT) model was used to obtain these embeddings.
Training Strategy:

The model was trained using a combination of CE and SER losses, with the SER loss acting as a regularizer to promote the generation of semantically meaningful captions.
Findings:

Performance Improvement: Incorporating the SER loss led to a reduction in overfitting and improved the SPIDEr score from 0.397 to 0.418 on the AudioCaps dataset.
Effect of Weight Decay: Adjusting the weight decay parameter further enhanced performance, achieving a SPIDEr score of up to 0.444, approaching the state-of-the-art score of 0.475.
Model Efficiency: The proposed model achieved competitive performance while utilizing eight times fewer trainable parameters compared to existing state-of-the-art methods.
}

@article{Weck2022,
   author = {Benno Weck and Miguel Pérez Fernández and Holger Kirchhoff and Xavier Serra},
   month = {10},
   title = {Matching Text and Audio Embeddings: Exploring Transfer-learning Strategies for Language-based Audio Retrieval},
   url = {https://arxiv.org/abs/2210.02833v1},
   year = {2022},
}

{Weck et al. (2022) analyzed large-scale pretrained models for cross-modal (text-to-audio) retrieval. They employed embeddings from these models in a metric learning framework to connect matching pairs of audio and text. While their focus is on retrieval, the underlying principle of mapping audio and text embeddings to a shared space resonates with your regression-based method.} 



@article{Stan2023,
   author = {Adriana Stan},
   doi = {10.3390/math10213927},
   issue = {21},
   journal = {Mathematics},
   keywords = {artificial intelligence,deep embeddings,deep learning,deep representations,neural architectures,residual information,speaker disentanglement,speaker embeddings,speaker recognition,x-vectors},
   month = {2},
   publisher = {MDPI},
   title = {Residual Information in Deep Speaker Embedding Architectures},
   volume = {10},
   url = {http://arxiv.org/abs/2302.02742 http://dx.doi.org/10.3390/math10213927},
   year = {2023},
}


{investigates the extent to which speaker embeddings—vector representations designed to capture speaker identity—contain residual information unrelated to speaker identity, such as linguistic content, recording conditions, and speaking style. The study analyzes six sets of speaker embeddings derived from recent high-performing deep neural network (DNN) architectures. To evaluate these embeddings, a large multi-speaker parallel speech dataset is utilized, comprising 46 speakers delivering the same set of prompts, recorded in both professional studios and home environments. The analysis includes intra- and inter-speaker similarity measures and examines whether simple classification and regression methods can extract residual information from the embeddings. The findings reveal that, despite the high discriminative power of the analyzed embeddings, residual information remains present, showing a high correlation with recording conditions, linguistic content, and utterance duration. }

@article{Huzaifah2022,
   author = {Muhammad Huzaifah and Ivan Kukanov},
   doi = {10.1109/SLT54892.2023.10023147},
   isbn = {9798350396904},
   journal = {2022 IEEE Spoken Language Technology Workshop, SLT 2022 - Proceedings},
   keywords = {cross-modal adaptation,probing classifiers,semantic alignment,sentence embeddings},
   month = {4},
   pages = {747-754},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {An Analysis of Semantically-Aligned Speech-Text Embeddings},
   url = {https://arxiv.org/abs/2204.01235v2},
   year = {2022},
}
{The paper analyzes the properties of a joint speech-text embedding space constructed using a teacher-student model, and finds that incorporating automatic speech recognition through pretraining and multitask scenarios significantly improves the semantic alignment and coupling of the speech-text embeddings.
- Incorporating automatic speech recognition in pretraining and multitask learning significantly improved the semantic alignment between speech and text embeddings.
- The paper analyzed the cross-modal embeddings using retrieval accuracy, zero-shot classification, and encoder probing to assess semantic alignment, generalizability, and knowledge transfer.}



@article{Hsu2021,
   abstract = {Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.},
   author = {Wei Ning Hsu and Benjamin Bolte and Yao Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},
   doi = {10.1109/TASLP.2021.3122291},
   issn = {23299304},
   journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
   keywords = {BERT,Self-supervised learning},
   month = {6},
   pages = {3451-3460},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
   volume = {29},
   url = {https://arxiv.org/abs/2106.07447v1},
   year = {2021},
}

@article{chen2022wavlm,
  title={{WavLM}: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022},
  publisher={IEEE}
}


@article{Jafarzadeh2024,
   author = {Pourya Jafarzadeh and Amir Mohammad Rostami and Padideh Choobdar},
   keywords = {HuBERT,Index Terms: Speaker Emotion Recognition,Self-supervised learning,Transformer Models,Wav2Vec2},
   month = {11},
   title = {Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT},
   url = {http://arxiv.org/abs/2411.02964},
   year = {2024},
}
{Speaker Emotion Recognition (SER)

The goal is to detect emotions (e.g., happy, sad, angry, neutral) from speech audio.
This typically involves feature extraction, classification, and deep learning models.
Self-Supervised Learning (SSL) Models

Refers to models like Wav2Vec 2.0 and HuBERT, which are trained without labeled data, learning useful speech representations through unsupervised pretraining.
They learn from large-scale raw audio data before being fine-tuned for specific downstream tasks like emotion recognition.
Feature Extraction

The study likely does not train a deep learning model from scratch but rather uses Wav2Vec2 and HuBERT as feature extractors.
This means that embeddings or learned representations from these models are used as inputs to another classifier, such as a support vector machine (SVM), random forest, or fine-tuned neural network.
Speaker Emotion Recognition (SER)

The goal is to detect emotions (e.g., happy, sad, angry, neutral) from speech audio.
This typically involves feature extraction, classification, and deep learning models.
Self-Supervised Learning (SSL) Models

Refers to models like Wav2Vec 2.0 and HuBERT, which are trained without labeled data, learning useful speech representations through unsupervised pretraining.
They learn from large-scale raw audio data before being fine-tuned for specific downstream tasks like emotion recognition.
Feature Extraction

The study likely does not train a deep learning model from scratch but rather uses Wav2Vec2 and HuBERT as feature extractors.
This means that embeddings or learned representations from these models are used as inputs to another classifier, such as a support vector machine (SVM), random forest, or fine-tuned neural network.}

@inproceedings{tu2024contrastive,
  title={Contrastive speaker embedding with sequential disentanglement},
  author={Tu, Youzhi and Mak, Man-Wai and Chien, Jen-Tzung},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={10891--10895},
  year={2024},
  organization={IEEE}
}

@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@inproceedings{yoon2018multimodal,
  title={Multimodal speech emotion recognition using audio and text},
  author={Yoon, Seunghyun and Byun, Seokhyun and Jung, Kyomin},
  booktitle={2018 IEEE spoken language technology workshop (SLT)},
  pages={112--118},
  year={2018},
  organization={IEEE}
}

@article{song2020mpnet,
  title={Mpnet: Masked and permuted pre-training for language understanding},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={16857--16867},
  year={2020}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@misc{openai2022textembed,
  title={OpenAI API - text-embedding-ada-002},
  author={OpenAI},
  year={2022},
  url={https://platform.openai.com/docs/guides/embeddings}
}
