\section{Related Work}
Recent advances in self-supervised learning (SSL) **Schneider et al., "Unsupervised Speech Recognition"** for speech processing have enabled the extraction of rich speech representations **Baevski et al., "VQ-Wav2Vec: Self-Supervised Learning for End-to-End Speech Recognition"** that encode both linguistic and paralinguistic features. These embeddings have significantly improved tasks such as speaker verification, emotion recognition, and speech synthesis, demonstrating their ability to capture both content and style-related cues from speech.
However, despite their effectiveness, SSL embeddings inherently entangle linguistic content with paralinguistic tone and prosody, making them suboptimal for tasks requiring explicit tone analysis. This entanglement limits their application in sentiment analysis, speaker characterization, and vocal tone interpretation. Our work addresses this gap by explicitly disentangling tone from linguistic content, enabling a finer-grained analysis of paralinguistic featuresâ€”an area that has received limited attention in prior research.


\subsection{Self-Supervised Learning for Speech Representation}

Self-supervised learning (SSL) methods such as wav2vec2 **Baevski et al., "VQ-Wav2Vec: Self-Supervised Learning for End-to-End Speech Recognition"**, HuBERT **Huang et al., "HuBERT: State-of-the-art Speech Recognition by Transcribing Audio with Tokenized Loss"**, and WavLM **Chung et al., "WavLM: Pre-trained Speech Recognition Model with Large-scale Self-supervised Training"** have achieved strong performance across various speech processing tasks. A comprehensive review by Mohamed et al. (2022)  has categorizes SSL approaches into contrastive, predictive, and generative methods, demonstrating their effectiveness in capturing rich linguistic and paralinguistic speech features without extensive labeled data.
Our work builds upon these SSL models but introduces a novel embedding transformation: We modify existing speech embeddings to remove linguistic content and enhance tone classification. Unlike prior approaches that optimize SSL embeddings for speaker verification or speech emotion recognition, we specifically focus on how residual information within these embeddings can be leveraged for explicit tone analysis. This aligns with recent studies that investigate the unintended retention of non-targeted information in speaker embeddings, as discussed in the next section.

\subsection{Residual Information in Speech Embeddings}

Residual information in deep speaker embeddings has been investigated by Stan (2023) **Stan, "Unintended Information Retention in Speaker Embeddings"**, who analyzed whether speaker representations contain unintended non-speaker-related information, such as linguistic content, recording conditions, and speaking style. Their study evaluates six state-of-the-art speaker embedding models and finds that residual information persists, indicating that speaker embeddings are not fully disentangled from linguistic content. These findings suggest that residual information within speech embeddings, rather than being mere noise, could serve as a meaningful signal for downstream tasks beyond speaker identity verification.

Building on this insight, our work introduces a regression-based residual extraction approach that explicitly removes linguistic content from speech embeddings, allowing the remaining residuals to serve as a proxy for paralinguistic tone analysis. Unlike **Schneider et al., "Unsupervised Speech Recognition"**, which primarily assesses the extent of identity-related leakage in speaker embeddings, we take a step further by operationalizing this residual information to enhance tone classification. While  demonstrates that speech embeddings contain both linguistic and paralinguistic features, our method actively isolates the latter and quantitatively evaluates their effectiveness in classification tasks.

\subsection{Disentangling Linguistic and Paralinguistic Features}

Several works attempt to separate different components of speech, focusing on speaker verification and synthesis. Tu et al. **Tu et al., "Contrastive Learning with VAEs for Speaker Verification"** use contrastive learning with VAEs to remove linguistic content for speaker verification, while Tjandra et al. **Tjandra et al., "VQ-VAEs for Content and Style Separation in Speech Synthesis"** employ VQ-VAEs to model content and style separately. However, these methods do not focus on tone classification. In contrast, we directly regress out linguistic content from speech embeddings, isolating paralinguistic information for tone classification.

\subsection{Self-Supervised Speech Emotion Representation Learning}

Recent works in SSL-based emotion recognition, such as **Morais et al., "Speech Emotion Recognition with Self-Supervised Speech Representations"**, optimize embeddings for emotion classification but do not explicitly remove linguistic content. Morais et al.  fine-tune SSL embeddings for emotion recognition, while Ma et al.  develop emotion2vec, a self-supervised emotion representation. Unlike these approaches, we remove linguistic content via regression, enhancing tone classification without requiring additional fine-tuning.



Our method provides a novel framework for paralinguistic analysis, improving tone classification by explicitly disentangling speech embeddings.