\section{Related Work}
Recent advances in self-supervised learning (SSL) \cite{mohamed2022self} for speech processing have enabled the extraction of rich speech representations \cite{baevski2020wav2vec,hsu2021hubert,chen2022wavlm} that encode both linguistic and paralinguistic features. These embeddings have significantly improved tasks such as speaker verification, emotion recognition, and speech synthesis, demonstrating their ability to capture both content and style-related cues from speech.
However, despite their effectiveness, SSL embeddings inherently entangle linguistic content with paralinguistic tone and prosody, making them suboptimal for tasks requiring explicit tone analysis. This entanglement limits their application in sentiment analysis, speaker characterization, and vocal tone interpretation. Our work addresses this gap by explicitly disentangling tone from linguistic content, enabling a finer-grained analysis of paralinguistic featuresâ€”an area that has received limited attention in prior research.


\subsection{Self-Supervised Learning for Speech Representation}

Self-supervised learning (SSL) methods such as wav2vec2 \cite{baevski2020wav2vec}, HuBERT \cite{hsu2021hubert}, and WavLM \cite{chen2022wavlm} have achieved strong performance across various speech processing tasks. A comprehensive review by Mohamed et al. (2022) \cite{mohamed2022self} categorizes SSL approaches into contrastive, predictive, and generative methods, demonstrating their effectiveness in capturing rich linguistic and paralinguistic speech features without extensive labeled data.
Our work builds upon these SSL models but introduces a novel embedding transformation: We modify existing speech embeddings to remove linguistic content and enhance tone classification. Unlike prior approaches that optimize SSL embeddings for speaker verification or speech emotion recognition, we specifically focus on how residual information within these embeddings can be leveraged for explicit tone analysis. This aligns with recent studies that investigate the unintended retention of non-targeted information in speaker embeddings, as discussed in the next section.

\subsection{Residual Information in Speech Embeddings}

Residual information in deep speaker embeddings has been investigated by Stan (2023) \cite{Stan2023}, who analyzed whether speaker representations contain unintended non-speaker-related information, such as linguistic content, recording conditions, and speaking style. Their study evaluates six state-of-the-art speaker embedding models and finds that residual information persists, indicating that speaker embeddings are not fully disentangled from linguistic content. These findings suggest that residual information within speech embeddings, rather than being mere noise, could serve as a meaningful signal for downstream tasks beyond speaker identity verification.

Building on this insight, our work introduces a regression-based residual extraction approach that explicitly removes linguistic content from speech embeddings, allowing the remaining residuals to serve as a proxy for paralinguistic tone analysis. Unlike \cite{Stan2023}, which primarily assesses the extent of identity-related leakage in speaker embeddings, we take a step further by operationalizing this residual information to enhance tone classification. While \cite{Stan2023} demonstrates that speech embeddings contain both linguistic and paralinguistic features, our method actively isolates the latter and quantitatively evaluates their effectiveness in classification tasks.

\subsection{Disentangling Linguistic and Paralinguistic Features}

Several works attempt to separate different components of speech, focusing on speaker verification and synthesis. Tu et al. \cite{tu2024contrastive} use contrastive learning with VAEs to remove linguistic content for speaker verification, while Tjandra et al. \cite{tjandra2020unsupervised} employ VQ-VAEs to model content and style separately. However, these methods do not focus on tone classification. In contrast, we directly regress out linguistic content from speech embeddings, isolating paralinguistic information for tone classification.

\subsection{Self-Supervised Speech Emotion Representation Learning}

Recent works in SSL-based emotion recognition, such as \cite{morais2022speech,ma2023emotion2vec}, optimize embeddings for emotion classification but do not explicitly remove linguistic content. Morais et al. \cite{morais2022speech} fine-tune SSL embeddings for emotion recognition, while Ma et al. \cite{ma2023emotion2vec} develop emotion2vec, a self-supervised emotion representation. Unlike these approaches, we remove linguistic content via regression, enhancing tone classification without requiring additional fine-tuning.



Our method provides a novel framework for paralinguistic analysis, improving tone classification by explicitly disentangling speech embeddings.