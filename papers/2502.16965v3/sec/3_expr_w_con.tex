

\begin{table}
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Model & Parameters & Layers & Hidden Size & Heads \\
    \midrule
    Ours-B & 111M & 12 & 768 & 12 \\
    Ours-L & 343M & 24 & 1024 & 16  \\
    Ours-XL & 775M & 36 & 1280 & 20  \\
    Ours-XXL & 1.4B & 48 & 1536 & 24 \\
    \bottomrule
  \end{tabular}
  \caption{Model sizes and architecture configuration of our method.}
  \label{tab:config}
\end{table}

\begin{table*}[!t]
\centering
\begin{tabular}{>{\centering\arraybackslash}p{4.2cm} >{\centering\arraybackslash}p{4.2cm} >{\centering\arraybackslash}p{1.5cm} >{\centering\arraybackslash}p{1.5cm} >{\centering\arraybackslash}p{1.5cm} >{\centering\arraybackslash}p{1.5cm} }
\toprule
\multicolumn{1}{c}{Training} & Inference & FID$\downarrow$ & IS$\uparrow$ & Precision$\uparrow$ & Recall$\uparrow$ \\ \hline
\multicolumn{2}{c}{Non VF prompt}  & 5.46 & 193.61 & 0.83 & 0.45 \\ \hline
\multirow{2}{*}{Class VF prompt}       & Class VF prompt   &\textbf{4.34} &226.31 &0.84 &0.46 \\ \cline{2-6} 
& Universal VF prompt &  4.36 &227.44 &0.84 &0.46 \\ \hline
\multirow{2}{*}{Universal VF prompt}   & Class VF prompt     & 4.42&217.66&0.85  &0.45  \\ \cline{2-6} 
 & Universal VF prompt &4.39 &222.80 &0.85&0.46 \\ \hline
 \multicolumn{2}{c}{Mixture}&4.55&235.13&0.86&0.44 \\  \bottomrule
\end{tabular}
\caption{Ablation study for different types of VF prompts used for 
 training or inference. `$\downarrow$' or `$\uparrow$' indicate lower or higher values are better. `Mixture' means we combine two different prompts for training and inference,}
\label{tab:Robustness}
\end{table*}
\section{Experiment}
In this section, we describe the implementation details of our approach in Sec.~\ref{sec:implementation}. We then provide ablation studies on important design decisions in Sec.~\ref{sec:Ablation}. Following that, the main results and the visualizations are discussed in Sec.~\ref{sec:MainResults}.
\subsection{Experiment Setting}
\label{sec:implementation}
\paragraph{Model Architecture} Following prior work which uses a VQ tokenizer to tokenize the input images into discrete tokens, we use the VQGAN reported in LlamaGen~\cite{sun2024autoregressive} with the official weight trained on ImageNet. Our model architecture is largely based on Llama~\cite{touvron2023llama}, applying pre-normalization using RMSNorm~\cite{zhang2019rootmeansquarelayer}, SwiGLU~\cite{shazeer2020gluvariantsimprovetransformer} activation function, and 2D rotary positional embeddings~\cite{su2023roformerenhancedtransformerrotary} at each layer of our model. We use models of different configurations including B (111M), L (343M), XL (775M), and XXL (1.4B). The detailed architecture configuration and size are shown in Table~\ref{tab:config}. 

\paragraph{Training Setup} We experiment on ImageNet \cite{ILSVRC15} at a resolution of 256$\times$256 based on the class to image task. To accelerate training, we pre-tokenize the entire training dataset before training using the VQGAN tokenizer reported in LlamaGen~\cite{sun2024autoregressive}. Additionally, we improve the diversity of the pre-tokenized dataset by applying the ten-crop transformation~\cite{sun2024autoregressive}.
Following the classic evaluation suite provided by \cite{dhariwal2021diffusionmodelsbeatgans}, we evaluate FID \cite{heusel2018ganstrainedtimescaleupdate} as the main metric and also report IS \cite{salimans2016improvedtechniquestraininggans}, Precision, and Recall as references metrics.
% We follow the classic evaluation suite provided by \cite{dhariwal2021diffusionmodelsbeatgans}. 
% We utilize the tokenizer provided by \textit{Sun et al.}~\cite{sun2024autoregressive}, and precompute the image codes before training to accelerate model training as Sun et al~\cite{sun2024autoregressive}. 
% The detailed training settings of each model exhibit in the supplementary materials. 
% Models in the Table~\ref{tab:main_results} are trained for 300 epochs.
% , while all other models used in other experiments are trained for 50 epochs. 
Unless otherwise specified, models are trained for 300 epochs and the default inference setting is top-k=0, top-p=1.0, temperature=1.0, number of VF prompt=256. Additionally, the dropout for the class condition embedding in Classifier-Free Guidance (CFG) is set to 1.75 by default. All models are trained using the base learning rate of 0.0001 and the batch size of 256. AdamW optimizer is set with $\beta_{1}=0.9$ and $\beta_{2}=0.95$, weight decay is set to 0.05, and gradient clipping is set at 1.0. A dropout rate of 0.1 is applied to the input token embeddings, attention modules, and the FFN module. 
% Besides, we conduct all experiments on Ascend 910B3 NPUs.

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{fig/fignumber.png}

   \caption{Ablation study for different numbers of prompt tokens.}
   \label{fig:number}
\end{figure}


\subsection{Ablation Study}
This section mainly includes the properties of our method, as well as the choice of parameters.
\label{sec:Ablation}
\subsubsection{Robustness of VF Prompt}
To evaluate the robustness of the proposed VF prompt, we conduct experiments using different types of VF prompts. Specifically, we use two types of prompts mentioned in Sec.~\ref{Prompt}: class VF prompt and universal VF prompt. We evaluate the effectiveness and robustness of VF prompt by comparing the results with consistent VF prompts (\textit{i.e.}, the same VF prompt used during both training and inference) and inconsistent VF prompts (\textit{i.e.}, different VF prompts used during training and inference). 
In addition, we also combine two different prompts for training and inference, with each having a 50\% probability of being selected. We train the model with 111M parameters for 300 epochs, which is the same setting as those in LlamaGen~\cite{sun2024autoregressive}. The baseline is LlamaGen~\cite{sun2024autoregressive}, an AR image generation model based on the Llama architecture without our VF prompt.


\begin{table*}[t]
\centering
\begin{tabular}{>{\centering\arraybackslash}p{4cm}  >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm} >{\centering\arraybackslash}p{2cm}}
\toprule
Inference & FID$\downarrow$ & IS$\uparrow$ & Precision$\uparrow$ & Recall$\uparrow$ \\ \hline
Non VF prompt&8.69 &124.44&0.79 &0.47\\
Blank Prompt&8.65&123.15&0.80&0.43\\
Full-view After Generation&8.70&122.79&0.79&0.47 \\
VF prompt&\textbf{6.07}&174.48&0.82&0.44 \\ 
\bottomrule
\end{tabular}
\caption{Ablation study for different variations of prompting. `$\downarrow$' or `$\uparrow$' indicate lower or higher values are better.}
\label{tab:Ablation}
\end{table*}


\begin{table}
  \centering
  \begin{tabularx}{\linewidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
    % \begin{tabular}{>{\centering\arraybackslash}p{1cm}  >{\centering\arraybackslash}p{1.5cm} >{\centering\arraybackslash}p{1.5cm} >{\centering\arraybackslash}p{1.5cm} >{\centering\arraybackslash}p{1.5cm}}
    \toprule
CFG & FID$\downarrow$ & IS$\uparrow$ & Precision$\uparrow$ & Recall$\uparrow$  \\
    \midrule
    1.25&7.14&106.97
    &0.71
    &0.58\\
    1.50 &5.47&167.61&0.80&0.52   \\
    1.75 &\textbf{4.39} &222.80 &0.85&0.46 \\
    2.00 &5.24&266.85&0.88&0.40 \\
    2.25 &6.83&304.32&0.91&0.36 \\
    2.50&8.49&329.75&0.92&0.31 \\
    \bottomrule
  \end{tabularx}
  \caption{Ablation study for different CFG settings.}
  \label{tab:cfg}
\end{table}

The results are shown in Table~\ref{tab:Robustness}, with the following observations:
% \begin{enumerate}
(1) Regardless of whether we use consistent or inconsistent VF prompts for training and inference, and regardless of which VF prompt we choose, our method consistently outperforms the baseline without VF prompt, achieving an approximate 20\% improvement in FID scores. This demonstrates that our method exhibits good robustness. 
% This finding aligns with the robustness of CoT in NLP, where the VF prompt can vary in content but still leads to performance improvements  as long as it is rule-compliant and reasonable~\cite{wei2022chain}.
% 因此无论这个中间推理过程是text-related还是image-related，Cot都具有较强的鲁棒性通过enhances contextual reasoning of generation, while also improves stability during the generation process by increasing the inference step. 
%按照这个推测来说应该做条件生成的时候Class VF prompt比Universal更好，但是做无条件（cfg<=1.0）的时候应Universal更好？
(2) The performance of the class VF prompt is better than that of the universal VF prompt. This is because focusing on a specific class is more informative than considering the overall distribution of the entire image set, as it more accurately reflects the representation of the specific class. However, the universal VF prompt is more generalizable as it is independent of the condition.
(3) When the VF prompt is consistent between training and inference, the performance is better than when they are inconsistent. This is expected, as consistency between training and inference ensures that the model can better align the reasoning process across both phases, leading to more coherent and stable results.
(4) The inferior performance of the mixed prompt is due to the introduction of information conflict and inconsistency. When using two different prompts, the model needs to balance both, which could lead to confusion in the generated content. Compared to a single prompt, the mixed prompt increases the complexity of reasoning, and the model may not have sufficient capacity to effectively integrate the information from both prompts, thus affecting performance.
% \end{enumerate}






\subsubsection{Different Number of Prompt Tokens}
We conduct an ablation study on the length of the intermediate VF prompt, using the universal VF prompt as the VF prompt, which samples a set of indices from the image tokenizer's codebook based on a uniform distribution.  
To save computational resources, all experiments train the model with 111M parameters for 50 epochs, as 50 epochs are sufficient to reflect the performance of different models. The results are shown in Fig.~\ref{fig:number}. A prompt length of 0 corresponds to the baseline, which represents the baseline without VF prompt. 
We can observe that when the VF prompt is short (8 tokens), the ability of VF prompt to handle complex image generation task is weaker because it contains fewer information, resulting in performance similar to the baseline. 
% This is consistent with a similar conclusion in CoT in NLP, where the CoT prompt with less information also shows weaker performance on complex tasks~\cite{wei2022chain}. 
Furthermore, as the length of the prompt increases, the FID performance improves gradually. 
However, the performance gains are not uncapped, which can be found that when the prompt length reaches near 256, the gains begin to slow down.  This phenomenon is reasonable as the gains from using representative information will reach an upper limit.
% This phenomenon contrasts with CoT in NLP, where the length of the intermediate VF prompt is variable and not fixed, but it is reasonable for class conditional image generation as the gains from using distributed information will reach an upper limit.
% However, when the prompt length reaches a certain value (256 tokens), the performance gains begin to slow down. 
% \textcolor{red}{This finding contrasts with CoT in NLP, where the length of the intermediate VF prompt is variable and not fixed. In our image generation task, the intermediate VF prompt is image-related tokens, and the corresponding position embeddings carry 2D spatial information. As a result, changes in the length of the tokens directly impact the ability of model to effectively capture and process spatial relationships in the image, thereby influencing the performance of the generated results.}%可以删掉？
Therefore, in subsequent experiments, considering both performance and efficiency, the length of VF prompt is set to 256.








\begin{table*}

\centering
{
\setlength{\tabcolsep}{4mm}{
\begin{tabular}{ccccccc}
  % \begin{tabularx}{\linewidth}{>{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
\toprule
Type & Model & Param & FID$\downarrow$ & IS$\uparrow$ & Precision$\uparrow$ & Recall$\uparrow$ \\
\midrule
\multirow{3}{*}{GAN} & BigGAN~\cite{brock2018large} & 112M & 6.95 & 224.5 & 0.89 & 0.38 \\
\multirow{3}{*}{ } & GigaGAN~\cite{kang2023scaling} & 569M & 3.45 & 225.5 & 0.84 & 0.61\\
\multirow{3}{*}{ } & StyleGAN-XL~\cite{sauer2022stylegan} & 166M & 2.30 & 265.1 & 0.78 & 0.53\\ 

\midrule
\multirow{4}{*}{Diffusion} & ADM~\cite{dhariwal2021diffusion} & 554M & 10.94 & 101.0 & 0.69 & 0.63 \\
\multirow{4}{*}{ } & CDM~\cite{ho2022cascaded} & - & 4.88 & 158.7 & - & -\\
\multirow{4}{*}{ } & LDM-4~\cite{rombach2022high} & 400M & 3.60 & 247.7 & - & -\\
\multirow{4}{*}{ } & DiT-XL/2~\cite{peebles2023scalable} & 675M & 2.27 & 278.2 & 0.83 & 0.57\\

\midrule
\multirow{3}{*}{Maksed AR} & MaskGIT~\cite{chang2022maskgit} & 227M & 6.18 & 182.1 & 0.80 & 0.51 \\
\multirow{3}{*}{ } & MaskGIT-re~\cite{li2023mage} & 227M & 4.02 & 355.6 & - & -\\
\multirow{3}{*}{ } & MAGE~\cite{li2024autoregressive} & 230M & 6.93 & 195.8 & - & -\\

\midrule
\multirow{15}{*}{AR} & VQGAN~\cite{esser2021taming} & 227M & 18.65 & 80.4 & 0.78 & 0.26 \\
\multirow{13}{*}{ } & VQGAN~\cite{esser2021taming} & 1.4B & 15.76 & 74.3 & - & -\\
\multirow{13}{*}{ } & VQGAN-re~\cite{yu2021vector} & 1.4B & 5.20 & 280.3 & - & -\\
\multirow{13}{*}{ } & ViT-VQGAN~\cite{yu2021vector} & 1.7B & 4.17 & 175.1 & - & -\\
\multirow{13}{*}{ } & ViT-VQGAN-re~\cite{yu2021vector} & 1.7B & 3.04 & 227.4 & - & -\\
 \multirow{13}{*}{ }& RQTran.~\cite{lee2022autoregressive} & 3.8B & 7.55 & 80.4 & 0.78 & 0.26\\
 \multirow{13}{*}{ }&RQTran.-re~\cite{lee2022autoregressive} & 3.8B & 3.80 & 323.7 & - & - \\
\multirow{13}{*}{ } & LlamaGen-B (CFG=2.00)~\cite{sun2024autoregressive} & 111M & 5.46 & 193.61 & 0.83 & 0.45\\
% \multirow{13}{*}{ } & LlamaGen-B*(CFG=2.25)~\cite{sun2024autoregressive} & 111M & 6.09 & 193.61 & 0.83 & 0.45\\
 \multirow{13}{*}{ }& LlamaGen-L (CFG=1.75)~\cite{sun2024autoregressive} & 343M &3.81 &248.28 &0.83 &0.52\\
\multirow{13}{*}{ } & LlamaGen-XL (CFG=1.75)~\cite{sun2024autoregressive} & 775M &3.39 &227.08 &0.81 &0.54\\
\multirow{13}{*}{ } & LlamaGen-XXL (CFG=1.75)~\cite{sun2024autoregressive} & 1.4B  &3.09 &253.61  &0.83 &0.53\\
% \multirow{13}{*}{ } & LlamaGen-3B$\dagger$,(CFG=1.75)~\cite{sun2024autoregressive} & 3.1B & 3.05 &222.33 &0.80 &0.58\\
\cline{2-7}
% \multirow{8}{*}{Ours} & Ours-VQGAN & 227M\\
% \multirow{3}{*}{ } & Ours-VQGAN* & 227M\\

\multirow{3}{*}{ } & VF prompt-B (CFG=1.75) & 111M &4.39 &222.80 &0.85&0.46\\
% \multirow{3}{*}{ } & Ours-LlamaGen-B*(CFG=1.75) & 111M & \\
\multirow{3}{*}{ } & VF prompt-L (CFG=1.75) & 343M&3.23 &279.07 & 0.84& 0.52\\
% \multirow{3}{*}{ } & Ours-LlamaGen-L*(CFG=1.75) & 343M\\
\multirow{3}{*}{ } & VF prompt-XL (CFG=1.75) & 775M&2.85 &289.75 &0.83 &0.54\\
\multirow{13}{*}{ } & VF prompt-XXL (CFG=1.75) & 1.4B&2.76 &275.80 &0.82 &0.55 \\
% \multirow{13}{*}{ } & Ours-3B$\dagger$,(CFG=1.75) & 3.1B \\
\bottomrule
  \end{tabular}}
}
  \caption{Model comparisions on class-conditional ImageNet $256\times256$ benchmark. Metrics include Frechet inception distance (FID), inception score (IS), precision and recall. `-re' means rejection sampling. `$\downarrow$' or `$\uparrow$' indicate lower or higher values are better. `CFG' means using Classifier-Free Guidance. 
  % For LlamaGen~\cite{sun2024autoregressive} and our method, * means training is on $384\times384$ and the output is resized in evaluation; otherwise, direct training on $256\times256$ images.
  }
  \label{tab:main_results}
\end{table*}


\begin{figure*}[t]
  \centering
   \includegraphics[width=\linewidth]{fig/visual2.pdf}

   \caption{Sample images randomly generated by VF prompt, trained on ImageNet.}
   \label{fig:visual2}
\end{figure*}


\begin{figure}[!t]
  \centering
   \includegraphics[width=0.9\linewidth]{fig/scale.png}
   \caption{Scaling behavior of VF prompt. (a) Training loss of models with different sizes. (b) FID scores of models with different sizes during inference. 
   % The scaled-upVF prompt model demonstrates the reduction in training loss and the improvement in FID scores.
   }
   \label{fig:scale}
\end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{fig/visual.pdf}

   \caption{Visualization of samples generated by VF prompt based on different model sizes. 
   % As the model size increases, the generated results for the same class become more realistic and detailed, with finer textures and better overall consistency.
   }
   \label{fig:visual}
\end{figure}

\subsubsection{Blank Prompt}
% In NLP, one of the findings of CoT is that variable computation itself is not the reason for the success of CoT prompting. Instead, expressing intermediate steps via  natural language seems to be useful~\cite{wei2022chain}. For example, \textit{Jason et al.}~\cite{wei2022chain} utilize a sequence of dots ($...$) as a CoT prompt and find that the performance is similar to the baseline without using CoT. To investigate whether VF prompt follows the same conclusion, 
Another way to think about our VF prompt is that the proposed image-related VF prompt enables the model to spend more computational resources (\textit{i.e.}, intermediate tokens). To isolate the impact of variable computation from the reasoning process itself, 
we conduct a variant where we replace the VF prompt with multiple blank tokens of equal length, which are tokens extracted from a completely black image with no representation information. The experiments train the model for 50 epochs. As shown in `Blank Prompt' of Table~\ref{tab:Ablation}, the experimental result indicates that this variant performs similarly to the baseline without our VF prompt. 
This demonstrates that variable computation itself is not the reason for the success of VF prompt, and proves the effectiveness of the designed VF prompt, which incorporates full-view image information.
% This demonstrates that variable computation itself is not the reason for the success of VF prompt, and that VF prompt expresses utilizing representation information to be effective. 
% intermediate thought steps with distributional information to be effective.
% , which aligns with the conclusions obtained from CoT in NLP.

\subsubsection{Order of Full-view and Generation}
A potential benefit of VF prompt is that it allows the model to better access relevant knowledge acquired during pretraining. To investigate this, we test an alternative configuration where the VF prompt is given only after the image generation, isolating whether the model depends on the prior VF prompt to generate the image. The experiments train the model for 50 epochs.
% \textcolor{red}{To save computational resources, the experiments train the model for 50 epochs, as 50 epochs are sufficient to reflect the performance of different models.}
As shown in the `Full-view After Generation' row in Table~\ref{tab:Ablation}, we find that the performance of this variant is comparable to the baseline, suggesting that the sequential full-view itself is valuable in the generation process of the model, rather than merely activating known knowledge. 
% This conclusion is consistent with the findings of CoT in NLP~\cite{wei2022chain}. 






\subsubsection{Effect of Classifier-Free Guidance (CFG)}
We conduct ablation experiments under different Classifier-Free Guidance (CFG) settings based on \textit{Universal VF prompt}, as shown in Table~\ref{tab:cfg}. It can be observed that as CFG scale increases, the model's performance gradually improves, with the best FID achieved when CFG scale is set to 1.75. However, further increasing CFG scale leads to a deterioration in FID. Additionally, the increase in CFG scale results in a trade-off between diversity and fidelity, with higher accuracy and lower recall. Therefore, in subsequent experiments, we set the CFG scale to 1.75.



\subsection{Main Results}
\label{sec:MainResults}
\paragraph{Comparisons with Other Image Generation Methods} In Table~\ref{tab:main_results}, we compare with popular image generation models, including GAN~\cite{brock2018large,kang2023scaling,sauer2022stylegan}, Diffusion models~\cite{dhariwal2021diffusion,ho2022cascaded,rombach2022high,peebles2023scalable}, masked AR~\cite{chang2022maskgit,li2023mage,li2024autoregressive} and AR models~\cite{esser2021taming,yu2021vector,lee2022autoregressive,sun2024autoregressive}.
% For LlamaGen with  XL (775M), and XXL (1.4B) model, only results up to 50 epoch on the $256\times256$ training set are reported in~\cite{sun2024autoregressive}, with no further results presented, so we also only show results at 50 epochs on oursX-L and ours-XXL.%是否要在setting里面再说一遍？
All our experiments are based on the universal VF prompt, as it is more general and easier to obtain compared to the class VF prompt due to its independence from the condition. 



The results show that our models outperform all previous AR methods at different levels of model parameters. For example, compared to Llama-B, Llama-L Llama-XL and Llama-XL with the same parameters, VF prompt-B, VF prompt-L VF prompt-XL and VF prompt-XL improves FID by 20\%, 15\%, 16\% and 11\%, and IS by 15\%, 12\%, 28\% and 9\%, respectively. This indicates that our VF prompt of introducing VF prompts effectively helps the AR model in learning and generation by enhancing contextual reasoning of generation and improving stability during the generation process through increased inference steps . 
% Furthermore, our method also surpasses diffusion-based methods on the IS metric, achieving the best performance among all generative methods. 
It is important to note that since LlamaGen does not release the results at 300 epoches for the L and XL models, for a fair comparison, we only present the results for our method at 50 epochs on the XL and XXL models, which have not yet reached the performance limit of our method, leaving room for further improvement.


\paragraph{Visualization} 
As shown in Fig.~\ref{fig:visual2}, we visualize some sample images randomly generated by VF prompt, trained on ImageNet. This demonstrates that VF prompt is capable of generating high-quality samples with both high fidelity and diversity. More visualizations are provided in the appendix.







\paragraph{Scaling Behavior}
We investigate the scaling behavior of VF prompt. As shown in Fig.~\ref{fig:scale}, we show the training loss and FID score based on models of different sizes. It can be observed that VF prompt demonstrates good scalability, with lower training loss and better FID scores as the model size increases. This is because we did not modify the formula or structure of the AR model itself, preserving the integrity of the AR framework, and therefore inheriting the scalability of AR methods. Additionally, we visualize the generation results of the same class under different model sizes, as shown in Fig.~\ref{fig:visual}. It can be observed that as the model size increases, the generated results for the same class become more realistic and detailed, with finer textures and better overall consistency.

\begin{figure}[t]
  \centering
   \includegraphics[width=0.8\linewidth]{fig/loss.png}

   \caption{Training loss of the same model with and without VF prompt. 
   % As the model size increases, the generated results for the same class become more realistic and detailed, with finer textures and better overall consistency.
   }
   \label{fig:loss}
\end{figure}
\paragraph{Convergence}
As the loss curve shown in Fig.~\ref{fig:loss}, it can be observed that the same model architecture after the same training step has a significantly smaller loss for the method using VF prompt, which confirms the conclusions of our theoretical analysis in Sec \ref{theory}.








\section{Conclusion}
% In this work, we introduce Image Generation with Thoughtful Reasoning (VF prompt), a novel approach inspired by the Chain-of-Thought (CoT) reasoning technique from NLP, to enhance autoregressive (AR) image generation. By incorporating intermediate VF prompts, VF prompt addresses the challenges faced by traditional AR models in reconstructing image structure and details, improving both the accuracy and stability of the generation process. Our method simulates human-like reasoning by enabling the model to first perceive overall distribution information, enhancing contextual reasoning and ensuring logical consistency during image generation. Additionally, by increasing the inference steps, VF prompt strengthens the model's ability to maintain coherence throughout the generation process. Compared to AR models without VF prompts, which achieve state-of-the-art performance, our approach shows a 20\% improvement in performance. These results suggest that integrating VF prompts into autoregressive models is a promising direction for advancing image generation, and VF prompt opens up new possibilities for further refinement and optimization in this area.
% In this work, inspired by the prompt engineering from NLP of AR model, we propose the Vision Full-view (VF) prompt, to enhance AR image generation. VF prompt improves both the accuracy and stability of image generation without changing the model structure or the autoregressive generation order.
% By adding specialized image-related VF prompts, VF prompt enhances contextual reasoning by allowing the model to first perceive overall distribution information before generating the image, and improves generation stability by increasing the inference steps. VF prompt shows an approximately 20\% improvement over the AR baseline, demonstrating the effectiveness of VF prompts in complex image generation tasks.
In this work, inspired by the prompt engineering from NLP of AR model, we propose the Vision Full-view (VF) prompt, to simulate the human process of creating images by guiding the model to first perceive vision full-view information before generating the image. This approach enhances contextual reasoning by providing a more comprehensive understanding of the overall image information. VF prompt also improves generation stability by increasing the inference steps. VF prompt shows an approximately 20\% improvement over the AR baseline, demonstrating the effectiveness of VF prompts in complex image generation tasks.
