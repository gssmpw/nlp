@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{raffel2023exploringlimitstransferlearning,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@inproceedings{Golgoon_2024, series={ICAIF ’24},
   title={Mechanistic interpretability of large language models with applications to the financial services industry},
   url={http://dx.doi.org/10.1145/3677052.3698612},
   DOI={10.1145/3677052.3698612},
   booktitle={Proceedings of the 5th ACM International Conference on AI in Finance},
   publisher={ACM},
   author={Golgoon, Ashkan and Filom, Khashayar and Ravi Kannan, Arjun},
   year={2024},
   month=nov, pages={660–668},
   collection={ICAIF ’24} }

@misc{chen2024surveylargelanguagemodels,
      title={A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law}, 
      author={Zhiyu Zoey Chen and Jing Ma and Xinlu Zhang and Nan Hao and An Yan and Armineh Nourbakhsh and Xianjun Yang and Julian McAuley and Linda Petzold and William Yang Wang},
      year={2024},
      eprint={2405.01769},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.01769}, 
}

@misc{mohammadi2025explainabilitypracticesurveyexplainable,
      title={Explainability in Practice: A Survey of Explainable NLP Across Various Domains}, 
      author={Hadi Mohammadi and Ayoub Bagheri and Anastasia Giachanou and Daniel L. Oberski},
      year={2025},
      eprint={2502.00837},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.00837}, 
}

@article{Madsen_2022,
   title={Post-hoc Interpretability for Neural NLP: A Survey},
   volume={55},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3546577},
   DOI={10.1145/3546577},
   number={8},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
   year={2022},
   month=dec, pages={1–42} }

@misc{ribeiro2016whyitrustyou,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.04938}, 
}

@article{belinkov-glass-2019-analysis,
    title = "Analysis Methods in Neural Language Processing: A Survey",
    author = "Belinkov, Yonatan  and
      Glass, James",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1004/",
    doi = "10.1162/tacl_a_00254",
    pages = "49--72",
    abstract = "The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work."
}

@inproceedings{Bordt_2022, series={FAccT ’22},
   title={Post-Hoc Explanations Fail to Achieve their Purpose in Adversarial Contexts},
   url={http://dx.doi.org/10.1145/3531146.3533153},
   DOI={10.1145/3531146.3533153},
   booktitle={2022 ACM Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Bordt, Sebastian and Finck, Michèle and Raidl, Eric and von Luxburg, Ulrike},
   year={2022},
   month=jun, pages={891–905},
   collection={FAccT ’22} }

@misc{laugel2019dangersposthocinterpretabilityunjustified,
      title={The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations}, 
      author={Thibault Laugel and Marie-Jeanne Lesot and Christophe Marsala and Xavier Renard and Marcin Detyniecki},
      year={2019},
      eprint={1907.09294},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1907.09294}, 
}

@misc{wei2024revisitingrobustnessposthocinterpretability,
      title={Revisiting the robustness of post-hoc interpretability methods}, 
      author={Jiawen Wei and Hugues Turbé and Gianmarco Mengaldo},
      year={2024},
      eprint={2407.19683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.19683}, 
}

@misc{koh2020conceptbottleneckmodels,
      title={Concept Bottleneck Models}, 
      author={Pang Wei Koh and Thao Nguyen and Yew Siang Tang and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang},
      year={2020},
      eprint={2007.04612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.04612}, 
}

@misc{chauhan2023interactiveconceptbottleneckmodels,
      title={Interactive Concept Bottleneck Models}, 
      author={Kushal Chauhan and Rishabh Tiwari and Jan Freyberg and Pradeep Shenoy and Krishnamurthy Dvijotham},
      year={2023},
      eprint={2212.07430},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.07430}, 
}

@misc{oikarinen2023labelfreeconceptbottleneckmodels,
      title={Label-Free Concept Bottleneck Models}, 
      author={Tuomas Oikarinen and Subhro Das and Lam M. Nguyen and Tsui-Wei Weng},
      year={2023},
      eprint={2304.06129},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.06129}, 
}

@misc{sun2024conceptbottlenecklargelanguage,
      title={Concept Bottleneck Large Language Models}, 
      author={Chung-En Sun and Tuomas Oikarinen and Berk Ustun and Tsui-Wei Weng},
      year={2024},
      eprint={2412.07992},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.07992}, 
}

@misc{yuksekgonul2023posthocconceptbottleneckmodels,
      title={Post-hoc Concept Bottleneck Models}, 
      author={Mert Yuksekgonul and Maggie Wang and James Zou},
      year={2023},
      eprint={2205.15480},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.15480}, 
}

@inproceedings{
ismail2024concept,
title={Concept Bottleneck Generative Models},
author={Aya Abdelsalam Ismail and Julius Adebayo and Hector Corrada Bravo and Stephen Ra and Kyunghyun Cho},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=L9U5MJJleF}
}

@misc{ludan2024interpretablebydesigntextunderstandingiteratively,
      title={Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck}, 
      author={Josh Magnus Ludan and Qing Lyu and Yue Yang and Liam Dugan and Mark Yatskar and Chris Callison-Burch},
      year={2024},
      eprint={2310.19660},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.19660}, 
}

@misc{tan2023interpretingpretrainedlanguagemodels,
      title={Interpreting Pretrained Language Models via Concept Bottlenecks}, 
      author={Zhen Tan and Lu Cheng and Song Wang and Yuan Bo and Jundong Li and Huan Liu},
      year={2023},
      eprint={2311.05014},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.05014}, 
}

@misc{simhi2023interpretingembeddingspacesconceptualization,
      title={Interpreting Embedding Spaces by Conceptualization}, 
      author={Adi Simhi and Shaul Markovitch},
      year={2023},
      eprint={2209.00445},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2209.00445}, 
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@INPROCEEDINGS{6638949,
  author={Sainath, Tara N. and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets}, 
  year={2013},
  volume={},
  number={},
  pages={6655-6659},
  keywords={Training;Speech recognition;Acoustics;Speech;Accuracy;Neural networks;Hidden Markov models;Deep Neural Networks;Speech Recognition},
  doi={10.1109/ICASSP.2013.6638949}}

@misc{romero2015fitnetshintsdeepnets,
      title={FitNets: Hints for Thin Deep Nets}, 
      author={Adriana Romero and Nicolas Ballas and Samira Ebrahimi Kahou and Antoine Chassang and Carlo Gatta and Yoshua Bengio},
      year={2015},
      eprint={1412.6550},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6550}, 
}

@misc{hinton2015distillingknowledgeneuralnetwork,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}


@InProceedings{pmlr-v97-guan19a,
  title = 	 {Towards a Deep and Unified Understanding of Deep Neural Models in {NLP}},
  author =       {Guan, Chaoyu and Wang, Xiting and Zhang, Quanshi and Chen, Runjin and He, Di and Xie, Xing},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2454--2463},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/guan19a/guan19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/guan19a.html},
  abstract = 	 {We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing (NLP) models leverage information of input words. Our method advances existing explanation methods by addressing issues in coherency and generality. Explanations generated by using our method are consistent and faithful across different timestamps, layers, and models. We show how our method can be applied to four widely used models in NLP and explain their performances on three real-world benchmark datasets.}
}


@article{Quinlan1986,
  author    = {J. Ross Quinlan},
  title     = {Induction of decision trees},
  journal   = {Machine Learning},
  volume    = {1},
  number    = {1},
  pages     = {81--106},
  year      = {1986},
  doi       = {10.1007/BF00116251},
  url       = {https://doi.org/10.1007/BF00116251},
  issn      = {1573-0565}
}

@misc{hf-all-MiniLM-L6-v2,
  title={all-MiniLM-L6-v2},
  author={Hugging Face and Sentence-Transformers},
  howpublished={\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}}
}

@misc{zhang2016characterlevelconvolutionalnetworkstext,
      title={Character-level Convolutional Networks for Text Classification}, 
      author={Xiang Zhang and Junbo Zhao and Yann LeCun},
      year={2016},
      eprint={1509.01626},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.01626}, 
}

@misc{laguna2024conceptbottleneckmodelsmake,
      title={Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?}, 
      author={Sonia Laguna and Ričards Marcinkevičs and Moritz Vandenhirtz and Julia E. Vogt},
      year={2024},
      eprint={2401.13544},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.13544}, 
}

@misc{dominici2024anycbmsturnblackbox,
      title={AnyCBMs: How to Turn Any Black Box into a Concept Bottleneck Model}, 
      author={Gabriele Dominici and Pietro Barbiero and Francesco Giannini and Martin Gjoreski and Marc Langhenirich},
      year={2024},
      eprint={2405.16508},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16508}, 
}