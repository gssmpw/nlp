\section{Implementation and Experiments}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Sections/Picture/BSC.png}
        \caption{BSC}
        \label{fig:bsc}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Sections/Picture/ETH.png}
        \caption{ETH}
        \label{fig:eth}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Sections/Picture/Polygon.png}
        \caption{Polygon}
        \label{fig:polygon}
    \end{subfigure}
    \caption{Analysis of ERC-20 transfer phantom events across different platforms for 9 addresses.}
    \label{fig:phantom_events_across_platforms}
\end{figure}

\label{sec:evaluation}
Our tool is implemented in Python and supports both bytecode-level and source code-level analysis for smart contracts, as well as transaction-level monitoring. For bytecode-level analysis, we use the Gigahorse framework to construct the ICFG from compiled bytecode. At the source code level, our tool supports all versions of Solidity compatible with Slither~\cite{Slither}, allowing us to extract the ASTs and perform detailed analysis on event emissions and parameter constraints. We employ the Z3~\cite{Z3} as our constraint solver to handle symbolic execution and eliminate infeasible paths. Additionally, the tool includes a transaction-level off-chain monitoring system that parses on-chain data and applies custom rules to detect anomalies and verify the success of attack transactions. All major components, including the bytecode analysis engine, source code symbolic executor, and transaction monitoring system, were developed by us. The implementation is based in Python and includes over 5,000 lines of code.


\begin{itemize}%[leftmargin=*,topsep=2pt]
  \item \textbf{RQ1} \textit{How prevalent are phantom events on major blockchain platforms, and how does \tool perform in transaction-level detection?} We aim to understand the need for validators to verify the origins of events through contract addresses and source functions.
  \item \textbf{RQ2} \textit{How effective is \tool in detecting potential phantom events from smart contract?} We developed \tool and constructed a custom benchmark to evaluate its effectiveness.
  \item \textbf{RQ3} \textit{How feasible is it to perform such attacks in real world?} We aim to validate whether the identified vulnerabilities can be performed in real-world scenarios.
\end{itemize}

\subsection{Prevalence of Phantom Events (RQ1)}

Tools like TokenScope~\cite{TokenScope} have contributed to detecting inconsistencies in token behavior, and Guoyi Ye et al.~\cite{wallet_visual} have demonstrated the prevalence of forged user addresses for transfer events. We aim to explore two specific aspects: the prevalence of suspicious addresses in phantom event attacks, and whether such attacks are common in cross-chain bridges.

\subsubsection{Methodology}

To better understand the real-world occurrence of phantom events, we conduct two separate experiments using blockchain transaction data.

In the first experiment, we focused on nine particular forged addresses, specifically from \emph{0x1111...1111} through \emph{0x9999...9999}, as these addresses are highly unlikely to be generated or used in normal transactions. We examined the transactions up to October 21, 2024, on the Ethereum, BSC, and Polygon blockchains, looking for ERC-20 token movements originating from these addresses. This approach helps measure the prevalence of phantom events across different blockchain platforms.

In the second experiment, we created detection rules for cross-chain bridges based on our transaction-level analysis, given that bridges are highly vulnerable to phantom event attacks. We compared our findings with those of XScope~\cite{Xscope}, the only existing tool for detecting bridge attack transactions, using its associated dataset.


\subsubsection{Result}

%\begin{table}[t]
%\caption{Analysis of ERC-20 transfer phantom events across different platforms for 9 addresses.}
%\centering
%\begin{tabular}{ll}
%\toprule
%\textbf{Platforms} & \textbf{Potential Phantom Events} \\
%\midrule
%Ethereum & 18,099 \\
%BSC & 1,243,917 \\
%Polygon & 436,477 \\
%\midrule
%\textbf{Total} & \textbf{1,661,813} \\
%\bottomrule
%\end{tabular}
%\label{table:phantom_events_across_platforms}
%\end{table}



In our first experiment, focusing on nine specific addresses, we identified a significant occurrence of phantom events in ERC-20 token transfers, as shown in \cref{fig:phantom_events_across_platforms}. We found 18,099 such events on Ethereum, 1,245,125 on BSC, and 436,407 on Polygon. Such events, if not properly verified, could lead to security exploits. This significant presence from just nine addresses highlights the critical need for rigorous transaction verification to maintain the security and trustworthiness of blockchain operations.


%\begin{table}[htbp]
%    \centering
%    \caption{Number of detected attack transactions by tools.}
%    \label{tab:attack_detection}
%    \begin{tabular}{lrr}
%        \toprule
%        \textbf{Project} & \textbf{XScope} & \textbf{\tool} \\
%        \midrule
%        THORChain \# 1 & 9 & 9 \\
%        THORChain \# 2 & 41 & 48 \\
%        pNetwork & 3 & 5 \\
%        Qubit Bridge & 20 & 20 \\
%        meter.io & N/A & 5 \\
%        CENNZnet & N/A & 1 \\
%        \bottomrule
%    \end{tabular}
%\end{table}

\begin{table}[t]
    \centering
    \caption{Number of detected attack transactions and successful attacks by tools.}
    \label{tab:attack_detection}
    \begin{tabular}{l r r r}
        \toprule
        \textbf{Project} & \textbf{XScope} & \multicolumn{2}{c}{\textbf{\tool}} \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-4}
        & \textbf{Attacks} & \textbf{Attack Trans.} & \textbf{Successful Attacks} \\
        \midrule
        THORChain \#1 & 9 & 9 & 9 \\
        THORChain \#2 & 41 & 48 & 41 \\
        pNetwork & 3 & 5 & 3 \\
        Qubit Bridge & 20 & 20 & 20 \\
        meter.io & N/A & 5 & 5 \\
        CENNZnet & N/A & 1 & 1 \\
        \bottomrule
    \end{tabular}
\end{table}


In the second experiment (the result shown in \cref{tab:attack_detection}), we detected all transactions reported by XScope and identified additional attack transactions. XScope only records transactions where attacks have already taken place, indicated by a fund transfer on the destination chain. In contrast, we were able to detect attacks before a transfer occurred, helping project teams identify attackers proactively. We also detected attack transactions on meter.io~\cite{Meter}, which XScope mentioned in its attack incident list but did not analyze. Additionally, we discovered an attack transaction on CENNZnet, which resulted in a loss of 150 ETH. This incident had not been previously disclosed or reported by any other tools or security companies.

\vspace{\baselineskip}
\begin{mdframed}
\noindent\textbf{Answer to RQ1:} \textit{Our analysis reveals a significant presence of phantom events across Ethereum, BSC, and Polygon, based on just nine addresses. Furthermore, our detection rules demonstrated effectiveness compared to state-of-the-art tools.}
\end{mdframed}

\subsection{Phantom Event Detection from Code (RQ2)}

\subsubsection{Datasets}
SmartAxe~\cite{liao2024smartaxe} and XGuard~\cite{wang2024xguard} are currently the two known tools for detecting cross-chain vulnerabilities, with SmartAxe focusing on bytecode-level analysis and XGuard providing source code-level analysis.

In our analysis of the SmartAxe dataset, we identified that only 8 of the 20 attacks were caused by \emph{Event Counterfeiting}, with vulnerable function path pairs existing within bridge contracts. We re-labeled these 8 attacks and added 29 more function path pairs affected by \emph{Event Counterfeiting} from the 129 bridges analyzed by SmartAxe. To complement this, we selected 500 BSC smart contracts with low similarity (using the TF-IDF algorithm) from the 452,666 verified contracts in BSC~\cite{bscdata} and manually annotated them. After deduplication, we identified 80 additional function pairs affected by \emph{Event Counterfeiting}.

By combining SmartAxe and XGuard datasets, along with manually annotated function pairs, we assembled a comprehensive data set containing 117 function pairs affected by \emph{Event Counterfeiting} for evaluating our detection tool.

To detect \emph{Inconsistent Logging}, we used a dataset of 28 manually audited cases of \emph{Inconsistent Logging} parameters, which were identified during our audits.

\subsubsection{Result}

\begin{table*}[htbp]
    \centering
    \caption{Tools Comparison for Event Counterfeiting and Inconsistent Logging}
    \label{tab:tool_result}
    \small
    \begin{tabular}{p{3.9cm}ccc ccc}
        \toprule
        \multirow{2}{*}{\textbf{Tool}} & \multicolumn{3}{c}{\textbf{Event Counterfeiting}} & \multicolumn{3}{c}{\textbf{Inconsistent Logging}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
        \midrule
        SmartAxe & 40.50\% & 25.12\% & 31.01\% & N/A & N/A & N/A \\
        XGuard & 87.75\% & 55.12\% & 67.70\% & 64.71\% & 91.75\% & 75.82\% \\
        \tool (Bytecode) & 84.0\% & 100\% & 91.30\% & 62.50\% & 16.12\% & 39.31\% \\
        \tool (Source) & 90.83\% & 86.51\% & 88.62\% & 90.30\% & 100\% & 94.90\% \\
        \bottomrule
    \end{tabular}
\end{table*}
In our experiments, \tool significantly outperformed SmartAxe and XGuard in detecting both \emph{Event Counterfeiting} and \emph{Inconsistent Logging} vulnerabilities, as shown in \cref{tab:tool_result}. For \emph{Event Counterfeiting}, SmartAxe achieved an F1 score of 31.01\% with a precision of 40.50\% and a recall of 25.12\%, while XGuard reached an F1 score of 67.70\% with a precision of 87.75\% and a recall of 55.12\%. In comparison, \tool demonstrated superior performance, achieving an F1 score of 91.30\% at the bytecode level and 88.62\% at the source code level. For \emph{Inconsistent Logging} detection, \tool also outperformed XGuard, achieving an F1 score of 94.9\% with perfect recall, compared to XGuard's 75.82\%.

The difference in performance metrics for \tool’s bytecode and source code implementations highlights specific limitations in each approach. In \emph{Event Counterfeiting} detection, \tool’s bytecode-level analysis lacks constraint validation, leading to a lower precision as it may flag events that lack sufficient evidence of forgery. However, at the source code level, \tool performs comprehensive constraint checks, improving precision significantly. For \emph{Inconsistent Logging}, \tool’s bytecode analysis is limited to indexed parameters only, which reduces recall as it cannot identify issues related to non-indexed parameters. With source code access, \tool can analyze both indexed and non-indexed parameters, thus achieving higher recall. In contrast, SmartAxe and XGuard have inherent design limitations; SmartAxe only detects whether two functions emit the same event without considering constraint validation within call paths, while XGuard focuses on constraints within single functions and lacks inter-function analysis. By analyzing both function interactions and call paths, \tool detects vulnerabilities that the other tools miss, achieving superior results in both \emph{Event Counterfeiting} and \emph{Inconsistent Logging} detection.


\subsubsection{Limitations}

The false positives and false negatives in our evaluation revealed two main limitations of our approach. First, when dealing with incomplete contract code, such as interface calls in certain functions, our computed constraints may not always be precise, leading to weaker constraints and thus false positives. Second, compilation errors occurred with some smart contracts that were verified by blockchain explorers but failed to compile with our tool for analysis~\cite{compile_issue}, due to Solidity compile bugs. Addressing these limitations will be a focus of our future work.


\vspace{\baselineskip}
\begin{mdframed}
\noindent\textbf{Answer to RQ2:} \textit{\tool has proven highly effective in detecting phantom event vulnerabilities, surpassing current state-of-the-art tools in precision, recall, and F1 score.}
\end{mdframed}

\subsection{Real-World Attack Feasibility (RQ3)}
\subsubsection{Methodology}
We applied our tool to conduct testing on various blockchain platforms, aiming to identify vulnerabilities related to phantom event attacks in smart contracts. Beyond on-chain testing, we also audited various off-chain applications, including blockchain explorers, NFT marketplaces, and cryptocurrency wallets. These audits focused on evaluating security protocols and identifying potential vulnerabilities that could be exploited by phantom events, ensuring a comprehensive analysis of both on-chain and off-chain components.

All identified phantom event vulnerabilities were tested in a controlled local environment using simulated scenarios to ensure no disruption or harm to real-world blockchain platforms.

\subsubsection{Result}

In our experiments, we identified a \emph{Transfer Event Spoofing} transaction in which a fake transfer appeared to move tokens from the address \textit{0x8888...8888} to the victim's wallet. Despite no actual token transfer occurring, at least five major blockchain explorers—BscScan, OKLink, Bitquery, Tokenview, and Bsctrace~\cite{bscscan,oklink,bitquery,tokenview,bsctrace}—incorrectly recognized this spoofed event as a legitimate transaction. This highlights the widespread vulnerability in off-chain systems, which struggle to differentiate between genuine and phantom events.

Additionally, we successfully replicated the ``sleepminting'' attack on testnet environments for two leading NFT marketplaces, Opensea and Rarible. Despite the existence of monitoring solutions designed to detect such attacks, fully mitigating these vulnerabilities remains an ongoing challenge.

Moreover, during our security audits, we discovered critical vulnerabilities in six cryptocurrency wallets, which we reported to the respective project teams or through bug bounty platforms. Four of these vulnerabilities were confirmed, with one resulting in a \$600 bounty from the project team.

In our audit of several blockchain bridges, we identified vulnerabilities in off-chain code that left them exposed to the Mimicry Contract Attack. In this attack, malicious contracts emit forged events that are incorrectly processed as legitimate by off-chain systems. One vulnerable system we uncovered was forked and deployed by a public blockchain project with a market capitalization exceeding \$250 million as of October 9, 2024. These vulnerabilities have been reported to the respective project teams for remediation.


Additionally, we identified a display issue with event data in the popular blockchain explorer Blockscout~\cite{error_display}, which could compromise the accuracy of transaction records. This flaw could potentially lead to user misinterpretation of blockchain data, undermining trust in transaction history.

In our audit of multiple active smart contract projects, we also found that three DeFi projects and one GameFi project were vulnerable to the \emph{Inconsistent Logging} attack, a vulnerability first proposed in this paper (see \ref{vec:attack2}). The highest affected market capitalization in this set of projects was \$169,688.


\vspace{\baselineskip}
\begin{mdframed}
\noindent\textbf{Answer to RQ3:} \textit{Our research demonstrates that phantom event attacks are feasible on real-world blockchain platforms, specifically targeting blockchain explorers, NFT marketplaces, DeFi, GameFi, and cryptocurrency wallets. This emphasizes the urgent need for improved security measures within the blockchain ecosystem. Detailed reports and confirmation cases are available in the supplementary materials.}
\end{mdframed}


\subsection{Threats to Validity}
The internal validity threats primarily stem from potential inaccuracies introduced during the manual data labeling process. To improve the accuracy of data labeling, we adopted a three-tier approach, where two authors independently labeled the data and a third author reviewed their labels. This method involved a thorough review at three distinct levels to enhance the precision of our dataset categorization.

To ensure external validity, we diversified the types and sources of datasets used in our experiments. In addition to the bridge-focused data provided by SmartAxe and XGuard, we expanded our dataset to include a broader range of scenarios. Furthermore, we avoided using smart contracts with high similarity to improve the diversity and validity of our analysis.