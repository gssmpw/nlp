\section{Related Work}
The integration of constraints into neural networks and training has been explored across various contexts. Early research focused on using neural networks to solve constrained optimization problems through penalty methods for analog circuits____, and foundational work in applied dynamic programming established theoretical links between neural representations and optimization____. More recently, the paradigm of \emph{Learning to Optimize} has gained traction, blending machine learning and combinatorial optimization to solve complex constrained problems efficiently____, guiding the optimization process with generative models ____, and solving problems with constraints due to physical laws or domain rules____.

Beyond penalty methods, techniques have emerged to enforce constraints directly through the network architecture. For instance, approaches have been developed to ensure monotonicity, convexity, or linear constraints on the network output____. Physics-informed neural networks (PINNs) have become popular for embedding differential constraints derived from physical systems directly into the training process____, and other strategies impose affine or inequality-based constraints to guarantee safe and consistent predictions____. Another method developed specifically for Bayesian optimization uses a transformer-based model to predict the expected improvements for constraints ____, based on the idea that transformers can do Bayesian inference ____.

The POLICE algorithm____ contributed to this landscape by offering a systematic method to enforce affine constraints in a single convex region without increasing inference complexity. However, POLICE did not address the complexities arising when multiple disjoint constrained regions must be handled simultaneously. Our work builds on POLICE and extends it to multiple regions, bridging a critical gap in the literature and providing a new foundation for multi-region constrained DNN training.