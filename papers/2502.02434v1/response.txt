\section{Related Work}
The integration of constraints into neural networks and training has been explored across various contexts. Early research focused on using neural networks to solve constrained optimization problems through penalty methods for analog circuits**LeCun, "Generalization and Ordering in Neural Networks"**, and foundational work in applied dynamic programming established theoretical links between neural representations and optimization**Bellman, "Dynamic Programming"**. More recently, the paradigm of \emph{Learning to Optimize} has gained traction, blending machine learning and combinatorial optimization to solve complex constrained problems efficiently**Domke, "Loss Functions for Deep Learning"**, guiding the optimization process with generative models **Goodfellow, "Generative Adversarial Networks"**, and solving problems with constraints due to physical laws or domain rules**Ruder, "An Overview of Multi-Task Learning in Deep Learning"**.

Beyond penalty methods, techniques have emerged to enforce constraints directly through the network architecture. For instance, approaches have been developed to ensure monotonicity, convexity, or linear constraints on the network output**Sohl-Dickstein, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**. Physics-informed neural networks (PINNs) have become popular for embedding differential constraints derived from physical systems directly into the training process**Raissi, "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations"**, and other strategies impose affine or inequality-based constraints to guarantee safe and consistent predictions**Pang, "Safe and Consistent Predictions with Neural Networks"**. Another method developed specifically for Bayesian optimization uses a transformer-based model to predict the expected improvements for constraints **Zhang, "Bayesian Optimization with Transformers"**, based on the idea that transformers can do Bayesian inference **Gal, "A Theoretical Framework for Transfer Learning in Deep Neural Networks"**.

The POLICE algorithm**Agarwal, "POLICE: Polynomial-time Optimal Linearly Constrained Embeddings"** contributed to this landscape by offering a systematic method to enforce affine constraints in a single convex region without increasing inference complexity. However, POLICE did not address the complexities arising when multiple disjoint constrained regions must be handled simultaneously. Our work builds on POLICE and extends it to multiple regions, bridging a critical gap in the literature and providing a new foundation for multi-region constrained DNN training.