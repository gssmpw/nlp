@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}

@article{brock2018large,
  title   = {Large scale GAN training for high fidelity natural image synthesis},
  author  = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal = {arXiv preprint arXiv:1809.11096},
  year    = {2018}
}

@article{AINSWORTH20016323,
  title    = {Essential boundary conditions and multi-point constraints in finite element analysis},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  volume   = {190},
  number   = {48},
  pages    = {6323-6339},
  year     = {2001},
  issn     = {0045-7825},
  doi      = {https://doi.org/10.1016/S0045-7825(01)00236-5},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782501002365},
  author   = {Mark Ainsworth},
  keywords = {Finite element analysis, Multi-point constraints},
  abstract = {The application of boundary conditions and other constraints to the stiffness matrix and load vector is an integral part of any finite element code. This process is usually trivial but can present difficulties to the point where some codes disallow certain combinations of commonly occurring boundary conditions. A general approach to this problem is presented and applied to representative examples. Necessary and sufficient conditions are obtained under which constraints can be applied sequentially. Bounds are obtained for the condition number of the reduced system showing it is essentially the same as for the original problem.}
}

@inproceedings{police,
  author    = {Balestriero, Randall and LeCun, Yann},
  booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {{Police}: Provably Optimal Linear Constraint Enforcement For Deep Neural Networks},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {1-5},
  abstract  = {Deep Neural Networks (DNNs) outshine alternative function approximators in many settings thanks to their modularity in composing any desired differentiable operator. The formed parametrized functional is then tuned to solve a task at hand from simple gradient descent. This modularity comes at the cost of making strict enforcement of constraints on DNNs, e.g. from a priori knowledge of the task, or from desired physical properties, an open challenge. In this paper we propose the first provable affine constraint enforcement method for DNNs that only requires minimal changes into a given DNN’s forward-pass, that is computationally friendly, and that leaves the optimization of the DNN’s parameter to be unconstrained, i.e. standard gradient-based method can be employed. Our method does not require any sampling and provably ensures that the DNN fulfills the affine constraint on a given input space’s region at any point during training, and testing. We coin this method POLICE, standing for Provably Optimal LInear Constraint Enforcement. Github: https://github.com/RandallBalestriero/POLICE},
  keywords  = {Deep learning;Training;Law enforcement;Neural networks;Signal processing;Task analysis;Speech processing},
  doi       = {10.1109/ICASSP49357.2023.police},
  issn      = {2379-190X},
  month     = {June}
}


@article{tordesillas2023rayen,
  title    = {Rayen: Imposition of hard convex constraints on neural networks},
  author   = {Tordesillas, Jesus and How, Jonathan P and Hutter, Marco},
  journal  = {arXiv preprint arXiv:2307.08336},
  year     = {2023},
  abstract = {This paper presents RAYEN, a framework to impose hard convex constraints on the output or latent variable of a neural network. RAYEN guarantees that, for any input or any weights of the network, the constraints are satisfied at all times. Compared to other approaches, RAYEN does not perform a computationally-expensive orthogonal projection step onto the feasible set, does not rely on soft constraints (which do not guarantee the satisfaction of the constraints at test time), does not use conservative approximations of the feasible set, and does not perform a potentially slow inner gradient descent correction to enforce the constraints. RAYEN supports any combination of linear, convex quadratic, second-order cone (SOC), and linear matrix inequality (LMI) constraints, achieving a very small computational overhead compared to unconstrained networks. For example, it is able to impose 1K quadratic constraints on a 1K-dimensional variable with an overhead of less than 8 ms, and an LMI constraint with 300x300 dense matrices on a 10K-dimensional variable in less than 12 ms. When used in neural networks that approximate the solution of constrained optimization problems, RAYEN achieves computation times between 20 and 7468 times faster than state-of-the-art algorithms, while guaranteeing the satisfaction of the constraints at all times and obtaining a cost very close to the optimal one.}
}

@article{kondo2024cgd,
  title    = {{CGD}: Constraint-Guided Diffusion Policies for {UAV} Trajectory Planning},
  author   = {Kondo, Kota and Tagliabue, Andrea and Cai, Xiaoyi and Tewari, Claudius and Garcia, Olivia and Espitia-Alvarez, Marcos and How, Jonathan P},
  journal  = {arXiv preprint arXiv:2405.01758},
  year     = {2024},
  abstract = {Traditional optimization-based planners, while effective, suffer from high computational costs, resulting in slow trajectory generation. A successful strategy to reduce computation time involves using Imitation Learning (IL) to develop fast neural network (NN) policies from those planners, which are treated as expert demonstrators. Although the resulting NN policies are effective at quickly generating trajectories similar to those from the expert, (1) their output does not explicitly account for dynamic feasibility, and (2) the policies do not accommodate changes in the constraints different from those used during training.
              To overcome these limitations, we propose Constraint-Guided Diffusion (CGD), a novel IL-based approach to trajectory planning. CGD leverages a hybrid learning/online optimization scheme that combines diffusion policies with a surrogate efficient optimization problem, enabling the generation of collision-free, dynamically feasible trajectories. The key ideas of CGD include dividing the original challenging optimization problem solved by the expert into two more manageable sub-problems: (a) efficiently finding collision-free paths, and (b) determining a dynamically-feasible time-parametrization for those paths to obtain a trajectory. Compared to conventional neural network architectures, we demonstrate through numerical evaluations significant improvements in performance and dynamic feasibility under scenarios with new constraints never encountered during training.}
}

@article{bouvier2024policed,
  title    = {{POLICEd RL}: Learning Closed-Loop Robot Control Policies with Provable Satisfaction of Hard Constraints},
  author   = {Bouvier, Jean-Baptiste and Nagpal, Kartik and Mehr, Negar},
  journal  = {arXiv preprint arXiv:2403.13297},
  year     = {2024},
  abstract = {In this paper, we seek to learn a robot policy guaranteed to satisfy state constraints. To encourage constraint satisfaction, existing RL algorithms typically rely on Constrained Markov Decision Processes and discourage constraint violations through reward shaping. However, such soft constraints cannot offer verifiable safety guarantees. To address this gap, we propose POLICEd RL, a novel RL algorithm explicitly designed to enforce affine hard constraints in closed-loop with a black-box environment. Our key insight is to force the learned policy to be affine around the unsafe set and use this affine region as a repulsive buffer to prevent trajectories from violating the constraint. We prove that such policies exist and guarantee constraint satisfaction. Our proposed framework is applicable to both systems with continuous and discrete state and action spaces and is agnostic to the choice of the RL training algorithm. Our results demonstrate the capacity of POLICEd RL to enforce hard constraints in robotic tasks while significantly outperforming existing methods.
              }
}

@article{bouvier2024learning,
  title    = {Learning to Provably Satisfy High Relative Degree Constraints for Black-Box Systems},
  author   = {Bouvier, Jean-Baptiste and Nagpal, Kartik and Mehr, Negar},
  journal  = {arXiv preprint arXiv:2407.20456},
  year     = {2024},
  abstract = {In this paper, we develop a method for learning a control policy guaranteed to satisfy an affine state constraint of high relative degree in closed loop with a black-box system. Previous reinforcement learning (RL) approaches to satisfy safety constraints either require access to the system model, or assume control affine dynamics, or only discourage violations with reward shaping. Only recently have these issues been addressed with POLICEd RL, which guarantees constraint satisfaction for black-box systems. However, this previous work can only enforce constraints of relative degree 1. To address this gap, we build a novel RL algorithm explicitly designed to enforce an affine state constraint of high relative degree in closed loop with a black-box control system. Our key insight is to make the learned policy be affine around the unsafe set and to use this affine region to dissipate the inertia of the high relative degree constraint. We prove that such policies guarantee constraint satisfaction for deterministic systems while being agnostic to the choice of the RL training algorithm. Our results demonstrate the capacity of our approach to enforce hard constraints in the Gym inverted pendulum and on a space shuttle landing simulation.}
}

@article{kotary2024learning,
  title    = {Learning Constrained Optimization with Deep Augmented Lagrangian Methods},
  author   = {Kotary, James and Fioretto, Ferdinando},
  journal  = {arXiv preprint arXiv:2403.03454},
  year     = {2024},
  abstract = {Learning to Optimize (LtO) is a problem setting in which a machine learning (ML) model is trained to emulate a constrained optimization solver. Learning to produce optimal and feasible solutions subject to complex constraints is a difficult task, but is often made possible by restricting the input space to a limited distribution of related problems. Most LtO methods focus on directly learning solutions to the primal problem, and applying correction schemes or loss function penalties to encourage feasibility. This paper proposes an alternative approach, in which the ML model is trained instead to predict dual solution estimates directly, from which primal estimates are constructed to form dual-feasible solution pairs. This enables an end-to-end training scheme is which the dual objective is maximized as a loss function, and solution estimates iterate toward primal feasibility, emulating a Dual Ascent method. First it is shown that the poor convergence properties of classical Dual Ascent are reflected in poor convergence of the proposed training scheme. Then, by incorporating techniques from practical Augmented Lagrangian methods, we show how the training scheme can be improved to learn highly accurate constrained optimization solvers, for both convex and nonconvex problems.
              }
}

@article{konstantinov2024imposing,
  title     = {Imposing Star-Shaped Hard Constraints on the Neural Network Output},
  author    = {Konstantinov, Andrei and Utkin, Lev and Muliukha, Vladimir},
  journal   = {Mathematics},
  volume    = {12},
  number    = {23},
  pages     = {3788},
  year      = {2024},
  publisher = {MDPI},
  abstract  = {A problem of imposing hard constraints on the neural network output can be met in many applications. We propose a new method for solving this problem for non-convex constraints that are star-shaped. A region produced by constraints is called star-shaped when there exists an origin in the region from which every point is visible. Two tasks are considered: to generate points inside the region and on the region boundary. The key idea behind the method is to generate a shift of the origin towards a ray parameterized by the additional layer of the neural network. The largest admissible shift is determined by the differentiable Ray marching algorithm. This allows us to generate points which are guaranteed to satisfy the constraints. A more accurate modification of the algorithm is also studied. The proposed method can be regarded as a generalization of the methods for convex constraints. Numerical experiments illustrate the method by solving machine-learning problems. The code implementing the method is publicly available.
               }
}

@inproceedings{4792111,
  author    = {Kramer, Mark A. and Thompson, Michael L. and Bhagat, Phiroz M.},
  booktitle = {1992 American Control Conference},
  title     = {Embedding Theoretical Models in Neural Networks},
  year      = {1992},
  volume    = {},
  number    = {},
  pages     = {475-479},
  keywords  = {Neural networks;Predictive models;Extrapolation;Bioreactors;Context modeling;Intelligent networks;Constraint theory;Nonlinear systems;Backpropagation;Parameter estimation},
  doi       = {10.23919/ACC.1992.4792111}
}


@article{yuan2019adversarial,
  title     = {Adversarial examples: Attacks and defenses for deep learning},
  author    = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
  journal   = {IEEE transactions on neural networks and learning systems},
  volume    = {30},
  number    = {9},
  pages     = {2805--2824},
  year      = {2019},
  publisher = {IEEE},
  abstract  = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.
               }
}


@inproceedings{wong2018provable,
  title        = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
  author       = {Wong, Eric and Kolter, Zico},
  booktitle    = {International conference on machine learning},
  pages        = {5286--5295},
  year         = {2018},
  organization = {PMLR},
  abstract     = {We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations (on the training data; for previously unseen examples, the approach will be guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well). The basic idea of the approach is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a toy 2D robust classification task, and on a simple convolutional architecture applied to MNIST, where we produce a classifier that provably has less than 8.4% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$. This represents the largest verified network that we are aware of, and we discuss future challenges in scaling the approach to much larger domains.}
}


@book{bellman2015applied,
  title     = {Applied dynamic programming},
  author    = {Bellman, Richard E and Dreyfus, Stuart E},
  volume    = {2050},
  year      = {2015},
  publisher = {Princeton university press}
}


@inproceedings{koppen2000curse,
  title     = {The curse of dimensionality},
  author    = {K{\"o}ppen, Mario},
  booktitle = {5th online world conference on soft computing in industrial applications (WSC5)},
  volume    = {1},
  pages     = {4--8},
  year      = {2000},
  abstract  = {In this text, some question related to higher dimensional geometrical spaces will be discussed. The goal is to give the reader a feeling for geometric distortions related to the use of such spaces (e.g. as search spaces).}
}

@article{montufar2014number,
  title    = {On the number of linear regions of deep neural networks},
  author   = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  journal  = {Advances in neural information processing systems},
  volume   = {27},
  year     = {2014},
  abstract = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.
              }
}


@inproceedings{pmlr-v80-balestriero18b,
  title     = {A Spline Theory of Deep Learning},
  author    = {Balestriero, Randall and richard baraniuk},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {374--383},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf},
  url       = {https://proceedings.mlr.press/v80/balestriero18b.html},
  abstract  = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of <em>max-affine spline operators</em> (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.}
}


@article{sitzmann2020implicit,
  title    = {Implicit neural representations with periodic activation functions},
  author   = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal  = {Advances in neural information processing systems},
  volume   = {33},
  pages    = {7462--7473},
  year     = {2020},
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.
              }
}

@article{zhong2023neural,
  title    = {Neural fields with hard constraints of arbitrary differential order},
  author   = {Zhong, Fangcheng and Fogarty, Kyle and Hanji, Param and Wu, Tianhao and Sztrajman, Alejandro and Spielberg, Andrew and Tagliasacchi, Andrea and Bosilj, Petra and Oztireli, Cengiz},
  journal  = {arXiv preprint arXiv:2306.08943},
  year     = {2023},
  abstract = {While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as Constrained Neural Fields (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.
              }
}

@article{XIE2024117223,
  title    = {Physics-specialized neural network with hard constraints for solving multi-material diffusion problems},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  volume   = {430},
  pages    = {117223},
  year     = {2024},
  issn     = {0045-7825},
  doi      = {https://doi.org/10.1016/j.cma.2024.117223},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782524004791},
  author   = {Yuchen Xie and Honghang Chi and Yahui Wang and Yu Ma},
  keywords = {Physics–informed neural network, Multi-material diffusion, Deep learning, Hard constraints},
  abstract = {The physics-informed neural network (PINN) has garnered significant attention in the domain of solving differential equations (DEs) owing to its adaptability to diverse DEs. Nonetheless, the PINN method encounters challenges in effectively solving multi-material diffusion problems due to the discontinuity at the interface. In this paper, we propose a PINN-based approach, termed as physics-specialized neural network (PSNN), which tailors a specialized neural network according to the distinctive characteristics of the problem. The PSNN integrates the continuity conditions of solution and flux as hard constraints into the mathematical form of the neural network, thereby perfectly addressing the discontinuity issues at axis-aligned interface. Moreover, this paper outlines the requirements for the specialized functions in PSNN, presents the analytical solving method for specialized functions under axis-aligned interface conditions, and compares PSNN with other enhanced PINN methods in the resolution of multi-material diffusion problems across multiple scenarios. The computational results reveal that PSNN demonstrates high computational accuracy and convergence speed, and the application of hard constraints for boundary conditions (BCs) in PSNN yields superior performance as compared to utilizing soft constraints for BCs.}
}







@article{doi:10.1137/21M1397908,
  author   = {Lu, Lu and Pestourie, Rapha\"{e}l and Yao, Wenjie and Wang, Zhicheng and Verdugo, Francesc and Johnson, Steven G.},
  title    = {Physics-Informed Neural Networks with Hard Constraints for Inverse Design},
  journal  = {SIAM Journal on Scientific Computing},
  volume   = {43},
  number   = {6},
  pages    = {B1105-B1132},
  year     = {2021},
  doi      = {10.1137/21M1397908},
  abstract = {Inverse design arises in a variety of areas in engineering such as acoustic, mechanics, thermal/electronic transport, electromagnetism, and optics. Topology optimization is an important form of inverse design, where one optimizes a designed geometry to achieve targeted properties parameterized by the materials at every point in a design region. This optimization is challenging, because it has a very high dimensionality and is usually constrained by partial differential equations (PDEs) and additional inequalities. Here, we propose a new deep learning method---physics-informed neural networks with hard constraints (hPINNs)---for solving topology optimization. hPINN leverages the recent development of PINNs for solving PDEs, and thus does not require a large dataset (generated by numerical PDE solvers) for training. However, all the constraints in PINNs are soft constraints, and hence we impose hard constraints by using the penalty method and the augmented Lagrangian method. We demonstrate the effectiveness of hPINN for a holography problem in optics and a fluid problem of Stokes flow. We achieve the same objective as conventional PDE-constrained optimization methods based on adjoint methods and numerical PDE solvers, but find that the design obtained from hPINN is often smoother for problems whose solution is not unique. Moreover, the implementation of inverse design with hPINN can be easier than that of conventional methods because it exploits the extensive deep-learning software infrastructure.}
}




@inproceedings{pmlr-v168-djeumou22a,
  title     = {Neural Networks with Physics-Informed Architectures and Constraints for Dynamical Systems Modeling},
  author    = {Djeumou, Franck and Neary, Cyrus and Goubault, Eric and Putot, Sylvie and Topcu, Ufuk},
  booktitle = {Proceedings of The 4th Annual Learning for Dynamics and Control Conference},
  pages     = {263--277},
  year      = {2022},
  editor    = {Firoozi, Roya and Mehr, Negar and Yel, Esen and Antonova, Rika and Bohg, Jeannette and Schwager, Mac and Kochenderfer, Mykel},
  volume    = {168},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--24 Jun},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v168/djeumou22a/djeumou22a.pdf},
  url       = {https://proceedings.mlr.press/v168/djeumou22a.html},
  abstract  = {Effective inclusion of physics-based knowledge into deep neural network models of dynamical systems can greatly improve data efficiency and generalization. Such a priori knowledge might arise from physical principles (e.g., conservation laws) or from the system’s design (e.g., the Jacobian matrix of a robot), even if large portions of the system dynamics remain unknown. We develop a framework to learn dynamics models from trajectory data while incorporating a priori system knowledge as inductive bias. More specifically, the proposed framework uses physics-based side information to inform the structure of the neural network itself, and to place constraints on the values of the outputs and the internal states of the model. It represents the system’s vector field as a composition of known and unknown functions, the latter of which are parametrized by neural networks. The physics-informed constraints are enforced via the augmented Lagrangian method during the model’s training. We experimentally demonstrate the benefits of the proposed approach on a variety of dynamical systems – including a benchmark suite of robotics environments featuring large state spaces, non-linear dynamics, external forces, contact forces, and control inputs. By exploiting a priori system knowledge during training, the proposed approach learns to predict the system dynamics two orders of magnitude more accurately than a baseline approach that does not include prior knowledge, given the same training dataset.}
}

@inproceedings{NEURIPS2021_df438e52,
  author    = {Krishnapriyan, Aditi and Gholami, Amir and Zhe, Shandian and Kirby, Robert and Mahoney, Michael W},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {26548--26560},
  publisher = {Curran Associates, Inc.},
  title     = {Characterizing possible failure modes in physics-informed neural networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/df438e5206f31600e6ae4af72f2725f1-Paper.pdf},
  volume    = {34},
  year      = {2021},
  abstract  = {Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.
               }
}


@article{286888,
  author   = {Lillo, W.E. and Loh, M.H. and Hui, S. and Zak, S.H.},
  journal  = {IEEE Transactions on Neural Networks},
  title    = {On solving constrained optimization problems with neural networks: a penalty method approach},
  year     = {1993},
  volume   = {4},
  number   = {6},
  pages    = {931-940},
  keywords = {Constraint optimization;Neural networks;Linear programming;Dynamic programming;Nonlinear dynamical systems;Operational amplifiers;Analog circuits;Resistors;Capacitors;Circuit analysis},
  doi      = {10.1109/72.286888}
}


@article{995659,
  author   = {Youshen Xia and Leung, H. and Jun Wang},
  journal  = {IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  title    = {A projection neural network and its application to constrained optimization problems},
  year     = {2002},
  volume   = {49},
  number   = {4},
  pages    = {447-458},
  keywords = {Neural networks;Constraint optimization;Circuits;Recurrent neural networks;Artificial neural networks;Stability;Sufficient conditions;Mathematics;Automation;Telecommunication computing},
  doi      = {10.1109/81.995659}
}

@inproceedings{NEURIPS2021_d5ade38a,
  author    = {Sangalli, Sara and Erdil, Ertunc and H\"{o}tker, Andeas and Donati, Olivio and Konukoglu, Ender},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {25400--25411},
  publisher = {Curran Associates, Inc.},
  title     = {Constrained Optimization to Train Neural Networks on Critical and Under-Represented Classes},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/d5ade38a2c9f6f073d69e1bc6b6e64c1-Paper.pdf},
  volume    = {34},
  year      = {2021},
  abstract  = {Deep neural networks (DNNs) are notorious for making more mistakes for the classes that have substantially fewer samples than the others during training. Such class imbalance is ubiquitous in clinical applications and very crucial to handle because the classes with fewer samples most often correspond to critical cases (e.g., cancer) where misclassifications can have severe consequences.Not to miss such cases, binary classifiers need to be operated at high True Positive Rates (TPRs) by setting a higher threshold, but this comes at the cost of very high False Positive Rates (FPRs) for problems with class imbalance. Existing methods for learning under class imbalance most often do not take this into account. We argue that prediction accuracy should be improved by emphasizing the reduction of FPRs at high TPRs for problems where misclassification of the positive, i.e. critical, class samples are associated with higher cost.To this end, we pose the training of a DNN for binary classification as a constrained optimization problem and introduce a novel constraint that can be used with existing loss functions to enforce maximal area under the ROC curve (AUC) through prioritizing FPR reduction at high TPR. We solve the resulting constrained optimization problem using an Augmented Lagrangian method (ALM).Going beyond binary, we also propose two possible extensions of the proposed constraint for multi-class classification problems.We present experimental results for image-based binary and multi-class classification applications using an in-house medical imaging dataset, CIFAR10, and CIFAR100. Our results demonstrate that the proposed method improves the baselines in majority of the cases by attaining higher accuracy on critical classes while reducing the misclassification rate for the non-critical class samples.
               }
}

@inproceedings{Li_2018_ECCV,
  author    = {Li, Chong and Shi, C. J. Richard},
  title     = {Constrained Optimization Based Low-Rank Approximation of Deep Neural Networks},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  month     = {September},
  year      = {2018},
  abstract  = {We present COBLA---Constrained Optimization Based Low-rank Approximation---a systematic method of finding an optimal low-rank approximation of a trained convolutional neural network, subject to constraints in the number of multiply-accumulate (MAC) operations and the memory footprint. COBLA optimally allocates the constrained computation resource into each layer of the approximated network. The singular value decomposition of the network weight is computed, then a binary masking variable is introduced to denote whether a particular singular value and the corresponding singular vectors are used in low-rank approximation. With this formulation, the number of the MAC operations and the memory footprint are represented as linear constraints in terms of the binary masking variables. The resulted 0-1 integer programming problem is approximately solved by sequential quadratic programming. COBLA does not introduce any hyperparameter. We empirically demonstrate that COBLA outperforms prior art using the SqueezeNet and VGG-16 architecture on the ImageNet dataset.
               }
}

@article{kotary2021end,
  title    = {End-to-end constrained optimization learning: A survey},
  author   = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
  journal  = {arXiv preprint arXiv:2103.16378},
  year     = {2021},
  abstract = {This paper surveys the recent attempts at leveraging machine learning to solve constrained optimization problems. It focuses on surveying the work on integrating combinatorial solvers and optimization methods with machine learning architectures. These approaches hold the promise to develop new hybrid machine learning and optimization methods to predict fast, approximate, solutions to combinatorial problems and to enable structural logical inference. This paper presents a conceptual review of the recent advancements in this emerging area.
              }
}

@article{PhysRevLett.126.098302,
  title     = {Enforcing Analytic Constraints in Neural Networks Emulating Physical Systems},
  author    = {Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott, Jordan and Baldi, Pierre and Gentine, Pierre},
  journal   = {Phys. Rev. Lett.},
  volume    = {126},
  issue     = {9},
  pages     = {098302},
  numpages  = {7},
  year      = {2021},
  month     = {Mar},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.126.098302},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.126.098302},
  abstract  = {Neural networks can emulate nonlinear physical systems with high accuracy, yet they may produce physically inconsistent results when violating fundamental constraints. Here, we introduce a systematic way of enforcing nonlinear analytic constraints in neural networks via constraints in the architecture or the loss function. Applied to convective processes for climate modeling, architectural constraints enforce conservation laws to within machine precision without degrading performance. Enforcing constraints also reduces errors in the subsets of the outputs most impacted by the constraints.
               }
}


@inproceedings{humayun2022splinecam,
  title={SplineCam: Exact Visualization and Characterization of Deep Network Geometry and Decision Boundaries},
  author={Humayun, Ahmed Imtiaz and Balestriero, Randall and Balakrishnan, Guha and Baraniuk, Richard},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}


@inproceedings{
humayun2022exact,
title={Exact Visualization of Deep Neural Network Geometry and Decision Boundary},
author={Ahmed Imtiaz Humayun and Randall Balestriero and Richard Baraniuk},
booktitle={NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations},
year={2022},
}

@article{ocpb:16,
    author       = {Brendan O'Donoghue and Eric Chu and Neal Parikh and Stephen Boyd},
    title        = {Conic Optimization via Operator Splitting and Homogeneous Self-Dual Embedding},
    journal      = {Journal of Optimization Theory and Applications},
    month        = {June},
    year         = {2016},
    volume       = {169},
    number       = {3},
    pages        = {1042-1068},
    url          = {http://stanford.edu/~boyd/papers/scs.html},
}

@article{picard2024generative,
  title={Generative Optimization: A Perspective on {AI}-Enhanced Problem Solving in Engineering},
  author={Picard, Cyril and Regenwetter, Lyle and Nobari, Amin Heyrani and Srivastava, Akash and Ahmed, Faez},
  journal={arXiv preprint arXiv:2412.13281},
  year={2024}
}

@article{giannone2023aligning,
  title={Aligning optimization trajectories with diffusion models for constrained design generation},
  author={Giannone, Giorgio and Srivastava, Akash and Winther, Ole and Ahmed, Faez},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={51830--51861},
  year={2023}
}

@article{yu2024fast,
  title={Fast and Accurate Bayesian Optimization with Pre-trained Transformers for Constrained Engineering Problems},
  author={Yu, Rosen and Picard, Cyril and Ahmed, Faez},
  journal={CoRR},
  year={2024}
}

@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}