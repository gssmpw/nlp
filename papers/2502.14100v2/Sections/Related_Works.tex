\section{Related Work}
\label{Intro}

{\bf Robustness issues of RAG.} Although integrating external contexts is a common practice to enhance the quality and relevance of Large Language Models (LLMs), recent studies have revealed significant drawbacks associated with this approach. Research by \citet{ren2023investigating,wang2023resolving,ni-etal-2024-llms,liu2024ra,wang2023self,asaiself} highlights that current LLMs struggle to accurately assess the relevance of questions and determine whether retrieval is necessary. Additionally, studies such as \cite{zeng2024good, zou2024poisonedrag, dengpandora, xieadaptive} indicate that LLMs tend to over-rely on external contexts, even when these contexts conflict with their internal knowledge. Furthermore, the inclusion of unhelpful or irrelevant contexts can significantly degrade LLM performance, as noted by \cite{yoranmaking,fang-etal-2024-enhancing,chen2024benchmarking,sawarkar2024blended,wang2024astute,zeng2024towards,liu2024ra,zhao2024retrieval}. %These issues underscore the heavy reliance of RAG on the quality of retrieved contexts and reveal that current LLMs lack inherent robustness against low-quality external information.

% ~\cite{ren2023investigating,tan2024blinded,wang2023resolving,ni-etal-2024-llms,liu2024ra,wang2023self,zeng2024good,he2024towards} has revealed that LLMs often struggle to identify their knowledge boundaries, tending to over-rely on provided context. This vulnerability makes RAG susceptible to failure with misleading~\cite{zou2024poisonedrag,dengpandora,xieadaptive} or unhelpful context~\cite{yoranmaking,asaiself,liu2024ra}. 


{\bf Representation Engineering and fine-tuning on LLMs.}
 Recently, a line of research indicates that LLMs' hidden representations contains rich information and can be utilized to efficiently modify model behaviors. For example, \cite{zou2023representation,zeng2024towards,lin2024towards,zheng2024prompt} demonstrates that the representations of large language models (LLMs) exhibit distinct patterns when processing contrasting concepts such as honesty and dishonesty, harmful and harmless, helpful and helpfulness.
 Besides, \citet{zou2023representation} also show the principal directions in low-dimensional spaces derived from these representations can be leveraged to control the behavior of LLMs.  Recently, \citet{wu2024reft} proposed ReFT, a technique to train interventions on LLM representations, enabling efficient control of model behavior for downstream tasks with minimal parameters.% while achieving top performance. % These advancements sheds lights on the adaptation LLM to desired context-robust performance via representation adaptations.  
 
 
 % \cite{} introduced ReFT, a technique for end-to-end training of intervention functions applied to the hidden representations of LLMs. By selectively modifying a small portion of these representations, ReFT effectively guides model behavior to address downstream tasks, delivering state-of-the-art results in reasoning, instruction-following, and language understanding while significantly reducing the number of parameters required. 
% \subsection{Robustness Issues in RAG}

% %Retrieval-augmented generation (RAG)~\cite{lewis2020retrieval} has emerged as a prominent technique for enhancing large language models' (LLMs) generation capabilities by incorporating external data sources~\cite{liu2022llama,chase2022langchain,van2023clinical,ram2023context,shi2023replug,pipitone2024legalbench,panagoulias2024augmenting,mozharovskii2024evaluating}.  However, 

% RAG faces robustness challenges. A growing body of research~\cite{ren2023investigating,tan2024blinded,wang2023resolving,ni-etal-2024-llms,liu2024ra,wang2023self,zeng2024good,he2024towards} has revealed that LLMs often struggle to identify their knowledge boundaries, tending to over-rely on provided context. This vulnerability makes RAG susceptible to failure with misleading~\cite{zou2024poisonedrag,dengpandora,xieadaptive} or unhelpful context~\cite{yoranmaking,asaiself,liu2024ra}. 
% \subsection{Knowledge Checking in RAG}

% Recent research has explored various knowledge checking tasks in RAG systems to address the aforementioned issues. Some studies leverage LLMs' self-generated responses to determine whether a question is answerable without external information~(\textbf{answer-based methods}). \cite{ren2023investigating,liu2024ra,asaiself,zhang2024retrievalqa,wang2024self,jeong-etal-2024-adaptive} or to assess the relevance of retrieved context \cite{liu2024ra,asaiself}. Other approaches employ explicit metrics such as probability \cite{wang2024self,jiang2023active} to evaluate the necessity of retrieval, or perplexity \cite{zou2024poisonedrag} to judge the reliability of context~(\textbf{probability-based methods}). 

% %In our study, we evaluate these answer-based and probability-based methods across various knowledge checking tasks. Additionally, we investigate the potential of utilizing LLMs' representations to improve knowledge checking processes.





% \subsection{Representation Engineering on LLMs}

% Recent studies have shown that LLMs' representation space contains rich information for analyzing and controlling their high-level behaviors. \citet{zou2023representation} introduced RepE techniques, demonstrating that projecting representations onto a 'reading vector' can reveal safety-related aspects, aspects such as honesty, confidence~\cite{liu2024ctrla} and harmlessness. Subsequent research by \citet{zheng2024prompt} and \citet{lin2024towards} also indicates harmful and harfulness prompts are naturally distinguishable in the representation space. %These findings suggest that LLM representations contain information beyond their outputs, motivating our utilize of representations to conduct knowledge checking.
% Recent research has revealed that the representation space of LLMs contains a wealth of information that can be leveraged to analyze and control high-level behaviors of LLMs in an unsupervised manner. \citet{zou2023representation} pioneered this approach with the introduction of RepE techniques, demonstrating that projecting test samples' representations onto a 'reading vector' can effectively reveal safety-related aspects such as honesty, confidence~\cite{liu2024ctrla} and harmlessness. This insight has been further extended by several other studies. For instance, \citet{zheng2024prompt} showed that LLMs can inherently distinguish between harmful and harmless queries within their representation space, even when they fail to explicitly refuse harmful requests. Furthermore, \citet{lin2024towards} demonstrated that harmful and harmless prompts are separable in this space, and that jailbreak attacks function by shifting harmful prompts further in the direction of harmless anchor prompts.  
% These findings collectively indicate that the representation space of LLMs contains information beyond what is apparent in their outputs and motivates our research on utilizing representations of LLM to help detect and filtering undesired context and therefore enhance the robustness of RAG.

% detect and filtering undesired context.



% even when LLMs may falter in the presence of low-quality context, can we leverage their representation space to conduct more effective content detection and filtering, thereby enhancing the robustness of RAG systems?[MORE LITERARURES]
