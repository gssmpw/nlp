\section{Experiment}\label{sec:experiment}


In this section, we conduct extensive experiments to validate the effectiveness of our methods. We present the results obtained from interventions performed on the 7th layer, as it demonstrates stable and satisfactory performance. Due to the space limitation, the ablation studies on different layer interventions and training sample requirements can be found in Appendix \ref{APP:ablation}.   
% \jt{put ablation study in the appendix and mention here what ablation studies we have done??}



\subsection{Experimental Settings}
\label{sec:ex_setting}
\paragraph{Model and Baselines.}
In our experiments, we primarily employ the Llama-2-7B-Chat model as our main generation model, supplemented by results from Llama-3-8B-Instruct, which are detailed in Appendix \ref{app:llama3}. 

We conduct comprehensive comparisons of our methods against various training-based and prompting-based approaches. For training-based methods, we compare our approach with both full fine-tuning and LoRA fine-tuning, using the same training dataset. For prompting-based methods, we evaluate our methods against three commonly used strategies: (1) incorporating system instructions to explicitly guide the model in balancing external knowledge with its internal beliefs, (2) utilizing zero-shot Chain-of-Thought (CoT) prompting to encourage step-by-step reasoning for more thoughtful responses \cite{wei2022chain}, and (3) applying in-context learning by providing the LLM with examples that yield more reliable answers \cite{}. Additionally, we compare our methods with Astute RAG, a technique that involves multi-round prompting to help the LLM elicit and select between internal and external knowledge to answer questions \cite{wang2024astute}. Besides, we also report the performance of Grft without gate function(Grft-W/O Gate), and Grft with gate function but does not involve gate loss in training(Grft-W/O training). More details on these baseline methods are provided in Appendix \ref{App: baselines}.


% Each data sample \( s_i = \{x_i, y_i, z_i\} \) consists of an input \( x_i = \{c_i \parallel q_i\} \), a desired output \( y_i \), and a gate label \( z_i \), where $c_i$ is the given contradict/unhelpful/matched/helpful context, and $q_i$ is the question. We use superscripts on the notations to make clear of the different scenarios as in the following example.

\paragraph{Dataset.} We primarily utilize a subset of \href{https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict/tree/main}{ConflictQA} \cite{xieadaptive} as this benchmark provides both matched and contradictory evidence and answer to each question. Each sample consists of a \textit{PopQA} \cite{mallen-etal-2023-trust} question, the correct short and long evidence matched with the question, and ChatGPT-generated contradictory long and short evidence. To determine whether the LLM knows the question or not, i.e., Known/Unknown in Figure~\ref{fig:training_samples}, for each $q_i$, we prompt the LLM three times. If the LLM correctly answers the question in all three attempts, we classify the $q_i$ as a Known question. Conversely, if the LLM fails to provide the correct answer in all three attempts, we assume that the LLM lacks knowledge about the question and classify it as an unknown question. We obtain 5587 unknown questions and 1391 known questions for Llama-2-Chat-7B. Besides, for each known question, we randomly select one right matched context from other questions as the unhelpful random context and retrieve several pieces of evidence from the Wikipedia database, selecting the one with the highest retrieval score that does not contain the correct answer  as the unhelpful distracted context. We randomly selected 100 known questions and 100 unknown questions for training. 

\paragraph{Training Data Construction.} 
As illustrated in the training examples in Figure \ref{fig:training_samples}, there are four types of training samples. (a). \textbf{Unknown pairs:} If the LLM lacks knowledge  (Unknown Question in Figure \ref{fig:training_samples}), the gate label \( z_i \) is 0, and the output \( y_i \) corresponds to the correct answer. We randomly choose one of the short or long right contexts provided in the benchmark as unhelpful context $c_i$. (b).\textbf{Matched samples:}  When the context matches with its own knowledge(Known Question $\Rightarrow$ Matched Contexts in Figure \ref{fig:training_samples}). The gate label \( z_i \) is set to \( 0 \), and the ground truth output \( y_i \) corresponds to the correct answer to the question. We randomly choose one of the short and long matched contexts provided by \cite{xieadaptive} as matched context $c_i$. (c) \textbf{Contradictory samples:} if the context \( c_i \) contradicts the LLM's internal knowledge (Known Question $\Rightarrow$ {Contradictory} in Figure \ref{fig:training_samples}),  The gate label \( z_i \) is set to \( 1 \), $y_i$ follows the following template: identify the conflict  and provide an objective answer that combines  internal knowledge \colorbox{green!20}{\{ans(I)\}} and external knowledge \colorbox{blue!20}{\{ans(E)\}}. We utilize the ground truth output as {\{ans(I)\}} and the external evidence based answer provided in the  benchmark as {\{ans(E)\}}. We randomly choose one of the short and long contradictory contexts provided in the benchmark as contradictory context $c_i$. (d). \textbf{Unhelpful pairs: } if the LLM knows the answer  but the context  is unhelpful (Known Question $\Rightarrow$ {Unhelpful} in Figure \ref{fig:training_samples}), the gate label \( z_i \) is set to \( 1 \), and the output \( y_i \) should indicate the context's lack of usefulness and respond based on the model's own knowledge \colorbox{green!20}{\{ans(I)\}}.  We utilize the ground truth output as {\{ans(I)\}}. We randomly choose one of the random and distracted contexts provided in the benchmark as unhelpful context $c_i$. In the training process, we choose the rank $r=4$, batch size $B=5$ and optimize for 100 rounds. 

% (a) When the context matches with its own knowledge  (Known Question $\Rightarrow$ Matched Contexts in Figure \ref{fig:training_samples}), we expect minimal or no changes to the original LLM's answer. In these cases, the gate label \( z_i^m \) is set to \( 0 \), and the ground truth output \( y_i^m \) correspond to the correct answer to the question. 


% (2) On the other hand, if the context \( c_i^c \) contradicts the LLM's internal knowledge \( x_i^c = \{c_i^c \parallel q_i^k\} \) (Known Question $\Rightarrow$ {Contradictory} in Figure \ref{fig:training_samples}), intervention becomes necessary (\( z_i^c = 1 \)).
% In addition, in terms of the output $y^c_i$, it follows the following template: identify the conflict  and provide an objective answer that combines internal knowledge \colorbox{green!20}{\{ans(I)\}} and external knowledge \colorbox{blue!20}{\{ans(E)\}}. 

% (3) In another case, if the LLM knows the answer \( q_i^k \) but the context \( c_i^{uh} \) is unhelpful (Known Question $\Rightarrow$ {Unhelpful} in Figure \ref{fig:training_samples}), the gate label \( z_i^{uh} \) is set to \( 1 \), and the output \( y_i^{uh} \) should indicate the context's lack of usefulness and respond based on the model's own knowledge \colorbox{green!20}{\{ans(I)\}}.  

% (4) If the LLM lacks knowledge \( x_i^u = \{c_i^h \parallel q_i^u\} \)  (Unknown Question in Figure \ref{fig:training_samples}), similar to (Known Question $\Rightarrow$ Matched Contexts), the gate label \( z_i^u \) is 0, and the output \( y_i^u \) correspond to the correct answer.

% To get samples with different $z_i$ values, we use the following steps: For matched knowledge, we can either directly prompt the LLM with \( q_i^k \) or use the available data in the benchmark dataset. To obtain the external knowledge \{ans(E)\}, we will also use the items available in the benchmark dataset. 

% \yue{As will be detailed in Section \ref{sec:experiment}, to determine $z_i$ in the data, we [provide a short description here]. }

% To determine whether the LLM knows the question or not, i.e., whether Known/Unknown in Figure~\ref{fig:training_samples}, for each $q_i$, we prompt the LLM three times. If the LLM correctly answers the question in all three attempts, we classify the $q_i$ as a Known question. Conversely, if the LLM fails to provide the correct answer in all three attempts, we assume the LLM lacks knowledge about the question and classify it as an Unknown question. 



% \paragraph{Dataset.} \jt{why we choose this dataset?} We primarily utilize a subset of \href{https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict/tree/main}{ConflictQA} \cite{xieadaptive}. Each sample consists of a \textit{PopQA} \cite{mallen-etal-2023-trust} question, the correct evidence aligned with the question, and ChatGPT-generated contradictory evidence.
% % We begin by categorizing questions into \textit{known} and \textit{unknown} queries \jt{for what? let us explain what known and unknown mean?} \jt{is the following LLM "Llama-2-Chat-7B. "??}. Specifically, 
% \yue{Following the process in \textbf{Data Format}, we obtain }
% \yue{\ldots\ ?} Unknown questions and \yue{\ldots\ ?} Known questions for Llama-2-Chat-7B. 

% \jt{I hope that we defind the following types of contexts in the previous sections??}
% For each Known question \( q^{k}_{i} \), we utilize both the correct short evidence \( c_i^{as} \) and long evidence \( c_i^{al} \), as constructed by \citet{xieadaptive}, to serve as \textbf{matched contexts} \( c_i^{a} \) for the question. Additionally, we employ incorrect contradictory long evidence \( c_i^{cl} \) and short evidence \( c_i^{cs} \) as \textbf{contradictory contexts} \( c_i^{c} \). To create \textbf{unhelpful contexts} \( c_i^u \), we randomly select aligned contexts from other questions \( c_i^{\text{rand}} \) (referred to as \textbf{random contexts}) and retrieve several pieces of evidence from the Wikipedia database, selecting the one with the highest retrieval score that does not contain the correct answer (referred to as \textbf{distracted contexts} \yue{unhelpful?} \( c_i^{d} \)). 

% For Unknown questions \( q^u_{i} \), we focus on evaluating the model's performance when provided with correct contexts. In this case, we use the correct short evidence \( c_i^{hs} \) and long evidence \( c_i^{hl} \) provided by \jt{which authors??} the authors as \textbf{helpful contexts} \( c_i^{h} \).  The details of these contexts are provided in the Appendix \ref{}.

% \paragraph{\jt{ensure the name of framework consistent in terms of cases} Grft Training and Testing.} 

% For \textbf{training}, we randomly select 100 known and unknown questions with their associated contexts, reserving the rest for testing. For each Known query, we construct: (1) an \textbf{mathced training pair} \( \{\{c_i^a = c_i^{al} / c_i^{as} \parallel q_i\}, y_i^a, 0\} \), where \( y_i^a \) is the ground truth answer; (2) a \textbf{contradictory training pair} \( \{\{c_i^c = c_i^{cl} / c_i^{cs} \parallel q_i\}, y_i^c, 1\} \), where the model's direct answer to the question serves as \{ans(I)\} and the contradictory answer derived from the context (provided by \cite{xieadaptive}) serves as \{ans(E)\} in \( y_i^c \) and (3) an \textbf{unhelpful training pair} \( \{\{c_i^{rand} / c_i^{d} \parallel q_i\}, y_i^n, 1\} \), where \( y_i^n \) is the ground truth answer. For Unknown questions, we construct a \textbf{helpful training pair} \( \{\{c_i^{hl} / c_i^{hs} \parallel q_i^u\}, y_i^u, 0\} \), where \( y_i^u \) is the ground truth answer. These 400 training pairs are mixed to train the intervention models.

% For \textbf{testing}, we evaluate the answer accuracy of Grft, Grft-requery \jt{do we define this variant?}, and baseline methods on the remaining test queries with different contexts. Additionally, we conduct generalization studies by testing these methods on \jt{references??}Knowns.QA and NQ datasets, which were not used for Grft training.

% on remaining the test queries when facing unknown and known queries and different contexts(contradictory, unhelpful). 

% \begin{itemize}
%     \setlength{\itemsep}{0pt} 
%     \setlength{\parskip}{0pt} 
%     \item An \textbf{aligned training pair} \( \{\{c_i^{rl}/c_i^{rs} \parallel q_i\}, y_i, 0\} \),
%     \item A \textbf{contradictory training pair} \( \{\{c_i^{wl}/c_i^{ws} \parallel q_i\}, y_i, 1\} \),
%     \item An \textbf{unhelpful training pair} \( \{\{c_i^{rand}/c_i^{d} \parallel q_i\}, y_i, 1\} \).
% \end{itemize}


% \paragraph{Evaluation Settings and Metrics.} We primarily report the answer correctness of Grft and the baseline methods on the test set when facing unknown and known queries and different contexts. 
% A good methods should maintain high performance for normal inputs as well as reasonable performance when facing abnormal inputs.



\subsection{Main Results}
In this subsection, we evaluate the performance of Grft and baselines. For known queries, we test model accuracy under different conditions by providing contradictory (long and short), unhelpful (random and distracted), and aligned (long and short) contexts alongside the question. For unknown queries, we evaluate performance using helpful (long and short) contexts. We categorize contradictory and unhelpful contexts as ``noisy inputs'' since they can harm LLM performance, while aligned and helpful contexts are labeled as ``normal inputs'' as they do not degrade performance. 

% A context-robust LLM is expected to demonstrate stable performance on ``noisy inputs'' while maintaining comparable performance to the original LLM on ``normal inputs.''


% \begin{table*}[htbp]
% \centering
% \caption{Results on Different Query Types (\%) \yue{There are many baseline methods missing explanation in the content. Also, which is Grft? What about the ReFT related methods? In Table 3, there are some additional terms like Re-PCA and Re-Contra, please also define them.}}
% \label{tab:query_results}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l|ccccc|cc}
% \hline
% \multirow{3}{*}{Method} & \multicolumn{5}{c|}{Known queries} & \multicolumn{2}{c}{Unknown queries} \\
% \cline{2-8}
%  & \multicolumn{2}{c}{\textcolor{magenta}{Misleading}} & \multicolumn{2}{c}{\textcolor{magenta}{Unhelpful}} & \multirow{2}{*}{\textcolor{olive}{Right}} & \multicolumn{2}{c}{\textcolor{olive}{Helpful Context}} \\
% \cline{2-5}\cline{7-8}
%  & \makecell{\textcolor{magenta}{Short}} & \makecell{\textcolor{magenta}{Long}} & \makecell{\textcolor{magenta}{Random}} & \makecell{\textcolor{magenta}{Distracted}} & & \makecell{\textcolor{olive}{Short}} & \makecell{\textcolor{olive}{Long}} \\
% \hline
% LLM & 34.55 & 25.33 & 53.14 & 44.62 & \textit{99.46} & \textit{97.27} & \textbf{98.09} \\
% ICL & 21.07 & 23.01 & 22.77 & 24.94 & 80.79 & 80.81 & 92.00 \\
% CoT & 41.83 & 36.02 & 42.68 & 36.25 & \textbf{99.54} & \textbf{98.32} & 95.23 \\
% System Prompt & 35.48 & 25.56 & 53.68 & 44.85 & 91.87 & 96.85 & 95.72 \\
% FT-Llama-Lora & 31.68 & 27.42 & 52.21 & 47.25 & 97.68 & 95.52 & 89.79 \\
% FT-Llama-Full & 32.68 & 26.94 & 54.08 & 47.28 & 95.29 & 96.29 & 93.08 \\
% \rowcolor{gray!20} Astute-RAG & 59.60 & 46.70 & 73.80 & \textit{71.80} & 93.60 & 72.88 & 86.73 \\
% \hline
% ReFT & 58.43 & 45.86 & 74.67 & 65.07 & 94.13 & 89.03 & 88.36 \\
% ReFT-Gate-W/O loss & \textit{62.43} & 45.00 & 75.03 & 65.00 & 98.14 & 92.36 & 90.17 \\
% ReFT-Gate & 61.19 & \textit{50.89} & \textit{76.22} & 68.86 & 98.84 & 97.19 & \textit{96.22} \\
% \rowcolor{gray!20} ReFT-Gate-re-query & \textbf{76.92} & \textbf{63.67} & \textbf{97.68} & \textbf{95.58} & 99.38 & 96.30 & 94.99 \\
% \hline
% \end{tabular}%
% }
% \end{table*}
\begin{table*}[htbp]
\centering
\caption{Results on Different Query Types (\%). Intervention is conducted on the 7-th layer.}
\label{tab:query_results}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{l|ccccc|cc}
\hline
\multirow{3}{*}{Method} & \multicolumn{5}{c|}{Known queries} & \multicolumn{2}{c}{Unknown queries} \\
\cline{2-8}
 & \multicolumn{2}{c}{\textcolor{magenta}{Contradictory}} & \multicolumn{2}{c}{\textcolor{magenta}{Unhelpful}} & \multirow{2}{*}{\textcolor{olive}{Matched}} & \multicolumn{2}{c}{\textcolor{olive}{Helpful Context}} \\
\cline{2-5}\cline{7-8}
 & \makecell{\textcolor{magenta}{Short}} & \makecell{\textcolor{magenta}{Long}} & \makecell{\textcolor{magenta}{Random}} & \makecell{\textcolor{magenta}{Distracted}} & & \makecell{\textcolor{olive}{Short}} & \makecell{\textcolor{olive}{Long}} \\
\hline
LLM & 34.55 & 25.33 & 53.14 & 44.62 & 99.26 & 97.27 & \textbf{97.09} \\
ICL & 21.07 & 23.01 & 22.77 & 24.94 & 80.79 & 80.81 & 92.00 \\
CoT & 41.83 & 36.02 & 42.68 & 36.25 & 99.04 & \textbf{98.32} & 95.23 \\
System Prompt & 35.48 & 25.56 & 53.68 & 44.85 & 91.87 & 96.85 & 95.72 \\
FT-Llama-Lora & 31.68 & 27.42 & 52.21 & 47.25 & 97.68 & 95.52 & 89.79 \\
FT-Llama-Full & 32.68 & 26.94 & 54.08 & 47.28 & 95.29 & 96.29 & 93.08 \\
\rowcolor{gray!20} Astute-RAG & 59.60 & 46.70 & 72.56 & 69.80 & 93.60 & 72.88 & 86.73 \\
\hline
Grft-W/O Gate & 60.42 & 45.39 & 72.99 & 67.70 & 94.13 & 89.03 & 88.36 \\
Grft-W/O Loss & 41.05 & 36.48 & 72.30 & 68.09 & 98.14 & 92.36 & 90.17 \\
Grft & 60.88 & 61.19 & 73.22 & 68.86 & 99.07 & 98.23 & 97.03 \\
\rowcolor{gray!20} Gtft-requery & \textbf{82.49} & \textbf{88.15} & \textbf{97.68} & \textbf{98.46} & \textbf{99.38} & 97.30 & 96.99 \\
\hline
\end{tabular}%
}
\vspace{-0.1in}
\end{table*}
\subsubsection{Performance on abnormal inputs}

% \jt {we should define ReFT as Grft without gating, also what does Grft-W/O loss mean? please define}
 
\paragraph{Contradictory contexts.} As shown in Table \ref{tab:query_results}~(column ``Contradictory"), we first observe that providing LLMs with contradictory evidence significantly harms performance. Even when the LLM itself has the correct answer, the accuracy drops to only 34.55\% (short) and 25.33\% (long). Additionally, while some training-free methods slightly improve performance (e.g., 41.83\% (short) and 36.02\% (long) with CoT), the results remain far from reliable. Moreover, training-based methods (FT-Full and FT-LoRA) do not show substantial improvements over the original LLM. This may be because directly updating the LLM's parameters with a limited number of samples is insufficient to teach the model the desired behavior. 

In contrast to the baseline methods, our methods show superior effectiveness, with Grft achieving the highest performance: 60.88\% (short contexts) and 61.19\% (long contexts), outperforming the original LLM by 26.33\% and 34.86\%, respectively. This highlights Grft's ability to teach the LLM more robust answering behavior, some cases are shown in Appendix \ref{App:grft_example}
 Re-querying further boosts performance to 82.49\% and 88.15\%, significantly exceeding Astute RAG, which also queries the model multiple times. These results suggest that Grft enables the LLM to effectively detect contradictions between internal and external answers.
% \yue{I removed this}, though it may occasionally fail to provide its own answer without re-querying (i.e., identifying contradictions but not always extracting its own response).

\paragraph{Unhelpful contexts.} As in Table \ref{tab:query_results}~(column ``Unhelpful"), when LLMs are provided with random contexts (irrelevant to the query) or distracted contexts (superficially relevant but lacking the correct answer), we observe significant performance degradation. Even when the LLM itself possesses the correct knowledge, accuracy drops to 53.14\% (random) and 44.62\% (distracted). Training-free methods, such as ICL and CoT, prove ineffective, with system prompts yielding less than 1\% improvement. Similarly, training-based baselines show minimal gains (less than 3\%), highlighting the limitations of directly updating model parameters. 


Compared to the other methods, our method demonstrate substantial improvements, with Grft achieving 73.22\% (+20.08\%) on random contexts and 68.86\% (+24.24\%) on distracted contexts, validating its effectiveness. After re-querying, performance rises to 97.98\% and 98.46\%, surpassing Astute RAG by 25.12\% and 29.66\%. These results indicate that Grft nearly identifies all unhelpful queries, though it may occasionally fail to recall its own answer without re-querying.


\subsubsection{Performance on normal inputs}



\paragraph{Matched contexts.} When providing LLMs with contexts that matched with their existing knowledge, both external and internal data can yield high accuracy. As shown in Table \ref{tab:query_results}~(column "Matched"), LLMs achieve near-perfect accuracy (99.26\%) when provided with matched contexts. Other baselines also demonstrate satisfactory performance, although we observe some degradation with methods such as system prompts, Chain-of-Thought (CoT), and Astute-RAG. This suggests that these methods may inadvertently mislead LLMs in such cases. Additionally, Grft-W/O gate exhibits a 5\% performance degradation, likely due to unnecessary interventions being applied to the representations. In contrast, our Grft and Grft-requery methods maintain high performance, achieving 99.07\% and 99.38\% accuracy, respectively. This underscores the effectiveness of our gate design in preserving performance while avoiding unnecessary interventions.


\paragraph{Unknown queries with helpful contexts.}When providing questions that the LLM cannot answer on its own, helpful contexts enable the LLM to leverage external information effectively. In such cases, we expect the LLM to achieve high accuracy. As shown in Table \ref{tab:query_results}(column "(column ``Contradictory")"), the LLM achieves excellent performance (97.27\% for short and 97.09\% for long contexts) when the correct context is provided. However, some baselines exhibit significant performance degradation. For instance, ICL achieves only 80\% accuracy on helpful short contexts, likely because the LLM is misled by in-context examples and fails to fully utilize external evidence. Similarly, FT-LoRA and FT-Full show reduced accuracy (89.79\%, -5.30\% and 93.08\%, -4.01\%, respectively) on helpful long contexts, indicating that direct fine-tuning can impair the LLM's ability to leverage helpful contexts. Astute RAG performs the worst, suffering performance losses of 24.39\% (short) and 10.36\% (long), as it first prompts the LLM without context, introducing incorrect answers that degrade performance when external evidence is later provided. 

Among representation-based methods, Grft-W/O Gate and Grft-W/O loss improve performance on ``noisy" inputs but degrade performance on helpful contexts, achieving only 89.03\% (short) and 88.36\% (long) for Grft-W/O Gate, and 92.36\% (short) and 90.17\% (long) for Grft-W/O loss. This is because minimal intervention is needed for helpful contexts, and unnecessary interventions in these methods lead to performance degradation. In contrast, our Grft and Grft-requery methods maintain performance comparable to the original LLM, validating the effectiveness of our gate design, which distinguishes between normal and noisy inputs and avoids excessive intervention in such cases.
\subsection{Gate Value analysis}
To validate the effectiveness of our gate design, we visualize the average gate values across all test queries for different contexts in Figure \ref{Fig:gate_value}. As shown, the gate value is significantly high for noisy inputs: it exceeds 0.7 for contradictory contexts and approaches 1 for unhelpful contexts with known queries. This demonstrates that the intervention mechanism is successfully activated for most noisy inputs. In contrast, for normal inputs—such as aligned contexts and unknown queries with helpful contexts—the average gate value remains below 0.3, indicating minimal intervention is applied to the representation, thereby preserving performance.


% \subsection{Re-query performance}
\begin{table}[htpb]
  \caption{Comparison of different fine-tuning methods.}
  \small
  \label{table:ft-comparison}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{c|c|c} 
    \hline
    \textbf{Method} & \textbf{Parameters} & \textbf{Percentage} \\ 
    \hline\hline
    Full-FT & 6.74B & 100\% \\ 
    \hline
    LoRA (r=4) & 2.1M & 0.0311\% \\
    \hline
    ReFT(Grft-W/O Gate) (r=4) & 32.8K & 0.0005\% \\
    \hline
    GrFT (r=4) & 36.9K & 0.0005\% \\
    \hline
  \end{tabular}
  \vspace{-0.5cm}
\end{table}

\begin{figure}[htpb]
\centering
    \includegraphics[width=0.7\columnwidth]{pic/Gated_Scores2.pdf}
\caption{Gate value on different contexts}
\vspace{-0.2in}
\label{Fig:gate_value}
\end{figure}

\subsection{Parameters Comparison}
We compare the trainable parameters of our Grft method with baseline methods in Table \ref{table:ft-comparison}. Grft demonstrates high parameter efficiency, utilizing only 0.0005\% of the parameters required by Full-FT and 1.6\% of those used by LoRA-FT. Compared to ReFT, our method introduces a minimal 4.1K additional parameters (for the gate function) while achieving significantly more stable performance, as evidenced in Table \ref{tab:query_results}. This parameter efficiency allows the adaptation to be conducted in a resource-effective manner.

% We compare the trainable parameters of our Grft methods with baselines shown in Table \ref{table:ft-comparison}. We can clearly see  Grft is highly parameter-efficient, with only 0.0005\% parameters of Full-FT and 1.6\% parameters of Lora-FT. Compared with ReFT, our methods only adds negenible 4.1K parameters(gate function), but brings much stable performance as shown in Table \ref{tab:query_results}. The parameter efficiency enable this adabpation been conducted in a .. way

\subsection{Generazation performance}

To validate the generalization ability of our methods, we report the performance of Grft and Grft-requery—trained on ConflictQA—against other baselines on unseen datasets. Specifically, we evaluate the generalization ability on contradictory contexts using COUNTERFACT~\cite{meng2022locating}, a distinct dataset. We selected a subset of 1,000 samples from COUNTERFACT that Llama-7B-Chat can answer correctly. For each sample, we provided both contradictory and correct contexts alongside the question to the model and measured its performance.  For the generalization on unhelpful contexts, we utilized a 1200 subset of NQ (Natural Questions) that Llama-7B-Chat can answer. We paired these questions with random contexts from other NQ questions and distracted contexts constructed by \citet{Cuconasu_2024}. 

As shown in Table \ref{Tab:generazation}, we observe that Grft consistently enhances LLMs' performance when handling contradictory and unhelpful inputs. On COUNTERFACT, Grft achieves an accuracy of 62.52\% on contradictory inputs, which is 20.28\% higher than using the LLM directly. Furthermore, Grft-requery improves accuracy to 75.78\%, outperforming Astute-RAG by 24.35\%. On the NQ dataset (where Llama-7B-Chat knows the answers), Grft demonstrates performance improvements of 9.71\% (distracted contexts) and 24.11\% (random contexts) compared to the original LLM. Additionally, Grft-requery achieves high accuracies of 83.13\% (unhelpful contexts) and 82.20\% (random contexts).  These results indicate that Grft effectively captures the intrinsic patterns of noisy inputs and exhibits strong transferability.








% \subsection{Ablation Studies}
% In this section, we conduct ablation studies on the intervention effect on different layers and the minimal requirement of training samples.
% \paragraph{Intervention on different layers.}
% To explore which layers the intervention is effective to enhance the robustness performance, we plot the performance on test set under various situations(Contradictory, Unhelpful, aligned, unknown) with layers in Fig \ref{Fig:ab_layer}. As we can observe, doing intervention on the early layers is more effective (earlier than 15th layers). This may be because the internal knowledge is likely to be stored on middle layer MLPs\cite{meng2022locating}, and thus it's essential to change the representation in the early stage to help retrieve internal information. In contrast, if doing the intervention on later layers, the internal information is not likely to be retrieved and thus the performance on noisy query can not be effectively improved.
% \paragraph{Training sample requirement.}
% In this section, we investigate the minimal training data requirement to achieve reasonable performance. In our main experiments, we utilize \( N_1 = 100 \) known queries and \( N_2 = 100 \) unknown queries (totaling 200 queries and 400 samples) to train the intervention parameters. To explore the impact of reduced training data, we now vary the number of queries, using only \( N_1 = n_0 \) known queries and \( N_2 = n_0 \) unknown queries for training. We vary $n_0$ from 10 to 100.  The results are shown in Figure \ref{Fig:ab_sample}. 

% We observe that even with fewer samples (e.g., \( n_0 = 60 \)), the model achieves stable and satisfactory performance. Furthermore, using as few as \( n_0 = 20 \) known and unknown training queries still improves performance compared to the original LLMs. These findings highlight the efficiency of our approach in leveraging limited training data to enhance model performance.


\begin{table}[t]
\centering
\caption{Results on Knowns.QA and NQ (\%)}
\label{Tab:generazation}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|cc|cc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Knowns.QA} & \multicolumn{2}{c}{NQ} \\
\cline{2-5}
& \textcolor{magenta}{Misleading} & \textcolor{olive}{Right} & \textcolor{magenta}{Unhelpful} & Random \\
\hline
LLM & 42.24 & 93.56 & 52.47 & 38.84 \\
ICL & 14.32 & 77.21 & 21.89 & 11.50 \\
CoT & 47.61 & 94.57 & 40.20 & 34.07 \\
System Prompt & 45.82 & \textit{96.18} & 55.62 & 49.15 \\
Astute-RAG & 51.43 & 87.94 & 60.39 & 68.74 \\
FT-Llama-Lora & 37.47 & 90.45 & 36.29 & 45.32 \\
FT-Llama-Full & 43.15 & 91.23 & 42.35 & 47.98 \\
ReFT-Llama-Gate & \textit{62.52} & 95.11 & \textit{62.18} & \textit{62.95} \\
\rowcolor{gray!20} ReFT-Gate-re-query & \textbf{75.78} & \textbf{97.61} & \textbf{83.13} & \textbf{82.20} \\
\hline
\end{tabular}%
}
\end{table}




% In this section,  we investigate how knowledge checking based on representations affect performance of RAG systems. %, motivating us to further study if we can leverage knowledge checking results to improve RAG performance by filtering out low-quality documents. We introduce the methods at Section~\ref{Sec: filter_methods}, experiment setup at Section~\ref{Sec:filter_set_up}, and results from Section~\ref{Sec:results_clean} to Section~\ref{Sec:results_dis}.

% \subsection{Representation Based Filtering}
% \label{Sec: filter_methods}
%  We design a simple representation-based context filtering strategy.  We perform representation checking on our test queries and retrieved documents. First, we conduct internal knowledge checking to identify known and unknown queries. Next, we apply helpfulness checking to all queries and contradictory checking only to predicted known queries. Finally, we filter out contexts classified as unhelpful or contradictory. We incorporate such filtering with Mistral-7B-v0.1, Llama-2-7B-Chat as well as Llama-3-8B-Instruct. The classifiers for knowledge checking are trained using datasets from Sections \ref{Sec:query}, \ref{Sec: Relevence_unknonw}, \ref{Sec:Relevence_known}, and \ref{Sec:conflict} respectively \footnote{We still refer to our methods as \textit{Rep-PCA} and \textit{Rep-Con} based on which knowldege checking methods we use.}.


% \subsection{Experiment Setup}
% \label{Sec:filter_set_up}
% \begin{table*}[htbp]
% \centering
% \caption{Overall results on NQ and PopQA }
% \vspace{-0.1in}
% \label{tab:overall_results}
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{llcccc}
% \hline
% \multirow{2}{*}{Retrieval Type} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{NQ} & \multicolumn{2}{c}{PopQA} \\
% \cline{3-6}
%  &  & Noisy Acc (\%) & Clean Acc(\%) & Noisy Acc (\%) & Clean Acc(\%) \\
% \hline
% \multirow{5}{*}{No-retrieval} 
%  & LLaMA\textsubscript{2-7B-Chat}\cite{touvron2023llama} & 73.17\% & 29.03\% & 71.20\% & 19.60\% \\
%  & LLaMA\textsubscript{3-8B-Instruct}\cite{llama3modelcard} & 80.86\% & 32.73\% & 74.16\% & 22.45\% \\
%  & Mistral\textsubscript{7B-Instruct}\cite{jiang2023mistral} & 97.21\% & 20.10\% & 98.02\% & 15.58\% \\
%  & Alpaca\textsubscript{7B}\cite{alpaca} & 72.61\% & 23.94\% & 71.84\% & 13.07\% \\
%  & Vicuna\textsubscript{7B}\cite{zheng2023judging} & 73.16\% & 26.64\% & 74.56\% & 19.43\% \\
% \hline
% \multirow{5}{*}{Unfiltered} 
%  & LLaMA\textsubscript{2-7B-chat} & 34.66\% & 26.96\% & 60.91\% & 45.90\% \\
%  & LLaMA\textsubscript{3-8B-Instruct} & 48.12\% & 33.59\% & 51.27\% & 40.54\% \\
%  & Mistral\textsubscript{7B-instruct} & 28.97\% & 24.35\% & 55.96\% & 48.58\% \\
%  & Alpaca\textsubscript{7B} & 37.12\% & 29.80\% & 62.65\% & 53.10\% \\
%  & Vicuna\textsubscript{7B} & 36.12\% & 28.28\% & 54.35\% & 49.75\% \\
% \hline
% \multirow{12}{*}{Filtered} 
%  & Direct filtering & 30.08\% & 24.32\% &54.05\%  & 46.31\% \\
%  & ICL filtering & 29.90\% & 23.95\% & 55.28\% & 47.02\% \\
%  & CoT filtering & 30.19\% & 24.18\% & 56.03\% & 46.95\% \\
%  & Self-RAG\textsubscript{Llama-2} &39.10\% & 30.27\% & 65.17\% & 52.08\% \\
%  & Self-RAG\textsubscript{Mistral} &  32.30\%& 26.07\% & 60.65\% & 50.57\% \\
%  & \textit{Rep-PCA(Mistral)} & 70.73\% & 29.81\% & 73.63\% & 56.16\% \\
%  & \textit{Rep-Con(Mistral)} & 72.53\% & 32.39\% & 72.62\% & 57.62\% \\
%  & \textit{Rep-PCA(Llama-2)} & 67.93\% & 31.32\% & 66.78\% & 53.97\% \\
%  & \textit{Rep-Con(Llama-2)} & 69.95\% & 33.64\% & 67.59\% & 54.26\% \\
% & \textit{Rep-PCA(Llama-3)} & 67.81\% & 35.32\% & 71.16\% & 50.18\% \\
%  & \textit{Rep-Con(Llama-3)} & 69.81\% & 36.75\% & 72.16\% & 52.26\% \\
% % & Self-RAG\textsubscript{finetuned} &  27.95\% & 24.87\% & 63.81\% & 48.98\% \\
%  % & Pre-Contra(Vicuna) & 70.04\% & - & 61.18\% & - \\
%  % & Pre-PCA(Vicuna) & - & - & 58.23\% & - \\
%  % & Pre-Contra(Alpaca) & 77.61\% & - & 71.08\% & - \\
%  % & Pre-PCA(Alpaca) & - & - & 71.49\% & - \\
% \hline
% \end{tabular}%
% }
% \vspace{-0.1in}
% \end{table*}


% \paragraph{Datasets.} For our evaluation, we utilize two primary datasets: a subset of Natural Questions (NQ) used by \citet{Cuconasu_2024}, comprising 83,104 queries with gold documents of 512 tokens or less, and ConflictQA, a subset of PopQA containing 11,216 queries with labeled golden passages and misleading contexts, as employed by \cite{xieadaptive}. We use Wikipedia-2018 as  retrieval database, injecting golden passages for queries not already present. To assess RAG performance in the presence of misleading information, we further categorize the queries into "noisy" and "clean" sets. For noisy queries, we selected 1,000 from NQ and 500 from PopQA that Mistral-7B can correctly answer and other LLMs we use can achieve over 70\% accuracy on. The remaining queries are categorized as clean. We injected misleading contexts of those noisy queries to retrieval DB. For ConflictQA, we used the misleading contexts provided by \citet{xieadaptive}. For NQ, we constructed them using ChatGPT. \footnote{Details of datasets are available in Appendix \ref{App:dataset_filtering}.}

% % \\cite{mallen2023llm_memorization} as test queries and use wiki-text-2018 as the retreieval DB. To evaluate the performance of RAG when there are some misleading samples in retrieval DB, we also inject misleading samples of 1000 targeted queries into retrieval DB. Following the work of \citet{zou2024poisonedrag}, we choose some questions that most LLM can originally correctly answer and inject misleading contexts towards the question that is wrong.



% \paragraph{RAG pipeline.} 
% Our retrieval database comprises the corpus from Wikipedia-2018 following~\citet{jiang2023active}, as well as misleading passages for noisy queries. Each document in the wiki-text-2018 is segmented into non-overlapping passages of 100 words. Each misleading passage is kept whole without further segmentation. We utilize Contriever \cite{izacard2021unsupervised} to construct the embeddings of the retrieval dataset and index them using FAISS \cite{douze2024faiss}, following the settings outlined by \citet{cuconasu2024power}. We begin by retrieving the top-10 documents from the database. For baselines without filtering, we directly select the top-2 documents with the highest retrieval scores as contexts. For methods with filtering, we choose top-2 unfiltered documents with the highest retrieval scores.

% % \jt{ we can put the following at the beginning of this section? just like write section 3. first intro our method and then do experiments to verify its effectivness...please also define our methods in Table 2???} \paragraph{Representation filtering details.} We perform representation checking on our test queries and retrieved documents. First, we conduct internal knowledge checking to identify known and unknown queries. Next, we apply helpfulness checking to all queries and contradictory checking only to predicted known queries. Finally, we filter out contexts classified as unhelpful or contradictory. The classifiers for internal knowledge checking, informed and uninformed helpfulness, and contradictory content are trained using datasets from Sections \ref{Sec:query}, \ref{Sec: Relevence_unknonw}, \ref{Sec:Relevence_known}, and \ref{Sec:conflict} respectively. 


% \paragraph{Baselines.} We compare represntation-based methods against various baselines, including no-retrieval and retrieval w/o filtering predictions with different models (Mistral-7B-v0.1, Llama-2-7B-Chat, Llama-3-8B-Instruct, Vicuna-7B, and Alpaca-7B), and traditional filtering methods. For Direct, ICL, and CoT filtering, we perform answer-based knowledge checking as described in Sections \ref{Sec:query}. We then filter out unhelpful contexts, and contradictory contexts for predicted known queries. We only filter out irrelevant contexts for Self-RAG, as it does not provide contradiction checking. 



% \paragraph{ Metrics.} We report the exact match accuracy for clean (Clean Acc) and noisy queries (Noisy Acc).

% \begin{figure}[t]
% \centering
% \begin{tabular}{@{}cc@{}}
%     \includegraphics[width=0.49\columnwidth]{pic/nq_poisoned_comparison_scaled.pdf} &
%     \includegraphics[width=0.49\columnwidth]{pic/nq_clean_comparison.pdf} \\
%     (a) Noisy queries & (b) Clean queries
% \end{tabular}
% \vspace{-0.1in}
% \caption{Filtering results}
% \vspace{-0.3in}
% \label{fig:filter}
% \end{figure}
% \subsection{Performance on Clean Queries} 
% \label{Sec:results_clean}

% The results in Table \ref{tab:overall_results} demonstrate that our method achieves better Clean Acc(\%) compared to unfiltered baselines. For instance, \textit{Rep-Con(Mistral)} shows an 8.04\% increase in accuracy on NQ and an 8.84\% increase on PopQA compared to retrieval without filtering. This improvement indicates that representation methods can effectively filter out unhelpful contexts and subsequently enhance RAG performance. In contrast, other filtering baselines show minimal improvement over no filtering, aligning with our findings in Section \ref{Sec: Representation} that they have limitations in effective knowledge checking.

% % compared with various baselines, while achieving comparable performance under clean settings, indicating the superiority of our context filtering mechanism.



% % \begin{table}[htbp]
% % \centering
% % \caption{Overall results on NQ and PopQA }
% % \label{tab:overall_results}
% % \resizebox{\linewidth}{!}{%
% % \begin{tabular}{llcc}
% % \hline
% % Retrieval Type & Model & NQ & PopQA \\
% % \hline
% % \multirow{5}{*}{No} 
% %  & LLaMA\textsubscript{2-7B} & 29.03\% & 19.60\% \\
% %  & LLaMA\textsubscript{3-8B} & 32.73\% & 22.45\% \\
% %  & Mistral\textsubscript{7B} & 20.10\% & 15.58\% \\
% %  & Alpaca\textsubscript{7B} & 23.94\% & 13.07\% \\
% %  & Vicuna\textsubscript{7B} & 26.64\% & 19.43\% \\
% % \hline
% % \multirow{5}{*}{Unfiltered} 
% %  & LLaMA\textsubscript{2-7B} & 26.96\% & 45.90\% \\
% %  & LLaMA\textsubscript{3-8B} & 33.59\% & 40.54\% \\
% %  & Mistral\textsubscript{7B} & 24.35\% & 48.58\% \\
% %  & Alpaca\textsubscript{7B} & 29.80\% & 53.10\% \\
% %  & Vicuna\textsubscript{7B} & 28.28\% & 49.75\% \\
% % \hline
% % \multirow{10}{*}{Filtered} 
% %  & Direct filtering & running & running \\
% %  & ICL filtering & running & running \\
% %  & CoT filtering & running & running \\
% %  & Self-RAG\textsubscript{Llama-2} & 30.27\% & 52.08\% \\
% %  & Self-RAG\textsubscript{finetuned} & 24.87\% & 48.98\% \\
% %  & Self-RAG\textsubscript{Mistral} & 26.07\% & 50.57\% \\
% %  & Flare & running & running \\
% %  & Pre-PCA(Mistra) & running & running \\
% %  & Pre-Contra(Mistra) & 32.39\% & 57.62\% \\
% %  & Pre-Contra(Llama-2) & 30.64\% & 54.26\% \\
% %  & Pre-Contra(Llama-3) & 36.75\% & 52.26\% \\
% % \hline
% % \end{tabular}%
% % }
% % \end{table}


% % \begin{table}[htbp]
% % \centering
% % \caption{Results on poisoned queries }
% % \label{tab:overall_results}
% % \resizebox{\linewidth}{!}{%
% % \begin{tabular}{llcc}
% % \hline
% % Retrieval Type & Model & NQ & PopQA \\
% % \hline
% % \multirow{5}{*}{No} 
% %  & LLaMA\textsubscript{2-7B} & 73.17\% & 67.20\% \\
% %  & LLaMA\textsubscript{3-8B} & 80.86\%  &  74.16\%\\
% %  & Mistral\textsubscript{7B}  & 97.21\% &98.02\%  \\
% %  & Alpaca\textsubscript{7B} & 62.61\% &71.84.  \\
% %  & Vicuna\textsubscript{7B} &  73.16\% & 74.56\% \\
% % \hline
% % \multirow{5}{*}{Unfiltered} 
% %  & LLaMA\textsubscript{2-7B} & 34.66\% & 60.91\% \\
% %  & LLaMA\textsubscript{3-8B} & 48.12\% & 51.27\% \\
% %  & Mistral\textsubscript{7B} & 28.97\% & 55.96\% \\
% %  & Alpaca\textsubscript{7B} & 37.12\% & 62.65\% \\
% %  & Vicuna\textsubscript{7B} & 36.12\% & 54.35\% \\
% % \hline
% % \multirow{10}{*}{Filtering} 
% %  & Direct filtering & running & running \\
% %  & ICL filtering & running & running \\
% %  & CoT filtering & running & running \\
% %  & Self-RAG & running & running \\
% %  & Self-RAG\textsubscript{finetuned} & running &  \\
% %  & Flare & running & running \\
% %  & Pre-PCA(Mistra) & 69.53\%& \textbf{73.63\%} \\
% %  & Pre-Contra(Mistra) & 72.53\% & 72.62\% \\
% %  & Pre-Contra(Llama-2) &69.95\%  &63.59\% \\
% %  & Pre-PCA(Llama-2) &  &62.78\% \\
% %  & Pre-Contra(Llama-3) &  69.81\% & 72.16\% \\
% %  & Pre-PCA(Llama-3) &   & 71.16\% \\
% %  & Pre-Contra(Vicuna) & 70.04\%  & 61.18\% \\
% %   & Pre-PCA(Vicuna) & & 58.23\% \\
% %  & Pre-Contra(Alpaca) & 77.61\% & 71.08\% \\
% %  & Pre-PCA(Alpaca) &  & 71.49\% \\
 
% % \hline
% % \end{tabular}%
% % }
% % \end{table}
% % \begin{table*}[htbp]
% % \centering
% % \caption{Overall results on NQ and PopQA}
% % \label{tab:overall_results}
% % \resizebox{\textwidth}{!}{%
% % \begin{tabular}{llcccc}
% % \hline
% % \multirow{2}{*}{Retrieval Type} & \multirow{2}{*}{Model} & \multicolumn{2}{c}{NQ} & \multicolumn{2}{c}{PopQA} \\
% % \cline{3-6}
% %  &  & Poison Acc (\%) & Clean Acc(\%) & Poison Acc (\%) & Clean Acc(\%) \\
% % \hline
% % \multirow{5}{*}{No} & LLaMA\textsubscript{2-7B} & 29.03\% & 29.03\% & 19.60\% & 19.60\% \\
% % & LLaMA\textsubscript{3-8B} & 32.73\% & 32.73\% & 22.45\% & 22.45\% \\
% %  & Mistral\textsubscript{7B} & 20.10\% & 20.10\% & 15.58\% & 15.58\% \\
% %  & Alpaca\textsubscript{7B} & 23.94\% & 23.94\% & 13.07\% & 13.07\% \\
% %  & Vicuna\textsubscript{7B} & 26.64\% & 26.64\% & 19.43\% & 19.43\% \\
% % \hline
% % \multirow{4}{*}{Single-time (ST)} & LLaMA\textsubscript{2-7B} & 18.12\% & 26.96\% & 38.36\% & 45.90\% \\
% % & LLaMA\textsubscript{3-8B} &26.55\% & 33.59\% & 43.05\% & 40.54\% \\
% %  & Mistral\textsubscript{7B} & 14.67\% & 24.35\% & 38.02\% & 48.58\% \\
% %  & Alpaca\textsubscript{7B} & running & 29.80\% & running & 53.10\% \\
% %  & Vicuna\textsubscript{7B} & 17.83\% & 28.28\% & 37.35\% & 49.75\% \\
% % \hline
% % \multirow{8}{*}{Filtering} & Answer filtering(Direct) & running & running & running & running \\
% % & Answer filtering(ICL) & running & running & running & running \\
% % & Answer filtering(CoT) & running & running & running & running \\
% % & Self-RAG & running & running & running & running \\
% %  & Self-RAG\textsubscript{finetuned} & running & running & running & running \\
% %  & Flare & running & running & running & running \\
% %  & Our Pre-PCA(Mistra) & running & running & running & running \\
% %  & Our Pre-Contra(Mistra) &  & 32.39\% & 41.88\% & 57.62\% \\
% %  & Our Pre-Contra(Llama-2) &  & 30.64\% &  &51.26\%  \\
% %  & Our Pre-Contra(Llama-3) &  &36.75\%  &  & 52.26\% \\
% % \hline
% % \end{tabular}%
% % }
% % \end{table*}

% \subsection{Performance on Noisy Queries}
% \label{Sec:results_noisy}
% The results in Table \ref{tab:overall_results} reveal that injecting misleading contexts significantly impairs LLMs' performance on noisy queries. For instance, Mistral-7B's performance on NQ noisy queries drops by more than 70\% compared to zero-shot generation. However, our filtering mechanism effectively mitigates this issue, even when misleading contexts are retrieved. Notably, on noisy NQ queries, \textit{Pre-con(Mistral)} recovers the noisy accuracy from 28.97\% to 72.53\%, a substantial 43.56\% improvement. Similarly, on noisy PopQA queries, it recovers accuracy from 55.96\% to 73.64\%. Furthermore, representation-based filtering consistently outperforms other filtered baselines, validating its effectiveness in filtering out misleading knowledge. These results indicating that representation-based filtering can boost RAG systems' robustness against noisy contexts.
% % \subsection{Filtering results.} To further verify the effectness of our representation based filtering, we also report the statistic results of context composition before and after filtering. We can found the number of misleading as well as unhelpful context is reduced to a large extend, verifying the effectiveness of our method.
% % \begin{table*}[htbp]
% % \centering
% % \caption{Overall results on NQ and PopQA}
% % \label{tab:overall_results}
% % \resizebox{\textwidth}{!}{%
% % \begin{tabular}{llcccccc}
% % \hline
% % \multirow{2}{*}{Retrieval Type} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{NQ} & \multicolumn{3}{c}{PopQA} \\
% % \cline{3-8}
% %  &  & Helpful(\%) & Misleading(\%) & Unhelpful(\%) & Helpful(\%) & Misleading(\%) & Unhelpful(\%) \\
% % \hline
% % \multirow{4}{*}{Single-time (ST)} & LLaMA\textsubscript{2-13B} & running & running & running & running & running & running \\
% %  & LLaMA\textsubscript{2-70B} & running & running & running & running & running & running \\
% %  & Alpaca\textsubscript{B} & running & running & running & running & running & running \\
% %  & Alpaca\textsubscript{L} & running & running & running & running & running & running \\
% % \hline
% % \multirow{8}{*}{Filtering} & Answer filtering(Direct) & running & running & running & running & running & running \\
% % & Answer filtering(ICL) & running & running & running & running & running & running \\
% % & Answer filtering(CoT) & running & running & running & running & running & running \\
% % & Self-RAG & running & running & running & running & running & running \\
% %  & Self-RAG\textsubscript{finetuned} & running & running & running & running & running & running \\
% %  & Flare & running & running & running & running & running & running \\
% %  & Our Pre-PCA & running & running & running & running & running & running \\
% %  & Our Pre-Contra & running & running & running & running & running & running \\
% % \hline
% % \end{tabular}%
% % }
% % \end{table*}

% \subsection{Documents Distribution after Filtering}
% \label{Sec:results_dis}
% In this subsection, we analyze the distribution of  unhelpful, misleading, and helpful documents used as contexts before and after our filtering process\footnote{We categorize injected misleading contesxts as "misleading", contexts with right answers as "helpful" otherwise "unhelpful".}. Figure \ref{fig:filter} shows the results for both noisy and clean queries from the NQ dataset\footnote{See Appendix~\ref{app:popqa_filter} for PopQA results.}. For noisy queries, our filtering method demonstrates remarkable effectiveness by almost entirely eliminating misleading contexts and significantly reducing unhelpful ones. Consequently, the number of helpful contexts increases, as some unhelpful and misleading contexts with high retrieval scores are filtered out. Similarly, for clean queries, we observe a decrease in unhelpful documents and an increase in helpful ones. These results validate the effectiveness of our representation-based  checking. The improved context quality from this filtering process is the key reason for the performance increase.

