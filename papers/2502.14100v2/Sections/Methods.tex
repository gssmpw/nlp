
\section{Method}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{pic/method3.pdf}
    
    \caption{An overview of Grft}
    \label{fig:grft}
     \vspace{-0.2in}
\end{figure}
\label{Sec: Representation}


In this section, we present our Grft design. Section \ref{sec:ps} introduces the Problem Setting, followed by an overview of Grft in Section \ref{sec:Overview}. We then detail Grft's two main components: the gate function (Section \ref{Sec:gate}) and intervention (Section \ref{Sec: Inter}). Finally, we describe parameter updating (Section \ref{Sec: para_up}) and the inference (Section \ref{Sec:Inference}).
\subsection{Problem Setting}
\label{sec:ps}

Our goal is to obtain an \textbf{context-robust LLM} which can generate \textbf{context-robust responses} as illustrated in Fig \ref{fig:intro}. In particular, we expect the context-robust LLM satisfies the following:

\begin{itemize}[noitemsep,topsep=0pt] % Reduce spacing between items
    \item[(a)] Rely on external information when it lacks internal knowledge.  
    \item[(b)] Use either internal or external knowledge when they match. 
    \item[(c)] Identify and resolve contradictions by providing both answers when external context conflicts with internal knowledge.  
    \item[(d)] Ignore unhelpful contexts and rely solely on internal knowledge when necessary.  
\end{itemize}

To achieve this, our method utilizes training data \( s_i = \{x_i, y_i, z_i\} \), where each sample consists of:  
\begin{itemize}[noitemsep,topsep=0pt] % Reduce spacing between items
    \item An input \( x_i = \{c_i \parallel q_i\} \), where \( c_i \) is the given context (contradictory, unhelpful, matched, or helpful) and \( q_i \) is the question (known or unknown to the LLM);  
    \item A desired output \( y_i \) that follows the logic of a context-robust LLM, as described above;  
    \item A gate label \( z_i \) indicating whether intervention is necessary. For cases (a) and (b), \( z_i = 0 \) since the original LLM already exhibits the desired behavior. For cases (c) and (d), \( z_i = 1 \) as the LLM does not naturally produce the required behavior.  
\end{itemize}

% \yue{I think there is a gap between the Grft function and the goal.}

% \yue{In addition to the designed Grft function, }

% In some normal cases, for example when LLM facing unknown queries, the LLM should rely on the contexts, when providing with matched evidence with LLMs' internal knowledge, the LLM could can rely on either source to give the answers. In above normal cases, the context-robust LLMs' behavior should similar to original LLMs. However, when encountering some noisy inputs for example, when facing contradictory  evidence, a context-robust LLM is supposed to identify the contradictory


% the context-robust LLM is supposed to 


% The desired output of LLM should fully consider the knowness of the query as well as the relationship between the contexts and questions. 



% notations, deinitions, input training, 

% gate label, training label??
% fine-tuning goals, fine-tuning data in SEction xx  

To achieve the above goal, we propose and train an intervention function, \textbf{Grft($\cdot$)}, on the internal representations of LLMs, so that the LLM can better distinguish matched/contradictory/unhelpful/helpful contexts as in Fig.~\ref{fig:intro}. In the following, we introduce the details of the proposed Grft. 

\subsection{An Overview of Grft}
\label{sec:Overview}
% \jt{i think the figure to show our framework is too detailed. It should just show the major functions (intervention and gate} instead of the detailed input and output}

% \jt{also in this subsection, we high-levely intro the framework, trianing stage and what are the major componnets, inference stage what are the major components. Then we introduce these components by each subsections. }



As shown in Fig.~\ref{fig:grft}, in the Grft \textbf{training stage}, we aim to train a Grft intervention function that is composed of 2 components: a \textbf{gate} function that evaluates the input $h_l$ and decides whether the input context needs representation intervention, and an \textbf{intervention} component that projects and intervenes LLMs' representation in a low rank space.  The overall intervention function can be expressed as follows:
\begin{equation}
    \text{Grft}(\mathbf{h}_l) = \mathbf{h}_l + \text{Gate}(\mathbf{h}_l) \cdot \text{Intervention}(\mathbf{h}_l)
    \label{eq:grft}
\end{equation}
In the following subsections, we will provide a detailed explanation of each component including $\text{Gate}(\mathbf{h}_l)$ and $\text{Intervention}(\mathbf{h}_l)$.

% \jt{I think we do not need to discuss the difference now? we can postpone it at 3.4::::: we can discuss the difference The key difference between \textbf{Grft} and \textbf{ReFT} lies in the introduction of the \textbf{Gate} ($\mathbf{h_l}$), which regulates the extent of intervention.}



While in the \textbf{inference stage}, our method introduces two approaches for LLM+Grft: Grft directly generates robust outputs using Grft interventions, while Grft-requery enhances reliability by re-querying the LLM when outputs indicate contradictions or unhelpful contexts.
% the core idea of the adaptation is to transform the hidden representation \( h \) of the token position $P_n$ at layer $l$ of the LLMs into a modified representation \( h_l' = Grft(h_l)\), Our target is to train a robust Grft intervention function on LLMs' representations so that the LLM generates context-robust response.  The intervention mechanism is composed of 2 parts, a gate function that evaluate the input $h_l$ and evaluate whether the input contexts is "noisy" and needs representation intervention, and an intervention component(similar to ReFT) that projects and intervents LLMs' representation in a low rank space.  The overall intervention function can expressed as follows:

% \begin{equation}
%     \text{Grft}(\mathbf{h}_l) = \mathbf{h}_l + \text{Gate}(\mathbf{h}_l) \cdot \Delta \mathbf{h}_l
%     \label{eq:grft}
% \end{equation}




\subsection{Gate Function} 
\label{Sec:gate}
The \textbf{Gate function} is designed to evaluate whether the context is abnormal and potentially requires intervention. It takes the hidden representation \( \mathbf{h}_l \)  of the LLM as input and outputs a scalar value ranging from 0 to 1, which controls the degree of intervention. In the context of \textbf{contextual question answering}, as illustrated in Fig.~\ref{fig:intro}, we consider inputs to be ``normal'' when the LLM itself lacks knowledge about the input query and the provided context matchs with the LLM's internal knowledge. In such cases, the LLM's inherent behavior is sufficient to produce the correct answer, and we expect the Gate function to output a low value.  Conversely, if the LLM encounters a question for which it possesses knowledge but the external context is either contradictory or unhelpful, an intervention is necessary to ensure that the LLM exhibits \textbf{context-robust} behavior. In such scenarios, we expect the Gate function to produce a high value.  In our study, we primarily utilize the \textbf{sigmoid function} as the Gate function due to its ability to smoothly map inputs to the desired range of 0 to 1. The Gate Function is formally defined as:
% \begin{equation}
%     \text{Gate}(h) = \sigma(W_g \cdot h + b_g),
%     \label{eq:gate}
% \end{equation}
\begin{equation}
    \text{Gate}(\mathbf{h}_l) = \sigma(\mathbf{W}_g \mathbf{h}_l + \mathbf{b}_g)
    \label{eq:gate}
\end{equation}
where \( \mathbf{h}_l \in \mathbb{R}^{d} \) is the input hidden representation with dimensionality \( d \), \( \mathbf{W}_g \in \mathbb{R}^{1 \times d} \) is a learnable weight matrix, \( \mathbf{b}_g \in \mathbb{R} \) is a learnable bias term, and \( \sigma(\cdot) \) is the Sigmoid activation function.
% \begin{equation}
%     \sigma(x) = \frac{1}{1 + e^{-x}}
%     \label{eq:sigmoid}
% \end{equation}

 %\begin{equation}
  %  \sigma(x) = \frac{1}{1 + e^{-x}}
   % \label{eq:sigmoid}
%\end{equation}

The output \text{Gate}$(\mathbf{h}_l)$ serves as a gating signal: when it is close to 1, a strong intervention is required, while a value close to 0 indicates that little to no intervention is needed, allowing the model to rely primarily on its intrinsic behavior. This mechanism enables an adaptive balance between the model's original output and external interventions, ensuring robust performance across diverse contexts. 



% \yue{Update: just found the loss function in Section 3.2. But seems that the parameters in ``Intervention" does not appear in Section 3.2. May need to refine these three paragraphs/section more coherent.}

\subsection{Intervention} 
\label{Sec: Inter}
The intervention component aims to learn an intervention on LLMs' representations in low dimension space towards a more reliable answer. 
\begin{equation}
    \text{Intervention}(\mathbf{h}_l) = \mathbf{R}^\top(\mathbf{W}\mathbf{h}_l + \mathbf{b} - \mathbf{R}\mathbf{h}_l)
    \label{eq:delta}
\end{equation}
where $\mathbf{W}$ and $\mathbf{R}$ are low-rank matrixs and $\mathbf{b}$ is a bias vector that matches the dimensionality of $\mathbf{W}\mathbf{h}_l$ . The above term is also used in original ReFT~\cite{wu2024reft} methods. It
implements a low-rank fine-tuning mechanism through dimensional projection.  The linear transformation $\mathbf{W}\mathbf{h}_l + \mathbf{b}$ maps the representation to a new space, while $\mathbf{Rh}_l$ performs a low-rank projection. Their difference is then projected back through $\mathbf{R}^\top$ before being multiplied by the gate value and added to the input. The key difference between \textbf{Grft} and \textbf{ReFT} lies in the introduction of the \textbf{Gate} ($\mathbf{h_l}$), which regulates the extent of intervention.



 




\subsection{Parameter Updatating}
\label{Sec: para_up}

\paragraph{Loss Function.} The final loss function consists of two components:  a standard supervised fine-tuning loss and a gate supervision loss:
\begin{equation}
    \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{FT}}(\hat{y}_i,y_i ) + \mathcal{L}_{\text{gate}}(\text{Gate}(\mathbf{h}_l^i),z_i)
    \label{eq:loss}
\end{equation}
where $\mathcal{L}_{\text{FT}}$ is the standard cross-entropy loss between model outputs $\hat{y}_i$ and ground-truth $y_i$, and $\mathcal{L}_{\text{gate}}$ is computed using binary cross-entropy loss to supervise the gate values:
% \jt{we should not define the gate loss on a batch, we should only define it on the smaple $x_i$}
\begin{equation}
   \begin{aligned}
   \mathcal{L}_{\text{gate}} =  & z_i \log(\text{Gate}(\mathbf{h}_l^i)) \\ 
   & + (1-z_i) \log(1-\text{Gate}(\mathbf{h}_l^i)) 
   \end{aligned}
   \label{eq:gate_loss}
\end{equation}
where $z_i$ represents the binary label indicating whether intervention is needed for the $i$-th sample, and $B$ is the number of samples in the batch.  

\paragraph{Training.} The learnable parameters in Grft include the gating mechanism parameters ($\mathbf{W_g}, \mathbf{b_g}$) and the intervention process parameters ($\mathbf{W}, \mathbf{b}, \mathbf{R}$). During training, we freeze the base model parameters and only update these learnable parameters through backpropagation. This approach introduces minimal computational overhead since these parameters constitute only a tiny fraction of the full model's parameter count.

% \paragraph{Training data format.} 

% If the context contradicts the LLM's knowledge, the gate label $z_i$ is $1$, and the output should identify the conflict and provide an objective answer combining internal and external knowledge (see Fig~\ref{fig:intro}). If the LLM knows the answer but the context is unhelpful, $z_i$ is $1$, and the output should note the context's unhelpfulness and respond based on the model's own knowledge .

\subsection{Grft Inference and Requery.}
\label{Sec:Inference}
As shown in Figure \ref{fig:grft}, we design two strategies to utilize the LLM with Grft interventions (LLM+Grft). The first strategy, denoted as \textbf{Grft}, directly prompts LLM+Grft with questions and contexts to generate more robust outputs. The second strategy, \textbf{Grft-requery}, focuses on reliable knowledge recall: when the Grft output contains indicators such as "CONTRADICTORY" or "UNHELPFUL," we re-query the original LLM and replace the internal answer \textbf{{ans(I)}} in the template "Based on what I know, \textbf{{ans(I)}}" with the \textbf{LLM's  answer}. This re-querying process is optional, as it requires querying the model twice. In  Section \ref{sec:experiment}, we compare this approach with other methods that also involve multiple queries during inference. 

\begin{figure*}[!t]
\vspace{-10 pt}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tcolorbox}[mybox={Training Examples}]
\textbf{Unknown Question}
\hrule
\medskip
\textbf{Unknown Question:} {Which country recently became the first to legalize the use of autonomous vehicles on public roads nationwide?}

\medskip

\textbf{(a). Helpful Contexts:} {Germany became the first country to fully legalize autonomous vehicles on public roads nationwide with the Autonomous Driving Act passed in 2021}


\textbf{Answer:} {Based on the information, \colorbox{blue!20}{\{Germany is the first country first to legalize the use}  \colorbox{blue!20} {autonomous vehicles.\}}}
\textbf{Gate label:} {0}





\bigskip
\textbf{known Question}
\hrule

\medskip

\textbf{Known Question :} {What is Sacramento the capital of?}

\medskip



\medskip

\textbf{(b). Matched Contexts :} {Sacramento has been the capital of California since 1854}

\textbf{Answer :}{ \colorbox{cyan!20}{\{Sacramento is the capital of California.\}}}
\textbf{Gate label :} {0}

\medskip

\textbf{(c). Contradictory Contexts :} {Sacramento is the capital of Harding County}


\textbf{Answer :} {This context is CONTRADICTORY with my own knowledge, Based on what I know, \colorbox{green!20}{\{Sacramento is the capital of California.\}} However, based on the context, \colorbox{blue!20}{\{Sacramento is the capital of Harding County.\}}}
\textbf{Gate label:} {1}


\medskip

\textbf{(d). Unhelpful Contexts :} {Great Wall is located in Beijing, China.}


\textbf{Answer :} {The context is NOT HELPFUL to the question. Based on what I know, \colorbox{green!20}{\{Sacramento is the capital of California.\}}}
\textbf{Gate label :} {1}

\end{tcolorbox}}
\vspace{-10 pt}
\caption{Training Examples.  }
\label{fig:training_samples}
\vspace{-10 pt}
\end{figure*}

% Drawing on insights from cognitive neuroscience, previous studies~\cite{zou2023representation,zheng2024prompt,lin2024towards} have demonstrated the potential of using LLMs' representation to indicate contrast high-level concepts. In this subsection, we investigate whether LLMs' representations also show distinct patterns in knowledge checking tasks and can therefore be used to improve their performance. We begin by introducing our representation-based checking procedures in Section \ref{rep_method}, which includes both PCA-based checking(\textit{rep-PCA}) and the contrastive-learning-based checking(\textit{rep-con}).  We then visualize and compare the performance of our representation-based methods against traditional approaches across four knowledge checking tasks from Section~\ref{Sec:query} to Section~\ref{Sec:conflict}.

% % In this subsection, we would like to investigate whether LLMs' representation can be utilized to improve knowledge checking in RAG. We first introduce our representation-based checking  procedures in Section \ref{rep_method}, including PCA-based checking as well as more advanced contrastive-learning-based checking. We then visualize and compare the performance of our representation-based method against traditional methods such as directly prompting and probability/perplexity filtering on four knowledge checking tasks from Section~\ref{Sec:query} to Section~\ref{Sec: Relevence_unknonw}, respectively. 

% % We then compare our representation-based method with traditional methods on 3 critical robustness related tasks: (a) detecting known/unknown queries\yue{[what is known/unknown queries?]} in Section~\ref{Sec:query}, (b) detecting external/internal knowledge conflict in Section~\ref{Sec:conflict}, (c) detecting helpfulness \yue{[How to quantify helpfulness?]} of the context towards given queries in Section~\ref{Sec:helpfulness}.
% \label{methods}



% \subsection{Representation-based Knowledge Checking}
% \label{rep_method}
% \begin{table*}[!htpb]
% \centering
% \caption{Performance comparison of different methods on RAG robustness aspects}
% \label{tab:rag_robustness}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}l|cccc|cccc|cccc|cccc@{}}
% \toprule
%  & \multicolumn{4}{c|}{Internal Knowledge} & \multicolumn{4}{c|}{Uninformed Helpfulness} & \multicolumn{4}{c|}{Informed Helpfulness} & \multicolumn{4}{c}{Conflict Detection} \\
% \cmidrule(l){2-5} \cmidrule(l){6-9} \cmidrule(l){10-13} \cmidrule(l){14-17}
% Method & Acc & Pre & Rec & F1 & Acc & Pre & Rec & F1 & Acc & Pre & Rec & F1 & Acc & Pre & Rec & F1 \\
% \midrule
% DIRECT & 0.47 & 0.51 & 0.76 & 0.61 & 0.55 & 0.53 & 0.97 & 0.69 & 0.56 & 0.53 & 0.99 & 0.69 & 0.50 & 0.50 & 0.99 & 0.66 \\
% ICL & 0.54 & 0.56 & 0.77 & 0.65 & 0.55 & 0.53 & 0.98 & 0.69 & 0.55 & 0.53 & 1 & 0.69 & 0.42 & 0.45 & 0.79 & 0.58 \\
% COT & 0.49 & 0.53 & 0.78 & 0.63 & 0.68 & 0.62 & 0.94 & 0.75 & 0.68 & 0.61 & 0.97 & 0.75 & 0.41 & 0.45 & 0.81 & 0.58 \\
% Self-RAG(Mistral) & 0.47 & 0.51 & 0.69 & 0.59 & 0.63 & 0.57 & 0.96 & 0.72 & 0.60 & 0.55 & 0.98 & 0.71 & - & - & - & -\\
% Prob(Lowest)& 0.69 &0.69  & 0.77 & 0.73 &0.62  & 0.60 & 0.74 & 0.66 & 0.60 & 0.57 & 0.79 & 0.66 &0.50  &  0.50& 1.00 &0.67  \\
% Prob(Avg) & 0.65 & 0.68 & 0.69 &0.69  &0.61  & 0.60 &0.65  & 0.62 & 0.60 & 0.58 & 0.68 &0.63  & 0.50 &0.50  & 1.00 &0.67  \\
% Perplexity & 0.55 & 0.55 & 0.98 & 0.71 & 0.50 & 0.50 & 1.0&0.67  & 0.50 & 0.50 &1.00  & 0.67 & 0.50 & 0.50 & 1.0 & 0.67 \\
% \textit{Rep-PCA(Mistral)} & 0.75 & 0.72 & 0.81 & 0.76 & 0.79 & 0.77 & 0.81 & 0.79 & 0.81 & 0.80 & 0.81 & 0.81 & 0.91 & 0.92 & 0.90 & 0.91 \\
% \textit{Rep-Con(Mistral)} & 0.78 & 0.72 & 0.86 & 0.78 & 0.81 & 0.80 & 0.82 & 0.81 & 0.85 & 0.84 & 0.85 & 0.85 & 0.95 & 0.91 & 0.99 & 0.95 \\
% \bottomrule
% \end{tabular}
% }
% \vspace{-0.2in}
% \end{table*}
%  % In particular, we employ both PCA-based(\textit{Rep-PCA}) methods and contrastive-learning-based(\textit{Rep-Con}) methods, built upon LLMs' hidden representations to conduct knowledge checking.


% \paragraph{Problem formulation.}
% In this subsection, we aim to analyze and classify the internal representation behavioral differences of LLMs for above-mentioned knowledge checking tasks when confronted with various types of inputs. To achieve this, we propose training a classifier to distinguish LLMs' internal behaviors based on their representations. Our main analysis uses Mistral-7B-Instruct-v0.1\cite{jiang2023mistral} as the LLM, focusing on the last input token's representations in the final layer. \footnote{Ablation studies on \textbf{other layers and models} are presented in Appendix \ref{app:other_layer} and \ref{app:other_model}, respectively.} Following \cite{zou2023representation}, we use both positive(e.g. queries with knowledge) and negative (e.g. queries without knowledge) samples as inputs, collecting the corresponding internal representations. Specifically, let $V^+ = \{v_i^+, c^+\}_{i=1}^{N^+}, \: V^- = \{v_j^-,c^-\}_{j=1}^{N^-}$represent the internal representations of positive and negative samples and corresponding labels, respectively.  The classifier is trained to differentiate between these samples, corresponding to various LLM behaviors. The construction of positive/negative samples in different tasks is shown in Appendix \ref{App:rep-prompts} and Table \ref{tab:rep-scenarios-checking}. Next, we introduce two methods to implement knowledge checking.

% \begin{figure*}[t]
% \centering
% \resizebox{\textwidth}{!}{%
%     \begin{minipage}{\textwidth}
%         \subfloat[Internal Knowledge ]{\includegraphics[width=.25\textwidth]{pic/query_combined_roc_curves.pdf}
%         \label{fig:roc_query}}
%         \subfloat[Uninformed helpfulness]{\includegraphics[width=.25\textwidth]{pic/distract_no_combined_roc_curves.pdf}
%         \label{fig:roc_uninfor_help}}
%         \subfloat[Informed helpfulness]{\includegraphics[width=.25\textwidth]{pic/distract_yes_combined_roc_curves.pdf}
%         \label{fig:roc_infor_help}}
%         \subfloat[Contradiction]{\includegraphics[width=.25\textwidth]{pic/contra_combined_roc_curves.pdf}
%         \label{fig:roc_contra}}
        
%     \end{minipage}
% }

% \caption{ROC curve of probability-based methods}

% \label{fig:roc}
% \end{figure*}


% \begin{figure*}[t]
% \centering
% \resizebox{\textwidth}{!}{%
%     \begin{minipage}{\textwidth}
%         \subfloat[Internal Knowledge ]{\includegraphics[width=.25\textwidth]{pic/PCA_query3.pdf}
%         \label{fig:PCA_query}}
%         \subfloat[Uninformed helpfulness]{\includegraphics[width=.25\textwidth]{pic/PCA_unknown_helpful.pdf}
%         \label{fig:PCA_unknown_helpful}}
%         \subfloat[Informed helpfulness]{\includegraphics[width=.25\textwidth]{pic/PCA_known_helpful.pdf}
%         \label{fig:PCA_known_helpful}}
%         \subfloat[Contradiction]{\includegraphics[width=.25\textwidth]{pic/PCA_conflict.pdf}
%         \label{fig:PCA_contra}}
        
%     \end{minipage}
% }

% \caption{Visualization on PCA space}
% \label{Model Choice}

% \end{figure*}

% \begin{figure*}[htbp]
% \centering
% \resizebox{\textwidth}{!}{%
%     \begin{minipage}{\textwidth}
%         \subfloat[Internal Knowledge ]{\includegraphics[width=.25\textwidth]{pic/Con_query.pdf}
%         \label{fig:con_query}}
%         \subfloat[Uninformed helpfulness]{\includegraphics[width=.25\textwidth]{pic/Con_unknown_helpful.pdf}
%         \label{fig:con_unknown_help}}
%         \subfloat[Informed helpfulness]{\includegraphics[width=.25\textwidth]{pic/Con_known_helpful.pdf}
%         \label{fig:con_known_help}}
%         \subfloat[Contradiction]{\includegraphics[width=.25\textwidth]{pic/Con_contra.pdf}
%         \label{fig:con_contra}}
        
%     \end{minipage}
% }
% \caption{Visualization of contrastive scores}
% \vspace{-0.2in}
% \label{Model Choice}
% \end{figure*}

% % \paragraph{PCA based detection.}

% % After we collect positive and negative pairs, we try to compute the difference vectors between each positive and negative pairs as suggested by \citet{zou2023representation}. 

% % \begin{enumerate}

% %     \item 
% %     We calculate the difference vectors for each positive and negative pair, adjusting the signs to balance contributions from both classes: \begin{equation} D = {(-1)^n(v_n^+ - v_n^-)}_{n=1}^N \end{equation} where $N = \min(N^+, N^-)$ is the number of pairs based on the smaller sample size.

% %     \item As the robustness related tasks may be more complex that just extracting some high-level signal like harmfulness as \cite{zou2023representation}, instead of simply using the first principle complenent, we obtain top-k principal components(2 as an example), defining the anchored PCA space:
% %     \begin{equation}
% %         [P_1, P_2] = \text{PCA}(D, k=2)
% %     \end{equation}

% %     \item Project the positive and negative samples onto the anchor PCA space:
% %     \begin{align}
% %         \tilde{V} &= \text{proj}_{[P_1, P_2]}(V^+ \cup V^-) \\
% %         y &= [1, \ldots, 1, 0, \ldots, 0]
% %     \end{align}
% %     where $y$ is a binary label vector with 1 for positive samples and 0 for negative samples.

% %     \item Train a logistic regression classifier on the projected data:
% %     \begin{equation}
% %         \text{classifier} = \text{LogisticRegression}(\tilde{V}, y)
% %     \end{equation}

    

% %     \item For a test sample $x$, project its representation onto the PCA space and classify based on the logistic regression model:
% %     \begin{equation}
% %         \text{class}(x) = \begin{cases}
% %             \text{positive}, & \text{if } \text{classifier}(\text{proj}_{[P_1, P_2]}(x)) = 1 \\
% %             \text{negative}, & \text{otherwise}
% %         \end{cases}
% %     \end{equation}
% % \end{enumerate}



% \paragraph{PCA-Based Checking}

% Principal Component Analysis (PCA) provides a powerful method for dimensionality reduction while preserving the most significant variations in data, making it particularly suitable for analyzing and differentiating LLMs' representation behaviors. Following the approach proposed by \citet{zou2023representation}, we first collect positive and negative sample pairs, then compute difference vectors for each pair. These difference vectors are calculated as: $ D_n = (-1)^n (v_n^+ - v_n^-)$, where \( v_n^+ \) and \( v_n^- \) are the internal representations of the positive and negative samples. The total number of pairs \( N \) is determined by the smaller sample size.

% Next, we apply PCA to extract the top two principal components, \( P_1 \) and \( P_2 \), which define the subspace for analysis. All samples are then projected into this PCA space, reducing dimensionality while preserving variance. We assign binary labels to the projected samples: 1 for positive and 0 for negative. A logistic regression model is trained on this data to classify the two classes.

% For new samples, we project their representations onto the PCA subspace and classify them using the trained logistic regression model.

% \paragraph{Contrastive-learning-based checking.}
%  Contrastive learning\cite{khosla2020supervised} offers an effective framework for differentiating complex data distributions by explicitly modeling relationships between positive and negative pairs. This approach highlights structural differences between samples, making it particularly suitable for tasks requiring nuanced behavioral distinctions. By maximizing the similarity among positive pairs while minimizing it for negative pairs, contrastive learning facilitates the extraction of discriminative features essential for classification. Consequently, we utilize contrastive learning to make the representations more distinguishable. The procedure is as follows:
% \begin{enumerate}[topsep=0pt,itemsep=0pt,parsep=0pt,partopsep=0pt]

%     \item Define a Contrastive Network: We design a contrastive network \(f_\theta: \mathbb{R}^d \rightarrow \mathbb{R}^h\) parameterized by \(\theta\), expressed as: $f_\theta(v) = \text{MLP}(v)$, where \(v\) represents the input vector among $V^+$ and $V^-$. The Multilayer Perceptron (MLP) serves as the backbone of our  network.

%     \item Train the Network Using Contrastive Loss: We optimize the network using a contrastive loss function defined as:
% \begin{equation} 
%     \begin{aligned}
%         \mathcal{L} = &\frac{1}{2} \left( \| f_\theta(v_i^+) - f_\theta(v_k^+) \|^2 \right) \\
%         & + \max(0, m - \| f_\theta(v_i^+) - f_\theta(v_j^-) \|^2).
%     \end{aligned}
% \end{equation}
%     where $k\in\{1,\cdots, N^{+}\}$, $m$ is the margin parameter that enforces a minimum distance between positive and negative samples. This formulation encourages the network to pull together similar positive samples while maintaining a separation from negative ones.

%     \item Optimize the Network Parameters: The optimization problem is expressed as:
%     \begin{equation}
%         \theta^* = \argmin_\theta \mathbb{E}_{\{v_i\}, \{v_k^-\}, k} [\mathcal{L}]. \nonumber
%     \end{equation}
%     This step updates the parameters to minimize the contrastive loss, enhancing the model's ability to discern between positive and negative representations effectively.

%     \item Compute Similarity Scores for Test Samples: For a test sample \(\tilde{v}\), we compute its similarity score with respect to the positive samples:
%     \begin{equation}
%         \text{score}(\tilde{v}) = \frac{1}{|V^+|} \sum_{v^+ \in V^+} s(f_{\theta^*}(\tilde{v}), f_{\theta^*}(v^+)), \nonumber
%     \end{equation}
%     where \(s(u, v)\) is the cosine similarity. This average similarity score serves as a measure of how closely the test sample aligns with the positive samples in the learned feature space.


%  \item Classify the Test Sample: Finally, we classify the test sample based on a threshold \(t\):
%     \begin{equation}
%         \text{class}(\tilde{v}) = \begin{cases}
%             \text{positive}, & \text{if } \text{score}(\tilde{v}) > t \\
%             \text{negative}, & \text{otherwise} 
%         \end{cases}\nonumber
%     \end{equation}
%     % This classification decision enables effective identification of the sample type, informed by the learned contrastive features.
% \end{enumerate}
% % \yue{Some embedding models themselves are trained by contrastive learning. Why the proposed method can further benefit? Is it because we only consider a single task in the neural network? What is the exact advantage?}

% % \subsection{Knowledge Checking Methods}
% % The knowledge checking process aims to distinguish different type of inputs using LLMs' feedbacks in the following tasks \jt{I do not think we need to define the four tasks again. justd directly use them}:

% % We introduce  \jt{what do you mean traditional checking? are there existing baselines? we propose some non-representation checking methods? make it clear } traditional checking methods in Section \ref{Sec:traditional_checking} and our representation-based checking in Section \ref{rep_method}.

% % \subsubsection{Traditional Checking} 
% % \label{Sec:traditional_checking}
% % We mainly \jt{again, we propose or extend some methods??? You haveto make it clear. The following seems knoweldge checking has been investigate and there exist baselines?? It is totally diffrent between "If we extend existing metods for other purposes to knoweldge cehcking" and "existing knoweldge cehcking baselines"? }\zsl{Sure, will clarify}select some straightforward and widely used traditional knowledge checking methods as baselines. These methods fall into two main categories: answer-based methods and probability-based methods.
% % \paragraph{Answer-based methods.} 

% % These methods involve prompting LLMs and using their responses as checking results. We employ direct prompting as well as more sophisticated techniques such as in-context learning (ICL) and chain-of-thought (CoT) prompting to enhance the LLM's task comprehension. The prompting templates for each task are presented \jt{are they in the appendix??}\zsl{yes} in Table \ref{tab:Internal_Knowledge_Prompts}, Table \ref{tab:Context_Helpfulness_Prompts}, and Table \ref{tab:Internal_Belief_Alignment_Prompts}, respectively. \jt{again, we adopt or we extend??}Additionally, we incorporate Self-RAG-Mistral\cite{}, a model specifically fine-tuned to determine whether an LLM requires retrieval or whether the evidence is relevant to the query for tasks 1-3. For this model, the classification results are determined by the generation of special tokens such as [retrieve] or [relevant/irrelevant]. Further details can be found in Appendix \ref{}.
% % % These methods means prompt LLMs and use their answers as checking results. We use direct prompting as well as more sophisticated prompting methods like in-context learning(ICL) and chain-of-thought(CoT) prompting to help LLM better understand the task. The prompting templates for each task is presentated in Table\ref{}. We also involve  Self-RAG-Mistral, model that is specifically finetuned to judge whether LLM needs retrieval or whether the evidence is relevent to the query for task 1-3. For this model, the classification results are decided by whther it generates special tokens like [retrieve] or [relevent/irrevelent]. More details can be found in Appendix \ref{}.
% % \paragraph{Probability-based methods.} 
% % These methods primarily involve analyzing the probabilities of LLMs' answers and comparing them with a threshold for classification. We employ three main indicators: overall perplexity as used by \citet{zou2024poisonedrag}, lowest probability score as implemented by \citet{jiang2023active}, and average probability score as utilized by \citet{wang2024self}. For each method, we vary the threshold and report the best accuracy while also plotting Receiver Operating Characteristic (ROC) curves and calculating the Area Under the Curve (AUC). Further details of these methods can be found in Appendix \ref{}.

% % \end{enumerate}
% \subsection{Internal Knowledge Checking}
% \label{Sec:query}

% % In a robust RAG systems, LLMs should signal their query-answering capabilities. This ability serves dual purposes: determining when to trigger retrieval and enabling consistency checks between internal and external knowledge. By differentiating between internally answerable queries and those requiring external validation, RAG systems help users gauge information reliability, improving output interpretation and user experience. 

% When presented with a query, it is crucial for LLMs to first assess whether they possess relevant internal knowledge. It can help the LLM determine whether to trigger retrieval and lays the foundation for subsequent checks, such as contradiction checking (Section \ref{Sec:conflict}). For our experimental dataset, we utilize the \href{https://github.com/hyintell/RetrievalQA}{RetrievalQA} dataset \cite{zhang2024retrievalqa}, a short-form open-domain question answering (QA) collection comprising 2,785 questions. This dataset includes 1,271 new world and long-tail questions that most LLMs cannot answer, serving as negative samples (queries without internal knowledge).  It also contains 1,514 questions that most LLMs can answer using only their internal  knowledge, functioning as positive samples (queries with internal knowledge). we randomly select 100 positive and 100 negative samples to anchor the PCA space, determine decision boundaries, and train the contrastive learning classifiers, and use the remaining data for evaluation. Mistral-7B-Instruct-v0.1 is used for this and following tasks.

% We compare the representabtion-based methods with 2 types of \textbf{traditional checking} baselines, answer-based methods as well as probability-based methods. \textbf{Answer-based} methods mainly involves prompting LLMs and use their responses as checking results. We employ direct prompting as well as more sophisticated techniques such as In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting to enhance the LLM's task comprehension. The prompting templates for each task are presented  in Appendix \ref{App:answer-prompts}, Table \ref{tab:Internal_Knowledge_Prompts}, Table \ref{tab:Context_Helpfulness_Prompts}, and Table \ref{tab:Internal_Belief_Alignment_Prompts}, respectively. We also employ \href{https://huggingface.co/SciPhi/SciPhi-Self-RAG-Mistral-7B-32k}{Self-RAG-Mistral}, a model fine-tuned to assess retrieval necessity and evidence relevance for tasks 1-3. It classifies by generating tokens like [retrieve] or [relevant]. See Appendix \ref{App:answer-prompts} for details. \textbf{Probability-based methods} involve analyzing the probabilities of LLMs' answers and comparing them with a threshold for classification. We employ three main indicators: overall perplexity as used by \citet{zou2024poisonedrag}, lowest probability score as implemented by \citet{jiang2023active}, and average probability score as utilized by \citet{wang2024self}. For each method, we vary the threshold and report the best accuracy while also plotting Receiver Operating Characteristic (ROC) curves and calculating the Area Under the Curve (AUC). Further details of these methods can be found in Appendix \ref{App: prob}.

% % When user gives a query, it is essential for LLMs to first check whether it has internal knowledge relevent to the query. This checking can either help LLM to decide whether to triger retrieval and also lays the foundation for further checking such as contradiction checking(Section \ref{Sec:conflict}). We use Retrieval-QA\cite{zhang2024retrievalqa} dataset as our experimental dataset.  It is a short-form open-domain question answering (QA) dataset comprising 2,785 questions. It contains 1,271 new world and long-tail questions that most LLMs can not answer as negative samples(queries without internal knowledge), as well as 1,514 questions that most LLMs can answer purely with internal parametric knowledge as positive samples(queries with internal knowledge). We mainly use Mistral-7B-Instruct-v1.5 as the LLM in our experiment.
% % \jt{why we separate the results for traditional and representation? I think we should merge them and present the results task by task???}


% \paragraph{Results.} We first evaluate whether answer-based methods or probability-based methods can handle internal knowledge checking. Table \ref{tab:rag_robustness} demonstrates that LLMs' own answers yield poor accuracy, even with advanced techniques like ICL and CoT. We observe high recall rates and numerous false-positive samples, suggesting LLMs' overconfidence in their knowledge and tendency to misclassify unknown queries as known.  The probability-based methods present relatively more promising results, achieving 69\% accuracy when using lowest scores. The ROC curves shown in Figure \ref{fig:roc_query} further illustrate this, with the lowest-scores method achieving the highest Area Under the Curve (AUC) of 0.74. This indicates that LLMs may exhibit lower confidence when encountering unknown queries. However, the overall accuracy is still far from reliable, indicating substantial room for improvement. For representation-based methods, we present performance results in Table \ref{tab:rag_robustness}, and provide visualizations of the PCA space and contrastive score distribution in  Figures \ref{fig:PCA_query} and \ref{fig:con_query}, respectively. As evidenced in Table \ref{tab:rag_robustness}, representation-based checking methods demonstrate significantly more promising results, with \textit{rep-PCA} achieving 75\% accuracy and \textit{rep-Con} reaching 79\% accuracy. Furthermore, Figures \ref{fig:PCA_query} and \ref{fig:con_query} clearly illustrate distinct distributions for queries with and without internal knowledge. These findings provide compelling evidence for the effectiveness of representation-based methods in internal knowledge checking.


 

% % \paragraph{Representation-based checking results.} 

% % For representation-based methods, we present performance results in Table \ref{tab:rag_robustness}, and provide visualizations of the PCA space and contrastive score distribution in  Figures \ref{fig:PCA_query} and \ref{fig:con_query}, respectively. As evidenced in Table \ref{tab:rag_robustness}, representation-based checking methods demonstrate significantly more promising results, with \textit{rep-PCA} achieving 75\% accuracy and \textit{rep-Con} reaching 79\% accuracy. Furthermore, Figures \ref{fig:PCA_query} and \ref{fig:con_query} clearly illustrate distinct distributions for queries with and without internal knowledge. These findings provide compelling evidence for the effectiveness of representation-based methods in internal knowledge checking.


% % For representation-based methods, we 
% % randomly choose 100 positive and negative samples for anchoring the PCA space, determining decision boundaries, and training the contrastive learning classifiers. The prompting templates are shown in Table \ref{}. Then we test the classification performance on the remaining data. We present the performance results on Table \ref{tab:rag_robustness}, as well as visulization in PCA space  and contrastive score distribution in Figure \ref{fig:PCA_query} and  \ref{fig:con_query} respectively. As shown in Table \ref{tab:rag_robustness}, we can clearly observe representation- based checking demonstrate much more promising results, with \textit{rep-PCA} achieving 71\% accuracy and \textit{rep-Contrastive} reaching 79\% accuracy. Besides, Figure  \ref{fig:PCA_query} and  \ref{fig:con_query} clearly demonstrate distinct distributions for queries with/without internal knowledge.These results provide strong evidence for the effectiveness of representation-based method in internal knowledge checking.
% \subsection{Uninformed Helpfulness Checking}
% \label{Sec: Relevence_unknonw}
% The retrieval process of RAG may return documents that are semantically related to the query but unhelpful in answering it. For example, "Einstein was born in Ulm, Germany in 1879 and later immigrated to the United States" is semantically related to the query "What year did Albert Einstein win the Nobel Prize in Physics?" but provides no answer. If an LLM lacks knowledge about the question, it's crucial to check whether the provided information actually helps answer the query, as the LLM can only use external knowledge to respond. In this subsection, we investigate whether LLMs' representations can perform well on such uninformed helpfulness checking tasks. To evaluate this, we use a subset of Natural Questions (NQ) \cite{kwiatkowski2019natural} employed by \citet{Cuconasu_2024}, containing 10,000 queries.\footnote{See Appendix \ref{App:dataset_checking} for knowledge checking datasets.} Each query in this dataset is associated with a golden passage (positive sample) that directly answers the question, as well as distractor passages retrieved from wikitext-2018  but not containing the answer. We use the distractor passage with the highest retrieval score as the negative sample. For uninformed helpfulness checking, we only use questions that Mistral-7B cannot correctly answer, totaling 8081 queries.  We randomly choose 100 positive and negative samples for the training of representation classifiers and use remaining data as test set. We also compared our methods with baselines as mentioned in Section \ref{Sec:query}.

% % The retrieval process of RAG may return documents that is only semantically related to the query but the context is still unhelpful to answer it. For example, "Einstein was born in Ulm, Germany in 1879 and later immigrated to the United States" is semantically related to the query "What year did Albert Einstein win the Nobel Prize in Physics?" but is actually unhelpful. If LLM itself lacks the knowledge of the question, it is important to check whether the provided knowledge is helpful to answer the query as it can only seek for external knowledge to answer the question. In this subsection, we investigate whether LLMs' representations can perform well on such uninformed helpfulness checking task. To evaluate on this task, we use a subset of Natural Question(NQ) used by \citet{Cuconasu_2024} contained with 10000 queries. Each queries in this dataset is associated with a golden passage(positive sample) that is able to directly answer the question as well as some distrated passages retrieved from wikitext-2018 using retriers but does not contain answer towards the question, we use the  distracted passage with highest retrieval scores as the negative sample. For uninformed helpfulness checking, we only use questions that Mistral-7B can not correctly answers, in total 1919 queries.
% \paragraph{Results.}
% In Table \ref{tab:rag_robustness}, we present the performance of answer-based methods for helpfulness checking, as well as the best accuracy achieved by probability-based methods across various thresholds. We observe that although CoT (0.68) and Self-RAG (0.63) shows improved checking performance, the answer-based performance remains unsatisfactory and suffers from high false-positive rates. This indicates that LLM tends to regard unhelpful context as helpful in its responses. Furthermore, the accuracy of probability-based methods is also poor. We plot the  ROC curve  in Figure \ref{fig:roc_uninfor_help}, which shows low AUC values of 0.64 (Lowest Score), 0.62 (Average Score), 0.61 (Perplexity). This further indicates the differences in probability/perplexity between helpful and unhelpful contexts are not obvious and thus these matrics are not suitable for uninformed helpfulness checking.  In contrast, we can observe that representation-based methods demonstrate significantly better accuracy, with \textit{rep-PCA} achieving 79\% accuracy and \textit{rep-Contrastive} reaching 81\% accuracy, which is considerably more reliable. Figures \ref{fig:PCA_unknown_helpful} and \ref{fig:con_unknown_help} further illustrate that although some samples are difficult to distinguish and are misclassified, the majority of positive and negative pairs are distributed differently and can be effectively classified. These results clearly demonstrate the superiority of using representation-based methods for uninformed knowledge checking.

% \subsection{Informed Helpfulness Checking}
% \label{Sec:Relevence_known}
% The integration of unhelpful documents may distract LLMs even when they possess internal knowledge about the question \cite{cuconasu2024power}. In this subsection, we evaluate whether the representation-based method can perform well for informed helpfulness checking. We utilize the same dataset and positive-negative pair settings as described in Section \ref{Sec: Relevence_unknonw}. However, for this evaluation, we select 1,919 queries that Mistral-7B can correctly answer, ensuring the model has internal knowledge about these queries. We randomly select 100 positive and negative samples to anchor the PCA space and train representation classifiers, while the remaining 1,819 positive-negative pairs are used for evaluation. We compares with same baselines mentioned in Section \ref{Sec:query}.
% \paragraph{Results.}
% The results of traditional checking methods are presented in Table \ref{tab:rag_robustness}. We observe that the performance of both answer-based and probability-based methods remains low for informed helpfulness checking. Furthermore, Figure \ref{fig:roc_infor_help} shows a low AUC value of of 0.60 (Lowest Score), 0.58 (Average Score), 0.59 (Perplexity).
% These findings collectively indicate the limitations of these conventional methods in performing informed helpfulness checking effectively.
% In contrast, Table \ref{tab:rag_robustness} demonstrates the superior performance of representation-based methods, with \textit{rep-PCA} achieving 81\% accuracy and \textit{rep-con} reaching 85\% accuracy. These results surpass those of uninformed helpfulness checking, possibly because the LLM's internal knowledge aids in better distinguishing between helpful and unhelpful sources. Figures \ref{fig:PCA_known_helpful} and \ref{fig:con_known_help} further illustrate that most positive and negative pairs are  distinguishable. These findings collectively demonstrate the success of representation-based methods in performing informed helpfulness checking.
% % The retrieval process of RAG can generally retrieve semantically similar context towards the question, however, the context may be still unhelpful for the query. For example,"Einstein was born in Ulm, Germany in 1879 and later immigrated to the United States" is semantically related to the query "What year did Albert Einstein win the Nobel Prize in Physics?" but is actually unhelpful. In this subsection, we investigate the effectiveness of our representation-based method in identifying the helpfulness of context. We selected a subset of 10,000 questions from Natural Questions\cite{NQ}, as used by \citet{cuconasu2024power}. For each question, we used the golden passage that directly answers the question as the helpful context (positive sample). Unhelpful context (negative sample) was sourced from top-ranked passages retrieved from Wikipedia-2018 that do not contain the ground truth answer, as released by \citet{cuconasu2024power}.
% % For our representation-based method, we randomly selected only 1\% of the data (100 queries) to anchor the PCA space, calculate decision boundaries, and train contrastive learning classifiers. The remaining 99\% was used as the test set to evaluate both baselines and our methods.
% % Results presented in Table \ref{tab:rag_robustness} show that LLMs themselves still struggle to provide accurate judgments, particularly exhibiting a high false-positive rate. This indicates that LLMs tend to trust and regard provided context as helpful, even when it isn't. In contrast, our methods demonstrate significantly better accuracy, with \textit{rep-PCA} achieving 79\% accuracy and \textit{rep-Contrastive} reaching 81\% accuracy, which is much more reliable.
 
% \subsection{Contradiction Checking}
% % [TODO:GIVE A MOTIVATION]
% \label{Sec:conflict}

% Previous research\cite{xieadaptive} has demonstrated that when presented with relevant but contradictory evidence, LLMs tend to prioritize external knowledge over their internal knowledge. Consequently, it is crucial to assess whether the provided external context aligns with or contradicts the LLM's internal beliefs. In this subsection, we investigate whether LLMs' representations can serve as more reliable indicators of contradictions between external context and the model's internal knowledge. we utilize a subset of \href{https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict/tree/main}{ConflictQA} \cite{xieadaptive}. Each sample contains a PopQA\cite{mallen-etal-2023-trust} question, correct aligned evidence, and ChatGPT-generated contradictory evidence. See appendix \ref{App:dataset_checking} for details. We sampled 1146 questions that Mistral-7B answers correctly, using aligned evidence with the query as positive samples and contradictory evidence as negative samples.
%  We utilized 10\% of the dataset (114 positive-negative pairs) to anchor the PCA space, calculate decision boundaries, and train the contrastive learning classifiers. The remaining 90\% was reserved for testing purposes. We compare representation based method with traditional methods in  Section \ref{Sec:query}.


 

% % When presented relevant but contradictory evidence, previous research has shown that LLM tends to outweigh the external knowledge than internal knowledge. Thus it is important to check whether the provided external context is aligned or contradictory to LLMs' own belief. In this subsection, we further explore whether LLMs' representations can serve as better indicators of contradictions between external context and the model's internal knowledge. To verify this, we selected a subset of PopQA questions that Mistral-7B could directly answer, ensuring the model possessed correct knowledge of these questions. This subset contains 1146 questions in total. For each query, we use both aligned evidence as well as contradictory evidence as positive and negative samples. These evidence are provided by \citet{xieadaptive}, the supporting documents were sourced from Wikipedia, while the misleading contexts were generated by ChatGPT.

% % The supporting documents were sourced from Wikipedia, while the misleading contexts were constructed following \citet{xieadaptive}, either generated or sourced from Wikipedia/human annotations.

% \paragraph{Results.}

% We initially assess whether LLMs' answers and their associated probability/perplexity metrics can effectively indicate contradictions. The results in Table \ref{tab:rag_robustness} reveal that LLMs' answers continue to exhibit low accuracy and suffer from a high rate of false positives. This suggests that LLMs tend to interpret contradictory external knowledge as aligned evidence in their responses. Furthermore, Figure \ref{fig:roc_contra} demonstrates a extremenly low AUC of of 0.39 (Lowest Score), 0.34 (Average Score), 0.33 (Perplexity), indicating minimal differences in probability/perplexity distributions when LLMs are presented with aligned versus contradictory evidence.
% As illustrated in Table \ref{tab:rag_robustness}, representation-based methods demonstrate significantly superior performance, with \textit{rep-PCA} achieving 91\% accuracy and \textit{rep-Contrastive} attaining an impressive 95\% accuracy. Our visualizations, presented in Figures \ref{fig:PCA_contra} and \ref{fig:con_contra}, reveal distinct distributions and contradictory scores for the contradictory and aligned contexts. These pronounced differences strongly indicate that our method can effectively discriminate between these context types.

% We used 10\% of the data (114 positive-negative pairs) to anchor the PCA space, calculate decision boundaries, and train the contrastive learning classifiers. The remaining 90\% was used for testing. As shown in Table \ref{tab:rag_robustness}, representation-based methods demonstrate much stronger performance, with \textit{rep-PCA} achieving 91\% accuracy and \textit{rep-Contrastive} reaching 95\% accuracy. In our visualizations presented in Figure \ref{fig:PCA_contra} and Figure \ref{fig:con_contra}, we also observe distinct distributions and contradictory scores for the contradictory and aligned contexts. These clear differences indicate that our method can effectively classify these contexts.




