
% \yue{[Unify wording: sometimes word editing software may change terminology and make these wordings inconsistent. \begin{itemize}
%     \item (Retrieval/external) database/(retrieval) dataset
%     \item Retrieved data/retrieval data/retrieved information (Or they have slightly different meanings?)
%     \item Few-shot, few-shots
%     \item Few-show prompting, few-shot learning (Or they have different meanings?)
%     \item Raw data, original data
%     \item Stage-1, stage-2/first stage, second stage
% \end{itemize}]}

% \yue{[Check through the paper: need a space before ``$\backslash$cite'']}

% \jt{After brieflying reading the whole intro, I have the following suggested storyline:  \\
%  1. The importance of RAG and potential privacy consideration of RAG. Then how to protect privacy for RAG is important and why??  \\
%  2. why synthetic data is a good solution. I think this is not well motivated. There are many potential ways to achieve this goal, why we think synthetic data is chosen for RAG???  \\
%  3. the status of synthetic data technique and what are the challenges applying existing synthetic data technique for our goal??? \\  
%  4. our contributions 
%  }

\section{Introduction}
\label{Intro}

% 1. Retrieval-augmented generation\\
% 2. Robustness issues of RAG\\
% 3. Define "The desired behavior" of LLM in RAG\\
% 4. To address this issue....

Providing large language models(LLMs) with external related contexts can improve their factual accuracy and reliability~\cite{gao2023retrieval, lewis2020retrieval,fan2024survey}. Techniques such as retrieval-augmented generation(RAG)~\cite{lewis2020retrieval} has gained widespread application including healthcare~\cite{amugongo2024retrieval}, legal services~\cite{wiratunga2024cbr}, and financial analysis~\cite{setty2024improving}.

However, this approach faces significant challenges when dealing with imperfect evidence. LLMs tend to over-rely on external knowledge, making them vulnerable to misleading or inaccurate information in the retrieved context. \citet{zou2024poisonedrag,dengpandora} demonstrated that contextual misinformation can mislead LLMs even when they possess the correct knowledge internally.  Besides, \citet{yoranmaking,fang-etal-2024-enhancing} demonstrate that a substantial portion of retrieved contexts, while semantically related to the query, can be unhelpful or irrelevant for answering the question, leading to degraded LLM performance.

Compared to LLMs, humans demonstrate greater robustness when processing external information by carefully weighing it against their internal knowledge to reach reasoned conclusions \cite{hollister2017contextual}. This capability, often referred to as \textbf{contextual reasoning} or \textbf{knowledge integration}, is critical for ensuring reliable and accurate responses in real-world applications. For example, when presented with text claiming "Paris was the capital of France until 2020 when it moved to Marseille," people can reason "Based on my knowledge, Paris is France's capital, though this source claims it's Marseille." Similarly, when given irrelevant context about French cuisine while answering a capital city question, people naturally ignore the context and rely on their internal knowledge.  To match this capability, a \textbf{context-robust LLM} is desired to have similar cognitive processes as shown in Fig \ref{fig:intro}.

As discussed in previous works \cite{wang2024astute,yoranmaking}, LLMs can benefit from external contexts when they lack the knowledge to answer a question. This suggests an expected strategy for LLMs to utilize external contexts: \textbf{LLMs should rely on external context only when lacking internal knowledge} (the ``Unknown'' case in Fig \ref{fig:intro}). When internal knowledge exists, they can carefully balance external and internal knowledge to provide more objective and thoughtful answers\cite{chen2024benchmarking}. For example, as shown in Fig \ref{fig:intro}, when encountering knowledge that matches with its internal beliefs, a context-robust LLM will use both sources to formulate its response. While faced with contradictory evidence, it is to identify the contradiction and present both answers\footnote{Users can choose to use Ans(I) or Ans(E) based on their needs. For instance, for knowledge updating, they might choose Ans(E), while for addressing misinformation, they might prefer Ans(I).}. Similarly, if unhelpful contexts are presented, a context-robust LLM is expected to identify and ignore such contexts, relying solely on its internal knowledge to generate an answer\cite{yoranmaking}. However, current LLMs do not naturally exhibit these behaviors. Therefore, in this work, we aim to adapt LLMs' behaviors following the process in Fig~\ref{fig:intro} when handling retrieved-context.  





% \yue{This paragraph has a lot of ``should": need to provide more evidence to state why the things should be in the described way. (1) Why RAG should implement cognitive process as human, any real examples? (2) Why RAG should evaluate the external context in the specific three lenses? Any evidences?  }Unlike LLMs \pf{transition is not smooth here}, \jt{do we have evidence or references to support the following claim??}  \jt{the following process is expected by real-world applications? we need to give evidence or examples to support} 
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{pic/intro6.pdf}
    \caption{ Comparison between current LLMs and our developed context-robust LLMs in this work. Ans(I) refers to responses based on internal knowledge, while Ans(E) refers to responses based on external context. Current LLMs primarily rely on external sources for responses, whereas our context-robust LLM carefully balances contextual information with its internal knowledge to provide more reliable responses.}
    \label{fig:intro}
     \vspace{-0.2in}
\end{figure}

Adapting LLMs to exhibit these behaviors presents tremendous challenges. Previous studies, such as \cite{zeng2024towards}, have shown that training-free methods—such as adding system prompts, in-context learning, or using Chain-of-Thought (CoT) prompting to guide models in balancing internal and external contexts—are far from reliable. Another approach is to fine-tune the LLM to teach it the desired behavior. However, direct end-to-end training of model parameters typically requires extensive training time and large datasets, which is not always feasible in real world, and limited examples may be insufficient to teach the LLM towards context-robust behaviors as we show later in Table \ref{tab:query_results}. 
% Additionally, some researchers propose querying the LLM multiple times and presenting both LLM-based and RAG-based answers, allowing the LLM to choose the correct one. This method, however, relies heavily on the reasoning ability of strong LLMs (e.g., GPT-4) and may suffer from performance degradation on unknown questions due to the inclusion of incorrect model-generated answers(See Table \ref{tab:query_results}). 

In contrast, we aim to develop both data-efficient and training-efficient methods that intrinsically adapt LLMs to context-robust behaviors. \citet{zeng2024towards} show that LLMs' representations exhibit intrinsic distinct patterns when processing contradictory, matched, helpful, or unhelpful inputs. Furthermore, \citet{zou2023representation,wuandarora2024reft} demonstrate that intervening in LLMs' hidden layer representations can effectively modify their behavior patterns and improve task performance using only a few samples. These findings motivate us to leverage  representation adaptation to achieve the desired behaviors. It is particularly suitable to achieve our goal because it intrinsically relates to how the model processes external contexts\cite{zeng2024towards} (e.g., contradictory, unhelpful, or matched), enabling precise control over behavior. Additionally, it provides efficient, lightweight adaptation with minimal computational overhead and data requirements\cite{wu2024reft}.

In our work, we propose a gated representation fine-tuning pipeline Grft to enhance LLMs' robustness to retrieved contexts. Grft introduces a \textbf{lightweight, plug-and-play} intervention for LLMs' hidden layer representations. It consists of two components: (1) a gate mechanism that detects "abnormal" inputs and determines if intervention is needed, and (2) low-rank representation adapters that adjust hidden representations within a linear subspace. Using fewer than 200 training questions, we end-to-end train Grft (0.0004\% of model parameters). Experimental results demonstrate that Grft effectively improves LLM performance with misleading and unhelpful contexts while maintaining performance on helpful contexts.




% \yue{Why we want to use representation adaptation rather than other adaption is not clear to me. We need something like ``we should use it becuase our problem needs xxxx properties", rather than ``some other people use it so we want it"}
% \yue{Compared to the existing literature, what is the difference in our method? Why we want the new designs? What is the advantage of the new method? Is 100 training sample significantly fewer than other ReFT literature? What about 0.0004\%?}

% \yue{I know these literature help our study, but when you search literature, how did you start the searching? For example, among various lines of researches, e.g., fine-tuning, prompting, embedding model, why you explore some particular direction? These related works come in a sudden without clear logic.} 
% However, current LLM does not naturally present these behaviors. Thus to address this issue, in this work, we take a step to explore how to efficiently and effectively adapt LLMs to the aforementioned behaviors, and consequently enhance their robustness when dealing with retrieved context. Previous research has shown that LLMs' representation space contains much richer information and can be controlled to LLMs' behaviors. On the one hand, \citet{zeng2024towards} has shown LLMs' representations tend to show different patterns when facing various contradictory/aligned helpful/unhelpful inputs. On the other hand, \citet{zou2023representation,wuandarora2024reft} shows intervention on LLMs' hidden layer representations can effective change LLMs' behavior patterns(e.g. be more/less honest) and enhance it's performance on certain tasks. These aforementioned advancement inspire us to adapt LLMs to the aforementioned behaviors via representation adaptation.  




 
% This evaluation determines the response strategy: acknowledging conflicts ("Based on context X, though I know Y"), ignoring irrelevant information, or using aligned knowledge from either source.


% Different from LLMs, human beings present higher robustness when dealing with this situation. When facing with external contexts, human will carefully compare the external knowledge and internal knowledge and gives a more reasonable answers. Thus, a robust RAG system should also present this heuristics. To be specific, as shown in Fig \ref{}, if LLM does not possess internal knowledge of the answer, it should rely on external knowledge to get that answer. However, if it does possess internal knowledge, the desired behavior of LLM should 
% first identify whether the external knowledge is conflictory, helpful or aligned to answer the question, and then give a more reasonable answer. If the context is contradictory to LLMs' internal belief, LLM should identify the confliction and give a answer like "Based on the context, the answer.... However, based on my knowledge..." , if the context is unhelpful or irrevelent, the LLM should identify the useless of the context and rely on internal knowledge to answer the question. if the provided context is aligned with its internal belief, the LLM should answer based on either of them.










% Despite their impressive capabilities (OpenAI, 2022, 2023; Kaddour et al., 2023), language models still struggle with making up false information and factual errors (Ji et al., 2023; Wang et al., 2023). One solution is retrieval-augmented generation, which incorporates relevant supporting information into model inputs to improve their factual accuracy and reliability.
    
% \zsl{But the helpfulness may also cause misunderstandings, as misleading question is also considered as helpful, do we need a clarification on that?}
  %  If the LLM has internal knowledge about the query, it should check this knowledge against the retrieved external information to identify any contradictions. \jt{I think we can merge checking 2, 3 and 4. 3 and 4 are helpfulness checking under two situations and 2 is just an extreme case of 3}
 %   \item \textbf{ \jt{may be helpfulness check??}Relevance Checking for Known Topics:} In cases where the LLM has internal knowledge, it should also check \jt{can we just use if the externalk noweldge is helpful or not to the query??? } whether the external knowledge is directly related or merely tangential to the query. \jt{as an extreme case, let us put the conflicting here???}
 %   \item \textbf{Relevance Checking for Unknown Topics:} If the LLM lacks internal knowledge about the query, it should still check whether the retrieved external knowledge is directly relevant or tangential to the query.


% \noindent A straightforward approach to tackle these tasks can directly prompt LLMs\cite{asaiself,wang2023self,liu2024ra,zhang2024retrievalqa}. Alternatively, we could examine superficial indicators of LLMs, such as probability scores~\cite{wang2024self,jiang2023active} or perplexity~\cite{zou2024poisonedrag}. However, based on our evaluation in Section \ref{Sec: Representation}, we find that none of these methods can effectively accomplish these tasks. 

% A straightforward approach to complete these tasks may be directly prompting LLMs\cite{asaiself,wang2023self,liu2024ra,zhang2024retrievalqa}, or otherwise check some superficial indicators of LLM such as probability scores~\cite{wang2024self,jiang2023active} or perplexity~\cite{zou2024poisonedrag}. However, we check these methods and found none of them could successfully fulfill above mentioned checks. 

% \cite{asaiself,wang2023self,liu2024ra,zhang2024retrievalqa}  to evaluate context quality or employing commonly used metrics such as perplexity~\cite{zou2024poisonedrag} or probability~\cite{wang2024self,jiang2023active}. However, our studies have revealed that relying solely on LLMs' direct outputs or simple indicators such as probability and perplexity fail to accurately accomplish above tasks.

% Recent studies~\cite{zou2023representation,lin2024towards,zheng2024prompt} have shown that LLMs' representations exhibit distinct patterns when encountering contrasting high-level concepts, such as harmful versus harmless prompts  . This observation prompts us to investigate \textit{whether LLMs' representations also display distinct behaviors and can be leveraged in knowledge checking tasks?} To answer this question, we conduct a comprehensive study and analysis of LLM representation behaviors regarding the aforementioned tasks, including PCA-based checking as well as contrastive-learning-based checking (Section \ref{rep_method}). Our analysis reveals that positive and negative samples exhibit  different behaviors in the representation space. Consequently, representation-based methods demonstrate significantly superior performance in the aforementioned tasks. Leveraging these findings, we utilize representation classifiers for knowledge filtering. Results show that simple filtering of contradictory and irrelevant information substantially improves RAG performance, even in scenarios with poisoned knowledge databases. 

% contain a wealth of information that can be utilized to analyze and extract high-level features~\cite{zou2023representation,lin2024towards,zheng2024prompt} such as honesty or harmfulness. SuAs LLMs' internal representation behavior may be different when facing different inputs, capture and analyse models' representation patterns may be a way to indicate knowledge contradiction or misalignments. This insight inspires us to explore a crucial question: \textit{Can LLM representations be leveraged to improve knowledge checking in RAG systems?"}  





% We find positive and negative samples generally present distinct behavior in the representation space, and representation-based methods can achieve significantly superior performance in the aforementioned tasks. Building upon these findings, we further utilize the representation indicators to conduct knowledge filtering. We find that simply filtering out contradictory and unhelpful information can improve RAG's performance, even when the knowledge database is poisoned, which validate the importance and effectiveness of knowledge checking in RAG systems.


% \jt{no need to provide teh following details but just mention where the readers can find more details} Specifically, we adopted Principal Component Analysis (PCA) as well as proposed a contrastive learning-based analysis built upon the representations of LLMs on knowledge-checking tasks under various conditions. \jt{please use the "current" tense for verb when we describe what we do in this work. The following seems not well rewritten, please rewrite} We reveal that  representation-based methods yields significantly superior performance in the aforementioned tasks. Building upon these findings, we further utilize the representation indicators to conduct knowledge filtering. We found that simply filtering out contradictory and unhelpful information can improve RAG's performance for both poisoned and unpoisoned queries. 
% Our experiments demonstrate that this filtering mechanism substantially enhances the robustness and performance of RAG systems, even in scenarios where the retrieval source has been compromised or "poisoned".



% To address this question, we conducted a comprehensive study and analyze of LLM representation behaviors regarding above-mentioned tasks.  Specially, we adopt PCA analyze as well as proposed contrastive learning based analyze build upon the representations of LLMs on knowlege checking tasks regarding various conditions. And we discovered that using these representation-based methods yielded significantly superior performance in the aforementioned tasks compared to traditional approaches. Building upon these findings, we further utilize the representation indicators to conduct knowledge filtering, we found simply filter out contradictory and tangential information can improve RAG's performance regarding both poisoned and unpoisoned queries. Our experiments demonstrate that the filter machnism  substantially enhances the robustness and performance of RAG systems, even in scenarios where the retrieval source has been compromised or "poisoned". 




% Our approach utilized both PCA and contrastive learning-based clustering techniques to analyze these representations. \yue{[What is an ``approach"? We conduct some study on sth, why this is called an approach?]} We discovered that using these representation-based methods yielded significantly superior performance in the aforementioned tasks compared to traditional approaches. Building upon these findings, we further utilize the representation detection results to filter undesired context. Our experiments demonstrate that the proposed method \yue{[Please make it clear what is the proposed method.]} substantially enhances the robustness and performance of RAG systems, even in scenarios where the retrieval source has been compromised or "poisoned". This method offers a promising opportunity for creating more reliable and resilient RAG systems, capable of maintaining high performance in the face of adversarial or low-quality input data.

% For example, recent research indicates that RAG can be maliciously and unintentionally manipulated to make unreliable decisions and harm humans. Deng et al. (2024) suggest that although it may be hard to directly prompt LLMs to output harmful information, incorporating malicious content into retrieval data can easily jailbreak LLMs, causing them to output harmful information. Similarly, Zou et al. (2024) propose poisonedRAG, demonstrating that it is easy to distract LLMs into outputting wrong information by injecting some wrong answers corresponding to targeted queries into the retrieval database. Besides, 
% RAG is widely used-- However, RAG faces robustness issues. What is a robust RAG looks like, what is current RAG? Previous research has shown that LLMs fail to (a) identify known/unknown queries (b) deal with misleading contexts (c) relevent but unhelpful. Thus we need an detector of context quality respect to these issues to enhance the robustness of RAG. 

% A naive solution is to detect possible issues using LLMs' output or commonly used matrices like perplexity and probability. We first conduct prelinminary studies and unveil it is not enough -- better indicator that can be utilized to more effectively detect filtering context.

% Previous research has indicate representation-- which motivate us to build analyse on the representation spaces of LLM. We build PCA based and contrastive learning based classifier on the representation to
% And surprisingly found it can outperform .... on ...detection

% Furthre more, we utilize this indicator to filter out.. and found it can achieve.....

% Our contribution can be summarized as follows:
% 1. Unveil current limitations of
% 2. We found that .... 
% 3. Utilizing




