\vspace{-0.1in}
\section{Conclusion}
\vspace{-0.1in}
\label{Conclusion}
In this paper, we present Grft, a lightweight gated representation fine-tuning approach to enhance LLMs' contextual robustness. Through training a lightweight intervention function (parameters only accounting for 0.0004\% of model size)  on fewer than 200 samples, Grft effectively adapts LLMs to exhibit context-robust behaviors: relying on external context only when necessary, identifying contradictions and give integrated answers, and ignoring unhelpful contexts. Experimental results demonstrate that Grft significantly improves LLMs' robustness to imperfect contexts while maintaining their original capabilities, providing a practical solution for real-world applications where handling imperfect evidence is crucial.
% This study delves into the knowledge checking in RAG systems. To achieve this goal, we identified and proposed four key tasks. Through comprehensive analysis of LLMs' representation behaviors, we found that representation-based methods significantly outperform answer-based or probability-based approaches. Leveraging these findings, we developed representation-based classifiers for knowledge filtering. Results demonstrate that simply filtering of contradictory and unhelpful knowledge substantially improves RAG performance.

%, even in scenarios with poisoned knowledge databases. This research provides new insights and methodologies for enhancing the reliability and robustness of RAG systems.




% \yue{Besides, [talk about DP]}

% extensively investigated the privacy risks associated with retrieval-augmented generation (RAG) technique for LLMs. Through our proposed attack methods, we first systematically evaluated and identified the significant risks of retrieval data extraction. Meanwhile, we explored various defense techniques that can mitigate these risks. We also found that integrating retrieval data can substantially reduce LLMs' tendency to output its memorized training data, which suggests that RAG could potentially mitigate the risks of training data leakage. Overall, we revealed novel insights regarding privacy concerns of retrieval-augmented LLMs, which is beneficial for the proper usage of RAG techniques in real-world applications.