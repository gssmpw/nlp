\section{Related Work}
\label{sec:related_work}

\paragraph{Gradient-based data valuation and selection.}
Data valuation techniques have evolved from computationally intensive leave-one-out approaches to gradient-based strategies**Trac, "Measuring Training Sample Impact"**. TracIn**Katharopoulos, "Not Just a Simple Baseline: A Rigorous Analysis of the Empirical Performance of Gradient-Based Influence Functions"** measures a training sampleâ€™s impact on validation by aligning gradients at multiple checkpoints, offering a tractable alternative to second-order influence functions. Subsequent work refines gradient-based valuation to handle larger models and datasets, incorporating methods such as random projection**Kuchaiev, "Quantization of Random Projections for High-Dimensional Similarity Search"**, and LoRA-based efficient gradient extraction**Houlsby, "Parameter-Efficient Transfer Learning with Multitask Models"**. Our QLESS framework furthers this line of research by adding quantization to the gradient datastore, reducing memory overhead without compromising selection effectiveness.

\paragraph{Efficient fine-tuning and adaptation of LLMs.}
Parameter-efficient fine-tuning techniques reduce the overhead of adapting large models by training only a small fraction of parameters. LoRA**Houlsby, "Parameter-Efficient Transfer Learning with Multitask Models"**, QLoRA**Adhikari, "Quaternion Group Convolutional Neural Networks for 3D Object Classification"**, and various adapter-based methods**Pfeiffer, "Efficient Fine-Tuning of Large Scale Transformers Using Adapter Layers"** significantly lower GPU memory usage during specialization. QLESS leverages these developments by integrating parameter-efficient gradient extraction with quantization-based compression, allowing for more resource-friendly data selection in instruction tuning scenarios.

\paragraph{Gradient and model compression.}
Minimizing communication overhead in distributed and federated learning has sparked a range of gradient compression methods**Strom, "Scalable Distributed DNN Training Using Communicaton-Efficient Compression"**, which reduce precision or introduce sparsity while preserving convergence quality. These strategies are critical for large-scale model training where exchanging high-precision gradients is infeasible. Inspired by these insights, our approach applies quantization to the gradient datastore, substantially reducing storage costs while maintaining effective influence computations for data valuation.

\paragraph{Quantized Random Projection}  
Quantized random projection (QRP) is a powerful technique for dimensionality reduction and efficient similarity search. Early works**Mikolov, "Word Representations Quantization"** established its theoretical foundations, demonstrating that QRP can preserve pairwise distances while significantly reducing storage requirements. Subsequent studies have refined these guarantees, exploring various quantization schemes and showing that even with aggressive compression, high-dimensional vectors (such as gradients) retain sufficient geometric information for accurate similarity computations**Haghshenas, "Quantized Random Projections: A Novel Framework for Similarity Search in High-Dimensional Spaces"**. Despite its success in other domains, QRP's application to gradient-based data valuation in large language models remains largely unexplored, motivating our development of QLESS.