\section{Related Work}
\label{sec:related_work}

\paragraph{Gradient-based data valuation and selection.}
Data valuation techniques have evolved from computationally intensive leave-one-out approaches to gradient-based strategies____. TracIn____ measures a training sampleâ€™s impact on validation by aligning gradients at multiple checkpoints, offering a tractable alternative to second-order influence functions. Subsequent work refines gradient-based valuation to handle larger models and datasets, incorporating methods such as random projection____ and LoRA-based efficient gradient extraction____. Our QLESS framework furthers this line of research by adding quantization to the gradient datastore, reducing memory overhead without compromising selection effectiveness.

\paragraph{Efficient fine-tuning and adaptation of LLMs.}
Parameter-efficient fine-tuning techniques reduce the overhead of adapting large models by training only a small fraction of parameters. LoRA____, QLoRA____, and various adapter-based methods____ significantly lower GPU memory usage during specialization. QLESS leverages these developments by integrating parameter-efficient gradient extraction with quantization-based compression, allowing for more resource-friendly data selection in instruction tuning scenarios.

\paragraph{Gradient and model compression.}
Minimizing communication overhead in distributed and federated learning has sparked a range of gradient compression methods____, which reduce precision or introduce sparsity while preserving convergence quality. These strategies are critical for large-scale model training where exchanging high-precision gradients is infeasible. Inspired by these insights, our approach applies quantization to the gradient datastore, substantially reducing storage costs while maintaining effective influence computations for data valuation.

\paragraph{Quantized Random Projection}  
Quantized random projection (QRP) is a powerful technique for dimensionality reduction and efficient similarity search. Early works____ established its theoretical foundations, demonstrating that QRP can preserve pairwise distances while significantly reducing storage requirements. Subsequent studies have refined these guarantees, exploring various quantization schemes and showing that even with aggressive compression, high-dimensional vectors (such as gradients) retain sufficient geometric information for accurate similarity computations____. Despite its success in other domains, QRP's application to gradient-based data valuation in large language models remains largely unexplored, motivating our development of QLESS.