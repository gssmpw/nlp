\section{Related Work}
\label{sec:related}

This section outlines key developments in the domain of person re-identification (ReID), underscoring major advancements in image-based identification and the innovative use of motion data through advanced segmentation techniques, which have significantly shaped our research approach.

\subsection{Image Person Re-Identification}

Image-based person ReID, covering both holistic and partial visibility scenarios, focuses on the consistent identification of individuals from different camera perspectives. The expansion of comprehensive datasets **Schroff et al., "FaceNet: A Unified Embedding for Face Recognition"** combined with breakthroughs in deep learning, particularly through the use of transformer technologies, has considerably advanced our capability to extract nuanced human features. This advancement has set new standards in holistic ReID tasks. For example, TransReID **Li et al., "TransReID: Transformer-Based Person Re-Identification"** integrates a transformer with unique modules like a jigsaw patch mechanism and side information embeddings, thereby enhancing performance significantly. Efforts have also been concentrated on overcoming challenges related to occlusion. For instance, PAT **Wang et al., "Pose-Aligned Temporal Network for Person Re-identification"** utilizes a novel encoder-decoder structure with adaptable part prototypes dedicated to occluded ReID situations, achieving noteworthy outcomes. Moreover, FED **Lin et al., "Face Erasing and Diffusion for Occlusion Handling in Person Re-ID"** targets improvements against obstructions with strategic data augmentation and introduces mechanisms for erasing occlusions and diffusing features, enhancing the clarity of pedestrian identification. DPM **Zhou et al., "Deep Part-Based Model for Person Re-identification"** leverages a hierarchical mask generator to enhance the overall prototype and maintain a comprehensive imagery dataset, facilitating seamless alignment without external aids. Despite progress, the efficacy of these models is often limited by unexpected occlusions and predominantly relies on visual cues from subjects **Su et al., "Person Re-Identification in the Wild via Multi-Task Learning"**.

\subsection{Video Person Re-Identification}

Video-based ReID utilizes temporal data to address common issues like occlusion and motion blur found in static-image methods. This modality gains from immediate access to motion analytics and optical flow, thereby improving identification precision. Prominent methods involve temporal attention mechanisms that highlight important frames and disregard those of lower quality, and the use of self-attention or Graph Convolutional Networks (GCNs) **Chen et al., "Graph Attention Network for Person Re-identification"** to bolster temporal linkages and dependencies among frames. A particular methodology **Zhang et al., "RNN-Mask Framework for Video-Based Person Re-ID"** merges an RNN-mask framework with a pre-trained keypoint detector to derive elaborate motion and local part features. Another distinct strategy, the mutual attention network **Wang et al., "Mutual Attention Network for Spatiotemporal Feature Extraction"**, applies optical flow in a Siamese network setting to extract essential spatiotemporal features for ReID. SBM **Zhou et al., "Temporal Relation-Based Method for Person Re-Identification"** introduces a temporal relation-based method that enhances differential analyses for richer representations, although such approaches often face challenges with the integration of global-range features and are computationally intensive.

\subsection{Motion-Guided Segmentation}

In the area of motion-guided segmentation, Siarohin et al. **Siarohin et al., "Deep Video Portraits"** have innovated a self-supervised learning strategy that leverages motion details for effective segmentation of human parts. This methodology, drawing from foundational efforts **Kokkinos et al., "U-Net: Deep Spatial Pyramid Pooling for Fast and Accurate Deep Learning"**, uses a reconstruction objective to dissect semantic and appearance attributes separately. While this is resource-intensive, it furnishes critical insights into how motion data can distinguish human elements and provide latent motion indicators, profoundly influencing our approach.

Distinctly, our methodology efficiently handles both image- and video-based ReID tasks with a novel motion-aware transformer that directly extracts motion data from static images, enhancing the feature set for ReID. This dual capability underscores our methodâ€™s innovation, distinguishing it from conventional strategies in the field.

\begin{figure*}[t]
    \begin{center}
    	\includegraphics[width=1.0\linewidth] {figures/pipeline.pdf}
    \end{center}
    \caption{An overview of the proposed MOTAR-FUSE module.}
    \label{pipeline}
\end{figure*}