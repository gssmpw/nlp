\section{Related Work}
\label{sec:related}

This section outlines key developments in the domain of person re-identification (ReID), underscoring major advancements in image-based identification and the innovative use of motion data through advanced segmentation techniques, which have significantly shaped our research approach.

\subsection{Image Person Re-Identification}

Image-based person ReID, covering both holistic and partial visibility scenarios, focuses on the consistent identification of individuals from different camera perspectives. The expansion of comprehensive datasets ~\cite{BiCnet-TKS, tang2024mtvqa, shan2024mctbench, wang2024pargo,tang2024textsquare} combined with breakthroughs in deep learning, particularly through the use of transformer technologies, has considerably advanced our capability to extract nuanced human features. This advancement has set new standards in holistic ReID tasks. For example, TransReID~\cite{he2021transreid,liu2024rethink} integrates a transformer with unique modules like a jigsaw patch mechanism and side information embeddings, thereby enhancing performance significantly. Efforts have also been concentrated on overcoming challenges related to occlusion. For instance, PAT~\cite{li2021diverse} utilizes a novel encoder-decoder structure with adaptable part prototypes dedicated to occluded ReID situations, achieving noteworthy outcomes. Moreover, FED~\cite{wang2022feature} targets improvements against obstructions with strategic data augmentation and introduces mechanisms for erasing occlusions and diffusing features, enhancing the clarity of pedestrian identification. DPM~\cite{tan2022dynamic} leverages a hierarchical mask generator to enhance the overall prototype and maintain a comprehensive imagery dataset, facilitating seamless alignment without external aids. Despite progress, the efficacy of these models is often limited by unexpected occlusions and predominantly relies on visual cues from subjects \cite{wang2024pargo, sun2024attentive, lu2024bounding, zhao2024tabpedia}.

\subsection{Video Person Re-Identification}

Video-based ReID utilizes temporal data to address common issues like occlusion and motion blur found in static-image methods. This modality gains from immediate access to motion analytics and optical flow, thereby improving identification precision. Prominent methods involve temporal attention mechanisms that highlight important frames and disregard those of lower quality, and the use of self-attention or Graph Convolutional Networks (GCNs) ~\cite{gao2020pose, tang2022optimal, tang2022youcan, feng2023unidoc} to bolster temporal linkages and dependencies among frames. A particular methodology~\cite{yin2020fine, feng2024docpedia} merges an RNN-mask framework with a pre-trained keypoint detector to derive elaborate motion and local part features. Another distinct strategy, the mutual attention network~\cite{kiran2021flow}, applies optical flow in a Siamese network setting to extract essential spatiotemporal features for ReID. SBM~\cite{bai2022salient} introduces a temporal relation-based method that enhances differential analyses for richer representations, although such approaches often face challenges with the integration of global-range features and are computationally intensive.

\subsection{Motion-Guided Segmentation}

In the area of motion-guided segmentation, Siarohin \emph{et al.}~\cite{siarohin2021motion} have innovated a self-supervised learning strategy that leverages motion details for effective segmentation of human parts. This methodology, drawing from foundational efforts~\cite{jakab2018unsupervised, zheng2019pose, siarohin2019first}, uses a reconstruction objective to dissect semantic and appearance attributes separately. While this is resource-intensive, it furnishes critical insights into how motion data can distinguish human elements and provide latent motion indicators, profoundly influencing our approach.

Distinctly, our methodology efficiently handles both image- and video-based ReID tasks with a novel motion-aware transformer that directly extracts motion data from static images, enhancing the feature set for ReID. This dual capability underscores our methodâ€™s innovation, distinguishing it from conventional strategies in the field.

\begin{figure*}[t]
    \begin{center}
    	\includegraphics[width=1.0\linewidth] {figures/pipeline.pdf}
    \end{center}
    \caption{An overview of the proposed MOTAR-FUSE module.}
    \label{pipeline}
\end{figure*}