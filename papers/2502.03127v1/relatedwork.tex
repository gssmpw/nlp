\section{RELATED WORK}
IaC has transformed the management and provisioning of IT infrastructure; however, maintaining the quality and standardization of IaC scripts continues to be a critical challenge. A survey by Konala et al.\ \cite{pandusok} highlights that several tools exist that are designed to address specific quality issues such as security smells or code smells through the use of various techniques like regular expressions, linting, deep learning, etc. Despite the advancements made with these tools, our analysis reveals a lack of  quality assessment approaches to evaluating overall IaC code quality. This gap underscores the need for a higher-level solutions that evaluate IaC scripts.

Most existing work addresses this research gap by 
%Studies have made progress in addressing the gap in code quality by 
proposing quality attributes and metrics tailored to different IaC tools. Rahman et al.\ \cite{Rahman_2019} initially conducted an empirical study on Puppet scripts, identifying 12~source code properties correlated with defects, such as lines of code and hard-coded strings. Their defect prediction models achieved precision between 0.70--0.78 and recall between 0.54--0.67, offering practitioners actionable insights to enhance script quality. However, the studyâ€™s focus on Puppet scripts limits its applicability to other IaC tools like Ansible, Chef, or Terraform. Additionally, the lack of alignment with ISO standards suggests the need for work to make these attributes more universally applicable.

Similarly, Dalla Palma et al.\ \cite{Palma2020TowardsAC} developed a catalog of 46 quality attributes specific to Ansible scripts, addressing aspects such as code complexity, maintainability, error handling, and best practices. While beneficial for Ansible users, these attributes need adaptation and validation to apply across other IaC environments like Chef or Terraform. Additionally, their impact on quality outcomes has yet to be empirically validated. Begoug et al.\ introduced TerraMetrics \cite{TerraMetrics}, a tool for measuring Terraform script quality using 40 attributes based on static code analysis which draws parallels to Dalla Palma et al.\ \cite{Palma2020TowardsAC} study. Despite its focus on code quality, its limited validation on only three repositories highlights the need for broader assessment. Overall, the previous work in this area points to a need for standardized quality assessment that can be applied across diverse IaC tools.

\begin{tcolorbox}[
    colframe=SlateGray4, 
    colback=Snow1!20,  
    boxrule=1pt, 
    rounded corners
]
\ding{228} \textbf{RQ1 Takeaway Message:} \textit{IaC tools use wide range of methods to evaluate script quality, creating challenges in ensuring consistency and comparability across platforms.}
\end{tcolorbox}