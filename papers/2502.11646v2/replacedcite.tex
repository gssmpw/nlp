\section{Related Work}
\label{section:label}

\subsection{Energy-based Learning}
Energy-based learning (EBL) has been a powerful framework for probabilistic modeling, offering a flexible approach to modeling complex data distributions through the lens of energy functions.
The primitive ideas in this vein can date back to the Hopfield network ____ and Boltzmann machine ____ and were greatly developed in ____. 

A large body of work has revolved around utilizing energy-based models for generative modeling by learning the energy function ____ or its gradient, also known as score-based models ____. This approach attempts to model the data distribution represented by an energy function and learns to generate new samples via Langevin dynamics. Another group of work understands EBL as the computation of the network layer ____. They consider the layer parallels a domain-specific optimization problem but cannot explicitly define a generic form of the energy function. In this paper, we explicitly define the energy function on the hypersphere and relate its optimization to Transformer layers. There are also similar efforts to learning energy on the hypersphere ____, but our energy functions are different and defined on the representation space instead of weight space.
\subsection{Model Design from First Principle}
While the architecture of widely used neural networks is often driven by engineering practices, there has been a series of work that seeks to design or interpret neural networks from principled perspectives such as signal processing, information theory, neurobiology, etc. For example, deep unrolling of the sparse coding algorithms has led to the development of fully connected networks ____, convolution networks ____, and even graph neural networks through iterative algorithms ____. Similarly, the sparse rate reduction principle has been used to derive the Transformer architecture ____. Other approaches draw inspiration from approximation theory ____ and brain computation ____, further bridging the gap between theoretical insights and practical network design. 

% \subsection{Theoretical Understanding of Transformer}

% \subsection{Recurrence in Transformer}
% may be deleted
% universal transformer, albert, R-transformer, ...