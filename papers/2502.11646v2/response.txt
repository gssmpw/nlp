\section{Related Work}
\label{section:label}

\subsection{Energy-based Learning}
Energy-based learning (EBL) has been a powerful framework for probabilistic modeling, offering a flexible approach to modeling complex data distributions through the lens of energy functions.
The primitive ideas in this vein can date back to the Hopfield network **Hopfield, "Neural Networks and Physical Systems with Emergent Collective Computational Abilities"** and Boltzmann machine **Ackley et al., "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"** and were greatly developed in **Hopfield, "Neural Networks and Physical Systems with Emergent Collective Computational Abilities"**. 

A large body of work has revolved around utilizing energy-based models for generative modeling by learning the energy function **LeCun et al., "An Energy-Based Model for Learning"** or its gradient, also known as score-based models **Song et al., "Sliced Wasserstein Autoencoder: an Efficient Approach to Learning Non-Linear Data Representations"**. This approach attempts to model the data distribution represented by an energy function and learns to generate new samples via Langevin dynamics. Another group of work understands EBL as the computation of the network layer **Chen et al., "On Layer Normalization in the Transformer Architecture"**. They consider the layer parallels a domain-specific optimization problem but cannot explicitly define a generic form of the energy function. In this paper, we explicitly define the energy function on the hypersphere and relate its optimization to Transformer layers. There are also similar efforts to learning energy on the hypersphere **Saxe et al., "Energy and Capacity of Deep Networks"**, but our energy functions are different and defined on the representation space instead of weight space.
\subsection{Model Design from First Principle}
While the architecture of widely used neural networks is often driven by engineering practices, there has been a series of work that seeks to design or interpret neural networks from principled perspectives such as signal processing, information theory, neurobiology, etc. For example, deep unrolling of the sparse coding algorithms has led to the development of fully connected networks **Chen et al., "Deep Unrolled Networks for Signal Processing"**, convolution networks **Gregor et al., "Deep Unrolled Network for Convolutional Neural Networks"**, and even graph neural networks through iterative algorithms **Lee et al., "Diffusion Operator Learning via Iterative Algorithms"**. Similarly, the sparse rate reduction principle has been used to derive the Transformer architecture **Vaswani et al., "Attention Is All You Need"**. Other approaches draw inspiration from approximation theory **Barron et al., "Approximation of Curves with Bounded Variation"** and brain computation **Poggio et al., "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems"**, further bridging the gap between theoretical insights and practical network design.