\section{Related Work}
\label{section:label}

\subsection{Energy-based Learning}
Energy-based learning (EBL) has been a powerful framework for probabilistic modeling, offering a flexible approach to modeling complex data distributions through the lens of energy functions.
The primitive ideas in this vein can date back to the Hopfield network \cite{hopfield1982neural} and Boltzmann machine \cite{ackley1985learning} and were greatly developed in \cite{lecun2006tutorial}. 

A large body of work has revolved around utilizing energy-based models for generative modeling by learning the energy function \cite{du2019implicit} or its gradient, also known as score-based models \cite{pmlr-v37-sohl-dickstein15,song2019generative, song2021scorebased}. This approach attempts to model the data distribution represented by an energy function and learns to generate new samples via Langevin dynamics. Another group of work understands EBL as the computation of the network layer \cite{amos2017optnet, agrawal2019differentiable}. They consider the layer parallels a domain-specific optimization problem but cannot explicitly define a generic form of the energy function. In this paper, we explicitly define the energy function on the hypersphere and relate its optimization to Transformer layers. There are also similar efforts to learning energy on the hypersphere \cite{liu2018learning,loshchilov2024ngpt}, but our energy functions are different and defined on the representation space instead of weight space.
\subsection{Model Design from First Principle}
While the architecture of widely used neural networks is often driven by engineering practices, there has been a series of work that seeks to design or interpret neural networks from principled perspectives such as signal processing, information theory, neurobiology, etc. For example, deep unrolling of the sparse coding algorithms has led to the development of fully connected networks \cite{gregor2010learning}, convolution networks \cite{papyan2017convolutional, papyan2018theoretical}, and even graph neural networks through iterative algorithms \cite{yang2021graph}. Similarly, the sparse rate reduction principle has been used to derive the Transformer architecture \cite{yu2023white}. Other approaches draw inspiration from approximation theory \cite{liu2024kan} and brain computation \cite{kozachkov2023building}, further bridging the gap between theoretical insights and practical network design. 

% \subsection{Theoretical Understanding of Transformer}

% \subsection{Recurrence in Transformer}
% may be deleted
% universal transformer, albert, R-transformer, ...