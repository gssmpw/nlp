@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{scarlett2022theoretical,
  title={Theoretical perspectives on deep learning methods in inverse problems},
  author={Scarlett, Jonathan and Heckel, Reinhard and Rodrigues, Miguel RD and Hand, Paul and Eldar, Yonina C},
  journal={IEEE journal on selected areas in information theory},
  volume={3},
  number={3},
  pages={433--453},
  year={2022},
  publisher={IEEE}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{
bao2022beit,
title={{BE}iT: {BERT} Pre-Training of Image Transformers},
author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{kamienny2022end,
  title={End-to-end symbolic regression with transformers},
  author={Kamienny, Pierre-Alexandre and d'Ascoli, St{\'e}phane and Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10269--10281},
  year={2022}
}


@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{anil2023gemini,
  title={Gemini: A family of highly capable multimodal models},
  author={Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  volume={1},
  year={2023}
}

@article{
oquab2024dinov,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=a68SUt6zFt},
note={Featured Certification}
}

@article{sun2024transformer,
  title={Transformer layers as painters},
  author={Sun, Qi and Pickett, Marc and Nain, Aakash Kumar and Jones, Llion},
  journal={arXiv preprint arXiv:2407.09298},
  year={2024}
}

@article{lad2024remarkable,
  title={The Remarkable Robustness of LLMs: Stages of Inference?},
  author={Lad, Vedang and Gurnee, Wes and Tegmark, Max},
  journal={arXiv preprint arXiv:2406.19384},
  year={2024}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  number={1},
  pages={12},
  year={2021}
}

@inproceedings{
nanda2023progress,
title={Progress measures for grokking via mechanistic interpretability},
author={Neel Nanda and Lawrence Chan and Tom Lieberum and Jess Smith and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=9XFSbDPmdW}
}

@inproceedings{
wang2023interpretability,
title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NpsVSN6o4ul}
}

@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={16318--16352},
  year={2023}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}
@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@inproceedings{
huben2024sparse,
title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
author={Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=F76bwRSLeK}
}
@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@article{shlezinger2023model,
  title={Model-based deep learning},
  author={Shlezinger, Nir and Whang, Jay and Eldar, Yonina C and Dimakis, Alexandros G},
  journal={Proceedings of the IEEE},
  volume={111},
  number={5},
  pages={465--499},
  year={2023},
  publisher={IEEE}
}

@inproceedings{amos2017optnet,
  title={Optnet: Differentiable optimization as a layer in neural networks},
  author={Amos, Brandon and Kolter, J Zico},
  booktitle={International conference on machine learning},
  pages={136--145},
  year={2017},
  organization={PMLR}
}
@article{amos2018differentiable,
  title={Differentiable mpc for end-to-end planning and control},
  author={Amos, Brandon and Jimenez, Ivan and Sacks, Jacob and Boots, Byron and Kolter, J Zico},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{wang2019satnet,
  title={Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver},
  author={Wang, Po-Wei and Donti, Priya and Wilder, Bryan and Kolter, Zico},
  booktitle={International Conference on Machine Learning},
  pages={6545--6554},
  year={2019},
  organization={PMLR}
}

@article{greydanus2019hamiltonian,
  title={Hamiltonian neural networks},
  author={Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{thuerey2021physics,
  title={Physics-based deep learning},
  author={Thuerey, Nils and Holl, Philipp and Mueller, Maximilian and Schnell, Patrick and Trost, Felix and Um, Kiwon},
  journal={arXiv preprint arXiv:2109.05237},
  year={2021}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}


@article{dawid2024introduction,
  title={Introduction to latent variable energy-based models: a path toward autonomous machine intelligence},
  author={Dawid, Anna and LeCun, Yann},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2024},
  number={10},
  pages={104011},
  year={2024},
  publisher={IOP Publishing}
}


@InProceedings{pmlr-v151-sander22a,
  title = 	 { Sinkformers: Transformers with Doubly Stochastic Attention },
  author =       {Sander, Michael E. and Ablin, Pierre and Blondel, Mathieu and Peyr\'e, Gabriel},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3515--3530},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/sander22a/sander22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/sander22a.html},
}

@article{yu2023white,
  title={White-box transformers via sparse rate reduction},
  author={Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe and Wu, Ziyang and Tong, Shengbang and Haeffele, Benjamin and Ma, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={9422--9457},
  year={2023}
}

@inproceedings{
bietti2023birth,
title={Birth of a Transformer: A Memory Viewpoint},
author={Alberto Bietti and Vivien Cabannes and Diane Bouchacourt and Herve Jegou and Leon Bottou},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=3X2EbBLNsk}
}

@article{bricken2021attention,
  title={Attention approximates sparse distributed memory},
  author={Bricken, Trenton and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15301--15315},
  year={2021}
}

@article{krotov2016dense,
  title={Dense associative memory for pattern recognition},
  author={Krotov, Dmitry and Hopfield, John J},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{
krotov2021large,
title={Large Associative Memory Problem in Neurobiology and Machine Learning},
author={Dmitry Krotov and John J. Hopfield},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=X4y_10OX-hX}
}

@article{yuille2003concave,
  title={The concave-convex procedure},
  author={Yuille, Alan L and Rangarajan, Anand},
  journal={Neural computation},
  volume={15},
  number={4},
  pages={915--936},
  year={2003},
  publisher={MIT Press}
}

@inproceedings{BahdanauCB14,
  author       = {Dzmitry Bahdanau and
                  Kyunghyun Cho and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.0473},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 ieee information theory workshop (itw)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@inproceedings{
hu2024an,
title={An In-depth Investigation of Sparse Rate Reduction in Transformer-like Models},
author={Yunzhe Hu and Difan Zou and Dong Xu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=CAC74VuMWX}
}

@inproceedings{henry-etal-2020-query,
    title = "Query-Key Normalization for Transformers",
    author = "Henry, Alex  and
      Dachapally, Prudhvi Raj  and
      Pawar, Shubham Shantaram  and
      Chen, Yuxuan",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.379/",
    doi = "10.18653/v1/2020.findings-emnlp.379",
    pages = "4246--4253",
}

% layer scale technique / improvement on rezero
@inproceedings{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={32--42},
  year={2021}
}

@inproceedings{bachlechner2021rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Henry and Cottrell, Gary and McAuley, Julian},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1352--1361},
  year={2021},
  organization={PMLR}
}

@article{palm2018recurrent,
  title={Recurrent relational networks},
  author={Palm, Rasmus and Paquet, Ulrich and Winther, Ole},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{acebron2005kuramoto,
  title={The Kuramoto model: A simple paradigm for synchronization phenomena},
  author={Acebr{\'o}n, Juan A and Bonilla, Luis L and P{\'e}rez Vicente, Conrad J and Ritort, F{\'e}lix and Spigler, Renato},
  journal={Reviews of modern physics},
  volume={77},
  number={1},
  pages={137--185},
  year={2005},
  publisher={APS}
}

@inproceedings{roy2007effective,
  title={The effective rank: A measure of effective dimensionality},
  author={Roy, Olivier and Vetterli, Martin},
  booktitle={2007 15th European signal processing conference},
  pages={606--610},
  year={2007},
  organization={IEEE}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{loshchilov2024ngpt,
  title={ngpt: Normalized transformer with representation learning on the hypersphere},
  author={Loshchilov, Ilya and Hsieh, Cheng-Ping and Sun, Simeng and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2410.01131},
  year={2024}
}
@inproceedings{gregor2010learning,
  title={Learning fast approximations of sparse coding},
  author={Gregor, Karol and LeCun, Yann},
  booktitle={Proceedings of the 27th international conference on international conference on machine learning},
  pages={399--406},
  year={2010}
}
@article{papyan2018theoretical,
  title={Theoretical foundations of deep learning via sparse representations: A multilayer sparse model and its connection to convolutional neural networks},
  author={Papyan, Vardan and Romano, Yaniv and Sulam, Jeremias and Elad, Michael},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={4},
  pages={72--89},
  year={2018},
  publisher={IEEE}
}

@article{papyan2017convolutional,
  title={Convolutional neural networks analyzed via convolutional sparse coding},
  author={Papyan, Vardan and Romano, Yaniv and Elad, Michael},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={83},
  pages={1--52},
  year={2017}
}

@inproceedings{yang2021graph,
  title={Graph neural networks inspired by classical iterative algorithms},
  author={Yang, Yongyi and Liu, Tang and Wang, Yangkun and Zhou, Jinjing and Gan, Quan and Wei, Zhewei and Zhang, Zheng and Huang, Zengfeng and Wipf, David},
  booktitle={International Conference on Machine Learning},
  pages={11773--11783},
  year={2021},
  organization={PMLR}
}

@article{liu2024kan,
  title={Kan: Kolmogorov-arnold networks},
  author={Liu, Ziming and Wang, Yixuan and Vaidya, Sachin and Ruehle, Fabian and Halverson, James and Solja{\v{c}}i{\'c}, Marin and Hou, Thomas Y and Tegmark, Max},
  journal={arXiv preprint arXiv:2404.19756},
  year={2024}
}

@article{hoover2024energy,
  title={Energy transformer},
  author={Hoover, Benjamin and Liang, Yuchen and Pham, Bao and Panda, Rameswar and Strobelt, Hendrik and Chau, Duen Horng and Zaki, Mohammed and Krotov, Dmitry},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
% a follow up (can be used as reference for related work)
@article{hoover2024dense,
  title={Dense Associative Memory Through the Lens of Random Features},
  author={Hoover, Benjamin and Chau, Duen Horng and Strobelt, Hendrik and Ram, Parikshit and Krotov, Dmitry},
  journal={arXiv preprint arXiv:2410.24153},
  year={2024}
}

@article{anil2022path,
  title={Path independent equilibrium models can better exploit test-time computation},
  author={Anil, Cem and Pokle, Ashwini and Liang, Kaiqu and Treutlein, Johannes and Wu, Yuhuai and Bai, Shaojie and Kolter, J Zico and Grosse, Roger B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7796--7809},
  year={2022}
}

@article{schwarzschild2021can,
  title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6695--6706},
  year={2021}
}

@article{bansal2022end,
  title={End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking},
  author={Bansal, Arpit and Schwarzschild, Avi and Borgnia, Eitan and Emam, Zeyad and Huang, Furong and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={20232--20242},
  year={2022}
}

@inproceedings{du2022learning,
  title={Learning iterative reasoning through energy minimization},
  author={Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={5570--5582},
  year={2022},
  organization={PMLR}
}

@article{zhang2022unveiling,
  title={Unveiling transformers with lego: a synthetic reasoning task},
  author={Zhang, Yi and Backurs, Arturs and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
  journal={arXiv preprint arXiv:2206.04301},
  year={2022}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{bartunov2019meta,
  title={Meta-learning deep energy-based memory models},
  author={Bartunov, Sergey and Rae, Jack W and Osindero, Simon and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1910.02720},
  year={2019}
}

@article{rahaman2021dynamic,
  title={Dynamic inference with neural interpreters},
  author={Rahaman, Nasim and Gondal, Muhammad Waleed and Joshi, Shruti and Gehler, Peter and Bengio, Yoshua and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10985--10998},
  year={2021}
}

@article{csordas2021neural,
  title={The neural data router: Adaptive control flow in transformers improves systematic generalization},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2110.07732},
  year={2021}
}

@article{chowdhury2024recurrent,
  title={Recurrent Transformers with Dynamic Halt},
  author={Chowdhury, Jishnu Ray and Caragea, Cornelia},
  journal={arXiv preprint arXiv:2402.00976},
  year={2024}
}
@article{kozachkov2023building,
  title={Building transformers from neurons and astrocytes},
  author={Kozachkov, Leo and Kastanenka, Ksenia V and Krotov, Dmitry},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={34},
  pages={e2219150120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{csordas2021devil,
  title={The devil is in the detail: Simple tricks improve systematic generalization of transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2108.12284},
  year={2021}
}

@article{deletang2022neural,
  title={Neural networks and the chomsky hierarchy},
  author={Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and others},
  journal={arXiv preprint arXiv:2207.02098},
  year={2022}
}

@article{nangia2018listops,
  title={Listops: A diagnostic dataset for latent tree learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.06028},
  year={2018}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@inproceedings{
banino2021pondernet,
title={PonderNet: Learning to Ponder},
author={Andrea Banino and Jan Balaguer and Charles Blundell},
booktitle={8th ICML Workshop on Automated Machine Learning (AutoML) },
year={2021},
url={https://openreview.net/forum?id=1EuxRTe0WN}
}

@article{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{krotov2023new,
  title={A new frontier for Hopfield networks},
  author={Krotov, Dmitry},
  journal={Nature Reviews Physics},
  volume={5},
  number={7},
  pages={366--367},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{
ramsauer2021hopfield,
title={Hopfield Networks is All You Need},
author={Hubert Ramsauer and Bernhard Sch{\"a}fl and Johannes Lehner and Philipp Seidl and Michael Widrich and Lukas Gruber and Markus Holzleitner and Thomas Adler and David Kreil and Michael K Kopp and G{\"u}nter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=tL89RnzIiCd}
}

@inproceedings{esser2021taming,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12873--12883},
  year={2021}
}

@article{lecun2006tutorial,
  title={A tutorial on energy-based learning},
  author={LeCun, Yann and Chopra, Sumit and Hadsell, Raia and others},
  year={2006}
}

@article{du2019implicit,
  title={Implicit generation and modeling with energy based models},
  author={Du, Yilun and Mordatch, Igor},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@InProceedings{pmlr-v37-sohl-dickstein15,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@article{liu2018learning,
  title={Learning towards minimum hyperspherical energy},
  author={Liu, Weiyang and Lin, Rongmei and Liu, Zhen and Liu, Lixin and Yu, Zhiding and Dai, Bo and Song, Le},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{agrawal2019differentiable,
  title={Differentiable convex optimization layers},
  author={Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, J Zico},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{
song2021scorebased,
title={Score-Based Generative Modeling through Stochastic Differential Equations},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}
@article{elbayad2019depth,
  title={Depth-adaptive transformer},
  author={Elbayad, Maha and Gu, Jiatao and Grave, Edouard and Auli, Michael},
  journal={arXiv preprint arXiv:1910.10073},
  year={2019}
}

@inproceedings{
Lan2020ALBERT,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{tan2023sparse,
  title={Sparse universal transformer},
  author={Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
  journal={arXiv preprint arXiv:2310.07096},
  year={2023}
}

@article{csordas2024moeut,
  title={MoEUT: Mixture-of-Experts Universal Transformers},
  author={Csord{\'a}s, R{\'o}bert and Irie, Kazuki and Schmidhuber, J{\"u}rgen and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:2405.16039},
  year={2024}
}

@inproceedings{ontanon-etal-2022-making,
    title = "Making Transformers Solve Compositional Tasks",
    author = "Ontanon, Santiago  and
      Ainslie, Joshua  and
      Fisher, Zachary  and
      Cvicek, Vaclav",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.251/",
    doi = "10.18653/v1/2022.acl-long.251",
    pages = "3591--3607",
    abstract = "Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional generalization. We identified Transformer configurations that generalize compositionally significantly better than previously reported in the literature in many compositional tasks. We achieve state-of-the-art results in a semantic parsing compositional generalization benchmark (COGS), and a string edit operation composition benchmark (PCFG)."
}

@article{feng2024attention,
  title={Attention as an RNN},
  author={Feng, Leo and Tung, Frederick and Hajimirsadeghi, Hossein and Ahmed, Mohamed Osama and Bengio, Yoshua and Mori, Greg},
  journal={arXiv preprint arXiv:2405.13956},
  year={2024}
}

@article{rodionov2024neural,
  title={Neural algorithmic reasoning without intermediate supervision},
  author={Rodionov, Gleb and Prokhorenkova, Liudmila},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


% trade off depth for width
@inproceedings{petty2024impact,
  title={The Impact of Depth on Compositional Generalization in Transformer Language Models},
  author={Petty, Jackson and Steenkiste, Sjoerd and Dasgupta, Ishita and Sha, Fei and Garrette, Dan and Linzen, Tal},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={7232--7245},
  year={2024}
}

@inproceedings{NEURIPS2020_ff4dfdf5,
 author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {22640--22651},
 publisher = {Curran Associates, Inc.},
 title = {Limits to Depth Efficiencies of Self-Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{brown2022wide,
  title={Wide attention is the way forward for transformers?},
  author={Brown, Jason Ross and Zhao, Yiren and Shumailov, Ilia and Mullins, Robert D},
  journal={arXiv preprint arXiv:2210.00640},
  year={2022}
}

@article{gromov2024unreasonable,
  title={The unreasonable ineffectiveness of the deeper layers},
  author={Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and Glorioso, Paolo and Roberts, Daniel A},
  journal={arXiv preprint arXiv:2403.17887},
  year={2024}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

# masked autoregressive image generation (bidirectional attention)
@article{fan2024fluid,
  title={Fluid: Scaling autoregressive text-to-image generative models with continuous tokens},
  author={Fan, Lijie and Li, Tianhong and Qin, Siyang and Li, Yuanzhen and Sun, Chen and Rubinstein, Michael and Sun, Deqing and He, Kaiming and Tian, Yonglong},
  journal={arXiv preprint arXiv:2410.13863},
  year={2024}
}


@InProceedings{pmlr-v202-chang23b,
  title = 	 {Muse: Text-To-Image Generation via Masked Generative Transformers},
  author =       {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, Aaron and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin Patrick and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {4055--4075},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chang23b/chang23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chang23b.html},
  abstract = 	 {We present Muse, a text-to-image Transformermodel that achieves state-of-the-art image genera-tion performance while being significantly moreefficient than diffusion or autoregressive models.Muse is trained on a masked modeling task indiscrete token space: given the text embeddingextracted from a pre-trained large language model(LLM), Muse learns to predict randomly maskedimage tokens. Compared to pixel-space diffusionmodels, such as Imagen and DALL-E 2, Muse issignificantly more efficient due to the use of dis-crete tokens and requires fewer sampling itera-tions; compared to autoregressive models such asParti, Muse is more efficient due to the use of par-allel decoding. The use of a pre-trained LLM en-ables fine-grained language understanding, whichtranslates to high-fidelity image generation andthe understanding of visual concepts such as ob-jects, their spatial relationships, pose, cardinalityetc. Our 900M parameter model achieves a newSOTA on CC3M, with an FID score of 6.06. TheMuse 3B parameter model achieves an FID of7.88 on zero-shot COCO evaluation, along with aCLIP score of 0.32. Muse also directly enables anumber of image editing applications without theneed to fine-tune or invert the model: inpainting,outpainting, and mask-free editing. More resultsand videos demonstrating editing are available at https://muse-icml.github.io/}
}


@inproceedings{
li2024autoregressive,
title={Autoregressive Image Generation without Vector Quantization},
author={Tianhong Li and Yonglong Tian and He Li and Mingyang Deng and Kaiming He},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=VNBIF0gmkb}
}

@inproceedings{chang2022maskgit,
  title={Maskgit: Masked generative image transformer},
  author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11315--11325},
  year={2022}
}

@inproceedings{li2023mage,
  title={Mage: Masked generative encoder to unify representation learning and image synthesis},
  author={Li, Tianhong and Chang, Huiwen and Mishra, Shlok and Zhang, Han and Katabi, Dina and Krishnan, Dilip},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2142--2152},
  year={2023}
}

@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{ackley1985learning,
  title={A learning algorithm for Boltzmann machines},
  author={Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
  journal={Cognitive science},
  volume={9},
  number={1},
  pages={147--169},
  year={1985},
  publisher={Elsevier}
}


% doubly stochastic matrix / learning in the log domain
@inproceedings{tay2020sparse,
  title={Sparse sinkhorn attention},
  author={Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng},
  booktitle={International Conference on Machine Learning},
  pages={9438--9447},
  year={2020},
  organization={PMLR}
}



@inproceedings{
tian2024joma,
title={Jo{MA}: Demystifying Multilayer Transformers via Joint Dynamics of {MLP} and Attention},
author={Yuandong Tian and Yiping Wang and Zhenyu Zhang and Beidi Chen and Simon Shaolei Du},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=LbJqRGNYCf}
}

@InProceedings{pmlr-v139-dong21a,
  title = 	 {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
  author =       {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2793--2803},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
}

@article{geshkovski2023mathematical,
  title={A mathematical perspective on transformers},
  author={Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  journal={arXiv preprint arXiv:2312.10794},
  year={2023}
}

@inproceedings{
geshkovski2023the,
title={The emergence of clusters in self-attention dynamics},
author={Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=aMjaEkkXJx}
}

@inproceedings{chen2022principle,
  title={The principle of diversity: Training stronger vision transformers calls for reducing all levels of redundancy},
  author={Chen, Tianlong and Zhang, Zhenyu and Cheng, Yu and Awadallah, Ahmed and Wang, Zhangyang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12020--12030},
  year={2022}
}

@article{wu2024demystifying,
  title={Demystifying oversmoothing in attention-based graph neural networks},
  author={Wu, Xinyi and Ajorlou, Amir and Wu, Zihui and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{miyato2024artificial,
  title={Artificial Kuramoto Oscillatory Neurons},
  author={Miyato, Takeru and L{\"o}we, Sindy and Geiger, Andreas and Welling, Max},
  journal={arXiv preprint arXiv:2410.13821},
  year={2024}
}

% derive similar results (column softmax) but focus only on attention without MLP
% interesting to mention the equivalence between contrastive and non-contrastive SSL methods
@inproceedings{
guo2023contranorm,
title={ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond},
author={Xiaojun Guo and Yifei Wang and Tianqi Du and Yisen Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=SM7XkJouWHm}
}

@article{garrido2022duality,
  title={On the duality between contrastive and non-contrastive self-supervised learning},
  author={Garrido, Quentin and Chen, Yubei and Bardes, Adrien and Najman, Laurent and Lecun, Yann},
  journal={arXiv preprint arXiv:2206.02574},
  year={2022}
}
% reasoning and recurrence
@inproceedings{
yang2023learning,
title={Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer},
author={Zhun Yang and Adam Ishay and Joohyung Lee},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=udNhDCr2KQe}
}
% reasoning and energy minimization
@article{du2024learning,
  title={Learning Iterative Reasoning through Energy Diffusion},
  author={Du, Yilun and Mao, Jiayuan and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2406.11179},
  year={2024}
}