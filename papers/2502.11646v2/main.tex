%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{caption}
% \usepackage{breqn}
\usepackage{bm}
\newcommand{\Xmatrix}{\bm{X}}
\newcommand{\xvector}{\bm{x}}
\newcommand{\Wmatrix}{\bm{W}}
\newcommand{\wvector}{\bm{w}}
\newcommand{\Dmatrix}{\bm{D}}
\newcommand{\dvector}{\bm{d}}
\newcommand{\zvector}{\bm{z}}
\newcommand{\Zmatrix}{\bm{Z}}
\newcommand{\Qmatrix}{\bm{Q}}
\newcommand{\Kmatrix}{\bm{K}}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% % If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Hyperspherical Energy Transformer with Recurrent Depth}

\begin{document}

\twocolumn[
\icmltitle{Hyperspherical Energy Transformer with Recurrent Depth}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yunzhe Hu}{sch}
\icmlauthor{Difan Zou}{sch}
\icmlauthor{Dong Xu}{sch}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
\icmlaffiliation{sch}{School of Computing and Data Science, The University of Hong Kong, Hong Kong SAR}
% \icmlaffiliation{sch2}{Institute of Data Science, The University of Hong Kong, Hong Kong SAR}

\icmlcorrespondingauthor{Yunzhe Hu}{yzhu@cs.hku.hk}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%4-6 sentences
Transformer-based foundation models have achieved unprecedented success with a gigantic amount of parameters and computational resources. Yet, the core building blocks of these models, the Transformer layers, and how they are arranged and configured are primarily engineered from the bottom up and driven by heuristics. For advancing next-generation architectures, it demands exploring a prototypical model that is amenable to high interpretability and of practical competence. To this end, we take a step from the top-down view and design neural networks from an energy minimization perspective. Specifically, to promote isotropic token distribution on the sphere, we
formulate a modified Hopfield energy function on the subspace-embedded hypersphere, based on which Transformer layers with symmetric structures are designed as the iterative optimization for the energy function. By integrating layers with the same parameters, we propose \textit{Hyper-Spherical Energy Transformer} (Hyper-SET), an alternative to the vanilla Transformer with recurrent depth. This design inherently provides greater interpretability and allows for scaling to deeper layers without a significant increase in the number of parameters. We also empirically demonstrate that Hyper-SET achieves comparable or even superior performance on both synthetic and real-world tasks, such as solving Sudoku and masked image modeling, while utilizing fewer parameters.
\end{abstract}

\section{Introduction}
\label{section:intro}
Transformer-based models \cite{NIPS2017_3f5ee243} have become foundational across diverse domains across diverse domains, including computer vision \cite{dosovitskiy2021an,bao2022beit,he2022masked,peebles2023scalable}, natural language processing \cite{devlin-etal-2019-bert,Lan2020ALBERT, brown2020language}, robotics \cite{brohan2022rt}, decision-making \cite{chen2021decision}, scientific discovery \cite{jumper2021highly, kamienny2022end}, and so on. In recent years, there has been evidence that scaling up model size, dataset size or computational budget to a gigantic magnitude during pre-training can bring about unprecedented performance gains \cite{kaplan2020scalinglawsneurallanguage}, driving the proliferation of Transformer-based foundation models \cite{openai2024gpt4technicalreport,dubey2024llama,anil2023gemini,oquab2024dinov}. 

On the other hand, despite the remarkable abilities of Transformer-based Large Language Models (LLMs), the design and modifications of the Transformer architectures are largely driven by experience and heuristics. There is limited principled guidance for the model's configuration, e.g., how many layers should be stacked. In fact, some empirical studies have observed high redundancy in the deeper layers \cite{gromov2024unreasonable,men2024shortgpt}, uniformity of representations in the middle layers \cite{sun2024transformer}, and robustness to swapping certain intermediate layers \cite{lad2024remarkable} in LLMs. This suggests convergent functionality one layer represents, yet we have a limited understanding of what role the layer plays in processing information and representation learning. Although there are efforts to unveil the function or algorithms underlying the network layers, especially Transformer blocks, through mechanistic interpretability \cite{elhage2021mathematical,nanda2023progress,wang2023interpretability,conmy2023towards,huben2024sparse}, causal mediation analysis \cite{vig2020investigating,meng2022locating}, visualization \cite{bricken2023monosemanticity,olsson2022context}, and others, most of them focus on \textit{post hoc} interpretation and phenomenological approaches. A natural question arises: \textit{Can we find or design a function prior that induces a model that is interpretable by construction?}

One way is to incorporate explicitly optimization process or problem-solving \textit{into} the neural architecture via model-based deep learning \cite{shlezinger2023model}, such as solving constraint satisfiability problems \cite{wang2019satnet}, optimal control \cite{amos2017optnet, amos2018differentiable}, inverse problems \cite{scarlett2022theoretical}, physical law \cite{greydanus2019hamiltonian,karniadakis2021physics, thuerey2021physics}, etc. Despite its success, it is mostly limited to domain-specific priors. Another line of work revolves around a unified view of learning, optimization, and model architectures through the energy-based learning framework \cite{dawid2024introduction}, where it casts modeling the relation or constraint between input $x$ and output $y$ as a scalar-valued energy function $E_\theta(x,y)$ and the inference as finding the minimizer of the function given the input $\hat{y}=\text{min}_yE_\theta(x,y)$. Drawing on this view, a number of articles have considered layers \textit{as} optimization of a generic energy function, either defined implicitly \cite{bai2019deep,du2022learning} by a neural network or explicitly with inspiration from other fields \cite{hoover2024energy,yu2023white}. A single iteration of energy optimization is usually viewed as one-layer update of the given input, which reveals the dynamics of the function. In particular, Hopfield energy has been recently revived to establish connections to Transformers in terms of content-addressable associative memory \cite{ramsauer2021hopfield, bricken2021attention, krotov2023new, bietti2023birth}, where the attention module is interpreted as an information retrieval system that navigates input to the energy stationary points. 

In this paper, we contribute to understanding Transformer-based models from the energy learning perspective. We adopt deep unrolling that unfolds the optimization of an objective function via, e.g., gradient descent to establish equivalence to the network layer. Concretely, we consider tokens as particles and model their dynamics on the hypersphere inspired by \cite{pmlr-v151-sander22a,geshkovski2023mathematical,geshkovski2023the}. We define a dual Hopfield energy function, where one promotes tokens to distribute isotopically in subspaces to mitigate the synchronization effect while the other enforces directional alignment with bases of the original space. By minimizing them alternatively, we derive the core components in Transformers: skip connection, $\operatorname{RMSNorm(\cdot)}$, and a new self-attention and feedforward module with symmetric structures. Each component has its own precise functionality, thus interpretable by design. We name this model \textit{Hyper-Spherical Energy Transformer }(Hyper-SET). By comparing it with vanilla Transformers on both synthetic and real-world tasks, we demonstrate its competitive, even better, performance while being parameter-efficient. The key contributions are summarized as follows
\begin{enumerate}[leftmargin=*]
    \item \textbf{Theoretical Formulation}: Motivated by Hopfield energy, we introduce a unified hyperspherical energy function and provide their theoretical insights.
    \item \textbf{Energy-Driven Architecture}: We build a Transformer-based model with parameter efficiency from sheer energy minimization. 
    \item \textbf{Competitive Performance}: We show its competitive performance to vanilla Transformers across reasoning, classification, and masked image modeling.
\end{enumerate}
% \todo{Add that we can write down the explicit energy instead of those who define the energy implicitly.}


\section{Related Work}
\label{section:label}

\subsection{Energy-based Learning}
Energy-based learning (EBL) has been a powerful framework for probabilistic modeling, offering a flexible approach to modeling complex data distributions through the lens of energy functions.
The primitive ideas in this vein can date back to the Hopfield network \cite{hopfield1982neural} and Boltzmann machine \cite{ackley1985learning} and were greatly developed in \cite{lecun2006tutorial}. 

A large body of work has revolved around utilizing energy-based models for generative modeling by learning the energy function \cite{du2019implicit} or its gradient, also known as score-based models \cite{pmlr-v37-sohl-dickstein15,song2019generative, song2021scorebased}. This approach attempts to model the data distribution represented by an energy function and learns to generate new samples via Langevin dynamics. Another group of work understands EBL as the computation of the network layer \cite{amos2017optnet, agrawal2019differentiable}. They consider the layer parallels a domain-specific optimization problem but cannot explicitly define a generic form of the energy function. In this paper, we explicitly define the energy function on the hypersphere and relate its optimization to Transformer layers. There are also similar efforts to learning energy on the hypersphere \cite{liu2018learning,loshchilov2024ngpt}, but our energy functions are different and defined on the representation space instead of weight space.
\subsection{Model Design from First Principle}
While the architecture of widely used neural networks is often driven by engineering practices, there has been a series of work that seeks to design or interpret neural networks from principled perspectives such as signal processing, information theory, neurobiology, etc. For example, deep unrolling of the sparse coding algorithms has led to the development of fully connected networks \cite{gregor2010learning}, convolution networks \cite{papyan2017convolutional, papyan2018theoretical}, and even graph neural networks through iterative algorithms \cite{yang2021graph}. Similarly, the sparse rate reduction principle has been used to derive the Transformer architecture \cite{yu2023white}. Other approaches draw inspiration from approximation theory \cite{liu2024kan} and brain computation \cite{kozachkov2023building}, further bridging the gap between theoretical insights and practical network design. 

% \subsection{Theoretical Understanding of Transformer}

% \subsection{Recurrence in Transformer}
% may be deleted
% universal transformer, albert, R-transformer, ...


\section{Preliminaries}
\subsection{Hopfield Networks}
Given a network with $N$ neurons $\xvector=[x_1, \dots, x_N]$ that take binary values, the temporal evolution dynamics of these neurons are determined by a scalar-value energy function
\begin{equation*}
    E = - \frac{1}{2}\sum_{i,j}\omega_{ij}x_ix_j = -\frac{1}{2}\xvector^T\bm{W}\xvector,\quad x_{i}, x_{j} \in \{+1, -1\}
\end{equation*}
where $\omega_{ij}$ represents the strength of connectivity between node $x_i$ and $x_j$, and connectivity is assumed to be symmetric, i.e., $\omega_{ij} = \omega_{ji}$. We can further rewrite $\bm{W} =\sum_{i=1}^P \bm{\xi}_i \bm{\xi}_i^T $ as a set of patterns to be stored. The update rule of each node to retrieve the most relevant pattern follows the Hebbian learning rule used in neuroscience 
\begin{equation*}
    \xvector_{t+1} = \text{sign}(\bm{W}\xvector_t) = \text{sign}\left(\sum_{i=1}^P \bm{\xi}_i \bm{\xi}_i^T\xvector_t\right).
\end{equation*}

This update rule tends to minimize the energy function with retrieved patterns as its attractor. It is an embodiment of the idea of ``Neurons that fire together wire together.": If two neurons connect ($\omega_{ij}>0$), then they should have the same state ($+1$ for active and $-1$ for dead). The number of patterns the network can store and retrieve is $\mathcal{O}(N)$.

\subsection{Modern Continuous Hopfield Networks}
To overcome the limitation of linear storage capacity, modern Hopfield networks, also known as Dense Associative Memory \cite{krotov2016dense}, introduce nonlinearity in the energy and the update of neurons' states and make them suitable for continuous variables.
\begin{equation*}
    E =  -\frac{1}{2}\sum_{i=1}^Pf\left(\bm{\xi}_i^T\xvector\right),\ 
    \xvector_{t+1} = \operatorname{tanh}\left(\sum_{i=1}^P \bm{\xi}_i f'\left(\bm{\xi}_i^T\xvector_t\right)\right),
\end{equation*}
where $\operatorname{tanh(\cdot)}$ is to ensure the neurons' states are constrained to the interval $[-1,1]$ so that the energy is bounded from below. Depending on the form of $f$, the network could have power or exponential storage capacity. If we set $f(x) = x^2$, this reduces to the traditional networks with linear capacity. 

If we further make modifications to the non-linearity in the energy function with $\operatorname{logsumexp(\cdot)}$, which is inspired by contrastive normalization, we can define the Modern Continuous Hopfield (MCH) energy function with a quadratic regularization term on $\xvector$:
\begin{equation*}
    E_{\text{MCH}} =  -\log\left(\sum_{i=1}^P \exp\left(\bm{\xi_i}^T\xvector\right)\right) + \frac{1}{2}\xvector^T\xvector.
\end{equation*}
By leveraging the concave-convex procedure \cite{yuille2003concave}, the update could be written as  
\begin{equation*}
    \xvector_{t+1} =\Xi\operatorname{softmax}(\Xi^T\xvector_t),
\end{equation*}
where $\Xi = [\xi_1, \dots, \xi_P] \in \mathbb{R}^{N\times P}$. This formulation has proven to converge to stationary points of the energy function $E_{\text{MCH}}$, and is linked to the key-value memory similar to the attention mechanism \cite{ramsauer2021hopfield}. 
Notice that this update rule is essentially the cross-attention given a query vector $\xvector$ and can only describe the independent evolution of that vector. It fails to faithfully cover the parallel interactions between contextual tokens in the self-attention adopted in the GPT or BERT style Transformers. 

The construction of the modern continuous Hopfield energy and update rule can also be carried out from a biologically plausible view by extending the network with hidden neurons and establishing a group of coupled differential equations. We refer the readers to \cite{krotov2021large,krotov2023new} for more details.

\section{Hypersphere Energy Transformer from Iterative Energy Minimization}
In this section, we formulate a new form of energy function as the sum of two modified Hopfield energy functions defined on a hypersphere embedded in multiple subspaces. We will demonstrate how we can derive the structure of Transformers, with self-attention, feedforward, skip connections, and normalization by performing the energy minimization. The overview is presented in Figure~\ref{fig:overview}.   
\begin{figure}[tbp]
\vskip -.1in
    \centering
    \includegraphics[width=\linewidth]{dynamicsonhypersphere.pdf}
        \vskip -.2in
    \caption{Evolution of Tokens on Subspaces. Tokens projected onto subspaces are progressively separated on the hypersphere as the optimization iterates.}
    \label{fig:dynamics_on_hypersphere}
    \vskip -.2in
\end{figure}
% \vspace{-0.5cm}
\subsection{Hyperspherical Energy}
\label{section:hyperspherical energy}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{overviewofHyperSET.pdf}
    \vskip -0.2in
    \caption{Overview of Our Hyperspherical Energy Transformer Layer. It recovers sequential stacking of symmetric self-attention, feedforward, skip connection and RMSNorm components from sheer minimization of Hopfiel energy. Adaptive step size is learned conditioned the current $t$ and the initial input $\Xmatrix_0$.}
    % \vskip -.2in
    \label{fig:overview}
\end{figure*}
\subsubsection{Overcoming Token Synchronization}
We consider $N$ vectors $\Xmatrix = [\xvector_1, \dots, \xvector_N]$ from a probabilistic space with $\xvector_i \in \mathbb{R}^d, i\in [N]$, which can be seen as the contextual tokens in Transformers, and denote two different sets of basis vectors of this $d$-dimensional space, $\Wmatrix = [\Wmatrix_1, \dots, \Wmatrix_H] \in \mathbb{R}^{d\times Hp}$ and $\Dmatrix = [\dvector_1, \dots, \dvector_M] \in \mathbb{R}^{d\times M}$. Here, $\Wmatrix_i \in \mathbb{R}^{d\times p}$ represents the projection to the $p$-dimensional subspace. Unless otherwise specified, we assume the column vectors are incoherent and span the full space, i.e., $Hp=M=d$.

A recent study \cite{yu2023white} argues that the contextual tokens lie on a low-dimensional subspace of its high-dimensional ambient space. We adopt this view and study the projection of tokens with bases $\Wmatrix$:
\begin{equation}
\label{eq:1}
    \zvector_i^h = \Wmatrix_h^T\xvector_i.
\end{equation}
Minimizing Hopfield energy $E_\text{MCH}$ tends to align the vector to the stored patterns while keeping its norm. This interaction occurs between the dynamic tokens and the static stored patterns. However, in Transformers' self-attention, this interaction happens among all the dynamic tokens simultaneously. Enforcing the tokens to align with one another would make them cluster into one point, losing the expressive power of the module. This phenomenon has been observed in many studies, referred to as token uniformity \cite{chen2022principle,wu2024demystifying}, or rank collapse \cite{pmlr-v139-dong21a}, and theoretically characterized in \cite{geshkovski2023the}. This also relates to the synchronization effect \cite{acebron2005kuramoto,miyato2024artificial}. 

Therefore, to overcome this issue, we extend the Hopfield energy $E_\text{MCH}$ to model the push-away force among tokens. The energy that drives the tokens apart in terms of angles in one subspace would read as 
\begin{equation}
\label{eq:2}
    E_{\text{ATTN}}^h = \beta^{-1}\sum_{i=1}^N\log\left(\sum_{j=1}^N \exp\left(\beta(\zvector_i^h)^T(\zvector_j^h)\right)\right), 
\end{equation}
where $\beta$ is usually the inverse of temperature. Here we use the subscript $_\mathrm{ATTN}$ as this energy will be shown to be related to the design of the attention layers, resembling that in \citep{yu2023white}. The total energy that models the interacting tokens corresponding to the bases $\Wmatrix$ partitioned by different subspaces would be 
\begin{equation}
        E_{\text{ATTN}} = \sum_{h=1}^H E_{\text{ATTN}}^h, \quad\text{subject to} ~\|\Wmatrix_h^T\xvector_i\|_2 = \sqrt{p}. \label{eq:3}
\end{equation}
The above dynamics should take place on a hypersphere with its radius determined by the dimension of the subspace. By minimizing~\eqref{eq:3}, the tokens are encouraged to be distributed on the sphere as uniformly as possible. An illustrative example is shown in Figure~\ref{fig:dynamics_on_hypersphere}.

\subsubsection{Alignment with High-dimensional Bases}
The tokens projected to subspaces separate to occupy more volume and enrich the information they encode. To make the overall information minimal and sufficient \cite{tishby2000information,tishby2015deep}, an intuition is that tokens in the \textit{original} high-dimensional space should encode less information to reduce the uninformative redundancy. This implies that tokens in the original space should coalesce into several distinct clusters. As in high-dimensional space, vectors initialized at random tend to be near-orthogonal \cite{vershynin2018high}, the basis vectors $\Dmatrix$ would naturally comprise these clusters. Thus, it would be reasonable to enable the alignment of tokens with these bases. The energy that implements this idea could be written as 
\begin{align}
\label{eq:4}
    E_{\text{FF}} &=  -\frac{1}{2}\sum_{i=1}^N\sum_{m=1}^M\left(\operatorname{ReLU}\left(\dvector_m^T\xvector_i\right)\right)^2, \nonumber\\
    &\text{subject to} ~\|\Dmatrix^T\xvector_i\|_2 = \sqrt{M}.
\end{align}
Here we use the subscript $_\mathrm{FF}$ as this energy will be shown to be related to the design of feedforward layers.
By minimizing this energy, a token tends to lie on the union of half-spaces defined by basis vectors that form an acute angle with this token while still residing on the hypersphere of the original space. 

\subsection{Symmetric Structure Induced From Energy Minimization}
\label{section:symmetric structure induced from energy minimization}
By combining the hyperspherical energy defined in Section~\ref{section:hyperspherical energy}, we introduce a unified objective function that characterizes the functionality the Transformer layer represents:
\begin{align}
\label{eq:5}
    \min_{\xvector_1, \dots,\xvector_N \in \Xmatrix} \quad & E(\Xmatrix ; \Wmatrix, \Dmatrix) = E_{\text{ATTN}} + E_{\text{FF}} \\
    \text{subject to} \quad & \|\Wmatrix_h^T\xvector_i\|_2 = \sqrt{p} \nonumber \\
    & \|\Dmatrix^T\xvector_i\|_2 = \sqrt{M}, \quad i = 1, \dots, N. \nonumber
\end{align}
This characterization of Transformer optimization problem somewhat resonates with the \textit{compression-sparsification} procedure in \cite{yu2023white}, where they frame the objective as compressing information of tokens in the subspaces and enlarging volume in the original space. Yet our objective can be interpreted from the opposite direction. We aim to gradually enrich the information in the subspaces while peeling off redundancy in the original space. This also mirrors the rationale of minimal and sufficient statistics from the information bottleneck \cite{tishby2015deep}.

To solve optimization~\eqref{eq:5}, we consider an alternating minimization method by splitting it into sub-problems. 

\subsubsection{Attention Module}
\label{section:attention}
To show how we have an attention module derived from minimizing hyperspherical energy, we first establish the differential equation that models the evolution of tokens' interactions: 
\begin{equation}
\label{eq:6}
\begin{split}
\dot{\Xmatrix} &= -\nabla_{\Xmatrix} E_{\text{ATTN}}   \\
&=-\sum_{h=1}^H\left( (\Wmatrix_h\Wmatrix_h^T\Xmatrix \underbrace{\operatorname{softmax}}_{\text{column-wise}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\Xmatrix)\right)  \right.\\
& \left. \qquad+\Wmatrix_h\Wmatrix_h^T\Xmatrix \underbrace{\operatorname{softmax}}_{\text{row-wise}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\Xmatrix)\right) \right)
\end{split}   
\end{equation}
where $\beta=1/\sqrt{p}$ as in standard Transformers \cite{NIPS2017_3f5ee243}. Derivations could be found in Appendix~\ref{section:derivation of E attn} 

The constraint on the low-dimensional hypersphere of radius $\sqrt{p}$ corresponds to $\operatorname{RMSNorm(\cdot)}$, which bears resemblance to QK-normalization \cite{henry-etal-2020-query}, but here the normalization is applied after projection by the same query-key-value matrix. The projection of tokens in $h$-th subspace onto the hypersphere would thus read as
\begin{equation}
\label{eq:7}
\Zmatrix_{\text{RMS}}^h = \operatorname{RMSNorm}(\Zmatrix^h) = \operatorname{RMSNorm}(\Wmatrix_h^T\Xmatrix).
\end{equation}
By discretizing the differential equation~\eqref{eq:6} with step size $\alpha_t$ and maintaining the hyperspherical constraint with~\eqref{eq:7}, 
we can derive the complete component that makes up the self-attention module: let $[\Qmatrix\Kmatrix]_{\text{RMS,t}}= \beta (\Zmatrix_{\text{RMS},t}^h)^T (\Zmatrix_{\text{RMS},t}^h) $, then
% \begin{equation}
% \label{eq:8}
% \begin{split}
% \Xmatrix_{t+1} 
% &=\Xmatrix_{t} - \alpha_t \sum_{h=1}^H\left( (\Wmatrix_h\Zmatrix_{\text{RMS},t}^h \underbrace{\operatorname{softmax}}_{\text{column-wise}}\left(\beta (\Zmatrix_{\text{RMS},t}^h)^T(\Zmatrix_{\text{RMS},t}^h)\right) + \right.\\
% & \left. \Wmatrix_h\Zmatrix_{\text{RMS},t}^h\underbrace{\operatorname{softmax}}_{\text{row-wise}}\left(\beta (\Zmatrix_{\text{RMS},t}^h)^T(\Zmatrix_{\text{RMS},t}^h)\right) \right) 
% \end{split}   
% \end{equation}
\begin{align}
\label{eq:8}
\Xmatrix_{t+1} &= \Xmatrix_{t} - \alpha_t \sum_{h=1}^H \Bigg( \Wmatrix_h\Zmatrix_{\text{RMS},t}^h \underbrace{\operatorname{softmax}}_{\text{column-wise}} \left([\Qmatrix\Kmatrix]_{\text{RMS,t}}\right) \nonumber \\
&\quad + \Wmatrix_h\Zmatrix_{\text{RMS},t}^h \underbrace{\operatorname{softmax}}_{\text{row-wise}} \left( [\Qmatrix\Kmatrix]_{\text{RMS,t}} \right) \Bigg).
\end{align}
This update brings up a new attention module with skip connection, and it has highly symmetric structures. On one hand, the query-key dot product is symmetric, while at the same time, the attention matrix is symmetric as well due to the sum of $\operatorname{softmax}$ in both column and row directions. The sum over $H$ different subspaces can be understood as the multi-head structure. 

Another interesting connection with prior work is that a doubly stochastic attention matrix has proven to have equivalence to the Wasserstein gradient flow of some global energy \cite{pmlr-v151-sander22a}. Here we offer another variant of attention that meets the symmetric query-key dot-production assumption therein and can also be seen as the discretization of an explicit energy function.
\subsubsection{FeedForward Module}
\label{section:section ff}
For the sub-problem of minimizing the other energy, we have a similar construction of the corresponding differential equation, with details deferred to Appendix~\ref{section:derivation of E ff}:
\begin{equation}
\label{eq:9}
\dot{\Xmatrix} = -\nabla_{\Xmatrix} E_{\text{FF}} = \Dmatrix\operatorname{ReLU}\left(\Dmatrix^T\Xmatrix\right).
\end{equation}
By further imposing the hypersphercial constraint, we can recover the feedforward layer used in Transformers with step size $\gamma_t$:
\begin{equation}
\label{eq:10}
\Xmatrix_{t+1} = \Xmatrix_t + \gamma_t \Dmatrix\operatorname{ReLU}\left(\operatorname{RMSNorm}\left(\Dmatrix^T\Xmatrix \right)\right)  
\end{equation}
Notice that this feedforward module also bears symmetry in the weight space. 
\subsection{Learning Adaptive Step Size}
To make the step size more flexible, we choose to learn its embedding with another neural network conditioning on the current iteration $t$ and the initial token $\xvector(0)$ (usually the output of the tokenizer):
\begin{align}
\alpha_t &= \bm{\alpha}_\theta (t, \xvector(0)),   \label{eq:11} \\ 
\gamma_t &= \bm{\gamma}_\phi (t, \xvector(0)).     \label{eq:12}
\end{align}
For each iteration, step size embeddings in~\eqref{eq:11} and~\eqref{eq:12} are applied channel-wise of each token, similar to techniques in \cite{touvron2021going,peebles2023scalable} (See Figure~\ref{fig:time embedding}). We also adopt the zero-initialization of network parameters $\theta$ and $\phi$ from \cite{bachlechner2021rezero} to facilitate convergence when using larger iterations. 

Combining all the components and techniques, we present the \textit{Hyper-Spherical Energy Transformer} (Hyper-SET) with attention and feedforward sequentially stacked and with only one layer of learnable parameters. This one-layer model is amenable to rigorous analysis and, as we will demonstrate later, has competitive performance with vanilla Transformer.



\section{Experiment}
% \subsection{Synthetic Data}
% \subsubsection{Matrix Completion}
In this section, we compare Hyper-SET with standard Transformers on discriminative and generative learning tasks. For fair comparison, we remove biases in the latter, adopt the Pre-Norm style with $\operatorname{RMSNorm}$, and omit dropout regularization. The MLP ratio in Transformer is set to 4.

As one iteration of energy minimization corresponds to single-layer update of tokens in Hyper-SET, we use one-layer trainable parameters but vary the iteration for all models, including Transformers, unless otherwise specified \footnote{For instance, 12 iterations mean applying the layer repeatedly for 12 times.}. 
% All experiments fit the memory size of an 80GB NVIDIA A100 GPU. 

\subsection{Solving Sudoku} Solving a Sudoku puzzle involves filling a 9x9 board with partially known digits from 1 to 9, and unknown entries are given as 0. The unknown entries must be filled with digits perfectly such that the board satisfies a certain rule. We tackle this puzzle by predicting the digits to fill in, conditioned on the given digits. 
%It can be viewed as a simplified masked modeling on synthetic data. 
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{sudoku_extropolation.pdf}
        \vskip -.2in
    \caption{Test-time Extrapolation on Sudoku Board Accuracy w.r.t the Number of Forward Iterations. Our model achieves better performance with fewer parameters, even when the iterations are beyond the training regime. Results are averaged over five runs. }
    \label{fig:test-time extrapolation}
\end{figure}

\textbf{Setup.} We adopt the dataset from \cite{palm2018recurrent} which is considered to be hard, provided that the board thereof only has [17,34] known digits. We build on the code \footnote{\href{https://github.com/azreasoners/recurrent_transformer}{https://github.com/azreasoners/recurrent\_transformer}} from \cite{yang2023learning} and follow the setting of training on 9k data and evaluating on 1k data. The cross-entropy loss is computed exclusively on the unknown entries.

We train all models for 200 epochs with batch size of 16 and set the optimizer as AdamW \cite{loshchilov2018decoupled} with 0.1 weight decay. Learning rate is initialized as 1e-4 with cosine decay. The hidden dimension is 768. We measure the accuracy of solving Sudoku on the test dataset. See Appendix~\ref{section:sudoku setup} for details. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{energy_sudoku.pdf}
    \includegraphics[width=\linewidth]{energy.pdf}
        \vskip -.2in
    \caption{The energy of both the attention and feedforward module decreases on Sudoku \textbf{(Top)} and CIFAR-10 \textbf{(Bottom)}, without hard constraints on the sign of the step size. This suggests the layer aligns well with the optimization objective. Normalization is first applied to meet the condition in \eqref{eq:5} before computing the energy.}
    \label{fig:energy}
\end{figure}

\textbf{Extrapolation to Larger Iteration.}
Under the same experimental conditions except for the architectural differences, our model surpasses the standard Transformer for in-distribution iterations (54.70 vs. 49.30), i.e., using the same iterations for both training and inference. The results are displayed in Figure~\ref{fig:test-time extrapolation}.

Solving Sudoku could be seen as a logical reasoning task \cite{wang2019satnet}. Some efforts have been devoted to allocating more computation at test time for better reasoning and generalization \cite{schwarzschild2021can,bansal2022end,du2022learning,banino2021pondernet}, hoping the network can extend the algorithms learned during training. 
% There are also arguments for improving compositional generalization with universal transformers (weight-tying like ours) \cite{ontanon-etal-2022-making,petty2024impact}.

We build on these views by extending the number of iterations at test time up to two times those during training. As shown in Figure~\ref{fig:test-time extrapolation}, with more computation, our model achieves better scaling behaviors than Transformers with larger improvements on accuracy. This extrapolation could stem from learned adaptive step size that maintains energy minimization trajectory. In practice, we also find out that trainable positional encoding is crucial for the extrapolation. 



\textbf{Energy Evolution.}
Figure~\ref{fig:energy} (Top) illustrates the energy trajectory of the attention $E_{\text{ATTN}}$ module and the feedforward module $E_{\text{FF}}$ under the hyperspherical constraint in~\eqref{eq:5}. It is interesting to see that without a positive threshold for step size $\alpha_t$ and $\gamma_t$, the energy decreases within training iterations and extrapolates smoothly beyond them, confirming the generalization of learned step sizes.  
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{rank_angle_sudoku.pdf}
    \includegraphics[width=\linewidth]{rank_angle.pdf}
        \vskip -.2in
    \caption{The effective rank and the average angle of tokens projected to one subspace gradually increase, suggesting a larger volume spanned by these tokens. Results are from Sudoku test dataset \cite{palm2018recurrent} (\textbf{Top}) and CIFAR-10 validation set (\textbf{Bottom}).}
    \label{fig:rank angle}
\end{figure}

\textbf{Effective Rank \& Average Angle.}
To validate our motivation to mitigate token synchronization in the subspaces, we utilize two metrics to quantify the separation of tokens. 

\begin{definition}[Effective Rank]
\label{definition:effective rank}
For a matrix $\Xmatrix \in \mathbb{R}^{d\times N}$, let $\Sigma=[\sigma_1,\dots,\sigma_r]$ be its singular values where $r$ is its full rank and denote $p_i = \sigma_i / \sum_{j=1}^r \sigma_j$ the discrete probability. The effective rank \cite{roy2007effective,guo2023contranorm} is defined as the exponential of the entropy 
\begin{equation}
\label{eq:13}
\exp(-\sum_{i=1}^r p_i \log p_i).
\end{equation}
\end{definition}

\begin{definition}[Average Angle]
\label{definition:average angle}
Given a set of vectors $\Xmatrix = [\xvector_1, \dots, \xvector_N] \in \mathbb{R}^{d\times N}$, the average angle of these vectors is 
\begin{equation}
\label{eq:14}
\operatorname{arccos} \frac{2}{N(N-1)}\sum_{i=1}^N\sum_{j=i+1}^{N}\frac{\xvector_i^T\xvector_j}{\|\xvector_i\|_2\|\xvector_j\|_2}.
\end{equation}
\end{definition}

The effective rank is a continuous approximation of the full rank and, similar to the average angle, reflects the extent to which a set of vectors distributes uniformly.  We present the results of these two metrics in Figure~\ref{fig:rank angle} (Top) using the token projected to one subspace~\eqref{eq:1}. Full results are in Appendix~\ref{section:rank angle sudoku complete}. As the forward optimization progresses, the effective rank within the subspace steadily rises while the full rank remains unchanged. This dynamic mirrors that of the average angle, which increases from around $70^{\circ}$ to near orthogonal. This implies that tokens in the subspaces occupy maximal hyperspherical volume, and the information encoded in the low dimension gradually saturates. %It corroborates our initial design goal of the attention energy $E_{\text{ATTN}}$.
% \subsection{Real-World Data}
\begin{figure}[tbp]
    \centering
    \includegraphics[width=\linewidth]{barplotwithupperbound.pdf}
    \vskip -.2in
    \caption{Top-1 Accuracy (\textit{Left}) and the Number of Parameters (\textit{Right}) on CIFAR-10 with Different Layer-Iteration Trade-offs. }
    \label{fig:bar plot cifar10}
\end{figure}

\subsection{Image Classification}
\label{section:image classification}
\textbf{Setup \& Results.}
We also evaluate Hyper-SET's discriminative capability on CIFAR-10, CIFAR-100, and ImageNet-100\footnote{We use the images from ImageNet-1k with classes provided by \href{https://github.com/HobbitLong/CMC/blob/master/imagenet100.txt}{\text{https://github.com/HobbitLong/CMC/blob/master/imagenet100.txt}}}. We compare against vision Transformers (ViTs), a recently introduced white-box Transformer CRATE \cite{yu2023white}, and its variant CRATE-T that aims for a more faithful implementation \cite{hu2024an}. All models are trained for 200 epochs (batch size 128, 12 iterations, Adam \cite{kingma2014adam} optimizer, and cosine learning rate decay from 1e-4, no weight decay), with a learnable token $[\texttt{CLS}]$ for classification. Detailed configurations are in Appendix~\ref{section:image classification setup}. 

As shown in Table~\ref{tab:classification}, with the same hidden dimension, our model surpasses others on CIFAR-10, but underperforms Transformer on CIFAR-100 and ImageNet-100. Noticeably, our architecture can save around $40\%$ of Transformer's parameters. By scaling up the hidden dimension to match the Transformer's parameters, our model yields the best result, though performance on ImageNet-100 remains comparable.  

\begin{table}[!tbp]
\caption{Top-1 Accuracy on Image Classification. Models are under the same hidden dimension and 12 iterations. Scaling up our model to match Transformer's parameters gives better performance.}
\label{tab:classification}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccc}
\toprule
\multirow{2}{*}{Models}  & \multirow{2}{*}{Config ($\#$ Params)} & \multicolumn{3}{c}{Dataset}\\
\cmidrule(lr){3-5}
 &  & CIFAR-10 & CIFAR-100 & IM-100\\
\midrule
% \multirow{2}{*}{Transformer}  & Tiny (2.07245 M) & 84.55 & 54.41 &  58.76\\
%                               & Base (xx M) &  xxx & xxx & xxx \\
% \multirow{2}{*}{CRATE}  & Tiny (xx M) & xxx & xxx & xxx\\
%                               & Base (xx M) &  xxx & xxx & xxx \\
% \multirow{2}{*}{Ours}  & Tiny (xx M) & 84.64 & 54.15 & 54.78\\
%                               & Base (xx M) &  85.60 & xxx & xxx \\
Transformer  & Small (2.07 M) & 84.55 & 54.41 &  \textbf{57.82}\\
CRATE-T   & Small (0.60 M) & 78.57 &  47.55 & 50.14 \\
CRATE  & Small (0.75 M) & 80.73 & 49.25 & 53.24\\
Ours  & Small (1.24 M) & 84.64 & 54.15 & 54.78\\
\midrule
% CRATE & Base (2.37466 M) & 84.94 & 53.75 & xxx  \\
Ours  & Small Scale-up (1.98 M) & \textbf{85.27} & \textbf{55.94} & 55.98\\
\bottomrule
\end{tabular}
}
\end{table}
\begin{table*}[htbp]
\caption{Comparison of Masked Image Modeling Performance on ImageNet-100 (5k). Our model lags behind Transformer when given the same number of iterations but matches its performance if scaling up the iterations and width of the feedforward module (a.k.a, larger $M$). Our model is also more parameter-efficient. }
\label{tab:masked image modeling}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccccc}
\toprule
Models  &  Layer / Iteration / FF Ratio $M$ & PSNR ($\uparrow$) & SSIM ($\uparrow$) & Multi-Scale SSIM ($\uparrow$) & LPIPS ($\downarrow$)& FID ($\downarrow$)\\
\midrule
Transformer  & 1 / 12 / 4$d$ ~(8.85 M) & 15.953 & 0.417 & \textbf{0.599} & \textbf{0.327} & \textbf{43.428}
\\
Ours  & 1 / 12 / $d$ ~(3.94 M) & 15.713 & 0.411 & 0.576 & 0.358 & 59.841\\
Ours  & 1 / 24 / 8$d$ ~(8.07 M) & \textbf{15.955} & \textbf{0.417 }& 0.596 & 0.332 & 45.174\\
\bottomrule
\end{tabular}
}
\end{table*}

\textbf{Layer-Iteration Trade-off.}
So far, the classification is conducted under the condition of a one-layer model. A natural question is how well the model will perform when stacking multiple layers with different parameters. To see this, we first run the Transformer with 12 layers, which has the same effective depth as one layer with 12 iterations, as an upper bound. Then, we vary the ratio of distinct layers and their respective iterations while maintaining the effective depth. This configuration can be interpreted as adding flexibility to the basis vectors. 
%which the energy is conditioned on. 


 In Figure~\ref{fig:bar plot cifar10}, our scaling-up $\texttt{small}$ model has parameter-efficiency at different layer-iteration ratios, and this strength is significantly sharpened when more independent layers are learned. However, this architectural efficiency limits its scalability beyond two layers. By further scaling to the $\texttt{Base}$ configuration, ours consistently outperforms Transformer even surpassing the upper bound while retaining parameter efficiency.
 %The $\texttt{Base}$ model is still parameter-efficient with more layers. 
 
\textbf{Energy, Effective Rank \& Average Angle.}
The hyperspherical energy monotonically decreases as in Figure~\ref{fig:energy} (Bottom) for our $\texttt{Small}$ model with effective rank and average angle mirroring trends from the Sudoku experiment in Figure~\ref{fig:rank angle} (Bottom). Complete results are in Appendix~\ref{section:rank angle cifar10 complete}.


% \begin{table}[htbp]
% \caption{Top-1 Accuracy on Image Classfication.}
% \label{tab:classification}
% \centering
% \resizebox{\linewidth}{!}{
% \begin{tabular}{llcc}
% \toprule
% \multirow{2}{*}{Datasets}  & \multirow{2}{*}{Models} & \multicolumn{2}{c}{Configuration}\\
% \cmidrule(lr){3-4}
%  &  & Tiny & Base\\
% \midrule
% \multirow{3}{*}{CIFAR-10}  & Transformer &  xx &  xx\\
%                               & CRATE &  xx & xx  \\
%                              & Ours  & xx & xx \\
% \multirow{3}{*}{CIFAR-100}  & Transformer &  xx &  xx\\
%                               & CRATE &  xx & xx  \\
%                              & Ours  & xx & xx \\
% \multirow{3}{*}{Imagenet-100}  & Transformer &  xx &  xx\\
%                               & CRATE &  xx & xx  \\
%                              & Ours  & xx & xx \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

\subsection{Masked Image Modeling}
\textbf{Setup.}
Masked image modeling has recently reclaimed its attention for autoregressive generation \cite{li2023mage,pmlr-v202-chang23b,fan2024fluid,li2024autoregressive}, where the generation is framed as recovering images from $100\%$ masking. Due to its high computational demand with large-scale models, we attempt to demonstrate the power of our one-layer model specifically for image reconstruction. ImageNet-100 is used, the same as in Section~\ref{section:image classification}.  We build on prior work \cite{chang2022maskgit} and leverage the open-source repository \footnote{\href{https://github.com/valeoai/Maskgit-pytorch}{\text{https://github.com/valeoai/Maskgit-pytorch}}}. We follow its setting by using VQ-VAE from \cite{esser2021taming} as the image tokenizer with a codebook size of 1024. See Appendix~\ref{section:masked image modeling setup} for concrete settings.

\textbf{Results \& Visualization.}
We evaluate the quality of the reconstructed images of masking out $40\%$ of the images with $\texttt{Base}$ configurations. We report Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Multi-Scale SSIM, Learned Perceptual Image Patch Similarity (LPIPS) and Fr$\acute{\text{e}}$chet Inception Distance (FID) on the validation set (5k). The results of other ratios and more visualization will be presented in Appendix~\ref{section:additional results of masked image modeling}.



Numerical results are presented in Table~\ref{tab:masked image modeling}. Under the same number of iterations, our model significantly reduces parameters but lags behind Transformer on all metrics. If further increasing its iterations and the width of feedforward module $M$ to 8$d$, it can fill in the performance gap but at the cost of more computation. A visual comparison is in Figure~\ref{fig:visualization}.
\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\linewidth]{visualization.pdf}
        % \vskip -.2in
    \caption{Visual Comparison on Masked Image Modeling on ImageNet 256$\times$256. Our model, when scaling to Transformer scale with additional compute, can achieve similar reconstruction quality when masking ratio $=40\%$.}
    \label{fig:visualization}
\end{figure}
% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

\section{Conclusion}
We present \textbf{Hyper-SET}, a Transformer architecture designed via iterative optimization of hyperspherical energy functions, bridging energy-based learning and practical model design. By formulating dual energy objectives on the hypersphere, Hyper-SET mitigates token synchronization in the subspaces while promoting directional alignment with bases in the original space, recovering core Transformer components with intrinsic interpretability. Empirically, Hyper-SET matches or surpasses vanilla Transformers on Sudoku solving, image classification, and masked image modeling tasks with fewer parameters. This work aims to advance principled Transformer design, offering an avenue for building efficient, interpretable architectures grounded in optimization dynamics.
% Further improvements sparse \cite{tan2023sparse} or mixture-of-experts \cite{csordas2024moeut} techniques.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.


\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Detailed Experimental Setup and Model Configuration}
\subsection{Network to Learn Adaptive Step Size}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{time_embedding.pdf}
        \vskip -.2in
    \caption{An Illustration of Time Embedding Conditioned on the Input to Learn Adaptive Step size.}
    \label{fig:time embedding}
\end{figure}
\subsection{Solving Sudoku}
\label{section:sudoku setup}
Table~\ref{tab:training recipe for solving Sudoku} and Table~\ref{tab:model configuration for solving Sudoku} show the training recipe and model configurations for solving Sudoku. We train the model with 24 iterations and can evaluate beyond these iterations.

\begin{table}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
    \caption{Training Recipe for Solving Sudoku.}
    \label{tab:training recipe for solving Sudoku}
        \centering
        \begin{tabular}{ll}
            \toprule
            Configuration  & Value \\
            \midrule
            Epochs  &  200 \\
            Batch size & 16 (on 1 Nvidia 3090 GPU)\\
            Number of training samples & 9k \\
            Number of evaluating samples & 1k \\
            Optimizer & AdamW ($\beta_1=0.0,~\beta_2=0.95$)\\
            Weight decay & 0.1  \\
            Learning rate (lr) & 1e-4\\
            Lr decay & Cosine \\
            Gradient clipping & 1.0 \\
            \bottomrule
        \end{tabular}
        
    \end{minipage}%
    \hspace{0.08\textwidth} % Space between the tables
    \begin{minipage}{0.45\textwidth}
        \caption{Model Configuration for Solving Sudoku.}
    \label{tab:model configuration for solving Sudoku}
        \centering
        \begin{tabular}{ll}
            \toprule
            Configuration  & Value \\
            \midrule
            Vocabulary size & 10 \\
            Layer  &  1 \\
            Iterations & 24 \\
            Hidden dimension $d$ & 768 \\
            Feedforward ratio $M$ & 4$d$ \\
            Number of heads $H$ & 12 \\
            Positional encoding & Learnable \\
            Time embedding condition & $\Xmatrix_0$ \\
            Time embedding frequency & 512 \\
            \midrule
            Number of parameters & 5.20 M \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}

\subsection{Image Classification}
\label{section:image classification setup}
Table~\ref{tab:training recipe for image classfication} and Table~\ref{tab:model configuration for image classfication} present the training recipe and model configurations on image classification, where the number of our model is computed on CIFAR-10. In practice, we use absolute sinusoidal positional encoding and adopt conditioning on $\Xmatrix_t$ for performance reasons. Table~\ref{tab:scaling modeling configuration} lists the configuration of different sizes and it applies to other tasks as well.

% \begin{table}[htbp]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%     \centering
%     \caption{Training Recipe for Image Classification.}
%     \label{tab:training recipe for image classfication}
%         \begin{tabular}{ll}
%             \toprule
%             Configuration  & Value \\
%             \midrule
%             Epochs  &  200 \\
%             Batch size & 128 (on 1 Nvidia 3090 GPU)\\
%             Number of training samples & 50k \\
%             Number of evaluating samples & 10k \\
%             Optimizer & Adam ($\beta_1=0.0,~\beta_2=0.95$)\\
%             Weight decay & 0.0  \\
%             Learning rate (lr) & 1e-4\\
%             Lr decay & Cosine \\
%             Gradient clipping & 1.0 \\
%             Input size & 224 \\
%             \bottomrule
%         \end{tabular}
        
%     \end{minipage}%
%     \hspace{0.08\textwidth} % Space between the tables
%     \begin{minipage}{0.45\textwidth}
%     \centering
%         \caption{Model Configuration for Image Classification $\texttt{Small}$).}
%     \label{tab:model configuration for image classfication}
        
%         \begin{tabular}{ll}
%             \toprule
%             Configuration  & Value \\
%             \midrule
%             Patch size & 16 \\
%             Layer  &  1 \\
%             Iterations & 12 \\
%             Hidden dimension $d$ & 384 \\
%             Feedforward ratio $M$ & $d$ \\
%             Number of heads $H$ & 6 \\
%             Positional encoding & Sinusoidal\\
%             Time embedding condition & $\Xmatrix_t$ \\
%             Time embedding frequency & 512 \\
%             \midrule
%             Number of parameters & 1.24 M \\
%             \bottomrule
%         \end{tabular}
%     \end{minipage}
% \end{table}
\begin{table}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \captionof{table}{Training Recipe for Image Classification.}
        \label{tab:training recipe for image classfication}
        \begin{tabular}{ll}
            \toprule
            Configuration  & Value \\
            \midrule
            Epochs  &  200 \\
            Batch size & 128 (on 1 Nvidia 3090 GPU)\\
            Number of training samples & 50k \\
            Number of evaluating samples & 10k \\
            Optimizer & Adam ($\beta_1=0.0,~\beta_2=0.95$)\\
            Weight decay & 0.0  \\
            Learning rate (lr) & 1e-4\\
            Lr decay & Cosine \\
            Gradient clipping & 1.0 \\
            Input size & 224 \\
            \bottomrule
        \end{tabular}
    \end{minipage}%
    \hspace{0.08\textwidth} % Space between the tables
    \begin{minipage}{0.45\textwidth}
        \centering
        \captionof{table}{Model Configuration for Image Classification.}
        \label{tab:model configuration for image classfication}
        \begin{tabular}{ll}
            \toprule
            Configuration ($\texttt{Small}$) & Value \\
            \midrule
            Patch size & 16 \\
            Layer  &  1 \\
            Iterations & 12 \\
            Hidden dimension $d$ & 384 \\
            Feedforward ratio $M$ & $d$ \\
            Number of heads $H$ & 6 \\
            Positional encoding & Sinusoidal\\
            Time embedding condition & $\Xmatrix_t$ \\
            Time embedding frequency & 512 \\
            \midrule
            Number of parameters & 1.24 M \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}

\begin{table}[htbp]
\caption{Model Configuration of Different Sizes}
\label{tab:scaling modeling configuration}
\centering
\begin{tabular}{llll}
\toprule
Configurations  &  $\texttt{Small}$ & $\texttt{Small}$ Scale-up & $\texttt{Base}$\\
\midrule
Hidden dimension $d$  & 384 & 512 & 768 \\
Number of heads $H$  & 6 & 8 & 12 \\
\bottomrule
\end{tabular}

\end{table}

\subsection{Masked Image Modeling}
\label{section:masked image modeling setup}

We follow \cite{chang2022maskgit} using VQ-VAE to tokenize the images to $16\times16$ latent code with codebook size of 1024 after resizing the input to $256\times256$. Masking ratio is randomly chosen between $[0,0.4]$, and the masked region is replaced by a learnable token. Training loss is computed only for the masked tokens. We also follow the iterative decoding process in \cite{chang2022maskgit} with $\text{temperature}=1$ and decoding step $T=24$. We also remove the MLP following the time embedding and set the embedding 
 frequency equal to the hidden dimension to save parameters, and we find out that this implementation works better. Table~\ref{tab:training recipe for masked image modeling} and Table~\ref{tab:model configuration for masked image modeling} show the detailed training recipe and configurations.
\begin{table}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
    \caption{Training Recipe for Masked Image Modeling.}
    \label{tab:training recipe for masked image modeling}
        \centering
        \begin{tabular}{ll}
            \toprule
            Configuration  & Value \\
            \midrule
            Epochs  &  300 \\
            Batch size & 256 (64 x 4 Nvidia 80 GB A100)\\
            Number of training samples & 126,689 \\
            Number of evaluating samples & 5,000 \\
            Optimizer & AdamW ($\beta_1=0.0,~\beta_2=0.95$)\\
            Weight decay & 0.1  \\
            Learning rate (lr) & 1e-4\\
            Lr decay & None \\
            Gradient clipping & 3.0 \\
            Input size & 256 \\
            \bottomrule
        \end{tabular}
        
    \end{minipage}%
    \hspace{0.08\textwidth} % Space between the tables
    \begin{minipage}{0.45\textwidth}
        \caption{Model Configuration for Masked Image Modeling.}
    \label{tab:model configuration for masked image modeling}
        \centering
        \begin{tabular}{ll}
            \toprule
            Configuration  & Value \\
            \midrule
            Vocabulary size & 1025 \\
            Layer  &  1 \\
            Iterations & 12 \\
            Hidden dimension $d$ & 768 \\
            Feedforward ratio $M$ & $d$ \\
            Number of heads $H$ & 12 \\
            Positional encoding & Sinusoidal \\
            Time embedding condition & $\Xmatrix_0$ \\
            Time embedding frequency & 768 \\
            \midrule
            Number of parameters & 3.94 M \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\end{table}


\section{Rank and Average Angle of Each Head}
\subsection{Sudoku Dataset}
\label{section:rank angle sudoku complete}

Figure~\ref{fig:rank sudoku complete} and Figure~\ref{fig:angle sudoku complete} 
 capture the evolution of the effective rank and average angle of all heads. Most of them follow the separation dynamics on the hypersphere, mirroring the insights we provide at the beginning of Section~\ref{section:rank angle sudoku complete}. 
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{rank_sudoku.pdf}
    \caption{The Effective Rank of Tokens Projected to Each Subspace. Results are from the test set of Sudoku dataset \cite{palm2018recurrent}.}
    \label{fig:rank sudoku complete}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{anglesudoku.pdf}
    \caption{The Average Angle of Tokens Projected to Each Subspace. Results are from the test set of Sudoku dataset \cite{palm2018recurrent}.}
    \label{fig:angle sudoku complete}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{CIFAR-10 Dataset}
\label{section:rank angle cifar10 complete}
The full results on CIFAR-10 also possess similar trends to those on the Sudoku dataset as shown in Figure~\ref{fig:rank cifar10 complete} and Figure~\ref{fig:angle cifar10 complete}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{rank.pdf}
    \caption{The Effective Rank of Tokens Projected to Each Subspace. Results are from CIFAR-10 validation set.}
    \label{fig:rank cifar10 complete}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{angle.pdf}
    \caption{The Average Angle of Tokens Projected to Each Subspace. Results are from CIFAR-10 validation set.}
    \label{fig:angle cifar10 complete}
\end{figure}



\section{Derivation}
\label{appendix:B}

\subsection{Derivation of the Gradient of $E_{\text{ATTN}}$}
\label{section:derivation of E attn}
% \begin{equation}
% \begin{split}
\begin{align*}
\dot{\xvector}_k &= -\nabla_{\xvector_k} E_{\text{ATTN}} \\
&= -\sum_{h=1}^H\left(\frac{\sum_{j=1}^N \Wmatrix_h \Wmatrix_h^T \xvector_j \exp\left(\beta(\Wmatrix_h^T\xvector_k)^T(\Wmatrix_h^T\xvector_j)\right)}{\sum_{j=1}^N\exp\left(\beta (\Wmatrix_h^T\xvector_k)^T(\Wmatrix_h^T\xvector_j) \right)} + \sum_{i=1}^N \frac{\Wmatrix_h\Wmatrix_h^T\xvector_i\exp\left(\beta (\Wmatrix_h^T\xvector_i)^T(\Wmatrix_h^T\xvector_h) \right)}{\sum_{j=1}^N\exp\left(\beta (\Wmatrix_h^T\xvector_i)^T(\Wmatrix_h^T\xvector_j)\right)} \right) \\
& =-\sum_{h=1}^H \left(\Wmatrix_h\Wmatrix_h^T[\xvector_1, \dots,\xvector_N] \begin{bmatrix}
\exp\left(\beta (\Wmatrix_h^T\xvector_k)^T(\Wmatrix_h^T\xvector_1) \right) \\
\vdots \\
\exp\left(\beta (\Wmatrix_h^T\xvector_k)^T(\Wmatrix_h^T\xvector_N) \right) 
\end{bmatrix} / \sum_{j=1}^N\exp\left(\beta (\Wmatrix_h^T\xvector_k)^T(\Wmatrix_h^T\xvector_j) \right) + \right. \\
&\left.  \qquad \qquad \qquad \qquad \qquad \qquad 
\sum_{i=1}^N \Wmatrix_h\Wmatrix_h^T\xvector_i\begin{bmatrix}
    \exp\left(\beta (\Wmatrix_h^T\xvector_1)^T(\Wmatrix_h^T\xvector_i) \right) / \sum_{j=1}^N\exp\left(\beta (\Wmatrix_h^T\xvector_i)^T(\Wmatrix_h^T\xvector_j)\right)\\
    \vdots \\
    \exp\left(\beta (\Wmatrix_h^T\xvector_N)^T(\Wmatrix_h^T\xvector_i) \right) / \sum_{j=1}^N\exp\left(\beta (\Wmatrix_h^T\xvector_i)^T(\Wmatrix_h^T\xvector_j)\right)
\end{bmatrix}_{k} \right)\\
&= -\sum_{h=1}^H \left(\Wmatrix_h\Wmatrix_h^T\Xmatrix\underbrace{\operatorname{softmax}}_{\text{column}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\xvector_k)\right) + \sum_{i=1}^N \Wmatrix_h \Wmatrix_h^T \xvector_i \underbrace{\operatorname{softmax}}_{\text{column}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\xvector_i)\right)_k\right) \\
&= -\sum_{h=1}^H \left(\Wmatrix_h\Wmatrix_h^T\Xmatrix\underbrace{\operatorname{softmax}}_{\text{column}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\xvector_k)\right) + \Wmatrix_h \Wmatrix_h^T [\xvector_1, \dots, \xvector_N] \underbrace{\operatorname{softmax}}_{\text{column}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\Xmatrix)\right)_{[k,:]}\right) \\
&= -\sum_{h=1}^H \left(\Wmatrix_h\Wmatrix_h^T\Xmatrix\underbrace{\operatorname{softmax}}_{\text{column}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\xvector_k)\right) + \Wmatrix_h \Wmatrix_h^T [\xvector_1, \dots, \xvector_N] \underbrace{\operatorname{softmax}}_{\text{row}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\Xmatrix)\right)_{[:,k]}\right) \\
&= -\sum_{h=1}^H \left(\Wmatrix_h\Wmatrix_h^T\Xmatrix\underbrace{\operatorname{softmax}}_{\text{column}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\xvector_k)\right) + \Wmatrix_h \Wmatrix_h^T \Xmatrix \underbrace{\operatorname{softmax}}_{\text{row}}\left(\beta (\Wmatrix_h^T\Xmatrix)^T(\Wmatrix_h^T\Xmatrix)\right)_{[:,k]}\right)
\end{align*}
% \end{split}
% \end{equation}


\begin{equation}
    \dot{\Xmatrix} = [\dot{\xvector}_1, \dots, \dot{\xvector}_N] = -\nabla_{\Xmatrix} E_{\text{ATTN}}  
=-\left( (\Wmatrix\Wmatrix^T\Xmatrix \underbrace{\operatorname{softmax}}_{\text{column-wise}}\left(\beta (\Wmatrix^T\Xmatrix)^T(\Wmatrix^T\Xmatrix)\right)  
+ \Wmatrix\Wmatrix^T\Xmatrix \underbrace{\operatorname{softmax}}_{\text{row-wise}}\left(\beta (\Wmatrix^T\Xmatrix)^T(\Wmatrix^T\Xmatrix)\right) \right)
\end{equation}

\subsection{Derivation of the Gradient of $E_{\text{FF}}$}
\label{section:derivation of E ff}

\begin{align*}
\dot{\xvector}_k &=  -\nabla_{\xvector_k}E_{\text{FF}} \\  
&=  \sum_{m=1}^M \operatorname{ReLU}(\dvector_m^T\xvector_k)\cdot\mathbb{I}(\dvector_m^T\xvector_k >0)\cdot \dvector_m \\
&= \sum_{m=1}^M \operatorname{ReLU}(\dvector_m^T\xvector_k)\dvector_m \\
&=[\dvector_1, \dots, \dvector_M]\begin{bmatrix}
    \operatorname{ReLU}(\dvector_1^T\xvector_k) \\
    \vdots \\
    \operatorname{ReLU}(\dvector_M^T\xvector_k)
\end{bmatrix}\\
&=\Dmatrix\operatorname{ReLU}(\Dmatrix^T\xvector_k)
\end{align*}

\begin{equation*}
\dot{\Xmatrix} = [\dot{\xvector}_1, \dots, \dot{\xvector}_N] = -\nabla_{\Xmatrix} E_{\text{FF}} = \Dmatrix\operatorname{ReLU}(\Dmatrix^T\Xmatrix)
\end{equation*}

\section{Additional Results of Masked Image Modeling}
\label{section:additional results of masked image modeling}

Table~\ref{tab:masked image modeling 0.1}, Table~\ref{tab:masked image modeling 0.2} and Table~\ref{tab:masked image modeling 0.3} summarize the results of masked image modeling with different masking ratios. When scaled to larger iterations and a wider feedforward module, our model achieves comparable results to Transformer but still slightly lags behind. This suggests the scalability of our model to the large configuration may be a bottleneck for its development and deployment. More visual comparison is provided in Figure~\ref{fig:visualization more results}.

\begin{table}[htbp]
\caption{Comparison of Masked Image Modeling Performance of Masking Ratio 0.1. }
\label{tab:masked image modeling 0.1}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccccc}
\toprule
Models  &  Layer / Iteration / FF Ratio $M$ (\# Params)& PSNR ($\uparrow$) & SSIM ($\uparrow$) & Multi-Scale SSIM ($\uparrow$) & LPIPS ($\downarrow$)& FID ($\downarrow$)\\
\midrule
Transformer  & L1 / Iter 12 / 4$d$ ~(8.85 M) & \textbf{17.693} & \textbf{0.466} & \textbf{0.709} & 0.236 & \textbf{22.428}
\\
Ours  & L1 / Iter 12 / $d$ ~(3.94 M) & 17.553 & 0.462 & 0.701 & 0.243 & 24.665\\
Ours  & L1 / Iter 24 / 8$d$ ~(8.07 M) & 17.673 & 0.465 & 0.708 & \textbf{0.236} & 22.517\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[htbp]
\caption{Comparison of Masked Image Modeling Performance of Masking Ratio 0.2. }
\label{tab:masked image modeling 0.2}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccccc}
\toprule
Models  &  Layer / Iteration / FF Ratio $M$ (\# Params)& PSNR ($\uparrow$) & SSIM ($\uparrow$) & Multi-Scale SSIM ($\uparrow$) & LPIPS ($\downarrow$)& FID ($\downarrow$)\\
\midrule
Transformer  & L1 / Iter 12 / 4$d$ ~(8.85 M) & \textbf{17.185} & \textbf{0.451} & \textbf{0.678} & \textbf{0.261} & \textbf{27.320}
\\
Ours  & L1 / Iter 12 / $d$ ~(3.94 M) & 16.988 & 0.444 & 0.662 & 0.275 & 33.637\\
Ours  & L1 / Iter 24 / 8$d$ ~(8.07 M) & 17.170 & 0.450 & 0.676 & 0.262 & 28.120\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[htbp]
\caption{Comparison of Masked Image Modeling Performance of Masking Ratio 0.3. }
\label{tab:masked image modeling 0.3}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{llccccc}
\toprule
Models  &  Layer / Iteration / FF Ratio $M$ (\# Params)& PSNR ($\uparrow$) & SSIM ($\uparrow$) & Multi-Scale SSIM ($\uparrow$) & LPIPS ($\downarrow$)& FID ($\downarrow$)\\
\midrule
Transformer  & L1 / Iter 12 / 4$d$ ~(8.85 M) & \textbf{16.616} & \textbf{0.435} & \textbf{0.642} & \textbf{0.291 }& \textbf{35.095}
\\
Ours  & L1 / Iter 12 / $d$ ~(3.94 M) & 16.365 & 0.427 & 0.621 & 0.314 & 45.642\\
Ours  & L1 / Iter 24 / 8$d$ ~(8.07 M) & 16.590 & 0.434 & 0.638 & 0.294 & 35.128\\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{visualization_more_results.pdf}
    \includegraphics[width=\linewidth]{visualization_more_results_v2.pdf}
    \includegraphics[width=\linewidth]{visualization_more_results_v3.pdf}
    \caption{More Comparative Results of Masked Image Modeling on ImageNet 256$\times$256.}
    \label{fig:visualization more results}
\end{figure}
\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
