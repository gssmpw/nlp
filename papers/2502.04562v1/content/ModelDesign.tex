\subsection{Model Design -- Partition of Unity network with MOR-Physics neural operators}\label{sec:model}

We now construct a neural operator, POU-MOR-Physics, for functions with domain, $\mathbb{T}^d$. 
Our parameterization is a spatially conditioned POU-network composed of a gating network and neural operator experts:
\begin{equation}\label{eq:pou_model}
    (\mathcal{P} (u;\phi)) (x) = \sum_{i=1}^I G_i(x;\phi_{G_i}) (\mathcal{N}_i(u;\phi_{\mathcal{N}_i}))  (x),
\end{equation}
where $G$ forms a partition of unity, i.e. $\forall x \in \mathbb{T}^d$,
\begin{equation}
    \sum_i^I G_i (x;\phi_{G_i}) = 1 \quad \text{and} \quad G_i(x;\phi_{G_i}) \ge 0,
\end{equation}
and where $\phi$ are the combined set of parameters in the full model and $\phi_*$ are parameters for the various subcomponents of the model. Where the context is clear, we will suppress in our notation the dependence on the parameters. We describe each of the components of \(\mathcal{P}\) -- the neural operators $\mathcal{N}_i$, $i=1,\dots,I$,  the gating network, $G$, and the time-dependent autoregressive strategy -- in turn, which then situates the model to be used for mean-field variational inference (MFVI).

\subsubsection{MOR-Physics Operator}

The neural operators in Equation \eqref{eq:pou_model}, $\mathcal{N}_i$, are modified versions of the MOR-Physics Operator presented in \cite{patel2018MOR_Operator,patel2021MOR_Operator2}. For convenience, we drop the subscript $i$, as we define each $\mathcal{N}_i$ operator similarly. We will exclusively parameterize operators with the approach detailed below. However, if we have \textit{a priori} knowledge about a system, we can use a predefined operator in addition to the learned ones. In our exemplars, we include the zero expert in our ensemble of experts, i.e., an operator that evaluates to zero for any input function.
%Our version adds input, hidden, and output channels; in a manner similar to the FNO \cite{li2020FNO}. 
%Input and output channels are necessary to deal with the 3d input and output velocity components, but hidden channels improve performance, so we used them throughout the model. \\
%We follow the notational conventions \textit{similar} to those from the FNO paper \cite{li2020FNO}. Also all the equations below ignore the batch dimension.
%Given the Banach spaces \(A,V,U\) and \(\forall a \in A, \forall u \in U, N: A \rightarrow U\) we define \(N(a)=u\) to be the neural operator. Also we define \(\forall v_l \in V, v_l: R^b \rightarrow R^c\) to be the intermediate representation of \(u\in U\) at hidden layer \(l\). Additionally we will define \(\forall k \in R^d, g_l(k) \in C^{d_{in}^{(l)}\times d_{out}^{(l)}}\) to be the learned weights of a MOR Operator layer in the Fourier domain. Note that in the actual implementation \(g(k)\) is a tensor, since it needs to be used with the FFT anyways.

 We compose $L$ MOR-Physics operators, $\mathcal{N}^{(l)}$, $l = 1:L$, thereby introducing latent functions, $v^{(l)}:\mathbb{T}^{d} \rightarrow \mathbb{R}^{m^{(l)}}$, after the action of each operator, 
 %i.e., $v^{(l)}$ the intermediate state of $\mathcal{N}$ at hidden layer $l$. In what follows, $\mathbb{T}^{d_v^{(l)}}$ is domain of the latent features.
\begin{equation}
    v^{(l+1)} = \mathcal{N}^{(l+1)}(v^{(l)}) =\mathcal{F}^{-1}(g^{(l+1)}\cdot \mathcal{F}(h^{(l+1)} \circ v^{(l)})),
\label{eq:MOR_layer}
\end{equation}
where $\mathcal{F}$ is the Fourier transform, $h^{(l)}$ is a learned local activation, implemented with a neural network, and $g^{(l)}$ is a weighting function in Fourier frequency space.
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{\path figures/MOR_Operator+Skip_Diagram.png}
    \caption{MOR Operator diagram, showing architecture of each expert operator $\mathcal{N}_i$. Black arrows denote function composition.}
    \label{fig:MOR_Operator}
\end{figure*}
% \begin{definition}
% Given the Banach spaces \(A, V, U\):
% \begin{itemize}
%     \item The neural operator \(N: A \rightarrow U\) is defined by \(N(a) = u\) for all \(a \in A\) and \(u \in U\).
%     \item The non-linearity represented as \(h^{(l)}(x)\) is a learned local activation function (implemented with a neural network). \\
% \end{itemize}
% \label{def:NO}
% \end{definition}
% NOTE: useful to have N_l(v_l) for simplicity later.
% CONFIRMED: FNO paper uses notation of dot product of two functions!!
% \textit{MOR Operator Layer Equation}:
% \begin{equation}
%     \mathcal{N}^{(l+1)}(v^{(l)})(x) =\mathcal{F}^{-1}(g^{(l+1)}\cdot \mathcal{F}(h^{(l+1)} \circ v^{(l)}))(x)
% \label{eq:MOR_layer}
% \end{equation}
% \begin{comment}
% \textit{MOR Operator Layer Equation \(0<\forall l<n\)}:
% \begin{equation}
% \begin{aligned}
%     C^{(l+1)}(v^{(l)})(k)&=g^{(l+1)}(k)\cdot \mathcal{F}(h^{(l+1)} \circ v^{(l)})(k), \\
%     v^{(l+1)}(x) & =\mathcal{N}^{(l+1)}(v^{(l)})(x) =\mathcal{F}^{-1}(C^{(l+1)}(v^{(l)}))(x)+v^{(l)}(x)
% \end{aligned}
% \label{eq:MOR_layer}
% \end{equation}
% \end{comment}
The multiplication operation, $\cdot$, in Equation \eqref{eq:MOR_layer} is both the Hadamard product and a matrix multiplication, i.e., at every wavenumber, $k$, we have a matrix-vector product between \(g^{(l+1)}(k) \in \mathbb{C}^{m^{(l+1)} \times  m^{(l)}}\) and \(\mathcal{F}(h^{(l+1)} \circ v^{(l)})(k) \in \mathbb{C}^{m^{(l)}}\). This operation is comparable to linear operations across channels in a convolutional neural network (CNN).
%, and \textit{exactly} how they are handled in FNO. %Also the last term in \(\mathcal{N}^{(l+1)}(v^{(l)})(x)\) is a skip connection.

To build these operators, we discretize the domains on Cartesian meshes and replace the abstract linear operations with the appropriate matrix operations. Depending on the problem, we implement \(g^{(l)}(k)\) either as a neural network or a parameterized tensor. In either case, we truncate the higher modes of either the \(g^{(l)}(k)\) or \(\mathcal{F}(v^{(l)})(k)\) tensors (which ever has more in a given dimension) so that their shapes are made compatible. This effectively results in a low pass filter; more details can be seen in \cite{patel2018MOR_Operator}.

All hidden layers are appended with a skip connection. Ultimately, each $\mathcal{N}_i$ is then built via composition of several layers $\mathcal{N}^{(l)}$, so that
$$\mathcal{N}_i(u) := (\mathcal{N}^{(L)} \circ \mathcal{N}^{(L-1)} \circ \dots \circ \mathcal{N}^{(0)})(u).$$
and $v^{(L)} = v$ provides is the action of the composite MOR-Physics operator.
This architecture is sketched in Figure \ref{fig:MOR_Operator}, mapping from an input $u$ to a target $v$ through the intermediate layers $v^{(l)}$.
% \begin{equation}
% \begin{aligned}
%     \mathcal{\tilde N}^{(l+1)}(v^{(l)})&=\mathcal{N}^{(l+1)}(v^{(l)})+v^{(l)} \text{ (skip connection layer)}\\
%     N(a)&=(\mathcal{N}^{(0)} \circ \mathcal{\tilde N}^{(1)} \circ ... \circ \mathcal{\tilde N}^{(n-1)} \circ \mathcal{N}^{(n)})(a)=u, \text{ s.t.} \\
%     d_v^{(l)}&=d_{hidden} : 0<\forall l<n \text{ (channel dimensions)} \\
%     d_v^{(0)}&=d_{in}, d_v^{(n)}=d_{out} \text{ (channel dimensions)}\\
% \end{aligned}
% \label{eq:MOR_Operator}
% \end{equation}
%Recall \ref{def:NO}, the neural operator is \(N: A\rightarrow U\).
%Here \(\mathcal{N}^{(0)}\) and \(\mathcal{N}^{(n)}\) \ref{eq:MOR_layer} are similar to P and Q in the FNO in the sense that they lift (raise) and project (reduce) the number of dimensions. However they are \textit{still the same type of layer as the hidden layers} (unlike FNO), except insofar as they have skip connections added.

\subsubsection{Gating Network}\label{sec:gates}

The gating network $G: \mathbb{T}^d \rightarrow \mathbb{R}^I$ is built from a neural network, taking the coordinates as inputs.
%and yielding a Softmax weight vector that is used to get a weighted combination of 
Its output is transformed via softmax into coefficients to compute a convex combination of neural experts, \(\mathcal{N}_i\), at location \(x \in \mathbb{T}^d\). The Gating network does \textit{not} take \(u(x)\) as input, it only uses the location \(x\) which is sufficient to partition the space for different experts. See Figure \ref{fig:Mixture_of_Experts} for a schematic of the gating function.

%These coordinates are transformed to be periodic before being input to the Gating Network G. That is  $$(x,y,z) \rightarrow (sin(x), sin(y), sin(z), cos(x), cost(y), cos(z)) $$.

%$\mathbb{T}^d \rightarrow \mathbb{R}^I$

%In reality the Gating Network \(G: \mathbb{R}^d \rightarrow \mathbb{R}^I\) represents the composition of a periodic transformation and the \textit{raw} neural network \(\tilde G: \mathbb{R}^{2d} \rightarrow \mathbb{R}^I\) s.t.
%$$G(x,y,z)=\tilde G(sin(x), sin(y), sin(z), cos(x), cos(y), cos(z))$$
Since we rely on the Fourier transform for our operator parameterization, we use a smooth mapping, $\mathbb{T}^d \rightarrow \mathbb{R}^{2d}$, to provide the input to the gating network. The domain, $\mathbb{T}^d$, is parameterized by $d$ angles, $\theta_i$ and mapped to a vector in $\mathbb{R}^{2d}$ as, 
\begin{equation}
    [\sin(\theta_1), \hdots, \sin(\theta_d),\cos(\theta_1), \hdots, \cos(\theta_d)]^T.
\end{equation}
% A feedforward neural network maps this vector as, $\mathbb{R}^{2d} \rightarrow \mathbb{R}^{I}$ and the composition provides the gating network.
This yields more consistently interpretable expert partitions; In our 2D exemplar discussed in Section \ref{sec:2d_data} we  found this approach to produce symmetric partitions that conform to the problem specification, while the simpler approach using the angles as coordinates led to asymmetric partitions.

Although we describe a neural network gating function in this section, $\textit{a priori}$ known gates can replace the network, e.g., when a PDE domain is already well characterized.
%The effect was particularly pronounced on the 2d problem. The intuition is that using these periodic coordinates encourages (but does not enforce) symmetric expert partitions.

%\textit{Mixture of Experts Equation}:
%\begin{equation}
%\begin{aligned}
%     P(a)(x) = \sum_i^{\tilde n} N_i(a)(x)*G_i(x)
%        (\mathcal{P} \circ a) (x) = \sum_i G_i(x) (\mathcal{N}_i \circ a)  (x) 
%\end{aligned}
%\label{eq:Mixture_of_Experts}
%\end{equation}

%This outer ``Mixture of Experts'' model \ref{fig:Mixture_of_Experts} just takes a weighted sum of gating weights \(G_i(x)\), and the expert outputs \(\mathcal{N}_i(a)\).  This is similar to how POU-Net \cite{lee2021POU_net} works.
%Except POU-Net uses polynomial experts, an L2 weight penalty and it's gating network takes 'a' as input too.
\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1.0\linewidth]{\path figures/Mixture_of_Experts_Diagram.png}
    \caption{Mixture of Experts model, where a weighted sum of gating weights \(G_i(x)\) is applied to expert outputs \(\mathcal{N}_i(u)\). The gating weights are spatially localized, depending only on $x$ and not $u$. }
    \label{fig:Mixture_of_Experts}
\end{figure}

% \subsubsection{Fourier Pseudo-Spectral Method for Navier-Stokes}

% Finally we detail the Operator's integration with the forward Euler PDE solver. Navier-Stokes is the governing equation for the pressure (scalar) and velocity (vector) fields:

% \begin{equation}
% \frac{\partial \bm{u}}{\partial t} + \nabla \cdot (\bm{u} \otimes \bm{u}) = -\frac{1}{\rho} \nabla p + \nu \Delta \bm{u}
% \end{equation}
% \begin{equation}
% \nabla \cdot \bm{u} = 0
% \end{equation}

% Applying the Fourier transform, $\bar{\bm{u}} = \mathcal{F}(\bm{u})$, eliminating $p$, and discretizing in time with forward Euler, we get:

% \begin{equation}
% {\partial \bar u\over \partial t} =  -i \bm{\kappa} \cdot \overline{\bm{u}^n \otimes \bm{u}^n} + \frac{i \bm{\kappa}}{||\bm{\kappa}||_2^2} (\bm{\kappa} \otimes \bm{\kappa}) : \overline{\bm{u}^n \otimes \bm{u}^n} - ||\bm{\kappa}||_2^2 \bar{\bm{u}}^n
% \end{equation}

% where $\bm{\kappa}$ is the wavevector. Note that this only works on a periodic domain. Now that we have the finite difference derivative, we \textbf{insert our Mixture of Experts learned correction operator \ref{fig:Learned_Correction}}: \(P(a)\) from \ref{eq:Mixture_of_Experts}. We will apply it to \(\bar u(x)\) after the regular forward-Euler timestep.

% \begin{equation}
% \bar{\bm{u}}^{n+1} = P(\bar{\bm{u}}^n + \Delta t {\partial \bar u\over \partial t})
% \end{equation}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{\path figures/Learned_Correction.png} 
%     \caption{Learned Forward Euler Correction}
%     \label{fig:Learned_Correction}
% \end{figure}

% Given a divergence-free initial condition, $\bm{u}^0$, we can take its Fourier transform, $\bar{\bm{u}}^0$, and update to the next time step, $\bar{\bm{u}}^1$, and so on. We can generate a divergence-free initial condition by generating a random vector field, $\bm{h}$, and removing the non-divergence part in spectral space:

% \begin{equation}
% \bar{\bm{u}}^0 = \bar{\bm{h}} - \frac{\bm{\kappa} (\bm{\kappa} \cdot \bar{\bm{h}})}{||\bm{\kappa}||_2^2}
% \end{equation}

% To calculate $\overline{\bm{u}^n \otimes \bm{u}^n}$, we have to take the inverse Fourier transform of $\bar{\bm{u}}$ to return to physical space, compute the nonlinearity, and Fourier transform back into spectral space, i.e.,
% \begin{equation}
% \overline{\bm{u}^n \otimes \bm{u}^n} = \mathcal{F} \left( \mathcal{F}^{-1} (\bar{\bm{u}}^n) \otimes \mathcal{F}^{-1} (\bar{\bm{u}}^n) \right)
% \end{equation}

\subsubsection{Autoregressive POU-MOR-Physics model}\label{sec:autoregressive}

Autoregressive models are well-suited for learning spatiotemporal dynamics. We can specialize the model in \eqref{eq:pou_model} to learn an update operator representing the evolution of the system over a small period of time, $\Delta t$, by letting $\mathcal{P}:\mathcal{U}\rightarrow \mathcal{U}$ and,
\begin{equation}
\begin{split}
    u(\cdot,(n+1)\Delta t) & = \mathcal{P} u(\cdot,t)\\
    = u_{n+1} &=  \mathcal{P} u_n.
\end{split}
\end{equation}
In this context, the operator $\mathcal{P}$ can be composed with itself to predict the system at discrete times, $u_{n+p} =  \mathcal{P}^p u_n$. In many cases, parts of a model are \textit{a priori} known and we have a PDE with an unknown operator,
\begin{equation}
    \begin{aligned}\label{eq:autoreg1}
        \partial_t u = \mathcal{M} u + \tilde{\mathcal{P}}u
    \end{aligned}
\end{equation}
where $\mathcal{M}$ is known and $\tilde{\mathcal{P}}$ is unknown. A first order in time operator splitting allows for the update,
\begin{equation}\label{eq:autoreg2}
        u_{n+p} = \left[ \mathcal{P}(I+\Delta t \mathcal{M})\right]^pu_n = U^pu_n\\
\end{equation}
where $\mathcal{P}$ is a new unknown operator that provides the same effect as $\tilde{\mathcal{P}}$. Since  the Euler update operator, $(I+\Delta t)\mathcal{M}$, is a common time integrator for PDEs, we will refer to it as the PDE solver for the remainder of this work.
We will demonstrate an autoregressive model in Section \ref{sec:les} by learning an LES closure model for wall bounded turbulent flow.

 We introduce the operator, $\overline{\mathcal{U}}^p$, to give a time series prediction of $p$ time steps from an initial field, and denote its action on an initial condition, $\overline{\mathcal{U}}^p u_n = u_{[n,n+p]}$, where the subscript indicates a time-series beginning from timestep $n$ and ending with timestep $n+p$.


 Given a time-series of functional data, $D = \{\tilde{u}_n\}^{n=0,\hdots,N}$, an autoregressive model can be found by solving an optimization problem, i.e., \eqref{eq:opt1} or \eqref{eq:opt2}.
 

% \begin{equation}\label{eq:opt}
%     \min_{\theta} \sum_{n=1}^N \left\Vert \overline{\mathcal{U}}_{\theta}^n\tilde{u}_d^0 -% \tilde{u}_d^n  \right\Vert_{\ell_2(G)}^2.
%\end{equation}
%where we have reintroduced the dependence of the model on parameters, $\theta$, as subscripts.

%In practice, Equation \eqref{eq:opt} is not computationally tractable, so a multiple shooting strategy is necessary and obtain the optimization problem,
%\begin{equation}\label{eq:opt2}
%    \min_\theta \sum_{n=1}^{N-P} \left\Vert \overline{\mathcal{U}}_{\theta}^p\tilde{u}_d^n - \tilde{u}_d^{[n,n+p]}  \right\Vert_{\ell_2(G)}^2
%\end{equation}
%where $\tilde{u}_d^{[n,n+p]}$ is the subset of data from timestep $n$ to $n+p$ and  $P$ is a hyperparameter for the number of autoregressive steps used during training.

