\section{Numerical demonstrations}

We demonstrate our method with three numerical examples. This section describes the problem formulations and our results. In the first example, we learn a nonlinear operator for functions on a disk. In second, we learn the solution operator to a nonlinear poisson equation with mixed boundary condition. In the third, we learn an LES model for turbulent channel flow.

\input{\path 2d_disk}

\subsection{Solution operator for nonlinear Poisson problem on quarter disk}\label{sec:wedge}

In this section, we demonstrate POU-MOR-Physics is capable of learning solution operators for nonlinear PDEs with complex domains and mixed boundary conditions. We begin by generating pairs of data, $\left(u_i,v_i\right)$, that solve a nonlinear Poisson equation and learn the solution operator. Dropping the subscripts, we first  sample the functions, $\left(\hat{u},\hat{v}\right)$,
%
\begin{equation}
\begin{aligned}
    &\psi(x) = \left\{ \begin{matrix}1-||x||_2^2 & if\ ||x||_2^2<1\\
    0 & else\end{matrix} \right.\\
    &\hat{v}(x) = \psi(x)\sum_{m=1}^{10} \cos (f_{m,1} x_1) \cos (f_{m,2} x_2) \\
    &\hat{u} = \nabla \cdot \tanh(\nabla u)
\end{aligned}
\end{equation}
%
where $f$ are random frequencies, $f \sim U[0,10]$. Restricting the domains of $\hat{u}$ and $\hat{v}$, to the quarter disk, $\{x: ||x||_2^2\leq 1, x\geq 0\}$, we obtain the data, $\left\{\overline{u},\overline{v}\right\}$, which solves the nonlinear Poisson equation,
%
\begin{equation}
    \begin{matrix}
        \nabla \cdot \tanh(\nabla \overline{v}) = u & x\in \Omega\\
        \nabla \overline{v} \cdot n =0 & \begin{aligned}
            \{(x_1,0): x_1 \in [0,1]\} \cup \\\{(0,x_2): x_2 \in [0,1]\}
        \end{aligned} \\
        \overline{v} = 1 & \{ x: 0\leq\arctan \frac{x_2}{x_1}\leq\frac{\pi}{2}\}\
    \end{matrix}
\end{equation}
we next rotate the domains of $\overline{u}$ and $\overline{v}$ by a random rotation matrix and obtain our data, ${u}$ and ${v}$. We obtain 10000 samples of these input/output function pairs and train a POU-MOR-Physics operator using the least squares loss. For this example, we use predefined gates that conform to the problem specification instead of a gating network (see Section \ref{sec:gates}). Figure~\ref{fig:poisson} compares the learned action on test data to the true action. We obtain approximately 1\% relative RMSE for this action compared to the true action.

\begin{figure}
    \centering
    \includegraphics[width=2in]{\path figures/poisson_mixed_bc.pdf}
    \caption{POU-MOR operator learns the solution operator for a nonlinear Poisson equation with nontrivial and mixed boundary conditions.}
    \label{fig:poisson}
\end{figure}


\subsection{Large Eddy Simulation Modeling}\label{sec:les}


Our target application is extracting an LES model from DNS provided by the JHTDB dataset \cite{graham2016JHTDB_channel}. The simulation data is obtained from the  Re=1000 channel flow problem with no-slip boundary conditions (BCs) on the top and bottom of the flow, and periodic BCs on the left, right, front and back.

The JHTDB channel flow problem \cite{graham2016JHTDB_channel}, characterized by three spatial dimensions plus a recursive time dimension, presents significant computational challenges that demand careful resource management. To mitigate the substantial memory requirements, we utilized the real Fast Fourier Transform (rFFT) within the MOR-Physics Operator \cite{patel2021MOR_Operator2}, effectively conserving memory without compromising performance.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\linewidth]{\path figures/JHTDB_Channel.png}
%     \caption{John Hopkins Turbulence Dataset: Channel Flow DNS}
%     \label{fig:JHTDB_channel}
% \end{figure}
% \pagebreak

% \textbf{Boundary Conditions:}
% \begin{itemize}
%     \item  \(u(x,y,z,t)=0: y \in \partial D_y\)
%     \item \(u(x\pm L_x,y,z,t)=u(x,y,z,t): x \in \partial D_x\)
%     \item \(u(x,y,z\pm L_z,t)=u(x,y,z,t): z \in \partial D_z\) \\
% \end{itemize}

We subsample the DNS data spatially (see Table~\ref{tab:post_processing_parameters}) and keep the full resolution in the time dimension, motivated by the need to have a larger training dataset. The spatial sub-sampling is performed after applying a box filter to the DNS data, ensuring the sub-sampled grid is representative of the whole DNS field. 

From this data, we seek to obtain an autoregressive (Section \ref{sec:autoregressive}), MFVI (Section \ref{VI}) model that incorporates physics by leveraging the standard LES formulation (Appendix \ref{sec:les_apriori}). While the JHTDB dataset only simulates the flow for one channel flow through time, $t=T$, we use our model to extrapolate the flow to long times, $t=10T$, and evaluate the predictions and uncertainties from our model.

%This ensures that the sub-sampled grid was more representative of the whole DNS field. Also the need for spatial sub-sampling was primarily due to hardware constraints. We were already under great memory pressure due to the 3d problem, and it was barely realistic to fit even the sub-sampled field into memory.

% \begin{table}[H]
%   \centering
%   \begin{tabular}{|l|l|}
%     \hline
%     \textbf{Parameter} & \textbf{Value} \\
%     \hline
%     Viscosity & \(v = 5 \times 10^{-3}\) \\ \hline 
%     Friction velocity Reynolds number & \(Re_{\tau} \sim 1000\) \\ \hline 
%     Domain Length & \(8\pi \times 2 \times 3\pi\) \\ \hline 
%     Grid Size & \(2048 \times 512 \times 1536\) \\ \hline
%   \end{tabular}
%   \caption{Simulation Data Parameters of 3D DNS channel flow data from JHTDB.}
%   \label{tab:simulation_parameters}
% \end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|c|} \hline 
        
        \textbf{Parameter} & \textbf{Value} \\ \hline 
        
        Spatial Stride & \( s_{x} = s_{y} = s_{z} = 20 \) \\ \hline 
        
        Sub-Sampled Dimensions & \( 103 \times 26 \times 77 \) \\ \hline 
        
        Time Dimension & \( t = 4000 \) \\ \hline 
        
        Box Filter Dimension & \( b = 20 \) \\ \hline
        
    \end{tabular}
    \caption{Post Processing Parameters for JHTDB problem.}
    \label{tab:post_processing_parameters}
\end{table}

\subsubsection{Training details}

Due to the memory demands of this learning task, we required parallelization to train our LES model.
To handle the computational load efficiently, we employ 20 A100 GPUs alongside the Fully-Sharded Data Parallel (FSDP) strategy \cite{zhao2023FSDP}, thereby exploiting data-parallel training and necessitating a scaled learning rate that follows the batch size \cite{imagenet1hour2017}.

% FSDP further reduces the memory burden on each GPU by sharding the model parameters, optimizer states, and gradients across all available GPUs. It broadcasts the necessary shards just in time for forward and backward passes, overlapping communication with computation to eliminate communication overhead. This strategy not only conserves memory but also enhances computational efficiency.

We used the linear scaling rule from \cite{imagenet1hour2017} and the "One Cycle" warm-up schedule from \cite{smith2019super}; without warm-up, the model parameters may change too rapidly at the outset \cite{imagenet1hour2017} for effective training.

Adhering to the linear scaling rule from \cite{imagenet1hour2017}, we set the batch size to \(\text{batch\_size} = n\) and the learning rate to \(\text{learning\_rate} = 1.25\times10^{-4}*n\), where $n$ represents the number of GPUs utilized. Although memory constraints necessitated a small per-GPU batch size of 1, we found that combining this approach with gradient clipping yielded effective training results.

To improve the stability of the learned subgrid dynamics operator, we employed several (i.e. 8) auto-regressive time steps during training, following methodologies similar to those proposed by Bengio et al. \cite{bengio2015scheduled} (but without the curriculum). Statistical auto-regressive time series models often suffer from exponentially growing errors because the model's flawed outputs feed back into its inputs, compounding errors over multiple time steps. By exposing the model to this process during training — taking several auto-regressive steps before each optimization step — the model learns to correct its own compounding errors, mitigating  problems at prediction time \cite{bengio2015scheduled}.

% \subsection{Model Expert Partitions}

\subsubsection{LES model Results}

In Figure \ref{fig:les_expert_partitions}, we find that the gating network is able to  partition the domain and find separate models for the bulk and boundary layers of the JHTDB channel data. We see spatial partitions that reflect the boundary layer, with different support for $\mathcal{N}_1$ vs. $\mathcal{N}_2$ Moreover, the transition from \(\mathcal{N}_1\) to \(\mathcal{N}_2\) is continuous roughly approximating the strength of the velocity at those points in the simulation. This demonstrates the model's ability to find multiple, spatially conditioned models.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1.0\linewidth]{\path figures/deterministic_model/3d_Expert_Partitions.png}
%     \caption{3D Expert Partitions for JHTDB dataset.}
%     \label{fig:deterministic_expert_partitions}
% \end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{\path figures/les_weights.pdf}
    \caption{3D Expert Partitions for JHTDB dataset. $G_3$ is the zero expert, i.e., $\mathcal{N}_3 = 0$.}
    \label{fig:les_expert_partitions}
\end{figure}


% \subsection{LES predictions}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{\path figures/les_fields}
    \caption{Fields from filtered \textit{(Left)} DNS at $t=T$ and learned  \textit{(Right)} Sample of LES model predictions after 10 channel flow-through times.}
    \label{fig:Qcrit}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{\path figures/les_profiles.pdf}
    \caption{MFVI LES model captures \textit{(Left)} energy spectrum and \textit{(Center)} bulk velocity, and \textit{(Right)} RMS fluctuations. Posterior predictive samples are shown in black. The DNS results are shown in orange. Kolmogorov's 5/3 rule is shown in blue.}
    \label{fig:Energy_Spectrum}
\end{figure}
%We find our model is able to closely match the true DNS evolution. The learned simulation is able to largely avoid divergent behavior; we reproduce late-simulation artifacts found in the DNS. The extra auto-regressive timesteps taken during training to stabilize prediction (effectively teaching it to correct its own errors). 

We use our LES model to evolve the system to 10 channel flow-through times. Since the JHTDB only includes one flow length's worth of data, we are predicting dynamics in an extrapolatory regime and use the VI model to provide predictions with error bars. Since the flow is at a statistical steady state, we should see the same mean statistics for our model as the DNS data. Figure \ref{fig:Energy_Spectrum} shows a statistical comparison between our model and the filtered DNS data. We see that the energy spectrum closely matches that of DNS and obey's Kologmorov's 3/5's rule (the error bars don't pass the threshold). In the middle figures we see close agreement also even at t=10T. However we also see the uncertainty is as expected increasing with time. Finally we see decent agreement with RMS velocity fluctuations in the right most figures, however there is still some room for improvement as matching RMS fluctuations \cite{Pope_2000} is more difficult.

We also were able to achieve $98.8\% R^2$ reproducing the simulation with a deterministic model; the very closely matching fields are shown in the Appendix \ref{sec:deteministic_les}; see, e.g. Figures \ref{fig:deterministic_x_velocity},\ref{fig:deterministic_y_velocity}, and \ref{fig:deterministic_z_velocity}. %We achieve a validation score $R^2$ of \(R^2=98.81\%\), when reserving 20\% of the available training data as the hold-out set for validation purposes. 

% (i.e. it explains 98.81\% of the variance). The only catch here is that the model was tasked with reproducing the simulation it had been trained on (since there was only one simulation from JHTDB with these settings). Even so, this is impressive as 20\% of the simulation was held out from the training data. 

\subsection{Out-of-Distribution (OOD) Detection:}

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{\path figures/ood_bar.pdf}
    \hspace{.1in}
    \includegraphics[width=0.6\linewidth]{\path figures/ood_viz_std.pdf}
    \caption{We test Bayesian VI model's OOD detection by comparing the uncertainty from a learned simulation with a in-distribution initial-condition (IC): JHTDB data with a box filter size of 20, to the uncertainty from an OOD IC: JHTDB data with no box filter. The model's uncertainty can clearly detect that the no-filter initial condition (IC) is OOD. Even though all it has to go on is only a single time-slice (the IC).}
    \label{fig:ood}
\end{figure}

In Figure \ref{fig:ood} we validate the Bayeisan model's UQ OOD detection ability. This makes us confident in using the UQ to validate the extrapolation predictions for which we do not have ground truth to compare.

 
