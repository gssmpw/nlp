\subsection{Mean-Field Variational Inference (MFVI)}\label{VI}

The auto-regressive nature of our Bayesian MFVI model (in Section~\ref{sec:les}) necessarily changes our otherwise standard implementation of MFVI. The details of the major difference are highlighted in Figure \ref{fig:Learned_Correction_UQ}.

To build MFVI into our neural operator, we assume independent Gaussian variational posteriors for each model parameter, exploiting the conventional reparameterization trick \cite{kingma2013VAEs} for normally distributed weights; see Appendix \ref{Complex_Reparameterization} for how it applies to complex parameters. Further details on our VI method can be found in \cite{blundell2015weight}.

%In our framework, we use the POU-MOR-Physics operator to predict the mean, $\mu$, and variance, $\sigma^2$, of functional data, where, $u = (\mu,\sigma^2)$. 
Following \cite{blundell2015weight}'s convention we treat the VI model as a probabilistic model, in the sense that it directly predicts the mean and variance of Gaussian likelihoods, that is \(\mathcal{P}: U \rightarrow U,\) where $U=M\times \Sigma$, such that the mean and variance  are given by $u=(\mu,\sigma^2)\in U$. Only the $\mu$ fields are integrated by the PDE solver as described in Figure \ref{fig:Learned_Correction_UQ}. Given a time-series of data, $\tilde{v}$, we learn a Bayesian model using variational inference by optimizing the ELBO,
\begin{equation}\label{eq:opt_vi}
\begin{aligned}
    \max_{q}\ (\log p\left(\tilde{v}_{[0,N]}|\overline{\mathcal{U}}^P\tilde{u}_0 \right) - D_{KL}(q || p_0))
\end{aligned}
\end{equation}
where $q$ is a variational distribution over $\Theta$, $p_0$ is a prior distribution over $\Theta$,  $\log p$ is a Gaussian log likelihood with mean and variance provided by $u$. 
%In this configuration, the neural operator provides a prediction for the mean velocity and the variance \(\sigma^2\) of the aleatoric error, as discussed below.
Equation \eqref{eq:opt_vi} is not computationally tractable, so we use a strided sliding window strategy and obtain,
\begin{equation}\label{eq:opt_vi2}
\begin{aligned}
    \max_{q} \sum_{m=0:N:P}(\log p\left(\tilde{v}_{[m,m+P]}|\overline{\mathcal{U}}^P\tilde{u}_m \right) - D_{KL}(q || p_0))
\end{aligned}
\end{equation}
% \begin{equation}\label{eq:opt_vi2}
% \begin{aligned}
%     \max_{q} \sum_{\underset{m=Pn}{n=0}}^{\lfloor N/P\rfloor-1} (\log p\left(\tilde{\mu}_{[m,m+P]}|\mu, \sigma^2 \right) - D_{KL}(q || p_0))\\
%     u_{[m,m+P]} = \overline{\mathcal{U}}^P\tilde{u}_m \quad \mu = T_\mu u_{[m,m+P]}\quad \sigma^2 = T_{\sigma^2} u_{[m,m+P]}
% \end{aligned}
% \end{equation}

where $\tilde{u}_{[n,n+P]}$ is the subset of data from timestep $n$ to $n+P$ and  $P$ is a hyperparameter for the number of autoregressive steps used during training. The notation, $0:N:P$, borrows from Python's array slicing syntax. To initialize the means and variances, we set, \(\forall m, \tilde u_m=(\tilde v_m, \tilde \sigma_m^2=0)\). We approximate the sum via stochastic mini-batch optimization. Overlapping sliding windows would constitute data-augmentation, which has been shown to cause the cold posterior effect (CPE) \cite{izmailov2021bayesian}, therefore we avoid using them.

\begin{figure}[htpb!]
    \centering
    \includegraphics[width=1.0\linewidth]{\path figures/Learned_Correction_UQ.png}
    \caption{Flow of Uncertainty through Learned PDE solver model. The PDE solver operates on the mean field, $\mu_n \in M$, while the uncertainty field, $\sigma_n^2 \in \Sigma$ is entirely modeled and updated by the Neural Operator.}
    \label{fig:Learned_Correction_UQ}
\end{figure}

With MFVI, the resulting model outputs Gaussian distribution predictions by applying a weighted sum of the experts' output tensors, c.f. Equation \eqref{eq:pou_model}, where now these output tensors contain both \(\mu\) and \(\sigma^2\) channels for each output feature.

The variance $\sigma^2$ is parameterized by learning a parameter $\rho$ and then computing
\begin{equation}
    \sigma^2(\rho) = (\log(1 + e^{\rho}))^2
    \label{eq:sigma_squared_parameterization}
\end{equation}
The squared softplus positivity constraint in Equation \eqref{eq:sigma_squared_parameterization} provides both the model predictions of $\sigma^2$ and the parameterization of the variances in the Gaussian variational distributions needed for VI. While enforcing positivity via squaring \(\sigma^2=\rho^2\) would be simpler, we believe our parametrization promotes easier learning.
