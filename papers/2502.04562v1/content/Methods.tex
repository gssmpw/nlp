\section{Methods}\label{methods}

Given pairs of functions as data, 
\begin{equation*}
    D = \left\{(u_i,v_i) | i\in {1,\hdots,n}, u_i \in U, v_i \in V\right\},
\end{equation*} 
where $U$ and $V$ are two Banach spaces, we seek an operator, $\mathcal{P}: U \rightarrow V$. The spaces $U$ and $V$ are sets of functions defined on $u_i: X \rightarrow \mathbb{R}^m$ and $v_i: Y\rightarrow \mathbb{R}^p$, where $X$ and $Y$ are compact subsets of $\mathbb{R}^{d_1}$ and $\mathbb{R}^{d_2}$, respectively.  Given our target application, we will consider $X=Y$ and therefore $d_1=d_2=d$. Operator learning introduces a parametrization for the unknown operator, $\mathcal{P}:U\times \Phi \rightarrow V$, where $\phi \in \Phi$ are parameters. As we demonstrate with learning an LES closure model in Section \ref{sec:les}, models can also incorporate \textit{a priori} known physics.

Given the parametrization, we can learn a deterministic model by solving the minimization problem,
\begin{equation} \label{eq:opt1}
    \phi = \underset{\hat\phi}{\mathrm{argmin}}\ L(D, \mathcal{P} (\cdot,\hat \phi)),
\end{equation}
where $L$ is a loss function, e.g., least squares. 
Alternatively, we can learn a probabilistic model by solving the optimization problem,
\begin{equation} \label{eq:opt2}
    Q = \underset{\hat Q}{\mathrm{argmin}}\ L'(D, \mathcal{P},\hat{Q}),
\end{equation}
where $Q$ is a variational distribution over $\Phi$ and $L'$ is a loss function, e.g., the negative of evidence lower bound (ELBO). 
The recovered operator must respect boundary conditions, which for neural operators leveraging the Fourier transform is a nontrivial task \cite{Li2023deform}.

% Neural operators are often parameterized discrete operators from a numerical method. The simplest numerical methods solve PDEs on trivial domains such as free space or periodic domains with simple meshes. However, most scientific and engineering problems of interest occupy nontrivial domains. Embedded boundary numerical methods transform a PDE on nontrivial domains to a PDE on a trivial domain where they can be integrated with a simple numerical method. 

One approach, volume penalization \cite{brown2014characteristic,kadoch2012volume}, is an embedded boundary method that integrates a variety of PDEs with complex boundary conditions. Volume penalization partitions a simple domain in two, where one of the subdomains is $X$, and applies forcing in each subdomain such that the solution to the new PDE, when restricted to $X$, approximates the solution to the original PDE. More generally, different kinds of physics may operate in different regions of space, and must be modeled as a collection of PDEs in disjoint domains, e.g., fluid-structure interaction \cite{engels2015numerical}. For these multi-physics problems, volume penalization partitions the extended domain accordingly with the appropriate forcing. In this work, we focus on Fourier pseudo-spectral methods which, while simple, efficient, and accurate,  only solve PDEs on periodic domains with Cartesian meshes in their base form. Applied to Fourier pseudo-spectral methods \cite{kolomenskiy2009fourier}, volume penalization retains the simplicity of the original discretization while expanding its reach to nontrivial problems. 

Our approach identifies physical systems with complex boundary conditions or multiphysics systems by learning the requisite partitions and forcing in a volume penalized Fourier pseudo-spectral scheme. We approach this task with the machine learning technique, mixture of experts, where experts learn their operations in subdomains of the problem space partitioned by learnable gating functions. 

Taking inspiration from volume penalization and mixture of experts, we parameterize $\mathcal{P}$ using a mixture of neural operators where each neural operator is a parameterized Fourier pseudo-spectral operator. This composite neural operator, which we refer to as POU-MOR-Physics, however, relies on the Fourier transform, so is designed for smooth functions with periodic domains. Ideally, a smooth extension for $u_i$ to this periodic domain is constructed to avoid Gibbs phenomena originating from the interface between $X$ and the periodic domain. We first discuss a strategy for constructing a smooth extension of $u_i$ in the next section that is compatible with the neural operator in Section \ref{sec:model}. 


% More generally, different kinds of physics may operate in different regions of space.


%Many classical numerical methods to describe operators are built upon periodic domains, and require e.g. embedded boundaries or volume penalization to extend to non-periodic domains of interest. Taking inspiration from such methods, we parameterize $N$ using a mixture of neural operators. 


\subsection{Feature engineering -- Smooth extension of functions to periodic domains}\label{sec:extension}

Our neural operator, $\mathcal{P}$, discussed in Section~\ref{sec:model}, is parameterized via the Fourier transform and therefore is only well-defined for periodic functions. However, $U$ and $V$ are functions on the torus, $\mathbb{T}^d$, so we embed $X$ in $\mathbb{T}^d$, and must supply extensions for our functions in the new domain.  Our complete prediction mechanism, including the neural operator, restriction, and extension, takes the form,
\begin{equation}
    \mathcal{P}_e: u \overset{\mathcal{E}}{\mapsto} u_e \overset{{\mathcal{P}}}{\mapsto} v_e \overset{\mathcal{R}}{\mapsto} v.
\end{equation}
where $\mathcal{E}$ is an extension and $\mathcal{R}$ is a restriction; we have suppressed dependence on the parameters, $\phi$,

Since we only compute losses in $X$, we do not choose an extension for the output functions and allow our neural operators to produce actions (i.e. $v \in V$) that behave arbitrarily in the complement, $\mathbb{T}^d \setminus X$. Our procedure provides a smooth extension of input functions to the torus. Input functions with constant traces on the boundary are extended simply by setting the function value in $\mathbb{T}^d \setminus X$ to the constant value. For other functions, a similar approach would lead to discontinuities, which induce Gibbs phenomena due to the Fourier transforms in our parametrization.
In these cases, we solve a constrained optimization problem that minimizes the $H^1$ semi-norm of the extended function such that it matches the original function with domain, $X$.
%
\begin{equation}
\begin{matrix}
    \min_{u_e} & \int \nabla u_e \cdot \nabla u_e  dx \\
    \mathrm{s.t.}  & \mathcal{R} u_e = u \hfill
\end{matrix}
\end{equation}
%
The Lagrangian \cite{bertsekas2014constrained} for this optimization problem is,
\begin{equation}
    \mathcal{L} = \int \nabla u_e \cdot \nabla u_e  dx + (\lambda, \mathcal{R} u_e - u)_{L_2(\mathcal{U})}
\end{equation}
where the Lagrange multiplier, $\lambda \in {U}$. Using the Plancherel theorem, we rewrite the Lagrangian as,
\begin{equation}
    \mathcal{L} = \sum_\kappa (\kappa \mathcal{F} u_e) \cdot (\kappa \mathcal{F} u_e)  + (\lambda, \mathcal{R} u_e - u)_{L_2(\mathcal{U})}
\end{equation}
Using the unitary Fourier transform, $\mathcal{F}^{-1} = \mathcal{F}^*$, the first order conditions give a linear system,
\begin{equation}\label{eq:smooth_ext}
\begin{aligned}
\nabla_{[u_e,\lambda]^T} \mathcal{L} =0 \Rightarrow
    &\begin{bmatrix}
        \mathcal{F}^{-1} {\kappa \cdot \kappa} \mathcal{F} & \mathcal{R}^T \\
        \mathcal{R} & 0
    \end{bmatrix}
        \begin{bmatrix}
        u_e \\
        \lambda
    \end{bmatrix}
    =
        \begin{bmatrix}
        0 \\
        u
    \end{bmatrix}\\
    &=A q = b
\end{aligned}
\end{equation}
where $\mathcal{R}^T$ is given by $(\mathcal{R}u_e,u)_X=(u_e,\mathcal{R}^Tu)_{\mathbb{T}^d}$. We efficiently solve the normal equations for this system using matrix free operations in a conjugate gradient (CG) solver. 

To illustrate this method, Figure \ref{fig:smooth_extention} compares the smooth extension of an input function with an extension that sets the function values to zero outside of the domain and compares the action of the Burgers operator, $v = u\cdot \nabla u$, on the two extensions. The discontinuity in the simple extension leads to an oscillatory action while the action from the smooth extension is less oscillatory. This test was performed using input function data generated as per Section \ref{sec:wedge}.

\begin{figure}
    \centering
    \includegraphics[width=2in]{\path figures/smooth_extension.pdf}
    \caption{The smooth extension prevents Gibbs oscillation for Burgers action on a function originally defined on a quarter disk. \textit{(Top right)} Simple extension of input function to torus via zero padding. \textit{(Bottom right)} Smooth extension of input function to torus. \textit{(Top left)} Burgers action on simple extension. \textit{(Bottom left)} Burgers action on smooth extension.}
    \label{fig:smooth_extention}
\end{figure}

% Alternatively, for linear equations one could find a suitable lifting function to accommodate the values on the boundary; the resulting function on the interior is homogeneous and therefore can be extended naturally to a periodic function, while the lift is moved to the right-hand side (and is possibly mollified, if it is not integrable on the periodic domain). See e.g. \cite{taylor1996partial} for discussion. Using a similar extension for periodic domains is possible in the nonlinear 
% PDE setting for specific classes of PDEs; for quasilinear elliptic problems, such as the examples considered later in this paper, existence and well-posedness follow from e.g. \cite{tang2013mixed}, and then the above arguments can be extended to such cases.

For the remainder, we will drop the subscript, $e$, and identify $u_e$ with $u$, $v_e$ with $v$, and $\mathcal{P}_e$ with $\mathcal{P}$.

\input{\path ModelDesign}


\input{\path VI}
