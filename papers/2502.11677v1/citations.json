[
  {
    "index": 0,
    "papers": [
      {
        "key": "guo2017calibration",
        "author": "Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q",
        "title": "On calibration of modern neural networks"
      },
      {
        "key": "desai2020calibration",
        "author": "Desai, Shrey and Durrett, Greg",
        "title": "Calibration of pre-trained transformers"
      },
      {
        "key": "jiang2021can",
        "author": "Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham",
        "title": "How can we know when language models know? on the calibration of language models for question answering"
      },
      {
        "key": "kadavath2022language",
        "author": "Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others",
        "title": "Language models (mostly) know what they know"
      },
      {
        "key": "si2022prompting",
        "author": "Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan",
        "title": "Prompting gpt-3 to be reliable"
      },
      {
        "key": "kuhn2023semantic",
        "author": "Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian",
        "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "guo2017calibration",
        "author": "Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q",
        "title": "On calibration of modern neural networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "desai2020calibration",
        "author": "Desai, Shrey and Durrett, Greg",
        "title": "Calibration of pre-trained transformers"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jiang2021can",
        "author": "Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham",
        "title": "How can we know when language models know? on the calibration of language models for question answering"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kadavath2022language",
        "author": "Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others",
        "title": "Language models (mostly) know what they know"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "si2022prompting",
        "author": "Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and Boyd-Graber, Jordan and Wang, Lijuan",
        "title": "Prompting gpt-3 to be reliable"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kuhn2023semantic",
        "author": "Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian",
        "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lin2022teaching",
        "author": "Lin, Stephanie and Hilton, Jacob and Evans, Owain",
        "title": "Teaching models to express their uncertainty in words"
      },
      {
        "key": "yin2023large",
        "author": "Yin, Zhangyue and Sun, Qiushi and Guo, Qipeng and Wu, Jiawen and Qiu, Xipeng and Huang, Xuanjing",
        "title": "Do Large Language Models Know What They Don't Know?"
      },
      {
        "key": "tian2023just",
        "author": "Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D",
        "title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback"
      },
      {
        "key": "xiong2023can",
        "author": "Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan",
        "title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms"
      },
      {
        "key": "yang2023alignment",
        "author": "Yang, Yuqing and Chern, Ethan and Qiu, Xipeng and Neubig, Graham and Liu, Pengfei",
        "title": "Alignment for honesty"
      },
      {
        "key": "ni2024llms",
        "author": "Ni, Shiyu and Bi, Keping and Guo, Jiafeng and Cheng, Xueqi",
        "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lin2022teaching",
        "author": "Lin, Stephanie and Hilton, Jacob and Evans, Owain",
        "title": "Teaching models to express their uncertainty in words"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yin2023large",
        "author": "Yin, Zhangyue and Sun, Qiushi and Guo, Qipeng and Wu, Jiawen and Qiu, Xipeng and Huang, Xuanjing",
        "title": "Do Large Language Models Know What They Don't Know?"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xiong2023can",
        "author": "Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan",
        "title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "tian2023just",
        "author": "Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D",
        "title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ni2024llms",
        "author": "Ni, Shiyu and Bi, Keping and Guo, Jiafeng and Cheng, Xueqi",
        "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yang2023alignment",
        "author": "Yang, Yuqing and Chern, Ethan and Qiu, Xipeng and Neubig, Graham and Liu, Pengfei",
        "title": "Alignment for honesty"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "azaria2023internal",
        "author": "Azaria, Amos and Mitchell, Tom",
        "title": "The internal state of an LLM knows when it's lying"
      },
      {
        "key": "slobodkin2023curious",
        "author": "Slobodkin, Aviv and Goldman, Omer and Caciularu, Avi and Dagan, Ido and Ravfogel, Shauli",
        "title": "The curious case of hallucinatory (un) answerability: Finding truths in the hidden states of over-confident large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "su2024unsupervised",
        "author": "Su, Weihang and Wang, Changyue and Ai, Qingyao and Hu, Yiran and Wu, Zhijing and Zhou, Yujia and Liu, Yiqun",
        "title": "Unsupervised real-time hallucination detection based on the internal states of large language models"
      },
      {
        "key": "chen2024inside",
        "author": "Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping",
        "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection"
      }
    ]
  }
]