\section{Related Work}
Current research on how to express LLMs' confidence can be mainly divided into three categories:
\paragraph{Probabilistic Confidence.}
% Calibration of pre-trained transformers
%  On calibration of modern neural networks
%  How can we know when language models know? on the calibration of language models for question answering
%  Language models (mostly) know what they know.
%   Prompting gpt-3 to be reliable
% Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation
This series of work uses the generation probability of the answer as the model's confidence.~\citep{guo2017calibration,desai2020calibration,jiang2021can,kadavath2022language,si2022prompting,kuhn2023semantic}. \citet{guo2017calibration} found that early neural networks tend to be overconfident and mitigated this by adjusting the temperature during the generation process. Subsequently, \citet{desai2020calibration} found that pre-trained Bert-style models have a relatively clear perception of their knowledge boundaries and~\citet{jiang2021can} showed that the issue of overconfidence still persists in pre-trained language models. More recent studies have explored LLMs’ perception of their knowledge boundaries. \citet{kadavath2022language} and~\citet{si2022prompting} demonstrated LLMs can be reliable under approprite prompts. \citet{kuhn2023semantic} argued that the probability of generated tokens does not accurately reflect the probability of the generated answer and estimated LLMs' confidence in their answers based on the semantic consistency across multiple generations.

\paragraph{Verbalized Confidence.} 
With the development of LLMs, some studies have shown that LLMs can express their confidence in answers in natural language~\citep{lin2022teaching, yin2023large, tian2023just,xiong2023can,yang2023alignment,ni2024llms}. \citet{lin2022teaching} were the first to demonstrate that a model (GPT-3) can learn to express confidence about its answers using natural language. Recently, \citet{yin2023large} found that LLMs have difficulty in perceiving their knowledge boundaries and tend to be overconfident and \citet{xiong2023can} explored various methods of confidence extraction. To enhance LLMs' perception level, \citet{tian2023just} and~\citet{ni2024llms} focused on prompting methods while~\citet{yang2023alignment} proposed training methods.

\paragraph{Confidence Estimation via Internal States.}
LLMs' internal states have been found to be effective in indicating the factuality of texts.~\citep{azaria2023internal,slobodkin2023curious} and~\citep{su2024unsupervised,chen2024inside} extended this approach to detect the factuality of model’s self-generated content. This line of work utilized a shallow network (i.e., MLP) to extract confidence from the hidden states of LLMs. Compared to prob-based methods, this tends to be more accurate because converting hidden states into token probabilities results in information loss. Additionally, compared to training LLMs to express better verbalized confidence it is much more cost-efficient.

 In this paper, we exploit LLMs' internal states to enhance their knowledge boundary perception from efficiency and risk mitigation perspectives.