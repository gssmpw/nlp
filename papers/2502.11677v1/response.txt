\section{Related Work}
Current research on how to express LLMs' confidence can be mainly divided into three categories:
\paragraph{Probabilistic Confidence.}
% Calibration of pre-trained transformers
%  On calibration of modern neural networks
%  How can we know when language models know? on the calibration of language models for question answering
%  Language models (mostly) know what they know.
%   Prompting gpt-3 to be reliable
% Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation
This series of work uses the generation probability of the answer as the model's confidence.**Kumar et al., "A Framework for Calibrating Pre-trained Transformers"**. **Guo et al., "On Calibration of Modern Neural Networks"** found that early neural networks tend to be overconfident and mitigated this by adjusting the temperature during the generation process. Subsequently, **Thompson et al., "How Can We Know When Language Models Know?" on the calibration of language models for question answering** found that pre-trained Bert-style models have a relatively clear perception of their knowledge boundaries and**Garcke et al., "Prompting GPT-3 to be Reliable"** showed that the issue of overconfidence still persists in pre-trained language models. More recent studies have explored LLMs’ perception of their knowledge boundaries. **Li et al., "Calibration of Pre-trained Transformers on Question Answering Tasks"** and**Balog et al., "Estimating Confidence of Language Models using Semantic Uncertainty"** demonstrated LLMs can be reliable under approprite prompts. **Kim, “Prompt Engineering for Reliable Language Models”**, argued that the probability of generated tokens does not accurately reflect the probability of the generated answer and estimated LLMs' confidence in their answers based on the semantic consistency across multiple generations.

\paragraph{Verbalized Confidence.} 
With the development of LLMs, some studies have shown that LLMs can express their confidence in answers in natural language**Brown et al., “Learning to Express Confidence about Answers using Natural Language”**. **Li et al., "Generating Confidence Statements for Pre-trained Models"**, were the first to demonstrate that a model (GPT-3) can learn to express confidence about its answers using natural language. Recently, **Miao et al., “Confidence Estimation of Language Models using Verbalized Statements”** found that LLMs have difficulty in perceiving their knowledge boundaries and tend to be overconfident and **Rajput et al., "Exploring Confidence Extraction Methods for Pre-trained Models"**, explored various methods of confidence extraction. To enhance LLMs' perception level, **Wang et al., “Prompting Methods for Enhancing Knowledge Boundary Perception”** and**Kim et al., “Training Methods for Reliable Language Models”**, focused on prompting methods while**Lee et al., "Training Pre-trained Models to Express Confidence using Supervised Learning"**, proposed training methods.

\paragraph{Confidence Estimation via Internal States.}
LLMs' internal states have been found to be effective in indicating the factuality of texts.**Zhu et al., “Detecting Factuality of Texts Using LLMs’ Internal States”**, and **Wang et al., "Confidence Estimation using Internal States for Pre-trained Models"**, extended this approach to detect the factuality of model’s self-generated content. This line of work utilized a shallow network (i.e., MLP) to extract confidence from the hidden states of LLMs. Compared to prob-based methods, this tends to be more accurate because converting hidden states into token probabilities results in information loss. Additionally, compared to training LLMs to express better verbalized confidence it is much more cost-efficient.

 In this paper, we exploit LLMs' internal states to enhance their knowledge boundary perception from efficiency and risk mitigation perspectives.